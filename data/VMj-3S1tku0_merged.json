{"video_id": "VMj-3S1tku0", "title": "The spelled-out intro to neural networks and backpropagation: building micrograd", "description": "This is the most step-by-step spelled-out explanation of backpropagation and training of neural networks. It only assumes basic knowledge of Python and a vague recollection of calculus from high school.\n\nLinks:\n- micrograd on github: https://github.com/karpathy/micrograd\n- jupyter notebooks I built in this video: https://github.com/karpathy/nn-zero-to-hero/tree/master/lectures/micrograd\n- my website: https://karpathy.ai\n- my twitter: https://twitter.com/karpathy\n- \"discussion forum\": nvm, use youtube comments below for now :)\n- (new) Neural Networks: Zero to Hero series Discord channel: https://discord.gg/Hp2m3kheJn , for people who'd like to chat more and go beyond youtube comments\n\nExercises:\nyou should now be able to complete the following google collab, good luck!:\nhttps://colab.research.google.com/drive/1FPTx1RXtBfc4MaTkf7viZZD4U2F9gtKN?usp=sharing\n\nChapters:\n00:00:00 intro\n00:00:25 micrograd overview\n00:08:08 derivative of a simple function with one input\n00:14:12 derivative of a function with multiple inputs\n00:19:09 starting the core Value object of micrograd and its visualization\n00:32:10 manual backpropagation example #1: simple expression\n00:51:10 preview of a single optimization step\n00:52:52 manual backpropagation example #2: a neuron\n01:09:02 implementing the backward function for each operation\n01:17:32 implementing the backward function for a whole expression graph\n01:22:28 fixing a backprop bug when one node is used multiple times\n01:27:05 breaking up a tanh, exercising with more operations\n01:39:31 doing the same thing but in PyTorch: comparison\n01:43:55 building out a neural net library (multi-layer perceptron) in micrograd\n01:51:04 creating a tiny dataset, writing the loss function\n01:57:56 collecting all of the parameters of the neural net\n02:01:12 doing gradient descent optimization manually, training the network\n02:14:03 summary of what we learned, how to go towards modern neural nets\n02:16:46 walkthrough of the full code of micrograd on github\n02:21:10 real stuff: diving into PyTorch, finding their backward pass for tanh\n02:24:39 conclusion\n02:25:20 outtakes :)", "author": "Andrej Karpathy", "keywords": ["neural", "network", "backpropagation", "lecture"], "channel_url": "https://www.youtube.com/channel/UCXUPKJO5MZQN11PqgIvyuvQ", "length": 8752, "views": 189364, "publish_date": "11/02/2022", "timestamp": 1660608000, "entity": "Andrew Kaparthy", "transcript": {"text": " Hello, my name is Andrei and I've been training deep neural networks for a bit more than a decade and in this lecture I'd like to show you what neural network training looks like under the hood So in particular, we are going to start with a blank two pair notebook And by the end of this lecture, we will define and train a neural net You'll get to see everything that goes on under the hood and exactly sort of how that works on an intuitive level Now specifically what I would like to do is I would like to take you through Building of micro grad now micro grad is this library that I released on github about two years ago But at the time I only uploaded the source code and you'd have to go and buy yourself and really Figure out how it works So in this lecture, I will take you through it step by step and kind of comment on all the pieces of it So what's micro grad and why is it interesting? Good Micro grad is basically an autograd engine autograd is short for automatic gradient And really what it does is it implements backpropagation Now backpropagation is this algorithm that allows you to efficiently evaluate the gradient of Some kind of a loss function with respect to the weights of a neural network And what that allows us to do then is we can iteratively tune the weights of that neural network To minimize the loss function and therefore improve the accuracy of the network So backpropagation would be at the mathematical core of any modern deep neural network library like say PyTorch or JAX So the functionality of micro grad is I think best illustrated by an example. So if we just scroll down here you'll see that micro grad basically allows you to build out mathematical expressions and Here what we are doing is we have an expression that we're building out where you have two inputs a and b And you'll see that a and b are negative four and two but we are wrapping those Values into this value object that we are going to build out as part of micro grad So this value object will wrap the numbers themselves And then we are going to build out a mathematical expression here where a and b are Transformed into c d and eventually e f and g And i'm showing some of the function some of the functionality of micro grad and the operations that it supports So you can add two value objects you can multiply them you can raise them to a constant power You can offset by one negate squash at zero Square divide by constant divide by it etc And so we're building out an expression graph with with these two inputs a and b and we're creating an output value of g And micro grad will in the background build out this entire mathematical expression. So it will for example know that c is also a value C was a result of an addition operation and the child nodes of c are a and b because the and it will maintain pointers to a and b value objects So we'll basically know exactly how all of this is laid out And then not only can we do what we call the forward pass where we actually look at the value of g, of course That's pretty straightforward. We will access that using the dot data attribute And so the output of the forward pass the value of g is twenty four point seven. It turns out But the big deal is that we can also take this G value object and we can call dot backward and this will basically initialize back propagation at the node g And what back propagation is going to do is it's going to start at g and it's going to go backwards through that expression graph and it's going to recursively apply the chain rule from calculus And what that allows us to do then is we're going to evaluate basically the derivative of g with respect to all the internal nodes Like e d and c but also with respect to the inputs a and b And then we can actually query this derivative of g with respect to a for example That's a dot grad in this case It happens to be one thirty eight and the derivative of g with respect to b which also happens to be here six hundred and forty five And this derivative we'll see soon is very important information because it's telling us how a and b Are affecting g through this mathematical expression. So in particular a dot grad is one thirty eight So if we slightly nudge a and make it slightly larger One thirty eight is telling us that g will grow and the slope of that growth is going to be one thirty eight And the slope of growth of b is going to be six hundred and forty five So that's going to tell us about how g will respond if a and b get tweaked at the same time Okay, um now you might be confused about what this expression is that we built out here And this expression by the way is completely meaningless. I just made it up I'm just flexing about the kinds of operations that are supported by micro grad What we actually really care about our neural networks, but it turns out that neural networks are just mathematical expressions Just like this one, but actually slightly bit less crazy even So we're going to look at the expression of g with respect to a and b to this one, but actually a slightly bit less crazy even Neural networks are just a mathematical expression They take the input data as an input and they take the weights of a neural network as an input And it's a mathematical expression and the output are your predictions of your neural net or the loss function We'll see this in a bit But basically neural networks just happen to be a certain class of mathematical expressions But back propagation is actually significantly more general. It doesn't actually care about neural networks at all it only tells about arbitrary mathematical expressions, and then we happen to use that machinery for training of neural networks. Now one more note I would like to make at this stage is that as you see here, micrograd is a scalar-valued autograd engine. So it's working on the, you know, level of individual scalars like negative 4 and 2, and we're taking neural nets and we're breaking them down all the way to these atoms of individual scalars and all the little pluses and times and it's just excessive. And so obviously we'd never be doing any of this in production. It's really just for them for pedagogical reasons because it allows us to not have to deal with these n-dimensional tensors that you would use in modern deep neural network library. So this is really done so that you understand and refactor out the background application and chain rule and understanding of neural network training. And then if you actually want to train bigger networks, you have to be using these tensors, but none of the math changes. This is done purely for efficiency. We are basically taking scalar value, all the scalar values, we're packaging them up into tensors, which are just arrays of these scalars. And then because we have these large arrays, we're making operations on those large arrays that allows us to take advantage of the parallelism in a computer and all those operations can be done in parallel and then the whole thing runs faster. But really none of the math changes and that's done purely for efficiency. So I don't think that it's pedagogically useful to be dealing with tensors from scratch. And I think and that's why I fundamentally wrote micrograd, because you can understand how things work at the fundamental level and then you can speed it up later. Okay, so here's the fun part. My claim is that micrograd is what you need to train neural networks and everything else is just efficiency. So you'd think that micrograd would be a very complex piece of code and that turns out to not be the case. So if we just go to micrograd and you'll see that there's only two files here in micrograd. This is the actual engine. It doesn't know anything about neural nets and this is the entire neural nets library on top of micrograd. So engine and nn.py. So the actual back propagation autograd engine that gives you the power of neural networks is literally 100 lines of code of like very simple Python, which we'll understand by the end of this lecture. And then nn.py, this neural network library built on top of the autograd engine, is like a joke. It's like we have to define what is a neuron and then we have to define what is a layer of neurons and then we define what is a multilayer perceptron, which is just a sequence of layers of neurons. And so it's just a total joke. So basically, there's a lot of power that comes from only 150 lines of code. And that's all you need to understand to understand neural network training and everything else is just efficiency. And of course, there's a lot to efficiency, but fundamentally, that's all that's happening. Okay, so now let's dive right in and implement micrograd step by step. The first thing I'd like to do is I'd like to make sure that you have a very good understanding intuitively of what a derivative is and exactly what information it gives you. So let's start with some basic imports that I copy paste in every Jupyter notebook always. And let's define a function, a scalar-valued function, f of x as follows. So I just made this up randomly. I just wanted to scale a valid function that takes a single scalar x and returns a single scalar y. And we can call this function, of course, so we can pass in, say, 3.0 and get 20 back. Now, we can also plot this function to get a sense of its shape. You can tell from the mathematical expression that this is probably a parabola. It's quadratic. And so if we just create a set of scalar values that we can feed in using, for example, a range from negative 5 to 5 in steps of 0.25. So this is so x is just from negative 5 to 5, not including 5 in steps of 0.25. And we can actually call this function on this numpy array as well. So we get a set of y's if we call f on x's and these y's are basically also applying the function on every one of these elements independently. And we can plot this using Matplotlib. So plt.plot, x's and y's, and we get a nice parabola. So previously here, we fed in 3.0 somewhere here, and we received 20 back, which is here, the y coordinate. So now I'd like to think through what is the derivative of this function at any single input point x. Right. So what is the derivative at different points x of this function? Now, if you remember back to your calculus class, you've probably derived derivatives. So we take this mathematical expression 3x squared minus 4x plus 5, and you would write out on a piece of paper and you would apply the product rule and all the other rules and derive the mathematical expression of the great derivative of the original function. And then you could plug in different x's and see what the derivative is. We're not going to actually do that because no one in neural networks actually writes out the expression for the neural net. It would be a massive expression. It would be thousands, tens of thousands of terms. No one actually derives the derivative, of course. And so we're not going to take this kind of symbolic approach. Instead, what I'd like to do is I'd like to look at the definition of derivative and just make sure that we really understand what derivative is measuring, what it's telling you about the function. And so if we just look up derivative. We see that this is not a very good definition of derivative. This is a definition of what it means to be differentiable. But if you remember from your calculus, it is the limit as h goes to zero of f of x plus h minus f of x over h. So basically what it's saying is if you slightly bump up your at some point x that you're interested in or a and if you slightly bump up, you slightly increase it by small number h. How does the function respond with what sensitivity does it respond? Where's the slope at that point? Does the function go up or does it go down? And by how much? And that's the slope of that function, the slope of that response at that point. And so we can basically evaluate the derivative here numerically by taking a very small h. Of course, the definition would ask us to take h to zero. We're just going to pick a very small h, 0.001. And let's say we're interested in 0.3.0. So we can look at f of x, of course, as 20. And now f of x plus h. So if we slightly notch x in a positive direction, how is the function going to respond? And just looking at this, do you expect f of x plus h to be slightly greater than 20 or do you expect it to be slightly lower than 20? And since this 3 is here and this is 20, if we slightly go positively, the function will respond positively. So you'd expect this to be slightly greater than 20. And by how much is telling you the sort of the strength of that slope, right? The size of that slope. So f of x plus h minus f of x. This is how much the function responded in a positive direction. And we have to normalize by the run. So we have the rise over run to get the slope. So this, of course, is just a numerical approximation of the slope because we have to make a very, very small to converge to the exact amount. Now, if I'm doing too many zeros, at some point I'm going to get an incorrect answer because we're using floating point arithmetic and the representations of all these numbers in computer memory is finite. And at some point we get into trouble. So we can converge towards the right answer with this approach. But basically, at 3, the slope is 14. And you can see that by taking 3x squared minus 4x plus 5 and differentiating it in our head. So 3x squared would be 6x minus 4. And then we plug in x equals 3. So that's 18 minus 4 is 14. So this is correct. So that's a 3. Now, how about the slope at, say, negative 3? Would you expect, what would you expect for the slope? Now, telling the exact value is really hard. But what is the sign of that slope? So at negative 3, if we slightly go in the positive direction at x, the function would actually go down. And so that tells you that the slope would be negative. So we'll get a slight number below 20. And so if we take the slope, we expect something negative 22. And at some point here, of course, the slope would be 0. Now, for this specific function, I looked it up previously, and it's at point 2 over 3. So at roughly 2 over 3, that's somewhere here, this derivative would be 0. So basically, at that precise point, if we nudge in a positive direction, the function doesn't respond. This stays the same almost. And so that's why the slope is 0. OK, now let's look at a bit more complex case. So we're going to start complexifying a bit. So now we have a function here with output variable d that is a function of three scalar inputs, a, b, and c. So a, b, and c are some specific values, three inputs into our expression graph, and a single output d. And so if we just print d, we get 4. And now what I'd like to do is I'd like to, again, look at the derivatives of d with respect to a, b, and c. And think through, again, just the intuition of what this derivative is telling us. So in order to evaluate this derivative, we're going to get a bit hacky here. We're going to, again, have a very small value of h. And then we're going to fix the inputs at some values that we're interested in. So this is the point a, b, c at which we're going to be evaluating the derivative of d with respect to all a, b, and c at that point. So there are the inputs, and now we have d1 is that expression. And then we're going to, for example, look at the derivative of d with respect to a. So we'll take a, and we'll bump it by h, and then we'll get d2 to be the exact same function. And now we're going to print, you know, f1, d1 is d1, d2 is d2, and print slope. So the derivative or slope here will be, of course, d2 minus d1 divided by h. So d2 minus d1 is how much the function increased when we bumped the specific input that we're interested in by a tiny amount. And this is the normalized by h to get the slope. So, yeah. So this, so I just run this. We're going to print d1, which we know is 4. Now d2 will be bumped, a will be bumped by h. So let's just think through a little bit what d2 will be printed out here. In particular, d1 will be 4, will d2 be a number slightly greater than 4 or slightly lower than 4? And it's going to tell us the sign of the derivative. So we're bumping a by h, b is minus 3, c is 10. So you can just intuitively think through this derivative and what it's doing. So a will be slightly more positive and b is a negative number. So if a is slightly more positive, because b is negative 3, we're actually going to be adding less to d. So you'd actually expect that the value of the function will go down. So let's just see this. And so we went from 4 to 3.9996, and that tells you that the slope will be negative and then will be a negative number because we went down. And then the exact number of slope will be, exact number of slope is negative 3. And you can also convince yourself that negative 3 is the right answer mathematically and analytically, because if you have a times b plus c and you are, you know, you have calculus, then differentiating a times b plus c with respect to a gives you just b. And indeed, the value of b is negative 3, which is the derivative that we have. So you can tell that that's correct. So now if we do this with b, so if we bump b by a little bit in a positive direction, we'd get different slopes. So what is the influence of b on the output d? So if we bump b by a tiny amount in a positive direction, then because a is positive, we'll be adding more to d. Right. So and now what is the what is the sensitivity? What is the slope of that addition? And it might not surprise you that this should be 2. And why is it 2? Because d of d by db, the ratio with respect to b, would be, would give us a and the value of a is 2. So that's also working well. And then if c gets bumped a tiny amount in h by h, then, of course, a times b is unaffected. And now c becomes slightly bit higher. What does that do to the function? It makes it slightly bit higher because we're simply adding c and it makes it slightly bit higher by the exact same amount that we added to c. And so that tells you that the slope is 1. That will be the rate at which d will increase as we scale c. OK, so we now have some intuitive sense of what this derivative is telling you about the function. And we'd like to move to neural networks. Now, as I mentioned, neural networks will be pretty massive expressions, mathematical expressions. So we need some data structures that maintain these expressions. And that's what we're going to start to build out now. So we're going to build out this value object that I showed you in the readme page of micro grad. So let me copy paste a skeleton of the first very simple value object. So class value takes a single scalar value that it wraps and keeps track of. And that's it. So we can, for example, do value of 2.0 and then we can get we can look at its content. And Python will internally use the repr function to return this string. So this is a value object with data equals 2 that we're creating here. Now we'd like to do is like we'd like to be able to have not just like two values, but we'd like to do a bluffy. Right. We'd like to add them. So currently you would get an error because Python doesn't know how to add two value objects. So we have to tell it. So here's addition. So you have to basically use these special double underscore methods in Python to define these operators for these objects. So if we call the if we use this plus operator, Python will internally call a dot add of B. That's what will happen internally. And so B will be the other and self will be a. And so we see that we're going to return as a new value object. And it's just it's going to be wrapping the plus of their data. But remember now, because data is the actual like numbered Python number. So this operator here is just the typical floating point plus addition. Now it's not an addition of value objects and will return a new value. So now a plus B should work and it should print value of negative one because that's two plus minus three. There we go. OK. Let's now implement multiply just so we can recreate this expression here. So multiply. I think it won't surprise you. Will be fairly similar. So instead of add, we're going to be using mole. And then here, of course, we want to do times. And so now we can create a C value object, which will be 10.0. And now we should be able to do a times B. Well, let's just do a times B first. That's value of negative six now. And by the way, I skipped over this a little bit. So suppose that I didn't have the repr function here, then it's just that you'll get some kind of an ugly expression. So what repr is doing is it's providing us a way to print out like a nicer looking expression in Python. So we don't just have something cryptic. We actually are, you know, it's value of negative six. So this gives us a times. And then this we should now be able to add C to it because we've defined and told the Python how to do mole and add. And so this will call this will basically be equivalent to a dot mole of B. And then this new value object will be dot add of C. And so let's see if that worked. Yep. So that worked well. That gave us four, which is what we expect from before. And I believe we can just call them manually as well. There we go. So. OK, so now what we are missing is the connective tissue of this expression. As I mentioned, we want to keep these expression graphs. So we need to know and keep pointers about what values produce what other values. So here, for example, we are going to introduce a new variable, which we'll call children. And by default, it will be an empty tuple. And then we're actually going to keep a slightly different variable in the class, which we'll call underscore prime, which will be the set of children. This is how I done. I did it in the original micro grad looking at my code here. I can't remember exactly the reason. I believe it was efficiency. But this underscore children will be a tuple for convenience. But then when we actually maintain it in the class, it will be just the set. I believe for efficiency. So now when we are creating a value like this with a constructor, children will be empty and prep will be the empty set. But when we are creating a value through addition or multiplication, we're going to feed in the children of this value, which in this case is self and other. So those are the children here. So now we can do the dot prep and we'll see that the children of the we now know are this value of negative six and value of 10. And this, of course, is the value resulting from a times B and the C value, which is 10. Now, the last piece of information we don't know. So we know now the children of every single value, but we don't know what operation created this value. So we need one more element here. Let's call it underscore pop. And by default, this is the empty set for leaves. And then we'll just maintain it here. And now the operation will be just a simple string. And in the case of addition, it's plus in the case of multiplication is times. So now we not just have data prep. We also have a D.up. And we know that D was produced by an addition of those two values. And so now we have the full mathematical expression and we're building out this data structure. And we know exactly how each value came to be by what expression and from what other values. Now, because these expressions are about to get quite a bit larger, we'd like a way to nicely visualize these expressions that we're building out. So for that, I'm going to copy paste a bunch of slightly scary code that's going to visualize this expression graphs for us. So here's the code, and I'll explain it in a bit. But first, let me just show you what this code does. Basically, what it does is it creates a new function draw dot that we can call on some root node and then it's going to visualize it. So if we call draw dot on D, which is this final value here, that is a times people see it create something like this. So this is D. And you see that this is a times B creating an interpret value plus C gives us this output node D. So that's drawn out of the and I'm not going to go through this in complete detail. You can take a look at graphics and its API graphics is a open source graphics visualization software. And what we're doing here is we're building out this graph in graphics API. And you can basically see that trace is this helper function that enumerates all the nodes and edges in the graph. So that just builds a set of all the nodes and edges. And then we iterate through all the nodes and we create special node objects for them in using dot node. And then we also create edges using dot dot edge. And the only thing that's slightly tricky here is you'll notice that I basically add these fake nodes, which are these operation nodes. So, for example, this node here is just like a plus node. And I create these special nodes here and I connect them accordingly. So these nodes, of course, are not actual nodes in the original graph. They're not actually a value object. The only value objects here are the things in squares. Those are actual value objects or representations thereof. And these up nodes are just created in this draw dot routine so that it looks nice. Let's also add labels to these graphs just so we know what variables are where. So let's create a special underscore label or let's just do label equals empty by default and save it in each node. And then here we're going to do label is a label is the label is C. And then let's create a special equals a times B and double label will be. It's kind of naughty and E will be E plus C and a D dot label will be B. OK, so nothing really changes. I just added this new E function, new E variable. And then here, when we are printing this, I'm going to print the label here. So this will be a percent as bar and this will be end up label. And so now we have the label on the left here. So this is a B creating me and then E plus C creates D just like we have it here. And finally, let's make this expression just one layer deeper. So D will not be the final output node. Instead, after D, we are going to create a new value object called F. We're going to start running out of variable soon. F will be negative two point zero and its label will, of course, just be F. And then L, capital L, will be the output of our graph. And L will be D times F. So L will be negative eight is the output. So now we don't just draw a D, we draw L. And somehow the label of L is undefined. L.label has to be explicitly given to it. There we go. So L is the output. So let's quickly recap what we've done so far. We are able to build out mathematical expressions using only plus and times so far. They are scalar valued along the way. And we can do this forward pass and build out a mathematical expression. So we have multiple inputs here, A, B, C, and F, going into a mathematical expression that produces a single output L. And this here is visualizing the forward pass. So the output of the forward pass is negative eight. That's the value. Now, what we'd like to do next is we'd like to run backpropagation. And in backpropagation, we are going to start here at the end, and we're going to reverse and calculate the gradient along all these intermediate values. And really what we're computing for every single value here, we're going to compute the derivative of that node with respect to L. So the derivative of L with respect to L is just one. And then we're going to derive what is the derivative of L with respect to F, with respect to D, with respect to C, with respect to E, with respect to B, and with respect to A. And in a neural network setting, you'd be very interested in the derivative of basically this loss function L with respect to the weights of a neural network. And here, of course, we have just these variables A, B, C, and F, but some of these will eventually represent the weights of a neural net. And so we'll need to know how those weights are impacting the loss function. So we'll be interested basically in the derivative of the output with respect to some of its leaf nodes. And those leaf nodes will be the weights of the neural net. And the other leaf nodes, of course, will be the data itself. But usually we will not want or use the derivative of the loss function with respect to data because the data is fixed. But the weights will be iterated on using the gradient information. So next, we are going to create a variable inside the value class that maintains the derivative of L with respect to that value. And we will call this variable grad. So there is a dot data and there's a self dot grad. And initially, it will be zero. And remember that zero is basically means no effect. So at initialization, we're assuming that every value does not impact, does not affect the output. Right. Because if the gradient is zero, that means that changing this variable is not changing the loss function. So by default, we assume that the gradient is zero. And then now that we have grad and it's zero point zero, we are going to be able to visualize it here after data. So here grad is point four F. And this will be in that crowd. And now we are going to be showing both the data and the grad initialize that zero. And we are just about getting ready to calculate the back propagation. And of course, this grad again, as I mentioned, is representing the derivative of the output in this case L with respect to this value. So with respect to. So this is the derivative of L with respect to F with respect to D and so on. So let's now fill in those gradients and actually do back propagation manually. So let's start filling in these gradients and start all the way at the end, as I mentioned here. First, we are interested to fill in this gradient here. So what is the derivative of L with respect to L? In other words, if I change L by a tiny amount H. How much does L change? It changes by H. So it's proportional and therefore the derivative will be one. We can, of course, measure these or estimate these numerical gradients numerically, just like we've seen before. So if I take this expression and I create a def lol function here and put this here. The reason I'm creating a gating function lol here is because I don't want to pollute or mess up the global scope here. This is just kind of like a little staging area. And as you know, in Python, all of these will be local variables to this function. So I'm not changing any of the global scope here. So here, L1 will be L. And then copy pasting this expression, we're going to add a small amount H. For example, a right. And this would be measuring the derivative of L with respect to a. So here this will be L2. And then we want to print that derivative. So print L2 minus L1, which is how much L changed and then normalize it by H. So this is the rise over run. And we have to be careful because L is a value node. So we actually want its data so that these are floats divided by H. And this should print the derivative of L with respect to a, because a is the one that we bumped a little bit by H. So what is the derivative of L with respect to a? It's six. OK, and obviously, if we change L by H, then that would be here effectively. This looks really awkward, but changing L by H, you see the derivative here is one. That's kind of like the base case of what we are doing here. So basically, we can come up here and we can manually set L.grad to one. This is our manual backpropagation. L.grad is one. And let's redraw. And we'll see that we filled in grad is one for L. We're now going to continue the backpropagation. So let's here look at the derivatives of L with respect to D and F. Let's do D first. So what we are interested in, if I create a markdown on here, is we'd like to know, basically, we have that L is D times F. And we'd like to know what is DL by DD. What is that? And if you know your calculus, L is D times F. So what is DL by DD? It would be F. And if you don't believe me, we can also just derive it because the proof would be fairly straightforward. We go to the definition of the derivative, which is F of X plus H minus F of X divide H. As a limit, limit of H goes to zero of this kind of expression. So when we have L is D times F, then increasing D by H would give us the output of D plus H times F. That's basically F of X plus H, right? Minus D times F. And then divide H. And symbolically, expanding out here, we would have basically D times F plus H times F minus D times F divide H. And then you see how the DF minus DF cancels. So you're left with H times F divide H, which is F. So in the limit as H goes to zero of, you know, derivative definition, we just get F in the case of D times F. So symmetrically, DL by DF will just be D. So what we have is that F dot grad we see now is just the value of D, which is four. And we see that D dot grad is just the value of F. And so the value of F is negative two. So we'll set those manually. Let me erase this markdown node and then let's redraw what we have. OK. And let's just make sure that these were correct. So we seem to think that DL by DD is negative two. So let's double check. Let me erase this plus H from before. And now we want the derivative with respect to F. So let's just come here when I create F and let's do a plus H here. And this should print a derivative of L with respect to F. So we expect to see four. Yeah. And this is four up to floating point funkiness. And then DL by DD should be F, which is negative two. Grad is negative two. So if we again come here and we change D. D dot data plus equals H right here. So we expect so we've added a little H and then we see how L changed. And we expect to print negative two. There we go. So we've numerically verified. What we're doing here is kind of like an inline gradient check. Gradient check is when we are deriving this like backpropagation and getting the derivative with respect to all the intermediate results. And then numerical gradient is just estimating it using small step size. Now we're getting to the crux of backpropagation. So this will be the most important node to understand. Because if you understand the gradient for this node, you understand all of backpropagation and all of training of neural nets basically. So we need to derive DL by DC. In other words, the derivative of L with respect to C. Because we've computed all these other gradients already. Now we're coming here and we're continuing the backpropagation manually. So we want DL by DC and then we'll also derive DL by DE. Now here's the problem. How do we derive DL by DC? We actually know the derivative L with respect to D. So we know how L is sensitive to D. But how is L sensitive to C? So if we wiggle C, how does that impact L through D? So we know DL by DC. And we also here know how C impacts D. And so just very intuitively, if you know the impact that C is having on D and the impact that D is having on L, then you should be able to somehow put that information together to figure out how C impacts L. And indeed this is what we can actually do. So in particular, we know, just concentrating on D first, let's look at how, what is the derivative basically of D with respect to C. So in other words, what is DD by DC? So here we know that D is C plus E. That's what we know. And now we're interested in DD by DC. If you just know your calculus again and you remember that differentiating C plus E with respect to C, you know that that gives you 1.0. And we can also go back to the basics and derive this. Because again, we can go to our f of x plus h minus f of x divided by h. That's the definition of a derivative as h goes to zero. And so here, focusing on C and its effect on D, we can basically do the f of x plus h will be C is incremented by h plus C. That's the first evaluation of our function, minus C plus E. And then divide h. And so what is this? Just expanding this out, this will be C plus h plus E minus C minus E divided by h. And then you see here how C minus C cancels, E minus E cancels. We're left with h over h, which is 1.0. And so by symmetry also, DD by DE will be 1.0 as well. So basically, the derivative of a sum expression is very simple. And this is the local derivative. So I call this the local derivative because we have the final output value all the way at the end of this graph. And we're now like a small node here. And this is a little plus node. And the little plus node doesn't know anything about the rest of the graph that it's embedded in. All it knows is that it did a plus. It took a C and an E, added them, and created a D. And this plus node also knows the local influence of C on D, or rather the derivative of D with respect to C. And it also knows the derivative of D with respect to E. But that's not what we want. That's just a local derivative. What we actually want is DL by DC. And L is here just one step away. But in a general case, this little plus node could be embedded in a massive graph. So again, we know how L impacts D. And now we know how C and E impact D. How do we put that information together to write DL by DC? And the answer, of course, is the chain rule in calculus. And so I pulled up a chain rule here from Wikipedia. And I'm going to go through this very briefly. So chain rule, Wikipedia sometimes can be very confusing, and calculus can be very confusing. This is the way I learned chain rule, and it was very confusing. What is happening? It's just complicated. So I like this expression much better. If a variable z depends on a variable y, which itself depends on a variable x, then z depends on x as well, obviously, through the intermediate variable y. And in this case, the chain rule is expressed as, if you want dz by dx, then you take the dz by dy, and you multiply it by dy by dx. So the chain rule fundamentally is telling you how we chain these derivatives together correctly. So to differentiate through a function composition, we have to apply a multiplication of those derivatives. So that's really what chain rule is telling us. And there's a nice little intuitive explanation here, which I also think is kind of cute. The chain rule states that knowing the instantaneous rate of change of z with respect to y, and y relative to x allows one to calculate the instantaneous rate of change of z relative to x as a product of those two rates of change, simply the product of those two. So here's a good one. If a car travels twice as fast as a bicycle, and the bicycle is four times as fast as a walking man, then the car travels two times four, eight times as fast as a man. And so this makes it very clear that the correct thing to do, sort of, is to multiply. So car is twice as fast as a bicycle, and bicycle is four times as fast as man. So the car will be eight times as fast as the man. And so we can take these intermediate rates of change, if you will, and multiply them together. And that justifies the chain rule intuitively. So have a look at chain rule. But here, really what it means for us is there's a very simple recipe for deriving what we want, which is dl by dc. And what we have so far is we know what and we know what is the impact of d on l. So we know dl by dd, the derivative of l with respect to dd. We know that that's negative 2. And now, because of this local reasoning that we've done here, we know dd by dc. So how does c impact d? And in particular, this is a plus node. So the local derivative is simply 1.0. It's very simple. And so the chain rule tells us that dl by dc, going through this intermediate variable, will just be simply dl by dd times dd by dc. That's chain rule. So this is identical to what's happening here, except z is rl, y is rd, and x is rc. So we literally just have to multiply these. And because these local derivatives, like dd by dc, are just 1, we basically just copy over dl by dd, because this is just times 1. So because dl by dd is negative 2, what is dl by dc? Well, it's the local gradient, 1.0, times dl by dd, which is negative 2. So literally, what a plus node does, you can look at it that way, is it literally just routes the gradient, because the plus node's local derivatives are just 1. And so in the chain rule, 1 times dl by dd is just dl by dd. And so that derivative just gets routed to both c and to e in this case. So basically, we have that e dot grad, or let's start with c, since that's the one we looked at, is negative 2 times 1, negative 2. And in the same way, by symmetry, e dot grad will be negative 2. That's the claim. So we can set those, we can redraw, and you see how we just assign negative 2, negative 2? So this backpropagating signal, which is carrying the information of, like, what is the derivative of L with respect to all the intermediate nodes, we can imagine it almost like flowing backwards through the graph, and a plus node will simply distribute the derivative to all the children nodes of it. So this is the claim, and now let's verify it. So let me remove the plus h here from before. And now instead what we're going to do is we want to increment c. So c dot data will be incremented by h, and when I run this, we expect to see negative 2. And then of course for e, so e dot data plus equals h, and we expect to see negative 2. Simple. So those are the derivatives of these internal nodes. And now we're going to recurse our way backwards again, and we're again going to apply the chain rule. So here we go, our second application of chain rule, and we will apply it all the way through the graph, which just happened to only have one more node remaining. We have that dl by de, as we have just calculated, is negative 2. So we know that. So we know the derivative of l with respect to e. And now we want dl by da, right? And the chain rule is telling us that that's just dl by de, negative 2, times the local gradient. So what is the local gradient? Basically, de by da. We have to look at that. So I'm a little times node inside a massive graph, and I only know that I did a times b, and I produced an e. So now what is de by da and de by dv? That's the only thing that I sort of know about. That's my local gradient. So because we have that e is a times b, we're asking what is de by da? And of course, we just did that here. We had a times, so I'm not going to re-derive it. But if you want to differentiate this with respect to a, you'll just get b, right? The value of b, which in this case is negative 3.0. So basically, we have that dl by da. Well, let me just do it right here. We have that a dot grad, and we are applying chain rule here, is dl by de, which we see here is negative 2, times what is de by da? It's the value of b, which is negative 3. That's it. And then we have b dot grad, is again, dl by de, which is negative 2, just the same way, times what is de by db? It's the value of a, which is 2.0. That's the value of a. So these are our claimed derivatives. Let's redraw. And we see here that a dot grad turns out to be 6, because that is negative 2 times negative 3. And b dot grad is negative 2 times 2, which is negative 4. So those are our claims. Let's delete this and let's verify them. We have a here, a dot data plus equals h. So the claim is that a dot grad is 6. Let's verify. 6. And we have b dot data plus equals h. So nudging b by h and looking at what happens, we claim it's negative 4. And indeed, it's negative 4, plus minus, again, float, oddness. And that's it. That was the manual backpropagation all the way from here to all the delete nodes. And we've done it piece by piece. And really, all we've done is, as you saw, we iterated through all the nodes one by one and locally applied the chain rule. We always know what is the derivative of L with respect to this little output. And then we look at how this output was produced. This output was produced through some operation. And we have the pointers to the children nodes of this operation. And so in this little operation, we know what the local derivatives are and we just multiply them onto the derivative always. So we just go through and recursively multiply on the local derivatives. And that's what backpropagation is. It's just a recursive application of chain rule backwards through the computation graph. Let's see this power in action just very briefly. What we're going to do is we're going to nudge our inputs to try to make L go up. So in particular, what we're doing is we want a dot data. We're going to change it. And if we want L to go up, that means we just have to go in the direction of the gradient. So A should increase in the direction of gradient by like some small step amount. This is the step size. And we don't just want this for B, but also for B. Also for C. Also for F. Those are leaf nodes, which we usually have control over. And if we nudge in direction of the gradient, we expect a positive influence on L. So we expect L to go up positively. So it should become less negative. It should go up to say negative six or something like that. It's hard to tell exactly. And we have to rerun the forward pass. So let me just do that here. This would be the forward pass. F would be unchanged. This is effectively the forward pass. And now if we print L.data, we expect because we nudged all the values, all the inputs in the direction of gradient, we expected less negative L. We expected to go up. So maybe it's negative six or so. Let's see what happens. Okay, negative seven. And this is basically one step of an optimization that will end up running. And really, this gradient just give us some power because we know how to influence the final outcome. And this will be extremely useful for training neural nets as well as CMC. So now I would like to do one more example of manual backpropagation using a bit more complex and useful example. We are going to backpropagate through a neuron. So we want to eventually build up neural networks. And in the simplest case, these are multilayer perceptrons, as they're called. So this is a two layer neural net. And it's got these little layers made up of neurons. And these neurons are fully connected to each other. Now, biologically, neurons are very complicated devices, but we have very simple mathematical models of them. And so this is a very simple mathematical model of a neuron. You have some inputs, axis, and then you have these synapses that have weights on them. So the W's are weights. And then the synapse interacts with the input to this neuron multiplicatively. So what flows to the cell body of this neuron is W times X. But there's multiple inputs. So there's many W times X's flowing to the cell body. The cell body then has also like some bias. So this is kind of like the inner innate sort of trigger happiness of this neuron. So this bias can make it a bit more trigger happy or a bit less trigger happy, regardless of the input. But basically, we're taking all the W times X of all the inputs, adding the bias, and then we take it through an activation function. And this activation function is usually some kind of a squashing function like a sigmoid or 10H or something like that. So as an example, we're going to use the 10H in this example. NumPy has a NP dot 10H. So we can call it on a range and we can plot it. This is the 10H function. And you see that the inputs as they come in get squashed on the white coordinate here. So right at zero, we're going to get exactly zero. And then as you go more positive in the input, then you'll see that the function will only go up to one and then plateau out. And so if you pass in very positive inputs, we're going to cap it smoothly at one. And on the negative side, we're going to cap it smoothly to negative one. So that's 10H. And that's the squashing function or an activation function. And what comes out of this neuron is just the activation function applied to the dot product of the weights and the inputs. So let's write one out. I'm going to copy paste because I don't want to type too much. But OK, so here we have the inputs X1, X2. So this is a two dimensional neuron. So two inputs are going to come in. These are thought of as the weights of this neuron, weights W1, W2. And these weights, again, are the synaptic strings for each input. And this is the bias of the neuron B. And now what we want to do is, according to this model, we need to multiply X1 times W1 and X2 times W2. And then we need to add bias on top of it. And it gets a little messy here, but all we are trying to do is X1 W1 plus X2 W2 plus B. And these are multiplied here, except I'm doing it in small steps so that we actually have pointers to all these intermediate nodes. So we have X1 W1 variable, X times X2 W2 variable, and I'm also labeling them. So N is now the cell body raw activation without the activation function for now. And this should be enough to basically plot it. So draw a dot of N gives us X1 times W1, X2 times W2 being added. Then the bias gets added on top of this. And this N is this sum. So we're now going to take it through an activation function. And let's say we use the tanh so that we produce the output. So what we'd like to do here is we'd like to do the output and I'll call it O is N dot tanh. OK, but we haven't yet written the tanh. Now, the reason that we need to implement another tanh function here is that tanh is a hyperbolic function. And we've only so far implemented a plus and a times and you can't make a tanh out of just pluses and times. You also need exponentiation. So tanh is this kind of a formula here. You can use either one of these. And you see that there is exponentiation involved, which we have not implemented yet for our low value node here. So we're not going to be able to produce tanh yet. And we have to go back up and implement something like it. Now, one option here is we could actually implement exponentiation. And we could return the exp of a value instead of a tanh of a value, because if we had exp, then we have everything else that we need. So because we know how to add and we know how to we know how to add and we know how to multiply. So we'd be able to create tanh if we knew how to exp. But for the purposes of this example, I specifically wanted to show you that we don't necessarily need to have the most atomic pieces in in this value object. We can actually like create functions at arbitrary points of abstraction. They can be complicated functions, but they can be also very, very simple functions like a plus. And it's totally up to us. The only thing that matters is that we know how to differentiate through any one function. So we take some inputs and we make an output. The only thing that matters can be arbitrarily complex function. As long as you know how to create the local derivative, if you know the local derivative of how the inputs impact the output, then that's all you need. So we're going to cluster up all of this expression and we're not going to break it down to its atomic pieces. We're just going to directly implement tanh. So let's do that. Depth and h and then out will be a value of and we need this expression here. So let me actually copy paste. Let's grab n, which is a salt data. And then this, I believe, is the tanh math dot exp of. Two, no, and minus one over two and plus one. Maybe I can call this X. Just so that it matches exactly. OK, and now this will be T. And children of this node, there's just one child and I'm wrapping it in a tuple. So this is a tuple of one object, just self. And here the name of this operation will be ten each and we're going to return that. OK, so now values should be implementing ten each and now we can scroll all the way down here and we can actually do and dot ten each. And that's going to return the ten aged output of. And now we should be able to draw it out of, oh, not of. So let's see how that worked. There we go. And went through ten age to produce this up. So now ten age is a sort of our little micro grad supported node here as an operation. And as long as we know derivative of ten age, then we'll be able to back propagate through it. Now, let's see this ten age in action. Currently, it's not squashing too much because the input to it is pretty low. So the bias was increased to say eight. Then we'll see that what's flowing into the ten age now is two and ten age is squashing it to point nine six. So we're already hitting the tail of this ten age and it will sort of smoothly go up to one and then plateau out over there. OK, so I'm going to do something slightly strange. I'm going to change this bias from eight to this number six point eight eight, et cetera. And I'm going to do this for specific reasons because we're about to start back propagation. And I want to make sure that our numbers come out nice. They're not like very crazy numbers. They're nice numbers that we can sort of understand in our head. Let me also add both label. Oh, it's short for output here. So that's your OK. So point eight flows into ten age comes out point seven. So now we're going to do back propagation and we're going to fill in all the greens. So what is the derivative? Oh, with respect to all the inputs here. And of course, in a typical neural network setting, what we really care about the most is the derivative of these neurons on the weights, specifically the W two and W one, because those are the weights that we're going to be changing part of the optimization. And the other thing that we have to remember is here we have only a single neuron, but in the neural net, you typically have many neurons and they're connected. So this is only like one small neuron, a piece of a much bigger puzzle. And eventually there's a loss function that sort of measures the accuracy of the neural net. And we're back propagating with respect to that accuracy and trying to increase it. So let's start off back propagation here in the end. What is the derivative of O with respect to O? The base case sort of we know always is that the gradient is just one point zero. So let me fill it in and then let me split out the drawing function here. And then here, cell clear this output here. So now when we draw O, we'll see that O is one. So now we're going to back propagate through the 10 H. So to back propagate through 10 H, we need to know the local derivative of 10 H. So if we have that O is 10 H of N, then what is D O by D N? Now, what you could do is you could come here and you could take this expression and you could do your calculus derivative taking. And that would work. But we can also just scroll down Wikipedia here into a section that hopefully tells us that derivative D by D X of 10 H of X is any of these. I like this one one minus 10 H square of X. So this is one minus 10 H of X squared. So basically what this is saying is that D O by D N is one minus 10 H of N squared. And we already have 10 H of N. It's just O. So it's one minus O squared. So O is the output here. So the output is this number. O dot data is this number. And then what this is saying is that D O by D N is one minus this squared. So one minus O dot data squared is point five conveniently. So the local derivative of this 10 H operation here is point five. And so that would be D O by D N. So we can fill in that N dot grad. Is point five. We'll just fill it in. So this is exactly point five point half. So now we're going to continue the back propagation. This is point five and this is a plus node. So how is back prop going to what is that going to do here? And if you remember our previous example, a plus is just a distributor of gradient. So this gradient will simply flow to both of these equally. And that's because the local derivative of this operation is one for every one of its nodes. So one times point five is point five. So therefore we know that this node here, which we called this, its grad is just point five. And we know that B dot grad is also point five. So let's set those and let's draw. So those are point five. Continuing, we have another plus point five again. We'll just distribute. So point five will flow to both of these so we can set theirs. So this is point five plus two W two as well grad is point five. And let's redraw. Pluses are my favorite operations to back propagate through because it's very simple. So now what's flowing into these expressions is point five. And so really, again, keep in mind what the derivative is telling us at every point in time along here. If we want the output of this neuron to increase, then the influence on these expressions is positive on the output. Both of them are positive contribution to the output. So now back propagating to X one W two first. This is a times node. So we know that the local derivative is the other term. So if we want to calculate X two dot grad, then can you think through what it's going to be? So extra grad will be W two dot data times. This X two W two grad. And W two that grad will be. X two that data times X two W two that grad. Right. So that's the local piece of chain rule. Let's set them and let's redraw. So here we see that the gradient on our weight two is zero because X two's data was zero. Right. But X two will have the green point five because data here was one. And so what's interesting here is because the input X two was zero, then because of the way the times works, of course, this gradient will be zero. And think about intuitively why that is. Derivative always tells us the influence of this on the final output. If I wiggle W two, how is the output changing? It's not changing because we're multiplying by zero. So because it's not changing, there is no derivative and zero is the correct answer. Because we're squashing that zero. And let's do it here. Point five should come here and flow through this times. And so we'll have that X one that grad is. Can you think through a little bit what this should be? Local derivative of times with respect to X one is going to be W one. W one's data times X one W one grad. And W one that grad will be X one that data times X one W two W one grad. Let's see what those came out to be. So this is point five. So this would be negative one point five. And this would be one. And we back propagated through this expression. These are the actual final derivatives. So if we want this neurons output to increase, we know that what's necessary is that W two, we have no gradient. W two doesn't actually matter to this neuron right now. But this neuron, this weight should go up. So this weight goes up, then this neurons output would have gone up and proportionally because the gradient is one. OK, so doing the back propagation manually is obviously ridiculous. So we are now going to put an end to this suffering and we're going to see how we can implement the backward pass a bit more automatically. We're not going to be doing all of it manually out here. It's now pretty obvious to us by example how these pluses and times are back propagating gradients. So let's go up to the value object and we're going to start codifying what we've seen in the examples below. So we're going to do this by storing a special self dot backward and underscore backward. And this will be a function which is going to do that little piece of chain rule at each little node that took inputs and produced output. We're going to store how we are going to chain the outputs gradient into the inputs gradients. So by default, this will be a function that doesn't do anything. So you can also see that here in the value in micro grad. So with this backward function by default doesn't do anything. This is an empty function. And that would be sort of the case, for example, for a leaf node. For leaf node, there's nothing to do. But now if when we're creating these out values, these out values are an addition of self and other. And so we will want to sell set outs backward to be the function that propagates the gradient. So let's define what should happen. And we're going to store it in a closure. Let's define what should happen when we call out grad. For addition, our job is to take out grad and propagate it into self grad and other grad. So basically we want to sell self grad to something and we want to set others grad to something. And the way we saw below how chain rule works, we want to take the local derivative times the sort of global derivative, I should call it, which is the derivative of the final output of the expression with respect to outs data. Respect to out. So the local derivative of self in an addition is one point zero. So it's just one point zero times outs grad. That's the chain rule. And others that grad will be one point zero times grad. And what you basically what you're seeing here is that outs grad will simply be copied onto self grad and others grad, as we saw happens for an addition operation. So we're going to later call this function to propagate the gradient, having done an addition. Let's not do multiplication. We're going to also define that backward. And we're going to set its backward to be backward. And we want to chain out grad into self grad and others grad. And this will be a little piece of chain rule for multiplication. So we'll have. So what should this be? Can you think through? So what is the local derivative here? The local derivative was others that data and then others that data and the times out grad. That's chain rule. And here we have self that data times out grad. That's what we've been doing. And finally, here for 10h, that backward. And then we want to set outs backwards to be just backward. And here we need to back propagate. We have out grad and we want to chain it into salt grad. And salt grad will be the local derivative of this operation that we've done here, which is 10h. And so we saw that the local gradient is one minus the 10h of x squared, which here is t. That's the local derivative because that's t is the output of this 10h. So one minus t squared is the local derivative. And then gradient has to be multiplied because of the chain rule. So out grad is chained through the local gradient into salt grad. And that should be basically it. So we're going to redefine our value node. We're going to swing all the way down here. And we're going to redefine our expression, make sure that all the grads are zero. OK, but now we don't have to do this manually anymore. We are going to basically be calling the dot backward in the right order. So first, we want to call out that backward. So O was the outcome of 10h. Right. So calling those backward will be this function. This is what it will do. Now we have to be careful because there's a times out dot grad. And out dot grad, remember, is initialized to zero. So here we see grad zero. So as a base case, we need to set oath dot grad to 1.0 to initialize this with one. And then once this is one, we can call out that backward. And what that should do is it should propagate this grad through 10h. So the local derivative times the global derivative, which is initialized at one. So this should. So I thought about redoing it, but I figured I should just leave the error in here because it's pretty funny. Why is not an object not callable? It's because I screwed up. We're trying to save these functions. So this is correct. This here, you don't want to call the function because that returns none. These functions return none. We just want to store the function. So let me redefine the value object. And then we're going to come back in, redefine the expression, draw dot. Everything is great. Oh, that grad is one, oh, that grad is one. And now, now this should work, of course. OK, so all that backward should have this grant should now be point five if we redraw and everything went correctly. Point five. Yay. OK, so now we need to call and that grad. And that's accurate. Sorry. And backward. So that seems to have worked. So instead of backward routed the gradient to both of these. So this is looking great. Now we can, of course, called called B grad B backward. Sorry. What's going to happen? Well, B doesn't have a backward B backward because B is a leaf node. Be backward is binationalization, the empty function. So nothing would happen. But we can call call it on it. But when we call this one backwards, then we expect this point five to get further routed. Right. So there we go. Point five point five. And then finally, we want to call it here on X to W to. And on X one W one. Let's do both of those. And there we go. So we get zero point five negative one point five and one exactly as we did before. But now we've done it through calling that backward sort of manually. So we have one last piece to get rid of, which is us calling underscore backward manually. So let's think through what we are actually doing. We've laid out a mathematical expression and now we're trying to go backwards through that expression. So going backwards through the expression just means that we never want to call a dot backward for any node before we've done sort of everything after it. So we have to do everything after it before ever going to call that backward on any one node. We have to get all of its full dependencies. Everything that it depends on has to propagate to it before we can continue back propagation. So this ordering of graphs can be achieved using something called topological sort. So topological sort is basically a laying out of a graph such that all the edges go only from left to right, basically. So here we have a graph, a direction acyclic graph, a DAG. And this is two different topological orders of it, I believe, where basically you'll see that it's a laying out of the nodes such that all the edges go only one way from left to right. And implementing topological sort, you can look in Wikipedia and so on. I'm not going to go through it in detail. But basically, this is what builds a topological graph. We maintain a set of visited nodes and then we are going through starting at some root node, which for us is O. That's where we want to start the topological sort. And starting at O, we go through all of its children and we need to lay them out from left to right. And basically, this starts at O. If it's not visited, then it marks it as visited. And then it iterates through all of its children and calls build topological on them. And then after it's gone through all the children, it adds itself. So basically, this node that we're going to call it on, like say O, is only going to add itself to the topo list after all of the children have been processed. And that's how this function is guaranteed that you're only going to be in the list once all your children are in the list. And that's the invariant that is being maintained. So if we build topo on O and then inspect this list, we're going to see that it ordered our value objects. And the last one is the value of 0.7 and 0.7, which is the output. So this is O and then this is N. And then all the other nodes get laid out before it. So that builds the topological graph. And really what we're doing now is we're just calling that underscore backward on all of the nodes in a topological order. So if we just reset the gradients, they're all zero. What did we do? We started by setting O.grad to be one. That's the base case. And then we built a topological order and then we went for node in reversed of topo. Now in the reverse order, because this list goes from, you know, we need to go through it in reversed order. So starting at O, node backward. And this should be it. There we go. Those are the correct derivatives. Finally, we are going to hide this functionality. So I'm going to copy this and we're going to hide it inside the value class because we don't want to have all that code lying around. So instead of an underscore backward, we're now going to define an actual backward. So that backward without the underscore and that's going to do all the stuff that we just arrived. So let me just clean this up a little bit. So we're first going to build a topological graph starting at self. So build topo of self will populate the topological order into the topo list, which is a local variable. Then we set self that grad to be one and then for each node in the reversed list. So starting at us and going to all the children underscore backward. And that should be it. So save. Come down here. We define. OK, all the grads are zero. And now we can do is backward without the underscore and. There we go. And that's that's back propagation. Please for one neuron. We shouldn't be too happy with ourselves, actually, because we have a bad bug and we have not surfaced the bug because of some specific conditions that we are. We have to think about right now. So here's the simplest case that shows the bug. Say I create a single node a. And then I create a B that is a plus a. And then I called backward. So what's going to happen is a is three and then a B is a plus a. So there's two arrows on top of each other here. Then we can see that B is, of course, the forward pass works. B is just a plus a, which is six. But the gradient here is not actually correct that we calculated automatically. And that's because, of course, just doing calculus in your head, the derivative of B with respect to a should be two. One plus one. It's not one. Intuitively, what's happening here, right? B is the result of a plus a and then we called backward on it. So let's go up and see what that does. B is a result of addition. So out as B. And then when we called backward, what happened is self that grad was set to one and then other that grad was set to one. But because we're doing a plus a self and other are actually the exact same object. So we are overriding the gradient. We are setting it to one and then we are setting it again to one. And that's why it stays at one. So that's a problem. There's another way to see this in a little bit more complicated expression. So here we have a and B and then D will be the multiplication of the two and E will be the addition of the two. And then we multiply E times D to get F and then we called backward. And these gradients, if you check, will be incorrect. So fundamentally, what's happening here again is basically we're going to see an issue any time we use a variable more than once. Until now, in these expressions above, every variable is used exactly once. So we didn't see the issue. But here, if a variable is used more than once, what's going to happen during backward pass? We're back propagating from F to E to D. So far, so good. But now equals it backward and it deposits its gradients to A and B. But then we come back to D and called backward and it overrides those gradients at A and B. So that's obviously a problem. And the solution here, if you look at the multivariate case of the chain rule and its generalization there, the solution there is basically that we have to accumulate these gradients, these gradients add. And so instead of setting those gradients, we can simply do plus equals. We need to accumulate those gradients. Plus equals, plus equals, plus equals, plus equals. And this will be OK, remember, because we are initializing them at zero. So they start at zero. And then any contribution that flows backwards will simply add. So now if we redefine this one, because the plus equals, this now works, because A grad started at zero and we called B backward, we deposit one and then we deposit one again. And now this is two, which is correct. And here this will also work and we'll get correct gradients because when we call E backward, we will deposit the gradients from this branch. And then we get to D backward, it will deposit its own gradients. And then those gradients simply add on top of each other. And so we just accumulate those gradients and that fixes the issue. OK, now before we move on, let me actually do a bit of cleanup here and delete some of these, some of this intermediate work. So I'm not going to need any of this now that we've derived all of it. We are going to keep this because I want to come back to it. Delete the 10h, delete our model example, delete the step, delete this, keep the code that draws, and then delete this example and leave behind only the definition of value. And now let's come back to this non-linearity here that we implemented the 10h. Now, I told you that we could have broken down 10h into its explicit atoms in terms of other expressions if we had the exp function. So if you remember, 10h is defined like this and we chose to develop 10h as a single function. And we can do that because we know it's derivative and we can backpropagate through it. But we can also break down 10h into and express it as a function of exp. And I would like to do that now because I want to prove to you that you get all the same results and all the same gradients. But also because it forces us to implement a few more expressions. It forces us to do exponentiation, addition, subtraction, division, and things like that. And I think it's a good exercise to go through a few more of these. OK, so let's scroll up to the definition of value. And here, one thing that we currently can't do is we can do like a value of, say, 2.0. But we can't do, you know, here, for example, we want to add a constant 1. And we can't do something like this. And we can't do it because it says int object has no attribute data. That's because a plus one comes right here to add. And then other is the integer one. And then here, Python is trying to access one dot data. And that's not a thing. That's because basically one is not a value object and we only have addition for value objects. So as a matter of convenience, so that we can create expressions like this and make them make sense, we can simply do something like this. Basically, we let other alone if other is an instance of value. But if it's not an instance of value, we're going to assume that it's a number like an integer or a float. And we're going to simply wrap it in value. And then other will just become value of other. And then other will have a data attribute. And this should work. So if I just say this, read farm value, then this should work. There we go. OK, now let's do the exact same thing for multiply because we can't do something like this again for the exact same reason. So we just have to go to mall. And if other is not a value, then let's wrap it in value. Let's redefine value. And now this works. Now, here's a kind of unfortunate and not obvious part. A times two works. We saw that. But two times a, is that going to work? You'd expect it to, right? But actually, it will not. And the reason it won't is because Python doesn't know. Like when you do a times two, basically, so a times two, Python will go and it will basically do something like a dot mall of two. That's basically what it will call. But to it, two times a is the same as two dot mall of a. And it doesn't, two can't multiply value. And so it's really confused about that. So instead, what happens is in Python, the way this works is you are free to define something called the R mall. And R mall is kind of like a fallback. So if Python can't do two times a, it will check if by any chance a knows how to multiply two. And that will be called into R mall. So because Python can't do two times a, it will check is there an R mall in value. And because there is, it will now call that. And what we'll do here is we will swap the order of the operands. So basically, two times a will redirect to R mall and R mall will basically call a times two. And that's how that will work. So redefining that with R mall, two times a becomes four. OK, now looking at the other elements that we still need, we need to know how to exponentiate and how to divide. So let's first do the exponentiation part. We're going to introduce a single function X here. And X is going to mirror 10H in the sense that it's a single function that transforms a single scalar value and outputs a single scalar value. So we pop out the Python number. We use math.x to exponentiate it, create a new value object. Everything that we've seen before. The tricky part, of course, is how do you back propagate through e to the X? And so here you can potentially pause the video and think about what should go here. OK, so basically, we need to know what is the local derivative of e to the X. So d by dx of e to the X is, famously, just e to the X. And we've already just calculated e to the X. And it's inside out.data. So we can do out.data times and out.grad. That's the chain mall. So we're just chaining on to the current running grad. And this is what the expression looks like. It looks a little confusing, but this is what it is. And that's the exponentiation. So redefining, we should now be able to call a.exp. And hopefully, the backward pass works as well. OK, and the last thing we'd like to do, of course, is we'd like to be able to divide. Now, I actually will implement something slightly more powerful than division, because division is just a special case of something a bit more powerful. So in particular, just by rearranging, if we have some kind of a b equals value of 4.0 here, we'd like to basically be able to do a divide b, and we'd like this to be able to give us 0.5. Now, division actually can be reshuffled as follows. If we have a divide b, that's actually the same as a multiplying 1 over b. And that's the same as a multiplying b to the power of negative 1. And so what I'd like to do instead is I'd basically like to implement the operation of X to the k for some constant k. So it's an integer or a float. And we would like to be able to differentiate this. And then as a special case, negative 1 will be division. And so I'm doing that just because it's more general. And yeah, you might as well do it that way. So basically what I'm saying is we can redefine division, which we will put here somewhere. Yeah, we can put it here somewhere. What I'm saying is that we can redefine division. So self divide other can actually be rewritten as self times other to the power of negative 1. And now value raised to the power of negative 1, we have now defined that. So here's so we need to implement the pow function. Where am I going to put the pow function? Maybe here somewhere. This is this call for it. So this function will be called when we try to raise a value to some power and other will be that power. Now, I'd like to make sure that other is only an int or a float. Usually other is some kind of a different value object. But here other will be forced to be an int or a float. Otherwise, the math won't work for what we're trying to achieve in this specific case. That would be a different derivative expression if we wanted other to be a value. So here we create the up the value, which is just this data raised to the power of other. And other here could be, for example, negative 1. That's what we are hoping to achieve. And then this is the backwards stub. And this is the fun part, which is what is the chain rule expression here for back for back propagating through the power function, where the power is to the power of some kind of a constant. So this is the exercise. So maybe pause the video here and see if you can figure it out yourself as to what we should put here. OK, so you can actually go here and look at derivative rules as an example. And we see lots of the rules that you can hopefully know from calculus. In particular, what we're looking for is the power rule, because that's telling us that if we're trying to take d by dx of x to the n, which is what we're doing here, then that is just n times x to the n minus one. Right. OK. So that's telling us about the local derivative of this power operation. So all we want here, basically, n is now other and self data is x. And so this now becomes other, which is n times self data, which is now a Python int or a float. It's not a value object. We're accessing the data attribute raised to the power of other minus one or n minus one. I can put brackets around this, but this doesn't matter because power takes precedence over multiply in Python. So that would have been OK. And that's the local derivative only. But now we have to chain it and we change it simply by multiplying by our dot grad. That's chain rule. And this should technically work. And we're going to find out soon. But now if we do this, this should now work. And we get point five. So the forward pass works, but does the backward pass work? And I realize that we actually also have to know how to subtract. So right now, a minus b will not work. To make it work, we need one more piece of code here. And basically, this is the subtraction. And the way we're going to implement subtraction is we're going to implement it by addition of a negation. And then to implement negation, we're going to multiply by negative one. So just again, using the stuff we've already built and just expressing it in terms of what we have. And a minus b is now working. OK, so now let's scroll again to this expression here for this neuron. And let's just compute the backward pass here once we've defined O. And let's draw it. So here's the gradients for all of these lead nodes for this two dimensional neuron that has a 10h that we've seen before. So now what I'd like to do is I'd like to break up this 10h into this expression here. So let me copy paste this here. And now instead of we'll preserve the label and we will change how we define O. So in particular, we're going to implement this formula here. So we need e to the 2x minus one over e to the x plus one. So e to the 2x, we need to take 2 times n and we need to exponentiate it. That's e to the 2x. And then because we're using it twice, let's create an intermediate variable e and then define O as e plus one over e minus one over e plus one. E minus one over e plus one. And that should be it. And then we should be able to draw that O. So now before I run this, what do we expect to see? Number one, we're expecting to see a much longer graph here because we've broken up 10h into a bunch of other operations. But those operations are mathematically equivalent. And so what we're expecting to see is number one, the same result here. So the forward pass works. And number two, because of that mathematical equivalence, we expect to see the same backward pass and the same gradients on these leaf nodes. So these gradients should be identical. So let's run this. So number one, let's verify that instead of a single 10h node, we have now x and we have plus we have times negative one. This is the division. And we end up with the same forward pass here. And then the gradients, we have to be careful because they're in slightly different order potentially. The gradients for w2x2 should be zero and point five. w2 and x2 are zero and point five. And w1x1 are one and negative one point five. One and negative one point five. So that means that both our forward passes and backward passes were correct because this turned out to be equivalent to 10h before. And so the reason I wanted to go through this exercise is number one. We got to practice a few more operations and writing more backwards passes. And number two, I wanted to illustrate the point that the the level at which you implement your operations is totally up to you. You can implement backward passes for tiny expressions like a single individual plus or a single times or you can implement that for, say, 10h, which is a kind of a potentially you can see it as a composite operation because it's made up of all these more atomic operations. But really, all of this is kind of like a fake concept. All that matters is we have some kind of inputs and some kind of an output. And this output is a function of the inputs in some way. And as long as you can do forward pass and the backward pass of that little operation, it doesn't matter what that operation is and how composite it is. If you can write the local gradients, you can change the gradient and you can continue back application. So the design of what those functions are is completely up to you. So now I would like to show you how you can do the exact same thing, but using a modern deep neural network library like, for example, PyTorch, which I've roughly modeled micro grad by. And so PyTorch is something you would use in production. And I'll show you how you can do the exact same thing, but in PyTorch API. So I'm just going to copy paste it in and walk you through it a little bit. This is what it looks like. So we're going to import PyTorch and then we need to define these value objects like we have here. Now micro grad is a scalar valued engine. So we only have scalar values like 2.0. But in PyTorch, everything is based around tensors. And like I mentioned, tensors are just n dimensional arrays of scalars. So that's why things get a little bit more complicated here. I just need a scalar valued tensor, a tensor with just a single element. But by default, when you work with PyTorch, you would use more complicated tensors like this. So if I import PyTorch, then I can create tensors like this. And this tensor, for example, is a 2 by 3 array of scalars in a single compact representation. So we can check its shape. We see that it's a 2 by 3 array and so on. So this is usually what you would work with in the actual libraries. So here I'm creating a tensor that has only a single element 2.0. And then I'm casting it to be double because Python is by default using double precision for its floating point numbers. So I'd like everything to be identical. By default, the data type of these tensors will be float 32. So it's only using a single precision float. So I'm casting it to double so that we have float 64 just like in Python. So I'm casting to double and then we get something similar to value of 2. The next thing I have to do is because these are leaf nodes, by default, PyTorch assumes that they do not require gradients. So I need to explicitly say that all of these nodes require gradients. OK, so this is going to construct scalar valued one element tensors. Make sure that PyTorch knows that they require gradients. Now, by default, these are set to false, by the way, because of efficiency reasons, because usually you would not want gradients for leaf nodes like the inputs to the network. And this is just trying to be efficient in the most common cases. So once we've defined all of our values in PyTorch land, we can perform arithmetic just like we can here in micro grad land. So this would just work. And then there's a torch dot 10h also. And when we get back as a tensor again, and we can just like in micro grad, it's got a data attribute and it's got grad attributes. So these tensor objects, just like in micro grad, have a dot data and a dot grad. And the only difference here is that we need to call it dot item because otherwise PyTorch dot item basically takes a single tensor of one element and it just returns that element stripping out the tensor. So let me just run this. And hopefully we are going to get this is going to print the forward pass, which is point seven, seven. And this will be the gradients, which hopefully are point five, zero, negative one point five and one. So if we just run this, there we go. Point seven. So the forward pass agrees. And then point five, zero, negative one point five and one. So PyTorch agrees with us. And just to show you here, basically, oh, here's a tensor with a single element and it's a double. And we can call that item on it to just get the single number out. So that's what item does. And O is a tensor object, like I mentioned, and it's got a backward function just like we've implemented. And then all of these also have a dot grad. So like X two, for example, and the grad and it's a tensor. And we can pop out the individual number with dot item. So basically, torches, torches can do what we did in micro grad as a special case when your tensors are all single element tensors. But the big deal with PyTorch is that everything is significantly more efficient because we are working with these tensor objects and we can do lots of operations in parallel on all of these tensors. But otherwise, what we've built very much agrees with the API of PyTorch. OK, so now that we have some machinery to build out pretty complicated mathematical expressions, we can also start building up neural nets. And as I mentioned, neural nets are just a specific class of mathematical expressions. So we're going to start building out a neural net piece by piece and eventually we'll build out a two layer multilayer layer perceptron, as it's called. And I'll show you exactly what that means. Let's start with a single individual neuron. We've implemented one here, but here I'm going to implement one that also subscribes to the PyTorch API and how it designs its neural network modules. So just like we saw that we can like match the API of PyTorch on the autograd side, we're going to try to do that on the neural network modules. So here's class neuron. And just for the sake of efficiency, I'm going to copy paste some sections that are relatively straightforward. So the constructor will take number of inputs to this neuron, which is how many inputs come to a neuron. So this one, for example, has three inputs. And then it's going to create a weight that is some random number between negative one and one for every one of those inputs and a bias that controls the overall trigger happiness of this neuron. And then we're going to implement a def underscore underscore call of self and X, some input X. And really what we don't do here is W times X plus B, where W times X here is a dot product specifically. Now, if you haven't seen call, let me just return 0.0 here for now. The way this works now is we can have an X, which is say like two point zero three point zero. Then we can initialize a neuron that is two dimensional because these are two numbers. And then we can feed those two numbers into that neuron to get an output. And so when you use this notation and of X, Python will use call. So currently, call just returns 0.0. Now we'd like to actually do the forward pass of this neuron instead. So we're going to do here first is we need to basically multiply all of the elements of W with all of the elements of X pairwise. We need to multiply them. So the first thing we're going to do is we're going to zip up, W and X. And in Python zip takes two iterators and it creates a new iterator that iterates over the topples of their corresponding entries. So, for example, just to show you, we can print this list and still return 0.0 here. So we see that these W's are paired up with the X's W with X. And now what we want to do is for WI, XI in, we want to multiply WI times XI. And then we want to sum all of that together to come up with an activation and add also SELTA B on top. So that's the raw activation. And then, of course, we need to pass that through a normality. So what we're going to be returning is act.10h. And here's out. So now we see that we are getting some outputs and we get a different output from a neuron each time because we are initializing different weights and biases. And then to be a bit more efficient here, actually, sum, by the way, takes a second optional parameter, which is the start. And by default, the start is zero. So these elements of this sum will be added on top of zero to begin with. But actually, we can just start with SELTA B and then we just have an expression like this. And then the generator expression here must be parenthesized in Python. There we go. So now we can forward a single neuron. Next up, we're going to define a layer of neurons. So here we have a schematic for a MLP. So we see that these MLPs, each layer, this is one layer, has actually a number of neurons and they're not connected to each other, but all of them are fully connected to the input. So what is a layer of neurons? It's just it's just a set of neurons evaluated independently. So in the interest of time, I'm going to do something fairly straightforward here. It's literally a layer. It's just a list of neurons. And then how many neurons do we have? We take that as an input argument here. How many neurons do you want in your layer? Number of outputs in this layer. And so we just initialize completely independent neurons with this given dimensionality. And when we call on it, we just independently evaluate them. So now instead of a neuron, we can make a layer of neurons. They are two dimensional neurons and let's have three of them. And now we see that we have three independent evaluations of three different neurons. Right. OK. And finally, let's complete this picture and define an entire multilayer perceptron or MLP. And as we can see here in an MLP, these layers just feed into each other sequentially. So let's come here and I'm just going to copy the code here in interest of time. So an MLP is very similar. We're taking the number of inputs as before. But now instead of taking a single and out, which is number of neurons in a single layer, we're going to take a list of an outs and this list defines the sizes of all the layers that we want in our MLP. So here we just put them all together and then iterate over consecutive pairs of these sizes and create layer objects for them. And then in the call function, we are just calling them sequentially. So that's an MLP really. And let's actually re-implement this picture. So we want three input neurons and then two layers of four and an output unit. So we want a three dimensional input. Say this is an example input. We want three inputs into two layers of four and one output. And this, of course, is an MLP. And there we go. That's a forward pass of an MLP. To make this a little bit nicer, you see how we have just a single element, but it's wrapped in a list because layer always returns lists. So for convenience, return outs at zero if len out is exactly a single element, else return fullest. And this will allow us to just get a single value out at the last layer that only has a single neuron. And finally, we should be able to draw dot of n of x. And as you might imagine, these expressions are now getting relatively involved. This is an entire MLP that we're defining now. All the way until a single output. And so obviously, you would never differentiate on pen and paper these expressions. But with microGrad, we will be able to back propagate all the way through this and back propagate into these weights of all these neurons. So let's see how that works. OK, so let's create ourselves a very simple example data set here. So this data set has four examples. And so we have four possible inputs into the neural net. And we have four desired targets. So we'd like the neural net to assign our output 1.0 when it's fed this example, negative one when it's fed these examples, and one when it's fed this example. So it's a very simple binary classifier neural net basically that we would like here. Now, let's think what the neural net currently thinks about these four examples. We can just get their predictions. Basically, we can just call n of x for x in excess. And then we can print. So these are the outputs of the neural net on those four examples. So the first one is point nine one. But we like it to be one. So we should push this one higher. This one we want to be higher. This one says point eight eight, and we want this to be negative one. This is point eight eight. We want it to be negative one. And this one is point eight eight. We want it to be one. So how do we make the neural net and how do we tune the weights to better predict the desired targets? And the trick used in deep learning to achieve this is to calculate a single number that somehow measures the total performance of your neural net. We call the single number the loss. So the loss first is a single number that we're going to define that basically measures how well the neural net is performing. Right now, we have the intuitive sense that it's not performing very well because we're not very much close to this. So the loss will be high and we'll want to minimize the loss. So in particular, in this case, what we're going to do is we're going to implement the mean squared error loss. So this is doing is we're going to basically iterate for why ground truth and why output in zip of wise and my friend. So we're going to pair up the ground truths with the predictions and the zip iterates over tuples of them. And for each, why ground truth and why output, we're going to subtract them and square them. So let's first see what these losses are. These are individual loss components. And so basically for each one of the four, we are taking the prediction and the ground truth. We are subtracting them and squaring them. So because this one is so close to its target, point nine one is almost one subtracting them gives a very small number. So here we would get like a negative point one and then squaring it just makes sure that regardless of whether we are more negative or more positive, we always get a positive number. Instead of squaring, we should. We could also take, for example, the absolute value. We need to discard the sign. And so you see that the expression is arranged so that you only get zero exactly when why out is equal to why ground truth. When those two are equal, so your prediction is exactly the target, you are going to get zero. And if your prediction is not the target, you are going to get some other number. So here, for example, we are way off. And so that's why the loss is quite high. And the more off we are, the greater the loss will be. So we don't want high loss. We want low loss. And so the final loss here will be just the sum of all of these numbers. So you see that this should be zero roughly plus zero roughly, but plus seven. So loss should be about seven here. And now we want to minimize the loss. We want the loss to be low because if loss is low, then every one of the predictions is equal to its target. So the loss, the lowest it can be is zero. And the greater it is, the worse off the neural net is predicting. So now, of course, if we do loss backward, something magical happened when I hit enter. And the magical thing, of course, that happened is that we can look at N dot layers dot neuron and that layers at, say, like the first layer that neurons at zero. Because remember that MLP has the layers, which is a list, and each layer has a neurons, which is a list. And that gives us an individual neuron. And then it's got some weights. And so we can, for example, look at the weights at zero. Oops, it's not called weights. It's called W. And that's a value. But now this value also has a grad because of the backward pass. And so we see that because this gradient here on this particular weight of this particular neuron of this particular layer is negative, we see that its influence on the loss is also negative. So slightly increasing this particular weight of this neuron of this layer would make the loss go down. And we actually have this information for every single one of our neurons and all their parameters. Actually, it's worth looking at also the draw dot loss, by the way. So previously we looked at the draw dot of a single neuron forward pass, and that was already a large expression. But what is this expression? We actually forwarded every one of those four examples, and then we have the loss on top of them with the mean squared error. And so this is a really massive graph because this graph that we've built up now. Oh, my gosh, this graph that we've built up now, which is kind of excessive. It's excessive because it has four forward passes of a neural net for every one of the examples. And then it has the loss on top and it ends with the value of the loss, which was 7.12. And this loss will now back propagate through all the four forward passes all the way through just every single intermediate value of the neural net, all the way back to, of course, the parameters of the weights, which are the input. So these weight parameters here are inputs to this neural net and these numbers here, these scalars are inputs to the neural net. So if we went around here, we will probably find some of these examples, this 1.0, potentially maybe this 1.0 or some of the others. And you'll see that they all have gradients as well. The thing is, these gradients on the input data are not that useful to us. And that's because the input data seems to be not changeable. It's a given to the problem. And so it's a fixed input. We're not going to be changing it or messing with it, even though we do have gradients for it. But some of these gradients here will be for the neural network parameters, the Ws and the Bs. And those, of course, we want to change. OK, so now we're going to want some convenience code to gather up all the parameters of the neural net so that we can operate on all of them simultaneously. And every one of them, we will nudge a tiny amount based on the gradient information. So let's collect the parameters of the neural net all in one array. So let's create a parameters of self that just returns self.w, which is a list, concatenated with a list of self.b. So this will just return a list. List plus list just gives you a list. So that's parameters of neuron. And I'm calling it this way because also PyTorch has parameters on every single NN module. And it does exactly what we're doing here. It just returns the parameter tensors for us as the parameter scalars. Now layer is also a module, so it will have parameters. And basically what we want to do here is something like this. Params is here. And then for neuron in salt.neurons, we want to get neuron.parameters. And we want to params.extend. So these are the parameters of this neuron. And then we want to put them on top of params. So params.extend of piece. And then when I return params. So this is way too much code. So actually there's a way to simplify this, which is return p for neuron in self.neurons for p in neuron.parameters. So it's a single list comprehension. In Python, you can sort of nest them like this and you can then create the desired array. So these are identical. We can take this out. And then let's do the same here. So we have parameters. And return a parameter for layer in self.layers for p in layer.parameters. And that should be good. Now let me pop out this so we don't reinitialize our network because we need to reinitialize our... OK, so unfortunately we will have to probably reinitialize the network because we just had functionality. Because this class, of course, I want to get all the end.parameters, but that's not going to work because this is the old class. So unfortunately we do have to reinitialize the network, which will change some of the numbers. But let me do that so that we pick up the new API. We can now do end.parameters. And these are all the weights and biases inside the entire neural net. So in total, this MLP has 41 parameters. And now we'll be able to change them. So if we recalculate the loss here, we see that unfortunately we have slightly different predictions and slightly different loss. But that's OK. OK, so we see that this neurons gradient is slightly negative. We can also look at its data right now, which is 0.85. So this is the current value of this neuron and this is its gradient on the loss. So what we want to do now is we want to iterate for every p in end.parameters. So for all the 41 parameters in this neural net, we actually want to change p.data slightly according to the gradient information. OK, so dot dot dot to do here. But this will be basically a tiny update in this gradient descent scheme. And gradient descent, we are thinking of the gradient as a vector pointing in the direction of increased loss. And so in gradient descent, we are modifying p.data by a small step size in the direction of the gradient. So the step size, as an example, could be like a very small number, like 0.01 is the step size, times p.grad. But we have to think through some of the signs here. So in particular, working with this specific example here, we see that if we just left it like this, then this neurons value would be currently increased by a tiny amount of the gradient. The gradient is negative. So this value of this neuron would go slightly down. It would become like 0.84 or something like that. But if this neurons value goes lower, that would actually increase the loss. That's because the derivative of this neuron is negative. So increasing this makes the loss go down. So increasing it is what we want to do instead of decreasing it. So basically what we're missing here is we're actually missing a negative sign. And again, this other interpretation, and that's because we want to minimize the loss. We don't want to maximize the loss. We want to decrease it. And the other interpretation, as I mentioned, is you can think of the gradient vector. So basically just the vector of all the gradients as pointing in the direction of increasing the loss. But then we want to decrease it. So we actually want to go in the opposite direction. And so you can convince yourself that this does the right thing here with a negative, because we want to minimize the loss. So if we notch all the parameters by tiny amount, then we'll see that this data will have changed a little bit. So now this neuron is a tiny amount greater value. So 0.854 went to 0.857. And that's a good thing, because slightly increasing this neuron data makes the loss go down according to the gradient. And so the correcting has happened sign-wise. And so now what we would expect, of course, is that because we've changed all these parameters, we expect that the loss should have gone down a bit. So we want to re-evaluate the loss. Let me basically... This is just a data definition that hasn't changed. But the forward pass here of the network, we can recalculate. And actually, let me do it outside here so that we can compare the two loss values. So here, if I recalculate the loss, we'd expect the new loss now to be slightly lower than this number. So hopefully, what we're getting now is a tiny bit lower than 4.846. And remember, the way we've arranged this is that low loss means that our predictions are matching the targets. So our predictions now are probably slightly closer to the targets. And now all we have to do is we have to iterate this process. So again, we've done the forward pass, and this is the loss. Now we can loss that backward. Let me take these out. And we can do a step size. And now we should have a slightly lower loss. 4.36 goes to 3.9. And OK, so we've done the forward pass. Here's the backward pass. Nudge. And now the loss is 3.66. 3.47. And you get the idea. We just continue doing this. And this is gradient descent. We're just iteratively doing forward pass, backward pass, update. Forward pass, backward pass, update. And the neural net is improving its predictions. So here, if we look at ypred now, we see that this value should be getting closer to 1. So this value should be getting more positive. These should be getting more negative. And this one should be also getting more positive. So if we just iterate this a few more times, actually, we may be able to afford to go a bit faster. Let's try a slightly higher loading rate. Oops. OK, there we go. So now we're at 0.31. If you go too fast, by the way, if you try to make it too big of a step, you may actually overstep. It's overconfidence. Because again, remember, we don't actually know exactly about the loss function. The loss function has all kinds of structure. And we only know about the very local dependence of all these parameters on the loss. But if we step too far, we may step into a part of the loss that is completely different. And that can destabilize training and make your loss actually blow up even. So the loss is now 0.04. So actually, the predictions should be really quite close. Let's take a look. So you see how this is almost 1, almost negative 1, almost 1. We can continue going. So, yep, backward, update. Oops, there we go. So we went way too fast. And we actually overstepped. So we got too eager. Where are we now? Oops. OK. 7e negative 9. So this is very, very low loss. And the predictions are basically perfect. So somehow we basically we were doing way too big updates and we briefly exploded. But then somehow we ended up getting into a really good spot. So usually this learning rate and the tuning of it is a subtle art. You want to set your learning rate. If it's too low, you're going to take way too long to converge. But if it's too high, the whole thing gets unstable and you might actually even explode the loss, depending on your loss function. So finding the step size to be just right, it's a pretty subtle art sometimes when you're using sort of vanilla gradient descent. But we happen to get into a good spot. We can look at n dot parameters. So this is the setting of weights and biases that makes our network predict the desired targets very, very close. And basically we've successfully trained a neural net. OK, let's make this a tiny bit more respectable and implement an actual training loop and what that looks like. So this is the data definition that states this is the forward pass. So for K in range, you know, we're going to take a bunch of steps. First, we do the forward pass. We validate the loss. Let's reinitialize the neural net from scratch. And here's the data. And we first do forward pass, then we do the backward pass. And then we do an update. That's gradient descent. And then we should be able to iterate this and we should be able to print the current step, the current loss. Let's just print the sort of number of the loss. And that should be it. And then the learning rate 0.01 is a little too small. 0.1 we saw is like a little bit dangerous if you buy. Let's go somewhere in between and we'll optimize this for not 10 steps, but let's go for, say, 20 steps. Let me erase all of this junk. And let's run the optimization. And you see how we've actually converged slower in a more controlled manner and got to a loss that is very low. So I expect wide bread to be quite good. There we go. And that's it. OK, so this is kind of embarrassing, but we actually have a really terrible bug in here and it's a subtle bug. And it's a very common bug. And I can't believe I've done it for the 20th time in my life, especially on camera. And I could have reshot the whole thing, but I think it's pretty funny. And, you know, you get to appreciate a bit what working with neural nets may be is like sometimes. We are guilty of common bug. I've actually tweeted the most common neural mistakes a long time ago now. And I'm not really going to explain any of these except for we are guilty of number three. You forgot to zero grad before dot backward. What is that? Basically, what's happening, and it's a subtle bug and I'm not sure if you saw it, is that all of these weights here have a dot data and a dot grad. And that grad starts at zero. And then we do backward and we fill in the gradients and then we do an update on the data, but we don't flush the grad. It stays there. So when we do the second forward pass and we do backward again, remember that all the backward operations do a plus equals on the grad. And so these gradients just add up and they never get reset to zero. So basically, we didn't zero grad. So here's how we zero grad before backward. We need to iterate over all the parameters and we need to make sure that p dot grad is set to zero. We need to reset it to zero just like it is in the constructor. So remember, all the way here for all these value nodes, grad is reset to zero. And then all these backward passes do a plus equals from that grad. But we need to make sure that we reset these grads to zero so that when we do backward, all of them start at zero and the actual backward pass accumulates the loss derivatives into the grads. So this is zero grad in PyTorch. And we will get a slightly different optimization. Let's reset the neural net. The data is the same. This is now, I think, correct. And we get a much more, you know, we get a much more slower descent. We still end up with pretty good results and we can continue to submit more to get down lower and lower and lower. Yeah. So the only reason that the previous thing worked, it's extremely buggy. The only reason that worked is that this is a very, very simple problem. And it's very easy for this neural net to fit this data. And so the grads ended up accumulating and it effectively gave us a massive step size and it made us converge extremely fast. But basically now we have to do more steps to get to very low values of loss and get WIPRED to be really good. We can try to step a bit greater. Yeah, we're going to get closer and closer to one minus one and one. So working with neural nets is sometimes tricky because you may have lots of bugs in the code and your network might actually work just like ours worked. But chances are is that if we had a more complex problem, then actually this bug would have made us not optimize the loss very well. And we were only able to get away with it because the problem is very simple. So let's now bring everything together and summarize what we learned. What are neural nets? Neural nets are these mathematical expressions, fairly simple mathematical expressions in the case of multilayer perceptron, that take input as the data and they take input, the weights and the parameters of the neural net, mathematical expression for the forward pass, followed by a loss function and the loss function tries to measure the accuracy of the predictions. And usually the loss will be low when your predictions are matching your targets or where the network is basically behaving well. So we manipulate the loss function so that when the loss is low, the network is doing what you want it to do on your problem. And then we backward the loss, use backpropagation to get the gradient, and then we know how to tune all the parameters to decrease the loss locally. But then we have to iterate that process many times in what's called the gradient descent. So we simply follow the gradient information and that minimizes the loss and the loss is arranged so that when the loss is minimized, the network is doing what you want it to do. And yeah, so we just have a blob of neural stuff and we can make it do arbitrary things. And that's what gives neural nets their power. It's, you know, this is a very tiny network with 41 parameters, but you can build significantly more complicated neural nets with billions, at this point, almost trillions of parameters. And it's a massive blob of neural tissue, simulated neural tissue, roughly speaking. And you can make it do extremely complex problems. And these neural nets then have all kinds of very fascinating emergent properties in when you try to make them do significantly hard problems. As in the case of GPT, for example, we have massive amounts of text from the Internet. And we're trying to get neural nets to predict, to take like a few words and try to predict the next word in a sequence. That's the learning problem. And it turns out that when you train this on all of Internet, the neural net actually has like really remarkable emergent properties. But that neural net would have hundreds of billions of parameters. But it works on fundamentally the exact same principles. The neural net, of course, will be a bit more complex. But otherwise, the value in the gradient is there and will be identical. And the gradient descent would be there and would be basically identical. But people usually use slightly different updates. This is a very simple stochastic gradient descent update. And the loss function would not be mean squared error. They would be using something called the cross entropy loss for predicting the next token. So there's a few more details. But fundamentally, the neural network setup and neural network training is identical and pervasive. And now you understand intuitively how that works under the hood. In the beginning of this video, I told you that by the end of it, you would understand everything in micro grad and we'd slowly build it up. Let me briefly prove that to you. So I'm going to step through all the code that is in micro grad as of today. Actually, potentially some of the code will change by the time you watch this video because I intend to continue developing micro grad. But let's look at what we have so far at least. In it, that pi is empty. When you go to engine that pi that has the value, everything here you should mostly recognize. So we have the data data that grad attributes with the backward function. We have the previous set of children and the operation that produced this value. We have addition, multiplication and raising to a scalar power. We have the relu non-linearity, which is slightly different type of non-linearity than tan age that we used in this video. Both of them are non-linearities and notably tan age is not actually present in micro grad as of right now. But I intend to add it later with the backward, which is identical. And then all of these other operations, which are built up on top of operations here. So values should be very recognizable, except for the non-linearity used in this video. There's no massive difference between relu and tan age and sigmoid and these other non-linearities. They're all roughly equivalent and can be used in MLPs. So I use tan age because it's a bit smoother and because it's a little bit more complicated than relu. And therefore it's stressed a little bit more the local gradients and working with those derivatives, which I thought would be useful. Nn.py is the neural networks library, as I mentioned. So you should recognize identical implementation of Neural, Layer and MLP. Notably, or not so much, we have a class module here. There's a parent class of all these modules. I did that because there's an nn.module class in PyTorch. And so this exactly matches that API. And nn.module in PyTorch has also a zero grad, which I refactored out here. So that's the end of micro grad, really. Then there's a test, which you'll see basically creates two chunks of code, one in micro grad and one in PyTorch. And we'll make sure that the forward and the backward paths agree identically. For a slightly less complicated expression, a slightly more complicated expression, everything agrees. So we agree with PyTorch on all of these operations. And finally, there's a demo that Ipyymb here. And it's a bit more complicated binary classification demo than the one I covered in this lecture. So we only had a tiny data set of four examples. Here we have a bit more complicated example with lots of blue points and lots of red points. And we're trying to, again, build a binary classifier to distinguish two dimensional points as red or blue. It's a bit more complicated MLP here with a bigger MLP. The loss is a bit more complicated because it supports batches. So because our data set was so tiny, we always did a forward pass on the entire data set of four examples. But when your data set is like a million examples, what we usually do in practice is we basically pick out some random subset. We call that a batch. And then we only process the batch forward, backward and update. So we don't have to forward the entire training set. So this supports batching because there's a lot more examples here. We do a forward pass. The loss is slightly more different. This is a max margin loss that I implement here. The one that we used was the mean squared error loss because it's the simplest one. There's also the binary cross entropy loss. All of them can be used for binary classification and don't make too much of a difference in the simple examples that we looked at so far. There's something called L2 regularization used here. This has to do with generalization of the neural net and controls the overfitting in machine learning setting. But I did not cover these concepts in this video potentially later. And the training loop you should recognize. So forward, backward, with, zero grad, and update, and so on. You'll notice that in the update here, the learning rate is scaled as a function of number of iterations and it shrinks. And this is something called learning rate decay. So in the beginning, you have a high learning rate. And as the network sort of stabilizes near the end, you bring down the learning rate to get some of the fine details in the end. And in the end, we see the decision surface of the neural net. And we see that it learns to separate out the red and the blue area based on the data points. So that's the slightly more complicated example in the demo demo that I buy one B that you're free to go over. But yeah, as of today, that is micro grad. I also wanted to show you a little bit of real stuff so that you get to see how this is actually implemented in production grade library like by torch. So in particular, I wanted to show I wanted to find and show you the backward password 10 H in my torch. So here in micro grad, we see that the backward password 10 H is one minus T square, where T is the output of the 10 H of X times of that grad, which is the chain rule. So we're looking for something that looks like this. Now, I went to my torch, which has an open source GitHub code base, and I looked through a lot of its code. And honestly, I spent about 15 minutes and I couldn't find 10 H. And that's because these libraries, unfortunately, they grow in size and entropy. And if you just search for 10 H, you get apparently two thousand eight hundred results and four hundred and four hundred and six files. So I don't know what these files are doing, honestly. And why there are so many measures of 10 H. But unfortunately, these libraries are quite complex. They're meant to be used, not really inspected. Eventually, I did stumble on someone who tries to change the 10 H backward code for some reason. And someone here pointed to the CPU kernel and the CUDA kernel for 10 H backward. So this basically depends on if you're using PyTorch on a CPU device or on a GPU, which these are different devices and I haven't covered this. But this is the 10 H backward kernel for CPU. And the reason it's so large is that, number one, this is like if you're using a complex type, which we haven't even talked about. If you're using a specific data type of B float 16, which we haven't talked about. And then if you're not, then this is the kernel. And deep here, we see something that resembles our backward pass. So they have eight times one minus B square. So this B here must be the output of the 10 H and this is the out.grad. So here we found it deep inside PyTorch on this location for some reason inside binary ops kernel when 10 H is not actually binary op. And then this is the GPU kernel. We're not complex. We're here. And here we go with one line of code. So we did find it, but basically, unfortunately, these code bases are very large and micro grad is very, very simple. But if you actually want to use real stuff, finding the code for it, you'll actually find that difficult. I also wanted to show you an example here where PyTorch is showing you how you can register a new type of function that you want to add to PyTorch as a Lego building block. So here, if you want to, for example, add a Legendre polynomial three, here's how you could do it. You will register it as a class that subclasses torch.out.grad that function. And then you have to tell PyTorch how to forward your new function and how to backward through it. So as long as you can do the forward pass of this little function piece that you want to add, and as long as you know the local derivative, the local gradients, which are implemented in the backward, PyTorch will be able to back propagate through your function. And then you can use this as a Lego block in a larger Lego castle of all the different Lego blocks that PyTorch already has. And so that's the only thing you have to tell PyTorch and everything would just work. And you can register new types of functions in this way, following this example. And that is everything that I wanted to cover in this lecture. So I hope you enjoyed building out micro grad with me. I hope you find it interesting, insightful. And yeah, I will post a lot of the links that are related to this video in the video description below. I will also probably post a link to a discussion forum or discussion group where you can ask questions related to this video. And then I can answer or someone else can answer your questions. And I may also do a follow up video that answers some of the most common questions. But for now, that's it. I hope you enjoyed it. If you did, then please like and subscribe so that YouTube knows to feature this video to more people. And that's it for now. I'll see you later. Now, here's the problem. We know DL by. Wait, what is the problem? And that's everything I wanted to cover in this lecture. So I hope you enjoyed us building out micro grad micro grad. OK, now let's do the exact same thing from multiple because we can't do something like eight times to. Oops, I know what happened.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.76, "text": " Hello, my name is Andrei and I've been training deep neural networks for a bit more than a decade and in this lecture", "tokens": [50364, 2425, 11, 452, 1315, 307, 400, 10271, 293, 286, 600, 668, 3097, 2452, 18161, 9590, 337, 257, 857, 544, 813, 257, 10378, 293, 294, 341, 7991, 50652, 50652, 286, 1116, 411, 281, 855, 291, 437, 18161, 3209, 3097, 1542, 411, 833, 264, 13376, 50822, 50838, 407, 294, 1729, 11, 321, 366, 516, 281, 722, 365, 257, 8247, 732, 6119, 21060, 50994, 50994, 400, 538, 264, 917, 295, 341, 7991, 11, 321, 486, 6964, 293, 3847, 257, 18161, 2533, 51194, 51194, 509, 603, 483, 281, 536, 1203, 300, 1709, 322, 833, 264, 13376, 293, 2293, 1333, 295, 577, 300, 1985, 322, 364, 21769, 1496, 51468, 51490, 823, 4682, 437, 286, 576, 411, 281, 360, 307, 286, 576, 411, 281, 747, 291, 807, 51654, 51692], "temperature": 0.0, "avg_logprob": -0.18804348754882813, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.0046026804484426975}, {"id": 1, "seek": 0, "start": 5.76, "end": 9.16, "text": " I'd like to show you what neural network training looks like under the hood", "tokens": [50364, 2425, 11, 452, 1315, 307, 400, 10271, 293, 286, 600, 668, 3097, 2452, 18161, 9590, 337, 257, 857, 544, 813, 257, 10378, 293, 294, 341, 7991, 50652, 50652, 286, 1116, 411, 281, 855, 291, 437, 18161, 3209, 3097, 1542, 411, 833, 264, 13376, 50822, 50838, 407, 294, 1729, 11, 321, 366, 516, 281, 722, 365, 257, 8247, 732, 6119, 21060, 50994, 50994, 400, 538, 264, 917, 295, 341, 7991, 11, 321, 486, 6964, 293, 3847, 257, 18161, 2533, 51194, 51194, 509, 603, 483, 281, 536, 1203, 300, 1709, 322, 833, 264, 13376, 293, 2293, 1333, 295, 577, 300, 1985, 322, 364, 21769, 1496, 51468, 51490, 823, 4682, 437, 286, 576, 411, 281, 360, 307, 286, 576, 411, 281, 747, 291, 807, 51654, 51692], "temperature": 0.0, "avg_logprob": -0.18804348754882813, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.0046026804484426975}, {"id": 2, "seek": 0, "start": 9.48, "end": 12.6, "text": " So in particular, we are going to start with a blank two pair notebook", "tokens": [50364, 2425, 11, 452, 1315, 307, 400, 10271, 293, 286, 600, 668, 3097, 2452, 18161, 9590, 337, 257, 857, 544, 813, 257, 10378, 293, 294, 341, 7991, 50652, 50652, 286, 1116, 411, 281, 855, 291, 437, 18161, 3209, 3097, 1542, 411, 833, 264, 13376, 50822, 50838, 407, 294, 1729, 11, 321, 366, 516, 281, 722, 365, 257, 8247, 732, 6119, 21060, 50994, 50994, 400, 538, 264, 917, 295, 341, 7991, 11, 321, 486, 6964, 293, 3847, 257, 18161, 2533, 51194, 51194, 509, 603, 483, 281, 536, 1203, 300, 1709, 322, 833, 264, 13376, 293, 2293, 1333, 295, 577, 300, 1985, 322, 364, 21769, 1496, 51468, 51490, 823, 4682, 437, 286, 576, 411, 281, 360, 307, 286, 576, 411, 281, 747, 291, 807, 51654, 51692], "temperature": 0.0, "avg_logprob": -0.18804348754882813, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.0046026804484426975}, {"id": 3, "seek": 0, "start": 12.6, "end": 16.6, "text": " And by the end of this lecture, we will define and train a neural net", "tokens": [50364, 2425, 11, 452, 1315, 307, 400, 10271, 293, 286, 600, 668, 3097, 2452, 18161, 9590, 337, 257, 857, 544, 813, 257, 10378, 293, 294, 341, 7991, 50652, 50652, 286, 1116, 411, 281, 855, 291, 437, 18161, 3209, 3097, 1542, 411, 833, 264, 13376, 50822, 50838, 407, 294, 1729, 11, 321, 366, 516, 281, 722, 365, 257, 8247, 732, 6119, 21060, 50994, 50994, 400, 538, 264, 917, 295, 341, 7991, 11, 321, 486, 6964, 293, 3847, 257, 18161, 2533, 51194, 51194, 509, 603, 483, 281, 536, 1203, 300, 1709, 322, 833, 264, 13376, 293, 2293, 1333, 295, 577, 300, 1985, 322, 364, 21769, 1496, 51468, 51490, 823, 4682, 437, 286, 576, 411, 281, 360, 307, 286, 576, 411, 281, 747, 291, 807, 51654, 51692], "temperature": 0.0, "avg_logprob": -0.18804348754882813, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.0046026804484426975}, {"id": 4, "seek": 0, "start": 16.6, "end": 22.080000000000002, "text": " You'll get to see everything that goes on under the hood and exactly sort of how that works on an intuitive level", "tokens": [50364, 2425, 11, 452, 1315, 307, 400, 10271, 293, 286, 600, 668, 3097, 2452, 18161, 9590, 337, 257, 857, 544, 813, 257, 10378, 293, 294, 341, 7991, 50652, 50652, 286, 1116, 411, 281, 855, 291, 437, 18161, 3209, 3097, 1542, 411, 833, 264, 13376, 50822, 50838, 407, 294, 1729, 11, 321, 366, 516, 281, 722, 365, 257, 8247, 732, 6119, 21060, 50994, 50994, 400, 538, 264, 917, 295, 341, 7991, 11, 321, 486, 6964, 293, 3847, 257, 18161, 2533, 51194, 51194, 509, 603, 483, 281, 536, 1203, 300, 1709, 322, 833, 264, 13376, 293, 2293, 1333, 295, 577, 300, 1985, 322, 364, 21769, 1496, 51468, 51490, 823, 4682, 437, 286, 576, 411, 281, 360, 307, 286, 576, 411, 281, 747, 291, 807, 51654, 51692], "temperature": 0.0, "avg_logprob": -0.18804348754882813, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.0046026804484426975}, {"id": 5, "seek": 0, "start": 22.52, "end": 25.8, "text": " Now specifically what I would like to do is I would like to take you through", "tokens": [50364, 2425, 11, 452, 1315, 307, 400, 10271, 293, 286, 600, 668, 3097, 2452, 18161, 9590, 337, 257, 857, 544, 813, 257, 10378, 293, 294, 341, 7991, 50652, 50652, 286, 1116, 411, 281, 855, 291, 437, 18161, 3209, 3097, 1542, 411, 833, 264, 13376, 50822, 50838, 407, 294, 1729, 11, 321, 366, 516, 281, 722, 365, 257, 8247, 732, 6119, 21060, 50994, 50994, 400, 538, 264, 917, 295, 341, 7991, 11, 321, 486, 6964, 293, 3847, 257, 18161, 2533, 51194, 51194, 509, 603, 483, 281, 536, 1203, 300, 1709, 322, 833, 264, 13376, 293, 2293, 1333, 295, 577, 300, 1985, 322, 364, 21769, 1496, 51468, 51490, 823, 4682, 437, 286, 576, 411, 281, 360, 307, 286, 576, 411, 281, 747, 291, 807, 51654, 51692], "temperature": 0.0, "avg_logprob": -0.18804348754882813, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.0046026804484426975}, {"id": 6, "seek": 2580, "start": 25.8, "end": 31.12, "text": " Building of micro grad now micro grad is this library that I released on github about two years ago", "tokens": [50364, 18974, 295, 4532, 2771, 586, 4532, 2771, 307, 341, 6405, 300, 286, 4736, 322, 290, 355, 836, 466, 732, 924, 2057, 50630, 50630, 583, 412, 264, 565, 286, 787, 17135, 264, 4009, 3089, 293, 291, 1116, 362, 281, 352, 293, 2256, 1803, 293, 534, 50858, 50894, 43225, 484, 577, 309, 1985, 50994, 50994, 407, 294, 341, 7991, 11, 286, 486, 747, 291, 807, 309, 1823, 538, 1823, 293, 733, 295, 2871, 322, 439, 264, 3755, 295, 309, 51218, 51218, 407, 437, 311, 4532, 2771, 293, 983, 307, 309, 1880, 30, 51342, 51394, 2205, 51494, 51522, 25642, 2771, 307, 1936, 364, 1476, 664, 6206, 2848, 1476, 664, 6206, 307, 2099, 337, 12509, 16235, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.32969175536057044, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.00017120063421316445}, {"id": 7, "seek": 2580, "start": 31.12, "end": 35.68, "text": " But at the time I only uploaded the source code and you'd have to go and buy yourself and really", "tokens": [50364, 18974, 295, 4532, 2771, 586, 4532, 2771, 307, 341, 6405, 300, 286, 4736, 322, 290, 355, 836, 466, 732, 924, 2057, 50630, 50630, 583, 412, 264, 565, 286, 787, 17135, 264, 4009, 3089, 293, 291, 1116, 362, 281, 352, 293, 2256, 1803, 293, 534, 50858, 50894, 43225, 484, 577, 309, 1985, 50994, 50994, 407, 294, 341, 7991, 11, 286, 486, 747, 291, 807, 309, 1823, 538, 1823, 293, 733, 295, 2871, 322, 439, 264, 3755, 295, 309, 51218, 51218, 407, 437, 311, 4532, 2771, 293, 983, 307, 309, 1880, 30, 51342, 51394, 2205, 51494, 51522, 25642, 2771, 307, 1936, 364, 1476, 664, 6206, 2848, 1476, 664, 6206, 307, 2099, 337, 12509, 16235, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.32969175536057044, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.00017120063421316445}, {"id": 8, "seek": 2580, "start": 36.4, "end": 38.4, "text": " Figure out how it works", "tokens": [50364, 18974, 295, 4532, 2771, 586, 4532, 2771, 307, 341, 6405, 300, 286, 4736, 322, 290, 355, 836, 466, 732, 924, 2057, 50630, 50630, 583, 412, 264, 565, 286, 787, 17135, 264, 4009, 3089, 293, 291, 1116, 362, 281, 352, 293, 2256, 1803, 293, 534, 50858, 50894, 43225, 484, 577, 309, 1985, 50994, 50994, 407, 294, 341, 7991, 11, 286, 486, 747, 291, 807, 309, 1823, 538, 1823, 293, 733, 295, 2871, 322, 439, 264, 3755, 295, 309, 51218, 51218, 407, 437, 311, 4532, 2771, 293, 983, 307, 309, 1880, 30, 51342, 51394, 2205, 51494, 51522, 25642, 2771, 307, 1936, 364, 1476, 664, 6206, 2848, 1476, 664, 6206, 307, 2099, 337, 12509, 16235, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.32969175536057044, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.00017120063421316445}, {"id": 9, "seek": 2580, "start": 38.4, "end": 42.88, "text": " So in this lecture, I will take you through it step by step and kind of comment on all the pieces of it", "tokens": [50364, 18974, 295, 4532, 2771, 586, 4532, 2771, 307, 341, 6405, 300, 286, 4736, 322, 290, 355, 836, 466, 732, 924, 2057, 50630, 50630, 583, 412, 264, 565, 286, 787, 17135, 264, 4009, 3089, 293, 291, 1116, 362, 281, 352, 293, 2256, 1803, 293, 534, 50858, 50894, 43225, 484, 577, 309, 1985, 50994, 50994, 407, 294, 341, 7991, 11, 286, 486, 747, 291, 807, 309, 1823, 538, 1823, 293, 733, 295, 2871, 322, 439, 264, 3755, 295, 309, 51218, 51218, 407, 437, 311, 4532, 2771, 293, 983, 307, 309, 1880, 30, 51342, 51394, 2205, 51494, 51522, 25642, 2771, 307, 1936, 364, 1476, 664, 6206, 2848, 1476, 664, 6206, 307, 2099, 337, 12509, 16235, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.32969175536057044, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.00017120063421316445}, {"id": 10, "seek": 2580, "start": 42.88, "end": 45.36, "text": " So what's micro grad and why is it interesting?", "tokens": [50364, 18974, 295, 4532, 2771, 586, 4532, 2771, 307, 341, 6405, 300, 286, 4736, 322, 290, 355, 836, 466, 732, 924, 2057, 50630, 50630, 583, 412, 264, 565, 286, 787, 17135, 264, 4009, 3089, 293, 291, 1116, 362, 281, 352, 293, 2256, 1803, 293, 534, 50858, 50894, 43225, 484, 577, 309, 1985, 50994, 50994, 407, 294, 341, 7991, 11, 286, 486, 747, 291, 807, 309, 1823, 538, 1823, 293, 733, 295, 2871, 322, 439, 264, 3755, 295, 309, 51218, 51218, 407, 437, 311, 4532, 2771, 293, 983, 307, 309, 1880, 30, 51342, 51394, 2205, 51494, 51522, 25642, 2771, 307, 1936, 364, 1476, 664, 6206, 2848, 1476, 664, 6206, 307, 2099, 337, 12509, 16235, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.32969175536057044, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.00017120063421316445}, {"id": 11, "seek": 2580, "start": 46.400000000000006, "end": 48.400000000000006, "text": " Good", "tokens": [50364, 18974, 295, 4532, 2771, 586, 4532, 2771, 307, 341, 6405, 300, 286, 4736, 322, 290, 355, 836, 466, 732, 924, 2057, 50630, 50630, 583, 412, 264, 565, 286, 787, 17135, 264, 4009, 3089, 293, 291, 1116, 362, 281, 352, 293, 2256, 1803, 293, 534, 50858, 50894, 43225, 484, 577, 309, 1985, 50994, 50994, 407, 294, 341, 7991, 11, 286, 486, 747, 291, 807, 309, 1823, 538, 1823, 293, 733, 295, 2871, 322, 439, 264, 3755, 295, 309, 51218, 51218, 407, 437, 311, 4532, 2771, 293, 983, 307, 309, 1880, 30, 51342, 51394, 2205, 51494, 51522, 25642, 2771, 307, 1936, 364, 1476, 664, 6206, 2848, 1476, 664, 6206, 307, 2099, 337, 12509, 16235, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.32969175536057044, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.00017120063421316445}, {"id": 12, "seek": 2580, "start": 48.96, "end": 53.36, "text": " Micro grad is basically an autograd engine autograd is short for automatic gradient", "tokens": [50364, 18974, 295, 4532, 2771, 586, 4532, 2771, 307, 341, 6405, 300, 286, 4736, 322, 290, 355, 836, 466, 732, 924, 2057, 50630, 50630, 583, 412, 264, 565, 286, 787, 17135, 264, 4009, 3089, 293, 291, 1116, 362, 281, 352, 293, 2256, 1803, 293, 534, 50858, 50894, 43225, 484, 577, 309, 1985, 50994, 50994, 407, 294, 341, 7991, 11, 286, 486, 747, 291, 807, 309, 1823, 538, 1823, 293, 733, 295, 2871, 322, 439, 264, 3755, 295, 309, 51218, 51218, 407, 437, 311, 4532, 2771, 293, 983, 307, 309, 1880, 30, 51342, 51394, 2205, 51494, 51522, 25642, 2771, 307, 1936, 364, 1476, 664, 6206, 2848, 1476, 664, 6206, 307, 2099, 337, 12509, 16235, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.32969175536057044, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.00017120063421316445}, {"id": 13, "seek": 5336, "start": 53.36, "end": 56.08, "text": " And really what it does is it implements backpropagation", "tokens": [50364, 400, 534, 437, 309, 775, 307, 309, 704, 17988, 646, 79, 1513, 559, 399, 50500, 50500, 823, 646, 79, 1513, 559, 399, 307, 341, 9284, 300, 4045, 291, 281, 19621, 13059, 264, 16235, 295, 50752, 50800, 2188, 733, 295, 257, 4470, 2445, 365, 3104, 281, 264, 17443, 295, 257, 18161, 3209, 51012, 51012, 400, 437, 300, 4045, 505, 281, 360, 550, 307, 321, 393, 17138, 19020, 10864, 264, 17443, 295, 300, 18161, 3209, 51232, 51232, 1407, 17522, 264, 4470, 2445, 293, 4412, 3470, 264, 14170, 295, 264, 3209, 51428, 51428, 407, 646, 79, 1513, 559, 399, 576, 312, 412, 264, 18894, 4965, 295, 604, 4363, 2452, 18161, 3209, 6405, 411, 584, 9953, 51, 284, 339, 420, 26401, 55, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.22498815567767033, "compression_ratio": 1.894927536231884, "no_speech_prob": 3.611645979617606e-06}, {"id": 14, "seek": 5336, "start": 56.08, "end": 61.12, "text": " Now backpropagation is this algorithm that allows you to efficiently evaluate the gradient of", "tokens": [50364, 400, 534, 437, 309, 775, 307, 309, 704, 17988, 646, 79, 1513, 559, 399, 50500, 50500, 823, 646, 79, 1513, 559, 399, 307, 341, 9284, 300, 4045, 291, 281, 19621, 13059, 264, 16235, 295, 50752, 50800, 2188, 733, 295, 257, 4470, 2445, 365, 3104, 281, 264, 17443, 295, 257, 18161, 3209, 51012, 51012, 400, 437, 300, 4045, 505, 281, 360, 550, 307, 321, 393, 17138, 19020, 10864, 264, 17443, 295, 300, 18161, 3209, 51232, 51232, 1407, 17522, 264, 4470, 2445, 293, 4412, 3470, 264, 14170, 295, 264, 3209, 51428, 51428, 407, 646, 79, 1513, 559, 399, 576, 312, 412, 264, 18894, 4965, 295, 604, 4363, 2452, 18161, 3209, 6405, 411, 584, 9953, 51, 284, 339, 420, 26401, 55, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.22498815567767033, "compression_ratio": 1.894927536231884, "no_speech_prob": 3.611645979617606e-06}, {"id": 15, "seek": 5336, "start": 62.08, "end": 66.32, "text": " Some kind of a loss function with respect to the weights of a neural network", "tokens": [50364, 400, 534, 437, 309, 775, 307, 309, 704, 17988, 646, 79, 1513, 559, 399, 50500, 50500, 823, 646, 79, 1513, 559, 399, 307, 341, 9284, 300, 4045, 291, 281, 19621, 13059, 264, 16235, 295, 50752, 50800, 2188, 733, 295, 257, 4470, 2445, 365, 3104, 281, 264, 17443, 295, 257, 18161, 3209, 51012, 51012, 400, 437, 300, 4045, 505, 281, 360, 550, 307, 321, 393, 17138, 19020, 10864, 264, 17443, 295, 300, 18161, 3209, 51232, 51232, 1407, 17522, 264, 4470, 2445, 293, 4412, 3470, 264, 14170, 295, 264, 3209, 51428, 51428, 407, 646, 79, 1513, 559, 399, 576, 312, 412, 264, 18894, 4965, 295, 604, 4363, 2452, 18161, 3209, 6405, 411, 584, 9953, 51, 284, 339, 420, 26401, 55, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.22498815567767033, "compression_ratio": 1.894927536231884, "no_speech_prob": 3.611645979617606e-06}, {"id": 16, "seek": 5336, "start": 66.32, "end": 70.72, "text": " And what that allows us to do then is we can iteratively tune the weights of that neural network", "tokens": [50364, 400, 534, 437, 309, 775, 307, 309, 704, 17988, 646, 79, 1513, 559, 399, 50500, 50500, 823, 646, 79, 1513, 559, 399, 307, 341, 9284, 300, 4045, 291, 281, 19621, 13059, 264, 16235, 295, 50752, 50800, 2188, 733, 295, 257, 4470, 2445, 365, 3104, 281, 264, 17443, 295, 257, 18161, 3209, 51012, 51012, 400, 437, 300, 4045, 505, 281, 360, 550, 307, 321, 393, 17138, 19020, 10864, 264, 17443, 295, 300, 18161, 3209, 51232, 51232, 1407, 17522, 264, 4470, 2445, 293, 4412, 3470, 264, 14170, 295, 264, 3209, 51428, 51428, 407, 646, 79, 1513, 559, 399, 576, 312, 412, 264, 18894, 4965, 295, 604, 4363, 2452, 18161, 3209, 6405, 411, 584, 9953, 51, 284, 339, 420, 26401, 55, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.22498815567767033, "compression_ratio": 1.894927536231884, "no_speech_prob": 3.611645979617606e-06}, {"id": 17, "seek": 5336, "start": 70.72, "end": 74.64, "text": " To minimize the loss function and therefore improve the accuracy of the network", "tokens": [50364, 400, 534, 437, 309, 775, 307, 309, 704, 17988, 646, 79, 1513, 559, 399, 50500, 50500, 823, 646, 79, 1513, 559, 399, 307, 341, 9284, 300, 4045, 291, 281, 19621, 13059, 264, 16235, 295, 50752, 50800, 2188, 733, 295, 257, 4470, 2445, 365, 3104, 281, 264, 17443, 295, 257, 18161, 3209, 51012, 51012, 400, 437, 300, 4045, 505, 281, 360, 550, 307, 321, 393, 17138, 19020, 10864, 264, 17443, 295, 300, 18161, 3209, 51232, 51232, 1407, 17522, 264, 4470, 2445, 293, 4412, 3470, 264, 14170, 295, 264, 3209, 51428, 51428, 407, 646, 79, 1513, 559, 399, 576, 312, 412, 264, 18894, 4965, 295, 604, 4363, 2452, 18161, 3209, 6405, 411, 584, 9953, 51, 284, 339, 420, 26401, 55, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.22498815567767033, "compression_ratio": 1.894927536231884, "no_speech_prob": 3.611645979617606e-06}, {"id": 18, "seek": 5336, "start": 74.64, "end": 81.68, "text": " So backpropagation would be at the mathematical core of any modern deep neural network library like say PyTorch or JAX", "tokens": [50364, 400, 534, 437, 309, 775, 307, 309, 704, 17988, 646, 79, 1513, 559, 399, 50500, 50500, 823, 646, 79, 1513, 559, 399, 307, 341, 9284, 300, 4045, 291, 281, 19621, 13059, 264, 16235, 295, 50752, 50800, 2188, 733, 295, 257, 4470, 2445, 365, 3104, 281, 264, 17443, 295, 257, 18161, 3209, 51012, 51012, 400, 437, 300, 4045, 505, 281, 360, 550, 307, 321, 393, 17138, 19020, 10864, 264, 17443, 295, 300, 18161, 3209, 51232, 51232, 1407, 17522, 264, 4470, 2445, 293, 4412, 3470, 264, 14170, 295, 264, 3209, 51428, 51428, 407, 646, 79, 1513, 559, 399, 576, 312, 412, 264, 18894, 4965, 295, 604, 4363, 2452, 18161, 3209, 6405, 411, 584, 9953, 51, 284, 339, 420, 26401, 55, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.22498815567767033, "compression_ratio": 1.894927536231884, "no_speech_prob": 3.611645979617606e-06}, {"id": 19, "seek": 8168, "start": 81.68, "end": 86.96000000000001, "text": " So the functionality of micro grad is I think best illustrated by an example. So if we just scroll down here", "tokens": [50364, 407, 264, 14980, 295, 4532, 2771, 307, 286, 519, 1151, 33875, 538, 364, 1365, 13, 407, 498, 321, 445, 11369, 760, 510, 50628, 50664, 291, 603, 536, 300, 4532, 2771, 1936, 4045, 291, 281, 1322, 484, 18894, 15277, 293, 50896, 50940, 1692, 437, 321, 366, 884, 307, 321, 362, 364, 6114, 300, 321, 434, 2390, 484, 689, 291, 362, 732, 15743, 257, 293, 272, 51188, 51220, 400, 291, 603, 536, 300, 257, 293, 272, 366, 3671, 1451, 293, 732, 457, 321, 366, 21993, 729, 51464, 51492, 7188, 1247, 666, 341, 2158, 2657, 300, 321, 366, 516, 281, 1322, 484, 382, 644, 295, 4532, 2771, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.2124715911017524, "compression_ratio": 1.810077519379845, "no_speech_prob": 1.9637393506855005e-06}, {"id": 20, "seek": 8168, "start": 87.68, "end": 92.32000000000001, "text": " you'll see that micro grad basically allows you to build out mathematical expressions and", "tokens": [50364, 407, 264, 14980, 295, 4532, 2771, 307, 286, 519, 1151, 33875, 538, 364, 1365, 13, 407, 498, 321, 445, 11369, 760, 510, 50628, 50664, 291, 603, 536, 300, 4532, 2771, 1936, 4045, 291, 281, 1322, 484, 18894, 15277, 293, 50896, 50940, 1692, 437, 321, 366, 884, 307, 321, 362, 364, 6114, 300, 321, 434, 2390, 484, 689, 291, 362, 732, 15743, 257, 293, 272, 51188, 51220, 400, 291, 603, 536, 300, 257, 293, 272, 366, 3671, 1451, 293, 732, 457, 321, 366, 21993, 729, 51464, 51492, 7188, 1247, 666, 341, 2158, 2657, 300, 321, 366, 516, 281, 1322, 484, 382, 644, 295, 4532, 2771, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.2124715911017524, "compression_ratio": 1.810077519379845, "no_speech_prob": 1.9637393506855005e-06}, {"id": 21, "seek": 8168, "start": 93.2, "end": 98.16000000000001, "text": " Here what we are doing is we have an expression that we're building out where you have two inputs a and b", "tokens": [50364, 407, 264, 14980, 295, 4532, 2771, 307, 286, 519, 1151, 33875, 538, 364, 1365, 13, 407, 498, 321, 445, 11369, 760, 510, 50628, 50664, 291, 603, 536, 300, 4532, 2771, 1936, 4045, 291, 281, 1322, 484, 18894, 15277, 293, 50896, 50940, 1692, 437, 321, 366, 884, 307, 321, 362, 364, 6114, 300, 321, 434, 2390, 484, 689, 291, 362, 732, 15743, 257, 293, 272, 51188, 51220, 400, 291, 603, 536, 300, 257, 293, 272, 366, 3671, 1451, 293, 732, 457, 321, 366, 21993, 729, 51464, 51492, 7188, 1247, 666, 341, 2158, 2657, 300, 321, 366, 516, 281, 1322, 484, 382, 644, 295, 4532, 2771, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.2124715911017524, "compression_ratio": 1.810077519379845, "no_speech_prob": 1.9637393506855005e-06}, {"id": 22, "seek": 8168, "start": 98.80000000000001, "end": 103.68, "text": " And you'll see that a and b are negative four and two but we are wrapping those", "tokens": [50364, 407, 264, 14980, 295, 4532, 2771, 307, 286, 519, 1151, 33875, 538, 364, 1365, 13, 407, 498, 321, 445, 11369, 760, 510, 50628, 50664, 291, 603, 536, 300, 4532, 2771, 1936, 4045, 291, 281, 1322, 484, 18894, 15277, 293, 50896, 50940, 1692, 437, 321, 366, 884, 307, 321, 362, 364, 6114, 300, 321, 434, 2390, 484, 689, 291, 362, 732, 15743, 257, 293, 272, 51188, 51220, 400, 291, 603, 536, 300, 257, 293, 272, 366, 3671, 1451, 293, 732, 457, 321, 366, 21993, 729, 51464, 51492, 7188, 1247, 666, 341, 2158, 2657, 300, 321, 366, 516, 281, 1322, 484, 382, 644, 295, 4532, 2771, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.2124715911017524, "compression_ratio": 1.810077519379845, "no_speech_prob": 1.9637393506855005e-06}, {"id": 23, "seek": 8168, "start": 104.24000000000001, "end": 108.56, "text": " Values into this value object that we are going to build out as part of micro grad", "tokens": [50364, 407, 264, 14980, 295, 4532, 2771, 307, 286, 519, 1151, 33875, 538, 364, 1365, 13, 407, 498, 321, 445, 11369, 760, 510, 50628, 50664, 291, 603, 536, 300, 4532, 2771, 1936, 4045, 291, 281, 1322, 484, 18894, 15277, 293, 50896, 50940, 1692, 437, 321, 366, 884, 307, 321, 362, 364, 6114, 300, 321, 434, 2390, 484, 689, 291, 362, 732, 15743, 257, 293, 272, 51188, 51220, 400, 291, 603, 536, 300, 257, 293, 272, 366, 3671, 1451, 293, 732, 457, 321, 366, 21993, 729, 51464, 51492, 7188, 1247, 666, 341, 2158, 2657, 300, 321, 366, 516, 281, 1322, 484, 382, 644, 295, 4532, 2771, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.2124715911017524, "compression_ratio": 1.810077519379845, "no_speech_prob": 1.9637393506855005e-06}, {"id": 24, "seek": 10856, "start": 108.56, "end": 112.08, "text": " So this value object will wrap the numbers themselves", "tokens": [50364, 407, 341, 2158, 2657, 486, 7019, 264, 3547, 2969, 50540, 50568, 400, 550, 321, 366, 516, 281, 1322, 484, 257, 18894, 6114, 510, 689, 257, 293, 272, 366, 50784, 50816, 6531, 22892, 666, 269, 274, 293, 4728, 308, 283, 293, 290, 50996, 51020, 400, 741, 478, 4099, 512, 295, 264, 2445, 512, 295, 264, 14980, 295, 4532, 2771, 293, 264, 7705, 300, 309, 9346, 51244, 51264, 407, 291, 393, 909, 732, 2158, 6565, 291, 393, 12972, 552, 291, 393, 5300, 552, 281, 257, 5754, 1347, 51532, 51552, 509, 393, 18687, 538, 472, 2485, 473, 30725, 412, 4018, 51700, 51732], "temperature": 0.0, "avg_logprob": -0.23413002845084313, "compression_ratio": 1.7222222222222223, "no_speech_prob": 8.664333108754363e-06}, {"id": 25, "seek": 10856, "start": 112.64, "end": 116.96000000000001, "text": " And then we are going to build out a mathematical expression here where a and b are", "tokens": [50364, 407, 341, 2158, 2657, 486, 7019, 264, 3547, 2969, 50540, 50568, 400, 550, 321, 366, 516, 281, 1322, 484, 257, 18894, 6114, 510, 689, 257, 293, 272, 366, 50784, 50816, 6531, 22892, 666, 269, 274, 293, 4728, 308, 283, 293, 290, 50996, 51020, 400, 741, 478, 4099, 512, 295, 264, 2445, 512, 295, 264, 14980, 295, 4532, 2771, 293, 264, 7705, 300, 309, 9346, 51244, 51264, 407, 291, 393, 909, 732, 2158, 6565, 291, 393, 12972, 552, 291, 393, 5300, 552, 281, 257, 5754, 1347, 51532, 51552, 509, 393, 18687, 538, 472, 2485, 473, 30725, 412, 4018, 51700, 51732], "temperature": 0.0, "avg_logprob": -0.23413002845084313, "compression_ratio": 1.7222222222222223, "no_speech_prob": 8.664333108754363e-06}, {"id": 26, "seek": 10856, "start": 117.60000000000001, "end": 121.2, "text": " Transformed into c d and eventually e f and g", "tokens": [50364, 407, 341, 2158, 2657, 486, 7019, 264, 3547, 2969, 50540, 50568, 400, 550, 321, 366, 516, 281, 1322, 484, 257, 18894, 6114, 510, 689, 257, 293, 272, 366, 50784, 50816, 6531, 22892, 666, 269, 274, 293, 4728, 308, 283, 293, 290, 50996, 51020, 400, 741, 478, 4099, 512, 295, 264, 2445, 512, 295, 264, 14980, 295, 4532, 2771, 293, 264, 7705, 300, 309, 9346, 51244, 51264, 407, 291, 393, 909, 732, 2158, 6565, 291, 393, 12972, 552, 291, 393, 5300, 552, 281, 257, 5754, 1347, 51532, 51552, 509, 393, 18687, 538, 472, 2485, 473, 30725, 412, 4018, 51700, 51732], "temperature": 0.0, "avg_logprob": -0.23413002845084313, "compression_ratio": 1.7222222222222223, "no_speech_prob": 8.664333108754363e-06}, {"id": 27, "seek": 10856, "start": 121.68, "end": 126.16, "text": " And i'm showing some of the function some of the functionality of micro grad and the operations that it supports", "tokens": [50364, 407, 341, 2158, 2657, 486, 7019, 264, 3547, 2969, 50540, 50568, 400, 550, 321, 366, 516, 281, 1322, 484, 257, 18894, 6114, 510, 689, 257, 293, 272, 366, 50784, 50816, 6531, 22892, 666, 269, 274, 293, 4728, 308, 283, 293, 290, 50996, 51020, 400, 741, 478, 4099, 512, 295, 264, 2445, 512, 295, 264, 14980, 295, 4532, 2771, 293, 264, 7705, 300, 309, 9346, 51244, 51264, 407, 291, 393, 909, 732, 2158, 6565, 291, 393, 12972, 552, 291, 393, 5300, 552, 281, 257, 5754, 1347, 51532, 51552, 509, 393, 18687, 538, 472, 2485, 473, 30725, 412, 4018, 51700, 51732], "temperature": 0.0, "avg_logprob": -0.23413002845084313, "compression_ratio": 1.7222222222222223, "no_speech_prob": 8.664333108754363e-06}, {"id": 28, "seek": 10856, "start": 126.56, "end": 131.92000000000002, "text": " So you can add two value objects you can multiply them you can raise them to a constant power", "tokens": [50364, 407, 341, 2158, 2657, 486, 7019, 264, 3547, 2969, 50540, 50568, 400, 550, 321, 366, 516, 281, 1322, 484, 257, 18894, 6114, 510, 689, 257, 293, 272, 366, 50784, 50816, 6531, 22892, 666, 269, 274, 293, 4728, 308, 283, 293, 290, 50996, 51020, 400, 741, 478, 4099, 512, 295, 264, 2445, 512, 295, 264, 14980, 295, 4532, 2771, 293, 264, 7705, 300, 309, 9346, 51244, 51264, 407, 291, 393, 909, 732, 2158, 6565, 291, 393, 12972, 552, 291, 393, 5300, 552, 281, 257, 5754, 1347, 51532, 51552, 509, 393, 18687, 538, 472, 2485, 473, 30725, 412, 4018, 51700, 51732], "temperature": 0.0, "avg_logprob": -0.23413002845084313, "compression_ratio": 1.7222222222222223, "no_speech_prob": 8.664333108754363e-06}, {"id": 29, "seek": 10856, "start": 132.32, "end": 135.28, "text": " You can offset by one negate squash at zero", "tokens": [50364, 407, 341, 2158, 2657, 486, 7019, 264, 3547, 2969, 50540, 50568, 400, 550, 321, 366, 516, 281, 1322, 484, 257, 18894, 6114, 510, 689, 257, 293, 272, 366, 50784, 50816, 6531, 22892, 666, 269, 274, 293, 4728, 308, 283, 293, 290, 50996, 51020, 400, 741, 478, 4099, 512, 295, 264, 2445, 512, 295, 264, 14980, 295, 4532, 2771, 293, 264, 7705, 300, 309, 9346, 51244, 51264, 407, 291, 393, 909, 732, 2158, 6565, 291, 393, 12972, 552, 291, 393, 5300, 552, 281, 257, 5754, 1347, 51532, 51552, 509, 393, 18687, 538, 472, 2485, 473, 30725, 412, 4018, 51700, 51732], "temperature": 0.0, "avg_logprob": -0.23413002845084313, "compression_ratio": 1.7222222222222223, "no_speech_prob": 8.664333108754363e-06}, {"id": 30, "seek": 13528, "start": 135.28, "end": 138.8, "text": " Square divide by constant divide by it etc", "tokens": [50364, 16463, 9845, 538, 5754, 9845, 538, 309, 5183, 50540, 50572, 400, 370, 321, 434, 2390, 484, 364, 6114, 4295, 365, 365, 613, 732, 15743, 257, 293, 272, 293, 321, 434, 4084, 364, 5598, 2158, 295, 290, 50912, 50952, 400, 4532, 2771, 486, 294, 264, 3678, 1322, 484, 341, 2302, 18894, 6114, 13, 407, 309, 486, 337, 1365, 458, 300, 269, 307, 611, 257, 2158, 51340, 51376, 383, 390, 257, 1874, 295, 364, 4500, 6916, 293, 264, 51536, 51572, 1440, 13891, 295, 269, 366, 51684, 51724], "temperature": 0.0, "avg_logprob": -0.2665830371023595, "compression_ratio": 1.6576576576576576, "no_speech_prob": 3.8069791230554983e-07}, {"id": 31, "seek": 13528, "start": 139.44, "end": 146.24, "text": " And so we're building out an expression graph with with these two inputs a and b and we're creating an output value of g", "tokens": [50364, 16463, 9845, 538, 5754, 9845, 538, 309, 5183, 50540, 50572, 400, 370, 321, 434, 2390, 484, 364, 6114, 4295, 365, 365, 613, 732, 15743, 257, 293, 272, 293, 321, 434, 4084, 364, 5598, 2158, 295, 290, 50912, 50952, 400, 4532, 2771, 486, 294, 264, 3678, 1322, 484, 341, 2302, 18894, 6114, 13, 407, 309, 486, 337, 1365, 458, 300, 269, 307, 611, 257, 2158, 51340, 51376, 383, 390, 257, 1874, 295, 364, 4500, 6916, 293, 264, 51536, 51572, 1440, 13891, 295, 269, 366, 51684, 51724], "temperature": 0.0, "avg_logprob": -0.2665830371023595, "compression_ratio": 1.6576576576576576, "no_speech_prob": 3.8069791230554983e-07}, {"id": 32, "seek": 13528, "start": 147.04, "end": 154.8, "text": " And micro grad will in the background build out this entire mathematical expression. So it will for example know that c is also a value", "tokens": [50364, 16463, 9845, 538, 5754, 9845, 538, 309, 5183, 50540, 50572, 400, 370, 321, 434, 2390, 484, 364, 6114, 4295, 365, 365, 613, 732, 15743, 257, 293, 272, 293, 321, 434, 4084, 364, 5598, 2158, 295, 290, 50912, 50952, 400, 4532, 2771, 486, 294, 264, 3678, 1322, 484, 341, 2302, 18894, 6114, 13, 407, 309, 486, 337, 1365, 458, 300, 269, 307, 611, 257, 2158, 51340, 51376, 383, 390, 257, 1874, 295, 364, 4500, 6916, 293, 264, 51536, 51572, 1440, 13891, 295, 269, 366, 51684, 51724], "temperature": 0.0, "avg_logprob": -0.2665830371023595, "compression_ratio": 1.6576576576576576, "no_speech_prob": 3.8069791230554983e-07}, {"id": 33, "seek": 13528, "start": 155.52, "end": 158.72, "text": " C was a result of an addition operation and the", "tokens": [50364, 16463, 9845, 538, 5754, 9845, 538, 309, 5183, 50540, 50572, 400, 370, 321, 434, 2390, 484, 364, 6114, 4295, 365, 365, 613, 732, 15743, 257, 293, 272, 293, 321, 434, 4084, 364, 5598, 2158, 295, 290, 50912, 50952, 400, 4532, 2771, 486, 294, 264, 3678, 1322, 484, 341, 2302, 18894, 6114, 13, 407, 309, 486, 337, 1365, 458, 300, 269, 307, 611, 257, 2158, 51340, 51376, 383, 390, 257, 1874, 295, 364, 4500, 6916, 293, 264, 51536, 51572, 1440, 13891, 295, 269, 366, 51684, 51724], "temperature": 0.0, "avg_logprob": -0.2665830371023595, "compression_ratio": 1.6576576576576576, "no_speech_prob": 3.8069791230554983e-07}, {"id": 34, "seek": 13528, "start": 159.44, "end": 161.68, "text": " child nodes of c are", "tokens": [50364, 16463, 9845, 538, 5754, 9845, 538, 309, 5183, 50540, 50572, 400, 370, 321, 434, 2390, 484, 364, 6114, 4295, 365, 365, 613, 732, 15743, 257, 293, 272, 293, 321, 434, 4084, 364, 5598, 2158, 295, 290, 50912, 50952, 400, 4532, 2771, 486, 294, 264, 3678, 1322, 484, 341, 2302, 18894, 6114, 13, 407, 309, 486, 337, 1365, 458, 300, 269, 307, 611, 257, 2158, 51340, 51376, 383, 390, 257, 1874, 295, 364, 4500, 6916, 293, 264, 51536, 51572, 1440, 13891, 295, 269, 366, 51684, 51724], "temperature": 0.0, "avg_logprob": -0.2665830371023595, "compression_ratio": 1.6576576576576576, "no_speech_prob": 3.8069791230554983e-07}, {"id": 35, "seek": 16168, "start": 161.68, "end": 166.08, "text": " a and b because the and it will maintain pointers to a and b value objects", "tokens": [50364, 257, 293, 272, 570, 264, 293, 309, 486, 6909, 44548, 281, 257, 293, 272, 2158, 6565, 50584, 50600, 407, 321, 603, 1936, 458, 2293, 577, 439, 295, 341, 307, 9897, 484, 50732, 50768, 400, 550, 406, 787, 393, 321, 360, 437, 321, 818, 264, 2128, 1320, 689, 321, 767, 574, 412, 264, 2158, 295, 290, 11, 295, 1164, 51020, 51020, 663, 311, 1238, 15325, 13, 492, 486, 2105, 300, 1228, 264, 5893, 1412, 19667, 51224, 51248, 400, 370, 264, 5598, 295, 264, 2128, 1320, 264, 2158, 295, 290, 307, 7699, 1451, 935, 3407, 13, 467, 4523, 484, 51520, 51548, 583, 264, 955, 2028, 307, 300, 321, 393, 611, 747, 341, 51676, 51708], "temperature": 0.0, "avg_logprob": -0.21641128941586144, "compression_ratio": 1.7350746268656716, "no_speech_prob": 5.338008577382425e-06}, {"id": 36, "seek": 16168, "start": 166.4, "end": 169.04000000000002, "text": " So we'll basically know exactly how all of this is laid out", "tokens": [50364, 257, 293, 272, 570, 264, 293, 309, 486, 6909, 44548, 281, 257, 293, 272, 2158, 6565, 50584, 50600, 407, 321, 603, 1936, 458, 2293, 577, 439, 295, 341, 307, 9897, 484, 50732, 50768, 400, 550, 406, 787, 393, 321, 360, 437, 321, 818, 264, 2128, 1320, 689, 321, 767, 574, 412, 264, 2158, 295, 290, 11, 295, 1164, 51020, 51020, 663, 311, 1238, 15325, 13, 492, 486, 2105, 300, 1228, 264, 5893, 1412, 19667, 51224, 51248, 400, 370, 264, 5598, 295, 264, 2128, 1320, 264, 2158, 295, 290, 307, 7699, 1451, 935, 3407, 13, 467, 4523, 484, 51520, 51548, 583, 264, 955, 2028, 307, 300, 321, 393, 611, 747, 341, 51676, 51708], "temperature": 0.0, "avg_logprob": -0.21641128941586144, "compression_ratio": 1.7350746268656716, "no_speech_prob": 5.338008577382425e-06}, {"id": 37, "seek": 16168, "start": 169.76000000000002, "end": 174.8, "text": " And then not only can we do what we call the forward pass where we actually look at the value of g, of course", "tokens": [50364, 257, 293, 272, 570, 264, 293, 309, 486, 6909, 44548, 281, 257, 293, 272, 2158, 6565, 50584, 50600, 407, 321, 603, 1936, 458, 2293, 577, 439, 295, 341, 307, 9897, 484, 50732, 50768, 400, 550, 406, 787, 393, 321, 360, 437, 321, 818, 264, 2128, 1320, 689, 321, 767, 574, 412, 264, 2158, 295, 290, 11, 295, 1164, 51020, 51020, 663, 311, 1238, 15325, 13, 492, 486, 2105, 300, 1228, 264, 5893, 1412, 19667, 51224, 51248, 400, 370, 264, 5598, 295, 264, 2128, 1320, 264, 2158, 295, 290, 307, 7699, 1451, 935, 3407, 13, 467, 4523, 484, 51520, 51548, 583, 264, 955, 2028, 307, 300, 321, 393, 611, 747, 341, 51676, 51708], "temperature": 0.0, "avg_logprob": -0.21641128941586144, "compression_ratio": 1.7350746268656716, "no_speech_prob": 5.338008577382425e-06}, {"id": 38, "seek": 16168, "start": 174.8, "end": 178.88, "text": " That's pretty straightforward. We will access that using the dot data attribute", "tokens": [50364, 257, 293, 272, 570, 264, 293, 309, 486, 6909, 44548, 281, 257, 293, 272, 2158, 6565, 50584, 50600, 407, 321, 603, 1936, 458, 2293, 577, 439, 295, 341, 307, 9897, 484, 50732, 50768, 400, 550, 406, 787, 393, 321, 360, 437, 321, 818, 264, 2128, 1320, 689, 321, 767, 574, 412, 264, 2158, 295, 290, 11, 295, 1164, 51020, 51020, 663, 311, 1238, 15325, 13, 492, 486, 2105, 300, 1228, 264, 5893, 1412, 19667, 51224, 51248, 400, 370, 264, 5598, 295, 264, 2128, 1320, 264, 2158, 295, 290, 307, 7699, 1451, 935, 3407, 13, 467, 4523, 484, 51520, 51548, 583, 264, 955, 2028, 307, 300, 321, 393, 611, 747, 341, 51676, 51708], "temperature": 0.0, "avg_logprob": -0.21641128941586144, "compression_ratio": 1.7350746268656716, "no_speech_prob": 5.338008577382425e-06}, {"id": 39, "seek": 16168, "start": 179.36, "end": 184.8, "text": " And so the output of the forward pass the value of g is twenty four point seven. It turns out", "tokens": [50364, 257, 293, 272, 570, 264, 293, 309, 486, 6909, 44548, 281, 257, 293, 272, 2158, 6565, 50584, 50600, 407, 321, 603, 1936, 458, 2293, 577, 439, 295, 341, 307, 9897, 484, 50732, 50768, 400, 550, 406, 787, 393, 321, 360, 437, 321, 818, 264, 2128, 1320, 689, 321, 767, 574, 412, 264, 2158, 295, 290, 11, 295, 1164, 51020, 51020, 663, 311, 1238, 15325, 13, 492, 486, 2105, 300, 1228, 264, 5893, 1412, 19667, 51224, 51248, 400, 370, 264, 5598, 295, 264, 2128, 1320, 264, 2158, 295, 290, 307, 7699, 1451, 935, 3407, 13, 467, 4523, 484, 51520, 51548, 583, 264, 955, 2028, 307, 300, 321, 393, 611, 747, 341, 51676, 51708], "temperature": 0.0, "avg_logprob": -0.21641128941586144, "compression_ratio": 1.7350746268656716, "no_speech_prob": 5.338008577382425e-06}, {"id": 40, "seek": 16168, "start": 185.36, "end": 187.92000000000002, "text": " But the big deal is that we can also take this", "tokens": [50364, 257, 293, 272, 570, 264, 293, 309, 486, 6909, 44548, 281, 257, 293, 272, 2158, 6565, 50584, 50600, 407, 321, 603, 1936, 458, 2293, 577, 439, 295, 341, 307, 9897, 484, 50732, 50768, 400, 550, 406, 787, 393, 321, 360, 437, 321, 818, 264, 2128, 1320, 689, 321, 767, 574, 412, 264, 2158, 295, 290, 11, 295, 1164, 51020, 51020, 663, 311, 1238, 15325, 13, 492, 486, 2105, 300, 1228, 264, 5893, 1412, 19667, 51224, 51248, 400, 370, 264, 5598, 295, 264, 2128, 1320, 264, 2158, 295, 290, 307, 7699, 1451, 935, 3407, 13, 467, 4523, 484, 51520, 51548, 583, 264, 955, 2028, 307, 300, 321, 393, 611, 747, 341, 51676, 51708], "temperature": 0.0, "avg_logprob": -0.21641128941586144, "compression_ratio": 1.7350746268656716, "no_speech_prob": 5.338008577382425e-06}, {"id": 41, "seek": 18792, "start": 187.92, "end": 194.79999999999998, "text": " G value object and we can call dot backward and this will basically initialize back propagation at the node g", "tokens": [50364, 460, 2158, 2657, 293, 321, 393, 818, 5893, 23897, 293, 341, 486, 1936, 5883, 1125, 646, 38377, 412, 264, 9984, 290, 50708, 50760, 400, 437, 646, 38377, 307, 516, 281, 360, 307, 309, 311, 516, 281, 722, 412, 290, 293, 309, 311, 516, 281, 352, 50952, 50980, 12204, 807, 300, 6114, 4295, 293, 309, 311, 516, 281, 20560, 3413, 3079, 264, 5021, 4978, 490, 33400, 51232, 51268, 400, 437, 300, 4045, 505, 281, 360, 550, 307, 321, 434, 516, 281, 13059, 1936, 264, 13760, 295, 290, 365, 3104, 281, 439, 264, 6920, 13891, 51640, 51676], "temperature": 0.0, "avg_logprob": -0.19132022267764376, "compression_ratio": 1.8669527896995708, "no_speech_prob": 4.495080702326959e-06}, {"id": 42, "seek": 18792, "start": 195.83999999999997, "end": 199.67999999999998, "text": " And what back propagation is going to do is it's going to start at g and it's going to go", "tokens": [50364, 460, 2158, 2657, 293, 321, 393, 818, 5893, 23897, 293, 341, 486, 1936, 5883, 1125, 646, 38377, 412, 264, 9984, 290, 50708, 50760, 400, 437, 646, 38377, 307, 516, 281, 360, 307, 309, 311, 516, 281, 722, 412, 290, 293, 309, 311, 516, 281, 352, 50952, 50980, 12204, 807, 300, 6114, 4295, 293, 309, 311, 516, 281, 20560, 3413, 3079, 264, 5021, 4978, 490, 33400, 51232, 51268, 400, 437, 300, 4045, 505, 281, 360, 550, 307, 321, 434, 516, 281, 13059, 1936, 264, 13760, 295, 290, 365, 3104, 281, 439, 264, 6920, 13891, 51640, 51676], "temperature": 0.0, "avg_logprob": -0.19132022267764376, "compression_ratio": 1.8669527896995708, "no_speech_prob": 4.495080702326959e-06}, {"id": 43, "seek": 18792, "start": 200.23999999999998, "end": 205.27999999999997, "text": " backwards through that expression graph and it's going to recursively apply the chain rule from calculus", "tokens": [50364, 460, 2158, 2657, 293, 321, 393, 818, 5893, 23897, 293, 341, 486, 1936, 5883, 1125, 646, 38377, 412, 264, 9984, 290, 50708, 50760, 400, 437, 646, 38377, 307, 516, 281, 360, 307, 309, 311, 516, 281, 722, 412, 290, 293, 309, 311, 516, 281, 352, 50952, 50980, 12204, 807, 300, 6114, 4295, 293, 309, 311, 516, 281, 20560, 3413, 3079, 264, 5021, 4978, 490, 33400, 51232, 51268, 400, 437, 300, 4045, 505, 281, 360, 550, 307, 321, 434, 516, 281, 13059, 1936, 264, 13760, 295, 290, 365, 3104, 281, 439, 264, 6920, 13891, 51640, 51676], "temperature": 0.0, "avg_logprob": -0.19132022267764376, "compression_ratio": 1.8669527896995708, "no_speech_prob": 4.495080702326959e-06}, {"id": 44, "seek": 18792, "start": 206.0, "end": 213.44, "text": " And what that allows us to do then is we're going to evaluate basically the derivative of g with respect to all the internal nodes", "tokens": [50364, 460, 2158, 2657, 293, 321, 393, 818, 5893, 23897, 293, 341, 486, 1936, 5883, 1125, 646, 38377, 412, 264, 9984, 290, 50708, 50760, 400, 437, 646, 38377, 307, 516, 281, 360, 307, 309, 311, 516, 281, 722, 412, 290, 293, 309, 311, 516, 281, 352, 50952, 50980, 12204, 807, 300, 6114, 4295, 293, 309, 311, 516, 281, 20560, 3413, 3079, 264, 5021, 4978, 490, 33400, 51232, 51268, 400, 437, 300, 4045, 505, 281, 360, 550, 307, 321, 434, 516, 281, 13059, 1936, 264, 13760, 295, 290, 365, 3104, 281, 439, 264, 6920, 13891, 51640, 51676], "temperature": 0.0, "avg_logprob": -0.19132022267764376, "compression_ratio": 1.8669527896995708, "no_speech_prob": 4.495080702326959e-06}, {"id": 45, "seek": 21344, "start": 213.44, "end": 218.0, "text": " Like e d and c but also with respect to the inputs a and b", "tokens": [50364, 1743, 308, 274, 293, 269, 457, 611, 365, 3104, 281, 264, 15743, 257, 293, 272, 50592, 50620, 400, 550, 321, 393, 767, 14581, 341, 13760, 295, 290, 365, 3104, 281, 257, 337, 1365, 50876, 50876, 663, 311, 257, 5893, 2771, 294, 341, 1389, 50972, 50972, 467, 2314, 281, 312, 472, 11790, 3180, 293, 264, 13760, 295, 290, 365, 3104, 281, 272, 597, 611, 2314, 281, 312, 510, 2309, 3262, 293, 15815, 1732, 51304, 51332, 400, 341, 13760, 321, 603, 536, 2321, 307, 588, 1021, 1589, 570, 309, 311, 3585, 505, 577, 257, 293, 272, 51620, 51644], "temperature": 0.0, "avg_logprob": -0.1853451242252272, "compression_ratio": 1.8161434977578474, "no_speech_prob": 1.0289278179698158e-05}, {"id": 46, "seek": 21344, "start": 218.56, "end": 223.68, "text": " And then we can actually query this derivative of g with respect to a for example", "tokens": [50364, 1743, 308, 274, 293, 269, 457, 611, 365, 3104, 281, 264, 15743, 257, 293, 272, 50592, 50620, 400, 550, 321, 393, 767, 14581, 341, 13760, 295, 290, 365, 3104, 281, 257, 337, 1365, 50876, 50876, 663, 311, 257, 5893, 2771, 294, 341, 1389, 50972, 50972, 467, 2314, 281, 312, 472, 11790, 3180, 293, 264, 13760, 295, 290, 365, 3104, 281, 272, 597, 611, 2314, 281, 312, 510, 2309, 3262, 293, 15815, 1732, 51304, 51332, 400, 341, 13760, 321, 603, 536, 2321, 307, 588, 1021, 1589, 570, 309, 311, 3585, 505, 577, 257, 293, 272, 51620, 51644], "temperature": 0.0, "avg_logprob": -0.1853451242252272, "compression_ratio": 1.8161434977578474, "no_speech_prob": 1.0289278179698158e-05}, {"id": 47, "seek": 21344, "start": 223.68, "end": 225.6, "text": " That's a dot grad in this case", "tokens": [50364, 1743, 308, 274, 293, 269, 457, 611, 365, 3104, 281, 264, 15743, 257, 293, 272, 50592, 50620, 400, 550, 321, 393, 767, 14581, 341, 13760, 295, 290, 365, 3104, 281, 257, 337, 1365, 50876, 50876, 663, 311, 257, 5893, 2771, 294, 341, 1389, 50972, 50972, 467, 2314, 281, 312, 472, 11790, 3180, 293, 264, 13760, 295, 290, 365, 3104, 281, 272, 597, 611, 2314, 281, 312, 510, 2309, 3262, 293, 15815, 1732, 51304, 51332, 400, 341, 13760, 321, 603, 536, 2321, 307, 588, 1021, 1589, 570, 309, 311, 3585, 505, 577, 257, 293, 272, 51620, 51644], "temperature": 0.0, "avg_logprob": -0.1853451242252272, "compression_ratio": 1.8161434977578474, "no_speech_prob": 1.0289278179698158e-05}, {"id": 48, "seek": 21344, "start": 225.6, "end": 232.24, "text": " It happens to be one thirty eight and the derivative of g with respect to b which also happens to be here six hundred and forty five", "tokens": [50364, 1743, 308, 274, 293, 269, 457, 611, 365, 3104, 281, 264, 15743, 257, 293, 272, 50592, 50620, 400, 550, 321, 393, 767, 14581, 341, 13760, 295, 290, 365, 3104, 281, 257, 337, 1365, 50876, 50876, 663, 311, 257, 5893, 2771, 294, 341, 1389, 50972, 50972, 467, 2314, 281, 312, 472, 11790, 3180, 293, 264, 13760, 295, 290, 365, 3104, 281, 272, 597, 611, 2314, 281, 312, 510, 2309, 3262, 293, 15815, 1732, 51304, 51332, 400, 341, 13760, 321, 603, 536, 2321, 307, 588, 1021, 1589, 570, 309, 311, 3585, 505, 577, 257, 293, 272, 51620, 51644], "temperature": 0.0, "avg_logprob": -0.1853451242252272, "compression_ratio": 1.8161434977578474, "no_speech_prob": 1.0289278179698158e-05}, {"id": 49, "seek": 21344, "start": 232.8, "end": 238.56, "text": " And this derivative we'll see soon is very important information because it's telling us how a and b", "tokens": [50364, 1743, 308, 274, 293, 269, 457, 611, 365, 3104, 281, 264, 15743, 257, 293, 272, 50592, 50620, 400, 550, 321, 393, 767, 14581, 341, 13760, 295, 290, 365, 3104, 281, 257, 337, 1365, 50876, 50876, 663, 311, 257, 5893, 2771, 294, 341, 1389, 50972, 50972, 467, 2314, 281, 312, 472, 11790, 3180, 293, 264, 13760, 295, 290, 365, 3104, 281, 272, 597, 611, 2314, 281, 312, 510, 2309, 3262, 293, 15815, 1732, 51304, 51332, 400, 341, 13760, 321, 603, 536, 2321, 307, 588, 1021, 1589, 570, 309, 311, 3585, 505, 577, 257, 293, 272, 51620, 51644], "temperature": 0.0, "avg_logprob": -0.1853451242252272, "compression_ratio": 1.8161434977578474, "no_speech_prob": 1.0289278179698158e-05}, {"id": 50, "seek": 23856, "start": 238.56, "end": 245.2, "text": " Are affecting g through this mathematical expression. So in particular a dot grad is one thirty eight", "tokens": [50364, 2014, 17476, 290, 807, 341, 18894, 6114, 13, 407, 294, 1729, 257, 5893, 2771, 307, 472, 11790, 3180, 50696, 50716, 407, 498, 321, 4748, 297, 16032, 257, 293, 652, 309, 4748, 4833, 50900, 50940, 1485, 11790, 3180, 307, 3585, 505, 300, 290, 486, 1852, 293, 264, 13525, 295, 300, 4599, 307, 516, 281, 312, 472, 11790, 3180, 51208, 51232, 400, 264, 13525, 295, 4599, 295, 272, 307, 516, 281, 312, 2309, 3262, 293, 15815, 1732, 51404, 51420, 407, 300, 311, 516, 281, 980, 505, 466, 577, 290, 486, 4196, 498, 257, 293, 272, 483, 6986, 7301, 412, 264, 912, 565, 51644, 51676], "temperature": 0.0, "avg_logprob": -0.2490364955021785, "compression_ratio": 1.8565217391304347, "no_speech_prob": 2.2827269276604056e-05}, {"id": 51, "seek": 23856, "start": 245.6, "end": 249.28, "text": " So if we slightly nudge a and make it slightly larger", "tokens": [50364, 2014, 17476, 290, 807, 341, 18894, 6114, 13, 407, 294, 1729, 257, 5893, 2771, 307, 472, 11790, 3180, 50696, 50716, 407, 498, 321, 4748, 297, 16032, 257, 293, 652, 309, 4748, 4833, 50900, 50940, 1485, 11790, 3180, 307, 3585, 505, 300, 290, 486, 1852, 293, 264, 13525, 295, 300, 4599, 307, 516, 281, 312, 472, 11790, 3180, 51208, 51232, 400, 264, 13525, 295, 4599, 295, 272, 307, 516, 281, 312, 2309, 3262, 293, 15815, 1732, 51404, 51420, 407, 300, 311, 516, 281, 980, 505, 466, 577, 290, 486, 4196, 498, 257, 293, 272, 483, 6986, 7301, 412, 264, 912, 565, 51644, 51676], "temperature": 0.0, "avg_logprob": -0.2490364955021785, "compression_ratio": 1.8565217391304347, "no_speech_prob": 2.2827269276604056e-05}, {"id": 52, "seek": 23856, "start": 250.08, "end": 255.44, "text": " One thirty eight is telling us that g will grow and the slope of that growth is going to be one thirty eight", "tokens": [50364, 2014, 17476, 290, 807, 341, 18894, 6114, 13, 407, 294, 1729, 257, 5893, 2771, 307, 472, 11790, 3180, 50696, 50716, 407, 498, 321, 4748, 297, 16032, 257, 293, 652, 309, 4748, 4833, 50900, 50940, 1485, 11790, 3180, 307, 3585, 505, 300, 290, 486, 1852, 293, 264, 13525, 295, 300, 4599, 307, 516, 281, 312, 472, 11790, 3180, 51208, 51232, 400, 264, 13525, 295, 4599, 295, 272, 307, 516, 281, 312, 2309, 3262, 293, 15815, 1732, 51404, 51420, 407, 300, 311, 516, 281, 980, 505, 466, 577, 290, 486, 4196, 498, 257, 293, 272, 483, 6986, 7301, 412, 264, 912, 565, 51644, 51676], "temperature": 0.0, "avg_logprob": -0.2490364955021785, "compression_ratio": 1.8565217391304347, "no_speech_prob": 2.2827269276604056e-05}, {"id": 53, "seek": 23856, "start": 255.92000000000002, "end": 259.36, "text": " And the slope of growth of b is going to be six hundred and forty five", "tokens": [50364, 2014, 17476, 290, 807, 341, 18894, 6114, 13, 407, 294, 1729, 257, 5893, 2771, 307, 472, 11790, 3180, 50696, 50716, 407, 498, 321, 4748, 297, 16032, 257, 293, 652, 309, 4748, 4833, 50900, 50940, 1485, 11790, 3180, 307, 3585, 505, 300, 290, 486, 1852, 293, 264, 13525, 295, 300, 4599, 307, 516, 281, 312, 472, 11790, 3180, 51208, 51232, 400, 264, 13525, 295, 4599, 295, 272, 307, 516, 281, 312, 2309, 3262, 293, 15815, 1732, 51404, 51420, 407, 300, 311, 516, 281, 980, 505, 466, 577, 290, 486, 4196, 498, 257, 293, 272, 483, 6986, 7301, 412, 264, 912, 565, 51644, 51676], "temperature": 0.0, "avg_logprob": -0.2490364955021785, "compression_ratio": 1.8565217391304347, "no_speech_prob": 2.2827269276604056e-05}, {"id": 54, "seek": 23856, "start": 259.68, "end": 264.16, "text": " So that's going to tell us about how g will respond if a and b get tweaked at the same time", "tokens": [50364, 2014, 17476, 290, 807, 341, 18894, 6114, 13, 407, 294, 1729, 257, 5893, 2771, 307, 472, 11790, 3180, 50696, 50716, 407, 498, 321, 4748, 297, 16032, 257, 293, 652, 309, 4748, 4833, 50900, 50940, 1485, 11790, 3180, 307, 3585, 505, 300, 290, 486, 1852, 293, 264, 13525, 295, 300, 4599, 307, 516, 281, 312, 472, 11790, 3180, 51208, 51232, 400, 264, 13525, 295, 4599, 295, 272, 307, 516, 281, 312, 2309, 3262, 293, 15815, 1732, 51404, 51420, 407, 300, 311, 516, 281, 980, 505, 466, 577, 290, 486, 4196, 498, 257, 293, 272, 483, 6986, 7301, 412, 264, 912, 565, 51644, 51676], "temperature": 0.0, "avg_logprob": -0.2490364955021785, "compression_ratio": 1.8565217391304347, "no_speech_prob": 2.2827269276604056e-05}, {"id": 55, "seek": 26416, "start": 264.16, "end": 270.48, "text": " Okay, um now you might be confused about what this expression is that we built out here", "tokens": [50364, 1033, 11, 1105, 586, 291, 1062, 312, 9019, 466, 437, 341, 6114, 307, 300, 321, 3094, 484, 510, 50680, 50680, 400, 341, 6114, 538, 264, 636, 307, 2584, 33232, 13, 286, 445, 1027, 309, 493, 50860, 50860, 286, 478, 445, 5896, 278, 466, 264, 3685, 295, 7705, 300, 366, 8104, 538, 4532, 2771, 51028, 51052, 708, 321, 767, 534, 1127, 466, 527, 18161, 9590, 11, 457, 309, 4523, 484, 300, 18161, 9590, 366, 445, 18894, 15277, 51336, 51336, 1449, 411, 341, 472, 11, 457, 767, 4748, 857, 1570, 3219, 754, 51488, 51516, 407, 321, 434, 516, 281, 574, 412, 264, 6114, 295, 290, 365, 3104, 281, 257, 293, 272, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.43027698888187915, "compression_ratio": 1.740484429065744, "no_speech_prob": 1.750215596985072e-05}, {"id": 56, "seek": 26416, "start": 270.48, "end": 274.08000000000004, "text": " And this expression by the way is completely meaningless. I just made it up", "tokens": [50364, 1033, 11, 1105, 586, 291, 1062, 312, 9019, 466, 437, 341, 6114, 307, 300, 321, 3094, 484, 510, 50680, 50680, 400, 341, 6114, 538, 264, 636, 307, 2584, 33232, 13, 286, 445, 1027, 309, 493, 50860, 50860, 286, 478, 445, 5896, 278, 466, 264, 3685, 295, 7705, 300, 366, 8104, 538, 4532, 2771, 51028, 51052, 708, 321, 767, 534, 1127, 466, 527, 18161, 9590, 11, 457, 309, 4523, 484, 300, 18161, 9590, 366, 445, 18894, 15277, 51336, 51336, 1449, 411, 341, 472, 11, 457, 767, 4748, 857, 1570, 3219, 754, 51488, 51516, 407, 321, 434, 516, 281, 574, 412, 264, 6114, 295, 290, 365, 3104, 281, 257, 293, 272, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.43027698888187915, "compression_ratio": 1.740484429065744, "no_speech_prob": 1.750215596985072e-05}, {"id": 57, "seek": 26416, "start": 274.08000000000004, "end": 277.44000000000005, "text": " I'm just flexing about the kinds of operations that are supported by micro grad", "tokens": [50364, 1033, 11, 1105, 586, 291, 1062, 312, 9019, 466, 437, 341, 6114, 307, 300, 321, 3094, 484, 510, 50680, 50680, 400, 341, 6114, 538, 264, 636, 307, 2584, 33232, 13, 286, 445, 1027, 309, 493, 50860, 50860, 286, 478, 445, 5896, 278, 466, 264, 3685, 295, 7705, 300, 366, 8104, 538, 4532, 2771, 51028, 51052, 708, 321, 767, 534, 1127, 466, 527, 18161, 9590, 11, 457, 309, 4523, 484, 300, 18161, 9590, 366, 445, 18894, 15277, 51336, 51336, 1449, 411, 341, 472, 11, 457, 767, 4748, 857, 1570, 3219, 754, 51488, 51516, 407, 321, 434, 516, 281, 574, 412, 264, 6114, 295, 290, 365, 3104, 281, 257, 293, 272, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.43027698888187915, "compression_ratio": 1.740484429065744, "no_speech_prob": 1.750215596985072e-05}, {"id": 58, "seek": 26416, "start": 277.92, "end": 283.6, "text": " What we actually really care about our neural networks, but it turns out that neural networks are just mathematical expressions", "tokens": [50364, 1033, 11, 1105, 586, 291, 1062, 312, 9019, 466, 437, 341, 6114, 307, 300, 321, 3094, 484, 510, 50680, 50680, 400, 341, 6114, 538, 264, 636, 307, 2584, 33232, 13, 286, 445, 1027, 309, 493, 50860, 50860, 286, 478, 445, 5896, 278, 466, 264, 3685, 295, 7705, 300, 366, 8104, 538, 4532, 2771, 51028, 51052, 708, 321, 767, 534, 1127, 466, 527, 18161, 9590, 11, 457, 309, 4523, 484, 300, 18161, 9590, 366, 445, 18894, 15277, 51336, 51336, 1449, 411, 341, 472, 11, 457, 767, 4748, 857, 1570, 3219, 754, 51488, 51516, 407, 321, 434, 516, 281, 574, 412, 264, 6114, 295, 290, 365, 3104, 281, 257, 293, 272, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.43027698888187915, "compression_ratio": 1.740484429065744, "no_speech_prob": 1.750215596985072e-05}, {"id": 59, "seek": 26416, "start": 283.6, "end": 286.64000000000004, "text": " Just like this one, but actually slightly bit less crazy even", "tokens": [50364, 1033, 11, 1105, 586, 291, 1062, 312, 9019, 466, 437, 341, 6114, 307, 300, 321, 3094, 484, 510, 50680, 50680, 400, 341, 6114, 538, 264, 636, 307, 2584, 33232, 13, 286, 445, 1027, 309, 493, 50860, 50860, 286, 478, 445, 5896, 278, 466, 264, 3685, 295, 7705, 300, 366, 8104, 538, 4532, 2771, 51028, 51052, 708, 321, 767, 534, 1127, 466, 527, 18161, 9590, 11, 457, 309, 4523, 484, 300, 18161, 9590, 366, 445, 18894, 15277, 51336, 51336, 1449, 411, 341, 472, 11, 457, 767, 4748, 857, 1570, 3219, 754, 51488, 51516, 407, 321, 434, 516, 281, 574, 412, 264, 6114, 295, 290, 365, 3104, 281, 257, 293, 272, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.43027698888187915, "compression_ratio": 1.740484429065744, "no_speech_prob": 1.750215596985072e-05}, {"id": 60, "seek": 26416, "start": 287.20000000000005, "end": 291.28000000000003, "text": " So we're going to look at the expression of g with respect to a and b", "tokens": [50364, 1033, 11, 1105, 586, 291, 1062, 312, 9019, 466, 437, 341, 6114, 307, 300, 321, 3094, 484, 510, 50680, 50680, 400, 341, 6114, 538, 264, 636, 307, 2584, 33232, 13, 286, 445, 1027, 309, 493, 50860, 50860, 286, 478, 445, 5896, 278, 466, 264, 3685, 295, 7705, 300, 366, 8104, 538, 4532, 2771, 51028, 51052, 708, 321, 767, 534, 1127, 466, 527, 18161, 9590, 11, 457, 309, 4523, 484, 300, 18161, 9590, 366, 445, 18894, 15277, 51336, 51336, 1449, 411, 341, 472, 11, 457, 767, 4748, 857, 1570, 3219, 754, 51488, 51516, 407, 321, 434, 516, 281, 574, 412, 264, 6114, 295, 290, 365, 3104, 281, 257, 293, 272, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.43027698888187915, "compression_ratio": 1.740484429065744, "no_speech_prob": 1.750215596985072e-05}, {"id": 61, "seek": 29128, "start": 291.28, "end": 294.47999999999996, "text": " to this one, but actually a slightly bit less crazy even", "tokens": [50364, 281, 341, 472, 11, 457, 767, 257, 4748, 857, 1570, 3219, 754, 50524, 50560, 1734, 1807, 9590, 366, 445, 257, 18894, 6114, 50656, 50656, 814, 747, 264, 4846, 1412, 382, 364, 4846, 293, 436, 747, 264, 17443, 295, 257, 18161, 3209, 382, 364, 4846, 50904, 50916, 400, 309, 311, 257, 18894, 6114, 293, 264, 5598, 366, 428, 21264, 295, 428, 18161, 2533, 420, 264, 4470, 2445, 51180, 51180, 492, 603, 536, 341, 294, 257, 857, 51252, 51252, 583, 1936, 18161, 9590, 445, 1051, 281, 312, 257, 1629, 1508, 295, 18894, 15277, 51468, 51496, 583, 646, 38377, 307, 767, 10591, 544, 2674, 13, 467, 1177, 380, 767, 1127, 466, 18161, 9590, 412, 439, 51752, 51760], "temperature": 0.8, "avg_logprob": -0.21559947112510944, "compression_ratio": 1.9781818181818183, "no_speech_prob": 3.6476063542068005e-05}, {"id": 62, "seek": 29128, "start": 295.2, "end": 297.11999999999995, "text": " Neural networks are just a mathematical expression", "tokens": [50364, 281, 341, 472, 11, 457, 767, 257, 4748, 857, 1570, 3219, 754, 50524, 50560, 1734, 1807, 9590, 366, 445, 257, 18894, 6114, 50656, 50656, 814, 747, 264, 4846, 1412, 382, 364, 4846, 293, 436, 747, 264, 17443, 295, 257, 18161, 3209, 382, 364, 4846, 50904, 50916, 400, 309, 311, 257, 18894, 6114, 293, 264, 5598, 366, 428, 21264, 295, 428, 18161, 2533, 420, 264, 4470, 2445, 51180, 51180, 492, 603, 536, 341, 294, 257, 857, 51252, 51252, 583, 1936, 18161, 9590, 445, 1051, 281, 312, 257, 1629, 1508, 295, 18894, 15277, 51468, 51496, 583, 646, 38377, 307, 767, 10591, 544, 2674, 13, 467, 1177, 380, 767, 1127, 466, 18161, 9590, 412, 439, 51752, 51760], "temperature": 0.8, "avg_logprob": -0.21559947112510944, "compression_ratio": 1.9781818181818183, "no_speech_prob": 3.6476063542068005e-05}, {"id": 63, "seek": 29128, "start": 297.11999999999995, "end": 302.08, "text": " They take the input data as an input and they take the weights of a neural network as an input", "tokens": [50364, 281, 341, 472, 11, 457, 767, 257, 4748, 857, 1570, 3219, 754, 50524, 50560, 1734, 1807, 9590, 366, 445, 257, 18894, 6114, 50656, 50656, 814, 747, 264, 4846, 1412, 382, 364, 4846, 293, 436, 747, 264, 17443, 295, 257, 18161, 3209, 382, 364, 4846, 50904, 50916, 400, 309, 311, 257, 18894, 6114, 293, 264, 5598, 366, 428, 21264, 295, 428, 18161, 2533, 420, 264, 4470, 2445, 51180, 51180, 492, 603, 536, 341, 294, 257, 857, 51252, 51252, 583, 1936, 18161, 9590, 445, 1051, 281, 312, 257, 1629, 1508, 295, 18894, 15277, 51468, 51496, 583, 646, 38377, 307, 767, 10591, 544, 2674, 13, 467, 1177, 380, 767, 1127, 466, 18161, 9590, 412, 439, 51752, 51760], "temperature": 0.8, "avg_logprob": -0.21559947112510944, "compression_ratio": 1.9781818181818183, "no_speech_prob": 3.6476063542068005e-05}, {"id": 64, "seek": 29128, "start": 302.32, "end": 307.59999999999997, "text": " And it's a mathematical expression and the output are your predictions of your neural net or the loss function", "tokens": [50364, 281, 341, 472, 11, 457, 767, 257, 4748, 857, 1570, 3219, 754, 50524, 50560, 1734, 1807, 9590, 366, 445, 257, 18894, 6114, 50656, 50656, 814, 747, 264, 4846, 1412, 382, 364, 4846, 293, 436, 747, 264, 17443, 295, 257, 18161, 3209, 382, 364, 4846, 50904, 50916, 400, 309, 311, 257, 18894, 6114, 293, 264, 5598, 366, 428, 21264, 295, 428, 18161, 2533, 420, 264, 4470, 2445, 51180, 51180, 492, 603, 536, 341, 294, 257, 857, 51252, 51252, 583, 1936, 18161, 9590, 445, 1051, 281, 312, 257, 1629, 1508, 295, 18894, 15277, 51468, 51496, 583, 646, 38377, 307, 767, 10591, 544, 2674, 13, 467, 1177, 380, 767, 1127, 466, 18161, 9590, 412, 439, 51752, 51760], "temperature": 0.8, "avg_logprob": -0.21559947112510944, "compression_ratio": 1.9781818181818183, "no_speech_prob": 3.6476063542068005e-05}, {"id": 65, "seek": 29128, "start": 307.59999999999997, "end": 309.03999999999996, "text": " We'll see this in a bit", "tokens": [50364, 281, 341, 472, 11, 457, 767, 257, 4748, 857, 1570, 3219, 754, 50524, 50560, 1734, 1807, 9590, 366, 445, 257, 18894, 6114, 50656, 50656, 814, 747, 264, 4846, 1412, 382, 364, 4846, 293, 436, 747, 264, 17443, 295, 257, 18161, 3209, 382, 364, 4846, 50904, 50916, 400, 309, 311, 257, 18894, 6114, 293, 264, 5598, 366, 428, 21264, 295, 428, 18161, 2533, 420, 264, 4470, 2445, 51180, 51180, 492, 603, 536, 341, 294, 257, 857, 51252, 51252, 583, 1936, 18161, 9590, 445, 1051, 281, 312, 257, 1629, 1508, 295, 18894, 15277, 51468, 51496, 583, 646, 38377, 307, 767, 10591, 544, 2674, 13, 467, 1177, 380, 767, 1127, 466, 18161, 9590, 412, 439, 51752, 51760], "temperature": 0.8, "avg_logprob": -0.21559947112510944, "compression_ratio": 1.9781818181818183, "no_speech_prob": 3.6476063542068005e-05}, {"id": 66, "seek": 29128, "start": 309.03999999999996, "end": 313.35999999999996, "text": " But basically neural networks just happen to be a certain class of mathematical expressions", "tokens": [50364, 281, 341, 472, 11, 457, 767, 257, 4748, 857, 1570, 3219, 754, 50524, 50560, 1734, 1807, 9590, 366, 445, 257, 18894, 6114, 50656, 50656, 814, 747, 264, 4846, 1412, 382, 364, 4846, 293, 436, 747, 264, 17443, 295, 257, 18161, 3209, 382, 364, 4846, 50904, 50916, 400, 309, 311, 257, 18894, 6114, 293, 264, 5598, 366, 428, 21264, 295, 428, 18161, 2533, 420, 264, 4470, 2445, 51180, 51180, 492, 603, 536, 341, 294, 257, 857, 51252, 51252, 583, 1936, 18161, 9590, 445, 1051, 281, 312, 257, 1629, 1508, 295, 18894, 15277, 51468, 51496, 583, 646, 38377, 307, 767, 10591, 544, 2674, 13, 467, 1177, 380, 767, 1127, 466, 18161, 9590, 412, 439, 51752, 51760], "temperature": 0.8, "avg_logprob": -0.21559947112510944, "compression_ratio": 1.9781818181818183, "no_speech_prob": 3.6476063542068005e-05}, {"id": 67, "seek": 29128, "start": 313.91999999999996, "end": 319.03999999999996, "text": " But back propagation is actually significantly more general. It doesn't actually care about neural networks at all", "tokens": [50364, 281, 341, 472, 11, 457, 767, 257, 4748, 857, 1570, 3219, 754, 50524, 50560, 1734, 1807, 9590, 366, 445, 257, 18894, 6114, 50656, 50656, 814, 747, 264, 4846, 1412, 382, 364, 4846, 293, 436, 747, 264, 17443, 295, 257, 18161, 3209, 382, 364, 4846, 50904, 50916, 400, 309, 311, 257, 18894, 6114, 293, 264, 5598, 366, 428, 21264, 295, 428, 18161, 2533, 420, 264, 4470, 2445, 51180, 51180, 492, 603, 536, 341, 294, 257, 857, 51252, 51252, 583, 1936, 18161, 9590, 445, 1051, 281, 312, 257, 1629, 1508, 295, 18894, 15277, 51468, 51496, 583, 646, 38377, 307, 767, 10591, 544, 2674, 13, 467, 1177, 380, 767, 1127, 466, 18161, 9590, 412, 439, 51752, 51760], "temperature": 0.8, "avg_logprob": -0.21559947112510944, "compression_ratio": 1.9781818181818183, "no_speech_prob": 3.6476063542068005e-05}, {"id": 68, "seek": 31904, "start": 319.04, "end": 326.04, "text": " it only tells about arbitrary mathematical expressions, and then we happen to use that machinery for training of neural networks.", "tokens": [50364, 309, 787, 5112, 466, 23211, 18894, 15277, 11, 293, 550, 321, 1051, 281, 764, 300, 27302, 337, 3097, 295, 18161, 9590, 13, 50714, 50722, 823, 472, 544, 3637, 286, 576, 411, 281, 652, 412, 341, 3233, 307, 300, 382, 291, 536, 510, 11, 4532, 7165, 307, 257, 39684, 12, 3337, 5827, 1476, 664, 6206, 2848, 13, 51015, 51018, 407, 309, 311, 1364, 322, 264, 11, 291, 458, 11, 1496, 295, 2609, 15664, 685, 411, 3671, 1017, 293, 568, 11, 293, 321, 434, 1940, 18161, 36170, 51305, 51305, 293, 321, 434, 7697, 552, 760, 439, 264, 636, 281, 613, 16871, 295, 2609, 15664, 685, 293, 439, 264, 707, 1804, 279, 293, 1413, 293, 309, 311, 445, 51589, 51606, 22704, 13, 400, 370, 2745, 321, 1116, 1128, 312, 884, 604, 295, 341, 294, 4265, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.20365318242650832, "compression_ratio": 1.7220543806646527, "no_speech_prob": 0.12573885917663574}, {"id": 69, "seek": 31904, "start": 326.20000000000005, "end": 332.06, "text": " Now one more note I would like to make at this stage is that as you see here, micrograd is a scalar-valued autograd engine.", "tokens": [50364, 309, 787, 5112, 466, 23211, 18894, 15277, 11, 293, 550, 321, 1051, 281, 764, 300, 27302, 337, 3097, 295, 18161, 9590, 13, 50714, 50722, 823, 472, 544, 3637, 286, 576, 411, 281, 652, 412, 341, 3233, 307, 300, 382, 291, 536, 510, 11, 4532, 7165, 307, 257, 39684, 12, 3337, 5827, 1476, 664, 6206, 2848, 13, 51015, 51018, 407, 309, 311, 1364, 322, 264, 11, 291, 458, 11, 1496, 295, 2609, 15664, 685, 411, 3671, 1017, 293, 568, 11, 293, 321, 434, 1940, 18161, 36170, 51305, 51305, 293, 321, 434, 7697, 552, 760, 439, 264, 636, 281, 613, 16871, 295, 2609, 15664, 685, 293, 439, 264, 707, 1804, 279, 293, 1413, 293, 309, 311, 445, 51589, 51606, 22704, 13, 400, 370, 2745, 321, 1116, 1128, 312, 884, 604, 295, 341, 294, 4265, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.20365318242650832, "compression_ratio": 1.7220543806646527, "no_speech_prob": 0.12573885917663574}, {"id": 70, "seek": 31904, "start": 332.12, "end": 337.86, "text": " So it's working on the, you know, level of individual scalars like negative 4 and 2, and we're taking neural nets", "tokens": [50364, 309, 787, 5112, 466, 23211, 18894, 15277, 11, 293, 550, 321, 1051, 281, 764, 300, 27302, 337, 3097, 295, 18161, 9590, 13, 50714, 50722, 823, 472, 544, 3637, 286, 576, 411, 281, 652, 412, 341, 3233, 307, 300, 382, 291, 536, 510, 11, 4532, 7165, 307, 257, 39684, 12, 3337, 5827, 1476, 664, 6206, 2848, 13, 51015, 51018, 407, 309, 311, 1364, 322, 264, 11, 291, 458, 11, 1496, 295, 2609, 15664, 685, 411, 3671, 1017, 293, 568, 11, 293, 321, 434, 1940, 18161, 36170, 51305, 51305, 293, 321, 434, 7697, 552, 760, 439, 264, 636, 281, 613, 16871, 295, 2609, 15664, 685, 293, 439, 264, 707, 1804, 279, 293, 1413, 293, 309, 311, 445, 51589, 51606, 22704, 13, 400, 370, 2745, 321, 1116, 1128, 312, 884, 604, 295, 341, 294, 4265, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.20365318242650832, "compression_ratio": 1.7220543806646527, "no_speech_prob": 0.12573885917663574}, {"id": 71, "seek": 31904, "start": 337.86, "end": 343.54, "text": " and we're breaking them down all the way to these atoms of individual scalars and all the little pluses and times and it's just", "tokens": [50364, 309, 787, 5112, 466, 23211, 18894, 15277, 11, 293, 550, 321, 1051, 281, 764, 300, 27302, 337, 3097, 295, 18161, 9590, 13, 50714, 50722, 823, 472, 544, 3637, 286, 576, 411, 281, 652, 412, 341, 3233, 307, 300, 382, 291, 536, 510, 11, 4532, 7165, 307, 257, 39684, 12, 3337, 5827, 1476, 664, 6206, 2848, 13, 51015, 51018, 407, 309, 311, 1364, 322, 264, 11, 291, 458, 11, 1496, 295, 2609, 15664, 685, 411, 3671, 1017, 293, 568, 11, 293, 321, 434, 1940, 18161, 36170, 51305, 51305, 293, 321, 434, 7697, 552, 760, 439, 264, 636, 281, 613, 16871, 295, 2609, 15664, 685, 293, 439, 264, 707, 1804, 279, 293, 1413, 293, 309, 311, 445, 51589, 51606, 22704, 13, 400, 370, 2745, 321, 1116, 1128, 312, 884, 604, 295, 341, 294, 4265, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.20365318242650832, "compression_ratio": 1.7220543806646527, "no_speech_prob": 0.12573885917663574}, {"id": 72, "seek": 31904, "start": 343.88, "end": 347.76, "text": " excessive. And so obviously we'd never be doing any of this in production.", "tokens": [50364, 309, 787, 5112, 466, 23211, 18894, 15277, 11, 293, 550, 321, 1051, 281, 764, 300, 27302, 337, 3097, 295, 18161, 9590, 13, 50714, 50722, 823, 472, 544, 3637, 286, 576, 411, 281, 652, 412, 341, 3233, 307, 300, 382, 291, 536, 510, 11, 4532, 7165, 307, 257, 39684, 12, 3337, 5827, 1476, 664, 6206, 2848, 13, 51015, 51018, 407, 309, 311, 1364, 322, 264, 11, 291, 458, 11, 1496, 295, 2609, 15664, 685, 411, 3671, 1017, 293, 568, 11, 293, 321, 434, 1940, 18161, 36170, 51305, 51305, 293, 321, 434, 7697, 552, 760, 439, 264, 636, 281, 613, 16871, 295, 2609, 15664, 685, 293, 439, 264, 707, 1804, 279, 293, 1413, 293, 309, 311, 445, 51589, 51606, 22704, 13, 400, 370, 2745, 321, 1116, 1128, 312, 884, 604, 295, 341, 294, 4265, 13, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.20365318242650832, "compression_ratio": 1.7220543806646527, "no_speech_prob": 0.12573885917663574}, {"id": 73, "seek": 34776, "start": 347.76, "end": 352.32, "text": " It's really just for them for pedagogical reasons because it allows us to not have to deal with these", "tokens": [50364, 467, 311, 534, 445, 337, 552, 337, 5670, 31599, 804, 4112, 570, 309, 4045, 505, 281, 406, 362, 281, 2028, 365, 613, 50592, 50610, 297, 12, 18759, 10688, 830, 300, 291, 576, 764, 294, 4363, 2452, 18161, 3209, 6405, 13, 50800, 50806, 407, 341, 307, 534, 1096, 370, 300, 291, 1223, 293, 1895, 15104, 484, 264, 3678, 3861, 293, 5021, 4978, 293, 3701, 295, 18161, 3209, 3097, 13, 51208, 51222, 400, 550, 498, 291, 767, 528, 281, 3847, 3801, 9590, 11, 291, 362, 281, 312, 1228, 613, 10688, 830, 11, 457, 6022, 295, 264, 5221, 2962, 13, 51494, 51494, 639, 307, 1096, 17491, 337, 10493, 13, 492, 366, 1936, 1940, 39684, 2158, 11, 439, 264, 39684, 4190, 11, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.1646274347774318, "compression_ratio": 1.7774193548387096, "no_speech_prob": 2.5861898393486626e-05}, {"id": 74, "seek": 34776, "start": 352.68, "end": 356.48, "text": " n-dimensional tensors that you would use in modern deep neural network library.", "tokens": [50364, 467, 311, 534, 445, 337, 552, 337, 5670, 31599, 804, 4112, 570, 309, 4045, 505, 281, 406, 362, 281, 2028, 365, 613, 50592, 50610, 297, 12, 18759, 10688, 830, 300, 291, 576, 764, 294, 4363, 2452, 18161, 3209, 6405, 13, 50800, 50806, 407, 341, 307, 534, 1096, 370, 300, 291, 1223, 293, 1895, 15104, 484, 264, 3678, 3861, 293, 5021, 4978, 293, 3701, 295, 18161, 3209, 3097, 13, 51208, 51222, 400, 550, 498, 291, 767, 528, 281, 3847, 3801, 9590, 11, 291, 362, 281, 312, 1228, 613, 10688, 830, 11, 457, 6022, 295, 264, 5221, 2962, 13, 51494, 51494, 639, 307, 1096, 17491, 337, 10493, 13, 492, 366, 1936, 1940, 39684, 2158, 11, 439, 264, 39684, 4190, 11, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.1646274347774318, "compression_ratio": 1.7774193548387096, "no_speech_prob": 2.5861898393486626e-05}, {"id": 75, "seek": 34776, "start": 356.59999999999997, "end": 364.64, "text": " So this is really done so that you understand and refactor out the background application and chain rule and understanding of neural network training.", "tokens": [50364, 467, 311, 534, 445, 337, 552, 337, 5670, 31599, 804, 4112, 570, 309, 4045, 505, 281, 406, 362, 281, 2028, 365, 613, 50592, 50610, 297, 12, 18759, 10688, 830, 300, 291, 576, 764, 294, 4363, 2452, 18161, 3209, 6405, 13, 50800, 50806, 407, 341, 307, 534, 1096, 370, 300, 291, 1223, 293, 1895, 15104, 484, 264, 3678, 3861, 293, 5021, 4978, 293, 3701, 295, 18161, 3209, 3097, 13, 51208, 51222, 400, 550, 498, 291, 767, 528, 281, 3847, 3801, 9590, 11, 291, 362, 281, 312, 1228, 613, 10688, 830, 11, 457, 6022, 295, 264, 5221, 2962, 13, 51494, 51494, 639, 307, 1096, 17491, 337, 10493, 13, 492, 366, 1936, 1940, 39684, 2158, 11, 439, 264, 39684, 4190, 11, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.1646274347774318, "compression_ratio": 1.7774193548387096, "no_speech_prob": 2.5861898393486626e-05}, {"id": 76, "seek": 34776, "start": 364.92, "end": 370.36, "text": " And then if you actually want to train bigger networks, you have to be using these tensors, but none of the math changes.", "tokens": [50364, 467, 311, 534, 445, 337, 552, 337, 5670, 31599, 804, 4112, 570, 309, 4045, 505, 281, 406, 362, 281, 2028, 365, 613, 50592, 50610, 297, 12, 18759, 10688, 830, 300, 291, 576, 764, 294, 4363, 2452, 18161, 3209, 6405, 13, 50800, 50806, 407, 341, 307, 534, 1096, 370, 300, 291, 1223, 293, 1895, 15104, 484, 264, 3678, 3861, 293, 5021, 4978, 293, 3701, 295, 18161, 3209, 3097, 13, 51208, 51222, 400, 550, 498, 291, 767, 528, 281, 3847, 3801, 9590, 11, 291, 362, 281, 312, 1228, 613, 10688, 830, 11, 457, 6022, 295, 264, 5221, 2962, 13, 51494, 51494, 639, 307, 1096, 17491, 337, 10493, 13, 492, 366, 1936, 1940, 39684, 2158, 11, 439, 264, 39684, 4190, 11, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.1646274347774318, "compression_ratio": 1.7774193548387096, "no_speech_prob": 2.5861898393486626e-05}, {"id": 77, "seek": 34776, "start": 370.36, "end": 375.28, "text": " This is done purely for efficiency. We are basically taking scalar value, all the scalar values,", "tokens": [50364, 467, 311, 534, 445, 337, 552, 337, 5670, 31599, 804, 4112, 570, 309, 4045, 505, 281, 406, 362, 281, 2028, 365, 613, 50592, 50610, 297, 12, 18759, 10688, 830, 300, 291, 576, 764, 294, 4363, 2452, 18161, 3209, 6405, 13, 50800, 50806, 407, 341, 307, 534, 1096, 370, 300, 291, 1223, 293, 1895, 15104, 484, 264, 3678, 3861, 293, 5021, 4978, 293, 3701, 295, 18161, 3209, 3097, 13, 51208, 51222, 400, 550, 498, 291, 767, 528, 281, 3847, 3801, 9590, 11, 291, 362, 281, 312, 1228, 613, 10688, 830, 11, 457, 6022, 295, 264, 5221, 2962, 13, 51494, 51494, 639, 307, 1096, 17491, 337, 10493, 13, 492, 366, 1936, 1940, 39684, 2158, 11, 439, 264, 39684, 4190, 11, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.1646274347774318, "compression_ratio": 1.7774193548387096, "no_speech_prob": 2.5861898393486626e-05}, {"id": 78, "seek": 37528, "start": 375.28, "end": 379.08, "text": " we're packaging them up into tensors, which are just arrays of these scalars.", "tokens": [50364, 321, 434, 16836, 552, 493, 666, 10688, 830, 11, 597, 366, 445, 41011, 295, 613, 15664, 685, 13, 50554, 50554, 400, 550, 570, 321, 362, 613, 2416, 41011, 11, 50684, 50688, 321, 434, 1455, 7705, 322, 729, 2416, 41011, 300, 4045, 505, 281, 747, 5002, 295, 264, 8952, 1434, 294, 257, 3820, 293, 50976, 51004, 439, 729, 7705, 393, 312, 1096, 294, 8952, 293, 550, 264, 1379, 551, 6676, 4663, 13, 51200, 51206, 583, 534, 6022, 295, 264, 5221, 2962, 293, 300, 311, 1096, 17491, 337, 10493, 13, 51359, 51362, 407, 286, 500, 380, 519, 300, 309, 311, 5670, 31599, 984, 4420, 281, 312, 6260, 365, 10688, 830, 490, 8459, 13, 51554, 51566, 400, 286, 519, 293, 300, 311, 983, 286, 17879, 4114, 4532, 7165, 11, 570, 291, 393, 1223, 577, 721, 589, 51794, 51810], "temperature": 0.0, "avg_logprob": -0.1591441022218579, "compression_ratio": 1.8317757009345794, "no_speech_prob": 8.266922122857068e-06}, {"id": 79, "seek": 37528, "start": 379.08, "end": 381.67999999999995, "text": " And then because we have these large arrays,", "tokens": [50364, 321, 434, 16836, 552, 493, 666, 10688, 830, 11, 597, 366, 445, 41011, 295, 613, 15664, 685, 13, 50554, 50554, 400, 550, 570, 321, 362, 613, 2416, 41011, 11, 50684, 50688, 321, 434, 1455, 7705, 322, 729, 2416, 41011, 300, 4045, 505, 281, 747, 5002, 295, 264, 8952, 1434, 294, 257, 3820, 293, 50976, 51004, 439, 729, 7705, 393, 312, 1096, 294, 8952, 293, 550, 264, 1379, 551, 6676, 4663, 13, 51200, 51206, 583, 534, 6022, 295, 264, 5221, 2962, 293, 300, 311, 1096, 17491, 337, 10493, 13, 51359, 51362, 407, 286, 500, 380, 519, 300, 309, 311, 5670, 31599, 984, 4420, 281, 312, 6260, 365, 10688, 830, 490, 8459, 13, 51554, 51566, 400, 286, 519, 293, 300, 311, 983, 286, 17879, 4114, 4532, 7165, 11, 570, 291, 393, 1223, 577, 721, 589, 51794, 51810], "temperature": 0.0, "avg_logprob": -0.1591441022218579, "compression_ratio": 1.8317757009345794, "no_speech_prob": 8.266922122857068e-06}, {"id": 80, "seek": 37528, "start": 381.76, "end": 387.52, "text": " we're making operations on those large arrays that allows us to take advantage of the parallelism in a computer and", "tokens": [50364, 321, 434, 16836, 552, 493, 666, 10688, 830, 11, 597, 366, 445, 41011, 295, 613, 15664, 685, 13, 50554, 50554, 400, 550, 570, 321, 362, 613, 2416, 41011, 11, 50684, 50688, 321, 434, 1455, 7705, 322, 729, 2416, 41011, 300, 4045, 505, 281, 747, 5002, 295, 264, 8952, 1434, 294, 257, 3820, 293, 50976, 51004, 439, 729, 7705, 393, 312, 1096, 294, 8952, 293, 550, 264, 1379, 551, 6676, 4663, 13, 51200, 51206, 583, 534, 6022, 295, 264, 5221, 2962, 293, 300, 311, 1096, 17491, 337, 10493, 13, 51359, 51362, 407, 286, 500, 380, 519, 300, 309, 311, 5670, 31599, 984, 4420, 281, 312, 6260, 365, 10688, 830, 490, 8459, 13, 51554, 51566, 400, 286, 519, 293, 300, 311, 983, 286, 17879, 4114, 4532, 7165, 11, 570, 291, 393, 1223, 577, 721, 589, 51794, 51810], "temperature": 0.0, "avg_logprob": -0.1591441022218579, "compression_ratio": 1.8317757009345794, "no_speech_prob": 8.266922122857068e-06}, {"id": 81, "seek": 37528, "start": 388.08, "end": 392.0, "text": " all those operations can be done in parallel and then the whole thing runs faster.", "tokens": [50364, 321, 434, 16836, 552, 493, 666, 10688, 830, 11, 597, 366, 445, 41011, 295, 613, 15664, 685, 13, 50554, 50554, 400, 550, 570, 321, 362, 613, 2416, 41011, 11, 50684, 50688, 321, 434, 1455, 7705, 322, 729, 2416, 41011, 300, 4045, 505, 281, 747, 5002, 295, 264, 8952, 1434, 294, 257, 3820, 293, 50976, 51004, 439, 729, 7705, 393, 312, 1096, 294, 8952, 293, 550, 264, 1379, 551, 6676, 4663, 13, 51200, 51206, 583, 534, 6022, 295, 264, 5221, 2962, 293, 300, 311, 1096, 17491, 337, 10493, 13, 51359, 51362, 407, 286, 500, 380, 519, 300, 309, 311, 5670, 31599, 984, 4420, 281, 312, 6260, 365, 10688, 830, 490, 8459, 13, 51554, 51566, 400, 286, 519, 293, 300, 311, 983, 286, 17879, 4114, 4532, 7165, 11, 570, 291, 393, 1223, 577, 721, 589, 51794, 51810], "temperature": 0.0, "avg_logprob": -0.1591441022218579, "compression_ratio": 1.8317757009345794, "no_speech_prob": 8.266922122857068e-06}, {"id": 82, "seek": 37528, "start": 392.11999999999995, "end": 395.17999999999995, "text": " But really none of the math changes and that's done purely for efficiency.", "tokens": [50364, 321, 434, 16836, 552, 493, 666, 10688, 830, 11, 597, 366, 445, 41011, 295, 613, 15664, 685, 13, 50554, 50554, 400, 550, 570, 321, 362, 613, 2416, 41011, 11, 50684, 50688, 321, 434, 1455, 7705, 322, 729, 2416, 41011, 300, 4045, 505, 281, 747, 5002, 295, 264, 8952, 1434, 294, 257, 3820, 293, 50976, 51004, 439, 729, 7705, 393, 312, 1096, 294, 8952, 293, 550, 264, 1379, 551, 6676, 4663, 13, 51200, 51206, 583, 534, 6022, 295, 264, 5221, 2962, 293, 300, 311, 1096, 17491, 337, 10493, 13, 51359, 51362, 407, 286, 500, 380, 519, 300, 309, 311, 5670, 31599, 984, 4420, 281, 312, 6260, 365, 10688, 830, 490, 8459, 13, 51554, 51566, 400, 286, 519, 293, 300, 311, 983, 286, 17879, 4114, 4532, 7165, 11, 570, 291, 393, 1223, 577, 721, 589, 51794, 51810], "temperature": 0.0, "avg_logprob": -0.1591441022218579, "compression_ratio": 1.8317757009345794, "no_speech_prob": 8.266922122857068e-06}, {"id": 83, "seek": 37528, "start": 395.23999999999995, "end": 399.08, "text": " So I don't think that it's pedagogically useful to be dealing with tensors from scratch.", "tokens": [50364, 321, 434, 16836, 552, 493, 666, 10688, 830, 11, 597, 366, 445, 41011, 295, 613, 15664, 685, 13, 50554, 50554, 400, 550, 570, 321, 362, 613, 2416, 41011, 11, 50684, 50688, 321, 434, 1455, 7705, 322, 729, 2416, 41011, 300, 4045, 505, 281, 747, 5002, 295, 264, 8952, 1434, 294, 257, 3820, 293, 50976, 51004, 439, 729, 7705, 393, 312, 1096, 294, 8952, 293, 550, 264, 1379, 551, 6676, 4663, 13, 51200, 51206, 583, 534, 6022, 295, 264, 5221, 2962, 293, 300, 311, 1096, 17491, 337, 10493, 13, 51359, 51362, 407, 286, 500, 380, 519, 300, 309, 311, 5670, 31599, 984, 4420, 281, 312, 6260, 365, 10688, 830, 490, 8459, 13, 51554, 51566, 400, 286, 519, 293, 300, 311, 983, 286, 17879, 4114, 4532, 7165, 11, 570, 291, 393, 1223, 577, 721, 589, 51794, 51810], "temperature": 0.0, "avg_logprob": -0.1591441022218579, "compression_ratio": 1.8317757009345794, "no_speech_prob": 8.266922122857068e-06}, {"id": 84, "seek": 37528, "start": 399.32, "end": 403.88, "text": " And I think and that's why I fundamentally wrote micrograd, because you can understand how things work", "tokens": [50364, 321, 434, 16836, 552, 493, 666, 10688, 830, 11, 597, 366, 445, 41011, 295, 613, 15664, 685, 13, 50554, 50554, 400, 550, 570, 321, 362, 613, 2416, 41011, 11, 50684, 50688, 321, 434, 1455, 7705, 322, 729, 2416, 41011, 300, 4045, 505, 281, 747, 5002, 295, 264, 8952, 1434, 294, 257, 3820, 293, 50976, 51004, 439, 729, 7705, 393, 312, 1096, 294, 8952, 293, 550, 264, 1379, 551, 6676, 4663, 13, 51200, 51206, 583, 534, 6022, 295, 264, 5221, 2962, 293, 300, 311, 1096, 17491, 337, 10493, 13, 51359, 51362, 407, 286, 500, 380, 519, 300, 309, 311, 5670, 31599, 984, 4420, 281, 312, 6260, 365, 10688, 830, 490, 8459, 13, 51554, 51566, 400, 286, 519, 293, 300, 311, 983, 286, 17879, 4114, 4532, 7165, 11, 570, 291, 393, 1223, 577, 721, 589, 51794, 51810], "temperature": 0.0, "avg_logprob": -0.1591441022218579, "compression_ratio": 1.8317757009345794, "no_speech_prob": 8.266922122857068e-06}, {"id": 85, "seek": 40388, "start": 403.88, "end": 407.28, "text": " at the fundamental level and then you can speed it up later.", "tokens": [50364, 412, 264, 8088, 1496, 293, 550, 291, 393, 3073, 309, 493, 1780, 13, 50534, 50568, 1033, 11, 370, 510, 311, 264, 1019, 644, 13, 1222, 3932, 307, 300, 4532, 7165, 307, 437, 291, 643, 281, 3847, 18161, 9590, 293, 1203, 1646, 307, 445, 10493, 13, 50894, 50900, 407, 291, 1116, 519, 300, 4532, 7165, 576, 312, 257, 588, 3997, 2522, 295, 3089, 293, 300, 4523, 484, 281, 406, 312, 264, 1389, 13, 51192, 51212, 407, 498, 321, 445, 352, 281, 4532, 7165, 293, 51316, 51356, 291, 603, 536, 300, 456, 311, 787, 732, 7098, 510, 294, 4532, 7165, 13, 639, 307, 264, 3539, 2848, 13, 51580, 51580, 467, 1177, 380, 458, 1340, 466, 18161, 36170, 293, 341, 307, 264, 2302, 18161, 36170, 6405, 322, 1192, 295, 4532, 7165, 13, 51856], "temperature": 0.0, "avg_logprob": -0.1439253922664758, "compression_ratio": 1.821917808219178, "no_speech_prob": 4.565817107504699e-06}, {"id": 86, "seek": 40388, "start": 407.96, "end": 414.48, "text": " Okay, so here's the fun part. My claim is that micrograd is what you need to train neural networks and everything else is just efficiency.", "tokens": [50364, 412, 264, 8088, 1496, 293, 550, 291, 393, 3073, 309, 493, 1780, 13, 50534, 50568, 1033, 11, 370, 510, 311, 264, 1019, 644, 13, 1222, 3932, 307, 300, 4532, 7165, 307, 437, 291, 643, 281, 3847, 18161, 9590, 293, 1203, 1646, 307, 445, 10493, 13, 50894, 50900, 407, 291, 1116, 519, 300, 4532, 7165, 576, 312, 257, 588, 3997, 2522, 295, 3089, 293, 300, 4523, 484, 281, 406, 312, 264, 1389, 13, 51192, 51212, 407, 498, 321, 445, 352, 281, 4532, 7165, 293, 51316, 51356, 291, 603, 536, 300, 456, 311, 787, 732, 7098, 510, 294, 4532, 7165, 13, 639, 307, 264, 3539, 2848, 13, 51580, 51580, 467, 1177, 380, 458, 1340, 466, 18161, 36170, 293, 341, 307, 264, 2302, 18161, 36170, 6405, 322, 1192, 295, 4532, 7165, 13, 51856], "temperature": 0.0, "avg_logprob": -0.1439253922664758, "compression_ratio": 1.821917808219178, "no_speech_prob": 4.565817107504699e-06}, {"id": 87, "seek": 40388, "start": 414.6, "end": 420.44, "text": " So you'd think that micrograd would be a very complex piece of code and that turns out to not be the case.", "tokens": [50364, 412, 264, 8088, 1496, 293, 550, 291, 393, 3073, 309, 493, 1780, 13, 50534, 50568, 1033, 11, 370, 510, 311, 264, 1019, 644, 13, 1222, 3932, 307, 300, 4532, 7165, 307, 437, 291, 643, 281, 3847, 18161, 9590, 293, 1203, 1646, 307, 445, 10493, 13, 50894, 50900, 407, 291, 1116, 519, 300, 4532, 7165, 576, 312, 257, 588, 3997, 2522, 295, 3089, 293, 300, 4523, 484, 281, 406, 312, 264, 1389, 13, 51192, 51212, 407, 498, 321, 445, 352, 281, 4532, 7165, 293, 51316, 51356, 291, 603, 536, 300, 456, 311, 787, 732, 7098, 510, 294, 4532, 7165, 13, 639, 307, 264, 3539, 2848, 13, 51580, 51580, 467, 1177, 380, 458, 1340, 466, 18161, 36170, 293, 341, 307, 264, 2302, 18161, 36170, 6405, 322, 1192, 295, 4532, 7165, 13, 51856], "temperature": 0.0, "avg_logprob": -0.1439253922664758, "compression_ratio": 1.821917808219178, "no_speech_prob": 4.565817107504699e-06}, {"id": 88, "seek": 40388, "start": 420.84, "end": 422.92, "text": " So if we just go to micrograd and", "tokens": [50364, 412, 264, 8088, 1496, 293, 550, 291, 393, 3073, 309, 493, 1780, 13, 50534, 50568, 1033, 11, 370, 510, 311, 264, 1019, 644, 13, 1222, 3932, 307, 300, 4532, 7165, 307, 437, 291, 643, 281, 3847, 18161, 9590, 293, 1203, 1646, 307, 445, 10493, 13, 50894, 50900, 407, 291, 1116, 519, 300, 4532, 7165, 576, 312, 257, 588, 3997, 2522, 295, 3089, 293, 300, 4523, 484, 281, 406, 312, 264, 1389, 13, 51192, 51212, 407, 498, 321, 445, 352, 281, 4532, 7165, 293, 51316, 51356, 291, 603, 536, 300, 456, 311, 787, 732, 7098, 510, 294, 4532, 7165, 13, 639, 307, 264, 3539, 2848, 13, 51580, 51580, 467, 1177, 380, 458, 1340, 466, 18161, 36170, 293, 341, 307, 264, 2302, 18161, 36170, 6405, 322, 1192, 295, 4532, 7165, 13, 51856], "temperature": 0.0, "avg_logprob": -0.1439253922664758, "compression_ratio": 1.821917808219178, "no_speech_prob": 4.565817107504699e-06}, {"id": 89, "seek": 40388, "start": 423.71999999999997, "end": 428.2, "text": " you'll see that there's only two files here in micrograd. This is the actual engine.", "tokens": [50364, 412, 264, 8088, 1496, 293, 550, 291, 393, 3073, 309, 493, 1780, 13, 50534, 50568, 1033, 11, 370, 510, 311, 264, 1019, 644, 13, 1222, 3932, 307, 300, 4532, 7165, 307, 437, 291, 643, 281, 3847, 18161, 9590, 293, 1203, 1646, 307, 445, 10493, 13, 50894, 50900, 407, 291, 1116, 519, 300, 4532, 7165, 576, 312, 257, 588, 3997, 2522, 295, 3089, 293, 300, 4523, 484, 281, 406, 312, 264, 1389, 13, 51192, 51212, 407, 498, 321, 445, 352, 281, 4532, 7165, 293, 51316, 51356, 291, 603, 536, 300, 456, 311, 787, 732, 7098, 510, 294, 4532, 7165, 13, 639, 307, 264, 3539, 2848, 13, 51580, 51580, 467, 1177, 380, 458, 1340, 466, 18161, 36170, 293, 341, 307, 264, 2302, 18161, 36170, 6405, 322, 1192, 295, 4532, 7165, 13, 51856], "temperature": 0.0, "avg_logprob": -0.1439253922664758, "compression_ratio": 1.821917808219178, "no_speech_prob": 4.565817107504699e-06}, {"id": 90, "seek": 42820, "start": 428.2, "end": 435.03999999999996, "text": " It doesn't know anything about neural nets and this is the entire neural nets library on top of micrograd. So engine and", "tokens": [50364, 467, 1177, 380, 458, 1340, 466, 18161, 36170, 293, 341, 307, 264, 2302, 18161, 36170, 6405, 322, 1192, 295, 4532, 7165, 13, 407, 2848, 293, 50706, 50742, 297, 77, 13, 8200, 13, 407, 264, 3539, 646, 38377, 1476, 664, 6206, 2848, 50972, 51008, 300, 2709, 291, 264, 1347, 295, 18161, 9590, 307, 3736, 51168, 51264, 2319, 3876, 295, 3089, 295, 411, 588, 2199, 15329, 11, 51404, 51448, 597, 321, 603, 1223, 538, 264, 917, 295, 341, 7991, 13, 400, 550, 297, 77, 13, 8200, 11, 341, 18161, 3209, 6405, 3094, 322, 1192, 295, 264, 1476, 664, 6206, 2848, 51814], "temperature": 0.0, "avg_logprob": -0.2016012172887821, "compression_ratio": 1.728813559322034, "no_speech_prob": 1.2029198842355981e-05}, {"id": 91, "seek": 42820, "start": 435.76, "end": 440.36, "text": " nn.py. So the actual back propagation autograd engine", "tokens": [50364, 467, 1177, 380, 458, 1340, 466, 18161, 36170, 293, 341, 307, 264, 2302, 18161, 36170, 6405, 322, 1192, 295, 4532, 7165, 13, 407, 2848, 293, 50706, 50742, 297, 77, 13, 8200, 13, 407, 264, 3539, 646, 38377, 1476, 664, 6206, 2848, 50972, 51008, 300, 2709, 291, 264, 1347, 295, 18161, 9590, 307, 3736, 51168, 51264, 2319, 3876, 295, 3089, 295, 411, 588, 2199, 15329, 11, 51404, 51448, 597, 321, 603, 1223, 538, 264, 917, 295, 341, 7991, 13, 400, 550, 297, 77, 13, 8200, 11, 341, 18161, 3209, 6405, 3094, 322, 1192, 295, 264, 1476, 664, 6206, 2848, 51814], "temperature": 0.0, "avg_logprob": -0.2016012172887821, "compression_ratio": 1.728813559322034, "no_speech_prob": 1.2029198842355981e-05}, {"id": 92, "seek": 42820, "start": 441.08, "end": 444.28, "text": " that gives you the power of neural networks is literally", "tokens": [50364, 467, 1177, 380, 458, 1340, 466, 18161, 36170, 293, 341, 307, 264, 2302, 18161, 36170, 6405, 322, 1192, 295, 4532, 7165, 13, 407, 2848, 293, 50706, 50742, 297, 77, 13, 8200, 13, 407, 264, 3539, 646, 38377, 1476, 664, 6206, 2848, 50972, 51008, 300, 2709, 291, 264, 1347, 295, 18161, 9590, 307, 3736, 51168, 51264, 2319, 3876, 295, 3089, 295, 411, 588, 2199, 15329, 11, 51404, 51448, 597, 321, 603, 1223, 538, 264, 917, 295, 341, 7991, 13, 400, 550, 297, 77, 13, 8200, 11, 341, 18161, 3209, 6405, 3094, 322, 1192, 295, 264, 1476, 664, 6206, 2848, 51814], "temperature": 0.0, "avg_logprob": -0.2016012172887821, "compression_ratio": 1.728813559322034, "no_speech_prob": 1.2029198842355981e-05}, {"id": 93, "seek": 42820, "start": 446.2, "end": 449.0, "text": " 100 lines of code of like very simple Python,", "tokens": [50364, 467, 1177, 380, 458, 1340, 466, 18161, 36170, 293, 341, 307, 264, 2302, 18161, 36170, 6405, 322, 1192, 295, 4532, 7165, 13, 407, 2848, 293, 50706, 50742, 297, 77, 13, 8200, 13, 407, 264, 3539, 646, 38377, 1476, 664, 6206, 2848, 50972, 51008, 300, 2709, 291, 264, 1347, 295, 18161, 9590, 307, 3736, 51168, 51264, 2319, 3876, 295, 3089, 295, 411, 588, 2199, 15329, 11, 51404, 51448, 597, 321, 603, 1223, 538, 264, 917, 295, 341, 7991, 13, 400, 550, 297, 77, 13, 8200, 11, 341, 18161, 3209, 6405, 3094, 322, 1192, 295, 264, 1476, 664, 6206, 2848, 51814], "temperature": 0.0, "avg_logprob": -0.2016012172887821, "compression_ratio": 1.728813559322034, "no_speech_prob": 1.2029198842355981e-05}, {"id": 94, "seek": 44900, "start": 449.0, "end": 457.6, "text": " which we'll understand by the end of this lecture. And then nn.py, this neural network library built on top of the autograd engine,", "tokens": [50364, 597, 321, 603, 1223, 538, 264, 917, 295, 341, 7991, 13, 400, 550, 297, 77, 13, 8200, 11, 341, 18161, 3209, 6405, 3094, 322, 1192, 295, 264, 1476, 664, 6206, 2848, 11, 50794, 50814, 307, 411, 257, 7647, 13, 467, 311, 411, 321, 362, 281, 6964, 437, 307, 257, 34090, 293, 550, 321, 362, 281, 6964, 437, 307, 257, 4583, 295, 22027, 293, 550, 321, 6964, 437, 307, 257, 2120, 388, 11167, 43276, 2044, 11, 597, 307, 445, 257, 8310, 295, 7914, 295, 22027, 13, 51364, 51364, 400, 370, 309, 311, 445, 257, 3217, 7647, 13, 407, 1936, 11, 456, 311, 257, 688, 295, 1347, 300, 1487, 490, 787, 8451, 3876, 295, 3089, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.21045547420695676, "compression_ratio": 1.7976190476190477, "no_speech_prob": 7.527604793722276e-06}, {"id": 95, "seek": 44900, "start": 458.0, "end": 469.0, "text": " is like a joke. It's like we have to define what is a neuron and then we have to define what is a layer of neurons and then we define what is a multilayer perceptron, which is just a sequence of layers of neurons.", "tokens": [50364, 597, 321, 603, 1223, 538, 264, 917, 295, 341, 7991, 13, 400, 550, 297, 77, 13, 8200, 11, 341, 18161, 3209, 6405, 3094, 322, 1192, 295, 264, 1476, 664, 6206, 2848, 11, 50794, 50814, 307, 411, 257, 7647, 13, 467, 311, 411, 321, 362, 281, 6964, 437, 307, 257, 34090, 293, 550, 321, 362, 281, 6964, 437, 307, 257, 4583, 295, 22027, 293, 550, 321, 6964, 437, 307, 257, 2120, 388, 11167, 43276, 2044, 11, 597, 307, 445, 257, 8310, 295, 7914, 295, 22027, 13, 51364, 51364, 400, 370, 309, 311, 445, 257, 3217, 7647, 13, 407, 1936, 11, 456, 311, 257, 688, 295, 1347, 300, 1487, 490, 787, 8451, 3876, 295, 3089, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.21045547420695676, "compression_ratio": 1.7976190476190477, "no_speech_prob": 7.527604793722276e-06}, {"id": 96, "seek": 44900, "start": 469.0, "end": 477.0, "text": " And so it's just a total joke. So basically, there's a lot of power that comes from only 150 lines of code.", "tokens": [50364, 597, 321, 603, 1223, 538, 264, 917, 295, 341, 7991, 13, 400, 550, 297, 77, 13, 8200, 11, 341, 18161, 3209, 6405, 3094, 322, 1192, 295, 264, 1476, 664, 6206, 2848, 11, 50794, 50814, 307, 411, 257, 7647, 13, 467, 311, 411, 321, 362, 281, 6964, 437, 307, 257, 34090, 293, 550, 321, 362, 281, 6964, 437, 307, 257, 4583, 295, 22027, 293, 550, 321, 6964, 437, 307, 257, 2120, 388, 11167, 43276, 2044, 11, 597, 307, 445, 257, 8310, 295, 7914, 295, 22027, 13, 51364, 51364, 400, 370, 309, 311, 445, 257, 3217, 7647, 13, 407, 1936, 11, 456, 311, 257, 688, 295, 1347, 300, 1487, 490, 787, 8451, 3876, 295, 3089, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.21045547420695676, "compression_ratio": 1.7976190476190477, "no_speech_prob": 7.527604793722276e-06}, {"id": 97, "seek": 47700, "start": 477.0, "end": 482.0, "text": " And that's all you need to understand to understand neural network training and everything else is just efficiency.", "tokens": [50364, 400, 300, 311, 439, 291, 643, 281, 1223, 281, 1223, 18161, 3209, 3097, 293, 1203, 1646, 307, 445, 10493, 13, 50614, 50614, 400, 295, 1164, 11, 456, 311, 257, 688, 281, 10493, 11, 457, 17879, 11, 300, 311, 439, 300, 311, 2737, 13, 50864, 50864, 1033, 11, 370, 586, 718, 311, 9192, 558, 294, 293, 4445, 4532, 7165, 1823, 538, 1823, 13, 51064, 51064, 440, 700, 551, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 652, 988, 300, 291, 362, 257, 588, 665, 3701, 46506, 295, 437, 257, 13760, 307, 293, 2293, 437, 1589, 309, 2709, 291, 13, 51464, 51464, 407, 718, 311, 722, 365, 512, 3875, 41596, 300, 286, 5055, 9163, 294, 633, 22125, 88, 391, 21060, 1009, 13, 51714, 51764], "temperature": 0.0, "avg_logprob": -0.08196490908425952, "compression_ratio": 1.7491961414790997, "no_speech_prob": 1.3006378139834851e-05}, {"id": 98, "seek": 47700, "start": 482.0, "end": 487.0, "text": " And of course, there's a lot to efficiency, but fundamentally, that's all that's happening.", "tokens": [50364, 400, 300, 311, 439, 291, 643, 281, 1223, 281, 1223, 18161, 3209, 3097, 293, 1203, 1646, 307, 445, 10493, 13, 50614, 50614, 400, 295, 1164, 11, 456, 311, 257, 688, 281, 10493, 11, 457, 17879, 11, 300, 311, 439, 300, 311, 2737, 13, 50864, 50864, 1033, 11, 370, 586, 718, 311, 9192, 558, 294, 293, 4445, 4532, 7165, 1823, 538, 1823, 13, 51064, 51064, 440, 700, 551, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 652, 988, 300, 291, 362, 257, 588, 665, 3701, 46506, 295, 437, 257, 13760, 307, 293, 2293, 437, 1589, 309, 2709, 291, 13, 51464, 51464, 407, 718, 311, 722, 365, 512, 3875, 41596, 300, 286, 5055, 9163, 294, 633, 22125, 88, 391, 21060, 1009, 13, 51714, 51764], "temperature": 0.0, "avg_logprob": -0.08196490908425952, "compression_ratio": 1.7491961414790997, "no_speech_prob": 1.3006378139834851e-05}, {"id": 99, "seek": 47700, "start": 487.0, "end": 491.0, "text": " Okay, so now let's dive right in and implement micrograd step by step.", "tokens": [50364, 400, 300, 311, 439, 291, 643, 281, 1223, 281, 1223, 18161, 3209, 3097, 293, 1203, 1646, 307, 445, 10493, 13, 50614, 50614, 400, 295, 1164, 11, 456, 311, 257, 688, 281, 10493, 11, 457, 17879, 11, 300, 311, 439, 300, 311, 2737, 13, 50864, 50864, 1033, 11, 370, 586, 718, 311, 9192, 558, 294, 293, 4445, 4532, 7165, 1823, 538, 1823, 13, 51064, 51064, 440, 700, 551, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 652, 988, 300, 291, 362, 257, 588, 665, 3701, 46506, 295, 437, 257, 13760, 307, 293, 2293, 437, 1589, 309, 2709, 291, 13, 51464, 51464, 407, 718, 311, 722, 365, 512, 3875, 41596, 300, 286, 5055, 9163, 294, 633, 22125, 88, 391, 21060, 1009, 13, 51714, 51764], "temperature": 0.0, "avg_logprob": -0.08196490908425952, "compression_ratio": 1.7491961414790997, "no_speech_prob": 1.3006378139834851e-05}, {"id": 100, "seek": 47700, "start": 491.0, "end": 499.0, "text": " The first thing I'd like to do is I'd like to make sure that you have a very good understanding intuitively of what a derivative is and exactly what information it gives you.", "tokens": [50364, 400, 300, 311, 439, 291, 643, 281, 1223, 281, 1223, 18161, 3209, 3097, 293, 1203, 1646, 307, 445, 10493, 13, 50614, 50614, 400, 295, 1164, 11, 456, 311, 257, 688, 281, 10493, 11, 457, 17879, 11, 300, 311, 439, 300, 311, 2737, 13, 50864, 50864, 1033, 11, 370, 586, 718, 311, 9192, 558, 294, 293, 4445, 4532, 7165, 1823, 538, 1823, 13, 51064, 51064, 440, 700, 551, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 652, 988, 300, 291, 362, 257, 588, 665, 3701, 46506, 295, 437, 257, 13760, 307, 293, 2293, 437, 1589, 309, 2709, 291, 13, 51464, 51464, 407, 718, 311, 722, 365, 512, 3875, 41596, 300, 286, 5055, 9163, 294, 633, 22125, 88, 391, 21060, 1009, 13, 51714, 51764], "temperature": 0.0, "avg_logprob": -0.08196490908425952, "compression_ratio": 1.7491961414790997, "no_speech_prob": 1.3006378139834851e-05}, {"id": 101, "seek": 47700, "start": 499.0, "end": 504.0, "text": " So let's start with some basic imports that I copy paste in every Jupyter notebook always.", "tokens": [50364, 400, 300, 311, 439, 291, 643, 281, 1223, 281, 1223, 18161, 3209, 3097, 293, 1203, 1646, 307, 445, 10493, 13, 50614, 50614, 400, 295, 1164, 11, 456, 311, 257, 688, 281, 10493, 11, 457, 17879, 11, 300, 311, 439, 300, 311, 2737, 13, 50864, 50864, 1033, 11, 370, 586, 718, 311, 9192, 558, 294, 293, 4445, 4532, 7165, 1823, 538, 1823, 13, 51064, 51064, 440, 700, 551, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 652, 988, 300, 291, 362, 257, 588, 665, 3701, 46506, 295, 437, 257, 13760, 307, 293, 2293, 437, 1589, 309, 2709, 291, 13, 51464, 51464, 407, 718, 311, 722, 365, 512, 3875, 41596, 300, 286, 5055, 9163, 294, 633, 22125, 88, 391, 21060, 1009, 13, 51714, 51764], "temperature": 0.0, "avg_logprob": -0.08196490908425952, "compression_ratio": 1.7491961414790997, "no_speech_prob": 1.3006378139834851e-05}, {"id": 102, "seek": 50400, "start": 504.0, "end": 510.0, "text": " And let's define a function, a scalar-valued function, f of x as follows.", "tokens": [50364, 400, 718, 311, 6964, 257, 2445, 11, 257, 39684, 12, 3337, 5827, 2445, 11, 283, 295, 2031, 382, 10002, 13, 50664, 50664, 407, 286, 445, 1027, 341, 493, 16979, 13, 286, 445, 1415, 281, 4373, 257, 7363, 2445, 300, 2516, 257, 2167, 39684, 2031, 293, 11247, 257, 2167, 39684, 288, 13, 51014, 51014, 400, 321, 393, 818, 341, 2445, 11, 295, 1164, 11, 370, 321, 393, 1320, 294, 11, 584, 11, 805, 13, 15, 293, 483, 945, 646, 13, 51264, 51314, 823, 11, 321, 393, 611, 7542, 341, 2445, 281, 483, 257, 2020, 295, 1080, 3909, 13, 51464, 51464, 509, 393, 980, 490, 264, 18894, 6114, 300, 341, 307, 1391, 257, 45729, 4711, 13, 467, 311, 37262, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12870532176533683, "compression_ratio": 1.7196969696969697, "no_speech_prob": 2.9310154786799103e-05}, {"id": 103, "seek": 50400, "start": 510.0, "end": 517.0, "text": " So I just made this up randomly. I just wanted to scale a valid function that takes a single scalar x and returns a single scalar y.", "tokens": [50364, 400, 718, 311, 6964, 257, 2445, 11, 257, 39684, 12, 3337, 5827, 2445, 11, 283, 295, 2031, 382, 10002, 13, 50664, 50664, 407, 286, 445, 1027, 341, 493, 16979, 13, 286, 445, 1415, 281, 4373, 257, 7363, 2445, 300, 2516, 257, 2167, 39684, 2031, 293, 11247, 257, 2167, 39684, 288, 13, 51014, 51014, 400, 321, 393, 818, 341, 2445, 11, 295, 1164, 11, 370, 321, 393, 1320, 294, 11, 584, 11, 805, 13, 15, 293, 483, 945, 646, 13, 51264, 51314, 823, 11, 321, 393, 611, 7542, 341, 2445, 281, 483, 257, 2020, 295, 1080, 3909, 13, 51464, 51464, 509, 393, 980, 490, 264, 18894, 6114, 300, 341, 307, 1391, 257, 45729, 4711, 13, 467, 311, 37262, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12870532176533683, "compression_ratio": 1.7196969696969697, "no_speech_prob": 2.9310154786799103e-05}, {"id": 104, "seek": 50400, "start": 517.0, "end": 522.0, "text": " And we can call this function, of course, so we can pass in, say, 3.0 and get 20 back.", "tokens": [50364, 400, 718, 311, 6964, 257, 2445, 11, 257, 39684, 12, 3337, 5827, 2445, 11, 283, 295, 2031, 382, 10002, 13, 50664, 50664, 407, 286, 445, 1027, 341, 493, 16979, 13, 286, 445, 1415, 281, 4373, 257, 7363, 2445, 300, 2516, 257, 2167, 39684, 2031, 293, 11247, 257, 2167, 39684, 288, 13, 51014, 51014, 400, 321, 393, 818, 341, 2445, 11, 295, 1164, 11, 370, 321, 393, 1320, 294, 11, 584, 11, 805, 13, 15, 293, 483, 945, 646, 13, 51264, 51314, 823, 11, 321, 393, 611, 7542, 341, 2445, 281, 483, 257, 2020, 295, 1080, 3909, 13, 51464, 51464, 509, 393, 980, 490, 264, 18894, 6114, 300, 341, 307, 1391, 257, 45729, 4711, 13, 467, 311, 37262, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12870532176533683, "compression_ratio": 1.7196969696969697, "no_speech_prob": 2.9310154786799103e-05}, {"id": 105, "seek": 50400, "start": 523.0, "end": 526.0, "text": " Now, we can also plot this function to get a sense of its shape.", "tokens": [50364, 400, 718, 311, 6964, 257, 2445, 11, 257, 39684, 12, 3337, 5827, 2445, 11, 283, 295, 2031, 382, 10002, 13, 50664, 50664, 407, 286, 445, 1027, 341, 493, 16979, 13, 286, 445, 1415, 281, 4373, 257, 7363, 2445, 300, 2516, 257, 2167, 39684, 2031, 293, 11247, 257, 2167, 39684, 288, 13, 51014, 51014, 400, 321, 393, 818, 341, 2445, 11, 295, 1164, 11, 370, 321, 393, 1320, 294, 11, 584, 11, 805, 13, 15, 293, 483, 945, 646, 13, 51264, 51314, 823, 11, 321, 393, 611, 7542, 341, 2445, 281, 483, 257, 2020, 295, 1080, 3909, 13, 51464, 51464, 509, 393, 980, 490, 264, 18894, 6114, 300, 341, 307, 1391, 257, 45729, 4711, 13, 467, 311, 37262, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12870532176533683, "compression_ratio": 1.7196969696969697, "no_speech_prob": 2.9310154786799103e-05}, {"id": 106, "seek": 50400, "start": 526.0, "end": 531.0, "text": " You can tell from the mathematical expression that this is probably a parabola. It's quadratic.", "tokens": [50364, 400, 718, 311, 6964, 257, 2445, 11, 257, 39684, 12, 3337, 5827, 2445, 11, 283, 295, 2031, 382, 10002, 13, 50664, 50664, 407, 286, 445, 1027, 341, 493, 16979, 13, 286, 445, 1415, 281, 4373, 257, 7363, 2445, 300, 2516, 257, 2167, 39684, 2031, 293, 11247, 257, 2167, 39684, 288, 13, 51014, 51014, 400, 321, 393, 818, 341, 2445, 11, 295, 1164, 11, 370, 321, 393, 1320, 294, 11, 584, 11, 805, 13, 15, 293, 483, 945, 646, 13, 51264, 51314, 823, 11, 321, 393, 611, 7542, 341, 2445, 281, 483, 257, 2020, 295, 1080, 3909, 13, 51464, 51464, 509, 393, 980, 490, 264, 18894, 6114, 300, 341, 307, 1391, 257, 45729, 4711, 13, 467, 311, 37262, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12870532176533683, "compression_ratio": 1.7196969696969697, "no_speech_prob": 2.9310154786799103e-05}, {"id": 107, "seek": 53100, "start": 531.0, "end": 543.0, "text": " And so if we just create a set of scalar values that we can feed in using, for example, a range from negative 5 to 5 in steps of 0.25.", "tokens": [50364, 400, 370, 498, 321, 445, 1884, 257, 992, 295, 39684, 4190, 300, 321, 393, 3154, 294, 1228, 11, 337, 1365, 11, 257, 3613, 490, 3671, 1025, 281, 1025, 294, 4439, 295, 1958, 13, 6074, 13, 50964, 50964, 407, 341, 307, 370, 2031, 307, 445, 490, 3671, 1025, 281, 1025, 11, 406, 3009, 1025, 294, 4439, 295, 1958, 13, 6074, 13, 51364, 51364, 400, 321, 393, 767, 818, 341, 2445, 322, 341, 1031, 8200, 10225, 382, 731, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13602878429271556, "compression_ratio": 1.630057803468208, "no_speech_prob": 2.5866735086310655e-05}, {"id": 108, "seek": 53100, "start": 543.0, "end": 551.0, "text": " So this is so x is just from negative 5 to 5, not including 5 in steps of 0.25.", "tokens": [50364, 400, 370, 498, 321, 445, 1884, 257, 992, 295, 39684, 4190, 300, 321, 393, 3154, 294, 1228, 11, 337, 1365, 11, 257, 3613, 490, 3671, 1025, 281, 1025, 294, 4439, 295, 1958, 13, 6074, 13, 50964, 50964, 407, 341, 307, 370, 2031, 307, 445, 490, 3671, 1025, 281, 1025, 11, 406, 3009, 1025, 294, 4439, 295, 1958, 13, 6074, 13, 51364, 51364, 400, 321, 393, 767, 818, 341, 2445, 322, 341, 1031, 8200, 10225, 382, 731, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13602878429271556, "compression_ratio": 1.630057803468208, "no_speech_prob": 2.5866735086310655e-05}, {"id": 109, "seek": 53100, "start": 551.0, "end": 554.0, "text": " And we can actually call this function on this numpy array as well.", "tokens": [50364, 400, 370, 498, 321, 445, 1884, 257, 992, 295, 39684, 4190, 300, 321, 393, 3154, 294, 1228, 11, 337, 1365, 11, 257, 3613, 490, 3671, 1025, 281, 1025, 294, 4439, 295, 1958, 13, 6074, 13, 50964, 50964, 407, 341, 307, 370, 2031, 307, 445, 490, 3671, 1025, 281, 1025, 11, 406, 3009, 1025, 294, 4439, 295, 1958, 13, 6074, 13, 51364, 51364, 400, 321, 393, 767, 818, 341, 2445, 322, 341, 1031, 8200, 10225, 382, 731, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13602878429271556, "compression_ratio": 1.630057803468208, "no_speech_prob": 2.5866735086310655e-05}, {"id": 110, "seek": 55400, "start": 554.0, "end": 565.0, "text": " So we get a set of y's if we call f on x's and these y's are basically also applying the function on every one of these elements independently.", "tokens": [50364, 407, 321, 483, 257, 992, 295, 288, 311, 498, 321, 818, 283, 322, 2031, 311, 293, 613, 288, 311, 366, 1936, 611, 9275, 264, 2445, 322, 633, 472, 295, 613, 4959, 21761, 13, 50914, 50914, 400, 321, 393, 7542, 341, 1228, 6789, 564, 310, 38270, 13, 407, 499, 83, 13, 564, 310, 11, 2031, 311, 293, 288, 311, 11, 293, 321, 483, 257, 1481, 45729, 4711, 13, 51264, 51264, 407, 8046, 510, 11, 321, 4636, 294, 805, 13, 15, 4079, 510, 11, 293, 321, 4613, 945, 646, 11, 597, 307, 510, 11, 264, 288, 15670, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11103299140930176, "compression_ratio": 1.5779816513761469, "no_speech_prob": 8.267681550933048e-06}, {"id": 111, "seek": 55400, "start": 565.0, "end": 572.0, "text": " And we can plot this using Matplotlib. So plt.plot, x's and y's, and we get a nice parabola.", "tokens": [50364, 407, 321, 483, 257, 992, 295, 288, 311, 498, 321, 818, 283, 322, 2031, 311, 293, 613, 288, 311, 366, 1936, 611, 9275, 264, 2445, 322, 633, 472, 295, 613, 4959, 21761, 13, 50914, 50914, 400, 321, 393, 7542, 341, 1228, 6789, 564, 310, 38270, 13, 407, 499, 83, 13, 564, 310, 11, 2031, 311, 293, 288, 311, 11, 293, 321, 483, 257, 1481, 45729, 4711, 13, 51264, 51264, 407, 8046, 510, 11, 321, 4636, 294, 805, 13, 15, 4079, 510, 11, 293, 321, 4613, 945, 646, 11, 597, 307, 510, 11, 264, 288, 15670, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11103299140930176, "compression_ratio": 1.5779816513761469, "no_speech_prob": 8.267681550933048e-06}, {"id": 112, "seek": 55400, "start": 572.0, "end": 579.0, "text": " So previously here, we fed in 3.0 somewhere here, and we received 20 back, which is here, the y coordinate.", "tokens": [50364, 407, 321, 483, 257, 992, 295, 288, 311, 498, 321, 818, 283, 322, 2031, 311, 293, 613, 288, 311, 366, 1936, 611, 9275, 264, 2445, 322, 633, 472, 295, 613, 4959, 21761, 13, 50914, 50914, 400, 321, 393, 7542, 341, 1228, 6789, 564, 310, 38270, 13, 407, 499, 83, 13, 564, 310, 11, 2031, 311, 293, 288, 311, 11, 293, 321, 483, 257, 1481, 45729, 4711, 13, 51264, 51264, 407, 8046, 510, 11, 321, 4636, 294, 805, 13, 15, 4079, 510, 11, 293, 321, 4613, 945, 646, 11, 597, 307, 510, 11, 264, 288, 15670, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11103299140930176, "compression_ratio": 1.5779816513761469, "no_speech_prob": 8.267681550933048e-06}, {"id": 113, "seek": 57900, "start": 579.0, "end": 585.0, "text": " So now I'd like to think through what is the derivative of this function at any single input point x.", "tokens": [50364, 407, 586, 286, 1116, 411, 281, 519, 807, 437, 307, 264, 13760, 295, 341, 2445, 412, 604, 2167, 4846, 935, 2031, 13, 50664, 50664, 1779, 13, 407, 437, 307, 264, 13760, 412, 819, 2793, 2031, 295, 341, 2445, 30, 50864, 50864, 823, 11, 498, 291, 1604, 646, 281, 428, 33400, 1508, 11, 291, 600, 1391, 18949, 33733, 13, 51064, 51064, 407, 321, 747, 341, 18894, 6114, 805, 87, 8889, 3175, 1017, 87, 1804, 1025, 11, 293, 291, 576, 2464, 484, 322, 257, 2522, 295, 3035, 293, 291, 576, 3079, 264, 1674, 4978, 293, 439, 264, 661, 4474, 293, 28446, 264, 18894, 6114, 295, 264, 869, 13760, 295, 264, 3380, 2445, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10715736720872962, "compression_ratio": 1.8350877192982455, "no_speech_prob": 2.4299240976688452e-05}, {"id": 114, "seek": 57900, "start": 585.0, "end": 589.0, "text": " Right. So what is the derivative at different points x of this function?", "tokens": [50364, 407, 586, 286, 1116, 411, 281, 519, 807, 437, 307, 264, 13760, 295, 341, 2445, 412, 604, 2167, 4846, 935, 2031, 13, 50664, 50664, 1779, 13, 407, 437, 307, 264, 13760, 412, 819, 2793, 2031, 295, 341, 2445, 30, 50864, 50864, 823, 11, 498, 291, 1604, 646, 281, 428, 33400, 1508, 11, 291, 600, 1391, 18949, 33733, 13, 51064, 51064, 407, 321, 747, 341, 18894, 6114, 805, 87, 8889, 3175, 1017, 87, 1804, 1025, 11, 293, 291, 576, 2464, 484, 322, 257, 2522, 295, 3035, 293, 291, 576, 3079, 264, 1674, 4978, 293, 439, 264, 661, 4474, 293, 28446, 264, 18894, 6114, 295, 264, 869, 13760, 295, 264, 3380, 2445, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10715736720872962, "compression_ratio": 1.8350877192982455, "no_speech_prob": 2.4299240976688452e-05}, {"id": 115, "seek": 57900, "start": 589.0, "end": 593.0, "text": " Now, if you remember back to your calculus class, you've probably derived derivatives.", "tokens": [50364, 407, 586, 286, 1116, 411, 281, 519, 807, 437, 307, 264, 13760, 295, 341, 2445, 412, 604, 2167, 4846, 935, 2031, 13, 50664, 50664, 1779, 13, 407, 437, 307, 264, 13760, 412, 819, 2793, 2031, 295, 341, 2445, 30, 50864, 50864, 823, 11, 498, 291, 1604, 646, 281, 428, 33400, 1508, 11, 291, 600, 1391, 18949, 33733, 13, 51064, 51064, 407, 321, 747, 341, 18894, 6114, 805, 87, 8889, 3175, 1017, 87, 1804, 1025, 11, 293, 291, 576, 2464, 484, 322, 257, 2522, 295, 3035, 293, 291, 576, 3079, 264, 1674, 4978, 293, 439, 264, 661, 4474, 293, 28446, 264, 18894, 6114, 295, 264, 869, 13760, 295, 264, 3380, 2445, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10715736720872962, "compression_ratio": 1.8350877192982455, "no_speech_prob": 2.4299240976688452e-05}, {"id": 116, "seek": 57900, "start": 593.0, "end": 606.0, "text": " So we take this mathematical expression 3x squared minus 4x plus 5, and you would write out on a piece of paper and you would apply the product rule and all the other rules and derive the mathematical expression of the great derivative of the original function.", "tokens": [50364, 407, 586, 286, 1116, 411, 281, 519, 807, 437, 307, 264, 13760, 295, 341, 2445, 412, 604, 2167, 4846, 935, 2031, 13, 50664, 50664, 1779, 13, 407, 437, 307, 264, 13760, 412, 819, 2793, 2031, 295, 341, 2445, 30, 50864, 50864, 823, 11, 498, 291, 1604, 646, 281, 428, 33400, 1508, 11, 291, 600, 1391, 18949, 33733, 13, 51064, 51064, 407, 321, 747, 341, 18894, 6114, 805, 87, 8889, 3175, 1017, 87, 1804, 1025, 11, 293, 291, 576, 2464, 484, 322, 257, 2522, 295, 3035, 293, 291, 576, 3079, 264, 1674, 4978, 293, 439, 264, 661, 4474, 293, 28446, 264, 18894, 6114, 295, 264, 869, 13760, 295, 264, 3380, 2445, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10715736720872962, "compression_ratio": 1.8350877192982455, "no_speech_prob": 2.4299240976688452e-05}, {"id": 117, "seek": 60600, "start": 606.0, "end": 610.0, "text": " And then you could plug in different x's and see what the derivative is.", "tokens": [50364, 400, 550, 291, 727, 5452, 294, 819, 2031, 311, 293, 536, 437, 264, 13760, 307, 13, 50564, 50564, 492, 434, 406, 516, 281, 767, 360, 300, 570, 572, 472, 294, 18161, 9590, 767, 13657, 484, 264, 6114, 337, 264, 18161, 2533, 13, 50864, 50864, 467, 576, 312, 257, 5994, 6114, 13, 467, 576, 312, 5383, 11, 10688, 295, 5383, 295, 2115, 13, 883, 472, 767, 1163, 1539, 264, 13760, 11, 295, 1164, 13, 51214, 51214, 400, 370, 321, 434, 406, 516, 281, 747, 341, 733, 295, 25755, 3109, 13, 7156, 11, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 574, 412, 264, 7123, 295, 13760, 293, 445, 652, 988, 300, 321, 534, 1223, 437, 13760, 307, 13389, 11, 437, 309, 311, 3585, 291, 466, 264, 2445, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0668035905752609, "compression_ratio": 1.9223300970873787, "no_speech_prob": 1.280485957977362e-05}, {"id": 118, "seek": 60600, "start": 610.0, "end": 616.0, "text": " We're not going to actually do that because no one in neural networks actually writes out the expression for the neural net.", "tokens": [50364, 400, 550, 291, 727, 5452, 294, 819, 2031, 311, 293, 536, 437, 264, 13760, 307, 13, 50564, 50564, 492, 434, 406, 516, 281, 767, 360, 300, 570, 572, 472, 294, 18161, 9590, 767, 13657, 484, 264, 6114, 337, 264, 18161, 2533, 13, 50864, 50864, 467, 576, 312, 257, 5994, 6114, 13, 467, 576, 312, 5383, 11, 10688, 295, 5383, 295, 2115, 13, 883, 472, 767, 1163, 1539, 264, 13760, 11, 295, 1164, 13, 51214, 51214, 400, 370, 321, 434, 406, 516, 281, 747, 341, 733, 295, 25755, 3109, 13, 7156, 11, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 574, 412, 264, 7123, 295, 13760, 293, 445, 652, 988, 300, 321, 534, 1223, 437, 13760, 307, 13389, 11, 437, 309, 311, 3585, 291, 466, 264, 2445, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0668035905752609, "compression_ratio": 1.9223300970873787, "no_speech_prob": 1.280485957977362e-05}, {"id": 119, "seek": 60600, "start": 616.0, "end": 623.0, "text": " It would be a massive expression. It would be thousands, tens of thousands of terms. No one actually derives the derivative, of course.", "tokens": [50364, 400, 550, 291, 727, 5452, 294, 819, 2031, 311, 293, 536, 437, 264, 13760, 307, 13, 50564, 50564, 492, 434, 406, 516, 281, 767, 360, 300, 570, 572, 472, 294, 18161, 9590, 767, 13657, 484, 264, 6114, 337, 264, 18161, 2533, 13, 50864, 50864, 467, 576, 312, 257, 5994, 6114, 13, 467, 576, 312, 5383, 11, 10688, 295, 5383, 295, 2115, 13, 883, 472, 767, 1163, 1539, 264, 13760, 11, 295, 1164, 13, 51214, 51214, 400, 370, 321, 434, 406, 516, 281, 747, 341, 733, 295, 25755, 3109, 13, 7156, 11, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 574, 412, 264, 7123, 295, 13760, 293, 445, 652, 988, 300, 321, 534, 1223, 437, 13760, 307, 13389, 11, 437, 309, 311, 3585, 291, 466, 264, 2445, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0668035905752609, "compression_ratio": 1.9223300970873787, "no_speech_prob": 1.280485957977362e-05}, {"id": 120, "seek": 60600, "start": 623.0, "end": 635.0, "text": " And so we're not going to take this kind of symbolic approach. Instead, what I'd like to do is I'd like to look at the definition of derivative and just make sure that we really understand what derivative is measuring, what it's telling you about the function.", "tokens": [50364, 400, 550, 291, 727, 5452, 294, 819, 2031, 311, 293, 536, 437, 264, 13760, 307, 13, 50564, 50564, 492, 434, 406, 516, 281, 767, 360, 300, 570, 572, 472, 294, 18161, 9590, 767, 13657, 484, 264, 6114, 337, 264, 18161, 2533, 13, 50864, 50864, 467, 576, 312, 257, 5994, 6114, 13, 467, 576, 312, 5383, 11, 10688, 295, 5383, 295, 2115, 13, 883, 472, 767, 1163, 1539, 264, 13760, 11, 295, 1164, 13, 51214, 51214, 400, 370, 321, 434, 406, 516, 281, 747, 341, 733, 295, 25755, 3109, 13, 7156, 11, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 574, 412, 264, 7123, 295, 13760, 293, 445, 652, 988, 300, 321, 534, 1223, 437, 13760, 307, 13389, 11, 437, 309, 311, 3585, 291, 466, 264, 2445, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0668035905752609, "compression_ratio": 1.9223300970873787, "no_speech_prob": 1.280485957977362e-05}, {"id": 121, "seek": 63500, "start": 635.0, "end": 642.0, "text": " And so if we just look up derivative.", "tokens": [50364, 400, 370, 498, 321, 445, 574, 493, 13760, 13, 50714, 50714, 492, 536, 300, 341, 307, 406, 257, 588, 665, 7123, 295, 13760, 13, 639, 307, 257, 7123, 295, 437, 309, 1355, 281, 312, 819, 9364, 13, 51014, 51014, 583, 498, 291, 1604, 490, 428, 33400, 11, 309, 307, 264, 4948, 382, 276, 1709, 281, 4018, 295, 283, 295, 2031, 1804, 276, 3175, 283, 295, 2031, 670, 276, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08563928081564708, "compression_ratio": 1.5664739884393064, "no_speech_prob": 1.6963980669970624e-05}, {"id": 122, "seek": 63500, "start": 642.0, "end": 648.0, "text": " We see that this is not a very good definition of derivative. This is a definition of what it means to be differentiable.", "tokens": [50364, 400, 370, 498, 321, 445, 574, 493, 13760, 13, 50714, 50714, 492, 536, 300, 341, 307, 406, 257, 588, 665, 7123, 295, 13760, 13, 639, 307, 257, 7123, 295, 437, 309, 1355, 281, 312, 819, 9364, 13, 51014, 51014, 583, 498, 291, 1604, 490, 428, 33400, 11, 309, 307, 264, 4948, 382, 276, 1709, 281, 4018, 295, 283, 295, 2031, 1804, 276, 3175, 283, 295, 2031, 670, 276, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08563928081564708, "compression_ratio": 1.5664739884393064, "no_speech_prob": 1.6963980669970624e-05}, {"id": 123, "seek": 63500, "start": 648.0, "end": 656.0, "text": " But if you remember from your calculus, it is the limit as h goes to zero of f of x plus h minus f of x over h.", "tokens": [50364, 400, 370, 498, 321, 445, 574, 493, 13760, 13, 50714, 50714, 492, 536, 300, 341, 307, 406, 257, 588, 665, 7123, 295, 13760, 13, 639, 307, 257, 7123, 295, 437, 309, 1355, 281, 312, 819, 9364, 13, 51014, 51014, 583, 498, 291, 1604, 490, 428, 33400, 11, 309, 307, 264, 4948, 382, 276, 1709, 281, 4018, 295, 283, 295, 2031, 1804, 276, 3175, 283, 295, 2031, 670, 276, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08563928081564708, "compression_ratio": 1.5664739884393064, "no_speech_prob": 1.6963980669970624e-05}, {"id": 124, "seek": 65600, "start": 656.0, "end": 668.0, "text": " So basically what it's saying is if you slightly bump up your at some point x that you're interested in or a and if you slightly bump up, you slightly increase it by small number h.", "tokens": [50364, 407, 1936, 437, 309, 311, 1566, 307, 498, 291, 4748, 9961, 493, 428, 412, 512, 935, 2031, 300, 291, 434, 3102, 294, 420, 257, 293, 498, 291, 4748, 9961, 493, 11, 291, 4748, 3488, 309, 538, 1359, 1230, 276, 13, 50964, 50964, 1012, 775, 264, 2445, 4196, 365, 437, 19392, 775, 309, 4196, 30, 2305, 311, 264, 13525, 412, 300, 935, 30, 4402, 264, 2445, 352, 493, 420, 775, 309, 352, 760, 30, 400, 538, 577, 709, 30, 51364, 51364, 400, 300, 311, 264, 13525, 295, 300, 2445, 11, 264, 13525, 295, 300, 4134, 412, 300, 935, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12796603932100184, "compression_ratio": 1.905829596412556, "no_speech_prob": 3.4464853797544492e-06}, {"id": 125, "seek": 65600, "start": 668.0, "end": 676.0, "text": " How does the function respond with what sensitivity does it respond? Where's the slope at that point? Does the function go up or does it go down? And by how much?", "tokens": [50364, 407, 1936, 437, 309, 311, 1566, 307, 498, 291, 4748, 9961, 493, 428, 412, 512, 935, 2031, 300, 291, 434, 3102, 294, 420, 257, 293, 498, 291, 4748, 9961, 493, 11, 291, 4748, 3488, 309, 538, 1359, 1230, 276, 13, 50964, 50964, 1012, 775, 264, 2445, 4196, 365, 437, 19392, 775, 309, 4196, 30, 2305, 311, 264, 13525, 412, 300, 935, 30, 4402, 264, 2445, 352, 493, 420, 775, 309, 352, 760, 30, 400, 538, 577, 709, 30, 51364, 51364, 400, 300, 311, 264, 13525, 295, 300, 2445, 11, 264, 13525, 295, 300, 4134, 412, 300, 935, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12796603932100184, "compression_ratio": 1.905829596412556, "no_speech_prob": 3.4464853797544492e-06}, {"id": 126, "seek": 65600, "start": 676.0, "end": 682.0, "text": " And that's the slope of that function, the slope of that response at that point.", "tokens": [50364, 407, 1936, 437, 309, 311, 1566, 307, 498, 291, 4748, 9961, 493, 428, 412, 512, 935, 2031, 300, 291, 434, 3102, 294, 420, 257, 293, 498, 291, 4748, 9961, 493, 11, 291, 4748, 3488, 309, 538, 1359, 1230, 276, 13, 50964, 50964, 1012, 775, 264, 2445, 4196, 365, 437, 19392, 775, 309, 4196, 30, 2305, 311, 264, 13525, 412, 300, 935, 30, 4402, 264, 2445, 352, 493, 420, 775, 309, 352, 760, 30, 400, 538, 577, 709, 30, 51364, 51364, 400, 300, 311, 264, 13525, 295, 300, 2445, 11, 264, 13525, 295, 300, 4134, 412, 300, 935, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12796603932100184, "compression_ratio": 1.905829596412556, "no_speech_prob": 3.4464853797544492e-06}, {"id": 127, "seek": 68200, "start": 682.0, "end": 688.0, "text": " And so we can basically evaluate the derivative here numerically by taking a very small h.", "tokens": [50364, 400, 370, 321, 393, 1936, 13059, 264, 13760, 510, 7866, 984, 538, 1940, 257, 588, 1359, 276, 13, 50664, 50664, 2720, 1164, 11, 264, 7123, 576, 1029, 505, 281, 747, 276, 281, 4018, 13, 492, 434, 445, 516, 281, 1888, 257, 588, 1359, 276, 11, 1958, 13, 628, 16, 13, 400, 718, 311, 584, 321, 434, 3102, 294, 1958, 13, 18, 13, 15, 13, 51064, 51064, 407, 321, 393, 574, 412, 283, 295, 2031, 11, 295, 1164, 11, 382, 945, 13, 400, 586, 283, 295, 2031, 1804, 276, 13, 407, 498, 321, 4748, 26109, 2031, 294, 257, 3353, 3513, 11, 577, 307, 264, 2445, 516, 281, 4196, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1211337787764413, "compression_ratio": 1.5783132530120483, "no_speech_prob": 2.04624066100223e-05}, {"id": 128, "seek": 68200, "start": 688.0, "end": 696.0, "text": " Of course, the definition would ask us to take h to zero. We're just going to pick a very small h, 0.001. And let's say we're interested in 0.3.0.", "tokens": [50364, 400, 370, 321, 393, 1936, 13059, 264, 13760, 510, 7866, 984, 538, 1940, 257, 588, 1359, 276, 13, 50664, 50664, 2720, 1164, 11, 264, 7123, 576, 1029, 505, 281, 747, 276, 281, 4018, 13, 492, 434, 445, 516, 281, 1888, 257, 588, 1359, 276, 11, 1958, 13, 628, 16, 13, 400, 718, 311, 584, 321, 434, 3102, 294, 1958, 13, 18, 13, 15, 13, 51064, 51064, 407, 321, 393, 574, 412, 283, 295, 2031, 11, 295, 1164, 11, 382, 945, 13, 400, 586, 283, 295, 2031, 1804, 276, 13, 407, 498, 321, 4748, 26109, 2031, 294, 257, 3353, 3513, 11, 577, 307, 264, 2445, 516, 281, 4196, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1211337787764413, "compression_ratio": 1.5783132530120483, "no_speech_prob": 2.04624066100223e-05}, {"id": 129, "seek": 68200, "start": 696.0, "end": 706.0, "text": " So we can look at f of x, of course, as 20. And now f of x plus h. So if we slightly notch x in a positive direction, how is the function going to respond?", "tokens": [50364, 400, 370, 321, 393, 1936, 13059, 264, 13760, 510, 7866, 984, 538, 1940, 257, 588, 1359, 276, 13, 50664, 50664, 2720, 1164, 11, 264, 7123, 576, 1029, 505, 281, 747, 276, 281, 4018, 13, 492, 434, 445, 516, 281, 1888, 257, 588, 1359, 276, 11, 1958, 13, 628, 16, 13, 400, 718, 311, 584, 321, 434, 3102, 294, 1958, 13, 18, 13, 15, 13, 51064, 51064, 407, 321, 393, 574, 412, 283, 295, 2031, 11, 295, 1164, 11, 382, 945, 13, 400, 586, 283, 295, 2031, 1804, 276, 13, 407, 498, 321, 4748, 26109, 2031, 294, 257, 3353, 3513, 11, 577, 307, 264, 2445, 516, 281, 4196, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1211337787764413, "compression_ratio": 1.5783132530120483, "no_speech_prob": 2.04624066100223e-05}, {"id": 130, "seek": 70600, "start": 706.0, "end": 715.0, "text": " And just looking at this, do you expect f of x plus h to be slightly greater than 20 or do you expect it to be slightly lower than 20?", "tokens": [50364, 400, 445, 1237, 412, 341, 11, 360, 291, 2066, 283, 295, 2031, 1804, 276, 281, 312, 4748, 5044, 813, 945, 420, 360, 291, 2066, 309, 281, 312, 4748, 3126, 813, 945, 30, 50814, 50814, 400, 1670, 341, 805, 307, 510, 293, 341, 307, 945, 11, 498, 321, 4748, 352, 25795, 11, 264, 2445, 486, 4196, 25795, 13, 51114, 51114, 407, 291, 1116, 2066, 341, 281, 312, 4748, 5044, 813, 945, 13, 400, 538, 577, 709, 307, 3585, 291, 264, 1333, 295, 264, 3800, 295, 300, 13525, 11, 558, 30, 440, 2744, 295, 300, 13525, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11192065537577928, "compression_ratio": 1.817351598173516, "no_speech_prob": 3.555954890543944e-06}, {"id": 131, "seek": 70600, "start": 715.0, "end": 721.0, "text": " And since this 3 is here and this is 20, if we slightly go positively, the function will respond positively.", "tokens": [50364, 400, 445, 1237, 412, 341, 11, 360, 291, 2066, 283, 295, 2031, 1804, 276, 281, 312, 4748, 5044, 813, 945, 420, 360, 291, 2066, 309, 281, 312, 4748, 3126, 813, 945, 30, 50814, 50814, 400, 1670, 341, 805, 307, 510, 293, 341, 307, 945, 11, 498, 321, 4748, 352, 25795, 11, 264, 2445, 486, 4196, 25795, 13, 51114, 51114, 407, 291, 1116, 2066, 341, 281, 312, 4748, 5044, 813, 945, 13, 400, 538, 577, 709, 307, 3585, 291, 264, 1333, 295, 264, 3800, 295, 300, 13525, 11, 558, 30, 440, 2744, 295, 300, 13525, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11192065537577928, "compression_ratio": 1.817351598173516, "no_speech_prob": 3.555954890543944e-06}, {"id": 132, "seek": 70600, "start": 721.0, "end": 731.0, "text": " So you'd expect this to be slightly greater than 20. And by how much is telling you the sort of the strength of that slope, right? The size of that slope.", "tokens": [50364, 400, 445, 1237, 412, 341, 11, 360, 291, 2066, 283, 295, 2031, 1804, 276, 281, 312, 4748, 5044, 813, 945, 420, 360, 291, 2066, 309, 281, 312, 4748, 3126, 813, 945, 30, 50814, 50814, 400, 1670, 341, 805, 307, 510, 293, 341, 307, 945, 11, 498, 321, 4748, 352, 25795, 11, 264, 2445, 486, 4196, 25795, 13, 51114, 51114, 407, 291, 1116, 2066, 341, 281, 312, 4748, 5044, 813, 945, 13, 400, 538, 577, 709, 307, 3585, 291, 264, 1333, 295, 264, 3800, 295, 300, 13525, 11, 558, 30, 440, 2744, 295, 300, 13525, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11192065537577928, "compression_ratio": 1.817351598173516, "no_speech_prob": 3.555954890543944e-06}, {"id": 133, "seek": 73100, "start": 731.0, "end": 737.0, "text": " So f of x plus h minus f of x. This is how much the function responded in a positive direction.", "tokens": [50364, 407, 283, 295, 2031, 1804, 276, 3175, 283, 295, 2031, 13, 639, 307, 577, 709, 264, 2445, 15806, 294, 257, 3353, 3513, 13, 50664, 50664, 400, 321, 362, 281, 2710, 1125, 538, 264, 1190, 13, 407, 321, 362, 264, 6272, 670, 1190, 281, 483, 264, 13525, 13, 50964, 50964, 407, 341, 11, 295, 1164, 11, 307, 445, 257, 29054, 28023, 295, 264, 13525, 570, 321, 362, 281, 652, 257, 588, 11, 588, 1359, 281, 41881, 281, 264, 1900, 2372, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08458649544488817, "compression_ratio": 1.6019900497512438, "no_speech_prob": 2.468228376528714e-05}, {"id": 134, "seek": 73100, "start": 737.0, "end": 743.0, "text": " And we have to normalize by the run. So we have the rise over run to get the slope.", "tokens": [50364, 407, 283, 295, 2031, 1804, 276, 3175, 283, 295, 2031, 13, 639, 307, 577, 709, 264, 2445, 15806, 294, 257, 3353, 3513, 13, 50664, 50664, 400, 321, 362, 281, 2710, 1125, 538, 264, 1190, 13, 407, 321, 362, 264, 6272, 670, 1190, 281, 483, 264, 13525, 13, 50964, 50964, 407, 341, 11, 295, 1164, 11, 307, 445, 257, 29054, 28023, 295, 264, 13525, 570, 321, 362, 281, 652, 257, 588, 11, 588, 1359, 281, 41881, 281, 264, 1900, 2372, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08458649544488817, "compression_ratio": 1.6019900497512438, "no_speech_prob": 2.468228376528714e-05}, {"id": 135, "seek": 73100, "start": 743.0, "end": 752.0, "text": " So this, of course, is just a numerical approximation of the slope because we have to make a very, very small to converge to the exact amount.", "tokens": [50364, 407, 283, 295, 2031, 1804, 276, 3175, 283, 295, 2031, 13, 639, 307, 577, 709, 264, 2445, 15806, 294, 257, 3353, 3513, 13, 50664, 50664, 400, 321, 362, 281, 2710, 1125, 538, 264, 1190, 13, 407, 321, 362, 264, 6272, 670, 1190, 281, 483, 264, 13525, 13, 50964, 50964, 407, 341, 11, 295, 1164, 11, 307, 445, 257, 29054, 28023, 295, 264, 13525, 570, 321, 362, 281, 652, 257, 588, 11, 588, 1359, 281, 41881, 281, 264, 1900, 2372, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08458649544488817, "compression_ratio": 1.6019900497512438, "no_speech_prob": 2.468228376528714e-05}, {"id": 136, "seek": 75200, "start": 752.0, "end": 764.0, "text": " Now, if I'm doing too many zeros, at some point I'm going to get an incorrect answer because we're using floating point arithmetic and the representations of all these numbers in computer memory is finite.", "tokens": [50364, 823, 11, 498, 286, 478, 884, 886, 867, 35193, 11, 412, 512, 935, 286, 478, 516, 281, 483, 364, 18424, 1867, 570, 321, 434, 1228, 12607, 935, 42973, 293, 264, 33358, 295, 439, 613, 3547, 294, 3820, 4675, 307, 19362, 13, 50964, 50964, 400, 412, 512, 935, 321, 483, 666, 5253, 13, 407, 321, 393, 41881, 3030, 264, 558, 1867, 365, 341, 3109, 13, 51264, 51264, 583, 1936, 11, 412, 805, 11, 264, 13525, 307, 3499, 13, 400, 291, 393, 536, 300, 538, 1940, 805, 87, 8889, 3175, 1017, 87, 1804, 1025, 293, 27372, 990, 309, 294, 527, 1378, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08731584365551288, "compression_ratio": 1.5884476534296028, "no_speech_prob": 8.66447498992784e-06}, {"id": 137, "seek": 75200, "start": 764.0, "end": 770.0, "text": " And at some point we get into trouble. So we can converge towards the right answer with this approach.", "tokens": [50364, 823, 11, 498, 286, 478, 884, 886, 867, 35193, 11, 412, 512, 935, 286, 478, 516, 281, 483, 364, 18424, 1867, 570, 321, 434, 1228, 12607, 935, 42973, 293, 264, 33358, 295, 439, 613, 3547, 294, 3820, 4675, 307, 19362, 13, 50964, 50964, 400, 412, 512, 935, 321, 483, 666, 5253, 13, 407, 321, 393, 41881, 3030, 264, 558, 1867, 365, 341, 3109, 13, 51264, 51264, 583, 1936, 11, 412, 805, 11, 264, 13525, 307, 3499, 13, 400, 291, 393, 536, 300, 538, 1940, 805, 87, 8889, 3175, 1017, 87, 1804, 1025, 293, 27372, 990, 309, 294, 527, 1378, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08731584365551288, "compression_ratio": 1.5884476534296028, "no_speech_prob": 8.66447498992784e-06}, {"id": 138, "seek": 75200, "start": 770.0, "end": 780.0, "text": " But basically, at 3, the slope is 14. And you can see that by taking 3x squared minus 4x plus 5 and differentiating it in our head.", "tokens": [50364, 823, 11, 498, 286, 478, 884, 886, 867, 35193, 11, 412, 512, 935, 286, 478, 516, 281, 483, 364, 18424, 1867, 570, 321, 434, 1228, 12607, 935, 42973, 293, 264, 33358, 295, 439, 613, 3547, 294, 3820, 4675, 307, 19362, 13, 50964, 50964, 400, 412, 512, 935, 321, 483, 666, 5253, 13, 407, 321, 393, 41881, 3030, 264, 558, 1867, 365, 341, 3109, 13, 51264, 51264, 583, 1936, 11, 412, 805, 11, 264, 13525, 307, 3499, 13, 400, 291, 393, 536, 300, 538, 1940, 805, 87, 8889, 3175, 1017, 87, 1804, 1025, 293, 27372, 990, 309, 294, 527, 1378, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08731584365551288, "compression_ratio": 1.5884476534296028, "no_speech_prob": 8.66447498992784e-06}, {"id": 139, "seek": 78000, "start": 780.0, "end": 791.0, "text": " So 3x squared would be 6x minus 4. And then we plug in x equals 3. So that's 18 minus 4 is 14. So this is correct.", "tokens": [50364, 407, 805, 87, 8889, 576, 312, 1386, 87, 3175, 1017, 13, 400, 550, 321, 5452, 294, 2031, 6915, 805, 13, 407, 300, 311, 2443, 3175, 1017, 307, 3499, 13, 407, 341, 307, 3006, 13, 50914, 50914, 407, 300, 311, 257, 805, 13, 823, 11, 577, 466, 264, 13525, 412, 11, 584, 11, 3671, 805, 30, 6068, 291, 2066, 11, 437, 576, 291, 2066, 337, 264, 13525, 30, 51364, 51364, 823, 11, 3585, 264, 1900, 2158, 307, 534, 1152, 13, 583, 437, 307, 264, 1465, 295, 300, 13525, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09452869581139606, "compression_ratio": 1.5472636815920398, "no_speech_prob": 8.939594408730045e-06}, {"id": 140, "seek": 78000, "start": 791.0, "end": 800.0, "text": " So that's a 3. Now, how about the slope at, say, negative 3? Would you expect, what would you expect for the slope?", "tokens": [50364, 407, 805, 87, 8889, 576, 312, 1386, 87, 3175, 1017, 13, 400, 550, 321, 5452, 294, 2031, 6915, 805, 13, 407, 300, 311, 2443, 3175, 1017, 307, 3499, 13, 407, 341, 307, 3006, 13, 50914, 50914, 407, 300, 311, 257, 805, 13, 823, 11, 577, 466, 264, 13525, 412, 11, 584, 11, 3671, 805, 30, 6068, 291, 2066, 11, 437, 576, 291, 2066, 337, 264, 13525, 30, 51364, 51364, 823, 11, 3585, 264, 1900, 2158, 307, 534, 1152, 13, 583, 437, 307, 264, 1465, 295, 300, 13525, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09452869581139606, "compression_ratio": 1.5472636815920398, "no_speech_prob": 8.939594408730045e-06}, {"id": 141, "seek": 78000, "start": 800.0, "end": 805.0, "text": " Now, telling the exact value is really hard. But what is the sign of that slope?", "tokens": [50364, 407, 805, 87, 8889, 576, 312, 1386, 87, 3175, 1017, 13, 400, 550, 321, 5452, 294, 2031, 6915, 805, 13, 407, 300, 311, 2443, 3175, 1017, 307, 3499, 13, 407, 341, 307, 3006, 13, 50914, 50914, 407, 300, 311, 257, 805, 13, 823, 11, 577, 466, 264, 13525, 412, 11, 584, 11, 3671, 805, 30, 6068, 291, 2066, 11, 437, 576, 291, 2066, 337, 264, 13525, 30, 51364, 51364, 823, 11, 3585, 264, 1900, 2158, 307, 534, 1152, 13, 583, 437, 307, 264, 1465, 295, 300, 13525, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09452869581139606, "compression_ratio": 1.5472636815920398, "no_speech_prob": 8.939594408730045e-06}, {"id": 142, "seek": 80500, "start": 805.0, "end": 812.0, "text": " So at negative 3, if we slightly go in the positive direction at x, the function would actually go down.", "tokens": [50364, 407, 412, 3671, 805, 11, 498, 321, 4748, 352, 294, 264, 3353, 3513, 412, 2031, 11, 264, 2445, 576, 767, 352, 760, 13, 50714, 50714, 400, 370, 300, 5112, 291, 300, 264, 13525, 576, 312, 3671, 13, 407, 321, 603, 483, 257, 4036, 1230, 2507, 945, 13, 51014, 51014, 400, 370, 498, 321, 747, 264, 13525, 11, 321, 2066, 746, 3671, 5853, 13, 51314, 51314, 400, 412, 512, 935, 510, 11, 295, 1164, 11, 264, 13525, 576, 312, 1958, 13, 823, 11, 337, 341, 2685, 2445, 11, 286, 2956, 309, 493, 8046, 11, 293, 309, 311, 412, 935, 568, 670, 805, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0675697461614069, "compression_ratio": 1.7276595744680852, "no_speech_prob": 5.771855285274796e-06}, {"id": 143, "seek": 80500, "start": 812.0, "end": 818.0, "text": " And so that tells you that the slope would be negative. So we'll get a slight number below 20.", "tokens": [50364, 407, 412, 3671, 805, 11, 498, 321, 4748, 352, 294, 264, 3353, 3513, 412, 2031, 11, 264, 2445, 576, 767, 352, 760, 13, 50714, 50714, 400, 370, 300, 5112, 291, 300, 264, 13525, 576, 312, 3671, 13, 407, 321, 603, 483, 257, 4036, 1230, 2507, 945, 13, 51014, 51014, 400, 370, 498, 321, 747, 264, 13525, 11, 321, 2066, 746, 3671, 5853, 13, 51314, 51314, 400, 412, 512, 935, 510, 11, 295, 1164, 11, 264, 13525, 576, 312, 1958, 13, 823, 11, 337, 341, 2685, 2445, 11, 286, 2956, 309, 493, 8046, 11, 293, 309, 311, 412, 935, 568, 670, 805, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0675697461614069, "compression_ratio": 1.7276595744680852, "no_speech_prob": 5.771855285274796e-06}, {"id": 144, "seek": 80500, "start": 818.0, "end": 824.0, "text": " And so if we take the slope, we expect something negative 22.", "tokens": [50364, 407, 412, 3671, 805, 11, 498, 321, 4748, 352, 294, 264, 3353, 3513, 412, 2031, 11, 264, 2445, 576, 767, 352, 760, 13, 50714, 50714, 400, 370, 300, 5112, 291, 300, 264, 13525, 576, 312, 3671, 13, 407, 321, 603, 483, 257, 4036, 1230, 2507, 945, 13, 51014, 51014, 400, 370, 498, 321, 747, 264, 13525, 11, 321, 2066, 746, 3671, 5853, 13, 51314, 51314, 400, 412, 512, 935, 510, 11, 295, 1164, 11, 264, 13525, 576, 312, 1958, 13, 823, 11, 337, 341, 2685, 2445, 11, 286, 2956, 309, 493, 8046, 11, 293, 309, 311, 412, 935, 568, 670, 805, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0675697461614069, "compression_ratio": 1.7276595744680852, "no_speech_prob": 5.771855285274796e-06}, {"id": 145, "seek": 80500, "start": 824.0, "end": 832.0, "text": " And at some point here, of course, the slope would be 0. Now, for this specific function, I looked it up previously, and it's at point 2 over 3.", "tokens": [50364, 407, 412, 3671, 805, 11, 498, 321, 4748, 352, 294, 264, 3353, 3513, 412, 2031, 11, 264, 2445, 576, 767, 352, 760, 13, 50714, 50714, 400, 370, 300, 5112, 291, 300, 264, 13525, 576, 312, 3671, 13, 407, 321, 603, 483, 257, 4036, 1230, 2507, 945, 13, 51014, 51014, 400, 370, 498, 321, 747, 264, 13525, 11, 321, 2066, 746, 3671, 5853, 13, 51314, 51314, 400, 412, 512, 935, 510, 11, 295, 1164, 11, 264, 13525, 576, 312, 1958, 13, 823, 11, 337, 341, 2685, 2445, 11, 286, 2956, 309, 493, 8046, 11, 293, 309, 311, 412, 935, 568, 670, 805, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0675697461614069, "compression_ratio": 1.7276595744680852, "no_speech_prob": 5.771855285274796e-06}, {"id": 146, "seek": 83200, "start": 832.0, "end": 839.0, "text": " So at roughly 2 over 3, that's somewhere here, this derivative would be 0.", "tokens": [50364, 407, 412, 9810, 568, 670, 805, 11, 300, 311, 4079, 510, 11, 341, 13760, 576, 312, 1958, 13, 50714, 50714, 407, 1936, 11, 412, 300, 13600, 935, 11, 498, 321, 297, 16032, 294, 257, 3353, 3513, 11, 264, 2445, 1177, 380, 4196, 13, 51164, 51164, 639, 10834, 264, 912, 1920, 13, 400, 370, 300, 311, 983, 264, 13525, 307, 1958, 13, 51314, 51314, 2264, 11, 586, 718, 311, 574, 412, 257, 857, 544, 3997, 1389, 13, 407, 321, 434, 516, 281, 722, 3997, 5489, 257, 857, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07512413800417722, "compression_ratio": 1.5159817351598173, "no_speech_prob": 4.157343028055038e-06}, {"id": 147, "seek": 83200, "start": 839.0, "end": 848.0, "text": " So basically, at that precise point, if we nudge in a positive direction, the function doesn't respond.", "tokens": [50364, 407, 412, 9810, 568, 670, 805, 11, 300, 311, 4079, 510, 11, 341, 13760, 576, 312, 1958, 13, 50714, 50714, 407, 1936, 11, 412, 300, 13600, 935, 11, 498, 321, 297, 16032, 294, 257, 3353, 3513, 11, 264, 2445, 1177, 380, 4196, 13, 51164, 51164, 639, 10834, 264, 912, 1920, 13, 400, 370, 300, 311, 983, 264, 13525, 307, 1958, 13, 51314, 51314, 2264, 11, 586, 718, 311, 574, 412, 257, 857, 544, 3997, 1389, 13, 407, 321, 434, 516, 281, 722, 3997, 5489, 257, 857, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07512413800417722, "compression_ratio": 1.5159817351598173, "no_speech_prob": 4.157343028055038e-06}, {"id": 148, "seek": 83200, "start": 848.0, "end": 851.0, "text": " This stays the same almost. And so that's why the slope is 0.", "tokens": [50364, 407, 412, 9810, 568, 670, 805, 11, 300, 311, 4079, 510, 11, 341, 13760, 576, 312, 1958, 13, 50714, 50714, 407, 1936, 11, 412, 300, 13600, 935, 11, 498, 321, 297, 16032, 294, 257, 3353, 3513, 11, 264, 2445, 1177, 380, 4196, 13, 51164, 51164, 639, 10834, 264, 912, 1920, 13, 400, 370, 300, 311, 983, 264, 13525, 307, 1958, 13, 51314, 51314, 2264, 11, 586, 718, 311, 574, 412, 257, 857, 544, 3997, 1389, 13, 407, 321, 434, 516, 281, 722, 3997, 5489, 257, 857, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07512413800417722, "compression_ratio": 1.5159817351598173, "no_speech_prob": 4.157343028055038e-06}, {"id": 149, "seek": 83200, "start": 851.0, "end": 857.0, "text": " OK, now let's look at a bit more complex case. So we're going to start complexifying a bit.", "tokens": [50364, 407, 412, 9810, 568, 670, 805, 11, 300, 311, 4079, 510, 11, 341, 13760, 576, 312, 1958, 13, 50714, 50714, 407, 1936, 11, 412, 300, 13600, 935, 11, 498, 321, 297, 16032, 294, 257, 3353, 3513, 11, 264, 2445, 1177, 380, 4196, 13, 51164, 51164, 639, 10834, 264, 912, 1920, 13, 400, 370, 300, 311, 983, 264, 13525, 307, 1958, 13, 51314, 51314, 2264, 11, 586, 718, 311, 574, 412, 257, 857, 544, 3997, 1389, 13, 407, 321, 434, 516, 281, 722, 3997, 5489, 257, 857, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07512413800417722, "compression_ratio": 1.5159817351598173, "no_speech_prob": 4.157343028055038e-06}, {"id": 150, "seek": 85700, "start": 857.0, "end": 866.0, "text": " So now we have a function here with output variable d that is a function of three scalar inputs, a, b, and c.", "tokens": [50364, 407, 586, 321, 362, 257, 2445, 510, 365, 5598, 7006, 274, 300, 307, 257, 2445, 295, 1045, 39684, 15743, 11, 257, 11, 272, 11, 293, 269, 13, 50814, 50814, 407, 257, 11, 272, 11, 293, 269, 366, 512, 2685, 4190, 11, 1045, 15743, 666, 527, 6114, 4295, 11, 293, 257, 2167, 5598, 274, 13, 51164, 51164, 400, 370, 498, 321, 445, 4482, 274, 11, 321, 483, 1017, 13, 51314, 51314, 400, 586, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 11, 797, 11, 574, 412, 264, 33733, 295, 274, 365, 3104, 281, 257, 11, 272, 11, 293, 269, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08305582010521079, "compression_ratio": 1.6854460093896713, "no_speech_prob": 1.2411303032422438e-05}, {"id": 151, "seek": 85700, "start": 866.0, "end": 873.0, "text": " So a, b, and c are some specific values, three inputs into our expression graph, and a single output d.", "tokens": [50364, 407, 586, 321, 362, 257, 2445, 510, 365, 5598, 7006, 274, 300, 307, 257, 2445, 295, 1045, 39684, 15743, 11, 257, 11, 272, 11, 293, 269, 13, 50814, 50814, 407, 257, 11, 272, 11, 293, 269, 366, 512, 2685, 4190, 11, 1045, 15743, 666, 527, 6114, 4295, 11, 293, 257, 2167, 5598, 274, 13, 51164, 51164, 400, 370, 498, 321, 445, 4482, 274, 11, 321, 483, 1017, 13, 51314, 51314, 400, 586, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 11, 797, 11, 574, 412, 264, 33733, 295, 274, 365, 3104, 281, 257, 11, 272, 11, 293, 269, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08305582010521079, "compression_ratio": 1.6854460093896713, "no_speech_prob": 1.2411303032422438e-05}, {"id": 152, "seek": 85700, "start": 873.0, "end": 876.0, "text": " And so if we just print d, we get 4.", "tokens": [50364, 407, 586, 321, 362, 257, 2445, 510, 365, 5598, 7006, 274, 300, 307, 257, 2445, 295, 1045, 39684, 15743, 11, 257, 11, 272, 11, 293, 269, 13, 50814, 50814, 407, 257, 11, 272, 11, 293, 269, 366, 512, 2685, 4190, 11, 1045, 15743, 666, 527, 6114, 4295, 11, 293, 257, 2167, 5598, 274, 13, 51164, 51164, 400, 370, 498, 321, 445, 4482, 274, 11, 321, 483, 1017, 13, 51314, 51314, 400, 586, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 11, 797, 11, 574, 412, 264, 33733, 295, 274, 365, 3104, 281, 257, 11, 272, 11, 293, 269, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08305582010521079, "compression_ratio": 1.6854460093896713, "no_speech_prob": 1.2411303032422438e-05}, {"id": 153, "seek": 85700, "start": 876.0, "end": 882.0, "text": " And now what I'd like to do is I'd like to, again, look at the derivatives of d with respect to a, b, and c.", "tokens": [50364, 407, 586, 321, 362, 257, 2445, 510, 365, 5598, 7006, 274, 300, 307, 257, 2445, 295, 1045, 39684, 15743, 11, 257, 11, 272, 11, 293, 269, 13, 50814, 50814, 407, 257, 11, 272, 11, 293, 269, 366, 512, 2685, 4190, 11, 1045, 15743, 666, 527, 6114, 4295, 11, 293, 257, 2167, 5598, 274, 13, 51164, 51164, 400, 370, 498, 321, 445, 4482, 274, 11, 321, 483, 1017, 13, 51314, 51314, 400, 586, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 11, 797, 11, 574, 412, 264, 33733, 295, 274, 365, 3104, 281, 257, 11, 272, 11, 293, 269, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08305582010521079, "compression_ratio": 1.6854460093896713, "no_speech_prob": 1.2411303032422438e-05}, {"id": 154, "seek": 88200, "start": 882.0, "end": 887.0, "text": " And think through, again, just the intuition of what this derivative is telling us.", "tokens": [50364, 400, 519, 807, 11, 797, 11, 445, 264, 24002, 295, 437, 341, 13760, 307, 3585, 505, 13, 50614, 50614, 407, 294, 1668, 281, 13059, 341, 13760, 11, 321, 434, 516, 281, 483, 257, 857, 10339, 88, 510, 13, 50864, 50864, 492, 434, 516, 281, 11, 797, 11, 362, 257, 588, 1359, 2158, 295, 276, 13, 51014, 51014, 400, 550, 321, 434, 516, 281, 3191, 264, 15743, 412, 512, 4190, 300, 321, 434, 3102, 294, 13, 51264, 51264, 407, 341, 307, 264, 935, 257, 11, 272, 11, 269, 412, 597, 321, 434, 516, 281, 312, 27479, 264, 13760, 295, 274, 365, 3104, 281, 439, 257, 11, 272, 11, 293, 269, 412, 300, 935, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04262570438221989, "compression_ratio": 1.8448275862068966, "no_speech_prob": 1.095286279451102e-05}, {"id": 155, "seek": 88200, "start": 887.0, "end": 892.0, "text": " So in order to evaluate this derivative, we're going to get a bit hacky here.", "tokens": [50364, 400, 519, 807, 11, 797, 11, 445, 264, 24002, 295, 437, 341, 13760, 307, 3585, 505, 13, 50614, 50614, 407, 294, 1668, 281, 13059, 341, 13760, 11, 321, 434, 516, 281, 483, 257, 857, 10339, 88, 510, 13, 50864, 50864, 492, 434, 516, 281, 11, 797, 11, 362, 257, 588, 1359, 2158, 295, 276, 13, 51014, 51014, 400, 550, 321, 434, 516, 281, 3191, 264, 15743, 412, 512, 4190, 300, 321, 434, 3102, 294, 13, 51264, 51264, 407, 341, 307, 264, 935, 257, 11, 272, 11, 269, 412, 597, 321, 434, 516, 281, 312, 27479, 264, 13760, 295, 274, 365, 3104, 281, 439, 257, 11, 272, 11, 293, 269, 412, 300, 935, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04262570438221989, "compression_ratio": 1.8448275862068966, "no_speech_prob": 1.095286279451102e-05}, {"id": 156, "seek": 88200, "start": 892.0, "end": 895.0, "text": " We're going to, again, have a very small value of h.", "tokens": [50364, 400, 519, 807, 11, 797, 11, 445, 264, 24002, 295, 437, 341, 13760, 307, 3585, 505, 13, 50614, 50614, 407, 294, 1668, 281, 13059, 341, 13760, 11, 321, 434, 516, 281, 483, 257, 857, 10339, 88, 510, 13, 50864, 50864, 492, 434, 516, 281, 11, 797, 11, 362, 257, 588, 1359, 2158, 295, 276, 13, 51014, 51014, 400, 550, 321, 434, 516, 281, 3191, 264, 15743, 412, 512, 4190, 300, 321, 434, 3102, 294, 13, 51264, 51264, 407, 341, 307, 264, 935, 257, 11, 272, 11, 269, 412, 597, 321, 434, 516, 281, 312, 27479, 264, 13760, 295, 274, 365, 3104, 281, 439, 257, 11, 272, 11, 293, 269, 412, 300, 935, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04262570438221989, "compression_ratio": 1.8448275862068966, "no_speech_prob": 1.095286279451102e-05}, {"id": 157, "seek": 88200, "start": 895.0, "end": 900.0, "text": " And then we're going to fix the inputs at some values that we're interested in.", "tokens": [50364, 400, 519, 807, 11, 797, 11, 445, 264, 24002, 295, 437, 341, 13760, 307, 3585, 505, 13, 50614, 50614, 407, 294, 1668, 281, 13059, 341, 13760, 11, 321, 434, 516, 281, 483, 257, 857, 10339, 88, 510, 13, 50864, 50864, 492, 434, 516, 281, 11, 797, 11, 362, 257, 588, 1359, 2158, 295, 276, 13, 51014, 51014, 400, 550, 321, 434, 516, 281, 3191, 264, 15743, 412, 512, 4190, 300, 321, 434, 3102, 294, 13, 51264, 51264, 407, 341, 307, 264, 935, 257, 11, 272, 11, 269, 412, 597, 321, 434, 516, 281, 312, 27479, 264, 13760, 295, 274, 365, 3104, 281, 439, 257, 11, 272, 11, 293, 269, 412, 300, 935, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04262570438221989, "compression_ratio": 1.8448275862068966, "no_speech_prob": 1.095286279451102e-05}, {"id": 158, "seek": 88200, "start": 900.0, "end": 909.0, "text": " So this is the point a, b, c at which we're going to be evaluating the derivative of d with respect to all a, b, and c at that point.", "tokens": [50364, 400, 519, 807, 11, 797, 11, 445, 264, 24002, 295, 437, 341, 13760, 307, 3585, 505, 13, 50614, 50614, 407, 294, 1668, 281, 13059, 341, 13760, 11, 321, 434, 516, 281, 483, 257, 857, 10339, 88, 510, 13, 50864, 50864, 492, 434, 516, 281, 11, 797, 11, 362, 257, 588, 1359, 2158, 295, 276, 13, 51014, 51014, 400, 550, 321, 434, 516, 281, 3191, 264, 15743, 412, 512, 4190, 300, 321, 434, 3102, 294, 13, 51264, 51264, 407, 341, 307, 264, 935, 257, 11, 272, 11, 269, 412, 597, 321, 434, 516, 281, 312, 27479, 264, 13760, 295, 274, 365, 3104, 281, 439, 257, 11, 272, 11, 293, 269, 412, 300, 935, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04262570438221989, "compression_ratio": 1.8448275862068966, "no_speech_prob": 1.095286279451102e-05}, {"id": 159, "seek": 90900, "start": 909.0, "end": 913.0, "text": " So there are the inputs, and now we have d1 is that expression.", "tokens": [50364, 407, 456, 366, 264, 15743, 11, 293, 586, 321, 362, 274, 16, 307, 300, 6114, 13, 50564, 50564, 400, 550, 321, 434, 516, 281, 11, 337, 1365, 11, 574, 412, 264, 13760, 295, 274, 365, 3104, 281, 257, 13, 50764, 50764, 407, 321, 603, 747, 257, 11, 293, 321, 603, 9961, 309, 538, 276, 11, 293, 550, 321, 603, 483, 274, 17, 281, 312, 264, 1900, 912, 2445, 13, 51064, 51064, 400, 586, 321, 434, 516, 281, 4482, 11, 291, 458, 11, 283, 16, 11, 274, 16, 307, 274, 16, 11, 274, 17, 307, 274, 17, 11, 293, 4482, 13525, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07043096451532273, "compression_ratio": 1.6069651741293531, "no_speech_prob": 1.670129677222576e-05}, {"id": 160, "seek": 90900, "start": 913.0, "end": 917.0, "text": " And then we're going to, for example, look at the derivative of d with respect to a.", "tokens": [50364, 407, 456, 366, 264, 15743, 11, 293, 586, 321, 362, 274, 16, 307, 300, 6114, 13, 50564, 50564, 400, 550, 321, 434, 516, 281, 11, 337, 1365, 11, 574, 412, 264, 13760, 295, 274, 365, 3104, 281, 257, 13, 50764, 50764, 407, 321, 603, 747, 257, 11, 293, 321, 603, 9961, 309, 538, 276, 11, 293, 550, 321, 603, 483, 274, 17, 281, 312, 264, 1900, 912, 2445, 13, 51064, 51064, 400, 586, 321, 434, 516, 281, 4482, 11, 291, 458, 11, 283, 16, 11, 274, 16, 307, 274, 16, 11, 274, 17, 307, 274, 17, 11, 293, 4482, 13525, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07043096451532273, "compression_ratio": 1.6069651741293531, "no_speech_prob": 1.670129677222576e-05}, {"id": 161, "seek": 90900, "start": 917.0, "end": 923.0, "text": " So we'll take a, and we'll bump it by h, and then we'll get d2 to be the exact same function.", "tokens": [50364, 407, 456, 366, 264, 15743, 11, 293, 586, 321, 362, 274, 16, 307, 300, 6114, 13, 50564, 50564, 400, 550, 321, 434, 516, 281, 11, 337, 1365, 11, 574, 412, 264, 13760, 295, 274, 365, 3104, 281, 257, 13, 50764, 50764, 407, 321, 603, 747, 257, 11, 293, 321, 603, 9961, 309, 538, 276, 11, 293, 550, 321, 603, 483, 274, 17, 281, 312, 264, 1900, 912, 2445, 13, 51064, 51064, 400, 586, 321, 434, 516, 281, 4482, 11, 291, 458, 11, 283, 16, 11, 274, 16, 307, 274, 16, 11, 274, 17, 307, 274, 17, 11, 293, 4482, 13525, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07043096451532273, "compression_ratio": 1.6069651741293531, "no_speech_prob": 1.670129677222576e-05}, {"id": 162, "seek": 90900, "start": 923.0, "end": 935.0, "text": " And now we're going to print, you know, f1, d1 is d1, d2 is d2, and print slope.", "tokens": [50364, 407, 456, 366, 264, 15743, 11, 293, 586, 321, 362, 274, 16, 307, 300, 6114, 13, 50564, 50564, 400, 550, 321, 434, 516, 281, 11, 337, 1365, 11, 574, 412, 264, 13760, 295, 274, 365, 3104, 281, 257, 13, 50764, 50764, 407, 321, 603, 747, 257, 11, 293, 321, 603, 9961, 309, 538, 276, 11, 293, 550, 321, 603, 483, 274, 17, 281, 312, 264, 1900, 912, 2445, 13, 51064, 51064, 400, 586, 321, 434, 516, 281, 4482, 11, 291, 458, 11, 283, 16, 11, 274, 16, 307, 274, 16, 11, 274, 17, 307, 274, 17, 11, 293, 4482, 13525, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07043096451532273, "compression_ratio": 1.6069651741293531, "no_speech_prob": 1.670129677222576e-05}, {"id": 163, "seek": 93500, "start": 935.0, "end": 944.0, "text": " So the derivative or slope here will be, of course, d2 minus d1 divided by h.", "tokens": [50364, 407, 264, 13760, 420, 13525, 510, 486, 312, 11, 295, 1164, 11, 274, 17, 3175, 274, 16, 6666, 538, 276, 13, 50814, 50814, 407, 274, 17, 3175, 274, 16, 307, 577, 709, 264, 2445, 6505, 562, 321, 42696, 264, 2685, 4846, 300, 321, 434, 3102, 294, 538, 257, 5870, 2372, 13, 51414, 51414, 400, 341, 307, 264, 48704, 538, 276, 281, 483, 264, 13525, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05978567012842151, "compression_ratio": 1.4682080924855492, "no_speech_prob": 1.2805318874598015e-05}, {"id": 164, "seek": 93500, "start": 944.0, "end": 956.0, "text": " So d2 minus d1 is how much the function increased when we bumped the specific input that we're interested in by a tiny amount.", "tokens": [50364, 407, 264, 13760, 420, 13525, 510, 486, 312, 11, 295, 1164, 11, 274, 17, 3175, 274, 16, 6666, 538, 276, 13, 50814, 50814, 407, 274, 17, 3175, 274, 16, 307, 577, 709, 264, 2445, 6505, 562, 321, 42696, 264, 2685, 4846, 300, 321, 434, 3102, 294, 538, 257, 5870, 2372, 13, 51414, 51414, 400, 341, 307, 264, 48704, 538, 276, 281, 483, 264, 13525, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05978567012842151, "compression_ratio": 1.4682080924855492, "no_speech_prob": 1.2805318874598015e-05}, {"id": 165, "seek": 93500, "start": 956.0, "end": 963.0, "text": " And this is the normalized by h to get the slope.", "tokens": [50364, 407, 264, 13760, 420, 13525, 510, 486, 312, 11, 295, 1164, 11, 274, 17, 3175, 274, 16, 6666, 538, 276, 13, 50814, 50814, 407, 274, 17, 3175, 274, 16, 307, 577, 709, 264, 2445, 6505, 562, 321, 42696, 264, 2685, 4846, 300, 321, 434, 3102, 294, 538, 257, 5870, 2372, 13, 51414, 51414, 400, 341, 307, 264, 48704, 538, 276, 281, 483, 264, 13525, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05978567012842151, "compression_ratio": 1.4682080924855492, "no_speech_prob": 1.2805318874598015e-05}, {"id": 166, "seek": 96300, "start": 963.0, "end": 966.0, "text": " So, yeah.", "tokens": [50364, 407, 11, 1338, 13, 50514, 50514, 407, 341, 11, 370, 286, 445, 1190, 341, 13, 50664, 50664, 492, 434, 516, 281, 4482, 274, 16, 11, 597, 321, 458, 307, 1017, 13, 50964, 50964, 823, 274, 17, 486, 312, 42696, 11, 257, 486, 312, 42696, 538, 276, 13, 51214, 51214, 407, 718, 311, 445, 519, 807, 257, 707, 857, 437, 274, 17, 486, 312, 13567, 484, 510, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13147859170403278, "compression_ratio": 1.4166666666666667, "no_speech_prob": 2.2473259377875365e-05}, {"id": 167, "seek": 96300, "start": 966.0, "end": 969.0, "text": " So this, so I just run this.", "tokens": [50364, 407, 11, 1338, 13, 50514, 50514, 407, 341, 11, 370, 286, 445, 1190, 341, 13, 50664, 50664, 492, 434, 516, 281, 4482, 274, 16, 11, 597, 321, 458, 307, 1017, 13, 50964, 50964, 823, 274, 17, 486, 312, 42696, 11, 257, 486, 312, 42696, 538, 276, 13, 51214, 51214, 407, 718, 311, 445, 519, 807, 257, 707, 857, 437, 274, 17, 486, 312, 13567, 484, 510, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13147859170403278, "compression_ratio": 1.4166666666666667, "no_speech_prob": 2.2473259377875365e-05}, {"id": 168, "seek": 96300, "start": 969.0, "end": 975.0, "text": " We're going to print d1, which we know is 4.", "tokens": [50364, 407, 11, 1338, 13, 50514, 50514, 407, 341, 11, 370, 286, 445, 1190, 341, 13, 50664, 50664, 492, 434, 516, 281, 4482, 274, 16, 11, 597, 321, 458, 307, 1017, 13, 50964, 50964, 823, 274, 17, 486, 312, 42696, 11, 257, 486, 312, 42696, 538, 276, 13, 51214, 51214, 407, 718, 311, 445, 519, 807, 257, 707, 857, 437, 274, 17, 486, 312, 13567, 484, 510, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13147859170403278, "compression_ratio": 1.4166666666666667, "no_speech_prob": 2.2473259377875365e-05}, {"id": 169, "seek": 96300, "start": 975.0, "end": 980.0, "text": " Now d2 will be bumped, a will be bumped by h.", "tokens": [50364, 407, 11, 1338, 13, 50514, 50514, 407, 341, 11, 370, 286, 445, 1190, 341, 13, 50664, 50664, 492, 434, 516, 281, 4482, 274, 16, 11, 597, 321, 458, 307, 1017, 13, 50964, 50964, 823, 274, 17, 486, 312, 42696, 11, 257, 486, 312, 42696, 538, 276, 13, 51214, 51214, 407, 718, 311, 445, 519, 807, 257, 707, 857, 437, 274, 17, 486, 312, 13567, 484, 510, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13147859170403278, "compression_ratio": 1.4166666666666667, "no_speech_prob": 2.2473259377875365e-05}, {"id": 170, "seek": 96300, "start": 980.0, "end": 988.0, "text": " So let's just think through a little bit what d2 will be printed out here.", "tokens": [50364, 407, 11, 1338, 13, 50514, 50514, 407, 341, 11, 370, 286, 445, 1190, 341, 13, 50664, 50664, 492, 434, 516, 281, 4482, 274, 16, 11, 597, 321, 458, 307, 1017, 13, 50964, 50964, 823, 274, 17, 486, 312, 42696, 11, 257, 486, 312, 42696, 538, 276, 13, 51214, 51214, 407, 718, 311, 445, 519, 807, 257, 707, 857, 437, 274, 17, 486, 312, 13567, 484, 510, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13147859170403278, "compression_ratio": 1.4166666666666667, "no_speech_prob": 2.2473259377875365e-05}, {"id": 171, "seek": 98800, "start": 988.0, "end": 996.0, "text": " In particular, d1 will be 4, will d2 be a number slightly greater than 4 or slightly lower than 4?", "tokens": [50364, 682, 1729, 11, 274, 16, 486, 312, 1017, 11, 486, 274, 17, 312, 257, 1230, 4748, 5044, 813, 1017, 420, 4748, 3126, 813, 1017, 30, 50764, 50764, 400, 309, 311, 516, 281, 980, 505, 264, 1465, 295, 264, 13760, 13, 50964, 50964, 407, 321, 434, 9961, 278, 257, 538, 276, 11, 272, 307, 3175, 805, 11, 269, 307, 1266, 13, 51414, 51414, 407, 291, 393, 445, 46506, 519, 807, 341, 13760, 293, 437, 309, 311, 884, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08657175228919511, "compression_ratio": 1.4840425531914894, "no_speech_prob": 2.3552354832645506e-05}, {"id": 172, "seek": 98800, "start": 996.0, "end": 1000.0, "text": " And it's going to tell us the sign of the derivative.", "tokens": [50364, 682, 1729, 11, 274, 16, 486, 312, 1017, 11, 486, 274, 17, 312, 257, 1230, 4748, 5044, 813, 1017, 420, 4748, 3126, 813, 1017, 30, 50764, 50764, 400, 309, 311, 516, 281, 980, 505, 264, 1465, 295, 264, 13760, 13, 50964, 50964, 407, 321, 434, 9961, 278, 257, 538, 276, 11, 272, 307, 3175, 805, 11, 269, 307, 1266, 13, 51414, 51414, 407, 291, 393, 445, 46506, 519, 807, 341, 13760, 293, 437, 309, 311, 884, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08657175228919511, "compression_ratio": 1.4840425531914894, "no_speech_prob": 2.3552354832645506e-05}, {"id": 173, "seek": 98800, "start": 1000.0, "end": 1009.0, "text": " So we're bumping a by h, b is minus 3, c is 10.", "tokens": [50364, 682, 1729, 11, 274, 16, 486, 312, 1017, 11, 486, 274, 17, 312, 257, 1230, 4748, 5044, 813, 1017, 420, 4748, 3126, 813, 1017, 30, 50764, 50764, 400, 309, 311, 516, 281, 980, 505, 264, 1465, 295, 264, 13760, 13, 50964, 50964, 407, 321, 434, 9961, 278, 257, 538, 276, 11, 272, 307, 3175, 805, 11, 269, 307, 1266, 13, 51414, 51414, 407, 291, 393, 445, 46506, 519, 807, 341, 13760, 293, 437, 309, 311, 884, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08657175228919511, "compression_ratio": 1.4840425531914894, "no_speech_prob": 2.3552354832645506e-05}, {"id": 174, "seek": 98800, "start": 1009.0, "end": 1012.0, "text": " So you can just intuitively think through this derivative and what it's doing.", "tokens": [50364, 682, 1729, 11, 274, 16, 486, 312, 1017, 11, 486, 274, 17, 312, 257, 1230, 4748, 5044, 813, 1017, 420, 4748, 3126, 813, 1017, 30, 50764, 50764, 400, 309, 311, 516, 281, 980, 505, 264, 1465, 295, 264, 13760, 13, 50964, 50964, 407, 321, 434, 9961, 278, 257, 538, 276, 11, 272, 307, 3175, 805, 11, 269, 307, 1266, 13, 51414, 51414, 407, 291, 393, 445, 46506, 519, 807, 341, 13760, 293, 437, 309, 311, 884, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08657175228919511, "compression_ratio": 1.4840425531914894, "no_speech_prob": 2.3552354832645506e-05}, {"id": 175, "seek": 101200, "start": 1012.0, "end": 1018.0, "text": " So a will be slightly more positive and b is a negative number.", "tokens": [50364, 407, 257, 486, 312, 4748, 544, 3353, 293, 272, 307, 257, 3671, 1230, 13, 50664, 50664, 407, 498, 257, 307, 4748, 544, 3353, 11, 570, 272, 307, 3671, 805, 11, 321, 434, 767, 516, 281, 312, 5127, 1570, 281, 274, 13, 51164, 51164, 407, 291, 1116, 767, 2066, 300, 264, 2158, 295, 264, 2445, 486, 352, 760, 13, 51464, 51464, 407, 718, 311, 445, 536, 341, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10040975624406842, "compression_ratio": 1.5975609756097562, "no_speech_prob": 1.834258910093922e-05}, {"id": 176, "seek": 101200, "start": 1018.0, "end": 1028.0, "text": " So if a is slightly more positive, because b is negative 3, we're actually going to be adding less to d.", "tokens": [50364, 407, 257, 486, 312, 4748, 544, 3353, 293, 272, 307, 257, 3671, 1230, 13, 50664, 50664, 407, 498, 257, 307, 4748, 544, 3353, 11, 570, 272, 307, 3671, 805, 11, 321, 434, 767, 516, 281, 312, 5127, 1570, 281, 274, 13, 51164, 51164, 407, 291, 1116, 767, 2066, 300, 264, 2158, 295, 264, 2445, 486, 352, 760, 13, 51464, 51464, 407, 718, 311, 445, 536, 341, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10040975624406842, "compression_ratio": 1.5975609756097562, "no_speech_prob": 1.834258910093922e-05}, {"id": 177, "seek": 101200, "start": 1028.0, "end": 1034.0, "text": " So you'd actually expect that the value of the function will go down.", "tokens": [50364, 407, 257, 486, 312, 4748, 544, 3353, 293, 272, 307, 257, 3671, 1230, 13, 50664, 50664, 407, 498, 257, 307, 4748, 544, 3353, 11, 570, 272, 307, 3671, 805, 11, 321, 434, 767, 516, 281, 312, 5127, 1570, 281, 274, 13, 51164, 51164, 407, 291, 1116, 767, 2066, 300, 264, 2158, 295, 264, 2445, 486, 352, 760, 13, 51464, 51464, 407, 718, 311, 445, 536, 341, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10040975624406842, "compression_ratio": 1.5975609756097562, "no_speech_prob": 1.834258910093922e-05}, {"id": 178, "seek": 101200, "start": 1034.0, "end": 1037.0, "text": " So let's just see this.", "tokens": [50364, 407, 257, 486, 312, 4748, 544, 3353, 293, 272, 307, 257, 3671, 1230, 13, 50664, 50664, 407, 498, 257, 307, 4748, 544, 3353, 11, 570, 272, 307, 3671, 805, 11, 321, 434, 767, 516, 281, 312, 5127, 1570, 281, 274, 13, 51164, 51164, 407, 291, 1116, 767, 2066, 300, 264, 2158, 295, 264, 2445, 486, 352, 760, 13, 51464, 51464, 407, 718, 311, 445, 536, 341, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10040975624406842, "compression_ratio": 1.5975609756097562, "no_speech_prob": 1.834258910093922e-05}, {"id": 179, "seek": 103700, "start": 1037.0, "end": 1048.0, "text": " And so we went from 4 to 3.9996, and that tells you that the slope will be negative and then will be a negative number because we went down.", "tokens": [50364, 400, 370, 321, 1437, 490, 1017, 281, 805, 13, 8494, 22962, 11, 293, 300, 5112, 291, 300, 264, 13525, 486, 312, 3671, 293, 550, 486, 312, 257, 3671, 1230, 570, 321, 1437, 760, 13, 50914, 50914, 400, 550, 264, 1900, 1230, 295, 13525, 486, 312, 11, 1900, 1230, 295, 13525, 307, 3671, 805, 13, 51214, 51214, 400, 291, 393, 611, 13447, 1803, 300, 3671, 805, 307, 264, 558, 1867, 44003, 293, 10783, 984, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10191532893058582, "compression_ratio": 1.8465909090909092, "no_speech_prob": 7.25394711480476e-05}, {"id": 180, "seek": 103700, "start": 1048.0, "end": 1054.0, "text": " And then the exact number of slope will be, exact number of slope is negative 3.", "tokens": [50364, 400, 370, 321, 1437, 490, 1017, 281, 805, 13, 8494, 22962, 11, 293, 300, 5112, 291, 300, 264, 13525, 486, 312, 3671, 293, 550, 486, 312, 257, 3671, 1230, 570, 321, 1437, 760, 13, 50914, 50914, 400, 550, 264, 1900, 1230, 295, 13525, 486, 312, 11, 1900, 1230, 295, 13525, 307, 3671, 805, 13, 51214, 51214, 400, 291, 393, 611, 13447, 1803, 300, 3671, 805, 307, 264, 558, 1867, 44003, 293, 10783, 984, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10191532893058582, "compression_ratio": 1.8465909090909092, "no_speech_prob": 7.25394711480476e-05}, {"id": 181, "seek": 103700, "start": 1054.0, "end": 1059.0, "text": " And you can also convince yourself that negative 3 is the right answer mathematically and analytically,", "tokens": [50364, 400, 370, 321, 1437, 490, 1017, 281, 805, 13, 8494, 22962, 11, 293, 300, 5112, 291, 300, 264, 13525, 486, 312, 3671, 293, 550, 486, 312, 257, 3671, 1230, 570, 321, 1437, 760, 13, 50914, 50914, 400, 550, 264, 1900, 1230, 295, 13525, 486, 312, 11, 1900, 1230, 295, 13525, 307, 3671, 805, 13, 51214, 51214, 400, 291, 393, 611, 13447, 1803, 300, 3671, 805, 307, 264, 558, 1867, 44003, 293, 10783, 984, 11, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10191532893058582, "compression_ratio": 1.8465909090909092, "no_speech_prob": 7.25394711480476e-05}, {"id": 182, "seek": 105900, "start": 1059.0, "end": 1068.0, "text": " because if you have a times b plus c and you are, you know, you have calculus, then differentiating a times b plus c with respect to a gives you just b.", "tokens": [50364, 570, 498, 291, 362, 257, 1413, 272, 1804, 269, 293, 291, 366, 11, 291, 458, 11, 291, 362, 33400, 11, 550, 27372, 990, 257, 1413, 272, 1804, 269, 365, 3104, 281, 257, 2709, 291, 445, 272, 13, 50814, 50814, 400, 6451, 11, 264, 2158, 295, 272, 307, 3671, 805, 11, 597, 307, 264, 13760, 300, 321, 362, 13, 51014, 51014, 407, 291, 393, 980, 300, 300, 311, 3006, 13, 51164, 51164, 407, 586, 498, 321, 360, 341, 365, 272, 11, 370, 498, 321, 9961, 272, 538, 257, 707, 857, 294, 257, 3353, 3513, 11, 321, 1116, 483, 819, 37725, 13, 51514, 51514, 407, 437, 307, 264, 6503, 295, 272, 322, 264, 5598, 274, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06926670721021749, "compression_ratio": 1.7091633466135459, "no_speech_prob": 1.2218915799167007e-05}, {"id": 183, "seek": 105900, "start": 1068.0, "end": 1072.0, "text": " And indeed, the value of b is negative 3, which is the derivative that we have.", "tokens": [50364, 570, 498, 291, 362, 257, 1413, 272, 1804, 269, 293, 291, 366, 11, 291, 458, 11, 291, 362, 33400, 11, 550, 27372, 990, 257, 1413, 272, 1804, 269, 365, 3104, 281, 257, 2709, 291, 445, 272, 13, 50814, 50814, 400, 6451, 11, 264, 2158, 295, 272, 307, 3671, 805, 11, 597, 307, 264, 13760, 300, 321, 362, 13, 51014, 51014, 407, 291, 393, 980, 300, 300, 311, 3006, 13, 51164, 51164, 407, 586, 498, 321, 360, 341, 365, 272, 11, 370, 498, 321, 9961, 272, 538, 257, 707, 857, 294, 257, 3353, 3513, 11, 321, 1116, 483, 819, 37725, 13, 51514, 51514, 407, 437, 307, 264, 6503, 295, 272, 322, 264, 5598, 274, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06926670721021749, "compression_ratio": 1.7091633466135459, "no_speech_prob": 1.2218915799167007e-05}, {"id": 184, "seek": 105900, "start": 1072.0, "end": 1075.0, "text": " So you can tell that that's correct.", "tokens": [50364, 570, 498, 291, 362, 257, 1413, 272, 1804, 269, 293, 291, 366, 11, 291, 458, 11, 291, 362, 33400, 11, 550, 27372, 990, 257, 1413, 272, 1804, 269, 365, 3104, 281, 257, 2709, 291, 445, 272, 13, 50814, 50814, 400, 6451, 11, 264, 2158, 295, 272, 307, 3671, 805, 11, 597, 307, 264, 13760, 300, 321, 362, 13, 51014, 51014, 407, 291, 393, 980, 300, 300, 311, 3006, 13, 51164, 51164, 407, 586, 498, 321, 360, 341, 365, 272, 11, 370, 498, 321, 9961, 272, 538, 257, 707, 857, 294, 257, 3353, 3513, 11, 321, 1116, 483, 819, 37725, 13, 51514, 51514, 407, 437, 307, 264, 6503, 295, 272, 322, 264, 5598, 274, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06926670721021749, "compression_ratio": 1.7091633466135459, "no_speech_prob": 1.2218915799167007e-05}, {"id": 185, "seek": 105900, "start": 1075.0, "end": 1082.0, "text": " So now if we do this with b, so if we bump b by a little bit in a positive direction, we'd get different slopes.", "tokens": [50364, 570, 498, 291, 362, 257, 1413, 272, 1804, 269, 293, 291, 366, 11, 291, 458, 11, 291, 362, 33400, 11, 550, 27372, 990, 257, 1413, 272, 1804, 269, 365, 3104, 281, 257, 2709, 291, 445, 272, 13, 50814, 50814, 400, 6451, 11, 264, 2158, 295, 272, 307, 3671, 805, 11, 597, 307, 264, 13760, 300, 321, 362, 13, 51014, 51014, 407, 291, 393, 980, 300, 300, 311, 3006, 13, 51164, 51164, 407, 586, 498, 321, 360, 341, 365, 272, 11, 370, 498, 321, 9961, 272, 538, 257, 707, 857, 294, 257, 3353, 3513, 11, 321, 1116, 483, 819, 37725, 13, 51514, 51514, 407, 437, 307, 264, 6503, 295, 272, 322, 264, 5598, 274, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06926670721021749, "compression_ratio": 1.7091633466135459, "no_speech_prob": 1.2218915799167007e-05}, {"id": 186, "seek": 105900, "start": 1082.0, "end": 1086.0, "text": " So what is the influence of b on the output d?", "tokens": [50364, 570, 498, 291, 362, 257, 1413, 272, 1804, 269, 293, 291, 366, 11, 291, 458, 11, 291, 362, 33400, 11, 550, 27372, 990, 257, 1413, 272, 1804, 269, 365, 3104, 281, 257, 2709, 291, 445, 272, 13, 50814, 50814, 400, 6451, 11, 264, 2158, 295, 272, 307, 3671, 805, 11, 597, 307, 264, 13760, 300, 321, 362, 13, 51014, 51014, 407, 291, 393, 980, 300, 300, 311, 3006, 13, 51164, 51164, 407, 586, 498, 321, 360, 341, 365, 272, 11, 370, 498, 321, 9961, 272, 538, 257, 707, 857, 294, 257, 3353, 3513, 11, 321, 1116, 483, 819, 37725, 13, 51514, 51514, 407, 437, 307, 264, 6503, 295, 272, 322, 264, 5598, 274, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06926670721021749, "compression_ratio": 1.7091633466135459, "no_speech_prob": 1.2218915799167007e-05}, {"id": 187, "seek": 108600, "start": 1086.0, "end": 1094.0, "text": " So if we bump b by a tiny amount in a positive direction, then because a is positive, we'll be adding more to d.", "tokens": [50364, 407, 498, 321, 9961, 272, 538, 257, 5870, 2372, 294, 257, 3353, 3513, 11, 550, 570, 257, 307, 3353, 11, 321, 603, 312, 5127, 544, 281, 274, 13, 50764, 50764, 1779, 13, 407, 293, 586, 437, 307, 264, 437, 307, 264, 19392, 30, 50914, 50914, 708, 307, 264, 13525, 295, 300, 4500, 30, 51064, 51064, 400, 309, 1062, 406, 6365, 291, 300, 341, 820, 312, 568, 13, 51264, 51264, 400, 983, 307, 309, 568, 30, 1436, 274, 295, 274, 538, 274, 65, 11, 264, 8509, 365, 3104, 281, 272, 11, 576, 312, 11, 576, 976, 505, 257, 293, 264, 2158, 295, 257, 307, 568, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13584192083516253, "compression_ratio": 1.6375545851528384, "no_speech_prob": 1.0129843758477364e-05}, {"id": 188, "seek": 108600, "start": 1094.0, "end": 1097.0, "text": " Right. So and now what is the what is the sensitivity?", "tokens": [50364, 407, 498, 321, 9961, 272, 538, 257, 5870, 2372, 294, 257, 3353, 3513, 11, 550, 570, 257, 307, 3353, 11, 321, 603, 312, 5127, 544, 281, 274, 13, 50764, 50764, 1779, 13, 407, 293, 586, 437, 307, 264, 437, 307, 264, 19392, 30, 50914, 50914, 708, 307, 264, 13525, 295, 300, 4500, 30, 51064, 51064, 400, 309, 1062, 406, 6365, 291, 300, 341, 820, 312, 568, 13, 51264, 51264, 400, 983, 307, 309, 568, 30, 1436, 274, 295, 274, 538, 274, 65, 11, 264, 8509, 365, 3104, 281, 272, 11, 576, 312, 11, 576, 976, 505, 257, 293, 264, 2158, 295, 257, 307, 568, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13584192083516253, "compression_ratio": 1.6375545851528384, "no_speech_prob": 1.0129843758477364e-05}, {"id": 189, "seek": 108600, "start": 1097.0, "end": 1100.0, "text": " What is the slope of that addition?", "tokens": [50364, 407, 498, 321, 9961, 272, 538, 257, 5870, 2372, 294, 257, 3353, 3513, 11, 550, 570, 257, 307, 3353, 11, 321, 603, 312, 5127, 544, 281, 274, 13, 50764, 50764, 1779, 13, 407, 293, 586, 437, 307, 264, 437, 307, 264, 19392, 30, 50914, 50914, 708, 307, 264, 13525, 295, 300, 4500, 30, 51064, 51064, 400, 309, 1062, 406, 6365, 291, 300, 341, 820, 312, 568, 13, 51264, 51264, 400, 983, 307, 309, 568, 30, 1436, 274, 295, 274, 538, 274, 65, 11, 264, 8509, 365, 3104, 281, 272, 11, 576, 312, 11, 576, 976, 505, 257, 293, 264, 2158, 295, 257, 307, 568, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13584192083516253, "compression_ratio": 1.6375545851528384, "no_speech_prob": 1.0129843758477364e-05}, {"id": 190, "seek": 108600, "start": 1100.0, "end": 1104.0, "text": " And it might not surprise you that this should be 2.", "tokens": [50364, 407, 498, 321, 9961, 272, 538, 257, 5870, 2372, 294, 257, 3353, 3513, 11, 550, 570, 257, 307, 3353, 11, 321, 603, 312, 5127, 544, 281, 274, 13, 50764, 50764, 1779, 13, 407, 293, 586, 437, 307, 264, 437, 307, 264, 19392, 30, 50914, 50914, 708, 307, 264, 13525, 295, 300, 4500, 30, 51064, 51064, 400, 309, 1062, 406, 6365, 291, 300, 341, 820, 312, 568, 13, 51264, 51264, 400, 983, 307, 309, 568, 30, 1436, 274, 295, 274, 538, 274, 65, 11, 264, 8509, 365, 3104, 281, 272, 11, 576, 312, 11, 576, 976, 505, 257, 293, 264, 2158, 295, 257, 307, 568, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13584192083516253, "compression_ratio": 1.6375545851528384, "no_speech_prob": 1.0129843758477364e-05}, {"id": 191, "seek": 108600, "start": 1104.0, "end": 1113.0, "text": " And why is it 2? Because d of d by db, the ratio with respect to b, would be, would give us a and the value of a is 2.", "tokens": [50364, 407, 498, 321, 9961, 272, 538, 257, 5870, 2372, 294, 257, 3353, 3513, 11, 550, 570, 257, 307, 3353, 11, 321, 603, 312, 5127, 544, 281, 274, 13, 50764, 50764, 1779, 13, 407, 293, 586, 437, 307, 264, 437, 307, 264, 19392, 30, 50914, 50914, 708, 307, 264, 13525, 295, 300, 4500, 30, 51064, 51064, 400, 309, 1062, 406, 6365, 291, 300, 341, 820, 312, 568, 13, 51264, 51264, 400, 983, 307, 309, 568, 30, 1436, 274, 295, 274, 538, 274, 65, 11, 264, 8509, 365, 3104, 281, 272, 11, 576, 312, 11, 576, 976, 505, 257, 293, 264, 2158, 295, 257, 307, 568, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13584192083516253, "compression_ratio": 1.6375545851528384, "no_speech_prob": 1.0129843758477364e-05}, {"id": 192, "seek": 111300, "start": 1113.0, "end": 1122.0, "text": " So that's also working well. And then if c gets bumped a tiny amount in h by h, then, of course, a times b is unaffected.", "tokens": [50364, 407, 300, 311, 611, 1364, 731, 13, 400, 550, 498, 269, 2170, 42696, 257, 5870, 2372, 294, 276, 538, 276, 11, 550, 11, 295, 1164, 11, 257, 1413, 272, 307, 2002, 11259, 292, 13, 50814, 50814, 400, 586, 269, 3643, 4748, 857, 2946, 13, 708, 775, 300, 360, 281, 264, 2445, 30, 51014, 51014, 467, 1669, 309, 4748, 857, 2946, 570, 321, 434, 2935, 5127, 269, 293, 309, 1669, 309, 4748, 857, 2946, 538, 264, 1900, 912, 2372, 300, 321, 3869, 281, 269, 13, 51364, 51364, 400, 370, 300, 5112, 291, 300, 264, 13525, 307, 502, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08495043764019956, "compression_ratio": 1.7162162162162162, "no_speech_prob": 1.1659401025099214e-05}, {"id": 193, "seek": 111300, "start": 1122.0, "end": 1126.0, "text": " And now c becomes slightly bit higher. What does that do to the function?", "tokens": [50364, 407, 300, 311, 611, 1364, 731, 13, 400, 550, 498, 269, 2170, 42696, 257, 5870, 2372, 294, 276, 538, 276, 11, 550, 11, 295, 1164, 11, 257, 1413, 272, 307, 2002, 11259, 292, 13, 50814, 50814, 400, 586, 269, 3643, 4748, 857, 2946, 13, 708, 775, 300, 360, 281, 264, 2445, 30, 51014, 51014, 467, 1669, 309, 4748, 857, 2946, 570, 321, 434, 2935, 5127, 269, 293, 309, 1669, 309, 4748, 857, 2946, 538, 264, 1900, 912, 2372, 300, 321, 3869, 281, 269, 13, 51364, 51364, 400, 370, 300, 5112, 291, 300, 264, 13525, 307, 502, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08495043764019956, "compression_ratio": 1.7162162162162162, "no_speech_prob": 1.1659401025099214e-05}, {"id": 194, "seek": 111300, "start": 1126.0, "end": 1133.0, "text": " It makes it slightly bit higher because we're simply adding c and it makes it slightly bit higher by the exact same amount that we added to c.", "tokens": [50364, 407, 300, 311, 611, 1364, 731, 13, 400, 550, 498, 269, 2170, 42696, 257, 5870, 2372, 294, 276, 538, 276, 11, 550, 11, 295, 1164, 11, 257, 1413, 272, 307, 2002, 11259, 292, 13, 50814, 50814, 400, 586, 269, 3643, 4748, 857, 2946, 13, 708, 775, 300, 360, 281, 264, 2445, 30, 51014, 51014, 467, 1669, 309, 4748, 857, 2946, 570, 321, 434, 2935, 5127, 269, 293, 309, 1669, 309, 4748, 857, 2946, 538, 264, 1900, 912, 2372, 300, 321, 3869, 281, 269, 13, 51364, 51364, 400, 370, 300, 5112, 291, 300, 264, 13525, 307, 502, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08495043764019956, "compression_ratio": 1.7162162162162162, "no_speech_prob": 1.1659401025099214e-05}, {"id": 195, "seek": 111300, "start": 1133.0, "end": 1136.0, "text": " And so that tells you that the slope is 1.", "tokens": [50364, 407, 300, 311, 611, 1364, 731, 13, 400, 550, 498, 269, 2170, 42696, 257, 5870, 2372, 294, 276, 538, 276, 11, 550, 11, 295, 1164, 11, 257, 1413, 272, 307, 2002, 11259, 292, 13, 50814, 50814, 400, 586, 269, 3643, 4748, 857, 2946, 13, 708, 775, 300, 360, 281, 264, 2445, 30, 51014, 51014, 467, 1669, 309, 4748, 857, 2946, 570, 321, 434, 2935, 5127, 269, 293, 309, 1669, 309, 4748, 857, 2946, 538, 264, 1900, 912, 2372, 300, 321, 3869, 281, 269, 13, 51364, 51364, 400, 370, 300, 5112, 291, 300, 264, 13525, 307, 502, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08495043764019956, "compression_ratio": 1.7162162162162162, "no_speech_prob": 1.1659401025099214e-05}, {"id": 196, "seek": 113600, "start": 1136.0, "end": 1145.0, "text": " That will be the rate at which d will increase as we scale c.", "tokens": [50364, 663, 486, 312, 264, 3314, 412, 597, 274, 486, 3488, 382, 321, 4373, 269, 13, 50814, 50814, 2264, 11, 370, 321, 586, 362, 512, 21769, 2020, 295, 437, 341, 13760, 307, 3585, 291, 466, 264, 2445, 13, 51014, 51014, 400, 321, 1116, 411, 281, 1286, 281, 18161, 9590, 13, 51114, 51114, 823, 11, 382, 286, 2835, 11, 18161, 9590, 486, 312, 1238, 5994, 15277, 11, 18894, 15277, 13, 51314, 51314, 407, 321, 643, 512, 1412, 9227, 300, 6909, 613, 15277, 13, 51464, 51464, 400, 300, 311, 437, 321, 434, 516, 281, 722, 281, 1322, 484, 586, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04667251889068302, "compression_ratio": 1.684, "no_speech_prob": 8.01327496446902e-06}, {"id": 197, "seek": 113600, "start": 1145.0, "end": 1149.0, "text": " OK, so we now have some intuitive sense of what this derivative is telling you about the function.", "tokens": [50364, 663, 486, 312, 264, 3314, 412, 597, 274, 486, 3488, 382, 321, 4373, 269, 13, 50814, 50814, 2264, 11, 370, 321, 586, 362, 512, 21769, 2020, 295, 437, 341, 13760, 307, 3585, 291, 466, 264, 2445, 13, 51014, 51014, 400, 321, 1116, 411, 281, 1286, 281, 18161, 9590, 13, 51114, 51114, 823, 11, 382, 286, 2835, 11, 18161, 9590, 486, 312, 1238, 5994, 15277, 11, 18894, 15277, 13, 51314, 51314, 407, 321, 643, 512, 1412, 9227, 300, 6909, 613, 15277, 13, 51464, 51464, 400, 300, 311, 437, 321, 434, 516, 281, 722, 281, 1322, 484, 586, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04667251889068302, "compression_ratio": 1.684, "no_speech_prob": 8.01327496446902e-06}, {"id": 198, "seek": 113600, "start": 1149.0, "end": 1151.0, "text": " And we'd like to move to neural networks.", "tokens": [50364, 663, 486, 312, 264, 3314, 412, 597, 274, 486, 3488, 382, 321, 4373, 269, 13, 50814, 50814, 2264, 11, 370, 321, 586, 362, 512, 21769, 2020, 295, 437, 341, 13760, 307, 3585, 291, 466, 264, 2445, 13, 51014, 51014, 400, 321, 1116, 411, 281, 1286, 281, 18161, 9590, 13, 51114, 51114, 823, 11, 382, 286, 2835, 11, 18161, 9590, 486, 312, 1238, 5994, 15277, 11, 18894, 15277, 13, 51314, 51314, 407, 321, 643, 512, 1412, 9227, 300, 6909, 613, 15277, 13, 51464, 51464, 400, 300, 311, 437, 321, 434, 516, 281, 722, 281, 1322, 484, 586, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04667251889068302, "compression_ratio": 1.684, "no_speech_prob": 8.01327496446902e-06}, {"id": 199, "seek": 113600, "start": 1151.0, "end": 1155.0, "text": " Now, as I mentioned, neural networks will be pretty massive expressions, mathematical expressions.", "tokens": [50364, 663, 486, 312, 264, 3314, 412, 597, 274, 486, 3488, 382, 321, 4373, 269, 13, 50814, 50814, 2264, 11, 370, 321, 586, 362, 512, 21769, 2020, 295, 437, 341, 13760, 307, 3585, 291, 466, 264, 2445, 13, 51014, 51014, 400, 321, 1116, 411, 281, 1286, 281, 18161, 9590, 13, 51114, 51114, 823, 11, 382, 286, 2835, 11, 18161, 9590, 486, 312, 1238, 5994, 15277, 11, 18894, 15277, 13, 51314, 51314, 407, 321, 643, 512, 1412, 9227, 300, 6909, 613, 15277, 13, 51464, 51464, 400, 300, 311, 437, 321, 434, 516, 281, 722, 281, 1322, 484, 586, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04667251889068302, "compression_ratio": 1.684, "no_speech_prob": 8.01327496446902e-06}, {"id": 200, "seek": 113600, "start": 1155.0, "end": 1158.0, "text": " So we need some data structures that maintain these expressions.", "tokens": [50364, 663, 486, 312, 264, 3314, 412, 597, 274, 486, 3488, 382, 321, 4373, 269, 13, 50814, 50814, 2264, 11, 370, 321, 586, 362, 512, 21769, 2020, 295, 437, 341, 13760, 307, 3585, 291, 466, 264, 2445, 13, 51014, 51014, 400, 321, 1116, 411, 281, 1286, 281, 18161, 9590, 13, 51114, 51114, 823, 11, 382, 286, 2835, 11, 18161, 9590, 486, 312, 1238, 5994, 15277, 11, 18894, 15277, 13, 51314, 51314, 407, 321, 643, 512, 1412, 9227, 300, 6909, 613, 15277, 13, 51464, 51464, 400, 300, 311, 437, 321, 434, 516, 281, 722, 281, 1322, 484, 586, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04667251889068302, "compression_ratio": 1.684, "no_speech_prob": 8.01327496446902e-06}, {"id": 201, "seek": 113600, "start": 1158.0, "end": 1160.0, "text": " And that's what we're going to start to build out now.", "tokens": [50364, 663, 486, 312, 264, 3314, 412, 597, 274, 486, 3488, 382, 321, 4373, 269, 13, 50814, 50814, 2264, 11, 370, 321, 586, 362, 512, 21769, 2020, 295, 437, 341, 13760, 307, 3585, 291, 466, 264, 2445, 13, 51014, 51014, 400, 321, 1116, 411, 281, 1286, 281, 18161, 9590, 13, 51114, 51114, 823, 11, 382, 286, 2835, 11, 18161, 9590, 486, 312, 1238, 5994, 15277, 11, 18894, 15277, 13, 51314, 51314, 407, 321, 643, 512, 1412, 9227, 300, 6909, 613, 15277, 13, 51464, 51464, 400, 300, 311, 437, 321, 434, 516, 281, 722, 281, 1322, 484, 586, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04667251889068302, "compression_ratio": 1.684, "no_speech_prob": 8.01327496446902e-06}, {"id": 202, "seek": 116000, "start": 1160.0, "end": 1167.0, "text": " So we're going to build out this value object that I showed you in the readme page of micro grad.", "tokens": [50364, 407, 321, 434, 516, 281, 1322, 484, 341, 2158, 2657, 300, 286, 4712, 291, 294, 264, 1401, 1398, 3028, 295, 4532, 2771, 13, 50714, 50714, 407, 718, 385, 5055, 9163, 257, 25204, 295, 264, 700, 588, 2199, 2158, 2657, 13, 51014, 51014, 407, 1508, 2158, 2516, 257, 2167, 39684, 2158, 300, 309, 25831, 293, 5965, 2837, 295, 13, 51314, 51314, 400, 300, 311, 309, 13, 407, 321, 393, 11, 337, 1365, 11, 360, 2158, 295, 568, 13, 15, 293, 550, 321, 393, 483, 321, 393, 574, 412, 1080, 2701, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08932295251399913, "compression_ratio": 1.6111111111111112, "no_speech_prob": 4.356747467681998e-06}, {"id": 203, "seek": 116000, "start": 1167.0, "end": 1173.0, "text": " So let me copy paste a skeleton of the first very simple value object.", "tokens": [50364, 407, 321, 434, 516, 281, 1322, 484, 341, 2158, 2657, 300, 286, 4712, 291, 294, 264, 1401, 1398, 3028, 295, 4532, 2771, 13, 50714, 50714, 407, 718, 385, 5055, 9163, 257, 25204, 295, 264, 700, 588, 2199, 2158, 2657, 13, 51014, 51014, 407, 1508, 2158, 2516, 257, 2167, 39684, 2158, 300, 309, 25831, 293, 5965, 2837, 295, 13, 51314, 51314, 400, 300, 311, 309, 13, 407, 321, 393, 11, 337, 1365, 11, 360, 2158, 295, 568, 13, 15, 293, 550, 321, 393, 483, 321, 393, 574, 412, 1080, 2701, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08932295251399913, "compression_ratio": 1.6111111111111112, "no_speech_prob": 4.356747467681998e-06}, {"id": 204, "seek": 116000, "start": 1173.0, "end": 1179.0, "text": " So class value takes a single scalar value that it wraps and keeps track of.", "tokens": [50364, 407, 321, 434, 516, 281, 1322, 484, 341, 2158, 2657, 300, 286, 4712, 291, 294, 264, 1401, 1398, 3028, 295, 4532, 2771, 13, 50714, 50714, 407, 718, 385, 5055, 9163, 257, 25204, 295, 264, 700, 588, 2199, 2158, 2657, 13, 51014, 51014, 407, 1508, 2158, 2516, 257, 2167, 39684, 2158, 300, 309, 25831, 293, 5965, 2837, 295, 13, 51314, 51314, 400, 300, 311, 309, 13, 407, 321, 393, 11, 337, 1365, 11, 360, 2158, 295, 568, 13, 15, 293, 550, 321, 393, 483, 321, 393, 574, 412, 1080, 2701, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08932295251399913, "compression_ratio": 1.6111111111111112, "no_speech_prob": 4.356747467681998e-06}, {"id": 205, "seek": 116000, "start": 1179.0, "end": 1187.0, "text": " And that's it. So we can, for example, do value of 2.0 and then we can get we can look at its content.", "tokens": [50364, 407, 321, 434, 516, 281, 1322, 484, 341, 2158, 2657, 300, 286, 4712, 291, 294, 264, 1401, 1398, 3028, 295, 4532, 2771, 13, 50714, 50714, 407, 718, 385, 5055, 9163, 257, 25204, 295, 264, 700, 588, 2199, 2158, 2657, 13, 51014, 51014, 407, 1508, 2158, 2516, 257, 2167, 39684, 2158, 300, 309, 25831, 293, 5965, 2837, 295, 13, 51314, 51314, 400, 300, 311, 309, 13, 407, 321, 393, 11, 337, 1365, 11, 360, 2158, 295, 568, 13, 15, 293, 550, 321, 393, 483, 321, 393, 574, 412, 1080, 2701, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08932295251399913, "compression_ratio": 1.6111111111111112, "no_speech_prob": 4.356747467681998e-06}, {"id": 206, "seek": 118700, "start": 1187.0, "end": 1198.0, "text": " And Python will internally use the repr function to return this string.", "tokens": [50364, 400, 15329, 486, 19501, 764, 264, 1085, 81, 2445, 281, 2736, 341, 6798, 13, 50914, 50914, 407, 341, 307, 257, 2158, 2657, 365, 1412, 6915, 568, 300, 321, 434, 4084, 510, 13, 51164, 51164, 823, 321, 1116, 411, 281, 360, 307, 411, 321, 1116, 411, 281, 312, 1075, 281, 362, 406, 445, 411, 732, 4190, 11, 457, 321, 1116, 411, 281, 360, 257, 44191, 88, 13, 51564, 51564, 1779, 13, 492, 1116, 411, 281, 909, 552, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16181581991690178, "compression_ratio": 1.565934065934066, "no_speech_prob": 4.28921839557006e-06}, {"id": 207, "seek": 118700, "start": 1198.0, "end": 1203.0, "text": " So this is a value object with data equals 2 that we're creating here.", "tokens": [50364, 400, 15329, 486, 19501, 764, 264, 1085, 81, 2445, 281, 2736, 341, 6798, 13, 50914, 50914, 407, 341, 307, 257, 2158, 2657, 365, 1412, 6915, 568, 300, 321, 434, 4084, 510, 13, 51164, 51164, 823, 321, 1116, 411, 281, 360, 307, 411, 321, 1116, 411, 281, 312, 1075, 281, 362, 406, 445, 411, 732, 4190, 11, 457, 321, 1116, 411, 281, 360, 257, 44191, 88, 13, 51564, 51564, 1779, 13, 492, 1116, 411, 281, 909, 552, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16181581991690178, "compression_ratio": 1.565934065934066, "no_speech_prob": 4.28921839557006e-06}, {"id": 208, "seek": 118700, "start": 1203.0, "end": 1211.0, "text": " Now we'd like to do is like we'd like to be able to have not just like two values, but we'd like to do a bluffy.", "tokens": [50364, 400, 15329, 486, 19501, 764, 264, 1085, 81, 2445, 281, 2736, 341, 6798, 13, 50914, 50914, 407, 341, 307, 257, 2158, 2657, 365, 1412, 6915, 568, 300, 321, 434, 4084, 510, 13, 51164, 51164, 823, 321, 1116, 411, 281, 360, 307, 411, 321, 1116, 411, 281, 312, 1075, 281, 362, 406, 445, 411, 732, 4190, 11, 457, 321, 1116, 411, 281, 360, 257, 44191, 88, 13, 51564, 51564, 1779, 13, 492, 1116, 411, 281, 909, 552, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16181581991690178, "compression_ratio": 1.565934065934066, "no_speech_prob": 4.28921839557006e-06}, {"id": 209, "seek": 118700, "start": 1211.0, "end": 1213.0, "text": " Right. We'd like to add them.", "tokens": [50364, 400, 15329, 486, 19501, 764, 264, 1085, 81, 2445, 281, 2736, 341, 6798, 13, 50914, 50914, 407, 341, 307, 257, 2158, 2657, 365, 1412, 6915, 568, 300, 321, 434, 4084, 510, 13, 51164, 51164, 823, 321, 1116, 411, 281, 360, 307, 411, 321, 1116, 411, 281, 312, 1075, 281, 362, 406, 445, 411, 732, 4190, 11, 457, 321, 1116, 411, 281, 360, 257, 44191, 88, 13, 51564, 51564, 1779, 13, 492, 1116, 411, 281, 909, 552, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16181581991690178, "compression_ratio": 1.565934065934066, "no_speech_prob": 4.28921839557006e-06}, {"id": 210, "seek": 121300, "start": 1213.0, "end": 1219.0, "text": " So currently you would get an error because Python doesn't know how to add two value objects.", "tokens": [50364, 407, 4362, 291, 576, 483, 364, 6713, 570, 15329, 1177, 380, 458, 577, 281, 909, 732, 2158, 6565, 13, 50664, 50664, 407, 321, 362, 281, 980, 309, 13, 407, 510, 311, 4500, 13, 51014, 51014, 407, 291, 362, 281, 1936, 764, 613, 2121, 3834, 37556, 7150, 294, 15329, 281, 6964, 613, 19077, 337, 613, 6565, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.07211098670959473, "compression_ratio": 1.5476190476190477, "no_speech_prob": 1.1125387572974432e-05}, {"id": 211, "seek": 121300, "start": 1219.0, "end": 1226.0, "text": " So we have to tell it. So here's addition.", "tokens": [50364, 407, 4362, 291, 576, 483, 364, 6713, 570, 15329, 1177, 380, 458, 577, 281, 909, 732, 2158, 6565, 13, 50664, 50664, 407, 321, 362, 281, 980, 309, 13, 407, 510, 311, 4500, 13, 51014, 51014, 407, 291, 362, 281, 1936, 764, 613, 2121, 3834, 37556, 7150, 294, 15329, 281, 6964, 613, 19077, 337, 613, 6565, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.07211098670959473, "compression_ratio": 1.5476190476190477, "no_speech_prob": 1.1125387572974432e-05}, {"id": 212, "seek": 121300, "start": 1226.0, "end": 1233.0, "text": " So you have to basically use these special double underscore methods in Python to define these operators for these objects.", "tokens": [50364, 407, 4362, 291, 576, 483, 364, 6713, 570, 15329, 1177, 380, 458, 577, 281, 909, 732, 2158, 6565, 13, 50664, 50664, 407, 321, 362, 281, 980, 309, 13, 407, 510, 311, 4500, 13, 51014, 51014, 407, 291, 362, 281, 1936, 764, 613, 2121, 3834, 37556, 7150, 294, 15329, 281, 6964, 613, 19077, 337, 613, 6565, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.07211098670959473, "compression_ratio": 1.5476190476190477, "no_speech_prob": 1.1125387572974432e-05}, {"id": 213, "seek": 123300, "start": 1233.0, "end": 1244.0, "text": " So if we call the if we use this plus operator, Python will internally call a dot add of B.", "tokens": [50364, 407, 498, 321, 818, 264, 498, 321, 764, 341, 1804, 12973, 11, 15329, 486, 19501, 818, 257, 5893, 909, 295, 363, 13, 50914, 50914, 663, 311, 437, 486, 1051, 19501, 13, 400, 370, 363, 486, 312, 264, 661, 293, 2698, 486, 312, 257, 13, 51264, 51264, 400, 370, 321, 536, 300, 321, 434, 516, 281, 2736, 382, 257, 777, 2158, 2657, 13, 51414, 51414, 400, 309, 311, 445, 309, 311, 516, 281, 312, 21993, 264, 1804, 295, 641, 1412, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08156615779513404, "compression_ratio": 1.6593406593406594, "no_speech_prob": 2.1567400381172774e-06}, {"id": 214, "seek": 123300, "start": 1244.0, "end": 1251.0, "text": " That's what will happen internally. And so B will be the other and self will be a.", "tokens": [50364, 407, 498, 321, 818, 264, 498, 321, 764, 341, 1804, 12973, 11, 15329, 486, 19501, 818, 257, 5893, 909, 295, 363, 13, 50914, 50914, 663, 311, 437, 486, 1051, 19501, 13, 400, 370, 363, 486, 312, 264, 661, 293, 2698, 486, 312, 257, 13, 51264, 51264, 400, 370, 321, 536, 300, 321, 434, 516, 281, 2736, 382, 257, 777, 2158, 2657, 13, 51414, 51414, 400, 309, 311, 445, 309, 311, 516, 281, 312, 21993, 264, 1804, 295, 641, 1412, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08156615779513404, "compression_ratio": 1.6593406593406594, "no_speech_prob": 2.1567400381172774e-06}, {"id": 215, "seek": 123300, "start": 1251.0, "end": 1254.0, "text": " And so we see that we're going to return as a new value object.", "tokens": [50364, 407, 498, 321, 818, 264, 498, 321, 764, 341, 1804, 12973, 11, 15329, 486, 19501, 818, 257, 5893, 909, 295, 363, 13, 50914, 50914, 663, 311, 437, 486, 1051, 19501, 13, 400, 370, 363, 486, 312, 264, 661, 293, 2698, 486, 312, 257, 13, 51264, 51264, 400, 370, 321, 536, 300, 321, 434, 516, 281, 2736, 382, 257, 777, 2158, 2657, 13, 51414, 51414, 400, 309, 311, 445, 309, 311, 516, 281, 312, 21993, 264, 1804, 295, 641, 1412, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08156615779513404, "compression_ratio": 1.6593406593406594, "no_speech_prob": 2.1567400381172774e-06}, {"id": 216, "seek": 123300, "start": 1254.0, "end": 1260.0, "text": " And it's just it's going to be wrapping the plus of their data.", "tokens": [50364, 407, 498, 321, 818, 264, 498, 321, 764, 341, 1804, 12973, 11, 15329, 486, 19501, 818, 257, 5893, 909, 295, 363, 13, 50914, 50914, 663, 311, 437, 486, 1051, 19501, 13, 400, 370, 363, 486, 312, 264, 661, 293, 2698, 486, 312, 257, 13, 51264, 51264, 400, 370, 321, 536, 300, 321, 434, 516, 281, 2736, 382, 257, 777, 2158, 2657, 13, 51414, 51414, 400, 309, 311, 445, 309, 311, 516, 281, 312, 21993, 264, 1804, 295, 641, 1412, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08156615779513404, "compression_ratio": 1.6593406593406594, "no_speech_prob": 2.1567400381172774e-06}, {"id": 217, "seek": 126000, "start": 1260.0, "end": 1264.0, "text": " But remember now, because data is the actual like numbered Python number.", "tokens": [50364, 583, 1604, 586, 11, 570, 1412, 307, 264, 3539, 411, 40936, 15329, 1230, 13, 50564, 50564, 407, 341, 12973, 510, 307, 445, 264, 7476, 12607, 935, 1804, 4500, 13, 50814, 50814, 823, 309, 311, 406, 364, 4500, 295, 2158, 6565, 293, 486, 2736, 257, 777, 2158, 13, 51064, 51064, 407, 586, 257, 1804, 363, 820, 589, 293, 309, 820, 4482, 2158, 295, 3671, 472, 570, 300, 311, 732, 1804, 3175, 1045, 13, 51364, 51364, 821, 321, 352, 13, 2264, 13, 961, 311, 586, 4445, 12972, 445, 370, 321, 393, 25833, 341, 6114, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10948585977359694, "compression_ratio": 1.6274509803921569, "no_speech_prob": 1.3631180081574712e-05}, {"id": 218, "seek": 126000, "start": 1264.0, "end": 1269.0, "text": " So this operator here is just the typical floating point plus addition.", "tokens": [50364, 583, 1604, 586, 11, 570, 1412, 307, 264, 3539, 411, 40936, 15329, 1230, 13, 50564, 50564, 407, 341, 12973, 510, 307, 445, 264, 7476, 12607, 935, 1804, 4500, 13, 50814, 50814, 823, 309, 311, 406, 364, 4500, 295, 2158, 6565, 293, 486, 2736, 257, 777, 2158, 13, 51064, 51064, 407, 586, 257, 1804, 363, 820, 589, 293, 309, 820, 4482, 2158, 295, 3671, 472, 570, 300, 311, 732, 1804, 3175, 1045, 13, 51364, 51364, 821, 321, 352, 13, 2264, 13, 961, 311, 586, 4445, 12972, 445, 370, 321, 393, 25833, 341, 6114, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10948585977359694, "compression_ratio": 1.6274509803921569, "no_speech_prob": 1.3631180081574712e-05}, {"id": 219, "seek": 126000, "start": 1269.0, "end": 1274.0, "text": " Now it's not an addition of value objects and will return a new value.", "tokens": [50364, 583, 1604, 586, 11, 570, 1412, 307, 264, 3539, 411, 40936, 15329, 1230, 13, 50564, 50564, 407, 341, 12973, 510, 307, 445, 264, 7476, 12607, 935, 1804, 4500, 13, 50814, 50814, 823, 309, 311, 406, 364, 4500, 295, 2158, 6565, 293, 486, 2736, 257, 777, 2158, 13, 51064, 51064, 407, 586, 257, 1804, 363, 820, 589, 293, 309, 820, 4482, 2158, 295, 3671, 472, 570, 300, 311, 732, 1804, 3175, 1045, 13, 51364, 51364, 821, 321, 352, 13, 2264, 13, 961, 311, 586, 4445, 12972, 445, 370, 321, 393, 25833, 341, 6114, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10948585977359694, "compression_ratio": 1.6274509803921569, "no_speech_prob": 1.3631180081574712e-05}, {"id": 220, "seek": 126000, "start": 1274.0, "end": 1280.0, "text": " So now a plus B should work and it should print value of negative one because that's two plus minus three.", "tokens": [50364, 583, 1604, 586, 11, 570, 1412, 307, 264, 3539, 411, 40936, 15329, 1230, 13, 50564, 50564, 407, 341, 12973, 510, 307, 445, 264, 7476, 12607, 935, 1804, 4500, 13, 50814, 50814, 823, 309, 311, 406, 364, 4500, 295, 2158, 6565, 293, 486, 2736, 257, 777, 2158, 13, 51064, 51064, 407, 586, 257, 1804, 363, 820, 589, 293, 309, 820, 4482, 2158, 295, 3671, 472, 570, 300, 311, 732, 1804, 3175, 1045, 13, 51364, 51364, 821, 321, 352, 13, 2264, 13, 961, 311, 586, 4445, 12972, 445, 370, 321, 393, 25833, 341, 6114, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10948585977359694, "compression_ratio": 1.6274509803921569, "no_speech_prob": 1.3631180081574712e-05}, {"id": 221, "seek": 126000, "start": 1280.0, "end": 1287.0, "text": " There we go. OK. Let's now implement multiply just so we can recreate this expression here.", "tokens": [50364, 583, 1604, 586, 11, 570, 1412, 307, 264, 3539, 411, 40936, 15329, 1230, 13, 50564, 50564, 407, 341, 12973, 510, 307, 445, 264, 7476, 12607, 935, 1804, 4500, 13, 50814, 50814, 823, 309, 311, 406, 364, 4500, 295, 2158, 6565, 293, 486, 2736, 257, 777, 2158, 13, 51064, 51064, 407, 586, 257, 1804, 363, 820, 589, 293, 309, 820, 4482, 2158, 295, 3671, 472, 570, 300, 311, 732, 1804, 3175, 1045, 13, 51364, 51364, 821, 321, 352, 13, 2264, 13, 961, 311, 586, 4445, 12972, 445, 370, 321, 393, 25833, 341, 6114, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10948585977359694, "compression_ratio": 1.6274509803921569, "no_speech_prob": 1.3631180081574712e-05}, {"id": 222, "seek": 128700, "start": 1287.0, "end": 1292.0, "text": " So multiply. I think it won't surprise you. Will be fairly similar.", "tokens": [50364, 407, 12972, 13, 286, 519, 309, 1582, 380, 6365, 291, 13, 3099, 312, 6457, 2531, 13, 50614, 50614, 407, 2602, 295, 909, 11, 321, 434, 516, 281, 312, 1228, 6353, 13, 400, 550, 510, 11, 295, 1164, 11, 321, 528, 281, 360, 1413, 13, 50864, 50864, 400, 370, 586, 321, 393, 1884, 257, 383, 2158, 2657, 11, 597, 486, 312, 1266, 13, 15, 13, 51064, 51064, 400, 586, 321, 820, 312, 1075, 281, 360, 257, 1413, 363, 13, 1042, 11, 718, 311, 445, 360, 257, 1413, 363, 700, 13, 51464, 51464, 663, 311, 2158, 295, 3671, 2309, 586, 13, 400, 538, 264, 636, 11, 286, 30193, 670, 341, 257, 707, 857, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12369488847666774, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.318625593034085e-05}, {"id": 223, "seek": 128700, "start": 1292.0, "end": 1297.0, "text": " So instead of add, we're going to be using mole. And then here, of course, we want to do times.", "tokens": [50364, 407, 12972, 13, 286, 519, 309, 1582, 380, 6365, 291, 13, 3099, 312, 6457, 2531, 13, 50614, 50614, 407, 2602, 295, 909, 11, 321, 434, 516, 281, 312, 1228, 6353, 13, 400, 550, 510, 11, 295, 1164, 11, 321, 528, 281, 360, 1413, 13, 50864, 50864, 400, 370, 586, 321, 393, 1884, 257, 383, 2158, 2657, 11, 597, 486, 312, 1266, 13, 15, 13, 51064, 51064, 400, 586, 321, 820, 312, 1075, 281, 360, 257, 1413, 363, 13, 1042, 11, 718, 311, 445, 360, 257, 1413, 363, 700, 13, 51464, 51464, 663, 311, 2158, 295, 3671, 2309, 586, 13, 400, 538, 264, 636, 11, 286, 30193, 670, 341, 257, 707, 857, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12369488847666774, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.318625593034085e-05}, {"id": 224, "seek": 128700, "start": 1297.0, "end": 1301.0, "text": " And so now we can create a C value object, which will be 10.0.", "tokens": [50364, 407, 12972, 13, 286, 519, 309, 1582, 380, 6365, 291, 13, 3099, 312, 6457, 2531, 13, 50614, 50614, 407, 2602, 295, 909, 11, 321, 434, 516, 281, 312, 1228, 6353, 13, 400, 550, 510, 11, 295, 1164, 11, 321, 528, 281, 360, 1413, 13, 50864, 50864, 400, 370, 586, 321, 393, 1884, 257, 383, 2158, 2657, 11, 597, 486, 312, 1266, 13, 15, 13, 51064, 51064, 400, 586, 321, 820, 312, 1075, 281, 360, 257, 1413, 363, 13, 1042, 11, 718, 311, 445, 360, 257, 1413, 363, 700, 13, 51464, 51464, 663, 311, 2158, 295, 3671, 2309, 586, 13, 400, 538, 264, 636, 11, 286, 30193, 670, 341, 257, 707, 857, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12369488847666774, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.318625593034085e-05}, {"id": 225, "seek": 128700, "start": 1301.0, "end": 1309.0, "text": " And now we should be able to do a times B. Well, let's just do a times B first.", "tokens": [50364, 407, 12972, 13, 286, 519, 309, 1582, 380, 6365, 291, 13, 3099, 312, 6457, 2531, 13, 50614, 50614, 407, 2602, 295, 909, 11, 321, 434, 516, 281, 312, 1228, 6353, 13, 400, 550, 510, 11, 295, 1164, 11, 321, 528, 281, 360, 1413, 13, 50864, 50864, 400, 370, 586, 321, 393, 1884, 257, 383, 2158, 2657, 11, 597, 486, 312, 1266, 13, 15, 13, 51064, 51064, 400, 586, 321, 820, 312, 1075, 281, 360, 257, 1413, 363, 13, 1042, 11, 718, 311, 445, 360, 257, 1413, 363, 700, 13, 51464, 51464, 663, 311, 2158, 295, 3671, 2309, 586, 13, 400, 538, 264, 636, 11, 286, 30193, 670, 341, 257, 707, 857, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12369488847666774, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.318625593034085e-05}, {"id": 226, "seek": 128700, "start": 1309.0, "end": 1313.0, "text": " That's value of negative six now. And by the way, I skipped over this a little bit.", "tokens": [50364, 407, 12972, 13, 286, 519, 309, 1582, 380, 6365, 291, 13, 3099, 312, 6457, 2531, 13, 50614, 50614, 407, 2602, 295, 909, 11, 321, 434, 516, 281, 312, 1228, 6353, 13, 400, 550, 510, 11, 295, 1164, 11, 321, 528, 281, 360, 1413, 13, 50864, 50864, 400, 370, 586, 321, 393, 1884, 257, 383, 2158, 2657, 11, 597, 486, 312, 1266, 13, 15, 13, 51064, 51064, 400, 586, 321, 820, 312, 1075, 281, 360, 257, 1413, 363, 13, 1042, 11, 718, 311, 445, 360, 257, 1413, 363, 700, 13, 51464, 51464, 663, 311, 2158, 295, 3671, 2309, 586, 13, 400, 538, 264, 636, 11, 286, 30193, 670, 341, 257, 707, 857, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12369488847666774, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.318625593034085e-05}, {"id": 227, "seek": 131300, "start": 1313.0, "end": 1319.0, "text": " So suppose that I didn't have the repr function here, then it's just that you'll get some kind of an ugly expression.", "tokens": [50364, 407, 7297, 300, 286, 994, 380, 362, 264, 1085, 81, 2445, 510, 11, 550, 309, 311, 445, 300, 291, 603, 483, 512, 733, 295, 364, 12246, 6114, 13, 50664, 50664, 407, 437, 1085, 81, 307, 884, 307, 309, 311, 6530, 505, 257, 636, 281, 4482, 484, 411, 257, 22842, 1237, 6114, 294, 15329, 13, 50964, 50964, 407, 321, 500, 380, 445, 362, 746, 9844, 299, 13, 492, 767, 366, 11, 291, 458, 11, 309, 311, 2158, 295, 3671, 2309, 13, 51314, 51314, 407, 341, 2709, 505, 257, 1413, 13, 400, 550, 341, 321, 820, 586, 312, 1075, 281, 909, 383, 281, 309, 570, 321, 600, 7642, 293, 1907, 264, 15329, 577, 281, 360, 6353, 293, 909, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09209051211018207, "compression_ratio": 1.6523297491039426, "no_speech_prob": 1.5206443094939459e-05}, {"id": 228, "seek": 131300, "start": 1319.0, "end": 1325.0, "text": " So what repr is doing is it's providing us a way to print out like a nicer looking expression in Python.", "tokens": [50364, 407, 7297, 300, 286, 994, 380, 362, 264, 1085, 81, 2445, 510, 11, 550, 309, 311, 445, 300, 291, 603, 483, 512, 733, 295, 364, 12246, 6114, 13, 50664, 50664, 407, 437, 1085, 81, 307, 884, 307, 309, 311, 6530, 505, 257, 636, 281, 4482, 484, 411, 257, 22842, 1237, 6114, 294, 15329, 13, 50964, 50964, 407, 321, 500, 380, 445, 362, 746, 9844, 299, 13, 492, 767, 366, 11, 291, 458, 11, 309, 311, 2158, 295, 3671, 2309, 13, 51314, 51314, 407, 341, 2709, 505, 257, 1413, 13, 400, 550, 341, 321, 820, 586, 312, 1075, 281, 909, 383, 281, 309, 570, 321, 600, 7642, 293, 1907, 264, 15329, 577, 281, 360, 6353, 293, 909, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09209051211018207, "compression_ratio": 1.6523297491039426, "no_speech_prob": 1.5206443094939459e-05}, {"id": 229, "seek": 131300, "start": 1325.0, "end": 1332.0, "text": " So we don't just have something cryptic. We actually are, you know, it's value of negative six.", "tokens": [50364, 407, 7297, 300, 286, 994, 380, 362, 264, 1085, 81, 2445, 510, 11, 550, 309, 311, 445, 300, 291, 603, 483, 512, 733, 295, 364, 12246, 6114, 13, 50664, 50664, 407, 437, 1085, 81, 307, 884, 307, 309, 311, 6530, 505, 257, 636, 281, 4482, 484, 411, 257, 22842, 1237, 6114, 294, 15329, 13, 50964, 50964, 407, 321, 500, 380, 445, 362, 746, 9844, 299, 13, 492, 767, 366, 11, 291, 458, 11, 309, 311, 2158, 295, 3671, 2309, 13, 51314, 51314, 407, 341, 2709, 505, 257, 1413, 13, 400, 550, 341, 321, 820, 586, 312, 1075, 281, 909, 383, 281, 309, 570, 321, 600, 7642, 293, 1907, 264, 15329, 577, 281, 360, 6353, 293, 909, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09209051211018207, "compression_ratio": 1.6523297491039426, "no_speech_prob": 1.5206443094939459e-05}, {"id": 230, "seek": 131300, "start": 1332.0, "end": 1341.0, "text": " So this gives us a times. And then this we should now be able to add C to it because we've defined and told the Python how to do mole and add.", "tokens": [50364, 407, 7297, 300, 286, 994, 380, 362, 264, 1085, 81, 2445, 510, 11, 550, 309, 311, 445, 300, 291, 603, 483, 512, 733, 295, 364, 12246, 6114, 13, 50664, 50664, 407, 437, 1085, 81, 307, 884, 307, 309, 311, 6530, 505, 257, 636, 281, 4482, 484, 411, 257, 22842, 1237, 6114, 294, 15329, 13, 50964, 50964, 407, 321, 500, 380, 445, 362, 746, 9844, 299, 13, 492, 767, 366, 11, 291, 458, 11, 309, 311, 2158, 295, 3671, 2309, 13, 51314, 51314, 407, 341, 2709, 505, 257, 1413, 13, 400, 550, 341, 321, 820, 586, 312, 1075, 281, 909, 383, 281, 309, 570, 321, 600, 7642, 293, 1907, 264, 15329, 577, 281, 360, 6353, 293, 909, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09209051211018207, "compression_ratio": 1.6523297491039426, "no_speech_prob": 1.5206443094939459e-05}, {"id": 231, "seek": 134100, "start": 1341.0, "end": 1348.0, "text": " And so this will call this will basically be equivalent to a dot mole of B.", "tokens": [50364, 400, 370, 341, 486, 818, 341, 486, 1936, 312, 10344, 281, 257, 5893, 6353, 295, 363, 13, 50714, 50714, 400, 550, 341, 777, 2158, 2657, 486, 312, 5893, 909, 295, 383, 13, 400, 370, 718, 311, 536, 498, 300, 2732, 13, 51064, 51064, 7010, 13, 407, 300, 2732, 731, 13, 663, 2729, 505, 1451, 11, 597, 307, 437, 321, 2066, 490, 949, 13, 51264, 51264, 400, 286, 1697, 321, 393, 445, 818, 552, 16945, 382, 731, 13, 821, 321, 352, 13, 407, 13, 51564, 51564, 2264, 11, 370, 586, 437, 321, 366, 5361, 307, 264, 1745, 488, 12404, 295, 341, 6114, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12781815258961804, "compression_ratio": 1.638655462184874, "no_speech_prob": 4.8324658564524725e-05}, {"id": 232, "seek": 134100, "start": 1348.0, "end": 1355.0, "text": " And then this new value object will be dot add of C. And so let's see if that worked.", "tokens": [50364, 400, 370, 341, 486, 818, 341, 486, 1936, 312, 10344, 281, 257, 5893, 6353, 295, 363, 13, 50714, 50714, 400, 550, 341, 777, 2158, 2657, 486, 312, 5893, 909, 295, 383, 13, 400, 370, 718, 311, 536, 498, 300, 2732, 13, 51064, 51064, 7010, 13, 407, 300, 2732, 731, 13, 663, 2729, 505, 1451, 11, 597, 307, 437, 321, 2066, 490, 949, 13, 51264, 51264, 400, 286, 1697, 321, 393, 445, 818, 552, 16945, 382, 731, 13, 821, 321, 352, 13, 407, 13, 51564, 51564, 2264, 11, 370, 586, 437, 321, 366, 5361, 307, 264, 1745, 488, 12404, 295, 341, 6114, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12781815258961804, "compression_ratio": 1.638655462184874, "no_speech_prob": 4.8324658564524725e-05}, {"id": 233, "seek": 134100, "start": 1355.0, "end": 1359.0, "text": " Yep. So that worked well. That gave us four, which is what we expect from before.", "tokens": [50364, 400, 370, 341, 486, 818, 341, 486, 1936, 312, 10344, 281, 257, 5893, 6353, 295, 363, 13, 50714, 50714, 400, 550, 341, 777, 2158, 2657, 486, 312, 5893, 909, 295, 383, 13, 400, 370, 718, 311, 536, 498, 300, 2732, 13, 51064, 51064, 7010, 13, 407, 300, 2732, 731, 13, 663, 2729, 505, 1451, 11, 597, 307, 437, 321, 2066, 490, 949, 13, 51264, 51264, 400, 286, 1697, 321, 393, 445, 818, 552, 16945, 382, 731, 13, 821, 321, 352, 13, 407, 13, 51564, 51564, 2264, 11, 370, 586, 437, 321, 366, 5361, 307, 264, 1745, 488, 12404, 295, 341, 6114, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12781815258961804, "compression_ratio": 1.638655462184874, "no_speech_prob": 4.8324658564524725e-05}, {"id": 234, "seek": 134100, "start": 1359.0, "end": 1365.0, "text": " And I believe we can just call them manually as well. There we go. So.", "tokens": [50364, 400, 370, 341, 486, 818, 341, 486, 1936, 312, 10344, 281, 257, 5893, 6353, 295, 363, 13, 50714, 50714, 400, 550, 341, 777, 2158, 2657, 486, 312, 5893, 909, 295, 383, 13, 400, 370, 718, 311, 536, 498, 300, 2732, 13, 51064, 51064, 7010, 13, 407, 300, 2732, 731, 13, 663, 2729, 505, 1451, 11, 597, 307, 437, 321, 2066, 490, 949, 13, 51264, 51264, 400, 286, 1697, 321, 393, 445, 818, 552, 16945, 382, 731, 13, 821, 321, 352, 13, 407, 13, 51564, 51564, 2264, 11, 370, 586, 437, 321, 366, 5361, 307, 264, 1745, 488, 12404, 295, 341, 6114, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12781815258961804, "compression_ratio": 1.638655462184874, "no_speech_prob": 4.8324658564524725e-05}, {"id": 235, "seek": 134100, "start": 1365.0, "end": 1369.0, "text": " OK, so now what we are missing is the connective tissue of this expression.", "tokens": [50364, 400, 370, 341, 486, 818, 341, 486, 1936, 312, 10344, 281, 257, 5893, 6353, 295, 363, 13, 50714, 50714, 400, 550, 341, 777, 2158, 2657, 486, 312, 5893, 909, 295, 383, 13, 400, 370, 718, 311, 536, 498, 300, 2732, 13, 51064, 51064, 7010, 13, 407, 300, 2732, 731, 13, 663, 2729, 505, 1451, 11, 597, 307, 437, 321, 2066, 490, 949, 13, 51264, 51264, 400, 286, 1697, 321, 393, 445, 818, 552, 16945, 382, 731, 13, 821, 321, 352, 13, 407, 13, 51564, 51564, 2264, 11, 370, 586, 437, 321, 366, 5361, 307, 264, 1745, 488, 12404, 295, 341, 6114, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12781815258961804, "compression_ratio": 1.638655462184874, "no_speech_prob": 4.8324658564524725e-05}, {"id": 236, "seek": 136900, "start": 1369.0, "end": 1377.0, "text": " As I mentioned, we want to keep these expression graphs. So we need to know and keep pointers about what values produce what other values.", "tokens": [50364, 1018, 286, 2835, 11, 321, 528, 281, 1066, 613, 6114, 24877, 13, 407, 321, 643, 281, 458, 293, 1066, 44548, 466, 437, 4190, 5258, 437, 661, 4190, 13, 50764, 50764, 407, 510, 11, 337, 1365, 11, 321, 366, 516, 281, 5366, 257, 777, 7006, 11, 597, 321, 603, 818, 2227, 13, 50964, 50964, 400, 538, 7576, 11, 309, 486, 312, 364, 6707, 2604, 781, 13, 400, 550, 321, 434, 767, 516, 281, 1066, 257, 4748, 819, 7006, 294, 264, 1508, 11, 597, 321, 603, 818, 37556, 5835, 11, 597, 486, 312, 264, 992, 295, 2227, 13, 51464, 51464, 639, 307, 577, 286, 1096, 13, 286, 630, 309, 294, 264, 3380, 4532, 2771, 1237, 412, 452, 3089, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09677065395918048, "compression_ratio": 1.7474048442906573, "no_speech_prob": 1.0782989193103276e-05}, {"id": 237, "seek": 136900, "start": 1377.0, "end": 1381.0, "text": " So here, for example, we are going to introduce a new variable, which we'll call children.", "tokens": [50364, 1018, 286, 2835, 11, 321, 528, 281, 1066, 613, 6114, 24877, 13, 407, 321, 643, 281, 458, 293, 1066, 44548, 466, 437, 4190, 5258, 437, 661, 4190, 13, 50764, 50764, 407, 510, 11, 337, 1365, 11, 321, 366, 516, 281, 5366, 257, 777, 7006, 11, 597, 321, 603, 818, 2227, 13, 50964, 50964, 400, 538, 7576, 11, 309, 486, 312, 364, 6707, 2604, 781, 13, 400, 550, 321, 434, 767, 516, 281, 1066, 257, 4748, 819, 7006, 294, 264, 1508, 11, 597, 321, 603, 818, 37556, 5835, 11, 597, 486, 312, 264, 992, 295, 2227, 13, 51464, 51464, 639, 307, 577, 286, 1096, 13, 286, 630, 309, 294, 264, 3380, 4532, 2771, 1237, 412, 452, 3089, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09677065395918048, "compression_ratio": 1.7474048442906573, "no_speech_prob": 1.0782989193103276e-05}, {"id": 238, "seek": 136900, "start": 1381.0, "end": 1391.0, "text": " And by default, it will be an empty tuple. And then we're actually going to keep a slightly different variable in the class, which we'll call underscore prime, which will be the set of children.", "tokens": [50364, 1018, 286, 2835, 11, 321, 528, 281, 1066, 613, 6114, 24877, 13, 407, 321, 643, 281, 458, 293, 1066, 44548, 466, 437, 4190, 5258, 437, 661, 4190, 13, 50764, 50764, 407, 510, 11, 337, 1365, 11, 321, 366, 516, 281, 5366, 257, 777, 7006, 11, 597, 321, 603, 818, 2227, 13, 50964, 50964, 400, 538, 7576, 11, 309, 486, 312, 364, 6707, 2604, 781, 13, 400, 550, 321, 434, 767, 516, 281, 1066, 257, 4748, 819, 7006, 294, 264, 1508, 11, 597, 321, 603, 818, 37556, 5835, 11, 597, 486, 312, 264, 992, 295, 2227, 13, 51464, 51464, 639, 307, 577, 286, 1096, 13, 286, 630, 309, 294, 264, 3380, 4532, 2771, 1237, 412, 452, 3089, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09677065395918048, "compression_ratio": 1.7474048442906573, "no_speech_prob": 1.0782989193103276e-05}, {"id": 239, "seek": 136900, "start": 1391.0, "end": 1396.0, "text": " This is how I done. I did it in the original micro grad looking at my code here.", "tokens": [50364, 1018, 286, 2835, 11, 321, 528, 281, 1066, 613, 6114, 24877, 13, 407, 321, 643, 281, 458, 293, 1066, 44548, 466, 437, 4190, 5258, 437, 661, 4190, 13, 50764, 50764, 407, 510, 11, 337, 1365, 11, 321, 366, 516, 281, 5366, 257, 777, 7006, 11, 597, 321, 603, 818, 2227, 13, 50964, 50964, 400, 538, 7576, 11, 309, 486, 312, 364, 6707, 2604, 781, 13, 400, 550, 321, 434, 767, 516, 281, 1066, 257, 4748, 819, 7006, 294, 264, 1508, 11, 597, 321, 603, 818, 37556, 5835, 11, 597, 486, 312, 264, 992, 295, 2227, 13, 51464, 51464, 639, 307, 577, 286, 1096, 13, 286, 630, 309, 294, 264, 3380, 4532, 2771, 1237, 412, 452, 3089, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09677065395918048, "compression_ratio": 1.7474048442906573, "no_speech_prob": 1.0782989193103276e-05}, {"id": 240, "seek": 139600, "start": 1396.0, "end": 1402.0, "text": " I can't remember exactly the reason. I believe it was efficiency. But this underscore children will be a tuple for convenience.", "tokens": [50364, 286, 393, 380, 1604, 2293, 264, 1778, 13, 286, 1697, 309, 390, 10493, 13, 583, 341, 37556, 2227, 486, 312, 257, 2604, 781, 337, 19283, 13, 50664, 50664, 583, 550, 562, 321, 767, 6909, 309, 294, 264, 1508, 11, 309, 486, 312, 445, 264, 992, 13, 286, 1697, 337, 10493, 13, 51014, 51014, 407, 586, 562, 321, 366, 4084, 257, 2158, 411, 341, 365, 257, 47479, 11, 2227, 486, 312, 6707, 293, 2666, 486, 312, 264, 6707, 992, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.08599487557468644, "compression_ratio": 1.7121951219512195, "no_speech_prob": 3.1200939702102914e-05}, {"id": 241, "seek": 139600, "start": 1402.0, "end": 1409.0, "text": " But then when we actually maintain it in the class, it will be just the set. I believe for efficiency.", "tokens": [50364, 286, 393, 380, 1604, 2293, 264, 1778, 13, 286, 1697, 309, 390, 10493, 13, 583, 341, 37556, 2227, 486, 312, 257, 2604, 781, 337, 19283, 13, 50664, 50664, 583, 550, 562, 321, 767, 6909, 309, 294, 264, 1508, 11, 309, 486, 312, 445, 264, 992, 13, 286, 1697, 337, 10493, 13, 51014, 51014, 407, 586, 562, 321, 366, 4084, 257, 2158, 411, 341, 365, 257, 47479, 11, 2227, 486, 312, 6707, 293, 2666, 486, 312, 264, 6707, 992, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.08599487557468644, "compression_ratio": 1.7121951219512195, "no_speech_prob": 3.1200939702102914e-05}, {"id": 242, "seek": 139600, "start": 1409.0, "end": 1416.0, "text": " So now when we are creating a value like this with a constructor, children will be empty and prep will be the empty set.", "tokens": [50364, 286, 393, 380, 1604, 2293, 264, 1778, 13, 286, 1697, 309, 390, 10493, 13, 583, 341, 37556, 2227, 486, 312, 257, 2604, 781, 337, 19283, 13, 50664, 50664, 583, 550, 562, 321, 767, 6909, 309, 294, 264, 1508, 11, 309, 486, 312, 445, 264, 992, 13, 286, 1697, 337, 10493, 13, 51014, 51014, 407, 586, 562, 321, 366, 4084, 257, 2158, 411, 341, 365, 257, 47479, 11, 2227, 486, 312, 6707, 293, 2666, 486, 312, 264, 6707, 992, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.08599487557468644, "compression_ratio": 1.7121951219512195, "no_speech_prob": 3.1200939702102914e-05}, {"id": 243, "seek": 141600, "start": 1416.0, "end": 1426.0, "text": " But when we are creating a value through addition or multiplication, we're going to feed in the children of this value, which in this case is self and other.", "tokens": [50364, 583, 562, 321, 366, 4084, 257, 2158, 807, 4500, 420, 27290, 11, 321, 434, 516, 281, 3154, 294, 264, 2227, 295, 341, 2158, 11, 597, 294, 341, 1389, 307, 2698, 293, 661, 13, 50864, 50864, 407, 729, 366, 264, 2227, 510, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.10197652941164763, "compression_ratio": 1.4427480916030535, "no_speech_prob": 3.8829683035146445e-05}, {"id": 244, "seek": 141600, "start": 1426.0, "end": 1431.0, "text": " So those are the children here.", "tokens": [50364, 583, 562, 321, 366, 4084, 257, 2158, 807, 4500, 420, 27290, 11, 321, 434, 516, 281, 3154, 294, 264, 2227, 295, 341, 2158, 11, 597, 294, 341, 1389, 307, 2698, 293, 661, 13, 50864, 50864, 407, 729, 366, 264, 2227, 510, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.10197652941164763, "compression_ratio": 1.4427480916030535, "no_speech_prob": 3.8829683035146445e-05}, {"id": 245, "seek": 143100, "start": 1431.0, "end": 1447.0, "text": " So now we can do the dot prep and we'll see that the children of the we now know are this value of negative six and value of 10. And this, of course, is the value resulting from a times B and the C value, which is 10.", "tokens": [50364, 407, 586, 321, 393, 360, 264, 5893, 2666, 293, 321, 603, 536, 300, 264, 2227, 295, 264, 321, 586, 458, 366, 341, 2158, 295, 3671, 2309, 293, 2158, 295, 1266, 13, 400, 341, 11, 295, 1164, 11, 307, 264, 2158, 16505, 490, 257, 1413, 363, 293, 264, 383, 2158, 11, 597, 307, 1266, 13, 51164, 51164, 823, 11, 264, 1036, 2522, 295, 1589, 321, 500, 380, 458, 13, 407, 321, 458, 586, 264, 2227, 295, 633, 2167, 2158, 11, 457, 321, 500, 380, 458, 437, 6916, 2942, 341, 2158, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11527512935881919, "compression_ratio": 1.7255813953488373, "no_speech_prob": 9.972568477678578e-06}, {"id": 246, "seek": 143100, "start": 1447.0, "end": 1455.0, "text": " Now, the last piece of information we don't know. So we know now the children of every single value, but we don't know what operation created this value.", "tokens": [50364, 407, 586, 321, 393, 360, 264, 5893, 2666, 293, 321, 603, 536, 300, 264, 2227, 295, 264, 321, 586, 458, 366, 341, 2158, 295, 3671, 2309, 293, 2158, 295, 1266, 13, 400, 341, 11, 295, 1164, 11, 307, 264, 2158, 16505, 490, 257, 1413, 363, 293, 264, 383, 2158, 11, 597, 307, 1266, 13, 51164, 51164, 823, 11, 264, 1036, 2522, 295, 1589, 321, 500, 380, 458, 13, 407, 321, 458, 586, 264, 2227, 295, 633, 2167, 2158, 11, 457, 321, 500, 380, 458, 437, 6916, 2942, 341, 2158, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11527512935881919, "compression_ratio": 1.7255813953488373, "no_speech_prob": 9.972568477678578e-06}, {"id": 247, "seek": 145500, "start": 1455.0, "end": 1466.0, "text": " So we need one more element here. Let's call it underscore pop. And by default, this is the empty set for leaves. And then we'll just maintain it here.", "tokens": [50364, 407, 321, 643, 472, 544, 4478, 510, 13, 961, 311, 818, 309, 37556, 1665, 13, 400, 538, 7576, 11, 341, 307, 264, 6707, 992, 337, 5510, 13, 400, 550, 321, 603, 445, 6909, 309, 510, 13, 50914, 50914, 400, 586, 264, 6916, 486, 312, 445, 257, 2199, 6798, 13, 400, 294, 264, 1389, 295, 4500, 11, 309, 311, 1804, 294, 264, 1389, 295, 27290, 307, 1413, 13, 51314, 51314, 407, 586, 321, 406, 445, 362, 1412, 2666, 13, 492, 611, 362, 257, 413, 13, 1010, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.15763359069824218, "compression_ratio": 1.6095238095238096, "no_speech_prob": 4.860330591327511e-06}, {"id": 248, "seek": 145500, "start": 1466.0, "end": 1474.0, "text": " And now the operation will be just a simple string. And in the case of addition, it's plus in the case of multiplication is times.", "tokens": [50364, 407, 321, 643, 472, 544, 4478, 510, 13, 961, 311, 818, 309, 37556, 1665, 13, 400, 538, 7576, 11, 341, 307, 264, 6707, 992, 337, 5510, 13, 400, 550, 321, 603, 445, 6909, 309, 510, 13, 50914, 50914, 400, 586, 264, 6916, 486, 312, 445, 257, 2199, 6798, 13, 400, 294, 264, 1389, 295, 4500, 11, 309, 311, 1804, 294, 264, 1389, 295, 27290, 307, 1413, 13, 51314, 51314, 407, 586, 321, 406, 445, 362, 1412, 2666, 13, 492, 611, 362, 257, 413, 13, 1010, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.15763359069824218, "compression_ratio": 1.6095238095238096, "no_speech_prob": 4.860330591327511e-06}, {"id": 249, "seek": 145500, "start": 1474.0, "end": 1479.0, "text": " So now we not just have data prep. We also have a D.up.", "tokens": [50364, 407, 321, 643, 472, 544, 4478, 510, 13, 961, 311, 818, 309, 37556, 1665, 13, 400, 538, 7576, 11, 341, 307, 264, 6707, 992, 337, 5510, 13, 400, 550, 321, 603, 445, 6909, 309, 510, 13, 50914, 50914, 400, 586, 264, 6916, 486, 312, 445, 257, 2199, 6798, 13, 400, 294, 264, 1389, 295, 4500, 11, 309, 311, 1804, 294, 264, 1389, 295, 27290, 307, 1413, 13, 51314, 51314, 407, 586, 321, 406, 445, 362, 1412, 2666, 13, 492, 611, 362, 257, 413, 13, 1010, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.15763359069824218, "compression_ratio": 1.6095238095238096, "no_speech_prob": 4.860330591327511e-06}, {"id": 250, "seek": 147900, "start": 1479.0, "end": 1488.0, "text": " And we know that D was produced by an addition of those two values. And so now we have the full mathematical expression and we're building out this data structure.", "tokens": [50364, 400, 321, 458, 300, 413, 390, 7126, 538, 364, 4500, 295, 729, 732, 4190, 13, 400, 370, 586, 321, 362, 264, 1577, 18894, 6114, 293, 321, 434, 2390, 484, 341, 1412, 3877, 13, 50814, 50814, 400, 321, 458, 2293, 577, 1184, 2158, 1361, 281, 312, 538, 437, 6114, 293, 490, 437, 661, 4190, 13, 51164, 51164, 823, 11, 570, 613, 15277, 366, 466, 281, 483, 1596, 257, 857, 4833, 11, 321, 1116, 411, 257, 636, 281, 9594, 23273, 613, 15277, 300, 321, 434, 2390, 484, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06547414461771647, "compression_ratio": 1.7445887445887447, "no_speech_prob": 5.0145722525485326e-06}, {"id": 251, "seek": 147900, "start": 1488.0, "end": 1495.0, "text": " And we know exactly how each value came to be by what expression and from what other values.", "tokens": [50364, 400, 321, 458, 300, 413, 390, 7126, 538, 364, 4500, 295, 729, 732, 4190, 13, 400, 370, 586, 321, 362, 264, 1577, 18894, 6114, 293, 321, 434, 2390, 484, 341, 1412, 3877, 13, 50814, 50814, 400, 321, 458, 2293, 577, 1184, 2158, 1361, 281, 312, 538, 437, 6114, 293, 490, 437, 661, 4190, 13, 51164, 51164, 823, 11, 570, 613, 15277, 366, 466, 281, 483, 1596, 257, 857, 4833, 11, 321, 1116, 411, 257, 636, 281, 9594, 23273, 613, 15277, 300, 321, 434, 2390, 484, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06547414461771647, "compression_ratio": 1.7445887445887447, "no_speech_prob": 5.0145722525485326e-06}, {"id": 252, "seek": 147900, "start": 1495.0, "end": 1502.0, "text": " Now, because these expressions are about to get quite a bit larger, we'd like a way to nicely visualize these expressions that we're building out.", "tokens": [50364, 400, 321, 458, 300, 413, 390, 7126, 538, 364, 4500, 295, 729, 732, 4190, 13, 400, 370, 586, 321, 362, 264, 1577, 18894, 6114, 293, 321, 434, 2390, 484, 341, 1412, 3877, 13, 50814, 50814, 400, 321, 458, 2293, 577, 1184, 2158, 1361, 281, 312, 538, 437, 6114, 293, 490, 437, 661, 4190, 13, 51164, 51164, 823, 11, 570, 613, 15277, 366, 466, 281, 483, 1596, 257, 857, 4833, 11, 321, 1116, 411, 257, 636, 281, 9594, 23273, 613, 15277, 300, 321, 434, 2390, 484, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06547414461771647, "compression_ratio": 1.7445887445887447, "no_speech_prob": 5.0145722525485326e-06}, {"id": 253, "seek": 150200, "start": 1502.0, "end": 1509.0, "text": " So for that, I'm going to copy paste a bunch of slightly scary code that's going to visualize this expression graphs for us.", "tokens": [50364, 407, 337, 300, 11, 286, 478, 516, 281, 5055, 9163, 257, 3840, 295, 4748, 6958, 3089, 300, 311, 516, 281, 23273, 341, 6114, 24877, 337, 505, 13, 50714, 50714, 407, 510, 311, 264, 3089, 11, 293, 286, 603, 2903, 309, 294, 257, 857, 13, 583, 700, 11, 718, 385, 445, 855, 291, 437, 341, 3089, 775, 13, 51014, 51014, 8537, 11, 437, 309, 775, 307, 309, 7829, 257, 777, 2445, 2642, 5893, 300, 321, 393, 818, 322, 512, 5593, 9984, 293, 550, 309, 311, 516, 281, 23273, 309, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.08361421092864006, "compression_ratio": 1.6261261261261262, "no_speech_prob": 8.139502824633382e-06}, {"id": 254, "seek": 150200, "start": 1509.0, "end": 1515.0, "text": " So here's the code, and I'll explain it in a bit. But first, let me just show you what this code does.", "tokens": [50364, 407, 337, 300, 11, 286, 478, 516, 281, 5055, 9163, 257, 3840, 295, 4748, 6958, 3089, 300, 311, 516, 281, 23273, 341, 6114, 24877, 337, 505, 13, 50714, 50714, 407, 510, 311, 264, 3089, 11, 293, 286, 603, 2903, 309, 294, 257, 857, 13, 583, 700, 11, 718, 385, 445, 855, 291, 437, 341, 3089, 775, 13, 51014, 51014, 8537, 11, 437, 309, 775, 307, 309, 7829, 257, 777, 2445, 2642, 5893, 300, 321, 393, 818, 322, 512, 5593, 9984, 293, 550, 309, 311, 516, 281, 23273, 309, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.08361421092864006, "compression_ratio": 1.6261261261261262, "no_speech_prob": 8.139502824633382e-06}, {"id": 255, "seek": 150200, "start": 1515.0, "end": 1522.0, "text": " Basically, what it does is it creates a new function draw dot that we can call on some root node and then it's going to visualize it.", "tokens": [50364, 407, 337, 300, 11, 286, 478, 516, 281, 5055, 9163, 257, 3840, 295, 4748, 6958, 3089, 300, 311, 516, 281, 23273, 341, 6114, 24877, 337, 505, 13, 50714, 50714, 407, 510, 311, 264, 3089, 11, 293, 286, 603, 2903, 309, 294, 257, 857, 13, 583, 700, 11, 718, 385, 445, 855, 291, 437, 341, 3089, 775, 13, 51014, 51014, 8537, 11, 437, 309, 775, 307, 309, 7829, 257, 777, 2445, 2642, 5893, 300, 321, 393, 818, 322, 512, 5593, 9984, 293, 550, 309, 311, 516, 281, 23273, 309, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.08361421092864006, "compression_ratio": 1.6261261261261262, "no_speech_prob": 8.139502824633382e-06}, {"id": 256, "seek": 152200, "start": 1522.0, "end": 1532.0, "text": " So if we call draw dot on D, which is this final value here, that is a times people see it create something like this. So this is D.", "tokens": [50364, 407, 498, 321, 818, 2642, 5893, 322, 413, 11, 597, 307, 341, 2572, 2158, 510, 11, 300, 307, 257, 1413, 561, 536, 309, 1884, 746, 411, 341, 13, 407, 341, 307, 413, 13, 50864, 50864, 400, 291, 536, 300, 341, 307, 257, 1413, 363, 4084, 364, 7302, 2158, 1804, 383, 2709, 505, 341, 5598, 9984, 413, 13, 51264, 51264, 407, 300, 311, 10117, 484, 295, 264, 293, 286, 478, 406, 516, 281, 352, 807, 341, 294, 3566, 2607, 13, 51514, 51514, 509, 393, 747, 257, 574, 412, 11837, 293, 1080, 9362, 11837, 307, 257, 1269, 4009, 11837, 25801, 4722, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19304636808542106, "compression_ratio": 1.6456692913385826, "no_speech_prob": 4.222719780955231e-06}, {"id": 257, "seek": 152200, "start": 1532.0, "end": 1540.0, "text": " And you see that this is a times B creating an interpret value plus C gives us this output node D.", "tokens": [50364, 407, 498, 321, 818, 2642, 5893, 322, 413, 11, 597, 307, 341, 2572, 2158, 510, 11, 300, 307, 257, 1413, 561, 536, 309, 1884, 746, 411, 341, 13, 407, 341, 307, 413, 13, 50864, 50864, 400, 291, 536, 300, 341, 307, 257, 1413, 363, 4084, 364, 7302, 2158, 1804, 383, 2709, 505, 341, 5598, 9984, 413, 13, 51264, 51264, 407, 300, 311, 10117, 484, 295, 264, 293, 286, 478, 406, 516, 281, 352, 807, 341, 294, 3566, 2607, 13, 51514, 51514, 509, 393, 747, 257, 574, 412, 11837, 293, 1080, 9362, 11837, 307, 257, 1269, 4009, 11837, 25801, 4722, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19304636808542106, "compression_ratio": 1.6456692913385826, "no_speech_prob": 4.222719780955231e-06}, {"id": 258, "seek": 152200, "start": 1540.0, "end": 1545.0, "text": " So that's drawn out of the and I'm not going to go through this in complete detail.", "tokens": [50364, 407, 498, 321, 818, 2642, 5893, 322, 413, 11, 597, 307, 341, 2572, 2158, 510, 11, 300, 307, 257, 1413, 561, 536, 309, 1884, 746, 411, 341, 13, 407, 341, 307, 413, 13, 50864, 50864, 400, 291, 536, 300, 341, 307, 257, 1413, 363, 4084, 364, 7302, 2158, 1804, 383, 2709, 505, 341, 5598, 9984, 413, 13, 51264, 51264, 407, 300, 311, 10117, 484, 295, 264, 293, 286, 478, 406, 516, 281, 352, 807, 341, 294, 3566, 2607, 13, 51514, 51514, 509, 393, 747, 257, 574, 412, 11837, 293, 1080, 9362, 11837, 307, 257, 1269, 4009, 11837, 25801, 4722, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19304636808542106, "compression_ratio": 1.6456692913385826, "no_speech_prob": 4.222719780955231e-06}, {"id": 259, "seek": 152200, "start": 1545.0, "end": 1551.0, "text": " You can take a look at graphics and its API graphics is a open source graphics visualization software.", "tokens": [50364, 407, 498, 321, 818, 2642, 5893, 322, 413, 11, 597, 307, 341, 2572, 2158, 510, 11, 300, 307, 257, 1413, 561, 536, 309, 1884, 746, 411, 341, 13, 407, 341, 307, 413, 13, 50864, 50864, 400, 291, 536, 300, 341, 307, 257, 1413, 363, 4084, 364, 7302, 2158, 1804, 383, 2709, 505, 341, 5598, 9984, 413, 13, 51264, 51264, 407, 300, 311, 10117, 484, 295, 264, 293, 286, 478, 406, 516, 281, 352, 807, 341, 294, 3566, 2607, 13, 51514, 51514, 509, 393, 747, 257, 574, 412, 11837, 293, 1080, 9362, 11837, 307, 257, 1269, 4009, 11837, 25801, 4722, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19304636808542106, "compression_ratio": 1.6456692913385826, "no_speech_prob": 4.222719780955231e-06}, {"id": 260, "seek": 155100, "start": 1551.0, "end": 1555.0, "text": " And what we're doing here is we're building out this graph in graphics API.", "tokens": [50364, 400, 437, 321, 434, 884, 510, 307, 321, 434, 2390, 484, 341, 4295, 294, 11837, 9362, 13, 50564, 50564, 400, 291, 393, 1936, 536, 300, 13508, 307, 341, 36133, 2445, 300, 465, 15583, 1024, 439, 264, 13891, 293, 8819, 294, 264, 4295, 13, 50914, 50914, 407, 300, 445, 15182, 257, 992, 295, 439, 264, 13891, 293, 8819, 13, 51064, 51064, 400, 550, 321, 44497, 807, 439, 264, 13891, 293, 321, 1884, 2121, 9984, 6565, 337, 552, 294, 1228, 5893, 9984, 13, 51464, 51464, 400, 550, 321, 611, 1884, 8819, 1228, 5893, 5893, 4691, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08157257644497619, "compression_ratio": 1.8644859813084111, "no_speech_prob": 1.0288954399584327e-05}, {"id": 261, "seek": 155100, "start": 1555.0, "end": 1562.0, "text": " And you can basically see that trace is this helper function that enumerates all the nodes and edges in the graph.", "tokens": [50364, 400, 437, 321, 434, 884, 510, 307, 321, 434, 2390, 484, 341, 4295, 294, 11837, 9362, 13, 50564, 50564, 400, 291, 393, 1936, 536, 300, 13508, 307, 341, 36133, 2445, 300, 465, 15583, 1024, 439, 264, 13891, 293, 8819, 294, 264, 4295, 13, 50914, 50914, 407, 300, 445, 15182, 257, 992, 295, 439, 264, 13891, 293, 8819, 13, 51064, 51064, 400, 550, 321, 44497, 807, 439, 264, 13891, 293, 321, 1884, 2121, 9984, 6565, 337, 552, 294, 1228, 5893, 9984, 13, 51464, 51464, 400, 550, 321, 611, 1884, 8819, 1228, 5893, 5893, 4691, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08157257644497619, "compression_ratio": 1.8644859813084111, "no_speech_prob": 1.0288954399584327e-05}, {"id": 262, "seek": 155100, "start": 1562.0, "end": 1565.0, "text": " So that just builds a set of all the nodes and edges.", "tokens": [50364, 400, 437, 321, 434, 884, 510, 307, 321, 434, 2390, 484, 341, 4295, 294, 11837, 9362, 13, 50564, 50564, 400, 291, 393, 1936, 536, 300, 13508, 307, 341, 36133, 2445, 300, 465, 15583, 1024, 439, 264, 13891, 293, 8819, 294, 264, 4295, 13, 50914, 50914, 407, 300, 445, 15182, 257, 992, 295, 439, 264, 13891, 293, 8819, 13, 51064, 51064, 400, 550, 321, 44497, 807, 439, 264, 13891, 293, 321, 1884, 2121, 9984, 6565, 337, 552, 294, 1228, 5893, 9984, 13, 51464, 51464, 400, 550, 321, 611, 1884, 8819, 1228, 5893, 5893, 4691, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08157257644497619, "compression_ratio": 1.8644859813084111, "no_speech_prob": 1.0288954399584327e-05}, {"id": 263, "seek": 155100, "start": 1565.0, "end": 1573.0, "text": " And then we iterate through all the nodes and we create special node objects for them in using dot node.", "tokens": [50364, 400, 437, 321, 434, 884, 510, 307, 321, 434, 2390, 484, 341, 4295, 294, 11837, 9362, 13, 50564, 50564, 400, 291, 393, 1936, 536, 300, 13508, 307, 341, 36133, 2445, 300, 465, 15583, 1024, 439, 264, 13891, 293, 8819, 294, 264, 4295, 13, 50914, 50914, 407, 300, 445, 15182, 257, 992, 295, 439, 264, 13891, 293, 8819, 13, 51064, 51064, 400, 550, 321, 44497, 807, 439, 264, 13891, 293, 321, 1884, 2121, 9984, 6565, 337, 552, 294, 1228, 5893, 9984, 13, 51464, 51464, 400, 550, 321, 611, 1884, 8819, 1228, 5893, 5893, 4691, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08157257644497619, "compression_ratio": 1.8644859813084111, "no_speech_prob": 1.0288954399584327e-05}, {"id": 264, "seek": 155100, "start": 1573.0, "end": 1576.0, "text": " And then we also create edges using dot dot edge.", "tokens": [50364, 400, 437, 321, 434, 884, 510, 307, 321, 434, 2390, 484, 341, 4295, 294, 11837, 9362, 13, 50564, 50564, 400, 291, 393, 1936, 536, 300, 13508, 307, 341, 36133, 2445, 300, 465, 15583, 1024, 439, 264, 13891, 293, 8819, 294, 264, 4295, 13, 50914, 50914, 407, 300, 445, 15182, 257, 992, 295, 439, 264, 13891, 293, 8819, 13, 51064, 51064, 400, 550, 321, 44497, 807, 439, 264, 13891, 293, 321, 1884, 2121, 9984, 6565, 337, 552, 294, 1228, 5893, 9984, 13, 51464, 51464, 400, 550, 321, 611, 1884, 8819, 1228, 5893, 5893, 4691, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08157257644497619, "compression_ratio": 1.8644859813084111, "no_speech_prob": 1.0288954399584327e-05}, {"id": 265, "seek": 157600, "start": 1576.0, "end": 1583.0, "text": " And the only thing that's slightly tricky here is you'll notice that I basically add these fake nodes, which are these operation nodes.", "tokens": [50364, 400, 264, 787, 551, 300, 311, 4748, 12414, 510, 307, 291, 603, 3449, 300, 286, 1936, 909, 613, 7592, 13891, 11, 597, 366, 613, 6916, 13891, 13, 50714, 50714, 407, 11, 337, 1365, 11, 341, 9984, 510, 307, 445, 411, 257, 1804, 9984, 13, 50914, 50914, 400, 286, 1884, 613, 2121, 13891, 510, 293, 286, 1745, 552, 19717, 13, 51364, 51364, 407, 613, 13891, 11, 295, 1164, 11, 366, 406, 3539, 13891, 294, 264, 3380, 4295, 13, 51614, 51614, 814, 434, 406, 767, 257, 2158, 2657, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05836024651160607, "compression_ratio": 1.663677130044843, "no_speech_prob": 1.544565020594746e-05}, {"id": 266, "seek": 157600, "start": 1583.0, "end": 1587.0, "text": " So, for example, this node here is just like a plus node.", "tokens": [50364, 400, 264, 787, 551, 300, 311, 4748, 12414, 510, 307, 291, 603, 3449, 300, 286, 1936, 909, 613, 7592, 13891, 11, 597, 366, 613, 6916, 13891, 13, 50714, 50714, 407, 11, 337, 1365, 11, 341, 9984, 510, 307, 445, 411, 257, 1804, 9984, 13, 50914, 50914, 400, 286, 1884, 613, 2121, 13891, 510, 293, 286, 1745, 552, 19717, 13, 51364, 51364, 407, 613, 13891, 11, 295, 1164, 11, 366, 406, 3539, 13891, 294, 264, 3380, 4295, 13, 51614, 51614, 814, 434, 406, 767, 257, 2158, 2657, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05836024651160607, "compression_ratio": 1.663677130044843, "no_speech_prob": 1.544565020594746e-05}, {"id": 267, "seek": 157600, "start": 1587.0, "end": 1596.0, "text": " And I create these special nodes here and I connect them accordingly.", "tokens": [50364, 400, 264, 787, 551, 300, 311, 4748, 12414, 510, 307, 291, 603, 3449, 300, 286, 1936, 909, 613, 7592, 13891, 11, 597, 366, 613, 6916, 13891, 13, 50714, 50714, 407, 11, 337, 1365, 11, 341, 9984, 510, 307, 445, 411, 257, 1804, 9984, 13, 50914, 50914, 400, 286, 1884, 613, 2121, 13891, 510, 293, 286, 1745, 552, 19717, 13, 51364, 51364, 407, 613, 13891, 11, 295, 1164, 11, 366, 406, 3539, 13891, 294, 264, 3380, 4295, 13, 51614, 51614, 814, 434, 406, 767, 257, 2158, 2657, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05836024651160607, "compression_ratio": 1.663677130044843, "no_speech_prob": 1.544565020594746e-05}, {"id": 268, "seek": 157600, "start": 1596.0, "end": 1601.0, "text": " So these nodes, of course, are not actual nodes in the original graph.", "tokens": [50364, 400, 264, 787, 551, 300, 311, 4748, 12414, 510, 307, 291, 603, 3449, 300, 286, 1936, 909, 613, 7592, 13891, 11, 597, 366, 613, 6916, 13891, 13, 50714, 50714, 407, 11, 337, 1365, 11, 341, 9984, 510, 307, 445, 411, 257, 1804, 9984, 13, 50914, 50914, 400, 286, 1884, 613, 2121, 13891, 510, 293, 286, 1745, 552, 19717, 13, 51364, 51364, 407, 613, 13891, 11, 295, 1164, 11, 366, 406, 3539, 13891, 294, 264, 3380, 4295, 13, 51614, 51614, 814, 434, 406, 767, 257, 2158, 2657, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05836024651160607, "compression_ratio": 1.663677130044843, "no_speech_prob": 1.544565020594746e-05}, {"id": 269, "seek": 157600, "start": 1601.0, "end": 1603.0, "text": " They're not actually a value object.", "tokens": [50364, 400, 264, 787, 551, 300, 311, 4748, 12414, 510, 307, 291, 603, 3449, 300, 286, 1936, 909, 613, 7592, 13891, 11, 597, 366, 613, 6916, 13891, 13, 50714, 50714, 407, 11, 337, 1365, 11, 341, 9984, 510, 307, 445, 411, 257, 1804, 9984, 13, 50914, 50914, 400, 286, 1884, 613, 2121, 13891, 510, 293, 286, 1745, 552, 19717, 13, 51364, 51364, 407, 613, 13891, 11, 295, 1164, 11, 366, 406, 3539, 13891, 294, 264, 3380, 4295, 13, 51614, 51614, 814, 434, 406, 767, 257, 2158, 2657, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05836024651160607, "compression_ratio": 1.663677130044843, "no_speech_prob": 1.544565020594746e-05}, {"id": 270, "seek": 160300, "start": 1603.0, "end": 1607.0, "text": " The only value objects here are the things in squares.", "tokens": [50364, 440, 787, 2158, 6565, 510, 366, 264, 721, 294, 19368, 13, 50564, 50564, 3950, 366, 3539, 2158, 6565, 420, 33358, 456, 2670, 13, 50714, 50714, 400, 613, 493, 13891, 366, 445, 2942, 294, 341, 2642, 5893, 9927, 370, 300, 309, 1542, 1481, 13, 50964, 50964, 961, 311, 611, 909, 16949, 281, 613, 24877, 445, 370, 321, 458, 437, 9102, 366, 689, 13, 51214, 51214, 407, 718, 311, 1884, 257, 2121, 37556, 7645, 420, 718, 311, 445, 360, 7645, 6915, 6707, 538, 7576, 293, 3155, 309, 294, 1184, 9984, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07868408900435253, "compression_ratio": 1.6652542372881356, "no_speech_prob": 8.530149898433592e-06}, {"id": 271, "seek": 160300, "start": 1607.0, "end": 1610.0, "text": " Those are actual value objects or representations thereof.", "tokens": [50364, 440, 787, 2158, 6565, 510, 366, 264, 721, 294, 19368, 13, 50564, 50564, 3950, 366, 3539, 2158, 6565, 420, 33358, 456, 2670, 13, 50714, 50714, 400, 613, 493, 13891, 366, 445, 2942, 294, 341, 2642, 5893, 9927, 370, 300, 309, 1542, 1481, 13, 50964, 50964, 961, 311, 611, 909, 16949, 281, 613, 24877, 445, 370, 321, 458, 437, 9102, 366, 689, 13, 51214, 51214, 407, 718, 311, 1884, 257, 2121, 37556, 7645, 420, 718, 311, 445, 360, 7645, 6915, 6707, 538, 7576, 293, 3155, 309, 294, 1184, 9984, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07868408900435253, "compression_ratio": 1.6652542372881356, "no_speech_prob": 8.530149898433592e-06}, {"id": 272, "seek": 160300, "start": 1610.0, "end": 1615.0, "text": " And these up nodes are just created in this draw dot routine so that it looks nice.", "tokens": [50364, 440, 787, 2158, 6565, 510, 366, 264, 721, 294, 19368, 13, 50564, 50564, 3950, 366, 3539, 2158, 6565, 420, 33358, 456, 2670, 13, 50714, 50714, 400, 613, 493, 13891, 366, 445, 2942, 294, 341, 2642, 5893, 9927, 370, 300, 309, 1542, 1481, 13, 50964, 50964, 961, 311, 611, 909, 16949, 281, 613, 24877, 445, 370, 321, 458, 437, 9102, 366, 689, 13, 51214, 51214, 407, 718, 311, 1884, 257, 2121, 37556, 7645, 420, 718, 311, 445, 360, 7645, 6915, 6707, 538, 7576, 293, 3155, 309, 294, 1184, 9984, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07868408900435253, "compression_ratio": 1.6652542372881356, "no_speech_prob": 8.530149898433592e-06}, {"id": 273, "seek": 160300, "start": 1615.0, "end": 1620.0, "text": " Let's also add labels to these graphs just so we know what variables are where.", "tokens": [50364, 440, 787, 2158, 6565, 510, 366, 264, 721, 294, 19368, 13, 50564, 50564, 3950, 366, 3539, 2158, 6565, 420, 33358, 456, 2670, 13, 50714, 50714, 400, 613, 493, 13891, 366, 445, 2942, 294, 341, 2642, 5893, 9927, 370, 300, 309, 1542, 1481, 13, 50964, 50964, 961, 311, 611, 909, 16949, 281, 613, 24877, 445, 370, 321, 458, 437, 9102, 366, 689, 13, 51214, 51214, 407, 718, 311, 1884, 257, 2121, 37556, 7645, 420, 718, 311, 445, 360, 7645, 6915, 6707, 538, 7576, 293, 3155, 309, 294, 1184, 9984, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07868408900435253, "compression_ratio": 1.6652542372881356, "no_speech_prob": 8.530149898433592e-06}, {"id": 274, "seek": 160300, "start": 1620.0, "end": 1631.0, "text": " So let's create a special underscore label or let's just do label equals empty by default and save it in each node.", "tokens": [50364, 440, 787, 2158, 6565, 510, 366, 264, 721, 294, 19368, 13, 50564, 50564, 3950, 366, 3539, 2158, 6565, 420, 33358, 456, 2670, 13, 50714, 50714, 400, 613, 493, 13891, 366, 445, 2942, 294, 341, 2642, 5893, 9927, 370, 300, 309, 1542, 1481, 13, 50964, 50964, 961, 311, 611, 909, 16949, 281, 613, 24877, 445, 370, 321, 458, 437, 9102, 366, 689, 13, 51214, 51214, 407, 718, 311, 1884, 257, 2121, 37556, 7645, 420, 718, 311, 445, 360, 7645, 6915, 6707, 538, 7576, 293, 3155, 309, 294, 1184, 9984, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07868408900435253, "compression_ratio": 1.6652542372881356, "no_speech_prob": 8.530149898433592e-06}, {"id": 275, "seek": 163100, "start": 1631.0, "end": 1642.0, "text": " And then here we're going to do label is a label is the label is C.", "tokens": [50364, 400, 550, 510, 321, 434, 516, 281, 360, 7645, 307, 257, 7645, 307, 264, 7645, 307, 383, 13, 50914, 50914, 400, 550, 718, 311, 1884, 257, 2121, 6915, 257, 1413, 363, 293, 3834, 7645, 486, 312, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.27947081589117284, "compression_ratio": 1.4059405940594059, "no_speech_prob": 6.814491644036025e-05}, {"id": 276, "seek": 163100, "start": 1642.0, "end": 1654.0, "text": " And then let's create a special equals a times B and double label will be.", "tokens": [50364, 400, 550, 510, 321, 434, 516, 281, 360, 7645, 307, 257, 7645, 307, 264, 7645, 307, 383, 13, 50914, 50914, 400, 550, 718, 311, 1884, 257, 2121, 6915, 257, 1413, 363, 293, 3834, 7645, 486, 312, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.27947081589117284, "compression_ratio": 1.4059405940594059, "no_speech_prob": 6.814491644036025e-05}, {"id": 277, "seek": 165400, "start": 1654.0, "end": 1662.0, "text": " It's kind of naughty and E will be E plus C and a D dot label will be B.", "tokens": [50364, 467, 311, 733, 295, 32154, 293, 462, 486, 312, 462, 1804, 383, 293, 257, 413, 5893, 7645, 486, 312, 363, 13, 50764, 50764, 2264, 11, 370, 1825, 534, 2962, 13, 50864, 50864, 286, 445, 3869, 341, 777, 462, 2445, 11, 777, 462, 7006, 13, 51064, 51064, 400, 550, 510, 11, 562, 321, 366, 14699, 341, 11, 286, 478, 516, 281, 4482, 264, 7645, 510, 13, 51364, 51364, 407, 341, 486, 312, 257, 3043, 382, 2159, 293, 341, 486, 312, 917, 493, 7645, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18216443335872956, "compression_ratio": 1.5638297872340425, "no_speech_prob": 3.763483982766047e-05}, {"id": 278, "seek": 165400, "start": 1662.0, "end": 1664.0, "text": " OK, so nothing really changes.", "tokens": [50364, 467, 311, 733, 295, 32154, 293, 462, 486, 312, 462, 1804, 383, 293, 257, 413, 5893, 7645, 486, 312, 363, 13, 50764, 50764, 2264, 11, 370, 1825, 534, 2962, 13, 50864, 50864, 286, 445, 3869, 341, 777, 462, 2445, 11, 777, 462, 7006, 13, 51064, 51064, 400, 550, 510, 11, 562, 321, 366, 14699, 341, 11, 286, 478, 516, 281, 4482, 264, 7645, 510, 13, 51364, 51364, 407, 341, 486, 312, 257, 3043, 382, 2159, 293, 341, 486, 312, 917, 493, 7645, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18216443335872956, "compression_ratio": 1.5638297872340425, "no_speech_prob": 3.763483982766047e-05}, {"id": 279, "seek": 165400, "start": 1664.0, "end": 1668.0, "text": " I just added this new E function, new E variable.", "tokens": [50364, 467, 311, 733, 295, 32154, 293, 462, 486, 312, 462, 1804, 383, 293, 257, 413, 5893, 7645, 486, 312, 363, 13, 50764, 50764, 2264, 11, 370, 1825, 534, 2962, 13, 50864, 50864, 286, 445, 3869, 341, 777, 462, 2445, 11, 777, 462, 7006, 13, 51064, 51064, 400, 550, 510, 11, 562, 321, 366, 14699, 341, 11, 286, 478, 516, 281, 4482, 264, 7645, 510, 13, 51364, 51364, 407, 341, 486, 312, 257, 3043, 382, 2159, 293, 341, 486, 312, 917, 493, 7645, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18216443335872956, "compression_ratio": 1.5638297872340425, "no_speech_prob": 3.763483982766047e-05}, {"id": 280, "seek": 165400, "start": 1668.0, "end": 1674.0, "text": " And then here, when we are printing this, I'm going to print the label here.", "tokens": [50364, 467, 311, 733, 295, 32154, 293, 462, 486, 312, 462, 1804, 383, 293, 257, 413, 5893, 7645, 486, 312, 363, 13, 50764, 50764, 2264, 11, 370, 1825, 534, 2962, 13, 50864, 50864, 286, 445, 3869, 341, 777, 462, 2445, 11, 777, 462, 7006, 13, 51064, 51064, 400, 550, 510, 11, 562, 321, 366, 14699, 341, 11, 286, 478, 516, 281, 4482, 264, 7645, 510, 13, 51364, 51364, 407, 341, 486, 312, 257, 3043, 382, 2159, 293, 341, 486, 312, 917, 493, 7645, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18216443335872956, "compression_ratio": 1.5638297872340425, "no_speech_prob": 3.763483982766047e-05}, {"id": 281, "seek": 165400, "start": 1674.0, "end": 1681.0, "text": " So this will be a percent as bar and this will be end up label.", "tokens": [50364, 467, 311, 733, 295, 32154, 293, 462, 486, 312, 462, 1804, 383, 293, 257, 413, 5893, 7645, 486, 312, 363, 13, 50764, 50764, 2264, 11, 370, 1825, 534, 2962, 13, 50864, 50864, 286, 445, 3869, 341, 777, 462, 2445, 11, 777, 462, 7006, 13, 51064, 51064, 400, 550, 510, 11, 562, 321, 366, 14699, 341, 11, 286, 478, 516, 281, 4482, 264, 7645, 510, 13, 51364, 51364, 407, 341, 486, 312, 257, 3043, 382, 2159, 293, 341, 486, 312, 917, 493, 7645, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18216443335872956, "compression_ratio": 1.5638297872340425, "no_speech_prob": 3.763483982766047e-05}, {"id": 282, "seek": 168100, "start": 1681.0, "end": 1685.0, "text": " And so now we have the label on the left here.", "tokens": [50364, 400, 370, 586, 321, 362, 264, 7645, 322, 264, 1411, 510, 13, 50564, 50564, 407, 341, 307, 257, 363, 4084, 385, 293, 550, 462, 1804, 383, 7829, 413, 445, 411, 321, 362, 309, 510, 13, 50864, 50864, 400, 2721, 11, 718, 311, 652, 341, 6114, 445, 472, 4583, 7731, 13, 51014, 51014, 407, 413, 486, 406, 312, 264, 2572, 5598, 9984, 13, 51164, 51164, 7156, 11, 934, 413, 11, 321, 366, 516, 281, 1884, 257, 777, 2158, 2657, 1219, 479, 13, 51464, 51464, 492, 434, 516, 281, 722, 2614, 484, 295, 7006, 2321, 13, 51564, 51564, 479, 486, 312, 3671, 732, 935, 4018, 293, 1080, 7645, 486, 11, 295, 1164, 11, 445, 312, 479, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12068324529824137, "compression_ratio": 1.6273764258555132, "no_speech_prob": 1.2805260666937102e-05}, {"id": 283, "seek": 168100, "start": 1685.0, "end": 1691.0, "text": " So this is a B creating me and then E plus C creates D just like we have it here.", "tokens": [50364, 400, 370, 586, 321, 362, 264, 7645, 322, 264, 1411, 510, 13, 50564, 50564, 407, 341, 307, 257, 363, 4084, 385, 293, 550, 462, 1804, 383, 7829, 413, 445, 411, 321, 362, 309, 510, 13, 50864, 50864, 400, 2721, 11, 718, 311, 652, 341, 6114, 445, 472, 4583, 7731, 13, 51014, 51014, 407, 413, 486, 406, 312, 264, 2572, 5598, 9984, 13, 51164, 51164, 7156, 11, 934, 413, 11, 321, 366, 516, 281, 1884, 257, 777, 2158, 2657, 1219, 479, 13, 51464, 51464, 492, 434, 516, 281, 722, 2614, 484, 295, 7006, 2321, 13, 51564, 51564, 479, 486, 312, 3671, 732, 935, 4018, 293, 1080, 7645, 486, 11, 295, 1164, 11, 445, 312, 479, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12068324529824137, "compression_ratio": 1.6273764258555132, "no_speech_prob": 1.2805260666937102e-05}, {"id": 284, "seek": 168100, "start": 1691.0, "end": 1694.0, "text": " And finally, let's make this expression just one layer deeper.", "tokens": [50364, 400, 370, 586, 321, 362, 264, 7645, 322, 264, 1411, 510, 13, 50564, 50564, 407, 341, 307, 257, 363, 4084, 385, 293, 550, 462, 1804, 383, 7829, 413, 445, 411, 321, 362, 309, 510, 13, 50864, 50864, 400, 2721, 11, 718, 311, 652, 341, 6114, 445, 472, 4583, 7731, 13, 51014, 51014, 407, 413, 486, 406, 312, 264, 2572, 5598, 9984, 13, 51164, 51164, 7156, 11, 934, 413, 11, 321, 366, 516, 281, 1884, 257, 777, 2158, 2657, 1219, 479, 13, 51464, 51464, 492, 434, 516, 281, 722, 2614, 484, 295, 7006, 2321, 13, 51564, 51564, 479, 486, 312, 3671, 732, 935, 4018, 293, 1080, 7645, 486, 11, 295, 1164, 11, 445, 312, 479, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12068324529824137, "compression_ratio": 1.6273764258555132, "no_speech_prob": 1.2805260666937102e-05}, {"id": 285, "seek": 168100, "start": 1694.0, "end": 1697.0, "text": " So D will not be the final output node.", "tokens": [50364, 400, 370, 586, 321, 362, 264, 7645, 322, 264, 1411, 510, 13, 50564, 50564, 407, 341, 307, 257, 363, 4084, 385, 293, 550, 462, 1804, 383, 7829, 413, 445, 411, 321, 362, 309, 510, 13, 50864, 50864, 400, 2721, 11, 718, 311, 652, 341, 6114, 445, 472, 4583, 7731, 13, 51014, 51014, 407, 413, 486, 406, 312, 264, 2572, 5598, 9984, 13, 51164, 51164, 7156, 11, 934, 413, 11, 321, 366, 516, 281, 1884, 257, 777, 2158, 2657, 1219, 479, 13, 51464, 51464, 492, 434, 516, 281, 722, 2614, 484, 295, 7006, 2321, 13, 51564, 51564, 479, 486, 312, 3671, 732, 935, 4018, 293, 1080, 7645, 486, 11, 295, 1164, 11, 445, 312, 479, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12068324529824137, "compression_ratio": 1.6273764258555132, "no_speech_prob": 1.2805260666937102e-05}, {"id": 286, "seek": 168100, "start": 1697.0, "end": 1703.0, "text": " Instead, after D, we are going to create a new value object called F.", "tokens": [50364, 400, 370, 586, 321, 362, 264, 7645, 322, 264, 1411, 510, 13, 50564, 50564, 407, 341, 307, 257, 363, 4084, 385, 293, 550, 462, 1804, 383, 7829, 413, 445, 411, 321, 362, 309, 510, 13, 50864, 50864, 400, 2721, 11, 718, 311, 652, 341, 6114, 445, 472, 4583, 7731, 13, 51014, 51014, 407, 413, 486, 406, 312, 264, 2572, 5598, 9984, 13, 51164, 51164, 7156, 11, 934, 413, 11, 321, 366, 516, 281, 1884, 257, 777, 2158, 2657, 1219, 479, 13, 51464, 51464, 492, 434, 516, 281, 722, 2614, 484, 295, 7006, 2321, 13, 51564, 51564, 479, 486, 312, 3671, 732, 935, 4018, 293, 1080, 7645, 486, 11, 295, 1164, 11, 445, 312, 479, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12068324529824137, "compression_ratio": 1.6273764258555132, "no_speech_prob": 1.2805260666937102e-05}, {"id": 287, "seek": 168100, "start": 1703.0, "end": 1705.0, "text": " We're going to start running out of variable soon.", "tokens": [50364, 400, 370, 586, 321, 362, 264, 7645, 322, 264, 1411, 510, 13, 50564, 50564, 407, 341, 307, 257, 363, 4084, 385, 293, 550, 462, 1804, 383, 7829, 413, 445, 411, 321, 362, 309, 510, 13, 50864, 50864, 400, 2721, 11, 718, 311, 652, 341, 6114, 445, 472, 4583, 7731, 13, 51014, 51014, 407, 413, 486, 406, 312, 264, 2572, 5598, 9984, 13, 51164, 51164, 7156, 11, 934, 413, 11, 321, 366, 516, 281, 1884, 257, 777, 2158, 2657, 1219, 479, 13, 51464, 51464, 492, 434, 516, 281, 722, 2614, 484, 295, 7006, 2321, 13, 51564, 51564, 479, 486, 312, 3671, 732, 935, 4018, 293, 1080, 7645, 486, 11, 295, 1164, 11, 445, 312, 479, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12068324529824137, "compression_ratio": 1.6273764258555132, "no_speech_prob": 1.2805260666937102e-05}, {"id": 288, "seek": 168100, "start": 1705.0, "end": 1710.0, "text": " F will be negative two point zero and its label will, of course, just be F.", "tokens": [50364, 400, 370, 586, 321, 362, 264, 7645, 322, 264, 1411, 510, 13, 50564, 50564, 407, 341, 307, 257, 363, 4084, 385, 293, 550, 462, 1804, 383, 7829, 413, 445, 411, 321, 362, 309, 510, 13, 50864, 50864, 400, 2721, 11, 718, 311, 652, 341, 6114, 445, 472, 4583, 7731, 13, 51014, 51014, 407, 413, 486, 406, 312, 264, 2572, 5598, 9984, 13, 51164, 51164, 7156, 11, 934, 413, 11, 321, 366, 516, 281, 1884, 257, 777, 2158, 2657, 1219, 479, 13, 51464, 51464, 492, 434, 516, 281, 722, 2614, 484, 295, 7006, 2321, 13, 51564, 51564, 479, 486, 312, 3671, 732, 935, 4018, 293, 1080, 7645, 486, 11, 295, 1164, 11, 445, 312, 479, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12068324529824137, "compression_ratio": 1.6273764258555132, "no_speech_prob": 1.2805260666937102e-05}, {"id": 289, "seek": 171000, "start": 1710.0, "end": 1715.0, "text": " And then L, capital L, will be the output of our graph.", "tokens": [50364, 400, 550, 441, 11, 4238, 441, 11, 486, 312, 264, 5598, 295, 527, 4295, 13, 50614, 50614, 400, 441, 486, 312, 413, 1413, 479, 13, 50764, 50764, 407, 441, 486, 312, 3671, 3180, 307, 264, 5598, 13, 50964, 50964, 407, 586, 321, 500, 380, 445, 2642, 257, 413, 11, 321, 2642, 441, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.16049502188699288, "compression_ratio": 1.3983050847457628, "no_speech_prob": 5.8289271692046896e-05}, {"id": 290, "seek": 171000, "start": 1715.0, "end": 1718.0, "text": " And L will be D times F.", "tokens": [50364, 400, 550, 441, 11, 4238, 441, 11, 486, 312, 264, 5598, 295, 527, 4295, 13, 50614, 50614, 400, 441, 486, 312, 413, 1413, 479, 13, 50764, 50764, 407, 441, 486, 312, 3671, 3180, 307, 264, 5598, 13, 50964, 50964, 407, 586, 321, 500, 380, 445, 2642, 257, 413, 11, 321, 2642, 441, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.16049502188699288, "compression_ratio": 1.3983050847457628, "no_speech_prob": 5.8289271692046896e-05}, {"id": 291, "seek": 171000, "start": 1718.0, "end": 1722.0, "text": " So L will be negative eight is the output.", "tokens": [50364, 400, 550, 441, 11, 4238, 441, 11, 486, 312, 264, 5598, 295, 527, 4295, 13, 50614, 50614, 400, 441, 486, 312, 413, 1413, 479, 13, 50764, 50764, 407, 441, 486, 312, 3671, 3180, 307, 264, 5598, 13, 50964, 50964, 407, 586, 321, 500, 380, 445, 2642, 257, 413, 11, 321, 2642, 441, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.16049502188699288, "compression_ratio": 1.3983050847457628, "no_speech_prob": 5.8289271692046896e-05}, {"id": 292, "seek": 171000, "start": 1722.0, "end": 1730.0, "text": " So now we don't just draw a D, we draw L.", "tokens": [50364, 400, 550, 441, 11, 4238, 441, 11, 486, 312, 264, 5598, 295, 527, 4295, 13, 50614, 50614, 400, 441, 486, 312, 413, 1413, 479, 13, 50764, 50764, 407, 441, 486, 312, 3671, 3180, 307, 264, 5598, 13, 50964, 50964, 407, 586, 321, 500, 380, 445, 2642, 257, 413, 11, 321, 2642, 441, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.16049502188699288, "compression_ratio": 1.3983050847457628, "no_speech_prob": 5.8289271692046896e-05}, {"id": 293, "seek": 173000, "start": 1730.0, "end": 1740.0, "text": " And somehow the label of L is undefined. L.label has to be explicitly given to it.", "tokens": [50364, 400, 6063, 264, 7645, 295, 441, 307, 674, 5666, 2001, 13, 441, 13, 75, 18657, 575, 281, 312, 20803, 2212, 281, 309, 13, 50864, 50864, 821, 321, 352, 13, 407, 441, 307, 264, 5598, 13, 50964, 50964, 407, 718, 311, 2661, 20928, 437, 321, 600, 1096, 370, 1400, 13, 51064, 51064, 492, 366, 1075, 281, 1322, 484, 18894, 15277, 1228, 787, 1804, 293, 1413, 370, 1400, 13, 51314, 51314, 814, 366, 39684, 22608, 2051, 264, 636, 13, 51414, 51414, 400, 321, 393, 360, 341, 2128, 1320, 293, 1322, 484, 257, 18894, 6114, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09497409505942433, "compression_ratio": 1.63013698630137, "no_speech_prob": 5.338097707863199e-06}, {"id": 294, "seek": 173000, "start": 1740.0, "end": 1742.0, "text": " There we go. So L is the output.", "tokens": [50364, 400, 6063, 264, 7645, 295, 441, 307, 674, 5666, 2001, 13, 441, 13, 75, 18657, 575, 281, 312, 20803, 2212, 281, 309, 13, 50864, 50864, 821, 321, 352, 13, 407, 441, 307, 264, 5598, 13, 50964, 50964, 407, 718, 311, 2661, 20928, 437, 321, 600, 1096, 370, 1400, 13, 51064, 51064, 492, 366, 1075, 281, 1322, 484, 18894, 15277, 1228, 787, 1804, 293, 1413, 370, 1400, 13, 51314, 51314, 814, 366, 39684, 22608, 2051, 264, 636, 13, 51414, 51414, 400, 321, 393, 360, 341, 2128, 1320, 293, 1322, 484, 257, 18894, 6114, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09497409505942433, "compression_ratio": 1.63013698630137, "no_speech_prob": 5.338097707863199e-06}, {"id": 295, "seek": 173000, "start": 1742.0, "end": 1744.0, "text": " So let's quickly recap what we've done so far.", "tokens": [50364, 400, 6063, 264, 7645, 295, 441, 307, 674, 5666, 2001, 13, 441, 13, 75, 18657, 575, 281, 312, 20803, 2212, 281, 309, 13, 50864, 50864, 821, 321, 352, 13, 407, 441, 307, 264, 5598, 13, 50964, 50964, 407, 718, 311, 2661, 20928, 437, 321, 600, 1096, 370, 1400, 13, 51064, 51064, 492, 366, 1075, 281, 1322, 484, 18894, 15277, 1228, 787, 1804, 293, 1413, 370, 1400, 13, 51314, 51314, 814, 366, 39684, 22608, 2051, 264, 636, 13, 51414, 51414, 400, 321, 393, 360, 341, 2128, 1320, 293, 1322, 484, 257, 18894, 6114, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09497409505942433, "compression_ratio": 1.63013698630137, "no_speech_prob": 5.338097707863199e-06}, {"id": 296, "seek": 173000, "start": 1744.0, "end": 1749.0, "text": " We are able to build out mathematical expressions using only plus and times so far.", "tokens": [50364, 400, 6063, 264, 7645, 295, 441, 307, 674, 5666, 2001, 13, 441, 13, 75, 18657, 575, 281, 312, 20803, 2212, 281, 309, 13, 50864, 50864, 821, 321, 352, 13, 407, 441, 307, 264, 5598, 13, 50964, 50964, 407, 718, 311, 2661, 20928, 437, 321, 600, 1096, 370, 1400, 13, 51064, 51064, 492, 366, 1075, 281, 1322, 484, 18894, 15277, 1228, 787, 1804, 293, 1413, 370, 1400, 13, 51314, 51314, 814, 366, 39684, 22608, 2051, 264, 636, 13, 51414, 51414, 400, 321, 393, 360, 341, 2128, 1320, 293, 1322, 484, 257, 18894, 6114, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09497409505942433, "compression_ratio": 1.63013698630137, "no_speech_prob": 5.338097707863199e-06}, {"id": 297, "seek": 173000, "start": 1749.0, "end": 1751.0, "text": " They are scalar valued along the way.", "tokens": [50364, 400, 6063, 264, 7645, 295, 441, 307, 674, 5666, 2001, 13, 441, 13, 75, 18657, 575, 281, 312, 20803, 2212, 281, 309, 13, 50864, 50864, 821, 321, 352, 13, 407, 441, 307, 264, 5598, 13, 50964, 50964, 407, 718, 311, 2661, 20928, 437, 321, 600, 1096, 370, 1400, 13, 51064, 51064, 492, 366, 1075, 281, 1322, 484, 18894, 15277, 1228, 787, 1804, 293, 1413, 370, 1400, 13, 51314, 51314, 814, 366, 39684, 22608, 2051, 264, 636, 13, 51414, 51414, 400, 321, 393, 360, 341, 2128, 1320, 293, 1322, 484, 257, 18894, 6114, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09497409505942433, "compression_ratio": 1.63013698630137, "no_speech_prob": 5.338097707863199e-06}, {"id": 298, "seek": 173000, "start": 1751.0, "end": 1756.0, "text": " And we can do this forward pass and build out a mathematical expression.", "tokens": [50364, 400, 6063, 264, 7645, 295, 441, 307, 674, 5666, 2001, 13, 441, 13, 75, 18657, 575, 281, 312, 20803, 2212, 281, 309, 13, 50864, 50864, 821, 321, 352, 13, 407, 441, 307, 264, 5598, 13, 50964, 50964, 407, 718, 311, 2661, 20928, 437, 321, 600, 1096, 370, 1400, 13, 51064, 51064, 492, 366, 1075, 281, 1322, 484, 18894, 15277, 1228, 787, 1804, 293, 1413, 370, 1400, 13, 51314, 51314, 814, 366, 39684, 22608, 2051, 264, 636, 13, 51414, 51414, 400, 321, 393, 360, 341, 2128, 1320, 293, 1322, 484, 257, 18894, 6114, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09497409505942433, "compression_ratio": 1.63013698630137, "no_speech_prob": 5.338097707863199e-06}, {"id": 299, "seek": 175600, "start": 1756.0, "end": 1764.0, "text": " So we have multiple inputs here, A, B, C, and F, going into a mathematical expression that produces a single output L.", "tokens": [50364, 407, 321, 362, 3866, 15743, 510, 11, 316, 11, 363, 11, 383, 11, 293, 479, 11, 516, 666, 257, 18894, 6114, 300, 14725, 257, 2167, 5598, 441, 13, 50764, 50764, 400, 341, 510, 307, 5056, 3319, 264, 2128, 1320, 13, 50914, 50914, 407, 264, 5598, 295, 264, 2128, 1320, 307, 3671, 3180, 13, 663, 311, 264, 2158, 13, 51114, 51114, 823, 11, 437, 321, 1116, 411, 281, 360, 958, 307, 321, 1116, 411, 281, 1190, 646, 79, 1513, 559, 399, 13, 51314, 51314, 400, 294, 646, 79, 1513, 559, 399, 11, 321, 366, 516, 281, 722, 510, 412, 264, 917, 11, 293, 321, 434, 516, 281, 9943, 293, 8873, 264, 16235, 2051, 439, 613, 19376, 4190, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08111840240226305, "compression_ratio": 1.7153558052434457, "no_speech_prob": 9.515762940282002e-06}, {"id": 300, "seek": 175600, "start": 1764.0, "end": 1767.0, "text": " And this here is visualizing the forward pass.", "tokens": [50364, 407, 321, 362, 3866, 15743, 510, 11, 316, 11, 363, 11, 383, 11, 293, 479, 11, 516, 666, 257, 18894, 6114, 300, 14725, 257, 2167, 5598, 441, 13, 50764, 50764, 400, 341, 510, 307, 5056, 3319, 264, 2128, 1320, 13, 50914, 50914, 407, 264, 5598, 295, 264, 2128, 1320, 307, 3671, 3180, 13, 663, 311, 264, 2158, 13, 51114, 51114, 823, 11, 437, 321, 1116, 411, 281, 360, 958, 307, 321, 1116, 411, 281, 1190, 646, 79, 1513, 559, 399, 13, 51314, 51314, 400, 294, 646, 79, 1513, 559, 399, 11, 321, 366, 516, 281, 722, 510, 412, 264, 917, 11, 293, 321, 434, 516, 281, 9943, 293, 8873, 264, 16235, 2051, 439, 613, 19376, 4190, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08111840240226305, "compression_ratio": 1.7153558052434457, "no_speech_prob": 9.515762940282002e-06}, {"id": 301, "seek": 175600, "start": 1767.0, "end": 1771.0, "text": " So the output of the forward pass is negative eight. That's the value.", "tokens": [50364, 407, 321, 362, 3866, 15743, 510, 11, 316, 11, 363, 11, 383, 11, 293, 479, 11, 516, 666, 257, 18894, 6114, 300, 14725, 257, 2167, 5598, 441, 13, 50764, 50764, 400, 341, 510, 307, 5056, 3319, 264, 2128, 1320, 13, 50914, 50914, 407, 264, 5598, 295, 264, 2128, 1320, 307, 3671, 3180, 13, 663, 311, 264, 2158, 13, 51114, 51114, 823, 11, 437, 321, 1116, 411, 281, 360, 958, 307, 321, 1116, 411, 281, 1190, 646, 79, 1513, 559, 399, 13, 51314, 51314, 400, 294, 646, 79, 1513, 559, 399, 11, 321, 366, 516, 281, 722, 510, 412, 264, 917, 11, 293, 321, 434, 516, 281, 9943, 293, 8873, 264, 16235, 2051, 439, 613, 19376, 4190, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08111840240226305, "compression_ratio": 1.7153558052434457, "no_speech_prob": 9.515762940282002e-06}, {"id": 302, "seek": 175600, "start": 1771.0, "end": 1775.0, "text": " Now, what we'd like to do next is we'd like to run backpropagation.", "tokens": [50364, 407, 321, 362, 3866, 15743, 510, 11, 316, 11, 363, 11, 383, 11, 293, 479, 11, 516, 666, 257, 18894, 6114, 300, 14725, 257, 2167, 5598, 441, 13, 50764, 50764, 400, 341, 510, 307, 5056, 3319, 264, 2128, 1320, 13, 50914, 50914, 407, 264, 5598, 295, 264, 2128, 1320, 307, 3671, 3180, 13, 663, 311, 264, 2158, 13, 51114, 51114, 823, 11, 437, 321, 1116, 411, 281, 360, 958, 307, 321, 1116, 411, 281, 1190, 646, 79, 1513, 559, 399, 13, 51314, 51314, 400, 294, 646, 79, 1513, 559, 399, 11, 321, 366, 516, 281, 722, 510, 412, 264, 917, 11, 293, 321, 434, 516, 281, 9943, 293, 8873, 264, 16235, 2051, 439, 613, 19376, 4190, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08111840240226305, "compression_ratio": 1.7153558052434457, "no_speech_prob": 9.515762940282002e-06}, {"id": 303, "seek": 175600, "start": 1775.0, "end": 1785.0, "text": " And in backpropagation, we are going to start here at the end, and we're going to reverse and calculate the gradient along all these intermediate values.", "tokens": [50364, 407, 321, 362, 3866, 15743, 510, 11, 316, 11, 363, 11, 383, 11, 293, 479, 11, 516, 666, 257, 18894, 6114, 300, 14725, 257, 2167, 5598, 441, 13, 50764, 50764, 400, 341, 510, 307, 5056, 3319, 264, 2128, 1320, 13, 50914, 50914, 407, 264, 5598, 295, 264, 2128, 1320, 307, 3671, 3180, 13, 663, 311, 264, 2158, 13, 51114, 51114, 823, 11, 437, 321, 1116, 411, 281, 360, 958, 307, 321, 1116, 411, 281, 1190, 646, 79, 1513, 559, 399, 13, 51314, 51314, 400, 294, 646, 79, 1513, 559, 399, 11, 321, 366, 516, 281, 722, 510, 412, 264, 917, 11, 293, 321, 434, 516, 281, 9943, 293, 8873, 264, 16235, 2051, 439, 613, 19376, 4190, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08111840240226305, "compression_ratio": 1.7153558052434457, "no_speech_prob": 9.515762940282002e-06}, {"id": 304, "seek": 178500, "start": 1785.0, "end": 1795.0, "text": " And really what we're computing for every single value here, we're going to compute the derivative of that node with respect to L.", "tokens": [50364, 400, 534, 437, 321, 434, 15866, 337, 633, 2167, 2158, 510, 11, 321, 434, 516, 281, 14722, 264, 13760, 295, 300, 9984, 365, 3104, 281, 441, 13, 50864, 50864, 407, 264, 13760, 295, 441, 365, 3104, 281, 441, 307, 445, 472, 13, 51114, 51114, 400, 550, 321, 434, 516, 281, 28446, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 479, 11, 365, 3104, 281, 413, 11, 365, 3104, 281, 383, 11, 365, 3104, 281, 462, 11, 365, 3104, 281, 363, 11, 293, 365, 3104, 281, 316, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.04316378935523655, "compression_ratio": 2.25625, "no_speech_prob": 1.4509954780805856e-05}, {"id": 305, "seek": 178500, "start": 1795.0, "end": 1800.0, "text": " So the derivative of L with respect to L is just one.", "tokens": [50364, 400, 534, 437, 321, 434, 15866, 337, 633, 2167, 2158, 510, 11, 321, 434, 516, 281, 14722, 264, 13760, 295, 300, 9984, 365, 3104, 281, 441, 13, 50864, 50864, 407, 264, 13760, 295, 441, 365, 3104, 281, 441, 307, 445, 472, 13, 51114, 51114, 400, 550, 321, 434, 516, 281, 28446, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 479, 11, 365, 3104, 281, 413, 11, 365, 3104, 281, 383, 11, 365, 3104, 281, 462, 11, 365, 3104, 281, 363, 11, 293, 365, 3104, 281, 316, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.04316378935523655, "compression_ratio": 2.25625, "no_speech_prob": 1.4509954780805856e-05}, {"id": 306, "seek": 178500, "start": 1800.0, "end": 1810.0, "text": " And then we're going to derive what is the derivative of L with respect to F, with respect to D, with respect to C, with respect to E, with respect to B, and with respect to A.", "tokens": [50364, 400, 534, 437, 321, 434, 15866, 337, 633, 2167, 2158, 510, 11, 321, 434, 516, 281, 14722, 264, 13760, 295, 300, 9984, 365, 3104, 281, 441, 13, 50864, 50864, 407, 264, 13760, 295, 441, 365, 3104, 281, 441, 307, 445, 472, 13, 51114, 51114, 400, 550, 321, 434, 516, 281, 28446, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 479, 11, 365, 3104, 281, 413, 11, 365, 3104, 281, 383, 11, 365, 3104, 281, 462, 11, 365, 3104, 281, 363, 11, 293, 365, 3104, 281, 316, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.04316378935523655, "compression_ratio": 2.25625, "no_speech_prob": 1.4509954780805856e-05}, {"id": 307, "seek": 181000, "start": 1810.0, "end": 1819.0, "text": " And in a neural network setting, you'd be very interested in the derivative of basically this loss function L with respect to the weights of a neural network.", "tokens": [50364, 400, 294, 257, 18161, 3209, 3287, 11, 291, 1116, 312, 588, 3102, 294, 264, 13760, 295, 1936, 341, 4470, 2445, 441, 365, 3104, 281, 264, 17443, 295, 257, 18161, 3209, 13, 50814, 50814, 400, 510, 11, 295, 1164, 11, 321, 362, 445, 613, 9102, 316, 11, 363, 11, 383, 11, 293, 479, 11, 457, 512, 295, 613, 486, 4728, 2906, 264, 17443, 295, 257, 18161, 2533, 13, 51164, 51164, 400, 370, 321, 603, 643, 281, 458, 577, 729, 17443, 366, 29963, 264, 4470, 2445, 13, 51364, 51364, 407, 321, 603, 312, 3102, 1936, 294, 264, 13760, 295, 264, 5598, 365, 3104, 281, 512, 295, 1080, 10871, 13891, 13, 51614, 51614, 400, 729, 10871, 13891, 486, 312, 264, 17443, 295, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.043566332088680715, "compression_ratio": 2.003717472118959, "no_speech_prob": 7.183112302300287e-06}, {"id": 308, "seek": 181000, "start": 1819.0, "end": 1826.0, "text": " And here, of course, we have just these variables A, B, C, and F, but some of these will eventually represent the weights of a neural net.", "tokens": [50364, 400, 294, 257, 18161, 3209, 3287, 11, 291, 1116, 312, 588, 3102, 294, 264, 13760, 295, 1936, 341, 4470, 2445, 441, 365, 3104, 281, 264, 17443, 295, 257, 18161, 3209, 13, 50814, 50814, 400, 510, 11, 295, 1164, 11, 321, 362, 445, 613, 9102, 316, 11, 363, 11, 383, 11, 293, 479, 11, 457, 512, 295, 613, 486, 4728, 2906, 264, 17443, 295, 257, 18161, 2533, 13, 51164, 51164, 400, 370, 321, 603, 643, 281, 458, 577, 729, 17443, 366, 29963, 264, 4470, 2445, 13, 51364, 51364, 407, 321, 603, 312, 3102, 1936, 294, 264, 13760, 295, 264, 5598, 365, 3104, 281, 512, 295, 1080, 10871, 13891, 13, 51614, 51614, 400, 729, 10871, 13891, 486, 312, 264, 17443, 295, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.043566332088680715, "compression_ratio": 2.003717472118959, "no_speech_prob": 7.183112302300287e-06}, {"id": 309, "seek": 181000, "start": 1826.0, "end": 1830.0, "text": " And so we'll need to know how those weights are impacting the loss function.", "tokens": [50364, 400, 294, 257, 18161, 3209, 3287, 11, 291, 1116, 312, 588, 3102, 294, 264, 13760, 295, 1936, 341, 4470, 2445, 441, 365, 3104, 281, 264, 17443, 295, 257, 18161, 3209, 13, 50814, 50814, 400, 510, 11, 295, 1164, 11, 321, 362, 445, 613, 9102, 316, 11, 363, 11, 383, 11, 293, 479, 11, 457, 512, 295, 613, 486, 4728, 2906, 264, 17443, 295, 257, 18161, 2533, 13, 51164, 51164, 400, 370, 321, 603, 643, 281, 458, 577, 729, 17443, 366, 29963, 264, 4470, 2445, 13, 51364, 51364, 407, 321, 603, 312, 3102, 1936, 294, 264, 13760, 295, 264, 5598, 365, 3104, 281, 512, 295, 1080, 10871, 13891, 13, 51614, 51614, 400, 729, 10871, 13891, 486, 312, 264, 17443, 295, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.043566332088680715, "compression_ratio": 2.003717472118959, "no_speech_prob": 7.183112302300287e-06}, {"id": 310, "seek": 181000, "start": 1830.0, "end": 1835.0, "text": " So we'll be interested basically in the derivative of the output with respect to some of its leaf nodes.", "tokens": [50364, 400, 294, 257, 18161, 3209, 3287, 11, 291, 1116, 312, 588, 3102, 294, 264, 13760, 295, 1936, 341, 4470, 2445, 441, 365, 3104, 281, 264, 17443, 295, 257, 18161, 3209, 13, 50814, 50814, 400, 510, 11, 295, 1164, 11, 321, 362, 445, 613, 9102, 316, 11, 363, 11, 383, 11, 293, 479, 11, 457, 512, 295, 613, 486, 4728, 2906, 264, 17443, 295, 257, 18161, 2533, 13, 51164, 51164, 400, 370, 321, 603, 643, 281, 458, 577, 729, 17443, 366, 29963, 264, 4470, 2445, 13, 51364, 51364, 407, 321, 603, 312, 3102, 1936, 294, 264, 13760, 295, 264, 5598, 365, 3104, 281, 512, 295, 1080, 10871, 13891, 13, 51614, 51614, 400, 729, 10871, 13891, 486, 312, 264, 17443, 295, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.043566332088680715, "compression_ratio": 2.003717472118959, "no_speech_prob": 7.183112302300287e-06}, {"id": 311, "seek": 181000, "start": 1835.0, "end": 1838.0, "text": " And those leaf nodes will be the weights of the neural net.", "tokens": [50364, 400, 294, 257, 18161, 3209, 3287, 11, 291, 1116, 312, 588, 3102, 294, 264, 13760, 295, 1936, 341, 4470, 2445, 441, 365, 3104, 281, 264, 17443, 295, 257, 18161, 3209, 13, 50814, 50814, 400, 510, 11, 295, 1164, 11, 321, 362, 445, 613, 9102, 316, 11, 363, 11, 383, 11, 293, 479, 11, 457, 512, 295, 613, 486, 4728, 2906, 264, 17443, 295, 257, 18161, 2533, 13, 51164, 51164, 400, 370, 321, 603, 643, 281, 458, 577, 729, 17443, 366, 29963, 264, 4470, 2445, 13, 51364, 51364, 407, 321, 603, 312, 3102, 1936, 294, 264, 13760, 295, 264, 5598, 365, 3104, 281, 512, 295, 1080, 10871, 13891, 13, 51614, 51614, 400, 729, 10871, 13891, 486, 312, 264, 17443, 295, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.043566332088680715, "compression_ratio": 2.003717472118959, "no_speech_prob": 7.183112302300287e-06}, {"id": 312, "seek": 183800, "start": 1838.0, "end": 1841.0, "text": " And the other leaf nodes, of course, will be the data itself.", "tokens": [50364, 400, 264, 661, 10871, 13891, 11, 295, 1164, 11, 486, 312, 264, 1412, 2564, 13, 50514, 50514, 583, 2673, 321, 486, 406, 528, 420, 764, 264, 13760, 295, 264, 4470, 2445, 365, 3104, 281, 1412, 570, 264, 1412, 307, 6806, 13, 50814, 50814, 583, 264, 17443, 486, 312, 17138, 770, 322, 1228, 264, 16235, 1589, 13, 51064, 51064, 407, 958, 11, 321, 366, 516, 281, 1884, 257, 7006, 1854, 264, 2158, 1508, 300, 33385, 264, 13760, 295, 441, 365, 3104, 281, 300, 2158, 13, 51514, 51514, 400, 321, 486, 818, 341, 7006, 2771, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.042107582092285156, "compression_ratio": 1.7887931034482758, "no_speech_prob": 1.5689227438997477e-05}, {"id": 313, "seek": 183800, "start": 1841.0, "end": 1847.0, "text": " But usually we will not want or use the derivative of the loss function with respect to data because the data is fixed.", "tokens": [50364, 400, 264, 661, 10871, 13891, 11, 295, 1164, 11, 486, 312, 264, 1412, 2564, 13, 50514, 50514, 583, 2673, 321, 486, 406, 528, 420, 764, 264, 13760, 295, 264, 4470, 2445, 365, 3104, 281, 1412, 570, 264, 1412, 307, 6806, 13, 50814, 50814, 583, 264, 17443, 486, 312, 17138, 770, 322, 1228, 264, 16235, 1589, 13, 51064, 51064, 407, 958, 11, 321, 366, 516, 281, 1884, 257, 7006, 1854, 264, 2158, 1508, 300, 33385, 264, 13760, 295, 441, 365, 3104, 281, 300, 2158, 13, 51514, 51514, 400, 321, 486, 818, 341, 7006, 2771, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.042107582092285156, "compression_ratio": 1.7887931034482758, "no_speech_prob": 1.5689227438997477e-05}, {"id": 314, "seek": 183800, "start": 1847.0, "end": 1852.0, "text": " But the weights will be iterated on using the gradient information.", "tokens": [50364, 400, 264, 661, 10871, 13891, 11, 295, 1164, 11, 486, 312, 264, 1412, 2564, 13, 50514, 50514, 583, 2673, 321, 486, 406, 528, 420, 764, 264, 13760, 295, 264, 4470, 2445, 365, 3104, 281, 1412, 570, 264, 1412, 307, 6806, 13, 50814, 50814, 583, 264, 17443, 486, 312, 17138, 770, 322, 1228, 264, 16235, 1589, 13, 51064, 51064, 407, 958, 11, 321, 366, 516, 281, 1884, 257, 7006, 1854, 264, 2158, 1508, 300, 33385, 264, 13760, 295, 441, 365, 3104, 281, 300, 2158, 13, 51514, 51514, 400, 321, 486, 818, 341, 7006, 2771, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.042107582092285156, "compression_ratio": 1.7887931034482758, "no_speech_prob": 1.5689227438997477e-05}, {"id": 315, "seek": 183800, "start": 1852.0, "end": 1861.0, "text": " So next, we are going to create a variable inside the value class that maintains the derivative of L with respect to that value.", "tokens": [50364, 400, 264, 661, 10871, 13891, 11, 295, 1164, 11, 486, 312, 264, 1412, 2564, 13, 50514, 50514, 583, 2673, 321, 486, 406, 528, 420, 764, 264, 13760, 295, 264, 4470, 2445, 365, 3104, 281, 1412, 570, 264, 1412, 307, 6806, 13, 50814, 50814, 583, 264, 17443, 486, 312, 17138, 770, 322, 1228, 264, 16235, 1589, 13, 51064, 51064, 407, 958, 11, 321, 366, 516, 281, 1884, 257, 7006, 1854, 264, 2158, 1508, 300, 33385, 264, 13760, 295, 441, 365, 3104, 281, 300, 2158, 13, 51514, 51514, 400, 321, 486, 818, 341, 7006, 2771, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.042107582092285156, "compression_ratio": 1.7887931034482758, "no_speech_prob": 1.5689227438997477e-05}, {"id": 316, "seek": 183800, "start": 1861.0, "end": 1864.0, "text": " And we will call this variable grad.", "tokens": [50364, 400, 264, 661, 10871, 13891, 11, 295, 1164, 11, 486, 312, 264, 1412, 2564, 13, 50514, 50514, 583, 2673, 321, 486, 406, 528, 420, 764, 264, 13760, 295, 264, 4470, 2445, 365, 3104, 281, 1412, 570, 264, 1412, 307, 6806, 13, 50814, 50814, 583, 264, 17443, 486, 312, 17138, 770, 322, 1228, 264, 16235, 1589, 13, 51064, 51064, 407, 958, 11, 321, 366, 516, 281, 1884, 257, 7006, 1854, 264, 2158, 1508, 300, 33385, 264, 13760, 295, 441, 365, 3104, 281, 300, 2158, 13, 51514, 51514, 400, 321, 486, 818, 341, 7006, 2771, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.042107582092285156, "compression_ratio": 1.7887931034482758, "no_speech_prob": 1.5689227438997477e-05}, {"id": 317, "seek": 186400, "start": 1864.0, "end": 1869.0, "text": " So there is a dot data and there's a self dot grad. And initially, it will be zero.", "tokens": [50364, 407, 456, 307, 257, 5893, 1412, 293, 456, 311, 257, 2698, 5893, 2771, 13, 400, 9105, 11, 309, 486, 312, 4018, 13, 50614, 50614, 400, 1604, 300, 4018, 307, 1936, 1355, 572, 1802, 13, 50814, 50814, 407, 412, 5883, 2144, 11, 321, 434, 11926, 300, 633, 2158, 775, 406, 2712, 11, 775, 406, 3345, 264, 5598, 13, 51114, 51114, 1779, 13, 1436, 498, 264, 16235, 307, 4018, 11, 300, 1355, 300, 4473, 341, 7006, 307, 406, 4473, 264, 4470, 2445, 13, 51414, 51414, 407, 538, 7576, 11, 321, 6552, 300, 264, 16235, 307, 4018, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10799967640578145, "compression_ratio": 1.7587719298245614, "no_speech_prob": 7.411123078782111e-06}, {"id": 318, "seek": 186400, "start": 1869.0, "end": 1873.0, "text": " And remember that zero is basically means no effect.", "tokens": [50364, 407, 456, 307, 257, 5893, 1412, 293, 456, 311, 257, 2698, 5893, 2771, 13, 400, 9105, 11, 309, 486, 312, 4018, 13, 50614, 50614, 400, 1604, 300, 4018, 307, 1936, 1355, 572, 1802, 13, 50814, 50814, 407, 412, 5883, 2144, 11, 321, 434, 11926, 300, 633, 2158, 775, 406, 2712, 11, 775, 406, 3345, 264, 5598, 13, 51114, 51114, 1779, 13, 1436, 498, 264, 16235, 307, 4018, 11, 300, 1355, 300, 4473, 341, 7006, 307, 406, 4473, 264, 4470, 2445, 13, 51414, 51414, 407, 538, 7576, 11, 321, 6552, 300, 264, 16235, 307, 4018, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10799967640578145, "compression_ratio": 1.7587719298245614, "no_speech_prob": 7.411123078782111e-06}, {"id": 319, "seek": 186400, "start": 1873.0, "end": 1879.0, "text": " So at initialization, we're assuming that every value does not impact, does not affect the output.", "tokens": [50364, 407, 456, 307, 257, 5893, 1412, 293, 456, 311, 257, 2698, 5893, 2771, 13, 400, 9105, 11, 309, 486, 312, 4018, 13, 50614, 50614, 400, 1604, 300, 4018, 307, 1936, 1355, 572, 1802, 13, 50814, 50814, 407, 412, 5883, 2144, 11, 321, 434, 11926, 300, 633, 2158, 775, 406, 2712, 11, 775, 406, 3345, 264, 5598, 13, 51114, 51114, 1779, 13, 1436, 498, 264, 16235, 307, 4018, 11, 300, 1355, 300, 4473, 341, 7006, 307, 406, 4473, 264, 4470, 2445, 13, 51414, 51414, 407, 538, 7576, 11, 321, 6552, 300, 264, 16235, 307, 4018, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10799967640578145, "compression_ratio": 1.7587719298245614, "no_speech_prob": 7.411123078782111e-06}, {"id": 320, "seek": 186400, "start": 1879.0, "end": 1885.0, "text": " Right. Because if the gradient is zero, that means that changing this variable is not changing the loss function.", "tokens": [50364, 407, 456, 307, 257, 5893, 1412, 293, 456, 311, 257, 2698, 5893, 2771, 13, 400, 9105, 11, 309, 486, 312, 4018, 13, 50614, 50614, 400, 1604, 300, 4018, 307, 1936, 1355, 572, 1802, 13, 50814, 50814, 407, 412, 5883, 2144, 11, 321, 434, 11926, 300, 633, 2158, 775, 406, 2712, 11, 775, 406, 3345, 264, 5598, 13, 51114, 51114, 1779, 13, 1436, 498, 264, 16235, 307, 4018, 11, 300, 1355, 300, 4473, 341, 7006, 307, 406, 4473, 264, 4470, 2445, 13, 51414, 51414, 407, 538, 7576, 11, 321, 6552, 300, 264, 16235, 307, 4018, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10799967640578145, "compression_ratio": 1.7587719298245614, "no_speech_prob": 7.411123078782111e-06}, {"id": 321, "seek": 186400, "start": 1885.0, "end": 1888.0, "text": " So by default, we assume that the gradient is zero.", "tokens": [50364, 407, 456, 307, 257, 5893, 1412, 293, 456, 311, 257, 2698, 5893, 2771, 13, 400, 9105, 11, 309, 486, 312, 4018, 13, 50614, 50614, 400, 1604, 300, 4018, 307, 1936, 1355, 572, 1802, 13, 50814, 50814, 407, 412, 5883, 2144, 11, 321, 434, 11926, 300, 633, 2158, 775, 406, 2712, 11, 775, 406, 3345, 264, 5598, 13, 51114, 51114, 1779, 13, 1436, 498, 264, 16235, 307, 4018, 11, 300, 1355, 300, 4473, 341, 7006, 307, 406, 4473, 264, 4470, 2445, 13, 51414, 51414, 407, 538, 7576, 11, 321, 6552, 300, 264, 16235, 307, 4018, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10799967640578145, "compression_ratio": 1.7587719298245614, "no_speech_prob": 7.411123078782111e-06}, {"id": 322, "seek": 188800, "start": 1888.0, "end": 1899.0, "text": " And then now that we have grad and it's zero point zero, we are going to be able to visualize it here after data.", "tokens": [50364, 400, 550, 586, 300, 321, 362, 2771, 293, 309, 311, 4018, 935, 4018, 11, 321, 366, 516, 281, 312, 1075, 281, 23273, 309, 510, 934, 1412, 13, 50914, 50914, 407, 510, 2771, 307, 935, 1451, 479, 13, 400, 341, 486, 312, 294, 300, 6919, 13, 51214, 51214, 400, 586, 321, 366, 516, 281, 312, 4099, 1293, 264, 1412, 293, 264, 2771, 5883, 1125, 300, 4018, 13, 51614, 51614, 400, 321, 366, 445, 466, 1242, 1919, 281, 8873, 264, 646, 38377, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1639294231639189, "compression_ratio": 1.71875, "no_speech_prob": 1.9637686818896327e-06}, {"id": 323, "seek": 188800, "start": 1899.0, "end": 1905.0, "text": " So here grad is point four F. And this will be in that crowd.", "tokens": [50364, 400, 550, 586, 300, 321, 362, 2771, 293, 309, 311, 4018, 935, 4018, 11, 321, 366, 516, 281, 312, 1075, 281, 23273, 309, 510, 934, 1412, 13, 50914, 50914, 407, 510, 2771, 307, 935, 1451, 479, 13, 400, 341, 486, 312, 294, 300, 6919, 13, 51214, 51214, 400, 586, 321, 366, 516, 281, 312, 4099, 1293, 264, 1412, 293, 264, 2771, 5883, 1125, 300, 4018, 13, 51614, 51614, 400, 321, 366, 445, 466, 1242, 1919, 281, 8873, 264, 646, 38377, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1639294231639189, "compression_ratio": 1.71875, "no_speech_prob": 1.9637686818896327e-06}, {"id": 324, "seek": 188800, "start": 1905.0, "end": 1913.0, "text": " And now we are going to be showing both the data and the grad initialize that zero.", "tokens": [50364, 400, 550, 586, 300, 321, 362, 2771, 293, 309, 311, 4018, 935, 4018, 11, 321, 366, 516, 281, 312, 1075, 281, 23273, 309, 510, 934, 1412, 13, 50914, 50914, 407, 510, 2771, 307, 935, 1451, 479, 13, 400, 341, 486, 312, 294, 300, 6919, 13, 51214, 51214, 400, 586, 321, 366, 516, 281, 312, 4099, 1293, 264, 1412, 293, 264, 2771, 5883, 1125, 300, 4018, 13, 51614, 51614, 400, 321, 366, 445, 466, 1242, 1919, 281, 8873, 264, 646, 38377, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1639294231639189, "compression_ratio": 1.71875, "no_speech_prob": 1.9637686818896327e-06}, {"id": 325, "seek": 188800, "start": 1913.0, "end": 1917.0, "text": " And we are just about getting ready to calculate the back propagation.", "tokens": [50364, 400, 550, 586, 300, 321, 362, 2771, 293, 309, 311, 4018, 935, 4018, 11, 321, 366, 516, 281, 312, 1075, 281, 23273, 309, 510, 934, 1412, 13, 50914, 50914, 407, 510, 2771, 307, 935, 1451, 479, 13, 400, 341, 486, 312, 294, 300, 6919, 13, 51214, 51214, 400, 586, 321, 366, 516, 281, 312, 4099, 1293, 264, 1412, 293, 264, 2771, 5883, 1125, 300, 4018, 13, 51614, 51614, 400, 321, 366, 445, 466, 1242, 1919, 281, 8873, 264, 646, 38377, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1639294231639189, "compression_ratio": 1.71875, "no_speech_prob": 1.9637686818896327e-06}, {"id": 326, "seek": 191700, "start": 1917.0, "end": 1925.0, "text": " And of course, this grad again, as I mentioned, is representing the derivative of the output in this case L with respect to this value.", "tokens": [50364, 400, 295, 1164, 11, 341, 2771, 797, 11, 382, 286, 2835, 11, 307, 13460, 264, 13760, 295, 264, 5598, 294, 341, 1389, 441, 365, 3104, 281, 341, 2158, 13, 50764, 50764, 407, 365, 3104, 281, 13, 407, 341, 307, 264, 13760, 295, 441, 365, 3104, 281, 479, 365, 3104, 281, 413, 293, 370, 322, 13, 51014, 51014, 407, 718, 311, 586, 2836, 294, 729, 2771, 2448, 293, 767, 360, 646, 38377, 16945, 13, 51214, 51214, 407, 718, 311, 722, 10623, 294, 613, 2771, 2448, 293, 722, 439, 264, 636, 412, 264, 917, 11, 382, 286, 2835, 510, 13, 51414, 51414, 2386, 11, 321, 366, 3102, 281, 2836, 294, 341, 16235, 510, 13, 51564, 51564, 407, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 441, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0790899790250338, "compression_ratio": 2.0890688259109313, "no_speech_prob": 2.429966116324067e-05}, {"id": 327, "seek": 191700, "start": 1925.0, "end": 1930.0, "text": " So with respect to. So this is the derivative of L with respect to F with respect to D and so on.", "tokens": [50364, 400, 295, 1164, 11, 341, 2771, 797, 11, 382, 286, 2835, 11, 307, 13460, 264, 13760, 295, 264, 5598, 294, 341, 1389, 441, 365, 3104, 281, 341, 2158, 13, 50764, 50764, 407, 365, 3104, 281, 13, 407, 341, 307, 264, 13760, 295, 441, 365, 3104, 281, 479, 365, 3104, 281, 413, 293, 370, 322, 13, 51014, 51014, 407, 718, 311, 586, 2836, 294, 729, 2771, 2448, 293, 767, 360, 646, 38377, 16945, 13, 51214, 51214, 407, 718, 311, 722, 10623, 294, 613, 2771, 2448, 293, 722, 439, 264, 636, 412, 264, 917, 11, 382, 286, 2835, 510, 13, 51414, 51414, 2386, 11, 321, 366, 3102, 281, 2836, 294, 341, 16235, 510, 13, 51564, 51564, 407, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 441, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0790899790250338, "compression_ratio": 2.0890688259109313, "no_speech_prob": 2.429966116324067e-05}, {"id": 328, "seek": 191700, "start": 1930.0, "end": 1934.0, "text": " So let's now fill in those gradients and actually do back propagation manually.", "tokens": [50364, 400, 295, 1164, 11, 341, 2771, 797, 11, 382, 286, 2835, 11, 307, 13460, 264, 13760, 295, 264, 5598, 294, 341, 1389, 441, 365, 3104, 281, 341, 2158, 13, 50764, 50764, 407, 365, 3104, 281, 13, 407, 341, 307, 264, 13760, 295, 441, 365, 3104, 281, 479, 365, 3104, 281, 413, 293, 370, 322, 13, 51014, 51014, 407, 718, 311, 586, 2836, 294, 729, 2771, 2448, 293, 767, 360, 646, 38377, 16945, 13, 51214, 51214, 407, 718, 311, 722, 10623, 294, 613, 2771, 2448, 293, 722, 439, 264, 636, 412, 264, 917, 11, 382, 286, 2835, 510, 13, 51414, 51414, 2386, 11, 321, 366, 3102, 281, 2836, 294, 341, 16235, 510, 13, 51564, 51564, 407, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 441, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0790899790250338, "compression_ratio": 2.0890688259109313, "no_speech_prob": 2.429966116324067e-05}, {"id": 329, "seek": 191700, "start": 1934.0, "end": 1938.0, "text": " So let's start filling in these gradients and start all the way at the end, as I mentioned here.", "tokens": [50364, 400, 295, 1164, 11, 341, 2771, 797, 11, 382, 286, 2835, 11, 307, 13460, 264, 13760, 295, 264, 5598, 294, 341, 1389, 441, 365, 3104, 281, 341, 2158, 13, 50764, 50764, 407, 365, 3104, 281, 13, 407, 341, 307, 264, 13760, 295, 441, 365, 3104, 281, 479, 365, 3104, 281, 413, 293, 370, 322, 13, 51014, 51014, 407, 718, 311, 586, 2836, 294, 729, 2771, 2448, 293, 767, 360, 646, 38377, 16945, 13, 51214, 51214, 407, 718, 311, 722, 10623, 294, 613, 2771, 2448, 293, 722, 439, 264, 636, 412, 264, 917, 11, 382, 286, 2835, 510, 13, 51414, 51414, 2386, 11, 321, 366, 3102, 281, 2836, 294, 341, 16235, 510, 13, 51564, 51564, 407, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 441, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0790899790250338, "compression_ratio": 2.0890688259109313, "no_speech_prob": 2.429966116324067e-05}, {"id": 330, "seek": 191700, "start": 1938.0, "end": 1941.0, "text": " First, we are interested to fill in this gradient here.", "tokens": [50364, 400, 295, 1164, 11, 341, 2771, 797, 11, 382, 286, 2835, 11, 307, 13460, 264, 13760, 295, 264, 5598, 294, 341, 1389, 441, 365, 3104, 281, 341, 2158, 13, 50764, 50764, 407, 365, 3104, 281, 13, 407, 341, 307, 264, 13760, 295, 441, 365, 3104, 281, 479, 365, 3104, 281, 413, 293, 370, 322, 13, 51014, 51014, 407, 718, 311, 586, 2836, 294, 729, 2771, 2448, 293, 767, 360, 646, 38377, 16945, 13, 51214, 51214, 407, 718, 311, 722, 10623, 294, 613, 2771, 2448, 293, 722, 439, 264, 636, 412, 264, 917, 11, 382, 286, 2835, 510, 13, 51414, 51414, 2386, 11, 321, 366, 3102, 281, 2836, 294, 341, 16235, 510, 13, 51564, 51564, 407, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 441, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0790899790250338, "compression_ratio": 2.0890688259109313, "no_speech_prob": 2.429966116324067e-05}, {"id": 331, "seek": 191700, "start": 1941.0, "end": 1945.0, "text": " So what is the derivative of L with respect to L?", "tokens": [50364, 400, 295, 1164, 11, 341, 2771, 797, 11, 382, 286, 2835, 11, 307, 13460, 264, 13760, 295, 264, 5598, 294, 341, 1389, 441, 365, 3104, 281, 341, 2158, 13, 50764, 50764, 407, 365, 3104, 281, 13, 407, 341, 307, 264, 13760, 295, 441, 365, 3104, 281, 479, 365, 3104, 281, 413, 293, 370, 322, 13, 51014, 51014, 407, 718, 311, 586, 2836, 294, 729, 2771, 2448, 293, 767, 360, 646, 38377, 16945, 13, 51214, 51214, 407, 718, 311, 722, 10623, 294, 613, 2771, 2448, 293, 722, 439, 264, 636, 412, 264, 917, 11, 382, 286, 2835, 510, 13, 51414, 51414, 2386, 11, 321, 366, 3102, 281, 2836, 294, 341, 16235, 510, 13, 51564, 51564, 407, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 441, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0790899790250338, "compression_ratio": 2.0890688259109313, "no_speech_prob": 2.429966116324067e-05}, {"id": 332, "seek": 194500, "start": 1945.0, "end": 1949.0, "text": " In other words, if I change L by a tiny amount H.", "tokens": [50364, 682, 661, 2283, 11, 498, 286, 1319, 441, 538, 257, 5870, 2372, 389, 13, 50564, 50564, 1012, 709, 775, 441, 1319, 30, 467, 2962, 538, 389, 13, 50814, 50814, 407, 309, 311, 24969, 293, 4412, 264, 13760, 486, 312, 472, 13, 50964, 50964, 492, 393, 11, 295, 1164, 11, 3481, 613, 420, 12539, 613, 29054, 2771, 2448, 7866, 984, 11, 445, 411, 321, 600, 1612, 949, 13, 51264, 51264, 407, 498, 286, 747, 341, 6114, 293, 286, 1884, 257, 1060, 10065, 2445, 510, 293, 829, 341, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11534527073735776, "compression_ratio": 1.56, "no_speech_prob": 5.307346873451024e-05}, {"id": 333, "seek": 194500, "start": 1949.0, "end": 1954.0, "text": " How much does L change? It changes by H.", "tokens": [50364, 682, 661, 2283, 11, 498, 286, 1319, 441, 538, 257, 5870, 2372, 389, 13, 50564, 50564, 1012, 709, 775, 441, 1319, 30, 467, 2962, 538, 389, 13, 50814, 50814, 407, 309, 311, 24969, 293, 4412, 264, 13760, 486, 312, 472, 13, 50964, 50964, 492, 393, 11, 295, 1164, 11, 3481, 613, 420, 12539, 613, 29054, 2771, 2448, 7866, 984, 11, 445, 411, 321, 600, 1612, 949, 13, 51264, 51264, 407, 498, 286, 747, 341, 6114, 293, 286, 1884, 257, 1060, 10065, 2445, 510, 293, 829, 341, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11534527073735776, "compression_ratio": 1.56, "no_speech_prob": 5.307346873451024e-05}, {"id": 334, "seek": 194500, "start": 1954.0, "end": 1957.0, "text": " So it's proportional and therefore the derivative will be one.", "tokens": [50364, 682, 661, 2283, 11, 498, 286, 1319, 441, 538, 257, 5870, 2372, 389, 13, 50564, 50564, 1012, 709, 775, 441, 1319, 30, 467, 2962, 538, 389, 13, 50814, 50814, 407, 309, 311, 24969, 293, 4412, 264, 13760, 486, 312, 472, 13, 50964, 50964, 492, 393, 11, 295, 1164, 11, 3481, 613, 420, 12539, 613, 29054, 2771, 2448, 7866, 984, 11, 445, 411, 321, 600, 1612, 949, 13, 51264, 51264, 407, 498, 286, 747, 341, 6114, 293, 286, 1884, 257, 1060, 10065, 2445, 510, 293, 829, 341, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11534527073735776, "compression_ratio": 1.56, "no_speech_prob": 5.307346873451024e-05}, {"id": 335, "seek": 194500, "start": 1957.0, "end": 1963.0, "text": " We can, of course, measure these or estimate these numerical gradients numerically, just like we've seen before.", "tokens": [50364, 682, 661, 2283, 11, 498, 286, 1319, 441, 538, 257, 5870, 2372, 389, 13, 50564, 50564, 1012, 709, 775, 441, 1319, 30, 467, 2962, 538, 389, 13, 50814, 50814, 407, 309, 311, 24969, 293, 4412, 264, 13760, 486, 312, 472, 13, 50964, 50964, 492, 393, 11, 295, 1164, 11, 3481, 613, 420, 12539, 613, 29054, 2771, 2448, 7866, 984, 11, 445, 411, 321, 600, 1612, 949, 13, 51264, 51264, 407, 498, 286, 747, 341, 6114, 293, 286, 1884, 257, 1060, 10065, 2445, 510, 293, 829, 341, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11534527073735776, "compression_ratio": 1.56, "no_speech_prob": 5.307346873451024e-05}, {"id": 336, "seek": 194500, "start": 1963.0, "end": 1971.0, "text": " So if I take this expression and I create a def lol function here and put this here.", "tokens": [50364, 682, 661, 2283, 11, 498, 286, 1319, 441, 538, 257, 5870, 2372, 389, 13, 50564, 50564, 1012, 709, 775, 441, 1319, 30, 467, 2962, 538, 389, 13, 50814, 50814, 407, 309, 311, 24969, 293, 4412, 264, 13760, 486, 312, 472, 13, 50964, 50964, 492, 393, 11, 295, 1164, 11, 3481, 613, 420, 12539, 613, 29054, 2771, 2448, 7866, 984, 11, 445, 411, 321, 600, 1612, 949, 13, 51264, 51264, 407, 498, 286, 747, 341, 6114, 293, 286, 1884, 257, 1060, 10065, 2445, 510, 293, 829, 341, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11534527073735776, "compression_ratio": 1.56, "no_speech_prob": 5.307346873451024e-05}, {"id": 337, "seek": 197100, "start": 1971.0, "end": 1977.0, "text": " The reason I'm creating a gating function lol here is because I don't want to pollute or mess up the global scope here.", "tokens": [50364, 440, 1778, 286, 478, 4084, 257, 290, 990, 2445, 10065, 510, 307, 570, 286, 500, 380, 528, 281, 6418, 1169, 420, 2082, 493, 264, 4338, 11923, 510, 13, 50664, 50664, 639, 307, 445, 733, 295, 411, 257, 707, 41085, 1859, 13, 50764, 50764, 400, 382, 291, 458, 11, 294, 15329, 11, 439, 295, 613, 486, 312, 2654, 9102, 281, 341, 2445, 13, 50964, 50964, 407, 286, 478, 406, 4473, 604, 295, 264, 4338, 11923, 510, 13, 51114, 51114, 407, 510, 11, 441, 16, 486, 312, 441, 13, 51314, 51314, 400, 550, 5055, 1791, 278, 341, 6114, 11, 321, 434, 516, 281, 909, 257, 1359, 2372, 389, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07189444808272628, "compression_ratio": 1.606425702811245, "no_speech_prob": 2.1782298063044436e-05}, {"id": 338, "seek": 197100, "start": 1977.0, "end": 1979.0, "text": " This is just kind of like a little staging area.", "tokens": [50364, 440, 1778, 286, 478, 4084, 257, 290, 990, 2445, 10065, 510, 307, 570, 286, 500, 380, 528, 281, 6418, 1169, 420, 2082, 493, 264, 4338, 11923, 510, 13, 50664, 50664, 639, 307, 445, 733, 295, 411, 257, 707, 41085, 1859, 13, 50764, 50764, 400, 382, 291, 458, 11, 294, 15329, 11, 439, 295, 613, 486, 312, 2654, 9102, 281, 341, 2445, 13, 50964, 50964, 407, 286, 478, 406, 4473, 604, 295, 264, 4338, 11923, 510, 13, 51114, 51114, 407, 510, 11, 441, 16, 486, 312, 441, 13, 51314, 51314, 400, 550, 5055, 1791, 278, 341, 6114, 11, 321, 434, 516, 281, 909, 257, 1359, 2372, 389, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07189444808272628, "compression_ratio": 1.606425702811245, "no_speech_prob": 2.1782298063044436e-05}, {"id": 339, "seek": 197100, "start": 1979.0, "end": 1983.0, "text": " And as you know, in Python, all of these will be local variables to this function.", "tokens": [50364, 440, 1778, 286, 478, 4084, 257, 290, 990, 2445, 10065, 510, 307, 570, 286, 500, 380, 528, 281, 6418, 1169, 420, 2082, 493, 264, 4338, 11923, 510, 13, 50664, 50664, 639, 307, 445, 733, 295, 411, 257, 707, 41085, 1859, 13, 50764, 50764, 400, 382, 291, 458, 11, 294, 15329, 11, 439, 295, 613, 486, 312, 2654, 9102, 281, 341, 2445, 13, 50964, 50964, 407, 286, 478, 406, 4473, 604, 295, 264, 4338, 11923, 510, 13, 51114, 51114, 407, 510, 11, 441, 16, 486, 312, 441, 13, 51314, 51314, 400, 550, 5055, 1791, 278, 341, 6114, 11, 321, 434, 516, 281, 909, 257, 1359, 2372, 389, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07189444808272628, "compression_ratio": 1.606425702811245, "no_speech_prob": 2.1782298063044436e-05}, {"id": 340, "seek": 197100, "start": 1983.0, "end": 1986.0, "text": " So I'm not changing any of the global scope here.", "tokens": [50364, 440, 1778, 286, 478, 4084, 257, 290, 990, 2445, 10065, 510, 307, 570, 286, 500, 380, 528, 281, 6418, 1169, 420, 2082, 493, 264, 4338, 11923, 510, 13, 50664, 50664, 639, 307, 445, 733, 295, 411, 257, 707, 41085, 1859, 13, 50764, 50764, 400, 382, 291, 458, 11, 294, 15329, 11, 439, 295, 613, 486, 312, 2654, 9102, 281, 341, 2445, 13, 50964, 50964, 407, 286, 478, 406, 4473, 604, 295, 264, 4338, 11923, 510, 13, 51114, 51114, 407, 510, 11, 441, 16, 486, 312, 441, 13, 51314, 51314, 400, 550, 5055, 1791, 278, 341, 6114, 11, 321, 434, 516, 281, 909, 257, 1359, 2372, 389, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07189444808272628, "compression_ratio": 1.606425702811245, "no_speech_prob": 2.1782298063044436e-05}, {"id": 341, "seek": 197100, "start": 1986.0, "end": 1990.0, "text": " So here, L1 will be L.", "tokens": [50364, 440, 1778, 286, 478, 4084, 257, 290, 990, 2445, 10065, 510, 307, 570, 286, 500, 380, 528, 281, 6418, 1169, 420, 2082, 493, 264, 4338, 11923, 510, 13, 50664, 50664, 639, 307, 445, 733, 295, 411, 257, 707, 41085, 1859, 13, 50764, 50764, 400, 382, 291, 458, 11, 294, 15329, 11, 439, 295, 613, 486, 312, 2654, 9102, 281, 341, 2445, 13, 50964, 50964, 407, 286, 478, 406, 4473, 604, 295, 264, 4338, 11923, 510, 13, 51114, 51114, 407, 510, 11, 441, 16, 486, 312, 441, 13, 51314, 51314, 400, 550, 5055, 1791, 278, 341, 6114, 11, 321, 434, 516, 281, 909, 257, 1359, 2372, 389, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07189444808272628, "compression_ratio": 1.606425702811245, "no_speech_prob": 2.1782298063044436e-05}, {"id": 342, "seek": 197100, "start": 1990.0, "end": 1998.0, "text": " And then copy pasting this expression, we're going to add a small amount H.", "tokens": [50364, 440, 1778, 286, 478, 4084, 257, 290, 990, 2445, 10065, 510, 307, 570, 286, 500, 380, 528, 281, 6418, 1169, 420, 2082, 493, 264, 4338, 11923, 510, 13, 50664, 50664, 639, 307, 445, 733, 295, 411, 257, 707, 41085, 1859, 13, 50764, 50764, 400, 382, 291, 458, 11, 294, 15329, 11, 439, 295, 613, 486, 312, 2654, 9102, 281, 341, 2445, 13, 50964, 50964, 407, 286, 478, 406, 4473, 604, 295, 264, 4338, 11923, 510, 13, 51114, 51114, 407, 510, 11, 441, 16, 486, 312, 441, 13, 51314, 51314, 400, 550, 5055, 1791, 278, 341, 6114, 11, 321, 434, 516, 281, 909, 257, 1359, 2372, 389, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07189444808272628, "compression_ratio": 1.606425702811245, "no_speech_prob": 2.1782298063044436e-05}, {"id": 343, "seek": 199800, "start": 1998.0, "end": 2001.0, "text": " For example, a right.", "tokens": [50364, 1171, 1365, 11, 257, 558, 13, 50514, 50514, 400, 341, 576, 312, 13389, 264, 13760, 295, 441, 365, 3104, 281, 257, 13, 50714, 50714, 407, 510, 341, 486, 312, 441, 17, 13, 50864, 50864, 400, 550, 321, 528, 281, 4482, 300, 13760, 13, 50964, 50964, 407, 4482, 441, 17, 3175, 441, 16, 11, 597, 307, 577, 709, 441, 3105, 293, 550, 2710, 1125, 309, 538, 389, 13, 51314, 51314, 407, 341, 307, 264, 6272, 670, 1190, 13, 51414, 51414, 400, 321, 362, 281, 312, 5026, 570, 441, 307, 257, 2158, 9984, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1011217435201009, "compression_ratio": 1.5314009661835748, "no_speech_prob": 4.8322595830541104e-05}, {"id": 344, "seek": 199800, "start": 2001.0, "end": 2005.0, "text": " And this would be measuring the derivative of L with respect to a.", "tokens": [50364, 1171, 1365, 11, 257, 558, 13, 50514, 50514, 400, 341, 576, 312, 13389, 264, 13760, 295, 441, 365, 3104, 281, 257, 13, 50714, 50714, 407, 510, 341, 486, 312, 441, 17, 13, 50864, 50864, 400, 550, 321, 528, 281, 4482, 300, 13760, 13, 50964, 50964, 407, 4482, 441, 17, 3175, 441, 16, 11, 597, 307, 577, 709, 441, 3105, 293, 550, 2710, 1125, 309, 538, 389, 13, 51314, 51314, 407, 341, 307, 264, 6272, 670, 1190, 13, 51414, 51414, 400, 321, 362, 281, 312, 5026, 570, 441, 307, 257, 2158, 9984, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1011217435201009, "compression_ratio": 1.5314009661835748, "no_speech_prob": 4.8322595830541104e-05}, {"id": 345, "seek": 199800, "start": 2005.0, "end": 2008.0, "text": " So here this will be L2.", "tokens": [50364, 1171, 1365, 11, 257, 558, 13, 50514, 50514, 400, 341, 576, 312, 13389, 264, 13760, 295, 441, 365, 3104, 281, 257, 13, 50714, 50714, 407, 510, 341, 486, 312, 441, 17, 13, 50864, 50864, 400, 550, 321, 528, 281, 4482, 300, 13760, 13, 50964, 50964, 407, 4482, 441, 17, 3175, 441, 16, 11, 597, 307, 577, 709, 441, 3105, 293, 550, 2710, 1125, 309, 538, 389, 13, 51314, 51314, 407, 341, 307, 264, 6272, 670, 1190, 13, 51414, 51414, 400, 321, 362, 281, 312, 5026, 570, 441, 307, 257, 2158, 9984, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1011217435201009, "compression_ratio": 1.5314009661835748, "no_speech_prob": 4.8322595830541104e-05}, {"id": 346, "seek": 199800, "start": 2008.0, "end": 2010.0, "text": " And then we want to print that derivative.", "tokens": [50364, 1171, 1365, 11, 257, 558, 13, 50514, 50514, 400, 341, 576, 312, 13389, 264, 13760, 295, 441, 365, 3104, 281, 257, 13, 50714, 50714, 407, 510, 341, 486, 312, 441, 17, 13, 50864, 50864, 400, 550, 321, 528, 281, 4482, 300, 13760, 13, 50964, 50964, 407, 4482, 441, 17, 3175, 441, 16, 11, 597, 307, 577, 709, 441, 3105, 293, 550, 2710, 1125, 309, 538, 389, 13, 51314, 51314, 407, 341, 307, 264, 6272, 670, 1190, 13, 51414, 51414, 400, 321, 362, 281, 312, 5026, 570, 441, 307, 257, 2158, 9984, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1011217435201009, "compression_ratio": 1.5314009661835748, "no_speech_prob": 4.8322595830541104e-05}, {"id": 347, "seek": 199800, "start": 2010.0, "end": 2017.0, "text": " So print L2 minus L1, which is how much L changed and then normalize it by H.", "tokens": [50364, 1171, 1365, 11, 257, 558, 13, 50514, 50514, 400, 341, 576, 312, 13389, 264, 13760, 295, 441, 365, 3104, 281, 257, 13, 50714, 50714, 407, 510, 341, 486, 312, 441, 17, 13, 50864, 50864, 400, 550, 321, 528, 281, 4482, 300, 13760, 13, 50964, 50964, 407, 4482, 441, 17, 3175, 441, 16, 11, 597, 307, 577, 709, 441, 3105, 293, 550, 2710, 1125, 309, 538, 389, 13, 51314, 51314, 407, 341, 307, 264, 6272, 670, 1190, 13, 51414, 51414, 400, 321, 362, 281, 312, 5026, 570, 441, 307, 257, 2158, 9984, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1011217435201009, "compression_ratio": 1.5314009661835748, "no_speech_prob": 4.8322595830541104e-05}, {"id": 348, "seek": 199800, "start": 2017.0, "end": 2019.0, "text": " So this is the rise over run.", "tokens": [50364, 1171, 1365, 11, 257, 558, 13, 50514, 50514, 400, 341, 576, 312, 13389, 264, 13760, 295, 441, 365, 3104, 281, 257, 13, 50714, 50714, 407, 510, 341, 486, 312, 441, 17, 13, 50864, 50864, 400, 550, 321, 528, 281, 4482, 300, 13760, 13, 50964, 50964, 407, 4482, 441, 17, 3175, 441, 16, 11, 597, 307, 577, 709, 441, 3105, 293, 550, 2710, 1125, 309, 538, 389, 13, 51314, 51314, 407, 341, 307, 264, 6272, 670, 1190, 13, 51414, 51414, 400, 321, 362, 281, 312, 5026, 570, 441, 307, 257, 2158, 9984, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1011217435201009, "compression_ratio": 1.5314009661835748, "no_speech_prob": 4.8322595830541104e-05}, {"id": 349, "seek": 199800, "start": 2019.0, "end": 2022.0, "text": " And we have to be careful because L is a value node.", "tokens": [50364, 1171, 1365, 11, 257, 558, 13, 50514, 50514, 400, 341, 576, 312, 13389, 264, 13760, 295, 441, 365, 3104, 281, 257, 13, 50714, 50714, 407, 510, 341, 486, 312, 441, 17, 13, 50864, 50864, 400, 550, 321, 528, 281, 4482, 300, 13760, 13, 50964, 50964, 407, 4482, 441, 17, 3175, 441, 16, 11, 597, 307, 577, 709, 441, 3105, 293, 550, 2710, 1125, 309, 538, 389, 13, 51314, 51314, 407, 341, 307, 264, 6272, 670, 1190, 13, 51414, 51414, 400, 321, 362, 281, 312, 5026, 570, 441, 307, 257, 2158, 9984, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1011217435201009, "compression_ratio": 1.5314009661835748, "no_speech_prob": 4.8322595830541104e-05}, {"id": 350, "seek": 202200, "start": 2022.0, "end": 2029.0, "text": " So we actually want its data so that these are floats divided by H.", "tokens": [50364, 407, 321, 767, 528, 1080, 1412, 370, 300, 613, 366, 37878, 6666, 538, 389, 13, 50714, 50714, 400, 341, 820, 4482, 264, 13760, 295, 441, 365, 3104, 281, 257, 11, 570, 257, 307, 264, 472, 300, 321, 42696, 257, 707, 857, 538, 389, 13, 51014, 51014, 407, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 257, 30, 51214, 51214, 467, 311, 2309, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.10371364565456614, "compression_ratio": 1.5576923076923077, "no_speech_prob": 3.8829784898553044e-05}, {"id": 351, "seek": 202200, "start": 2029.0, "end": 2035.0, "text": " And this should print the derivative of L with respect to a, because a is the one that we bumped a little bit by H.", "tokens": [50364, 407, 321, 767, 528, 1080, 1412, 370, 300, 613, 366, 37878, 6666, 538, 389, 13, 50714, 50714, 400, 341, 820, 4482, 264, 13760, 295, 441, 365, 3104, 281, 257, 11, 570, 257, 307, 264, 472, 300, 321, 42696, 257, 707, 857, 538, 389, 13, 51014, 51014, 407, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 257, 30, 51214, 51214, 467, 311, 2309, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.10371364565456614, "compression_ratio": 1.5576923076923077, "no_speech_prob": 3.8829784898553044e-05}, {"id": 352, "seek": 202200, "start": 2035.0, "end": 2039.0, "text": " So what is the derivative of L with respect to a?", "tokens": [50364, 407, 321, 767, 528, 1080, 1412, 370, 300, 613, 366, 37878, 6666, 538, 389, 13, 50714, 50714, 400, 341, 820, 4482, 264, 13760, 295, 441, 365, 3104, 281, 257, 11, 570, 257, 307, 264, 472, 300, 321, 42696, 257, 707, 857, 538, 389, 13, 51014, 51014, 407, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 257, 30, 51214, 51214, 467, 311, 2309, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.10371364565456614, "compression_ratio": 1.5576923076923077, "no_speech_prob": 3.8829784898553044e-05}, {"id": 353, "seek": 202200, "start": 2039.0, "end": 2041.0, "text": " It's six.", "tokens": [50364, 407, 321, 767, 528, 1080, 1412, 370, 300, 613, 366, 37878, 6666, 538, 389, 13, 50714, 50714, 400, 341, 820, 4482, 264, 13760, 295, 441, 365, 3104, 281, 257, 11, 570, 257, 307, 264, 472, 300, 321, 42696, 257, 707, 857, 538, 389, 13, 51014, 51014, 407, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 257, 30, 51214, 51214, 467, 311, 2309, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.10371364565456614, "compression_ratio": 1.5576923076923077, "no_speech_prob": 3.8829784898553044e-05}, {"id": 354, "seek": 204100, "start": 2041.0, "end": 2052.0, "text": " OK, and obviously, if we change L by H, then that would be here effectively.", "tokens": [50364, 2264, 11, 293, 2745, 11, 498, 321, 1319, 441, 538, 389, 11, 550, 300, 576, 312, 510, 8659, 13, 50914, 50914, 639, 1542, 534, 11411, 11, 457, 4473, 441, 538, 389, 11, 291, 536, 264, 13760, 510, 307, 472, 13, 51364, 51364, 663, 311, 733, 295, 411, 264, 3096, 1389, 295, 437, 321, 366, 884, 510, 13, 51564, 51564, 407, 1936, 11, 321, 393, 808, 493, 510, 293, 321, 393, 16945, 992, 441, 13, 7165, 281, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0912067076055015, "compression_ratio": 1.53125, "no_speech_prob": 5.0643317081267014e-05}, {"id": 355, "seek": 204100, "start": 2052.0, "end": 2061.0, "text": " This looks really awkward, but changing L by H, you see the derivative here is one.", "tokens": [50364, 2264, 11, 293, 2745, 11, 498, 321, 1319, 441, 538, 389, 11, 550, 300, 576, 312, 510, 8659, 13, 50914, 50914, 639, 1542, 534, 11411, 11, 457, 4473, 441, 538, 389, 11, 291, 536, 264, 13760, 510, 307, 472, 13, 51364, 51364, 663, 311, 733, 295, 411, 264, 3096, 1389, 295, 437, 321, 366, 884, 510, 13, 51564, 51564, 407, 1936, 11, 321, 393, 808, 493, 510, 293, 321, 393, 16945, 992, 441, 13, 7165, 281, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0912067076055015, "compression_ratio": 1.53125, "no_speech_prob": 5.0643317081267014e-05}, {"id": 356, "seek": 204100, "start": 2061.0, "end": 2065.0, "text": " That's kind of like the base case of what we are doing here.", "tokens": [50364, 2264, 11, 293, 2745, 11, 498, 321, 1319, 441, 538, 389, 11, 550, 300, 576, 312, 510, 8659, 13, 50914, 50914, 639, 1542, 534, 11411, 11, 457, 4473, 441, 538, 389, 11, 291, 536, 264, 13760, 510, 307, 472, 13, 51364, 51364, 663, 311, 733, 295, 411, 264, 3096, 1389, 295, 437, 321, 366, 884, 510, 13, 51564, 51564, 407, 1936, 11, 321, 393, 808, 493, 510, 293, 321, 393, 16945, 992, 441, 13, 7165, 281, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0912067076055015, "compression_ratio": 1.53125, "no_speech_prob": 5.0643317081267014e-05}, {"id": 357, "seek": 204100, "start": 2065.0, "end": 2070.0, "text": " So basically, we can come up here and we can manually set L.grad to one.", "tokens": [50364, 2264, 11, 293, 2745, 11, 498, 321, 1319, 441, 538, 389, 11, 550, 300, 576, 312, 510, 8659, 13, 50914, 50914, 639, 1542, 534, 11411, 11, 457, 4473, 441, 538, 389, 11, 291, 536, 264, 13760, 510, 307, 472, 13, 51364, 51364, 663, 311, 733, 295, 411, 264, 3096, 1389, 295, 437, 321, 366, 884, 510, 13, 51564, 51564, 407, 1936, 11, 321, 393, 808, 493, 510, 293, 321, 393, 16945, 992, 441, 13, 7165, 281, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0912067076055015, "compression_ratio": 1.53125, "no_speech_prob": 5.0643317081267014e-05}, {"id": 358, "seek": 207000, "start": 2070.0, "end": 2072.0, "text": " This is our manual backpropagation.", "tokens": [50364, 639, 307, 527, 9688, 646, 79, 1513, 559, 399, 13, 50464, 50464, 441, 13, 7165, 307, 472, 13, 50514, 50514, 400, 718, 311, 2182, 5131, 13, 50664, 50664, 400, 321, 603, 536, 300, 321, 6412, 294, 2771, 307, 472, 337, 441, 13, 50814, 50814, 492, 434, 586, 516, 281, 2354, 264, 646, 79, 1513, 559, 399, 13, 50914, 50914, 407, 718, 311, 510, 574, 412, 264, 33733, 295, 441, 365, 3104, 281, 413, 293, 479, 13, 51114, 51114, 961, 311, 360, 413, 700, 13, 51264, 51264, 407, 437, 321, 366, 3102, 294, 11, 498, 286, 1884, 257, 1491, 5093, 322, 510, 11, 307, 321, 1116, 411, 281, 458, 11, 1936, 11, 321, 362, 300, 441, 307, 413, 1413, 479, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09018289658331102, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.59116165782325e-05}, {"id": 359, "seek": 207000, "start": 2072.0, "end": 2073.0, "text": " L.grad is one.", "tokens": [50364, 639, 307, 527, 9688, 646, 79, 1513, 559, 399, 13, 50464, 50464, 441, 13, 7165, 307, 472, 13, 50514, 50514, 400, 718, 311, 2182, 5131, 13, 50664, 50664, 400, 321, 603, 536, 300, 321, 6412, 294, 2771, 307, 472, 337, 441, 13, 50814, 50814, 492, 434, 586, 516, 281, 2354, 264, 646, 79, 1513, 559, 399, 13, 50914, 50914, 407, 718, 311, 510, 574, 412, 264, 33733, 295, 441, 365, 3104, 281, 413, 293, 479, 13, 51114, 51114, 961, 311, 360, 413, 700, 13, 51264, 51264, 407, 437, 321, 366, 3102, 294, 11, 498, 286, 1884, 257, 1491, 5093, 322, 510, 11, 307, 321, 1116, 411, 281, 458, 11, 1936, 11, 321, 362, 300, 441, 307, 413, 1413, 479, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09018289658331102, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.59116165782325e-05}, {"id": 360, "seek": 207000, "start": 2073.0, "end": 2076.0, "text": " And let's redraw.", "tokens": [50364, 639, 307, 527, 9688, 646, 79, 1513, 559, 399, 13, 50464, 50464, 441, 13, 7165, 307, 472, 13, 50514, 50514, 400, 718, 311, 2182, 5131, 13, 50664, 50664, 400, 321, 603, 536, 300, 321, 6412, 294, 2771, 307, 472, 337, 441, 13, 50814, 50814, 492, 434, 586, 516, 281, 2354, 264, 646, 79, 1513, 559, 399, 13, 50914, 50914, 407, 718, 311, 510, 574, 412, 264, 33733, 295, 441, 365, 3104, 281, 413, 293, 479, 13, 51114, 51114, 961, 311, 360, 413, 700, 13, 51264, 51264, 407, 437, 321, 366, 3102, 294, 11, 498, 286, 1884, 257, 1491, 5093, 322, 510, 11, 307, 321, 1116, 411, 281, 458, 11, 1936, 11, 321, 362, 300, 441, 307, 413, 1413, 479, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09018289658331102, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.59116165782325e-05}, {"id": 361, "seek": 207000, "start": 2076.0, "end": 2079.0, "text": " And we'll see that we filled in grad is one for L.", "tokens": [50364, 639, 307, 527, 9688, 646, 79, 1513, 559, 399, 13, 50464, 50464, 441, 13, 7165, 307, 472, 13, 50514, 50514, 400, 718, 311, 2182, 5131, 13, 50664, 50664, 400, 321, 603, 536, 300, 321, 6412, 294, 2771, 307, 472, 337, 441, 13, 50814, 50814, 492, 434, 586, 516, 281, 2354, 264, 646, 79, 1513, 559, 399, 13, 50914, 50914, 407, 718, 311, 510, 574, 412, 264, 33733, 295, 441, 365, 3104, 281, 413, 293, 479, 13, 51114, 51114, 961, 311, 360, 413, 700, 13, 51264, 51264, 407, 437, 321, 366, 3102, 294, 11, 498, 286, 1884, 257, 1491, 5093, 322, 510, 11, 307, 321, 1116, 411, 281, 458, 11, 1936, 11, 321, 362, 300, 441, 307, 413, 1413, 479, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09018289658331102, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.59116165782325e-05}, {"id": 362, "seek": 207000, "start": 2079.0, "end": 2081.0, "text": " We're now going to continue the backpropagation.", "tokens": [50364, 639, 307, 527, 9688, 646, 79, 1513, 559, 399, 13, 50464, 50464, 441, 13, 7165, 307, 472, 13, 50514, 50514, 400, 718, 311, 2182, 5131, 13, 50664, 50664, 400, 321, 603, 536, 300, 321, 6412, 294, 2771, 307, 472, 337, 441, 13, 50814, 50814, 492, 434, 586, 516, 281, 2354, 264, 646, 79, 1513, 559, 399, 13, 50914, 50914, 407, 718, 311, 510, 574, 412, 264, 33733, 295, 441, 365, 3104, 281, 413, 293, 479, 13, 51114, 51114, 961, 311, 360, 413, 700, 13, 51264, 51264, 407, 437, 321, 366, 3102, 294, 11, 498, 286, 1884, 257, 1491, 5093, 322, 510, 11, 307, 321, 1116, 411, 281, 458, 11, 1936, 11, 321, 362, 300, 441, 307, 413, 1413, 479, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09018289658331102, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.59116165782325e-05}, {"id": 363, "seek": 207000, "start": 2081.0, "end": 2085.0, "text": " So let's here look at the derivatives of L with respect to D and F.", "tokens": [50364, 639, 307, 527, 9688, 646, 79, 1513, 559, 399, 13, 50464, 50464, 441, 13, 7165, 307, 472, 13, 50514, 50514, 400, 718, 311, 2182, 5131, 13, 50664, 50664, 400, 321, 603, 536, 300, 321, 6412, 294, 2771, 307, 472, 337, 441, 13, 50814, 50814, 492, 434, 586, 516, 281, 2354, 264, 646, 79, 1513, 559, 399, 13, 50914, 50914, 407, 718, 311, 510, 574, 412, 264, 33733, 295, 441, 365, 3104, 281, 413, 293, 479, 13, 51114, 51114, 961, 311, 360, 413, 700, 13, 51264, 51264, 407, 437, 321, 366, 3102, 294, 11, 498, 286, 1884, 257, 1491, 5093, 322, 510, 11, 307, 321, 1116, 411, 281, 458, 11, 1936, 11, 321, 362, 300, 441, 307, 413, 1413, 479, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09018289658331102, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.59116165782325e-05}, {"id": 364, "seek": 207000, "start": 2085.0, "end": 2088.0, "text": " Let's do D first.", "tokens": [50364, 639, 307, 527, 9688, 646, 79, 1513, 559, 399, 13, 50464, 50464, 441, 13, 7165, 307, 472, 13, 50514, 50514, 400, 718, 311, 2182, 5131, 13, 50664, 50664, 400, 321, 603, 536, 300, 321, 6412, 294, 2771, 307, 472, 337, 441, 13, 50814, 50814, 492, 434, 586, 516, 281, 2354, 264, 646, 79, 1513, 559, 399, 13, 50914, 50914, 407, 718, 311, 510, 574, 412, 264, 33733, 295, 441, 365, 3104, 281, 413, 293, 479, 13, 51114, 51114, 961, 311, 360, 413, 700, 13, 51264, 51264, 407, 437, 321, 366, 3102, 294, 11, 498, 286, 1884, 257, 1491, 5093, 322, 510, 11, 307, 321, 1116, 411, 281, 458, 11, 1936, 11, 321, 362, 300, 441, 307, 413, 1413, 479, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09018289658331102, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.59116165782325e-05}, {"id": 365, "seek": 207000, "start": 2088.0, "end": 2094.0, "text": " So what we are interested in, if I create a markdown on here, is we'd like to know, basically, we have that L is D times F.", "tokens": [50364, 639, 307, 527, 9688, 646, 79, 1513, 559, 399, 13, 50464, 50464, 441, 13, 7165, 307, 472, 13, 50514, 50514, 400, 718, 311, 2182, 5131, 13, 50664, 50664, 400, 321, 603, 536, 300, 321, 6412, 294, 2771, 307, 472, 337, 441, 13, 50814, 50814, 492, 434, 586, 516, 281, 2354, 264, 646, 79, 1513, 559, 399, 13, 50914, 50914, 407, 718, 311, 510, 574, 412, 264, 33733, 295, 441, 365, 3104, 281, 413, 293, 479, 13, 51114, 51114, 961, 311, 360, 413, 700, 13, 51264, 51264, 407, 437, 321, 366, 3102, 294, 11, 498, 286, 1884, 257, 1491, 5093, 322, 510, 11, 307, 321, 1116, 411, 281, 458, 11, 1936, 11, 321, 362, 300, 441, 307, 413, 1413, 479, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09018289658331102, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.59116165782325e-05}, {"id": 366, "seek": 209400, "start": 2094.0, "end": 2100.0, "text": " And we'd like to know what is DL by DD.", "tokens": [50364, 400, 321, 1116, 411, 281, 458, 437, 307, 413, 43, 538, 30778, 13, 50664, 50664, 708, 307, 300, 30, 50764, 50764, 400, 498, 291, 458, 428, 33400, 11, 441, 307, 413, 1413, 479, 13, 50864, 50864, 407, 437, 307, 413, 43, 538, 30778, 30, 50964, 50964, 467, 576, 312, 479, 13, 51064, 51064, 400, 498, 291, 500, 380, 1697, 385, 11, 321, 393, 611, 445, 28446, 309, 570, 264, 8177, 576, 312, 6457, 15325, 13, 51314, 51314, 492, 352, 281, 264, 7123, 295, 264, 13760, 11, 597, 307, 479, 295, 1783, 1804, 389, 3175, 479, 295, 1783, 9845, 389, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07322739179317768, "compression_ratio": 1.5514018691588785, "no_speech_prob": 4.006164817837998e-05}, {"id": 367, "seek": 209400, "start": 2100.0, "end": 2102.0, "text": " What is that?", "tokens": [50364, 400, 321, 1116, 411, 281, 458, 437, 307, 413, 43, 538, 30778, 13, 50664, 50664, 708, 307, 300, 30, 50764, 50764, 400, 498, 291, 458, 428, 33400, 11, 441, 307, 413, 1413, 479, 13, 50864, 50864, 407, 437, 307, 413, 43, 538, 30778, 30, 50964, 50964, 467, 576, 312, 479, 13, 51064, 51064, 400, 498, 291, 500, 380, 1697, 385, 11, 321, 393, 611, 445, 28446, 309, 570, 264, 8177, 576, 312, 6457, 15325, 13, 51314, 51314, 492, 352, 281, 264, 7123, 295, 264, 13760, 11, 597, 307, 479, 295, 1783, 1804, 389, 3175, 479, 295, 1783, 9845, 389, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07322739179317768, "compression_ratio": 1.5514018691588785, "no_speech_prob": 4.006164817837998e-05}, {"id": 368, "seek": 209400, "start": 2102.0, "end": 2104.0, "text": " And if you know your calculus, L is D times F.", "tokens": [50364, 400, 321, 1116, 411, 281, 458, 437, 307, 413, 43, 538, 30778, 13, 50664, 50664, 708, 307, 300, 30, 50764, 50764, 400, 498, 291, 458, 428, 33400, 11, 441, 307, 413, 1413, 479, 13, 50864, 50864, 407, 437, 307, 413, 43, 538, 30778, 30, 50964, 50964, 467, 576, 312, 479, 13, 51064, 51064, 400, 498, 291, 500, 380, 1697, 385, 11, 321, 393, 611, 445, 28446, 309, 570, 264, 8177, 576, 312, 6457, 15325, 13, 51314, 51314, 492, 352, 281, 264, 7123, 295, 264, 13760, 11, 597, 307, 479, 295, 1783, 1804, 389, 3175, 479, 295, 1783, 9845, 389, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07322739179317768, "compression_ratio": 1.5514018691588785, "no_speech_prob": 4.006164817837998e-05}, {"id": 369, "seek": 209400, "start": 2104.0, "end": 2106.0, "text": " So what is DL by DD?", "tokens": [50364, 400, 321, 1116, 411, 281, 458, 437, 307, 413, 43, 538, 30778, 13, 50664, 50664, 708, 307, 300, 30, 50764, 50764, 400, 498, 291, 458, 428, 33400, 11, 441, 307, 413, 1413, 479, 13, 50864, 50864, 407, 437, 307, 413, 43, 538, 30778, 30, 50964, 50964, 467, 576, 312, 479, 13, 51064, 51064, 400, 498, 291, 500, 380, 1697, 385, 11, 321, 393, 611, 445, 28446, 309, 570, 264, 8177, 576, 312, 6457, 15325, 13, 51314, 51314, 492, 352, 281, 264, 7123, 295, 264, 13760, 11, 597, 307, 479, 295, 1783, 1804, 389, 3175, 479, 295, 1783, 9845, 389, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07322739179317768, "compression_ratio": 1.5514018691588785, "no_speech_prob": 4.006164817837998e-05}, {"id": 370, "seek": 209400, "start": 2106.0, "end": 2108.0, "text": " It would be F.", "tokens": [50364, 400, 321, 1116, 411, 281, 458, 437, 307, 413, 43, 538, 30778, 13, 50664, 50664, 708, 307, 300, 30, 50764, 50764, 400, 498, 291, 458, 428, 33400, 11, 441, 307, 413, 1413, 479, 13, 50864, 50864, 407, 437, 307, 413, 43, 538, 30778, 30, 50964, 50964, 467, 576, 312, 479, 13, 51064, 51064, 400, 498, 291, 500, 380, 1697, 385, 11, 321, 393, 611, 445, 28446, 309, 570, 264, 8177, 576, 312, 6457, 15325, 13, 51314, 51314, 492, 352, 281, 264, 7123, 295, 264, 13760, 11, 597, 307, 479, 295, 1783, 1804, 389, 3175, 479, 295, 1783, 9845, 389, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07322739179317768, "compression_ratio": 1.5514018691588785, "no_speech_prob": 4.006164817837998e-05}, {"id": 371, "seek": 209400, "start": 2108.0, "end": 2113.0, "text": " And if you don't believe me, we can also just derive it because the proof would be fairly straightforward.", "tokens": [50364, 400, 321, 1116, 411, 281, 458, 437, 307, 413, 43, 538, 30778, 13, 50664, 50664, 708, 307, 300, 30, 50764, 50764, 400, 498, 291, 458, 428, 33400, 11, 441, 307, 413, 1413, 479, 13, 50864, 50864, 407, 437, 307, 413, 43, 538, 30778, 30, 50964, 50964, 467, 576, 312, 479, 13, 51064, 51064, 400, 498, 291, 500, 380, 1697, 385, 11, 321, 393, 611, 445, 28446, 309, 570, 264, 8177, 576, 312, 6457, 15325, 13, 51314, 51314, 492, 352, 281, 264, 7123, 295, 264, 13760, 11, 597, 307, 479, 295, 1783, 1804, 389, 3175, 479, 295, 1783, 9845, 389, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07322739179317768, "compression_ratio": 1.5514018691588785, "no_speech_prob": 4.006164817837998e-05}, {"id": 372, "seek": 209400, "start": 2113.0, "end": 2122.0, "text": " We go to the definition of the derivative, which is F of X plus H minus F of X divide H.", "tokens": [50364, 400, 321, 1116, 411, 281, 458, 437, 307, 413, 43, 538, 30778, 13, 50664, 50664, 708, 307, 300, 30, 50764, 50764, 400, 498, 291, 458, 428, 33400, 11, 441, 307, 413, 1413, 479, 13, 50864, 50864, 407, 437, 307, 413, 43, 538, 30778, 30, 50964, 50964, 467, 576, 312, 479, 13, 51064, 51064, 400, 498, 291, 500, 380, 1697, 385, 11, 321, 393, 611, 445, 28446, 309, 570, 264, 8177, 576, 312, 6457, 15325, 13, 51314, 51314, 492, 352, 281, 264, 7123, 295, 264, 13760, 11, 597, 307, 479, 295, 1783, 1804, 389, 3175, 479, 295, 1783, 9845, 389, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07322739179317768, "compression_ratio": 1.5514018691588785, "no_speech_prob": 4.006164817837998e-05}, {"id": 373, "seek": 212200, "start": 2122.0, "end": 2126.0, "text": " As a limit, limit of H goes to zero of this kind of expression.", "tokens": [50364, 1018, 257, 4948, 11, 4948, 295, 389, 1709, 281, 4018, 295, 341, 733, 295, 6114, 13, 50564, 50564, 407, 562, 321, 362, 441, 307, 413, 1413, 479, 11, 550, 5662, 413, 538, 389, 576, 976, 505, 264, 5598, 295, 413, 1804, 389, 1413, 479, 13, 51064, 51064, 663, 311, 1936, 479, 295, 1783, 1804, 389, 11, 558, 30, 51214, 51214, 2829, 301, 413, 1413, 479, 13, 51364, 51364, 400, 550, 9845, 389, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08691320171603908, "compression_ratio": 1.4142011834319526, "no_speech_prob": 1.0289288184139878e-05}, {"id": 374, "seek": 212200, "start": 2126.0, "end": 2136.0, "text": " So when we have L is D times F, then increasing D by H would give us the output of D plus H times F.", "tokens": [50364, 1018, 257, 4948, 11, 4948, 295, 389, 1709, 281, 4018, 295, 341, 733, 295, 6114, 13, 50564, 50564, 407, 562, 321, 362, 441, 307, 413, 1413, 479, 11, 550, 5662, 413, 538, 389, 576, 976, 505, 264, 5598, 295, 413, 1804, 389, 1413, 479, 13, 51064, 51064, 663, 311, 1936, 479, 295, 1783, 1804, 389, 11, 558, 30, 51214, 51214, 2829, 301, 413, 1413, 479, 13, 51364, 51364, 400, 550, 9845, 389, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08691320171603908, "compression_ratio": 1.4142011834319526, "no_speech_prob": 1.0289288184139878e-05}, {"id": 375, "seek": 212200, "start": 2136.0, "end": 2139.0, "text": " That's basically F of X plus H, right?", "tokens": [50364, 1018, 257, 4948, 11, 4948, 295, 389, 1709, 281, 4018, 295, 341, 733, 295, 6114, 13, 50564, 50564, 407, 562, 321, 362, 441, 307, 413, 1413, 479, 11, 550, 5662, 413, 538, 389, 576, 976, 505, 264, 5598, 295, 413, 1804, 389, 1413, 479, 13, 51064, 51064, 663, 311, 1936, 479, 295, 1783, 1804, 389, 11, 558, 30, 51214, 51214, 2829, 301, 413, 1413, 479, 13, 51364, 51364, 400, 550, 9845, 389, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08691320171603908, "compression_ratio": 1.4142011834319526, "no_speech_prob": 1.0289288184139878e-05}, {"id": 376, "seek": 212200, "start": 2139.0, "end": 2142.0, "text": " Minus D times F.", "tokens": [50364, 1018, 257, 4948, 11, 4948, 295, 389, 1709, 281, 4018, 295, 341, 733, 295, 6114, 13, 50564, 50564, 407, 562, 321, 362, 441, 307, 413, 1413, 479, 11, 550, 5662, 413, 538, 389, 576, 976, 505, 264, 5598, 295, 413, 1804, 389, 1413, 479, 13, 51064, 51064, 663, 311, 1936, 479, 295, 1783, 1804, 389, 11, 558, 30, 51214, 51214, 2829, 301, 413, 1413, 479, 13, 51364, 51364, 400, 550, 9845, 389, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08691320171603908, "compression_ratio": 1.4142011834319526, "no_speech_prob": 1.0289288184139878e-05}, {"id": 377, "seek": 212200, "start": 2142.0, "end": 2144.0, "text": " And then divide H.", "tokens": [50364, 1018, 257, 4948, 11, 4948, 295, 389, 1709, 281, 4018, 295, 341, 733, 295, 6114, 13, 50564, 50564, 407, 562, 321, 362, 441, 307, 413, 1413, 479, 11, 550, 5662, 413, 538, 389, 576, 976, 505, 264, 5598, 295, 413, 1804, 389, 1413, 479, 13, 51064, 51064, 663, 311, 1936, 479, 295, 1783, 1804, 389, 11, 558, 30, 51214, 51214, 2829, 301, 413, 1413, 479, 13, 51364, 51364, 400, 550, 9845, 389, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08691320171603908, "compression_ratio": 1.4142011834319526, "no_speech_prob": 1.0289288184139878e-05}, {"id": 378, "seek": 214400, "start": 2144.0, "end": 2152.0, "text": " And symbolically, expanding out here, we would have basically D times F plus H times F minus D times F divide H.", "tokens": [50364, 400, 5986, 984, 11, 14702, 484, 510, 11, 321, 576, 362, 1936, 413, 1413, 479, 1804, 389, 1413, 479, 3175, 413, 1413, 479, 9845, 389, 13, 50764, 50764, 400, 550, 291, 536, 577, 264, 48336, 3175, 48336, 393, 66, 1625, 13, 50914, 50914, 407, 291, 434, 1411, 365, 389, 1413, 479, 9845, 389, 11, 597, 307, 479, 13, 51164, 51164, 407, 294, 264, 4948, 382, 389, 1709, 281, 4018, 295, 11, 291, 458, 11, 13760, 7123, 11, 321, 445, 483, 479, 294, 264, 1389, 295, 413, 1413, 479, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07094294025051978, "compression_ratio": 1.6130653266331658, "no_speech_prob": 1.4510093024000525e-05}, {"id": 379, "seek": 214400, "start": 2152.0, "end": 2155.0, "text": " And then you see how the DF minus DF cancels.", "tokens": [50364, 400, 5986, 984, 11, 14702, 484, 510, 11, 321, 576, 362, 1936, 413, 1413, 479, 1804, 389, 1413, 479, 3175, 413, 1413, 479, 9845, 389, 13, 50764, 50764, 400, 550, 291, 536, 577, 264, 48336, 3175, 48336, 393, 66, 1625, 13, 50914, 50914, 407, 291, 434, 1411, 365, 389, 1413, 479, 9845, 389, 11, 597, 307, 479, 13, 51164, 51164, 407, 294, 264, 4948, 382, 389, 1709, 281, 4018, 295, 11, 291, 458, 11, 13760, 7123, 11, 321, 445, 483, 479, 294, 264, 1389, 295, 413, 1413, 479, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07094294025051978, "compression_ratio": 1.6130653266331658, "no_speech_prob": 1.4510093024000525e-05}, {"id": 380, "seek": 214400, "start": 2155.0, "end": 2160.0, "text": " So you're left with H times F divide H, which is F.", "tokens": [50364, 400, 5986, 984, 11, 14702, 484, 510, 11, 321, 576, 362, 1936, 413, 1413, 479, 1804, 389, 1413, 479, 3175, 413, 1413, 479, 9845, 389, 13, 50764, 50764, 400, 550, 291, 536, 577, 264, 48336, 3175, 48336, 393, 66, 1625, 13, 50914, 50914, 407, 291, 434, 1411, 365, 389, 1413, 479, 9845, 389, 11, 597, 307, 479, 13, 51164, 51164, 407, 294, 264, 4948, 382, 389, 1709, 281, 4018, 295, 11, 291, 458, 11, 13760, 7123, 11, 321, 445, 483, 479, 294, 264, 1389, 295, 413, 1413, 479, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07094294025051978, "compression_ratio": 1.6130653266331658, "no_speech_prob": 1.4510093024000525e-05}, {"id": 381, "seek": 214400, "start": 2160.0, "end": 2172.0, "text": " So in the limit as H goes to zero of, you know, derivative definition, we just get F in the case of D times F.", "tokens": [50364, 400, 5986, 984, 11, 14702, 484, 510, 11, 321, 576, 362, 1936, 413, 1413, 479, 1804, 389, 1413, 479, 3175, 413, 1413, 479, 9845, 389, 13, 50764, 50764, 400, 550, 291, 536, 577, 264, 48336, 3175, 48336, 393, 66, 1625, 13, 50914, 50914, 407, 291, 434, 1411, 365, 389, 1413, 479, 9845, 389, 11, 597, 307, 479, 13, 51164, 51164, 407, 294, 264, 4948, 382, 389, 1709, 281, 4018, 295, 11, 291, 458, 11, 13760, 7123, 11, 321, 445, 483, 479, 294, 264, 1389, 295, 413, 1413, 479, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07094294025051978, "compression_ratio": 1.6130653266331658, "no_speech_prob": 1.4510093024000525e-05}, {"id": 382, "seek": 217200, "start": 2172.0, "end": 2178.0, "text": " So symmetrically, DL by DF will just be D.", "tokens": [50364, 407, 14232, 27965, 984, 11, 413, 43, 538, 48336, 486, 445, 312, 413, 13, 50664, 50664, 407, 437, 321, 362, 307, 300, 479, 5893, 2771, 321, 536, 586, 307, 445, 264, 2158, 295, 413, 11, 597, 307, 1451, 13, 51164, 51164, 400, 321, 536, 300, 413, 5893, 2771, 307, 445, 264, 2158, 295, 479, 13, 51564, 51564, 400, 370, 264, 2158, 295, 479, 307, 3671, 732, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08894732300664338, "compression_ratio": 1.583941605839416, "no_speech_prob": 3.50078835253953e-06}, {"id": 383, "seek": 217200, "start": 2178.0, "end": 2188.0, "text": " So what we have is that F dot grad we see now is just the value of D, which is four.", "tokens": [50364, 407, 14232, 27965, 984, 11, 413, 43, 538, 48336, 486, 445, 312, 413, 13, 50664, 50664, 407, 437, 321, 362, 307, 300, 479, 5893, 2771, 321, 536, 586, 307, 445, 264, 2158, 295, 413, 11, 597, 307, 1451, 13, 51164, 51164, 400, 321, 536, 300, 413, 5893, 2771, 307, 445, 264, 2158, 295, 479, 13, 51564, 51564, 400, 370, 264, 2158, 295, 479, 307, 3671, 732, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08894732300664338, "compression_ratio": 1.583941605839416, "no_speech_prob": 3.50078835253953e-06}, {"id": 384, "seek": 217200, "start": 2188.0, "end": 2196.0, "text": " And we see that D dot grad is just the value of F.", "tokens": [50364, 407, 14232, 27965, 984, 11, 413, 43, 538, 48336, 486, 445, 312, 413, 13, 50664, 50664, 407, 437, 321, 362, 307, 300, 479, 5893, 2771, 321, 536, 586, 307, 445, 264, 2158, 295, 413, 11, 597, 307, 1451, 13, 51164, 51164, 400, 321, 536, 300, 413, 5893, 2771, 307, 445, 264, 2158, 295, 479, 13, 51564, 51564, 400, 370, 264, 2158, 295, 479, 307, 3671, 732, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08894732300664338, "compression_ratio": 1.583941605839416, "no_speech_prob": 3.50078835253953e-06}, {"id": 385, "seek": 217200, "start": 2196.0, "end": 2201.0, "text": " And so the value of F is negative two.", "tokens": [50364, 407, 14232, 27965, 984, 11, 413, 43, 538, 48336, 486, 445, 312, 413, 13, 50664, 50664, 407, 437, 321, 362, 307, 300, 479, 5893, 2771, 321, 536, 586, 307, 445, 264, 2158, 295, 413, 11, 597, 307, 1451, 13, 51164, 51164, 400, 321, 536, 300, 413, 5893, 2771, 307, 445, 264, 2158, 295, 479, 13, 51564, 51564, 400, 370, 264, 2158, 295, 479, 307, 3671, 732, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08894732300664338, "compression_ratio": 1.583941605839416, "no_speech_prob": 3.50078835253953e-06}, {"id": 386, "seek": 220100, "start": 2201.0, "end": 2205.0, "text": " So we'll set those manually.", "tokens": [50364, 407, 321, 603, 992, 729, 16945, 13, 50564, 50564, 961, 385, 23525, 341, 1491, 5093, 9984, 293, 550, 718, 311, 2182, 5131, 437, 321, 362, 13, 50864, 50864, 2264, 13, 400, 718, 311, 445, 652, 988, 300, 613, 645, 3006, 13, 51014, 51014, 407, 321, 1643, 281, 519, 300, 413, 43, 538, 30778, 307, 3671, 732, 13, 51164, 51164, 407, 718, 311, 3834, 1520, 13, 51314, 51314, 961, 385, 23525, 341, 1804, 389, 490, 949, 13, 51414, 51414, 400, 586, 321, 528, 264, 13760, 365, 3104, 281, 479, 13, 51564, 51564, 407, 718, 311, 445, 808, 510, 562, 286, 1884, 479, 293, 718, 311, 360, 257, 1804, 389, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07285117266470925, "compression_ratio": 1.6379310344827587, "no_speech_prob": 1.5206652278720867e-05}, {"id": 387, "seek": 220100, "start": 2205.0, "end": 2211.0, "text": " Let me erase this markdown node and then let's redraw what we have.", "tokens": [50364, 407, 321, 603, 992, 729, 16945, 13, 50564, 50564, 961, 385, 23525, 341, 1491, 5093, 9984, 293, 550, 718, 311, 2182, 5131, 437, 321, 362, 13, 50864, 50864, 2264, 13, 400, 718, 311, 445, 652, 988, 300, 613, 645, 3006, 13, 51014, 51014, 407, 321, 1643, 281, 519, 300, 413, 43, 538, 30778, 307, 3671, 732, 13, 51164, 51164, 407, 718, 311, 3834, 1520, 13, 51314, 51314, 961, 385, 23525, 341, 1804, 389, 490, 949, 13, 51414, 51414, 400, 586, 321, 528, 264, 13760, 365, 3104, 281, 479, 13, 51564, 51564, 407, 718, 311, 445, 808, 510, 562, 286, 1884, 479, 293, 718, 311, 360, 257, 1804, 389, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07285117266470925, "compression_ratio": 1.6379310344827587, "no_speech_prob": 1.5206652278720867e-05}, {"id": 388, "seek": 220100, "start": 2211.0, "end": 2214.0, "text": " OK. And let's just make sure that these were correct.", "tokens": [50364, 407, 321, 603, 992, 729, 16945, 13, 50564, 50564, 961, 385, 23525, 341, 1491, 5093, 9984, 293, 550, 718, 311, 2182, 5131, 437, 321, 362, 13, 50864, 50864, 2264, 13, 400, 718, 311, 445, 652, 988, 300, 613, 645, 3006, 13, 51014, 51014, 407, 321, 1643, 281, 519, 300, 413, 43, 538, 30778, 307, 3671, 732, 13, 51164, 51164, 407, 718, 311, 3834, 1520, 13, 51314, 51314, 961, 385, 23525, 341, 1804, 389, 490, 949, 13, 51414, 51414, 400, 586, 321, 528, 264, 13760, 365, 3104, 281, 479, 13, 51564, 51564, 407, 718, 311, 445, 808, 510, 562, 286, 1884, 479, 293, 718, 311, 360, 257, 1804, 389, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07285117266470925, "compression_ratio": 1.6379310344827587, "no_speech_prob": 1.5206652278720867e-05}, {"id": 389, "seek": 220100, "start": 2214.0, "end": 2217.0, "text": " So we seem to think that DL by DD is negative two.", "tokens": [50364, 407, 321, 603, 992, 729, 16945, 13, 50564, 50564, 961, 385, 23525, 341, 1491, 5093, 9984, 293, 550, 718, 311, 2182, 5131, 437, 321, 362, 13, 50864, 50864, 2264, 13, 400, 718, 311, 445, 652, 988, 300, 613, 645, 3006, 13, 51014, 51014, 407, 321, 1643, 281, 519, 300, 413, 43, 538, 30778, 307, 3671, 732, 13, 51164, 51164, 407, 718, 311, 3834, 1520, 13, 51314, 51314, 961, 385, 23525, 341, 1804, 389, 490, 949, 13, 51414, 51414, 400, 586, 321, 528, 264, 13760, 365, 3104, 281, 479, 13, 51564, 51564, 407, 718, 311, 445, 808, 510, 562, 286, 1884, 479, 293, 718, 311, 360, 257, 1804, 389, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07285117266470925, "compression_ratio": 1.6379310344827587, "no_speech_prob": 1.5206652278720867e-05}, {"id": 390, "seek": 220100, "start": 2217.0, "end": 2220.0, "text": " So let's double check.", "tokens": [50364, 407, 321, 603, 992, 729, 16945, 13, 50564, 50564, 961, 385, 23525, 341, 1491, 5093, 9984, 293, 550, 718, 311, 2182, 5131, 437, 321, 362, 13, 50864, 50864, 2264, 13, 400, 718, 311, 445, 652, 988, 300, 613, 645, 3006, 13, 51014, 51014, 407, 321, 1643, 281, 519, 300, 413, 43, 538, 30778, 307, 3671, 732, 13, 51164, 51164, 407, 718, 311, 3834, 1520, 13, 51314, 51314, 961, 385, 23525, 341, 1804, 389, 490, 949, 13, 51414, 51414, 400, 586, 321, 528, 264, 13760, 365, 3104, 281, 479, 13, 51564, 51564, 407, 718, 311, 445, 808, 510, 562, 286, 1884, 479, 293, 718, 311, 360, 257, 1804, 389, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07285117266470925, "compression_ratio": 1.6379310344827587, "no_speech_prob": 1.5206652278720867e-05}, {"id": 391, "seek": 220100, "start": 2220.0, "end": 2222.0, "text": " Let me erase this plus H from before.", "tokens": [50364, 407, 321, 603, 992, 729, 16945, 13, 50564, 50564, 961, 385, 23525, 341, 1491, 5093, 9984, 293, 550, 718, 311, 2182, 5131, 437, 321, 362, 13, 50864, 50864, 2264, 13, 400, 718, 311, 445, 652, 988, 300, 613, 645, 3006, 13, 51014, 51014, 407, 321, 1643, 281, 519, 300, 413, 43, 538, 30778, 307, 3671, 732, 13, 51164, 51164, 407, 718, 311, 3834, 1520, 13, 51314, 51314, 961, 385, 23525, 341, 1804, 389, 490, 949, 13, 51414, 51414, 400, 586, 321, 528, 264, 13760, 365, 3104, 281, 479, 13, 51564, 51564, 407, 718, 311, 445, 808, 510, 562, 286, 1884, 479, 293, 718, 311, 360, 257, 1804, 389, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07285117266470925, "compression_ratio": 1.6379310344827587, "no_speech_prob": 1.5206652278720867e-05}, {"id": 392, "seek": 220100, "start": 2222.0, "end": 2225.0, "text": " And now we want the derivative with respect to F.", "tokens": [50364, 407, 321, 603, 992, 729, 16945, 13, 50564, 50564, 961, 385, 23525, 341, 1491, 5093, 9984, 293, 550, 718, 311, 2182, 5131, 437, 321, 362, 13, 50864, 50864, 2264, 13, 400, 718, 311, 445, 652, 988, 300, 613, 645, 3006, 13, 51014, 51014, 407, 321, 1643, 281, 519, 300, 413, 43, 538, 30778, 307, 3671, 732, 13, 51164, 51164, 407, 718, 311, 3834, 1520, 13, 51314, 51314, 961, 385, 23525, 341, 1804, 389, 490, 949, 13, 51414, 51414, 400, 586, 321, 528, 264, 13760, 365, 3104, 281, 479, 13, 51564, 51564, 407, 718, 311, 445, 808, 510, 562, 286, 1884, 479, 293, 718, 311, 360, 257, 1804, 389, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07285117266470925, "compression_ratio": 1.6379310344827587, "no_speech_prob": 1.5206652278720867e-05}, {"id": 393, "seek": 220100, "start": 2225.0, "end": 2229.0, "text": " So let's just come here when I create F and let's do a plus H here.", "tokens": [50364, 407, 321, 603, 992, 729, 16945, 13, 50564, 50564, 961, 385, 23525, 341, 1491, 5093, 9984, 293, 550, 718, 311, 2182, 5131, 437, 321, 362, 13, 50864, 50864, 2264, 13, 400, 718, 311, 445, 652, 988, 300, 613, 645, 3006, 13, 51014, 51014, 407, 321, 1643, 281, 519, 300, 413, 43, 538, 30778, 307, 3671, 732, 13, 51164, 51164, 407, 718, 311, 3834, 1520, 13, 51314, 51314, 961, 385, 23525, 341, 1804, 389, 490, 949, 13, 51414, 51414, 400, 586, 321, 528, 264, 13760, 365, 3104, 281, 479, 13, 51564, 51564, 407, 718, 311, 445, 808, 510, 562, 286, 1884, 479, 293, 718, 311, 360, 257, 1804, 389, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07285117266470925, "compression_ratio": 1.6379310344827587, "no_speech_prob": 1.5206652278720867e-05}, {"id": 394, "seek": 222900, "start": 2229.0, "end": 2232.0, "text": " And this should print a derivative of L with respect to F.", "tokens": [50364, 400, 341, 820, 4482, 257, 13760, 295, 441, 365, 3104, 281, 479, 13, 50514, 50514, 407, 321, 2066, 281, 536, 1451, 13, 50614, 50614, 865, 13, 400, 341, 307, 1451, 493, 281, 12607, 935, 26476, 1324, 13, 50864, 50864, 400, 550, 413, 43, 538, 30778, 820, 312, 479, 11, 597, 307, 3671, 732, 13, 51164, 51164, 16710, 307, 3671, 732, 13, 51264, 51264, 407, 498, 321, 797, 808, 510, 293, 321, 1319, 413, 13, 51514, 51514, 413, 5893, 1412, 1804, 6915, 389, 558, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0898663488666663, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.3845868124917615e-05}, {"id": 395, "seek": 222900, "start": 2232.0, "end": 2234.0, "text": " So we expect to see four.", "tokens": [50364, 400, 341, 820, 4482, 257, 13760, 295, 441, 365, 3104, 281, 479, 13, 50514, 50514, 407, 321, 2066, 281, 536, 1451, 13, 50614, 50614, 865, 13, 400, 341, 307, 1451, 493, 281, 12607, 935, 26476, 1324, 13, 50864, 50864, 400, 550, 413, 43, 538, 30778, 820, 312, 479, 11, 597, 307, 3671, 732, 13, 51164, 51164, 16710, 307, 3671, 732, 13, 51264, 51264, 407, 498, 321, 797, 808, 510, 293, 321, 1319, 413, 13, 51514, 51514, 413, 5893, 1412, 1804, 6915, 389, 558, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0898663488666663, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.3845868124917615e-05}, {"id": 396, "seek": 222900, "start": 2234.0, "end": 2239.0, "text": " Yeah. And this is four up to floating point funkiness.", "tokens": [50364, 400, 341, 820, 4482, 257, 13760, 295, 441, 365, 3104, 281, 479, 13, 50514, 50514, 407, 321, 2066, 281, 536, 1451, 13, 50614, 50614, 865, 13, 400, 341, 307, 1451, 493, 281, 12607, 935, 26476, 1324, 13, 50864, 50864, 400, 550, 413, 43, 538, 30778, 820, 312, 479, 11, 597, 307, 3671, 732, 13, 51164, 51164, 16710, 307, 3671, 732, 13, 51264, 51264, 407, 498, 321, 797, 808, 510, 293, 321, 1319, 413, 13, 51514, 51514, 413, 5893, 1412, 1804, 6915, 389, 558, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0898663488666663, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.3845868124917615e-05}, {"id": 397, "seek": 222900, "start": 2239.0, "end": 2245.0, "text": " And then DL by DD should be F, which is negative two.", "tokens": [50364, 400, 341, 820, 4482, 257, 13760, 295, 441, 365, 3104, 281, 479, 13, 50514, 50514, 407, 321, 2066, 281, 536, 1451, 13, 50614, 50614, 865, 13, 400, 341, 307, 1451, 493, 281, 12607, 935, 26476, 1324, 13, 50864, 50864, 400, 550, 413, 43, 538, 30778, 820, 312, 479, 11, 597, 307, 3671, 732, 13, 51164, 51164, 16710, 307, 3671, 732, 13, 51264, 51264, 407, 498, 321, 797, 808, 510, 293, 321, 1319, 413, 13, 51514, 51514, 413, 5893, 1412, 1804, 6915, 389, 558, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0898663488666663, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.3845868124917615e-05}, {"id": 398, "seek": 222900, "start": 2245.0, "end": 2247.0, "text": " Grad is negative two.", "tokens": [50364, 400, 341, 820, 4482, 257, 13760, 295, 441, 365, 3104, 281, 479, 13, 50514, 50514, 407, 321, 2066, 281, 536, 1451, 13, 50614, 50614, 865, 13, 400, 341, 307, 1451, 493, 281, 12607, 935, 26476, 1324, 13, 50864, 50864, 400, 550, 413, 43, 538, 30778, 820, 312, 479, 11, 597, 307, 3671, 732, 13, 51164, 51164, 16710, 307, 3671, 732, 13, 51264, 51264, 407, 498, 321, 797, 808, 510, 293, 321, 1319, 413, 13, 51514, 51514, 413, 5893, 1412, 1804, 6915, 389, 558, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0898663488666663, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.3845868124917615e-05}, {"id": 399, "seek": 222900, "start": 2247.0, "end": 2252.0, "text": " So if we again come here and we change D.", "tokens": [50364, 400, 341, 820, 4482, 257, 13760, 295, 441, 365, 3104, 281, 479, 13, 50514, 50514, 407, 321, 2066, 281, 536, 1451, 13, 50614, 50614, 865, 13, 400, 341, 307, 1451, 493, 281, 12607, 935, 26476, 1324, 13, 50864, 50864, 400, 550, 413, 43, 538, 30778, 820, 312, 479, 11, 597, 307, 3671, 732, 13, 51164, 51164, 16710, 307, 3671, 732, 13, 51264, 51264, 407, 498, 321, 797, 808, 510, 293, 321, 1319, 413, 13, 51514, 51514, 413, 5893, 1412, 1804, 6915, 389, 558, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0898663488666663, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.3845868124917615e-05}, {"id": 400, "seek": 222900, "start": 2252.0, "end": 2255.0, "text": " D dot data plus equals H right here.", "tokens": [50364, 400, 341, 820, 4482, 257, 13760, 295, 441, 365, 3104, 281, 479, 13, 50514, 50514, 407, 321, 2066, 281, 536, 1451, 13, 50614, 50614, 865, 13, 400, 341, 307, 1451, 493, 281, 12607, 935, 26476, 1324, 13, 50864, 50864, 400, 550, 413, 43, 538, 30778, 820, 312, 479, 11, 597, 307, 3671, 732, 13, 51164, 51164, 16710, 307, 3671, 732, 13, 51264, 51264, 407, 498, 321, 797, 808, 510, 293, 321, 1319, 413, 13, 51514, 51514, 413, 5893, 1412, 1804, 6915, 389, 558, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0898663488666663, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.3845868124917615e-05}, {"id": 401, "seek": 225500, "start": 2255.0, "end": 2260.0, "text": " So we expect so we've added a little H and then we see how L changed.", "tokens": [50364, 407, 321, 2066, 370, 321, 600, 3869, 257, 707, 389, 293, 550, 321, 536, 577, 441, 3105, 13, 50614, 50614, 400, 321, 2066, 281, 4482, 3671, 732, 13, 50864, 50864, 821, 321, 352, 13, 50964, 50964, 407, 321, 600, 7866, 984, 31197, 13, 51064, 51064, 708, 321, 434, 884, 510, 307, 733, 295, 411, 364, 294, 1889, 16235, 1520, 13, 51214, 51214, 16710, 1196, 1520, 307, 562, 321, 366, 1163, 2123, 341, 411, 646, 79, 1513, 559, 399, 51414, 51414, 293, 1242, 264, 13760, 365, 3104, 281, 439, 264, 19376, 3542, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11382830142974854, "compression_ratio": 1.6603773584905661, "no_speech_prob": 5.255331871012459e-06}, {"id": 402, "seek": 225500, "start": 2260.0, "end": 2265.0, "text": " And we expect to print negative two.", "tokens": [50364, 407, 321, 2066, 370, 321, 600, 3869, 257, 707, 389, 293, 550, 321, 536, 577, 441, 3105, 13, 50614, 50614, 400, 321, 2066, 281, 4482, 3671, 732, 13, 50864, 50864, 821, 321, 352, 13, 50964, 50964, 407, 321, 600, 7866, 984, 31197, 13, 51064, 51064, 708, 321, 434, 884, 510, 307, 733, 295, 411, 364, 294, 1889, 16235, 1520, 13, 51214, 51214, 16710, 1196, 1520, 307, 562, 321, 366, 1163, 2123, 341, 411, 646, 79, 1513, 559, 399, 51414, 51414, 293, 1242, 264, 13760, 365, 3104, 281, 439, 264, 19376, 3542, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11382830142974854, "compression_ratio": 1.6603773584905661, "no_speech_prob": 5.255331871012459e-06}, {"id": 403, "seek": 225500, "start": 2265.0, "end": 2267.0, "text": " There we go.", "tokens": [50364, 407, 321, 2066, 370, 321, 600, 3869, 257, 707, 389, 293, 550, 321, 536, 577, 441, 3105, 13, 50614, 50614, 400, 321, 2066, 281, 4482, 3671, 732, 13, 50864, 50864, 821, 321, 352, 13, 50964, 50964, 407, 321, 600, 7866, 984, 31197, 13, 51064, 51064, 708, 321, 434, 884, 510, 307, 733, 295, 411, 364, 294, 1889, 16235, 1520, 13, 51214, 51214, 16710, 1196, 1520, 307, 562, 321, 366, 1163, 2123, 341, 411, 646, 79, 1513, 559, 399, 51414, 51414, 293, 1242, 264, 13760, 365, 3104, 281, 439, 264, 19376, 3542, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11382830142974854, "compression_ratio": 1.6603773584905661, "no_speech_prob": 5.255331871012459e-06}, {"id": 404, "seek": 225500, "start": 2267.0, "end": 2269.0, "text": " So we've numerically verified.", "tokens": [50364, 407, 321, 2066, 370, 321, 600, 3869, 257, 707, 389, 293, 550, 321, 536, 577, 441, 3105, 13, 50614, 50614, 400, 321, 2066, 281, 4482, 3671, 732, 13, 50864, 50864, 821, 321, 352, 13, 50964, 50964, 407, 321, 600, 7866, 984, 31197, 13, 51064, 51064, 708, 321, 434, 884, 510, 307, 733, 295, 411, 364, 294, 1889, 16235, 1520, 13, 51214, 51214, 16710, 1196, 1520, 307, 562, 321, 366, 1163, 2123, 341, 411, 646, 79, 1513, 559, 399, 51414, 51414, 293, 1242, 264, 13760, 365, 3104, 281, 439, 264, 19376, 3542, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11382830142974854, "compression_ratio": 1.6603773584905661, "no_speech_prob": 5.255331871012459e-06}, {"id": 405, "seek": 225500, "start": 2269.0, "end": 2272.0, "text": " What we're doing here is kind of like an inline gradient check.", "tokens": [50364, 407, 321, 2066, 370, 321, 600, 3869, 257, 707, 389, 293, 550, 321, 536, 577, 441, 3105, 13, 50614, 50614, 400, 321, 2066, 281, 4482, 3671, 732, 13, 50864, 50864, 821, 321, 352, 13, 50964, 50964, 407, 321, 600, 7866, 984, 31197, 13, 51064, 51064, 708, 321, 434, 884, 510, 307, 733, 295, 411, 364, 294, 1889, 16235, 1520, 13, 51214, 51214, 16710, 1196, 1520, 307, 562, 321, 366, 1163, 2123, 341, 411, 646, 79, 1513, 559, 399, 51414, 51414, 293, 1242, 264, 13760, 365, 3104, 281, 439, 264, 19376, 3542, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11382830142974854, "compression_ratio": 1.6603773584905661, "no_speech_prob": 5.255331871012459e-06}, {"id": 406, "seek": 225500, "start": 2272.0, "end": 2276.0, "text": " Gradient check is when we are deriving this like backpropagation", "tokens": [50364, 407, 321, 2066, 370, 321, 600, 3869, 257, 707, 389, 293, 550, 321, 536, 577, 441, 3105, 13, 50614, 50614, 400, 321, 2066, 281, 4482, 3671, 732, 13, 50864, 50864, 821, 321, 352, 13, 50964, 50964, 407, 321, 600, 7866, 984, 31197, 13, 51064, 51064, 708, 321, 434, 884, 510, 307, 733, 295, 411, 364, 294, 1889, 16235, 1520, 13, 51214, 51214, 16710, 1196, 1520, 307, 562, 321, 366, 1163, 2123, 341, 411, 646, 79, 1513, 559, 399, 51414, 51414, 293, 1242, 264, 13760, 365, 3104, 281, 439, 264, 19376, 3542, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11382830142974854, "compression_ratio": 1.6603773584905661, "no_speech_prob": 5.255331871012459e-06}, {"id": 407, "seek": 225500, "start": 2276.0, "end": 2280.0, "text": " and getting the derivative with respect to all the intermediate results.", "tokens": [50364, 407, 321, 2066, 370, 321, 600, 3869, 257, 707, 389, 293, 550, 321, 536, 577, 441, 3105, 13, 50614, 50614, 400, 321, 2066, 281, 4482, 3671, 732, 13, 50864, 50864, 821, 321, 352, 13, 50964, 50964, 407, 321, 600, 7866, 984, 31197, 13, 51064, 51064, 708, 321, 434, 884, 510, 307, 733, 295, 411, 364, 294, 1889, 16235, 1520, 13, 51214, 51214, 16710, 1196, 1520, 307, 562, 321, 366, 1163, 2123, 341, 411, 646, 79, 1513, 559, 399, 51414, 51414, 293, 1242, 264, 13760, 365, 3104, 281, 439, 264, 19376, 3542, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11382830142974854, "compression_ratio": 1.6603773584905661, "no_speech_prob": 5.255331871012459e-06}, {"id": 408, "seek": 228000, "start": 2280.0, "end": 2286.0, "text": " And then numerical gradient is just estimating it using small step size.", "tokens": [50364, 400, 550, 29054, 16235, 307, 445, 8017, 990, 309, 1228, 1359, 1823, 2744, 13, 50664, 50664, 823, 321, 434, 1242, 281, 264, 5140, 87, 295, 646, 79, 1513, 559, 399, 13, 50814, 50814, 407, 341, 486, 312, 264, 881, 1021, 9984, 281, 1223, 13, 50964, 50964, 1436, 498, 291, 1223, 264, 16235, 337, 341, 9984, 11, 51114, 51114, 291, 1223, 439, 295, 646, 79, 1513, 559, 399, 293, 439, 295, 3097, 295, 18161, 36170, 1936, 13, 51314, 51314, 407, 321, 643, 281, 28446, 413, 43, 538, 9114, 13, 51514, 51514, 682, 661, 2283, 11, 264, 13760, 295, 441, 365, 3104, 281, 383, 13, 51664, 51664, 1436, 321, 600, 40610, 439, 613, 661, 2771, 2448, 1217, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06142282485961914, "compression_ratio": 1.7452471482889733, "no_speech_prob": 5.561982106883079e-05}, {"id": 409, "seek": 228000, "start": 2286.0, "end": 2289.0, "text": " Now we're getting to the crux of backpropagation.", "tokens": [50364, 400, 550, 29054, 16235, 307, 445, 8017, 990, 309, 1228, 1359, 1823, 2744, 13, 50664, 50664, 823, 321, 434, 1242, 281, 264, 5140, 87, 295, 646, 79, 1513, 559, 399, 13, 50814, 50814, 407, 341, 486, 312, 264, 881, 1021, 9984, 281, 1223, 13, 50964, 50964, 1436, 498, 291, 1223, 264, 16235, 337, 341, 9984, 11, 51114, 51114, 291, 1223, 439, 295, 646, 79, 1513, 559, 399, 293, 439, 295, 3097, 295, 18161, 36170, 1936, 13, 51314, 51314, 407, 321, 643, 281, 28446, 413, 43, 538, 9114, 13, 51514, 51514, 682, 661, 2283, 11, 264, 13760, 295, 441, 365, 3104, 281, 383, 13, 51664, 51664, 1436, 321, 600, 40610, 439, 613, 661, 2771, 2448, 1217, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06142282485961914, "compression_ratio": 1.7452471482889733, "no_speech_prob": 5.561982106883079e-05}, {"id": 410, "seek": 228000, "start": 2289.0, "end": 2292.0, "text": " So this will be the most important node to understand.", "tokens": [50364, 400, 550, 29054, 16235, 307, 445, 8017, 990, 309, 1228, 1359, 1823, 2744, 13, 50664, 50664, 823, 321, 434, 1242, 281, 264, 5140, 87, 295, 646, 79, 1513, 559, 399, 13, 50814, 50814, 407, 341, 486, 312, 264, 881, 1021, 9984, 281, 1223, 13, 50964, 50964, 1436, 498, 291, 1223, 264, 16235, 337, 341, 9984, 11, 51114, 51114, 291, 1223, 439, 295, 646, 79, 1513, 559, 399, 293, 439, 295, 3097, 295, 18161, 36170, 1936, 13, 51314, 51314, 407, 321, 643, 281, 28446, 413, 43, 538, 9114, 13, 51514, 51514, 682, 661, 2283, 11, 264, 13760, 295, 441, 365, 3104, 281, 383, 13, 51664, 51664, 1436, 321, 600, 40610, 439, 613, 661, 2771, 2448, 1217, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06142282485961914, "compression_ratio": 1.7452471482889733, "no_speech_prob": 5.561982106883079e-05}, {"id": 411, "seek": 228000, "start": 2292.0, "end": 2295.0, "text": " Because if you understand the gradient for this node,", "tokens": [50364, 400, 550, 29054, 16235, 307, 445, 8017, 990, 309, 1228, 1359, 1823, 2744, 13, 50664, 50664, 823, 321, 434, 1242, 281, 264, 5140, 87, 295, 646, 79, 1513, 559, 399, 13, 50814, 50814, 407, 341, 486, 312, 264, 881, 1021, 9984, 281, 1223, 13, 50964, 50964, 1436, 498, 291, 1223, 264, 16235, 337, 341, 9984, 11, 51114, 51114, 291, 1223, 439, 295, 646, 79, 1513, 559, 399, 293, 439, 295, 3097, 295, 18161, 36170, 1936, 13, 51314, 51314, 407, 321, 643, 281, 28446, 413, 43, 538, 9114, 13, 51514, 51514, 682, 661, 2283, 11, 264, 13760, 295, 441, 365, 3104, 281, 383, 13, 51664, 51664, 1436, 321, 600, 40610, 439, 613, 661, 2771, 2448, 1217, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06142282485961914, "compression_ratio": 1.7452471482889733, "no_speech_prob": 5.561982106883079e-05}, {"id": 412, "seek": 228000, "start": 2295.0, "end": 2299.0, "text": " you understand all of backpropagation and all of training of neural nets basically.", "tokens": [50364, 400, 550, 29054, 16235, 307, 445, 8017, 990, 309, 1228, 1359, 1823, 2744, 13, 50664, 50664, 823, 321, 434, 1242, 281, 264, 5140, 87, 295, 646, 79, 1513, 559, 399, 13, 50814, 50814, 407, 341, 486, 312, 264, 881, 1021, 9984, 281, 1223, 13, 50964, 50964, 1436, 498, 291, 1223, 264, 16235, 337, 341, 9984, 11, 51114, 51114, 291, 1223, 439, 295, 646, 79, 1513, 559, 399, 293, 439, 295, 3097, 295, 18161, 36170, 1936, 13, 51314, 51314, 407, 321, 643, 281, 28446, 413, 43, 538, 9114, 13, 51514, 51514, 682, 661, 2283, 11, 264, 13760, 295, 441, 365, 3104, 281, 383, 13, 51664, 51664, 1436, 321, 600, 40610, 439, 613, 661, 2771, 2448, 1217, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06142282485961914, "compression_ratio": 1.7452471482889733, "no_speech_prob": 5.561982106883079e-05}, {"id": 413, "seek": 228000, "start": 2299.0, "end": 2303.0, "text": " So we need to derive DL by DC.", "tokens": [50364, 400, 550, 29054, 16235, 307, 445, 8017, 990, 309, 1228, 1359, 1823, 2744, 13, 50664, 50664, 823, 321, 434, 1242, 281, 264, 5140, 87, 295, 646, 79, 1513, 559, 399, 13, 50814, 50814, 407, 341, 486, 312, 264, 881, 1021, 9984, 281, 1223, 13, 50964, 50964, 1436, 498, 291, 1223, 264, 16235, 337, 341, 9984, 11, 51114, 51114, 291, 1223, 439, 295, 646, 79, 1513, 559, 399, 293, 439, 295, 3097, 295, 18161, 36170, 1936, 13, 51314, 51314, 407, 321, 643, 281, 28446, 413, 43, 538, 9114, 13, 51514, 51514, 682, 661, 2283, 11, 264, 13760, 295, 441, 365, 3104, 281, 383, 13, 51664, 51664, 1436, 321, 600, 40610, 439, 613, 661, 2771, 2448, 1217, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06142282485961914, "compression_ratio": 1.7452471482889733, "no_speech_prob": 5.561982106883079e-05}, {"id": 414, "seek": 228000, "start": 2303.0, "end": 2306.0, "text": " In other words, the derivative of L with respect to C.", "tokens": [50364, 400, 550, 29054, 16235, 307, 445, 8017, 990, 309, 1228, 1359, 1823, 2744, 13, 50664, 50664, 823, 321, 434, 1242, 281, 264, 5140, 87, 295, 646, 79, 1513, 559, 399, 13, 50814, 50814, 407, 341, 486, 312, 264, 881, 1021, 9984, 281, 1223, 13, 50964, 50964, 1436, 498, 291, 1223, 264, 16235, 337, 341, 9984, 11, 51114, 51114, 291, 1223, 439, 295, 646, 79, 1513, 559, 399, 293, 439, 295, 3097, 295, 18161, 36170, 1936, 13, 51314, 51314, 407, 321, 643, 281, 28446, 413, 43, 538, 9114, 13, 51514, 51514, 682, 661, 2283, 11, 264, 13760, 295, 441, 365, 3104, 281, 383, 13, 51664, 51664, 1436, 321, 600, 40610, 439, 613, 661, 2771, 2448, 1217, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06142282485961914, "compression_ratio": 1.7452471482889733, "no_speech_prob": 5.561982106883079e-05}, {"id": 415, "seek": 228000, "start": 2306.0, "end": 2309.0, "text": " Because we've computed all these other gradients already.", "tokens": [50364, 400, 550, 29054, 16235, 307, 445, 8017, 990, 309, 1228, 1359, 1823, 2744, 13, 50664, 50664, 823, 321, 434, 1242, 281, 264, 5140, 87, 295, 646, 79, 1513, 559, 399, 13, 50814, 50814, 407, 341, 486, 312, 264, 881, 1021, 9984, 281, 1223, 13, 50964, 50964, 1436, 498, 291, 1223, 264, 16235, 337, 341, 9984, 11, 51114, 51114, 291, 1223, 439, 295, 646, 79, 1513, 559, 399, 293, 439, 295, 3097, 295, 18161, 36170, 1936, 13, 51314, 51314, 407, 321, 643, 281, 28446, 413, 43, 538, 9114, 13, 51514, 51514, 682, 661, 2283, 11, 264, 13760, 295, 441, 365, 3104, 281, 383, 13, 51664, 51664, 1436, 321, 600, 40610, 439, 613, 661, 2771, 2448, 1217, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06142282485961914, "compression_ratio": 1.7452471482889733, "no_speech_prob": 5.561982106883079e-05}, {"id": 416, "seek": 230900, "start": 2309.0, "end": 2313.0, "text": " Now we're coming here and we're continuing the backpropagation manually.", "tokens": [50364, 823, 321, 434, 1348, 510, 293, 321, 434, 9289, 264, 646, 79, 1513, 559, 399, 16945, 13, 50564, 50564, 407, 321, 528, 413, 43, 538, 9114, 293, 550, 321, 603, 611, 28446, 413, 43, 538, 10113, 13, 50814, 50814, 823, 510, 311, 264, 1154, 13, 50914, 50914, 1012, 360, 321, 28446, 413, 43, 538, 9114, 30, 51114, 51114, 492, 767, 458, 264, 13760, 441, 365, 3104, 281, 413, 13, 51264, 51264, 407, 321, 458, 577, 441, 307, 9477, 281, 413, 13, 51414, 51414, 583, 577, 307, 441, 9477, 281, 383, 30, 51564, 51564, 407, 498, 321, 33377, 383, 11, 577, 775, 300, 2712, 441, 807, 413, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04676564964088234, "compression_ratio": 1.625, "no_speech_prob": 2.546560972405132e-05}, {"id": 417, "seek": 230900, "start": 2313.0, "end": 2318.0, "text": " So we want DL by DC and then we'll also derive DL by DE.", "tokens": [50364, 823, 321, 434, 1348, 510, 293, 321, 434, 9289, 264, 646, 79, 1513, 559, 399, 16945, 13, 50564, 50564, 407, 321, 528, 413, 43, 538, 9114, 293, 550, 321, 603, 611, 28446, 413, 43, 538, 10113, 13, 50814, 50814, 823, 510, 311, 264, 1154, 13, 50914, 50914, 1012, 360, 321, 28446, 413, 43, 538, 9114, 30, 51114, 51114, 492, 767, 458, 264, 13760, 441, 365, 3104, 281, 413, 13, 51264, 51264, 407, 321, 458, 577, 441, 307, 9477, 281, 413, 13, 51414, 51414, 583, 577, 307, 441, 9477, 281, 383, 30, 51564, 51564, 407, 498, 321, 33377, 383, 11, 577, 775, 300, 2712, 441, 807, 413, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04676564964088234, "compression_ratio": 1.625, "no_speech_prob": 2.546560972405132e-05}, {"id": 418, "seek": 230900, "start": 2318.0, "end": 2320.0, "text": " Now here's the problem.", "tokens": [50364, 823, 321, 434, 1348, 510, 293, 321, 434, 9289, 264, 646, 79, 1513, 559, 399, 16945, 13, 50564, 50564, 407, 321, 528, 413, 43, 538, 9114, 293, 550, 321, 603, 611, 28446, 413, 43, 538, 10113, 13, 50814, 50814, 823, 510, 311, 264, 1154, 13, 50914, 50914, 1012, 360, 321, 28446, 413, 43, 538, 9114, 30, 51114, 51114, 492, 767, 458, 264, 13760, 441, 365, 3104, 281, 413, 13, 51264, 51264, 407, 321, 458, 577, 441, 307, 9477, 281, 413, 13, 51414, 51414, 583, 577, 307, 441, 9477, 281, 383, 30, 51564, 51564, 407, 498, 321, 33377, 383, 11, 577, 775, 300, 2712, 441, 807, 413, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04676564964088234, "compression_ratio": 1.625, "no_speech_prob": 2.546560972405132e-05}, {"id": 419, "seek": 230900, "start": 2320.0, "end": 2324.0, "text": " How do we derive DL by DC?", "tokens": [50364, 823, 321, 434, 1348, 510, 293, 321, 434, 9289, 264, 646, 79, 1513, 559, 399, 16945, 13, 50564, 50564, 407, 321, 528, 413, 43, 538, 9114, 293, 550, 321, 603, 611, 28446, 413, 43, 538, 10113, 13, 50814, 50814, 823, 510, 311, 264, 1154, 13, 50914, 50914, 1012, 360, 321, 28446, 413, 43, 538, 9114, 30, 51114, 51114, 492, 767, 458, 264, 13760, 441, 365, 3104, 281, 413, 13, 51264, 51264, 407, 321, 458, 577, 441, 307, 9477, 281, 413, 13, 51414, 51414, 583, 577, 307, 441, 9477, 281, 383, 30, 51564, 51564, 407, 498, 321, 33377, 383, 11, 577, 775, 300, 2712, 441, 807, 413, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04676564964088234, "compression_ratio": 1.625, "no_speech_prob": 2.546560972405132e-05}, {"id": 420, "seek": 230900, "start": 2324.0, "end": 2327.0, "text": " We actually know the derivative L with respect to D.", "tokens": [50364, 823, 321, 434, 1348, 510, 293, 321, 434, 9289, 264, 646, 79, 1513, 559, 399, 16945, 13, 50564, 50564, 407, 321, 528, 413, 43, 538, 9114, 293, 550, 321, 603, 611, 28446, 413, 43, 538, 10113, 13, 50814, 50814, 823, 510, 311, 264, 1154, 13, 50914, 50914, 1012, 360, 321, 28446, 413, 43, 538, 9114, 30, 51114, 51114, 492, 767, 458, 264, 13760, 441, 365, 3104, 281, 413, 13, 51264, 51264, 407, 321, 458, 577, 441, 307, 9477, 281, 413, 13, 51414, 51414, 583, 577, 307, 441, 9477, 281, 383, 30, 51564, 51564, 407, 498, 321, 33377, 383, 11, 577, 775, 300, 2712, 441, 807, 413, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04676564964088234, "compression_ratio": 1.625, "no_speech_prob": 2.546560972405132e-05}, {"id": 421, "seek": 230900, "start": 2327.0, "end": 2330.0, "text": " So we know how L is sensitive to D.", "tokens": [50364, 823, 321, 434, 1348, 510, 293, 321, 434, 9289, 264, 646, 79, 1513, 559, 399, 16945, 13, 50564, 50564, 407, 321, 528, 413, 43, 538, 9114, 293, 550, 321, 603, 611, 28446, 413, 43, 538, 10113, 13, 50814, 50814, 823, 510, 311, 264, 1154, 13, 50914, 50914, 1012, 360, 321, 28446, 413, 43, 538, 9114, 30, 51114, 51114, 492, 767, 458, 264, 13760, 441, 365, 3104, 281, 413, 13, 51264, 51264, 407, 321, 458, 577, 441, 307, 9477, 281, 413, 13, 51414, 51414, 583, 577, 307, 441, 9477, 281, 383, 30, 51564, 51564, 407, 498, 321, 33377, 383, 11, 577, 775, 300, 2712, 441, 807, 413, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04676564964088234, "compression_ratio": 1.625, "no_speech_prob": 2.546560972405132e-05}, {"id": 422, "seek": 230900, "start": 2330.0, "end": 2333.0, "text": " But how is L sensitive to C?", "tokens": [50364, 823, 321, 434, 1348, 510, 293, 321, 434, 9289, 264, 646, 79, 1513, 559, 399, 16945, 13, 50564, 50564, 407, 321, 528, 413, 43, 538, 9114, 293, 550, 321, 603, 611, 28446, 413, 43, 538, 10113, 13, 50814, 50814, 823, 510, 311, 264, 1154, 13, 50914, 50914, 1012, 360, 321, 28446, 413, 43, 538, 9114, 30, 51114, 51114, 492, 767, 458, 264, 13760, 441, 365, 3104, 281, 413, 13, 51264, 51264, 407, 321, 458, 577, 441, 307, 9477, 281, 413, 13, 51414, 51414, 583, 577, 307, 441, 9477, 281, 383, 30, 51564, 51564, 407, 498, 321, 33377, 383, 11, 577, 775, 300, 2712, 441, 807, 413, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04676564964088234, "compression_ratio": 1.625, "no_speech_prob": 2.546560972405132e-05}, {"id": 423, "seek": 230900, "start": 2333.0, "end": 2338.0, "text": " So if we wiggle C, how does that impact L through D?", "tokens": [50364, 823, 321, 434, 1348, 510, 293, 321, 434, 9289, 264, 646, 79, 1513, 559, 399, 16945, 13, 50564, 50564, 407, 321, 528, 413, 43, 538, 9114, 293, 550, 321, 603, 611, 28446, 413, 43, 538, 10113, 13, 50814, 50814, 823, 510, 311, 264, 1154, 13, 50914, 50914, 1012, 360, 321, 28446, 413, 43, 538, 9114, 30, 51114, 51114, 492, 767, 458, 264, 13760, 441, 365, 3104, 281, 413, 13, 51264, 51264, 407, 321, 458, 577, 441, 307, 9477, 281, 413, 13, 51414, 51414, 583, 577, 307, 441, 9477, 281, 383, 30, 51564, 51564, 407, 498, 321, 33377, 383, 11, 577, 775, 300, 2712, 441, 807, 413, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04676564964088234, "compression_ratio": 1.625, "no_speech_prob": 2.546560972405132e-05}, {"id": 424, "seek": 233800, "start": 2338.0, "end": 2342.0, "text": " So we know DL by DC.", "tokens": [50364, 407, 321, 458, 413, 43, 538, 9114, 13, 50564, 50564, 400, 321, 611, 510, 458, 577, 383, 11606, 413, 13, 50714, 50714, 400, 370, 445, 588, 46506, 11, 498, 291, 458, 264, 2712, 300, 383, 307, 1419, 322, 413, 50914, 50914, 293, 264, 2712, 300, 413, 307, 1419, 322, 441, 11, 51014, 51014, 550, 291, 820, 312, 1075, 281, 6063, 829, 300, 1589, 1214, 51164, 51164, 281, 2573, 484, 577, 383, 11606, 441, 13, 51264, 51264, 400, 6451, 341, 307, 437, 321, 393, 767, 360, 13, 51414, 51414, 407, 294, 1729, 11, 321, 458, 11, 445, 40571, 322, 413, 700, 11, 51564, 51564, 718, 311, 574, 412, 577, 11, 437, 307, 264, 13760, 1936, 295, 413, 365, 3104, 281, 383, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07932541656494141, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.887465082807466e-05}, {"id": 425, "seek": 233800, "start": 2342.0, "end": 2345.0, "text": " And we also here know how C impacts D.", "tokens": [50364, 407, 321, 458, 413, 43, 538, 9114, 13, 50564, 50564, 400, 321, 611, 510, 458, 577, 383, 11606, 413, 13, 50714, 50714, 400, 370, 445, 588, 46506, 11, 498, 291, 458, 264, 2712, 300, 383, 307, 1419, 322, 413, 50914, 50914, 293, 264, 2712, 300, 413, 307, 1419, 322, 441, 11, 51014, 51014, 550, 291, 820, 312, 1075, 281, 6063, 829, 300, 1589, 1214, 51164, 51164, 281, 2573, 484, 577, 383, 11606, 441, 13, 51264, 51264, 400, 6451, 341, 307, 437, 321, 393, 767, 360, 13, 51414, 51414, 407, 294, 1729, 11, 321, 458, 11, 445, 40571, 322, 413, 700, 11, 51564, 51564, 718, 311, 574, 412, 577, 11, 437, 307, 264, 13760, 1936, 295, 413, 365, 3104, 281, 383, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07932541656494141, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.887465082807466e-05}, {"id": 426, "seek": 233800, "start": 2345.0, "end": 2349.0, "text": " And so just very intuitively, if you know the impact that C is having on D", "tokens": [50364, 407, 321, 458, 413, 43, 538, 9114, 13, 50564, 50564, 400, 321, 611, 510, 458, 577, 383, 11606, 413, 13, 50714, 50714, 400, 370, 445, 588, 46506, 11, 498, 291, 458, 264, 2712, 300, 383, 307, 1419, 322, 413, 50914, 50914, 293, 264, 2712, 300, 413, 307, 1419, 322, 441, 11, 51014, 51014, 550, 291, 820, 312, 1075, 281, 6063, 829, 300, 1589, 1214, 51164, 51164, 281, 2573, 484, 577, 383, 11606, 441, 13, 51264, 51264, 400, 6451, 341, 307, 437, 321, 393, 767, 360, 13, 51414, 51414, 407, 294, 1729, 11, 321, 458, 11, 445, 40571, 322, 413, 700, 11, 51564, 51564, 718, 311, 574, 412, 577, 11, 437, 307, 264, 13760, 1936, 295, 413, 365, 3104, 281, 383, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07932541656494141, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.887465082807466e-05}, {"id": 427, "seek": 233800, "start": 2349.0, "end": 2351.0, "text": " and the impact that D is having on L,", "tokens": [50364, 407, 321, 458, 413, 43, 538, 9114, 13, 50564, 50564, 400, 321, 611, 510, 458, 577, 383, 11606, 413, 13, 50714, 50714, 400, 370, 445, 588, 46506, 11, 498, 291, 458, 264, 2712, 300, 383, 307, 1419, 322, 413, 50914, 50914, 293, 264, 2712, 300, 413, 307, 1419, 322, 441, 11, 51014, 51014, 550, 291, 820, 312, 1075, 281, 6063, 829, 300, 1589, 1214, 51164, 51164, 281, 2573, 484, 577, 383, 11606, 441, 13, 51264, 51264, 400, 6451, 341, 307, 437, 321, 393, 767, 360, 13, 51414, 51414, 407, 294, 1729, 11, 321, 458, 11, 445, 40571, 322, 413, 700, 11, 51564, 51564, 718, 311, 574, 412, 577, 11, 437, 307, 264, 13760, 1936, 295, 413, 365, 3104, 281, 383, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07932541656494141, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.887465082807466e-05}, {"id": 428, "seek": 233800, "start": 2351.0, "end": 2354.0, "text": " then you should be able to somehow put that information together", "tokens": [50364, 407, 321, 458, 413, 43, 538, 9114, 13, 50564, 50564, 400, 321, 611, 510, 458, 577, 383, 11606, 413, 13, 50714, 50714, 400, 370, 445, 588, 46506, 11, 498, 291, 458, 264, 2712, 300, 383, 307, 1419, 322, 413, 50914, 50914, 293, 264, 2712, 300, 413, 307, 1419, 322, 441, 11, 51014, 51014, 550, 291, 820, 312, 1075, 281, 6063, 829, 300, 1589, 1214, 51164, 51164, 281, 2573, 484, 577, 383, 11606, 441, 13, 51264, 51264, 400, 6451, 341, 307, 437, 321, 393, 767, 360, 13, 51414, 51414, 407, 294, 1729, 11, 321, 458, 11, 445, 40571, 322, 413, 700, 11, 51564, 51564, 718, 311, 574, 412, 577, 11, 437, 307, 264, 13760, 1936, 295, 413, 365, 3104, 281, 383, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07932541656494141, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.887465082807466e-05}, {"id": 429, "seek": 233800, "start": 2354.0, "end": 2356.0, "text": " to figure out how C impacts L.", "tokens": [50364, 407, 321, 458, 413, 43, 538, 9114, 13, 50564, 50564, 400, 321, 611, 510, 458, 577, 383, 11606, 413, 13, 50714, 50714, 400, 370, 445, 588, 46506, 11, 498, 291, 458, 264, 2712, 300, 383, 307, 1419, 322, 413, 50914, 50914, 293, 264, 2712, 300, 413, 307, 1419, 322, 441, 11, 51014, 51014, 550, 291, 820, 312, 1075, 281, 6063, 829, 300, 1589, 1214, 51164, 51164, 281, 2573, 484, 577, 383, 11606, 441, 13, 51264, 51264, 400, 6451, 341, 307, 437, 321, 393, 767, 360, 13, 51414, 51414, 407, 294, 1729, 11, 321, 458, 11, 445, 40571, 322, 413, 700, 11, 51564, 51564, 718, 311, 574, 412, 577, 11, 437, 307, 264, 13760, 1936, 295, 413, 365, 3104, 281, 383, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07932541656494141, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.887465082807466e-05}, {"id": 430, "seek": 233800, "start": 2356.0, "end": 2359.0, "text": " And indeed this is what we can actually do.", "tokens": [50364, 407, 321, 458, 413, 43, 538, 9114, 13, 50564, 50564, 400, 321, 611, 510, 458, 577, 383, 11606, 413, 13, 50714, 50714, 400, 370, 445, 588, 46506, 11, 498, 291, 458, 264, 2712, 300, 383, 307, 1419, 322, 413, 50914, 50914, 293, 264, 2712, 300, 413, 307, 1419, 322, 441, 11, 51014, 51014, 550, 291, 820, 312, 1075, 281, 6063, 829, 300, 1589, 1214, 51164, 51164, 281, 2573, 484, 577, 383, 11606, 441, 13, 51264, 51264, 400, 6451, 341, 307, 437, 321, 393, 767, 360, 13, 51414, 51414, 407, 294, 1729, 11, 321, 458, 11, 445, 40571, 322, 413, 700, 11, 51564, 51564, 718, 311, 574, 412, 577, 11, 437, 307, 264, 13760, 1936, 295, 413, 365, 3104, 281, 383, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07932541656494141, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.887465082807466e-05}, {"id": 431, "seek": 233800, "start": 2359.0, "end": 2362.0, "text": " So in particular, we know, just concentrating on D first,", "tokens": [50364, 407, 321, 458, 413, 43, 538, 9114, 13, 50564, 50564, 400, 321, 611, 510, 458, 577, 383, 11606, 413, 13, 50714, 50714, 400, 370, 445, 588, 46506, 11, 498, 291, 458, 264, 2712, 300, 383, 307, 1419, 322, 413, 50914, 50914, 293, 264, 2712, 300, 413, 307, 1419, 322, 441, 11, 51014, 51014, 550, 291, 820, 312, 1075, 281, 6063, 829, 300, 1589, 1214, 51164, 51164, 281, 2573, 484, 577, 383, 11606, 441, 13, 51264, 51264, 400, 6451, 341, 307, 437, 321, 393, 767, 360, 13, 51414, 51414, 407, 294, 1729, 11, 321, 458, 11, 445, 40571, 322, 413, 700, 11, 51564, 51564, 718, 311, 574, 412, 577, 11, 437, 307, 264, 13760, 1936, 295, 413, 365, 3104, 281, 383, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07932541656494141, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.887465082807466e-05}, {"id": 432, "seek": 233800, "start": 2362.0, "end": 2366.0, "text": " let's look at how, what is the derivative basically of D with respect to C.", "tokens": [50364, 407, 321, 458, 413, 43, 538, 9114, 13, 50564, 50564, 400, 321, 611, 510, 458, 577, 383, 11606, 413, 13, 50714, 50714, 400, 370, 445, 588, 46506, 11, 498, 291, 458, 264, 2712, 300, 383, 307, 1419, 322, 413, 50914, 50914, 293, 264, 2712, 300, 413, 307, 1419, 322, 441, 11, 51014, 51014, 550, 291, 820, 312, 1075, 281, 6063, 829, 300, 1589, 1214, 51164, 51164, 281, 2573, 484, 577, 383, 11606, 441, 13, 51264, 51264, 400, 6451, 341, 307, 437, 321, 393, 767, 360, 13, 51414, 51414, 407, 294, 1729, 11, 321, 458, 11, 445, 40571, 322, 413, 700, 11, 51564, 51564, 718, 311, 574, 412, 577, 11, 437, 307, 264, 13760, 1936, 295, 413, 365, 3104, 281, 383, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07932541656494141, "compression_ratio": 1.7286821705426356, "no_speech_prob": 8.887465082807466e-05}, {"id": 433, "seek": 236600, "start": 2366.0, "end": 2371.0, "text": " So in other words, what is DD by DC?", "tokens": [50364, 407, 294, 661, 2283, 11, 437, 307, 30778, 538, 9114, 30, 50614, 50614, 407, 510, 321, 458, 300, 413, 307, 383, 1804, 462, 13, 663, 311, 437, 321, 458, 13, 50864, 50864, 400, 586, 321, 434, 3102, 294, 30778, 538, 9114, 13, 51014, 51014, 759, 291, 445, 458, 428, 33400, 797, 293, 291, 1604, 51114, 51114, 300, 27372, 990, 383, 1804, 462, 365, 3104, 281, 383, 11, 51264, 51264, 291, 458, 300, 300, 2709, 291, 502, 13, 15, 13, 51414, 51414, 400, 321, 393, 611, 352, 646, 281, 264, 14688, 293, 28446, 341, 13, 51564, 51564, 1436, 797, 11, 321, 393, 352, 281, 527, 283, 295, 2031, 1804, 276, 3175, 283, 295, 2031, 6666, 538, 276, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09698731446069134, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.00012338261876720935}, {"id": 434, "seek": 236600, "start": 2371.0, "end": 2376.0, "text": " So here we know that D is C plus E. That's what we know.", "tokens": [50364, 407, 294, 661, 2283, 11, 437, 307, 30778, 538, 9114, 30, 50614, 50614, 407, 510, 321, 458, 300, 413, 307, 383, 1804, 462, 13, 663, 311, 437, 321, 458, 13, 50864, 50864, 400, 586, 321, 434, 3102, 294, 30778, 538, 9114, 13, 51014, 51014, 759, 291, 445, 458, 428, 33400, 797, 293, 291, 1604, 51114, 51114, 300, 27372, 990, 383, 1804, 462, 365, 3104, 281, 383, 11, 51264, 51264, 291, 458, 300, 300, 2709, 291, 502, 13, 15, 13, 51414, 51414, 400, 321, 393, 611, 352, 646, 281, 264, 14688, 293, 28446, 341, 13, 51564, 51564, 1436, 797, 11, 321, 393, 352, 281, 527, 283, 295, 2031, 1804, 276, 3175, 283, 295, 2031, 6666, 538, 276, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09698731446069134, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.00012338261876720935}, {"id": 435, "seek": 236600, "start": 2376.0, "end": 2379.0, "text": " And now we're interested in DD by DC.", "tokens": [50364, 407, 294, 661, 2283, 11, 437, 307, 30778, 538, 9114, 30, 50614, 50614, 407, 510, 321, 458, 300, 413, 307, 383, 1804, 462, 13, 663, 311, 437, 321, 458, 13, 50864, 50864, 400, 586, 321, 434, 3102, 294, 30778, 538, 9114, 13, 51014, 51014, 759, 291, 445, 458, 428, 33400, 797, 293, 291, 1604, 51114, 51114, 300, 27372, 990, 383, 1804, 462, 365, 3104, 281, 383, 11, 51264, 51264, 291, 458, 300, 300, 2709, 291, 502, 13, 15, 13, 51414, 51414, 400, 321, 393, 611, 352, 646, 281, 264, 14688, 293, 28446, 341, 13, 51564, 51564, 1436, 797, 11, 321, 393, 352, 281, 527, 283, 295, 2031, 1804, 276, 3175, 283, 295, 2031, 6666, 538, 276, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09698731446069134, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.00012338261876720935}, {"id": 436, "seek": 236600, "start": 2379.0, "end": 2381.0, "text": " If you just know your calculus again and you remember", "tokens": [50364, 407, 294, 661, 2283, 11, 437, 307, 30778, 538, 9114, 30, 50614, 50614, 407, 510, 321, 458, 300, 413, 307, 383, 1804, 462, 13, 663, 311, 437, 321, 458, 13, 50864, 50864, 400, 586, 321, 434, 3102, 294, 30778, 538, 9114, 13, 51014, 51014, 759, 291, 445, 458, 428, 33400, 797, 293, 291, 1604, 51114, 51114, 300, 27372, 990, 383, 1804, 462, 365, 3104, 281, 383, 11, 51264, 51264, 291, 458, 300, 300, 2709, 291, 502, 13, 15, 13, 51414, 51414, 400, 321, 393, 611, 352, 646, 281, 264, 14688, 293, 28446, 341, 13, 51564, 51564, 1436, 797, 11, 321, 393, 352, 281, 527, 283, 295, 2031, 1804, 276, 3175, 283, 295, 2031, 6666, 538, 276, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09698731446069134, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.00012338261876720935}, {"id": 437, "seek": 236600, "start": 2381.0, "end": 2384.0, "text": " that differentiating C plus E with respect to C,", "tokens": [50364, 407, 294, 661, 2283, 11, 437, 307, 30778, 538, 9114, 30, 50614, 50614, 407, 510, 321, 458, 300, 413, 307, 383, 1804, 462, 13, 663, 311, 437, 321, 458, 13, 50864, 50864, 400, 586, 321, 434, 3102, 294, 30778, 538, 9114, 13, 51014, 51014, 759, 291, 445, 458, 428, 33400, 797, 293, 291, 1604, 51114, 51114, 300, 27372, 990, 383, 1804, 462, 365, 3104, 281, 383, 11, 51264, 51264, 291, 458, 300, 300, 2709, 291, 502, 13, 15, 13, 51414, 51414, 400, 321, 393, 611, 352, 646, 281, 264, 14688, 293, 28446, 341, 13, 51564, 51564, 1436, 797, 11, 321, 393, 352, 281, 527, 283, 295, 2031, 1804, 276, 3175, 283, 295, 2031, 6666, 538, 276, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09698731446069134, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.00012338261876720935}, {"id": 438, "seek": 236600, "start": 2384.0, "end": 2387.0, "text": " you know that that gives you 1.0.", "tokens": [50364, 407, 294, 661, 2283, 11, 437, 307, 30778, 538, 9114, 30, 50614, 50614, 407, 510, 321, 458, 300, 413, 307, 383, 1804, 462, 13, 663, 311, 437, 321, 458, 13, 50864, 50864, 400, 586, 321, 434, 3102, 294, 30778, 538, 9114, 13, 51014, 51014, 759, 291, 445, 458, 428, 33400, 797, 293, 291, 1604, 51114, 51114, 300, 27372, 990, 383, 1804, 462, 365, 3104, 281, 383, 11, 51264, 51264, 291, 458, 300, 300, 2709, 291, 502, 13, 15, 13, 51414, 51414, 400, 321, 393, 611, 352, 646, 281, 264, 14688, 293, 28446, 341, 13, 51564, 51564, 1436, 797, 11, 321, 393, 352, 281, 527, 283, 295, 2031, 1804, 276, 3175, 283, 295, 2031, 6666, 538, 276, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09698731446069134, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.00012338261876720935}, {"id": 439, "seek": 236600, "start": 2387.0, "end": 2390.0, "text": " And we can also go back to the basics and derive this.", "tokens": [50364, 407, 294, 661, 2283, 11, 437, 307, 30778, 538, 9114, 30, 50614, 50614, 407, 510, 321, 458, 300, 413, 307, 383, 1804, 462, 13, 663, 311, 437, 321, 458, 13, 50864, 50864, 400, 586, 321, 434, 3102, 294, 30778, 538, 9114, 13, 51014, 51014, 759, 291, 445, 458, 428, 33400, 797, 293, 291, 1604, 51114, 51114, 300, 27372, 990, 383, 1804, 462, 365, 3104, 281, 383, 11, 51264, 51264, 291, 458, 300, 300, 2709, 291, 502, 13, 15, 13, 51414, 51414, 400, 321, 393, 611, 352, 646, 281, 264, 14688, 293, 28446, 341, 13, 51564, 51564, 1436, 797, 11, 321, 393, 352, 281, 527, 283, 295, 2031, 1804, 276, 3175, 283, 295, 2031, 6666, 538, 276, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09698731446069134, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.00012338261876720935}, {"id": 440, "seek": 236600, "start": 2390.0, "end": 2395.0, "text": " Because again, we can go to our f of x plus h minus f of x divided by h.", "tokens": [50364, 407, 294, 661, 2283, 11, 437, 307, 30778, 538, 9114, 30, 50614, 50614, 407, 510, 321, 458, 300, 413, 307, 383, 1804, 462, 13, 663, 311, 437, 321, 458, 13, 50864, 50864, 400, 586, 321, 434, 3102, 294, 30778, 538, 9114, 13, 51014, 51014, 759, 291, 445, 458, 428, 33400, 797, 293, 291, 1604, 51114, 51114, 300, 27372, 990, 383, 1804, 462, 365, 3104, 281, 383, 11, 51264, 51264, 291, 458, 300, 300, 2709, 291, 502, 13, 15, 13, 51414, 51414, 400, 321, 393, 611, 352, 646, 281, 264, 14688, 293, 28446, 341, 13, 51564, 51564, 1436, 797, 11, 321, 393, 352, 281, 527, 283, 295, 2031, 1804, 276, 3175, 283, 295, 2031, 6666, 538, 276, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09698731446069134, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.00012338261876720935}, {"id": 441, "seek": 239500, "start": 2395.0, "end": 2399.0, "text": " That's the definition of a derivative as h goes to zero.", "tokens": [50364, 663, 311, 264, 7123, 295, 257, 13760, 382, 276, 1709, 281, 4018, 13, 50564, 50564, 400, 370, 510, 11, 8416, 322, 383, 293, 1080, 1802, 322, 413, 11, 50814, 50814, 321, 393, 1936, 360, 264, 283, 295, 2031, 1804, 276, 486, 312, 383, 307, 1946, 14684, 538, 276, 1804, 383, 13, 51114, 51114, 663, 311, 264, 700, 13344, 295, 527, 2445, 11, 3175, 383, 1804, 462, 13, 51414, 51414, 400, 550, 9845, 276, 13, 400, 370, 437, 307, 341, 30, 51614, 51614, 1449, 14702, 341, 484, 11, 341, 486, 312, 383, 1804, 276, 1804, 462, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10719025737107402, "compression_ratio": 1.6105769230769231, "no_speech_prob": 6.013890742906369e-05}, {"id": 442, "seek": 239500, "start": 2399.0, "end": 2404.0, "text": " And so here, focusing on C and its effect on D,", "tokens": [50364, 663, 311, 264, 7123, 295, 257, 13760, 382, 276, 1709, 281, 4018, 13, 50564, 50564, 400, 370, 510, 11, 8416, 322, 383, 293, 1080, 1802, 322, 413, 11, 50814, 50814, 321, 393, 1936, 360, 264, 283, 295, 2031, 1804, 276, 486, 312, 383, 307, 1946, 14684, 538, 276, 1804, 383, 13, 51114, 51114, 663, 311, 264, 700, 13344, 295, 527, 2445, 11, 3175, 383, 1804, 462, 13, 51414, 51414, 400, 550, 9845, 276, 13, 400, 370, 437, 307, 341, 30, 51614, 51614, 1449, 14702, 341, 484, 11, 341, 486, 312, 383, 1804, 276, 1804, 462, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10719025737107402, "compression_ratio": 1.6105769230769231, "no_speech_prob": 6.013890742906369e-05}, {"id": 443, "seek": 239500, "start": 2404.0, "end": 2410.0, "text": " we can basically do the f of x plus h will be C is incremented by h plus C.", "tokens": [50364, 663, 311, 264, 7123, 295, 257, 13760, 382, 276, 1709, 281, 4018, 13, 50564, 50564, 400, 370, 510, 11, 8416, 322, 383, 293, 1080, 1802, 322, 413, 11, 50814, 50814, 321, 393, 1936, 360, 264, 283, 295, 2031, 1804, 276, 486, 312, 383, 307, 1946, 14684, 538, 276, 1804, 383, 13, 51114, 51114, 663, 311, 264, 700, 13344, 295, 527, 2445, 11, 3175, 383, 1804, 462, 13, 51414, 51414, 400, 550, 9845, 276, 13, 400, 370, 437, 307, 341, 30, 51614, 51614, 1449, 14702, 341, 484, 11, 341, 486, 312, 383, 1804, 276, 1804, 462, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10719025737107402, "compression_ratio": 1.6105769230769231, "no_speech_prob": 6.013890742906369e-05}, {"id": 444, "seek": 239500, "start": 2410.0, "end": 2416.0, "text": " That's the first evaluation of our function, minus C plus E.", "tokens": [50364, 663, 311, 264, 7123, 295, 257, 13760, 382, 276, 1709, 281, 4018, 13, 50564, 50564, 400, 370, 510, 11, 8416, 322, 383, 293, 1080, 1802, 322, 413, 11, 50814, 50814, 321, 393, 1936, 360, 264, 283, 295, 2031, 1804, 276, 486, 312, 383, 307, 1946, 14684, 538, 276, 1804, 383, 13, 51114, 51114, 663, 311, 264, 700, 13344, 295, 527, 2445, 11, 3175, 383, 1804, 462, 13, 51414, 51414, 400, 550, 9845, 276, 13, 400, 370, 437, 307, 341, 30, 51614, 51614, 1449, 14702, 341, 484, 11, 341, 486, 312, 383, 1804, 276, 1804, 462, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10719025737107402, "compression_ratio": 1.6105769230769231, "no_speech_prob": 6.013890742906369e-05}, {"id": 445, "seek": 239500, "start": 2416.0, "end": 2420.0, "text": " And then divide h. And so what is this?", "tokens": [50364, 663, 311, 264, 7123, 295, 257, 13760, 382, 276, 1709, 281, 4018, 13, 50564, 50564, 400, 370, 510, 11, 8416, 322, 383, 293, 1080, 1802, 322, 413, 11, 50814, 50814, 321, 393, 1936, 360, 264, 283, 295, 2031, 1804, 276, 486, 312, 383, 307, 1946, 14684, 538, 276, 1804, 383, 13, 51114, 51114, 663, 311, 264, 700, 13344, 295, 527, 2445, 11, 3175, 383, 1804, 462, 13, 51414, 51414, 400, 550, 9845, 276, 13, 400, 370, 437, 307, 341, 30, 51614, 51614, 1449, 14702, 341, 484, 11, 341, 486, 312, 383, 1804, 276, 1804, 462, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10719025737107402, "compression_ratio": 1.6105769230769231, "no_speech_prob": 6.013890742906369e-05}, {"id": 446, "seek": 239500, "start": 2420.0, "end": 2423.0, "text": " Just expanding this out, this will be C plus h plus E", "tokens": [50364, 663, 311, 264, 7123, 295, 257, 13760, 382, 276, 1709, 281, 4018, 13, 50564, 50564, 400, 370, 510, 11, 8416, 322, 383, 293, 1080, 1802, 322, 413, 11, 50814, 50814, 321, 393, 1936, 360, 264, 283, 295, 2031, 1804, 276, 486, 312, 383, 307, 1946, 14684, 538, 276, 1804, 383, 13, 51114, 51114, 663, 311, 264, 700, 13344, 295, 527, 2445, 11, 3175, 383, 1804, 462, 13, 51414, 51414, 400, 550, 9845, 276, 13, 400, 370, 437, 307, 341, 30, 51614, 51614, 1449, 14702, 341, 484, 11, 341, 486, 312, 383, 1804, 276, 1804, 462, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10719025737107402, "compression_ratio": 1.6105769230769231, "no_speech_prob": 6.013890742906369e-05}, {"id": 447, "seek": 242300, "start": 2423.0, "end": 2426.0, "text": " minus C minus E divided by h.", "tokens": [50364, 3175, 383, 3175, 462, 6666, 538, 276, 13, 50514, 50514, 400, 550, 291, 536, 510, 577, 383, 3175, 383, 393, 66, 1625, 11, 462, 3175, 462, 393, 66, 1625, 13, 50714, 50714, 492, 434, 1411, 365, 276, 670, 276, 11, 597, 307, 502, 13, 15, 13, 50864, 50864, 400, 370, 538, 25440, 611, 11, 30778, 538, 10113, 486, 312, 502, 13, 15, 382, 731, 13, 51364, 51364, 407, 1936, 11, 264, 13760, 295, 257, 2408, 6114, 307, 588, 2199, 13, 51514, 51514, 400, 341, 307, 264, 2654, 13760, 13, 51614, 51614, 407, 286, 818, 341, 264, 2654, 13760, 570, 321, 362, 264, 2572, 5598, 2158, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06974227712788714, "compression_ratio": 1.6233183856502242, "no_speech_prob": 2.0145484086242504e-05}, {"id": 448, "seek": 242300, "start": 2426.0, "end": 2430.0, "text": " And then you see here how C minus C cancels, E minus E cancels.", "tokens": [50364, 3175, 383, 3175, 462, 6666, 538, 276, 13, 50514, 50514, 400, 550, 291, 536, 510, 577, 383, 3175, 383, 393, 66, 1625, 11, 462, 3175, 462, 393, 66, 1625, 13, 50714, 50714, 492, 434, 1411, 365, 276, 670, 276, 11, 597, 307, 502, 13, 15, 13, 50864, 50864, 400, 370, 538, 25440, 611, 11, 30778, 538, 10113, 486, 312, 502, 13, 15, 382, 731, 13, 51364, 51364, 407, 1936, 11, 264, 13760, 295, 257, 2408, 6114, 307, 588, 2199, 13, 51514, 51514, 400, 341, 307, 264, 2654, 13760, 13, 51614, 51614, 407, 286, 818, 341, 264, 2654, 13760, 570, 321, 362, 264, 2572, 5598, 2158, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06974227712788714, "compression_ratio": 1.6233183856502242, "no_speech_prob": 2.0145484086242504e-05}, {"id": 449, "seek": 242300, "start": 2430.0, "end": 2433.0, "text": " We're left with h over h, which is 1.0.", "tokens": [50364, 3175, 383, 3175, 462, 6666, 538, 276, 13, 50514, 50514, 400, 550, 291, 536, 510, 577, 383, 3175, 383, 393, 66, 1625, 11, 462, 3175, 462, 393, 66, 1625, 13, 50714, 50714, 492, 434, 1411, 365, 276, 670, 276, 11, 597, 307, 502, 13, 15, 13, 50864, 50864, 400, 370, 538, 25440, 611, 11, 30778, 538, 10113, 486, 312, 502, 13, 15, 382, 731, 13, 51364, 51364, 407, 1936, 11, 264, 13760, 295, 257, 2408, 6114, 307, 588, 2199, 13, 51514, 51514, 400, 341, 307, 264, 2654, 13760, 13, 51614, 51614, 407, 286, 818, 341, 264, 2654, 13760, 570, 321, 362, 264, 2572, 5598, 2158, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06974227712788714, "compression_ratio": 1.6233183856502242, "no_speech_prob": 2.0145484086242504e-05}, {"id": 450, "seek": 242300, "start": 2433.0, "end": 2443.0, "text": " And so by symmetry also, DD by DE will be 1.0 as well.", "tokens": [50364, 3175, 383, 3175, 462, 6666, 538, 276, 13, 50514, 50514, 400, 550, 291, 536, 510, 577, 383, 3175, 383, 393, 66, 1625, 11, 462, 3175, 462, 393, 66, 1625, 13, 50714, 50714, 492, 434, 1411, 365, 276, 670, 276, 11, 597, 307, 502, 13, 15, 13, 50864, 50864, 400, 370, 538, 25440, 611, 11, 30778, 538, 10113, 486, 312, 502, 13, 15, 382, 731, 13, 51364, 51364, 407, 1936, 11, 264, 13760, 295, 257, 2408, 6114, 307, 588, 2199, 13, 51514, 51514, 400, 341, 307, 264, 2654, 13760, 13, 51614, 51614, 407, 286, 818, 341, 264, 2654, 13760, 570, 321, 362, 264, 2572, 5598, 2158, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06974227712788714, "compression_ratio": 1.6233183856502242, "no_speech_prob": 2.0145484086242504e-05}, {"id": 451, "seek": 242300, "start": 2443.0, "end": 2446.0, "text": " So basically, the derivative of a sum expression is very simple.", "tokens": [50364, 3175, 383, 3175, 462, 6666, 538, 276, 13, 50514, 50514, 400, 550, 291, 536, 510, 577, 383, 3175, 383, 393, 66, 1625, 11, 462, 3175, 462, 393, 66, 1625, 13, 50714, 50714, 492, 434, 1411, 365, 276, 670, 276, 11, 597, 307, 502, 13, 15, 13, 50864, 50864, 400, 370, 538, 25440, 611, 11, 30778, 538, 10113, 486, 312, 502, 13, 15, 382, 731, 13, 51364, 51364, 407, 1936, 11, 264, 13760, 295, 257, 2408, 6114, 307, 588, 2199, 13, 51514, 51514, 400, 341, 307, 264, 2654, 13760, 13, 51614, 51614, 407, 286, 818, 341, 264, 2654, 13760, 570, 321, 362, 264, 2572, 5598, 2158, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06974227712788714, "compression_ratio": 1.6233183856502242, "no_speech_prob": 2.0145484086242504e-05}, {"id": 452, "seek": 242300, "start": 2446.0, "end": 2448.0, "text": " And this is the local derivative.", "tokens": [50364, 3175, 383, 3175, 462, 6666, 538, 276, 13, 50514, 50514, 400, 550, 291, 536, 510, 577, 383, 3175, 383, 393, 66, 1625, 11, 462, 3175, 462, 393, 66, 1625, 13, 50714, 50714, 492, 434, 1411, 365, 276, 670, 276, 11, 597, 307, 502, 13, 15, 13, 50864, 50864, 400, 370, 538, 25440, 611, 11, 30778, 538, 10113, 486, 312, 502, 13, 15, 382, 731, 13, 51364, 51364, 407, 1936, 11, 264, 13760, 295, 257, 2408, 6114, 307, 588, 2199, 13, 51514, 51514, 400, 341, 307, 264, 2654, 13760, 13, 51614, 51614, 407, 286, 818, 341, 264, 2654, 13760, 570, 321, 362, 264, 2572, 5598, 2158, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06974227712788714, "compression_ratio": 1.6233183856502242, "no_speech_prob": 2.0145484086242504e-05}, {"id": 453, "seek": 242300, "start": 2448.0, "end": 2452.0, "text": " So I call this the local derivative because we have the final output value", "tokens": [50364, 3175, 383, 3175, 462, 6666, 538, 276, 13, 50514, 50514, 400, 550, 291, 536, 510, 577, 383, 3175, 383, 393, 66, 1625, 11, 462, 3175, 462, 393, 66, 1625, 13, 50714, 50714, 492, 434, 1411, 365, 276, 670, 276, 11, 597, 307, 502, 13, 15, 13, 50864, 50864, 400, 370, 538, 25440, 611, 11, 30778, 538, 10113, 486, 312, 502, 13, 15, 382, 731, 13, 51364, 51364, 407, 1936, 11, 264, 13760, 295, 257, 2408, 6114, 307, 588, 2199, 13, 51514, 51514, 400, 341, 307, 264, 2654, 13760, 13, 51614, 51614, 407, 286, 818, 341, 264, 2654, 13760, 570, 321, 362, 264, 2572, 5598, 2158, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06974227712788714, "compression_ratio": 1.6233183856502242, "no_speech_prob": 2.0145484086242504e-05}, {"id": 454, "seek": 245200, "start": 2452.0, "end": 2454.0, "text": " all the way at the end of this graph.", "tokens": [50364, 439, 264, 636, 412, 264, 917, 295, 341, 4295, 13, 50464, 50464, 400, 321, 434, 586, 411, 257, 1359, 9984, 510, 13, 50564, 50564, 400, 341, 307, 257, 707, 1804, 9984, 13, 50664, 50664, 400, 264, 707, 1804, 9984, 1177, 380, 458, 1340, 466, 264, 1472, 295, 264, 4295, 50864, 50864, 300, 309, 311, 16741, 294, 13, 50964, 50964, 1057, 309, 3255, 307, 300, 309, 630, 257, 1804, 13, 51064, 51064, 467, 1890, 257, 383, 293, 364, 462, 11, 3869, 552, 11, 293, 2942, 257, 413, 13, 51214, 51214, 400, 341, 1804, 9984, 611, 3255, 264, 2654, 6503, 295, 383, 322, 413, 11, 51414, 51414, 420, 2831, 264, 13760, 295, 413, 365, 3104, 281, 383, 13, 51564, 51564, 400, 309, 611, 3255, 264, 13760, 295, 413, 365, 3104, 281, 462, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06001216747142651, "compression_ratio": 1.9125, "no_speech_prob": 4.6837762056384236e-05}, {"id": 455, "seek": 245200, "start": 2454.0, "end": 2456.0, "text": " And we're now like a small node here.", "tokens": [50364, 439, 264, 636, 412, 264, 917, 295, 341, 4295, 13, 50464, 50464, 400, 321, 434, 586, 411, 257, 1359, 9984, 510, 13, 50564, 50564, 400, 341, 307, 257, 707, 1804, 9984, 13, 50664, 50664, 400, 264, 707, 1804, 9984, 1177, 380, 458, 1340, 466, 264, 1472, 295, 264, 4295, 50864, 50864, 300, 309, 311, 16741, 294, 13, 50964, 50964, 1057, 309, 3255, 307, 300, 309, 630, 257, 1804, 13, 51064, 51064, 467, 1890, 257, 383, 293, 364, 462, 11, 3869, 552, 11, 293, 2942, 257, 413, 13, 51214, 51214, 400, 341, 1804, 9984, 611, 3255, 264, 2654, 6503, 295, 383, 322, 413, 11, 51414, 51414, 420, 2831, 264, 13760, 295, 413, 365, 3104, 281, 383, 13, 51564, 51564, 400, 309, 611, 3255, 264, 13760, 295, 413, 365, 3104, 281, 462, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06001216747142651, "compression_ratio": 1.9125, "no_speech_prob": 4.6837762056384236e-05}, {"id": 456, "seek": 245200, "start": 2456.0, "end": 2458.0, "text": " And this is a little plus node.", "tokens": [50364, 439, 264, 636, 412, 264, 917, 295, 341, 4295, 13, 50464, 50464, 400, 321, 434, 586, 411, 257, 1359, 9984, 510, 13, 50564, 50564, 400, 341, 307, 257, 707, 1804, 9984, 13, 50664, 50664, 400, 264, 707, 1804, 9984, 1177, 380, 458, 1340, 466, 264, 1472, 295, 264, 4295, 50864, 50864, 300, 309, 311, 16741, 294, 13, 50964, 50964, 1057, 309, 3255, 307, 300, 309, 630, 257, 1804, 13, 51064, 51064, 467, 1890, 257, 383, 293, 364, 462, 11, 3869, 552, 11, 293, 2942, 257, 413, 13, 51214, 51214, 400, 341, 1804, 9984, 611, 3255, 264, 2654, 6503, 295, 383, 322, 413, 11, 51414, 51414, 420, 2831, 264, 13760, 295, 413, 365, 3104, 281, 383, 13, 51564, 51564, 400, 309, 611, 3255, 264, 13760, 295, 413, 365, 3104, 281, 462, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06001216747142651, "compression_ratio": 1.9125, "no_speech_prob": 4.6837762056384236e-05}, {"id": 457, "seek": 245200, "start": 2458.0, "end": 2462.0, "text": " And the little plus node doesn't know anything about the rest of the graph", "tokens": [50364, 439, 264, 636, 412, 264, 917, 295, 341, 4295, 13, 50464, 50464, 400, 321, 434, 586, 411, 257, 1359, 9984, 510, 13, 50564, 50564, 400, 341, 307, 257, 707, 1804, 9984, 13, 50664, 50664, 400, 264, 707, 1804, 9984, 1177, 380, 458, 1340, 466, 264, 1472, 295, 264, 4295, 50864, 50864, 300, 309, 311, 16741, 294, 13, 50964, 50964, 1057, 309, 3255, 307, 300, 309, 630, 257, 1804, 13, 51064, 51064, 467, 1890, 257, 383, 293, 364, 462, 11, 3869, 552, 11, 293, 2942, 257, 413, 13, 51214, 51214, 400, 341, 1804, 9984, 611, 3255, 264, 2654, 6503, 295, 383, 322, 413, 11, 51414, 51414, 420, 2831, 264, 13760, 295, 413, 365, 3104, 281, 383, 13, 51564, 51564, 400, 309, 611, 3255, 264, 13760, 295, 413, 365, 3104, 281, 462, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06001216747142651, "compression_ratio": 1.9125, "no_speech_prob": 4.6837762056384236e-05}, {"id": 458, "seek": 245200, "start": 2462.0, "end": 2464.0, "text": " that it's embedded in.", "tokens": [50364, 439, 264, 636, 412, 264, 917, 295, 341, 4295, 13, 50464, 50464, 400, 321, 434, 586, 411, 257, 1359, 9984, 510, 13, 50564, 50564, 400, 341, 307, 257, 707, 1804, 9984, 13, 50664, 50664, 400, 264, 707, 1804, 9984, 1177, 380, 458, 1340, 466, 264, 1472, 295, 264, 4295, 50864, 50864, 300, 309, 311, 16741, 294, 13, 50964, 50964, 1057, 309, 3255, 307, 300, 309, 630, 257, 1804, 13, 51064, 51064, 467, 1890, 257, 383, 293, 364, 462, 11, 3869, 552, 11, 293, 2942, 257, 413, 13, 51214, 51214, 400, 341, 1804, 9984, 611, 3255, 264, 2654, 6503, 295, 383, 322, 413, 11, 51414, 51414, 420, 2831, 264, 13760, 295, 413, 365, 3104, 281, 383, 13, 51564, 51564, 400, 309, 611, 3255, 264, 13760, 295, 413, 365, 3104, 281, 462, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06001216747142651, "compression_ratio": 1.9125, "no_speech_prob": 4.6837762056384236e-05}, {"id": 459, "seek": 245200, "start": 2464.0, "end": 2466.0, "text": " All it knows is that it did a plus.", "tokens": [50364, 439, 264, 636, 412, 264, 917, 295, 341, 4295, 13, 50464, 50464, 400, 321, 434, 586, 411, 257, 1359, 9984, 510, 13, 50564, 50564, 400, 341, 307, 257, 707, 1804, 9984, 13, 50664, 50664, 400, 264, 707, 1804, 9984, 1177, 380, 458, 1340, 466, 264, 1472, 295, 264, 4295, 50864, 50864, 300, 309, 311, 16741, 294, 13, 50964, 50964, 1057, 309, 3255, 307, 300, 309, 630, 257, 1804, 13, 51064, 51064, 467, 1890, 257, 383, 293, 364, 462, 11, 3869, 552, 11, 293, 2942, 257, 413, 13, 51214, 51214, 400, 341, 1804, 9984, 611, 3255, 264, 2654, 6503, 295, 383, 322, 413, 11, 51414, 51414, 420, 2831, 264, 13760, 295, 413, 365, 3104, 281, 383, 13, 51564, 51564, 400, 309, 611, 3255, 264, 13760, 295, 413, 365, 3104, 281, 462, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06001216747142651, "compression_ratio": 1.9125, "no_speech_prob": 4.6837762056384236e-05}, {"id": 460, "seek": 245200, "start": 2466.0, "end": 2469.0, "text": " It took a C and an E, added them, and created a D.", "tokens": [50364, 439, 264, 636, 412, 264, 917, 295, 341, 4295, 13, 50464, 50464, 400, 321, 434, 586, 411, 257, 1359, 9984, 510, 13, 50564, 50564, 400, 341, 307, 257, 707, 1804, 9984, 13, 50664, 50664, 400, 264, 707, 1804, 9984, 1177, 380, 458, 1340, 466, 264, 1472, 295, 264, 4295, 50864, 50864, 300, 309, 311, 16741, 294, 13, 50964, 50964, 1057, 309, 3255, 307, 300, 309, 630, 257, 1804, 13, 51064, 51064, 467, 1890, 257, 383, 293, 364, 462, 11, 3869, 552, 11, 293, 2942, 257, 413, 13, 51214, 51214, 400, 341, 1804, 9984, 611, 3255, 264, 2654, 6503, 295, 383, 322, 413, 11, 51414, 51414, 420, 2831, 264, 13760, 295, 413, 365, 3104, 281, 383, 13, 51564, 51564, 400, 309, 611, 3255, 264, 13760, 295, 413, 365, 3104, 281, 462, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06001216747142651, "compression_ratio": 1.9125, "no_speech_prob": 4.6837762056384236e-05}, {"id": 461, "seek": 245200, "start": 2469.0, "end": 2473.0, "text": " And this plus node also knows the local influence of C on D,", "tokens": [50364, 439, 264, 636, 412, 264, 917, 295, 341, 4295, 13, 50464, 50464, 400, 321, 434, 586, 411, 257, 1359, 9984, 510, 13, 50564, 50564, 400, 341, 307, 257, 707, 1804, 9984, 13, 50664, 50664, 400, 264, 707, 1804, 9984, 1177, 380, 458, 1340, 466, 264, 1472, 295, 264, 4295, 50864, 50864, 300, 309, 311, 16741, 294, 13, 50964, 50964, 1057, 309, 3255, 307, 300, 309, 630, 257, 1804, 13, 51064, 51064, 467, 1890, 257, 383, 293, 364, 462, 11, 3869, 552, 11, 293, 2942, 257, 413, 13, 51214, 51214, 400, 341, 1804, 9984, 611, 3255, 264, 2654, 6503, 295, 383, 322, 413, 11, 51414, 51414, 420, 2831, 264, 13760, 295, 413, 365, 3104, 281, 383, 13, 51564, 51564, 400, 309, 611, 3255, 264, 13760, 295, 413, 365, 3104, 281, 462, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06001216747142651, "compression_ratio": 1.9125, "no_speech_prob": 4.6837762056384236e-05}, {"id": 462, "seek": 245200, "start": 2473.0, "end": 2476.0, "text": " or rather the derivative of D with respect to C.", "tokens": [50364, 439, 264, 636, 412, 264, 917, 295, 341, 4295, 13, 50464, 50464, 400, 321, 434, 586, 411, 257, 1359, 9984, 510, 13, 50564, 50564, 400, 341, 307, 257, 707, 1804, 9984, 13, 50664, 50664, 400, 264, 707, 1804, 9984, 1177, 380, 458, 1340, 466, 264, 1472, 295, 264, 4295, 50864, 50864, 300, 309, 311, 16741, 294, 13, 50964, 50964, 1057, 309, 3255, 307, 300, 309, 630, 257, 1804, 13, 51064, 51064, 467, 1890, 257, 383, 293, 364, 462, 11, 3869, 552, 11, 293, 2942, 257, 413, 13, 51214, 51214, 400, 341, 1804, 9984, 611, 3255, 264, 2654, 6503, 295, 383, 322, 413, 11, 51414, 51414, 420, 2831, 264, 13760, 295, 413, 365, 3104, 281, 383, 13, 51564, 51564, 400, 309, 611, 3255, 264, 13760, 295, 413, 365, 3104, 281, 462, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06001216747142651, "compression_ratio": 1.9125, "no_speech_prob": 4.6837762056384236e-05}, {"id": 463, "seek": 245200, "start": 2476.0, "end": 2480.0, "text": " And it also knows the derivative of D with respect to E.", "tokens": [50364, 439, 264, 636, 412, 264, 917, 295, 341, 4295, 13, 50464, 50464, 400, 321, 434, 586, 411, 257, 1359, 9984, 510, 13, 50564, 50564, 400, 341, 307, 257, 707, 1804, 9984, 13, 50664, 50664, 400, 264, 707, 1804, 9984, 1177, 380, 458, 1340, 466, 264, 1472, 295, 264, 4295, 50864, 50864, 300, 309, 311, 16741, 294, 13, 50964, 50964, 1057, 309, 3255, 307, 300, 309, 630, 257, 1804, 13, 51064, 51064, 467, 1890, 257, 383, 293, 364, 462, 11, 3869, 552, 11, 293, 2942, 257, 413, 13, 51214, 51214, 400, 341, 1804, 9984, 611, 3255, 264, 2654, 6503, 295, 383, 322, 413, 11, 51414, 51414, 420, 2831, 264, 13760, 295, 413, 365, 3104, 281, 383, 13, 51564, 51564, 400, 309, 611, 3255, 264, 13760, 295, 413, 365, 3104, 281, 462, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06001216747142651, "compression_ratio": 1.9125, "no_speech_prob": 4.6837762056384236e-05}, {"id": 464, "seek": 248000, "start": 2480.0, "end": 2483.0, "text": " But that's not what we want. That's just a local derivative.", "tokens": [50364, 583, 300, 311, 406, 437, 321, 528, 13, 663, 311, 445, 257, 2654, 13760, 13, 50514, 50514, 708, 321, 767, 528, 307, 413, 43, 538, 9114, 13, 50664, 50664, 400, 441, 307, 510, 445, 472, 1823, 1314, 13, 50814, 50814, 583, 294, 257, 2674, 1389, 11, 341, 707, 1804, 9984, 727, 312, 16741, 294, 257, 5994, 4295, 13, 51114, 51114, 407, 797, 11, 321, 458, 577, 441, 11606, 413, 13, 51264, 51264, 400, 586, 321, 458, 577, 383, 293, 462, 2712, 413, 13, 51414, 51414, 1012, 360, 321, 829, 300, 1589, 1214, 281, 2464, 413, 43, 538, 9114, 30, 51564, 51564, 400, 264, 1867, 11, 295, 1164, 11, 307, 264, 5021, 4978, 294, 33400, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.056903899216852266, "compression_ratio": 1.5826771653543308, "no_speech_prob": 1.5206426724034827e-05}, {"id": 465, "seek": 248000, "start": 2483.0, "end": 2486.0, "text": " What we actually want is DL by DC.", "tokens": [50364, 583, 300, 311, 406, 437, 321, 528, 13, 663, 311, 445, 257, 2654, 13760, 13, 50514, 50514, 708, 321, 767, 528, 307, 413, 43, 538, 9114, 13, 50664, 50664, 400, 441, 307, 510, 445, 472, 1823, 1314, 13, 50814, 50814, 583, 294, 257, 2674, 1389, 11, 341, 707, 1804, 9984, 727, 312, 16741, 294, 257, 5994, 4295, 13, 51114, 51114, 407, 797, 11, 321, 458, 577, 441, 11606, 413, 13, 51264, 51264, 400, 586, 321, 458, 577, 383, 293, 462, 2712, 413, 13, 51414, 51414, 1012, 360, 321, 829, 300, 1589, 1214, 281, 2464, 413, 43, 538, 9114, 30, 51564, 51564, 400, 264, 1867, 11, 295, 1164, 11, 307, 264, 5021, 4978, 294, 33400, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.056903899216852266, "compression_ratio": 1.5826771653543308, "no_speech_prob": 1.5206426724034827e-05}, {"id": 466, "seek": 248000, "start": 2486.0, "end": 2489.0, "text": " And L is here just one step away.", "tokens": [50364, 583, 300, 311, 406, 437, 321, 528, 13, 663, 311, 445, 257, 2654, 13760, 13, 50514, 50514, 708, 321, 767, 528, 307, 413, 43, 538, 9114, 13, 50664, 50664, 400, 441, 307, 510, 445, 472, 1823, 1314, 13, 50814, 50814, 583, 294, 257, 2674, 1389, 11, 341, 707, 1804, 9984, 727, 312, 16741, 294, 257, 5994, 4295, 13, 51114, 51114, 407, 797, 11, 321, 458, 577, 441, 11606, 413, 13, 51264, 51264, 400, 586, 321, 458, 577, 383, 293, 462, 2712, 413, 13, 51414, 51414, 1012, 360, 321, 829, 300, 1589, 1214, 281, 2464, 413, 43, 538, 9114, 30, 51564, 51564, 400, 264, 1867, 11, 295, 1164, 11, 307, 264, 5021, 4978, 294, 33400, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.056903899216852266, "compression_ratio": 1.5826771653543308, "no_speech_prob": 1.5206426724034827e-05}, {"id": 467, "seek": 248000, "start": 2489.0, "end": 2495.0, "text": " But in a general case, this little plus node could be embedded in a massive graph.", "tokens": [50364, 583, 300, 311, 406, 437, 321, 528, 13, 663, 311, 445, 257, 2654, 13760, 13, 50514, 50514, 708, 321, 767, 528, 307, 413, 43, 538, 9114, 13, 50664, 50664, 400, 441, 307, 510, 445, 472, 1823, 1314, 13, 50814, 50814, 583, 294, 257, 2674, 1389, 11, 341, 707, 1804, 9984, 727, 312, 16741, 294, 257, 5994, 4295, 13, 51114, 51114, 407, 797, 11, 321, 458, 577, 441, 11606, 413, 13, 51264, 51264, 400, 586, 321, 458, 577, 383, 293, 462, 2712, 413, 13, 51414, 51414, 1012, 360, 321, 829, 300, 1589, 1214, 281, 2464, 413, 43, 538, 9114, 30, 51564, 51564, 400, 264, 1867, 11, 295, 1164, 11, 307, 264, 5021, 4978, 294, 33400, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.056903899216852266, "compression_ratio": 1.5826771653543308, "no_speech_prob": 1.5206426724034827e-05}, {"id": 468, "seek": 248000, "start": 2495.0, "end": 2498.0, "text": " So again, we know how L impacts D.", "tokens": [50364, 583, 300, 311, 406, 437, 321, 528, 13, 663, 311, 445, 257, 2654, 13760, 13, 50514, 50514, 708, 321, 767, 528, 307, 413, 43, 538, 9114, 13, 50664, 50664, 400, 441, 307, 510, 445, 472, 1823, 1314, 13, 50814, 50814, 583, 294, 257, 2674, 1389, 11, 341, 707, 1804, 9984, 727, 312, 16741, 294, 257, 5994, 4295, 13, 51114, 51114, 407, 797, 11, 321, 458, 577, 441, 11606, 413, 13, 51264, 51264, 400, 586, 321, 458, 577, 383, 293, 462, 2712, 413, 13, 51414, 51414, 1012, 360, 321, 829, 300, 1589, 1214, 281, 2464, 413, 43, 538, 9114, 30, 51564, 51564, 400, 264, 1867, 11, 295, 1164, 11, 307, 264, 5021, 4978, 294, 33400, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.056903899216852266, "compression_ratio": 1.5826771653543308, "no_speech_prob": 1.5206426724034827e-05}, {"id": 469, "seek": 248000, "start": 2498.0, "end": 2501.0, "text": " And now we know how C and E impact D.", "tokens": [50364, 583, 300, 311, 406, 437, 321, 528, 13, 663, 311, 445, 257, 2654, 13760, 13, 50514, 50514, 708, 321, 767, 528, 307, 413, 43, 538, 9114, 13, 50664, 50664, 400, 441, 307, 510, 445, 472, 1823, 1314, 13, 50814, 50814, 583, 294, 257, 2674, 1389, 11, 341, 707, 1804, 9984, 727, 312, 16741, 294, 257, 5994, 4295, 13, 51114, 51114, 407, 797, 11, 321, 458, 577, 441, 11606, 413, 13, 51264, 51264, 400, 586, 321, 458, 577, 383, 293, 462, 2712, 413, 13, 51414, 51414, 1012, 360, 321, 829, 300, 1589, 1214, 281, 2464, 413, 43, 538, 9114, 30, 51564, 51564, 400, 264, 1867, 11, 295, 1164, 11, 307, 264, 5021, 4978, 294, 33400, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.056903899216852266, "compression_ratio": 1.5826771653543308, "no_speech_prob": 1.5206426724034827e-05}, {"id": 470, "seek": 248000, "start": 2501.0, "end": 2504.0, "text": " How do we put that information together to write DL by DC?", "tokens": [50364, 583, 300, 311, 406, 437, 321, 528, 13, 663, 311, 445, 257, 2654, 13760, 13, 50514, 50514, 708, 321, 767, 528, 307, 413, 43, 538, 9114, 13, 50664, 50664, 400, 441, 307, 510, 445, 472, 1823, 1314, 13, 50814, 50814, 583, 294, 257, 2674, 1389, 11, 341, 707, 1804, 9984, 727, 312, 16741, 294, 257, 5994, 4295, 13, 51114, 51114, 407, 797, 11, 321, 458, 577, 441, 11606, 413, 13, 51264, 51264, 400, 586, 321, 458, 577, 383, 293, 462, 2712, 413, 13, 51414, 51414, 1012, 360, 321, 829, 300, 1589, 1214, 281, 2464, 413, 43, 538, 9114, 30, 51564, 51564, 400, 264, 1867, 11, 295, 1164, 11, 307, 264, 5021, 4978, 294, 33400, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.056903899216852266, "compression_ratio": 1.5826771653543308, "no_speech_prob": 1.5206426724034827e-05}, {"id": 471, "seek": 248000, "start": 2504.0, "end": 2508.0, "text": " And the answer, of course, is the chain rule in calculus.", "tokens": [50364, 583, 300, 311, 406, 437, 321, 528, 13, 663, 311, 445, 257, 2654, 13760, 13, 50514, 50514, 708, 321, 767, 528, 307, 413, 43, 538, 9114, 13, 50664, 50664, 400, 441, 307, 510, 445, 472, 1823, 1314, 13, 50814, 50814, 583, 294, 257, 2674, 1389, 11, 341, 707, 1804, 9984, 727, 312, 16741, 294, 257, 5994, 4295, 13, 51114, 51114, 407, 797, 11, 321, 458, 577, 441, 11606, 413, 13, 51264, 51264, 400, 586, 321, 458, 577, 383, 293, 462, 2712, 413, 13, 51414, 51414, 1012, 360, 321, 829, 300, 1589, 1214, 281, 2464, 413, 43, 538, 9114, 30, 51564, 51564, 400, 264, 1867, 11, 295, 1164, 11, 307, 264, 5021, 4978, 294, 33400, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.056903899216852266, "compression_ratio": 1.5826771653543308, "no_speech_prob": 1.5206426724034827e-05}, {"id": 472, "seek": 250800, "start": 2508.0, "end": 2513.0, "text": " And so I pulled up a chain rule here from Wikipedia.", "tokens": [50364, 400, 370, 286, 7373, 493, 257, 5021, 4978, 510, 490, 28999, 13, 50614, 50614, 400, 286, 478, 516, 281, 352, 807, 341, 588, 10515, 13, 50764, 50764, 407, 5021, 4978, 11, 28999, 2171, 393, 312, 588, 13181, 11, 50914, 50914, 293, 33400, 393, 312, 588, 13181, 13, 51064, 51064, 639, 307, 264, 636, 286, 3264, 5021, 4978, 11, 293, 309, 390, 588, 13181, 13, 51314, 51314, 708, 307, 2737, 30, 467, 311, 445, 6179, 13, 51464, 51464, 407, 286, 411, 341, 6114, 709, 1101, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09824350978551286, "compression_ratio": 1.645631067961165, "no_speech_prob": 2.753494118223898e-05}, {"id": 473, "seek": 250800, "start": 2513.0, "end": 2516.0, "text": " And I'm going to go through this very briefly.", "tokens": [50364, 400, 370, 286, 7373, 493, 257, 5021, 4978, 510, 490, 28999, 13, 50614, 50614, 400, 286, 478, 516, 281, 352, 807, 341, 588, 10515, 13, 50764, 50764, 407, 5021, 4978, 11, 28999, 2171, 393, 312, 588, 13181, 11, 50914, 50914, 293, 33400, 393, 312, 588, 13181, 13, 51064, 51064, 639, 307, 264, 636, 286, 3264, 5021, 4978, 11, 293, 309, 390, 588, 13181, 13, 51314, 51314, 708, 307, 2737, 30, 467, 311, 445, 6179, 13, 51464, 51464, 407, 286, 411, 341, 6114, 709, 1101, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09824350978551286, "compression_ratio": 1.645631067961165, "no_speech_prob": 2.753494118223898e-05}, {"id": 474, "seek": 250800, "start": 2516.0, "end": 2519.0, "text": " So chain rule, Wikipedia sometimes can be very confusing,", "tokens": [50364, 400, 370, 286, 7373, 493, 257, 5021, 4978, 510, 490, 28999, 13, 50614, 50614, 400, 286, 478, 516, 281, 352, 807, 341, 588, 10515, 13, 50764, 50764, 407, 5021, 4978, 11, 28999, 2171, 393, 312, 588, 13181, 11, 50914, 50914, 293, 33400, 393, 312, 588, 13181, 13, 51064, 51064, 639, 307, 264, 636, 286, 3264, 5021, 4978, 11, 293, 309, 390, 588, 13181, 13, 51314, 51314, 708, 307, 2737, 30, 467, 311, 445, 6179, 13, 51464, 51464, 407, 286, 411, 341, 6114, 709, 1101, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09824350978551286, "compression_ratio": 1.645631067961165, "no_speech_prob": 2.753494118223898e-05}, {"id": 475, "seek": 250800, "start": 2519.0, "end": 2522.0, "text": " and calculus can be very confusing.", "tokens": [50364, 400, 370, 286, 7373, 493, 257, 5021, 4978, 510, 490, 28999, 13, 50614, 50614, 400, 286, 478, 516, 281, 352, 807, 341, 588, 10515, 13, 50764, 50764, 407, 5021, 4978, 11, 28999, 2171, 393, 312, 588, 13181, 11, 50914, 50914, 293, 33400, 393, 312, 588, 13181, 13, 51064, 51064, 639, 307, 264, 636, 286, 3264, 5021, 4978, 11, 293, 309, 390, 588, 13181, 13, 51314, 51314, 708, 307, 2737, 30, 467, 311, 445, 6179, 13, 51464, 51464, 407, 286, 411, 341, 6114, 709, 1101, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09824350978551286, "compression_ratio": 1.645631067961165, "no_speech_prob": 2.753494118223898e-05}, {"id": 476, "seek": 250800, "start": 2522.0, "end": 2527.0, "text": " This is the way I learned chain rule, and it was very confusing.", "tokens": [50364, 400, 370, 286, 7373, 493, 257, 5021, 4978, 510, 490, 28999, 13, 50614, 50614, 400, 286, 478, 516, 281, 352, 807, 341, 588, 10515, 13, 50764, 50764, 407, 5021, 4978, 11, 28999, 2171, 393, 312, 588, 13181, 11, 50914, 50914, 293, 33400, 393, 312, 588, 13181, 13, 51064, 51064, 639, 307, 264, 636, 286, 3264, 5021, 4978, 11, 293, 309, 390, 588, 13181, 13, 51314, 51314, 708, 307, 2737, 30, 467, 311, 445, 6179, 13, 51464, 51464, 407, 286, 411, 341, 6114, 709, 1101, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09824350978551286, "compression_ratio": 1.645631067961165, "no_speech_prob": 2.753494118223898e-05}, {"id": 477, "seek": 250800, "start": 2527.0, "end": 2530.0, "text": " What is happening? It's just complicated.", "tokens": [50364, 400, 370, 286, 7373, 493, 257, 5021, 4978, 510, 490, 28999, 13, 50614, 50614, 400, 286, 478, 516, 281, 352, 807, 341, 588, 10515, 13, 50764, 50764, 407, 5021, 4978, 11, 28999, 2171, 393, 312, 588, 13181, 11, 50914, 50914, 293, 33400, 393, 312, 588, 13181, 13, 51064, 51064, 639, 307, 264, 636, 286, 3264, 5021, 4978, 11, 293, 309, 390, 588, 13181, 13, 51314, 51314, 708, 307, 2737, 30, 467, 311, 445, 6179, 13, 51464, 51464, 407, 286, 411, 341, 6114, 709, 1101, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09824350978551286, "compression_ratio": 1.645631067961165, "no_speech_prob": 2.753494118223898e-05}, {"id": 478, "seek": 250800, "start": 2530.0, "end": 2533.0, "text": " So I like this expression much better.", "tokens": [50364, 400, 370, 286, 7373, 493, 257, 5021, 4978, 510, 490, 28999, 13, 50614, 50614, 400, 286, 478, 516, 281, 352, 807, 341, 588, 10515, 13, 50764, 50764, 407, 5021, 4978, 11, 28999, 2171, 393, 312, 588, 13181, 11, 50914, 50914, 293, 33400, 393, 312, 588, 13181, 13, 51064, 51064, 639, 307, 264, 636, 286, 3264, 5021, 4978, 11, 293, 309, 390, 588, 13181, 13, 51314, 51314, 708, 307, 2737, 30, 467, 311, 445, 6179, 13, 51464, 51464, 407, 286, 411, 341, 6114, 709, 1101, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09824350978551286, "compression_ratio": 1.645631067961165, "no_speech_prob": 2.753494118223898e-05}, {"id": 479, "seek": 253300, "start": 2533.0, "end": 2538.0, "text": " If a variable z depends on a variable y, which itself depends on a variable x,", "tokens": [50364, 759, 257, 7006, 710, 5946, 322, 257, 7006, 288, 11, 597, 2564, 5946, 322, 257, 7006, 2031, 11, 50614, 50614, 550, 710, 5946, 322, 2031, 382, 731, 11, 2745, 11, 807, 264, 19376, 7006, 288, 13, 50814, 50814, 400, 294, 341, 1389, 11, 264, 5021, 4978, 307, 12675, 382, 11, 50964, 50964, 498, 291, 528, 9758, 538, 30017, 11, 550, 291, 747, 264, 9758, 538, 14584, 11, 51214, 51214, 293, 291, 12972, 309, 538, 14584, 538, 30017, 13, 51414, 51414, 407, 264, 5021, 4978, 17879, 307, 3585, 291, 577, 321, 5021, 613, 33733, 1214, 8944, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07964139938354492, "compression_ratio": 1.787037037037037, "no_speech_prob": 2.3186927137430757e-05}, {"id": 480, "seek": 253300, "start": 2538.0, "end": 2542.0, "text": " then z depends on x as well, obviously, through the intermediate variable y.", "tokens": [50364, 759, 257, 7006, 710, 5946, 322, 257, 7006, 288, 11, 597, 2564, 5946, 322, 257, 7006, 2031, 11, 50614, 50614, 550, 710, 5946, 322, 2031, 382, 731, 11, 2745, 11, 807, 264, 19376, 7006, 288, 13, 50814, 50814, 400, 294, 341, 1389, 11, 264, 5021, 4978, 307, 12675, 382, 11, 50964, 50964, 498, 291, 528, 9758, 538, 30017, 11, 550, 291, 747, 264, 9758, 538, 14584, 11, 51214, 51214, 293, 291, 12972, 309, 538, 14584, 538, 30017, 13, 51414, 51414, 407, 264, 5021, 4978, 17879, 307, 3585, 291, 577, 321, 5021, 613, 33733, 1214, 8944, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07964139938354492, "compression_ratio": 1.787037037037037, "no_speech_prob": 2.3186927137430757e-05}, {"id": 481, "seek": 253300, "start": 2542.0, "end": 2545.0, "text": " And in this case, the chain rule is expressed as,", "tokens": [50364, 759, 257, 7006, 710, 5946, 322, 257, 7006, 288, 11, 597, 2564, 5946, 322, 257, 7006, 2031, 11, 50614, 50614, 550, 710, 5946, 322, 2031, 382, 731, 11, 2745, 11, 807, 264, 19376, 7006, 288, 13, 50814, 50814, 400, 294, 341, 1389, 11, 264, 5021, 4978, 307, 12675, 382, 11, 50964, 50964, 498, 291, 528, 9758, 538, 30017, 11, 550, 291, 747, 264, 9758, 538, 14584, 11, 51214, 51214, 293, 291, 12972, 309, 538, 14584, 538, 30017, 13, 51414, 51414, 407, 264, 5021, 4978, 17879, 307, 3585, 291, 577, 321, 5021, 613, 33733, 1214, 8944, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07964139938354492, "compression_ratio": 1.787037037037037, "no_speech_prob": 2.3186927137430757e-05}, {"id": 482, "seek": 253300, "start": 2545.0, "end": 2550.0, "text": " if you want dz by dx, then you take the dz by dy,", "tokens": [50364, 759, 257, 7006, 710, 5946, 322, 257, 7006, 288, 11, 597, 2564, 5946, 322, 257, 7006, 2031, 11, 50614, 50614, 550, 710, 5946, 322, 2031, 382, 731, 11, 2745, 11, 807, 264, 19376, 7006, 288, 13, 50814, 50814, 400, 294, 341, 1389, 11, 264, 5021, 4978, 307, 12675, 382, 11, 50964, 50964, 498, 291, 528, 9758, 538, 30017, 11, 550, 291, 747, 264, 9758, 538, 14584, 11, 51214, 51214, 293, 291, 12972, 309, 538, 14584, 538, 30017, 13, 51414, 51414, 407, 264, 5021, 4978, 17879, 307, 3585, 291, 577, 321, 5021, 613, 33733, 1214, 8944, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07964139938354492, "compression_ratio": 1.787037037037037, "no_speech_prob": 2.3186927137430757e-05}, {"id": 483, "seek": 253300, "start": 2550.0, "end": 2554.0, "text": " and you multiply it by dy by dx.", "tokens": [50364, 759, 257, 7006, 710, 5946, 322, 257, 7006, 288, 11, 597, 2564, 5946, 322, 257, 7006, 2031, 11, 50614, 50614, 550, 710, 5946, 322, 2031, 382, 731, 11, 2745, 11, 807, 264, 19376, 7006, 288, 13, 50814, 50814, 400, 294, 341, 1389, 11, 264, 5021, 4978, 307, 12675, 382, 11, 50964, 50964, 498, 291, 528, 9758, 538, 30017, 11, 550, 291, 747, 264, 9758, 538, 14584, 11, 51214, 51214, 293, 291, 12972, 309, 538, 14584, 538, 30017, 13, 51414, 51414, 407, 264, 5021, 4978, 17879, 307, 3585, 291, 577, 321, 5021, 613, 33733, 1214, 8944, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07964139938354492, "compression_ratio": 1.787037037037037, "no_speech_prob": 2.3186927137430757e-05}, {"id": 484, "seek": 253300, "start": 2554.0, "end": 2562.0, "text": " So the chain rule fundamentally is telling you how we chain these derivatives together correctly.", "tokens": [50364, 759, 257, 7006, 710, 5946, 322, 257, 7006, 288, 11, 597, 2564, 5946, 322, 257, 7006, 2031, 11, 50614, 50614, 550, 710, 5946, 322, 2031, 382, 731, 11, 2745, 11, 807, 264, 19376, 7006, 288, 13, 50814, 50814, 400, 294, 341, 1389, 11, 264, 5021, 4978, 307, 12675, 382, 11, 50964, 50964, 498, 291, 528, 9758, 538, 30017, 11, 550, 291, 747, 264, 9758, 538, 14584, 11, 51214, 51214, 293, 291, 12972, 309, 538, 14584, 538, 30017, 13, 51414, 51414, 407, 264, 5021, 4978, 17879, 307, 3585, 291, 577, 321, 5021, 613, 33733, 1214, 8944, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07964139938354492, "compression_ratio": 1.787037037037037, "no_speech_prob": 2.3186927137430757e-05}, {"id": 485, "seek": 256200, "start": 2562.0, "end": 2566.0, "text": " So to differentiate through a function composition,", "tokens": [50364, 407, 281, 23203, 807, 257, 2445, 12686, 11, 50564, 50564, 321, 362, 281, 3079, 257, 27290, 295, 729, 33733, 13, 50814, 50814, 407, 300, 311, 534, 437, 5021, 4978, 307, 3585, 505, 13, 50964, 50964, 400, 456, 311, 257, 1481, 707, 21769, 10835, 510, 11, 597, 286, 611, 519, 307, 733, 295, 4052, 13, 51214, 51214, 440, 5021, 4978, 4368, 300, 5276, 264, 45596, 3314, 295, 1319, 295, 710, 365, 3104, 281, 288, 11, 51414, 51414, 293, 288, 4972, 281, 2031, 4045, 472, 281, 8873, 264, 45596, 3314, 295, 1319, 295, 710, 4972, 281, 2031, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.053858154951924024, "compression_ratio": 1.7601626016260163, "no_speech_prob": 1.1125316632387694e-05}, {"id": 486, "seek": 256200, "start": 2566.0, "end": 2571.0, "text": " we have to apply a multiplication of those derivatives.", "tokens": [50364, 407, 281, 23203, 807, 257, 2445, 12686, 11, 50564, 50564, 321, 362, 281, 3079, 257, 27290, 295, 729, 33733, 13, 50814, 50814, 407, 300, 311, 534, 437, 5021, 4978, 307, 3585, 505, 13, 50964, 50964, 400, 456, 311, 257, 1481, 707, 21769, 10835, 510, 11, 597, 286, 611, 519, 307, 733, 295, 4052, 13, 51214, 51214, 440, 5021, 4978, 4368, 300, 5276, 264, 45596, 3314, 295, 1319, 295, 710, 365, 3104, 281, 288, 11, 51414, 51414, 293, 288, 4972, 281, 2031, 4045, 472, 281, 8873, 264, 45596, 3314, 295, 1319, 295, 710, 4972, 281, 2031, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.053858154951924024, "compression_ratio": 1.7601626016260163, "no_speech_prob": 1.1125316632387694e-05}, {"id": 487, "seek": 256200, "start": 2571.0, "end": 2574.0, "text": " So that's really what chain rule is telling us.", "tokens": [50364, 407, 281, 23203, 807, 257, 2445, 12686, 11, 50564, 50564, 321, 362, 281, 3079, 257, 27290, 295, 729, 33733, 13, 50814, 50814, 407, 300, 311, 534, 437, 5021, 4978, 307, 3585, 505, 13, 50964, 50964, 400, 456, 311, 257, 1481, 707, 21769, 10835, 510, 11, 597, 286, 611, 519, 307, 733, 295, 4052, 13, 51214, 51214, 440, 5021, 4978, 4368, 300, 5276, 264, 45596, 3314, 295, 1319, 295, 710, 365, 3104, 281, 288, 11, 51414, 51414, 293, 288, 4972, 281, 2031, 4045, 472, 281, 8873, 264, 45596, 3314, 295, 1319, 295, 710, 4972, 281, 2031, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.053858154951924024, "compression_ratio": 1.7601626016260163, "no_speech_prob": 1.1125316632387694e-05}, {"id": 488, "seek": 256200, "start": 2574.0, "end": 2579.0, "text": " And there's a nice little intuitive explanation here, which I also think is kind of cute.", "tokens": [50364, 407, 281, 23203, 807, 257, 2445, 12686, 11, 50564, 50564, 321, 362, 281, 3079, 257, 27290, 295, 729, 33733, 13, 50814, 50814, 407, 300, 311, 534, 437, 5021, 4978, 307, 3585, 505, 13, 50964, 50964, 400, 456, 311, 257, 1481, 707, 21769, 10835, 510, 11, 597, 286, 611, 519, 307, 733, 295, 4052, 13, 51214, 51214, 440, 5021, 4978, 4368, 300, 5276, 264, 45596, 3314, 295, 1319, 295, 710, 365, 3104, 281, 288, 11, 51414, 51414, 293, 288, 4972, 281, 2031, 4045, 472, 281, 8873, 264, 45596, 3314, 295, 1319, 295, 710, 4972, 281, 2031, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.053858154951924024, "compression_ratio": 1.7601626016260163, "no_speech_prob": 1.1125316632387694e-05}, {"id": 489, "seek": 256200, "start": 2579.0, "end": 2583.0, "text": " The chain rule states that knowing the instantaneous rate of change of z with respect to y,", "tokens": [50364, 407, 281, 23203, 807, 257, 2445, 12686, 11, 50564, 50564, 321, 362, 281, 3079, 257, 27290, 295, 729, 33733, 13, 50814, 50814, 407, 300, 311, 534, 437, 5021, 4978, 307, 3585, 505, 13, 50964, 50964, 400, 456, 311, 257, 1481, 707, 21769, 10835, 510, 11, 597, 286, 611, 519, 307, 733, 295, 4052, 13, 51214, 51214, 440, 5021, 4978, 4368, 300, 5276, 264, 45596, 3314, 295, 1319, 295, 710, 365, 3104, 281, 288, 11, 51414, 51414, 293, 288, 4972, 281, 2031, 4045, 472, 281, 8873, 264, 45596, 3314, 295, 1319, 295, 710, 4972, 281, 2031, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.053858154951924024, "compression_ratio": 1.7601626016260163, "no_speech_prob": 1.1125316632387694e-05}, {"id": 490, "seek": 256200, "start": 2583.0, "end": 2588.0, "text": " and y relative to x allows one to calculate the instantaneous rate of change of z relative to x", "tokens": [50364, 407, 281, 23203, 807, 257, 2445, 12686, 11, 50564, 50564, 321, 362, 281, 3079, 257, 27290, 295, 729, 33733, 13, 50814, 50814, 407, 300, 311, 534, 437, 5021, 4978, 307, 3585, 505, 13, 50964, 50964, 400, 456, 311, 257, 1481, 707, 21769, 10835, 510, 11, 597, 286, 611, 519, 307, 733, 295, 4052, 13, 51214, 51214, 440, 5021, 4978, 4368, 300, 5276, 264, 45596, 3314, 295, 1319, 295, 710, 365, 3104, 281, 288, 11, 51414, 51414, 293, 288, 4972, 281, 2031, 4045, 472, 281, 8873, 264, 45596, 3314, 295, 1319, 295, 710, 4972, 281, 2031, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.053858154951924024, "compression_ratio": 1.7601626016260163, "no_speech_prob": 1.1125316632387694e-05}, {"id": 491, "seek": 258800, "start": 2588.0, "end": 2593.0, "text": " as a product of those two rates of change, simply the product of those two.", "tokens": [50364, 382, 257, 1674, 295, 729, 732, 6846, 295, 1319, 11, 2935, 264, 1674, 295, 729, 732, 13, 50614, 50614, 407, 510, 311, 257, 665, 472, 13, 50714, 50714, 759, 257, 1032, 19863, 6091, 382, 2370, 382, 257, 20888, 11, 293, 264, 20888, 307, 1451, 1413, 382, 2370, 382, 257, 4494, 587, 11, 50964, 50964, 550, 264, 1032, 19863, 732, 1413, 1451, 11, 3180, 1413, 382, 2370, 382, 257, 587, 13, 51214, 51214, 400, 370, 341, 1669, 309, 588, 1850, 300, 264, 3006, 551, 281, 360, 11, 1333, 295, 11, 307, 281, 12972, 13, 51514, 51514, 407, 1032, 307, 6091, 382, 2370, 382, 257, 20888, 11, 293, 20888, 307, 1451, 1413, 382, 2370, 382, 587, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07367299985484917, "compression_ratio": 2.0669856459330145, "no_speech_prob": 7.483737135771662e-05}, {"id": 492, "seek": 258800, "start": 2593.0, "end": 2595.0, "text": " So here's a good one.", "tokens": [50364, 382, 257, 1674, 295, 729, 732, 6846, 295, 1319, 11, 2935, 264, 1674, 295, 729, 732, 13, 50614, 50614, 407, 510, 311, 257, 665, 472, 13, 50714, 50714, 759, 257, 1032, 19863, 6091, 382, 2370, 382, 257, 20888, 11, 293, 264, 20888, 307, 1451, 1413, 382, 2370, 382, 257, 4494, 587, 11, 50964, 50964, 550, 264, 1032, 19863, 732, 1413, 1451, 11, 3180, 1413, 382, 2370, 382, 257, 587, 13, 51214, 51214, 400, 370, 341, 1669, 309, 588, 1850, 300, 264, 3006, 551, 281, 360, 11, 1333, 295, 11, 307, 281, 12972, 13, 51514, 51514, 407, 1032, 307, 6091, 382, 2370, 382, 257, 20888, 11, 293, 20888, 307, 1451, 1413, 382, 2370, 382, 587, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07367299985484917, "compression_ratio": 2.0669856459330145, "no_speech_prob": 7.483737135771662e-05}, {"id": 493, "seek": 258800, "start": 2595.0, "end": 2600.0, "text": " If a car travels twice as fast as a bicycle, and the bicycle is four times as fast as a walking man,", "tokens": [50364, 382, 257, 1674, 295, 729, 732, 6846, 295, 1319, 11, 2935, 264, 1674, 295, 729, 732, 13, 50614, 50614, 407, 510, 311, 257, 665, 472, 13, 50714, 50714, 759, 257, 1032, 19863, 6091, 382, 2370, 382, 257, 20888, 11, 293, 264, 20888, 307, 1451, 1413, 382, 2370, 382, 257, 4494, 587, 11, 50964, 50964, 550, 264, 1032, 19863, 732, 1413, 1451, 11, 3180, 1413, 382, 2370, 382, 257, 587, 13, 51214, 51214, 400, 370, 341, 1669, 309, 588, 1850, 300, 264, 3006, 551, 281, 360, 11, 1333, 295, 11, 307, 281, 12972, 13, 51514, 51514, 407, 1032, 307, 6091, 382, 2370, 382, 257, 20888, 11, 293, 20888, 307, 1451, 1413, 382, 2370, 382, 587, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07367299985484917, "compression_ratio": 2.0669856459330145, "no_speech_prob": 7.483737135771662e-05}, {"id": 494, "seek": 258800, "start": 2600.0, "end": 2605.0, "text": " then the car travels two times four, eight times as fast as a man.", "tokens": [50364, 382, 257, 1674, 295, 729, 732, 6846, 295, 1319, 11, 2935, 264, 1674, 295, 729, 732, 13, 50614, 50614, 407, 510, 311, 257, 665, 472, 13, 50714, 50714, 759, 257, 1032, 19863, 6091, 382, 2370, 382, 257, 20888, 11, 293, 264, 20888, 307, 1451, 1413, 382, 2370, 382, 257, 4494, 587, 11, 50964, 50964, 550, 264, 1032, 19863, 732, 1413, 1451, 11, 3180, 1413, 382, 2370, 382, 257, 587, 13, 51214, 51214, 400, 370, 341, 1669, 309, 588, 1850, 300, 264, 3006, 551, 281, 360, 11, 1333, 295, 11, 307, 281, 12972, 13, 51514, 51514, 407, 1032, 307, 6091, 382, 2370, 382, 257, 20888, 11, 293, 20888, 307, 1451, 1413, 382, 2370, 382, 587, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07367299985484917, "compression_ratio": 2.0669856459330145, "no_speech_prob": 7.483737135771662e-05}, {"id": 495, "seek": 258800, "start": 2605.0, "end": 2611.0, "text": " And so this makes it very clear that the correct thing to do, sort of, is to multiply.", "tokens": [50364, 382, 257, 1674, 295, 729, 732, 6846, 295, 1319, 11, 2935, 264, 1674, 295, 729, 732, 13, 50614, 50614, 407, 510, 311, 257, 665, 472, 13, 50714, 50714, 759, 257, 1032, 19863, 6091, 382, 2370, 382, 257, 20888, 11, 293, 264, 20888, 307, 1451, 1413, 382, 2370, 382, 257, 4494, 587, 11, 50964, 50964, 550, 264, 1032, 19863, 732, 1413, 1451, 11, 3180, 1413, 382, 2370, 382, 257, 587, 13, 51214, 51214, 400, 370, 341, 1669, 309, 588, 1850, 300, 264, 3006, 551, 281, 360, 11, 1333, 295, 11, 307, 281, 12972, 13, 51514, 51514, 407, 1032, 307, 6091, 382, 2370, 382, 257, 20888, 11, 293, 20888, 307, 1451, 1413, 382, 2370, 382, 587, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07367299985484917, "compression_ratio": 2.0669856459330145, "no_speech_prob": 7.483737135771662e-05}, {"id": 496, "seek": 258800, "start": 2611.0, "end": 2617.0, "text": " So car is twice as fast as a bicycle, and bicycle is four times as fast as man.", "tokens": [50364, 382, 257, 1674, 295, 729, 732, 6846, 295, 1319, 11, 2935, 264, 1674, 295, 729, 732, 13, 50614, 50614, 407, 510, 311, 257, 665, 472, 13, 50714, 50714, 759, 257, 1032, 19863, 6091, 382, 2370, 382, 257, 20888, 11, 293, 264, 20888, 307, 1451, 1413, 382, 2370, 382, 257, 4494, 587, 11, 50964, 50964, 550, 264, 1032, 19863, 732, 1413, 1451, 11, 3180, 1413, 382, 2370, 382, 257, 587, 13, 51214, 51214, 400, 370, 341, 1669, 309, 588, 1850, 300, 264, 3006, 551, 281, 360, 11, 1333, 295, 11, 307, 281, 12972, 13, 51514, 51514, 407, 1032, 307, 6091, 382, 2370, 382, 257, 20888, 11, 293, 20888, 307, 1451, 1413, 382, 2370, 382, 587, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07367299985484917, "compression_ratio": 2.0669856459330145, "no_speech_prob": 7.483737135771662e-05}, {"id": 497, "seek": 261700, "start": 2617.0, "end": 2621.0, "text": " So the car will be eight times as fast as the man.", "tokens": [50364, 407, 264, 1032, 486, 312, 3180, 1413, 382, 2370, 382, 264, 587, 13, 50564, 50564, 400, 370, 321, 393, 747, 613, 19376, 6846, 295, 1319, 11, 498, 291, 486, 11, 293, 12972, 552, 1214, 13, 50864, 50864, 400, 300, 445, 11221, 264, 5021, 4978, 46506, 13, 51064, 51064, 407, 362, 257, 574, 412, 5021, 4978, 13, 51114, 51114, 583, 510, 11, 534, 437, 309, 1355, 337, 505, 307, 456, 311, 257, 588, 2199, 6782, 337, 1163, 2123, 437, 321, 528, 11, 51364, 51364, 597, 307, 37873, 538, 274, 66, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06226614181031572, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.0001334074913756922}, {"id": 498, "seek": 261700, "start": 2621.0, "end": 2627.0, "text": " And so we can take these intermediate rates of change, if you will, and multiply them together.", "tokens": [50364, 407, 264, 1032, 486, 312, 3180, 1413, 382, 2370, 382, 264, 587, 13, 50564, 50564, 400, 370, 321, 393, 747, 613, 19376, 6846, 295, 1319, 11, 498, 291, 486, 11, 293, 12972, 552, 1214, 13, 50864, 50864, 400, 300, 445, 11221, 264, 5021, 4978, 46506, 13, 51064, 51064, 407, 362, 257, 574, 412, 5021, 4978, 13, 51114, 51114, 583, 510, 11, 534, 437, 309, 1355, 337, 505, 307, 456, 311, 257, 588, 2199, 6782, 337, 1163, 2123, 437, 321, 528, 11, 51364, 51364, 597, 307, 37873, 538, 274, 66, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06226614181031572, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.0001334074913756922}, {"id": 499, "seek": 261700, "start": 2627.0, "end": 2631.0, "text": " And that justifies the chain rule intuitively.", "tokens": [50364, 407, 264, 1032, 486, 312, 3180, 1413, 382, 2370, 382, 264, 587, 13, 50564, 50564, 400, 370, 321, 393, 747, 613, 19376, 6846, 295, 1319, 11, 498, 291, 486, 11, 293, 12972, 552, 1214, 13, 50864, 50864, 400, 300, 445, 11221, 264, 5021, 4978, 46506, 13, 51064, 51064, 407, 362, 257, 574, 412, 5021, 4978, 13, 51114, 51114, 583, 510, 11, 534, 437, 309, 1355, 337, 505, 307, 456, 311, 257, 588, 2199, 6782, 337, 1163, 2123, 437, 321, 528, 11, 51364, 51364, 597, 307, 37873, 538, 274, 66, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06226614181031572, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.0001334074913756922}, {"id": 500, "seek": 261700, "start": 2631.0, "end": 2632.0, "text": " So have a look at chain rule.", "tokens": [50364, 407, 264, 1032, 486, 312, 3180, 1413, 382, 2370, 382, 264, 587, 13, 50564, 50564, 400, 370, 321, 393, 747, 613, 19376, 6846, 295, 1319, 11, 498, 291, 486, 11, 293, 12972, 552, 1214, 13, 50864, 50864, 400, 300, 445, 11221, 264, 5021, 4978, 46506, 13, 51064, 51064, 407, 362, 257, 574, 412, 5021, 4978, 13, 51114, 51114, 583, 510, 11, 534, 437, 309, 1355, 337, 505, 307, 456, 311, 257, 588, 2199, 6782, 337, 1163, 2123, 437, 321, 528, 11, 51364, 51364, 597, 307, 37873, 538, 274, 66, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06226614181031572, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.0001334074913756922}, {"id": 501, "seek": 261700, "start": 2632.0, "end": 2637.0, "text": " But here, really what it means for us is there's a very simple recipe for deriving what we want,", "tokens": [50364, 407, 264, 1032, 486, 312, 3180, 1413, 382, 2370, 382, 264, 587, 13, 50564, 50564, 400, 370, 321, 393, 747, 613, 19376, 6846, 295, 1319, 11, 498, 291, 486, 11, 293, 12972, 552, 1214, 13, 50864, 50864, 400, 300, 445, 11221, 264, 5021, 4978, 46506, 13, 51064, 51064, 407, 362, 257, 574, 412, 5021, 4978, 13, 51114, 51114, 583, 510, 11, 534, 437, 309, 1355, 337, 505, 307, 456, 311, 257, 588, 2199, 6782, 337, 1163, 2123, 437, 321, 528, 11, 51364, 51364, 597, 307, 37873, 538, 274, 66, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06226614181031572, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.0001334074913756922}, {"id": 502, "seek": 261700, "start": 2637.0, "end": 2640.0, "text": " which is dl by dc.", "tokens": [50364, 407, 264, 1032, 486, 312, 3180, 1413, 382, 2370, 382, 264, 587, 13, 50564, 50564, 400, 370, 321, 393, 747, 613, 19376, 6846, 295, 1319, 11, 498, 291, 486, 11, 293, 12972, 552, 1214, 13, 50864, 50864, 400, 300, 445, 11221, 264, 5021, 4978, 46506, 13, 51064, 51064, 407, 362, 257, 574, 412, 5021, 4978, 13, 51114, 51114, 583, 510, 11, 534, 437, 309, 1355, 337, 505, 307, 456, 311, 257, 588, 2199, 6782, 337, 1163, 2123, 437, 321, 528, 11, 51364, 51364, 597, 307, 37873, 538, 274, 66, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06226614181031572, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.0001334074913756922}, {"id": 503, "seek": 264000, "start": 2640.0, "end": 2651.0, "text": " And what we have so far is we know what and we know what is the impact of d on l.", "tokens": [50364, 400, 437, 321, 362, 370, 1400, 307, 321, 458, 437, 293, 321, 458, 437, 307, 264, 2712, 295, 274, 322, 287, 13, 50914, 50914, 407, 321, 458, 37873, 538, 274, 67, 11, 264, 13760, 295, 287, 365, 3104, 281, 274, 67, 13, 51164, 51164, 492, 458, 300, 300, 311, 3671, 568, 13, 51264, 51264, 400, 586, 11, 570, 295, 341, 2654, 21577, 300, 321, 600, 1096, 510, 11, 321, 458, 274, 67, 538, 274, 66, 13, 51614, 51614, 407, 577, 775, 269, 2712, 274, 30, 51714, 51714, 400, 294, 1729, 11, 341, 307, 257, 1804, 9984, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.056894935003601675, "compression_ratio": 1.619289340101523, "no_speech_prob": 4.264670133125037e-05}, {"id": 504, "seek": 264000, "start": 2651.0, "end": 2656.0, "text": " So we know dl by dd, the derivative of l with respect to dd.", "tokens": [50364, 400, 437, 321, 362, 370, 1400, 307, 321, 458, 437, 293, 321, 458, 437, 307, 264, 2712, 295, 274, 322, 287, 13, 50914, 50914, 407, 321, 458, 37873, 538, 274, 67, 11, 264, 13760, 295, 287, 365, 3104, 281, 274, 67, 13, 51164, 51164, 492, 458, 300, 300, 311, 3671, 568, 13, 51264, 51264, 400, 586, 11, 570, 295, 341, 2654, 21577, 300, 321, 600, 1096, 510, 11, 321, 458, 274, 67, 538, 274, 66, 13, 51614, 51614, 407, 577, 775, 269, 2712, 274, 30, 51714, 51714, 400, 294, 1729, 11, 341, 307, 257, 1804, 9984, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.056894935003601675, "compression_ratio": 1.619289340101523, "no_speech_prob": 4.264670133125037e-05}, {"id": 505, "seek": 264000, "start": 2656.0, "end": 2658.0, "text": " We know that that's negative 2.", "tokens": [50364, 400, 437, 321, 362, 370, 1400, 307, 321, 458, 437, 293, 321, 458, 437, 307, 264, 2712, 295, 274, 322, 287, 13, 50914, 50914, 407, 321, 458, 37873, 538, 274, 67, 11, 264, 13760, 295, 287, 365, 3104, 281, 274, 67, 13, 51164, 51164, 492, 458, 300, 300, 311, 3671, 568, 13, 51264, 51264, 400, 586, 11, 570, 295, 341, 2654, 21577, 300, 321, 600, 1096, 510, 11, 321, 458, 274, 67, 538, 274, 66, 13, 51614, 51614, 407, 577, 775, 269, 2712, 274, 30, 51714, 51714, 400, 294, 1729, 11, 341, 307, 257, 1804, 9984, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.056894935003601675, "compression_ratio": 1.619289340101523, "no_speech_prob": 4.264670133125037e-05}, {"id": 506, "seek": 264000, "start": 2658.0, "end": 2665.0, "text": " And now, because of this local reasoning that we've done here, we know dd by dc.", "tokens": [50364, 400, 437, 321, 362, 370, 1400, 307, 321, 458, 437, 293, 321, 458, 437, 307, 264, 2712, 295, 274, 322, 287, 13, 50914, 50914, 407, 321, 458, 37873, 538, 274, 67, 11, 264, 13760, 295, 287, 365, 3104, 281, 274, 67, 13, 51164, 51164, 492, 458, 300, 300, 311, 3671, 568, 13, 51264, 51264, 400, 586, 11, 570, 295, 341, 2654, 21577, 300, 321, 600, 1096, 510, 11, 321, 458, 274, 67, 538, 274, 66, 13, 51614, 51614, 407, 577, 775, 269, 2712, 274, 30, 51714, 51714, 400, 294, 1729, 11, 341, 307, 257, 1804, 9984, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.056894935003601675, "compression_ratio": 1.619289340101523, "no_speech_prob": 4.264670133125037e-05}, {"id": 507, "seek": 264000, "start": 2665.0, "end": 2667.0, "text": " So how does c impact d?", "tokens": [50364, 400, 437, 321, 362, 370, 1400, 307, 321, 458, 437, 293, 321, 458, 437, 307, 264, 2712, 295, 274, 322, 287, 13, 50914, 50914, 407, 321, 458, 37873, 538, 274, 67, 11, 264, 13760, 295, 287, 365, 3104, 281, 274, 67, 13, 51164, 51164, 492, 458, 300, 300, 311, 3671, 568, 13, 51264, 51264, 400, 586, 11, 570, 295, 341, 2654, 21577, 300, 321, 600, 1096, 510, 11, 321, 458, 274, 67, 538, 274, 66, 13, 51614, 51614, 407, 577, 775, 269, 2712, 274, 30, 51714, 51714, 400, 294, 1729, 11, 341, 307, 257, 1804, 9984, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.056894935003601675, "compression_ratio": 1.619289340101523, "no_speech_prob": 4.264670133125037e-05}, {"id": 508, "seek": 264000, "start": 2667.0, "end": 2669.0, "text": " And in particular, this is a plus node.", "tokens": [50364, 400, 437, 321, 362, 370, 1400, 307, 321, 458, 437, 293, 321, 458, 437, 307, 264, 2712, 295, 274, 322, 287, 13, 50914, 50914, 407, 321, 458, 37873, 538, 274, 67, 11, 264, 13760, 295, 287, 365, 3104, 281, 274, 67, 13, 51164, 51164, 492, 458, 300, 300, 311, 3671, 568, 13, 51264, 51264, 400, 586, 11, 570, 295, 341, 2654, 21577, 300, 321, 600, 1096, 510, 11, 321, 458, 274, 67, 538, 274, 66, 13, 51614, 51614, 407, 577, 775, 269, 2712, 274, 30, 51714, 51714, 400, 294, 1729, 11, 341, 307, 257, 1804, 9984, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.056894935003601675, "compression_ratio": 1.619289340101523, "no_speech_prob": 4.264670133125037e-05}, {"id": 509, "seek": 266900, "start": 2669.0, "end": 2673.0, "text": " So the local derivative is simply 1.0. It's very simple.", "tokens": [50364, 407, 264, 2654, 13760, 307, 2935, 502, 13, 15, 13, 467, 311, 588, 2199, 13, 50564, 50564, 400, 370, 264, 5021, 4978, 5112, 505, 300, 37873, 538, 274, 66, 11, 516, 807, 341, 19376, 7006, 11, 50914, 50914, 486, 445, 312, 2935, 37873, 538, 274, 67, 1413, 274, 67, 538, 274, 66, 13, 51464, 51464, 663, 311, 5021, 4978, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.052163828164339066, "compression_ratio": 1.4149659863945578, "no_speech_prob": 1.0129860129381996e-05}, {"id": 510, "seek": 266900, "start": 2673.0, "end": 2680.0, "text": " And so the chain rule tells us that dl by dc, going through this intermediate variable,", "tokens": [50364, 407, 264, 2654, 13760, 307, 2935, 502, 13, 15, 13, 467, 311, 588, 2199, 13, 50564, 50564, 400, 370, 264, 5021, 4978, 5112, 505, 300, 37873, 538, 274, 66, 11, 516, 807, 341, 19376, 7006, 11, 50914, 50914, 486, 445, 312, 2935, 37873, 538, 274, 67, 1413, 274, 67, 538, 274, 66, 13, 51464, 51464, 663, 311, 5021, 4978, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.052163828164339066, "compression_ratio": 1.4149659863945578, "no_speech_prob": 1.0129860129381996e-05}, {"id": 511, "seek": 266900, "start": 2680.0, "end": 2691.0, "text": " will just be simply dl by dd times dd by dc.", "tokens": [50364, 407, 264, 2654, 13760, 307, 2935, 502, 13, 15, 13, 467, 311, 588, 2199, 13, 50564, 50564, 400, 370, 264, 5021, 4978, 5112, 505, 300, 37873, 538, 274, 66, 11, 516, 807, 341, 19376, 7006, 11, 50914, 50914, 486, 445, 312, 2935, 37873, 538, 274, 67, 1413, 274, 67, 538, 274, 66, 13, 51464, 51464, 663, 311, 5021, 4978, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.052163828164339066, "compression_ratio": 1.4149659863945578, "no_speech_prob": 1.0129860129381996e-05}, {"id": 512, "seek": 266900, "start": 2691.0, "end": 2693.0, "text": " That's chain rule.", "tokens": [50364, 407, 264, 2654, 13760, 307, 2935, 502, 13, 15, 13, 467, 311, 588, 2199, 13, 50564, 50564, 400, 370, 264, 5021, 4978, 5112, 505, 300, 37873, 538, 274, 66, 11, 516, 807, 341, 19376, 7006, 11, 50914, 50914, 486, 445, 312, 2935, 37873, 538, 274, 67, 1413, 274, 67, 538, 274, 66, 13, 51464, 51464, 663, 311, 5021, 4978, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.052163828164339066, "compression_ratio": 1.4149659863945578, "no_speech_prob": 1.0129860129381996e-05}, {"id": 513, "seek": 269300, "start": 2693.0, "end": 2704.0, "text": " So this is identical to what's happening here, except z is rl, y is rd, and x is rc.", "tokens": [50364, 407, 341, 307, 14800, 281, 437, 311, 2737, 510, 11, 3993, 710, 307, 367, 75, 11, 288, 307, 367, 67, 11, 293, 2031, 307, 367, 66, 13, 50914, 50914, 407, 321, 3736, 445, 362, 281, 12972, 613, 13, 51064, 51064, 400, 570, 613, 2654, 33733, 11, 411, 274, 67, 538, 274, 66, 11, 366, 445, 502, 11, 51464, 51464, 321, 1936, 445, 5055, 670, 37873, 538, 274, 67, 11, 570, 341, 307, 445, 1413, 502, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06825392246246338, "compression_ratio": 1.5174418604651163, "no_speech_prob": 3.288717607574654e-06}, {"id": 514, "seek": 269300, "start": 2704.0, "end": 2707.0, "text": " So we literally just have to multiply these.", "tokens": [50364, 407, 341, 307, 14800, 281, 437, 311, 2737, 510, 11, 3993, 710, 307, 367, 75, 11, 288, 307, 367, 67, 11, 293, 2031, 307, 367, 66, 13, 50914, 50914, 407, 321, 3736, 445, 362, 281, 12972, 613, 13, 51064, 51064, 400, 570, 613, 2654, 33733, 11, 411, 274, 67, 538, 274, 66, 11, 366, 445, 502, 11, 51464, 51464, 321, 1936, 445, 5055, 670, 37873, 538, 274, 67, 11, 570, 341, 307, 445, 1413, 502, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06825392246246338, "compression_ratio": 1.5174418604651163, "no_speech_prob": 3.288717607574654e-06}, {"id": 515, "seek": 269300, "start": 2707.0, "end": 2715.0, "text": " And because these local derivatives, like dd by dc, are just 1,", "tokens": [50364, 407, 341, 307, 14800, 281, 437, 311, 2737, 510, 11, 3993, 710, 307, 367, 75, 11, 288, 307, 367, 67, 11, 293, 2031, 307, 367, 66, 13, 50914, 50914, 407, 321, 3736, 445, 362, 281, 12972, 613, 13, 51064, 51064, 400, 570, 613, 2654, 33733, 11, 411, 274, 67, 538, 274, 66, 11, 366, 445, 502, 11, 51464, 51464, 321, 1936, 445, 5055, 670, 37873, 538, 274, 67, 11, 570, 341, 307, 445, 1413, 502, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06825392246246338, "compression_ratio": 1.5174418604651163, "no_speech_prob": 3.288717607574654e-06}, {"id": 516, "seek": 269300, "start": 2715.0, "end": 2720.0, "text": " we basically just copy over dl by dd, because this is just times 1.", "tokens": [50364, 407, 341, 307, 14800, 281, 437, 311, 2737, 510, 11, 3993, 710, 307, 367, 75, 11, 288, 307, 367, 67, 11, 293, 2031, 307, 367, 66, 13, 50914, 50914, 407, 321, 3736, 445, 362, 281, 12972, 613, 13, 51064, 51064, 400, 570, 613, 2654, 33733, 11, 411, 274, 67, 538, 274, 66, 11, 366, 445, 502, 11, 51464, 51464, 321, 1936, 445, 5055, 670, 37873, 538, 274, 67, 11, 570, 341, 307, 445, 1413, 502, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06825392246246338, "compression_ratio": 1.5174418604651163, "no_speech_prob": 3.288717607574654e-06}, {"id": 517, "seek": 272000, "start": 2720.0, "end": 2726.0, "text": " So because dl by dd is negative 2, what is dl by dc?", "tokens": [50364, 407, 570, 37873, 538, 274, 67, 307, 3671, 568, 11, 437, 307, 37873, 538, 274, 66, 30, 50664, 50664, 1042, 11, 309, 311, 264, 2654, 16235, 11, 502, 13, 15, 11, 1413, 37873, 538, 274, 67, 11, 597, 307, 3671, 568, 13, 50914, 50914, 407, 3736, 11, 437, 257, 1804, 9984, 775, 11, 291, 393, 574, 412, 309, 300, 636, 11, 51114, 51114, 307, 309, 3736, 445, 18242, 264, 16235, 11, 570, 264, 1804, 9984, 311, 2654, 33733, 366, 445, 502, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.05360195803087811, "compression_ratio": 1.6494252873563218, "no_speech_prob": 2.144438803952653e-05}, {"id": 518, "seek": 272000, "start": 2726.0, "end": 2731.0, "text": " Well, it's the local gradient, 1.0, times dl by dd, which is negative 2.", "tokens": [50364, 407, 570, 37873, 538, 274, 67, 307, 3671, 568, 11, 437, 307, 37873, 538, 274, 66, 30, 50664, 50664, 1042, 11, 309, 311, 264, 2654, 16235, 11, 502, 13, 15, 11, 1413, 37873, 538, 274, 67, 11, 597, 307, 3671, 568, 13, 50914, 50914, 407, 3736, 11, 437, 257, 1804, 9984, 775, 11, 291, 393, 574, 412, 309, 300, 636, 11, 51114, 51114, 307, 309, 3736, 445, 18242, 264, 16235, 11, 570, 264, 1804, 9984, 311, 2654, 33733, 366, 445, 502, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.05360195803087811, "compression_ratio": 1.6494252873563218, "no_speech_prob": 2.144438803952653e-05}, {"id": 519, "seek": 272000, "start": 2731.0, "end": 2735.0, "text": " So literally, what a plus node does, you can look at it that way,", "tokens": [50364, 407, 570, 37873, 538, 274, 67, 307, 3671, 568, 11, 437, 307, 37873, 538, 274, 66, 30, 50664, 50664, 1042, 11, 309, 311, 264, 2654, 16235, 11, 502, 13, 15, 11, 1413, 37873, 538, 274, 67, 11, 597, 307, 3671, 568, 13, 50914, 50914, 407, 3736, 11, 437, 257, 1804, 9984, 775, 11, 291, 393, 574, 412, 309, 300, 636, 11, 51114, 51114, 307, 309, 3736, 445, 18242, 264, 16235, 11, 570, 264, 1804, 9984, 311, 2654, 33733, 366, 445, 502, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.05360195803087811, "compression_ratio": 1.6494252873563218, "no_speech_prob": 2.144438803952653e-05}, {"id": 520, "seek": 272000, "start": 2735.0, "end": 2741.0, "text": " is it literally just routes the gradient, because the plus node's local derivatives are just 1.", "tokens": [50364, 407, 570, 37873, 538, 274, 67, 307, 3671, 568, 11, 437, 307, 37873, 538, 274, 66, 30, 50664, 50664, 1042, 11, 309, 311, 264, 2654, 16235, 11, 502, 13, 15, 11, 1413, 37873, 538, 274, 67, 11, 597, 307, 3671, 568, 13, 50914, 50914, 407, 3736, 11, 437, 257, 1804, 9984, 775, 11, 291, 393, 574, 412, 309, 300, 636, 11, 51114, 51114, 307, 309, 3736, 445, 18242, 264, 16235, 11, 570, 264, 1804, 9984, 311, 2654, 33733, 366, 445, 502, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.05360195803087811, "compression_ratio": 1.6494252873563218, "no_speech_prob": 2.144438803952653e-05}, {"id": 521, "seek": 274100, "start": 2741.0, "end": 2750.0, "text": " And so in the chain rule, 1 times dl by dd is just dl by dd.", "tokens": [50364, 400, 370, 294, 264, 5021, 4978, 11, 502, 1413, 37873, 538, 274, 67, 307, 445, 37873, 538, 274, 67, 13, 50814, 50814, 400, 370, 300, 13760, 445, 2170, 4020, 292, 281, 1293, 269, 293, 281, 308, 294, 341, 1389, 13, 51114, 51114, 407, 1936, 11, 321, 362, 300, 308, 5893, 2771, 11, 420, 718, 311, 722, 365, 269, 11, 1670, 300, 311, 264, 472, 321, 2956, 412, 11, 51464, 51464, 307, 3671, 568, 1413, 502, 11, 3671, 568, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07143538256725633, "compression_ratio": 1.5588235294117647, "no_speech_prob": 5.5075470299925655e-06}, {"id": 522, "seek": 274100, "start": 2750.0, "end": 2756.0, "text": " And so that derivative just gets routed to both c and to e in this case.", "tokens": [50364, 400, 370, 294, 264, 5021, 4978, 11, 502, 1413, 37873, 538, 274, 67, 307, 445, 37873, 538, 274, 67, 13, 50814, 50814, 400, 370, 300, 13760, 445, 2170, 4020, 292, 281, 1293, 269, 293, 281, 308, 294, 341, 1389, 13, 51114, 51114, 407, 1936, 11, 321, 362, 300, 308, 5893, 2771, 11, 420, 718, 311, 722, 365, 269, 11, 1670, 300, 311, 264, 472, 321, 2956, 412, 11, 51464, 51464, 307, 3671, 568, 1413, 502, 11, 3671, 568, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07143538256725633, "compression_ratio": 1.5588235294117647, "no_speech_prob": 5.5075470299925655e-06}, {"id": 523, "seek": 274100, "start": 2756.0, "end": 2763.0, "text": " So basically, we have that e dot grad, or let's start with c, since that's the one we looked at,", "tokens": [50364, 400, 370, 294, 264, 5021, 4978, 11, 502, 1413, 37873, 538, 274, 67, 307, 445, 37873, 538, 274, 67, 13, 50814, 50814, 400, 370, 300, 13760, 445, 2170, 4020, 292, 281, 1293, 269, 293, 281, 308, 294, 341, 1389, 13, 51114, 51114, 407, 1936, 11, 321, 362, 300, 308, 5893, 2771, 11, 420, 718, 311, 722, 365, 269, 11, 1670, 300, 311, 264, 472, 321, 2956, 412, 11, 51464, 51464, 307, 3671, 568, 1413, 502, 11, 3671, 568, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07143538256725633, "compression_ratio": 1.5588235294117647, "no_speech_prob": 5.5075470299925655e-06}, {"id": 524, "seek": 274100, "start": 2763.0, "end": 2769.0, "text": " is negative 2 times 1, negative 2.", "tokens": [50364, 400, 370, 294, 264, 5021, 4978, 11, 502, 1413, 37873, 538, 274, 67, 307, 445, 37873, 538, 274, 67, 13, 50814, 50814, 400, 370, 300, 13760, 445, 2170, 4020, 292, 281, 1293, 269, 293, 281, 308, 294, 341, 1389, 13, 51114, 51114, 407, 1936, 11, 321, 362, 300, 308, 5893, 2771, 11, 420, 718, 311, 722, 365, 269, 11, 1670, 300, 311, 264, 472, 321, 2956, 412, 11, 51464, 51464, 307, 3671, 568, 1413, 502, 11, 3671, 568, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07143538256725633, "compression_ratio": 1.5588235294117647, "no_speech_prob": 5.5075470299925655e-06}, {"id": 525, "seek": 276900, "start": 2769.0, "end": 2773.0, "text": " And in the same way, by symmetry, e dot grad will be negative 2.", "tokens": [50364, 400, 294, 264, 912, 636, 11, 538, 25440, 11, 308, 5893, 2771, 486, 312, 3671, 568, 13, 50564, 50564, 663, 311, 264, 3932, 13, 50664, 50664, 407, 321, 393, 992, 729, 11, 321, 393, 2182, 5131, 11, 293, 291, 536, 577, 321, 445, 6269, 3671, 568, 11, 3671, 568, 30, 51014, 51014, 407, 341, 646, 79, 1513, 559, 990, 6358, 11, 597, 307, 9792, 264, 1589, 295, 11, 411, 11, 51214, 51214, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 439, 264, 19376, 13891, 11, 51364, 51364, 321, 393, 3811, 309, 1920, 411, 13974, 12204, 807, 264, 4295, 11, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11077194947462815, "compression_ratio": 1.603305785123967, "no_speech_prob": 5.1440754759823903e-05}, {"id": 526, "seek": 276900, "start": 2773.0, "end": 2775.0, "text": " That's the claim.", "tokens": [50364, 400, 294, 264, 912, 636, 11, 538, 25440, 11, 308, 5893, 2771, 486, 312, 3671, 568, 13, 50564, 50564, 663, 311, 264, 3932, 13, 50664, 50664, 407, 321, 393, 992, 729, 11, 321, 393, 2182, 5131, 11, 293, 291, 536, 577, 321, 445, 6269, 3671, 568, 11, 3671, 568, 30, 51014, 51014, 407, 341, 646, 79, 1513, 559, 990, 6358, 11, 597, 307, 9792, 264, 1589, 295, 11, 411, 11, 51214, 51214, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 439, 264, 19376, 13891, 11, 51364, 51364, 321, 393, 3811, 309, 1920, 411, 13974, 12204, 807, 264, 4295, 11, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11077194947462815, "compression_ratio": 1.603305785123967, "no_speech_prob": 5.1440754759823903e-05}, {"id": 527, "seek": 276900, "start": 2775.0, "end": 2782.0, "text": " So we can set those, we can redraw, and you see how we just assign negative 2, negative 2?", "tokens": [50364, 400, 294, 264, 912, 636, 11, 538, 25440, 11, 308, 5893, 2771, 486, 312, 3671, 568, 13, 50564, 50564, 663, 311, 264, 3932, 13, 50664, 50664, 407, 321, 393, 992, 729, 11, 321, 393, 2182, 5131, 11, 293, 291, 536, 577, 321, 445, 6269, 3671, 568, 11, 3671, 568, 30, 51014, 51014, 407, 341, 646, 79, 1513, 559, 990, 6358, 11, 597, 307, 9792, 264, 1589, 295, 11, 411, 11, 51214, 51214, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 439, 264, 19376, 13891, 11, 51364, 51364, 321, 393, 3811, 309, 1920, 411, 13974, 12204, 807, 264, 4295, 11, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11077194947462815, "compression_ratio": 1.603305785123967, "no_speech_prob": 5.1440754759823903e-05}, {"id": 528, "seek": 276900, "start": 2782.0, "end": 2786.0, "text": " So this backpropagating signal, which is carrying the information of, like,", "tokens": [50364, 400, 294, 264, 912, 636, 11, 538, 25440, 11, 308, 5893, 2771, 486, 312, 3671, 568, 13, 50564, 50564, 663, 311, 264, 3932, 13, 50664, 50664, 407, 321, 393, 992, 729, 11, 321, 393, 2182, 5131, 11, 293, 291, 536, 577, 321, 445, 6269, 3671, 568, 11, 3671, 568, 30, 51014, 51014, 407, 341, 646, 79, 1513, 559, 990, 6358, 11, 597, 307, 9792, 264, 1589, 295, 11, 411, 11, 51214, 51214, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 439, 264, 19376, 13891, 11, 51364, 51364, 321, 393, 3811, 309, 1920, 411, 13974, 12204, 807, 264, 4295, 11, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11077194947462815, "compression_ratio": 1.603305785123967, "no_speech_prob": 5.1440754759823903e-05}, {"id": 529, "seek": 276900, "start": 2786.0, "end": 2789.0, "text": " what is the derivative of L with respect to all the intermediate nodes,", "tokens": [50364, 400, 294, 264, 912, 636, 11, 538, 25440, 11, 308, 5893, 2771, 486, 312, 3671, 568, 13, 50564, 50564, 663, 311, 264, 3932, 13, 50664, 50664, 407, 321, 393, 992, 729, 11, 321, 393, 2182, 5131, 11, 293, 291, 536, 577, 321, 445, 6269, 3671, 568, 11, 3671, 568, 30, 51014, 51014, 407, 341, 646, 79, 1513, 559, 990, 6358, 11, 597, 307, 9792, 264, 1589, 295, 11, 411, 11, 51214, 51214, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 439, 264, 19376, 13891, 11, 51364, 51364, 321, 393, 3811, 309, 1920, 411, 13974, 12204, 807, 264, 4295, 11, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11077194947462815, "compression_ratio": 1.603305785123967, "no_speech_prob": 5.1440754759823903e-05}, {"id": 530, "seek": 276900, "start": 2789.0, "end": 2792.0, "text": " we can imagine it almost like flowing backwards through the graph,", "tokens": [50364, 400, 294, 264, 912, 636, 11, 538, 25440, 11, 308, 5893, 2771, 486, 312, 3671, 568, 13, 50564, 50564, 663, 311, 264, 3932, 13, 50664, 50664, 407, 321, 393, 992, 729, 11, 321, 393, 2182, 5131, 11, 293, 291, 536, 577, 321, 445, 6269, 3671, 568, 11, 3671, 568, 30, 51014, 51014, 407, 341, 646, 79, 1513, 559, 990, 6358, 11, 597, 307, 9792, 264, 1589, 295, 11, 411, 11, 51214, 51214, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 439, 264, 19376, 13891, 11, 51364, 51364, 321, 393, 3811, 309, 1920, 411, 13974, 12204, 807, 264, 4295, 11, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.11077194947462815, "compression_ratio": 1.603305785123967, "no_speech_prob": 5.1440754759823903e-05}, {"id": 531, "seek": 279200, "start": 2792.0, "end": 2799.0, "text": " and a plus node will simply distribute the derivative to all the children nodes of it.", "tokens": [50364, 293, 257, 1804, 9984, 486, 2935, 20594, 264, 13760, 281, 439, 264, 2227, 13891, 295, 309, 13, 50714, 50714, 407, 341, 307, 264, 3932, 11, 293, 586, 718, 311, 16888, 309, 13, 50864, 50864, 407, 718, 385, 4159, 264, 1804, 276, 510, 490, 949, 13, 51014, 51014, 400, 586, 2602, 437, 321, 434, 516, 281, 360, 307, 321, 528, 281, 26200, 269, 13, 51164, 51164, 407, 269, 5893, 1412, 486, 312, 1946, 14684, 538, 276, 11, 293, 562, 286, 1190, 341, 11, 321, 2066, 281, 536, 3671, 568, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08449152977235856, "compression_ratio": 1.5876777251184835, "no_speech_prob": 1.321173294854816e-05}, {"id": 532, "seek": 279200, "start": 2799.0, "end": 2802.0, "text": " So this is the claim, and now let's verify it.", "tokens": [50364, 293, 257, 1804, 9984, 486, 2935, 20594, 264, 13760, 281, 439, 264, 2227, 13891, 295, 309, 13, 50714, 50714, 407, 341, 307, 264, 3932, 11, 293, 586, 718, 311, 16888, 309, 13, 50864, 50864, 407, 718, 385, 4159, 264, 1804, 276, 510, 490, 949, 13, 51014, 51014, 400, 586, 2602, 437, 321, 434, 516, 281, 360, 307, 321, 528, 281, 26200, 269, 13, 51164, 51164, 407, 269, 5893, 1412, 486, 312, 1946, 14684, 538, 276, 11, 293, 562, 286, 1190, 341, 11, 321, 2066, 281, 536, 3671, 568, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08449152977235856, "compression_ratio": 1.5876777251184835, "no_speech_prob": 1.321173294854816e-05}, {"id": 533, "seek": 279200, "start": 2802.0, "end": 2805.0, "text": " So let me remove the plus h here from before.", "tokens": [50364, 293, 257, 1804, 9984, 486, 2935, 20594, 264, 13760, 281, 439, 264, 2227, 13891, 295, 309, 13, 50714, 50714, 407, 341, 307, 264, 3932, 11, 293, 586, 718, 311, 16888, 309, 13, 50864, 50864, 407, 718, 385, 4159, 264, 1804, 276, 510, 490, 949, 13, 51014, 51014, 400, 586, 2602, 437, 321, 434, 516, 281, 360, 307, 321, 528, 281, 26200, 269, 13, 51164, 51164, 407, 269, 5893, 1412, 486, 312, 1946, 14684, 538, 276, 11, 293, 562, 286, 1190, 341, 11, 321, 2066, 281, 536, 3671, 568, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08449152977235856, "compression_ratio": 1.5876777251184835, "no_speech_prob": 1.321173294854816e-05}, {"id": 534, "seek": 279200, "start": 2805.0, "end": 2808.0, "text": " And now instead what we're going to do is we want to increment c.", "tokens": [50364, 293, 257, 1804, 9984, 486, 2935, 20594, 264, 13760, 281, 439, 264, 2227, 13891, 295, 309, 13, 50714, 50714, 407, 341, 307, 264, 3932, 11, 293, 586, 718, 311, 16888, 309, 13, 50864, 50864, 407, 718, 385, 4159, 264, 1804, 276, 510, 490, 949, 13, 51014, 51014, 400, 586, 2602, 437, 321, 434, 516, 281, 360, 307, 321, 528, 281, 26200, 269, 13, 51164, 51164, 407, 269, 5893, 1412, 486, 312, 1946, 14684, 538, 276, 11, 293, 562, 286, 1190, 341, 11, 321, 2066, 281, 536, 3671, 568, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08449152977235856, "compression_ratio": 1.5876777251184835, "no_speech_prob": 1.321173294854816e-05}, {"id": 535, "seek": 279200, "start": 2808.0, "end": 2816.0, "text": " So c dot data will be incremented by h, and when I run this, we expect to see negative 2.", "tokens": [50364, 293, 257, 1804, 9984, 486, 2935, 20594, 264, 13760, 281, 439, 264, 2227, 13891, 295, 309, 13, 50714, 50714, 407, 341, 307, 264, 3932, 11, 293, 586, 718, 311, 16888, 309, 13, 50864, 50864, 407, 718, 385, 4159, 264, 1804, 276, 510, 490, 949, 13, 51014, 51014, 400, 586, 2602, 437, 321, 434, 516, 281, 360, 307, 321, 528, 281, 26200, 269, 13, 51164, 51164, 407, 269, 5893, 1412, 486, 312, 1946, 14684, 538, 276, 11, 293, 562, 286, 1190, 341, 11, 321, 2066, 281, 536, 3671, 568, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08449152977235856, "compression_ratio": 1.5876777251184835, "no_speech_prob": 1.321173294854816e-05}, {"id": 536, "seek": 281600, "start": 2816.0, "end": 2823.0, "text": " And then of course for e, so e dot data plus equals h, and we expect to see negative 2.", "tokens": [50364, 400, 550, 295, 1164, 337, 308, 11, 370, 308, 5893, 1412, 1804, 6915, 276, 11, 293, 321, 2066, 281, 536, 3671, 568, 13, 50714, 50714, 21532, 13, 50914, 50914, 407, 729, 366, 264, 33733, 295, 613, 6920, 13891, 13, 51114, 51114, 400, 586, 321, 434, 516, 281, 18680, 405, 527, 636, 12204, 797, 11, 51314, 51314, 293, 321, 434, 797, 516, 281, 3079, 264, 5021, 4978, 13, 51464, 51464, 407, 510, 321, 352, 11, 527, 1150, 3861, 295, 5021, 4978, 11, 51564, 51564, 293, 321, 486, 3079, 309, 439, 264, 636, 807, 264, 4295, 11, 51664, 51664, 597, 445, 2011, 281, 787, 362, 472, 544, 9984, 8877, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.051094808748790195, "compression_ratio": 1.6733870967741935, "no_speech_prob": 6.962174211366801e-06}, {"id": 537, "seek": 281600, "start": 2823.0, "end": 2827.0, "text": " Simple.", "tokens": [50364, 400, 550, 295, 1164, 337, 308, 11, 370, 308, 5893, 1412, 1804, 6915, 276, 11, 293, 321, 2066, 281, 536, 3671, 568, 13, 50714, 50714, 21532, 13, 50914, 50914, 407, 729, 366, 264, 33733, 295, 613, 6920, 13891, 13, 51114, 51114, 400, 586, 321, 434, 516, 281, 18680, 405, 527, 636, 12204, 797, 11, 51314, 51314, 293, 321, 434, 797, 516, 281, 3079, 264, 5021, 4978, 13, 51464, 51464, 407, 510, 321, 352, 11, 527, 1150, 3861, 295, 5021, 4978, 11, 51564, 51564, 293, 321, 486, 3079, 309, 439, 264, 636, 807, 264, 4295, 11, 51664, 51664, 597, 445, 2011, 281, 787, 362, 472, 544, 9984, 8877, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.051094808748790195, "compression_ratio": 1.6733870967741935, "no_speech_prob": 6.962174211366801e-06}, {"id": 538, "seek": 281600, "start": 2827.0, "end": 2831.0, "text": " So those are the derivatives of these internal nodes.", "tokens": [50364, 400, 550, 295, 1164, 337, 308, 11, 370, 308, 5893, 1412, 1804, 6915, 276, 11, 293, 321, 2066, 281, 536, 3671, 568, 13, 50714, 50714, 21532, 13, 50914, 50914, 407, 729, 366, 264, 33733, 295, 613, 6920, 13891, 13, 51114, 51114, 400, 586, 321, 434, 516, 281, 18680, 405, 527, 636, 12204, 797, 11, 51314, 51314, 293, 321, 434, 797, 516, 281, 3079, 264, 5021, 4978, 13, 51464, 51464, 407, 510, 321, 352, 11, 527, 1150, 3861, 295, 5021, 4978, 11, 51564, 51564, 293, 321, 486, 3079, 309, 439, 264, 636, 807, 264, 4295, 11, 51664, 51664, 597, 445, 2011, 281, 787, 362, 472, 544, 9984, 8877, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.051094808748790195, "compression_ratio": 1.6733870967741935, "no_speech_prob": 6.962174211366801e-06}, {"id": 539, "seek": 281600, "start": 2831.0, "end": 2835.0, "text": " And now we're going to recurse our way backwards again,", "tokens": [50364, 400, 550, 295, 1164, 337, 308, 11, 370, 308, 5893, 1412, 1804, 6915, 276, 11, 293, 321, 2066, 281, 536, 3671, 568, 13, 50714, 50714, 21532, 13, 50914, 50914, 407, 729, 366, 264, 33733, 295, 613, 6920, 13891, 13, 51114, 51114, 400, 586, 321, 434, 516, 281, 18680, 405, 527, 636, 12204, 797, 11, 51314, 51314, 293, 321, 434, 797, 516, 281, 3079, 264, 5021, 4978, 13, 51464, 51464, 407, 510, 321, 352, 11, 527, 1150, 3861, 295, 5021, 4978, 11, 51564, 51564, 293, 321, 486, 3079, 309, 439, 264, 636, 807, 264, 4295, 11, 51664, 51664, 597, 445, 2011, 281, 787, 362, 472, 544, 9984, 8877, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.051094808748790195, "compression_ratio": 1.6733870967741935, "no_speech_prob": 6.962174211366801e-06}, {"id": 540, "seek": 281600, "start": 2835.0, "end": 2838.0, "text": " and we're again going to apply the chain rule.", "tokens": [50364, 400, 550, 295, 1164, 337, 308, 11, 370, 308, 5893, 1412, 1804, 6915, 276, 11, 293, 321, 2066, 281, 536, 3671, 568, 13, 50714, 50714, 21532, 13, 50914, 50914, 407, 729, 366, 264, 33733, 295, 613, 6920, 13891, 13, 51114, 51114, 400, 586, 321, 434, 516, 281, 18680, 405, 527, 636, 12204, 797, 11, 51314, 51314, 293, 321, 434, 797, 516, 281, 3079, 264, 5021, 4978, 13, 51464, 51464, 407, 510, 321, 352, 11, 527, 1150, 3861, 295, 5021, 4978, 11, 51564, 51564, 293, 321, 486, 3079, 309, 439, 264, 636, 807, 264, 4295, 11, 51664, 51664, 597, 445, 2011, 281, 787, 362, 472, 544, 9984, 8877, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.051094808748790195, "compression_ratio": 1.6733870967741935, "no_speech_prob": 6.962174211366801e-06}, {"id": 541, "seek": 281600, "start": 2838.0, "end": 2840.0, "text": " So here we go, our second application of chain rule,", "tokens": [50364, 400, 550, 295, 1164, 337, 308, 11, 370, 308, 5893, 1412, 1804, 6915, 276, 11, 293, 321, 2066, 281, 536, 3671, 568, 13, 50714, 50714, 21532, 13, 50914, 50914, 407, 729, 366, 264, 33733, 295, 613, 6920, 13891, 13, 51114, 51114, 400, 586, 321, 434, 516, 281, 18680, 405, 527, 636, 12204, 797, 11, 51314, 51314, 293, 321, 434, 797, 516, 281, 3079, 264, 5021, 4978, 13, 51464, 51464, 407, 510, 321, 352, 11, 527, 1150, 3861, 295, 5021, 4978, 11, 51564, 51564, 293, 321, 486, 3079, 309, 439, 264, 636, 807, 264, 4295, 11, 51664, 51664, 597, 445, 2011, 281, 787, 362, 472, 544, 9984, 8877, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.051094808748790195, "compression_ratio": 1.6733870967741935, "no_speech_prob": 6.962174211366801e-06}, {"id": 542, "seek": 281600, "start": 2840.0, "end": 2842.0, "text": " and we will apply it all the way through the graph,", "tokens": [50364, 400, 550, 295, 1164, 337, 308, 11, 370, 308, 5893, 1412, 1804, 6915, 276, 11, 293, 321, 2066, 281, 536, 3671, 568, 13, 50714, 50714, 21532, 13, 50914, 50914, 407, 729, 366, 264, 33733, 295, 613, 6920, 13891, 13, 51114, 51114, 400, 586, 321, 434, 516, 281, 18680, 405, 527, 636, 12204, 797, 11, 51314, 51314, 293, 321, 434, 797, 516, 281, 3079, 264, 5021, 4978, 13, 51464, 51464, 407, 510, 321, 352, 11, 527, 1150, 3861, 295, 5021, 4978, 11, 51564, 51564, 293, 321, 486, 3079, 309, 439, 264, 636, 807, 264, 4295, 11, 51664, 51664, 597, 445, 2011, 281, 787, 362, 472, 544, 9984, 8877, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.051094808748790195, "compression_ratio": 1.6733870967741935, "no_speech_prob": 6.962174211366801e-06}, {"id": 543, "seek": 281600, "start": 2842.0, "end": 2845.0, "text": " which just happened to only have one more node remaining.", "tokens": [50364, 400, 550, 295, 1164, 337, 308, 11, 370, 308, 5893, 1412, 1804, 6915, 276, 11, 293, 321, 2066, 281, 536, 3671, 568, 13, 50714, 50714, 21532, 13, 50914, 50914, 407, 729, 366, 264, 33733, 295, 613, 6920, 13891, 13, 51114, 51114, 400, 586, 321, 434, 516, 281, 18680, 405, 527, 636, 12204, 797, 11, 51314, 51314, 293, 321, 434, 797, 516, 281, 3079, 264, 5021, 4978, 13, 51464, 51464, 407, 510, 321, 352, 11, 527, 1150, 3861, 295, 5021, 4978, 11, 51564, 51564, 293, 321, 486, 3079, 309, 439, 264, 636, 807, 264, 4295, 11, 51664, 51664, 597, 445, 2011, 281, 787, 362, 472, 544, 9984, 8877, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.051094808748790195, "compression_ratio": 1.6733870967741935, "no_speech_prob": 6.962174211366801e-06}, {"id": 544, "seek": 284500, "start": 2845.0, "end": 2851.0, "text": " We have that dl by de, as we have just calculated, is negative 2.", "tokens": [50364, 492, 362, 300, 37873, 538, 368, 11, 382, 321, 362, 445, 15598, 11, 307, 3671, 568, 13, 50664, 50664, 407, 321, 458, 300, 13, 50764, 50764, 407, 321, 458, 264, 13760, 295, 287, 365, 3104, 281, 308, 13, 50964, 50964, 400, 586, 321, 528, 37873, 538, 1120, 11, 558, 30, 51264, 51264, 400, 264, 5021, 4978, 307, 3585, 505, 300, 300, 311, 445, 37873, 538, 368, 11, 51564, 51564, 3671, 568, 11, 1413, 264, 2654, 16235, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07920902746695059, "compression_ratio": 1.593939393939394, "no_speech_prob": 5.1440838433336467e-05}, {"id": 545, "seek": 284500, "start": 2851.0, "end": 2853.0, "text": " So we know that.", "tokens": [50364, 492, 362, 300, 37873, 538, 368, 11, 382, 321, 362, 445, 15598, 11, 307, 3671, 568, 13, 50664, 50664, 407, 321, 458, 300, 13, 50764, 50764, 407, 321, 458, 264, 13760, 295, 287, 365, 3104, 281, 308, 13, 50964, 50964, 400, 586, 321, 528, 37873, 538, 1120, 11, 558, 30, 51264, 51264, 400, 264, 5021, 4978, 307, 3585, 505, 300, 300, 311, 445, 37873, 538, 368, 11, 51564, 51564, 3671, 568, 11, 1413, 264, 2654, 16235, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07920902746695059, "compression_ratio": 1.593939393939394, "no_speech_prob": 5.1440838433336467e-05}, {"id": 546, "seek": 284500, "start": 2853.0, "end": 2857.0, "text": " So we know the derivative of l with respect to e.", "tokens": [50364, 492, 362, 300, 37873, 538, 368, 11, 382, 321, 362, 445, 15598, 11, 307, 3671, 568, 13, 50664, 50664, 407, 321, 458, 300, 13, 50764, 50764, 407, 321, 458, 264, 13760, 295, 287, 365, 3104, 281, 308, 13, 50964, 50964, 400, 586, 321, 528, 37873, 538, 1120, 11, 558, 30, 51264, 51264, 400, 264, 5021, 4978, 307, 3585, 505, 300, 300, 311, 445, 37873, 538, 368, 11, 51564, 51564, 3671, 568, 11, 1413, 264, 2654, 16235, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07920902746695059, "compression_ratio": 1.593939393939394, "no_speech_prob": 5.1440838433336467e-05}, {"id": 547, "seek": 284500, "start": 2857.0, "end": 2863.0, "text": " And now we want dl by da, right?", "tokens": [50364, 492, 362, 300, 37873, 538, 368, 11, 382, 321, 362, 445, 15598, 11, 307, 3671, 568, 13, 50664, 50664, 407, 321, 458, 300, 13, 50764, 50764, 407, 321, 458, 264, 13760, 295, 287, 365, 3104, 281, 308, 13, 50964, 50964, 400, 586, 321, 528, 37873, 538, 1120, 11, 558, 30, 51264, 51264, 400, 264, 5021, 4978, 307, 3585, 505, 300, 300, 311, 445, 37873, 538, 368, 11, 51564, 51564, 3671, 568, 11, 1413, 264, 2654, 16235, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07920902746695059, "compression_ratio": 1.593939393939394, "no_speech_prob": 5.1440838433336467e-05}, {"id": 548, "seek": 284500, "start": 2863.0, "end": 2869.0, "text": " And the chain rule is telling us that that's just dl by de,", "tokens": [50364, 492, 362, 300, 37873, 538, 368, 11, 382, 321, 362, 445, 15598, 11, 307, 3671, 568, 13, 50664, 50664, 407, 321, 458, 300, 13, 50764, 50764, 407, 321, 458, 264, 13760, 295, 287, 365, 3104, 281, 308, 13, 50964, 50964, 400, 586, 321, 528, 37873, 538, 1120, 11, 558, 30, 51264, 51264, 400, 264, 5021, 4978, 307, 3585, 505, 300, 300, 311, 445, 37873, 538, 368, 11, 51564, 51564, 3671, 568, 11, 1413, 264, 2654, 16235, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07920902746695059, "compression_ratio": 1.593939393939394, "no_speech_prob": 5.1440838433336467e-05}, {"id": 549, "seek": 284500, "start": 2869.0, "end": 2872.0, "text": " negative 2, times the local gradient.", "tokens": [50364, 492, 362, 300, 37873, 538, 368, 11, 382, 321, 362, 445, 15598, 11, 307, 3671, 568, 13, 50664, 50664, 407, 321, 458, 300, 13, 50764, 50764, 407, 321, 458, 264, 13760, 295, 287, 365, 3104, 281, 308, 13, 50964, 50964, 400, 586, 321, 528, 37873, 538, 1120, 11, 558, 30, 51264, 51264, 400, 264, 5021, 4978, 307, 3585, 505, 300, 300, 311, 445, 37873, 538, 368, 11, 51564, 51564, 3671, 568, 11, 1413, 264, 2654, 16235, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07920902746695059, "compression_ratio": 1.593939393939394, "no_speech_prob": 5.1440838433336467e-05}, {"id": 550, "seek": 287200, "start": 2872.0, "end": 2877.0, "text": " So what is the local gradient? Basically, de by da.", "tokens": [50364, 407, 437, 307, 264, 2654, 16235, 30, 8537, 11, 368, 538, 1120, 13, 50614, 50614, 492, 362, 281, 574, 412, 300, 13, 50764, 50764, 407, 286, 478, 257, 707, 1413, 9984, 1854, 257, 5994, 4295, 11, 50964, 50964, 293, 286, 787, 458, 300, 286, 630, 257, 1413, 272, 11, 293, 286, 7126, 364, 308, 13, 51214, 51214, 407, 586, 437, 307, 368, 538, 1120, 293, 368, 538, 274, 85, 30, 51414, 51414, 663, 311, 264, 787, 551, 300, 286, 1333, 295, 458, 466, 13, 663, 311, 452, 2654, 16235, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0631552554191427, "compression_ratio": 1.633879781420765, "no_speech_prob": 6.54030100122327e-06}, {"id": 551, "seek": 287200, "start": 2877.0, "end": 2880.0, "text": " We have to look at that.", "tokens": [50364, 407, 437, 307, 264, 2654, 16235, 30, 8537, 11, 368, 538, 1120, 13, 50614, 50614, 492, 362, 281, 574, 412, 300, 13, 50764, 50764, 407, 286, 478, 257, 707, 1413, 9984, 1854, 257, 5994, 4295, 11, 50964, 50964, 293, 286, 787, 458, 300, 286, 630, 257, 1413, 272, 11, 293, 286, 7126, 364, 308, 13, 51214, 51214, 407, 586, 437, 307, 368, 538, 1120, 293, 368, 538, 274, 85, 30, 51414, 51414, 663, 311, 264, 787, 551, 300, 286, 1333, 295, 458, 466, 13, 663, 311, 452, 2654, 16235, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0631552554191427, "compression_ratio": 1.633879781420765, "no_speech_prob": 6.54030100122327e-06}, {"id": 552, "seek": 287200, "start": 2880.0, "end": 2884.0, "text": " So I'm a little times node inside a massive graph,", "tokens": [50364, 407, 437, 307, 264, 2654, 16235, 30, 8537, 11, 368, 538, 1120, 13, 50614, 50614, 492, 362, 281, 574, 412, 300, 13, 50764, 50764, 407, 286, 478, 257, 707, 1413, 9984, 1854, 257, 5994, 4295, 11, 50964, 50964, 293, 286, 787, 458, 300, 286, 630, 257, 1413, 272, 11, 293, 286, 7126, 364, 308, 13, 51214, 51214, 407, 586, 437, 307, 368, 538, 1120, 293, 368, 538, 274, 85, 30, 51414, 51414, 663, 311, 264, 787, 551, 300, 286, 1333, 295, 458, 466, 13, 663, 311, 452, 2654, 16235, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0631552554191427, "compression_ratio": 1.633879781420765, "no_speech_prob": 6.54030100122327e-06}, {"id": 553, "seek": 287200, "start": 2884.0, "end": 2889.0, "text": " and I only know that I did a times b, and I produced an e.", "tokens": [50364, 407, 437, 307, 264, 2654, 16235, 30, 8537, 11, 368, 538, 1120, 13, 50614, 50614, 492, 362, 281, 574, 412, 300, 13, 50764, 50764, 407, 286, 478, 257, 707, 1413, 9984, 1854, 257, 5994, 4295, 11, 50964, 50964, 293, 286, 787, 458, 300, 286, 630, 257, 1413, 272, 11, 293, 286, 7126, 364, 308, 13, 51214, 51214, 407, 586, 437, 307, 368, 538, 1120, 293, 368, 538, 274, 85, 30, 51414, 51414, 663, 311, 264, 787, 551, 300, 286, 1333, 295, 458, 466, 13, 663, 311, 452, 2654, 16235, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0631552554191427, "compression_ratio": 1.633879781420765, "no_speech_prob": 6.54030100122327e-06}, {"id": 554, "seek": 287200, "start": 2889.0, "end": 2893.0, "text": " So now what is de by da and de by dv?", "tokens": [50364, 407, 437, 307, 264, 2654, 16235, 30, 8537, 11, 368, 538, 1120, 13, 50614, 50614, 492, 362, 281, 574, 412, 300, 13, 50764, 50764, 407, 286, 478, 257, 707, 1413, 9984, 1854, 257, 5994, 4295, 11, 50964, 50964, 293, 286, 787, 458, 300, 286, 630, 257, 1413, 272, 11, 293, 286, 7126, 364, 308, 13, 51214, 51214, 407, 586, 437, 307, 368, 538, 1120, 293, 368, 538, 274, 85, 30, 51414, 51414, 663, 311, 264, 787, 551, 300, 286, 1333, 295, 458, 466, 13, 663, 311, 452, 2654, 16235, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0631552554191427, "compression_ratio": 1.633879781420765, "no_speech_prob": 6.54030100122327e-06}, {"id": 555, "seek": 287200, "start": 2893.0, "end": 2897.0, "text": " That's the only thing that I sort of know about. That's my local gradient.", "tokens": [50364, 407, 437, 307, 264, 2654, 16235, 30, 8537, 11, 368, 538, 1120, 13, 50614, 50614, 492, 362, 281, 574, 412, 300, 13, 50764, 50764, 407, 286, 478, 257, 707, 1413, 9984, 1854, 257, 5994, 4295, 11, 50964, 50964, 293, 286, 787, 458, 300, 286, 630, 257, 1413, 272, 11, 293, 286, 7126, 364, 308, 13, 51214, 51214, 407, 586, 437, 307, 368, 538, 1120, 293, 368, 538, 274, 85, 30, 51414, 51414, 663, 311, 264, 787, 551, 300, 286, 1333, 295, 458, 466, 13, 663, 311, 452, 2654, 16235, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0631552554191427, "compression_ratio": 1.633879781420765, "no_speech_prob": 6.54030100122327e-06}, {"id": 556, "seek": 289700, "start": 2897.0, "end": 2904.0, "text": " So because we have that e is a times b, we're asking what is de by da?", "tokens": [50364, 407, 570, 321, 362, 300, 308, 307, 257, 1413, 272, 11, 321, 434, 3365, 437, 307, 368, 538, 1120, 30, 50714, 50714, 400, 295, 1164, 11, 321, 445, 630, 300, 510, 13, 492, 632, 257, 1413, 11, 370, 286, 478, 406, 516, 281, 319, 12, 1068, 488, 309, 13, 51014, 51014, 583, 498, 291, 528, 281, 23203, 341, 365, 3104, 281, 257, 11, 291, 603, 445, 483, 272, 11, 558, 30, 51264, 51264, 440, 2158, 295, 272, 11, 597, 294, 341, 1389, 307, 3671, 805, 13, 15, 13, 51564, 51564, 407, 1936, 11, 321, 362, 300, 37873, 538, 1120, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09089379127209003, "compression_ratio": 1.5207373271889402, "no_speech_prob": 2.3551889171358198e-05}, {"id": 557, "seek": 289700, "start": 2904.0, "end": 2910.0, "text": " And of course, we just did that here. We had a times, so I'm not going to re-derive it.", "tokens": [50364, 407, 570, 321, 362, 300, 308, 307, 257, 1413, 272, 11, 321, 434, 3365, 437, 307, 368, 538, 1120, 30, 50714, 50714, 400, 295, 1164, 11, 321, 445, 630, 300, 510, 13, 492, 632, 257, 1413, 11, 370, 286, 478, 406, 516, 281, 319, 12, 1068, 488, 309, 13, 51014, 51014, 583, 498, 291, 528, 281, 23203, 341, 365, 3104, 281, 257, 11, 291, 603, 445, 483, 272, 11, 558, 30, 51264, 51264, 440, 2158, 295, 272, 11, 597, 294, 341, 1389, 307, 3671, 805, 13, 15, 13, 51564, 51564, 407, 1936, 11, 321, 362, 300, 37873, 538, 1120, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09089379127209003, "compression_ratio": 1.5207373271889402, "no_speech_prob": 2.3551889171358198e-05}, {"id": 558, "seek": 289700, "start": 2910.0, "end": 2915.0, "text": " But if you want to differentiate this with respect to a, you'll just get b, right?", "tokens": [50364, 407, 570, 321, 362, 300, 308, 307, 257, 1413, 272, 11, 321, 434, 3365, 437, 307, 368, 538, 1120, 30, 50714, 50714, 400, 295, 1164, 11, 321, 445, 630, 300, 510, 13, 492, 632, 257, 1413, 11, 370, 286, 478, 406, 516, 281, 319, 12, 1068, 488, 309, 13, 51014, 51014, 583, 498, 291, 528, 281, 23203, 341, 365, 3104, 281, 257, 11, 291, 603, 445, 483, 272, 11, 558, 30, 51264, 51264, 440, 2158, 295, 272, 11, 597, 294, 341, 1389, 307, 3671, 805, 13, 15, 13, 51564, 51564, 407, 1936, 11, 321, 362, 300, 37873, 538, 1120, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09089379127209003, "compression_ratio": 1.5207373271889402, "no_speech_prob": 2.3551889171358198e-05}, {"id": 559, "seek": 289700, "start": 2915.0, "end": 2921.0, "text": " The value of b, which in this case is negative 3.0.", "tokens": [50364, 407, 570, 321, 362, 300, 308, 307, 257, 1413, 272, 11, 321, 434, 3365, 437, 307, 368, 538, 1120, 30, 50714, 50714, 400, 295, 1164, 11, 321, 445, 630, 300, 510, 13, 492, 632, 257, 1413, 11, 370, 286, 478, 406, 516, 281, 319, 12, 1068, 488, 309, 13, 51014, 51014, 583, 498, 291, 528, 281, 23203, 341, 365, 3104, 281, 257, 11, 291, 603, 445, 483, 272, 11, 558, 30, 51264, 51264, 440, 2158, 295, 272, 11, 597, 294, 341, 1389, 307, 3671, 805, 13, 15, 13, 51564, 51564, 407, 1936, 11, 321, 362, 300, 37873, 538, 1120, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09089379127209003, "compression_ratio": 1.5207373271889402, "no_speech_prob": 2.3551889171358198e-05}, {"id": 560, "seek": 289700, "start": 2921.0, "end": 2925.0, "text": " So basically, we have that dl by da.", "tokens": [50364, 407, 570, 321, 362, 300, 308, 307, 257, 1413, 272, 11, 321, 434, 3365, 437, 307, 368, 538, 1120, 30, 50714, 50714, 400, 295, 1164, 11, 321, 445, 630, 300, 510, 13, 492, 632, 257, 1413, 11, 370, 286, 478, 406, 516, 281, 319, 12, 1068, 488, 309, 13, 51014, 51014, 583, 498, 291, 528, 281, 23203, 341, 365, 3104, 281, 257, 11, 291, 603, 445, 483, 272, 11, 558, 30, 51264, 51264, 440, 2158, 295, 272, 11, 597, 294, 341, 1389, 307, 3671, 805, 13, 15, 13, 51564, 51564, 407, 1936, 11, 321, 362, 300, 37873, 538, 1120, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09089379127209003, "compression_ratio": 1.5207373271889402, "no_speech_prob": 2.3551889171358198e-05}, {"id": 561, "seek": 292500, "start": 2925.0, "end": 2929.0, "text": " Well, let me just do it right here. We have that a dot grad,", "tokens": [50364, 1042, 11, 718, 385, 445, 360, 309, 558, 510, 13, 492, 362, 300, 257, 5893, 2771, 11, 50564, 50564, 293, 321, 366, 9275, 5021, 4978, 510, 11, 307, 37873, 538, 368, 11, 597, 321, 536, 510, 307, 3671, 568, 11, 50914, 50914, 1413, 437, 307, 368, 538, 1120, 30, 51114, 51114, 467, 311, 264, 2158, 295, 272, 11, 597, 307, 3671, 805, 13, 51364, 51364, 663, 311, 309, 13, 51514, 51514, 400, 550, 321, 362, 272, 5893, 2771, 11, 307, 797, 11, 37873, 538, 368, 11, 597, 307, 3671, 568, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09410789891293174, "compression_ratio": 1.6384180790960452, "no_speech_prob": 3.4267559385625646e-05}, {"id": 562, "seek": 292500, "start": 2929.0, "end": 2936.0, "text": " and we are applying chain rule here, is dl by de, which we see here is negative 2,", "tokens": [50364, 1042, 11, 718, 385, 445, 360, 309, 558, 510, 13, 492, 362, 300, 257, 5893, 2771, 11, 50564, 50564, 293, 321, 366, 9275, 5021, 4978, 510, 11, 307, 37873, 538, 368, 11, 597, 321, 536, 510, 307, 3671, 568, 11, 50914, 50914, 1413, 437, 307, 368, 538, 1120, 30, 51114, 51114, 467, 311, 264, 2158, 295, 272, 11, 597, 307, 3671, 805, 13, 51364, 51364, 663, 311, 309, 13, 51514, 51514, 400, 550, 321, 362, 272, 5893, 2771, 11, 307, 797, 11, 37873, 538, 368, 11, 597, 307, 3671, 568, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09410789891293174, "compression_ratio": 1.6384180790960452, "no_speech_prob": 3.4267559385625646e-05}, {"id": 563, "seek": 292500, "start": 2936.0, "end": 2940.0, "text": " times what is de by da?", "tokens": [50364, 1042, 11, 718, 385, 445, 360, 309, 558, 510, 13, 492, 362, 300, 257, 5893, 2771, 11, 50564, 50564, 293, 321, 366, 9275, 5021, 4978, 510, 11, 307, 37873, 538, 368, 11, 597, 321, 536, 510, 307, 3671, 568, 11, 50914, 50914, 1413, 437, 307, 368, 538, 1120, 30, 51114, 51114, 467, 311, 264, 2158, 295, 272, 11, 597, 307, 3671, 805, 13, 51364, 51364, 663, 311, 309, 13, 51514, 51514, 400, 550, 321, 362, 272, 5893, 2771, 11, 307, 797, 11, 37873, 538, 368, 11, 597, 307, 3671, 568, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09410789891293174, "compression_ratio": 1.6384180790960452, "no_speech_prob": 3.4267559385625646e-05}, {"id": 564, "seek": 292500, "start": 2940.0, "end": 2945.0, "text": " It's the value of b, which is negative 3.", "tokens": [50364, 1042, 11, 718, 385, 445, 360, 309, 558, 510, 13, 492, 362, 300, 257, 5893, 2771, 11, 50564, 50564, 293, 321, 366, 9275, 5021, 4978, 510, 11, 307, 37873, 538, 368, 11, 597, 321, 536, 510, 307, 3671, 568, 11, 50914, 50914, 1413, 437, 307, 368, 538, 1120, 30, 51114, 51114, 467, 311, 264, 2158, 295, 272, 11, 597, 307, 3671, 805, 13, 51364, 51364, 663, 311, 309, 13, 51514, 51514, 400, 550, 321, 362, 272, 5893, 2771, 11, 307, 797, 11, 37873, 538, 368, 11, 597, 307, 3671, 568, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09410789891293174, "compression_ratio": 1.6384180790960452, "no_speech_prob": 3.4267559385625646e-05}, {"id": 565, "seek": 292500, "start": 2945.0, "end": 2948.0, "text": " That's it.", "tokens": [50364, 1042, 11, 718, 385, 445, 360, 309, 558, 510, 13, 492, 362, 300, 257, 5893, 2771, 11, 50564, 50564, 293, 321, 366, 9275, 5021, 4978, 510, 11, 307, 37873, 538, 368, 11, 597, 321, 536, 510, 307, 3671, 568, 11, 50914, 50914, 1413, 437, 307, 368, 538, 1120, 30, 51114, 51114, 467, 311, 264, 2158, 295, 272, 11, 597, 307, 3671, 805, 13, 51364, 51364, 663, 311, 309, 13, 51514, 51514, 400, 550, 321, 362, 272, 5893, 2771, 11, 307, 797, 11, 37873, 538, 368, 11, 597, 307, 3671, 568, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09410789891293174, "compression_ratio": 1.6384180790960452, "no_speech_prob": 3.4267559385625646e-05}, {"id": 566, "seek": 292500, "start": 2948.0, "end": 2953.0, "text": " And then we have b dot grad, is again, dl by de, which is negative 2,", "tokens": [50364, 1042, 11, 718, 385, 445, 360, 309, 558, 510, 13, 492, 362, 300, 257, 5893, 2771, 11, 50564, 50564, 293, 321, 366, 9275, 5021, 4978, 510, 11, 307, 37873, 538, 368, 11, 597, 321, 536, 510, 307, 3671, 568, 11, 50914, 50914, 1413, 437, 307, 368, 538, 1120, 30, 51114, 51114, 467, 311, 264, 2158, 295, 272, 11, 597, 307, 3671, 805, 13, 51364, 51364, 663, 311, 309, 13, 51514, 51514, 400, 550, 321, 362, 272, 5893, 2771, 11, 307, 797, 11, 37873, 538, 368, 11, 597, 307, 3671, 568, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09410789891293174, "compression_ratio": 1.6384180790960452, "no_speech_prob": 3.4267559385625646e-05}, {"id": 567, "seek": 295300, "start": 2953.0, "end": 2959.0, "text": " just the same way, times what is de by db?", "tokens": [50364, 445, 264, 912, 636, 11, 1413, 437, 307, 368, 538, 274, 65, 30, 50664, 50664, 467, 311, 264, 2158, 295, 257, 11, 597, 307, 568, 13, 15, 13, 663, 311, 264, 2158, 295, 257, 13, 50964, 50964, 407, 613, 366, 527, 12941, 33733, 13, 961, 311, 2182, 5131, 13, 51314, 51314, 400, 321, 536, 510, 300, 257, 5893, 2771, 4523, 484, 281, 312, 1386, 11, 570, 300, 307, 3671, 568, 1413, 3671, 805, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08147192001342773, "compression_ratio": 1.467455621301775, "no_speech_prob": 3.763554195757024e-05}, {"id": 568, "seek": 295300, "start": 2959.0, "end": 2965.0, "text": " It's the value of a, which is 2.0. That's the value of a.", "tokens": [50364, 445, 264, 912, 636, 11, 1413, 437, 307, 368, 538, 274, 65, 30, 50664, 50664, 467, 311, 264, 2158, 295, 257, 11, 597, 307, 568, 13, 15, 13, 663, 311, 264, 2158, 295, 257, 13, 50964, 50964, 407, 613, 366, 527, 12941, 33733, 13, 961, 311, 2182, 5131, 13, 51314, 51314, 400, 321, 536, 510, 300, 257, 5893, 2771, 4523, 484, 281, 312, 1386, 11, 570, 300, 307, 3671, 568, 1413, 3671, 805, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08147192001342773, "compression_ratio": 1.467455621301775, "no_speech_prob": 3.763554195757024e-05}, {"id": 569, "seek": 295300, "start": 2965.0, "end": 2972.0, "text": " So these are our claimed derivatives. Let's redraw.", "tokens": [50364, 445, 264, 912, 636, 11, 1413, 437, 307, 368, 538, 274, 65, 30, 50664, 50664, 467, 311, 264, 2158, 295, 257, 11, 597, 307, 568, 13, 15, 13, 663, 311, 264, 2158, 295, 257, 13, 50964, 50964, 407, 613, 366, 527, 12941, 33733, 13, 961, 311, 2182, 5131, 13, 51314, 51314, 400, 321, 536, 510, 300, 257, 5893, 2771, 4523, 484, 281, 312, 1386, 11, 570, 300, 307, 3671, 568, 1413, 3671, 805, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08147192001342773, "compression_ratio": 1.467455621301775, "no_speech_prob": 3.763554195757024e-05}, {"id": 570, "seek": 295300, "start": 2972.0, "end": 2978.0, "text": " And we see here that a dot grad turns out to be 6, because that is negative 2 times negative 3.", "tokens": [50364, 445, 264, 912, 636, 11, 1413, 437, 307, 368, 538, 274, 65, 30, 50664, 50664, 467, 311, 264, 2158, 295, 257, 11, 597, 307, 568, 13, 15, 13, 663, 311, 264, 2158, 295, 257, 13, 50964, 50964, 407, 613, 366, 527, 12941, 33733, 13, 961, 311, 2182, 5131, 13, 51314, 51314, 400, 321, 536, 510, 300, 257, 5893, 2771, 4523, 484, 281, 312, 1386, 11, 570, 300, 307, 3671, 568, 1413, 3671, 805, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08147192001342773, "compression_ratio": 1.467455621301775, "no_speech_prob": 3.763554195757024e-05}, {"id": 571, "seek": 297800, "start": 2978.0, "end": 2985.0, "text": " And b dot grad is negative 2 times 2, which is negative 4.", "tokens": [50364, 400, 272, 5893, 2771, 307, 3671, 568, 1413, 568, 11, 597, 307, 3671, 1017, 13, 50714, 50714, 407, 729, 366, 527, 9441, 13, 961, 311, 12097, 341, 293, 718, 311, 16888, 552, 13, 50964, 50964, 492, 362, 257, 510, 11, 257, 5893, 1412, 1804, 6915, 276, 13, 51314, 51314, 407, 264, 3932, 307, 300, 257, 5893, 2771, 307, 1386, 13, 961, 311, 16888, 13, 1386, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0763878549848284, "compression_ratio": 1.483221476510067, "no_speech_prob": 7.527871275669895e-06}, {"id": 572, "seek": 297800, "start": 2985.0, "end": 2990.0, "text": " So those are our claims. Let's delete this and let's verify them.", "tokens": [50364, 400, 272, 5893, 2771, 307, 3671, 568, 1413, 568, 11, 597, 307, 3671, 1017, 13, 50714, 50714, 407, 729, 366, 527, 9441, 13, 961, 311, 12097, 341, 293, 718, 311, 16888, 552, 13, 50964, 50964, 492, 362, 257, 510, 11, 257, 5893, 1412, 1804, 6915, 276, 13, 51314, 51314, 407, 264, 3932, 307, 300, 257, 5893, 2771, 307, 1386, 13, 961, 311, 16888, 13, 1386, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0763878549848284, "compression_ratio": 1.483221476510067, "no_speech_prob": 7.527871275669895e-06}, {"id": 573, "seek": 297800, "start": 2990.0, "end": 2997.0, "text": " We have a here, a dot data plus equals h.", "tokens": [50364, 400, 272, 5893, 2771, 307, 3671, 568, 1413, 568, 11, 597, 307, 3671, 1017, 13, 50714, 50714, 407, 729, 366, 527, 9441, 13, 961, 311, 12097, 341, 293, 718, 311, 16888, 552, 13, 50964, 50964, 492, 362, 257, 510, 11, 257, 5893, 1412, 1804, 6915, 276, 13, 51314, 51314, 407, 264, 3932, 307, 300, 257, 5893, 2771, 307, 1386, 13, 961, 311, 16888, 13, 1386, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0763878549848284, "compression_ratio": 1.483221476510067, "no_speech_prob": 7.527871275669895e-06}, {"id": 574, "seek": 297800, "start": 2997.0, "end": 3005.0, "text": " So the claim is that a dot grad is 6. Let's verify. 6.", "tokens": [50364, 400, 272, 5893, 2771, 307, 3671, 568, 1413, 568, 11, 597, 307, 3671, 1017, 13, 50714, 50714, 407, 729, 366, 527, 9441, 13, 961, 311, 12097, 341, 293, 718, 311, 16888, 552, 13, 50964, 50964, 492, 362, 257, 510, 11, 257, 5893, 1412, 1804, 6915, 276, 13, 51314, 51314, 407, 264, 3932, 307, 300, 257, 5893, 2771, 307, 1386, 13, 961, 311, 16888, 13, 1386, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0763878549848284, "compression_ratio": 1.483221476510067, "no_speech_prob": 7.527871275669895e-06}, {"id": 575, "seek": 300500, "start": 3005.0, "end": 3009.0, "text": " And we have b dot data plus equals h.", "tokens": [50364, 400, 321, 362, 272, 5893, 1412, 1804, 6915, 276, 13, 50564, 50564, 407, 40045, 3249, 272, 538, 276, 293, 1237, 412, 437, 2314, 11, 321, 3932, 309, 311, 3671, 1017, 13, 50864, 50864, 400, 6451, 11, 309, 311, 3671, 1017, 11, 1804, 3175, 11, 797, 11, 15706, 11, 7401, 1287, 13, 51214, 51214, 400, 300, 311, 309, 13, 663, 390, 264, 9688, 646, 79, 1513, 559, 399, 439, 264, 636, 490, 510, 281, 439, 264, 12097, 13891, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13001623386290015, "compression_ratio": 1.511111111111111, "no_speech_prob": 5.594233243755298e-06}, {"id": 576, "seek": 300500, "start": 3009.0, "end": 3015.0, "text": " So nudging b by h and looking at what happens, we claim it's negative 4.", "tokens": [50364, 400, 321, 362, 272, 5893, 1412, 1804, 6915, 276, 13, 50564, 50564, 407, 40045, 3249, 272, 538, 276, 293, 1237, 412, 437, 2314, 11, 321, 3932, 309, 311, 3671, 1017, 13, 50864, 50864, 400, 6451, 11, 309, 311, 3671, 1017, 11, 1804, 3175, 11, 797, 11, 15706, 11, 7401, 1287, 13, 51214, 51214, 400, 300, 311, 309, 13, 663, 390, 264, 9688, 646, 79, 1513, 559, 399, 439, 264, 636, 490, 510, 281, 439, 264, 12097, 13891, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13001623386290015, "compression_ratio": 1.511111111111111, "no_speech_prob": 5.594233243755298e-06}, {"id": 577, "seek": 300500, "start": 3015.0, "end": 3022.0, "text": " And indeed, it's negative 4, plus minus, again, float, oddness.", "tokens": [50364, 400, 321, 362, 272, 5893, 1412, 1804, 6915, 276, 13, 50564, 50564, 407, 40045, 3249, 272, 538, 276, 293, 1237, 412, 437, 2314, 11, 321, 3932, 309, 311, 3671, 1017, 13, 50864, 50864, 400, 6451, 11, 309, 311, 3671, 1017, 11, 1804, 3175, 11, 797, 11, 15706, 11, 7401, 1287, 13, 51214, 51214, 400, 300, 311, 309, 13, 663, 390, 264, 9688, 646, 79, 1513, 559, 399, 439, 264, 636, 490, 510, 281, 439, 264, 12097, 13891, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13001623386290015, "compression_ratio": 1.511111111111111, "no_speech_prob": 5.594233243755298e-06}, {"id": 578, "seek": 300500, "start": 3022.0, "end": 3031.0, "text": " And that's it. That was the manual backpropagation all the way from here to all the delete nodes.", "tokens": [50364, 400, 321, 362, 272, 5893, 1412, 1804, 6915, 276, 13, 50564, 50564, 407, 40045, 3249, 272, 538, 276, 293, 1237, 412, 437, 2314, 11, 321, 3932, 309, 311, 3671, 1017, 13, 50864, 50864, 400, 6451, 11, 309, 311, 3671, 1017, 11, 1804, 3175, 11, 797, 11, 15706, 11, 7401, 1287, 13, 51214, 51214, 400, 300, 311, 309, 13, 663, 390, 264, 9688, 646, 79, 1513, 559, 399, 439, 264, 636, 490, 510, 281, 439, 264, 12097, 13891, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13001623386290015, "compression_ratio": 1.511111111111111, "no_speech_prob": 5.594233243755298e-06}, {"id": 579, "seek": 303100, "start": 3031.0, "end": 3040.0, "text": " And we've done it piece by piece. And really, all we've done is, as you saw, we iterated through all the nodes one by one and locally applied the chain rule.", "tokens": [50364, 400, 321, 600, 1096, 309, 2522, 538, 2522, 13, 400, 534, 11, 439, 321, 600, 1096, 307, 11, 382, 291, 1866, 11, 321, 17138, 770, 807, 439, 264, 13891, 472, 538, 472, 293, 16143, 6456, 264, 5021, 4978, 13, 50814, 50814, 492, 1009, 458, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 341, 707, 5598, 13, 51014, 51014, 400, 550, 321, 574, 412, 577, 341, 5598, 390, 7126, 13, 639, 5598, 390, 7126, 807, 512, 6916, 13, 51264, 51264, 400, 321, 362, 264, 44548, 281, 264, 2227, 13891, 295, 341, 6916, 13, 51414, 51414, 400, 370, 294, 341, 707, 6916, 11, 321, 458, 437, 264, 2654, 33733, 366, 293, 321, 445, 12972, 552, 3911, 264, 13760, 1009, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08192101145178322, "compression_ratio": 1.9304029304029304, "no_speech_prob": 7.842866034479812e-05}, {"id": 580, "seek": 303100, "start": 3040.0, "end": 3044.0, "text": " We always know what is the derivative of L with respect to this little output.", "tokens": [50364, 400, 321, 600, 1096, 309, 2522, 538, 2522, 13, 400, 534, 11, 439, 321, 600, 1096, 307, 11, 382, 291, 1866, 11, 321, 17138, 770, 807, 439, 264, 13891, 472, 538, 472, 293, 16143, 6456, 264, 5021, 4978, 13, 50814, 50814, 492, 1009, 458, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 341, 707, 5598, 13, 51014, 51014, 400, 550, 321, 574, 412, 577, 341, 5598, 390, 7126, 13, 639, 5598, 390, 7126, 807, 512, 6916, 13, 51264, 51264, 400, 321, 362, 264, 44548, 281, 264, 2227, 13891, 295, 341, 6916, 13, 51414, 51414, 400, 370, 294, 341, 707, 6916, 11, 321, 458, 437, 264, 2654, 33733, 366, 293, 321, 445, 12972, 552, 3911, 264, 13760, 1009, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08192101145178322, "compression_ratio": 1.9304029304029304, "no_speech_prob": 7.842866034479812e-05}, {"id": 581, "seek": 303100, "start": 3044.0, "end": 3049.0, "text": " And then we look at how this output was produced. This output was produced through some operation.", "tokens": [50364, 400, 321, 600, 1096, 309, 2522, 538, 2522, 13, 400, 534, 11, 439, 321, 600, 1096, 307, 11, 382, 291, 1866, 11, 321, 17138, 770, 807, 439, 264, 13891, 472, 538, 472, 293, 16143, 6456, 264, 5021, 4978, 13, 50814, 50814, 492, 1009, 458, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 341, 707, 5598, 13, 51014, 51014, 400, 550, 321, 574, 412, 577, 341, 5598, 390, 7126, 13, 639, 5598, 390, 7126, 807, 512, 6916, 13, 51264, 51264, 400, 321, 362, 264, 44548, 281, 264, 2227, 13891, 295, 341, 6916, 13, 51414, 51414, 400, 370, 294, 341, 707, 6916, 11, 321, 458, 437, 264, 2654, 33733, 366, 293, 321, 445, 12972, 552, 3911, 264, 13760, 1009, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08192101145178322, "compression_ratio": 1.9304029304029304, "no_speech_prob": 7.842866034479812e-05}, {"id": 582, "seek": 303100, "start": 3049.0, "end": 3052.0, "text": " And we have the pointers to the children nodes of this operation.", "tokens": [50364, 400, 321, 600, 1096, 309, 2522, 538, 2522, 13, 400, 534, 11, 439, 321, 600, 1096, 307, 11, 382, 291, 1866, 11, 321, 17138, 770, 807, 439, 264, 13891, 472, 538, 472, 293, 16143, 6456, 264, 5021, 4978, 13, 50814, 50814, 492, 1009, 458, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 341, 707, 5598, 13, 51014, 51014, 400, 550, 321, 574, 412, 577, 341, 5598, 390, 7126, 13, 639, 5598, 390, 7126, 807, 512, 6916, 13, 51264, 51264, 400, 321, 362, 264, 44548, 281, 264, 2227, 13891, 295, 341, 6916, 13, 51414, 51414, 400, 370, 294, 341, 707, 6916, 11, 321, 458, 437, 264, 2654, 33733, 366, 293, 321, 445, 12972, 552, 3911, 264, 13760, 1009, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08192101145178322, "compression_ratio": 1.9304029304029304, "no_speech_prob": 7.842866034479812e-05}, {"id": 583, "seek": 303100, "start": 3052.0, "end": 3059.0, "text": " And so in this little operation, we know what the local derivatives are and we just multiply them onto the derivative always.", "tokens": [50364, 400, 321, 600, 1096, 309, 2522, 538, 2522, 13, 400, 534, 11, 439, 321, 600, 1096, 307, 11, 382, 291, 1866, 11, 321, 17138, 770, 807, 439, 264, 13891, 472, 538, 472, 293, 16143, 6456, 264, 5021, 4978, 13, 50814, 50814, 492, 1009, 458, 437, 307, 264, 13760, 295, 441, 365, 3104, 281, 341, 707, 5598, 13, 51014, 51014, 400, 550, 321, 574, 412, 577, 341, 5598, 390, 7126, 13, 639, 5598, 390, 7126, 807, 512, 6916, 13, 51264, 51264, 400, 321, 362, 264, 44548, 281, 264, 2227, 13891, 295, 341, 6916, 13, 51414, 51414, 400, 370, 294, 341, 707, 6916, 11, 321, 458, 437, 264, 2654, 33733, 366, 293, 321, 445, 12972, 552, 3911, 264, 13760, 1009, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08192101145178322, "compression_ratio": 1.9304029304029304, "no_speech_prob": 7.842866034479812e-05}, {"id": 584, "seek": 305900, "start": 3059.0, "end": 3064.0, "text": " So we just go through and recursively multiply on the local derivatives.", "tokens": [50364, 407, 321, 445, 352, 807, 293, 20560, 3413, 12972, 322, 264, 2654, 33733, 13, 50614, 50614, 400, 300, 311, 437, 646, 79, 1513, 559, 399, 307, 13, 467, 311, 445, 257, 20560, 488, 3861, 295, 5021, 4978, 12204, 807, 264, 24903, 4295, 13, 50914, 50914, 961, 311, 536, 341, 1347, 294, 3069, 445, 588, 10515, 13, 51064, 51064, 708, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 297, 16032, 527, 15743, 281, 853, 281, 652, 441, 352, 493, 13, 51364, 51364, 407, 294, 1729, 11, 437, 321, 434, 884, 307, 321, 528, 257, 5893, 1412, 13, 492, 434, 516, 281, 1319, 309, 13, 51614, 51614, 400, 498, 321, 528, 441, 281, 352, 493, 11, 300, 1355, 321, 445, 362, 281, 352, 294, 264, 3513, 295, 264, 16235, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.038752370805882695, "compression_ratio": 1.7809187279151943, "no_speech_prob": 7.76676824898459e-06}, {"id": 585, "seek": 305900, "start": 3064.0, "end": 3070.0, "text": " And that's what backpropagation is. It's just a recursive application of chain rule backwards through the computation graph.", "tokens": [50364, 407, 321, 445, 352, 807, 293, 20560, 3413, 12972, 322, 264, 2654, 33733, 13, 50614, 50614, 400, 300, 311, 437, 646, 79, 1513, 559, 399, 307, 13, 467, 311, 445, 257, 20560, 488, 3861, 295, 5021, 4978, 12204, 807, 264, 24903, 4295, 13, 50914, 50914, 961, 311, 536, 341, 1347, 294, 3069, 445, 588, 10515, 13, 51064, 51064, 708, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 297, 16032, 527, 15743, 281, 853, 281, 652, 441, 352, 493, 13, 51364, 51364, 407, 294, 1729, 11, 437, 321, 434, 884, 307, 321, 528, 257, 5893, 1412, 13, 492, 434, 516, 281, 1319, 309, 13, 51614, 51614, 400, 498, 321, 528, 441, 281, 352, 493, 11, 300, 1355, 321, 445, 362, 281, 352, 294, 264, 3513, 295, 264, 16235, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.038752370805882695, "compression_ratio": 1.7809187279151943, "no_speech_prob": 7.76676824898459e-06}, {"id": 586, "seek": 305900, "start": 3070.0, "end": 3073.0, "text": " Let's see this power in action just very briefly.", "tokens": [50364, 407, 321, 445, 352, 807, 293, 20560, 3413, 12972, 322, 264, 2654, 33733, 13, 50614, 50614, 400, 300, 311, 437, 646, 79, 1513, 559, 399, 307, 13, 467, 311, 445, 257, 20560, 488, 3861, 295, 5021, 4978, 12204, 807, 264, 24903, 4295, 13, 50914, 50914, 961, 311, 536, 341, 1347, 294, 3069, 445, 588, 10515, 13, 51064, 51064, 708, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 297, 16032, 527, 15743, 281, 853, 281, 652, 441, 352, 493, 13, 51364, 51364, 407, 294, 1729, 11, 437, 321, 434, 884, 307, 321, 528, 257, 5893, 1412, 13, 492, 434, 516, 281, 1319, 309, 13, 51614, 51614, 400, 498, 321, 528, 441, 281, 352, 493, 11, 300, 1355, 321, 445, 362, 281, 352, 294, 264, 3513, 295, 264, 16235, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.038752370805882695, "compression_ratio": 1.7809187279151943, "no_speech_prob": 7.76676824898459e-06}, {"id": 587, "seek": 305900, "start": 3073.0, "end": 3079.0, "text": " What we're going to do is we're going to nudge our inputs to try to make L go up.", "tokens": [50364, 407, 321, 445, 352, 807, 293, 20560, 3413, 12972, 322, 264, 2654, 33733, 13, 50614, 50614, 400, 300, 311, 437, 646, 79, 1513, 559, 399, 307, 13, 467, 311, 445, 257, 20560, 488, 3861, 295, 5021, 4978, 12204, 807, 264, 24903, 4295, 13, 50914, 50914, 961, 311, 536, 341, 1347, 294, 3069, 445, 588, 10515, 13, 51064, 51064, 708, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 297, 16032, 527, 15743, 281, 853, 281, 652, 441, 352, 493, 13, 51364, 51364, 407, 294, 1729, 11, 437, 321, 434, 884, 307, 321, 528, 257, 5893, 1412, 13, 492, 434, 516, 281, 1319, 309, 13, 51614, 51614, 400, 498, 321, 528, 441, 281, 352, 493, 11, 300, 1355, 321, 445, 362, 281, 352, 294, 264, 3513, 295, 264, 16235, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.038752370805882695, "compression_ratio": 1.7809187279151943, "no_speech_prob": 7.76676824898459e-06}, {"id": 588, "seek": 305900, "start": 3079.0, "end": 3084.0, "text": " So in particular, what we're doing is we want a dot data. We're going to change it.", "tokens": [50364, 407, 321, 445, 352, 807, 293, 20560, 3413, 12972, 322, 264, 2654, 33733, 13, 50614, 50614, 400, 300, 311, 437, 646, 79, 1513, 559, 399, 307, 13, 467, 311, 445, 257, 20560, 488, 3861, 295, 5021, 4978, 12204, 807, 264, 24903, 4295, 13, 50914, 50914, 961, 311, 536, 341, 1347, 294, 3069, 445, 588, 10515, 13, 51064, 51064, 708, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 297, 16032, 527, 15743, 281, 853, 281, 652, 441, 352, 493, 13, 51364, 51364, 407, 294, 1729, 11, 437, 321, 434, 884, 307, 321, 528, 257, 5893, 1412, 13, 492, 434, 516, 281, 1319, 309, 13, 51614, 51614, 400, 498, 321, 528, 441, 281, 352, 493, 11, 300, 1355, 321, 445, 362, 281, 352, 294, 264, 3513, 295, 264, 16235, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.038752370805882695, "compression_ratio": 1.7809187279151943, "no_speech_prob": 7.76676824898459e-06}, {"id": 589, "seek": 305900, "start": 3084.0, "end": 3088.0, "text": " And if we want L to go up, that means we just have to go in the direction of the gradient.", "tokens": [50364, 407, 321, 445, 352, 807, 293, 20560, 3413, 12972, 322, 264, 2654, 33733, 13, 50614, 50614, 400, 300, 311, 437, 646, 79, 1513, 559, 399, 307, 13, 467, 311, 445, 257, 20560, 488, 3861, 295, 5021, 4978, 12204, 807, 264, 24903, 4295, 13, 50914, 50914, 961, 311, 536, 341, 1347, 294, 3069, 445, 588, 10515, 13, 51064, 51064, 708, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 297, 16032, 527, 15743, 281, 853, 281, 652, 441, 352, 493, 13, 51364, 51364, 407, 294, 1729, 11, 437, 321, 434, 884, 307, 321, 528, 257, 5893, 1412, 13, 492, 434, 516, 281, 1319, 309, 13, 51614, 51614, 400, 498, 321, 528, 441, 281, 352, 493, 11, 300, 1355, 321, 445, 362, 281, 352, 294, 264, 3513, 295, 264, 16235, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.038752370805882695, "compression_ratio": 1.7809187279151943, "no_speech_prob": 7.76676824898459e-06}, {"id": 590, "seek": 308800, "start": 3088.0, "end": 3095.0, "text": " So A should increase in the direction of gradient by like some small step amount.", "tokens": [50364, 407, 316, 820, 3488, 294, 264, 3513, 295, 16235, 538, 411, 512, 1359, 1823, 2372, 13, 50714, 50714, 639, 307, 264, 1823, 2744, 13, 400, 321, 500, 380, 445, 528, 341, 337, 363, 11, 457, 611, 337, 363, 13, 51014, 51014, 2743, 337, 383, 13, 2743, 337, 479, 13, 3950, 366, 10871, 13891, 11, 597, 321, 2673, 362, 1969, 670, 13, 51464, 51464, 400, 498, 321, 297, 16032, 294, 3513, 295, 264, 16235, 11, 321, 2066, 257, 3353, 6503, 322, 441, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11006031479946403, "compression_ratio": 1.5458937198067633, "no_speech_prob": 6.302588008111343e-05}, {"id": 591, "seek": 308800, "start": 3095.0, "end": 3101.0, "text": " This is the step size. And we don't just want this for B, but also for B.", "tokens": [50364, 407, 316, 820, 3488, 294, 264, 3513, 295, 16235, 538, 411, 512, 1359, 1823, 2372, 13, 50714, 50714, 639, 307, 264, 1823, 2744, 13, 400, 321, 500, 380, 445, 528, 341, 337, 363, 11, 457, 611, 337, 363, 13, 51014, 51014, 2743, 337, 383, 13, 2743, 337, 479, 13, 3950, 366, 10871, 13891, 11, 597, 321, 2673, 362, 1969, 670, 13, 51464, 51464, 400, 498, 321, 297, 16032, 294, 3513, 295, 264, 16235, 11, 321, 2066, 257, 3353, 6503, 322, 441, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11006031479946403, "compression_ratio": 1.5458937198067633, "no_speech_prob": 6.302588008111343e-05}, {"id": 592, "seek": 308800, "start": 3101.0, "end": 3110.0, "text": " Also for C. Also for F. Those are leaf nodes, which we usually have control over.", "tokens": [50364, 407, 316, 820, 3488, 294, 264, 3513, 295, 16235, 538, 411, 512, 1359, 1823, 2372, 13, 50714, 50714, 639, 307, 264, 1823, 2744, 13, 400, 321, 500, 380, 445, 528, 341, 337, 363, 11, 457, 611, 337, 363, 13, 51014, 51014, 2743, 337, 383, 13, 2743, 337, 479, 13, 3950, 366, 10871, 13891, 11, 597, 321, 2673, 362, 1969, 670, 13, 51464, 51464, 400, 498, 321, 297, 16032, 294, 3513, 295, 264, 16235, 11, 321, 2066, 257, 3353, 6503, 322, 441, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11006031479946403, "compression_ratio": 1.5458937198067633, "no_speech_prob": 6.302588008111343e-05}, {"id": 593, "seek": 308800, "start": 3110.0, "end": 3115.0, "text": " And if we nudge in direction of the gradient, we expect a positive influence on L.", "tokens": [50364, 407, 316, 820, 3488, 294, 264, 3513, 295, 16235, 538, 411, 512, 1359, 1823, 2372, 13, 50714, 50714, 639, 307, 264, 1823, 2744, 13, 400, 321, 500, 380, 445, 528, 341, 337, 363, 11, 457, 611, 337, 363, 13, 51014, 51014, 2743, 337, 383, 13, 2743, 337, 479, 13, 3950, 366, 10871, 13891, 11, 597, 321, 2673, 362, 1969, 670, 13, 51464, 51464, 400, 498, 321, 297, 16032, 294, 3513, 295, 264, 16235, 11, 321, 2066, 257, 3353, 6503, 322, 441, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11006031479946403, "compression_ratio": 1.5458937198067633, "no_speech_prob": 6.302588008111343e-05}, {"id": 594, "seek": 311500, "start": 3115.0, "end": 3121.0, "text": " So we expect L to go up positively. So it should become less negative.", "tokens": [50364, 407, 321, 2066, 441, 281, 352, 493, 25795, 13, 407, 309, 820, 1813, 1570, 3671, 13, 50664, 50664, 467, 820, 352, 493, 281, 584, 3671, 2309, 420, 746, 411, 300, 13, 467, 311, 1152, 281, 980, 2293, 13, 50964, 50964, 400, 321, 362, 281, 43819, 409, 264, 2128, 1320, 13, 407, 718, 385, 445, 360, 300, 510, 13, 51414, 51414, 639, 576, 312, 264, 2128, 1320, 13, 479, 576, 312, 44553, 13, 639, 307, 8659, 264, 2128, 1320, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08690966755510812, "compression_ratio": 1.7135135135135136, "no_speech_prob": 1.1478628039185423e-05}, {"id": 595, "seek": 311500, "start": 3121.0, "end": 3127.0, "text": " It should go up to say negative six or something like that. It's hard to tell exactly.", "tokens": [50364, 407, 321, 2066, 441, 281, 352, 493, 25795, 13, 407, 309, 820, 1813, 1570, 3671, 13, 50664, 50664, 467, 820, 352, 493, 281, 584, 3671, 2309, 420, 746, 411, 300, 13, 467, 311, 1152, 281, 980, 2293, 13, 50964, 50964, 400, 321, 362, 281, 43819, 409, 264, 2128, 1320, 13, 407, 718, 385, 445, 360, 300, 510, 13, 51414, 51414, 639, 576, 312, 264, 2128, 1320, 13, 479, 576, 312, 44553, 13, 639, 307, 8659, 264, 2128, 1320, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08690966755510812, "compression_ratio": 1.7135135135135136, "no_speech_prob": 1.1478628039185423e-05}, {"id": 596, "seek": 311500, "start": 3127.0, "end": 3136.0, "text": " And we have to rerun the forward pass. So let me just do that here.", "tokens": [50364, 407, 321, 2066, 441, 281, 352, 493, 25795, 13, 407, 309, 820, 1813, 1570, 3671, 13, 50664, 50664, 467, 820, 352, 493, 281, 584, 3671, 2309, 420, 746, 411, 300, 13, 467, 311, 1152, 281, 980, 2293, 13, 50964, 50964, 400, 321, 362, 281, 43819, 409, 264, 2128, 1320, 13, 407, 718, 385, 445, 360, 300, 510, 13, 51414, 51414, 639, 576, 312, 264, 2128, 1320, 13, 479, 576, 312, 44553, 13, 639, 307, 8659, 264, 2128, 1320, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08690966755510812, "compression_ratio": 1.7135135135135136, "no_speech_prob": 1.1478628039185423e-05}, {"id": 597, "seek": 311500, "start": 3136.0, "end": 3141.0, "text": " This would be the forward pass. F would be unchanged. This is effectively the forward pass.", "tokens": [50364, 407, 321, 2066, 441, 281, 352, 493, 25795, 13, 407, 309, 820, 1813, 1570, 3671, 13, 50664, 50664, 467, 820, 352, 493, 281, 584, 3671, 2309, 420, 746, 411, 300, 13, 467, 311, 1152, 281, 980, 2293, 13, 50964, 50964, 400, 321, 362, 281, 43819, 409, 264, 2128, 1320, 13, 407, 718, 385, 445, 360, 300, 510, 13, 51414, 51414, 639, 576, 312, 264, 2128, 1320, 13, 479, 576, 312, 44553, 13, 639, 307, 8659, 264, 2128, 1320, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08690966755510812, "compression_ratio": 1.7135135135135136, "no_speech_prob": 1.1478628039185423e-05}, {"id": 598, "seek": 314100, "start": 3141.0, "end": 3149.0, "text": " And now if we print L.data, we expect because we nudged all the values, all the inputs in the direction of gradient,", "tokens": [50364, 400, 586, 498, 321, 4482, 441, 13, 67, 3274, 11, 321, 2066, 570, 321, 40045, 3004, 439, 264, 4190, 11, 439, 264, 15743, 294, 264, 3513, 295, 16235, 11, 50764, 50764, 321, 5176, 1570, 3671, 441, 13, 492, 5176, 281, 352, 493, 13, 407, 1310, 309, 311, 3671, 2309, 420, 370, 13, 51064, 51064, 961, 311, 536, 437, 2314, 13, 1033, 11, 3671, 3407, 13, 51214, 51214, 400, 341, 307, 1936, 472, 1823, 295, 364, 19618, 300, 486, 917, 493, 2614, 13, 51514, 51514, 400, 534, 11, 341, 16235, 445, 976, 505, 512, 1347, 570, 321, 458, 577, 281, 6503, 264, 2572, 9700, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11685676044887966, "compression_ratio": 1.6732283464566928, "no_speech_prob": 1.2606689779204316e-05}, {"id": 599, "seek": 314100, "start": 3149.0, "end": 3155.0, "text": " we expected less negative L. We expected to go up. So maybe it's negative six or so.", "tokens": [50364, 400, 586, 498, 321, 4482, 441, 13, 67, 3274, 11, 321, 2066, 570, 321, 40045, 3004, 439, 264, 4190, 11, 439, 264, 15743, 294, 264, 3513, 295, 16235, 11, 50764, 50764, 321, 5176, 1570, 3671, 441, 13, 492, 5176, 281, 352, 493, 13, 407, 1310, 309, 311, 3671, 2309, 420, 370, 13, 51064, 51064, 961, 311, 536, 437, 2314, 13, 1033, 11, 3671, 3407, 13, 51214, 51214, 400, 341, 307, 1936, 472, 1823, 295, 364, 19618, 300, 486, 917, 493, 2614, 13, 51514, 51514, 400, 534, 11, 341, 16235, 445, 976, 505, 512, 1347, 570, 321, 458, 577, 281, 6503, 264, 2572, 9700, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11685676044887966, "compression_ratio": 1.6732283464566928, "no_speech_prob": 1.2606689779204316e-05}, {"id": 600, "seek": 314100, "start": 3155.0, "end": 3158.0, "text": " Let's see what happens. Okay, negative seven.", "tokens": [50364, 400, 586, 498, 321, 4482, 441, 13, 67, 3274, 11, 321, 2066, 570, 321, 40045, 3004, 439, 264, 4190, 11, 439, 264, 15743, 294, 264, 3513, 295, 16235, 11, 50764, 50764, 321, 5176, 1570, 3671, 441, 13, 492, 5176, 281, 352, 493, 13, 407, 1310, 309, 311, 3671, 2309, 420, 370, 13, 51064, 51064, 961, 311, 536, 437, 2314, 13, 1033, 11, 3671, 3407, 13, 51214, 51214, 400, 341, 307, 1936, 472, 1823, 295, 364, 19618, 300, 486, 917, 493, 2614, 13, 51514, 51514, 400, 534, 11, 341, 16235, 445, 976, 505, 512, 1347, 570, 321, 458, 577, 281, 6503, 264, 2572, 9700, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11685676044887966, "compression_ratio": 1.6732283464566928, "no_speech_prob": 1.2606689779204316e-05}, {"id": 601, "seek": 314100, "start": 3158.0, "end": 3164.0, "text": " And this is basically one step of an optimization that will end up running.", "tokens": [50364, 400, 586, 498, 321, 4482, 441, 13, 67, 3274, 11, 321, 2066, 570, 321, 40045, 3004, 439, 264, 4190, 11, 439, 264, 15743, 294, 264, 3513, 295, 16235, 11, 50764, 50764, 321, 5176, 1570, 3671, 441, 13, 492, 5176, 281, 352, 493, 13, 407, 1310, 309, 311, 3671, 2309, 420, 370, 13, 51064, 51064, 961, 311, 536, 437, 2314, 13, 1033, 11, 3671, 3407, 13, 51214, 51214, 400, 341, 307, 1936, 472, 1823, 295, 364, 19618, 300, 486, 917, 493, 2614, 13, 51514, 51514, 400, 534, 11, 341, 16235, 445, 976, 505, 512, 1347, 570, 321, 458, 577, 281, 6503, 264, 2572, 9700, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11685676044887966, "compression_ratio": 1.6732283464566928, "no_speech_prob": 1.2606689779204316e-05}, {"id": 602, "seek": 314100, "start": 3164.0, "end": 3169.0, "text": " And really, this gradient just give us some power because we know how to influence the final outcome.", "tokens": [50364, 400, 586, 498, 321, 4482, 441, 13, 67, 3274, 11, 321, 2066, 570, 321, 40045, 3004, 439, 264, 4190, 11, 439, 264, 15743, 294, 264, 3513, 295, 16235, 11, 50764, 50764, 321, 5176, 1570, 3671, 441, 13, 492, 5176, 281, 352, 493, 13, 407, 1310, 309, 311, 3671, 2309, 420, 370, 13, 51064, 51064, 961, 311, 536, 437, 2314, 13, 1033, 11, 3671, 3407, 13, 51214, 51214, 400, 341, 307, 1936, 472, 1823, 295, 364, 19618, 300, 486, 917, 493, 2614, 13, 51514, 51514, 400, 534, 11, 341, 16235, 445, 976, 505, 512, 1347, 570, 321, 458, 577, 281, 6503, 264, 2572, 9700, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11685676044887966, "compression_ratio": 1.6732283464566928, "no_speech_prob": 1.2606689779204316e-05}, {"id": 603, "seek": 316900, "start": 3169.0, "end": 3173.0, "text": " And this will be extremely useful for training neural nets as well as CMC.", "tokens": [50364, 400, 341, 486, 312, 4664, 4420, 337, 3097, 18161, 36170, 382, 731, 382, 20424, 34, 13, 50564, 50564, 407, 586, 286, 576, 411, 281, 360, 472, 544, 1365, 295, 9688, 646, 79, 1513, 559, 399, 1228, 257, 857, 544, 3997, 293, 4420, 1365, 13, 51014, 51014, 492, 366, 516, 281, 646, 79, 1513, 559, 473, 807, 257, 34090, 13, 407, 321, 528, 281, 4728, 1322, 493, 18161, 9590, 13, 51414, 51414, 400, 294, 264, 22811, 1389, 11, 613, 366, 2120, 388, 11167, 43276, 13270, 11, 382, 436, 434, 1219, 13, 51564, 51564, 407, 341, 307, 257, 732, 4583, 18161, 2533, 13, 400, 309, 311, 658, 613, 707, 7914, 1027, 493, 295, 22027, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10592965183095035, "compression_ratio": 1.687732342007435, "no_speech_prob": 7.071454547258327e-06}, {"id": 604, "seek": 316900, "start": 3173.0, "end": 3182.0, "text": " So now I would like to do one more example of manual backpropagation using a bit more complex and useful example.", "tokens": [50364, 400, 341, 486, 312, 4664, 4420, 337, 3097, 18161, 36170, 382, 731, 382, 20424, 34, 13, 50564, 50564, 407, 586, 286, 576, 411, 281, 360, 472, 544, 1365, 295, 9688, 646, 79, 1513, 559, 399, 1228, 257, 857, 544, 3997, 293, 4420, 1365, 13, 51014, 51014, 492, 366, 516, 281, 646, 79, 1513, 559, 473, 807, 257, 34090, 13, 407, 321, 528, 281, 4728, 1322, 493, 18161, 9590, 13, 51414, 51414, 400, 294, 264, 22811, 1389, 11, 613, 366, 2120, 388, 11167, 43276, 13270, 11, 382, 436, 434, 1219, 13, 51564, 51564, 407, 341, 307, 257, 732, 4583, 18161, 2533, 13, 400, 309, 311, 658, 613, 707, 7914, 1027, 493, 295, 22027, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10592965183095035, "compression_ratio": 1.687732342007435, "no_speech_prob": 7.071454547258327e-06}, {"id": 605, "seek": 316900, "start": 3182.0, "end": 3190.0, "text": " We are going to backpropagate through a neuron. So we want to eventually build up neural networks.", "tokens": [50364, 400, 341, 486, 312, 4664, 4420, 337, 3097, 18161, 36170, 382, 731, 382, 20424, 34, 13, 50564, 50564, 407, 586, 286, 576, 411, 281, 360, 472, 544, 1365, 295, 9688, 646, 79, 1513, 559, 399, 1228, 257, 857, 544, 3997, 293, 4420, 1365, 13, 51014, 51014, 492, 366, 516, 281, 646, 79, 1513, 559, 473, 807, 257, 34090, 13, 407, 321, 528, 281, 4728, 1322, 493, 18161, 9590, 13, 51414, 51414, 400, 294, 264, 22811, 1389, 11, 613, 366, 2120, 388, 11167, 43276, 13270, 11, 382, 436, 434, 1219, 13, 51564, 51564, 407, 341, 307, 257, 732, 4583, 18161, 2533, 13, 400, 309, 311, 658, 613, 707, 7914, 1027, 493, 295, 22027, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10592965183095035, "compression_ratio": 1.687732342007435, "no_speech_prob": 7.071454547258327e-06}, {"id": 606, "seek": 316900, "start": 3190.0, "end": 3193.0, "text": " And in the simplest case, these are multilayer perceptrons, as they're called.", "tokens": [50364, 400, 341, 486, 312, 4664, 4420, 337, 3097, 18161, 36170, 382, 731, 382, 20424, 34, 13, 50564, 50564, 407, 586, 286, 576, 411, 281, 360, 472, 544, 1365, 295, 9688, 646, 79, 1513, 559, 399, 1228, 257, 857, 544, 3997, 293, 4420, 1365, 13, 51014, 51014, 492, 366, 516, 281, 646, 79, 1513, 559, 473, 807, 257, 34090, 13, 407, 321, 528, 281, 4728, 1322, 493, 18161, 9590, 13, 51414, 51414, 400, 294, 264, 22811, 1389, 11, 613, 366, 2120, 388, 11167, 43276, 13270, 11, 382, 436, 434, 1219, 13, 51564, 51564, 407, 341, 307, 257, 732, 4583, 18161, 2533, 13, 400, 309, 311, 658, 613, 707, 7914, 1027, 493, 295, 22027, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10592965183095035, "compression_ratio": 1.687732342007435, "no_speech_prob": 7.071454547258327e-06}, {"id": 607, "seek": 316900, "start": 3193.0, "end": 3198.0, "text": " So this is a two layer neural net. And it's got these little layers made up of neurons.", "tokens": [50364, 400, 341, 486, 312, 4664, 4420, 337, 3097, 18161, 36170, 382, 731, 382, 20424, 34, 13, 50564, 50564, 407, 586, 286, 576, 411, 281, 360, 472, 544, 1365, 295, 9688, 646, 79, 1513, 559, 399, 1228, 257, 857, 544, 3997, 293, 4420, 1365, 13, 51014, 51014, 492, 366, 516, 281, 646, 79, 1513, 559, 473, 807, 257, 34090, 13, 407, 321, 528, 281, 4728, 1322, 493, 18161, 9590, 13, 51414, 51414, 400, 294, 264, 22811, 1389, 11, 613, 366, 2120, 388, 11167, 43276, 13270, 11, 382, 436, 434, 1219, 13, 51564, 51564, 407, 341, 307, 257, 732, 4583, 18161, 2533, 13, 400, 309, 311, 658, 613, 707, 7914, 1027, 493, 295, 22027, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10592965183095035, "compression_ratio": 1.687732342007435, "no_speech_prob": 7.071454547258327e-06}, {"id": 608, "seek": 319800, "start": 3198.0, "end": 3200.0, "text": " And these neurons are fully connected to each other.", "tokens": [50364, 400, 613, 22027, 366, 4498, 4582, 281, 1184, 661, 13, 50464, 50464, 823, 11, 3228, 17157, 11, 22027, 366, 588, 6179, 5759, 11, 457, 321, 362, 588, 2199, 18894, 5245, 295, 552, 13, 50764, 50764, 400, 370, 341, 307, 257, 588, 2199, 18894, 2316, 295, 257, 34090, 13, 509, 362, 512, 15743, 11, 10298, 11, 293, 550, 291, 362, 613, 5451, 2382, 279, 300, 362, 17443, 322, 552, 13, 51214, 51214, 407, 264, 343, 311, 366, 17443, 13, 400, 550, 264, 5451, 11145, 43582, 365, 264, 4846, 281, 341, 34090, 17596, 19020, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07537649095672921, "compression_ratio": 1.8237885462555066, "no_speech_prob": 2.3920825697132386e-05}, {"id": 609, "seek": 319800, "start": 3200.0, "end": 3206.0, "text": " Now, biologically, neurons are very complicated devices, but we have very simple mathematical models of them.", "tokens": [50364, 400, 613, 22027, 366, 4498, 4582, 281, 1184, 661, 13, 50464, 50464, 823, 11, 3228, 17157, 11, 22027, 366, 588, 6179, 5759, 11, 457, 321, 362, 588, 2199, 18894, 5245, 295, 552, 13, 50764, 50764, 400, 370, 341, 307, 257, 588, 2199, 18894, 2316, 295, 257, 34090, 13, 509, 362, 512, 15743, 11, 10298, 11, 293, 550, 291, 362, 613, 5451, 2382, 279, 300, 362, 17443, 322, 552, 13, 51214, 51214, 407, 264, 343, 311, 366, 17443, 13, 400, 550, 264, 5451, 11145, 43582, 365, 264, 4846, 281, 341, 34090, 17596, 19020, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07537649095672921, "compression_ratio": 1.8237885462555066, "no_speech_prob": 2.3920825697132386e-05}, {"id": 610, "seek": 319800, "start": 3206.0, "end": 3215.0, "text": " And so this is a very simple mathematical model of a neuron. You have some inputs, axis, and then you have these synapses that have weights on them.", "tokens": [50364, 400, 613, 22027, 366, 4498, 4582, 281, 1184, 661, 13, 50464, 50464, 823, 11, 3228, 17157, 11, 22027, 366, 588, 6179, 5759, 11, 457, 321, 362, 588, 2199, 18894, 5245, 295, 552, 13, 50764, 50764, 400, 370, 341, 307, 257, 588, 2199, 18894, 2316, 295, 257, 34090, 13, 509, 362, 512, 15743, 11, 10298, 11, 293, 550, 291, 362, 613, 5451, 2382, 279, 300, 362, 17443, 322, 552, 13, 51214, 51214, 407, 264, 343, 311, 366, 17443, 13, 400, 550, 264, 5451, 11145, 43582, 365, 264, 4846, 281, 341, 34090, 17596, 19020, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07537649095672921, "compression_ratio": 1.8237885462555066, "no_speech_prob": 2.3920825697132386e-05}, {"id": 611, "seek": 319800, "start": 3215.0, "end": 3224.0, "text": " So the W's are weights. And then the synapse interacts with the input to this neuron multiplicatively.", "tokens": [50364, 400, 613, 22027, 366, 4498, 4582, 281, 1184, 661, 13, 50464, 50464, 823, 11, 3228, 17157, 11, 22027, 366, 588, 6179, 5759, 11, 457, 321, 362, 588, 2199, 18894, 5245, 295, 552, 13, 50764, 50764, 400, 370, 341, 307, 257, 588, 2199, 18894, 2316, 295, 257, 34090, 13, 509, 362, 512, 15743, 11, 10298, 11, 293, 550, 291, 362, 613, 5451, 2382, 279, 300, 362, 17443, 322, 552, 13, 51214, 51214, 407, 264, 343, 311, 366, 17443, 13, 400, 550, 264, 5451, 11145, 43582, 365, 264, 4846, 281, 341, 34090, 17596, 19020, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07537649095672921, "compression_ratio": 1.8237885462555066, "no_speech_prob": 2.3920825697132386e-05}, {"id": 612, "seek": 322400, "start": 3224.0, "end": 3231.0, "text": " So what flows to the cell body of this neuron is W times X. But there's multiple inputs.", "tokens": [50364, 407, 437, 12867, 281, 264, 2815, 1772, 295, 341, 34090, 307, 343, 1413, 1783, 13, 583, 456, 311, 3866, 15743, 13, 50714, 50714, 407, 456, 311, 867, 343, 1413, 1783, 311, 13974, 281, 264, 2815, 1772, 13, 440, 2815, 1772, 550, 575, 611, 411, 512, 12577, 13, 51064, 51064, 407, 341, 307, 733, 295, 411, 264, 7284, 41766, 1333, 295, 7875, 8324, 295, 341, 34090, 13, 51314, 51314, 407, 341, 12577, 393, 652, 309, 257, 857, 544, 7875, 2055, 420, 257, 857, 1570, 7875, 2055, 11, 10060, 295, 264, 4846, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09204328436600534, "compression_ratio": 1.8838383838383839, "no_speech_prob": 4.9855934776132926e-05}, {"id": 613, "seek": 322400, "start": 3231.0, "end": 3238.0, "text": " So there's many W times X's flowing to the cell body. The cell body then has also like some bias.", "tokens": [50364, 407, 437, 12867, 281, 264, 2815, 1772, 295, 341, 34090, 307, 343, 1413, 1783, 13, 583, 456, 311, 3866, 15743, 13, 50714, 50714, 407, 456, 311, 867, 343, 1413, 1783, 311, 13974, 281, 264, 2815, 1772, 13, 440, 2815, 1772, 550, 575, 611, 411, 512, 12577, 13, 51064, 51064, 407, 341, 307, 733, 295, 411, 264, 7284, 41766, 1333, 295, 7875, 8324, 295, 341, 34090, 13, 51314, 51314, 407, 341, 12577, 393, 652, 309, 257, 857, 544, 7875, 2055, 420, 257, 857, 1570, 7875, 2055, 11, 10060, 295, 264, 4846, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09204328436600534, "compression_ratio": 1.8838383838383839, "no_speech_prob": 4.9855934776132926e-05}, {"id": 614, "seek": 322400, "start": 3238.0, "end": 3243.0, "text": " So this is kind of like the inner innate sort of trigger happiness of this neuron.", "tokens": [50364, 407, 437, 12867, 281, 264, 2815, 1772, 295, 341, 34090, 307, 343, 1413, 1783, 13, 583, 456, 311, 3866, 15743, 13, 50714, 50714, 407, 456, 311, 867, 343, 1413, 1783, 311, 13974, 281, 264, 2815, 1772, 13, 440, 2815, 1772, 550, 575, 611, 411, 512, 12577, 13, 51064, 51064, 407, 341, 307, 733, 295, 411, 264, 7284, 41766, 1333, 295, 7875, 8324, 295, 341, 34090, 13, 51314, 51314, 407, 341, 12577, 393, 652, 309, 257, 857, 544, 7875, 2055, 420, 257, 857, 1570, 7875, 2055, 11, 10060, 295, 264, 4846, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09204328436600534, "compression_ratio": 1.8838383838383839, "no_speech_prob": 4.9855934776132926e-05}, {"id": 615, "seek": 322400, "start": 3243.0, "end": 3248.0, "text": " So this bias can make it a bit more trigger happy or a bit less trigger happy, regardless of the input.", "tokens": [50364, 407, 437, 12867, 281, 264, 2815, 1772, 295, 341, 34090, 307, 343, 1413, 1783, 13, 583, 456, 311, 3866, 15743, 13, 50714, 50714, 407, 456, 311, 867, 343, 1413, 1783, 311, 13974, 281, 264, 2815, 1772, 13, 440, 2815, 1772, 550, 575, 611, 411, 512, 12577, 13, 51064, 51064, 407, 341, 307, 733, 295, 411, 264, 7284, 41766, 1333, 295, 7875, 8324, 295, 341, 34090, 13, 51314, 51314, 407, 341, 12577, 393, 652, 309, 257, 857, 544, 7875, 2055, 420, 257, 857, 1570, 7875, 2055, 11, 10060, 295, 264, 4846, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09204328436600534, "compression_ratio": 1.8838383838383839, "no_speech_prob": 4.9855934776132926e-05}, {"id": 616, "seek": 324800, "start": 3248.0, "end": 3257.0, "text": " But basically, we're taking all the W times X of all the inputs, adding the bias, and then we take it through an activation function.", "tokens": [50364, 583, 1936, 11, 321, 434, 1940, 439, 264, 343, 1413, 1783, 295, 439, 264, 15743, 11, 5127, 264, 12577, 11, 293, 550, 321, 747, 309, 807, 364, 24433, 2445, 13, 50814, 50814, 400, 341, 24433, 2445, 307, 2673, 512, 733, 295, 257, 2339, 11077, 2445, 411, 257, 4556, 3280, 327, 420, 1266, 39, 420, 746, 411, 300, 13, 51114, 51114, 407, 382, 364, 1365, 11, 321, 434, 516, 281, 764, 264, 1266, 39, 294, 341, 1365, 13, 51364, 51364, 22592, 47, 88, 575, 257, 38611, 5893, 1266, 39, 13, 407, 321, 393, 818, 309, 322, 257, 3613, 293, 321, 393, 7542, 309, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08568881382451993, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.0451258276589215e-05}, {"id": 617, "seek": 324800, "start": 3257.0, "end": 3263.0, "text": " And this activation function is usually some kind of a squashing function like a sigmoid or 10H or something like that.", "tokens": [50364, 583, 1936, 11, 321, 434, 1940, 439, 264, 343, 1413, 1783, 295, 439, 264, 15743, 11, 5127, 264, 12577, 11, 293, 550, 321, 747, 309, 807, 364, 24433, 2445, 13, 50814, 50814, 400, 341, 24433, 2445, 307, 2673, 512, 733, 295, 257, 2339, 11077, 2445, 411, 257, 4556, 3280, 327, 420, 1266, 39, 420, 746, 411, 300, 13, 51114, 51114, 407, 382, 364, 1365, 11, 321, 434, 516, 281, 764, 264, 1266, 39, 294, 341, 1365, 13, 51364, 51364, 22592, 47, 88, 575, 257, 38611, 5893, 1266, 39, 13, 407, 321, 393, 818, 309, 322, 257, 3613, 293, 321, 393, 7542, 309, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08568881382451993, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.0451258276589215e-05}, {"id": 618, "seek": 324800, "start": 3263.0, "end": 3268.0, "text": " So as an example, we're going to use the 10H in this example.", "tokens": [50364, 583, 1936, 11, 321, 434, 1940, 439, 264, 343, 1413, 1783, 295, 439, 264, 15743, 11, 5127, 264, 12577, 11, 293, 550, 321, 747, 309, 807, 364, 24433, 2445, 13, 50814, 50814, 400, 341, 24433, 2445, 307, 2673, 512, 733, 295, 257, 2339, 11077, 2445, 411, 257, 4556, 3280, 327, 420, 1266, 39, 420, 746, 411, 300, 13, 51114, 51114, 407, 382, 364, 1365, 11, 321, 434, 516, 281, 764, 264, 1266, 39, 294, 341, 1365, 13, 51364, 51364, 22592, 47, 88, 575, 257, 38611, 5893, 1266, 39, 13, 407, 321, 393, 818, 309, 322, 257, 3613, 293, 321, 393, 7542, 309, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08568881382451993, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.0451258276589215e-05}, {"id": 619, "seek": 324800, "start": 3268.0, "end": 3276.0, "text": " NumPy has a NP dot 10H. So we can call it on a range and we can plot it.", "tokens": [50364, 583, 1936, 11, 321, 434, 1940, 439, 264, 343, 1413, 1783, 295, 439, 264, 15743, 11, 5127, 264, 12577, 11, 293, 550, 321, 747, 309, 807, 364, 24433, 2445, 13, 50814, 50814, 400, 341, 24433, 2445, 307, 2673, 512, 733, 295, 257, 2339, 11077, 2445, 411, 257, 4556, 3280, 327, 420, 1266, 39, 420, 746, 411, 300, 13, 51114, 51114, 407, 382, 364, 1365, 11, 321, 434, 516, 281, 764, 264, 1266, 39, 294, 341, 1365, 13, 51364, 51364, 22592, 47, 88, 575, 257, 38611, 5893, 1266, 39, 13, 407, 321, 393, 818, 309, 322, 257, 3613, 293, 321, 393, 7542, 309, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08568881382451993, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.0451258276589215e-05}, {"id": 620, "seek": 327600, "start": 3276.0, "end": 3284.0, "text": " This is the 10H function. And you see that the inputs as they come in get squashed on the white coordinate here.", "tokens": [50364, 639, 307, 264, 1266, 39, 2445, 13, 400, 291, 536, 300, 264, 15743, 382, 436, 808, 294, 483, 2339, 12219, 322, 264, 2418, 15670, 510, 13, 50764, 50764, 407, 558, 412, 4018, 11, 321, 434, 516, 281, 483, 2293, 4018, 13, 400, 550, 382, 291, 352, 544, 3353, 294, 264, 4846, 11, 51114, 51114, 550, 291, 603, 536, 300, 264, 2445, 486, 787, 352, 493, 281, 472, 293, 550, 39885, 484, 13, 51364, 51364, 400, 370, 498, 291, 1320, 294, 588, 3353, 15743, 11, 321, 434, 516, 281, 1410, 309, 19565, 412, 472, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07557790133417869, "compression_ratio": 1.7064220183486238, "no_speech_prob": 2.4299428332597017e-05}, {"id": 621, "seek": 327600, "start": 3284.0, "end": 3291.0, "text": " So right at zero, we're going to get exactly zero. And then as you go more positive in the input,", "tokens": [50364, 639, 307, 264, 1266, 39, 2445, 13, 400, 291, 536, 300, 264, 15743, 382, 436, 808, 294, 483, 2339, 12219, 322, 264, 2418, 15670, 510, 13, 50764, 50764, 407, 558, 412, 4018, 11, 321, 434, 516, 281, 483, 2293, 4018, 13, 400, 550, 382, 291, 352, 544, 3353, 294, 264, 4846, 11, 51114, 51114, 550, 291, 603, 536, 300, 264, 2445, 486, 787, 352, 493, 281, 472, 293, 550, 39885, 484, 13, 51364, 51364, 400, 370, 498, 291, 1320, 294, 588, 3353, 15743, 11, 321, 434, 516, 281, 1410, 309, 19565, 412, 472, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07557790133417869, "compression_ratio": 1.7064220183486238, "no_speech_prob": 2.4299428332597017e-05}, {"id": 622, "seek": 327600, "start": 3291.0, "end": 3296.0, "text": " then you'll see that the function will only go up to one and then plateau out.", "tokens": [50364, 639, 307, 264, 1266, 39, 2445, 13, 400, 291, 536, 300, 264, 15743, 382, 436, 808, 294, 483, 2339, 12219, 322, 264, 2418, 15670, 510, 13, 50764, 50764, 407, 558, 412, 4018, 11, 321, 434, 516, 281, 483, 2293, 4018, 13, 400, 550, 382, 291, 352, 544, 3353, 294, 264, 4846, 11, 51114, 51114, 550, 291, 603, 536, 300, 264, 2445, 486, 787, 352, 493, 281, 472, 293, 550, 39885, 484, 13, 51364, 51364, 400, 370, 498, 291, 1320, 294, 588, 3353, 15743, 11, 321, 434, 516, 281, 1410, 309, 19565, 412, 472, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07557790133417869, "compression_ratio": 1.7064220183486238, "no_speech_prob": 2.4299428332597017e-05}, {"id": 623, "seek": 327600, "start": 3296.0, "end": 3301.0, "text": " And so if you pass in very positive inputs, we're going to cap it smoothly at one.", "tokens": [50364, 639, 307, 264, 1266, 39, 2445, 13, 400, 291, 536, 300, 264, 15743, 382, 436, 808, 294, 483, 2339, 12219, 322, 264, 2418, 15670, 510, 13, 50764, 50764, 407, 558, 412, 4018, 11, 321, 434, 516, 281, 483, 2293, 4018, 13, 400, 550, 382, 291, 352, 544, 3353, 294, 264, 4846, 11, 51114, 51114, 550, 291, 603, 536, 300, 264, 2445, 486, 787, 352, 493, 281, 472, 293, 550, 39885, 484, 13, 51364, 51364, 400, 370, 498, 291, 1320, 294, 588, 3353, 15743, 11, 321, 434, 516, 281, 1410, 309, 19565, 412, 472, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07557790133417869, "compression_ratio": 1.7064220183486238, "no_speech_prob": 2.4299428332597017e-05}, {"id": 624, "seek": 330100, "start": 3301.0, "end": 3306.0, "text": " And on the negative side, we're going to cap it smoothly to negative one. So that's 10H.", "tokens": [50364, 400, 322, 264, 3671, 1252, 11, 321, 434, 516, 281, 1410, 309, 19565, 281, 3671, 472, 13, 407, 300, 311, 1266, 39, 13, 50614, 50614, 400, 300, 311, 264, 2339, 11077, 2445, 420, 364, 24433, 2445, 13, 50814, 50814, 400, 437, 1487, 484, 295, 341, 34090, 307, 445, 264, 24433, 2445, 6456, 281, 264, 5893, 1674, 295, 264, 17443, 293, 264, 15743, 13, 51214, 51214, 407, 718, 311, 2464, 472, 484, 13, 286, 478, 516, 281, 5055, 9163, 570, 286, 500, 380, 528, 281, 2010, 886, 709, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.03361031283502993, "compression_ratio": 1.643835616438356, "no_speech_prob": 3.169153933413327e-05}, {"id": 625, "seek": 330100, "start": 3306.0, "end": 3310.0, "text": " And that's the squashing function or an activation function.", "tokens": [50364, 400, 322, 264, 3671, 1252, 11, 321, 434, 516, 281, 1410, 309, 19565, 281, 3671, 472, 13, 407, 300, 311, 1266, 39, 13, 50614, 50614, 400, 300, 311, 264, 2339, 11077, 2445, 420, 364, 24433, 2445, 13, 50814, 50814, 400, 437, 1487, 484, 295, 341, 34090, 307, 445, 264, 24433, 2445, 6456, 281, 264, 5893, 1674, 295, 264, 17443, 293, 264, 15743, 13, 51214, 51214, 407, 718, 311, 2464, 472, 484, 13, 286, 478, 516, 281, 5055, 9163, 570, 286, 500, 380, 528, 281, 2010, 886, 709, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.03361031283502993, "compression_ratio": 1.643835616438356, "no_speech_prob": 3.169153933413327e-05}, {"id": 626, "seek": 330100, "start": 3310.0, "end": 3318.0, "text": " And what comes out of this neuron is just the activation function applied to the dot product of the weights and the inputs.", "tokens": [50364, 400, 322, 264, 3671, 1252, 11, 321, 434, 516, 281, 1410, 309, 19565, 281, 3671, 472, 13, 407, 300, 311, 1266, 39, 13, 50614, 50614, 400, 300, 311, 264, 2339, 11077, 2445, 420, 364, 24433, 2445, 13, 50814, 50814, 400, 437, 1487, 484, 295, 341, 34090, 307, 445, 264, 24433, 2445, 6456, 281, 264, 5893, 1674, 295, 264, 17443, 293, 264, 15743, 13, 51214, 51214, 407, 718, 311, 2464, 472, 484, 13, 286, 478, 516, 281, 5055, 9163, 570, 286, 500, 380, 528, 281, 2010, 886, 709, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.03361031283502993, "compression_ratio": 1.643835616438356, "no_speech_prob": 3.169153933413327e-05}, {"id": 627, "seek": 330100, "start": 3318.0, "end": 3329.0, "text": " So let's write one out. I'm going to copy paste because I don't want to type too much.", "tokens": [50364, 400, 322, 264, 3671, 1252, 11, 321, 434, 516, 281, 1410, 309, 19565, 281, 3671, 472, 13, 407, 300, 311, 1266, 39, 13, 50614, 50614, 400, 300, 311, 264, 2339, 11077, 2445, 420, 364, 24433, 2445, 13, 50814, 50814, 400, 437, 1487, 484, 295, 341, 34090, 307, 445, 264, 24433, 2445, 6456, 281, 264, 5893, 1674, 295, 264, 17443, 293, 264, 15743, 13, 51214, 51214, 407, 718, 311, 2464, 472, 484, 13, 286, 478, 516, 281, 5055, 9163, 570, 286, 500, 380, 528, 281, 2010, 886, 709, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.03361031283502993, "compression_ratio": 1.643835616438356, "no_speech_prob": 3.169153933413327e-05}, {"id": 628, "seek": 332900, "start": 3329.0, "end": 3334.0, "text": " But OK, so here we have the inputs X1, X2. So this is a two dimensional neuron.", "tokens": [50364, 583, 2264, 11, 370, 510, 321, 362, 264, 15743, 1783, 16, 11, 1783, 17, 13, 407, 341, 307, 257, 732, 18795, 34090, 13, 50614, 50614, 407, 732, 15743, 366, 516, 281, 808, 294, 13, 1981, 366, 1194, 295, 382, 264, 17443, 295, 341, 34090, 11, 17443, 343, 16, 11, 343, 17, 13, 50964, 50964, 400, 613, 17443, 11, 797, 11, 366, 264, 5451, 2796, 299, 13985, 337, 1184, 4846, 13, 400, 341, 307, 264, 12577, 295, 264, 34090, 363, 13, 51364, 51364, 400, 586, 437, 321, 528, 281, 360, 307, 11, 4650, 281, 341, 2316, 11, 321, 643, 281, 12972, 1783, 16, 1413, 343, 16, 293, 1783, 17, 1413, 343, 17, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10419830782660122, "compression_ratio": 1.6866952789699572, "no_speech_prob": 5.144024544279091e-05}, {"id": 629, "seek": 332900, "start": 3334.0, "end": 3341.0, "text": " So two inputs are going to come in. These are thought of as the weights of this neuron, weights W1, W2.", "tokens": [50364, 583, 2264, 11, 370, 510, 321, 362, 264, 15743, 1783, 16, 11, 1783, 17, 13, 407, 341, 307, 257, 732, 18795, 34090, 13, 50614, 50614, 407, 732, 15743, 366, 516, 281, 808, 294, 13, 1981, 366, 1194, 295, 382, 264, 17443, 295, 341, 34090, 11, 17443, 343, 16, 11, 343, 17, 13, 50964, 50964, 400, 613, 17443, 11, 797, 11, 366, 264, 5451, 2796, 299, 13985, 337, 1184, 4846, 13, 400, 341, 307, 264, 12577, 295, 264, 34090, 363, 13, 51364, 51364, 400, 586, 437, 321, 528, 281, 360, 307, 11, 4650, 281, 341, 2316, 11, 321, 643, 281, 12972, 1783, 16, 1413, 343, 16, 293, 1783, 17, 1413, 343, 17, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10419830782660122, "compression_ratio": 1.6866952789699572, "no_speech_prob": 5.144024544279091e-05}, {"id": 630, "seek": 332900, "start": 3341.0, "end": 3349.0, "text": " And these weights, again, are the synaptic strings for each input. And this is the bias of the neuron B.", "tokens": [50364, 583, 2264, 11, 370, 510, 321, 362, 264, 15743, 1783, 16, 11, 1783, 17, 13, 407, 341, 307, 257, 732, 18795, 34090, 13, 50614, 50614, 407, 732, 15743, 366, 516, 281, 808, 294, 13, 1981, 366, 1194, 295, 382, 264, 17443, 295, 341, 34090, 11, 17443, 343, 16, 11, 343, 17, 13, 50964, 50964, 400, 613, 17443, 11, 797, 11, 366, 264, 5451, 2796, 299, 13985, 337, 1184, 4846, 13, 400, 341, 307, 264, 12577, 295, 264, 34090, 363, 13, 51364, 51364, 400, 586, 437, 321, 528, 281, 360, 307, 11, 4650, 281, 341, 2316, 11, 321, 643, 281, 12972, 1783, 16, 1413, 343, 16, 293, 1783, 17, 1413, 343, 17, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10419830782660122, "compression_ratio": 1.6866952789699572, "no_speech_prob": 5.144024544279091e-05}, {"id": 631, "seek": 332900, "start": 3349.0, "end": 3358.0, "text": " And now what we want to do is, according to this model, we need to multiply X1 times W1 and X2 times W2.", "tokens": [50364, 583, 2264, 11, 370, 510, 321, 362, 264, 15743, 1783, 16, 11, 1783, 17, 13, 407, 341, 307, 257, 732, 18795, 34090, 13, 50614, 50614, 407, 732, 15743, 366, 516, 281, 808, 294, 13, 1981, 366, 1194, 295, 382, 264, 17443, 295, 341, 34090, 11, 17443, 343, 16, 11, 343, 17, 13, 50964, 50964, 400, 613, 17443, 11, 797, 11, 366, 264, 5451, 2796, 299, 13985, 337, 1184, 4846, 13, 400, 341, 307, 264, 12577, 295, 264, 34090, 363, 13, 51364, 51364, 400, 586, 437, 321, 528, 281, 360, 307, 11, 4650, 281, 341, 2316, 11, 321, 643, 281, 12972, 1783, 16, 1413, 343, 16, 293, 1783, 17, 1413, 343, 17, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10419830782660122, "compression_ratio": 1.6866952789699572, "no_speech_prob": 5.144024544279091e-05}, {"id": 632, "seek": 335800, "start": 3358.0, "end": 3368.0, "text": " And then we need to add bias on top of it. And it gets a little messy here, but all we are trying to do is X1 W1 plus X2 W2 plus B.", "tokens": [50364, 400, 550, 321, 643, 281, 909, 12577, 322, 1192, 295, 309, 13, 400, 309, 2170, 257, 707, 16191, 510, 11, 457, 439, 321, 366, 1382, 281, 360, 307, 1783, 16, 343, 16, 1804, 1783, 17, 343, 17, 1804, 363, 13, 50864, 50864, 400, 613, 366, 17207, 510, 11, 3993, 286, 478, 884, 309, 294, 1359, 4439, 370, 300, 321, 767, 362, 44548, 281, 439, 613, 19376, 13891, 13, 51214, 51214, 407, 321, 362, 1783, 16, 343, 16, 7006, 11, 1783, 1413, 1783, 17, 343, 17, 7006, 11, 293, 286, 478, 611, 40244, 552, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07427563472669951, "compression_ratio": 1.587962962962963, "no_speech_prob": 6.302387919276953e-05}, {"id": 633, "seek": 335800, "start": 3368.0, "end": 3375.0, "text": " And these are multiplied here, except I'm doing it in small steps so that we actually have pointers to all these intermediate nodes.", "tokens": [50364, 400, 550, 321, 643, 281, 909, 12577, 322, 1192, 295, 309, 13, 400, 309, 2170, 257, 707, 16191, 510, 11, 457, 439, 321, 366, 1382, 281, 360, 307, 1783, 16, 343, 16, 1804, 1783, 17, 343, 17, 1804, 363, 13, 50864, 50864, 400, 613, 366, 17207, 510, 11, 3993, 286, 478, 884, 309, 294, 1359, 4439, 370, 300, 321, 767, 362, 44548, 281, 439, 613, 19376, 13891, 13, 51214, 51214, 407, 321, 362, 1783, 16, 343, 16, 7006, 11, 1783, 1413, 1783, 17, 343, 17, 7006, 11, 293, 286, 478, 611, 40244, 552, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07427563472669951, "compression_ratio": 1.587962962962963, "no_speech_prob": 6.302387919276953e-05}, {"id": 634, "seek": 335800, "start": 3375.0, "end": 3382.0, "text": " So we have X1 W1 variable, X times X2 W2 variable, and I'm also labeling them.", "tokens": [50364, 400, 550, 321, 643, 281, 909, 12577, 322, 1192, 295, 309, 13, 400, 309, 2170, 257, 707, 16191, 510, 11, 457, 439, 321, 366, 1382, 281, 360, 307, 1783, 16, 343, 16, 1804, 1783, 17, 343, 17, 1804, 363, 13, 50864, 50864, 400, 613, 366, 17207, 510, 11, 3993, 286, 478, 884, 309, 294, 1359, 4439, 370, 300, 321, 767, 362, 44548, 281, 439, 613, 19376, 13891, 13, 51214, 51214, 407, 321, 362, 1783, 16, 343, 16, 7006, 11, 1783, 1413, 1783, 17, 343, 17, 7006, 11, 293, 286, 478, 611, 40244, 552, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07427563472669951, "compression_ratio": 1.587962962962963, "no_speech_prob": 6.302387919276953e-05}, {"id": 635, "seek": 338200, "start": 3382.0, "end": 3391.0, "text": " So N is now the cell body raw activation without the activation function for now.", "tokens": [50364, 407, 426, 307, 586, 264, 2815, 1772, 8936, 24433, 1553, 264, 24433, 2445, 337, 586, 13, 50814, 50814, 400, 341, 820, 312, 1547, 281, 1936, 7542, 309, 13, 407, 2642, 257, 5893, 295, 426, 2709, 505, 1783, 16, 1413, 343, 16, 11, 1783, 17, 1413, 343, 17, 885, 3869, 13, 51414, 51414, 1396, 264, 12577, 2170, 3869, 322, 1192, 295, 341, 13, 400, 341, 426, 307, 341, 2408, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11098540319155341, "compression_ratio": 1.5116279069767442, "no_speech_prob": 3.647506673587486e-05}, {"id": 636, "seek": 338200, "start": 3391.0, "end": 3403.0, "text": " And this should be enough to basically plot it. So draw a dot of N gives us X1 times W1, X2 times W2 being added.", "tokens": [50364, 407, 426, 307, 586, 264, 2815, 1772, 8936, 24433, 1553, 264, 24433, 2445, 337, 586, 13, 50814, 50814, 400, 341, 820, 312, 1547, 281, 1936, 7542, 309, 13, 407, 2642, 257, 5893, 295, 426, 2709, 505, 1783, 16, 1413, 343, 16, 11, 1783, 17, 1413, 343, 17, 885, 3869, 13, 51414, 51414, 1396, 264, 12577, 2170, 3869, 322, 1192, 295, 341, 13, 400, 341, 426, 307, 341, 2408, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11098540319155341, "compression_ratio": 1.5116279069767442, "no_speech_prob": 3.647506673587486e-05}, {"id": 637, "seek": 338200, "start": 3403.0, "end": 3409.0, "text": " Then the bias gets added on top of this. And this N is this sum.", "tokens": [50364, 407, 426, 307, 586, 264, 2815, 1772, 8936, 24433, 1553, 264, 24433, 2445, 337, 586, 13, 50814, 50814, 400, 341, 820, 312, 1547, 281, 1936, 7542, 309, 13, 407, 2642, 257, 5893, 295, 426, 2709, 505, 1783, 16, 1413, 343, 16, 11, 1783, 17, 1413, 343, 17, 885, 3869, 13, 51414, 51414, 1396, 264, 12577, 2170, 3869, 322, 1192, 295, 341, 13, 400, 341, 426, 307, 341, 2408, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11098540319155341, "compression_ratio": 1.5116279069767442, "no_speech_prob": 3.647506673587486e-05}, {"id": 638, "seek": 340900, "start": 3409.0, "end": 3416.0, "text": " So we're now going to take it through an activation function. And let's say we use the tanh so that we produce the output.", "tokens": [50364, 407, 321, 434, 586, 516, 281, 747, 309, 807, 364, 24433, 2445, 13, 400, 718, 311, 584, 321, 764, 264, 7603, 71, 370, 300, 321, 5258, 264, 5598, 13, 50714, 50714, 407, 437, 321, 1116, 411, 281, 360, 510, 307, 321, 1116, 411, 281, 360, 264, 5598, 293, 286, 603, 818, 309, 422, 307, 426, 5893, 7603, 71, 13, 51164, 51164, 2264, 11, 457, 321, 2378, 380, 1939, 3720, 264, 7603, 71, 13, 51314, 51314, 823, 11, 264, 1778, 300, 321, 643, 281, 4445, 1071, 7603, 71, 2445, 510, 307, 300, 7603, 71, 307, 257, 9848, 65, 7940, 2445, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08054800216968243, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.1781977920909412e-05}, {"id": 639, "seek": 340900, "start": 3416.0, "end": 3425.0, "text": " So what we'd like to do here is we'd like to do the output and I'll call it O is N dot tanh.", "tokens": [50364, 407, 321, 434, 586, 516, 281, 747, 309, 807, 364, 24433, 2445, 13, 400, 718, 311, 584, 321, 764, 264, 7603, 71, 370, 300, 321, 5258, 264, 5598, 13, 50714, 50714, 407, 437, 321, 1116, 411, 281, 360, 510, 307, 321, 1116, 411, 281, 360, 264, 5598, 293, 286, 603, 818, 309, 422, 307, 426, 5893, 7603, 71, 13, 51164, 51164, 2264, 11, 457, 321, 2378, 380, 1939, 3720, 264, 7603, 71, 13, 51314, 51314, 823, 11, 264, 1778, 300, 321, 643, 281, 4445, 1071, 7603, 71, 2445, 510, 307, 300, 7603, 71, 307, 257, 9848, 65, 7940, 2445, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08054800216968243, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.1781977920909412e-05}, {"id": 640, "seek": 340900, "start": 3425.0, "end": 3428.0, "text": " OK, but we haven't yet written the tanh.", "tokens": [50364, 407, 321, 434, 586, 516, 281, 747, 309, 807, 364, 24433, 2445, 13, 400, 718, 311, 584, 321, 764, 264, 7603, 71, 370, 300, 321, 5258, 264, 5598, 13, 50714, 50714, 407, 437, 321, 1116, 411, 281, 360, 510, 307, 321, 1116, 411, 281, 360, 264, 5598, 293, 286, 603, 818, 309, 422, 307, 426, 5893, 7603, 71, 13, 51164, 51164, 2264, 11, 457, 321, 2378, 380, 1939, 3720, 264, 7603, 71, 13, 51314, 51314, 823, 11, 264, 1778, 300, 321, 643, 281, 4445, 1071, 7603, 71, 2445, 510, 307, 300, 7603, 71, 307, 257, 9848, 65, 7940, 2445, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08054800216968243, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.1781977920909412e-05}, {"id": 641, "seek": 340900, "start": 3428.0, "end": 3436.0, "text": " Now, the reason that we need to implement another tanh function here is that tanh is a hyperbolic function.", "tokens": [50364, 407, 321, 434, 586, 516, 281, 747, 309, 807, 364, 24433, 2445, 13, 400, 718, 311, 584, 321, 764, 264, 7603, 71, 370, 300, 321, 5258, 264, 5598, 13, 50714, 50714, 407, 437, 321, 1116, 411, 281, 360, 510, 307, 321, 1116, 411, 281, 360, 264, 5598, 293, 286, 603, 818, 309, 422, 307, 426, 5893, 7603, 71, 13, 51164, 51164, 2264, 11, 457, 321, 2378, 380, 1939, 3720, 264, 7603, 71, 13, 51314, 51314, 823, 11, 264, 1778, 300, 321, 643, 281, 4445, 1071, 7603, 71, 2445, 510, 307, 300, 7603, 71, 307, 257, 9848, 65, 7940, 2445, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08054800216968243, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.1781977920909412e-05}, {"id": 642, "seek": 343600, "start": 3436.0, "end": 3442.0, "text": " And we've only so far implemented a plus and a times and you can't make a tanh out of just pluses and times.", "tokens": [50364, 400, 321, 600, 787, 370, 1400, 12270, 257, 1804, 293, 257, 1413, 293, 291, 393, 380, 652, 257, 7603, 71, 484, 295, 445, 1804, 279, 293, 1413, 13, 50664, 50664, 509, 611, 643, 37871, 6642, 13, 407, 7603, 71, 307, 341, 733, 295, 257, 8513, 510, 13, 50914, 50914, 509, 393, 764, 2139, 472, 295, 613, 13, 400, 291, 536, 300, 456, 307, 37871, 6642, 3288, 11, 597, 321, 362, 406, 12270, 1939, 337, 527, 2295, 2158, 9984, 510, 13, 51264, 51264, 407, 321, 434, 406, 516, 281, 312, 1075, 281, 5258, 7603, 71, 1939, 13, 400, 321, 362, 281, 352, 646, 493, 293, 4445, 746, 411, 309, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0613274700873721, "compression_ratio": 1.7469879518072289, "no_speech_prob": 1.2606540622073226e-05}, {"id": 643, "seek": 343600, "start": 3442.0, "end": 3447.0, "text": " You also need exponentiation. So tanh is this kind of a formula here.", "tokens": [50364, 400, 321, 600, 787, 370, 1400, 12270, 257, 1804, 293, 257, 1413, 293, 291, 393, 380, 652, 257, 7603, 71, 484, 295, 445, 1804, 279, 293, 1413, 13, 50664, 50664, 509, 611, 643, 37871, 6642, 13, 407, 7603, 71, 307, 341, 733, 295, 257, 8513, 510, 13, 50914, 50914, 509, 393, 764, 2139, 472, 295, 613, 13, 400, 291, 536, 300, 456, 307, 37871, 6642, 3288, 11, 597, 321, 362, 406, 12270, 1939, 337, 527, 2295, 2158, 9984, 510, 13, 51264, 51264, 407, 321, 434, 406, 516, 281, 312, 1075, 281, 5258, 7603, 71, 1939, 13, 400, 321, 362, 281, 352, 646, 493, 293, 4445, 746, 411, 309, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0613274700873721, "compression_ratio": 1.7469879518072289, "no_speech_prob": 1.2606540622073226e-05}, {"id": 644, "seek": 343600, "start": 3447.0, "end": 3454.0, "text": " You can use either one of these. And you see that there is exponentiation involved, which we have not implemented yet for our low value node here.", "tokens": [50364, 400, 321, 600, 787, 370, 1400, 12270, 257, 1804, 293, 257, 1413, 293, 291, 393, 380, 652, 257, 7603, 71, 484, 295, 445, 1804, 279, 293, 1413, 13, 50664, 50664, 509, 611, 643, 37871, 6642, 13, 407, 7603, 71, 307, 341, 733, 295, 257, 8513, 510, 13, 50914, 50914, 509, 393, 764, 2139, 472, 295, 613, 13, 400, 291, 536, 300, 456, 307, 37871, 6642, 3288, 11, 597, 321, 362, 406, 12270, 1939, 337, 527, 2295, 2158, 9984, 510, 13, 51264, 51264, 407, 321, 434, 406, 516, 281, 312, 1075, 281, 5258, 7603, 71, 1939, 13, 400, 321, 362, 281, 352, 646, 493, 293, 4445, 746, 411, 309, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0613274700873721, "compression_ratio": 1.7469879518072289, "no_speech_prob": 1.2606540622073226e-05}, {"id": 645, "seek": 343600, "start": 3454.0, "end": 3459.0, "text": " So we're not going to be able to produce tanh yet. And we have to go back up and implement something like it.", "tokens": [50364, 400, 321, 600, 787, 370, 1400, 12270, 257, 1804, 293, 257, 1413, 293, 291, 393, 380, 652, 257, 7603, 71, 484, 295, 445, 1804, 279, 293, 1413, 13, 50664, 50664, 509, 611, 643, 37871, 6642, 13, 407, 7603, 71, 307, 341, 733, 295, 257, 8513, 510, 13, 50914, 50914, 509, 393, 764, 2139, 472, 295, 613, 13, 400, 291, 536, 300, 456, 307, 37871, 6642, 3288, 11, 597, 321, 362, 406, 12270, 1939, 337, 527, 2295, 2158, 9984, 510, 13, 51264, 51264, 407, 321, 434, 406, 516, 281, 312, 1075, 281, 5258, 7603, 71, 1939, 13, 400, 321, 362, 281, 352, 646, 493, 293, 4445, 746, 411, 309, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0613274700873721, "compression_ratio": 1.7469879518072289, "no_speech_prob": 1.2606540622073226e-05}, {"id": 646, "seek": 345900, "start": 3459.0, "end": 3467.0, "text": " Now, one option here is we could actually implement exponentiation.", "tokens": [50364, 823, 11, 472, 3614, 510, 307, 321, 727, 767, 4445, 37871, 6642, 13, 50764, 50764, 400, 321, 727, 2736, 264, 1278, 295, 257, 2158, 2602, 295, 257, 7603, 71, 295, 257, 2158, 11, 570, 498, 321, 632, 1278, 11, 550, 321, 362, 1203, 1646, 300, 321, 643, 13, 51214, 51214, 407, 570, 321, 458, 577, 281, 909, 293, 321, 458, 577, 281, 321, 458, 577, 281, 909, 293, 321, 458, 577, 281, 12972, 13, 51564, 51564, 407, 321, 1116, 312, 1075, 281, 1884, 7603, 71, 498, 321, 2586, 577, 281, 1278, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09683280189832051, "compression_ratio": 1.8195876288659794, "no_speech_prob": 1.321122454100987e-05}, {"id": 647, "seek": 345900, "start": 3467.0, "end": 3476.0, "text": " And we could return the exp of a value instead of a tanh of a value, because if we had exp, then we have everything else that we need.", "tokens": [50364, 823, 11, 472, 3614, 510, 307, 321, 727, 767, 4445, 37871, 6642, 13, 50764, 50764, 400, 321, 727, 2736, 264, 1278, 295, 257, 2158, 2602, 295, 257, 7603, 71, 295, 257, 2158, 11, 570, 498, 321, 632, 1278, 11, 550, 321, 362, 1203, 1646, 300, 321, 643, 13, 51214, 51214, 407, 570, 321, 458, 577, 281, 909, 293, 321, 458, 577, 281, 321, 458, 577, 281, 909, 293, 321, 458, 577, 281, 12972, 13, 51564, 51564, 407, 321, 1116, 312, 1075, 281, 1884, 7603, 71, 498, 321, 2586, 577, 281, 1278, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09683280189832051, "compression_ratio": 1.8195876288659794, "no_speech_prob": 1.321122454100987e-05}, {"id": 648, "seek": 345900, "start": 3476.0, "end": 3483.0, "text": " So because we know how to add and we know how to we know how to add and we know how to multiply.", "tokens": [50364, 823, 11, 472, 3614, 510, 307, 321, 727, 767, 4445, 37871, 6642, 13, 50764, 50764, 400, 321, 727, 2736, 264, 1278, 295, 257, 2158, 2602, 295, 257, 7603, 71, 295, 257, 2158, 11, 570, 498, 321, 632, 1278, 11, 550, 321, 362, 1203, 1646, 300, 321, 643, 13, 51214, 51214, 407, 570, 321, 458, 577, 281, 909, 293, 321, 458, 577, 281, 321, 458, 577, 281, 909, 293, 321, 458, 577, 281, 12972, 13, 51564, 51564, 407, 321, 1116, 312, 1075, 281, 1884, 7603, 71, 498, 321, 2586, 577, 281, 1278, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09683280189832051, "compression_ratio": 1.8195876288659794, "no_speech_prob": 1.321122454100987e-05}, {"id": 649, "seek": 345900, "start": 3483.0, "end": 3487.0, "text": " So we'd be able to create tanh if we knew how to exp.", "tokens": [50364, 823, 11, 472, 3614, 510, 307, 321, 727, 767, 4445, 37871, 6642, 13, 50764, 50764, 400, 321, 727, 2736, 264, 1278, 295, 257, 2158, 2602, 295, 257, 7603, 71, 295, 257, 2158, 11, 570, 498, 321, 632, 1278, 11, 550, 321, 362, 1203, 1646, 300, 321, 643, 13, 51214, 51214, 407, 570, 321, 458, 577, 281, 909, 293, 321, 458, 577, 281, 321, 458, 577, 281, 909, 293, 321, 458, 577, 281, 12972, 13, 51564, 51564, 407, 321, 1116, 312, 1075, 281, 1884, 7603, 71, 498, 321, 2586, 577, 281, 1278, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09683280189832051, "compression_ratio": 1.8195876288659794, "no_speech_prob": 1.321122454100987e-05}, {"id": 650, "seek": 348700, "start": 3487.0, "end": 3498.0, "text": " But for the purposes of this example, I specifically wanted to show you that we don't necessarily need to have the most atomic pieces in in this value object.", "tokens": [50364, 583, 337, 264, 9932, 295, 341, 1365, 11, 286, 4682, 1415, 281, 855, 291, 300, 321, 500, 380, 4725, 643, 281, 362, 264, 881, 22275, 3755, 294, 294, 341, 2158, 2657, 13, 50914, 50914, 492, 393, 767, 411, 1884, 6828, 412, 23211, 2793, 295, 37765, 13, 51214, 51214, 814, 393, 312, 6179, 6828, 11, 457, 436, 393, 312, 611, 588, 11, 588, 2199, 6828, 411, 257, 1804, 13, 400, 309, 311, 3879, 493, 281, 505, 13, 51514, 51514, 440, 787, 551, 300, 7001, 307, 300, 321, 458, 577, 281, 23203, 807, 604, 472, 2445, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04059639844027432, "compression_ratio": 1.6741573033707866, "no_speech_prob": 1.2411059287842363e-05}, {"id": 651, "seek": 348700, "start": 3498.0, "end": 3504.0, "text": " We can actually like create functions at arbitrary points of abstraction.", "tokens": [50364, 583, 337, 264, 9932, 295, 341, 1365, 11, 286, 4682, 1415, 281, 855, 291, 300, 321, 500, 380, 4725, 643, 281, 362, 264, 881, 22275, 3755, 294, 294, 341, 2158, 2657, 13, 50914, 50914, 492, 393, 767, 411, 1884, 6828, 412, 23211, 2793, 295, 37765, 13, 51214, 51214, 814, 393, 312, 6179, 6828, 11, 457, 436, 393, 312, 611, 588, 11, 588, 2199, 6828, 411, 257, 1804, 13, 400, 309, 311, 3879, 493, 281, 505, 13, 51514, 51514, 440, 787, 551, 300, 7001, 307, 300, 321, 458, 577, 281, 23203, 807, 604, 472, 2445, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04059639844027432, "compression_ratio": 1.6741573033707866, "no_speech_prob": 1.2411059287842363e-05}, {"id": 652, "seek": 348700, "start": 3504.0, "end": 3510.0, "text": " They can be complicated functions, but they can be also very, very simple functions like a plus. And it's totally up to us.", "tokens": [50364, 583, 337, 264, 9932, 295, 341, 1365, 11, 286, 4682, 1415, 281, 855, 291, 300, 321, 500, 380, 4725, 643, 281, 362, 264, 881, 22275, 3755, 294, 294, 341, 2158, 2657, 13, 50914, 50914, 492, 393, 767, 411, 1884, 6828, 412, 23211, 2793, 295, 37765, 13, 51214, 51214, 814, 393, 312, 6179, 6828, 11, 457, 436, 393, 312, 611, 588, 11, 588, 2199, 6828, 411, 257, 1804, 13, 400, 309, 311, 3879, 493, 281, 505, 13, 51514, 51514, 440, 787, 551, 300, 7001, 307, 300, 321, 458, 577, 281, 23203, 807, 604, 472, 2445, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04059639844027432, "compression_ratio": 1.6741573033707866, "no_speech_prob": 1.2411059287842363e-05}, {"id": 653, "seek": 348700, "start": 3510.0, "end": 3514.0, "text": " The only thing that matters is that we know how to differentiate through any one function.", "tokens": [50364, 583, 337, 264, 9932, 295, 341, 1365, 11, 286, 4682, 1415, 281, 855, 291, 300, 321, 500, 380, 4725, 643, 281, 362, 264, 881, 22275, 3755, 294, 294, 341, 2158, 2657, 13, 50914, 50914, 492, 393, 767, 411, 1884, 6828, 412, 23211, 2793, 295, 37765, 13, 51214, 51214, 814, 393, 312, 6179, 6828, 11, 457, 436, 393, 312, 611, 588, 11, 588, 2199, 6828, 411, 257, 1804, 13, 400, 309, 311, 3879, 493, 281, 505, 13, 51514, 51514, 440, 787, 551, 300, 7001, 307, 300, 321, 458, 577, 281, 23203, 807, 604, 472, 2445, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04059639844027432, "compression_ratio": 1.6741573033707866, "no_speech_prob": 1.2411059287842363e-05}, {"id": 654, "seek": 351400, "start": 3514.0, "end": 3520.0, "text": " So we take some inputs and we make an output. The only thing that matters can be arbitrarily complex function.", "tokens": [50364, 407, 321, 747, 512, 15743, 293, 321, 652, 364, 5598, 13, 440, 787, 551, 300, 7001, 393, 312, 19071, 3289, 3997, 2445, 13, 50664, 50664, 1018, 938, 382, 291, 458, 577, 281, 1884, 264, 2654, 13760, 11, 498, 291, 458, 264, 2654, 13760, 295, 577, 264, 15743, 2712, 264, 5598, 11, 550, 300, 311, 439, 291, 643, 13, 51014, 51014, 407, 321, 434, 516, 281, 13630, 493, 439, 295, 341, 6114, 293, 321, 434, 406, 516, 281, 1821, 309, 760, 281, 1080, 22275, 3755, 13, 51314, 51314, 492, 434, 445, 516, 281, 3838, 4445, 7603, 71, 13, 407, 718, 311, 360, 300, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05914950593609676, "compression_ratio": 1.7301587301587302, "no_speech_prob": 2.3551456251880154e-05}, {"id": 655, "seek": 351400, "start": 3520.0, "end": 3527.0, "text": " As long as you know how to create the local derivative, if you know the local derivative of how the inputs impact the output, then that's all you need.", "tokens": [50364, 407, 321, 747, 512, 15743, 293, 321, 652, 364, 5598, 13, 440, 787, 551, 300, 7001, 393, 312, 19071, 3289, 3997, 2445, 13, 50664, 50664, 1018, 938, 382, 291, 458, 577, 281, 1884, 264, 2654, 13760, 11, 498, 291, 458, 264, 2654, 13760, 295, 577, 264, 15743, 2712, 264, 5598, 11, 550, 300, 311, 439, 291, 643, 13, 51014, 51014, 407, 321, 434, 516, 281, 13630, 493, 439, 295, 341, 6114, 293, 321, 434, 406, 516, 281, 1821, 309, 760, 281, 1080, 22275, 3755, 13, 51314, 51314, 492, 434, 445, 516, 281, 3838, 4445, 7603, 71, 13, 407, 718, 311, 360, 300, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05914950593609676, "compression_ratio": 1.7301587301587302, "no_speech_prob": 2.3551456251880154e-05}, {"id": 656, "seek": 351400, "start": 3527.0, "end": 3533.0, "text": " So we're going to cluster up all of this expression and we're not going to break it down to its atomic pieces.", "tokens": [50364, 407, 321, 747, 512, 15743, 293, 321, 652, 364, 5598, 13, 440, 787, 551, 300, 7001, 393, 312, 19071, 3289, 3997, 2445, 13, 50664, 50664, 1018, 938, 382, 291, 458, 577, 281, 1884, 264, 2654, 13760, 11, 498, 291, 458, 264, 2654, 13760, 295, 577, 264, 15743, 2712, 264, 5598, 11, 550, 300, 311, 439, 291, 643, 13, 51014, 51014, 407, 321, 434, 516, 281, 13630, 493, 439, 295, 341, 6114, 293, 321, 434, 406, 516, 281, 1821, 309, 760, 281, 1080, 22275, 3755, 13, 51314, 51314, 492, 434, 445, 516, 281, 3838, 4445, 7603, 71, 13, 407, 718, 311, 360, 300, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05914950593609676, "compression_ratio": 1.7301587301587302, "no_speech_prob": 2.3551456251880154e-05}, {"id": 657, "seek": 351400, "start": 3533.0, "end": 3537.0, "text": " We're just going to directly implement tanh. So let's do that.", "tokens": [50364, 407, 321, 747, 512, 15743, 293, 321, 652, 364, 5598, 13, 440, 787, 551, 300, 7001, 393, 312, 19071, 3289, 3997, 2445, 13, 50664, 50664, 1018, 938, 382, 291, 458, 577, 281, 1884, 264, 2654, 13760, 11, 498, 291, 458, 264, 2654, 13760, 295, 577, 264, 15743, 2712, 264, 5598, 11, 550, 300, 311, 439, 291, 643, 13, 51014, 51014, 407, 321, 434, 516, 281, 13630, 493, 439, 295, 341, 6114, 293, 321, 434, 406, 516, 281, 1821, 309, 760, 281, 1080, 22275, 3755, 13, 51314, 51314, 492, 434, 445, 516, 281, 3838, 4445, 7603, 71, 13, 407, 718, 311, 360, 300, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05914950593609676, "compression_ratio": 1.7301587301587302, "no_speech_prob": 2.3551456251880154e-05}, {"id": 658, "seek": 353700, "start": 3537.0, "end": 3545.0, "text": " Depth and h and then out will be a value of and we need this expression here.", "tokens": [50364, 4056, 392, 293, 276, 293, 550, 484, 486, 312, 257, 2158, 295, 293, 321, 643, 341, 6114, 510, 13, 50764, 50764, 407, 718, 385, 767, 5055, 9163, 13, 51214, 51214, 961, 311, 4444, 297, 11, 597, 307, 257, 5139, 1412, 13, 51364, 51364, 400, 550, 341, 11, 286, 1697, 11, 307, 264, 7603, 71, 5221, 5893, 1278, 295, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.2666740114726717, "compression_ratio": 1.3537414965986394, "no_speech_prob": 5.82797329116147e-05}, {"id": 659, "seek": 353700, "start": 3545.0, "end": 3554.0, "text": " So let me actually copy paste.", "tokens": [50364, 4056, 392, 293, 276, 293, 550, 484, 486, 312, 257, 2158, 295, 293, 321, 643, 341, 6114, 510, 13, 50764, 50764, 407, 718, 385, 767, 5055, 9163, 13, 51214, 51214, 961, 311, 4444, 297, 11, 597, 307, 257, 5139, 1412, 13, 51364, 51364, 400, 550, 341, 11, 286, 1697, 11, 307, 264, 7603, 71, 5221, 5893, 1278, 295, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.2666740114726717, "compression_ratio": 1.3537414965986394, "no_speech_prob": 5.82797329116147e-05}, {"id": 660, "seek": 353700, "start": 3554.0, "end": 3557.0, "text": " Let's grab n, which is a salt data.", "tokens": [50364, 4056, 392, 293, 276, 293, 550, 484, 486, 312, 257, 2158, 295, 293, 321, 643, 341, 6114, 510, 13, 50764, 50764, 407, 718, 385, 767, 5055, 9163, 13, 51214, 51214, 961, 311, 4444, 297, 11, 597, 307, 257, 5139, 1412, 13, 51364, 51364, 400, 550, 341, 11, 286, 1697, 11, 307, 264, 7603, 71, 5221, 5893, 1278, 295, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.2666740114726717, "compression_ratio": 1.3537414965986394, "no_speech_prob": 5.82797329116147e-05}, {"id": 661, "seek": 353700, "start": 3557.0, "end": 3564.0, "text": " And then this, I believe, is the tanh math dot exp of.", "tokens": [50364, 4056, 392, 293, 276, 293, 550, 484, 486, 312, 257, 2158, 295, 293, 321, 643, 341, 6114, 510, 13, 50764, 50764, 407, 718, 385, 767, 5055, 9163, 13, 51214, 51214, 961, 311, 4444, 297, 11, 597, 307, 257, 5139, 1412, 13, 51364, 51364, 400, 550, 341, 11, 286, 1697, 11, 307, 264, 7603, 71, 5221, 5893, 1278, 295, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.2666740114726717, "compression_ratio": 1.3537414965986394, "no_speech_prob": 5.82797329116147e-05}, {"id": 662, "seek": 356400, "start": 3564.0, "end": 3570.0, "text": " Two, no, and minus one over two and plus one.", "tokens": [50364, 4453, 11, 572, 11, 293, 3175, 472, 670, 732, 293, 1804, 472, 13, 50664, 50664, 2704, 286, 393, 818, 341, 1783, 13, 1449, 370, 300, 309, 10676, 2293, 13, 50914, 50914, 2264, 11, 293, 586, 341, 486, 312, 314, 13, 51164, 51164, 400, 2227, 295, 341, 9984, 11, 456, 311, 445, 472, 1440, 293, 286, 478, 21993, 309, 294, 257, 2604, 781, 13, 51464, 51464, 407, 341, 307, 257, 2604, 781, 295, 472, 2657, 11, 445, 2698, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.22012745461812833, "compression_ratio": 1.4742857142857142, "no_speech_prob": 0.00010553613537922502}, {"id": 663, "seek": 356400, "start": 3570.0, "end": 3575.0, "text": " Maybe I can call this X. Just so that it matches exactly.", "tokens": [50364, 4453, 11, 572, 11, 293, 3175, 472, 670, 732, 293, 1804, 472, 13, 50664, 50664, 2704, 286, 393, 818, 341, 1783, 13, 1449, 370, 300, 309, 10676, 2293, 13, 50914, 50914, 2264, 11, 293, 586, 341, 486, 312, 314, 13, 51164, 51164, 400, 2227, 295, 341, 9984, 11, 456, 311, 445, 472, 1440, 293, 286, 478, 21993, 309, 294, 257, 2604, 781, 13, 51464, 51464, 407, 341, 307, 257, 2604, 781, 295, 472, 2657, 11, 445, 2698, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.22012745461812833, "compression_ratio": 1.4742857142857142, "no_speech_prob": 0.00010553613537922502}, {"id": 664, "seek": 356400, "start": 3575.0, "end": 3580.0, "text": " OK, and now this will be T.", "tokens": [50364, 4453, 11, 572, 11, 293, 3175, 472, 670, 732, 293, 1804, 472, 13, 50664, 50664, 2704, 286, 393, 818, 341, 1783, 13, 1449, 370, 300, 309, 10676, 2293, 13, 50914, 50914, 2264, 11, 293, 586, 341, 486, 312, 314, 13, 51164, 51164, 400, 2227, 295, 341, 9984, 11, 456, 311, 445, 472, 1440, 293, 286, 478, 21993, 309, 294, 257, 2604, 781, 13, 51464, 51464, 407, 341, 307, 257, 2604, 781, 295, 472, 2657, 11, 445, 2698, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.22012745461812833, "compression_ratio": 1.4742857142857142, "no_speech_prob": 0.00010553613537922502}, {"id": 665, "seek": 356400, "start": 3580.0, "end": 3586.0, "text": " And children of this node, there's just one child and I'm wrapping it in a tuple.", "tokens": [50364, 4453, 11, 572, 11, 293, 3175, 472, 670, 732, 293, 1804, 472, 13, 50664, 50664, 2704, 286, 393, 818, 341, 1783, 13, 1449, 370, 300, 309, 10676, 2293, 13, 50914, 50914, 2264, 11, 293, 586, 341, 486, 312, 314, 13, 51164, 51164, 400, 2227, 295, 341, 9984, 11, 456, 311, 445, 472, 1440, 293, 286, 478, 21993, 309, 294, 257, 2604, 781, 13, 51464, 51464, 407, 341, 307, 257, 2604, 781, 295, 472, 2657, 11, 445, 2698, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.22012745461812833, "compression_ratio": 1.4742857142857142, "no_speech_prob": 0.00010553613537922502}, {"id": 666, "seek": 356400, "start": 3586.0, "end": 3588.0, "text": " So this is a tuple of one object, just self.", "tokens": [50364, 4453, 11, 572, 11, 293, 3175, 472, 670, 732, 293, 1804, 472, 13, 50664, 50664, 2704, 286, 393, 818, 341, 1783, 13, 1449, 370, 300, 309, 10676, 2293, 13, 50914, 50914, 2264, 11, 293, 586, 341, 486, 312, 314, 13, 51164, 51164, 400, 2227, 295, 341, 9984, 11, 456, 311, 445, 472, 1440, 293, 286, 478, 21993, 309, 294, 257, 2604, 781, 13, 51464, 51464, 407, 341, 307, 257, 2604, 781, 295, 472, 2657, 11, 445, 2698, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.22012745461812833, "compression_ratio": 1.4742857142857142, "no_speech_prob": 0.00010553613537922502}, {"id": 667, "seek": 358800, "start": 3588.0, "end": 3596.0, "text": " And here the name of this operation will be ten each and we're going to return that.", "tokens": [50364, 400, 510, 264, 1315, 295, 341, 6916, 486, 312, 2064, 1184, 293, 321, 434, 516, 281, 2736, 300, 13, 50764, 50764, 2264, 11, 370, 586, 4190, 820, 312, 18114, 2064, 1184, 293, 586, 321, 393, 11369, 439, 264, 636, 760, 510, 293, 321, 393, 767, 360, 293, 5893, 2064, 1184, 13, 51264, 51264, 400, 300, 311, 516, 281, 2736, 264, 2064, 21213, 5598, 295, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.29766057885211444, "compression_ratio": 1.6265060240963856, "no_speech_prob": 2.8854983611381613e-05}, {"id": 668, "seek": 358800, "start": 3596.0, "end": 3606.0, "text": " OK, so now values should be implementing ten each and now we can scroll all the way down here and we can actually do and dot ten each.", "tokens": [50364, 400, 510, 264, 1315, 295, 341, 6916, 486, 312, 2064, 1184, 293, 321, 434, 516, 281, 2736, 300, 13, 50764, 50764, 2264, 11, 370, 586, 4190, 820, 312, 18114, 2064, 1184, 293, 586, 321, 393, 11369, 439, 264, 636, 760, 510, 293, 321, 393, 767, 360, 293, 5893, 2064, 1184, 13, 51264, 51264, 400, 300, 311, 516, 281, 2736, 264, 2064, 21213, 5598, 295, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.29766057885211444, "compression_ratio": 1.6265060240963856, "no_speech_prob": 2.8854983611381613e-05}, {"id": 669, "seek": 358800, "start": 3606.0, "end": 3611.0, "text": " And that's going to return the ten aged output of.", "tokens": [50364, 400, 510, 264, 1315, 295, 341, 6916, 486, 312, 2064, 1184, 293, 321, 434, 516, 281, 2736, 300, 13, 50764, 50764, 2264, 11, 370, 586, 4190, 820, 312, 18114, 2064, 1184, 293, 586, 321, 393, 11369, 439, 264, 636, 760, 510, 293, 321, 393, 767, 360, 293, 5893, 2064, 1184, 13, 51264, 51264, 400, 300, 311, 516, 281, 2736, 264, 2064, 21213, 5598, 295, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.29766057885211444, "compression_ratio": 1.6265060240963856, "no_speech_prob": 2.8854983611381613e-05}, {"id": 670, "seek": 361100, "start": 3611.0, "end": 3618.0, "text": " And now we should be able to draw it out of, oh, not of. So let's see how that worked.", "tokens": [50364, 400, 586, 321, 820, 312, 1075, 281, 2642, 309, 484, 295, 11, 1954, 11, 406, 295, 13, 407, 718, 311, 536, 577, 300, 2732, 13, 50714, 50714, 821, 321, 352, 13, 400, 1437, 807, 2064, 3205, 281, 5258, 341, 493, 13, 51014, 51014, 407, 586, 2064, 3205, 307, 257, 1333, 295, 527, 707, 4532, 2771, 8104, 9984, 510, 382, 364, 6916, 13, 51464, 51464, 400, 382, 938, 382, 321, 458, 13760, 295, 2064, 3205, 11, 550, 321, 603, 312, 1075, 281, 646, 48256, 807, 309, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20236437055799697, "compression_ratio": 1.5980392156862746, "no_speech_prob": 4.0692812035558745e-05}, {"id": 671, "seek": 361100, "start": 3618.0, "end": 3624.0, "text": " There we go. And went through ten age to produce this up.", "tokens": [50364, 400, 586, 321, 820, 312, 1075, 281, 2642, 309, 484, 295, 11, 1954, 11, 406, 295, 13, 407, 718, 311, 536, 577, 300, 2732, 13, 50714, 50714, 821, 321, 352, 13, 400, 1437, 807, 2064, 3205, 281, 5258, 341, 493, 13, 51014, 51014, 407, 586, 2064, 3205, 307, 257, 1333, 295, 527, 707, 4532, 2771, 8104, 9984, 510, 382, 364, 6916, 13, 51464, 51464, 400, 382, 938, 382, 321, 458, 13760, 295, 2064, 3205, 11, 550, 321, 603, 312, 1075, 281, 646, 48256, 807, 309, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20236437055799697, "compression_ratio": 1.5980392156862746, "no_speech_prob": 4.0692812035558745e-05}, {"id": 672, "seek": 361100, "start": 3624.0, "end": 3633.0, "text": " So now ten age is a sort of our little micro grad supported node here as an operation.", "tokens": [50364, 400, 586, 321, 820, 312, 1075, 281, 2642, 309, 484, 295, 11, 1954, 11, 406, 295, 13, 407, 718, 311, 536, 577, 300, 2732, 13, 50714, 50714, 821, 321, 352, 13, 400, 1437, 807, 2064, 3205, 281, 5258, 341, 493, 13, 51014, 51014, 407, 586, 2064, 3205, 307, 257, 1333, 295, 527, 707, 4532, 2771, 8104, 9984, 510, 382, 364, 6916, 13, 51464, 51464, 400, 382, 938, 382, 321, 458, 13760, 295, 2064, 3205, 11, 550, 321, 603, 312, 1075, 281, 646, 48256, 807, 309, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20236437055799697, "compression_ratio": 1.5980392156862746, "no_speech_prob": 4.0692812035558745e-05}, {"id": 673, "seek": 361100, "start": 3633.0, "end": 3638.0, "text": " And as long as we know derivative of ten age, then we'll be able to back propagate through it.", "tokens": [50364, 400, 586, 321, 820, 312, 1075, 281, 2642, 309, 484, 295, 11, 1954, 11, 406, 295, 13, 407, 718, 311, 536, 577, 300, 2732, 13, 50714, 50714, 821, 321, 352, 13, 400, 1437, 807, 2064, 3205, 281, 5258, 341, 493, 13, 51014, 51014, 407, 586, 2064, 3205, 307, 257, 1333, 295, 527, 707, 4532, 2771, 8104, 9984, 510, 382, 364, 6916, 13, 51464, 51464, 400, 382, 938, 382, 321, 458, 13760, 295, 2064, 3205, 11, 550, 321, 603, 312, 1075, 281, 646, 48256, 807, 309, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20236437055799697, "compression_ratio": 1.5980392156862746, "no_speech_prob": 4.0692812035558745e-05}, {"id": 674, "seek": 363800, "start": 3638.0, "end": 3645.0, "text": " Now, let's see this ten age in action. Currently, it's not squashing too much because the input to it is pretty low.", "tokens": [50364, 823, 11, 718, 311, 536, 341, 2064, 3205, 294, 3069, 13, 19964, 11, 309, 311, 406, 2339, 11077, 886, 709, 570, 264, 4846, 281, 309, 307, 1238, 2295, 13, 50714, 50714, 407, 264, 12577, 390, 6505, 281, 584, 3180, 13, 50914, 50914, 1396, 321, 603, 536, 300, 437, 311, 13974, 666, 264, 2064, 3205, 586, 307, 732, 293, 2064, 3205, 307, 2339, 11077, 309, 281, 935, 4949, 2309, 13, 51314, 51314, 407, 321, 434, 1217, 8850, 264, 6838, 295, 341, 2064, 3205, 293, 309, 486, 1333, 295, 19565, 352, 493, 281, 472, 293, 550, 39885, 484, 670, 456, 13, 51614, 51614, 2264, 11, 370, 286, 478, 516, 281, 360, 746, 4748, 5861, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1175945070054796, "compression_ratio": 1.6742424242424243, "no_speech_prob": 4.006223753094673e-05}, {"id": 675, "seek": 363800, "start": 3645.0, "end": 3649.0, "text": " So the bias was increased to say eight.", "tokens": [50364, 823, 11, 718, 311, 536, 341, 2064, 3205, 294, 3069, 13, 19964, 11, 309, 311, 406, 2339, 11077, 886, 709, 570, 264, 4846, 281, 309, 307, 1238, 2295, 13, 50714, 50714, 407, 264, 12577, 390, 6505, 281, 584, 3180, 13, 50914, 50914, 1396, 321, 603, 536, 300, 437, 311, 13974, 666, 264, 2064, 3205, 586, 307, 732, 293, 2064, 3205, 307, 2339, 11077, 309, 281, 935, 4949, 2309, 13, 51314, 51314, 407, 321, 434, 1217, 8850, 264, 6838, 295, 341, 2064, 3205, 293, 309, 486, 1333, 295, 19565, 352, 493, 281, 472, 293, 550, 39885, 484, 670, 456, 13, 51614, 51614, 2264, 11, 370, 286, 478, 516, 281, 360, 746, 4748, 5861, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1175945070054796, "compression_ratio": 1.6742424242424243, "no_speech_prob": 4.006223753094673e-05}, {"id": 676, "seek": 363800, "start": 3649.0, "end": 3657.0, "text": " Then we'll see that what's flowing into the ten age now is two and ten age is squashing it to point nine six.", "tokens": [50364, 823, 11, 718, 311, 536, 341, 2064, 3205, 294, 3069, 13, 19964, 11, 309, 311, 406, 2339, 11077, 886, 709, 570, 264, 4846, 281, 309, 307, 1238, 2295, 13, 50714, 50714, 407, 264, 12577, 390, 6505, 281, 584, 3180, 13, 50914, 50914, 1396, 321, 603, 536, 300, 437, 311, 13974, 666, 264, 2064, 3205, 586, 307, 732, 293, 2064, 3205, 307, 2339, 11077, 309, 281, 935, 4949, 2309, 13, 51314, 51314, 407, 321, 434, 1217, 8850, 264, 6838, 295, 341, 2064, 3205, 293, 309, 486, 1333, 295, 19565, 352, 493, 281, 472, 293, 550, 39885, 484, 670, 456, 13, 51614, 51614, 2264, 11, 370, 286, 478, 516, 281, 360, 746, 4748, 5861, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1175945070054796, "compression_ratio": 1.6742424242424243, "no_speech_prob": 4.006223753094673e-05}, {"id": 677, "seek": 363800, "start": 3657.0, "end": 3663.0, "text": " So we're already hitting the tail of this ten age and it will sort of smoothly go up to one and then plateau out over there.", "tokens": [50364, 823, 11, 718, 311, 536, 341, 2064, 3205, 294, 3069, 13, 19964, 11, 309, 311, 406, 2339, 11077, 886, 709, 570, 264, 4846, 281, 309, 307, 1238, 2295, 13, 50714, 50714, 407, 264, 12577, 390, 6505, 281, 584, 3180, 13, 50914, 50914, 1396, 321, 603, 536, 300, 437, 311, 13974, 666, 264, 2064, 3205, 586, 307, 732, 293, 2064, 3205, 307, 2339, 11077, 309, 281, 935, 4949, 2309, 13, 51314, 51314, 407, 321, 434, 1217, 8850, 264, 6838, 295, 341, 2064, 3205, 293, 309, 486, 1333, 295, 19565, 352, 493, 281, 472, 293, 550, 39885, 484, 670, 456, 13, 51614, 51614, 2264, 11, 370, 286, 478, 516, 281, 360, 746, 4748, 5861, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1175945070054796, "compression_ratio": 1.6742424242424243, "no_speech_prob": 4.006223753094673e-05}, {"id": 678, "seek": 363800, "start": 3663.0, "end": 3666.0, "text": " OK, so I'm going to do something slightly strange.", "tokens": [50364, 823, 11, 718, 311, 536, 341, 2064, 3205, 294, 3069, 13, 19964, 11, 309, 311, 406, 2339, 11077, 886, 709, 570, 264, 4846, 281, 309, 307, 1238, 2295, 13, 50714, 50714, 407, 264, 12577, 390, 6505, 281, 584, 3180, 13, 50914, 50914, 1396, 321, 603, 536, 300, 437, 311, 13974, 666, 264, 2064, 3205, 586, 307, 732, 293, 2064, 3205, 307, 2339, 11077, 309, 281, 935, 4949, 2309, 13, 51314, 51314, 407, 321, 434, 1217, 8850, 264, 6838, 295, 341, 2064, 3205, 293, 309, 486, 1333, 295, 19565, 352, 493, 281, 472, 293, 550, 39885, 484, 670, 456, 13, 51614, 51614, 2264, 11, 370, 286, 478, 516, 281, 360, 746, 4748, 5861, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1175945070054796, "compression_ratio": 1.6742424242424243, "no_speech_prob": 4.006223753094673e-05}, {"id": 679, "seek": 366600, "start": 3666.0, "end": 3672.0, "text": " I'm going to change this bias from eight to this number six point eight eight, et cetera.", "tokens": [50364, 286, 478, 516, 281, 1319, 341, 12577, 490, 3180, 281, 341, 1230, 2309, 935, 3180, 3180, 11, 1030, 11458, 13, 50664, 50664, 400, 286, 478, 516, 281, 360, 341, 337, 2685, 4112, 570, 321, 434, 466, 281, 722, 646, 38377, 13, 50914, 50914, 400, 286, 528, 281, 652, 988, 300, 527, 3547, 808, 484, 1481, 13, 814, 434, 406, 411, 588, 3219, 3547, 13, 51164, 51164, 814, 434, 1481, 3547, 300, 321, 393, 1333, 295, 1223, 294, 527, 1378, 13, 51314, 51314, 961, 385, 611, 909, 1293, 7645, 13, 876, 11, 309, 311, 2099, 337, 5598, 510, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11952359068627451, "compression_ratio": 1.646090534979424, "no_speech_prob": 1.723051354929339e-05}, {"id": 680, "seek": 366600, "start": 3672.0, "end": 3677.0, "text": " And I'm going to do this for specific reasons because we're about to start back propagation.", "tokens": [50364, 286, 478, 516, 281, 1319, 341, 12577, 490, 3180, 281, 341, 1230, 2309, 935, 3180, 3180, 11, 1030, 11458, 13, 50664, 50664, 400, 286, 478, 516, 281, 360, 341, 337, 2685, 4112, 570, 321, 434, 466, 281, 722, 646, 38377, 13, 50914, 50914, 400, 286, 528, 281, 652, 988, 300, 527, 3547, 808, 484, 1481, 13, 814, 434, 406, 411, 588, 3219, 3547, 13, 51164, 51164, 814, 434, 1481, 3547, 300, 321, 393, 1333, 295, 1223, 294, 527, 1378, 13, 51314, 51314, 961, 385, 611, 909, 1293, 7645, 13, 876, 11, 309, 311, 2099, 337, 5598, 510, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11952359068627451, "compression_ratio": 1.646090534979424, "no_speech_prob": 1.723051354929339e-05}, {"id": 681, "seek": 366600, "start": 3677.0, "end": 3682.0, "text": " And I want to make sure that our numbers come out nice. They're not like very crazy numbers.", "tokens": [50364, 286, 478, 516, 281, 1319, 341, 12577, 490, 3180, 281, 341, 1230, 2309, 935, 3180, 3180, 11, 1030, 11458, 13, 50664, 50664, 400, 286, 478, 516, 281, 360, 341, 337, 2685, 4112, 570, 321, 434, 466, 281, 722, 646, 38377, 13, 50914, 50914, 400, 286, 528, 281, 652, 988, 300, 527, 3547, 808, 484, 1481, 13, 814, 434, 406, 411, 588, 3219, 3547, 13, 51164, 51164, 814, 434, 1481, 3547, 300, 321, 393, 1333, 295, 1223, 294, 527, 1378, 13, 51314, 51314, 961, 385, 611, 909, 1293, 7645, 13, 876, 11, 309, 311, 2099, 337, 5598, 510, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11952359068627451, "compression_ratio": 1.646090534979424, "no_speech_prob": 1.723051354929339e-05}, {"id": 682, "seek": 366600, "start": 3682.0, "end": 3685.0, "text": " They're nice numbers that we can sort of understand in our head.", "tokens": [50364, 286, 478, 516, 281, 1319, 341, 12577, 490, 3180, 281, 341, 1230, 2309, 935, 3180, 3180, 11, 1030, 11458, 13, 50664, 50664, 400, 286, 478, 516, 281, 360, 341, 337, 2685, 4112, 570, 321, 434, 466, 281, 722, 646, 38377, 13, 50914, 50914, 400, 286, 528, 281, 652, 988, 300, 527, 3547, 808, 484, 1481, 13, 814, 434, 406, 411, 588, 3219, 3547, 13, 51164, 51164, 814, 434, 1481, 3547, 300, 321, 393, 1333, 295, 1223, 294, 527, 1378, 13, 51314, 51314, 961, 385, 611, 909, 1293, 7645, 13, 876, 11, 309, 311, 2099, 337, 5598, 510, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11952359068627451, "compression_ratio": 1.646090534979424, "no_speech_prob": 1.723051354929339e-05}, {"id": 683, "seek": 366600, "start": 3685.0, "end": 3690.0, "text": " Let me also add both label. Oh, it's short for output here.", "tokens": [50364, 286, 478, 516, 281, 1319, 341, 12577, 490, 3180, 281, 341, 1230, 2309, 935, 3180, 3180, 11, 1030, 11458, 13, 50664, 50664, 400, 286, 478, 516, 281, 360, 341, 337, 2685, 4112, 570, 321, 434, 466, 281, 722, 646, 38377, 13, 50914, 50914, 400, 286, 528, 281, 652, 988, 300, 527, 3547, 808, 484, 1481, 13, 814, 434, 406, 411, 588, 3219, 3547, 13, 51164, 51164, 814, 434, 1481, 3547, 300, 321, 393, 1333, 295, 1223, 294, 527, 1378, 13, 51314, 51314, 961, 385, 611, 909, 1293, 7645, 13, 876, 11, 309, 311, 2099, 337, 5598, 510, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11952359068627451, "compression_ratio": 1.646090534979424, "no_speech_prob": 1.723051354929339e-05}, {"id": 684, "seek": 369000, "start": 3690.0, "end": 3700.0, "text": " So that's your OK. So point eight flows into ten age comes out point seven. So now we're going to do back propagation and we're going to fill in all the greens.", "tokens": [50364, 407, 300, 311, 428, 2264, 13, 407, 935, 3180, 12867, 666, 2064, 3205, 1487, 484, 935, 3407, 13, 407, 586, 321, 434, 516, 281, 360, 646, 38377, 293, 321, 434, 516, 281, 2836, 294, 439, 264, 22897, 13, 50864, 50864, 407, 437, 307, 264, 13760, 30, 876, 11, 365, 3104, 281, 439, 264, 15743, 510, 13, 51164, 51164, 400, 295, 1164, 11, 294, 257, 7476, 18161, 3209, 3287, 11, 437, 321, 534, 1127, 466, 264, 881, 307, 264, 13760, 295, 613, 22027, 322, 264, 17443, 11, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.16436968909369576, "compression_ratio": 1.641255605381166, "no_speech_prob": 6.3389952629222535e-06}, {"id": 685, "seek": 369000, "start": 3700.0, "end": 3706.0, "text": " So what is the derivative? Oh, with respect to all the inputs here.", "tokens": [50364, 407, 300, 311, 428, 2264, 13, 407, 935, 3180, 12867, 666, 2064, 3205, 1487, 484, 935, 3407, 13, 407, 586, 321, 434, 516, 281, 360, 646, 38377, 293, 321, 434, 516, 281, 2836, 294, 439, 264, 22897, 13, 50864, 50864, 407, 437, 307, 264, 13760, 30, 876, 11, 365, 3104, 281, 439, 264, 15743, 510, 13, 51164, 51164, 400, 295, 1164, 11, 294, 257, 7476, 18161, 3209, 3287, 11, 437, 321, 534, 1127, 466, 264, 881, 307, 264, 13760, 295, 613, 22027, 322, 264, 17443, 11, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.16436968909369576, "compression_ratio": 1.641255605381166, "no_speech_prob": 6.3389952629222535e-06}, {"id": 686, "seek": 369000, "start": 3706.0, "end": 3713.0, "text": " And of course, in a typical neural network setting, what we really care about the most is the derivative of these neurons on the weights,", "tokens": [50364, 407, 300, 311, 428, 2264, 13, 407, 935, 3180, 12867, 666, 2064, 3205, 1487, 484, 935, 3407, 13, 407, 586, 321, 434, 516, 281, 360, 646, 38377, 293, 321, 434, 516, 281, 2836, 294, 439, 264, 22897, 13, 50864, 50864, 407, 437, 307, 264, 13760, 30, 876, 11, 365, 3104, 281, 439, 264, 15743, 510, 13, 51164, 51164, 400, 295, 1164, 11, 294, 257, 7476, 18161, 3209, 3287, 11, 437, 321, 534, 1127, 466, 264, 881, 307, 264, 13760, 295, 613, 22027, 322, 264, 17443, 11, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.16436968909369576, "compression_ratio": 1.641255605381166, "no_speech_prob": 6.3389952629222535e-06}, {"id": 687, "seek": 371300, "start": 3713.0, "end": 3720.0, "text": " specifically the W two and W one, because those are the weights that we're going to be changing part of the optimization.", "tokens": [50364, 4682, 264, 343, 732, 293, 343, 472, 11, 570, 729, 366, 264, 17443, 300, 321, 434, 516, 281, 312, 4473, 644, 295, 264, 19618, 13, 50714, 50714, 400, 264, 661, 551, 300, 321, 362, 281, 1604, 307, 510, 321, 362, 787, 257, 2167, 34090, 11, 457, 294, 264, 18161, 2533, 11, 291, 5850, 362, 867, 22027, 293, 436, 434, 4582, 13, 51064, 51064, 407, 341, 307, 787, 411, 472, 1359, 34090, 11, 257, 2522, 295, 257, 709, 3801, 12805, 13, 51214, 51214, 400, 4728, 456, 311, 257, 4470, 2445, 300, 1333, 295, 8000, 264, 14170, 295, 264, 18161, 2533, 13, 51414, 51414, 400, 321, 434, 646, 12425, 990, 365, 3104, 281, 300, 14170, 293, 1382, 281, 3488, 309, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09947532560767197, "compression_ratio": 1.7939189189189189, "no_speech_prob": 7.0715718720748555e-06}, {"id": 688, "seek": 371300, "start": 3720.0, "end": 3727.0, "text": " And the other thing that we have to remember is here we have only a single neuron, but in the neural net, you typically have many neurons and they're connected.", "tokens": [50364, 4682, 264, 343, 732, 293, 343, 472, 11, 570, 729, 366, 264, 17443, 300, 321, 434, 516, 281, 312, 4473, 644, 295, 264, 19618, 13, 50714, 50714, 400, 264, 661, 551, 300, 321, 362, 281, 1604, 307, 510, 321, 362, 787, 257, 2167, 34090, 11, 457, 294, 264, 18161, 2533, 11, 291, 5850, 362, 867, 22027, 293, 436, 434, 4582, 13, 51064, 51064, 407, 341, 307, 787, 411, 472, 1359, 34090, 11, 257, 2522, 295, 257, 709, 3801, 12805, 13, 51214, 51214, 400, 4728, 456, 311, 257, 4470, 2445, 300, 1333, 295, 8000, 264, 14170, 295, 264, 18161, 2533, 13, 51414, 51414, 400, 321, 434, 646, 12425, 990, 365, 3104, 281, 300, 14170, 293, 1382, 281, 3488, 309, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09947532560767197, "compression_ratio": 1.7939189189189189, "no_speech_prob": 7.0715718720748555e-06}, {"id": 689, "seek": 371300, "start": 3727.0, "end": 3730.0, "text": " So this is only like one small neuron, a piece of a much bigger puzzle.", "tokens": [50364, 4682, 264, 343, 732, 293, 343, 472, 11, 570, 729, 366, 264, 17443, 300, 321, 434, 516, 281, 312, 4473, 644, 295, 264, 19618, 13, 50714, 50714, 400, 264, 661, 551, 300, 321, 362, 281, 1604, 307, 510, 321, 362, 787, 257, 2167, 34090, 11, 457, 294, 264, 18161, 2533, 11, 291, 5850, 362, 867, 22027, 293, 436, 434, 4582, 13, 51064, 51064, 407, 341, 307, 787, 411, 472, 1359, 34090, 11, 257, 2522, 295, 257, 709, 3801, 12805, 13, 51214, 51214, 400, 4728, 456, 311, 257, 4470, 2445, 300, 1333, 295, 8000, 264, 14170, 295, 264, 18161, 2533, 13, 51414, 51414, 400, 321, 434, 646, 12425, 990, 365, 3104, 281, 300, 14170, 293, 1382, 281, 3488, 309, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09947532560767197, "compression_ratio": 1.7939189189189189, "no_speech_prob": 7.0715718720748555e-06}, {"id": 690, "seek": 371300, "start": 3730.0, "end": 3734.0, "text": " And eventually there's a loss function that sort of measures the accuracy of the neural net.", "tokens": [50364, 4682, 264, 343, 732, 293, 343, 472, 11, 570, 729, 366, 264, 17443, 300, 321, 434, 516, 281, 312, 4473, 644, 295, 264, 19618, 13, 50714, 50714, 400, 264, 661, 551, 300, 321, 362, 281, 1604, 307, 510, 321, 362, 787, 257, 2167, 34090, 11, 457, 294, 264, 18161, 2533, 11, 291, 5850, 362, 867, 22027, 293, 436, 434, 4582, 13, 51064, 51064, 407, 341, 307, 787, 411, 472, 1359, 34090, 11, 257, 2522, 295, 257, 709, 3801, 12805, 13, 51214, 51214, 400, 4728, 456, 311, 257, 4470, 2445, 300, 1333, 295, 8000, 264, 14170, 295, 264, 18161, 2533, 13, 51414, 51414, 400, 321, 434, 646, 12425, 990, 365, 3104, 281, 300, 14170, 293, 1382, 281, 3488, 309, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09947532560767197, "compression_ratio": 1.7939189189189189, "no_speech_prob": 7.0715718720748555e-06}, {"id": 691, "seek": 371300, "start": 3734.0, "end": 3739.0, "text": " And we're back propagating with respect to that accuracy and trying to increase it.", "tokens": [50364, 4682, 264, 343, 732, 293, 343, 472, 11, 570, 729, 366, 264, 17443, 300, 321, 434, 516, 281, 312, 4473, 644, 295, 264, 19618, 13, 50714, 50714, 400, 264, 661, 551, 300, 321, 362, 281, 1604, 307, 510, 321, 362, 787, 257, 2167, 34090, 11, 457, 294, 264, 18161, 2533, 11, 291, 5850, 362, 867, 22027, 293, 436, 434, 4582, 13, 51064, 51064, 407, 341, 307, 787, 411, 472, 1359, 34090, 11, 257, 2522, 295, 257, 709, 3801, 12805, 13, 51214, 51214, 400, 4728, 456, 311, 257, 4470, 2445, 300, 1333, 295, 8000, 264, 14170, 295, 264, 18161, 2533, 13, 51414, 51414, 400, 321, 434, 646, 12425, 990, 365, 3104, 281, 300, 14170, 293, 1382, 281, 3488, 309, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09947532560767197, "compression_ratio": 1.7939189189189189, "no_speech_prob": 7.0715718720748555e-06}, {"id": 692, "seek": 373900, "start": 3739.0, "end": 3745.0, "text": " So let's start off back propagation here in the end. What is the derivative of O with respect to O?", "tokens": [50364, 407, 718, 311, 722, 766, 646, 38377, 510, 294, 264, 917, 13, 708, 307, 264, 13760, 295, 422, 365, 3104, 281, 422, 30, 50664, 50664, 440, 3096, 1389, 1333, 295, 321, 458, 1009, 307, 300, 264, 16235, 307, 445, 472, 935, 4018, 13, 50914, 50914, 407, 718, 385, 2836, 309, 294, 293, 550, 718, 385, 7472, 484, 264, 6316, 2445, 510, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12285697821414832, "compression_ratio": 1.4825581395348837, "no_speech_prob": 1.241102199855959e-05}, {"id": 693, "seek": 373900, "start": 3745.0, "end": 3750.0, "text": " The base case sort of we know always is that the gradient is just one point zero.", "tokens": [50364, 407, 718, 311, 722, 766, 646, 38377, 510, 294, 264, 917, 13, 708, 307, 264, 13760, 295, 422, 365, 3104, 281, 422, 30, 50664, 50664, 440, 3096, 1389, 1333, 295, 321, 458, 1009, 307, 300, 264, 16235, 307, 445, 472, 935, 4018, 13, 50914, 50914, 407, 718, 385, 2836, 309, 294, 293, 550, 718, 385, 7472, 484, 264, 6316, 2445, 510, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12285697821414832, "compression_ratio": 1.4825581395348837, "no_speech_prob": 1.241102199855959e-05}, {"id": 694, "seek": 373900, "start": 3750.0, "end": 3763.0, "text": " So let me fill it in and then let me split out the drawing function here.", "tokens": [50364, 407, 718, 311, 722, 766, 646, 38377, 510, 294, 264, 917, 13, 708, 307, 264, 13760, 295, 422, 365, 3104, 281, 422, 30, 50664, 50664, 440, 3096, 1389, 1333, 295, 321, 458, 1009, 307, 300, 264, 16235, 307, 445, 472, 935, 4018, 13, 50914, 50914, 407, 718, 385, 2836, 309, 294, 293, 550, 718, 385, 7472, 484, 264, 6316, 2445, 510, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.12285697821414832, "compression_ratio": 1.4825581395348837, "no_speech_prob": 1.241102199855959e-05}, {"id": 695, "seek": 376300, "start": 3763.0, "end": 3770.0, "text": " And then here, cell clear this output here.", "tokens": [50364, 400, 550, 510, 11, 2815, 1850, 341, 5598, 510, 13, 50714, 50714, 407, 586, 562, 321, 2642, 422, 11, 321, 603, 536, 300, 422, 307, 472, 13, 50914, 50914, 407, 586, 321, 434, 516, 281, 646, 48256, 807, 264, 1266, 389, 13, 407, 281, 646, 48256, 807, 1266, 389, 11, 321, 643, 281, 458, 264, 2654, 13760, 295, 1266, 389, 13, 51264, 51264, 407, 498, 321, 362, 300, 422, 307, 1266, 389, 295, 426, 11, 550, 437, 307, 413, 422, 538, 413, 426, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.15945800867947665, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.00012729274749290198}, {"id": 696, "seek": 376300, "start": 3770.0, "end": 3774.0, "text": " So now when we draw O, we'll see that O is one.", "tokens": [50364, 400, 550, 510, 11, 2815, 1850, 341, 5598, 510, 13, 50714, 50714, 407, 586, 562, 321, 2642, 422, 11, 321, 603, 536, 300, 422, 307, 472, 13, 50914, 50914, 407, 586, 321, 434, 516, 281, 646, 48256, 807, 264, 1266, 389, 13, 407, 281, 646, 48256, 807, 1266, 389, 11, 321, 643, 281, 458, 264, 2654, 13760, 295, 1266, 389, 13, 51264, 51264, 407, 498, 321, 362, 300, 422, 307, 1266, 389, 295, 426, 11, 550, 437, 307, 413, 422, 538, 413, 426, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.15945800867947665, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.00012729274749290198}, {"id": 697, "seek": 376300, "start": 3774.0, "end": 3781.0, "text": " So now we're going to back propagate through the 10 H. So to back propagate through 10 H, we need to know the local derivative of 10 H.", "tokens": [50364, 400, 550, 510, 11, 2815, 1850, 341, 5598, 510, 13, 50714, 50714, 407, 586, 562, 321, 2642, 422, 11, 321, 603, 536, 300, 422, 307, 472, 13, 50914, 50914, 407, 586, 321, 434, 516, 281, 646, 48256, 807, 264, 1266, 389, 13, 407, 281, 646, 48256, 807, 1266, 389, 11, 321, 643, 281, 458, 264, 2654, 13760, 295, 1266, 389, 13, 51264, 51264, 407, 498, 321, 362, 300, 422, 307, 1266, 389, 295, 426, 11, 550, 437, 307, 413, 422, 538, 413, 426, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.15945800867947665, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.00012729274749290198}, {"id": 698, "seek": 376300, "start": 3781.0, "end": 3792.0, "text": " So if we have that O is 10 H of N, then what is D O by D N?", "tokens": [50364, 400, 550, 510, 11, 2815, 1850, 341, 5598, 510, 13, 50714, 50714, 407, 586, 562, 321, 2642, 422, 11, 321, 603, 536, 300, 422, 307, 472, 13, 50914, 50914, 407, 586, 321, 434, 516, 281, 646, 48256, 807, 264, 1266, 389, 13, 407, 281, 646, 48256, 807, 1266, 389, 11, 321, 643, 281, 458, 264, 2654, 13760, 295, 1266, 389, 13, 51264, 51264, 407, 498, 321, 362, 300, 422, 307, 1266, 389, 295, 426, 11, 550, 437, 307, 413, 422, 538, 413, 426, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.15945800867947665, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.00012729274749290198}, {"id": 699, "seek": 379200, "start": 3792.0, "end": 3799.0, "text": " Now, what you could do is you could come here and you could take this expression and you could do your calculus derivative taking.", "tokens": [50364, 823, 11, 437, 291, 727, 360, 307, 291, 727, 808, 510, 293, 291, 727, 747, 341, 6114, 293, 291, 727, 360, 428, 33400, 13760, 1940, 13, 50714, 50714, 400, 300, 576, 589, 13, 583, 321, 393, 611, 445, 11369, 760, 28999, 510, 666, 257, 3541, 300, 4696, 5112, 505, 300, 13760, 413, 538, 413, 1783, 295, 1266, 389, 295, 1783, 307, 604, 295, 613, 13, 51364, 51364, 286, 411, 341, 472, 472, 3175, 1266, 389, 3732, 295, 1783, 13, 407, 341, 307, 472, 3175, 1266, 389, 295, 1783, 8889, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10502793941091984, "compression_ratio": 1.6981981981981982, "no_speech_prob": 6.814276275690645e-05}, {"id": 700, "seek": 379200, "start": 3799.0, "end": 3812.0, "text": " And that would work. But we can also just scroll down Wikipedia here into a section that hopefully tells us that derivative D by D X of 10 H of X is any of these.", "tokens": [50364, 823, 11, 437, 291, 727, 360, 307, 291, 727, 808, 510, 293, 291, 727, 747, 341, 6114, 293, 291, 727, 360, 428, 33400, 13760, 1940, 13, 50714, 50714, 400, 300, 576, 589, 13, 583, 321, 393, 611, 445, 11369, 760, 28999, 510, 666, 257, 3541, 300, 4696, 5112, 505, 300, 13760, 413, 538, 413, 1783, 295, 1266, 389, 295, 1783, 307, 604, 295, 613, 13, 51364, 51364, 286, 411, 341, 472, 472, 3175, 1266, 389, 3732, 295, 1783, 13, 407, 341, 307, 472, 3175, 1266, 389, 295, 1783, 8889, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10502793941091984, "compression_ratio": 1.6981981981981982, "no_speech_prob": 6.814276275690645e-05}, {"id": 701, "seek": 379200, "start": 3812.0, "end": 3819.0, "text": " I like this one one minus 10 H square of X. So this is one minus 10 H of X squared.", "tokens": [50364, 823, 11, 437, 291, 727, 360, 307, 291, 727, 808, 510, 293, 291, 727, 747, 341, 6114, 293, 291, 727, 360, 428, 33400, 13760, 1940, 13, 50714, 50714, 400, 300, 576, 589, 13, 583, 321, 393, 611, 445, 11369, 760, 28999, 510, 666, 257, 3541, 300, 4696, 5112, 505, 300, 13760, 413, 538, 413, 1783, 295, 1266, 389, 295, 1783, 307, 604, 295, 613, 13, 51364, 51364, 286, 411, 341, 472, 472, 3175, 1266, 389, 3732, 295, 1783, 13, 407, 341, 307, 472, 3175, 1266, 389, 295, 1783, 8889, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10502793941091984, "compression_ratio": 1.6981981981981982, "no_speech_prob": 6.814276275690645e-05}, {"id": 702, "seek": 381900, "start": 3819.0, "end": 3831.0, "text": " So basically what this is saying is that D O by D N is one minus 10 H of N squared.", "tokens": [50364, 407, 1936, 437, 341, 307, 1566, 307, 300, 413, 422, 538, 413, 426, 307, 472, 3175, 1266, 389, 295, 426, 8889, 13, 50964, 50964, 400, 321, 1217, 362, 1266, 389, 295, 426, 13, 467, 311, 445, 422, 13, 407, 309, 311, 472, 3175, 422, 8889, 13, 407, 422, 307, 264, 5598, 510, 13, 51314, 51314, 407, 264, 5598, 307, 341, 1230, 13, 422, 5893, 1412, 307, 341, 1230, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1068325891886672, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.704984555952251e-05}, {"id": 703, "seek": 381900, "start": 3831.0, "end": 3838.0, "text": " And we already have 10 H of N. It's just O. So it's one minus O squared. So O is the output here.", "tokens": [50364, 407, 1936, 437, 341, 307, 1566, 307, 300, 413, 422, 538, 413, 426, 307, 472, 3175, 1266, 389, 295, 426, 8889, 13, 50964, 50964, 400, 321, 1217, 362, 1266, 389, 295, 426, 13, 467, 311, 445, 422, 13, 407, 309, 311, 472, 3175, 422, 8889, 13, 407, 422, 307, 264, 5598, 510, 13, 51314, 51314, 407, 264, 5598, 307, 341, 1230, 13, 422, 5893, 1412, 307, 341, 1230, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1068325891886672, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.704984555952251e-05}, {"id": 704, "seek": 381900, "start": 3838.0, "end": 3846.0, "text": " So the output is this number. O dot data is this number.", "tokens": [50364, 407, 1936, 437, 341, 307, 1566, 307, 300, 413, 422, 538, 413, 426, 307, 472, 3175, 1266, 389, 295, 426, 8889, 13, 50964, 50964, 400, 321, 1217, 362, 1266, 389, 295, 426, 13, 467, 311, 445, 422, 13, 407, 309, 311, 472, 3175, 422, 8889, 13, 407, 422, 307, 264, 5598, 510, 13, 51314, 51314, 407, 264, 5598, 307, 341, 1230, 13, 422, 5893, 1412, 307, 341, 1230, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1068325891886672, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.704984555952251e-05}, {"id": 705, "seek": 384600, "start": 3846.0, "end": 3852.0, "text": " And then what this is saying is that D O by D N is one minus this squared.", "tokens": [50364, 400, 550, 437, 341, 307, 1566, 307, 300, 413, 422, 538, 413, 426, 307, 472, 3175, 341, 8889, 13, 50664, 50664, 407, 472, 3175, 422, 5893, 1412, 8889, 307, 935, 1732, 44375, 13, 51014, 51014, 407, 264, 2654, 13760, 295, 341, 1266, 389, 6916, 510, 307, 935, 1732, 13, 51264, 51264, 400, 370, 300, 576, 312, 413, 422, 538, 413, 426, 13, 407, 321, 393, 2836, 294, 300, 426, 5893, 2771, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0919196605682373, "compression_ratio": 1.5917159763313609, "no_speech_prob": 1.544586666568648e-05}, {"id": 706, "seek": 384600, "start": 3852.0, "end": 3859.0, "text": " So one minus O dot data squared is point five conveniently.", "tokens": [50364, 400, 550, 437, 341, 307, 1566, 307, 300, 413, 422, 538, 413, 426, 307, 472, 3175, 341, 8889, 13, 50664, 50664, 407, 472, 3175, 422, 5893, 1412, 8889, 307, 935, 1732, 44375, 13, 51014, 51014, 407, 264, 2654, 13760, 295, 341, 1266, 389, 6916, 510, 307, 935, 1732, 13, 51264, 51264, 400, 370, 300, 576, 312, 413, 422, 538, 413, 426, 13, 407, 321, 393, 2836, 294, 300, 426, 5893, 2771, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0919196605682373, "compression_ratio": 1.5917159763313609, "no_speech_prob": 1.544586666568648e-05}, {"id": 707, "seek": 384600, "start": 3859.0, "end": 3864.0, "text": " So the local derivative of this 10 H operation here is point five.", "tokens": [50364, 400, 550, 437, 341, 307, 1566, 307, 300, 413, 422, 538, 413, 426, 307, 472, 3175, 341, 8889, 13, 50664, 50664, 407, 472, 3175, 422, 5893, 1412, 8889, 307, 935, 1732, 44375, 13, 51014, 51014, 407, 264, 2654, 13760, 295, 341, 1266, 389, 6916, 510, 307, 935, 1732, 13, 51264, 51264, 400, 370, 300, 576, 312, 413, 422, 538, 413, 426, 13, 407, 321, 393, 2836, 294, 300, 426, 5893, 2771, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0919196605682373, "compression_ratio": 1.5917159763313609, "no_speech_prob": 1.544586666568648e-05}, {"id": 708, "seek": 384600, "start": 3864.0, "end": 3873.0, "text": " And so that would be D O by D N. So we can fill in that N dot grad.", "tokens": [50364, 400, 550, 437, 341, 307, 1566, 307, 300, 413, 422, 538, 413, 426, 307, 472, 3175, 341, 8889, 13, 50664, 50664, 407, 472, 3175, 422, 5893, 1412, 8889, 307, 935, 1732, 44375, 13, 51014, 51014, 407, 264, 2654, 13760, 295, 341, 1266, 389, 6916, 510, 307, 935, 1732, 13, 51264, 51264, 400, 370, 300, 576, 312, 413, 422, 538, 413, 426, 13, 407, 321, 393, 2836, 294, 300, 426, 5893, 2771, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0919196605682373, "compression_ratio": 1.5917159763313609, "no_speech_prob": 1.544586666568648e-05}, {"id": 709, "seek": 387300, "start": 3873.0, "end": 3882.0, "text": " Is point five. We'll just fill it in.", "tokens": [50364, 1119, 935, 1732, 13, 492, 603, 445, 2836, 309, 294, 13, 50814, 50814, 407, 341, 307, 2293, 935, 1732, 935, 1922, 13, 407, 586, 321, 434, 516, 281, 2354, 264, 646, 38377, 13, 51164, 51164, 639, 307, 935, 1732, 293, 341, 307, 257, 1804, 9984, 13, 407, 577, 307, 646, 2365, 516, 281, 437, 307, 300, 516, 281, 360, 510, 30, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.20999292226938102, "compression_ratio": 1.6275862068965516, "no_speech_prob": 8.530107152182609e-06}, {"id": 710, "seek": 387300, "start": 3882.0, "end": 3889.0, "text": " So this is exactly point five point half. So now we're going to continue the back propagation.", "tokens": [50364, 1119, 935, 1732, 13, 492, 603, 445, 2836, 309, 294, 13, 50814, 50814, 407, 341, 307, 2293, 935, 1732, 935, 1922, 13, 407, 586, 321, 434, 516, 281, 2354, 264, 646, 38377, 13, 51164, 51164, 639, 307, 935, 1732, 293, 341, 307, 257, 1804, 9984, 13, 407, 577, 307, 646, 2365, 516, 281, 437, 307, 300, 516, 281, 360, 510, 30, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.20999292226938102, "compression_ratio": 1.6275862068965516, "no_speech_prob": 8.530107152182609e-06}, {"id": 711, "seek": 387300, "start": 3889.0, "end": 3896.0, "text": " This is point five and this is a plus node. So how is back prop going to what is that going to do here?", "tokens": [50364, 1119, 935, 1732, 13, 492, 603, 445, 2836, 309, 294, 13, 50814, 50814, 407, 341, 307, 2293, 935, 1732, 935, 1922, 13, 407, 586, 321, 434, 516, 281, 2354, 264, 646, 38377, 13, 51164, 51164, 639, 307, 935, 1732, 293, 341, 307, 257, 1804, 9984, 13, 407, 577, 307, 646, 2365, 516, 281, 437, 307, 300, 516, 281, 360, 510, 30, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.20999292226938102, "compression_ratio": 1.6275862068965516, "no_speech_prob": 8.530107152182609e-06}, {"id": 712, "seek": 389600, "start": 3896.0, "end": 3905.0, "text": " And if you remember our previous example, a plus is just a distributor of gradient. So this gradient will simply flow to both of these equally.", "tokens": [50364, 400, 498, 291, 1604, 527, 3894, 1365, 11, 257, 1804, 307, 445, 257, 49192, 295, 16235, 13, 407, 341, 16235, 486, 2935, 3095, 281, 1293, 295, 613, 12309, 13, 50814, 50814, 400, 300, 311, 570, 264, 2654, 13760, 295, 341, 6916, 307, 472, 337, 633, 472, 295, 1080, 13891, 13, 51064, 51064, 407, 472, 1413, 935, 1732, 307, 935, 1732, 13, 407, 4412, 321, 458, 300, 341, 9984, 510, 11, 597, 321, 1219, 341, 11, 1080, 2771, 307, 445, 935, 1732, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06473850094994833, "compression_ratio": 1.6727272727272726, "no_speech_prob": 9.080255040316842e-06}, {"id": 713, "seek": 389600, "start": 3905.0, "end": 3910.0, "text": " And that's because the local derivative of this operation is one for every one of its nodes.", "tokens": [50364, 400, 498, 291, 1604, 527, 3894, 1365, 11, 257, 1804, 307, 445, 257, 49192, 295, 16235, 13, 407, 341, 16235, 486, 2935, 3095, 281, 1293, 295, 613, 12309, 13, 50814, 50814, 400, 300, 311, 570, 264, 2654, 13760, 295, 341, 6916, 307, 472, 337, 633, 472, 295, 1080, 13891, 13, 51064, 51064, 407, 472, 1413, 935, 1732, 307, 935, 1732, 13, 407, 4412, 321, 458, 300, 341, 9984, 510, 11, 597, 321, 1219, 341, 11, 1080, 2771, 307, 445, 935, 1732, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06473850094994833, "compression_ratio": 1.6727272727272726, "no_speech_prob": 9.080255040316842e-06}, {"id": 714, "seek": 389600, "start": 3910.0, "end": 3921.0, "text": " So one times point five is point five. So therefore we know that this node here, which we called this, its grad is just point five.", "tokens": [50364, 400, 498, 291, 1604, 527, 3894, 1365, 11, 257, 1804, 307, 445, 257, 49192, 295, 16235, 13, 407, 341, 16235, 486, 2935, 3095, 281, 1293, 295, 613, 12309, 13, 50814, 50814, 400, 300, 311, 570, 264, 2654, 13760, 295, 341, 6916, 307, 472, 337, 633, 472, 295, 1080, 13891, 13, 51064, 51064, 407, 472, 1413, 935, 1732, 307, 935, 1732, 13, 407, 4412, 321, 458, 300, 341, 9984, 510, 11, 597, 321, 1219, 341, 11, 1080, 2771, 307, 445, 935, 1732, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06473850094994833, "compression_ratio": 1.6727272727272726, "no_speech_prob": 9.080255040316842e-06}, {"id": 715, "seek": 392100, "start": 3921.0, "end": 3929.0, "text": " And we know that B dot grad is also point five. So let's set those and let's draw.", "tokens": [50364, 400, 321, 458, 300, 363, 5893, 2771, 307, 611, 935, 1732, 13, 407, 718, 311, 992, 729, 293, 718, 311, 2642, 13, 50764, 50764, 407, 729, 366, 935, 1732, 13, 47585, 11, 321, 362, 1071, 1804, 935, 1732, 797, 13, 492, 603, 445, 20594, 13, 51064, 51064, 407, 935, 1732, 486, 3095, 281, 1293, 295, 613, 370, 321, 393, 992, 22760, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13741065516616358, "compression_ratio": 1.564102564102564, "no_speech_prob": 9.972642146749422e-06}, {"id": 716, "seek": 392100, "start": 3929.0, "end": 3935.0, "text": " So those are point five. Continuing, we have another plus point five again. We'll just distribute.", "tokens": [50364, 400, 321, 458, 300, 363, 5893, 2771, 307, 611, 935, 1732, 13, 407, 718, 311, 992, 729, 293, 718, 311, 2642, 13, 50764, 50764, 407, 729, 366, 935, 1732, 13, 47585, 11, 321, 362, 1071, 1804, 935, 1732, 797, 13, 492, 603, 445, 20594, 13, 51064, 51064, 407, 935, 1732, 486, 3095, 281, 1293, 295, 613, 370, 321, 393, 992, 22760, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13741065516616358, "compression_ratio": 1.564102564102564, "no_speech_prob": 9.972642146749422e-06}, {"id": 717, "seek": 392100, "start": 3935.0, "end": 3944.0, "text": " So point five will flow to both of these so we can set theirs.", "tokens": [50364, 400, 321, 458, 300, 363, 5893, 2771, 307, 611, 935, 1732, 13, 407, 718, 311, 992, 729, 293, 718, 311, 2642, 13, 50764, 50764, 407, 729, 366, 935, 1732, 13, 47585, 11, 321, 362, 1071, 1804, 935, 1732, 797, 13, 492, 603, 445, 20594, 13, 51064, 51064, 407, 935, 1732, 486, 3095, 281, 1293, 295, 613, 370, 321, 393, 992, 22760, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13741065516616358, "compression_ratio": 1.564102564102564, "no_speech_prob": 9.972642146749422e-06}, {"id": 718, "seek": 394400, "start": 3944.0, "end": 3952.0, "text": " So this is point five plus two W two as well grad is point five. And let's redraw.", "tokens": [50364, 407, 341, 307, 935, 1732, 1804, 732, 343, 732, 382, 731, 2771, 307, 935, 1732, 13, 400, 718, 311, 2182, 5131, 13, 50764, 50764, 7721, 279, 366, 452, 2954, 7705, 281, 646, 48256, 807, 570, 309, 311, 588, 2199, 13, 51064, 51064, 407, 586, 437, 311, 13974, 666, 613, 15277, 307, 935, 1732, 13, 51214, 51214, 400, 370, 534, 11, 797, 11, 1066, 294, 1575, 437, 264, 13760, 307, 3585, 505, 412, 633, 935, 294, 565, 2051, 510, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.2535288247717432, "compression_ratio": 1.5809523809523809, "no_speech_prob": 1.6536620250917622e-06}, {"id": 719, "seek": 394400, "start": 3952.0, "end": 3958.0, "text": " Pluses are my favorite operations to back propagate through because it's very simple.", "tokens": [50364, 407, 341, 307, 935, 1732, 1804, 732, 343, 732, 382, 731, 2771, 307, 935, 1732, 13, 400, 718, 311, 2182, 5131, 13, 50764, 50764, 7721, 279, 366, 452, 2954, 7705, 281, 646, 48256, 807, 570, 309, 311, 588, 2199, 13, 51064, 51064, 407, 586, 437, 311, 13974, 666, 613, 15277, 307, 935, 1732, 13, 51214, 51214, 400, 370, 534, 11, 797, 11, 1066, 294, 1575, 437, 264, 13760, 307, 3585, 505, 412, 633, 935, 294, 565, 2051, 510, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.2535288247717432, "compression_ratio": 1.5809523809523809, "no_speech_prob": 1.6536620250917622e-06}, {"id": 720, "seek": 394400, "start": 3958.0, "end": 3961.0, "text": " So now what's flowing into these expressions is point five.", "tokens": [50364, 407, 341, 307, 935, 1732, 1804, 732, 343, 732, 382, 731, 2771, 307, 935, 1732, 13, 400, 718, 311, 2182, 5131, 13, 50764, 50764, 7721, 279, 366, 452, 2954, 7705, 281, 646, 48256, 807, 570, 309, 311, 588, 2199, 13, 51064, 51064, 407, 586, 437, 311, 13974, 666, 613, 15277, 307, 935, 1732, 13, 51214, 51214, 400, 370, 534, 11, 797, 11, 1066, 294, 1575, 437, 264, 13760, 307, 3585, 505, 412, 633, 935, 294, 565, 2051, 510, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.2535288247717432, "compression_ratio": 1.5809523809523809, "no_speech_prob": 1.6536620250917622e-06}, {"id": 721, "seek": 394400, "start": 3961.0, "end": 3965.0, "text": " And so really, again, keep in mind what the derivative is telling us at every point in time along here.", "tokens": [50364, 407, 341, 307, 935, 1732, 1804, 732, 343, 732, 382, 731, 2771, 307, 935, 1732, 13, 400, 718, 311, 2182, 5131, 13, 50764, 50764, 7721, 279, 366, 452, 2954, 7705, 281, 646, 48256, 807, 570, 309, 311, 588, 2199, 13, 51064, 51064, 407, 586, 437, 311, 13974, 666, 613, 15277, 307, 935, 1732, 13, 51214, 51214, 400, 370, 534, 11, 797, 11, 1066, 294, 1575, 437, 264, 13760, 307, 3585, 505, 412, 633, 935, 294, 565, 2051, 510, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.2535288247717432, "compression_ratio": 1.5809523809523809, "no_speech_prob": 1.6536620250917622e-06}, {"id": 722, "seek": 396500, "start": 3965.0, "end": 3974.0, "text": " If we want the output of this neuron to increase, then the influence on these expressions is positive on the output.", "tokens": [50364, 759, 321, 528, 264, 5598, 295, 341, 34090, 281, 3488, 11, 550, 264, 6503, 322, 613, 15277, 307, 3353, 322, 264, 5598, 13, 50814, 50814, 6767, 295, 552, 366, 3353, 13150, 281, 264, 5598, 13, 51164, 51164, 407, 586, 646, 12425, 990, 281, 1783, 472, 343, 732, 700, 13, 639, 307, 257, 1413, 9984, 13, 51414, 51414, 407, 321, 458, 300, 264, 2654, 13760, 307, 264, 661, 1433, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13901907777133055, "compression_ratio": 1.5806451612903225, "no_speech_prob": 2.2958743102208246e-06}, {"id": 723, "seek": 396500, "start": 3974.0, "end": 3981.0, "text": " Both of them are positive contribution to the output.", "tokens": [50364, 759, 321, 528, 264, 5598, 295, 341, 34090, 281, 3488, 11, 550, 264, 6503, 322, 613, 15277, 307, 3353, 322, 264, 5598, 13, 50814, 50814, 6767, 295, 552, 366, 3353, 13150, 281, 264, 5598, 13, 51164, 51164, 407, 586, 646, 12425, 990, 281, 1783, 472, 343, 732, 700, 13, 639, 307, 257, 1413, 9984, 13, 51414, 51414, 407, 321, 458, 300, 264, 2654, 13760, 307, 264, 661, 1433, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13901907777133055, "compression_ratio": 1.5806451612903225, "no_speech_prob": 2.2958743102208246e-06}, {"id": 724, "seek": 396500, "start": 3981.0, "end": 3986.0, "text": " So now back propagating to X one W two first. This is a times node.", "tokens": [50364, 759, 321, 528, 264, 5598, 295, 341, 34090, 281, 3488, 11, 550, 264, 6503, 322, 613, 15277, 307, 3353, 322, 264, 5598, 13, 50814, 50814, 6767, 295, 552, 366, 3353, 13150, 281, 264, 5598, 13, 51164, 51164, 407, 586, 646, 12425, 990, 281, 1783, 472, 343, 732, 700, 13, 639, 307, 257, 1413, 9984, 13, 51414, 51414, 407, 321, 458, 300, 264, 2654, 13760, 307, 264, 661, 1433, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13901907777133055, "compression_ratio": 1.5806451612903225, "no_speech_prob": 2.2958743102208246e-06}, {"id": 725, "seek": 396500, "start": 3986.0, "end": 3989.0, "text": " So we know that the local derivative is the other term.", "tokens": [50364, 759, 321, 528, 264, 5598, 295, 341, 34090, 281, 3488, 11, 550, 264, 6503, 322, 613, 15277, 307, 3353, 322, 264, 5598, 13, 50814, 50814, 6767, 295, 552, 366, 3353, 13150, 281, 264, 5598, 13, 51164, 51164, 407, 586, 646, 12425, 990, 281, 1783, 472, 343, 732, 700, 13, 639, 307, 257, 1413, 9984, 13, 51414, 51414, 407, 321, 458, 300, 264, 2654, 13760, 307, 264, 661, 1433, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13901907777133055, "compression_ratio": 1.5806451612903225, "no_speech_prob": 2.2958743102208246e-06}, {"id": 726, "seek": 398900, "start": 3989.0, "end": 4001.0, "text": " So if we want to calculate X two dot grad, then can you think through what it's going to be?", "tokens": [50364, 407, 498, 321, 528, 281, 8873, 1783, 732, 5893, 2771, 11, 550, 393, 291, 519, 807, 437, 309, 311, 516, 281, 312, 30, 50964, 50964, 407, 2857, 2771, 486, 312, 343, 732, 5893, 1412, 1413, 13, 51214, 51214, 639, 1783, 732, 343, 732, 2771, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.23339874890385842, "compression_ratio": 1.3032786885245902, "no_speech_prob": 1.0782448953250423e-05}, {"id": 727, "seek": 398900, "start": 4001.0, "end": 4006.0, "text": " So extra grad will be W two dot data times.", "tokens": [50364, 407, 498, 321, 528, 281, 8873, 1783, 732, 5893, 2771, 11, 550, 393, 291, 519, 807, 437, 309, 311, 516, 281, 312, 30, 50964, 50964, 407, 2857, 2771, 486, 312, 343, 732, 5893, 1412, 1413, 13, 51214, 51214, 639, 1783, 732, 343, 732, 2771, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.23339874890385842, "compression_ratio": 1.3032786885245902, "no_speech_prob": 1.0782448953250423e-05}, {"id": 728, "seek": 398900, "start": 4006.0, "end": 4011.0, "text": " This X two W two grad.", "tokens": [50364, 407, 498, 321, 528, 281, 8873, 1783, 732, 5893, 2771, 11, 550, 393, 291, 519, 807, 437, 309, 311, 516, 281, 312, 30, 50964, 50964, 407, 2857, 2771, 486, 312, 343, 732, 5893, 1412, 1413, 13, 51214, 51214, 639, 1783, 732, 343, 732, 2771, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.23339874890385842, "compression_ratio": 1.3032786885245902, "no_speech_prob": 1.0782448953250423e-05}, {"id": 729, "seek": 401100, "start": 4011.0, "end": 4021.0, "text": " And W two that grad will be. X two that data times X two W two that grad.", "tokens": [50364, 400, 343, 732, 300, 2771, 486, 312, 13, 1783, 732, 300, 1412, 1413, 1783, 732, 343, 732, 300, 2771, 13, 50864, 50864, 1779, 13, 407, 300, 311, 264, 2654, 2522, 295, 5021, 4978, 13, 51164, 51164, 961, 311, 992, 552, 293, 718, 311, 2182, 5131, 13, 51314, 51314, 407, 510, 321, 536, 300, 264, 16235, 322, 527, 3364, 732, 307, 4018, 570, 1783, 732, 311, 1412, 390, 4018, 13, 51614, 51614, 1779, 13, 583, 1783, 732, 486, 362, 264, 3092, 935, 1732, 570, 1412, 510, 390, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16482753339021103, "compression_ratio": 1.7150537634408602, "no_speech_prob": 1.0615699466143269e-05}, {"id": 730, "seek": 401100, "start": 4021.0, "end": 4027.0, "text": " Right. So that's the local piece of chain rule.", "tokens": [50364, 400, 343, 732, 300, 2771, 486, 312, 13, 1783, 732, 300, 1412, 1413, 1783, 732, 343, 732, 300, 2771, 13, 50864, 50864, 1779, 13, 407, 300, 311, 264, 2654, 2522, 295, 5021, 4978, 13, 51164, 51164, 961, 311, 992, 552, 293, 718, 311, 2182, 5131, 13, 51314, 51314, 407, 510, 321, 536, 300, 264, 16235, 322, 527, 3364, 732, 307, 4018, 570, 1783, 732, 311, 1412, 390, 4018, 13, 51614, 51614, 1779, 13, 583, 1783, 732, 486, 362, 264, 3092, 935, 1732, 570, 1412, 510, 390, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16482753339021103, "compression_ratio": 1.7150537634408602, "no_speech_prob": 1.0615699466143269e-05}, {"id": 731, "seek": 401100, "start": 4027.0, "end": 4030.0, "text": " Let's set them and let's redraw.", "tokens": [50364, 400, 343, 732, 300, 2771, 486, 312, 13, 1783, 732, 300, 1412, 1413, 1783, 732, 343, 732, 300, 2771, 13, 50864, 50864, 1779, 13, 407, 300, 311, 264, 2654, 2522, 295, 5021, 4978, 13, 51164, 51164, 961, 311, 992, 552, 293, 718, 311, 2182, 5131, 13, 51314, 51314, 407, 510, 321, 536, 300, 264, 16235, 322, 527, 3364, 732, 307, 4018, 570, 1783, 732, 311, 1412, 390, 4018, 13, 51614, 51614, 1779, 13, 583, 1783, 732, 486, 362, 264, 3092, 935, 1732, 570, 1412, 510, 390, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16482753339021103, "compression_ratio": 1.7150537634408602, "no_speech_prob": 1.0615699466143269e-05}, {"id": 732, "seek": 401100, "start": 4030.0, "end": 4036.0, "text": " So here we see that the gradient on our weight two is zero because X two's data was zero.", "tokens": [50364, 400, 343, 732, 300, 2771, 486, 312, 13, 1783, 732, 300, 1412, 1413, 1783, 732, 343, 732, 300, 2771, 13, 50864, 50864, 1779, 13, 407, 300, 311, 264, 2654, 2522, 295, 5021, 4978, 13, 51164, 51164, 961, 311, 992, 552, 293, 718, 311, 2182, 5131, 13, 51314, 51314, 407, 510, 321, 536, 300, 264, 16235, 322, 527, 3364, 732, 307, 4018, 570, 1783, 732, 311, 1412, 390, 4018, 13, 51614, 51614, 1779, 13, 583, 1783, 732, 486, 362, 264, 3092, 935, 1732, 570, 1412, 510, 390, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16482753339021103, "compression_ratio": 1.7150537634408602, "no_speech_prob": 1.0615699466143269e-05}, {"id": 733, "seek": 401100, "start": 4036.0, "end": 4040.0, "text": " Right. But X two will have the green point five because data here was one.", "tokens": [50364, 400, 343, 732, 300, 2771, 486, 312, 13, 1783, 732, 300, 1412, 1413, 1783, 732, 343, 732, 300, 2771, 13, 50864, 50864, 1779, 13, 407, 300, 311, 264, 2654, 2522, 295, 5021, 4978, 13, 51164, 51164, 961, 311, 992, 552, 293, 718, 311, 2182, 5131, 13, 51314, 51314, 407, 510, 321, 536, 300, 264, 16235, 322, 527, 3364, 732, 307, 4018, 570, 1783, 732, 311, 1412, 390, 4018, 13, 51614, 51614, 1779, 13, 583, 1783, 732, 486, 362, 264, 3092, 935, 1732, 570, 1412, 510, 390, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16482753339021103, "compression_ratio": 1.7150537634408602, "no_speech_prob": 1.0615699466143269e-05}, {"id": 734, "seek": 404000, "start": 4040.0, "end": 4045.0, "text": " And so what's interesting here is because the input X two was zero,", "tokens": [50364, 400, 370, 437, 311, 1880, 510, 307, 570, 264, 4846, 1783, 732, 390, 4018, 11, 50614, 50614, 550, 570, 295, 264, 636, 264, 1413, 1985, 11, 295, 1164, 11, 341, 16235, 486, 312, 4018, 13, 50864, 50864, 400, 519, 466, 46506, 983, 300, 307, 13, 51014, 51014, 5618, 592, 1166, 1009, 5112, 505, 264, 6503, 295, 341, 322, 264, 2572, 5598, 13, 51264, 51264, 759, 286, 33377, 343, 732, 11, 577, 307, 264, 5598, 4473, 30, 51414, 51414, 467, 311, 406, 4473, 570, 321, 434, 30955, 538, 4018, 13, 51564, 51564, 407, 570, 309, 311, 406, 4473, 11, 456, 307, 572, 13760, 293, 4018, 307, 264, 3006, 1867, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08844740200886685, "compression_ratio": 1.73046875, "no_speech_prob": 4.936887307849247e-06}, {"id": 735, "seek": 404000, "start": 4045.0, "end": 4050.0, "text": " then because of the way the times works, of course, this gradient will be zero.", "tokens": [50364, 400, 370, 437, 311, 1880, 510, 307, 570, 264, 4846, 1783, 732, 390, 4018, 11, 50614, 50614, 550, 570, 295, 264, 636, 264, 1413, 1985, 11, 295, 1164, 11, 341, 16235, 486, 312, 4018, 13, 50864, 50864, 400, 519, 466, 46506, 983, 300, 307, 13, 51014, 51014, 5618, 592, 1166, 1009, 5112, 505, 264, 6503, 295, 341, 322, 264, 2572, 5598, 13, 51264, 51264, 759, 286, 33377, 343, 732, 11, 577, 307, 264, 5598, 4473, 30, 51414, 51414, 467, 311, 406, 4473, 570, 321, 434, 30955, 538, 4018, 13, 51564, 51564, 407, 570, 309, 311, 406, 4473, 11, 456, 307, 572, 13760, 293, 4018, 307, 264, 3006, 1867, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08844740200886685, "compression_ratio": 1.73046875, "no_speech_prob": 4.936887307849247e-06}, {"id": 736, "seek": 404000, "start": 4050.0, "end": 4053.0, "text": " And think about intuitively why that is.", "tokens": [50364, 400, 370, 437, 311, 1880, 510, 307, 570, 264, 4846, 1783, 732, 390, 4018, 11, 50614, 50614, 550, 570, 295, 264, 636, 264, 1413, 1985, 11, 295, 1164, 11, 341, 16235, 486, 312, 4018, 13, 50864, 50864, 400, 519, 466, 46506, 983, 300, 307, 13, 51014, 51014, 5618, 592, 1166, 1009, 5112, 505, 264, 6503, 295, 341, 322, 264, 2572, 5598, 13, 51264, 51264, 759, 286, 33377, 343, 732, 11, 577, 307, 264, 5598, 4473, 30, 51414, 51414, 467, 311, 406, 4473, 570, 321, 434, 30955, 538, 4018, 13, 51564, 51564, 407, 570, 309, 311, 406, 4473, 11, 456, 307, 572, 13760, 293, 4018, 307, 264, 3006, 1867, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08844740200886685, "compression_ratio": 1.73046875, "no_speech_prob": 4.936887307849247e-06}, {"id": 737, "seek": 404000, "start": 4053.0, "end": 4058.0, "text": " Derivative always tells us the influence of this on the final output.", "tokens": [50364, 400, 370, 437, 311, 1880, 510, 307, 570, 264, 4846, 1783, 732, 390, 4018, 11, 50614, 50614, 550, 570, 295, 264, 636, 264, 1413, 1985, 11, 295, 1164, 11, 341, 16235, 486, 312, 4018, 13, 50864, 50864, 400, 519, 466, 46506, 983, 300, 307, 13, 51014, 51014, 5618, 592, 1166, 1009, 5112, 505, 264, 6503, 295, 341, 322, 264, 2572, 5598, 13, 51264, 51264, 759, 286, 33377, 343, 732, 11, 577, 307, 264, 5598, 4473, 30, 51414, 51414, 467, 311, 406, 4473, 570, 321, 434, 30955, 538, 4018, 13, 51564, 51564, 407, 570, 309, 311, 406, 4473, 11, 456, 307, 572, 13760, 293, 4018, 307, 264, 3006, 1867, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08844740200886685, "compression_ratio": 1.73046875, "no_speech_prob": 4.936887307849247e-06}, {"id": 738, "seek": 404000, "start": 4058.0, "end": 4061.0, "text": " If I wiggle W two, how is the output changing?", "tokens": [50364, 400, 370, 437, 311, 1880, 510, 307, 570, 264, 4846, 1783, 732, 390, 4018, 11, 50614, 50614, 550, 570, 295, 264, 636, 264, 1413, 1985, 11, 295, 1164, 11, 341, 16235, 486, 312, 4018, 13, 50864, 50864, 400, 519, 466, 46506, 983, 300, 307, 13, 51014, 51014, 5618, 592, 1166, 1009, 5112, 505, 264, 6503, 295, 341, 322, 264, 2572, 5598, 13, 51264, 51264, 759, 286, 33377, 343, 732, 11, 577, 307, 264, 5598, 4473, 30, 51414, 51414, 467, 311, 406, 4473, 570, 321, 434, 30955, 538, 4018, 13, 51564, 51564, 407, 570, 309, 311, 406, 4473, 11, 456, 307, 572, 13760, 293, 4018, 307, 264, 3006, 1867, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08844740200886685, "compression_ratio": 1.73046875, "no_speech_prob": 4.936887307849247e-06}, {"id": 739, "seek": 404000, "start": 4061.0, "end": 4064.0, "text": " It's not changing because we're multiplying by zero.", "tokens": [50364, 400, 370, 437, 311, 1880, 510, 307, 570, 264, 4846, 1783, 732, 390, 4018, 11, 50614, 50614, 550, 570, 295, 264, 636, 264, 1413, 1985, 11, 295, 1164, 11, 341, 16235, 486, 312, 4018, 13, 50864, 50864, 400, 519, 466, 46506, 983, 300, 307, 13, 51014, 51014, 5618, 592, 1166, 1009, 5112, 505, 264, 6503, 295, 341, 322, 264, 2572, 5598, 13, 51264, 51264, 759, 286, 33377, 343, 732, 11, 577, 307, 264, 5598, 4473, 30, 51414, 51414, 467, 311, 406, 4473, 570, 321, 434, 30955, 538, 4018, 13, 51564, 51564, 407, 570, 309, 311, 406, 4473, 11, 456, 307, 572, 13760, 293, 4018, 307, 264, 3006, 1867, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08844740200886685, "compression_ratio": 1.73046875, "no_speech_prob": 4.936887307849247e-06}, {"id": 740, "seek": 404000, "start": 4064.0, "end": 4068.0, "text": " So because it's not changing, there is no derivative and zero is the correct answer.", "tokens": [50364, 400, 370, 437, 311, 1880, 510, 307, 570, 264, 4846, 1783, 732, 390, 4018, 11, 50614, 50614, 550, 570, 295, 264, 636, 264, 1413, 1985, 11, 295, 1164, 11, 341, 16235, 486, 312, 4018, 13, 50864, 50864, 400, 519, 466, 46506, 983, 300, 307, 13, 51014, 51014, 5618, 592, 1166, 1009, 5112, 505, 264, 6503, 295, 341, 322, 264, 2572, 5598, 13, 51264, 51264, 759, 286, 33377, 343, 732, 11, 577, 307, 264, 5598, 4473, 30, 51414, 51414, 467, 311, 406, 4473, 570, 321, 434, 30955, 538, 4018, 13, 51564, 51564, 407, 570, 309, 311, 406, 4473, 11, 456, 307, 572, 13760, 293, 4018, 307, 264, 3006, 1867, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08844740200886685, "compression_ratio": 1.73046875, "no_speech_prob": 4.936887307849247e-06}, {"id": 741, "seek": 406800, "start": 4068.0, "end": 4071.0, "text": " Because we're squashing that zero.", "tokens": [50364, 1436, 321, 434, 2339, 11077, 300, 4018, 13, 50514, 50514, 400, 718, 311, 360, 309, 510, 13, 12387, 1732, 820, 808, 510, 293, 3095, 807, 341, 1413, 13, 50814, 50814, 400, 370, 321, 603, 362, 300, 1783, 472, 300, 2771, 307, 13, 51014, 51014, 1664, 291, 519, 807, 257, 707, 857, 437, 341, 820, 312, 30, 51314, 51314, 22755, 13760, 295, 1413, 365, 3104, 281, 1783, 472, 307, 516, 281, 312, 343, 472, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1650089361728766, "compression_ratio": 1.5080213903743316, "no_speech_prob": 4.610898031387478e-05}, {"id": 742, "seek": 406800, "start": 4071.0, "end": 4077.0, "text": " And let's do it here. Point five should come here and flow through this times.", "tokens": [50364, 1436, 321, 434, 2339, 11077, 300, 4018, 13, 50514, 50514, 400, 718, 311, 360, 309, 510, 13, 12387, 1732, 820, 808, 510, 293, 3095, 807, 341, 1413, 13, 50814, 50814, 400, 370, 321, 603, 362, 300, 1783, 472, 300, 2771, 307, 13, 51014, 51014, 1664, 291, 519, 807, 257, 707, 857, 437, 341, 820, 312, 30, 51314, 51314, 22755, 13760, 295, 1413, 365, 3104, 281, 1783, 472, 307, 516, 281, 312, 343, 472, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1650089361728766, "compression_ratio": 1.5080213903743316, "no_speech_prob": 4.610898031387478e-05}, {"id": 743, "seek": 406800, "start": 4077.0, "end": 4081.0, "text": " And so we'll have that X one that grad is.", "tokens": [50364, 1436, 321, 434, 2339, 11077, 300, 4018, 13, 50514, 50514, 400, 718, 311, 360, 309, 510, 13, 12387, 1732, 820, 808, 510, 293, 3095, 807, 341, 1413, 13, 50814, 50814, 400, 370, 321, 603, 362, 300, 1783, 472, 300, 2771, 307, 13, 51014, 51014, 1664, 291, 519, 807, 257, 707, 857, 437, 341, 820, 312, 30, 51314, 51314, 22755, 13760, 295, 1413, 365, 3104, 281, 1783, 472, 307, 516, 281, 312, 343, 472, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1650089361728766, "compression_ratio": 1.5080213903743316, "no_speech_prob": 4.610898031387478e-05}, {"id": 744, "seek": 406800, "start": 4081.0, "end": 4087.0, "text": " Can you think through a little bit what this should be?", "tokens": [50364, 1436, 321, 434, 2339, 11077, 300, 4018, 13, 50514, 50514, 400, 718, 311, 360, 309, 510, 13, 12387, 1732, 820, 808, 510, 293, 3095, 807, 341, 1413, 13, 50814, 50814, 400, 370, 321, 603, 362, 300, 1783, 472, 300, 2771, 307, 13, 51014, 51014, 1664, 291, 519, 807, 257, 707, 857, 437, 341, 820, 312, 30, 51314, 51314, 22755, 13760, 295, 1413, 365, 3104, 281, 1783, 472, 307, 516, 281, 312, 343, 472, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1650089361728766, "compression_ratio": 1.5080213903743316, "no_speech_prob": 4.610898031387478e-05}, {"id": 745, "seek": 406800, "start": 4087.0, "end": 4092.0, "text": " Local derivative of times with respect to X one is going to be W one.", "tokens": [50364, 1436, 321, 434, 2339, 11077, 300, 4018, 13, 50514, 50514, 400, 718, 311, 360, 309, 510, 13, 12387, 1732, 820, 808, 510, 293, 3095, 807, 341, 1413, 13, 50814, 50814, 400, 370, 321, 603, 362, 300, 1783, 472, 300, 2771, 307, 13, 51014, 51014, 1664, 291, 519, 807, 257, 707, 857, 437, 341, 820, 312, 30, 51314, 51314, 22755, 13760, 295, 1413, 365, 3104, 281, 1783, 472, 307, 516, 281, 312, 343, 472, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1650089361728766, "compression_ratio": 1.5080213903743316, "no_speech_prob": 4.610898031387478e-05}, {"id": 746, "seek": 409200, "start": 4092.0, "end": 4098.0, "text": " W one's data times X one W one grad.", "tokens": [50364, 343, 472, 311, 1412, 1413, 1783, 472, 343, 472, 2771, 13, 50664, 50664, 400, 343, 472, 300, 2771, 486, 312, 1783, 472, 300, 1412, 1413, 1783, 472, 343, 732, 343, 472, 2771, 13, 51114, 51114, 961, 311, 536, 437, 729, 1361, 484, 281, 312, 13, 51214, 51214, 407, 341, 307, 935, 1732, 13, 407, 341, 576, 312, 3671, 472, 935, 1732, 13, 51364, 51364, 400, 341, 576, 312, 472, 13, 51464, 51464, 400, 321, 646, 12425, 770, 807, 341, 6114, 13, 1981, 366, 264, 3539, 2572, 33733, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.172203426775725, "compression_ratio": 1.7944444444444445, "no_speech_prob": 8.397474630328361e-06}, {"id": 747, "seek": 409200, "start": 4098.0, "end": 4107.0, "text": " And W one that grad will be X one that data times X one W two W one grad.", "tokens": [50364, 343, 472, 311, 1412, 1413, 1783, 472, 343, 472, 2771, 13, 50664, 50664, 400, 343, 472, 300, 2771, 486, 312, 1783, 472, 300, 1412, 1413, 1783, 472, 343, 732, 343, 472, 2771, 13, 51114, 51114, 961, 311, 536, 437, 729, 1361, 484, 281, 312, 13, 51214, 51214, 407, 341, 307, 935, 1732, 13, 407, 341, 576, 312, 3671, 472, 935, 1732, 13, 51364, 51364, 400, 341, 576, 312, 472, 13, 51464, 51464, 400, 321, 646, 12425, 770, 807, 341, 6114, 13, 1981, 366, 264, 3539, 2572, 33733, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.172203426775725, "compression_ratio": 1.7944444444444445, "no_speech_prob": 8.397474630328361e-06}, {"id": 748, "seek": 409200, "start": 4107.0, "end": 4109.0, "text": " Let's see what those came out to be.", "tokens": [50364, 343, 472, 311, 1412, 1413, 1783, 472, 343, 472, 2771, 13, 50664, 50664, 400, 343, 472, 300, 2771, 486, 312, 1783, 472, 300, 1412, 1413, 1783, 472, 343, 732, 343, 472, 2771, 13, 51114, 51114, 961, 311, 536, 437, 729, 1361, 484, 281, 312, 13, 51214, 51214, 407, 341, 307, 935, 1732, 13, 407, 341, 576, 312, 3671, 472, 935, 1732, 13, 51364, 51364, 400, 341, 576, 312, 472, 13, 51464, 51464, 400, 321, 646, 12425, 770, 807, 341, 6114, 13, 1981, 366, 264, 3539, 2572, 33733, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.172203426775725, "compression_ratio": 1.7944444444444445, "no_speech_prob": 8.397474630328361e-06}, {"id": 749, "seek": 409200, "start": 4109.0, "end": 4112.0, "text": " So this is point five. So this would be negative one point five.", "tokens": [50364, 343, 472, 311, 1412, 1413, 1783, 472, 343, 472, 2771, 13, 50664, 50664, 400, 343, 472, 300, 2771, 486, 312, 1783, 472, 300, 1412, 1413, 1783, 472, 343, 732, 343, 472, 2771, 13, 51114, 51114, 961, 311, 536, 437, 729, 1361, 484, 281, 312, 13, 51214, 51214, 407, 341, 307, 935, 1732, 13, 407, 341, 576, 312, 3671, 472, 935, 1732, 13, 51364, 51364, 400, 341, 576, 312, 472, 13, 51464, 51464, 400, 321, 646, 12425, 770, 807, 341, 6114, 13, 1981, 366, 264, 3539, 2572, 33733, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.172203426775725, "compression_ratio": 1.7944444444444445, "no_speech_prob": 8.397474630328361e-06}, {"id": 750, "seek": 409200, "start": 4112.0, "end": 4114.0, "text": " And this would be one.", "tokens": [50364, 343, 472, 311, 1412, 1413, 1783, 472, 343, 472, 2771, 13, 50664, 50664, 400, 343, 472, 300, 2771, 486, 312, 1783, 472, 300, 1412, 1413, 1783, 472, 343, 732, 343, 472, 2771, 13, 51114, 51114, 961, 311, 536, 437, 729, 1361, 484, 281, 312, 13, 51214, 51214, 407, 341, 307, 935, 1732, 13, 407, 341, 576, 312, 3671, 472, 935, 1732, 13, 51364, 51364, 400, 341, 576, 312, 472, 13, 51464, 51464, 400, 321, 646, 12425, 770, 807, 341, 6114, 13, 1981, 366, 264, 3539, 2572, 33733, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.172203426775725, "compression_ratio": 1.7944444444444445, "no_speech_prob": 8.397474630328361e-06}, {"id": 751, "seek": 409200, "start": 4114.0, "end": 4119.0, "text": " And we back propagated through this expression. These are the actual final derivatives.", "tokens": [50364, 343, 472, 311, 1412, 1413, 1783, 472, 343, 472, 2771, 13, 50664, 50664, 400, 343, 472, 300, 2771, 486, 312, 1783, 472, 300, 1412, 1413, 1783, 472, 343, 732, 343, 472, 2771, 13, 51114, 51114, 961, 311, 536, 437, 729, 1361, 484, 281, 312, 13, 51214, 51214, 407, 341, 307, 935, 1732, 13, 407, 341, 576, 312, 3671, 472, 935, 1732, 13, 51364, 51364, 400, 341, 576, 312, 472, 13, 51464, 51464, 400, 321, 646, 12425, 770, 807, 341, 6114, 13, 1981, 366, 264, 3539, 2572, 33733, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.172203426775725, "compression_ratio": 1.7944444444444445, "no_speech_prob": 8.397474630328361e-06}, {"id": 752, "seek": 411900, "start": 4119.0, "end": 4129.0, "text": " So if we want this neurons output to increase, we know that what's necessary is that W two, we have no gradient.", "tokens": [50364, 407, 498, 321, 528, 341, 22027, 5598, 281, 3488, 11, 321, 458, 300, 437, 311, 4818, 307, 300, 343, 732, 11, 321, 362, 572, 16235, 13, 50864, 50864, 343, 732, 1177, 380, 767, 1871, 281, 341, 34090, 558, 586, 13, 51014, 51014, 583, 341, 34090, 11, 341, 3364, 820, 352, 493, 13, 51164, 51164, 407, 341, 3364, 1709, 493, 11, 550, 341, 22027, 5598, 576, 362, 2780, 493, 293, 16068, 379, 570, 264, 16235, 307, 472, 13, 51514, 51514, 2264, 11, 370, 884, 264, 646, 38377, 16945, 307, 2745, 11083, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11293489556563528, "compression_ratio": 1.7555555555555555, "no_speech_prob": 1.4738578101969324e-05}, {"id": 753, "seek": 411900, "start": 4129.0, "end": 4132.0, "text": " W two doesn't actually matter to this neuron right now.", "tokens": [50364, 407, 498, 321, 528, 341, 22027, 5598, 281, 3488, 11, 321, 458, 300, 437, 311, 4818, 307, 300, 343, 732, 11, 321, 362, 572, 16235, 13, 50864, 50864, 343, 732, 1177, 380, 767, 1871, 281, 341, 34090, 558, 586, 13, 51014, 51014, 583, 341, 34090, 11, 341, 3364, 820, 352, 493, 13, 51164, 51164, 407, 341, 3364, 1709, 493, 11, 550, 341, 22027, 5598, 576, 362, 2780, 493, 293, 16068, 379, 570, 264, 16235, 307, 472, 13, 51514, 51514, 2264, 11, 370, 884, 264, 646, 38377, 16945, 307, 2745, 11083, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11293489556563528, "compression_ratio": 1.7555555555555555, "no_speech_prob": 1.4738578101969324e-05}, {"id": 754, "seek": 411900, "start": 4132.0, "end": 4135.0, "text": " But this neuron, this weight should go up.", "tokens": [50364, 407, 498, 321, 528, 341, 22027, 5598, 281, 3488, 11, 321, 458, 300, 437, 311, 4818, 307, 300, 343, 732, 11, 321, 362, 572, 16235, 13, 50864, 50864, 343, 732, 1177, 380, 767, 1871, 281, 341, 34090, 558, 586, 13, 51014, 51014, 583, 341, 34090, 11, 341, 3364, 820, 352, 493, 13, 51164, 51164, 407, 341, 3364, 1709, 493, 11, 550, 341, 22027, 5598, 576, 362, 2780, 493, 293, 16068, 379, 570, 264, 16235, 307, 472, 13, 51514, 51514, 2264, 11, 370, 884, 264, 646, 38377, 16945, 307, 2745, 11083, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11293489556563528, "compression_ratio": 1.7555555555555555, "no_speech_prob": 1.4738578101969324e-05}, {"id": 755, "seek": 411900, "start": 4135.0, "end": 4142.0, "text": " So this weight goes up, then this neurons output would have gone up and proportionally because the gradient is one.", "tokens": [50364, 407, 498, 321, 528, 341, 22027, 5598, 281, 3488, 11, 321, 458, 300, 437, 311, 4818, 307, 300, 343, 732, 11, 321, 362, 572, 16235, 13, 50864, 50864, 343, 732, 1177, 380, 767, 1871, 281, 341, 34090, 558, 586, 13, 51014, 51014, 583, 341, 34090, 11, 341, 3364, 820, 352, 493, 13, 51164, 51164, 407, 341, 3364, 1709, 493, 11, 550, 341, 22027, 5598, 576, 362, 2780, 493, 293, 16068, 379, 570, 264, 16235, 307, 472, 13, 51514, 51514, 2264, 11, 370, 884, 264, 646, 38377, 16945, 307, 2745, 11083, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11293489556563528, "compression_ratio": 1.7555555555555555, "no_speech_prob": 1.4738578101969324e-05}, {"id": 756, "seek": 411900, "start": 4142.0, "end": 4145.0, "text": " OK, so doing the back propagation manually is obviously ridiculous.", "tokens": [50364, 407, 498, 321, 528, 341, 22027, 5598, 281, 3488, 11, 321, 458, 300, 437, 311, 4818, 307, 300, 343, 732, 11, 321, 362, 572, 16235, 13, 50864, 50864, 343, 732, 1177, 380, 767, 1871, 281, 341, 34090, 558, 586, 13, 51014, 51014, 583, 341, 34090, 11, 341, 3364, 820, 352, 493, 13, 51164, 51164, 407, 341, 3364, 1709, 493, 11, 550, 341, 22027, 5598, 576, 362, 2780, 493, 293, 16068, 379, 570, 264, 16235, 307, 472, 13, 51514, 51514, 2264, 11, 370, 884, 264, 646, 38377, 16945, 307, 2745, 11083, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11293489556563528, "compression_ratio": 1.7555555555555555, "no_speech_prob": 1.4738578101969324e-05}, {"id": 757, "seek": 414500, "start": 4145.0, "end": 4152.0, "text": " So we are now going to put an end to this suffering and we're going to see how we can implement the backward pass a bit more automatically.", "tokens": [50364, 407, 321, 366, 586, 516, 281, 829, 364, 917, 281, 341, 7755, 293, 321, 434, 516, 281, 536, 577, 321, 393, 4445, 264, 23897, 1320, 257, 857, 544, 6772, 13, 50714, 50714, 492, 434, 406, 516, 281, 312, 884, 439, 295, 309, 16945, 484, 510, 13, 50864, 50864, 467, 311, 586, 1238, 6322, 281, 505, 538, 1365, 577, 613, 1804, 279, 293, 1413, 366, 646, 12425, 990, 2771, 2448, 13, 51114, 51114, 407, 718, 311, 352, 493, 281, 264, 2158, 2657, 293, 321, 434, 516, 281, 722, 17656, 5489, 437, 321, 600, 1612, 294, 264, 5110, 2507, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.043213026196348904, "compression_ratio": 1.6945606694560669, "no_speech_prob": 4.0293043639394455e-06}, {"id": 758, "seek": 414500, "start": 4152.0, "end": 4155.0, "text": " We're not going to be doing all of it manually out here.", "tokens": [50364, 407, 321, 366, 586, 516, 281, 829, 364, 917, 281, 341, 7755, 293, 321, 434, 516, 281, 536, 577, 321, 393, 4445, 264, 23897, 1320, 257, 857, 544, 6772, 13, 50714, 50714, 492, 434, 406, 516, 281, 312, 884, 439, 295, 309, 16945, 484, 510, 13, 50864, 50864, 467, 311, 586, 1238, 6322, 281, 505, 538, 1365, 577, 613, 1804, 279, 293, 1413, 366, 646, 12425, 990, 2771, 2448, 13, 51114, 51114, 407, 718, 311, 352, 493, 281, 264, 2158, 2657, 293, 321, 434, 516, 281, 722, 17656, 5489, 437, 321, 600, 1612, 294, 264, 5110, 2507, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.043213026196348904, "compression_ratio": 1.6945606694560669, "no_speech_prob": 4.0293043639394455e-06}, {"id": 759, "seek": 414500, "start": 4155.0, "end": 4160.0, "text": " It's now pretty obvious to us by example how these pluses and times are back propagating gradients.", "tokens": [50364, 407, 321, 366, 586, 516, 281, 829, 364, 917, 281, 341, 7755, 293, 321, 434, 516, 281, 536, 577, 321, 393, 4445, 264, 23897, 1320, 257, 857, 544, 6772, 13, 50714, 50714, 492, 434, 406, 516, 281, 312, 884, 439, 295, 309, 16945, 484, 510, 13, 50864, 50864, 467, 311, 586, 1238, 6322, 281, 505, 538, 1365, 577, 613, 1804, 279, 293, 1413, 366, 646, 12425, 990, 2771, 2448, 13, 51114, 51114, 407, 718, 311, 352, 493, 281, 264, 2158, 2657, 293, 321, 434, 516, 281, 722, 17656, 5489, 437, 321, 600, 1612, 294, 264, 5110, 2507, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.043213026196348904, "compression_ratio": 1.6945606694560669, "no_speech_prob": 4.0293043639394455e-06}, {"id": 760, "seek": 414500, "start": 4160.0, "end": 4169.0, "text": " So let's go up to the value object and we're going to start codifying what we've seen in the examples below.", "tokens": [50364, 407, 321, 366, 586, 516, 281, 829, 364, 917, 281, 341, 7755, 293, 321, 434, 516, 281, 536, 577, 321, 393, 4445, 264, 23897, 1320, 257, 857, 544, 6772, 13, 50714, 50714, 492, 434, 406, 516, 281, 312, 884, 439, 295, 309, 16945, 484, 510, 13, 50864, 50864, 467, 311, 586, 1238, 6322, 281, 505, 538, 1365, 577, 613, 1804, 279, 293, 1413, 366, 646, 12425, 990, 2771, 2448, 13, 51114, 51114, 407, 718, 311, 352, 493, 281, 264, 2158, 2657, 293, 321, 434, 516, 281, 722, 17656, 5489, 437, 321, 600, 1612, 294, 264, 5110, 2507, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.043213026196348904, "compression_ratio": 1.6945606694560669, "no_speech_prob": 4.0293043639394455e-06}, {"id": 761, "seek": 416900, "start": 4169.0, "end": 4176.0, "text": " So we're going to do this by storing a special self dot backward and underscore backward.", "tokens": [50364, 407, 321, 434, 516, 281, 360, 341, 538, 26085, 257, 2121, 2698, 5893, 23897, 293, 37556, 23897, 13, 50714, 50714, 400, 341, 486, 312, 257, 2445, 597, 307, 516, 281, 360, 300, 707, 2522, 295, 5021, 4978, 412, 1184, 707, 9984, 300, 1890, 15743, 293, 7126, 5598, 13, 51164, 51164, 492, 434, 516, 281, 3531, 577, 321, 366, 516, 281, 5021, 264, 23930, 16235, 666, 264, 15743, 2771, 2448, 13, 51514, 51514, 407, 538, 7576, 11, 341, 486, 312, 257, 2445, 300, 1177, 380, 360, 1340, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06392475798889831, "compression_ratio": 1.8215962441314555, "no_speech_prob": 3.5558550735004246e-06}, {"id": 762, "seek": 416900, "start": 4176.0, "end": 4185.0, "text": " And this will be a function which is going to do that little piece of chain rule at each little node that took inputs and produced output.", "tokens": [50364, 407, 321, 434, 516, 281, 360, 341, 538, 26085, 257, 2121, 2698, 5893, 23897, 293, 37556, 23897, 13, 50714, 50714, 400, 341, 486, 312, 257, 2445, 597, 307, 516, 281, 360, 300, 707, 2522, 295, 5021, 4978, 412, 1184, 707, 9984, 300, 1890, 15743, 293, 7126, 5598, 13, 51164, 51164, 492, 434, 516, 281, 3531, 577, 321, 366, 516, 281, 5021, 264, 23930, 16235, 666, 264, 15743, 2771, 2448, 13, 51514, 51514, 407, 538, 7576, 11, 341, 486, 312, 257, 2445, 300, 1177, 380, 360, 1340, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06392475798889831, "compression_ratio": 1.8215962441314555, "no_speech_prob": 3.5558550735004246e-06}, {"id": 763, "seek": 416900, "start": 4185.0, "end": 4192.0, "text": " We're going to store how we are going to chain the outputs gradient into the inputs gradients.", "tokens": [50364, 407, 321, 434, 516, 281, 360, 341, 538, 26085, 257, 2121, 2698, 5893, 23897, 293, 37556, 23897, 13, 50714, 50714, 400, 341, 486, 312, 257, 2445, 597, 307, 516, 281, 360, 300, 707, 2522, 295, 5021, 4978, 412, 1184, 707, 9984, 300, 1890, 15743, 293, 7126, 5598, 13, 51164, 51164, 492, 434, 516, 281, 3531, 577, 321, 366, 516, 281, 5021, 264, 23930, 16235, 666, 264, 15743, 2771, 2448, 13, 51514, 51514, 407, 538, 7576, 11, 341, 486, 312, 257, 2445, 300, 1177, 380, 360, 1340, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06392475798889831, "compression_ratio": 1.8215962441314555, "no_speech_prob": 3.5558550735004246e-06}, {"id": 764, "seek": 416900, "start": 4192.0, "end": 4198.0, "text": " So by default, this will be a function that doesn't do anything.", "tokens": [50364, 407, 321, 434, 516, 281, 360, 341, 538, 26085, 257, 2121, 2698, 5893, 23897, 293, 37556, 23897, 13, 50714, 50714, 400, 341, 486, 312, 257, 2445, 597, 307, 516, 281, 360, 300, 707, 2522, 295, 5021, 4978, 412, 1184, 707, 9984, 300, 1890, 15743, 293, 7126, 5598, 13, 51164, 51164, 492, 434, 516, 281, 3531, 577, 321, 366, 516, 281, 5021, 264, 23930, 16235, 666, 264, 15743, 2771, 2448, 13, 51514, 51514, 407, 538, 7576, 11, 341, 486, 312, 257, 2445, 300, 1177, 380, 360, 1340, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06392475798889831, "compression_ratio": 1.8215962441314555, "no_speech_prob": 3.5558550735004246e-06}, {"id": 765, "seek": 419800, "start": 4198.0, "end": 4203.0, "text": " So you can also see that here in the value in micro grad.", "tokens": [50364, 407, 291, 393, 611, 536, 300, 510, 294, 264, 2158, 294, 4532, 2771, 13, 50614, 50614, 407, 365, 341, 23897, 2445, 538, 7576, 1177, 380, 360, 1340, 13, 50864, 50864, 639, 307, 364, 6707, 2445, 13, 50964, 50964, 400, 300, 576, 312, 1333, 295, 264, 1389, 11, 337, 1365, 11, 337, 257, 10871, 9984, 13, 51064, 51064, 1171, 10871, 9984, 11, 456, 311, 1825, 281, 360, 13, 51214, 51214, 583, 586, 498, 562, 321, 434, 4084, 613, 484, 4190, 11, 613, 484, 4190, 366, 364, 4500, 295, 2698, 293, 661, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1303843247263055, "compression_ratio": 1.6221198156682028, "no_speech_prob": 7.183014076872496e-06}, {"id": 766, "seek": 419800, "start": 4203.0, "end": 4208.0, "text": " So with this backward function by default doesn't do anything.", "tokens": [50364, 407, 291, 393, 611, 536, 300, 510, 294, 264, 2158, 294, 4532, 2771, 13, 50614, 50614, 407, 365, 341, 23897, 2445, 538, 7576, 1177, 380, 360, 1340, 13, 50864, 50864, 639, 307, 364, 6707, 2445, 13, 50964, 50964, 400, 300, 576, 312, 1333, 295, 264, 1389, 11, 337, 1365, 11, 337, 257, 10871, 9984, 13, 51064, 51064, 1171, 10871, 9984, 11, 456, 311, 1825, 281, 360, 13, 51214, 51214, 583, 586, 498, 562, 321, 434, 4084, 613, 484, 4190, 11, 613, 484, 4190, 366, 364, 4500, 295, 2698, 293, 661, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1303843247263055, "compression_ratio": 1.6221198156682028, "no_speech_prob": 7.183014076872496e-06}, {"id": 767, "seek": 419800, "start": 4208.0, "end": 4210.0, "text": " This is an empty function.", "tokens": [50364, 407, 291, 393, 611, 536, 300, 510, 294, 264, 2158, 294, 4532, 2771, 13, 50614, 50614, 407, 365, 341, 23897, 2445, 538, 7576, 1177, 380, 360, 1340, 13, 50864, 50864, 639, 307, 364, 6707, 2445, 13, 50964, 50964, 400, 300, 576, 312, 1333, 295, 264, 1389, 11, 337, 1365, 11, 337, 257, 10871, 9984, 13, 51064, 51064, 1171, 10871, 9984, 11, 456, 311, 1825, 281, 360, 13, 51214, 51214, 583, 586, 498, 562, 321, 434, 4084, 613, 484, 4190, 11, 613, 484, 4190, 366, 364, 4500, 295, 2698, 293, 661, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1303843247263055, "compression_ratio": 1.6221198156682028, "no_speech_prob": 7.183014076872496e-06}, {"id": 768, "seek": 419800, "start": 4210.0, "end": 4212.0, "text": " And that would be sort of the case, for example, for a leaf node.", "tokens": [50364, 407, 291, 393, 611, 536, 300, 510, 294, 264, 2158, 294, 4532, 2771, 13, 50614, 50614, 407, 365, 341, 23897, 2445, 538, 7576, 1177, 380, 360, 1340, 13, 50864, 50864, 639, 307, 364, 6707, 2445, 13, 50964, 50964, 400, 300, 576, 312, 1333, 295, 264, 1389, 11, 337, 1365, 11, 337, 257, 10871, 9984, 13, 51064, 51064, 1171, 10871, 9984, 11, 456, 311, 1825, 281, 360, 13, 51214, 51214, 583, 586, 498, 562, 321, 434, 4084, 613, 484, 4190, 11, 613, 484, 4190, 366, 364, 4500, 295, 2698, 293, 661, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1303843247263055, "compression_ratio": 1.6221198156682028, "no_speech_prob": 7.183014076872496e-06}, {"id": 769, "seek": 419800, "start": 4212.0, "end": 4215.0, "text": " For leaf node, there's nothing to do.", "tokens": [50364, 407, 291, 393, 611, 536, 300, 510, 294, 264, 2158, 294, 4532, 2771, 13, 50614, 50614, 407, 365, 341, 23897, 2445, 538, 7576, 1177, 380, 360, 1340, 13, 50864, 50864, 639, 307, 364, 6707, 2445, 13, 50964, 50964, 400, 300, 576, 312, 1333, 295, 264, 1389, 11, 337, 1365, 11, 337, 257, 10871, 9984, 13, 51064, 51064, 1171, 10871, 9984, 11, 456, 311, 1825, 281, 360, 13, 51214, 51214, 583, 586, 498, 562, 321, 434, 4084, 613, 484, 4190, 11, 613, 484, 4190, 366, 364, 4500, 295, 2698, 293, 661, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1303843247263055, "compression_ratio": 1.6221198156682028, "no_speech_prob": 7.183014076872496e-06}, {"id": 770, "seek": 419800, "start": 4215.0, "end": 4224.0, "text": " But now if when we're creating these out values, these out values are an addition of self and other.", "tokens": [50364, 407, 291, 393, 611, 536, 300, 510, 294, 264, 2158, 294, 4532, 2771, 13, 50614, 50614, 407, 365, 341, 23897, 2445, 538, 7576, 1177, 380, 360, 1340, 13, 50864, 50864, 639, 307, 364, 6707, 2445, 13, 50964, 50964, 400, 300, 576, 312, 1333, 295, 264, 1389, 11, 337, 1365, 11, 337, 257, 10871, 9984, 13, 51064, 51064, 1171, 10871, 9984, 11, 456, 311, 1825, 281, 360, 13, 51214, 51214, 583, 586, 498, 562, 321, 434, 4084, 613, 484, 4190, 11, 613, 484, 4190, 366, 364, 4500, 295, 2698, 293, 661, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1303843247263055, "compression_ratio": 1.6221198156682028, "no_speech_prob": 7.183014076872496e-06}, {"id": 771, "seek": 422400, "start": 4224.0, "end": 4234.0, "text": " And so we will want to sell set outs backward to be the function that propagates the gradient.", "tokens": [50364, 400, 370, 321, 486, 528, 281, 3607, 992, 14758, 23897, 281, 312, 264, 2445, 300, 12425, 1024, 264, 16235, 13, 50864, 50864, 407, 718, 311, 6964, 437, 820, 1051, 13, 51164, 51164, 400, 321, 434, 516, 281, 3531, 309, 294, 257, 24653, 13, 51264, 51264, 961, 311, 6964, 437, 820, 1051, 562, 321, 818, 484, 2771, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.10514665040813509, "compression_ratio": 1.5547945205479452, "no_speech_prob": 4.425425686349627e-06}, {"id": 772, "seek": 422400, "start": 4234.0, "end": 4240.0, "text": " So let's define what should happen.", "tokens": [50364, 400, 370, 321, 486, 528, 281, 3607, 992, 14758, 23897, 281, 312, 264, 2445, 300, 12425, 1024, 264, 16235, 13, 50864, 50864, 407, 718, 311, 6964, 437, 820, 1051, 13, 51164, 51164, 400, 321, 434, 516, 281, 3531, 309, 294, 257, 24653, 13, 51264, 51264, 961, 311, 6964, 437, 820, 1051, 562, 321, 818, 484, 2771, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.10514665040813509, "compression_ratio": 1.5547945205479452, "no_speech_prob": 4.425425686349627e-06}, {"id": 773, "seek": 422400, "start": 4240.0, "end": 4242.0, "text": " And we're going to store it in a closure.", "tokens": [50364, 400, 370, 321, 486, 528, 281, 3607, 992, 14758, 23897, 281, 312, 264, 2445, 300, 12425, 1024, 264, 16235, 13, 50864, 50864, 407, 718, 311, 6964, 437, 820, 1051, 13, 51164, 51164, 400, 321, 434, 516, 281, 3531, 309, 294, 257, 24653, 13, 51264, 51264, 961, 311, 6964, 437, 820, 1051, 562, 321, 818, 484, 2771, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.10514665040813509, "compression_ratio": 1.5547945205479452, "no_speech_prob": 4.425425686349627e-06}, {"id": 774, "seek": 422400, "start": 4242.0, "end": 4247.0, "text": " Let's define what should happen when we call out grad.", "tokens": [50364, 400, 370, 321, 486, 528, 281, 3607, 992, 14758, 23897, 281, 312, 264, 2445, 300, 12425, 1024, 264, 16235, 13, 50864, 50864, 407, 718, 311, 6964, 437, 820, 1051, 13, 51164, 51164, 400, 321, 434, 516, 281, 3531, 309, 294, 257, 24653, 13, 51264, 51264, 961, 311, 6964, 437, 820, 1051, 562, 321, 818, 484, 2771, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.10514665040813509, "compression_ratio": 1.5547945205479452, "no_speech_prob": 4.425425686349627e-06}, {"id": 775, "seek": 424700, "start": 4247.0, "end": 4257.0, "text": " For addition, our job is to take out grad and propagate it into self grad and other grad.", "tokens": [50364, 1171, 4500, 11, 527, 1691, 307, 281, 747, 484, 2771, 293, 48256, 309, 666, 2698, 2771, 293, 661, 2771, 13, 50864, 50864, 407, 1936, 321, 528, 281, 3607, 2698, 2771, 281, 746, 293, 321, 528, 281, 992, 2357, 2771, 281, 746, 13, 51214, 51214, 400, 264, 636, 321, 1866, 2507, 577, 5021, 4978, 1985, 11, 321, 528, 281, 747, 264, 2654, 13760, 1413, 264, 1333, 295, 4338, 13760, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14917654533908792, "compression_ratio": 1.7745664739884393, "no_speech_prob": 1.4063648450246546e-05}, {"id": 776, "seek": 424700, "start": 4257.0, "end": 4264.0, "text": " So basically we want to sell self grad to something and we want to set others grad to something.", "tokens": [50364, 1171, 4500, 11, 527, 1691, 307, 281, 747, 484, 2771, 293, 48256, 309, 666, 2698, 2771, 293, 661, 2771, 13, 50864, 50864, 407, 1936, 321, 528, 281, 3607, 2698, 2771, 281, 746, 293, 321, 528, 281, 992, 2357, 2771, 281, 746, 13, 51214, 51214, 400, 264, 636, 321, 1866, 2507, 577, 5021, 4978, 1985, 11, 321, 528, 281, 747, 264, 2654, 13760, 1413, 264, 1333, 295, 4338, 13760, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14917654533908792, "compression_ratio": 1.7745664739884393, "no_speech_prob": 1.4063648450246546e-05}, {"id": 777, "seek": 424700, "start": 4264.0, "end": 4274.0, "text": " And the way we saw below how chain rule works, we want to take the local derivative times the sort of global derivative,", "tokens": [50364, 1171, 4500, 11, 527, 1691, 307, 281, 747, 484, 2771, 293, 48256, 309, 666, 2698, 2771, 293, 661, 2771, 13, 50864, 50864, 407, 1936, 321, 528, 281, 3607, 2698, 2771, 281, 746, 293, 321, 528, 281, 992, 2357, 2771, 281, 746, 13, 51214, 51214, 400, 264, 636, 321, 1866, 2507, 577, 5021, 4978, 1985, 11, 321, 528, 281, 747, 264, 2654, 13760, 1413, 264, 1333, 295, 4338, 13760, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14917654533908792, "compression_ratio": 1.7745664739884393, "no_speech_prob": 1.4063648450246546e-05}, {"id": 778, "seek": 427400, "start": 4274.0, "end": 4281.0, "text": " I should call it, which is the derivative of the final output of the expression with respect to outs data.", "tokens": [50364, 286, 820, 818, 309, 11, 597, 307, 264, 13760, 295, 264, 2572, 5598, 295, 264, 6114, 365, 3104, 281, 14758, 1412, 13, 50714, 50714, 39079, 281, 484, 13, 50814, 50814, 407, 264, 2654, 13760, 295, 2698, 294, 364, 4500, 307, 472, 935, 4018, 13, 51114, 51114, 407, 309, 311, 445, 472, 935, 4018, 1413, 14758, 2771, 13, 51364, 51364, 663, 311, 264, 5021, 4978, 13, 51464, 51464, 400, 2357, 300, 2771, 486, 312, 472, 935, 4018, 1413, 2771, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13238062341529203, "compression_ratio": 1.723756906077348, "no_speech_prob": 6.7479618337529246e-06}, {"id": 779, "seek": 427400, "start": 4281.0, "end": 4283.0, "text": " Respect to out.", "tokens": [50364, 286, 820, 818, 309, 11, 597, 307, 264, 13760, 295, 264, 2572, 5598, 295, 264, 6114, 365, 3104, 281, 14758, 1412, 13, 50714, 50714, 39079, 281, 484, 13, 50814, 50814, 407, 264, 2654, 13760, 295, 2698, 294, 364, 4500, 307, 472, 935, 4018, 13, 51114, 51114, 407, 309, 311, 445, 472, 935, 4018, 1413, 14758, 2771, 13, 51364, 51364, 663, 311, 264, 5021, 4978, 13, 51464, 51464, 400, 2357, 300, 2771, 486, 312, 472, 935, 4018, 1413, 2771, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13238062341529203, "compression_ratio": 1.723756906077348, "no_speech_prob": 6.7479618337529246e-06}, {"id": 780, "seek": 427400, "start": 4283.0, "end": 4289.0, "text": " So the local derivative of self in an addition is one point zero.", "tokens": [50364, 286, 820, 818, 309, 11, 597, 307, 264, 13760, 295, 264, 2572, 5598, 295, 264, 6114, 365, 3104, 281, 14758, 1412, 13, 50714, 50714, 39079, 281, 484, 13, 50814, 50814, 407, 264, 2654, 13760, 295, 2698, 294, 364, 4500, 307, 472, 935, 4018, 13, 51114, 51114, 407, 309, 311, 445, 472, 935, 4018, 1413, 14758, 2771, 13, 51364, 51364, 663, 311, 264, 5021, 4978, 13, 51464, 51464, 400, 2357, 300, 2771, 486, 312, 472, 935, 4018, 1413, 2771, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13238062341529203, "compression_ratio": 1.723756906077348, "no_speech_prob": 6.7479618337529246e-06}, {"id": 781, "seek": 427400, "start": 4289.0, "end": 4294.0, "text": " So it's just one point zero times outs grad.", "tokens": [50364, 286, 820, 818, 309, 11, 597, 307, 264, 13760, 295, 264, 2572, 5598, 295, 264, 6114, 365, 3104, 281, 14758, 1412, 13, 50714, 50714, 39079, 281, 484, 13, 50814, 50814, 407, 264, 2654, 13760, 295, 2698, 294, 364, 4500, 307, 472, 935, 4018, 13, 51114, 51114, 407, 309, 311, 445, 472, 935, 4018, 1413, 14758, 2771, 13, 51364, 51364, 663, 311, 264, 5021, 4978, 13, 51464, 51464, 400, 2357, 300, 2771, 486, 312, 472, 935, 4018, 1413, 2771, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13238062341529203, "compression_ratio": 1.723756906077348, "no_speech_prob": 6.7479618337529246e-06}, {"id": 782, "seek": 427400, "start": 4294.0, "end": 4296.0, "text": " That's the chain rule.", "tokens": [50364, 286, 820, 818, 309, 11, 597, 307, 264, 13760, 295, 264, 2572, 5598, 295, 264, 6114, 365, 3104, 281, 14758, 1412, 13, 50714, 50714, 39079, 281, 484, 13, 50814, 50814, 407, 264, 2654, 13760, 295, 2698, 294, 364, 4500, 307, 472, 935, 4018, 13, 51114, 51114, 407, 309, 311, 445, 472, 935, 4018, 1413, 14758, 2771, 13, 51364, 51364, 663, 311, 264, 5021, 4978, 13, 51464, 51464, 400, 2357, 300, 2771, 486, 312, 472, 935, 4018, 1413, 2771, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13238062341529203, "compression_ratio": 1.723756906077348, "no_speech_prob": 6.7479618337529246e-06}, {"id": 783, "seek": 427400, "start": 4296.0, "end": 4299.0, "text": " And others that grad will be one point zero times grad.", "tokens": [50364, 286, 820, 818, 309, 11, 597, 307, 264, 13760, 295, 264, 2572, 5598, 295, 264, 6114, 365, 3104, 281, 14758, 1412, 13, 50714, 50714, 39079, 281, 484, 13, 50814, 50814, 407, 264, 2654, 13760, 295, 2698, 294, 364, 4500, 307, 472, 935, 4018, 13, 51114, 51114, 407, 309, 311, 445, 472, 935, 4018, 1413, 14758, 2771, 13, 51364, 51364, 663, 311, 264, 5021, 4978, 13, 51464, 51464, 400, 2357, 300, 2771, 486, 312, 472, 935, 4018, 1413, 2771, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13238062341529203, "compression_ratio": 1.723756906077348, "no_speech_prob": 6.7479618337529246e-06}, {"id": 784, "seek": 429900, "start": 4299.0, "end": 4310.0, "text": " And what you basically what you're seeing here is that outs grad will simply be copied onto self grad and others grad, as we saw happens for an addition operation.", "tokens": [50364, 400, 437, 291, 1936, 437, 291, 434, 2577, 510, 307, 300, 14758, 2771, 486, 2935, 312, 25365, 3911, 2698, 2771, 293, 2357, 2771, 11, 382, 321, 1866, 2314, 337, 364, 4500, 6916, 13, 50914, 50914, 407, 321, 434, 516, 281, 1780, 818, 341, 2445, 281, 48256, 264, 16235, 11, 1419, 1096, 364, 4500, 13, 51214, 51214, 961, 311, 406, 360, 27290, 13, 492, 434, 516, 281, 611, 6964, 300, 23897, 13, 51514, 51514, 400, 321, 434, 516, 281, 992, 1080, 23897, 281, 312, 23897, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11817174547174003, "compression_ratio": 1.7477064220183487, "no_speech_prob": 3.0415183118748246e-06}, {"id": 785, "seek": 429900, "start": 4310.0, "end": 4316.0, "text": " So we're going to later call this function to propagate the gradient, having done an addition.", "tokens": [50364, 400, 437, 291, 1936, 437, 291, 434, 2577, 510, 307, 300, 14758, 2771, 486, 2935, 312, 25365, 3911, 2698, 2771, 293, 2357, 2771, 11, 382, 321, 1866, 2314, 337, 364, 4500, 6916, 13, 50914, 50914, 407, 321, 434, 516, 281, 1780, 818, 341, 2445, 281, 48256, 264, 16235, 11, 1419, 1096, 364, 4500, 13, 51214, 51214, 961, 311, 406, 360, 27290, 13, 492, 434, 516, 281, 611, 6964, 300, 23897, 13, 51514, 51514, 400, 321, 434, 516, 281, 992, 1080, 23897, 281, 312, 23897, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11817174547174003, "compression_ratio": 1.7477064220183487, "no_speech_prob": 3.0415183118748246e-06}, {"id": 786, "seek": 429900, "start": 4316.0, "end": 4322.0, "text": " Let's not do multiplication. We're going to also define that backward.", "tokens": [50364, 400, 437, 291, 1936, 437, 291, 434, 2577, 510, 307, 300, 14758, 2771, 486, 2935, 312, 25365, 3911, 2698, 2771, 293, 2357, 2771, 11, 382, 321, 1866, 2314, 337, 364, 4500, 6916, 13, 50914, 50914, 407, 321, 434, 516, 281, 1780, 818, 341, 2445, 281, 48256, 264, 16235, 11, 1419, 1096, 364, 4500, 13, 51214, 51214, 961, 311, 406, 360, 27290, 13, 492, 434, 516, 281, 611, 6964, 300, 23897, 13, 51514, 51514, 400, 321, 434, 516, 281, 992, 1080, 23897, 281, 312, 23897, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11817174547174003, "compression_ratio": 1.7477064220183487, "no_speech_prob": 3.0415183118748246e-06}, {"id": 787, "seek": 429900, "start": 4322.0, "end": 4328.0, "text": " And we're going to set its backward to be backward.", "tokens": [50364, 400, 437, 291, 1936, 437, 291, 434, 2577, 510, 307, 300, 14758, 2771, 486, 2935, 312, 25365, 3911, 2698, 2771, 293, 2357, 2771, 11, 382, 321, 1866, 2314, 337, 364, 4500, 6916, 13, 50914, 50914, 407, 321, 434, 516, 281, 1780, 818, 341, 2445, 281, 48256, 264, 16235, 11, 1419, 1096, 364, 4500, 13, 51214, 51214, 961, 311, 406, 360, 27290, 13, 492, 434, 516, 281, 611, 6964, 300, 23897, 13, 51514, 51514, 400, 321, 434, 516, 281, 992, 1080, 23897, 281, 312, 23897, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11817174547174003, "compression_ratio": 1.7477064220183487, "no_speech_prob": 3.0415183118748246e-06}, {"id": 788, "seek": 432800, "start": 4328.0, "end": 4337.0, "text": " And we want to chain out grad into self grad and others grad.", "tokens": [50364, 400, 321, 528, 281, 5021, 484, 2771, 666, 2698, 2771, 293, 2357, 2771, 13, 50814, 50814, 400, 341, 486, 312, 257, 707, 2522, 295, 5021, 4978, 337, 27290, 13, 50964, 50964, 407, 321, 603, 362, 13, 407, 437, 820, 341, 312, 30, 51114, 51114, 1664, 291, 519, 807, 30, 51364, 51364, 407, 437, 307, 264, 2654, 13760, 510, 30, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12666255708724733, "compression_ratio": 1.4645161290322581, "no_speech_prob": 9.817663340072613e-06}, {"id": 789, "seek": 432800, "start": 4337.0, "end": 4340.0, "text": " And this will be a little piece of chain rule for multiplication.", "tokens": [50364, 400, 321, 528, 281, 5021, 484, 2771, 666, 2698, 2771, 293, 2357, 2771, 13, 50814, 50814, 400, 341, 486, 312, 257, 707, 2522, 295, 5021, 4978, 337, 27290, 13, 50964, 50964, 407, 321, 603, 362, 13, 407, 437, 820, 341, 312, 30, 51114, 51114, 1664, 291, 519, 807, 30, 51364, 51364, 407, 437, 307, 264, 2654, 13760, 510, 30, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12666255708724733, "compression_ratio": 1.4645161290322581, "no_speech_prob": 9.817663340072613e-06}, {"id": 790, "seek": 432800, "start": 4340.0, "end": 4343.0, "text": " So we'll have. So what should this be?", "tokens": [50364, 400, 321, 528, 281, 5021, 484, 2771, 666, 2698, 2771, 293, 2357, 2771, 13, 50814, 50814, 400, 341, 486, 312, 257, 707, 2522, 295, 5021, 4978, 337, 27290, 13, 50964, 50964, 407, 321, 603, 362, 13, 407, 437, 820, 341, 312, 30, 51114, 51114, 1664, 291, 519, 807, 30, 51364, 51364, 407, 437, 307, 264, 2654, 13760, 510, 30, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12666255708724733, "compression_ratio": 1.4645161290322581, "no_speech_prob": 9.817663340072613e-06}, {"id": 791, "seek": 432800, "start": 4343.0, "end": 4348.0, "text": " Can you think through?", "tokens": [50364, 400, 321, 528, 281, 5021, 484, 2771, 666, 2698, 2771, 293, 2357, 2771, 13, 50814, 50814, 400, 341, 486, 312, 257, 707, 2522, 295, 5021, 4978, 337, 27290, 13, 50964, 50964, 407, 321, 603, 362, 13, 407, 437, 820, 341, 312, 30, 51114, 51114, 1664, 291, 519, 807, 30, 51364, 51364, 407, 437, 307, 264, 2654, 13760, 510, 30, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12666255708724733, "compression_ratio": 1.4645161290322581, "no_speech_prob": 9.817663340072613e-06}, {"id": 792, "seek": 432800, "start": 4348.0, "end": 4351.0, "text": " So what is the local derivative here?", "tokens": [50364, 400, 321, 528, 281, 5021, 484, 2771, 666, 2698, 2771, 293, 2357, 2771, 13, 50814, 50814, 400, 341, 486, 312, 257, 707, 2522, 295, 5021, 4978, 337, 27290, 13, 50964, 50964, 407, 321, 603, 362, 13, 407, 437, 820, 341, 312, 30, 51114, 51114, 1664, 291, 519, 807, 30, 51364, 51364, 407, 437, 307, 264, 2654, 13760, 510, 30, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12666255708724733, "compression_ratio": 1.4645161290322581, "no_speech_prob": 9.817663340072613e-06}, {"id": 793, "seek": 435100, "start": 4351.0, "end": 4360.0, "text": " The local derivative was others that data and then others that data and the times out grad.", "tokens": [50364, 440, 2654, 13760, 390, 2357, 300, 1412, 293, 550, 2357, 300, 1412, 293, 264, 1413, 484, 2771, 13, 50814, 50814, 663, 311, 5021, 4978, 13, 50914, 50914, 400, 510, 321, 362, 2698, 300, 1412, 1413, 484, 2771, 13, 51064, 51064, 663, 311, 437, 321, 600, 668, 884, 13, 51264, 51264, 400, 2721, 11, 510, 337, 1266, 71, 11, 300, 23897, 13, 51514, 51514, 400, 550, 321, 528, 281, 992, 14758, 12204, 281, 312, 445, 23897, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19067970514297486, "compression_ratio": 1.7682926829268293, "no_speech_prob": 2.6840011742024217e-06}, {"id": 794, "seek": 435100, "start": 4360.0, "end": 4362.0, "text": " That's chain rule.", "tokens": [50364, 440, 2654, 13760, 390, 2357, 300, 1412, 293, 550, 2357, 300, 1412, 293, 264, 1413, 484, 2771, 13, 50814, 50814, 663, 311, 5021, 4978, 13, 50914, 50914, 400, 510, 321, 362, 2698, 300, 1412, 1413, 484, 2771, 13, 51064, 51064, 663, 311, 437, 321, 600, 668, 884, 13, 51264, 51264, 400, 2721, 11, 510, 337, 1266, 71, 11, 300, 23897, 13, 51514, 51514, 400, 550, 321, 528, 281, 992, 14758, 12204, 281, 312, 445, 23897, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19067970514297486, "compression_ratio": 1.7682926829268293, "no_speech_prob": 2.6840011742024217e-06}, {"id": 795, "seek": 435100, "start": 4362.0, "end": 4365.0, "text": " And here we have self that data times out grad.", "tokens": [50364, 440, 2654, 13760, 390, 2357, 300, 1412, 293, 550, 2357, 300, 1412, 293, 264, 1413, 484, 2771, 13, 50814, 50814, 663, 311, 5021, 4978, 13, 50914, 50914, 400, 510, 321, 362, 2698, 300, 1412, 1413, 484, 2771, 13, 51064, 51064, 663, 311, 437, 321, 600, 668, 884, 13, 51264, 51264, 400, 2721, 11, 510, 337, 1266, 71, 11, 300, 23897, 13, 51514, 51514, 400, 550, 321, 528, 281, 992, 14758, 12204, 281, 312, 445, 23897, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19067970514297486, "compression_ratio": 1.7682926829268293, "no_speech_prob": 2.6840011742024217e-06}, {"id": 796, "seek": 435100, "start": 4365.0, "end": 4369.0, "text": " That's what we've been doing.", "tokens": [50364, 440, 2654, 13760, 390, 2357, 300, 1412, 293, 550, 2357, 300, 1412, 293, 264, 1413, 484, 2771, 13, 50814, 50814, 663, 311, 5021, 4978, 13, 50914, 50914, 400, 510, 321, 362, 2698, 300, 1412, 1413, 484, 2771, 13, 51064, 51064, 663, 311, 437, 321, 600, 668, 884, 13, 51264, 51264, 400, 2721, 11, 510, 337, 1266, 71, 11, 300, 23897, 13, 51514, 51514, 400, 550, 321, 528, 281, 992, 14758, 12204, 281, 312, 445, 23897, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19067970514297486, "compression_ratio": 1.7682926829268293, "no_speech_prob": 2.6840011742024217e-06}, {"id": 797, "seek": 435100, "start": 4369.0, "end": 4374.0, "text": " And finally, here for 10h, that backward.", "tokens": [50364, 440, 2654, 13760, 390, 2357, 300, 1412, 293, 550, 2357, 300, 1412, 293, 264, 1413, 484, 2771, 13, 50814, 50814, 663, 311, 5021, 4978, 13, 50914, 50914, 400, 510, 321, 362, 2698, 300, 1412, 1413, 484, 2771, 13, 51064, 51064, 663, 311, 437, 321, 600, 668, 884, 13, 51264, 51264, 400, 2721, 11, 510, 337, 1266, 71, 11, 300, 23897, 13, 51514, 51514, 400, 550, 321, 528, 281, 992, 14758, 12204, 281, 312, 445, 23897, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19067970514297486, "compression_ratio": 1.7682926829268293, "no_speech_prob": 2.6840011742024217e-06}, {"id": 798, "seek": 435100, "start": 4374.0, "end": 4380.0, "text": " And then we want to set outs backwards to be just backward.", "tokens": [50364, 440, 2654, 13760, 390, 2357, 300, 1412, 293, 550, 2357, 300, 1412, 293, 264, 1413, 484, 2771, 13, 50814, 50814, 663, 311, 5021, 4978, 13, 50914, 50914, 400, 510, 321, 362, 2698, 300, 1412, 1413, 484, 2771, 13, 51064, 51064, 663, 311, 437, 321, 600, 668, 884, 13, 51264, 51264, 400, 2721, 11, 510, 337, 1266, 71, 11, 300, 23897, 13, 51514, 51514, 400, 550, 321, 528, 281, 992, 14758, 12204, 281, 312, 445, 23897, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19067970514297486, "compression_ratio": 1.7682926829268293, "no_speech_prob": 2.6840011742024217e-06}, {"id": 799, "seek": 438000, "start": 4380.0, "end": 4383.0, "text": " And here we need to back propagate.", "tokens": [50364, 400, 510, 321, 643, 281, 646, 48256, 13, 50514, 50514, 492, 362, 484, 2771, 293, 321, 528, 281, 5021, 309, 666, 5139, 2771, 13, 50814, 50814, 400, 5139, 2771, 486, 312, 264, 2654, 13760, 295, 341, 6916, 300, 321, 600, 1096, 510, 11, 597, 307, 1266, 71, 13, 51164, 51164, 400, 370, 321, 1866, 300, 264, 2654, 16235, 307, 472, 3175, 264, 1266, 71, 295, 2031, 8889, 11, 597, 510, 307, 256, 13, 51514, 51514, 663, 311, 264, 2654, 13760, 570, 300, 311, 256, 307, 264, 5598, 295, 341, 1266, 71, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12792118390401205, "compression_ratio": 1.7121951219512195, "no_speech_prob": 2.1782061594421975e-05}, {"id": 800, "seek": 438000, "start": 4383.0, "end": 4389.0, "text": " We have out grad and we want to chain it into salt grad.", "tokens": [50364, 400, 510, 321, 643, 281, 646, 48256, 13, 50514, 50514, 492, 362, 484, 2771, 293, 321, 528, 281, 5021, 309, 666, 5139, 2771, 13, 50814, 50814, 400, 5139, 2771, 486, 312, 264, 2654, 13760, 295, 341, 6916, 300, 321, 600, 1096, 510, 11, 597, 307, 1266, 71, 13, 51164, 51164, 400, 370, 321, 1866, 300, 264, 2654, 16235, 307, 472, 3175, 264, 1266, 71, 295, 2031, 8889, 11, 597, 510, 307, 256, 13, 51514, 51514, 663, 311, 264, 2654, 13760, 570, 300, 311, 256, 307, 264, 5598, 295, 341, 1266, 71, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12792118390401205, "compression_ratio": 1.7121951219512195, "no_speech_prob": 2.1782061594421975e-05}, {"id": 801, "seek": 438000, "start": 4389.0, "end": 4396.0, "text": " And salt grad will be the local derivative of this operation that we've done here, which is 10h.", "tokens": [50364, 400, 510, 321, 643, 281, 646, 48256, 13, 50514, 50514, 492, 362, 484, 2771, 293, 321, 528, 281, 5021, 309, 666, 5139, 2771, 13, 50814, 50814, 400, 5139, 2771, 486, 312, 264, 2654, 13760, 295, 341, 6916, 300, 321, 600, 1096, 510, 11, 597, 307, 1266, 71, 13, 51164, 51164, 400, 370, 321, 1866, 300, 264, 2654, 16235, 307, 472, 3175, 264, 1266, 71, 295, 2031, 8889, 11, 597, 510, 307, 256, 13, 51514, 51514, 663, 311, 264, 2654, 13760, 570, 300, 311, 256, 307, 264, 5598, 295, 341, 1266, 71, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12792118390401205, "compression_ratio": 1.7121951219512195, "no_speech_prob": 2.1782061594421975e-05}, {"id": 802, "seek": 438000, "start": 4396.0, "end": 4403.0, "text": " And so we saw that the local gradient is one minus the 10h of x squared, which here is t.", "tokens": [50364, 400, 510, 321, 643, 281, 646, 48256, 13, 50514, 50514, 492, 362, 484, 2771, 293, 321, 528, 281, 5021, 309, 666, 5139, 2771, 13, 50814, 50814, 400, 5139, 2771, 486, 312, 264, 2654, 13760, 295, 341, 6916, 300, 321, 600, 1096, 510, 11, 597, 307, 1266, 71, 13, 51164, 51164, 400, 370, 321, 1866, 300, 264, 2654, 16235, 307, 472, 3175, 264, 1266, 71, 295, 2031, 8889, 11, 597, 510, 307, 256, 13, 51514, 51514, 663, 311, 264, 2654, 13760, 570, 300, 311, 256, 307, 264, 5598, 295, 341, 1266, 71, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12792118390401205, "compression_ratio": 1.7121951219512195, "no_speech_prob": 2.1782061594421975e-05}, {"id": 803, "seek": 438000, "start": 4403.0, "end": 4407.0, "text": " That's the local derivative because that's t is the output of this 10h.", "tokens": [50364, 400, 510, 321, 643, 281, 646, 48256, 13, 50514, 50514, 492, 362, 484, 2771, 293, 321, 528, 281, 5021, 309, 666, 5139, 2771, 13, 50814, 50814, 400, 5139, 2771, 486, 312, 264, 2654, 13760, 295, 341, 6916, 300, 321, 600, 1096, 510, 11, 597, 307, 1266, 71, 13, 51164, 51164, 400, 370, 321, 1866, 300, 264, 2654, 16235, 307, 472, 3175, 264, 1266, 71, 295, 2031, 8889, 11, 597, 510, 307, 256, 13, 51514, 51514, 663, 311, 264, 2654, 13760, 570, 300, 311, 256, 307, 264, 5598, 295, 341, 1266, 71, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12792118390401205, "compression_ratio": 1.7121951219512195, "no_speech_prob": 2.1782061594421975e-05}, {"id": 804, "seek": 440700, "start": 4407.0, "end": 4410.0, "text": " So one minus t squared is the local derivative.", "tokens": [50364, 407, 472, 3175, 256, 8889, 307, 264, 2654, 13760, 13, 50514, 50514, 400, 550, 16235, 575, 281, 312, 17207, 570, 295, 264, 5021, 4978, 13, 50764, 50764, 407, 484, 2771, 307, 417, 3563, 807, 264, 2654, 16235, 666, 5139, 2771, 13, 50964, 50964, 400, 300, 820, 312, 1936, 309, 13, 51064, 51064, 407, 321, 434, 516, 281, 38818, 533, 527, 2158, 9984, 13, 51264, 51264, 492, 434, 516, 281, 11173, 439, 264, 636, 760, 510, 13, 51414, 51414, 400, 321, 434, 516, 281, 38818, 533, 527, 6114, 11, 652, 988, 300, 439, 264, 2771, 82, 366, 4018, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10175537595561907, "compression_ratio": 1.7685185185185186, "no_speech_prob": 6.643268079642439e-06}, {"id": 805, "seek": 440700, "start": 4410.0, "end": 4415.0, "text": " And then gradient has to be multiplied because of the chain rule.", "tokens": [50364, 407, 472, 3175, 256, 8889, 307, 264, 2654, 13760, 13, 50514, 50514, 400, 550, 16235, 575, 281, 312, 17207, 570, 295, 264, 5021, 4978, 13, 50764, 50764, 407, 484, 2771, 307, 417, 3563, 807, 264, 2654, 16235, 666, 5139, 2771, 13, 50964, 50964, 400, 300, 820, 312, 1936, 309, 13, 51064, 51064, 407, 321, 434, 516, 281, 38818, 533, 527, 2158, 9984, 13, 51264, 51264, 492, 434, 516, 281, 11173, 439, 264, 636, 760, 510, 13, 51414, 51414, 400, 321, 434, 516, 281, 38818, 533, 527, 6114, 11, 652, 988, 300, 439, 264, 2771, 82, 366, 4018, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10175537595561907, "compression_ratio": 1.7685185185185186, "no_speech_prob": 6.643268079642439e-06}, {"id": 806, "seek": 440700, "start": 4415.0, "end": 4419.0, "text": " So out grad is chained through the local gradient into salt grad.", "tokens": [50364, 407, 472, 3175, 256, 8889, 307, 264, 2654, 13760, 13, 50514, 50514, 400, 550, 16235, 575, 281, 312, 17207, 570, 295, 264, 5021, 4978, 13, 50764, 50764, 407, 484, 2771, 307, 417, 3563, 807, 264, 2654, 16235, 666, 5139, 2771, 13, 50964, 50964, 400, 300, 820, 312, 1936, 309, 13, 51064, 51064, 407, 321, 434, 516, 281, 38818, 533, 527, 2158, 9984, 13, 51264, 51264, 492, 434, 516, 281, 11173, 439, 264, 636, 760, 510, 13, 51414, 51414, 400, 321, 434, 516, 281, 38818, 533, 527, 6114, 11, 652, 988, 300, 439, 264, 2771, 82, 366, 4018, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10175537595561907, "compression_ratio": 1.7685185185185186, "no_speech_prob": 6.643268079642439e-06}, {"id": 807, "seek": 440700, "start": 4419.0, "end": 4421.0, "text": " And that should be basically it.", "tokens": [50364, 407, 472, 3175, 256, 8889, 307, 264, 2654, 13760, 13, 50514, 50514, 400, 550, 16235, 575, 281, 312, 17207, 570, 295, 264, 5021, 4978, 13, 50764, 50764, 407, 484, 2771, 307, 417, 3563, 807, 264, 2654, 16235, 666, 5139, 2771, 13, 50964, 50964, 400, 300, 820, 312, 1936, 309, 13, 51064, 51064, 407, 321, 434, 516, 281, 38818, 533, 527, 2158, 9984, 13, 51264, 51264, 492, 434, 516, 281, 11173, 439, 264, 636, 760, 510, 13, 51414, 51414, 400, 321, 434, 516, 281, 38818, 533, 527, 6114, 11, 652, 988, 300, 439, 264, 2771, 82, 366, 4018, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10175537595561907, "compression_ratio": 1.7685185185185186, "no_speech_prob": 6.643268079642439e-06}, {"id": 808, "seek": 440700, "start": 4421.0, "end": 4425.0, "text": " So we're going to redefine our value node.", "tokens": [50364, 407, 472, 3175, 256, 8889, 307, 264, 2654, 13760, 13, 50514, 50514, 400, 550, 16235, 575, 281, 312, 17207, 570, 295, 264, 5021, 4978, 13, 50764, 50764, 407, 484, 2771, 307, 417, 3563, 807, 264, 2654, 16235, 666, 5139, 2771, 13, 50964, 50964, 400, 300, 820, 312, 1936, 309, 13, 51064, 51064, 407, 321, 434, 516, 281, 38818, 533, 527, 2158, 9984, 13, 51264, 51264, 492, 434, 516, 281, 11173, 439, 264, 636, 760, 510, 13, 51414, 51414, 400, 321, 434, 516, 281, 38818, 533, 527, 6114, 11, 652, 988, 300, 439, 264, 2771, 82, 366, 4018, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10175537595561907, "compression_ratio": 1.7685185185185186, "no_speech_prob": 6.643268079642439e-06}, {"id": 809, "seek": 440700, "start": 4425.0, "end": 4428.0, "text": " We're going to swing all the way down here.", "tokens": [50364, 407, 472, 3175, 256, 8889, 307, 264, 2654, 13760, 13, 50514, 50514, 400, 550, 16235, 575, 281, 312, 17207, 570, 295, 264, 5021, 4978, 13, 50764, 50764, 407, 484, 2771, 307, 417, 3563, 807, 264, 2654, 16235, 666, 5139, 2771, 13, 50964, 50964, 400, 300, 820, 312, 1936, 309, 13, 51064, 51064, 407, 321, 434, 516, 281, 38818, 533, 527, 2158, 9984, 13, 51264, 51264, 492, 434, 516, 281, 11173, 439, 264, 636, 760, 510, 13, 51414, 51414, 400, 321, 434, 516, 281, 38818, 533, 527, 6114, 11, 652, 988, 300, 439, 264, 2771, 82, 366, 4018, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10175537595561907, "compression_ratio": 1.7685185185185186, "no_speech_prob": 6.643268079642439e-06}, {"id": 810, "seek": 440700, "start": 4428.0, "end": 4435.0, "text": " And we're going to redefine our expression, make sure that all the grads are zero.", "tokens": [50364, 407, 472, 3175, 256, 8889, 307, 264, 2654, 13760, 13, 50514, 50514, 400, 550, 16235, 575, 281, 312, 17207, 570, 295, 264, 5021, 4978, 13, 50764, 50764, 407, 484, 2771, 307, 417, 3563, 807, 264, 2654, 16235, 666, 5139, 2771, 13, 50964, 50964, 400, 300, 820, 312, 1936, 309, 13, 51064, 51064, 407, 321, 434, 516, 281, 38818, 533, 527, 2158, 9984, 13, 51264, 51264, 492, 434, 516, 281, 11173, 439, 264, 636, 760, 510, 13, 51414, 51414, 400, 321, 434, 516, 281, 38818, 533, 527, 6114, 11, 652, 988, 300, 439, 264, 2771, 82, 366, 4018, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10175537595561907, "compression_ratio": 1.7685185185185186, "no_speech_prob": 6.643268079642439e-06}, {"id": 811, "seek": 443500, "start": 4435.0, "end": 4440.0, "text": " OK, but now we don't have to do this manually anymore.", "tokens": [50364, 2264, 11, 457, 586, 321, 500, 380, 362, 281, 360, 341, 16945, 3602, 13, 50614, 50614, 492, 366, 516, 281, 1936, 312, 5141, 264, 5893, 23897, 294, 264, 558, 1668, 13, 50814, 50814, 407, 700, 11, 321, 528, 281, 818, 484, 300, 23897, 13, 51314, 51314, 407, 422, 390, 264, 9700, 295, 1266, 71, 13, 51514, 51514, 1779, 13, 407, 5141, 729, 23897, 486, 312, 341, 2445, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.15801174110836452, "compression_ratio": 1.5449101796407185, "no_speech_prob": 1.5445803001057357e-05}, {"id": 812, "seek": 443500, "start": 4440.0, "end": 4444.0, "text": " We are going to basically be calling the dot backward in the right order.", "tokens": [50364, 2264, 11, 457, 586, 321, 500, 380, 362, 281, 360, 341, 16945, 3602, 13, 50614, 50614, 492, 366, 516, 281, 1936, 312, 5141, 264, 5893, 23897, 294, 264, 558, 1668, 13, 50814, 50814, 407, 700, 11, 321, 528, 281, 818, 484, 300, 23897, 13, 51314, 51314, 407, 422, 390, 264, 9700, 295, 1266, 71, 13, 51514, 51514, 1779, 13, 407, 5141, 729, 23897, 486, 312, 341, 2445, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.15801174110836452, "compression_ratio": 1.5449101796407185, "no_speech_prob": 1.5445803001057357e-05}, {"id": 813, "seek": 443500, "start": 4444.0, "end": 4454.0, "text": " So first, we want to call out that backward.", "tokens": [50364, 2264, 11, 457, 586, 321, 500, 380, 362, 281, 360, 341, 16945, 3602, 13, 50614, 50614, 492, 366, 516, 281, 1936, 312, 5141, 264, 5893, 23897, 294, 264, 558, 1668, 13, 50814, 50814, 407, 700, 11, 321, 528, 281, 818, 484, 300, 23897, 13, 51314, 51314, 407, 422, 390, 264, 9700, 295, 1266, 71, 13, 51514, 51514, 1779, 13, 407, 5141, 729, 23897, 486, 312, 341, 2445, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.15801174110836452, "compression_ratio": 1.5449101796407185, "no_speech_prob": 1.5445803001057357e-05}, {"id": 814, "seek": 443500, "start": 4454.0, "end": 4458.0, "text": " So O was the outcome of 10h.", "tokens": [50364, 2264, 11, 457, 586, 321, 500, 380, 362, 281, 360, 341, 16945, 3602, 13, 50614, 50614, 492, 366, 516, 281, 1936, 312, 5141, 264, 5893, 23897, 294, 264, 558, 1668, 13, 50814, 50814, 407, 700, 11, 321, 528, 281, 818, 484, 300, 23897, 13, 51314, 51314, 407, 422, 390, 264, 9700, 295, 1266, 71, 13, 51514, 51514, 1779, 13, 407, 5141, 729, 23897, 486, 312, 341, 2445, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.15801174110836452, "compression_ratio": 1.5449101796407185, "no_speech_prob": 1.5445803001057357e-05}, {"id": 815, "seek": 443500, "start": 4458.0, "end": 4464.0, "text": " Right. So calling those backward will be this function.", "tokens": [50364, 2264, 11, 457, 586, 321, 500, 380, 362, 281, 360, 341, 16945, 3602, 13, 50614, 50614, 492, 366, 516, 281, 1936, 312, 5141, 264, 5893, 23897, 294, 264, 558, 1668, 13, 50814, 50814, 407, 700, 11, 321, 528, 281, 818, 484, 300, 23897, 13, 51314, 51314, 407, 422, 390, 264, 9700, 295, 1266, 71, 13, 51514, 51514, 1779, 13, 407, 5141, 729, 23897, 486, 312, 341, 2445, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.15801174110836452, "compression_ratio": 1.5449101796407185, "no_speech_prob": 1.5445803001057357e-05}, {"id": 816, "seek": 446400, "start": 4464.0, "end": 4466.0, "text": " This is what it will do.", "tokens": [50364, 639, 307, 437, 309, 486, 360, 13, 50464, 50464, 823, 321, 362, 281, 312, 5026, 570, 456, 311, 257, 1413, 484, 5893, 2771, 13, 50764, 50764, 400, 484, 5893, 2771, 11, 1604, 11, 307, 5883, 1602, 281, 4018, 13, 51114, 51114, 407, 510, 321, 536, 2771, 4018, 13, 51214, 51214, 407, 382, 257, 3096, 1389, 11, 321, 643, 281, 992, 29450, 5893, 2771, 281, 502, 13, 15, 281, 5883, 1125, 341, 365, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12513312315329528, "compression_ratio": 1.5527950310559007, "no_speech_prob": 6.4386931626359e-06}, {"id": 817, "seek": 446400, "start": 4466.0, "end": 4472.0, "text": " Now we have to be careful because there's a times out dot grad.", "tokens": [50364, 639, 307, 437, 309, 486, 360, 13, 50464, 50464, 823, 321, 362, 281, 312, 5026, 570, 456, 311, 257, 1413, 484, 5893, 2771, 13, 50764, 50764, 400, 484, 5893, 2771, 11, 1604, 11, 307, 5883, 1602, 281, 4018, 13, 51114, 51114, 407, 510, 321, 536, 2771, 4018, 13, 51214, 51214, 407, 382, 257, 3096, 1389, 11, 321, 643, 281, 992, 29450, 5893, 2771, 281, 502, 13, 15, 281, 5883, 1125, 341, 365, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12513312315329528, "compression_ratio": 1.5527950310559007, "no_speech_prob": 6.4386931626359e-06}, {"id": 818, "seek": 446400, "start": 4472.0, "end": 4479.0, "text": " And out dot grad, remember, is initialized to zero.", "tokens": [50364, 639, 307, 437, 309, 486, 360, 13, 50464, 50464, 823, 321, 362, 281, 312, 5026, 570, 456, 311, 257, 1413, 484, 5893, 2771, 13, 50764, 50764, 400, 484, 5893, 2771, 11, 1604, 11, 307, 5883, 1602, 281, 4018, 13, 51114, 51114, 407, 510, 321, 536, 2771, 4018, 13, 51214, 51214, 407, 382, 257, 3096, 1389, 11, 321, 643, 281, 992, 29450, 5893, 2771, 281, 502, 13, 15, 281, 5883, 1125, 341, 365, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12513312315329528, "compression_ratio": 1.5527950310559007, "no_speech_prob": 6.4386931626359e-06}, {"id": 819, "seek": 446400, "start": 4479.0, "end": 4481.0, "text": " So here we see grad zero.", "tokens": [50364, 639, 307, 437, 309, 486, 360, 13, 50464, 50464, 823, 321, 362, 281, 312, 5026, 570, 456, 311, 257, 1413, 484, 5893, 2771, 13, 50764, 50764, 400, 484, 5893, 2771, 11, 1604, 11, 307, 5883, 1602, 281, 4018, 13, 51114, 51114, 407, 510, 321, 536, 2771, 4018, 13, 51214, 51214, 407, 382, 257, 3096, 1389, 11, 321, 643, 281, 992, 29450, 5893, 2771, 281, 502, 13, 15, 281, 5883, 1125, 341, 365, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12513312315329528, "compression_ratio": 1.5527950310559007, "no_speech_prob": 6.4386931626359e-06}, {"id": 820, "seek": 446400, "start": 4481.0, "end": 4493.0, "text": " So as a base case, we need to set oath dot grad to 1.0 to initialize this with one.", "tokens": [50364, 639, 307, 437, 309, 486, 360, 13, 50464, 50464, 823, 321, 362, 281, 312, 5026, 570, 456, 311, 257, 1413, 484, 5893, 2771, 13, 50764, 50764, 400, 484, 5893, 2771, 11, 1604, 11, 307, 5883, 1602, 281, 4018, 13, 51114, 51114, 407, 510, 321, 536, 2771, 4018, 13, 51214, 51214, 407, 382, 257, 3096, 1389, 11, 321, 643, 281, 992, 29450, 5893, 2771, 281, 502, 13, 15, 281, 5883, 1125, 341, 365, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12513312315329528, "compression_ratio": 1.5527950310559007, "no_speech_prob": 6.4386931626359e-06}, {"id": 821, "seek": 449300, "start": 4493.0, "end": 4497.0, "text": " And then once this is one, we can call out that backward.", "tokens": [50364, 400, 550, 1564, 341, 307, 472, 11, 321, 393, 818, 484, 300, 23897, 13, 50564, 50564, 400, 437, 300, 820, 360, 307, 309, 820, 48256, 341, 2771, 807, 1266, 71, 13, 50814, 50814, 407, 264, 2654, 13760, 1413, 264, 4338, 13760, 11, 597, 307, 5883, 1602, 412, 472, 13, 51064, 51064, 407, 341, 820, 13, 51514, 51514, 407, 286, 1194, 466, 29956, 278, 309, 11, 457, 286, 8932, 286, 820, 445, 1856, 264, 6713, 294, 510, 570, 309, 311, 1238, 4074, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08379147773565249, "compression_ratio": 1.580188679245283, "no_speech_prob": 4.495095708989538e-06}, {"id": 822, "seek": 449300, "start": 4497.0, "end": 4502.0, "text": " And what that should do is it should propagate this grad through 10h.", "tokens": [50364, 400, 550, 1564, 341, 307, 472, 11, 321, 393, 818, 484, 300, 23897, 13, 50564, 50564, 400, 437, 300, 820, 360, 307, 309, 820, 48256, 341, 2771, 807, 1266, 71, 13, 50814, 50814, 407, 264, 2654, 13760, 1413, 264, 4338, 13760, 11, 597, 307, 5883, 1602, 412, 472, 13, 51064, 51064, 407, 341, 820, 13, 51514, 51514, 407, 286, 1194, 466, 29956, 278, 309, 11, 457, 286, 8932, 286, 820, 445, 1856, 264, 6713, 294, 510, 570, 309, 311, 1238, 4074, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08379147773565249, "compression_ratio": 1.580188679245283, "no_speech_prob": 4.495095708989538e-06}, {"id": 823, "seek": 449300, "start": 4502.0, "end": 4507.0, "text": " So the local derivative times the global derivative, which is initialized at one.", "tokens": [50364, 400, 550, 1564, 341, 307, 472, 11, 321, 393, 818, 484, 300, 23897, 13, 50564, 50564, 400, 437, 300, 820, 360, 307, 309, 820, 48256, 341, 2771, 807, 1266, 71, 13, 50814, 50814, 407, 264, 2654, 13760, 1413, 264, 4338, 13760, 11, 597, 307, 5883, 1602, 412, 472, 13, 51064, 51064, 407, 341, 820, 13, 51514, 51514, 407, 286, 1194, 466, 29956, 278, 309, 11, 457, 286, 8932, 286, 820, 445, 1856, 264, 6713, 294, 510, 570, 309, 311, 1238, 4074, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08379147773565249, "compression_ratio": 1.580188679245283, "no_speech_prob": 4.495095708989538e-06}, {"id": 824, "seek": 449300, "start": 4507.0, "end": 4516.0, "text": " So this should.", "tokens": [50364, 400, 550, 1564, 341, 307, 472, 11, 321, 393, 818, 484, 300, 23897, 13, 50564, 50564, 400, 437, 300, 820, 360, 307, 309, 820, 48256, 341, 2771, 807, 1266, 71, 13, 50814, 50814, 407, 264, 2654, 13760, 1413, 264, 4338, 13760, 11, 597, 307, 5883, 1602, 412, 472, 13, 51064, 51064, 407, 341, 820, 13, 51514, 51514, 407, 286, 1194, 466, 29956, 278, 309, 11, 457, 286, 8932, 286, 820, 445, 1856, 264, 6713, 294, 510, 570, 309, 311, 1238, 4074, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08379147773565249, "compression_ratio": 1.580188679245283, "no_speech_prob": 4.495095708989538e-06}, {"id": 825, "seek": 449300, "start": 4516.0, "end": 4522.0, "text": " So I thought about redoing it, but I figured I should just leave the error in here because it's pretty funny.", "tokens": [50364, 400, 550, 1564, 341, 307, 472, 11, 321, 393, 818, 484, 300, 23897, 13, 50564, 50564, 400, 437, 300, 820, 360, 307, 309, 820, 48256, 341, 2771, 807, 1266, 71, 13, 50814, 50814, 407, 264, 2654, 13760, 1413, 264, 4338, 13760, 11, 597, 307, 5883, 1602, 412, 472, 13, 51064, 51064, 407, 341, 820, 13, 51514, 51514, 407, 286, 1194, 466, 29956, 278, 309, 11, 457, 286, 8932, 286, 820, 445, 1856, 264, 6713, 294, 510, 570, 309, 311, 1238, 4074, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08379147773565249, "compression_ratio": 1.580188679245283, "no_speech_prob": 4.495095708989538e-06}, {"id": 826, "seek": 452200, "start": 4522.0, "end": 4525.0, "text": " Why is not an object not callable?", "tokens": [50364, 1545, 307, 406, 364, 2657, 406, 818, 712, 30, 50514, 50514, 467, 311, 570, 286, 20331, 493, 13, 50664, 50664, 492, 434, 1382, 281, 3155, 613, 6828, 13, 50764, 50764, 407, 341, 307, 3006, 13, 50864, 50864, 639, 510, 11, 291, 500, 380, 528, 281, 818, 264, 2445, 570, 300, 11247, 6022, 13, 51064, 51064, 1981, 6828, 2736, 6022, 13, 51164, 51164, 492, 445, 528, 281, 3531, 264, 2445, 13, 51264, 51264, 407, 718, 385, 38818, 533, 264, 2158, 2657, 13, 51364, 51364, 400, 550, 321, 434, 516, 281, 808, 646, 294, 11, 38818, 533, 264, 6114, 11, 2642, 5893, 13, 51614, 51614, 5471, 307, 869, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09574777585966093, "compression_ratio": 1.6986899563318778, "no_speech_prob": 2.5866433134069666e-05}, {"id": 827, "seek": 452200, "start": 4525.0, "end": 4528.0, "text": " It's because I screwed up.", "tokens": [50364, 1545, 307, 406, 364, 2657, 406, 818, 712, 30, 50514, 50514, 467, 311, 570, 286, 20331, 493, 13, 50664, 50664, 492, 434, 1382, 281, 3155, 613, 6828, 13, 50764, 50764, 407, 341, 307, 3006, 13, 50864, 50864, 639, 510, 11, 291, 500, 380, 528, 281, 818, 264, 2445, 570, 300, 11247, 6022, 13, 51064, 51064, 1981, 6828, 2736, 6022, 13, 51164, 51164, 492, 445, 528, 281, 3531, 264, 2445, 13, 51264, 51264, 407, 718, 385, 38818, 533, 264, 2158, 2657, 13, 51364, 51364, 400, 550, 321, 434, 516, 281, 808, 646, 294, 11, 38818, 533, 264, 6114, 11, 2642, 5893, 13, 51614, 51614, 5471, 307, 869, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09574777585966093, "compression_ratio": 1.6986899563318778, "no_speech_prob": 2.5866433134069666e-05}, {"id": 828, "seek": 452200, "start": 4528.0, "end": 4530.0, "text": " We're trying to save these functions.", "tokens": [50364, 1545, 307, 406, 364, 2657, 406, 818, 712, 30, 50514, 50514, 467, 311, 570, 286, 20331, 493, 13, 50664, 50664, 492, 434, 1382, 281, 3155, 613, 6828, 13, 50764, 50764, 407, 341, 307, 3006, 13, 50864, 50864, 639, 510, 11, 291, 500, 380, 528, 281, 818, 264, 2445, 570, 300, 11247, 6022, 13, 51064, 51064, 1981, 6828, 2736, 6022, 13, 51164, 51164, 492, 445, 528, 281, 3531, 264, 2445, 13, 51264, 51264, 407, 718, 385, 38818, 533, 264, 2158, 2657, 13, 51364, 51364, 400, 550, 321, 434, 516, 281, 808, 646, 294, 11, 38818, 533, 264, 6114, 11, 2642, 5893, 13, 51614, 51614, 5471, 307, 869, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09574777585966093, "compression_ratio": 1.6986899563318778, "no_speech_prob": 2.5866433134069666e-05}, {"id": 829, "seek": 452200, "start": 4530.0, "end": 4532.0, "text": " So this is correct.", "tokens": [50364, 1545, 307, 406, 364, 2657, 406, 818, 712, 30, 50514, 50514, 467, 311, 570, 286, 20331, 493, 13, 50664, 50664, 492, 434, 1382, 281, 3155, 613, 6828, 13, 50764, 50764, 407, 341, 307, 3006, 13, 50864, 50864, 639, 510, 11, 291, 500, 380, 528, 281, 818, 264, 2445, 570, 300, 11247, 6022, 13, 51064, 51064, 1981, 6828, 2736, 6022, 13, 51164, 51164, 492, 445, 528, 281, 3531, 264, 2445, 13, 51264, 51264, 407, 718, 385, 38818, 533, 264, 2158, 2657, 13, 51364, 51364, 400, 550, 321, 434, 516, 281, 808, 646, 294, 11, 38818, 533, 264, 6114, 11, 2642, 5893, 13, 51614, 51614, 5471, 307, 869, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09574777585966093, "compression_ratio": 1.6986899563318778, "no_speech_prob": 2.5866433134069666e-05}, {"id": 830, "seek": 452200, "start": 4532.0, "end": 4536.0, "text": " This here, you don't want to call the function because that returns none.", "tokens": [50364, 1545, 307, 406, 364, 2657, 406, 818, 712, 30, 50514, 50514, 467, 311, 570, 286, 20331, 493, 13, 50664, 50664, 492, 434, 1382, 281, 3155, 613, 6828, 13, 50764, 50764, 407, 341, 307, 3006, 13, 50864, 50864, 639, 510, 11, 291, 500, 380, 528, 281, 818, 264, 2445, 570, 300, 11247, 6022, 13, 51064, 51064, 1981, 6828, 2736, 6022, 13, 51164, 51164, 492, 445, 528, 281, 3531, 264, 2445, 13, 51264, 51264, 407, 718, 385, 38818, 533, 264, 2158, 2657, 13, 51364, 51364, 400, 550, 321, 434, 516, 281, 808, 646, 294, 11, 38818, 533, 264, 6114, 11, 2642, 5893, 13, 51614, 51614, 5471, 307, 869, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09574777585966093, "compression_ratio": 1.6986899563318778, "no_speech_prob": 2.5866433134069666e-05}, {"id": 831, "seek": 452200, "start": 4536.0, "end": 4538.0, "text": " These functions return none.", "tokens": [50364, 1545, 307, 406, 364, 2657, 406, 818, 712, 30, 50514, 50514, 467, 311, 570, 286, 20331, 493, 13, 50664, 50664, 492, 434, 1382, 281, 3155, 613, 6828, 13, 50764, 50764, 407, 341, 307, 3006, 13, 50864, 50864, 639, 510, 11, 291, 500, 380, 528, 281, 818, 264, 2445, 570, 300, 11247, 6022, 13, 51064, 51064, 1981, 6828, 2736, 6022, 13, 51164, 51164, 492, 445, 528, 281, 3531, 264, 2445, 13, 51264, 51264, 407, 718, 385, 38818, 533, 264, 2158, 2657, 13, 51364, 51364, 400, 550, 321, 434, 516, 281, 808, 646, 294, 11, 38818, 533, 264, 6114, 11, 2642, 5893, 13, 51614, 51614, 5471, 307, 869, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09574777585966093, "compression_ratio": 1.6986899563318778, "no_speech_prob": 2.5866433134069666e-05}, {"id": 832, "seek": 452200, "start": 4538.0, "end": 4540.0, "text": " We just want to store the function.", "tokens": [50364, 1545, 307, 406, 364, 2657, 406, 818, 712, 30, 50514, 50514, 467, 311, 570, 286, 20331, 493, 13, 50664, 50664, 492, 434, 1382, 281, 3155, 613, 6828, 13, 50764, 50764, 407, 341, 307, 3006, 13, 50864, 50864, 639, 510, 11, 291, 500, 380, 528, 281, 818, 264, 2445, 570, 300, 11247, 6022, 13, 51064, 51064, 1981, 6828, 2736, 6022, 13, 51164, 51164, 492, 445, 528, 281, 3531, 264, 2445, 13, 51264, 51264, 407, 718, 385, 38818, 533, 264, 2158, 2657, 13, 51364, 51364, 400, 550, 321, 434, 516, 281, 808, 646, 294, 11, 38818, 533, 264, 6114, 11, 2642, 5893, 13, 51614, 51614, 5471, 307, 869, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09574777585966093, "compression_ratio": 1.6986899563318778, "no_speech_prob": 2.5866433134069666e-05}, {"id": 833, "seek": 452200, "start": 4540.0, "end": 4542.0, "text": " So let me redefine the value object.", "tokens": [50364, 1545, 307, 406, 364, 2657, 406, 818, 712, 30, 50514, 50514, 467, 311, 570, 286, 20331, 493, 13, 50664, 50664, 492, 434, 1382, 281, 3155, 613, 6828, 13, 50764, 50764, 407, 341, 307, 3006, 13, 50864, 50864, 639, 510, 11, 291, 500, 380, 528, 281, 818, 264, 2445, 570, 300, 11247, 6022, 13, 51064, 51064, 1981, 6828, 2736, 6022, 13, 51164, 51164, 492, 445, 528, 281, 3531, 264, 2445, 13, 51264, 51264, 407, 718, 385, 38818, 533, 264, 2158, 2657, 13, 51364, 51364, 400, 550, 321, 434, 516, 281, 808, 646, 294, 11, 38818, 533, 264, 6114, 11, 2642, 5893, 13, 51614, 51614, 5471, 307, 869, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09574777585966093, "compression_ratio": 1.6986899563318778, "no_speech_prob": 2.5866433134069666e-05}, {"id": 834, "seek": 452200, "start": 4542.0, "end": 4547.0, "text": " And then we're going to come back in, redefine the expression, draw dot.", "tokens": [50364, 1545, 307, 406, 364, 2657, 406, 818, 712, 30, 50514, 50514, 467, 311, 570, 286, 20331, 493, 13, 50664, 50664, 492, 434, 1382, 281, 3155, 613, 6828, 13, 50764, 50764, 407, 341, 307, 3006, 13, 50864, 50864, 639, 510, 11, 291, 500, 380, 528, 281, 818, 264, 2445, 570, 300, 11247, 6022, 13, 51064, 51064, 1981, 6828, 2736, 6022, 13, 51164, 51164, 492, 445, 528, 281, 3531, 264, 2445, 13, 51264, 51264, 407, 718, 385, 38818, 533, 264, 2158, 2657, 13, 51364, 51364, 400, 550, 321, 434, 516, 281, 808, 646, 294, 11, 38818, 533, 264, 6114, 11, 2642, 5893, 13, 51614, 51614, 5471, 307, 869, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09574777585966093, "compression_ratio": 1.6986899563318778, "no_speech_prob": 2.5866433134069666e-05}, {"id": 835, "seek": 452200, "start": 4547.0, "end": 4548.0, "text": " Everything is great.", "tokens": [50364, 1545, 307, 406, 364, 2657, 406, 818, 712, 30, 50514, 50514, 467, 311, 570, 286, 20331, 493, 13, 50664, 50664, 492, 434, 1382, 281, 3155, 613, 6828, 13, 50764, 50764, 407, 341, 307, 3006, 13, 50864, 50864, 639, 510, 11, 291, 500, 380, 528, 281, 818, 264, 2445, 570, 300, 11247, 6022, 13, 51064, 51064, 1981, 6828, 2736, 6022, 13, 51164, 51164, 492, 445, 528, 281, 3531, 264, 2445, 13, 51264, 51264, 407, 718, 385, 38818, 533, 264, 2158, 2657, 13, 51364, 51364, 400, 550, 321, 434, 516, 281, 808, 646, 294, 11, 38818, 533, 264, 6114, 11, 2642, 5893, 13, 51614, 51614, 5471, 307, 869, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09574777585966093, "compression_ratio": 1.6986899563318778, "no_speech_prob": 2.5866433134069666e-05}, {"id": 836, "seek": 454800, "start": 4548.0, "end": 4552.0, "text": " Oh, that grad is one, oh, that grad is one.", "tokens": [50364, 876, 11, 300, 2771, 307, 472, 11, 1954, 11, 300, 2771, 307, 472, 13, 50564, 50564, 400, 586, 11, 586, 341, 820, 589, 11, 295, 1164, 13, 50764, 50764, 2264, 11, 370, 439, 300, 23897, 820, 362, 341, 6386, 820, 586, 312, 935, 1732, 498, 321, 2182, 5131, 293, 1203, 1437, 8944, 13, 51114, 51114, 12387, 1732, 13, 51164, 51164, 13268, 13, 51214, 51214, 2264, 11, 370, 586, 321, 643, 281, 818, 293, 300, 2771, 13, 51464, 51464, 400, 300, 311, 8559, 13, 51514, 51514, 4919, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.3304412129161122, "compression_ratio": 1.6327683615819208, "no_speech_prob": 1.0783141988213174e-05}, {"id": 837, "seek": 454800, "start": 4552.0, "end": 4556.0, "text": " And now, now this should work, of course.", "tokens": [50364, 876, 11, 300, 2771, 307, 472, 11, 1954, 11, 300, 2771, 307, 472, 13, 50564, 50564, 400, 586, 11, 586, 341, 820, 589, 11, 295, 1164, 13, 50764, 50764, 2264, 11, 370, 439, 300, 23897, 820, 362, 341, 6386, 820, 586, 312, 935, 1732, 498, 321, 2182, 5131, 293, 1203, 1437, 8944, 13, 51114, 51114, 12387, 1732, 13, 51164, 51164, 13268, 13, 51214, 51214, 2264, 11, 370, 586, 321, 643, 281, 818, 293, 300, 2771, 13, 51464, 51464, 400, 300, 311, 8559, 13, 51514, 51514, 4919, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.3304412129161122, "compression_ratio": 1.6327683615819208, "no_speech_prob": 1.0783141988213174e-05}, {"id": 838, "seek": 454800, "start": 4556.0, "end": 4563.0, "text": " OK, so all that backward should have this grant should now be point five if we redraw and everything went correctly.", "tokens": [50364, 876, 11, 300, 2771, 307, 472, 11, 1954, 11, 300, 2771, 307, 472, 13, 50564, 50564, 400, 586, 11, 586, 341, 820, 589, 11, 295, 1164, 13, 50764, 50764, 2264, 11, 370, 439, 300, 23897, 820, 362, 341, 6386, 820, 586, 312, 935, 1732, 498, 321, 2182, 5131, 293, 1203, 1437, 8944, 13, 51114, 51114, 12387, 1732, 13, 51164, 51164, 13268, 13, 51214, 51214, 2264, 11, 370, 586, 321, 643, 281, 818, 293, 300, 2771, 13, 51464, 51464, 400, 300, 311, 8559, 13, 51514, 51514, 4919, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.3304412129161122, "compression_ratio": 1.6327683615819208, "no_speech_prob": 1.0783141988213174e-05}, {"id": 839, "seek": 454800, "start": 4563.0, "end": 4564.0, "text": " Point five.", "tokens": [50364, 876, 11, 300, 2771, 307, 472, 11, 1954, 11, 300, 2771, 307, 472, 13, 50564, 50564, 400, 586, 11, 586, 341, 820, 589, 11, 295, 1164, 13, 50764, 50764, 2264, 11, 370, 439, 300, 23897, 820, 362, 341, 6386, 820, 586, 312, 935, 1732, 498, 321, 2182, 5131, 293, 1203, 1437, 8944, 13, 51114, 51114, 12387, 1732, 13, 51164, 51164, 13268, 13, 51214, 51214, 2264, 11, 370, 586, 321, 643, 281, 818, 293, 300, 2771, 13, 51464, 51464, 400, 300, 311, 8559, 13, 51514, 51514, 4919, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.3304412129161122, "compression_ratio": 1.6327683615819208, "no_speech_prob": 1.0783141988213174e-05}, {"id": 840, "seek": 454800, "start": 4564.0, "end": 4565.0, "text": " Yay.", "tokens": [50364, 876, 11, 300, 2771, 307, 472, 11, 1954, 11, 300, 2771, 307, 472, 13, 50564, 50564, 400, 586, 11, 586, 341, 820, 589, 11, 295, 1164, 13, 50764, 50764, 2264, 11, 370, 439, 300, 23897, 820, 362, 341, 6386, 820, 586, 312, 935, 1732, 498, 321, 2182, 5131, 293, 1203, 1437, 8944, 13, 51114, 51114, 12387, 1732, 13, 51164, 51164, 13268, 13, 51214, 51214, 2264, 11, 370, 586, 321, 643, 281, 818, 293, 300, 2771, 13, 51464, 51464, 400, 300, 311, 8559, 13, 51514, 51514, 4919, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.3304412129161122, "compression_ratio": 1.6327683615819208, "no_speech_prob": 1.0783141988213174e-05}, {"id": 841, "seek": 454800, "start": 4565.0, "end": 4570.0, "text": " OK, so now we need to call and that grad.", "tokens": [50364, 876, 11, 300, 2771, 307, 472, 11, 1954, 11, 300, 2771, 307, 472, 13, 50564, 50564, 400, 586, 11, 586, 341, 820, 589, 11, 295, 1164, 13, 50764, 50764, 2264, 11, 370, 439, 300, 23897, 820, 362, 341, 6386, 820, 586, 312, 935, 1732, 498, 321, 2182, 5131, 293, 1203, 1437, 8944, 13, 51114, 51114, 12387, 1732, 13, 51164, 51164, 13268, 13, 51214, 51214, 2264, 11, 370, 586, 321, 643, 281, 818, 293, 300, 2771, 13, 51464, 51464, 400, 300, 311, 8559, 13, 51514, 51514, 4919, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.3304412129161122, "compression_ratio": 1.6327683615819208, "no_speech_prob": 1.0783141988213174e-05}, {"id": 842, "seek": 454800, "start": 4570.0, "end": 4571.0, "text": " And that's accurate.", "tokens": [50364, 876, 11, 300, 2771, 307, 472, 11, 1954, 11, 300, 2771, 307, 472, 13, 50564, 50564, 400, 586, 11, 586, 341, 820, 589, 11, 295, 1164, 13, 50764, 50764, 2264, 11, 370, 439, 300, 23897, 820, 362, 341, 6386, 820, 586, 312, 935, 1732, 498, 321, 2182, 5131, 293, 1203, 1437, 8944, 13, 51114, 51114, 12387, 1732, 13, 51164, 51164, 13268, 13, 51214, 51214, 2264, 11, 370, 586, 321, 643, 281, 818, 293, 300, 2771, 13, 51464, 51464, 400, 300, 311, 8559, 13, 51514, 51514, 4919, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.3304412129161122, "compression_ratio": 1.6327683615819208, "no_speech_prob": 1.0783141988213174e-05}, {"id": 843, "seek": 454800, "start": 4571.0, "end": 4573.0, "text": " Sorry.", "tokens": [50364, 876, 11, 300, 2771, 307, 472, 11, 1954, 11, 300, 2771, 307, 472, 13, 50564, 50564, 400, 586, 11, 586, 341, 820, 589, 11, 295, 1164, 13, 50764, 50764, 2264, 11, 370, 439, 300, 23897, 820, 362, 341, 6386, 820, 586, 312, 935, 1732, 498, 321, 2182, 5131, 293, 1203, 1437, 8944, 13, 51114, 51114, 12387, 1732, 13, 51164, 51164, 13268, 13, 51214, 51214, 2264, 11, 370, 586, 321, 643, 281, 818, 293, 300, 2771, 13, 51464, 51464, 400, 300, 311, 8559, 13, 51514, 51514, 4919, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.3304412129161122, "compression_ratio": 1.6327683615819208, "no_speech_prob": 1.0783141988213174e-05}, {"id": 844, "seek": 457300, "start": 4573.0, "end": 4578.0, "text": " And backward. So that seems to have worked.", "tokens": [50364, 400, 23897, 13, 407, 300, 2544, 281, 362, 2732, 13, 50614, 50614, 407, 2602, 295, 23897, 4020, 292, 264, 16235, 281, 1293, 295, 613, 13, 50814, 50814, 407, 341, 307, 1237, 869, 13, 50914, 50914, 823, 321, 393, 11, 295, 1164, 11, 1219, 1219, 363, 2771, 363, 23897, 13, 51114, 51114, 4919, 13, 51214, 51214, 708, 311, 516, 281, 1051, 30, 51314, 51314, 1042, 11, 363, 1177, 380, 362, 257, 23897, 363, 23897, 570, 363, 307, 257, 10871, 9984, 13, 51564, 51564, 879, 23897, 307, 5171, 1478, 2144, 11, 264, 6707, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2786327047446339, "compression_ratio": 1.6267942583732058, "no_speech_prob": 5.682260052708443e-06}, {"id": 845, "seek": 457300, "start": 4578.0, "end": 4582.0, "text": " So instead of backward routed the gradient to both of these.", "tokens": [50364, 400, 23897, 13, 407, 300, 2544, 281, 362, 2732, 13, 50614, 50614, 407, 2602, 295, 23897, 4020, 292, 264, 16235, 281, 1293, 295, 613, 13, 50814, 50814, 407, 341, 307, 1237, 869, 13, 50914, 50914, 823, 321, 393, 11, 295, 1164, 11, 1219, 1219, 363, 2771, 363, 23897, 13, 51114, 51114, 4919, 13, 51214, 51214, 708, 311, 516, 281, 1051, 30, 51314, 51314, 1042, 11, 363, 1177, 380, 362, 257, 23897, 363, 23897, 570, 363, 307, 257, 10871, 9984, 13, 51564, 51564, 879, 23897, 307, 5171, 1478, 2144, 11, 264, 6707, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2786327047446339, "compression_ratio": 1.6267942583732058, "no_speech_prob": 5.682260052708443e-06}, {"id": 846, "seek": 457300, "start": 4582.0, "end": 4584.0, "text": " So this is looking great.", "tokens": [50364, 400, 23897, 13, 407, 300, 2544, 281, 362, 2732, 13, 50614, 50614, 407, 2602, 295, 23897, 4020, 292, 264, 16235, 281, 1293, 295, 613, 13, 50814, 50814, 407, 341, 307, 1237, 869, 13, 50914, 50914, 823, 321, 393, 11, 295, 1164, 11, 1219, 1219, 363, 2771, 363, 23897, 13, 51114, 51114, 4919, 13, 51214, 51214, 708, 311, 516, 281, 1051, 30, 51314, 51314, 1042, 11, 363, 1177, 380, 362, 257, 23897, 363, 23897, 570, 363, 307, 257, 10871, 9984, 13, 51564, 51564, 879, 23897, 307, 5171, 1478, 2144, 11, 264, 6707, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2786327047446339, "compression_ratio": 1.6267942583732058, "no_speech_prob": 5.682260052708443e-06}, {"id": 847, "seek": 457300, "start": 4584.0, "end": 4588.0, "text": " Now we can, of course, called called B grad B backward.", "tokens": [50364, 400, 23897, 13, 407, 300, 2544, 281, 362, 2732, 13, 50614, 50614, 407, 2602, 295, 23897, 4020, 292, 264, 16235, 281, 1293, 295, 613, 13, 50814, 50814, 407, 341, 307, 1237, 869, 13, 50914, 50914, 823, 321, 393, 11, 295, 1164, 11, 1219, 1219, 363, 2771, 363, 23897, 13, 51114, 51114, 4919, 13, 51214, 51214, 708, 311, 516, 281, 1051, 30, 51314, 51314, 1042, 11, 363, 1177, 380, 362, 257, 23897, 363, 23897, 570, 363, 307, 257, 10871, 9984, 13, 51564, 51564, 879, 23897, 307, 5171, 1478, 2144, 11, 264, 6707, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2786327047446339, "compression_ratio": 1.6267942583732058, "no_speech_prob": 5.682260052708443e-06}, {"id": 848, "seek": 457300, "start": 4588.0, "end": 4590.0, "text": " Sorry.", "tokens": [50364, 400, 23897, 13, 407, 300, 2544, 281, 362, 2732, 13, 50614, 50614, 407, 2602, 295, 23897, 4020, 292, 264, 16235, 281, 1293, 295, 613, 13, 50814, 50814, 407, 341, 307, 1237, 869, 13, 50914, 50914, 823, 321, 393, 11, 295, 1164, 11, 1219, 1219, 363, 2771, 363, 23897, 13, 51114, 51114, 4919, 13, 51214, 51214, 708, 311, 516, 281, 1051, 30, 51314, 51314, 1042, 11, 363, 1177, 380, 362, 257, 23897, 363, 23897, 570, 363, 307, 257, 10871, 9984, 13, 51564, 51564, 879, 23897, 307, 5171, 1478, 2144, 11, 264, 6707, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2786327047446339, "compression_ratio": 1.6267942583732058, "no_speech_prob": 5.682260052708443e-06}, {"id": 849, "seek": 457300, "start": 4590.0, "end": 4592.0, "text": " What's going to happen?", "tokens": [50364, 400, 23897, 13, 407, 300, 2544, 281, 362, 2732, 13, 50614, 50614, 407, 2602, 295, 23897, 4020, 292, 264, 16235, 281, 1293, 295, 613, 13, 50814, 50814, 407, 341, 307, 1237, 869, 13, 50914, 50914, 823, 321, 393, 11, 295, 1164, 11, 1219, 1219, 363, 2771, 363, 23897, 13, 51114, 51114, 4919, 13, 51214, 51214, 708, 311, 516, 281, 1051, 30, 51314, 51314, 1042, 11, 363, 1177, 380, 362, 257, 23897, 363, 23897, 570, 363, 307, 257, 10871, 9984, 13, 51564, 51564, 879, 23897, 307, 5171, 1478, 2144, 11, 264, 6707, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2786327047446339, "compression_ratio": 1.6267942583732058, "no_speech_prob": 5.682260052708443e-06}, {"id": 850, "seek": 457300, "start": 4592.0, "end": 4597.0, "text": " Well, B doesn't have a backward B backward because B is a leaf node.", "tokens": [50364, 400, 23897, 13, 407, 300, 2544, 281, 362, 2732, 13, 50614, 50614, 407, 2602, 295, 23897, 4020, 292, 264, 16235, 281, 1293, 295, 613, 13, 50814, 50814, 407, 341, 307, 1237, 869, 13, 50914, 50914, 823, 321, 393, 11, 295, 1164, 11, 1219, 1219, 363, 2771, 363, 23897, 13, 51114, 51114, 4919, 13, 51214, 51214, 708, 311, 516, 281, 1051, 30, 51314, 51314, 1042, 11, 363, 1177, 380, 362, 257, 23897, 363, 23897, 570, 363, 307, 257, 10871, 9984, 13, 51564, 51564, 879, 23897, 307, 5171, 1478, 2144, 11, 264, 6707, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2786327047446339, "compression_ratio": 1.6267942583732058, "no_speech_prob": 5.682260052708443e-06}, {"id": 851, "seek": 457300, "start": 4597.0, "end": 4601.0, "text": " Be backward is binationalization, the empty function.", "tokens": [50364, 400, 23897, 13, 407, 300, 2544, 281, 362, 2732, 13, 50614, 50614, 407, 2602, 295, 23897, 4020, 292, 264, 16235, 281, 1293, 295, 613, 13, 50814, 50814, 407, 341, 307, 1237, 869, 13, 50914, 50914, 823, 321, 393, 11, 295, 1164, 11, 1219, 1219, 363, 2771, 363, 23897, 13, 51114, 51114, 4919, 13, 51214, 51214, 708, 311, 516, 281, 1051, 30, 51314, 51314, 1042, 11, 363, 1177, 380, 362, 257, 23897, 363, 23897, 570, 363, 307, 257, 10871, 9984, 13, 51564, 51564, 879, 23897, 307, 5171, 1478, 2144, 11, 264, 6707, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2786327047446339, "compression_ratio": 1.6267942583732058, "no_speech_prob": 5.682260052708443e-06}, {"id": 852, "seek": 460100, "start": 4601.0, "end": 4603.0, "text": " So nothing would happen.", "tokens": [50364, 407, 1825, 576, 1051, 13, 50464, 50464, 583, 321, 393, 818, 818, 309, 322, 309, 13, 50564, 50564, 583, 562, 321, 818, 341, 472, 12204, 11, 550, 321, 2066, 341, 935, 1732, 281, 483, 3052, 4020, 292, 13, 51164, 51164, 1779, 13, 407, 456, 321, 352, 13, 51264, 51264, 12387, 1732, 935, 1732, 13, 51364, 51364, 400, 550, 2721, 11, 321, 528, 281, 818, 309, 510, 322, 1783, 281, 343, 281, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1753678321838379, "compression_ratio": 1.5696202531645569, "no_speech_prob": 2.1567768726526992e-06}, {"id": 853, "seek": 460100, "start": 4603.0, "end": 4605.0, "text": " But we can call call it on it.", "tokens": [50364, 407, 1825, 576, 1051, 13, 50464, 50464, 583, 321, 393, 818, 818, 309, 322, 309, 13, 50564, 50564, 583, 562, 321, 818, 341, 472, 12204, 11, 550, 321, 2066, 341, 935, 1732, 281, 483, 3052, 4020, 292, 13, 51164, 51164, 1779, 13, 407, 456, 321, 352, 13, 51264, 51264, 12387, 1732, 935, 1732, 13, 51364, 51364, 400, 550, 2721, 11, 321, 528, 281, 818, 309, 510, 322, 1783, 281, 343, 281, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1753678321838379, "compression_ratio": 1.5696202531645569, "no_speech_prob": 2.1567768726526992e-06}, {"id": 854, "seek": 460100, "start": 4605.0, "end": 4617.0, "text": " But when we call this one backwards, then we expect this point five to get further routed.", "tokens": [50364, 407, 1825, 576, 1051, 13, 50464, 50464, 583, 321, 393, 818, 818, 309, 322, 309, 13, 50564, 50564, 583, 562, 321, 818, 341, 472, 12204, 11, 550, 321, 2066, 341, 935, 1732, 281, 483, 3052, 4020, 292, 13, 51164, 51164, 1779, 13, 407, 456, 321, 352, 13, 51264, 51264, 12387, 1732, 935, 1732, 13, 51364, 51364, 400, 550, 2721, 11, 321, 528, 281, 818, 309, 510, 322, 1783, 281, 343, 281, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1753678321838379, "compression_ratio": 1.5696202531645569, "no_speech_prob": 2.1567768726526992e-06}, {"id": 855, "seek": 460100, "start": 4617.0, "end": 4619.0, "text": " Right. So there we go.", "tokens": [50364, 407, 1825, 576, 1051, 13, 50464, 50464, 583, 321, 393, 818, 818, 309, 322, 309, 13, 50564, 50564, 583, 562, 321, 818, 341, 472, 12204, 11, 550, 321, 2066, 341, 935, 1732, 281, 483, 3052, 4020, 292, 13, 51164, 51164, 1779, 13, 407, 456, 321, 352, 13, 51264, 51264, 12387, 1732, 935, 1732, 13, 51364, 51364, 400, 550, 2721, 11, 321, 528, 281, 818, 309, 510, 322, 1783, 281, 343, 281, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1753678321838379, "compression_ratio": 1.5696202531645569, "no_speech_prob": 2.1567768726526992e-06}, {"id": 856, "seek": 460100, "start": 4619.0, "end": 4621.0, "text": " Point five point five.", "tokens": [50364, 407, 1825, 576, 1051, 13, 50464, 50464, 583, 321, 393, 818, 818, 309, 322, 309, 13, 50564, 50564, 583, 562, 321, 818, 341, 472, 12204, 11, 550, 321, 2066, 341, 935, 1732, 281, 483, 3052, 4020, 292, 13, 51164, 51164, 1779, 13, 407, 456, 321, 352, 13, 51264, 51264, 12387, 1732, 935, 1732, 13, 51364, 51364, 400, 550, 2721, 11, 321, 528, 281, 818, 309, 510, 322, 1783, 281, 343, 281, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1753678321838379, "compression_ratio": 1.5696202531645569, "no_speech_prob": 2.1567768726526992e-06}, {"id": 857, "seek": 460100, "start": 4621.0, "end": 4630.0, "text": " And then finally, we want to call it here on X to W to.", "tokens": [50364, 407, 1825, 576, 1051, 13, 50464, 50464, 583, 321, 393, 818, 818, 309, 322, 309, 13, 50564, 50564, 583, 562, 321, 818, 341, 472, 12204, 11, 550, 321, 2066, 341, 935, 1732, 281, 483, 3052, 4020, 292, 13, 51164, 51164, 1779, 13, 407, 456, 321, 352, 13, 51264, 51264, 12387, 1732, 935, 1732, 13, 51364, 51364, 400, 550, 2721, 11, 321, 528, 281, 818, 309, 510, 322, 1783, 281, 343, 281, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1753678321838379, "compression_ratio": 1.5696202531645569, "no_speech_prob": 2.1567768726526992e-06}, {"id": 858, "seek": 463000, "start": 4630.0, "end": 4636.0, "text": " And on X one W one.", "tokens": [50364, 400, 322, 1783, 472, 343, 472, 13, 50664, 50664, 961, 311, 360, 1293, 295, 729, 13, 50764, 50764, 400, 456, 321, 352, 13, 50864, 50864, 407, 321, 483, 4018, 935, 1732, 3671, 472, 935, 1732, 293, 472, 2293, 382, 321, 630, 949, 13, 51114, 51114, 583, 586, 321, 600, 1096, 309, 807, 5141, 300, 23897, 1333, 295, 16945, 13, 51464, 51464, 407, 321, 362, 472, 1036, 2522, 281, 483, 3973, 295, 11, 597, 307, 505, 5141, 37556, 23897, 16945, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11993429774329775, "compression_ratio": 1.5968586387434556, "no_speech_prob": 5.173612862563459e-06}, {"id": 859, "seek": 463000, "start": 4636.0, "end": 4638.0, "text": " Let's do both of those.", "tokens": [50364, 400, 322, 1783, 472, 343, 472, 13, 50664, 50664, 961, 311, 360, 1293, 295, 729, 13, 50764, 50764, 400, 456, 321, 352, 13, 50864, 50864, 407, 321, 483, 4018, 935, 1732, 3671, 472, 935, 1732, 293, 472, 2293, 382, 321, 630, 949, 13, 51114, 51114, 583, 586, 321, 600, 1096, 309, 807, 5141, 300, 23897, 1333, 295, 16945, 13, 51464, 51464, 407, 321, 362, 472, 1036, 2522, 281, 483, 3973, 295, 11, 597, 307, 505, 5141, 37556, 23897, 16945, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11993429774329775, "compression_ratio": 1.5968586387434556, "no_speech_prob": 5.173612862563459e-06}, {"id": 860, "seek": 463000, "start": 4638.0, "end": 4640.0, "text": " And there we go.", "tokens": [50364, 400, 322, 1783, 472, 343, 472, 13, 50664, 50664, 961, 311, 360, 1293, 295, 729, 13, 50764, 50764, 400, 456, 321, 352, 13, 50864, 50864, 407, 321, 483, 4018, 935, 1732, 3671, 472, 935, 1732, 293, 472, 2293, 382, 321, 630, 949, 13, 51114, 51114, 583, 586, 321, 600, 1096, 309, 807, 5141, 300, 23897, 1333, 295, 16945, 13, 51464, 51464, 407, 321, 362, 472, 1036, 2522, 281, 483, 3973, 295, 11, 597, 307, 505, 5141, 37556, 23897, 16945, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11993429774329775, "compression_ratio": 1.5968586387434556, "no_speech_prob": 5.173612862563459e-06}, {"id": 861, "seek": 463000, "start": 4640.0, "end": 4645.0, "text": " So we get zero point five negative one point five and one exactly as we did before.", "tokens": [50364, 400, 322, 1783, 472, 343, 472, 13, 50664, 50664, 961, 311, 360, 1293, 295, 729, 13, 50764, 50764, 400, 456, 321, 352, 13, 50864, 50864, 407, 321, 483, 4018, 935, 1732, 3671, 472, 935, 1732, 293, 472, 2293, 382, 321, 630, 949, 13, 51114, 51114, 583, 586, 321, 600, 1096, 309, 807, 5141, 300, 23897, 1333, 295, 16945, 13, 51464, 51464, 407, 321, 362, 472, 1036, 2522, 281, 483, 3973, 295, 11, 597, 307, 505, 5141, 37556, 23897, 16945, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11993429774329775, "compression_ratio": 1.5968586387434556, "no_speech_prob": 5.173612862563459e-06}, {"id": 862, "seek": 463000, "start": 4645.0, "end": 4652.0, "text": " But now we've done it through calling that backward sort of manually.", "tokens": [50364, 400, 322, 1783, 472, 343, 472, 13, 50664, 50664, 961, 311, 360, 1293, 295, 729, 13, 50764, 50764, 400, 456, 321, 352, 13, 50864, 50864, 407, 321, 483, 4018, 935, 1732, 3671, 472, 935, 1732, 293, 472, 2293, 382, 321, 630, 949, 13, 51114, 51114, 583, 586, 321, 600, 1096, 309, 807, 5141, 300, 23897, 1333, 295, 16945, 13, 51464, 51464, 407, 321, 362, 472, 1036, 2522, 281, 483, 3973, 295, 11, 597, 307, 505, 5141, 37556, 23897, 16945, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11993429774329775, "compression_ratio": 1.5968586387434556, "no_speech_prob": 5.173612862563459e-06}, {"id": 863, "seek": 463000, "start": 4652.0, "end": 4658.0, "text": " So we have one last piece to get rid of, which is us calling underscore backward manually.", "tokens": [50364, 400, 322, 1783, 472, 343, 472, 13, 50664, 50664, 961, 311, 360, 1293, 295, 729, 13, 50764, 50764, 400, 456, 321, 352, 13, 50864, 50864, 407, 321, 483, 4018, 935, 1732, 3671, 472, 935, 1732, 293, 472, 2293, 382, 321, 630, 949, 13, 51114, 51114, 583, 586, 321, 600, 1096, 309, 807, 5141, 300, 23897, 1333, 295, 16945, 13, 51464, 51464, 407, 321, 362, 472, 1036, 2522, 281, 483, 3973, 295, 11, 597, 307, 505, 5141, 37556, 23897, 16945, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11993429774329775, "compression_ratio": 1.5968586387434556, "no_speech_prob": 5.173612862563459e-06}, {"id": 864, "seek": 465800, "start": 4658.0, "end": 4661.0, "text": " So let's think through what we are actually doing.", "tokens": [50364, 407, 718, 311, 519, 807, 437, 321, 366, 767, 884, 13, 50514, 50514, 492, 600, 9897, 484, 257, 18894, 6114, 293, 586, 321, 434, 1382, 281, 352, 12204, 807, 300, 6114, 13, 50814, 50814, 407, 516, 12204, 807, 264, 6114, 445, 1355, 300, 321, 1128, 528, 281, 818, 257, 5893, 23897, 337, 604, 9984, 949, 321, 600, 1096, 1333, 295, 1203, 934, 309, 13, 51464, 51464, 407, 321, 362, 281, 360, 1203, 934, 309, 949, 1562, 516, 281, 818, 300, 23897, 322, 604, 472, 9984, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06951036983066135, "compression_ratio": 1.8611111111111112, "no_speech_prob": 4.888202624897531e-07}, {"id": 865, "seek": 465800, "start": 4661.0, "end": 4667.0, "text": " We've laid out a mathematical expression and now we're trying to go backwards through that expression.", "tokens": [50364, 407, 718, 311, 519, 807, 437, 321, 366, 767, 884, 13, 50514, 50514, 492, 600, 9897, 484, 257, 18894, 6114, 293, 586, 321, 434, 1382, 281, 352, 12204, 807, 300, 6114, 13, 50814, 50814, 407, 516, 12204, 807, 264, 6114, 445, 1355, 300, 321, 1128, 528, 281, 818, 257, 5893, 23897, 337, 604, 9984, 949, 321, 600, 1096, 1333, 295, 1203, 934, 309, 13, 51464, 51464, 407, 321, 362, 281, 360, 1203, 934, 309, 949, 1562, 516, 281, 818, 300, 23897, 322, 604, 472, 9984, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06951036983066135, "compression_ratio": 1.8611111111111112, "no_speech_prob": 4.888202624897531e-07}, {"id": 866, "seek": 465800, "start": 4667.0, "end": 4680.0, "text": " So going backwards through the expression just means that we never want to call a dot backward for any node before we've done sort of everything after it.", "tokens": [50364, 407, 718, 311, 519, 807, 437, 321, 366, 767, 884, 13, 50514, 50514, 492, 600, 9897, 484, 257, 18894, 6114, 293, 586, 321, 434, 1382, 281, 352, 12204, 807, 300, 6114, 13, 50814, 50814, 407, 516, 12204, 807, 264, 6114, 445, 1355, 300, 321, 1128, 528, 281, 818, 257, 5893, 23897, 337, 604, 9984, 949, 321, 600, 1096, 1333, 295, 1203, 934, 309, 13, 51464, 51464, 407, 321, 362, 281, 360, 1203, 934, 309, 949, 1562, 516, 281, 818, 300, 23897, 322, 604, 472, 9984, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06951036983066135, "compression_ratio": 1.8611111111111112, "no_speech_prob": 4.888202624897531e-07}, {"id": 867, "seek": 465800, "start": 4680.0, "end": 4684.0, "text": " So we have to do everything after it before ever going to call that backward on any one node.", "tokens": [50364, 407, 718, 311, 519, 807, 437, 321, 366, 767, 884, 13, 50514, 50514, 492, 600, 9897, 484, 257, 18894, 6114, 293, 586, 321, 434, 1382, 281, 352, 12204, 807, 300, 6114, 13, 50814, 50814, 407, 516, 12204, 807, 264, 6114, 445, 1355, 300, 321, 1128, 528, 281, 818, 257, 5893, 23897, 337, 604, 9984, 949, 321, 600, 1096, 1333, 295, 1203, 934, 309, 13, 51464, 51464, 407, 321, 362, 281, 360, 1203, 934, 309, 949, 1562, 516, 281, 818, 300, 23897, 322, 604, 472, 9984, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06951036983066135, "compression_ratio": 1.8611111111111112, "no_speech_prob": 4.888202624897531e-07}, {"id": 868, "seek": 468400, "start": 4684.0, "end": 4692.0, "text": " We have to get all of its full dependencies. Everything that it depends on has to propagate to it before we can continue back propagation.", "tokens": [50364, 492, 362, 281, 483, 439, 295, 1080, 1577, 36606, 13, 5471, 300, 309, 5946, 322, 575, 281, 48256, 281, 309, 949, 321, 393, 2354, 646, 38377, 13, 50764, 50764, 407, 341, 21739, 295, 24877, 393, 312, 11042, 1228, 746, 1219, 1192, 4383, 1333, 13, 51064, 51064, 407, 1192, 4383, 1333, 307, 1936, 257, 14903, 484, 295, 257, 4295, 1270, 300, 439, 264, 8819, 352, 787, 490, 1411, 281, 558, 11, 1936, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.06762546614596718, "compression_ratio": 1.6442307692307692, "no_speech_prob": 5.422069079941139e-06}, {"id": 869, "seek": 468400, "start": 4692.0, "end": 4698.0, "text": " So this ordering of graphs can be achieved using something called topological sort.", "tokens": [50364, 492, 362, 281, 483, 439, 295, 1080, 1577, 36606, 13, 5471, 300, 309, 5946, 322, 575, 281, 48256, 281, 309, 949, 321, 393, 2354, 646, 38377, 13, 50764, 50764, 407, 341, 21739, 295, 24877, 393, 312, 11042, 1228, 746, 1219, 1192, 4383, 1333, 13, 51064, 51064, 407, 1192, 4383, 1333, 307, 1936, 257, 14903, 484, 295, 257, 4295, 1270, 300, 439, 264, 8819, 352, 787, 490, 1411, 281, 558, 11, 1936, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.06762546614596718, "compression_ratio": 1.6442307692307692, "no_speech_prob": 5.422069079941139e-06}, {"id": 870, "seek": 468400, "start": 4698.0, "end": 4706.0, "text": " So topological sort is basically a laying out of a graph such that all the edges go only from left to right, basically.", "tokens": [50364, 492, 362, 281, 483, 439, 295, 1080, 1577, 36606, 13, 5471, 300, 309, 5946, 322, 575, 281, 48256, 281, 309, 949, 321, 393, 2354, 646, 38377, 13, 50764, 50764, 407, 341, 21739, 295, 24877, 393, 312, 11042, 1228, 746, 1219, 1192, 4383, 1333, 13, 51064, 51064, 407, 1192, 4383, 1333, 307, 1936, 257, 14903, 484, 295, 257, 4295, 1270, 300, 439, 264, 8819, 352, 787, 490, 1411, 281, 558, 11, 1936, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.06762546614596718, "compression_ratio": 1.6442307692307692, "no_speech_prob": 5.422069079941139e-06}, {"id": 871, "seek": 470600, "start": 4706.0, "end": 4715.0, "text": " So here we have a graph, a direction acyclic graph, a DAG. And this is two different topological orders of it, I believe,", "tokens": [50364, 407, 510, 321, 362, 257, 4295, 11, 257, 3513, 696, 88, 66, 1050, 4295, 11, 257, 9578, 38, 13, 400, 341, 307, 732, 819, 1192, 4383, 9470, 295, 309, 11, 286, 1697, 11, 50814, 50814, 689, 1936, 291, 603, 536, 300, 309, 311, 257, 14903, 484, 295, 264, 13891, 1270, 300, 439, 264, 8819, 352, 787, 472, 636, 490, 1411, 281, 558, 13, 51114, 51114, 400, 18114, 1192, 4383, 1333, 11, 291, 393, 574, 294, 28999, 293, 370, 322, 13, 286, 478, 406, 516, 281, 352, 807, 309, 294, 2607, 13, 51464, 51464, 583, 1936, 11, 341, 307, 437, 15182, 257, 1192, 4383, 4295, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1101392430996676, "compression_ratio": 1.6299212598425197, "no_speech_prob": 1.0615659448376391e-05}, {"id": 872, "seek": 470600, "start": 4715.0, "end": 4721.0, "text": " where basically you'll see that it's a laying out of the nodes such that all the edges go only one way from left to right.", "tokens": [50364, 407, 510, 321, 362, 257, 4295, 11, 257, 3513, 696, 88, 66, 1050, 4295, 11, 257, 9578, 38, 13, 400, 341, 307, 732, 819, 1192, 4383, 9470, 295, 309, 11, 286, 1697, 11, 50814, 50814, 689, 1936, 291, 603, 536, 300, 309, 311, 257, 14903, 484, 295, 264, 13891, 1270, 300, 439, 264, 8819, 352, 787, 472, 636, 490, 1411, 281, 558, 13, 51114, 51114, 400, 18114, 1192, 4383, 1333, 11, 291, 393, 574, 294, 28999, 293, 370, 322, 13, 286, 478, 406, 516, 281, 352, 807, 309, 294, 2607, 13, 51464, 51464, 583, 1936, 11, 341, 307, 437, 15182, 257, 1192, 4383, 4295, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1101392430996676, "compression_ratio": 1.6299212598425197, "no_speech_prob": 1.0615659448376391e-05}, {"id": 873, "seek": 470600, "start": 4721.0, "end": 4728.0, "text": " And implementing topological sort, you can look in Wikipedia and so on. I'm not going to go through it in detail.", "tokens": [50364, 407, 510, 321, 362, 257, 4295, 11, 257, 3513, 696, 88, 66, 1050, 4295, 11, 257, 9578, 38, 13, 400, 341, 307, 732, 819, 1192, 4383, 9470, 295, 309, 11, 286, 1697, 11, 50814, 50814, 689, 1936, 291, 603, 536, 300, 309, 311, 257, 14903, 484, 295, 264, 13891, 1270, 300, 439, 264, 8819, 352, 787, 472, 636, 490, 1411, 281, 558, 13, 51114, 51114, 400, 18114, 1192, 4383, 1333, 11, 291, 393, 574, 294, 28999, 293, 370, 322, 13, 286, 478, 406, 516, 281, 352, 807, 309, 294, 2607, 13, 51464, 51464, 583, 1936, 11, 341, 307, 437, 15182, 257, 1192, 4383, 4295, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1101392430996676, "compression_ratio": 1.6299212598425197, "no_speech_prob": 1.0615659448376391e-05}, {"id": 874, "seek": 470600, "start": 4728.0, "end": 4733.0, "text": " But basically, this is what builds a topological graph.", "tokens": [50364, 407, 510, 321, 362, 257, 4295, 11, 257, 3513, 696, 88, 66, 1050, 4295, 11, 257, 9578, 38, 13, 400, 341, 307, 732, 819, 1192, 4383, 9470, 295, 309, 11, 286, 1697, 11, 50814, 50814, 689, 1936, 291, 603, 536, 300, 309, 311, 257, 14903, 484, 295, 264, 13891, 1270, 300, 439, 264, 8819, 352, 787, 472, 636, 490, 1411, 281, 558, 13, 51114, 51114, 400, 18114, 1192, 4383, 1333, 11, 291, 393, 574, 294, 28999, 293, 370, 322, 13, 286, 478, 406, 516, 281, 352, 807, 309, 294, 2607, 13, 51464, 51464, 583, 1936, 11, 341, 307, 437, 15182, 257, 1192, 4383, 4295, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1101392430996676, "compression_ratio": 1.6299212598425197, "no_speech_prob": 1.0615659448376391e-05}, {"id": 875, "seek": 473300, "start": 4733.0, "end": 4742.0, "text": " We maintain a set of visited nodes and then we are going through starting at some root node,", "tokens": [50364, 492, 6909, 257, 992, 295, 11220, 13891, 293, 550, 321, 366, 516, 807, 2891, 412, 512, 5593, 9984, 11, 50814, 50814, 597, 337, 505, 307, 422, 13, 663, 311, 689, 321, 528, 281, 722, 264, 1192, 4383, 1333, 13, 50964, 50964, 400, 2891, 412, 422, 11, 321, 352, 807, 439, 295, 1080, 2227, 293, 321, 643, 281, 2360, 552, 484, 490, 1411, 281, 558, 13, 51314, 51314, 400, 1936, 11, 341, 3719, 412, 422, 13, 759, 309, 311, 406, 11220, 11, 550, 309, 10640, 309, 382, 11220, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08605113236800484, "compression_ratio": 1.6064814814814814, "no_speech_prob": 3.120007022516802e-05}, {"id": 876, "seek": 473300, "start": 4742.0, "end": 4745.0, "text": " which for us is O. That's where we want to start the topological sort.", "tokens": [50364, 492, 6909, 257, 992, 295, 11220, 13891, 293, 550, 321, 366, 516, 807, 2891, 412, 512, 5593, 9984, 11, 50814, 50814, 597, 337, 505, 307, 422, 13, 663, 311, 689, 321, 528, 281, 722, 264, 1192, 4383, 1333, 13, 50964, 50964, 400, 2891, 412, 422, 11, 321, 352, 807, 439, 295, 1080, 2227, 293, 321, 643, 281, 2360, 552, 484, 490, 1411, 281, 558, 13, 51314, 51314, 400, 1936, 11, 341, 3719, 412, 422, 13, 759, 309, 311, 406, 11220, 11, 550, 309, 10640, 309, 382, 11220, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08605113236800484, "compression_ratio": 1.6064814814814814, "no_speech_prob": 3.120007022516802e-05}, {"id": 877, "seek": 473300, "start": 4745.0, "end": 4752.0, "text": " And starting at O, we go through all of its children and we need to lay them out from left to right.", "tokens": [50364, 492, 6909, 257, 992, 295, 11220, 13891, 293, 550, 321, 366, 516, 807, 2891, 412, 512, 5593, 9984, 11, 50814, 50814, 597, 337, 505, 307, 422, 13, 663, 311, 689, 321, 528, 281, 722, 264, 1192, 4383, 1333, 13, 50964, 50964, 400, 2891, 412, 422, 11, 321, 352, 807, 439, 295, 1080, 2227, 293, 321, 643, 281, 2360, 552, 484, 490, 1411, 281, 558, 13, 51314, 51314, 400, 1936, 11, 341, 3719, 412, 422, 13, 759, 309, 311, 406, 11220, 11, 550, 309, 10640, 309, 382, 11220, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08605113236800484, "compression_ratio": 1.6064814814814814, "no_speech_prob": 3.120007022516802e-05}, {"id": 878, "seek": 473300, "start": 4752.0, "end": 4758.0, "text": " And basically, this starts at O. If it's not visited, then it marks it as visited.", "tokens": [50364, 492, 6909, 257, 992, 295, 11220, 13891, 293, 550, 321, 366, 516, 807, 2891, 412, 512, 5593, 9984, 11, 50814, 50814, 597, 337, 505, 307, 422, 13, 663, 311, 689, 321, 528, 281, 722, 264, 1192, 4383, 1333, 13, 50964, 50964, 400, 2891, 412, 422, 11, 321, 352, 807, 439, 295, 1080, 2227, 293, 321, 643, 281, 2360, 552, 484, 490, 1411, 281, 558, 13, 51314, 51314, 400, 1936, 11, 341, 3719, 412, 422, 13, 759, 309, 311, 406, 11220, 11, 550, 309, 10640, 309, 382, 11220, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08605113236800484, "compression_ratio": 1.6064814814814814, "no_speech_prob": 3.120007022516802e-05}, {"id": 879, "seek": 475800, "start": 4758.0, "end": 4764.0, "text": " And then it iterates through all of its children and calls build topological on them.", "tokens": [50364, 400, 550, 309, 17138, 1024, 807, 439, 295, 1080, 2227, 293, 5498, 1322, 1192, 4383, 322, 552, 13, 50664, 50664, 400, 550, 934, 309, 311, 2780, 807, 439, 264, 2227, 11, 309, 10860, 2564, 13, 50864, 50864, 407, 1936, 11, 341, 9984, 300, 321, 434, 516, 281, 818, 309, 322, 11, 411, 584, 422, 11, 307, 787, 516, 281, 909, 2564, 281, 264, 1192, 78, 1329, 934, 439, 295, 264, 2227, 362, 668, 18846, 13, 51414, 51414, 400, 300, 311, 577, 341, 2445, 307, 18031, 300, 291, 434, 787, 516, 281, 312, 294, 264, 1329, 1564, 439, 428, 2227, 366, 294, 264, 1329, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0765362227404559, "compression_ratio": 1.871244635193133, "no_speech_prob": 2.930945993284695e-05}, {"id": 880, "seek": 475800, "start": 4764.0, "end": 4768.0, "text": " And then after it's gone through all the children, it adds itself.", "tokens": [50364, 400, 550, 309, 17138, 1024, 807, 439, 295, 1080, 2227, 293, 5498, 1322, 1192, 4383, 322, 552, 13, 50664, 50664, 400, 550, 934, 309, 311, 2780, 807, 439, 264, 2227, 11, 309, 10860, 2564, 13, 50864, 50864, 407, 1936, 11, 341, 9984, 300, 321, 434, 516, 281, 818, 309, 322, 11, 411, 584, 422, 11, 307, 787, 516, 281, 909, 2564, 281, 264, 1192, 78, 1329, 934, 439, 295, 264, 2227, 362, 668, 18846, 13, 51414, 51414, 400, 300, 311, 577, 341, 2445, 307, 18031, 300, 291, 434, 787, 516, 281, 312, 294, 264, 1329, 1564, 439, 428, 2227, 366, 294, 264, 1329, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0765362227404559, "compression_ratio": 1.871244635193133, "no_speech_prob": 2.930945993284695e-05}, {"id": 881, "seek": 475800, "start": 4768.0, "end": 4779.0, "text": " So basically, this node that we're going to call it on, like say O, is only going to add itself to the topo list after all of the children have been processed.", "tokens": [50364, 400, 550, 309, 17138, 1024, 807, 439, 295, 1080, 2227, 293, 5498, 1322, 1192, 4383, 322, 552, 13, 50664, 50664, 400, 550, 934, 309, 311, 2780, 807, 439, 264, 2227, 11, 309, 10860, 2564, 13, 50864, 50864, 407, 1936, 11, 341, 9984, 300, 321, 434, 516, 281, 818, 309, 322, 11, 411, 584, 422, 11, 307, 787, 516, 281, 909, 2564, 281, 264, 1192, 78, 1329, 934, 439, 295, 264, 2227, 362, 668, 18846, 13, 51414, 51414, 400, 300, 311, 577, 341, 2445, 307, 18031, 300, 291, 434, 787, 516, 281, 312, 294, 264, 1329, 1564, 439, 428, 2227, 366, 294, 264, 1329, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0765362227404559, "compression_ratio": 1.871244635193133, "no_speech_prob": 2.930945993284695e-05}, {"id": 882, "seek": 475800, "start": 4779.0, "end": 4786.0, "text": " And that's how this function is guaranteed that you're only going to be in the list once all your children are in the list.", "tokens": [50364, 400, 550, 309, 17138, 1024, 807, 439, 295, 1080, 2227, 293, 5498, 1322, 1192, 4383, 322, 552, 13, 50664, 50664, 400, 550, 934, 309, 311, 2780, 807, 439, 264, 2227, 11, 309, 10860, 2564, 13, 50864, 50864, 407, 1936, 11, 341, 9984, 300, 321, 434, 516, 281, 818, 309, 322, 11, 411, 584, 422, 11, 307, 787, 516, 281, 909, 2564, 281, 264, 1192, 78, 1329, 934, 439, 295, 264, 2227, 362, 668, 18846, 13, 51414, 51414, 400, 300, 311, 577, 341, 2445, 307, 18031, 300, 291, 434, 787, 516, 281, 312, 294, 264, 1329, 1564, 439, 428, 2227, 366, 294, 264, 1329, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0765362227404559, "compression_ratio": 1.871244635193133, "no_speech_prob": 2.930945993284695e-05}, {"id": 883, "seek": 478600, "start": 4786.0, "end": 4788.0, "text": " And that's the invariant that is being maintained.", "tokens": [50364, 400, 300, 311, 264, 33270, 394, 300, 307, 885, 17578, 13, 50464, 50464, 407, 498, 321, 1322, 1192, 78, 322, 422, 293, 550, 15018, 341, 1329, 11, 321, 434, 516, 281, 536, 300, 309, 8866, 527, 2158, 6565, 13, 50864, 50864, 400, 264, 1036, 472, 307, 264, 2158, 295, 1958, 13, 22, 293, 1958, 13, 22, 11, 597, 307, 264, 5598, 13, 51164, 51164, 407, 341, 307, 422, 293, 550, 341, 307, 426, 13, 400, 550, 439, 264, 661, 13891, 483, 9897, 484, 949, 309, 13, 51564, 51564, 407, 300, 15182, 264, 1192, 4383, 4295, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10590170860290528, "compression_ratio": 1.6650717703349283, "no_speech_prob": 1.544564111100044e-05}, {"id": 884, "seek": 478600, "start": 4788.0, "end": 4796.0, "text": " So if we build topo on O and then inspect this list, we're going to see that it ordered our value objects.", "tokens": [50364, 400, 300, 311, 264, 33270, 394, 300, 307, 885, 17578, 13, 50464, 50464, 407, 498, 321, 1322, 1192, 78, 322, 422, 293, 550, 15018, 341, 1329, 11, 321, 434, 516, 281, 536, 300, 309, 8866, 527, 2158, 6565, 13, 50864, 50864, 400, 264, 1036, 472, 307, 264, 2158, 295, 1958, 13, 22, 293, 1958, 13, 22, 11, 597, 307, 264, 5598, 13, 51164, 51164, 407, 341, 307, 422, 293, 550, 341, 307, 426, 13, 400, 550, 439, 264, 661, 13891, 483, 9897, 484, 949, 309, 13, 51564, 51564, 407, 300, 15182, 264, 1192, 4383, 4295, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10590170860290528, "compression_ratio": 1.6650717703349283, "no_speech_prob": 1.544564111100044e-05}, {"id": 885, "seek": 478600, "start": 4796.0, "end": 4802.0, "text": " And the last one is the value of 0.7 and 0.7, which is the output.", "tokens": [50364, 400, 300, 311, 264, 33270, 394, 300, 307, 885, 17578, 13, 50464, 50464, 407, 498, 321, 1322, 1192, 78, 322, 422, 293, 550, 15018, 341, 1329, 11, 321, 434, 516, 281, 536, 300, 309, 8866, 527, 2158, 6565, 13, 50864, 50864, 400, 264, 1036, 472, 307, 264, 2158, 295, 1958, 13, 22, 293, 1958, 13, 22, 11, 597, 307, 264, 5598, 13, 51164, 51164, 407, 341, 307, 422, 293, 550, 341, 307, 426, 13, 400, 550, 439, 264, 661, 13891, 483, 9897, 484, 949, 309, 13, 51564, 51564, 407, 300, 15182, 264, 1192, 4383, 4295, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10590170860290528, "compression_ratio": 1.6650717703349283, "no_speech_prob": 1.544564111100044e-05}, {"id": 886, "seek": 478600, "start": 4802.0, "end": 4810.0, "text": " So this is O and then this is N. And then all the other nodes get laid out before it.", "tokens": [50364, 400, 300, 311, 264, 33270, 394, 300, 307, 885, 17578, 13, 50464, 50464, 407, 498, 321, 1322, 1192, 78, 322, 422, 293, 550, 15018, 341, 1329, 11, 321, 434, 516, 281, 536, 300, 309, 8866, 527, 2158, 6565, 13, 50864, 50864, 400, 264, 1036, 472, 307, 264, 2158, 295, 1958, 13, 22, 293, 1958, 13, 22, 11, 597, 307, 264, 5598, 13, 51164, 51164, 407, 341, 307, 422, 293, 550, 341, 307, 426, 13, 400, 550, 439, 264, 661, 13891, 483, 9897, 484, 949, 309, 13, 51564, 51564, 407, 300, 15182, 264, 1192, 4383, 4295, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10590170860290528, "compression_ratio": 1.6650717703349283, "no_speech_prob": 1.544564111100044e-05}, {"id": 887, "seek": 478600, "start": 4810.0, "end": 4812.0, "text": " So that builds the topological graph.", "tokens": [50364, 400, 300, 311, 264, 33270, 394, 300, 307, 885, 17578, 13, 50464, 50464, 407, 498, 321, 1322, 1192, 78, 322, 422, 293, 550, 15018, 341, 1329, 11, 321, 434, 516, 281, 536, 300, 309, 8866, 527, 2158, 6565, 13, 50864, 50864, 400, 264, 1036, 472, 307, 264, 2158, 295, 1958, 13, 22, 293, 1958, 13, 22, 11, 597, 307, 264, 5598, 13, 51164, 51164, 407, 341, 307, 422, 293, 550, 341, 307, 426, 13, 400, 550, 439, 264, 661, 13891, 483, 9897, 484, 949, 309, 13, 51564, 51564, 407, 300, 15182, 264, 1192, 4383, 4295, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10590170860290528, "compression_ratio": 1.6650717703349283, "no_speech_prob": 1.544564111100044e-05}, {"id": 888, "seek": 481200, "start": 4812.0, "end": 4820.0, "text": " And really what we're doing now is we're just calling that underscore backward on all of the nodes in a topological order.", "tokens": [50364, 400, 534, 437, 321, 434, 884, 586, 307, 321, 434, 445, 5141, 300, 37556, 23897, 322, 439, 295, 264, 13891, 294, 257, 1192, 4383, 1668, 13, 50764, 50764, 407, 498, 321, 445, 14322, 264, 2771, 2448, 11, 436, 434, 439, 4018, 13, 708, 630, 321, 360, 30, 51014, 51014, 492, 1409, 538, 3287, 422, 13, 7165, 281, 312, 472, 13, 663, 311, 264, 3096, 1389, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09946958678109305, "compression_ratio": 1.4709302325581395, "no_speech_prob": 2.0784205844393e-05}, {"id": 889, "seek": 481200, "start": 4820.0, "end": 4825.0, "text": " So if we just reset the gradients, they're all zero. What did we do?", "tokens": [50364, 400, 534, 437, 321, 434, 884, 586, 307, 321, 434, 445, 5141, 300, 37556, 23897, 322, 439, 295, 264, 13891, 294, 257, 1192, 4383, 1668, 13, 50764, 50764, 407, 498, 321, 445, 14322, 264, 2771, 2448, 11, 436, 434, 439, 4018, 13, 708, 630, 321, 360, 30, 51014, 51014, 492, 1409, 538, 3287, 422, 13, 7165, 281, 312, 472, 13, 663, 311, 264, 3096, 1389, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09946958678109305, "compression_ratio": 1.4709302325581395, "no_speech_prob": 2.0784205844393e-05}, {"id": 890, "seek": 481200, "start": 4825.0, "end": 4834.0, "text": " We started by setting O.grad to be one. That's the base case.", "tokens": [50364, 400, 534, 437, 321, 434, 884, 586, 307, 321, 434, 445, 5141, 300, 37556, 23897, 322, 439, 295, 264, 13891, 294, 257, 1192, 4383, 1668, 13, 50764, 50764, 407, 498, 321, 445, 14322, 264, 2771, 2448, 11, 436, 434, 439, 4018, 13, 708, 630, 321, 360, 30, 51014, 51014, 492, 1409, 538, 3287, 422, 13, 7165, 281, 312, 472, 13, 663, 311, 264, 3096, 1389, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09946958678109305, "compression_ratio": 1.4709302325581395, "no_speech_prob": 2.0784205844393e-05}, {"id": 891, "seek": 483400, "start": 4834.0, "end": 4847.0, "text": " And then we built a topological order and then we went for node in reversed of topo.", "tokens": [50364, 400, 550, 321, 3094, 257, 1192, 4383, 1668, 293, 550, 321, 1437, 337, 9984, 294, 30563, 295, 1192, 78, 13, 51014, 51014, 823, 294, 264, 9943, 1668, 11, 570, 341, 1329, 1709, 490, 11, 291, 458, 11, 321, 643, 281, 352, 807, 309, 294, 30563, 1668, 13, 51364, 51364, 407, 2891, 412, 422, 11, 9984, 23897, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1497191132092085, "compression_ratio": 1.5066666666666666, "no_speech_prob": 2.4682045477675274e-05}, {"id": 892, "seek": 483400, "start": 4847.0, "end": 4854.0, "text": " Now in the reverse order, because this list goes from, you know, we need to go through it in reversed order.", "tokens": [50364, 400, 550, 321, 3094, 257, 1192, 4383, 1668, 293, 550, 321, 1437, 337, 9984, 294, 30563, 295, 1192, 78, 13, 51014, 51014, 823, 294, 264, 9943, 1668, 11, 570, 341, 1329, 1709, 490, 11, 291, 458, 11, 321, 643, 281, 352, 807, 309, 294, 30563, 1668, 13, 51364, 51364, 407, 2891, 412, 422, 11, 9984, 23897, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1497191132092085, "compression_ratio": 1.5066666666666666, "no_speech_prob": 2.4682045477675274e-05}, {"id": 893, "seek": 483400, "start": 4854.0, "end": 4859.0, "text": " So starting at O, node backward.", "tokens": [50364, 400, 550, 321, 3094, 257, 1192, 4383, 1668, 293, 550, 321, 1437, 337, 9984, 294, 30563, 295, 1192, 78, 13, 51014, 51014, 823, 294, 264, 9943, 1668, 11, 570, 341, 1329, 1709, 490, 11, 291, 458, 11, 321, 643, 281, 352, 807, 309, 294, 30563, 1668, 13, 51364, 51364, 407, 2891, 412, 422, 11, 9984, 23897, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1497191132092085, "compression_ratio": 1.5066666666666666, "no_speech_prob": 2.4682045477675274e-05}, {"id": 894, "seek": 485900, "start": 4859.0, "end": 4866.0, "text": " And this should be it. There we go.", "tokens": [50364, 400, 341, 820, 312, 309, 13, 821, 321, 352, 13, 50714, 50714, 3950, 366, 264, 3006, 33733, 13, 50764, 50764, 6288, 11, 321, 366, 516, 281, 6479, 341, 14980, 13, 50914, 50914, 407, 286, 478, 516, 281, 5055, 341, 293, 321, 434, 516, 281, 6479, 309, 1854, 264, 2158, 1508, 570, 321, 500, 380, 528, 281, 362, 439, 300, 3089, 8493, 926, 13, 51314, 51314, 407, 2602, 295, 364, 37556, 23897, 11, 321, 434, 586, 516, 281, 6964, 364, 3539, 23897, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.056956756946652434, "compression_ratio": 1.6328502415458936, "no_speech_prob": 2.2826350686955266e-05}, {"id": 895, "seek": 485900, "start": 4866.0, "end": 4867.0, "text": " Those are the correct derivatives.", "tokens": [50364, 400, 341, 820, 312, 309, 13, 821, 321, 352, 13, 50714, 50714, 3950, 366, 264, 3006, 33733, 13, 50764, 50764, 6288, 11, 321, 366, 516, 281, 6479, 341, 14980, 13, 50914, 50914, 407, 286, 478, 516, 281, 5055, 341, 293, 321, 434, 516, 281, 6479, 309, 1854, 264, 2158, 1508, 570, 321, 500, 380, 528, 281, 362, 439, 300, 3089, 8493, 926, 13, 51314, 51314, 407, 2602, 295, 364, 37556, 23897, 11, 321, 434, 586, 516, 281, 6964, 364, 3539, 23897, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.056956756946652434, "compression_ratio": 1.6328502415458936, "no_speech_prob": 2.2826350686955266e-05}, {"id": 896, "seek": 485900, "start": 4867.0, "end": 4870.0, "text": " Finally, we are going to hide this functionality.", "tokens": [50364, 400, 341, 820, 312, 309, 13, 821, 321, 352, 13, 50714, 50714, 3950, 366, 264, 3006, 33733, 13, 50764, 50764, 6288, 11, 321, 366, 516, 281, 6479, 341, 14980, 13, 50914, 50914, 407, 286, 478, 516, 281, 5055, 341, 293, 321, 434, 516, 281, 6479, 309, 1854, 264, 2158, 1508, 570, 321, 500, 380, 528, 281, 362, 439, 300, 3089, 8493, 926, 13, 51314, 51314, 407, 2602, 295, 364, 37556, 23897, 11, 321, 434, 586, 516, 281, 6964, 364, 3539, 23897, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.056956756946652434, "compression_ratio": 1.6328502415458936, "no_speech_prob": 2.2826350686955266e-05}, {"id": 897, "seek": 485900, "start": 4870.0, "end": 4878.0, "text": " So I'm going to copy this and we're going to hide it inside the value class because we don't want to have all that code lying around.", "tokens": [50364, 400, 341, 820, 312, 309, 13, 821, 321, 352, 13, 50714, 50714, 3950, 366, 264, 3006, 33733, 13, 50764, 50764, 6288, 11, 321, 366, 516, 281, 6479, 341, 14980, 13, 50914, 50914, 407, 286, 478, 516, 281, 5055, 341, 293, 321, 434, 516, 281, 6479, 309, 1854, 264, 2158, 1508, 570, 321, 500, 380, 528, 281, 362, 439, 300, 3089, 8493, 926, 13, 51314, 51314, 407, 2602, 295, 364, 37556, 23897, 11, 321, 434, 586, 516, 281, 6964, 364, 3539, 23897, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.056956756946652434, "compression_ratio": 1.6328502415458936, "no_speech_prob": 2.2826350686955266e-05}, {"id": 898, "seek": 485900, "start": 4878.0, "end": 4882.0, "text": " So instead of an underscore backward, we're now going to define an actual backward.", "tokens": [50364, 400, 341, 820, 312, 309, 13, 821, 321, 352, 13, 50714, 50714, 3950, 366, 264, 3006, 33733, 13, 50764, 50764, 6288, 11, 321, 366, 516, 281, 6479, 341, 14980, 13, 50914, 50914, 407, 286, 478, 516, 281, 5055, 341, 293, 321, 434, 516, 281, 6479, 309, 1854, 264, 2158, 1508, 570, 321, 500, 380, 528, 281, 362, 439, 300, 3089, 8493, 926, 13, 51314, 51314, 407, 2602, 295, 364, 37556, 23897, 11, 321, 434, 586, 516, 281, 6964, 364, 3539, 23897, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.056956756946652434, "compression_ratio": 1.6328502415458936, "no_speech_prob": 2.2826350686955266e-05}, {"id": 899, "seek": 488200, "start": 4882.0, "end": 4889.0, "text": " So that backward without the underscore and that's going to do all the stuff that we just arrived.", "tokens": [50364, 407, 300, 23897, 1553, 264, 37556, 293, 300, 311, 516, 281, 360, 439, 264, 1507, 300, 321, 445, 6678, 13, 50714, 50714, 407, 718, 385, 445, 2541, 341, 493, 257, 707, 857, 13, 50814, 50814, 407, 321, 434, 700, 516, 281, 1322, 257, 1192, 4383, 4295, 2891, 412, 2698, 13, 51314, 51314, 407, 1322, 1192, 78, 295, 2698, 486, 1665, 5256, 264, 1192, 4383, 1668, 666, 264, 1192, 78, 1329, 11, 597, 307, 257, 2654, 7006, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08684216016604576, "compression_ratio": 1.6526315789473685, "no_speech_prob": 4.399783938424662e-05}, {"id": 900, "seek": 488200, "start": 4889.0, "end": 4891.0, "text": " So let me just clean this up a little bit.", "tokens": [50364, 407, 300, 23897, 1553, 264, 37556, 293, 300, 311, 516, 281, 360, 439, 264, 1507, 300, 321, 445, 6678, 13, 50714, 50714, 407, 718, 385, 445, 2541, 341, 493, 257, 707, 857, 13, 50814, 50814, 407, 321, 434, 700, 516, 281, 1322, 257, 1192, 4383, 4295, 2891, 412, 2698, 13, 51314, 51314, 407, 1322, 1192, 78, 295, 2698, 486, 1665, 5256, 264, 1192, 4383, 1668, 666, 264, 1192, 78, 1329, 11, 597, 307, 257, 2654, 7006, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08684216016604576, "compression_ratio": 1.6526315789473685, "no_speech_prob": 4.399783938424662e-05}, {"id": 901, "seek": 488200, "start": 4891.0, "end": 4901.0, "text": " So we're first going to build a topological graph starting at self.", "tokens": [50364, 407, 300, 23897, 1553, 264, 37556, 293, 300, 311, 516, 281, 360, 439, 264, 1507, 300, 321, 445, 6678, 13, 50714, 50714, 407, 718, 385, 445, 2541, 341, 493, 257, 707, 857, 13, 50814, 50814, 407, 321, 434, 700, 516, 281, 1322, 257, 1192, 4383, 4295, 2891, 412, 2698, 13, 51314, 51314, 407, 1322, 1192, 78, 295, 2698, 486, 1665, 5256, 264, 1192, 4383, 1668, 666, 264, 1192, 78, 1329, 11, 597, 307, 257, 2654, 7006, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08684216016604576, "compression_ratio": 1.6526315789473685, "no_speech_prob": 4.399783938424662e-05}, {"id": 902, "seek": 488200, "start": 4901.0, "end": 4909.0, "text": " So build topo of self will populate the topological order into the topo list, which is a local variable.", "tokens": [50364, 407, 300, 23897, 1553, 264, 37556, 293, 300, 311, 516, 281, 360, 439, 264, 1507, 300, 321, 445, 6678, 13, 50714, 50714, 407, 718, 385, 445, 2541, 341, 493, 257, 707, 857, 13, 50814, 50814, 407, 321, 434, 700, 516, 281, 1322, 257, 1192, 4383, 4295, 2891, 412, 2698, 13, 51314, 51314, 407, 1322, 1192, 78, 295, 2698, 486, 1665, 5256, 264, 1192, 4383, 1668, 666, 264, 1192, 78, 1329, 11, 597, 307, 257, 2654, 7006, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08684216016604576, "compression_ratio": 1.6526315789473685, "no_speech_prob": 4.399783938424662e-05}, {"id": 903, "seek": 490900, "start": 4909.0, "end": 4916.0, "text": " Then we set self that grad to be one and then for each node in the reversed list.", "tokens": [50364, 1396, 321, 992, 2698, 300, 2771, 281, 312, 472, 293, 550, 337, 1184, 9984, 294, 264, 30563, 1329, 13, 50714, 50714, 407, 2891, 412, 505, 293, 516, 281, 439, 264, 2227, 37556, 23897, 13, 51014, 51014, 400, 300, 820, 312, 309, 13, 51114, 51114, 407, 3155, 13, 51314, 51314, 2492, 760, 510, 13, 51364, 51364, 492, 6964, 13, 51464, 51464, 2264, 11, 439, 264, 2771, 82, 366, 4018, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.18050161126541764, "compression_ratio": 1.4363636363636363, "no_speech_prob": 7.36783113097772e-05}, {"id": 904, "seek": 490900, "start": 4916.0, "end": 4922.0, "text": " So starting at us and going to all the children underscore backward.", "tokens": [50364, 1396, 321, 992, 2698, 300, 2771, 281, 312, 472, 293, 550, 337, 1184, 9984, 294, 264, 30563, 1329, 13, 50714, 50714, 407, 2891, 412, 505, 293, 516, 281, 439, 264, 2227, 37556, 23897, 13, 51014, 51014, 400, 300, 820, 312, 309, 13, 51114, 51114, 407, 3155, 13, 51314, 51314, 2492, 760, 510, 13, 51364, 51364, 492, 6964, 13, 51464, 51464, 2264, 11, 439, 264, 2771, 82, 366, 4018, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.18050161126541764, "compression_ratio": 1.4363636363636363, "no_speech_prob": 7.36783113097772e-05}, {"id": 905, "seek": 490900, "start": 4922.0, "end": 4924.0, "text": " And that should be it.", "tokens": [50364, 1396, 321, 992, 2698, 300, 2771, 281, 312, 472, 293, 550, 337, 1184, 9984, 294, 264, 30563, 1329, 13, 50714, 50714, 407, 2891, 412, 505, 293, 516, 281, 439, 264, 2227, 37556, 23897, 13, 51014, 51014, 400, 300, 820, 312, 309, 13, 51114, 51114, 407, 3155, 13, 51314, 51314, 2492, 760, 510, 13, 51364, 51364, 492, 6964, 13, 51464, 51464, 2264, 11, 439, 264, 2771, 82, 366, 4018, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.18050161126541764, "compression_ratio": 1.4363636363636363, "no_speech_prob": 7.36783113097772e-05}, {"id": 906, "seek": 490900, "start": 4924.0, "end": 4928.0, "text": " So save.", "tokens": [50364, 1396, 321, 992, 2698, 300, 2771, 281, 312, 472, 293, 550, 337, 1184, 9984, 294, 264, 30563, 1329, 13, 50714, 50714, 407, 2891, 412, 505, 293, 516, 281, 439, 264, 2227, 37556, 23897, 13, 51014, 51014, 400, 300, 820, 312, 309, 13, 51114, 51114, 407, 3155, 13, 51314, 51314, 2492, 760, 510, 13, 51364, 51364, 492, 6964, 13, 51464, 51464, 2264, 11, 439, 264, 2771, 82, 366, 4018, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.18050161126541764, "compression_ratio": 1.4363636363636363, "no_speech_prob": 7.36783113097772e-05}, {"id": 907, "seek": 490900, "start": 4928.0, "end": 4929.0, "text": " Come down here.", "tokens": [50364, 1396, 321, 992, 2698, 300, 2771, 281, 312, 472, 293, 550, 337, 1184, 9984, 294, 264, 30563, 1329, 13, 50714, 50714, 407, 2891, 412, 505, 293, 516, 281, 439, 264, 2227, 37556, 23897, 13, 51014, 51014, 400, 300, 820, 312, 309, 13, 51114, 51114, 407, 3155, 13, 51314, 51314, 2492, 760, 510, 13, 51364, 51364, 492, 6964, 13, 51464, 51464, 2264, 11, 439, 264, 2771, 82, 366, 4018, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.18050161126541764, "compression_ratio": 1.4363636363636363, "no_speech_prob": 7.36783113097772e-05}, {"id": 908, "seek": 490900, "start": 4929.0, "end": 4931.0, "text": " We define.", "tokens": [50364, 1396, 321, 992, 2698, 300, 2771, 281, 312, 472, 293, 550, 337, 1184, 9984, 294, 264, 30563, 1329, 13, 50714, 50714, 407, 2891, 412, 505, 293, 516, 281, 439, 264, 2227, 37556, 23897, 13, 51014, 51014, 400, 300, 820, 312, 309, 13, 51114, 51114, 407, 3155, 13, 51314, 51314, 2492, 760, 510, 13, 51364, 51364, 492, 6964, 13, 51464, 51464, 2264, 11, 439, 264, 2771, 82, 366, 4018, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.18050161126541764, "compression_ratio": 1.4363636363636363, "no_speech_prob": 7.36783113097772e-05}, {"id": 909, "seek": 490900, "start": 4931.0, "end": 4933.0, "text": " OK, all the grads are zero.", "tokens": [50364, 1396, 321, 992, 2698, 300, 2771, 281, 312, 472, 293, 550, 337, 1184, 9984, 294, 264, 30563, 1329, 13, 50714, 50714, 407, 2891, 412, 505, 293, 516, 281, 439, 264, 2227, 37556, 23897, 13, 51014, 51014, 400, 300, 820, 312, 309, 13, 51114, 51114, 407, 3155, 13, 51314, 51314, 2492, 760, 510, 13, 51364, 51364, 492, 6964, 13, 51464, 51464, 2264, 11, 439, 264, 2771, 82, 366, 4018, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.18050161126541764, "compression_ratio": 1.4363636363636363, "no_speech_prob": 7.36783113097772e-05}, {"id": 910, "seek": 493300, "start": 4933.0, "end": 4941.0, "text": " And now we can do is backward without the underscore and.", "tokens": [50364, 400, 586, 321, 393, 360, 307, 23897, 1553, 264, 37556, 293, 13, 50764, 50764, 821, 321, 352, 13, 50864, 50864, 400, 300, 311, 300, 311, 646, 38377, 13, 51014, 51014, 2555, 337, 472, 34090, 13, 51114, 51114, 492, 4659, 380, 312, 886, 2055, 365, 4175, 11, 767, 11, 570, 321, 362, 257, 1578, 7426, 293, 321, 362, 406, 9684, 3839, 264, 7426, 570, 295, 512, 2685, 4487, 300, 321, 366, 13, 51564, 51564, 492, 362, 281, 519, 466, 558, 586, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15462054084329044, "compression_ratio": 1.5380952380952382, "no_speech_prob": 1.6027837546062074e-06}, {"id": 911, "seek": 493300, "start": 4941.0, "end": 4943.0, "text": " There we go.", "tokens": [50364, 400, 586, 321, 393, 360, 307, 23897, 1553, 264, 37556, 293, 13, 50764, 50764, 821, 321, 352, 13, 50864, 50864, 400, 300, 311, 300, 311, 646, 38377, 13, 51014, 51014, 2555, 337, 472, 34090, 13, 51114, 51114, 492, 4659, 380, 312, 886, 2055, 365, 4175, 11, 767, 11, 570, 321, 362, 257, 1578, 7426, 293, 321, 362, 406, 9684, 3839, 264, 7426, 570, 295, 512, 2685, 4487, 300, 321, 366, 13, 51564, 51564, 492, 362, 281, 519, 466, 558, 586, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15462054084329044, "compression_ratio": 1.5380952380952382, "no_speech_prob": 1.6027837546062074e-06}, {"id": 912, "seek": 493300, "start": 4943.0, "end": 4946.0, "text": " And that's that's back propagation.", "tokens": [50364, 400, 586, 321, 393, 360, 307, 23897, 1553, 264, 37556, 293, 13, 50764, 50764, 821, 321, 352, 13, 50864, 50864, 400, 300, 311, 300, 311, 646, 38377, 13, 51014, 51014, 2555, 337, 472, 34090, 13, 51114, 51114, 492, 4659, 380, 312, 886, 2055, 365, 4175, 11, 767, 11, 570, 321, 362, 257, 1578, 7426, 293, 321, 362, 406, 9684, 3839, 264, 7426, 570, 295, 512, 2685, 4487, 300, 321, 366, 13, 51564, 51564, 492, 362, 281, 519, 466, 558, 586, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15462054084329044, "compression_ratio": 1.5380952380952382, "no_speech_prob": 1.6027837546062074e-06}, {"id": 913, "seek": 493300, "start": 4946.0, "end": 4948.0, "text": " Please for one neuron.", "tokens": [50364, 400, 586, 321, 393, 360, 307, 23897, 1553, 264, 37556, 293, 13, 50764, 50764, 821, 321, 352, 13, 50864, 50864, 400, 300, 311, 300, 311, 646, 38377, 13, 51014, 51014, 2555, 337, 472, 34090, 13, 51114, 51114, 492, 4659, 380, 312, 886, 2055, 365, 4175, 11, 767, 11, 570, 321, 362, 257, 1578, 7426, 293, 321, 362, 406, 9684, 3839, 264, 7426, 570, 295, 512, 2685, 4487, 300, 321, 366, 13, 51564, 51564, 492, 362, 281, 519, 466, 558, 586, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15462054084329044, "compression_ratio": 1.5380952380952382, "no_speech_prob": 1.6027837546062074e-06}, {"id": 914, "seek": 493300, "start": 4948.0, "end": 4957.0, "text": " We shouldn't be too happy with ourselves, actually, because we have a bad bug and we have not surfaced the bug because of some specific conditions that we are.", "tokens": [50364, 400, 586, 321, 393, 360, 307, 23897, 1553, 264, 37556, 293, 13, 50764, 50764, 821, 321, 352, 13, 50864, 50864, 400, 300, 311, 300, 311, 646, 38377, 13, 51014, 51014, 2555, 337, 472, 34090, 13, 51114, 51114, 492, 4659, 380, 312, 886, 2055, 365, 4175, 11, 767, 11, 570, 321, 362, 257, 1578, 7426, 293, 321, 362, 406, 9684, 3839, 264, 7426, 570, 295, 512, 2685, 4487, 300, 321, 366, 13, 51564, 51564, 492, 362, 281, 519, 466, 558, 586, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15462054084329044, "compression_ratio": 1.5380952380952382, "no_speech_prob": 1.6027837546062074e-06}, {"id": 915, "seek": 493300, "start": 4957.0, "end": 4960.0, "text": " We have to think about right now.", "tokens": [50364, 400, 586, 321, 393, 360, 307, 23897, 1553, 264, 37556, 293, 13, 50764, 50764, 821, 321, 352, 13, 50864, 50864, 400, 300, 311, 300, 311, 646, 38377, 13, 51014, 51014, 2555, 337, 472, 34090, 13, 51114, 51114, 492, 4659, 380, 312, 886, 2055, 365, 4175, 11, 767, 11, 570, 321, 362, 257, 1578, 7426, 293, 321, 362, 406, 9684, 3839, 264, 7426, 570, 295, 512, 2685, 4487, 300, 321, 366, 13, 51564, 51564, 492, 362, 281, 519, 466, 558, 586, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15462054084329044, "compression_ratio": 1.5380952380952382, "no_speech_prob": 1.6027837546062074e-06}, {"id": 916, "seek": 496000, "start": 4960.0, "end": 4964.0, "text": " So here's the simplest case that shows the bug.", "tokens": [50364, 407, 510, 311, 264, 22811, 1389, 300, 3110, 264, 7426, 13, 50564, 50564, 6463, 286, 1884, 257, 2167, 9984, 257, 13, 50764, 50764, 400, 550, 286, 1884, 257, 363, 300, 307, 257, 1804, 257, 13, 50914, 50914, 400, 550, 286, 1219, 23897, 13, 51114, 51114, 407, 437, 311, 516, 281, 1051, 307, 257, 307, 1045, 293, 550, 257, 363, 307, 257, 1804, 257, 13, 51314, 51314, 407, 456, 311, 732, 19669, 322, 1192, 295, 1184, 661, 510, 13, 51514, 51514, 1396, 321, 393, 536, 300, 363, 307, 11, 295, 1164, 11, 264, 2128, 1320, 1985, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14240947723388672, "compression_ratio": 1.6770833333333333, "no_speech_prob": 7.527840807597386e-06}, {"id": 917, "seek": 496000, "start": 4964.0, "end": 4968.0, "text": " Say I create a single node a.", "tokens": [50364, 407, 510, 311, 264, 22811, 1389, 300, 3110, 264, 7426, 13, 50564, 50564, 6463, 286, 1884, 257, 2167, 9984, 257, 13, 50764, 50764, 400, 550, 286, 1884, 257, 363, 300, 307, 257, 1804, 257, 13, 50914, 50914, 400, 550, 286, 1219, 23897, 13, 51114, 51114, 407, 437, 311, 516, 281, 1051, 307, 257, 307, 1045, 293, 550, 257, 363, 307, 257, 1804, 257, 13, 51314, 51314, 407, 456, 311, 732, 19669, 322, 1192, 295, 1184, 661, 510, 13, 51514, 51514, 1396, 321, 393, 536, 300, 363, 307, 11, 295, 1164, 11, 264, 2128, 1320, 1985, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14240947723388672, "compression_ratio": 1.6770833333333333, "no_speech_prob": 7.527840807597386e-06}, {"id": 918, "seek": 496000, "start": 4968.0, "end": 4971.0, "text": " And then I create a B that is a plus a.", "tokens": [50364, 407, 510, 311, 264, 22811, 1389, 300, 3110, 264, 7426, 13, 50564, 50564, 6463, 286, 1884, 257, 2167, 9984, 257, 13, 50764, 50764, 400, 550, 286, 1884, 257, 363, 300, 307, 257, 1804, 257, 13, 50914, 50914, 400, 550, 286, 1219, 23897, 13, 51114, 51114, 407, 437, 311, 516, 281, 1051, 307, 257, 307, 1045, 293, 550, 257, 363, 307, 257, 1804, 257, 13, 51314, 51314, 407, 456, 311, 732, 19669, 322, 1192, 295, 1184, 661, 510, 13, 51514, 51514, 1396, 321, 393, 536, 300, 363, 307, 11, 295, 1164, 11, 264, 2128, 1320, 1985, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14240947723388672, "compression_ratio": 1.6770833333333333, "no_speech_prob": 7.527840807597386e-06}, {"id": 919, "seek": 496000, "start": 4971.0, "end": 4975.0, "text": " And then I called backward.", "tokens": [50364, 407, 510, 311, 264, 22811, 1389, 300, 3110, 264, 7426, 13, 50564, 50564, 6463, 286, 1884, 257, 2167, 9984, 257, 13, 50764, 50764, 400, 550, 286, 1884, 257, 363, 300, 307, 257, 1804, 257, 13, 50914, 50914, 400, 550, 286, 1219, 23897, 13, 51114, 51114, 407, 437, 311, 516, 281, 1051, 307, 257, 307, 1045, 293, 550, 257, 363, 307, 257, 1804, 257, 13, 51314, 51314, 407, 456, 311, 732, 19669, 322, 1192, 295, 1184, 661, 510, 13, 51514, 51514, 1396, 321, 393, 536, 300, 363, 307, 11, 295, 1164, 11, 264, 2128, 1320, 1985, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14240947723388672, "compression_ratio": 1.6770833333333333, "no_speech_prob": 7.527840807597386e-06}, {"id": 920, "seek": 496000, "start": 4975.0, "end": 4979.0, "text": " So what's going to happen is a is three and then a B is a plus a.", "tokens": [50364, 407, 510, 311, 264, 22811, 1389, 300, 3110, 264, 7426, 13, 50564, 50564, 6463, 286, 1884, 257, 2167, 9984, 257, 13, 50764, 50764, 400, 550, 286, 1884, 257, 363, 300, 307, 257, 1804, 257, 13, 50914, 50914, 400, 550, 286, 1219, 23897, 13, 51114, 51114, 407, 437, 311, 516, 281, 1051, 307, 257, 307, 1045, 293, 550, 257, 363, 307, 257, 1804, 257, 13, 51314, 51314, 407, 456, 311, 732, 19669, 322, 1192, 295, 1184, 661, 510, 13, 51514, 51514, 1396, 321, 393, 536, 300, 363, 307, 11, 295, 1164, 11, 264, 2128, 1320, 1985, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14240947723388672, "compression_ratio": 1.6770833333333333, "no_speech_prob": 7.527840807597386e-06}, {"id": 921, "seek": 496000, "start": 4979.0, "end": 4983.0, "text": " So there's two arrows on top of each other here.", "tokens": [50364, 407, 510, 311, 264, 22811, 1389, 300, 3110, 264, 7426, 13, 50564, 50564, 6463, 286, 1884, 257, 2167, 9984, 257, 13, 50764, 50764, 400, 550, 286, 1884, 257, 363, 300, 307, 257, 1804, 257, 13, 50914, 50914, 400, 550, 286, 1219, 23897, 13, 51114, 51114, 407, 437, 311, 516, 281, 1051, 307, 257, 307, 1045, 293, 550, 257, 363, 307, 257, 1804, 257, 13, 51314, 51314, 407, 456, 311, 732, 19669, 322, 1192, 295, 1184, 661, 510, 13, 51514, 51514, 1396, 321, 393, 536, 300, 363, 307, 11, 295, 1164, 11, 264, 2128, 1320, 1985, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14240947723388672, "compression_ratio": 1.6770833333333333, "no_speech_prob": 7.527840807597386e-06}, {"id": 922, "seek": 496000, "start": 4983.0, "end": 4987.0, "text": " Then we can see that B is, of course, the forward pass works.", "tokens": [50364, 407, 510, 311, 264, 22811, 1389, 300, 3110, 264, 7426, 13, 50564, 50564, 6463, 286, 1884, 257, 2167, 9984, 257, 13, 50764, 50764, 400, 550, 286, 1884, 257, 363, 300, 307, 257, 1804, 257, 13, 50914, 50914, 400, 550, 286, 1219, 23897, 13, 51114, 51114, 407, 437, 311, 516, 281, 1051, 307, 257, 307, 1045, 293, 550, 257, 363, 307, 257, 1804, 257, 13, 51314, 51314, 407, 456, 311, 732, 19669, 322, 1192, 295, 1184, 661, 510, 13, 51514, 51514, 1396, 321, 393, 536, 300, 363, 307, 11, 295, 1164, 11, 264, 2128, 1320, 1985, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14240947723388672, "compression_ratio": 1.6770833333333333, "no_speech_prob": 7.527840807597386e-06}, {"id": 923, "seek": 498700, "start": 4987.0, "end": 4990.0, "text": " B is just a plus a, which is six.", "tokens": [50364, 363, 307, 445, 257, 1804, 257, 11, 597, 307, 2309, 13, 50514, 50514, 583, 264, 16235, 510, 307, 406, 767, 3006, 300, 321, 15598, 6772, 13, 50814, 50814, 400, 300, 311, 570, 11, 295, 1164, 11, 445, 884, 33400, 294, 428, 1378, 11, 264, 13760, 295, 363, 365, 3104, 281, 257, 820, 312, 732, 13, 51414, 51414, 1485, 1804, 472, 13, 51464, 51464, 467, 311, 406, 472, 13, 51564, 51564, 5681, 1983, 3413, 11, 437, 311, 2737, 510, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12311415952794692, "compression_ratio": 1.5175879396984924, "no_speech_prob": 1.0030042858488741e-06}, {"id": 924, "seek": 498700, "start": 4990.0, "end": 4996.0, "text": " But the gradient here is not actually correct that we calculated automatically.", "tokens": [50364, 363, 307, 445, 257, 1804, 257, 11, 597, 307, 2309, 13, 50514, 50514, 583, 264, 16235, 510, 307, 406, 767, 3006, 300, 321, 15598, 6772, 13, 50814, 50814, 400, 300, 311, 570, 11, 295, 1164, 11, 445, 884, 33400, 294, 428, 1378, 11, 264, 13760, 295, 363, 365, 3104, 281, 257, 820, 312, 732, 13, 51414, 51414, 1485, 1804, 472, 13, 51464, 51464, 467, 311, 406, 472, 13, 51564, 51564, 5681, 1983, 3413, 11, 437, 311, 2737, 510, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12311415952794692, "compression_ratio": 1.5175879396984924, "no_speech_prob": 1.0030042858488741e-06}, {"id": 925, "seek": 498700, "start": 4996.0, "end": 5008.0, "text": " And that's because, of course, just doing calculus in your head, the derivative of B with respect to a should be two.", "tokens": [50364, 363, 307, 445, 257, 1804, 257, 11, 597, 307, 2309, 13, 50514, 50514, 583, 264, 16235, 510, 307, 406, 767, 3006, 300, 321, 15598, 6772, 13, 50814, 50814, 400, 300, 311, 570, 11, 295, 1164, 11, 445, 884, 33400, 294, 428, 1378, 11, 264, 13760, 295, 363, 365, 3104, 281, 257, 820, 312, 732, 13, 51414, 51414, 1485, 1804, 472, 13, 51464, 51464, 467, 311, 406, 472, 13, 51564, 51564, 5681, 1983, 3413, 11, 437, 311, 2737, 510, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12311415952794692, "compression_ratio": 1.5175879396984924, "no_speech_prob": 1.0030042858488741e-06}, {"id": 926, "seek": 498700, "start": 5008.0, "end": 5009.0, "text": " One plus one.", "tokens": [50364, 363, 307, 445, 257, 1804, 257, 11, 597, 307, 2309, 13, 50514, 50514, 583, 264, 16235, 510, 307, 406, 767, 3006, 300, 321, 15598, 6772, 13, 50814, 50814, 400, 300, 311, 570, 11, 295, 1164, 11, 445, 884, 33400, 294, 428, 1378, 11, 264, 13760, 295, 363, 365, 3104, 281, 257, 820, 312, 732, 13, 51414, 51414, 1485, 1804, 472, 13, 51464, 51464, 467, 311, 406, 472, 13, 51564, 51564, 5681, 1983, 3413, 11, 437, 311, 2737, 510, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12311415952794692, "compression_ratio": 1.5175879396984924, "no_speech_prob": 1.0030042858488741e-06}, {"id": 927, "seek": 498700, "start": 5009.0, "end": 5011.0, "text": " It's not one.", "tokens": [50364, 363, 307, 445, 257, 1804, 257, 11, 597, 307, 2309, 13, 50514, 50514, 583, 264, 16235, 510, 307, 406, 767, 3006, 300, 321, 15598, 6772, 13, 50814, 50814, 400, 300, 311, 570, 11, 295, 1164, 11, 445, 884, 33400, 294, 428, 1378, 11, 264, 13760, 295, 363, 365, 3104, 281, 257, 820, 312, 732, 13, 51414, 51414, 1485, 1804, 472, 13, 51464, 51464, 467, 311, 406, 472, 13, 51564, 51564, 5681, 1983, 3413, 11, 437, 311, 2737, 510, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12311415952794692, "compression_ratio": 1.5175879396984924, "no_speech_prob": 1.0030042858488741e-06}, {"id": 928, "seek": 498700, "start": 5011.0, "end": 5013.0, "text": " Intuitively, what's happening here, right?", "tokens": [50364, 363, 307, 445, 257, 1804, 257, 11, 597, 307, 2309, 13, 50514, 50514, 583, 264, 16235, 510, 307, 406, 767, 3006, 300, 321, 15598, 6772, 13, 50814, 50814, 400, 300, 311, 570, 11, 295, 1164, 11, 445, 884, 33400, 294, 428, 1378, 11, 264, 13760, 295, 363, 365, 3104, 281, 257, 820, 312, 732, 13, 51414, 51414, 1485, 1804, 472, 13, 51464, 51464, 467, 311, 406, 472, 13, 51564, 51564, 5681, 1983, 3413, 11, 437, 311, 2737, 510, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12311415952794692, "compression_ratio": 1.5175879396984924, "no_speech_prob": 1.0030042858488741e-06}, {"id": 929, "seek": 501300, "start": 5013.0, "end": 5017.0, "text": " B is the result of a plus a and then we called backward on it.", "tokens": [50364, 363, 307, 264, 1874, 295, 257, 1804, 257, 293, 550, 321, 1219, 23897, 322, 309, 13, 50564, 50564, 407, 718, 311, 352, 493, 293, 536, 437, 300, 775, 13, 50914, 50914, 363, 307, 257, 1874, 295, 4500, 13, 51014, 51014, 407, 484, 382, 363, 13, 51114, 51114, 400, 550, 562, 321, 1219, 23897, 11, 437, 2011, 307, 2698, 300, 2771, 390, 992, 281, 472, 293, 550, 661, 300, 2771, 390, 992, 281, 472, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09855005068656726, "compression_ratio": 1.7651006711409396, "no_speech_prob": 7.183078196248971e-06}, {"id": 930, "seek": 501300, "start": 5017.0, "end": 5024.0, "text": " So let's go up and see what that does.", "tokens": [50364, 363, 307, 264, 1874, 295, 257, 1804, 257, 293, 550, 321, 1219, 23897, 322, 309, 13, 50564, 50564, 407, 718, 311, 352, 493, 293, 536, 437, 300, 775, 13, 50914, 50914, 363, 307, 257, 1874, 295, 4500, 13, 51014, 51014, 407, 484, 382, 363, 13, 51114, 51114, 400, 550, 562, 321, 1219, 23897, 11, 437, 2011, 307, 2698, 300, 2771, 390, 992, 281, 472, 293, 550, 661, 300, 2771, 390, 992, 281, 472, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09855005068656726, "compression_ratio": 1.7651006711409396, "no_speech_prob": 7.183078196248971e-06}, {"id": 931, "seek": 501300, "start": 5024.0, "end": 5026.0, "text": " B is a result of addition.", "tokens": [50364, 363, 307, 264, 1874, 295, 257, 1804, 257, 293, 550, 321, 1219, 23897, 322, 309, 13, 50564, 50564, 407, 718, 311, 352, 493, 293, 536, 437, 300, 775, 13, 50914, 50914, 363, 307, 257, 1874, 295, 4500, 13, 51014, 51014, 407, 484, 382, 363, 13, 51114, 51114, 400, 550, 562, 321, 1219, 23897, 11, 437, 2011, 307, 2698, 300, 2771, 390, 992, 281, 472, 293, 550, 661, 300, 2771, 390, 992, 281, 472, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09855005068656726, "compression_ratio": 1.7651006711409396, "no_speech_prob": 7.183078196248971e-06}, {"id": 932, "seek": 501300, "start": 5026.0, "end": 5028.0, "text": " So out as B.", "tokens": [50364, 363, 307, 264, 1874, 295, 257, 1804, 257, 293, 550, 321, 1219, 23897, 322, 309, 13, 50564, 50564, 407, 718, 311, 352, 493, 293, 536, 437, 300, 775, 13, 50914, 50914, 363, 307, 257, 1874, 295, 4500, 13, 51014, 51014, 407, 484, 382, 363, 13, 51114, 51114, 400, 550, 562, 321, 1219, 23897, 11, 437, 2011, 307, 2698, 300, 2771, 390, 992, 281, 472, 293, 550, 661, 300, 2771, 390, 992, 281, 472, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09855005068656726, "compression_ratio": 1.7651006711409396, "no_speech_prob": 7.183078196248971e-06}, {"id": 933, "seek": 501300, "start": 5028.0, "end": 5037.0, "text": " And then when we called backward, what happened is self that grad was set to one and then other that grad was set to one.", "tokens": [50364, 363, 307, 264, 1874, 295, 257, 1804, 257, 293, 550, 321, 1219, 23897, 322, 309, 13, 50564, 50564, 407, 718, 311, 352, 493, 293, 536, 437, 300, 775, 13, 50914, 50914, 363, 307, 257, 1874, 295, 4500, 13, 51014, 51014, 407, 484, 382, 363, 13, 51114, 51114, 400, 550, 562, 321, 1219, 23897, 11, 437, 2011, 307, 2698, 300, 2771, 390, 992, 281, 472, 293, 550, 661, 300, 2771, 390, 992, 281, 472, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09855005068656726, "compression_ratio": 1.7651006711409396, "no_speech_prob": 7.183078196248971e-06}, {"id": 934, "seek": 503700, "start": 5037.0, "end": 5043.0, "text": " But because we're doing a plus a self and other are actually the exact same object.", "tokens": [50364, 583, 570, 321, 434, 884, 257, 1804, 257, 2698, 293, 661, 366, 767, 264, 1900, 912, 2657, 13, 50664, 50664, 407, 321, 366, 670, 81, 2819, 264, 16235, 13, 50814, 50814, 492, 366, 3287, 309, 281, 472, 293, 550, 321, 366, 3287, 309, 797, 281, 472, 13, 50964, 50964, 400, 300, 311, 983, 309, 10834, 412, 472, 13, 51164, 51164, 407, 300, 311, 257, 1154, 13, 51264, 51264, 821, 311, 1071, 636, 281, 536, 341, 294, 257, 707, 857, 544, 6179, 6114, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.061212441016887796, "compression_ratio": 1.5939086294416243, "no_speech_prob": 9.223247616318986e-06}, {"id": 935, "seek": 503700, "start": 5043.0, "end": 5046.0, "text": " So we are overriding the gradient.", "tokens": [50364, 583, 570, 321, 434, 884, 257, 1804, 257, 2698, 293, 661, 366, 767, 264, 1900, 912, 2657, 13, 50664, 50664, 407, 321, 366, 670, 81, 2819, 264, 16235, 13, 50814, 50814, 492, 366, 3287, 309, 281, 472, 293, 550, 321, 366, 3287, 309, 797, 281, 472, 13, 50964, 50964, 400, 300, 311, 983, 309, 10834, 412, 472, 13, 51164, 51164, 407, 300, 311, 257, 1154, 13, 51264, 51264, 821, 311, 1071, 636, 281, 536, 341, 294, 257, 707, 857, 544, 6179, 6114, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.061212441016887796, "compression_ratio": 1.5939086294416243, "no_speech_prob": 9.223247616318986e-06}, {"id": 936, "seek": 503700, "start": 5046.0, "end": 5049.0, "text": " We are setting it to one and then we are setting it again to one.", "tokens": [50364, 583, 570, 321, 434, 884, 257, 1804, 257, 2698, 293, 661, 366, 767, 264, 1900, 912, 2657, 13, 50664, 50664, 407, 321, 366, 670, 81, 2819, 264, 16235, 13, 50814, 50814, 492, 366, 3287, 309, 281, 472, 293, 550, 321, 366, 3287, 309, 797, 281, 472, 13, 50964, 50964, 400, 300, 311, 983, 309, 10834, 412, 472, 13, 51164, 51164, 407, 300, 311, 257, 1154, 13, 51264, 51264, 821, 311, 1071, 636, 281, 536, 341, 294, 257, 707, 857, 544, 6179, 6114, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.061212441016887796, "compression_ratio": 1.5939086294416243, "no_speech_prob": 9.223247616318986e-06}, {"id": 937, "seek": 503700, "start": 5049.0, "end": 5053.0, "text": " And that's why it stays at one.", "tokens": [50364, 583, 570, 321, 434, 884, 257, 1804, 257, 2698, 293, 661, 366, 767, 264, 1900, 912, 2657, 13, 50664, 50664, 407, 321, 366, 670, 81, 2819, 264, 16235, 13, 50814, 50814, 492, 366, 3287, 309, 281, 472, 293, 550, 321, 366, 3287, 309, 797, 281, 472, 13, 50964, 50964, 400, 300, 311, 983, 309, 10834, 412, 472, 13, 51164, 51164, 407, 300, 311, 257, 1154, 13, 51264, 51264, 821, 311, 1071, 636, 281, 536, 341, 294, 257, 707, 857, 544, 6179, 6114, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.061212441016887796, "compression_ratio": 1.5939086294416243, "no_speech_prob": 9.223247616318986e-06}, {"id": 938, "seek": 503700, "start": 5053.0, "end": 5055.0, "text": " So that's a problem.", "tokens": [50364, 583, 570, 321, 434, 884, 257, 1804, 257, 2698, 293, 661, 366, 767, 264, 1900, 912, 2657, 13, 50664, 50664, 407, 321, 366, 670, 81, 2819, 264, 16235, 13, 50814, 50814, 492, 366, 3287, 309, 281, 472, 293, 550, 321, 366, 3287, 309, 797, 281, 472, 13, 50964, 50964, 400, 300, 311, 983, 309, 10834, 412, 472, 13, 51164, 51164, 407, 300, 311, 257, 1154, 13, 51264, 51264, 821, 311, 1071, 636, 281, 536, 341, 294, 257, 707, 857, 544, 6179, 6114, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.061212441016887796, "compression_ratio": 1.5939086294416243, "no_speech_prob": 9.223247616318986e-06}, {"id": 939, "seek": 503700, "start": 5055.0, "end": 5061.0, "text": " There's another way to see this in a little bit more complicated expression.", "tokens": [50364, 583, 570, 321, 434, 884, 257, 1804, 257, 2698, 293, 661, 366, 767, 264, 1900, 912, 2657, 13, 50664, 50664, 407, 321, 366, 670, 81, 2819, 264, 16235, 13, 50814, 50814, 492, 366, 3287, 309, 281, 472, 293, 550, 321, 366, 3287, 309, 797, 281, 472, 13, 50964, 50964, 400, 300, 311, 983, 309, 10834, 412, 472, 13, 51164, 51164, 407, 300, 311, 257, 1154, 13, 51264, 51264, 821, 311, 1071, 636, 281, 536, 341, 294, 257, 707, 857, 544, 6179, 6114, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.061212441016887796, "compression_ratio": 1.5939086294416243, "no_speech_prob": 9.223247616318986e-06}, {"id": 940, "seek": 506100, "start": 5061.0, "end": 5072.0, "text": " So here we have a and B and then D will be the multiplication of the two and E will be the addition of the two.", "tokens": [50364, 407, 510, 321, 362, 257, 293, 363, 293, 550, 413, 486, 312, 264, 27290, 295, 264, 732, 293, 462, 486, 312, 264, 4500, 295, 264, 732, 13, 50914, 50914, 400, 550, 321, 12972, 462, 1413, 413, 281, 483, 479, 293, 550, 321, 1219, 23897, 13, 51164, 51164, 400, 613, 2771, 2448, 11, 498, 291, 1520, 11, 486, 312, 18424, 13, 51364, 51364, 407, 17879, 11, 437, 311, 2737, 510, 797, 307, 1936, 321, 434, 516, 281, 536, 364, 2734, 604, 565, 321, 764, 257, 7006, 544, 813, 1564, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12491520502234019, "compression_ratio": 1.6621004566210045, "no_speech_prob": 8.139078090607654e-06}, {"id": 941, "seek": 506100, "start": 5072.0, "end": 5077.0, "text": " And then we multiply E times D to get F and then we called backward.", "tokens": [50364, 407, 510, 321, 362, 257, 293, 363, 293, 550, 413, 486, 312, 264, 27290, 295, 264, 732, 293, 462, 486, 312, 264, 4500, 295, 264, 732, 13, 50914, 50914, 400, 550, 321, 12972, 462, 1413, 413, 281, 483, 479, 293, 550, 321, 1219, 23897, 13, 51164, 51164, 400, 613, 2771, 2448, 11, 498, 291, 1520, 11, 486, 312, 18424, 13, 51364, 51364, 407, 17879, 11, 437, 311, 2737, 510, 797, 307, 1936, 321, 434, 516, 281, 536, 364, 2734, 604, 565, 321, 764, 257, 7006, 544, 813, 1564, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12491520502234019, "compression_ratio": 1.6621004566210045, "no_speech_prob": 8.139078090607654e-06}, {"id": 942, "seek": 506100, "start": 5077.0, "end": 5081.0, "text": " And these gradients, if you check, will be incorrect.", "tokens": [50364, 407, 510, 321, 362, 257, 293, 363, 293, 550, 413, 486, 312, 264, 27290, 295, 264, 732, 293, 462, 486, 312, 264, 4500, 295, 264, 732, 13, 50914, 50914, 400, 550, 321, 12972, 462, 1413, 413, 281, 483, 479, 293, 550, 321, 1219, 23897, 13, 51164, 51164, 400, 613, 2771, 2448, 11, 498, 291, 1520, 11, 486, 312, 18424, 13, 51364, 51364, 407, 17879, 11, 437, 311, 2737, 510, 797, 307, 1936, 321, 434, 516, 281, 536, 364, 2734, 604, 565, 321, 764, 257, 7006, 544, 813, 1564, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12491520502234019, "compression_ratio": 1.6621004566210045, "no_speech_prob": 8.139078090607654e-06}, {"id": 943, "seek": 506100, "start": 5081.0, "end": 5089.0, "text": " So fundamentally, what's happening here again is basically we're going to see an issue any time we use a variable more than once.", "tokens": [50364, 407, 510, 321, 362, 257, 293, 363, 293, 550, 413, 486, 312, 264, 27290, 295, 264, 732, 293, 462, 486, 312, 264, 4500, 295, 264, 732, 13, 50914, 50914, 400, 550, 321, 12972, 462, 1413, 413, 281, 483, 479, 293, 550, 321, 1219, 23897, 13, 51164, 51164, 400, 613, 2771, 2448, 11, 498, 291, 1520, 11, 486, 312, 18424, 13, 51364, 51364, 407, 17879, 11, 437, 311, 2737, 510, 797, 307, 1936, 321, 434, 516, 281, 536, 364, 2734, 604, 565, 321, 764, 257, 7006, 544, 813, 1564, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12491520502234019, "compression_ratio": 1.6621004566210045, "no_speech_prob": 8.139078090607654e-06}, {"id": 944, "seek": 508900, "start": 5089.0, "end": 5093.0, "text": " Until now, in these expressions above, every variable is used exactly once.", "tokens": [50364, 9088, 586, 11, 294, 613, 15277, 3673, 11, 633, 7006, 307, 1143, 2293, 1564, 13, 50564, 50564, 407, 321, 994, 380, 536, 264, 2734, 13, 50664, 50664, 583, 510, 11, 498, 257, 7006, 307, 1143, 544, 813, 1564, 11, 437, 311, 516, 281, 1051, 1830, 23897, 1320, 30, 50864, 50864, 492, 434, 646, 12425, 990, 490, 479, 281, 462, 281, 413, 13, 51014, 51014, 407, 1400, 11, 370, 665, 13, 51064, 51064, 583, 586, 6915, 309, 23897, 293, 309, 30958, 1080, 2771, 2448, 281, 316, 293, 363, 13, 51264, 51264, 583, 550, 321, 808, 646, 281, 413, 293, 1219, 23897, 293, 309, 670, 81, 1875, 729, 2771, 2448, 412, 316, 293, 363, 13, 51614, 51614, 407, 300, 311, 2745, 257, 1154, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0851381468394446, "compression_ratio": 1.6958174904942966, "no_speech_prob": 2.260217570437817e-06}, {"id": 945, "seek": 508900, "start": 5093.0, "end": 5095.0, "text": " So we didn't see the issue.", "tokens": [50364, 9088, 586, 11, 294, 613, 15277, 3673, 11, 633, 7006, 307, 1143, 2293, 1564, 13, 50564, 50564, 407, 321, 994, 380, 536, 264, 2734, 13, 50664, 50664, 583, 510, 11, 498, 257, 7006, 307, 1143, 544, 813, 1564, 11, 437, 311, 516, 281, 1051, 1830, 23897, 1320, 30, 50864, 50864, 492, 434, 646, 12425, 990, 490, 479, 281, 462, 281, 413, 13, 51014, 51014, 407, 1400, 11, 370, 665, 13, 51064, 51064, 583, 586, 6915, 309, 23897, 293, 309, 30958, 1080, 2771, 2448, 281, 316, 293, 363, 13, 51264, 51264, 583, 550, 321, 808, 646, 281, 413, 293, 1219, 23897, 293, 309, 670, 81, 1875, 729, 2771, 2448, 412, 316, 293, 363, 13, 51614, 51614, 407, 300, 311, 2745, 257, 1154, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0851381468394446, "compression_ratio": 1.6958174904942966, "no_speech_prob": 2.260217570437817e-06}, {"id": 946, "seek": 508900, "start": 5095.0, "end": 5099.0, "text": " But here, if a variable is used more than once, what's going to happen during backward pass?", "tokens": [50364, 9088, 586, 11, 294, 613, 15277, 3673, 11, 633, 7006, 307, 1143, 2293, 1564, 13, 50564, 50564, 407, 321, 994, 380, 536, 264, 2734, 13, 50664, 50664, 583, 510, 11, 498, 257, 7006, 307, 1143, 544, 813, 1564, 11, 437, 311, 516, 281, 1051, 1830, 23897, 1320, 30, 50864, 50864, 492, 434, 646, 12425, 990, 490, 479, 281, 462, 281, 413, 13, 51014, 51014, 407, 1400, 11, 370, 665, 13, 51064, 51064, 583, 586, 6915, 309, 23897, 293, 309, 30958, 1080, 2771, 2448, 281, 316, 293, 363, 13, 51264, 51264, 583, 550, 321, 808, 646, 281, 413, 293, 1219, 23897, 293, 309, 670, 81, 1875, 729, 2771, 2448, 412, 316, 293, 363, 13, 51614, 51614, 407, 300, 311, 2745, 257, 1154, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0851381468394446, "compression_ratio": 1.6958174904942966, "no_speech_prob": 2.260217570437817e-06}, {"id": 947, "seek": 508900, "start": 5099.0, "end": 5102.0, "text": " We're back propagating from F to E to D.", "tokens": [50364, 9088, 586, 11, 294, 613, 15277, 3673, 11, 633, 7006, 307, 1143, 2293, 1564, 13, 50564, 50564, 407, 321, 994, 380, 536, 264, 2734, 13, 50664, 50664, 583, 510, 11, 498, 257, 7006, 307, 1143, 544, 813, 1564, 11, 437, 311, 516, 281, 1051, 1830, 23897, 1320, 30, 50864, 50864, 492, 434, 646, 12425, 990, 490, 479, 281, 462, 281, 413, 13, 51014, 51014, 407, 1400, 11, 370, 665, 13, 51064, 51064, 583, 586, 6915, 309, 23897, 293, 309, 30958, 1080, 2771, 2448, 281, 316, 293, 363, 13, 51264, 51264, 583, 550, 321, 808, 646, 281, 413, 293, 1219, 23897, 293, 309, 670, 81, 1875, 729, 2771, 2448, 412, 316, 293, 363, 13, 51614, 51614, 407, 300, 311, 2745, 257, 1154, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0851381468394446, "compression_ratio": 1.6958174904942966, "no_speech_prob": 2.260217570437817e-06}, {"id": 948, "seek": 508900, "start": 5102.0, "end": 5103.0, "text": " So far, so good.", "tokens": [50364, 9088, 586, 11, 294, 613, 15277, 3673, 11, 633, 7006, 307, 1143, 2293, 1564, 13, 50564, 50564, 407, 321, 994, 380, 536, 264, 2734, 13, 50664, 50664, 583, 510, 11, 498, 257, 7006, 307, 1143, 544, 813, 1564, 11, 437, 311, 516, 281, 1051, 1830, 23897, 1320, 30, 50864, 50864, 492, 434, 646, 12425, 990, 490, 479, 281, 462, 281, 413, 13, 51014, 51014, 407, 1400, 11, 370, 665, 13, 51064, 51064, 583, 586, 6915, 309, 23897, 293, 309, 30958, 1080, 2771, 2448, 281, 316, 293, 363, 13, 51264, 51264, 583, 550, 321, 808, 646, 281, 413, 293, 1219, 23897, 293, 309, 670, 81, 1875, 729, 2771, 2448, 412, 316, 293, 363, 13, 51614, 51614, 407, 300, 311, 2745, 257, 1154, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0851381468394446, "compression_ratio": 1.6958174904942966, "no_speech_prob": 2.260217570437817e-06}, {"id": 949, "seek": 508900, "start": 5103.0, "end": 5107.0, "text": " But now equals it backward and it deposits its gradients to A and B.", "tokens": [50364, 9088, 586, 11, 294, 613, 15277, 3673, 11, 633, 7006, 307, 1143, 2293, 1564, 13, 50564, 50564, 407, 321, 994, 380, 536, 264, 2734, 13, 50664, 50664, 583, 510, 11, 498, 257, 7006, 307, 1143, 544, 813, 1564, 11, 437, 311, 516, 281, 1051, 1830, 23897, 1320, 30, 50864, 50864, 492, 434, 646, 12425, 990, 490, 479, 281, 462, 281, 413, 13, 51014, 51014, 407, 1400, 11, 370, 665, 13, 51064, 51064, 583, 586, 6915, 309, 23897, 293, 309, 30958, 1080, 2771, 2448, 281, 316, 293, 363, 13, 51264, 51264, 583, 550, 321, 808, 646, 281, 413, 293, 1219, 23897, 293, 309, 670, 81, 1875, 729, 2771, 2448, 412, 316, 293, 363, 13, 51614, 51614, 407, 300, 311, 2745, 257, 1154, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0851381468394446, "compression_ratio": 1.6958174904942966, "no_speech_prob": 2.260217570437817e-06}, {"id": 950, "seek": 508900, "start": 5107.0, "end": 5114.0, "text": " But then we come back to D and called backward and it overrides those gradients at A and B.", "tokens": [50364, 9088, 586, 11, 294, 613, 15277, 3673, 11, 633, 7006, 307, 1143, 2293, 1564, 13, 50564, 50564, 407, 321, 994, 380, 536, 264, 2734, 13, 50664, 50664, 583, 510, 11, 498, 257, 7006, 307, 1143, 544, 813, 1564, 11, 437, 311, 516, 281, 1051, 1830, 23897, 1320, 30, 50864, 50864, 492, 434, 646, 12425, 990, 490, 479, 281, 462, 281, 413, 13, 51014, 51014, 407, 1400, 11, 370, 665, 13, 51064, 51064, 583, 586, 6915, 309, 23897, 293, 309, 30958, 1080, 2771, 2448, 281, 316, 293, 363, 13, 51264, 51264, 583, 550, 321, 808, 646, 281, 413, 293, 1219, 23897, 293, 309, 670, 81, 1875, 729, 2771, 2448, 412, 316, 293, 363, 13, 51614, 51614, 407, 300, 311, 2745, 257, 1154, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0851381468394446, "compression_ratio": 1.6958174904942966, "no_speech_prob": 2.260217570437817e-06}, {"id": 951, "seek": 508900, "start": 5114.0, "end": 5117.0, "text": " So that's obviously a problem.", "tokens": [50364, 9088, 586, 11, 294, 613, 15277, 3673, 11, 633, 7006, 307, 1143, 2293, 1564, 13, 50564, 50564, 407, 321, 994, 380, 536, 264, 2734, 13, 50664, 50664, 583, 510, 11, 498, 257, 7006, 307, 1143, 544, 813, 1564, 11, 437, 311, 516, 281, 1051, 1830, 23897, 1320, 30, 50864, 50864, 492, 434, 646, 12425, 990, 490, 479, 281, 462, 281, 413, 13, 51014, 51014, 407, 1400, 11, 370, 665, 13, 51064, 51064, 583, 586, 6915, 309, 23897, 293, 309, 30958, 1080, 2771, 2448, 281, 316, 293, 363, 13, 51264, 51264, 583, 550, 321, 808, 646, 281, 413, 293, 1219, 23897, 293, 309, 670, 81, 1875, 729, 2771, 2448, 412, 316, 293, 363, 13, 51614, 51614, 407, 300, 311, 2745, 257, 1154, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0851381468394446, "compression_ratio": 1.6958174904942966, "no_speech_prob": 2.260217570437817e-06}, {"id": 952, "seek": 511700, "start": 5117.0, "end": 5124.0, "text": " And the solution here, if you look at the multivariate case of the chain rule and its generalization there,", "tokens": [50364, 400, 264, 3827, 510, 11, 498, 291, 574, 412, 264, 2120, 592, 3504, 473, 1389, 295, 264, 5021, 4978, 293, 1080, 2674, 2144, 456, 11, 50714, 50714, 264, 3827, 456, 307, 1936, 300, 321, 362, 281, 33384, 613, 2771, 2448, 11, 613, 2771, 2448, 909, 13, 51014, 51014, 400, 370, 2602, 295, 3287, 729, 2771, 2448, 11, 321, 393, 2935, 360, 1804, 6915, 13, 51364, 51364, 492, 643, 281, 33384, 729, 2771, 2448, 13, 51464, 51464, 7721, 6915, 11, 1804, 6915, 11, 1804, 6915, 11, 1804, 6915, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06624766536380934, "compression_ratio": 1.9166666666666667, "no_speech_prob": 6.501814641524106e-05}, {"id": 953, "seek": 511700, "start": 5124.0, "end": 5130.0, "text": " the solution there is basically that we have to accumulate these gradients, these gradients add.", "tokens": [50364, 400, 264, 3827, 510, 11, 498, 291, 574, 412, 264, 2120, 592, 3504, 473, 1389, 295, 264, 5021, 4978, 293, 1080, 2674, 2144, 456, 11, 50714, 50714, 264, 3827, 456, 307, 1936, 300, 321, 362, 281, 33384, 613, 2771, 2448, 11, 613, 2771, 2448, 909, 13, 51014, 51014, 400, 370, 2602, 295, 3287, 729, 2771, 2448, 11, 321, 393, 2935, 360, 1804, 6915, 13, 51364, 51364, 492, 643, 281, 33384, 729, 2771, 2448, 13, 51464, 51464, 7721, 6915, 11, 1804, 6915, 11, 1804, 6915, 11, 1804, 6915, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06624766536380934, "compression_ratio": 1.9166666666666667, "no_speech_prob": 6.501814641524106e-05}, {"id": 954, "seek": 511700, "start": 5130.0, "end": 5137.0, "text": " And so instead of setting those gradients, we can simply do plus equals.", "tokens": [50364, 400, 264, 3827, 510, 11, 498, 291, 574, 412, 264, 2120, 592, 3504, 473, 1389, 295, 264, 5021, 4978, 293, 1080, 2674, 2144, 456, 11, 50714, 50714, 264, 3827, 456, 307, 1936, 300, 321, 362, 281, 33384, 613, 2771, 2448, 11, 613, 2771, 2448, 909, 13, 51014, 51014, 400, 370, 2602, 295, 3287, 729, 2771, 2448, 11, 321, 393, 2935, 360, 1804, 6915, 13, 51364, 51364, 492, 643, 281, 33384, 729, 2771, 2448, 13, 51464, 51464, 7721, 6915, 11, 1804, 6915, 11, 1804, 6915, 11, 1804, 6915, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06624766536380934, "compression_ratio": 1.9166666666666667, "no_speech_prob": 6.501814641524106e-05}, {"id": 955, "seek": 511700, "start": 5137.0, "end": 5139.0, "text": " We need to accumulate those gradients.", "tokens": [50364, 400, 264, 3827, 510, 11, 498, 291, 574, 412, 264, 2120, 592, 3504, 473, 1389, 295, 264, 5021, 4978, 293, 1080, 2674, 2144, 456, 11, 50714, 50714, 264, 3827, 456, 307, 1936, 300, 321, 362, 281, 33384, 613, 2771, 2448, 11, 613, 2771, 2448, 909, 13, 51014, 51014, 400, 370, 2602, 295, 3287, 729, 2771, 2448, 11, 321, 393, 2935, 360, 1804, 6915, 13, 51364, 51364, 492, 643, 281, 33384, 729, 2771, 2448, 13, 51464, 51464, 7721, 6915, 11, 1804, 6915, 11, 1804, 6915, 11, 1804, 6915, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06624766536380934, "compression_ratio": 1.9166666666666667, "no_speech_prob": 6.501814641524106e-05}, {"id": 956, "seek": 511700, "start": 5139.0, "end": 5146.0, "text": " Plus equals, plus equals, plus equals, plus equals.", "tokens": [50364, 400, 264, 3827, 510, 11, 498, 291, 574, 412, 264, 2120, 592, 3504, 473, 1389, 295, 264, 5021, 4978, 293, 1080, 2674, 2144, 456, 11, 50714, 50714, 264, 3827, 456, 307, 1936, 300, 321, 362, 281, 33384, 613, 2771, 2448, 11, 613, 2771, 2448, 909, 13, 51014, 51014, 400, 370, 2602, 295, 3287, 729, 2771, 2448, 11, 321, 393, 2935, 360, 1804, 6915, 13, 51364, 51364, 492, 643, 281, 33384, 729, 2771, 2448, 13, 51464, 51464, 7721, 6915, 11, 1804, 6915, 11, 1804, 6915, 11, 1804, 6915, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06624766536380934, "compression_ratio": 1.9166666666666667, "no_speech_prob": 6.501814641524106e-05}, {"id": 957, "seek": 514600, "start": 5146.0, "end": 5150.0, "text": " And this will be OK, remember, because we are initializing them at zero.", "tokens": [50364, 400, 341, 486, 312, 2264, 11, 1604, 11, 570, 321, 366, 5883, 3319, 552, 412, 4018, 13, 50564, 50564, 407, 436, 722, 412, 4018, 13, 50614, 50614, 400, 550, 604, 13150, 300, 12867, 12204, 486, 2935, 909, 13, 51014, 51014, 407, 586, 498, 321, 38818, 533, 341, 472, 11, 570, 264, 1804, 6915, 11, 341, 586, 1985, 11, 51364, 51364, 570, 316, 2771, 1409, 412, 4018, 293, 321, 1219, 363, 23897, 11, 321, 19107, 472, 293, 550, 321, 19107, 472, 797, 13, 51714, 51714, 400, 586, 341, 307, 732, 11, 597, 307, 3006, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12616865975516184, "compression_ratio": 1.7155963302752293, "no_speech_prob": 2.5070805349969305e-05}, {"id": 958, "seek": 514600, "start": 5150.0, "end": 5151.0, "text": " So they start at zero.", "tokens": [50364, 400, 341, 486, 312, 2264, 11, 1604, 11, 570, 321, 366, 5883, 3319, 552, 412, 4018, 13, 50564, 50564, 407, 436, 722, 412, 4018, 13, 50614, 50614, 400, 550, 604, 13150, 300, 12867, 12204, 486, 2935, 909, 13, 51014, 51014, 407, 586, 498, 321, 38818, 533, 341, 472, 11, 570, 264, 1804, 6915, 11, 341, 586, 1985, 11, 51364, 51364, 570, 316, 2771, 1409, 412, 4018, 293, 321, 1219, 363, 23897, 11, 321, 19107, 472, 293, 550, 321, 19107, 472, 797, 13, 51714, 51714, 400, 586, 341, 307, 732, 11, 597, 307, 3006, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12616865975516184, "compression_ratio": 1.7155963302752293, "no_speech_prob": 2.5070805349969305e-05}, {"id": 959, "seek": 514600, "start": 5151.0, "end": 5159.0, "text": " And then any contribution that flows backwards will simply add.", "tokens": [50364, 400, 341, 486, 312, 2264, 11, 1604, 11, 570, 321, 366, 5883, 3319, 552, 412, 4018, 13, 50564, 50564, 407, 436, 722, 412, 4018, 13, 50614, 50614, 400, 550, 604, 13150, 300, 12867, 12204, 486, 2935, 909, 13, 51014, 51014, 407, 586, 498, 321, 38818, 533, 341, 472, 11, 570, 264, 1804, 6915, 11, 341, 586, 1985, 11, 51364, 51364, 570, 316, 2771, 1409, 412, 4018, 293, 321, 1219, 363, 23897, 11, 321, 19107, 472, 293, 550, 321, 19107, 472, 797, 13, 51714, 51714, 400, 586, 341, 307, 732, 11, 597, 307, 3006, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12616865975516184, "compression_ratio": 1.7155963302752293, "no_speech_prob": 2.5070805349969305e-05}, {"id": 960, "seek": 514600, "start": 5159.0, "end": 5166.0, "text": " So now if we redefine this one, because the plus equals, this now works,", "tokens": [50364, 400, 341, 486, 312, 2264, 11, 1604, 11, 570, 321, 366, 5883, 3319, 552, 412, 4018, 13, 50564, 50564, 407, 436, 722, 412, 4018, 13, 50614, 50614, 400, 550, 604, 13150, 300, 12867, 12204, 486, 2935, 909, 13, 51014, 51014, 407, 586, 498, 321, 38818, 533, 341, 472, 11, 570, 264, 1804, 6915, 11, 341, 586, 1985, 11, 51364, 51364, 570, 316, 2771, 1409, 412, 4018, 293, 321, 1219, 363, 23897, 11, 321, 19107, 472, 293, 550, 321, 19107, 472, 797, 13, 51714, 51714, 400, 586, 341, 307, 732, 11, 597, 307, 3006, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12616865975516184, "compression_ratio": 1.7155963302752293, "no_speech_prob": 2.5070805349969305e-05}, {"id": 961, "seek": 514600, "start": 5166.0, "end": 5173.0, "text": " because A grad started at zero and we called B backward, we deposit one and then we deposit one again.", "tokens": [50364, 400, 341, 486, 312, 2264, 11, 1604, 11, 570, 321, 366, 5883, 3319, 552, 412, 4018, 13, 50564, 50564, 407, 436, 722, 412, 4018, 13, 50614, 50614, 400, 550, 604, 13150, 300, 12867, 12204, 486, 2935, 909, 13, 51014, 51014, 407, 586, 498, 321, 38818, 533, 341, 472, 11, 570, 264, 1804, 6915, 11, 341, 586, 1985, 11, 51364, 51364, 570, 316, 2771, 1409, 412, 4018, 293, 321, 1219, 363, 23897, 11, 321, 19107, 472, 293, 550, 321, 19107, 472, 797, 13, 51714, 51714, 400, 586, 341, 307, 732, 11, 597, 307, 3006, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12616865975516184, "compression_ratio": 1.7155963302752293, "no_speech_prob": 2.5070805349969305e-05}, {"id": 962, "seek": 514600, "start": 5173.0, "end": 5175.0, "text": " And now this is two, which is correct.", "tokens": [50364, 400, 341, 486, 312, 2264, 11, 1604, 11, 570, 321, 366, 5883, 3319, 552, 412, 4018, 13, 50564, 50564, 407, 436, 722, 412, 4018, 13, 50614, 50614, 400, 550, 604, 13150, 300, 12867, 12204, 486, 2935, 909, 13, 51014, 51014, 407, 586, 498, 321, 38818, 533, 341, 472, 11, 570, 264, 1804, 6915, 11, 341, 586, 1985, 11, 51364, 51364, 570, 316, 2771, 1409, 412, 4018, 293, 321, 1219, 363, 23897, 11, 321, 19107, 472, 293, 550, 321, 19107, 472, 797, 13, 51714, 51714, 400, 586, 341, 307, 732, 11, 597, 307, 3006, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12616865975516184, "compression_ratio": 1.7155963302752293, "no_speech_prob": 2.5070805349969305e-05}, {"id": 963, "seek": 517500, "start": 5175.0, "end": 5180.0, "text": " And here this will also work and we'll get correct gradients because when we call E backward,", "tokens": [50364, 400, 510, 341, 486, 611, 589, 293, 321, 603, 483, 3006, 2771, 2448, 570, 562, 321, 818, 462, 23897, 11, 50614, 50614, 321, 486, 19107, 264, 2771, 2448, 490, 341, 9819, 13, 50714, 50714, 400, 550, 321, 483, 281, 413, 23897, 11, 309, 486, 19107, 1080, 1065, 2771, 2448, 13, 50964, 50964, 400, 550, 729, 2771, 2448, 2935, 909, 322, 1192, 295, 1184, 661, 13, 51114, 51114, 400, 370, 321, 445, 33384, 729, 2771, 2448, 293, 300, 32539, 264, 2734, 13, 51264, 51264, 2264, 11, 586, 949, 321, 1286, 322, 11, 718, 385, 767, 360, 257, 857, 295, 40991, 510, 293, 12097, 512, 295, 613, 11, 512, 295, 341, 19376, 589, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09916700165847252, "compression_ratio": 1.7790697674418605, "no_speech_prob": 1.5206167518044822e-05}, {"id": 964, "seek": 517500, "start": 5180.0, "end": 5182.0, "text": " we will deposit the gradients from this branch.", "tokens": [50364, 400, 510, 341, 486, 611, 589, 293, 321, 603, 483, 3006, 2771, 2448, 570, 562, 321, 818, 462, 23897, 11, 50614, 50614, 321, 486, 19107, 264, 2771, 2448, 490, 341, 9819, 13, 50714, 50714, 400, 550, 321, 483, 281, 413, 23897, 11, 309, 486, 19107, 1080, 1065, 2771, 2448, 13, 50964, 50964, 400, 550, 729, 2771, 2448, 2935, 909, 322, 1192, 295, 1184, 661, 13, 51114, 51114, 400, 370, 321, 445, 33384, 729, 2771, 2448, 293, 300, 32539, 264, 2734, 13, 51264, 51264, 2264, 11, 586, 949, 321, 1286, 322, 11, 718, 385, 767, 360, 257, 857, 295, 40991, 510, 293, 12097, 512, 295, 613, 11, 512, 295, 341, 19376, 589, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09916700165847252, "compression_ratio": 1.7790697674418605, "no_speech_prob": 1.5206167518044822e-05}, {"id": 965, "seek": 517500, "start": 5182.0, "end": 5187.0, "text": " And then we get to D backward, it will deposit its own gradients.", "tokens": [50364, 400, 510, 341, 486, 611, 589, 293, 321, 603, 483, 3006, 2771, 2448, 570, 562, 321, 818, 462, 23897, 11, 50614, 50614, 321, 486, 19107, 264, 2771, 2448, 490, 341, 9819, 13, 50714, 50714, 400, 550, 321, 483, 281, 413, 23897, 11, 309, 486, 19107, 1080, 1065, 2771, 2448, 13, 50964, 50964, 400, 550, 729, 2771, 2448, 2935, 909, 322, 1192, 295, 1184, 661, 13, 51114, 51114, 400, 370, 321, 445, 33384, 729, 2771, 2448, 293, 300, 32539, 264, 2734, 13, 51264, 51264, 2264, 11, 586, 949, 321, 1286, 322, 11, 718, 385, 767, 360, 257, 857, 295, 40991, 510, 293, 12097, 512, 295, 613, 11, 512, 295, 341, 19376, 589, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09916700165847252, "compression_ratio": 1.7790697674418605, "no_speech_prob": 1.5206167518044822e-05}, {"id": 966, "seek": 517500, "start": 5187.0, "end": 5190.0, "text": " And then those gradients simply add on top of each other.", "tokens": [50364, 400, 510, 341, 486, 611, 589, 293, 321, 603, 483, 3006, 2771, 2448, 570, 562, 321, 818, 462, 23897, 11, 50614, 50614, 321, 486, 19107, 264, 2771, 2448, 490, 341, 9819, 13, 50714, 50714, 400, 550, 321, 483, 281, 413, 23897, 11, 309, 486, 19107, 1080, 1065, 2771, 2448, 13, 50964, 50964, 400, 550, 729, 2771, 2448, 2935, 909, 322, 1192, 295, 1184, 661, 13, 51114, 51114, 400, 370, 321, 445, 33384, 729, 2771, 2448, 293, 300, 32539, 264, 2734, 13, 51264, 51264, 2264, 11, 586, 949, 321, 1286, 322, 11, 718, 385, 767, 360, 257, 857, 295, 40991, 510, 293, 12097, 512, 295, 613, 11, 512, 295, 341, 19376, 589, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09916700165847252, "compression_ratio": 1.7790697674418605, "no_speech_prob": 1.5206167518044822e-05}, {"id": 967, "seek": 517500, "start": 5190.0, "end": 5193.0, "text": " And so we just accumulate those gradients and that fixes the issue.", "tokens": [50364, 400, 510, 341, 486, 611, 589, 293, 321, 603, 483, 3006, 2771, 2448, 570, 562, 321, 818, 462, 23897, 11, 50614, 50614, 321, 486, 19107, 264, 2771, 2448, 490, 341, 9819, 13, 50714, 50714, 400, 550, 321, 483, 281, 413, 23897, 11, 309, 486, 19107, 1080, 1065, 2771, 2448, 13, 50964, 50964, 400, 550, 729, 2771, 2448, 2935, 909, 322, 1192, 295, 1184, 661, 13, 51114, 51114, 400, 370, 321, 445, 33384, 729, 2771, 2448, 293, 300, 32539, 264, 2734, 13, 51264, 51264, 2264, 11, 586, 949, 321, 1286, 322, 11, 718, 385, 767, 360, 257, 857, 295, 40991, 510, 293, 12097, 512, 295, 613, 11, 512, 295, 341, 19376, 589, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09916700165847252, "compression_ratio": 1.7790697674418605, "no_speech_prob": 1.5206167518044822e-05}, {"id": 968, "seek": 517500, "start": 5193.0, "end": 5200.0, "text": " OK, now before we move on, let me actually do a bit of cleanup here and delete some of these, some of this intermediate work.", "tokens": [50364, 400, 510, 341, 486, 611, 589, 293, 321, 603, 483, 3006, 2771, 2448, 570, 562, 321, 818, 462, 23897, 11, 50614, 50614, 321, 486, 19107, 264, 2771, 2448, 490, 341, 9819, 13, 50714, 50714, 400, 550, 321, 483, 281, 413, 23897, 11, 309, 486, 19107, 1080, 1065, 2771, 2448, 13, 50964, 50964, 400, 550, 729, 2771, 2448, 2935, 909, 322, 1192, 295, 1184, 661, 13, 51114, 51114, 400, 370, 321, 445, 33384, 729, 2771, 2448, 293, 300, 32539, 264, 2734, 13, 51264, 51264, 2264, 11, 586, 949, 321, 1286, 322, 11, 718, 385, 767, 360, 257, 857, 295, 40991, 510, 293, 12097, 512, 295, 613, 11, 512, 295, 341, 19376, 589, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09916700165847252, "compression_ratio": 1.7790697674418605, "no_speech_prob": 1.5206167518044822e-05}, {"id": 969, "seek": 520000, "start": 5200.0, "end": 5206.0, "text": " So I'm not going to need any of this now that we've derived all of it.", "tokens": [50364, 407, 286, 478, 406, 516, 281, 643, 604, 295, 341, 586, 300, 321, 600, 18949, 439, 295, 309, 13, 50664, 50664, 492, 366, 516, 281, 1066, 341, 570, 286, 528, 281, 808, 646, 281, 309, 13, 50864, 50864, 49452, 264, 1266, 71, 11, 12097, 527, 2316, 1365, 11, 12097, 264, 1823, 11, 12097, 341, 11, 1066, 264, 3089, 300, 20045, 11, 51364, 51364, 293, 550, 12097, 341, 1365, 293, 1856, 2261, 787, 264, 7123, 295, 2158, 13, 51614, 51614, 400, 586, 718, 311, 808, 646, 281, 341, 2107, 12, 1889, 17409, 510, 300, 321, 12270, 264, 1266, 71, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14014070010879665, "compression_ratio": 1.75, "no_speech_prob": 3.3734162570908666e-05}, {"id": 970, "seek": 520000, "start": 5206.0, "end": 5210.0, "text": " We are going to keep this because I want to come back to it.", "tokens": [50364, 407, 286, 478, 406, 516, 281, 643, 604, 295, 341, 586, 300, 321, 600, 18949, 439, 295, 309, 13, 50664, 50664, 492, 366, 516, 281, 1066, 341, 570, 286, 528, 281, 808, 646, 281, 309, 13, 50864, 50864, 49452, 264, 1266, 71, 11, 12097, 527, 2316, 1365, 11, 12097, 264, 1823, 11, 12097, 341, 11, 1066, 264, 3089, 300, 20045, 11, 51364, 51364, 293, 550, 12097, 341, 1365, 293, 1856, 2261, 787, 264, 7123, 295, 2158, 13, 51614, 51614, 400, 586, 718, 311, 808, 646, 281, 341, 2107, 12, 1889, 17409, 510, 300, 321, 12270, 264, 1266, 71, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14014070010879665, "compression_ratio": 1.75, "no_speech_prob": 3.3734162570908666e-05}, {"id": 971, "seek": 520000, "start": 5210.0, "end": 5220.0, "text": " Delete the 10h, delete our model example, delete the step, delete this, keep the code that draws,", "tokens": [50364, 407, 286, 478, 406, 516, 281, 643, 604, 295, 341, 586, 300, 321, 600, 18949, 439, 295, 309, 13, 50664, 50664, 492, 366, 516, 281, 1066, 341, 570, 286, 528, 281, 808, 646, 281, 309, 13, 50864, 50864, 49452, 264, 1266, 71, 11, 12097, 527, 2316, 1365, 11, 12097, 264, 1823, 11, 12097, 341, 11, 1066, 264, 3089, 300, 20045, 11, 51364, 51364, 293, 550, 12097, 341, 1365, 293, 1856, 2261, 787, 264, 7123, 295, 2158, 13, 51614, 51614, 400, 586, 718, 311, 808, 646, 281, 341, 2107, 12, 1889, 17409, 510, 300, 321, 12270, 264, 1266, 71, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14014070010879665, "compression_ratio": 1.75, "no_speech_prob": 3.3734162570908666e-05}, {"id": 972, "seek": 520000, "start": 5220.0, "end": 5225.0, "text": " and then delete this example and leave behind only the definition of value.", "tokens": [50364, 407, 286, 478, 406, 516, 281, 643, 604, 295, 341, 586, 300, 321, 600, 18949, 439, 295, 309, 13, 50664, 50664, 492, 366, 516, 281, 1066, 341, 570, 286, 528, 281, 808, 646, 281, 309, 13, 50864, 50864, 49452, 264, 1266, 71, 11, 12097, 527, 2316, 1365, 11, 12097, 264, 1823, 11, 12097, 341, 11, 1066, 264, 3089, 300, 20045, 11, 51364, 51364, 293, 550, 12097, 341, 1365, 293, 1856, 2261, 787, 264, 7123, 295, 2158, 13, 51614, 51614, 400, 586, 718, 311, 808, 646, 281, 341, 2107, 12, 1889, 17409, 510, 300, 321, 12270, 264, 1266, 71, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14014070010879665, "compression_ratio": 1.75, "no_speech_prob": 3.3734162570908666e-05}, {"id": 973, "seek": 520000, "start": 5225.0, "end": 5229.0, "text": " And now let's come back to this non-linearity here that we implemented the 10h.", "tokens": [50364, 407, 286, 478, 406, 516, 281, 643, 604, 295, 341, 586, 300, 321, 600, 18949, 439, 295, 309, 13, 50664, 50664, 492, 366, 516, 281, 1066, 341, 570, 286, 528, 281, 808, 646, 281, 309, 13, 50864, 50864, 49452, 264, 1266, 71, 11, 12097, 527, 2316, 1365, 11, 12097, 264, 1823, 11, 12097, 341, 11, 1066, 264, 3089, 300, 20045, 11, 51364, 51364, 293, 550, 12097, 341, 1365, 293, 1856, 2261, 787, 264, 7123, 295, 2158, 13, 51614, 51614, 400, 586, 718, 311, 808, 646, 281, 341, 2107, 12, 1889, 17409, 510, 300, 321, 12270, 264, 1266, 71, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14014070010879665, "compression_ratio": 1.75, "no_speech_prob": 3.3734162570908666e-05}, {"id": 974, "seek": 522900, "start": 5229.0, "end": 5238.0, "text": " Now, I told you that we could have broken down 10h into its explicit atoms in terms of other expressions if we had the exp function.", "tokens": [50364, 823, 11, 286, 1907, 291, 300, 321, 727, 362, 5463, 760, 1266, 71, 666, 1080, 13691, 16871, 294, 2115, 295, 661, 15277, 498, 321, 632, 264, 1278, 2445, 13, 50814, 50814, 407, 498, 291, 1604, 11, 1266, 71, 307, 7642, 411, 341, 293, 321, 5111, 281, 1499, 1266, 71, 382, 257, 2167, 2445, 13, 51064, 51064, 400, 321, 393, 360, 300, 570, 321, 458, 309, 311, 13760, 293, 321, 393, 646, 79, 1513, 559, 473, 807, 309, 13, 51264, 51264, 583, 321, 393, 611, 1821, 760, 1266, 71, 666, 293, 5109, 309, 382, 257, 2445, 295, 1278, 13, 51464, 51464, 400, 286, 576, 411, 281, 360, 300, 586, 570, 286, 528, 281, 7081, 281, 291, 300, 291, 483, 439, 264, 912, 3542, 293, 439, 264, 912, 2771, 2448, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06010041559549203, "compression_ratio": 1.8191489361702127, "no_speech_prob": 1.5688230632804334e-05}, {"id": 975, "seek": 522900, "start": 5238.0, "end": 5243.0, "text": " So if you remember, 10h is defined like this and we chose to develop 10h as a single function.", "tokens": [50364, 823, 11, 286, 1907, 291, 300, 321, 727, 362, 5463, 760, 1266, 71, 666, 1080, 13691, 16871, 294, 2115, 295, 661, 15277, 498, 321, 632, 264, 1278, 2445, 13, 50814, 50814, 407, 498, 291, 1604, 11, 1266, 71, 307, 7642, 411, 341, 293, 321, 5111, 281, 1499, 1266, 71, 382, 257, 2167, 2445, 13, 51064, 51064, 400, 321, 393, 360, 300, 570, 321, 458, 309, 311, 13760, 293, 321, 393, 646, 79, 1513, 559, 473, 807, 309, 13, 51264, 51264, 583, 321, 393, 611, 1821, 760, 1266, 71, 666, 293, 5109, 309, 382, 257, 2445, 295, 1278, 13, 51464, 51464, 400, 286, 576, 411, 281, 360, 300, 586, 570, 286, 528, 281, 7081, 281, 291, 300, 291, 483, 439, 264, 912, 3542, 293, 439, 264, 912, 2771, 2448, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06010041559549203, "compression_ratio": 1.8191489361702127, "no_speech_prob": 1.5688230632804334e-05}, {"id": 976, "seek": 522900, "start": 5243.0, "end": 5247.0, "text": " And we can do that because we know it's derivative and we can backpropagate through it.", "tokens": [50364, 823, 11, 286, 1907, 291, 300, 321, 727, 362, 5463, 760, 1266, 71, 666, 1080, 13691, 16871, 294, 2115, 295, 661, 15277, 498, 321, 632, 264, 1278, 2445, 13, 50814, 50814, 407, 498, 291, 1604, 11, 1266, 71, 307, 7642, 411, 341, 293, 321, 5111, 281, 1499, 1266, 71, 382, 257, 2167, 2445, 13, 51064, 51064, 400, 321, 393, 360, 300, 570, 321, 458, 309, 311, 13760, 293, 321, 393, 646, 79, 1513, 559, 473, 807, 309, 13, 51264, 51264, 583, 321, 393, 611, 1821, 760, 1266, 71, 666, 293, 5109, 309, 382, 257, 2445, 295, 1278, 13, 51464, 51464, 400, 286, 576, 411, 281, 360, 300, 586, 570, 286, 528, 281, 7081, 281, 291, 300, 291, 483, 439, 264, 912, 3542, 293, 439, 264, 912, 2771, 2448, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06010041559549203, "compression_ratio": 1.8191489361702127, "no_speech_prob": 1.5688230632804334e-05}, {"id": 977, "seek": 522900, "start": 5247.0, "end": 5251.0, "text": " But we can also break down 10h into and express it as a function of exp.", "tokens": [50364, 823, 11, 286, 1907, 291, 300, 321, 727, 362, 5463, 760, 1266, 71, 666, 1080, 13691, 16871, 294, 2115, 295, 661, 15277, 498, 321, 632, 264, 1278, 2445, 13, 50814, 50814, 407, 498, 291, 1604, 11, 1266, 71, 307, 7642, 411, 341, 293, 321, 5111, 281, 1499, 1266, 71, 382, 257, 2167, 2445, 13, 51064, 51064, 400, 321, 393, 360, 300, 570, 321, 458, 309, 311, 13760, 293, 321, 393, 646, 79, 1513, 559, 473, 807, 309, 13, 51264, 51264, 583, 321, 393, 611, 1821, 760, 1266, 71, 666, 293, 5109, 309, 382, 257, 2445, 295, 1278, 13, 51464, 51464, 400, 286, 576, 411, 281, 360, 300, 586, 570, 286, 528, 281, 7081, 281, 291, 300, 291, 483, 439, 264, 912, 3542, 293, 439, 264, 912, 2771, 2448, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06010041559549203, "compression_ratio": 1.8191489361702127, "no_speech_prob": 1.5688230632804334e-05}, {"id": 978, "seek": 522900, "start": 5251.0, "end": 5257.0, "text": " And I would like to do that now because I want to prove to you that you get all the same results and all the same gradients.", "tokens": [50364, 823, 11, 286, 1907, 291, 300, 321, 727, 362, 5463, 760, 1266, 71, 666, 1080, 13691, 16871, 294, 2115, 295, 661, 15277, 498, 321, 632, 264, 1278, 2445, 13, 50814, 50814, 407, 498, 291, 1604, 11, 1266, 71, 307, 7642, 411, 341, 293, 321, 5111, 281, 1499, 1266, 71, 382, 257, 2167, 2445, 13, 51064, 51064, 400, 321, 393, 360, 300, 570, 321, 458, 309, 311, 13760, 293, 321, 393, 646, 79, 1513, 559, 473, 807, 309, 13, 51264, 51264, 583, 321, 393, 611, 1821, 760, 1266, 71, 666, 293, 5109, 309, 382, 257, 2445, 295, 1278, 13, 51464, 51464, 400, 286, 576, 411, 281, 360, 300, 586, 570, 286, 528, 281, 7081, 281, 291, 300, 291, 483, 439, 264, 912, 3542, 293, 439, 264, 912, 2771, 2448, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06010041559549203, "compression_ratio": 1.8191489361702127, "no_speech_prob": 1.5688230632804334e-05}, {"id": 979, "seek": 525700, "start": 5257.0, "end": 5260.0, "text": " But also because it forces us to implement a few more expressions.", "tokens": [50364, 583, 611, 570, 309, 5874, 505, 281, 4445, 257, 1326, 544, 15277, 13, 50514, 50514, 467, 5874, 505, 281, 360, 37871, 6642, 11, 4500, 11, 16390, 313, 11, 10044, 11, 293, 721, 411, 300, 13, 50764, 50764, 400, 286, 519, 309, 311, 257, 665, 5380, 281, 352, 807, 257, 1326, 544, 295, 613, 13, 50914, 50914, 2264, 11, 370, 718, 311, 11369, 493, 281, 264, 7123, 295, 2158, 13, 51114, 51114, 400, 510, 11, 472, 551, 300, 321, 4362, 393, 380, 360, 307, 321, 393, 360, 411, 257, 2158, 295, 11, 584, 11, 568, 13, 15, 13, 51414, 51414, 583, 321, 393, 380, 360, 11, 291, 458, 11, 510, 11, 337, 1365, 11, 321, 528, 281, 909, 257, 5754, 502, 13, 51614, 51614, 400, 321, 393, 380, 360, 746, 411, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07368347925298355, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.00016344129107892513}, {"id": 980, "seek": 525700, "start": 5260.0, "end": 5265.0, "text": " It forces us to do exponentiation, addition, subtraction, division, and things like that.", "tokens": [50364, 583, 611, 570, 309, 5874, 505, 281, 4445, 257, 1326, 544, 15277, 13, 50514, 50514, 467, 5874, 505, 281, 360, 37871, 6642, 11, 4500, 11, 16390, 313, 11, 10044, 11, 293, 721, 411, 300, 13, 50764, 50764, 400, 286, 519, 309, 311, 257, 665, 5380, 281, 352, 807, 257, 1326, 544, 295, 613, 13, 50914, 50914, 2264, 11, 370, 718, 311, 11369, 493, 281, 264, 7123, 295, 2158, 13, 51114, 51114, 400, 510, 11, 472, 551, 300, 321, 4362, 393, 380, 360, 307, 321, 393, 360, 411, 257, 2158, 295, 11, 584, 11, 568, 13, 15, 13, 51414, 51414, 583, 321, 393, 380, 360, 11, 291, 458, 11, 510, 11, 337, 1365, 11, 321, 528, 281, 909, 257, 5754, 502, 13, 51614, 51614, 400, 321, 393, 380, 360, 746, 411, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07368347925298355, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.00016344129107892513}, {"id": 981, "seek": 525700, "start": 5265.0, "end": 5268.0, "text": " And I think it's a good exercise to go through a few more of these.", "tokens": [50364, 583, 611, 570, 309, 5874, 505, 281, 4445, 257, 1326, 544, 15277, 13, 50514, 50514, 467, 5874, 505, 281, 360, 37871, 6642, 11, 4500, 11, 16390, 313, 11, 10044, 11, 293, 721, 411, 300, 13, 50764, 50764, 400, 286, 519, 309, 311, 257, 665, 5380, 281, 352, 807, 257, 1326, 544, 295, 613, 13, 50914, 50914, 2264, 11, 370, 718, 311, 11369, 493, 281, 264, 7123, 295, 2158, 13, 51114, 51114, 400, 510, 11, 472, 551, 300, 321, 4362, 393, 380, 360, 307, 321, 393, 360, 411, 257, 2158, 295, 11, 584, 11, 568, 13, 15, 13, 51414, 51414, 583, 321, 393, 380, 360, 11, 291, 458, 11, 510, 11, 337, 1365, 11, 321, 528, 281, 909, 257, 5754, 502, 13, 51614, 51614, 400, 321, 393, 380, 360, 746, 411, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07368347925298355, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.00016344129107892513}, {"id": 982, "seek": 525700, "start": 5268.0, "end": 5272.0, "text": " OK, so let's scroll up to the definition of value.", "tokens": [50364, 583, 611, 570, 309, 5874, 505, 281, 4445, 257, 1326, 544, 15277, 13, 50514, 50514, 467, 5874, 505, 281, 360, 37871, 6642, 11, 4500, 11, 16390, 313, 11, 10044, 11, 293, 721, 411, 300, 13, 50764, 50764, 400, 286, 519, 309, 311, 257, 665, 5380, 281, 352, 807, 257, 1326, 544, 295, 613, 13, 50914, 50914, 2264, 11, 370, 718, 311, 11369, 493, 281, 264, 7123, 295, 2158, 13, 51114, 51114, 400, 510, 11, 472, 551, 300, 321, 4362, 393, 380, 360, 307, 321, 393, 360, 411, 257, 2158, 295, 11, 584, 11, 568, 13, 15, 13, 51414, 51414, 583, 321, 393, 380, 360, 11, 291, 458, 11, 510, 11, 337, 1365, 11, 321, 528, 281, 909, 257, 5754, 502, 13, 51614, 51614, 400, 321, 393, 380, 360, 746, 411, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07368347925298355, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.00016344129107892513}, {"id": 983, "seek": 525700, "start": 5272.0, "end": 5278.0, "text": " And here, one thing that we currently can't do is we can do like a value of, say, 2.0.", "tokens": [50364, 583, 611, 570, 309, 5874, 505, 281, 4445, 257, 1326, 544, 15277, 13, 50514, 50514, 467, 5874, 505, 281, 360, 37871, 6642, 11, 4500, 11, 16390, 313, 11, 10044, 11, 293, 721, 411, 300, 13, 50764, 50764, 400, 286, 519, 309, 311, 257, 665, 5380, 281, 352, 807, 257, 1326, 544, 295, 613, 13, 50914, 50914, 2264, 11, 370, 718, 311, 11369, 493, 281, 264, 7123, 295, 2158, 13, 51114, 51114, 400, 510, 11, 472, 551, 300, 321, 4362, 393, 380, 360, 307, 321, 393, 360, 411, 257, 2158, 295, 11, 584, 11, 568, 13, 15, 13, 51414, 51414, 583, 321, 393, 380, 360, 11, 291, 458, 11, 510, 11, 337, 1365, 11, 321, 528, 281, 909, 257, 5754, 502, 13, 51614, 51614, 400, 321, 393, 380, 360, 746, 411, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07368347925298355, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.00016344129107892513}, {"id": 984, "seek": 525700, "start": 5278.0, "end": 5282.0, "text": " But we can't do, you know, here, for example, we want to add a constant 1.", "tokens": [50364, 583, 611, 570, 309, 5874, 505, 281, 4445, 257, 1326, 544, 15277, 13, 50514, 50514, 467, 5874, 505, 281, 360, 37871, 6642, 11, 4500, 11, 16390, 313, 11, 10044, 11, 293, 721, 411, 300, 13, 50764, 50764, 400, 286, 519, 309, 311, 257, 665, 5380, 281, 352, 807, 257, 1326, 544, 295, 613, 13, 50914, 50914, 2264, 11, 370, 718, 311, 11369, 493, 281, 264, 7123, 295, 2158, 13, 51114, 51114, 400, 510, 11, 472, 551, 300, 321, 4362, 393, 380, 360, 307, 321, 393, 360, 411, 257, 2158, 295, 11, 584, 11, 568, 13, 15, 13, 51414, 51414, 583, 321, 393, 380, 360, 11, 291, 458, 11, 510, 11, 337, 1365, 11, 321, 528, 281, 909, 257, 5754, 502, 13, 51614, 51614, 400, 321, 393, 380, 360, 746, 411, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07368347925298355, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.00016344129107892513}, {"id": 985, "seek": 525700, "start": 5282.0, "end": 5285.0, "text": " And we can't do something like this.", "tokens": [50364, 583, 611, 570, 309, 5874, 505, 281, 4445, 257, 1326, 544, 15277, 13, 50514, 50514, 467, 5874, 505, 281, 360, 37871, 6642, 11, 4500, 11, 16390, 313, 11, 10044, 11, 293, 721, 411, 300, 13, 50764, 50764, 400, 286, 519, 309, 311, 257, 665, 5380, 281, 352, 807, 257, 1326, 544, 295, 613, 13, 50914, 50914, 2264, 11, 370, 718, 311, 11369, 493, 281, 264, 7123, 295, 2158, 13, 51114, 51114, 400, 510, 11, 472, 551, 300, 321, 4362, 393, 380, 360, 307, 321, 393, 360, 411, 257, 2158, 295, 11, 584, 11, 568, 13, 15, 13, 51414, 51414, 583, 321, 393, 380, 360, 11, 291, 458, 11, 510, 11, 337, 1365, 11, 321, 528, 281, 909, 257, 5754, 502, 13, 51614, 51614, 400, 321, 393, 380, 360, 746, 411, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07368347925298355, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.00016344129107892513}, {"id": 986, "seek": 528500, "start": 5285.0, "end": 5288.0, "text": " And we can't do it because it says int object has no attribute data.", "tokens": [50364, 400, 321, 393, 380, 360, 309, 570, 309, 1619, 560, 2657, 575, 572, 19667, 1412, 13, 50514, 50514, 663, 311, 570, 257, 1804, 472, 1487, 558, 510, 281, 909, 13, 50714, 50714, 400, 550, 661, 307, 264, 24922, 472, 13, 50864, 50864, 400, 550, 510, 11, 15329, 307, 1382, 281, 2105, 472, 5893, 1412, 13, 50964, 50964, 400, 300, 311, 406, 257, 551, 13, 51064, 51064, 663, 311, 570, 1936, 472, 307, 406, 257, 2158, 2657, 293, 321, 787, 362, 4500, 337, 2158, 6565, 13, 51314, 51314, 407, 382, 257, 1871, 295, 19283, 11, 370, 300, 321, 393, 1884, 15277, 411, 341, 293, 652, 552, 652, 2020, 11, 321, 393, 2935, 360, 746, 411, 341, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09879856109619141, "compression_ratio": 1.817829457364341, "no_speech_prob": 9.223035704053473e-06}, {"id": 987, "seek": 528500, "start": 5288.0, "end": 5292.0, "text": " That's because a plus one comes right here to add.", "tokens": [50364, 400, 321, 393, 380, 360, 309, 570, 309, 1619, 560, 2657, 575, 572, 19667, 1412, 13, 50514, 50514, 663, 311, 570, 257, 1804, 472, 1487, 558, 510, 281, 909, 13, 50714, 50714, 400, 550, 661, 307, 264, 24922, 472, 13, 50864, 50864, 400, 550, 510, 11, 15329, 307, 1382, 281, 2105, 472, 5893, 1412, 13, 50964, 50964, 400, 300, 311, 406, 257, 551, 13, 51064, 51064, 663, 311, 570, 1936, 472, 307, 406, 257, 2158, 2657, 293, 321, 787, 362, 4500, 337, 2158, 6565, 13, 51314, 51314, 407, 382, 257, 1871, 295, 19283, 11, 370, 300, 321, 393, 1884, 15277, 411, 341, 293, 652, 552, 652, 2020, 11, 321, 393, 2935, 360, 746, 411, 341, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09879856109619141, "compression_ratio": 1.817829457364341, "no_speech_prob": 9.223035704053473e-06}, {"id": 988, "seek": 528500, "start": 5292.0, "end": 5295.0, "text": " And then other is the integer one.", "tokens": [50364, 400, 321, 393, 380, 360, 309, 570, 309, 1619, 560, 2657, 575, 572, 19667, 1412, 13, 50514, 50514, 663, 311, 570, 257, 1804, 472, 1487, 558, 510, 281, 909, 13, 50714, 50714, 400, 550, 661, 307, 264, 24922, 472, 13, 50864, 50864, 400, 550, 510, 11, 15329, 307, 1382, 281, 2105, 472, 5893, 1412, 13, 50964, 50964, 400, 300, 311, 406, 257, 551, 13, 51064, 51064, 663, 311, 570, 1936, 472, 307, 406, 257, 2158, 2657, 293, 321, 787, 362, 4500, 337, 2158, 6565, 13, 51314, 51314, 407, 382, 257, 1871, 295, 19283, 11, 370, 300, 321, 393, 1884, 15277, 411, 341, 293, 652, 552, 652, 2020, 11, 321, 393, 2935, 360, 746, 411, 341, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09879856109619141, "compression_ratio": 1.817829457364341, "no_speech_prob": 9.223035704053473e-06}, {"id": 989, "seek": 528500, "start": 5295.0, "end": 5297.0, "text": " And then here, Python is trying to access one dot data.", "tokens": [50364, 400, 321, 393, 380, 360, 309, 570, 309, 1619, 560, 2657, 575, 572, 19667, 1412, 13, 50514, 50514, 663, 311, 570, 257, 1804, 472, 1487, 558, 510, 281, 909, 13, 50714, 50714, 400, 550, 661, 307, 264, 24922, 472, 13, 50864, 50864, 400, 550, 510, 11, 15329, 307, 1382, 281, 2105, 472, 5893, 1412, 13, 50964, 50964, 400, 300, 311, 406, 257, 551, 13, 51064, 51064, 663, 311, 570, 1936, 472, 307, 406, 257, 2158, 2657, 293, 321, 787, 362, 4500, 337, 2158, 6565, 13, 51314, 51314, 407, 382, 257, 1871, 295, 19283, 11, 370, 300, 321, 393, 1884, 15277, 411, 341, 293, 652, 552, 652, 2020, 11, 321, 393, 2935, 360, 746, 411, 341, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09879856109619141, "compression_ratio": 1.817829457364341, "no_speech_prob": 9.223035704053473e-06}, {"id": 990, "seek": 528500, "start": 5297.0, "end": 5299.0, "text": " And that's not a thing.", "tokens": [50364, 400, 321, 393, 380, 360, 309, 570, 309, 1619, 560, 2657, 575, 572, 19667, 1412, 13, 50514, 50514, 663, 311, 570, 257, 1804, 472, 1487, 558, 510, 281, 909, 13, 50714, 50714, 400, 550, 661, 307, 264, 24922, 472, 13, 50864, 50864, 400, 550, 510, 11, 15329, 307, 1382, 281, 2105, 472, 5893, 1412, 13, 50964, 50964, 400, 300, 311, 406, 257, 551, 13, 51064, 51064, 663, 311, 570, 1936, 472, 307, 406, 257, 2158, 2657, 293, 321, 787, 362, 4500, 337, 2158, 6565, 13, 51314, 51314, 407, 382, 257, 1871, 295, 19283, 11, 370, 300, 321, 393, 1884, 15277, 411, 341, 293, 652, 552, 652, 2020, 11, 321, 393, 2935, 360, 746, 411, 341, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09879856109619141, "compression_ratio": 1.817829457364341, "no_speech_prob": 9.223035704053473e-06}, {"id": 991, "seek": 528500, "start": 5299.0, "end": 5304.0, "text": " That's because basically one is not a value object and we only have addition for value objects.", "tokens": [50364, 400, 321, 393, 380, 360, 309, 570, 309, 1619, 560, 2657, 575, 572, 19667, 1412, 13, 50514, 50514, 663, 311, 570, 257, 1804, 472, 1487, 558, 510, 281, 909, 13, 50714, 50714, 400, 550, 661, 307, 264, 24922, 472, 13, 50864, 50864, 400, 550, 510, 11, 15329, 307, 1382, 281, 2105, 472, 5893, 1412, 13, 50964, 50964, 400, 300, 311, 406, 257, 551, 13, 51064, 51064, 663, 311, 570, 1936, 472, 307, 406, 257, 2158, 2657, 293, 321, 787, 362, 4500, 337, 2158, 6565, 13, 51314, 51314, 407, 382, 257, 1871, 295, 19283, 11, 370, 300, 321, 393, 1884, 15277, 411, 341, 293, 652, 552, 652, 2020, 11, 321, 393, 2935, 360, 746, 411, 341, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09879856109619141, "compression_ratio": 1.817829457364341, "no_speech_prob": 9.223035704053473e-06}, {"id": 992, "seek": 528500, "start": 5304.0, "end": 5312.0, "text": " So as a matter of convenience, so that we can create expressions like this and make them make sense, we can simply do something like this.", "tokens": [50364, 400, 321, 393, 380, 360, 309, 570, 309, 1619, 560, 2657, 575, 572, 19667, 1412, 13, 50514, 50514, 663, 311, 570, 257, 1804, 472, 1487, 558, 510, 281, 909, 13, 50714, 50714, 400, 550, 661, 307, 264, 24922, 472, 13, 50864, 50864, 400, 550, 510, 11, 15329, 307, 1382, 281, 2105, 472, 5893, 1412, 13, 50964, 50964, 400, 300, 311, 406, 257, 551, 13, 51064, 51064, 663, 311, 570, 1936, 472, 307, 406, 257, 2158, 2657, 293, 321, 787, 362, 4500, 337, 2158, 6565, 13, 51314, 51314, 407, 382, 257, 1871, 295, 19283, 11, 370, 300, 321, 393, 1884, 15277, 411, 341, 293, 652, 552, 652, 2020, 11, 321, 393, 2935, 360, 746, 411, 341, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09879856109619141, "compression_ratio": 1.817829457364341, "no_speech_prob": 9.223035704053473e-06}, {"id": 993, "seek": 531200, "start": 5312.0, "end": 5317.0, "text": " Basically, we let other alone if other is an instance of value.", "tokens": [50364, 8537, 11, 321, 718, 661, 3312, 498, 661, 307, 364, 5197, 295, 2158, 13, 50614, 50614, 583, 498, 309, 311, 406, 364, 5197, 295, 2158, 11, 321, 434, 516, 281, 6552, 300, 309, 311, 257, 1230, 411, 364, 24922, 420, 257, 15706, 13, 50814, 50814, 400, 321, 434, 516, 281, 2935, 7019, 309, 294, 2158, 13, 50964, 50964, 400, 550, 661, 486, 445, 1813, 2158, 295, 661, 13, 51064, 51064, 400, 550, 661, 486, 362, 257, 1412, 19667, 13, 51164, 51164, 400, 341, 820, 589, 13, 51214, 51214, 407, 498, 286, 445, 584, 341, 11, 1401, 5421, 2158, 11, 550, 341, 820, 589, 13, 51414, 51414, 821, 321, 352, 13, 51464, 51464, 2264, 11, 586, 718, 311, 360, 264, 1900, 912, 551, 337, 12972, 570, 321, 393, 380, 360, 746, 411, 341, 797, 337, 264, 1900, 912, 1778, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08171645744697198, "compression_ratio": 1.9094202898550725, "no_speech_prob": 2.1111562091391534e-05}, {"id": 994, "seek": 531200, "start": 5317.0, "end": 5321.0, "text": " But if it's not an instance of value, we're going to assume that it's a number like an integer or a float.", "tokens": [50364, 8537, 11, 321, 718, 661, 3312, 498, 661, 307, 364, 5197, 295, 2158, 13, 50614, 50614, 583, 498, 309, 311, 406, 364, 5197, 295, 2158, 11, 321, 434, 516, 281, 6552, 300, 309, 311, 257, 1230, 411, 364, 24922, 420, 257, 15706, 13, 50814, 50814, 400, 321, 434, 516, 281, 2935, 7019, 309, 294, 2158, 13, 50964, 50964, 400, 550, 661, 486, 445, 1813, 2158, 295, 661, 13, 51064, 51064, 400, 550, 661, 486, 362, 257, 1412, 19667, 13, 51164, 51164, 400, 341, 820, 589, 13, 51214, 51214, 407, 498, 286, 445, 584, 341, 11, 1401, 5421, 2158, 11, 550, 341, 820, 589, 13, 51414, 51414, 821, 321, 352, 13, 51464, 51464, 2264, 11, 586, 718, 311, 360, 264, 1900, 912, 551, 337, 12972, 570, 321, 393, 380, 360, 746, 411, 341, 797, 337, 264, 1900, 912, 1778, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08171645744697198, "compression_ratio": 1.9094202898550725, "no_speech_prob": 2.1111562091391534e-05}, {"id": 995, "seek": 531200, "start": 5321.0, "end": 5324.0, "text": " And we're going to simply wrap it in value.", "tokens": [50364, 8537, 11, 321, 718, 661, 3312, 498, 661, 307, 364, 5197, 295, 2158, 13, 50614, 50614, 583, 498, 309, 311, 406, 364, 5197, 295, 2158, 11, 321, 434, 516, 281, 6552, 300, 309, 311, 257, 1230, 411, 364, 24922, 420, 257, 15706, 13, 50814, 50814, 400, 321, 434, 516, 281, 2935, 7019, 309, 294, 2158, 13, 50964, 50964, 400, 550, 661, 486, 445, 1813, 2158, 295, 661, 13, 51064, 51064, 400, 550, 661, 486, 362, 257, 1412, 19667, 13, 51164, 51164, 400, 341, 820, 589, 13, 51214, 51214, 407, 498, 286, 445, 584, 341, 11, 1401, 5421, 2158, 11, 550, 341, 820, 589, 13, 51414, 51414, 821, 321, 352, 13, 51464, 51464, 2264, 11, 586, 718, 311, 360, 264, 1900, 912, 551, 337, 12972, 570, 321, 393, 380, 360, 746, 411, 341, 797, 337, 264, 1900, 912, 1778, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08171645744697198, "compression_ratio": 1.9094202898550725, "no_speech_prob": 2.1111562091391534e-05}, {"id": 996, "seek": 531200, "start": 5324.0, "end": 5326.0, "text": " And then other will just become value of other.", "tokens": [50364, 8537, 11, 321, 718, 661, 3312, 498, 661, 307, 364, 5197, 295, 2158, 13, 50614, 50614, 583, 498, 309, 311, 406, 364, 5197, 295, 2158, 11, 321, 434, 516, 281, 6552, 300, 309, 311, 257, 1230, 411, 364, 24922, 420, 257, 15706, 13, 50814, 50814, 400, 321, 434, 516, 281, 2935, 7019, 309, 294, 2158, 13, 50964, 50964, 400, 550, 661, 486, 445, 1813, 2158, 295, 661, 13, 51064, 51064, 400, 550, 661, 486, 362, 257, 1412, 19667, 13, 51164, 51164, 400, 341, 820, 589, 13, 51214, 51214, 407, 498, 286, 445, 584, 341, 11, 1401, 5421, 2158, 11, 550, 341, 820, 589, 13, 51414, 51414, 821, 321, 352, 13, 51464, 51464, 2264, 11, 586, 718, 311, 360, 264, 1900, 912, 551, 337, 12972, 570, 321, 393, 380, 360, 746, 411, 341, 797, 337, 264, 1900, 912, 1778, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08171645744697198, "compression_ratio": 1.9094202898550725, "no_speech_prob": 2.1111562091391534e-05}, {"id": 997, "seek": 531200, "start": 5326.0, "end": 5328.0, "text": " And then other will have a data attribute.", "tokens": [50364, 8537, 11, 321, 718, 661, 3312, 498, 661, 307, 364, 5197, 295, 2158, 13, 50614, 50614, 583, 498, 309, 311, 406, 364, 5197, 295, 2158, 11, 321, 434, 516, 281, 6552, 300, 309, 311, 257, 1230, 411, 364, 24922, 420, 257, 15706, 13, 50814, 50814, 400, 321, 434, 516, 281, 2935, 7019, 309, 294, 2158, 13, 50964, 50964, 400, 550, 661, 486, 445, 1813, 2158, 295, 661, 13, 51064, 51064, 400, 550, 661, 486, 362, 257, 1412, 19667, 13, 51164, 51164, 400, 341, 820, 589, 13, 51214, 51214, 407, 498, 286, 445, 584, 341, 11, 1401, 5421, 2158, 11, 550, 341, 820, 589, 13, 51414, 51414, 821, 321, 352, 13, 51464, 51464, 2264, 11, 586, 718, 311, 360, 264, 1900, 912, 551, 337, 12972, 570, 321, 393, 380, 360, 746, 411, 341, 797, 337, 264, 1900, 912, 1778, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08171645744697198, "compression_ratio": 1.9094202898550725, "no_speech_prob": 2.1111562091391534e-05}, {"id": 998, "seek": 531200, "start": 5328.0, "end": 5329.0, "text": " And this should work.", "tokens": [50364, 8537, 11, 321, 718, 661, 3312, 498, 661, 307, 364, 5197, 295, 2158, 13, 50614, 50614, 583, 498, 309, 311, 406, 364, 5197, 295, 2158, 11, 321, 434, 516, 281, 6552, 300, 309, 311, 257, 1230, 411, 364, 24922, 420, 257, 15706, 13, 50814, 50814, 400, 321, 434, 516, 281, 2935, 7019, 309, 294, 2158, 13, 50964, 50964, 400, 550, 661, 486, 445, 1813, 2158, 295, 661, 13, 51064, 51064, 400, 550, 661, 486, 362, 257, 1412, 19667, 13, 51164, 51164, 400, 341, 820, 589, 13, 51214, 51214, 407, 498, 286, 445, 584, 341, 11, 1401, 5421, 2158, 11, 550, 341, 820, 589, 13, 51414, 51414, 821, 321, 352, 13, 51464, 51464, 2264, 11, 586, 718, 311, 360, 264, 1900, 912, 551, 337, 12972, 570, 321, 393, 380, 360, 746, 411, 341, 797, 337, 264, 1900, 912, 1778, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08171645744697198, "compression_ratio": 1.9094202898550725, "no_speech_prob": 2.1111562091391534e-05}, {"id": 999, "seek": 531200, "start": 5329.0, "end": 5333.0, "text": " So if I just say this, read farm value, then this should work.", "tokens": [50364, 8537, 11, 321, 718, 661, 3312, 498, 661, 307, 364, 5197, 295, 2158, 13, 50614, 50614, 583, 498, 309, 311, 406, 364, 5197, 295, 2158, 11, 321, 434, 516, 281, 6552, 300, 309, 311, 257, 1230, 411, 364, 24922, 420, 257, 15706, 13, 50814, 50814, 400, 321, 434, 516, 281, 2935, 7019, 309, 294, 2158, 13, 50964, 50964, 400, 550, 661, 486, 445, 1813, 2158, 295, 661, 13, 51064, 51064, 400, 550, 661, 486, 362, 257, 1412, 19667, 13, 51164, 51164, 400, 341, 820, 589, 13, 51214, 51214, 407, 498, 286, 445, 584, 341, 11, 1401, 5421, 2158, 11, 550, 341, 820, 589, 13, 51414, 51414, 821, 321, 352, 13, 51464, 51464, 2264, 11, 586, 718, 311, 360, 264, 1900, 912, 551, 337, 12972, 570, 321, 393, 380, 360, 746, 411, 341, 797, 337, 264, 1900, 912, 1778, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08171645744697198, "compression_ratio": 1.9094202898550725, "no_speech_prob": 2.1111562091391534e-05}, {"id": 1000, "seek": 531200, "start": 5333.0, "end": 5334.0, "text": " There we go.", "tokens": [50364, 8537, 11, 321, 718, 661, 3312, 498, 661, 307, 364, 5197, 295, 2158, 13, 50614, 50614, 583, 498, 309, 311, 406, 364, 5197, 295, 2158, 11, 321, 434, 516, 281, 6552, 300, 309, 311, 257, 1230, 411, 364, 24922, 420, 257, 15706, 13, 50814, 50814, 400, 321, 434, 516, 281, 2935, 7019, 309, 294, 2158, 13, 50964, 50964, 400, 550, 661, 486, 445, 1813, 2158, 295, 661, 13, 51064, 51064, 400, 550, 661, 486, 362, 257, 1412, 19667, 13, 51164, 51164, 400, 341, 820, 589, 13, 51214, 51214, 407, 498, 286, 445, 584, 341, 11, 1401, 5421, 2158, 11, 550, 341, 820, 589, 13, 51414, 51414, 821, 321, 352, 13, 51464, 51464, 2264, 11, 586, 718, 311, 360, 264, 1900, 912, 551, 337, 12972, 570, 321, 393, 380, 360, 746, 411, 341, 797, 337, 264, 1900, 912, 1778, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08171645744697198, "compression_ratio": 1.9094202898550725, "no_speech_prob": 2.1111562091391534e-05}, {"id": 1001, "seek": 531200, "start": 5334.0, "end": 5341.0, "text": " OK, now let's do the exact same thing for multiply because we can't do something like this again for the exact same reason.", "tokens": [50364, 8537, 11, 321, 718, 661, 3312, 498, 661, 307, 364, 5197, 295, 2158, 13, 50614, 50614, 583, 498, 309, 311, 406, 364, 5197, 295, 2158, 11, 321, 434, 516, 281, 6552, 300, 309, 311, 257, 1230, 411, 364, 24922, 420, 257, 15706, 13, 50814, 50814, 400, 321, 434, 516, 281, 2935, 7019, 309, 294, 2158, 13, 50964, 50964, 400, 550, 661, 486, 445, 1813, 2158, 295, 661, 13, 51064, 51064, 400, 550, 661, 486, 362, 257, 1412, 19667, 13, 51164, 51164, 400, 341, 820, 589, 13, 51214, 51214, 407, 498, 286, 445, 584, 341, 11, 1401, 5421, 2158, 11, 550, 341, 820, 589, 13, 51414, 51414, 821, 321, 352, 13, 51464, 51464, 2264, 11, 586, 718, 311, 360, 264, 1900, 912, 551, 337, 12972, 570, 321, 393, 380, 360, 746, 411, 341, 797, 337, 264, 1900, 912, 1778, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08171645744697198, "compression_ratio": 1.9094202898550725, "no_speech_prob": 2.1111562091391534e-05}, {"id": 1002, "seek": 534100, "start": 5341.0, "end": 5343.0, "text": " So we just have to go to mall.", "tokens": [50364, 407, 321, 445, 362, 281, 352, 281, 16026, 13, 50464, 50464, 400, 498, 661, 307, 406, 257, 2158, 11, 550, 718, 311, 7019, 309, 294, 2158, 13, 50664, 50664, 961, 311, 38818, 533, 2158, 13, 50764, 50764, 400, 586, 341, 1985, 13, 50814, 50814, 823, 11, 510, 311, 257, 733, 295, 17843, 293, 406, 6322, 644, 13, 51014, 51014, 316, 1413, 732, 1985, 13, 51064, 51064, 492, 1866, 300, 13, 51114, 51114, 583, 732, 1413, 257, 11, 307, 300, 516, 281, 589, 30, 51264, 51264, 509, 1116, 2066, 309, 281, 11, 558, 30, 51364, 51364, 583, 767, 11, 309, 486, 406, 13, 51464, 51464, 400, 264, 1778, 309, 1582, 380, 307, 570, 15329, 1177, 380, 458, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11333889606570409, "compression_ratio": 1.5659574468085107, "no_speech_prob": 3.500810407786048e-06}, {"id": 1003, "seek": 534100, "start": 5343.0, "end": 5347.0, "text": " And if other is not a value, then let's wrap it in value.", "tokens": [50364, 407, 321, 445, 362, 281, 352, 281, 16026, 13, 50464, 50464, 400, 498, 661, 307, 406, 257, 2158, 11, 550, 718, 311, 7019, 309, 294, 2158, 13, 50664, 50664, 961, 311, 38818, 533, 2158, 13, 50764, 50764, 400, 586, 341, 1985, 13, 50814, 50814, 823, 11, 510, 311, 257, 733, 295, 17843, 293, 406, 6322, 644, 13, 51014, 51014, 316, 1413, 732, 1985, 13, 51064, 51064, 492, 1866, 300, 13, 51114, 51114, 583, 732, 1413, 257, 11, 307, 300, 516, 281, 589, 30, 51264, 51264, 509, 1116, 2066, 309, 281, 11, 558, 30, 51364, 51364, 583, 767, 11, 309, 486, 406, 13, 51464, 51464, 400, 264, 1778, 309, 1582, 380, 307, 570, 15329, 1177, 380, 458, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11333889606570409, "compression_ratio": 1.5659574468085107, "no_speech_prob": 3.500810407786048e-06}, {"id": 1004, "seek": 534100, "start": 5347.0, "end": 5349.0, "text": " Let's redefine value.", "tokens": [50364, 407, 321, 445, 362, 281, 352, 281, 16026, 13, 50464, 50464, 400, 498, 661, 307, 406, 257, 2158, 11, 550, 718, 311, 7019, 309, 294, 2158, 13, 50664, 50664, 961, 311, 38818, 533, 2158, 13, 50764, 50764, 400, 586, 341, 1985, 13, 50814, 50814, 823, 11, 510, 311, 257, 733, 295, 17843, 293, 406, 6322, 644, 13, 51014, 51014, 316, 1413, 732, 1985, 13, 51064, 51064, 492, 1866, 300, 13, 51114, 51114, 583, 732, 1413, 257, 11, 307, 300, 516, 281, 589, 30, 51264, 51264, 509, 1116, 2066, 309, 281, 11, 558, 30, 51364, 51364, 583, 767, 11, 309, 486, 406, 13, 51464, 51464, 400, 264, 1778, 309, 1582, 380, 307, 570, 15329, 1177, 380, 458, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11333889606570409, "compression_ratio": 1.5659574468085107, "no_speech_prob": 3.500810407786048e-06}, {"id": 1005, "seek": 534100, "start": 5349.0, "end": 5350.0, "text": " And now this works.", "tokens": [50364, 407, 321, 445, 362, 281, 352, 281, 16026, 13, 50464, 50464, 400, 498, 661, 307, 406, 257, 2158, 11, 550, 718, 311, 7019, 309, 294, 2158, 13, 50664, 50664, 961, 311, 38818, 533, 2158, 13, 50764, 50764, 400, 586, 341, 1985, 13, 50814, 50814, 823, 11, 510, 311, 257, 733, 295, 17843, 293, 406, 6322, 644, 13, 51014, 51014, 316, 1413, 732, 1985, 13, 51064, 51064, 492, 1866, 300, 13, 51114, 51114, 583, 732, 1413, 257, 11, 307, 300, 516, 281, 589, 30, 51264, 51264, 509, 1116, 2066, 309, 281, 11, 558, 30, 51364, 51364, 583, 767, 11, 309, 486, 406, 13, 51464, 51464, 400, 264, 1778, 309, 1582, 380, 307, 570, 15329, 1177, 380, 458, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11333889606570409, "compression_ratio": 1.5659574468085107, "no_speech_prob": 3.500810407786048e-06}, {"id": 1006, "seek": 534100, "start": 5350.0, "end": 5354.0, "text": " Now, here's a kind of unfortunate and not obvious part.", "tokens": [50364, 407, 321, 445, 362, 281, 352, 281, 16026, 13, 50464, 50464, 400, 498, 661, 307, 406, 257, 2158, 11, 550, 718, 311, 7019, 309, 294, 2158, 13, 50664, 50664, 961, 311, 38818, 533, 2158, 13, 50764, 50764, 400, 586, 341, 1985, 13, 50814, 50814, 823, 11, 510, 311, 257, 733, 295, 17843, 293, 406, 6322, 644, 13, 51014, 51014, 316, 1413, 732, 1985, 13, 51064, 51064, 492, 1866, 300, 13, 51114, 51114, 583, 732, 1413, 257, 11, 307, 300, 516, 281, 589, 30, 51264, 51264, 509, 1116, 2066, 309, 281, 11, 558, 30, 51364, 51364, 583, 767, 11, 309, 486, 406, 13, 51464, 51464, 400, 264, 1778, 309, 1582, 380, 307, 570, 15329, 1177, 380, 458, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11333889606570409, "compression_ratio": 1.5659574468085107, "no_speech_prob": 3.500810407786048e-06}, {"id": 1007, "seek": 534100, "start": 5354.0, "end": 5355.0, "text": " A times two works.", "tokens": [50364, 407, 321, 445, 362, 281, 352, 281, 16026, 13, 50464, 50464, 400, 498, 661, 307, 406, 257, 2158, 11, 550, 718, 311, 7019, 309, 294, 2158, 13, 50664, 50664, 961, 311, 38818, 533, 2158, 13, 50764, 50764, 400, 586, 341, 1985, 13, 50814, 50814, 823, 11, 510, 311, 257, 733, 295, 17843, 293, 406, 6322, 644, 13, 51014, 51014, 316, 1413, 732, 1985, 13, 51064, 51064, 492, 1866, 300, 13, 51114, 51114, 583, 732, 1413, 257, 11, 307, 300, 516, 281, 589, 30, 51264, 51264, 509, 1116, 2066, 309, 281, 11, 558, 30, 51364, 51364, 583, 767, 11, 309, 486, 406, 13, 51464, 51464, 400, 264, 1778, 309, 1582, 380, 307, 570, 15329, 1177, 380, 458, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11333889606570409, "compression_ratio": 1.5659574468085107, "no_speech_prob": 3.500810407786048e-06}, {"id": 1008, "seek": 534100, "start": 5355.0, "end": 5356.0, "text": " We saw that.", "tokens": [50364, 407, 321, 445, 362, 281, 352, 281, 16026, 13, 50464, 50464, 400, 498, 661, 307, 406, 257, 2158, 11, 550, 718, 311, 7019, 309, 294, 2158, 13, 50664, 50664, 961, 311, 38818, 533, 2158, 13, 50764, 50764, 400, 586, 341, 1985, 13, 50814, 50814, 823, 11, 510, 311, 257, 733, 295, 17843, 293, 406, 6322, 644, 13, 51014, 51014, 316, 1413, 732, 1985, 13, 51064, 51064, 492, 1866, 300, 13, 51114, 51114, 583, 732, 1413, 257, 11, 307, 300, 516, 281, 589, 30, 51264, 51264, 509, 1116, 2066, 309, 281, 11, 558, 30, 51364, 51364, 583, 767, 11, 309, 486, 406, 13, 51464, 51464, 400, 264, 1778, 309, 1582, 380, 307, 570, 15329, 1177, 380, 458, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11333889606570409, "compression_ratio": 1.5659574468085107, "no_speech_prob": 3.500810407786048e-06}, {"id": 1009, "seek": 534100, "start": 5356.0, "end": 5359.0, "text": " But two times a, is that going to work?", "tokens": [50364, 407, 321, 445, 362, 281, 352, 281, 16026, 13, 50464, 50464, 400, 498, 661, 307, 406, 257, 2158, 11, 550, 718, 311, 7019, 309, 294, 2158, 13, 50664, 50664, 961, 311, 38818, 533, 2158, 13, 50764, 50764, 400, 586, 341, 1985, 13, 50814, 50814, 823, 11, 510, 311, 257, 733, 295, 17843, 293, 406, 6322, 644, 13, 51014, 51014, 316, 1413, 732, 1985, 13, 51064, 51064, 492, 1866, 300, 13, 51114, 51114, 583, 732, 1413, 257, 11, 307, 300, 516, 281, 589, 30, 51264, 51264, 509, 1116, 2066, 309, 281, 11, 558, 30, 51364, 51364, 583, 767, 11, 309, 486, 406, 13, 51464, 51464, 400, 264, 1778, 309, 1582, 380, 307, 570, 15329, 1177, 380, 458, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11333889606570409, "compression_ratio": 1.5659574468085107, "no_speech_prob": 3.500810407786048e-06}, {"id": 1010, "seek": 534100, "start": 5359.0, "end": 5361.0, "text": " You'd expect it to, right?", "tokens": [50364, 407, 321, 445, 362, 281, 352, 281, 16026, 13, 50464, 50464, 400, 498, 661, 307, 406, 257, 2158, 11, 550, 718, 311, 7019, 309, 294, 2158, 13, 50664, 50664, 961, 311, 38818, 533, 2158, 13, 50764, 50764, 400, 586, 341, 1985, 13, 50814, 50814, 823, 11, 510, 311, 257, 733, 295, 17843, 293, 406, 6322, 644, 13, 51014, 51014, 316, 1413, 732, 1985, 13, 51064, 51064, 492, 1866, 300, 13, 51114, 51114, 583, 732, 1413, 257, 11, 307, 300, 516, 281, 589, 30, 51264, 51264, 509, 1116, 2066, 309, 281, 11, 558, 30, 51364, 51364, 583, 767, 11, 309, 486, 406, 13, 51464, 51464, 400, 264, 1778, 309, 1582, 380, 307, 570, 15329, 1177, 380, 458, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11333889606570409, "compression_ratio": 1.5659574468085107, "no_speech_prob": 3.500810407786048e-06}, {"id": 1011, "seek": 534100, "start": 5361.0, "end": 5363.0, "text": " But actually, it will not.", "tokens": [50364, 407, 321, 445, 362, 281, 352, 281, 16026, 13, 50464, 50464, 400, 498, 661, 307, 406, 257, 2158, 11, 550, 718, 311, 7019, 309, 294, 2158, 13, 50664, 50664, 961, 311, 38818, 533, 2158, 13, 50764, 50764, 400, 586, 341, 1985, 13, 50814, 50814, 823, 11, 510, 311, 257, 733, 295, 17843, 293, 406, 6322, 644, 13, 51014, 51014, 316, 1413, 732, 1985, 13, 51064, 51064, 492, 1866, 300, 13, 51114, 51114, 583, 732, 1413, 257, 11, 307, 300, 516, 281, 589, 30, 51264, 51264, 509, 1116, 2066, 309, 281, 11, 558, 30, 51364, 51364, 583, 767, 11, 309, 486, 406, 13, 51464, 51464, 400, 264, 1778, 309, 1582, 380, 307, 570, 15329, 1177, 380, 458, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11333889606570409, "compression_ratio": 1.5659574468085107, "no_speech_prob": 3.500810407786048e-06}, {"id": 1012, "seek": 534100, "start": 5363.0, "end": 5366.0, "text": " And the reason it won't is because Python doesn't know.", "tokens": [50364, 407, 321, 445, 362, 281, 352, 281, 16026, 13, 50464, 50464, 400, 498, 661, 307, 406, 257, 2158, 11, 550, 718, 311, 7019, 309, 294, 2158, 13, 50664, 50664, 961, 311, 38818, 533, 2158, 13, 50764, 50764, 400, 586, 341, 1985, 13, 50814, 50814, 823, 11, 510, 311, 257, 733, 295, 17843, 293, 406, 6322, 644, 13, 51014, 51014, 316, 1413, 732, 1985, 13, 51064, 51064, 492, 1866, 300, 13, 51114, 51114, 583, 732, 1413, 257, 11, 307, 300, 516, 281, 589, 30, 51264, 51264, 509, 1116, 2066, 309, 281, 11, 558, 30, 51364, 51364, 583, 767, 11, 309, 486, 406, 13, 51464, 51464, 400, 264, 1778, 309, 1582, 380, 307, 570, 15329, 1177, 380, 458, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.11333889606570409, "compression_ratio": 1.5659574468085107, "no_speech_prob": 3.500810407786048e-06}, {"id": 1013, "seek": 536600, "start": 5366.0, "end": 5376.0, "text": " Like when you do a times two, basically, so a times two, Python will go and it will basically do something like a dot mall of two.", "tokens": [50364, 1743, 562, 291, 360, 257, 1413, 732, 11, 1936, 11, 370, 257, 1413, 732, 11, 15329, 486, 352, 293, 309, 486, 1936, 360, 746, 411, 257, 5893, 16026, 295, 732, 13, 50864, 50864, 663, 311, 1936, 437, 309, 486, 818, 13, 50914, 50914, 583, 281, 309, 11, 732, 1413, 257, 307, 264, 912, 382, 732, 5893, 16026, 295, 257, 13, 51164, 51164, 400, 309, 1177, 380, 11, 732, 393, 380, 12972, 2158, 13, 51314, 51314, 400, 370, 309, 311, 534, 9019, 466, 300, 13, 51414, 51414, 407, 2602, 11, 437, 2314, 307, 294, 15329, 11, 264, 636, 341, 1985, 307, 291, 366, 1737, 281, 6964, 746, 1219, 264, 497, 16026, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11509180483610734, "compression_ratio": 1.771186440677966, "no_speech_prob": 4.539620204013772e-05}, {"id": 1014, "seek": 536600, "start": 5376.0, "end": 5377.0, "text": " That's basically what it will call.", "tokens": [50364, 1743, 562, 291, 360, 257, 1413, 732, 11, 1936, 11, 370, 257, 1413, 732, 11, 15329, 486, 352, 293, 309, 486, 1936, 360, 746, 411, 257, 5893, 16026, 295, 732, 13, 50864, 50864, 663, 311, 1936, 437, 309, 486, 818, 13, 50914, 50914, 583, 281, 309, 11, 732, 1413, 257, 307, 264, 912, 382, 732, 5893, 16026, 295, 257, 13, 51164, 51164, 400, 309, 1177, 380, 11, 732, 393, 380, 12972, 2158, 13, 51314, 51314, 400, 370, 309, 311, 534, 9019, 466, 300, 13, 51414, 51414, 407, 2602, 11, 437, 2314, 307, 294, 15329, 11, 264, 636, 341, 1985, 307, 291, 366, 1737, 281, 6964, 746, 1219, 264, 497, 16026, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11509180483610734, "compression_ratio": 1.771186440677966, "no_speech_prob": 4.539620204013772e-05}, {"id": 1015, "seek": 536600, "start": 5377.0, "end": 5382.0, "text": " But to it, two times a is the same as two dot mall of a.", "tokens": [50364, 1743, 562, 291, 360, 257, 1413, 732, 11, 1936, 11, 370, 257, 1413, 732, 11, 15329, 486, 352, 293, 309, 486, 1936, 360, 746, 411, 257, 5893, 16026, 295, 732, 13, 50864, 50864, 663, 311, 1936, 437, 309, 486, 818, 13, 50914, 50914, 583, 281, 309, 11, 732, 1413, 257, 307, 264, 912, 382, 732, 5893, 16026, 295, 257, 13, 51164, 51164, 400, 309, 1177, 380, 11, 732, 393, 380, 12972, 2158, 13, 51314, 51314, 400, 370, 309, 311, 534, 9019, 466, 300, 13, 51414, 51414, 407, 2602, 11, 437, 2314, 307, 294, 15329, 11, 264, 636, 341, 1985, 307, 291, 366, 1737, 281, 6964, 746, 1219, 264, 497, 16026, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11509180483610734, "compression_ratio": 1.771186440677966, "no_speech_prob": 4.539620204013772e-05}, {"id": 1016, "seek": 536600, "start": 5382.0, "end": 5385.0, "text": " And it doesn't, two can't multiply value.", "tokens": [50364, 1743, 562, 291, 360, 257, 1413, 732, 11, 1936, 11, 370, 257, 1413, 732, 11, 15329, 486, 352, 293, 309, 486, 1936, 360, 746, 411, 257, 5893, 16026, 295, 732, 13, 50864, 50864, 663, 311, 1936, 437, 309, 486, 818, 13, 50914, 50914, 583, 281, 309, 11, 732, 1413, 257, 307, 264, 912, 382, 732, 5893, 16026, 295, 257, 13, 51164, 51164, 400, 309, 1177, 380, 11, 732, 393, 380, 12972, 2158, 13, 51314, 51314, 400, 370, 309, 311, 534, 9019, 466, 300, 13, 51414, 51414, 407, 2602, 11, 437, 2314, 307, 294, 15329, 11, 264, 636, 341, 1985, 307, 291, 366, 1737, 281, 6964, 746, 1219, 264, 497, 16026, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11509180483610734, "compression_ratio": 1.771186440677966, "no_speech_prob": 4.539620204013772e-05}, {"id": 1017, "seek": 536600, "start": 5385.0, "end": 5387.0, "text": " And so it's really confused about that.", "tokens": [50364, 1743, 562, 291, 360, 257, 1413, 732, 11, 1936, 11, 370, 257, 1413, 732, 11, 15329, 486, 352, 293, 309, 486, 1936, 360, 746, 411, 257, 5893, 16026, 295, 732, 13, 50864, 50864, 663, 311, 1936, 437, 309, 486, 818, 13, 50914, 50914, 583, 281, 309, 11, 732, 1413, 257, 307, 264, 912, 382, 732, 5893, 16026, 295, 257, 13, 51164, 51164, 400, 309, 1177, 380, 11, 732, 393, 380, 12972, 2158, 13, 51314, 51314, 400, 370, 309, 311, 534, 9019, 466, 300, 13, 51414, 51414, 407, 2602, 11, 437, 2314, 307, 294, 15329, 11, 264, 636, 341, 1985, 307, 291, 366, 1737, 281, 6964, 746, 1219, 264, 497, 16026, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11509180483610734, "compression_ratio": 1.771186440677966, "no_speech_prob": 4.539620204013772e-05}, {"id": 1018, "seek": 536600, "start": 5387.0, "end": 5394.0, "text": " So instead, what happens is in Python, the way this works is you are free to define something called the R mall.", "tokens": [50364, 1743, 562, 291, 360, 257, 1413, 732, 11, 1936, 11, 370, 257, 1413, 732, 11, 15329, 486, 352, 293, 309, 486, 1936, 360, 746, 411, 257, 5893, 16026, 295, 732, 13, 50864, 50864, 663, 311, 1936, 437, 309, 486, 818, 13, 50914, 50914, 583, 281, 309, 11, 732, 1413, 257, 307, 264, 912, 382, 732, 5893, 16026, 295, 257, 13, 51164, 51164, 400, 309, 1177, 380, 11, 732, 393, 380, 12972, 2158, 13, 51314, 51314, 400, 370, 309, 311, 534, 9019, 466, 300, 13, 51414, 51414, 407, 2602, 11, 437, 2314, 307, 294, 15329, 11, 264, 636, 341, 1985, 307, 291, 366, 1737, 281, 6964, 746, 1219, 264, 497, 16026, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11509180483610734, "compression_ratio": 1.771186440677966, "no_speech_prob": 4.539620204013772e-05}, {"id": 1019, "seek": 539400, "start": 5394.0, "end": 5397.0, "text": " And R mall is kind of like a fallback.", "tokens": [50364, 400, 497, 16026, 307, 733, 295, 411, 257, 2100, 3207, 13, 50514, 50514, 407, 498, 15329, 393, 380, 360, 732, 1413, 257, 11, 309, 486, 1520, 498, 538, 604, 2931, 257, 3255, 577, 281, 12972, 732, 13, 50914, 50914, 400, 300, 486, 312, 1219, 666, 497, 16026, 13, 51064, 51064, 407, 570, 15329, 393, 380, 360, 732, 1413, 257, 11, 309, 486, 1520, 307, 456, 364, 497, 16026, 294, 2158, 13, 51314, 51314, 400, 570, 456, 307, 11, 309, 486, 586, 818, 300, 13, 51464, 51464, 400, 437, 321, 603, 360, 510, 307, 321, 486, 18135, 264, 1668, 295, 264, 2208, 2967, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07185342378705462, "compression_ratio": 1.829145728643216, "no_speech_prob": 2.2473490389529616e-05}, {"id": 1020, "seek": 539400, "start": 5397.0, "end": 5405.0, "text": " So if Python can't do two times a, it will check if by any chance a knows how to multiply two.", "tokens": [50364, 400, 497, 16026, 307, 733, 295, 411, 257, 2100, 3207, 13, 50514, 50514, 407, 498, 15329, 393, 380, 360, 732, 1413, 257, 11, 309, 486, 1520, 498, 538, 604, 2931, 257, 3255, 577, 281, 12972, 732, 13, 50914, 50914, 400, 300, 486, 312, 1219, 666, 497, 16026, 13, 51064, 51064, 407, 570, 15329, 393, 380, 360, 732, 1413, 257, 11, 309, 486, 1520, 307, 456, 364, 497, 16026, 294, 2158, 13, 51314, 51314, 400, 570, 456, 307, 11, 309, 486, 586, 818, 300, 13, 51464, 51464, 400, 437, 321, 603, 360, 510, 307, 321, 486, 18135, 264, 1668, 295, 264, 2208, 2967, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07185342378705462, "compression_ratio": 1.829145728643216, "no_speech_prob": 2.2473490389529616e-05}, {"id": 1021, "seek": 539400, "start": 5405.0, "end": 5408.0, "text": " And that will be called into R mall.", "tokens": [50364, 400, 497, 16026, 307, 733, 295, 411, 257, 2100, 3207, 13, 50514, 50514, 407, 498, 15329, 393, 380, 360, 732, 1413, 257, 11, 309, 486, 1520, 498, 538, 604, 2931, 257, 3255, 577, 281, 12972, 732, 13, 50914, 50914, 400, 300, 486, 312, 1219, 666, 497, 16026, 13, 51064, 51064, 407, 570, 15329, 393, 380, 360, 732, 1413, 257, 11, 309, 486, 1520, 307, 456, 364, 497, 16026, 294, 2158, 13, 51314, 51314, 400, 570, 456, 307, 11, 309, 486, 586, 818, 300, 13, 51464, 51464, 400, 437, 321, 603, 360, 510, 307, 321, 486, 18135, 264, 1668, 295, 264, 2208, 2967, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07185342378705462, "compression_ratio": 1.829145728643216, "no_speech_prob": 2.2473490389529616e-05}, {"id": 1022, "seek": 539400, "start": 5408.0, "end": 5413.0, "text": " So because Python can't do two times a, it will check is there an R mall in value.", "tokens": [50364, 400, 497, 16026, 307, 733, 295, 411, 257, 2100, 3207, 13, 50514, 50514, 407, 498, 15329, 393, 380, 360, 732, 1413, 257, 11, 309, 486, 1520, 498, 538, 604, 2931, 257, 3255, 577, 281, 12972, 732, 13, 50914, 50914, 400, 300, 486, 312, 1219, 666, 497, 16026, 13, 51064, 51064, 407, 570, 15329, 393, 380, 360, 732, 1413, 257, 11, 309, 486, 1520, 307, 456, 364, 497, 16026, 294, 2158, 13, 51314, 51314, 400, 570, 456, 307, 11, 309, 486, 586, 818, 300, 13, 51464, 51464, 400, 437, 321, 603, 360, 510, 307, 321, 486, 18135, 264, 1668, 295, 264, 2208, 2967, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07185342378705462, "compression_ratio": 1.829145728643216, "no_speech_prob": 2.2473490389529616e-05}, {"id": 1023, "seek": 539400, "start": 5413.0, "end": 5416.0, "text": " And because there is, it will now call that.", "tokens": [50364, 400, 497, 16026, 307, 733, 295, 411, 257, 2100, 3207, 13, 50514, 50514, 407, 498, 15329, 393, 380, 360, 732, 1413, 257, 11, 309, 486, 1520, 498, 538, 604, 2931, 257, 3255, 577, 281, 12972, 732, 13, 50914, 50914, 400, 300, 486, 312, 1219, 666, 497, 16026, 13, 51064, 51064, 407, 570, 15329, 393, 380, 360, 732, 1413, 257, 11, 309, 486, 1520, 307, 456, 364, 497, 16026, 294, 2158, 13, 51314, 51314, 400, 570, 456, 307, 11, 309, 486, 586, 818, 300, 13, 51464, 51464, 400, 437, 321, 603, 360, 510, 307, 321, 486, 18135, 264, 1668, 295, 264, 2208, 2967, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07185342378705462, "compression_ratio": 1.829145728643216, "no_speech_prob": 2.2473490389529616e-05}, {"id": 1024, "seek": 539400, "start": 5416.0, "end": 5420.0, "text": " And what we'll do here is we will swap the order of the operands.", "tokens": [50364, 400, 497, 16026, 307, 733, 295, 411, 257, 2100, 3207, 13, 50514, 50514, 407, 498, 15329, 393, 380, 360, 732, 1413, 257, 11, 309, 486, 1520, 498, 538, 604, 2931, 257, 3255, 577, 281, 12972, 732, 13, 50914, 50914, 400, 300, 486, 312, 1219, 666, 497, 16026, 13, 51064, 51064, 407, 570, 15329, 393, 380, 360, 732, 1413, 257, 11, 309, 486, 1520, 307, 456, 364, 497, 16026, 294, 2158, 13, 51314, 51314, 400, 570, 456, 307, 11, 309, 486, 586, 818, 300, 13, 51464, 51464, 400, 437, 321, 603, 360, 510, 307, 321, 486, 18135, 264, 1668, 295, 264, 2208, 2967, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07185342378705462, "compression_ratio": 1.829145728643216, "no_speech_prob": 2.2473490389529616e-05}, {"id": 1025, "seek": 542000, "start": 5420.0, "end": 5426.0, "text": " So basically, two times a will redirect to R mall and R mall will basically call a times two.", "tokens": [50364, 407, 1936, 11, 732, 1413, 257, 486, 29066, 281, 497, 16026, 293, 497, 16026, 486, 1936, 818, 257, 1413, 732, 13, 50664, 50664, 400, 300, 311, 577, 300, 486, 589, 13, 50764, 50764, 407, 38818, 1760, 300, 365, 497, 16026, 11, 732, 1413, 257, 3643, 1451, 13, 50964, 50964, 2264, 11, 586, 1237, 412, 264, 661, 4959, 300, 321, 920, 643, 11, 321, 643, 281, 458, 577, 281, 37871, 13024, 293, 577, 281, 9845, 13, 51214, 51214, 407, 718, 311, 700, 360, 264, 37871, 6642, 644, 13, 51364, 51364, 492, 434, 516, 281, 5366, 257, 2167, 2445, 1783, 510, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1062160271864671, "compression_ratio": 1.7443946188340806, "no_speech_prob": 4.611126132658683e-05}, {"id": 1026, "seek": 542000, "start": 5426.0, "end": 5428.0, "text": " And that's how that will work.", "tokens": [50364, 407, 1936, 11, 732, 1413, 257, 486, 29066, 281, 497, 16026, 293, 497, 16026, 486, 1936, 818, 257, 1413, 732, 13, 50664, 50664, 400, 300, 311, 577, 300, 486, 589, 13, 50764, 50764, 407, 38818, 1760, 300, 365, 497, 16026, 11, 732, 1413, 257, 3643, 1451, 13, 50964, 50964, 2264, 11, 586, 1237, 412, 264, 661, 4959, 300, 321, 920, 643, 11, 321, 643, 281, 458, 577, 281, 37871, 13024, 293, 577, 281, 9845, 13, 51214, 51214, 407, 718, 311, 700, 360, 264, 37871, 6642, 644, 13, 51364, 51364, 492, 434, 516, 281, 5366, 257, 2167, 2445, 1783, 510, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1062160271864671, "compression_ratio": 1.7443946188340806, "no_speech_prob": 4.611126132658683e-05}, {"id": 1027, "seek": 542000, "start": 5428.0, "end": 5432.0, "text": " So redefining that with R mall, two times a becomes four.", "tokens": [50364, 407, 1936, 11, 732, 1413, 257, 486, 29066, 281, 497, 16026, 293, 497, 16026, 486, 1936, 818, 257, 1413, 732, 13, 50664, 50664, 400, 300, 311, 577, 300, 486, 589, 13, 50764, 50764, 407, 38818, 1760, 300, 365, 497, 16026, 11, 732, 1413, 257, 3643, 1451, 13, 50964, 50964, 2264, 11, 586, 1237, 412, 264, 661, 4959, 300, 321, 920, 643, 11, 321, 643, 281, 458, 577, 281, 37871, 13024, 293, 577, 281, 9845, 13, 51214, 51214, 407, 718, 311, 700, 360, 264, 37871, 6642, 644, 13, 51364, 51364, 492, 434, 516, 281, 5366, 257, 2167, 2445, 1783, 510, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1062160271864671, "compression_ratio": 1.7443946188340806, "no_speech_prob": 4.611126132658683e-05}, {"id": 1028, "seek": 542000, "start": 5432.0, "end": 5437.0, "text": " OK, now looking at the other elements that we still need, we need to know how to exponentiate and how to divide.", "tokens": [50364, 407, 1936, 11, 732, 1413, 257, 486, 29066, 281, 497, 16026, 293, 497, 16026, 486, 1936, 818, 257, 1413, 732, 13, 50664, 50664, 400, 300, 311, 577, 300, 486, 589, 13, 50764, 50764, 407, 38818, 1760, 300, 365, 497, 16026, 11, 732, 1413, 257, 3643, 1451, 13, 50964, 50964, 2264, 11, 586, 1237, 412, 264, 661, 4959, 300, 321, 920, 643, 11, 321, 643, 281, 458, 577, 281, 37871, 13024, 293, 577, 281, 9845, 13, 51214, 51214, 407, 718, 311, 700, 360, 264, 37871, 6642, 644, 13, 51364, 51364, 492, 434, 516, 281, 5366, 257, 2167, 2445, 1783, 510, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1062160271864671, "compression_ratio": 1.7443946188340806, "no_speech_prob": 4.611126132658683e-05}, {"id": 1029, "seek": 542000, "start": 5437.0, "end": 5440.0, "text": " So let's first do the exponentiation part.", "tokens": [50364, 407, 1936, 11, 732, 1413, 257, 486, 29066, 281, 497, 16026, 293, 497, 16026, 486, 1936, 818, 257, 1413, 732, 13, 50664, 50664, 400, 300, 311, 577, 300, 486, 589, 13, 50764, 50764, 407, 38818, 1760, 300, 365, 497, 16026, 11, 732, 1413, 257, 3643, 1451, 13, 50964, 50964, 2264, 11, 586, 1237, 412, 264, 661, 4959, 300, 321, 920, 643, 11, 321, 643, 281, 458, 577, 281, 37871, 13024, 293, 577, 281, 9845, 13, 51214, 51214, 407, 718, 311, 700, 360, 264, 37871, 6642, 644, 13, 51364, 51364, 492, 434, 516, 281, 5366, 257, 2167, 2445, 1783, 510, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1062160271864671, "compression_ratio": 1.7443946188340806, "no_speech_prob": 4.611126132658683e-05}, {"id": 1030, "seek": 542000, "start": 5440.0, "end": 5445.0, "text": " We're going to introduce a single function X here.", "tokens": [50364, 407, 1936, 11, 732, 1413, 257, 486, 29066, 281, 497, 16026, 293, 497, 16026, 486, 1936, 818, 257, 1413, 732, 13, 50664, 50664, 400, 300, 311, 577, 300, 486, 589, 13, 50764, 50764, 407, 38818, 1760, 300, 365, 497, 16026, 11, 732, 1413, 257, 3643, 1451, 13, 50964, 50964, 2264, 11, 586, 1237, 412, 264, 661, 4959, 300, 321, 920, 643, 11, 321, 643, 281, 458, 577, 281, 37871, 13024, 293, 577, 281, 9845, 13, 51214, 51214, 407, 718, 311, 700, 360, 264, 37871, 6642, 644, 13, 51364, 51364, 492, 434, 516, 281, 5366, 257, 2167, 2445, 1783, 510, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1062160271864671, "compression_ratio": 1.7443946188340806, "no_speech_prob": 4.611126132658683e-05}, {"id": 1031, "seek": 544500, "start": 5445.0, "end": 5453.0, "text": " And X is going to mirror 10H in the sense that it's a single function that transforms a single scalar value and outputs a single scalar value.", "tokens": [50364, 400, 1783, 307, 516, 281, 8013, 1266, 39, 294, 264, 2020, 300, 309, 311, 257, 2167, 2445, 300, 35592, 257, 2167, 39684, 2158, 293, 23930, 257, 2167, 39684, 2158, 13, 50764, 50764, 407, 321, 1665, 484, 264, 15329, 1230, 13, 50864, 50864, 492, 764, 5221, 13, 87, 281, 37871, 13024, 309, 11, 1884, 257, 777, 2158, 2657, 13, 51064, 51064, 5471, 300, 321, 600, 1612, 949, 13, 51164, 51164, 440, 12414, 644, 11, 295, 1164, 11, 307, 577, 360, 291, 646, 48256, 807, 308, 281, 264, 1783, 30, 51364, 51364, 400, 370, 510, 291, 393, 7263, 10465, 264, 960, 293, 519, 466, 437, 820, 352, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11797508033546242, "compression_ratio": 1.5977859778597785, "no_speech_prob": 1.1478389751573559e-05}, {"id": 1032, "seek": 544500, "start": 5453.0, "end": 5455.0, "text": " So we pop out the Python number.", "tokens": [50364, 400, 1783, 307, 516, 281, 8013, 1266, 39, 294, 264, 2020, 300, 309, 311, 257, 2167, 2445, 300, 35592, 257, 2167, 39684, 2158, 293, 23930, 257, 2167, 39684, 2158, 13, 50764, 50764, 407, 321, 1665, 484, 264, 15329, 1230, 13, 50864, 50864, 492, 764, 5221, 13, 87, 281, 37871, 13024, 309, 11, 1884, 257, 777, 2158, 2657, 13, 51064, 51064, 5471, 300, 321, 600, 1612, 949, 13, 51164, 51164, 440, 12414, 644, 11, 295, 1164, 11, 307, 577, 360, 291, 646, 48256, 807, 308, 281, 264, 1783, 30, 51364, 51364, 400, 370, 510, 291, 393, 7263, 10465, 264, 960, 293, 519, 466, 437, 820, 352, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11797508033546242, "compression_ratio": 1.5977859778597785, "no_speech_prob": 1.1478389751573559e-05}, {"id": 1033, "seek": 544500, "start": 5455.0, "end": 5459.0, "text": " We use math.x to exponentiate it, create a new value object.", "tokens": [50364, 400, 1783, 307, 516, 281, 8013, 1266, 39, 294, 264, 2020, 300, 309, 311, 257, 2167, 2445, 300, 35592, 257, 2167, 39684, 2158, 293, 23930, 257, 2167, 39684, 2158, 13, 50764, 50764, 407, 321, 1665, 484, 264, 15329, 1230, 13, 50864, 50864, 492, 764, 5221, 13, 87, 281, 37871, 13024, 309, 11, 1884, 257, 777, 2158, 2657, 13, 51064, 51064, 5471, 300, 321, 600, 1612, 949, 13, 51164, 51164, 440, 12414, 644, 11, 295, 1164, 11, 307, 577, 360, 291, 646, 48256, 807, 308, 281, 264, 1783, 30, 51364, 51364, 400, 370, 510, 291, 393, 7263, 10465, 264, 960, 293, 519, 466, 437, 820, 352, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11797508033546242, "compression_ratio": 1.5977859778597785, "no_speech_prob": 1.1478389751573559e-05}, {"id": 1034, "seek": 544500, "start": 5459.0, "end": 5461.0, "text": " Everything that we've seen before.", "tokens": [50364, 400, 1783, 307, 516, 281, 8013, 1266, 39, 294, 264, 2020, 300, 309, 311, 257, 2167, 2445, 300, 35592, 257, 2167, 39684, 2158, 293, 23930, 257, 2167, 39684, 2158, 13, 50764, 50764, 407, 321, 1665, 484, 264, 15329, 1230, 13, 50864, 50864, 492, 764, 5221, 13, 87, 281, 37871, 13024, 309, 11, 1884, 257, 777, 2158, 2657, 13, 51064, 51064, 5471, 300, 321, 600, 1612, 949, 13, 51164, 51164, 440, 12414, 644, 11, 295, 1164, 11, 307, 577, 360, 291, 646, 48256, 807, 308, 281, 264, 1783, 30, 51364, 51364, 400, 370, 510, 291, 393, 7263, 10465, 264, 960, 293, 519, 466, 437, 820, 352, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11797508033546242, "compression_ratio": 1.5977859778597785, "no_speech_prob": 1.1478389751573559e-05}, {"id": 1035, "seek": 544500, "start": 5461.0, "end": 5465.0, "text": " The tricky part, of course, is how do you back propagate through e to the X?", "tokens": [50364, 400, 1783, 307, 516, 281, 8013, 1266, 39, 294, 264, 2020, 300, 309, 311, 257, 2167, 2445, 300, 35592, 257, 2167, 39684, 2158, 293, 23930, 257, 2167, 39684, 2158, 13, 50764, 50764, 407, 321, 1665, 484, 264, 15329, 1230, 13, 50864, 50864, 492, 764, 5221, 13, 87, 281, 37871, 13024, 309, 11, 1884, 257, 777, 2158, 2657, 13, 51064, 51064, 5471, 300, 321, 600, 1612, 949, 13, 51164, 51164, 440, 12414, 644, 11, 295, 1164, 11, 307, 577, 360, 291, 646, 48256, 807, 308, 281, 264, 1783, 30, 51364, 51364, 400, 370, 510, 291, 393, 7263, 10465, 264, 960, 293, 519, 466, 437, 820, 352, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11797508033546242, "compression_ratio": 1.5977859778597785, "no_speech_prob": 1.1478389751573559e-05}, {"id": 1036, "seek": 544500, "start": 5465.0, "end": 5473.0, "text": " And so here you can potentially pause the video and think about what should go here.", "tokens": [50364, 400, 1783, 307, 516, 281, 8013, 1266, 39, 294, 264, 2020, 300, 309, 311, 257, 2167, 2445, 300, 35592, 257, 2167, 39684, 2158, 293, 23930, 257, 2167, 39684, 2158, 13, 50764, 50764, 407, 321, 1665, 484, 264, 15329, 1230, 13, 50864, 50864, 492, 764, 5221, 13, 87, 281, 37871, 13024, 309, 11, 1884, 257, 777, 2158, 2657, 13, 51064, 51064, 5471, 300, 321, 600, 1612, 949, 13, 51164, 51164, 440, 12414, 644, 11, 295, 1164, 11, 307, 577, 360, 291, 646, 48256, 807, 308, 281, 264, 1783, 30, 51364, 51364, 400, 370, 510, 291, 393, 7263, 10465, 264, 960, 293, 519, 466, 437, 820, 352, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11797508033546242, "compression_ratio": 1.5977859778597785, "no_speech_prob": 1.1478389751573559e-05}, {"id": 1037, "seek": 547300, "start": 5473.0, "end": 5478.0, "text": " OK, so basically, we need to know what is the local derivative of e to the X.", "tokens": [50364, 2264, 11, 370, 1936, 11, 321, 643, 281, 458, 437, 307, 264, 2654, 13760, 295, 308, 281, 264, 1783, 13, 50614, 50614, 407, 274, 538, 30017, 295, 308, 281, 264, 1783, 307, 11, 34360, 11, 445, 308, 281, 264, 1783, 13, 50814, 50814, 400, 321, 600, 1217, 445, 15598, 308, 281, 264, 1783, 13, 50914, 50914, 400, 309, 311, 1854, 484, 13, 67, 3274, 13, 51014, 51014, 407, 321, 393, 360, 484, 13, 67, 3274, 1413, 293, 484, 13, 7165, 13, 51214, 51214, 663, 311, 264, 5021, 16026, 13, 51314, 51314, 407, 321, 434, 445, 417, 3686, 322, 281, 264, 2190, 2614, 2771, 13, 51464, 51464, 400, 341, 307, 437, 264, 6114, 1542, 411, 13, 51564, 51564, 467, 1542, 257, 707, 13181, 11, 457, 341, 307, 437, 309, 307, 13, 51664, 51664, 400, 300, 311, 264, 37871, 6642, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10116627166321228, "compression_ratio": 1.7976190476190477, "no_speech_prob": 1.428478481102502e-05}, {"id": 1038, "seek": 547300, "start": 5478.0, "end": 5482.0, "text": " So d by dx of e to the X is, famously, just e to the X.", "tokens": [50364, 2264, 11, 370, 1936, 11, 321, 643, 281, 458, 437, 307, 264, 2654, 13760, 295, 308, 281, 264, 1783, 13, 50614, 50614, 407, 274, 538, 30017, 295, 308, 281, 264, 1783, 307, 11, 34360, 11, 445, 308, 281, 264, 1783, 13, 50814, 50814, 400, 321, 600, 1217, 445, 15598, 308, 281, 264, 1783, 13, 50914, 50914, 400, 309, 311, 1854, 484, 13, 67, 3274, 13, 51014, 51014, 407, 321, 393, 360, 484, 13, 67, 3274, 1413, 293, 484, 13, 7165, 13, 51214, 51214, 663, 311, 264, 5021, 16026, 13, 51314, 51314, 407, 321, 434, 445, 417, 3686, 322, 281, 264, 2190, 2614, 2771, 13, 51464, 51464, 400, 341, 307, 437, 264, 6114, 1542, 411, 13, 51564, 51564, 467, 1542, 257, 707, 13181, 11, 457, 341, 307, 437, 309, 307, 13, 51664, 51664, 400, 300, 311, 264, 37871, 6642, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10116627166321228, "compression_ratio": 1.7976190476190477, "no_speech_prob": 1.428478481102502e-05}, {"id": 1039, "seek": 547300, "start": 5482.0, "end": 5484.0, "text": " And we've already just calculated e to the X.", "tokens": [50364, 2264, 11, 370, 1936, 11, 321, 643, 281, 458, 437, 307, 264, 2654, 13760, 295, 308, 281, 264, 1783, 13, 50614, 50614, 407, 274, 538, 30017, 295, 308, 281, 264, 1783, 307, 11, 34360, 11, 445, 308, 281, 264, 1783, 13, 50814, 50814, 400, 321, 600, 1217, 445, 15598, 308, 281, 264, 1783, 13, 50914, 50914, 400, 309, 311, 1854, 484, 13, 67, 3274, 13, 51014, 51014, 407, 321, 393, 360, 484, 13, 67, 3274, 1413, 293, 484, 13, 7165, 13, 51214, 51214, 663, 311, 264, 5021, 16026, 13, 51314, 51314, 407, 321, 434, 445, 417, 3686, 322, 281, 264, 2190, 2614, 2771, 13, 51464, 51464, 400, 341, 307, 437, 264, 6114, 1542, 411, 13, 51564, 51564, 467, 1542, 257, 707, 13181, 11, 457, 341, 307, 437, 309, 307, 13, 51664, 51664, 400, 300, 311, 264, 37871, 6642, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10116627166321228, "compression_ratio": 1.7976190476190477, "no_speech_prob": 1.428478481102502e-05}, {"id": 1040, "seek": 547300, "start": 5484.0, "end": 5486.0, "text": " And it's inside out.data.", "tokens": [50364, 2264, 11, 370, 1936, 11, 321, 643, 281, 458, 437, 307, 264, 2654, 13760, 295, 308, 281, 264, 1783, 13, 50614, 50614, 407, 274, 538, 30017, 295, 308, 281, 264, 1783, 307, 11, 34360, 11, 445, 308, 281, 264, 1783, 13, 50814, 50814, 400, 321, 600, 1217, 445, 15598, 308, 281, 264, 1783, 13, 50914, 50914, 400, 309, 311, 1854, 484, 13, 67, 3274, 13, 51014, 51014, 407, 321, 393, 360, 484, 13, 67, 3274, 1413, 293, 484, 13, 7165, 13, 51214, 51214, 663, 311, 264, 5021, 16026, 13, 51314, 51314, 407, 321, 434, 445, 417, 3686, 322, 281, 264, 2190, 2614, 2771, 13, 51464, 51464, 400, 341, 307, 437, 264, 6114, 1542, 411, 13, 51564, 51564, 467, 1542, 257, 707, 13181, 11, 457, 341, 307, 437, 309, 307, 13, 51664, 51664, 400, 300, 311, 264, 37871, 6642, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10116627166321228, "compression_ratio": 1.7976190476190477, "no_speech_prob": 1.428478481102502e-05}, {"id": 1041, "seek": 547300, "start": 5486.0, "end": 5490.0, "text": " So we can do out.data times and out.grad.", "tokens": [50364, 2264, 11, 370, 1936, 11, 321, 643, 281, 458, 437, 307, 264, 2654, 13760, 295, 308, 281, 264, 1783, 13, 50614, 50614, 407, 274, 538, 30017, 295, 308, 281, 264, 1783, 307, 11, 34360, 11, 445, 308, 281, 264, 1783, 13, 50814, 50814, 400, 321, 600, 1217, 445, 15598, 308, 281, 264, 1783, 13, 50914, 50914, 400, 309, 311, 1854, 484, 13, 67, 3274, 13, 51014, 51014, 407, 321, 393, 360, 484, 13, 67, 3274, 1413, 293, 484, 13, 7165, 13, 51214, 51214, 663, 311, 264, 5021, 16026, 13, 51314, 51314, 407, 321, 434, 445, 417, 3686, 322, 281, 264, 2190, 2614, 2771, 13, 51464, 51464, 400, 341, 307, 437, 264, 6114, 1542, 411, 13, 51564, 51564, 467, 1542, 257, 707, 13181, 11, 457, 341, 307, 437, 309, 307, 13, 51664, 51664, 400, 300, 311, 264, 37871, 6642, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10116627166321228, "compression_ratio": 1.7976190476190477, "no_speech_prob": 1.428478481102502e-05}, {"id": 1042, "seek": 547300, "start": 5490.0, "end": 5492.0, "text": " That's the chain mall.", "tokens": [50364, 2264, 11, 370, 1936, 11, 321, 643, 281, 458, 437, 307, 264, 2654, 13760, 295, 308, 281, 264, 1783, 13, 50614, 50614, 407, 274, 538, 30017, 295, 308, 281, 264, 1783, 307, 11, 34360, 11, 445, 308, 281, 264, 1783, 13, 50814, 50814, 400, 321, 600, 1217, 445, 15598, 308, 281, 264, 1783, 13, 50914, 50914, 400, 309, 311, 1854, 484, 13, 67, 3274, 13, 51014, 51014, 407, 321, 393, 360, 484, 13, 67, 3274, 1413, 293, 484, 13, 7165, 13, 51214, 51214, 663, 311, 264, 5021, 16026, 13, 51314, 51314, 407, 321, 434, 445, 417, 3686, 322, 281, 264, 2190, 2614, 2771, 13, 51464, 51464, 400, 341, 307, 437, 264, 6114, 1542, 411, 13, 51564, 51564, 467, 1542, 257, 707, 13181, 11, 457, 341, 307, 437, 309, 307, 13, 51664, 51664, 400, 300, 311, 264, 37871, 6642, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10116627166321228, "compression_ratio": 1.7976190476190477, "no_speech_prob": 1.428478481102502e-05}, {"id": 1043, "seek": 547300, "start": 5492.0, "end": 5495.0, "text": " So we're just chaining on to the current running grad.", "tokens": [50364, 2264, 11, 370, 1936, 11, 321, 643, 281, 458, 437, 307, 264, 2654, 13760, 295, 308, 281, 264, 1783, 13, 50614, 50614, 407, 274, 538, 30017, 295, 308, 281, 264, 1783, 307, 11, 34360, 11, 445, 308, 281, 264, 1783, 13, 50814, 50814, 400, 321, 600, 1217, 445, 15598, 308, 281, 264, 1783, 13, 50914, 50914, 400, 309, 311, 1854, 484, 13, 67, 3274, 13, 51014, 51014, 407, 321, 393, 360, 484, 13, 67, 3274, 1413, 293, 484, 13, 7165, 13, 51214, 51214, 663, 311, 264, 5021, 16026, 13, 51314, 51314, 407, 321, 434, 445, 417, 3686, 322, 281, 264, 2190, 2614, 2771, 13, 51464, 51464, 400, 341, 307, 437, 264, 6114, 1542, 411, 13, 51564, 51564, 467, 1542, 257, 707, 13181, 11, 457, 341, 307, 437, 309, 307, 13, 51664, 51664, 400, 300, 311, 264, 37871, 6642, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10116627166321228, "compression_ratio": 1.7976190476190477, "no_speech_prob": 1.428478481102502e-05}, {"id": 1044, "seek": 547300, "start": 5495.0, "end": 5497.0, "text": " And this is what the expression looks like.", "tokens": [50364, 2264, 11, 370, 1936, 11, 321, 643, 281, 458, 437, 307, 264, 2654, 13760, 295, 308, 281, 264, 1783, 13, 50614, 50614, 407, 274, 538, 30017, 295, 308, 281, 264, 1783, 307, 11, 34360, 11, 445, 308, 281, 264, 1783, 13, 50814, 50814, 400, 321, 600, 1217, 445, 15598, 308, 281, 264, 1783, 13, 50914, 50914, 400, 309, 311, 1854, 484, 13, 67, 3274, 13, 51014, 51014, 407, 321, 393, 360, 484, 13, 67, 3274, 1413, 293, 484, 13, 7165, 13, 51214, 51214, 663, 311, 264, 5021, 16026, 13, 51314, 51314, 407, 321, 434, 445, 417, 3686, 322, 281, 264, 2190, 2614, 2771, 13, 51464, 51464, 400, 341, 307, 437, 264, 6114, 1542, 411, 13, 51564, 51564, 467, 1542, 257, 707, 13181, 11, 457, 341, 307, 437, 309, 307, 13, 51664, 51664, 400, 300, 311, 264, 37871, 6642, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10116627166321228, "compression_ratio": 1.7976190476190477, "no_speech_prob": 1.428478481102502e-05}, {"id": 1045, "seek": 547300, "start": 5497.0, "end": 5499.0, "text": " It looks a little confusing, but this is what it is.", "tokens": [50364, 2264, 11, 370, 1936, 11, 321, 643, 281, 458, 437, 307, 264, 2654, 13760, 295, 308, 281, 264, 1783, 13, 50614, 50614, 407, 274, 538, 30017, 295, 308, 281, 264, 1783, 307, 11, 34360, 11, 445, 308, 281, 264, 1783, 13, 50814, 50814, 400, 321, 600, 1217, 445, 15598, 308, 281, 264, 1783, 13, 50914, 50914, 400, 309, 311, 1854, 484, 13, 67, 3274, 13, 51014, 51014, 407, 321, 393, 360, 484, 13, 67, 3274, 1413, 293, 484, 13, 7165, 13, 51214, 51214, 663, 311, 264, 5021, 16026, 13, 51314, 51314, 407, 321, 434, 445, 417, 3686, 322, 281, 264, 2190, 2614, 2771, 13, 51464, 51464, 400, 341, 307, 437, 264, 6114, 1542, 411, 13, 51564, 51564, 467, 1542, 257, 707, 13181, 11, 457, 341, 307, 437, 309, 307, 13, 51664, 51664, 400, 300, 311, 264, 37871, 6642, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10116627166321228, "compression_ratio": 1.7976190476190477, "no_speech_prob": 1.428478481102502e-05}, {"id": 1046, "seek": 547300, "start": 5499.0, "end": 5501.0, "text": " And that's the exponentiation.", "tokens": [50364, 2264, 11, 370, 1936, 11, 321, 643, 281, 458, 437, 307, 264, 2654, 13760, 295, 308, 281, 264, 1783, 13, 50614, 50614, 407, 274, 538, 30017, 295, 308, 281, 264, 1783, 307, 11, 34360, 11, 445, 308, 281, 264, 1783, 13, 50814, 50814, 400, 321, 600, 1217, 445, 15598, 308, 281, 264, 1783, 13, 50914, 50914, 400, 309, 311, 1854, 484, 13, 67, 3274, 13, 51014, 51014, 407, 321, 393, 360, 484, 13, 67, 3274, 1413, 293, 484, 13, 7165, 13, 51214, 51214, 663, 311, 264, 5021, 16026, 13, 51314, 51314, 407, 321, 434, 445, 417, 3686, 322, 281, 264, 2190, 2614, 2771, 13, 51464, 51464, 400, 341, 307, 437, 264, 6114, 1542, 411, 13, 51564, 51564, 467, 1542, 257, 707, 13181, 11, 457, 341, 307, 437, 309, 307, 13, 51664, 51664, 400, 300, 311, 264, 37871, 6642, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10116627166321228, "compression_ratio": 1.7976190476190477, "no_speech_prob": 1.428478481102502e-05}, {"id": 1047, "seek": 550100, "start": 5501.0, "end": 5505.0, "text": " So redefining, we should now be able to call a.exp.", "tokens": [50364, 407, 38818, 1760, 11, 321, 820, 586, 312, 1075, 281, 818, 257, 13, 15952, 13, 50564, 50564, 400, 4696, 11, 264, 23897, 1320, 1985, 382, 731, 13, 50714, 50714, 2264, 11, 293, 264, 1036, 551, 321, 1116, 411, 281, 360, 11, 295, 1164, 11, 307, 321, 1116, 411, 281, 312, 1075, 281, 9845, 13, 50914, 50914, 823, 11, 286, 767, 486, 4445, 746, 4748, 544, 4005, 813, 10044, 11, 51114, 51114, 570, 10044, 307, 445, 257, 2121, 1389, 295, 746, 257, 857, 544, 4005, 13, 51314, 51314, 407, 294, 1729, 11, 445, 538, 29875, 9741, 11, 498, 321, 362, 512, 733, 295, 257, 272, 6915, 2158, 295, 1017, 13, 15, 510, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07134703932137325, "compression_ratio": 1.6203007518796992, "no_speech_prob": 6.708966975566e-05}, {"id": 1048, "seek": 550100, "start": 5505.0, "end": 5508.0, "text": " And hopefully, the backward pass works as well.", "tokens": [50364, 407, 38818, 1760, 11, 321, 820, 586, 312, 1075, 281, 818, 257, 13, 15952, 13, 50564, 50564, 400, 4696, 11, 264, 23897, 1320, 1985, 382, 731, 13, 50714, 50714, 2264, 11, 293, 264, 1036, 551, 321, 1116, 411, 281, 360, 11, 295, 1164, 11, 307, 321, 1116, 411, 281, 312, 1075, 281, 9845, 13, 50914, 50914, 823, 11, 286, 767, 486, 4445, 746, 4748, 544, 4005, 813, 10044, 11, 51114, 51114, 570, 10044, 307, 445, 257, 2121, 1389, 295, 746, 257, 857, 544, 4005, 13, 51314, 51314, 407, 294, 1729, 11, 445, 538, 29875, 9741, 11, 498, 321, 362, 512, 733, 295, 257, 272, 6915, 2158, 295, 1017, 13, 15, 510, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07134703932137325, "compression_ratio": 1.6203007518796992, "no_speech_prob": 6.708966975566e-05}, {"id": 1049, "seek": 550100, "start": 5508.0, "end": 5512.0, "text": " OK, and the last thing we'd like to do, of course, is we'd like to be able to divide.", "tokens": [50364, 407, 38818, 1760, 11, 321, 820, 586, 312, 1075, 281, 818, 257, 13, 15952, 13, 50564, 50564, 400, 4696, 11, 264, 23897, 1320, 1985, 382, 731, 13, 50714, 50714, 2264, 11, 293, 264, 1036, 551, 321, 1116, 411, 281, 360, 11, 295, 1164, 11, 307, 321, 1116, 411, 281, 312, 1075, 281, 9845, 13, 50914, 50914, 823, 11, 286, 767, 486, 4445, 746, 4748, 544, 4005, 813, 10044, 11, 51114, 51114, 570, 10044, 307, 445, 257, 2121, 1389, 295, 746, 257, 857, 544, 4005, 13, 51314, 51314, 407, 294, 1729, 11, 445, 538, 29875, 9741, 11, 498, 321, 362, 512, 733, 295, 257, 272, 6915, 2158, 295, 1017, 13, 15, 510, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07134703932137325, "compression_ratio": 1.6203007518796992, "no_speech_prob": 6.708966975566e-05}, {"id": 1050, "seek": 550100, "start": 5512.0, "end": 5516.0, "text": " Now, I actually will implement something slightly more powerful than division,", "tokens": [50364, 407, 38818, 1760, 11, 321, 820, 586, 312, 1075, 281, 818, 257, 13, 15952, 13, 50564, 50564, 400, 4696, 11, 264, 23897, 1320, 1985, 382, 731, 13, 50714, 50714, 2264, 11, 293, 264, 1036, 551, 321, 1116, 411, 281, 360, 11, 295, 1164, 11, 307, 321, 1116, 411, 281, 312, 1075, 281, 9845, 13, 50914, 50914, 823, 11, 286, 767, 486, 4445, 746, 4748, 544, 4005, 813, 10044, 11, 51114, 51114, 570, 10044, 307, 445, 257, 2121, 1389, 295, 746, 257, 857, 544, 4005, 13, 51314, 51314, 407, 294, 1729, 11, 445, 538, 29875, 9741, 11, 498, 321, 362, 512, 733, 295, 257, 272, 6915, 2158, 295, 1017, 13, 15, 510, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07134703932137325, "compression_ratio": 1.6203007518796992, "no_speech_prob": 6.708966975566e-05}, {"id": 1051, "seek": 550100, "start": 5516.0, "end": 5520.0, "text": " because division is just a special case of something a bit more powerful.", "tokens": [50364, 407, 38818, 1760, 11, 321, 820, 586, 312, 1075, 281, 818, 257, 13, 15952, 13, 50564, 50564, 400, 4696, 11, 264, 23897, 1320, 1985, 382, 731, 13, 50714, 50714, 2264, 11, 293, 264, 1036, 551, 321, 1116, 411, 281, 360, 11, 295, 1164, 11, 307, 321, 1116, 411, 281, 312, 1075, 281, 9845, 13, 50914, 50914, 823, 11, 286, 767, 486, 4445, 746, 4748, 544, 4005, 813, 10044, 11, 51114, 51114, 570, 10044, 307, 445, 257, 2121, 1389, 295, 746, 257, 857, 544, 4005, 13, 51314, 51314, 407, 294, 1729, 11, 445, 538, 29875, 9741, 11, 498, 321, 362, 512, 733, 295, 257, 272, 6915, 2158, 295, 1017, 13, 15, 510, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07134703932137325, "compression_ratio": 1.6203007518796992, "no_speech_prob": 6.708966975566e-05}, {"id": 1052, "seek": 550100, "start": 5520.0, "end": 5527.0, "text": " So in particular, just by rearranging, if we have some kind of a b equals value of 4.0 here,", "tokens": [50364, 407, 38818, 1760, 11, 321, 820, 586, 312, 1075, 281, 818, 257, 13, 15952, 13, 50564, 50564, 400, 4696, 11, 264, 23897, 1320, 1985, 382, 731, 13, 50714, 50714, 2264, 11, 293, 264, 1036, 551, 321, 1116, 411, 281, 360, 11, 295, 1164, 11, 307, 321, 1116, 411, 281, 312, 1075, 281, 9845, 13, 50914, 50914, 823, 11, 286, 767, 486, 4445, 746, 4748, 544, 4005, 813, 10044, 11, 51114, 51114, 570, 10044, 307, 445, 257, 2121, 1389, 295, 746, 257, 857, 544, 4005, 13, 51314, 51314, 407, 294, 1729, 11, 445, 538, 29875, 9741, 11, 498, 321, 362, 512, 733, 295, 257, 272, 6915, 2158, 295, 1017, 13, 15, 510, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07134703932137325, "compression_ratio": 1.6203007518796992, "no_speech_prob": 6.708966975566e-05}, {"id": 1053, "seek": 552700, "start": 5527.0, "end": 5531.0, "text": " we'd like to basically be able to do a divide b, and we'd like this to be able to give us 0.5.", "tokens": [50364, 321, 1116, 411, 281, 1936, 312, 1075, 281, 360, 257, 9845, 272, 11, 293, 321, 1116, 411, 341, 281, 312, 1075, 281, 976, 505, 1958, 13, 20, 13, 50564, 50564, 823, 11, 10044, 767, 393, 312, 725, 71, 33974, 382, 10002, 13, 50764, 50764, 759, 321, 362, 257, 9845, 272, 11, 300, 311, 767, 264, 912, 382, 257, 30955, 502, 670, 272, 13, 51014, 51014, 400, 300, 311, 264, 912, 382, 257, 30955, 272, 281, 264, 1347, 295, 3671, 502, 13, 51214, 51214, 400, 370, 437, 286, 1116, 411, 281, 360, 2602, 307, 286, 1116, 1936, 411, 281, 4445, 264, 6916, 295, 1783, 281, 264, 350, 337, 512, 5754, 350, 13, 51564, 51564, 407, 309, 311, 364, 24922, 420, 257, 15706, 13, 51714, 51714, 400, 321, 576, 411, 281, 312, 1075, 281, 23203, 341, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06353511398644757, "compression_ratio": 1.8389513108614233, "no_speech_prob": 1.1659335541480687e-05}, {"id": 1054, "seek": 552700, "start": 5531.0, "end": 5535.0, "text": " Now, division actually can be reshuffled as follows.", "tokens": [50364, 321, 1116, 411, 281, 1936, 312, 1075, 281, 360, 257, 9845, 272, 11, 293, 321, 1116, 411, 341, 281, 312, 1075, 281, 976, 505, 1958, 13, 20, 13, 50564, 50564, 823, 11, 10044, 767, 393, 312, 725, 71, 33974, 382, 10002, 13, 50764, 50764, 759, 321, 362, 257, 9845, 272, 11, 300, 311, 767, 264, 912, 382, 257, 30955, 502, 670, 272, 13, 51014, 51014, 400, 300, 311, 264, 912, 382, 257, 30955, 272, 281, 264, 1347, 295, 3671, 502, 13, 51214, 51214, 400, 370, 437, 286, 1116, 411, 281, 360, 2602, 307, 286, 1116, 1936, 411, 281, 4445, 264, 6916, 295, 1783, 281, 264, 350, 337, 512, 5754, 350, 13, 51564, 51564, 407, 309, 311, 364, 24922, 420, 257, 15706, 13, 51714, 51714, 400, 321, 576, 411, 281, 312, 1075, 281, 23203, 341, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06353511398644757, "compression_ratio": 1.8389513108614233, "no_speech_prob": 1.1659335541480687e-05}, {"id": 1055, "seek": 552700, "start": 5535.0, "end": 5540.0, "text": " If we have a divide b, that's actually the same as a multiplying 1 over b.", "tokens": [50364, 321, 1116, 411, 281, 1936, 312, 1075, 281, 360, 257, 9845, 272, 11, 293, 321, 1116, 411, 341, 281, 312, 1075, 281, 976, 505, 1958, 13, 20, 13, 50564, 50564, 823, 11, 10044, 767, 393, 312, 725, 71, 33974, 382, 10002, 13, 50764, 50764, 759, 321, 362, 257, 9845, 272, 11, 300, 311, 767, 264, 912, 382, 257, 30955, 502, 670, 272, 13, 51014, 51014, 400, 300, 311, 264, 912, 382, 257, 30955, 272, 281, 264, 1347, 295, 3671, 502, 13, 51214, 51214, 400, 370, 437, 286, 1116, 411, 281, 360, 2602, 307, 286, 1116, 1936, 411, 281, 4445, 264, 6916, 295, 1783, 281, 264, 350, 337, 512, 5754, 350, 13, 51564, 51564, 407, 309, 311, 364, 24922, 420, 257, 15706, 13, 51714, 51714, 400, 321, 576, 411, 281, 312, 1075, 281, 23203, 341, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06353511398644757, "compression_ratio": 1.8389513108614233, "no_speech_prob": 1.1659335541480687e-05}, {"id": 1056, "seek": 552700, "start": 5540.0, "end": 5544.0, "text": " And that's the same as a multiplying b to the power of negative 1.", "tokens": [50364, 321, 1116, 411, 281, 1936, 312, 1075, 281, 360, 257, 9845, 272, 11, 293, 321, 1116, 411, 341, 281, 312, 1075, 281, 976, 505, 1958, 13, 20, 13, 50564, 50564, 823, 11, 10044, 767, 393, 312, 725, 71, 33974, 382, 10002, 13, 50764, 50764, 759, 321, 362, 257, 9845, 272, 11, 300, 311, 767, 264, 912, 382, 257, 30955, 502, 670, 272, 13, 51014, 51014, 400, 300, 311, 264, 912, 382, 257, 30955, 272, 281, 264, 1347, 295, 3671, 502, 13, 51214, 51214, 400, 370, 437, 286, 1116, 411, 281, 360, 2602, 307, 286, 1116, 1936, 411, 281, 4445, 264, 6916, 295, 1783, 281, 264, 350, 337, 512, 5754, 350, 13, 51564, 51564, 407, 309, 311, 364, 24922, 420, 257, 15706, 13, 51714, 51714, 400, 321, 576, 411, 281, 312, 1075, 281, 23203, 341, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06353511398644757, "compression_ratio": 1.8389513108614233, "no_speech_prob": 1.1659335541480687e-05}, {"id": 1057, "seek": 552700, "start": 5544.0, "end": 5551.0, "text": " And so what I'd like to do instead is I'd basically like to implement the operation of X to the k for some constant k.", "tokens": [50364, 321, 1116, 411, 281, 1936, 312, 1075, 281, 360, 257, 9845, 272, 11, 293, 321, 1116, 411, 341, 281, 312, 1075, 281, 976, 505, 1958, 13, 20, 13, 50564, 50564, 823, 11, 10044, 767, 393, 312, 725, 71, 33974, 382, 10002, 13, 50764, 50764, 759, 321, 362, 257, 9845, 272, 11, 300, 311, 767, 264, 912, 382, 257, 30955, 502, 670, 272, 13, 51014, 51014, 400, 300, 311, 264, 912, 382, 257, 30955, 272, 281, 264, 1347, 295, 3671, 502, 13, 51214, 51214, 400, 370, 437, 286, 1116, 411, 281, 360, 2602, 307, 286, 1116, 1936, 411, 281, 4445, 264, 6916, 295, 1783, 281, 264, 350, 337, 512, 5754, 350, 13, 51564, 51564, 407, 309, 311, 364, 24922, 420, 257, 15706, 13, 51714, 51714, 400, 321, 576, 411, 281, 312, 1075, 281, 23203, 341, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06353511398644757, "compression_ratio": 1.8389513108614233, "no_speech_prob": 1.1659335541480687e-05}, {"id": 1058, "seek": 552700, "start": 5551.0, "end": 5554.0, "text": " So it's an integer or a float.", "tokens": [50364, 321, 1116, 411, 281, 1936, 312, 1075, 281, 360, 257, 9845, 272, 11, 293, 321, 1116, 411, 341, 281, 312, 1075, 281, 976, 505, 1958, 13, 20, 13, 50564, 50564, 823, 11, 10044, 767, 393, 312, 725, 71, 33974, 382, 10002, 13, 50764, 50764, 759, 321, 362, 257, 9845, 272, 11, 300, 311, 767, 264, 912, 382, 257, 30955, 502, 670, 272, 13, 51014, 51014, 400, 300, 311, 264, 912, 382, 257, 30955, 272, 281, 264, 1347, 295, 3671, 502, 13, 51214, 51214, 400, 370, 437, 286, 1116, 411, 281, 360, 2602, 307, 286, 1116, 1936, 411, 281, 4445, 264, 6916, 295, 1783, 281, 264, 350, 337, 512, 5754, 350, 13, 51564, 51564, 407, 309, 311, 364, 24922, 420, 257, 15706, 13, 51714, 51714, 400, 321, 576, 411, 281, 312, 1075, 281, 23203, 341, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06353511398644757, "compression_ratio": 1.8389513108614233, "no_speech_prob": 1.1659335541480687e-05}, {"id": 1059, "seek": 552700, "start": 5554.0, "end": 5556.0, "text": " And we would like to be able to differentiate this.", "tokens": [50364, 321, 1116, 411, 281, 1936, 312, 1075, 281, 360, 257, 9845, 272, 11, 293, 321, 1116, 411, 341, 281, 312, 1075, 281, 976, 505, 1958, 13, 20, 13, 50564, 50564, 823, 11, 10044, 767, 393, 312, 725, 71, 33974, 382, 10002, 13, 50764, 50764, 759, 321, 362, 257, 9845, 272, 11, 300, 311, 767, 264, 912, 382, 257, 30955, 502, 670, 272, 13, 51014, 51014, 400, 300, 311, 264, 912, 382, 257, 30955, 272, 281, 264, 1347, 295, 3671, 502, 13, 51214, 51214, 400, 370, 437, 286, 1116, 411, 281, 360, 2602, 307, 286, 1116, 1936, 411, 281, 4445, 264, 6916, 295, 1783, 281, 264, 350, 337, 512, 5754, 350, 13, 51564, 51564, 407, 309, 311, 364, 24922, 420, 257, 15706, 13, 51714, 51714, 400, 321, 576, 411, 281, 312, 1075, 281, 23203, 341, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06353511398644757, "compression_ratio": 1.8389513108614233, "no_speech_prob": 1.1659335541480687e-05}, {"id": 1060, "seek": 555600, "start": 5556.0, "end": 5561.0, "text": " And then as a special case, negative 1 will be division.", "tokens": [50364, 400, 550, 382, 257, 2121, 1389, 11, 3671, 502, 486, 312, 10044, 13, 50614, 50614, 400, 370, 286, 478, 884, 300, 445, 570, 309, 311, 544, 2674, 13, 50764, 50764, 400, 1338, 11, 291, 1062, 382, 731, 360, 309, 300, 636, 13, 50864, 50864, 407, 1936, 437, 286, 478, 1566, 307, 321, 393, 38818, 533, 10044, 11, 597, 321, 486, 829, 510, 4079, 13, 51264, 51264, 865, 11, 321, 393, 829, 309, 510, 4079, 13, 51364, 51364, 708, 286, 478, 1566, 307, 300, 321, 393, 38818, 533, 10044, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08631282724359984, "compression_ratio": 1.7382198952879582, "no_speech_prob": 0.00010229637700831518}, {"id": 1061, "seek": 555600, "start": 5561.0, "end": 5564.0, "text": " And so I'm doing that just because it's more general.", "tokens": [50364, 400, 550, 382, 257, 2121, 1389, 11, 3671, 502, 486, 312, 10044, 13, 50614, 50614, 400, 370, 286, 478, 884, 300, 445, 570, 309, 311, 544, 2674, 13, 50764, 50764, 400, 1338, 11, 291, 1062, 382, 731, 360, 309, 300, 636, 13, 50864, 50864, 407, 1936, 437, 286, 478, 1566, 307, 321, 393, 38818, 533, 10044, 11, 597, 321, 486, 829, 510, 4079, 13, 51264, 51264, 865, 11, 321, 393, 829, 309, 510, 4079, 13, 51364, 51364, 708, 286, 478, 1566, 307, 300, 321, 393, 38818, 533, 10044, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08631282724359984, "compression_ratio": 1.7382198952879582, "no_speech_prob": 0.00010229637700831518}, {"id": 1062, "seek": 555600, "start": 5564.0, "end": 5566.0, "text": " And yeah, you might as well do it that way.", "tokens": [50364, 400, 550, 382, 257, 2121, 1389, 11, 3671, 502, 486, 312, 10044, 13, 50614, 50614, 400, 370, 286, 478, 884, 300, 445, 570, 309, 311, 544, 2674, 13, 50764, 50764, 400, 1338, 11, 291, 1062, 382, 731, 360, 309, 300, 636, 13, 50864, 50864, 407, 1936, 437, 286, 478, 1566, 307, 321, 393, 38818, 533, 10044, 11, 597, 321, 486, 829, 510, 4079, 13, 51264, 51264, 865, 11, 321, 393, 829, 309, 510, 4079, 13, 51364, 51364, 708, 286, 478, 1566, 307, 300, 321, 393, 38818, 533, 10044, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08631282724359984, "compression_ratio": 1.7382198952879582, "no_speech_prob": 0.00010229637700831518}, {"id": 1063, "seek": 555600, "start": 5566.0, "end": 5574.0, "text": " So basically what I'm saying is we can redefine division, which we will put here somewhere.", "tokens": [50364, 400, 550, 382, 257, 2121, 1389, 11, 3671, 502, 486, 312, 10044, 13, 50614, 50614, 400, 370, 286, 478, 884, 300, 445, 570, 309, 311, 544, 2674, 13, 50764, 50764, 400, 1338, 11, 291, 1062, 382, 731, 360, 309, 300, 636, 13, 50864, 50864, 407, 1936, 437, 286, 478, 1566, 307, 321, 393, 38818, 533, 10044, 11, 597, 321, 486, 829, 510, 4079, 13, 51264, 51264, 865, 11, 321, 393, 829, 309, 510, 4079, 13, 51364, 51364, 708, 286, 478, 1566, 307, 300, 321, 393, 38818, 533, 10044, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08631282724359984, "compression_ratio": 1.7382198952879582, "no_speech_prob": 0.00010229637700831518}, {"id": 1064, "seek": 555600, "start": 5574.0, "end": 5576.0, "text": " Yeah, we can put it here somewhere.", "tokens": [50364, 400, 550, 382, 257, 2121, 1389, 11, 3671, 502, 486, 312, 10044, 13, 50614, 50614, 400, 370, 286, 478, 884, 300, 445, 570, 309, 311, 544, 2674, 13, 50764, 50764, 400, 1338, 11, 291, 1062, 382, 731, 360, 309, 300, 636, 13, 50864, 50864, 407, 1936, 437, 286, 478, 1566, 307, 321, 393, 38818, 533, 10044, 11, 597, 321, 486, 829, 510, 4079, 13, 51264, 51264, 865, 11, 321, 393, 829, 309, 510, 4079, 13, 51364, 51364, 708, 286, 478, 1566, 307, 300, 321, 393, 38818, 533, 10044, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08631282724359984, "compression_ratio": 1.7382198952879582, "no_speech_prob": 0.00010229637700831518}, {"id": 1065, "seek": 555600, "start": 5576.0, "end": 5579.0, "text": " What I'm saying is that we can redefine division.", "tokens": [50364, 400, 550, 382, 257, 2121, 1389, 11, 3671, 502, 486, 312, 10044, 13, 50614, 50614, 400, 370, 286, 478, 884, 300, 445, 570, 309, 311, 544, 2674, 13, 50764, 50764, 400, 1338, 11, 291, 1062, 382, 731, 360, 309, 300, 636, 13, 50864, 50864, 407, 1936, 437, 286, 478, 1566, 307, 321, 393, 38818, 533, 10044, 11, 597, 321, 486, 829, 510, 4079, 13, 51264, 51264, 865, 11, 321, 393, 829, 309, 510, 4079, 13, 51364, 51364, 708, 286, 478, 1566, 307, 300, 321, 393, 38818, 533, 10044, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08631282724359984, "compression_ratio": 1.7382198952879582, "no_speech_prob": 0.00010229637700831518}, {"id": 1066, "seek": 557900, "start": 5579.0, "end": 5586.0, "text": " So self divide other can actually be rewritten as self times other to the power of negative 1.", "tokens": [50364, 407, 2698, 9845, 661, 393, 767, 312, 319, 26859, 382, 2698, 1413, 661, 281, 264, 1347, 295, 3671, 502, 13, 50714, 50714, 400, 586, 2158, 6005, 281, 264, 1347, 295, 3671, 502, 11, 321, 362, 586, 7642, 300, 13, 51014, 51014, 407, 510, 311, 370, 321, 643, 281, 4445, 264, 3388, 2445, 13, 51214, 51214, 2305, 669, 286, 516, 281, 829, 264, 3388, 2445, 30, 51314, 51314, 2704, 510, 4079, 13, 51414, 51414, 639, 307, 341, 818, 337, 309, 13, 51514, 51514, 407, 341, 2445, 486, 312, 1219, 562, 321, 853, 281, 5300, 257, 2158, 281, 512, 1347, 293, 661, 486, 312, 300, 1347, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1313012761807223, "compression_ratio": 1.800865800865801, "no_speech_prob": 5.3072879381943494e-05}, {"id": 1067, "seek": 557900, "start": 5586.0, "end": 5592.0, "text": " And now value raised to the power of negative 1, we have now defined that.", "tokens": [50364, 407, 2698, 9845, 661, 393, 767, 312, 319, 26859, 382, 2698, 1413, 661, 281, 264, 1347, 295, 3671, 502, 13, 50714, 50714, 400, 586, 2158, 6005, 281, 264, 1347, 295, 3671, 502, 11, 321, 362, 586, 7642, 300, 13, 51014, 51014, 407, 510, 311, 370, 321, 643, 281, 4445, 264, 3388, 2445, 13, 51214, 51214, 2305, 669, 286, 516, 281, 829, 264, 3388, 2445, 30, 51314, 51314, 2704, 510, 4079, 13, 51414, 51414, 639, 307, 341, 818, 337, 309, 13, 51514, 51514, 407, 341, 2445, 486, 312, 1219, 562, 321, 853, 281, 5300, 257, 2158, 281, 512, 1347, 293, 661, 486, 312, 300, 1347, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1313012761807223, "compression_ratio": 1.800865800865801, "no_speech_prob": 5.3072879381943494e-05}, {"id": 1068, "seek": 557900, "start": 5592.0, "end": 5596.0, "text": " So here's so we need to implement the pow function.", "tokens": [50364, 407, 2698, 9845, 661, 393, 767, 312, 319, 26859, 382, 2698, 1413, 661, 281, 264, 1347, 295, 3671, 502, 13, 50714, 50714, 400, 586, 2158, 6005, 281, 264, 1347, 295, 3671, 502, 11, 321, 362, 586, 7642, 300, 13, 51014, 51014, 407, 510, 311, 370, 321, 643, 281, 4445, 264, 3388, 2445, 13, 51214, 51214, 2305, 669, 286, 516, 281, 829, 264, 3388, 2445, 30, 51314, 51314, 2704, 510, 4079, 13, 51414, 51414, 639, 307, 341, 818, 337, 309, 13, 51514, 51514, 407, 341, 2445, 486, 312, 1219, 562, 321, 853, 281, 5300, 257, 2158, 281, 512, 1347, 293, 661, 486, 312, 300, 1347, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1313012761807223, "compression_ratio": 1.800865800865801, "no_speech_prob": 5.3072879381943494e-05}, {"id": 1069, "seek": 557900, "start": 5596.0, "end": 5598.0, "text": " Where am I going to put the pow function?", "tokens": [50364, 407, 2698, 9845, 661, 393, 767, 312, 319, 26859, 382, 2698, 1413, 661, 281, 264, 1347, 295, 3671, 502, 13, 50714, 50714, 400, 586, 2158, 6005, 281, 264, 1347, 295, 3671, 502, 11, 321, 362, 586, 7642, 300, 13, 51014, 51014, 407, 510, 311, 370, 321, 643, 281, 4445, 264, 3388, 2445, 13, 51214, 51214, 2305, 669, 286, 516, 281, 829, 264, 3388, 2445, 30, 51314, 51314, 2704, 510, 4079, 13, 51414, 51414, 639, 307, 341, 818, 337, 309, 13, 51514, 51514, 407, 341, 2445, 486, 312, 1219, 562, 321, 853, 281, 5300, 257, 2158, 281, 512, 1347, 293, 661, 486, 312, 300, 1347, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1313012761807223, "compression_ratio": 1.800865800865801, "no_speech_prob": 5.3072879381943494e-05}, {"id": 1070, "seek": 557900, "start": 5598.0, "end": 5600.0, "text": " Maybe here somewhere.", "tokens": [50364, 407, 2698, 9845, 661, 393, 767, 312, 319, 26859, 382, 2698, 1413, 661, 281, 264, 1347, 295, 3671, 502, 13, 50714, 50714, 400, 586, 2158, 6005, 281, 264, 1347, 295, 3671, 502, 11, 321, 362, 586, 7642, 300, 13, 51014, 51014, 407, 510, 311, 370, 321, 643, 281, 4445, 264, 3388, 2445, 13, 51214, 51214, 2305, 669, 286, 516, 281, 829, 264, 3388, 2445, 30, 51314, 51314, 2704, 510, 4079, 13, 51414, 51414, 639, 307, 341, 818, 337, 309, 13, 51514, 51514, 407, 341, 2445, 486, 312, 1219, 562, 321, 853, 281, 5300, 257, 2158, 281, 512, 1347, 293, 661, 486, 312, 300, 1347, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1313012761807223, "compression_ratio": 1.800865800865801, "no_speech_prob": 5.3072879381943494e-05}, {"id": 1071, "seek": 557900, "start": 5600.0, "end": 5602.0, "text": " This is this call for it.", "tokens": [50364, 407, 2698, 9845, 661, 393, 767, 312, 319, 26859, 382, 2698, 1413, 661, 281, 264, 1347, 295, 3671, 502, 13, 50714, 50714, 400, 586, 2158, 6005, 281, 264, 1347, 295, 3671, 502, 11, 321, 362, 586, 7642, 300, 13, 51014, 51014, 407, 510, 311, 370, 321, 643, 281, 4445, 264, 3388, 2445, 13, 51214, 51214, 2305, 669, 286, 516, 281, 829, 264, 3388, 2445, 30, 51314, 51314, 2704, 510, 4079, 13, 51414, 51414, 639, 307, 341, 818, 337, 309, 13, 51514, 51514, 407, 341, 2445, 486, 312, 1219, 562, 321, 853, 281, 5300, 257, 2158, 281, 512, 1347, 293, 661, 486, 312, 300, 1347, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1313012761807223, "compression_ratio": 1.800865800865801, "no_speech_prob": 5.3072879381943494e-05}, {"id": 1072, "seek": 557900, "start": 5602.0, "end": 5608.0, "text": " So this function will be called when we try to raise a value to some power and other will be that power.", "tokens": [50364, 407, 2698, 9845, 661, 393, 767, 312, 319, 26859, 382, 2698, 1413, 661, 281, 264, 1347, 295, 3671, 502, 13, 50714, 50714, 400, 586, 2158, 6005, 281, 264, 1347, 295, 3671, 502, 11, 321, 362, 586, 7642, 300, 13, 51014, 51014, 407, 510, 311, 370, 321, 643, 281, 4445, 264, 3388, 2445, 13, 51214, 51214, 2305, 669, 286, 516, 281, 829, 264, 3388, 2445, 30, 51314, 51314, 2704, 510, 4079, 13, 51414, 51414, 639, 307, 341, 818, 337, 309, 13, 51514, 51514, 407, 341, 2445, 486, 312, 1219, 562, 321, 853, 281, 5300, 257, 2158, 281, 512, 1347, 293, 661, 486, 312, 300, 1347, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1313012761807223, "compression_ratio": 1.800865800865801, "no_speech_prob": 5.3072879381943494e-05}, {"id": 1073, "seek": 560800, "start": 5608.0, "end": 5612.0, "text": " Now, I'd like to make sure that other is only an int or a float.", "tokens": [50364, 823, 11, 286, 1116, 411, 281, 652, 988, 300, 661, 307, 787, 364, 560, 420, 257, 15706, 13, 50564, 50564, 11419, 661, 307, 512, 733, 295, 257, 819, 2158, 2657, 13, 50714, 50714, 583, 510, 661, 486, 312, 7579, 281, 312, 364, 560, 420, 257, 15706, 13, 50864, 50864, 10328, 11, 264, 5221, 1582, 380, 589, 337, 437, 321, 434, 1382, 281, 4584, 294, 341, 2685, 1389, 13, 51164, 51164, 663, 576, 312, 257, 819, 13760, 6114, 498, 321, 1415, 661, 281, 312, 257, 2158, 13, 51414, 51414, 407, 510, 321, 1884, 264, 493, 264, 2158, 11, 597, 307, 445, 341, 1412, 6005, 281, 264, 1347, 295, 661, 13, 51664, 51664, 400, 661, 510, 727, 312, 11, 337, 1365, 11, 3671, 502, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07230121882881706, "compression_ratio": 1.7163120567375887, "no_speech_prob": 8.013003935047891e-06}, {"id": 1074, "seek": 560800, "start": 5612.0, "end": 5615.0, "text": " Usually other is some kind of a different value object.", "tokens": [50364, 823, 11, 286, 1116, 411, 281, 652, 988, 300, 661, 307, 787, 364, 560, 420, 257, 15706, 13, 50564, 50564, 11419, 661, 307, 512, 733, 295, 257, 819, 2158, 2657, 13, 50714, 50714, 583, 510, 661, 486, 312, 7579, 281, 312, 364, 560, 420, 257, 15706, 13, 50864, 50864, 10328, 11, 264, 5221, 1582, 380, 589, 337, 437, 321, 434, 1382, 281, 4584, 294, 341, 2685, 1389, 13, 51164, 51164, 663, 576, 312, 257, 819, 13760, 6114, 498, 321, 1415, 661, 281, 312, 257, 2158, 13, 51414, 51414, 407, 510, 321, 1884, 264, 493, 264, 2158, 11, 597, 307, 445, 341, 1412, 6005, 281, 264, 1347, 295, 661, 13, 51664, 51664, 400, 661, 510, 727, 312, 11, 337, 1365, 11, 3671, 502, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07230121882881706, "compression_ratio": 1.7163120567375887, "no_speech_prob": 8.013003935047891e-06}, {"id": 1075, "seek": 560800, "start": 5615.0, "end": 5618.0, "text": " But here other will be forced to be an int or a float.", "tokens": [50364, 823, 11, 286, 1116, 411, 281, 652, 988, 300, 661, 307, 787, 364, 560, 420, 257, 15706, 13, 50564, 50564, 11419, 661, 307, 512, 733, 295, 257, 819, 2158, 2657, 13, 50714, 50714, 583, 510, 661, 486, 312, 7579, 281, 312, 364, 560, 420, 257, 15706, 13, 50864, 50864, 10328, 11, 264, 5221, 1582, 380, 589, 337, 437, 321, 434, 1382, 281, 4584, 294, 341, 2685, 1389, 13, 51164, 51164, 663, 576, 312, 257, 819, 13760, 6114, 498, 321, 1415, 661, 281, 312, 257, 2158, 13, 51414, 51414, 407, 510, 321, 1884, 264, 493, 264, 2158, 11, 597, 307, 445, 341, 1412, 6005, 281, 264, 1347, 295, 661, 13, 51664, 51664, 400, 661, 510, 727, 312, 11, 337, 1365, 11, 3671, 502, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07230121882881706, "compression_ratio": 1.7163120567375887, "no_speech_prob": 8.013003935047891e-06}, {"id": 1076, "seek": 560800, "start": 5618.0, "end": 5624.0, "text": " Otherwise, the math won't work for what we're trying to achieve in this specific case.", "tokens": [50364, 823, 11, 286, 1116, 411, 281, 652, 988, 300, 661, 307, 787, 364, 560, 420, 257, 15706, 13, 50564, 50564, 11419, 661, 307, 512, 733, 295, 257, 819, 2158, 2657, 13, 50714, 50714, 583, 510, 661, 486, 312, 7579, 281, 312, 364, 560, 420, 257, 15706, 13, 50864, 50864, 10328, 11, 264, 5221, 1582, 380, 589, 337, 437, 321, 434, 1382, 281, 4584, 294, 341, 2685, 1389, 13, 51164, 51164, 663, 576, 312, 257, 819, 13760, 6114, 498, 321, 1415, 661, 281, 312, 257, 2158, 13, 51414, 51414, 407, 510, 321, 1884, 264, 493, 264, 2158, 11, 597, 307, 445, 341, 1412, 6005, 281, 264, 1347, 295, 661, 13, 51664, 51664, 400, 661, 510, 727, 312, 11, 337, 1365, 11, 3671, 502, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07230121882881706, "compression_ratio": 1.7163120567375887, "no_speech_prob": 8.013003935047891e-06}, {"id": 1077, "seek": 560800, "start": 5624.0, "end": 5629.0, "text": " That would be a different derivative expression if we wanted other to be a value.", "tokens": [50364, 823, 11, 286, 1116, 411, 281, 652, 988, 300, 661, 307, 787, 364, 560, 420, 257, 15706, 13, 50564, 50564, 11419, 661, 307, 512, 733, 295, 257, 819, 2158, 2657, 13, 50714, 50714, 583, 510, 661, 486, 312, 7579, 281, 312, 364, 560, 420, 257, 15706, 13, 50864, 50864, 10328, 11, 264, 5221, 1582, 380, 589, 337, 437, 321, 434, 1382, 281, 4584, 294, 341, 2685, 1389, 13, 51164, 51164, 663, 576, 312, 257, 819, 13760, 6114, 498, 321, 1415, 661, 281, 312, 257, 2158, 13, 51414, 51414, 407, 510, 321, 1884, 264, 493, 264, 2158, 11, 597, 307, 445, 341, 1412, 6005, 281, 264, 1347, 295, 661, 13, 51664, 51664, 400, 661, 510, 727, 312, 11, 337, 1365, 11, 3671, 502, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07230121882881706, "compression_ratio": 1.7163120567375887, "no_speech_prob": 8.013003935047891e-06}, {"id": 1078, "seek": 560800, "start": 5629.0, "end": 5634.0, "text": " So here we create the up the value, which is just this data raised to the power of other.", "tokens": [50364, 823, 11, 286, 1116, 411, 281, 652, 988, 300, 661, 307, 787, 364, 560, 420, 257, 15706, 13, 50564, 50564, 11419, 661, 307, 512, 733, 295, 257, 819, 2158, 2657, 13, 50714, 50714, 583, 510, 661, 486, 312, 7579, 281, 312, 364, 560, 420, 257, 15706, 13, 50864, 50864, 10328, 11, 264, 5221, 1582, 380, 589, 337, 437, 321, 434, 1382, 281, 4584, 294, 341, 2685, 1389, 13, 51164, 51164, 663, 576, 312, 257, 819, 13760, 6114, 498, 321, 1415, 661, 281, 312, 257, 2158, 13, 51414, 51414, 407, 510, 321, 1884, 264, 493, 264, 2158, 11, 597, 307, 445, 341, 1412, 6005, 281, 264, 1347, 295, 661, 13, 51664, 51664, 400, 661, 510, 727, 312, 11, 337, 1365, 11, 3671, 502, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07230121882881706, "compression_ratio": 1.7163120567375887, "no_speech_prob": 8.013003935047891e-06}, {"id": 1079, "seek": 560800, "start": 5634.0, "end": 5636.0, "text": " And other here could be, for example, negative 1.", "tokens": [50364, 823, 11, 286, 1116, 411, 281, 652, 988, 300, 661, 307, 787, 364, 560, 420, 257, 15706, 13, 50564, 50564, 11419, 661, 307, 512, 733, 295, 257, 819, 2158, 2657, 13, 50714, 50714, 583, 510, 661, 486, 312, 7579, 281, 312, 364, 560, 420, 257, 15706, 13, 50864, 50864, 10328, 11, 264, 5221, 1582, 380, 589, 337, 437, 321, 434, 1382, 281, 4584, 294, 341, 2685, 1389, 13, 51164, 51164, 663, 576, 312, 257, 819, 13760, 6114, 498, 321, 1415, 661, 281, 312, 257, 2158, 13, 51414, 51414, 407, 510, 321, 1884, 264, 493, 264, 2158, 11, 597, 307, 445, 341, 1412, 6005, 281, 264, 1347, 295, 661, 13, 51664, 51664, 400, 661, 510, 727, 312, 11, 337, 1365, 11, 3671, 502, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07230121882881706, "compression_ratio": 1.7163120567375887, "no_speech_prob": 8.013003935047891e-06}, {"id": 1080, "seek": 563600, "start": 5636.0, "end": 5639.0, "text": " That's what we are hoping to achieve.", "tokens": [50364, 663, 311, 437, 321, 366, 7159, 281, 4584, 13, 50514, 50514, 400, 550, 341, 307, 264, 12204, 20266, 13, 50664, 50664, 400, 341, 307, 264, 1019, 644, 11, 597, 307, 437, 307, 264, 5021, 4978, 6114, 510, 337, 646, 337, 646, 12425, 990, 807, 264, 1347, 2445, 11, 51164, 51164, 689, 264, 1347, 307, 281, 264, 1347, 295, 512, 733, 295, 257, 5754, 13, 51314, 51314, 407, 341, 307, 264, 5380, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.1146010348671361, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.9439808460883796e-05}, {"id": 1081, "seek": 563600, "start": 5639.0, "end": 5642.0, "text": " And then this is the backwards stub.", "tokens": [50364, 663, 311, 437, 321, 366, 7159, 281, 4584, 13, 50514, 50514, 400, 550, 341, 307, 264, 12204, 20266, 13, 50664, 50664, 400, 341, 307, 264, 1019, 644, 11, 597, 307, 437, 307, 264, 5021, 4978, 6114, 510, 337, 646, 337, 646, 12425, 990, 807, 264, 1347, 2445, 11, 51164, 51164, 689, 264, 1347, 307, 281, 264, 1347, 295, 512, 733, 295, 257, 5754, 13, 51314, 51314, 407, 341, 307, 264, 5380, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.1146010348671361, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.9439808460883796e-05}, {"id": 1082, "seek": 563600, "start": 5642.0, "end": 5652.0, "text": " And this is the fun part, which is what is the chain rule expression here for back for back propagating through the power function,", "tokens": [50364, 663, 311, 437, 321, 366, 7159, 281, 4584, 13, 50514, 50514, 400, 550, 341, 307, 264, 12204, 20266, 13, 50664, 50664, 400, 341, 307, 264, 1019, 644, 11, 597, 307, 437, 307, 264, 5021, 4978, 6114, 510, 337, 646, 337, 646, 12425, 990, 807, 264, 1347, 2445, 11, 51164, 51164, 689, 264, 1347, 307, 281, 264, 1347, 295, 512, 733, 295, 257, 5754, 13, 51314, 51314, 407, 341, 307, 264, 5380, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.1146010348671361, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.9439808460883796e-05}, {"id": 1083, "seek": 563600, "start": 5652.0, "end": 5655.0, "text": " where the power is to the power of some kind of a constant.", "tokens": [50364, 663, 311, 437, 321, 366, 7159, 281, 4584, 13, 50514, 50514, 400, 550, 341, 307, 264, 12204, 20266, 13, 50664, 50664, 400, 341, 307, 264, 1019, 644, 11, 597, 307, 437, 307, 264, 5021, 4978, 6114, 510, 337, 646, 337, 646, 12425, 990, 807, 264, 1347, 2445, 11, 51164, 51164, 689, 264, 1347, 307, 281, 264, 1347, 295, 512, 733, 295, 257, 5754, 13, 51314, 51314, 407, 341, 307, 264, 5380, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.1146010348671361, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.9439808460883796e-05}, {"id": 1084, "seek": 563600, "start": 5655.0, "end": 5657.0, "text": " So this is the exercise.", "tokens": [50364, 663, 311, 437, 321, 366, 7159, 281, 4584, 13, 50514, 50514, 400, 550, 341, 307, 264, 12204, 20266, 13, 50664, 50664, 400, 341, 307, 264, 1019, 644, 11, 597, 307, 437, 307, 264, 5021, 4978, 6114, 510, 337, 646, 337, 646, 12425, 990, 807, 264, 1347, 2445, 11, 51164, 51164, 689, 264, 1347, 307, 281, 264, 1347, 295, 512, 733, 295, 257, 5754, 13, 51314, 51314, 407, 341, 307, 264, 5380, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.1146010348671361, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.9439808460883796e-05}, {"id": 1085, "seek": 565700, "start": 5657.0, "end": 5667.0, "text": " So maybe pause the video here and see if you can figure it out yourself as to what we should put here.", "tokens": [50364, 407, 1310, 10465, 264, 960, 510, 293, 536, 498, 291, 393, 2573, 309, 484, 1803, 382, 281, 437, 321, 820, 829, 510, 13, 50864, 50864, 2264, 11, 370, 291, 393, 767, 352, 510, 293, 574, 412, 13760, 4474, 382, 364, 1365, 13, 51164, 51164, 400, 321, 536, 3195, 295, 264, 4474, 300, 291, 393, 4696, 458, 490, 33400, 13, 51314, 51314, 682, 1729, 11, 437, 321, 434, 1237, 337, 307, 264, 1347, 4978, 11, 51464, 51464, 570, 300, 311, 3585, 505, 300, 498, 321, 434, 1382, 281, 747, 274, 538, 30017, 295, 2031, 281, 264, 297, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07668379509803092, "compression_ratio": 1.6244725738396624, "no_speech_prob": 9.665863217378501e-06}, {"id": 1086, "seek": 565700, "start": 5667.0, "end": 5673.0, "text": " OK, so you can actually go here and look at derivative rules as an example.", "tokens": [50364, 407, 1310, 10465, 264, 960, 510, 293, 536, 498, 291, 393, 2573, 309, 484, 1803, 382, 281, 437, 321, 820, 829, 510, 13, 50864, 50864, 2264, 11, 370, 291, 393, 767, 352, 510, 293, 574, 412, 13760, 4474, 382, 364, 1365, 13, 51164, 51164, 400, 321, 536, 3195, 295, 264, 4474, 300, 291, 393, 4696, 458, 490, 33400, 13, 51314, 51314, 682, 1729, 11, 437, 321, 434, 1237, 337, 307, 264, 1347, 4978, 11, 51464, 51464, 570, 300, 311, 3585, 505, 300, 498, 321, 434, 1382, 281, 747, 274, 538, 30017, 295, 2031, 281, 264, 297, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07668379509803092, "compression_ratio": 1.6244725738396624, "no_speech_prob": 9.665863217378501e-06}, {"id": 1087, "seek": 565700, "start": 5673.0, "end": 5676.0, "text": " And we see lots of the rules that you can hopefully know from calculus.", "tokens": [50364, 407, 1310, 10465, 264, 960, 510, 293, 536, 498, 291, 393, 2573, 309, 484, 1803, 382, 281, 437, 321, 820, 829, 510, 13, 50864, 50864, 2264, 11, 370, 291, 393, 767, 352, 510, 293, 574, 412, 13760, 4474, 382, 364, 1365, 13, 51164, 51164, 400, 321, 536, 3195, 295, 264, 4474, 300, 291, 393, 4696, 458, 490, 33400, 13, 51314, 51314, 682, 1729, 11, 437, 321, 434, 1237, 337, 307, 264, 1347, 4978, 11, 51464, 51464, 570, 300, 311, 3585, 505, 300, 498, 321, 434, 1382, 281, 747, 274, 538, 30017, 295, 2031, 281, 264, 297, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07668379509803092, "compression_ratio": 1.6244725738396624, "no_speech_prob": 9.665863217378501e-06}, {"id": 1088, "seek": 565700, "start": 5676.0, "end": 5679.0, "text": " In particular, what we're looking for is the power rule,", "tokens": [50364, 407, 1310, 10465, 264, 960, 510, 293, 536, 498, 291, 393, 2573, 309, 484, 1803, 382, 281, 437, 321, 820, 829, 510, 13, 50864, 50864, 2264, 11, 370, 291, 393, 767, 352, 510, 293, 574, 412, 13760, 4474, 382, 364, 1365, 13, 51164, 51164, 400, 321, 536, 3195, 295, 264, 4474, 300, 291, 393, 4696, 458, 490, 33400, 13, 51314, 51314, 682, 1729, 11, 437, 321, 434, 1237, 337, 307, 264, 1347, 4978, 11, 51464, 51464, 570, 300, 311, 3585, 505, 300, 498, 321, 434, 1382, 281, 747, 274, 538, 30017, 295, 2031, 281, 264, 297, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07668379509803092, "compression_ratio": 1.6244725738396624, "no_speech_prob": 9.665863217378501e-06}, {"id": 1089, "seek": 565700, "start": 5679.0, "end": 5683.0, "text": " because that's telling us that if we're trying to take d by dx of x to the n,", "tokens": [50364, 407, 1310, 10465, 264, 960, 510, 293, 536, 498, 291, 393, 2573, 309, 484, 1803, 382, 281, 437, 321, 820, 829, 510, 13, 50864, 50864, 2264, 11, 370, 291, 393, 767, 352, 510, 293, 574, 412, 13760, 4474, 382, 364, 1365, 13, 51164, 51164, 400, 321, 536, 3195, 295, 264, 4474, 300, 291, 393, 4696, 458, 490, 33400, 13, 51314, 51314, 682, 1729, 11, 437, 321, 434, 1237, 337, 307, 264, 1347, 4978, 11, 51464, 51464, 570, 300, 311, 3585, 505, 300, 498, 321, 434, 1382, 281, 747, 274, 538, 30017, 295, 2031, 281, 264, 297, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07668379509803092, "compression_ratio": 1.6244725738396624, "no_speech_prob": 9.665863217378501e-06}, {"id": 1090, "seek": 568300, "start": 5683.0, "end": 5688.0, "text": " which is what we're doing here, then that is just n times x to the n minus one.", "tokens": [50364, 597, 307, 437, 321, 434, 884, 510, 11, 550, 300, 307, 445, 297, 1413, 2031, 281, 264, 297, 3175, 472, 13, 50614, 50614, 1779, 13, 2264, 13, 50764, 50764, 407, 300, 311, 3585, 505, 466, 264, 2654, 13760, 295, 341, 1347, 6916, 13, 51014, 51014, 407, 439, 321, 528, 510, 11, 1936, 11, 297, 307, 586, 661, 293, 2698, 1412, 307, 2031, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.11406924831333445, "compression_ratio": 1.4110429447852761, "no_speech_prob": 2.6273442927049473e-05}, {"id": 1091, "seek": 568300, "start": 5688.0, "end": 5691.0, "text": " Right. OK.", "tokens": [50364, 597, 307, 437, 321, 434, 884, 510, 11, 550, 300, 307, 445, 297, 1413, 2031, 281, 264, 297, 3175, 472, 13, 50614, 50614, 1779, 13, 2264, 13, 50764, 50764, 407, 300, 311, 3585, 505, 466, 264, 2654, 13760, 295, 341, 1347, 6916, 13, 51014, 51014, 407, 439, 321, 528, 510, 11, 1936, 11, 297, 307, 586, 661, 293, 2698, 1412, 307, 2031, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.11406924831333445, "compression_ratio": 1.4110429447852761, "no_speech_prob": 2.6273442927049473e-05}, {"id": 1092, "seek": 568300, "start": 5691.0, "end": 5696.0, "text": " So that's telling us about the local derivative of this power operation.", "tokens": [50364, 597, 307, 437, 321, 434, 884, 510, 11, 550, 300, 307, 445, 297, 1413, 2031, 281, 264, 297, 3175, 472, 13, 50614, 50614, 1779, 13, 2264, 13, 50764, 50764, 407, 300, 311, 3585, 505, 466, 264, 2654, 13760, 295, 341, 1347, 6916, 13, 51014, 51014, 407, 439, 321, 528, 510, 11, 1936, 11, 297, 307, 586, 661, 293, 2698, 1412, 307, 2031, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.11406924831333445, "compression_ratio": 1.4110429447852761, "no_speech_prob": 2.6273442927049473e-05}, {"id": 1093, "seek": 568300, "start": 5696.0, "end": 5703.0, "text": " So all we want here, basically, n is now other and self data is x.", "tokens": [50364, 597, 307, 437, 321, 434, 884, 510, 11, 550, 300, 307, 445, 297, 1413, 2031, 281, 264, 297, 3175, 472, 13, 50614, 50614, 1779, 13, 2264, 13, 50764, 50764, 407, 300, 311, 3585, 505, 466, 264, 2654, 13760, 295, 341, 1347, 6916, 13, 51014, 51014, 407, 439, 321, 528, 510, 11, 1936, 11, 297, 307, 586, 661, 293, 2698, 1412, 307, 2031, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.11406924831333445, "compression_ratio": 1.4110429447852761, "no_speech_prob": 2.6273442927049473e-05}, {"id": 1094, "seek": 570300, "start": 5703.0, "end": 5713.0, "text": " And so this now becomes other, which is n times self data, which is now a Python int or a float.", "tokens": [50364, 400, 370, 341, 586, 3643, 661, 11, 597, 307, 297, 1413, 2698, 1412, 11, 597, 307, 586, 257, 15329, 560, 420, 257, 15706, 13, 50864, 50864, 467, 311, 406, 257, 2158, 2657, 13, 50914, 50914, 492, 434, 26440, 264, 1412, 19667, 6005, 281, 264, 1347, 295, 661, 3175, 472, 420, 297, 3175, 472, 13, 51264, 51264, 286, 393, 829, 26179, 926, 341, 11, 457, 341, 1177, 380, 1871, 570, 1347, 2516, 16969, 655, 670, 12972, 294, 15329, 13, 51614, 51614, 407, 300, 576, 362, 668, 2264, 13, 51664, 51664, 400, 300, 311, 264, 2654, 13760, 787, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08404099115050666, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.012967732094694e-05}, {"id": 1095, "seek": 570300, "start": 5713.0, "end": 5714.0, "text": " It's not a value object.", "tokens": [50364, 400, 370, 341, 586, 3643, 661, 11, 597, 307, 297, 1413, 2698, 1412, 11, 597, 307, 586, 257, 15329, 560, 420, 257, 15706, 13, 50864, 50864, 467, 311, 406, 257, 2158, 2657, 13, 50914, 50914, 492, 434, 26440, 264, 1412, 19667, 6005, 281, 264, 1347, 295, 661, 3175, 472, 420, 297, 3175, 472, 13, 51264, 51264, 286, 393, 829, 26179, 926, 341, 11, 457, 341, 1177, 380, 1871, 570, 1347, 2516, 16969, 655, 670, 12972, 294, 15329, 13, 51614, 51614, 407, 300, 576, 362, 668, 2264, 13, 51664, 51664, 400, 300, 311, 264, 2654, 13760, 787, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08404099115050666, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.012967732094694e-05}, {"id": 1096, "seek": 570300, "start": 5714.0, "end": 5721.0, "text": " We're accessing the data attribute raised to the power of other minus one or n minus one.", "tokens": [50364, 400, 370, 341, 586, 3643, 661, 11, 597, 307, 297, 1413, 2698, 1412, 11, 597, 307, 586, 257, 15329, 560, 420, 257, 15706, 13, 50864, 50864, 467, 311, 406, 257, 2158, 2657, 13, 50914, 50914, 492, 434, 26440, 264, 1412, 19667, 6005, 281, 264, 1347, 295, 661, 3175, 472, 420, 297, 3175, 472, 13, 51264, 51264, 286, 393, 829, 26179, 926, 341, 11, 457, 341, 1177, 380, 1871, 570, 1347, 2516, 16969, 655, 670, 12972, 294, 15329, 13, 51614, 51614, 407, 300, 576, 362, 668, 2264, 13, 51664, 51664, 400, 300, 311, 264, 2654, 13760, 787, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08404099115050666, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.012967732094694e-05}, {"id": 1097, "seek": 570300, "start": 5721.0, "end": 5728.0, "text": " I can put brackets around this, but this doesn't matter because power takes precedence over multiply in Python.", "tokens": [50364, 400, 370, 341, 586, 3643, 661, 11, 597, 307, 297, 1413, 2698, 1412, 11, 597, 307, 586, 257, 15329, 560, 420, 257, 15706, 13, 50864, 50864, 467, 311, 406, 257, 2158, 2657, 13, 50914, 50914, 492, 434, 26440, 264, 1412, 19667, 6005, 281, 264, 1347, 295, 661, 3175, 472, 420, 297, 3175, 472, 13, 51264, 51264, 286, 393, 829, 26179, 926, 341, 11, 457, 341, 1177, 380, 1871, 570, 1347, 2516, 16969, 655, 670, 12972, 294, 15329, 13, 51614, 51614, 407, 300, 576, 362, 668, 2264, 13, 51664, 51664, 400, 300, 311, 264, 2654, 13760, 787, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08404099115050666, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.012967732094694e-05}, {"id": 1098, "seek": 570300, "start": 5728.0, "end": 5729.0, "text": " So that would have been OK.", "tokens": [50364, 400, 370, 341, 586, 3643, 661, 11, 597, 307, 297, 1413, 2698, 1412, 11, 597, 307, 586, 257, 15329, 560, 420, 257, 15706, 13, 50864, 50864, 467, 311, 406, 257, 2158, 2657, 13, 50914, 50914, 492, 434, 26440, 264, 1412, 19667, 6005, 281, 264, 1347, 295, 661, 3175, 472, 420, 297, 3175, 472, 13, 51264, 51264, 286, 393, 829, 26179, 926, 341, 11, 457, 341, 1177, 380, 1871, 570, 1347, 2516, 16969, 655, 670, 12972, 294, 15329, 13, 51614, 51614, 407, 300, 576, 362, 668, 2264, 13, 51664, 51664, 400, 300, 311, 264, 2654, 13760, 787, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08404099115050666, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.012967732094694e-05}, {"id": 1099, "seek": 570300, "start": 5729.0, "end": 5731.0, "text": " And that's the local derivative only.", "tokens": [50364, 400, 370, 341, 586, 3643, 661, 11, 597, 307, 297, 1413, 2698, 1412, 11, 597, 307, 586, 257, 15329, 560, 420, 257, 15706, 13, 50864, 50864, 467, 311, 406, 257, 2158, 2657, 13, 50914, 50914, 492, 434, 26440, 264, 1412, 19667, 6005, 281, 264, 1347, 295, 661, 3175, 472, 420, 297, 3175, 472, 13, 51264, 51264, 286, 393, 829, 26179, 926, 341, 11, 457, 341, 1177, 380, 1871, 570, 1347, 2516, 16969, 655, 670, 12972, 294, 15329, 13, 51614, 51614, 407, 300, 576, 362, 668, 2264, 13, 51664, 51664, 400, 300, 311, 264, 2654, 13760, 787, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08404099115050666, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.012967732094694e-05}, {"id": 1100, "seek": 573100, "start": 5731.0, "end": 5735.0, "text": " But now we have to chain it and we change it simply by multiplying by our dot grad.", "tokens": [50364, 583, 586, 321, 362, 281, 5021, 309, 293, 321, 1319, 309, 2935, 538, 30955, 538, 527, 5893, 2771, 13, 50564, 50564, 663, 311, 5021, 4978, 13, 50614, 50614, 400, 341, 820, 12120, 589, 13, 50814, 50814, 400, 321, 434, 516, 281, 915, 484, 2321, 13, 50914, 50914, 583, 586, 498, 321, 360, 341, 11, 341, 820, 586, 589, 13, 51114, 51114, 400, 321, 483, 935, 1732, 13, 51214, 51214, 407, 264, 2128, 1320, 1985, 11, 457, 775, 264, 23897, 1320, 589, 30, 51364, 51364, 400, 286, 4325, 300, 321, 767, 611, 362, 281, 458, 577, 281, 16390, 13, 51514, 51514, 407, 558, 586, 11, 257, 3175, 272, 486, 406, 589, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12618793819261634, "compression_ratio": 1.6791666666666667, "no_speech_prob": 8.480159158352762e-05}, {"id": 1101, "seek": 573100, "start": 5735.0, "end": 5736.0, "text": " That's chain rule.", "tokens": [50364, 583, 586, 321, 362, 281, 5021, 309, 293, 321, 1319, 309, 2935, 538, 30955, 538, 527, 5893, 2771, 13, 50564, 50564, 663, 311, 5021, 4978, 13, 50614, 50614, 400, 341, 820, 12120, 589, 13, 50814, 50814, 400, 321, 434, 516, 281, 915, 484, 2321, 13, 50914, 50914, 583, 586, 498, 321, 360, 341, 11, 341, 820, 586, 589, 13, 51114, 51114, 400, 321, 483, 935, 1732, 13, 51214, 51214, 407, 264, 2128, 1320, 1985, 11, 457, 775, 264, 23897, 1320, 589, 30, 51364, 51364, 400, 286, 4325, 300, 321, 767, 611, 362, 281, 458, 577, 281, 16390, 13, 51514, 51514, 407, 558, 586, 11, 257, 3175, 272, 486, 406, 589, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12618793819261634, "compression_ratio": 1.6791666666666667, "no_speech_prob": 8.480159158352762e-05}, {"id": 1102, "seek": 573100, "start": 5736.0, "end": 5740.0, "text": " And this should technically work.", "tokens": [50364, 583, 586, 321, 362, 281, 5021, 309, 293, 321, 1319, 309, 2935, 538, 30955, 538, 527, 5893, 2771, 13, 50564, 50564, 663, 311, 5021, 4978, 13, 50614, 50614, 400, 341, 820, 12120, 589, 13, 50814, 50814, 400, 321, 434, 516, 281, 915, 484, 2321, 13, 50914, 50914, 583, 586, 498, 321, 360, 341, 11, 341, 820, 586, 589, 13, 51114, 51114, 400, 321, 483, 935, 1732, 13, 51214, 51214, 407, 264, 2128, 1320, 1985, 11, 457, 775, 264, 23897, 1320, 589, 30, 51364, 51364, 400, 286, 4325, 300, 321, 767, 611, 362, 281, 458, 577, 281, 16390, 13, 51514, 51514, 407, 558, 586, 11, 257, 3175, 272, 486, 406, 589, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12618793819261634, "compression_ratio": 1.6791666666666667, "no_speech_prob": 8.480159158352762e-05}, {"id": 1103, "seek": 573100, "start": 5740.0, "end": 5742.0, "text": " And we're going to find out soon.", "tokens": [50364, 583, 586, 321, 362, 281, 5021, 309, 293, 321, 1319, 309, 2935, 538, 30955, 538, 527, 5893, 2771, 13, 50564, 50564, 663, 311, 5021, 4978, 13, 50614, 50614, 400, 341, 820, 12120, 589, 13, 50814, 50814, 400, 321, 434, 516, 281, 915, 484, 2321, 13, 50914, 50914, 583, 586, 498, 321, 360, 341, 11, 341, 820, 586, 589, 13, 51114, 51114, 400, 321, 483, 935, 1732, 13, 51214, 51214, 407, 264, 2128, 1320, 1985, 11, 457, 775, 264, 23897, 1320, 589, 30, 51364, 51364, 400, 286, 4325, 300, 321, 767, 611, 362, 281, 458, 577, 281, 16390, 13, 51514, 51514, 407, 558, 586, 11, 257, 3175, 272, 486, 406, 589, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12618793819261634, "compression_ratio": 1.6791666666666667, "no_speech_prob": 8.480159158352762e-05}, {"id": 1104, "seek": 573100, "start": 5742.0, "end": 5746.0, "text": " But now if we do this, this should now work.", "tokens": [50364, 583, 586, 321, 362, 281, 5021, 309, 293, 321, 1319, 309, 2935, 538, 30955, 538, 527, 5893, 2771, 13, 50564, 50564, 663, 311, 5021, 4978, 13, 50614, 50614, 400, 341, 820, 12120, 589, 13, 50814, 50814, 400, 321, 434, 516, 281, 915, 484, 2321, 13, 50914, 50914, 583, 586, 498, 321, 360, 341, 11, 341, 820, 586, 589, 13, 51114, 51114, 400, 321, 483, 935, 1732, 13, 51214, 51214, 407, 264, 2128, 1320, 1985, 11, 457, 775, 264, 23897, 1320, 589, 30, 51364, 51364, 400, 286, 4325, 300, 321, 767, 611, 362, 281, 458, 577, 281, 16390, 13, 51514, 51514, 407, 558, 586, 11, 257, 3175, 272, 486, 406, 589, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12618793819261634, "compression_ratio": 1.6791666666666667, "no_speech_prob": 8.480159158352762e-05}, {"id": 1105, "seek": 573100, "start": 5746.0, "end": 5748.0, "text": " And we get point five.", "tokens": [50364, 583, 586, 321, 362, 281, 5021, 309, 293, 321, 1319, 309, 2935, 538, 30955, 538, 527, 5893, 2771, 13, 50564, 50564, 663, 311, 5021, 4978, 13, 50614, 50614, 400, 341, 820, 12120, 589, 13, 50814, 50814, 400, 321, 434, 516, 281, 915, 484, 2321, 13, 50914, 50914, 583, 586, 498, 321, 360, 341, 11, 341, 820, 586, 589, 13, 51114, 51114, 400, 321, 483, 935, 1732, 13, 51214, 51214, 407, 264, 2128, 1320, 1985, 11, 457, 775, 264, 23897, 1320, 589, 30, 51364, 51364, 400, 286, 4325, 300, 321, 767, 611, 362, 281, 458, 577, 281, 16390, 13, 51514, 51514, 407, 558, 586, 11, 257, 3175, 272, 486, 406, 589, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12618793819261634, "compression_ratio": 1.6791666666666667, "no_speech_prob": 8.480159158352762e-05}, {"id": 1106, "seek": 573100, "start": 5748.0, "end": 5751.0, "text": " So the forward pass works, but does the backward pass work?", "tokens": [50364, 583, 586, 321, 362, 281, 5021, 309, 293, 321, 1319, 309, 2935, 538, 30955, 538, 527, 5893, 2771, 13, 50564, 50564, 663, 311, 5021, 4978, 13, 50614, 50614, 400, 341, 820, 12120, 589, 13, 50814, 50814, 400, 321, 434, 516, 281, 915, 484, 2321, 13, 50914, 50914, 583, 586, 498, 321, 360, 341, 11, 341, 820, 586, 589, 13, 51114, 51114, 400, 321, 483, 935, 1732, 13, 51214, 51214, 407, 264, 2128, 1320, 1985, 11, 457, 775, 264, 23897, 1320, 589, 30, 51364, 51364, 400, 286, 4325, 300, 321, 767, 611, 362, 281, 458, 577, 281, 16390, 13, 51514, 51514, 407, 558, 586, 11, 257, 3175, 272, 486, 406, 589, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12618793819261634, "compression_ratio": 1.6791666666666667, "no_speech_prob": 8.480159158352762e-05}, {"id": 1107, "seek": 573100, "start": 5751.0, "end": 5754.0, "text": " And I realize that we actually also have to know how to subtract.", "tokens": [50364, 583, 586, 321, 362, 281, 5021, 309, 293, 321, 1319, 309, 2935, 538, 30955, 538, 527, 5893, 2771, 13, 50564, 50564, 663, 311, 5021, 4978, 13, 50614, 50614, 400, 341, 820, 12120, 589, 13, 50814, 50814, 400, 321, 434, 516, 281, 915, 484, 2321, 13, 50914, 50914, 583, 586, 498, 321, 360, 341, 11, 341, 820, 586, 589, 13, 51114, 51114, 400, 321, 483, 935, 1732, 13, 51214, 51214, 407, 264, 2128, 1320, 1985, 11, 457, 775, 264, 23897, 1320, 589, 30, 51364, 51364, 400, 286, 4325, 300, 321, 767, 611, 362, 281, 458, 577, 281, 16390, 13, 51514, 51514, 407, 558, 586, 11, 257, 3175, 272, 486, 406, 589, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12618793819261634, "compression_ratio": 1.6791666666666667, "no_speech_prob": 8.480159158352762e-05}, {"id": 1108, "seek": 573100, "start": 5754.0, "end": 5757.0, "text": " So right now, a minus b will not work.", "tokens": [50364, 583, 586, 321, 362, 281, 5021, 309, 293, 321, 1319, 309, 2935, 538, 30955, 538, 527, 5893, 2771, 13, 50564, 50564, 663, 311, 5021, 4978, 13, 50614, 50614, 400, 341, 820, 12120, 589, 13, 50814, 50814, 400, 321, 434, 516, 281, 915, 484, 2321, 13, 50914, 50914, 583, 586, 498, 321, 360, 341, 11, 341, 820, 586, 589, 13, 51114, 51114, 400, 321, 483, 935, 1732, 13, 51214, 51214, 407, 264, 2128, 1320, 1985, 11, 457, 775, 264, 23897, 1320, 589, 30, 51364, 51364, 400, 286, 4325, 300, 321, 767, 611, 362, 281, 458, 577, 281, 16390, 13, 51514, 51514, 407, 558, 586, 11, 257, 3175, 272, 486, 406, 589, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12618793819261634, "compression_ratio": 1.6791666666666667, "no_speech_prob": 8.480159158352762e-05}, {"id": 1109, "seek": 575700, "start": 5757.0, "end": 5761.0, "text": " To make it work, we need one more piece of code here.", "tokens": [50364, 1407, 652, 309, 589, 11, 321, 643, 472, 544, 2522, 295, 3089, 510, 13, 50564, 50564, 400, 1936, 11, 341, 307, 264, 16390, 313, 13, 50814, 50814, 400, 264, 636, 321, 434, 516, 281, 4445, 16390, 313, 307, 321, 434, 516, 281, 4445, 309, 538, 4500, 295, 257, 2485, 399, 13, 51064, 51064, 400, 550, 281, 4445, 2485, 399, 11, 321, 434, 516, 281, 12972, 538, 3671, 472, 13, 51214, 51214, 407, 445, 797, 11, 1228, 264, 1507, 321, 600, 1217, 3094, 293, 445, 22171, 309, 294, 2115, 295, 437, 321, 362, 13, 51464, 51464, 400, 257, 3175, 272, 307, 586, 1364, 13, 51564, 51564, 2264, 11, 370, 586, 718, 311, 11369, 797, 281, 341, 6114, 510, 337, 341, 34090, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06255852890014649, "compression_ratio": 1.823076923076923, "no_speech_prob": 1.3211575605964754e-05}, {"id": 1110, "seek": 575700, "start": 5761.0, "end": 5766.0, "text": " And basically, this is the subtraction.", "tokens": [50364, 1407, 652, 309, 589, 11, 321, 643, 472, 544, 2522, 295, 3089, 510, 13, 50564, 50564, 400, 1936, 11, 341, 307, 264, 16390, 313, 13, 50814, 50814, 400, 264, 636, 321, 434, 516, 281, 4445, 16390, 313, 307, 321, 434, 516, 281, 4445, 309, 538, 4500, 295, 257, 2485, 399, 13, 51064, 51064, 400, 550, 281, 4445, 2485, 399, 11, 321, 434, 516, 281, 12972, 538, 3671, 472, 13, 51214, 51214, 407, 445, 797, 11, 1228, 264, 1507, 321, 600, 1217, 3094, 293, 445, 22171, 309, 294, 2115, 295, 437, 321, 362, 13, 51464, 51464, 400, 257, 3175, 272, 307, 586, 1364, 13, 51564, 51564, 2264, 11, 370, 586, 718, 311, 11369, 797, 281, 341, 6114, 510, 337, 341, 34090, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06255852890014649, "compression_ratio": 1.823076923076923, "no_speech_prob": 1.3211575605964754e-05}, {"id": 1111, "seek": 575700, "start": 5766.0, "end": 5771.0, "text": " And the way we're going to implement subtraction is we're going to implement it by addition of a negation.", "tokens": [50364, 1407, 652, 309, 589, 11, 321, 643, 472, 544, 2522, 295, 3089, 510, 13, 50564, 50564, 400, 1936, 11, 341, 307, 264, 16390, 313, 13, 50814, 50814, 400, 264, 636, 321, 434, 516, 281, 4445, 16390, 313, 307, 321, 434, 516, 281, 4445, 309, 538, 4500, 295, 257, 2485, 399, 13, 51064, 51064, 400, 550, 281, 4445, 2485, 399, 11, 321, 434, 516, 281, 12972, 538, 3671, 472, 13, 51214, 51214, 407, 445, 797, 11, 1228, 264, 1507, 321, 600, 1217, 3094, 293, 445, 22171, 309, 294, 2115, 295, 437, 321, 362, 13, 51464, 51464, 400, 257, 3175, 272, 307, 586, 1364, 13, 51564, 51564, 2264, 11, 370, 586, 718, 311, 11369, 797, 281, 341, 6114, 510, 337, 341, 34090, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06255852890014649, "compression_ratio": 1.823076923076923, "no_speech_prob": 1.3211575605964754e-05}, {"id": 1112, "seek": 575700, "start": 5771.0, "end": 5774.0, "text": " And then to implement negation, we're going to multiply by negative one.", "tokens": [50364, 1407, 652, 309, 589, 11, 321, 643, 472, 544, 2522, 295, 3089, 510, 13, 50564, 50564, 400, 1936, 11, 341, 307, 264, 16390, 313, 13, 50814, 50814, 400, 264, 636, 321, 434, 516, 281, 4445, 16390, 313, 307, 321, 434, 516, 281, 4445, 309, 538, 4500, 295, 257, 2485, 399, 13, 51064, 51064, 400, 550, 281, 4445, 2485, 399, 11, 321, 434, 516, 281, 12972, 538, 3671, 472, 13, 51214, 51214, 407, 445, 797, 11, 1228, 264, 1507, 321, 600, 1217, 3094, 293, 445, 22171, 309, 294, 2115, 295, 437, 321, 362, 13, 51464, 51464, 400, 257, 3175, 272, 307, 586, 1364, 13, 51564, 51564, 2264, 11, 370, 586, 718, 311, 11369, 797, 281, 341, 6114, 510, 337, 341, 34090, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06255852890014649, "compression_ratio": 1.823076923076923, "no_speech_prob": 1.3211575605964754e-05}, {"id": 1113, "seek": 575700, "start": 5774.0, "end": 5779.0, "text": " So just again, using the stuff we've already built and just expressing it in terms of what we have.", "tokens": [50364, 1407, 652, 309, 589, 11, 321, 643, 472, 544, 2522, 295, 3089, 510, 13, 50564, 50564, 400, 1936, 11, 341, 307, 264, 16390, 313, 13, 50814, 50814, 400, 264, 636, 321, 434, 516, 281, 4445, 16390, 313, 307, 321, 434, 516, 281, 4445, 309, 538, 4500, 295, 257, 2485, 399, 13, 51064, 51064, 400, 550, 281, 4445, 2485, 399, 11, 321, 434, 516, 281, 12972, 538, 3671, 472, 13, 51214, 51214, 407, 445, 797, 11, 1228, 264, 1507, 321, 600, 1217, 3094, 293, 445, 22171, 309, 294, 2115, 295, 437, 321, 362, 13, 51464, 51464, 400, 257, 3175, 272, 307, 586, 1364, 13, 51564, 51564, 2264, 11, 370, 586, 718, 311, 11369, 797, 281, 341, 6114, 510, 337, 341, 34090, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06255852890014649, "compression_ratio": 1.823076923076923, "no_speech_prob": 1.3211575605964754e-05}, {"id": 1114, "seek": 575700, "start": 5779.0, "end": 5781.0, "text": " And a minus b is now working.", "tokens": [50364, 1407, 652, 309, 589, 11, 321, 643, 472, 544, 2522, 295, 3089, 510, 13, 50564, 50564, 400, 1936, 11, 341, 307, 264, 16390, 313, 13, 50814, 50814, 400, 264, 636, 321, 434, 516, 281, 4445, 16390, 313, 307, 321, 434, 516, 281, 4445, 309, 538, 4500, 295, 257, 2485, 399, 13, 51064, 51064, 400, 550, 281, 4445, 2485, 399, 11, 321, 434, 516, 281, 12972, 538, 3671, 472, 13, 51214, 51214, 407, 445, 797, 11, 1228, 264, 1507, 321, 600, 1217, 3094, 293, 445, 22171, 309, 294, 2115, 295, 437, 321, 362, 13, 51464, 51464, 400, 257, 3175, 272, 307, 586, 1364, 13, 51564, 51564, 2264, 11, 370, 586, 718, 311, 11369, 797, 281, 341, 6114, 510, 337, 341, 34090, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06255852890014649, "compression_ratio": 1.823076923076923, "no_speech_prob": 1.3211575605964754e-05}, {"id": 1115, "seek": 575700, "start": 5781.0, "end": 5785.0, "text": " OK, so now let's scroll again to this expression here for this neuron.", "tokens": [50364, 1407, 652, 309, 589, 11, 321, 643, 472, 544, 2522, 295, 3089, 510, 13, 50564, 50564, 400, 1936, 11, 341, 307, 264, 16390, 313, 13, 50814, 50814, 400, 264, 636, 321, 434, 516, 281, 4445, 16390, 313, 307, 321, 434, 516, 281, 4445, 309, 538, 4500, 295, 257, 2485, 399, 13, 51064, 51064, 400, 550, 281, 4445, 2485, 399, 11, 321, 434, 516, 281, 12972, 538, 3671, 472, 13, 51214, 51214, 407, 445, 797, 11, 1228, 264, 1507, 321, 600, 1217, 3094, 293, 445, 22171, 309, 294, 2115, 295, 437, 321, 362, 13, 51464, 51464, 400, 257, 3175, 272, 307, 586, 1364, 13, 51564, 51564, 2264, 11, 370, 586, 718, 311, 11369, 797, 281, 341, 6114, 510, 337, 341, 34090, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06255852890014649, "compression_ratio": 1.823076923076923, "no_speech_prob": 1.3211575605964754e-05}, {"id": 1116, "seek": 578500, "start": 5785.0, "end": 5790.0, "text": " And let's just compute the backward pass here once we've defined O.", "tokens": [50364, 400, 718, 311, 445, 14722, 264, 23897, 1320, 510, 1564, 321, 600, 7642, 422, 13, 50614, 50614, 400, 718, 311, 2642, 309, 13, 50714, 50714, 407, 510, 311, 264, 2771, 2448, 337, 439, 295, 613, 1477, 13891, 337, 341, 732, 18795, 34090, 300, 575, 257, 1266, 71, 300, 321, 600, 1612, 949, 13, 51014, 51014, 407, 586, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 1821, 493, 341, 1266, 71, 666, 341, 6114, 510, 13, 51314, 51314, 407, 718, 385, 5055, 9163, 341, 510, 13, 51464, 51464, 400, 586, 2602, 295, 321, 603, 15665, 264, 7645, 293, 321, 486, 1319, 577, 321, 6964, 422, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0901024341583252, "compression_ratio": 1.6721311475409837, "no_speech_prob": 1.5688872736063786e-05}, {"id": 1117, "seek": 578500, "start": 5790.0, "end": 5792.0, "text": " And let's draw it.", "tokens": [50364, 400, 718, 311, 445, 14722, 264, 23897, 1320, 510, 1564, 321, 600, 7642, 422, 13, 50614, 50614, 400, 718, 311, 2642, 309, 13, 50714, 50714, 407, 510, 311, 264, 2771, 2448, 337, 439, 295, 613, 1477, 13891, 337, 341, 732, 18795, 34090, 300, 575, 257, 1266, 71, 300, 321, 600, 1612, 949, 13, 51014, 51014, 407, 586, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 1821, 493, 341, 1266, 71, 666, 341, 6114, 510, 13, 51314, 51314, 407, 718, 385, 5055, 9163, 341, 510, 13, 51464, 51464, 400, 586, 2602, 295, 321, 603, 15665, 264, 7645, 293, 321, 486, 1319, 577, 321, 6964, 422, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0901024341583252, "compression_ratio": 1.6721311475409837, "no_speech_prob": 1.5688872736063786e-05}, {"id": 1118, "seek": 578500, "start": 5792.0, "end": 5798.0, "text": " So here's the gradients for all of these lead nodes for this two dimensional neuron that has a 10h that we've seen before.", "tokens": [50364, 400, 718, 311, 445, 14722, 264, 23897, 1320, 510, 1564, 321, 600, 7642, 422, 13, 50614, 50614, 400, 718, 311, 2642, 309, 13, 50714, 50714, 407, 510, 311, 264, 2771, 2448, 337, 439, 295, 613, 1477, 13891, 337, 341, 732, 18795, 34090, 300, 575, 257, 1266, 71, 300, 321, 600, 1612, 949, 13, 51014, 51014, 407, 586, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 1821, 493, 341, 1266, 71, 666, 341, 6114, 510, 13, 51314, 51314, 407, 718, 385, 5055, 9163, 341, 510, 13, 51464, 51464, 400, 586, 2602, 295, 321, 603, 15665, 264, 7645, 293, 321, 486, 1319, 577, 321, 6964, 422, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0901024341583252, "compression_ratio": 1.6721311475409837, "no_speech_prob": 1.5688872736063786e-05}, {"id": 1119, "seek": 578500, "start": 5798.0, "end": 5804.0, "text": " So now what I'd like to do is I'd like to break up this 10h into this expression here.", "tokens": [50364, 400, 718, 311, 445, 14722, 264, 23897, 1320, 510, 1564, 321, 600, 7642, 422, 13, 50614, 50614, 400, 718, 311, 2642, 309, 13, 50714, 50714, 407, 510, 311, 264, 2771, 2448, 337, 439, 295, 613, 1477, 13891, 337, 341, 732, 18795, 34090, 300, 575, 257, 1266, 71, 300, 321, 600, 1612, 949, 13, 51014, 51014, 407, 586, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 1821, 493, 341, 1266, 71, 666, 341, 6114, 510, 13, 51314, 51314, 407, 718, 385, 5055, 9163, 341, 510, 13, 51464, 51464, 400, 586, 2602, 295, 321, 603, 15665, 264, 7645, 293, 321, 486, 1319, 577, 321, 6964, 422, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0901024341583252, "compression_ratio": 1.6721311475409837, "no_speech_prob": 1.5688872736063786e-05}, {"id": 1120, "seek": 578500, "start": 5804.0, "end": 5807.0, "text": " So let me copy paste this here.", "tokens": [50364, 400, 718, 311, 445, 14722, 264, 23897, 1320, 510, 1564, 321, 600, 7642, 422, 13, 50614, 50614, 400, 718, 311, 2642, 309, 13, 50714, 50714, 407, 510, 311, 264, 2771, 2448, 337, 439, 295, 613, 1477, 13891, 337, 341, 732, 18795, 34090, 300, 575, 257, 1266, 71, 300, 321, 600, 1612, 949, 13, 51014, 51014, 407, 586, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 1821, 493, 341, 1266, 71, 666, 341, 6114, 510, 13, 51314, 51314, 407, 718, 385, 5055, 9163, 341, 510, 13, 51464, 51464, 400, 586, 2602, 295, 321, 603, 15665, 264, 7645, 293, 321, 486, 1319, 577, 321, 6964, 422, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0901024341583252, "compression_ratio": 1.6721311475409837, "no_speech_prob": 1.5688872736063786e-05}, {"id": 1121, "seek": 578500, "start": 5807.0, "end": 5813.0, "text": " And now instead of we'll preserve the label and we will change how we define O.", "tokens": [50364, 400, 718, 311, 445, 14722, 264, 23897, 1320, 510, 1564, 321, 600, 7642, 422, 13, 50614, 50614, 400, 718, 311, 2642, 309, 13, 50714, 50714, 407, 510, 311, 264, 2771, 2448, 337, 439, 295, 613, 1477, 13891, 337, 341, 732, 18795, 34090, 300, 575, 257, 1266, 71, 300, 321, 600, 1612, 949, 13, 51014, 51014, 407, 586, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 1821, 493, 341, 1266, 71, 666, 341, 6114, 510, 13, 51314, 51314, 407, 718, 385, 5055, 9163, 341, 510, 13, 51464, 51464, 400, 586, 2602, 295, 321, 603, 15665, 264, 7645, 293, 321, 486, 1319, 577, 321, 6964, 422, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0901024341583252, "compression_ratio": 1.6721311475409837, "no_speech_prob": 1.5688872736063786e-05}, {"id": 1122, "seek": 581300, "start": 5813.0, "end": 5816.0, "text": " So in particular, we're going to implement this formula here.", "tokens": [50364, 407, 294, 1729, 11, 321, 434, 516, 281, 4445, 341, 8513, 510, 13, 50514, 50514, 407, 321, 643, 308, 281, 264, 568, 87, 3175, 472, 670, 308, 281, 264, 2031, 1804, 472, 13, 50714, 50714, 407, 308, 281, 264, 568, 87, 11, 321, 643, 281, 747, 568, 1413, 297, 293, 321, 643, 281, 37871, 13024, 309, 13, 51014, 51014, 663, 311, 308, 281, 264, 568, 87, 13, 51114, 51114, 400, 550, 570, 321, 434, 1228, 309, 6091, 11, 718, 311, 1884, 364, 19376, 7006, 308, 293, 550, 6964, 422, 382, 308, 1804, 472, 670, 308, 3175, 472, 670, 308, 1804, 472, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10075389214281766, "compression_ratio": 1.7772277227722773, "no_speech_prob": 5.3070951253175735e-05}, {"id": 1123, "seek": 581300, "start": 5816.0, "end": 5820.0, "text": " So we need e to the 2x minus one over e to the x plus one.", "tokens": [50364, 407, 294, 1729, 11, 321, 434, 516, 281, 4445, 341, 8513, 510, 13, 50514, 50514, 407, 321, 643, 308, 281, 264, 568, 87, 3175, 472, 670, 308, 281, 264, 2031, 1804, 472, 13, 50714, 50714, 407, 308, 281, 264, 568, 87, 11, 321, 643, 281, 747, 568, 1413, 297, 293, 321, 643, 281, 37871, 13024, 309, 13, 51014, 51014, 663, 311, 308, 281, 264, 568, 87, 13, 51114, 51114, 400, 550, 570, 321, 434, 1228, 309, 6091, 11, 718, 311, 1884, 364, 19376, 7006, 308, 293, 550, 6964, 422, 382, 308, 1804, 472, 670, 308, 3175, 472, 670, 308, 1804, 472, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10075389214281766, "compression_ratio": 1.7772277227722773, "no_speech_prob": 5.3070951253175735e-05}, {"id": 1124, "seek": 581300, "start": 5820.0, "end": 5826.0, "text": " So e to the 2x, we need to take 2 times n and we need to exponentiate it.", "tokens": [50364, 407, 294, 1729, 11, 321, 434, 516, 281, 4445, 341, 8513, 510, 13, 50514, 50514, 407, 321, 643, 308, 281, 264, 568, 87, 3175, 472, 670, 308, 281, 264, 2031, 1804, 472, 13, 50714, 50714, 407, 308, 281, 264, 568, 87, 11, 321, 643, 281, 747, 568, 1413, 297, 293, 321, 643, 281, 37871, 13024, 309, 13, 51014, 51014, 663, 311, 308, 281, 264, 568, 87, 13, 51114, 51114, 400, 550, 570, 321, 434, 1228, 309, 6091, 11, 718, 311, 1884, 364, 19376, 7006, 308, 293, 550, 6964, 422, 382, 308, 1804, 472, 670, 308, 3175, 472, 670, 308, 1804, 472, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10075389214281766, "compression_ratio": 1.7772277227722773, "no_speech_prob": 5.3070951253175735e-05}, {"id": 1125, "seek": 581300, "start": 5826.0, "end": 5828.0, "text": " That's e to the 2x.", "tokens": [50364, 407, 294, 1729, 11, 321, 434, 516, 281, 4445, 341, 8513, 510, 13, 50514, 50514, 407, 321, 643, 308, 281, 264, 568, 87, 3175, 472, 670, 308, 281, 264, 2031, 1804, 472, 13, 50714, 50714, 407, 308, 281, 264, 568, 87, 11, 321, 643, 281, 747, 568, 1413, 297, 293, 321, 643, 281, 37871, 13024, 309, 13, 51014, 51014, 663, 311, 308, 281, 264, 568, 87, 13, 51114, 51114, 400, 550, 570, 321, 434, 1228, 309, 6091, 11, 718, 311, 1884, 364, 19376, 7006, 308, 293, 550, 6964, 422, 382, 308, 1804, 472, 670, 308, 3175, 472, 670, 308, 1804, 472, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10075389214281766, "compression_ratio": 1.7772277227722773, "no_speech_prob": 5.3070951253175735e-05}, {"id": 1126, "seek": 581300, "start": 5828.0, "end": 5838.0, "text": " And then because we're using it twice, let's create an intermediate variable e and then define O as e plus one over e minus one over e plus one.", "tokens": [50364, 407, 294, 1729, 11, 321, 434, 516, 281, 4445, 341, 8513, 510, 13, 50514, 50514, 407, 321, 643, 308, 281, 264, 568, 87, 3175, 472, 670, 308, 281, 264, 2031, 1804, 472, 13, 50714, 50714, 407, 308, 281, 264, 568, 87, 11, 321, 643, 281, 747, 568, 1413, 297, 293, 321, 643, 281, 37871, 13024, 309, 13, 51014, 51014, 663, 311, 308, 281, 264, 568, 87, 13, 51114, 51114, 400, 550, 570, 321, 434, 1228, 309, 6091, 11, 718, 311, 1884, 364, 19376, 7006, 308, 293, 550, 6964, 422, 382, 308, 1804, 472, 670, 308, 3175, 472, 670, 308, 1804, 472, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10075389214281766, "compression_ratio": 1.7772277227722773, "no_speech_prob": 5.3070951253175735e-05}, {"id": 1127, "seek": 583800, "start": 5838.0, "end": 5843.0, "text": " E minus one over e plus one.", "tokens": [50364, 462, 3175, 472, 670, 308, 1804, 472, 13, 50614, 50614, 400, 300, 820, 312, 309, 13, 400, 550, 321, 820, 312, 1075, 281, 2642, 300, 422, 13, 50814, 50814, 407, 586, 949, 286, 1190, 341, 11, 437, 360, 321, 2066, 281, 536, 30, 51014, 51014, 5118, 472, 11, 321, 434, 9650, 281, 536, 257, 709, 2854, 4295, 510, 570, 321, 600, 5463, 493, 1266, 71, 666, 257, 3840, 295, 661, 7705, 13, 51364, 51364, 583, 729, 7705, 366, 44003, 10344, 13, 51464, 51464, 400, 370, 437, 321, 434, 9650, 281, 536, 307, 1230, 472, 11, 264, 912, 1874, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09673828345078689, "compression_ratio": 1.6824034334763949, "no_speech_prob": 7.888895197538659e-06}, {"id": 1128, "seek": 583800, "start": 5843.0, "end": 5847.0, "text": " And that should be it. And then we should be able to draw that O.", "tokens": [50364, 462, 3175, 472, 670, 308, 1804, 472, 13, 50614, 50614, 400, 300, 820, 312, 309, 13, 400, 550, 321, 820, 312, 1075, 281, 2642, 300, 422, 13, 50814, 50814, 407, 586, 949, 286, 1190, 341, 11, 437, 360, 321, 2066, 281, 536, 30, 51014, 51014, 5118, 472, 11, 321, 434, 9650, 281, 536, 257, 709, 2854, 4295, 510, 570, 321, 600, 5463, 493, 1266, 71, 666, 257, 3840, 295, 661, 7705, 13, 51364, 51364, 583, 729, 7705, 366, 44003, 10344, 13, 51464, 51464, 400, 370, 437, 321, 434, 9650, 281, 536, 307, 1230, 472, 11, 264, 912, 1874, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09673828345078689, "compression_ratio": 1.6824034334763949, "no_speech_prob": 7.888895197538659e-06}, {"id": 1129, "seek": 583800, "start": 5847.0, "end": 5851.0, "text": " So now before I run this, what do we expect to see?", "tokens": [50364, 462, 3175, 472, 670, 308, 1804, 472, 13, 50614, 50614, 400, 300, 820, 312, 309, 13, 400, 550, 321, 820, 312, 1075, 281, 2642, 300, 422, 13, 50814, 50814, 407, 586, 949, 286, 1190, 341, 11, 437, 360, 321, 2066, 281, 536, 30, 51014, 51014, 5118, 472, 11, 321, 434, 9650, 281, 536, 257, 709, 2854, 4295, 510, 570, 321, 600, 5463, 493, 1266, 71, 666, 257, 3840, 295, 661, 7705, 13, 51364, 51364, 583, 729, 7705, 366, 44003, 10344, 13, 51464, 51464, 400, 370, 437, 321, 434, 9650, 281, 536, 307, 1230, 472, 11, 264, 912, 1874, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09673828345078689, "compression_ratio": 1.6824034334763949, "no_speech_prob": 7.888895197538659e-06}, {"id": 1130, "seek": 583800, "start": 5851.0, "end": 5858.0, "text": " Number one, we're expecting to see a much longer graph here because we've broken up 10h into a bunch of other operations.", "tokens": [50364, 462, 3175, 472, 670, 308, 1804, 472, 13, 50614, 50614, 400, 300, 820, 312, 309, 13, 400, 550, 321, 820, 312, 1075, 281, 2642, 300, 422, 13, 50814, 50814, 407, 586, 949, 286, 1190, 341, 11, 437, 360, 321, 2066, 281, 536, 30, 51014, 51014, 5118, 472, 11, 321, 434, 9650, 281, 536, 257, 709, 2854, 4295, 510, 570, 321, 600, 5463, 493, 1266, 71, 666, 257, 3840, 295, 661, 7705, 13, 51364, 51364, 583, 729, 7705, 366, 44003, 10344, 13, 51464, 51464, 400, 370, 437, 321, 434, 9650, 281, 536, 307, 1230, 472, 11, 264, 912, 1874, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09673828345078689, "compression_ratio": 1.6824034334763949, "no_speech_prob": 7.888895197538659e-06}, {"id": 1131, "seek": 583800, "start": 5858.0, "end": 5860.0, "text": " But those operations are mathematically equivalent.", "tokens": [50364, 462, 3175, 472, 670, 308, 1804, 472, 13, 50614, 50614, 400, 300, 820, 312, 309, 13, 400, 550, 321, 820, 312, 1075, 281, 2642, 300, 422, 13, 50814, 50814, 407, 586, 949, 286, 1190, 341, 11, 437, 360, 321, 2066, 281, 536, 30, 51014, 51014, 5118, 472, 11, 321, 434, 9650, 281, 536, 257, 709, 2854, 4295, 510, 570, 321, 600, 5463, 493, 1266, 71, 666, 257, 3840, 295, 661, 7705, 13, 51364, 51364, 583, 729, 7705, 366, 44003, 10344, 13, 51464, 51464, 400, 370, 437, 321, 434, 9650, 281, 536, 307, 1230, 472, 11, 264, 912, 1874, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09673828345078689, "compression_ratio": 1.6824034334763949, "no_speech_prob": 7.888895197538659e-06}, {"id": 1132, "seek": 583800, "start": 5860.0, "end": 5865.0, "text": " And so what we're expecting to see is number one, the same result here.", "tokens": [50364, 462, 3175, 472, 670, 308, 1804, 472, 13, 50614, 50614, 400, 300, 820, 312, 309, 13, 400, 550, 321, 820, 312, 1075, 281, 2642, 300, 422, 13, 50814, 50814, 407, 586, 949, 286, 1190, 341, 11, 437, 360, 321, 2066, 281, 536, 30, 51014, 51014, 5118, 472, 11, 321, 434, 9650, 281, 536, 257, 709, 2854, 4295, 510, 570, 321, 600, 5463, 493, 1266, 71, 666, 257, 3840, 295, 661, 7705, 13, 51364, 51364, 583, 729, 7705, 366, 44003, 10344, 13, 51464, 51464, 400, 370, 437, 321, 434, 9650, 281, 536, 307, 1230, 472, 11, 264, 912, 1874, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09673828345078689, "compression_ratio": 1.6824034334763949, "no_speech_prob": 7.888895197538659e-06}, {"id": 1133, "seek": 586500, "start": 5865.0, "end": 5873.0, "text": " So the forward pass works. And number two, because of that mathematical equivalence, we expect to see the same backward pass and the same gradients on these leaf nodes.", "tokens": [50364, 407, 264, 2128, 1320, 1985, 13, 400, 1230, 732, 11, 570, 295, 300, 18894, 9052, 655, 11, 321, 2066, 281, 536, 264, 912, 23897, 1320, 293, 264, 912, 2771, 2448, 322, 613, 10871, 13891, 13, 50764, 50764, 407, 613, 2771, 2448, 820, 312, 14800, 13, 50864, 50864, 407, 718, 311, 1190, 341, 13, 51014, 51014, 407, 1230, 472, 11, 718, 311, 16888, 300, 2602, 295, 257, 2167, 1266, 71, 9984, 11, 321, 362, 586, 2031, 293, 321, 362, 1804, 321, 362, 1413, 3671, 472, 13, 51464, 51464, 639, 307, 264, 10044, 13, 51564, 51564, 400, 321, 917, 493, 365, 264, 912, 2128, 1320, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08439702725191729, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.8572469545906642e-06}, {"id": 1134, "seek": 586500, "start": 5873.0, "end": 5875.0, "text": " So these gradients should be identical.", "tokens": [50364, 407, 264, 2128, 1320, 1985, 13, 400, 1230, 732, 11, 570, 295, 300, 18894, 9052, 655, 11, 321, 2066, 281, 536, 264, 912, 23897, 1320, 293, 264, 912, 2771, 2448, 322, 613, 10871, 13891, 13, 50764, 50764, 407, 613, 2771, 2448, 820, 312, 14800, 13, 50864, 50864, 407, 718, 311, 1190, 341, 13, 51014, 51014, 407, 1230, 472, 11, 718, 311, 16888, 300, 2602, 295, 257, 2167, 1266, 71, 9984, 11, 321, 362, 586, 2031, 293, 321, 362, 1804, 321, 362, 1413, 3671, 472, 13, 51464, 51464, 639, 307, 264, 10044, 13, 51564, 51564, 400, 321, 917, 493, 365, 264, 912, 2128, 1320, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08439702725191729, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.8572469545906642e-06}, {"id": 1135, "seek": 586500, "start": 5875.0, "end": 5878.0, "text": " So let's run this.", "tokens": [50364, 407, 264, 2128, 1320, 1985, 13, 400, 1230, 732, 11, 570, 295, 300, 18894, 9052, 655, 11, 321, 2066, 281, 536, 264, 912, 23897, 1320, 293, 264, 912, 2771, 2448, 322, 613, 10871, 13891, 13, 50764, 50764, 407, 613, 2771, 2448, 820, 312, 14800, 13, 50864, 50864, 407, 718, 311, 1190, 341, 13, 51014, 51014, 407, 1230, 472, 11, 718, 311, 16888, 300, 2602, 295, 257, 2167, 1266, 71, 9984, 11, 321, 362, 586, 2031, 293, 321, 362, 1804, 321, 362, 1413, 3671, 472, 13, 51464, 51464, 639, 307, 264, 10044, 13, 51564, 51564, 400, 321, 917, 493, 365, 264, 912, 2128, 1320, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08439702725191729, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.8572469545906642e-06}, {"id": 1136, "seek": 586500, "start": 5878.0, "end": 5887.0, "text": " So number one, let's verify that instead of a single 10h node, we have now x and we have plus we have times negative one.", "tokens": [50364, 407, 264, 2128, 1320, 1985, 13, 400, 1230, 732, 11, 570, 295, 300, 18894, 9052, 655, 11, 321, 2066, 281, 536, 264, 912, 23897, 1320, 293, 264, 912, 2771, 2448, 322, 613, 10871, 13891, 13, 50764, 50764, 407, 613, 2771, 2448, 820, 312, 14800, 13, 50864, 50864, 407, 718, 311, 1190, 341, 13, 51014, 51014, 407, 1230, 472, 11, 718, 311, 16888, 300, 2602, 295, 257, 2167, 1266, 71, 9984, 11, 321, 362, 586, 2031, 293, 321, 362, 1804, 321, 362, 1413, 3671, 472, 13, 51464, 51464, 639, 307, 264, 10044, 13, 51564, 51564, 400, 321, 917, 493, 365, 264, 912, 2128, 1320, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08439702725191729, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.8572469545906642e-06}, {"id": 1137, "seek": 586500, "start": 5887.0, "end": 5889.0, "text": " This is the division.", "tokens": [50364, 407, 264, 2128, 1320, 1985, 13, 400, 1230, 732, 11, 570, 295, 300, 18894, 9052, 655, 11, 321, 2066, 281, 536, 264, 912, 23897, 1320, 293, 264, 912, 2771, 2448, 322, 613, 10871, 13891, 13, 50764, 50764, 407, 613, 2771, 2448, 820, 312, 14800, 13, 50864, 50864, 407, 718, 311, 1190, 341, 13, 51014, 51014, 407, 1230, 472, 11, 718, 311, 16888, 300, 2602, 295, 257, 2167, 1266, 71, 9984, 11, 321, 362, 586, 2031, 293, 321, 362, 1804, 321, 362, 1413, 3671, 472, 13, 51464, 51464, 639, 307, 264, 10044, 13, 51564, 51564, 400, 321, 917, 493, 365, 264, 912, 2128, 1320, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08439702725191729, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.8572469545906642e-06}, {"id": 1138, "seek": 586500, "start": 5889.0, "end": 5892.0, "text": " And we end up with the same forward pass here.", "tokens": [50364, 407, 264, 2128, 1320, 1985, 13, 400, 1230, 732, 11, 570, 295, 300, 18894, 9052, 655, 11, 321, 2066, 281, 536, 264, 912, 23897, 1320, 293, 264, 912, 2771, 2448, 322, 613, 10871, 13891, 13, 50764, 50764, 407, 613, 2771, 2448, 820, 312, 14800, 13, 50864, 50864, 407, 718, 311, 1190, 341, 13, 51014, 51014, 407, 1230, 472, 11, 718, 311, 16888, 300, 2602, 295, 257, 2167, 1266, 71, 9984, 11, 321, 362, 586, 2031, 293, 321, 362, 1804, 321, 362, 1413, 3671, 472, 13, 51464, 51464, 639, 307, 264, 10044, 13, 51564, 51564, 400, 321, 917, 493, 365, 264, 912, 2128, 1320, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08439702725191729, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.8572469545906642e-06}, {"id": 1139, "seek": 589200, "start": 5892.0, "end": 5896.0, "text": " And then the gradients, we have to be careful because they're in slightly different order potentially.", "tokens": [50364, 400, 550, 264, 2771, 2448, 11, 321, 362, 281, 312, 5026, 570, 436, 434, 294, 4748, 819, 1668, 7263, 13, 50564, 50564, 440, 2771, 2448, 337, 261, 17, 87, 17, 820, 312, 4018, 293, 935, 1732, 13, 261, 17, 293, 2031, 17, 366, 4018, 293, 935, 1732, 13, 50864, 50864, 400, 261, 16, 87, 16, 366, 472, 293, 3671, 472, 935, 1732, 13, 1485, 293, 3671, 472, 935, 1732, 13, 51114, 51114, 407, 300, 1355, 300, 1293, 527, 2128, 11335, 293, 23897, 11335, 645, 3006, 570, 341, 3574, 484, 281, 312, 10344, 281, 1266, 71, 949, 13, 51564, 51564, 400, 370, 264, 1778, 286, 1415, 281, 352, 807, 341, 5380, 307, 1230, 472, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10352501626742089, "compression_ratio": 1.8084291187739463, "no_speech_prob": 1.3211367331678048e-05}, {"id": 1140, "seek": 589200, "start": 5896.0, "end": 5902.0, "text": " The gradients for w2x2 should be zero and point five. w2 and x2 are zero and point five.", "tokens": [50364, 400, 550, 264, 2771, 2448, 11, 321, 362, 281, 312, 5026, 570, 436, 434, 294, 4748, 819, 1668, 7263, 13, 50564, 50564, 440, 2771, 2448, 337, 261, 17, 87, 17, 820, 312, 4018, 293, 935, 1732, 13, 261, 17, 293, 2031, 17, 366, 4018, 293, 935, 1732, 13, 50864, 50864, 400, 261, 16, 87, 16, 366, 472, 293, 3671, 472, 935, 1732, 13, 1485, 293, 3671, 472, 935, 1732, 13, 51114, 51114, 407, 300, 1355, 300, 1293, 527, 2128, 11335, 293, 23897, 11335, 645, 3006, 570, 341, 3574, 484, 281, 312, 10344, 281, 1266, 71, 949, 13, 51564, 51564, 400, 370, 264, 1778, 286, 1415, 281, 352, 807, 341, 5380, 307, 1230, 472, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10352501626742089, "compression_ratio": 1.8084291187739463, "no_speech_prob": 1.3211367331678048e-05}, {"id": 1141, "seek": 589200, "start": 5902.0, "end": 5907.0, "text": " And w1x1 are one and negative one point five. One and negative one point five.", "tokens": [50364, 400, 550, 264, 2771, 2448, 11, 321, 362, 281, 312, 5026, 570, 436, 434, 294, 4748, 819, 1668, 7263, 13, 50564, 50564, 440, 2771, 2448, 337, 261, 17, 87, 17, 820, 312, 4018, 293, 935, 1732, 13, 261, 17, 293, 2031, 17, 366, 4018, 293, 935, 1732, 13, 50864, 50864, 400, 261, 16, 87, 16, 366, 472, 293, 3671, 472, 935, 1732, 13, 1485, 293, 3671, 472, 935, 1732, 13, 51114, 51114, 407, 300, 1355, 300, 1293, 527, 2128, 11335, 293, 23897, 11335, 645, 3006, 570, 341, 3574, 484, 281, 312, 10344, 281, 1266, 71, 949, 13, 51564, 51564, 400, 370, 264, 1778, 286, 1415, 281, 352, 807, 341, 5380, 307, 1230, 472, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10352501626742089, "compression_ratio": 1.8084291187739463, "no_speech_prob": 1.3211367331678048e-05}, {"id": 1142, "seek": 589200, "start": 5907.0, "end": 5916.0, "text": " So that means that both our forward passes and backward passes were correct because this turned out to be equivalent to 10h before.", "tokens": [50364, 400, 550, 264, 2771, 2448, 11, 321, 362, 281, 312, 5026, 570, 436, 434, 294, 4748, 819, 1668, 7263, 13, 50564, 50564, 440, 2771, 2448, 337, 261, 17, 87, 17, 820, 312, 4018, 293, 935, 1732, 13, 261, 17, 293, 2031, 17, 366, 4018, 293, 935, 1732, 13, 50864, 50864, 400, 261, 16, 87, 16, 366, 472, 293, 3671, 472, 935, 1732, 13, 1485, 293, 3671, 472, 935, 1732, 13, 51114, 51114, 407, 300, 1355, 300, 1293, 527, 2128, 11335, 293, 23897, 11335, 645, 3006, 570, 341, 3574, 484, 281, 312, 10344, 281, 1266, 71, 949, 13, 51564, 51564, 400, 370, 264, 1778, 286, 1415, 281, 352, 807, 341, 5380, 307, 1230, 472, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10352501626742089, "compression_ratio": 1.8084291187739463, "no_speech_prob": 1.3211367331678048e-05}, {"id": 1143, "seek": 589200, "start": 5916.0, "end": 5919.0, "text": " And so the reason I wanted to go through this exercise is number one.", "tokens": [50364, 400, 550, 264, 2771, 2448, 11, 321, 362, 281, 312, 5026, 570, 436, 434, 294, 4748, 819, 1668, 7263, 13, 50564, 50564, 440, 2771, 2448, 337, 261, 17, 87, 17, 820, 312, 4018, 293, 935, 1732, 13, 261, 17, 293, 2031, 17, 366, 4018, 293, 935, 1732, 13, 50864, 50864, 400, 261, 16, 87, 16, 366, 472, 293, 3671, 472, 935, 1732, 13, 1485, 293, 3671, 472, 935, 1732, 13, 51114, 51114, 407, 300, 1355, 300, 1293, 527, 2128, 11335, 293, 23897, 11335, 645, 3006, 570, 341, 3574, 484, 281, 312, 10344, 281, 1266, 71, 949, 13, 51564, 51564, 400, 370, 264, 1778, 286, 1415, 281, 352, 807, 341, 5380, 307, 1230, 472, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10352501626742089, "compression_ratio": 1.8084291187739463, "no_speech_prob": 1.3211367331678048e-05}, {"id": 1144, "seek": 591900, "start": 5919.0, "end": 5923.0, "text": " We got to practice a few more operations and writing more backwards passes.", "tokens": [50364, 492, 658, 281, 3124, 257, 1326, 544, 7705, 293, 3579, 544, 12204, 11335, 13, 50564, 50564, 400, 1230, 732, 11, 286, 1415, 281, 23221, 264, 935, 300, 264, 264, 1496, 412, 597, 291, 4445, 428, 7705, 307, 3879, 493, 281, 291, 13, 50964, 50964, 509, 393, 4445, 23897, 11335, 337, 5870, 15277, 411, 257, 2167, 2609, 1804, 420, 257, 2167, 1413, 420, 291, 393, 4445, 300, 337, 11, 584, 11, 1266, 71, 11, 51414, 51414, 597, 307, 257, 733, 295, 257, 7263, 291, 393, 536, 309, 382, 257, 25557, 6916, 570, 309, 311, 1027, 493, 295, 439, 613, 544, 22275, 7705, 13, 51714, 51714, 583, 534, 11, 439, 295, 341, 307, 733, 295, 411, 257, 7592, 3410, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07306469463911212, "compression_ratio": 1.7953020134228188, "no_speech_prob": 3.1380138807435287e-06}, {"id": 1145, "seek": 591900, "start": 5923.0, "end": 5931.0, "text": " And number two, I wanted to illustrate the point that the the level at which you implement your operations is totally up to you.", "tokens": [50364, 492, 658, 281, 3124, 257, 1326, 544, 7705, 293, 3579, 544, 12204, 11335, 13, 50564, 50564, 400, 1230, 732, 11, 286, 1415, 281, 23221, 264, 935, 300, 264, 264, 1496, 412, 597, 291, 4445, 428, 7705, 307, 3879, 493, 281, 291, 13, 50964, 50964, 509, 393, 4445, 23897, 11335, 337, 5870, 15277, 411, 257, 2167, 2609, 1804, 420, 257, 2167, 1413, 420, 291, 393, 4445, 300, 337, 11, 584, 11, 1266, 71, 11, 51414, 51414, 597, 307, 257, 733, 295, 257, 7263, 291, 393, 536, 309, 382, 257, 25557, 6916, 570, 309, 311, 1027, 493, 295, 439, 613, 544, 22275, 7705, 13, 51714, 51714, 583, 534, 11, 439, 295, 341, 307, 733, 295, 411, 257, 7592, 3410, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07306469463911212, "compression_ratio": 1.7953020134228188, "no_speech_prob": 3.1380138807435287e-06}, {"id": 1146, "seek": 591900, "start": 5931.0, "end": 5940.0, "text": " You can implement backward passes for tiny expressions like a single individual plus or a single times or you can implement that for, say, 10h,", "tokens": [50364, 492, 658, 281, 3124, 257, 1326, 544, 7705, 293, 3579, 544, 12204, 11335, 13, 50564, 50564, 400, 1230, 732, 11, 286, 1415, 281, 23221, 264, 935, 300, 264, 264, 1496, 412, 597, 291, 4445, 428, 7705, 307, 3879, 493, 281, 291, 13, 50964, 50964, 509, 393, 4445, 23897, 11335, 337, 5870, 15277, 411, 257, 2167, 2609, 1804, 420, 257, 2167, 1413, 420, 291, 393, 4445, 300, 337, 11, 584, 11, 1266, 71, 11, 51414, 51414, 597, 307, 257, 733, 295, 257, 7263, 291, 393, 536, 309, 382, 257, 25557, 6916, 570, 309, 311, 1027, 493, 295, 439, 613, 544, 22275, 7705, 13, 51714, 51714, 583, 534, 11, 439, 295, 341, 307, 733, 295, 411, 257, 7592, 3410, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07306469463911212, "compression_ratio": 1.7953020134228188, "no_speech_prob": 3.1380138807435287e-06}, {"id": 1147, "seek": 591900, "start": 5940.0, "end": 5946.0, "text": " which is a kind of a potentially you can see it as a composite operation because it's made up of all these more atomic operations.", "tokens": [50364, 492, 658, 281, 3124, 257, 1326, 544, 7705, 293, 3579, 544, 12204, 11335, 13, 50564, 50564, 400, 1230, 732, 11, 286, 1415, 281, 23221, 264, 935, 300, 264, 264, 1496, 412, 597, 291, 4445, 428, 7705, 307, 3879, 493, 281, 291, 13, 50964, 50964, 509, 393, 4445, 23897, 11335, 337, 5870, 15277, 411, 257, 2167, 2609, 1804, 420, 257, 2167, 1413, 420, 291, 393, 4445, 300, 337, 11, 584, 11, 1266, 71, 11, 51414, 51414, 597, 307, 257, 733, 295, 257, 7263, 291, 393, 536, 309, 382, 257, 25557, 6916, 570, 309, 311, 1027, 493, 295, 439, 613, 544, 22275, 7705, 13, 51714, 51714, 583, 534, 11, 439, 295, 341, 307, 733, 295, 411, 257, 7592, 3410, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07306469463911212, "compression_ratio": 1.7953020134228188, "no_speech_prob": 3.1380138807435287e-06}, {"id": 1148, "seek": 591900, "start": 5946.0, "end": 5948.0, "text": " But really, all of this is kind of like a fake concept.", "tokens": [50364, 492, 658, 281, 3124, 257, 1326, 544, 7705, 293, 3579, 544, 12204, 11335, 13, 50564, 50564, 400, 1230, 732, 11, 286, 1415, 281, 23221, 264, 935, 300, 264, 264, 1496, 412, 597, 291, 4445, 428, 7705, 307, 3879, 493, 281, 291, 13, 50964, 50964, 509, 393, 4445, 23897, 11335, 337, 5870, 15277, 411, 257, 2167, 2609, 1804, 420, 257, 2167, 1413, 420, 291, 393, 4445, 300, 337, 11, 584, 11, 1266, 71, 11, 51414, 51414, 597, 307, 257, 733, 295, 257, 7263, 291, 393, 536, 309, 382, 257, 25557, 6916, 570, 309, 311, 1027, 493, 295, 439, 613, 544, 22275, 7705, 13, 51714, 51714, 583, 534, 11, 439, 295, 341, 307, 733, 295, 411, 257, 7592, 3410, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07306469463911212, "compression_ratio": 1.7953020134228188, "no_speech_prob": 3.1380138807435287e-06}, {"id": 1149, "seek": 594800, "start": 5948.0, "end": 5951.0, "text": " All that matters is we have some kind of inputs and some kind of an output.", "tokens": [50364, 1057, 300, 7001, 307, 321, 362, 512, 733, 295, 15743, 293, 512, 733, 295, 364, 5598, 13, 50514, 50514, 400, 341, 5598, 307, 257, 2445, 295, 264, 15743, 294, 512, 636, 13, 50664, 50664, 400, 382, 938, 382, 291, 393, 360, 2128, 1320, 293, 264, 23897, 1320, 295, 300, 707, 6916, 11, 50864, 50864, 309, 1177, 380, 1871, 437, 300, 6916, 307, 293, 577, 25557, 309, 307, 13, 51114, 51114, 759, 291, 393, 2464, 264, 2654, 2771, 2448, 11, 291, 393, 1319, 264, 16235, 293, 291, 393, 2354, 646, 3861, 13, 51314, 51314, 407, 264, 1715, 295, 437, 729, 6828, 366, 307, 2584, 493, 281, 291, 13, 51564, 51564, 407, 586, 286, 576, 411, 281, 855, 291, 577, 291, 393, 360, 264, 1900, 912, 551, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0441493767958421, "compression_ratio": 1.8989169675090252, "no_speech_prob": 2.0579627744155005e-06}, {"id": 1150, "seek": 594800, "start": 5951.0, "end": 5954.0, "text": " And this output is a function of the inputs in some way.", "tokens": [50364, 1057, 300, 7001, 307, 321, 362, 512, 733, 295, 15743, 293, 512, 733, 295, 364, 5598, 13, 50514, 50514, 400, 341, 5598, 307, 257, 2445, 295, 264, 15743, 294, 512, 636, 13, 50664, 50664, 400, 382, 938, 382, 291, 393, 360, 2128, 1320, 293, 264, 23897, 1320, 295, 300, 707, 6916, 11, 50864, 50864, 309, 1177, 380, 1871, 437, 300, 6916, 307, 293, 577, 25557, 309, 307, 13, 51114, 51114, 759, 291, 393, 2464, 264, 2654, 2771, 2448, 11, 291, 393, 1319, 264, 16235, 293, 291, 393, 2354, 646, 3861, 13, 51314, 51314, 407, 264, 1715, 295, 437, 729, 6828, 366, 307, 2584, 493, 281, 291, 13, 51564, 51564, 407, 586, 286, 576, 411, 281, 855, 291, 577, 291, 393, 360, 264, 1900, 912, 551, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0441493767958421, "compression_ratio": 1.8989169675090252, "no_speech_prob": 2.0579627744155005e-06}, {"id": 1151, "seek": 594800, "start": 5954.0, "end": 5958.0, "text": " And as long as you can do forward pass and the backward pass of that little operation,", "tokens": [50364, 1057, 300, 7001, 307, 321, 362, 512, 733, 295, 15743, 293, 512, 733, 295, 364, 5598, 13, 50514, 50514, 400, 341, 5598, 307, 257, 2445, 295, 264, 15743, 294, 512, 636, 13, 50664, 50664, 400, 382, 938, 382, 291, 393, 360, 2128, 1320, 293, 264, 23897, 1320, 295, 300, 707, 6916, 11, 50864, 50864, 309, 1177, 380, 1871, 437, 300, 6916, 307, 293, 577, 25557, 309, 307, 13, 51114, 51114, 759, 291, 393, 2464, 264, 2654, 2771, 2448, 11, 291, 393, 1319, 264, 16235, 293, 291, 393, 2354, 646, 3861, 13, 51314, 51314, 407, 264, 1715, 295, 437, 729, 6828, 366, 307, 2584, 493, 281, 291, 13, 51564, 51564, 407, 586, 286, 576, 411, 281, 855, 291, 577, 291, 393, 360, 264, 1900, 912, 551, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0441493767958421, "compression_ratio": 1.8989169675090252, "no_speech_prob": 2.0579627744155005e-06}, {"id": 1152, "seek": 594800, "start": 5958.0, "end": 5963.0, "text": " it doesn't matter what that operation is and how composite it is.", "tokens": [50364, 1057, 300, 7001, 307, 321, 362, 512, 733, 295, 15743, 293, 512, 733, 295, 364, 5598, 13, 50514, 50514, 400, 341, 5598, 307, 257, 2445, 295, 264, 15743, 294, 512, 636, 13, 50664, 50664, 400, 382, 938, 382, 291, 393, 360, 2128, 1320, 293, 264, 23897, 1320, 295, 300, 707, 6916, 11, 50864, 50864, 309, 1177, 380, 1871, 437, 300, 6916, 307, 293, 577, 25557, 309, 307, 13, 51114, 51114, 759, 291, 393, 2464, 264, 2654, 2771, 2448, 11, 291, 393, 1319, 264, 16235, 293, 291, 393, 2354, 646, 3861, 13, 51314, 51314, 407, 264, 1715, 295, 437, 729, 6828, 366, 307, 2584, 493, 281, 291, 13, 51564, 51564, 407, 586, 286, 576, 411, 281, 855, 291, 577, 291, 393, 360, 264, 1900, 912, 551, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0441493767958421, "compression_ratio": 1.8989169675090252, "no_speech_prob": 2.0579627744155005e-06}, {"id": 1153, "seek": 594800, "start": 5963.0, "end": 5967.0, "text": " If you can write the local gradients, you can change the gradient and you can continue back application.", "tokens": [50364, 1057, 300, 7001, 307, 321, 362, 512, 733, 295, 15743, 293, 512, 733, 295, 364, 5598, 13, 50514, 50514, 400, 341, 5598, 307, 257, 2445, 295, 264, 15743, 294, 512, 636, 13, 50664, 50664, 400, 382, 938, 382, 291, 393, 360, 2128, 1320, 293, 264, 23897, 1320, 295, 300, 707, 6916, 11, 50864, 50864, 309, 1177, 380, 1871, 437, 300, 6916, 307, 293, 577, 25557, 309, 307, 13, 51114, 51114, 759, 291, 393, 2464, 264, 2654, 2771, 2448, 11, 291, 393, 1319, 264, 16235, 293, 291, 393, 2354, 646, 3861, 13, 51314, 51314, 407, 264, 1715, 295, 437, 729, 6828, 366, 307, 2584, 493, 281, 291, 13, 51564, 51564, 407, 586, 286, 576, 411, 281, 855, 291, 577, 291, 393, 360, 264, 1900, 912, 551, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0441493767958421, "compression_ratio": 1.8989169675090252, "no_speech_prob": 2.0579627744155005e-06}, {"id": 1154, "seek": 594800, "start": 5967.0, "end": 5972.0, "text": " So the design of what those functions are is completely up to you.", "tokens": [50364, 1057, 300, 7001, 307, 321, 362, 512, 733, 295, 15743, 293, 512, 733, 295, 364, 5598, 13, 50514, 50514, 400, 341, 5598, 307, 257, 2445, 295, 264, 15743, 294, 512, 636, 13, 50664, 50664, 400, 382, 938, 382, 291, 393, 360, 2128, 1320, 293, 264, 23897, 1320, 295, 300, 707, 6916, 11, 50864, 50864, 309, 1177, 380, 1871, 437, 300, 6916, 307, 293, 577, 25557, 309, 307, 13, 51114, 51114, 759, 291, 393, 2464, 264, 2654, 2771, 2448, 11, 291, 393, 1319, 264, 16235, 293, 291, 393, 2354, 646, 3861, 13, 51314, 51314, 407, 264, 1715, 295, 437, 729, 6828, 366, 307, 2584, 493, 281, 291, 13, 51564, 51564, 407, 586, 286, 576, 411, 281, 855, 291, 577, 291, 393, 360, 264, 1900, 912, 551, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0441493767958421, "compression_ratio": 1.8989169675090252, "no_speech_prob": 2.0579627744155005e-06}, {"id": 1155, "seek": 594800, "start": 5972.0, "end": 5975.0, "text": " So now I would like to show you how you can do the exact same thing,", "tokens": [50364, 1057, 300, 7001, 307, 321, 362, 512, 733, 295, 15743, 293, 512, 733, 295, 364, 5598, 13, 50514, 50514, 400, 341, 5598, 307, 257, 2445, 295, 264, 15743, 294, 512, 636, 13, 50664, 50664, 400, 382, 938, 382, 291, 393, 360, 2128, 1320, 293, 264, 23897, 1320, 295, 300, 707, 6916, 11, 50864, 50864, 309, 1177, 380, 1871, 437, 300, 6916, 307, 293, 577, 25557, 309, 307, 13, 51114, 51114, 759, 291, 393, 2464, 264, 2654, 2771, 2448, 11, 291, 393, 1319, 264, 16235, 293, 291, 393, 2354, 646, 3861, 13, 51314, 51314, 407, 264, 1715, 295, 437, 729, 6828, 366, 307, 2584, 493, 281, 291, 13, 51564, 51564, 407, 586, 286, 576, 411, 281, 855, 291, 577, 291, 393, 360, 264, 1900, 912, 551, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0441493767958421, "compression_ratio": 1.8989169675090252, "no_speech_prob": 2.0579627744155005e-06}, {"id": 1156, "seek": 597500, "start": 5975.0, "end": 5978.0, "text": " but using a modern deep neural network library like, for example,", "tokens": [50364, 457, 1228, 257, 4363, 2452, 18161, 3209, 6405, 411, 11, 337, 1365, 11, 50514, 50514, 9953, 51, 284, 339, 11, 597, 286, 600, 9810, 37140, 4532, 2771, 538, 13, 50764, 50764, 400, 370, 9953, 51, 284, 339, 307, 746, 291, 576, 764, 294, 4265, 13, 50914, 50914, 400, 286, 603, 855, 291, 577, 291, 393, 360, 264, 1900, 912, 551, 11, 457, 294, 9953, 51, 284, 339, 9362, 13, 51114, 51114, 407, 286, 478, 445, 516, 281, 5055, 9163, 309, 294, 293, 1792, 291, 807, 309, 257, 707, 857, 13, 51264, 51264, 639, 307, 437, 309, 1542, 411, 13, 51364, 51364, 407, 321, 434, 516, 281, 974, 9953, 51, 284, 339, 293, 550, 321, 643, 281, 6964, 613, 2158, 6565, 411, 321, 362, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06540710981502089, "compression_ratio": 1.628158844765343, "no_speech_prob": 7.224272735584236e-07}, {"id": 1157, "seek": 597500, "start": 5978.0, "end": 5983.0, "text": " PyTorch, which I've roughly modeled micro grad by.", "tokens": [50364, 457, 1228, 257, 4363, 2452, 18161, 3209, 6405, 411, 11, 337, 1365, 11, 50514, 50514, 9953, 51, 284, 339, 11, 597, 286, 600, 9810, 37140, 4532, 2771, 538, 13, 50764, 50764, 400, 370, 9953, 51, 284, 339, 307, 746, 291, 576, 764, 294, 4265, 13, 50914, 50914, 400, 286, 603, 855, 291, 577, 291, 393, 360, 264, 1900, 912, 551, 11, 457, 294, 9953, 51, 284, 339, 9362, 13, 51114, 51114, 407, 286, 478, 445, 516, 281, 5055, 9163, 309, 294, 293, 1792, 291, 807, 309, 257, 707, 857, 13, 51264, 51264, 639, 307, 437, 309, 1542, 411, 13, 51364, 51364, 407, 321, 434, 516, 281, 974, 9953, 51, 284, 339, 293, 550, 321, 643, 281, 6964, 613, 2158, 6565, 411, 321, 362, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06540710981502089, "compression_ratio": 1.628158844765343, "no_speech_prob": 7.224272735584236e-07}, {"id": 1158, "seek": 597500, "start": 5983.0, "end": 5986.0, "text": " And so PyTorch is something you would use in production.", "tokens": [50364, 457, 1228, 257, 4363, 2452, 18161, 3209, 6405, 411, 11, 337, 1365, 11, 50514, 50514, 9953, 51, 284, 339, 11, 597, 286, 600, 9810, 37140, 4532, 2771, 538, 13, 50764, 50764, 400, 370, 9953, 51, 284, 339, 307, 746, 291, 576, 764, 294, 4265, 13, 50914, 50914, 400, 286, 603, 855, 291, 577, 291, 393, 360, 264, 1900, 912, 551, 11, 457, 294, 9953, 51, 284, 339, 9362, 13, 51114, 51114, 407, 286, 478, 445, 516, 281, 5055, 9163, 309, 294, 293, 1792, 291, 807, 309, 257, 707, 857, 13, 51264, 51264, 639, 307, 437, 309, 1542, 411, 13, 51364, 51364, 407, 321, 434, 516, 281, 974, 9953, 51, 284, 339, 293, 550, 321, 643, 281, 6964, 613, 2158, 6565, 411, 321, 362, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06540710981502089, "compression_ratio": 1.628158844765343, "no_speech_prob": 7.224272735584236e-07}, {"id": 1159, "seek": 597500, "start": 5986.0, "end": 5990.0, "text": " And I'll show you how you can do the exact same thing, but in PyTorch API.", "tokens": [50364, 457, 1228, 257, 4363, 2452, 18161, 3209, 6405, 411, 11, 337, 1365, 11, 50514, 50514, 9953, 51, 284, 339, 11, 597, 286, 600, 9810, 37140, 4532, 2771, 538, 13, 50764, 50764, 400, 370, 9953, 51, 284, 339, 307, 746, 291, 576, 764, 294, 4265, 13, 50914, 50914, 400, 286, 603, 855, 291, 577, 291, 393, 360, 264, 1900, 912, 551, 11, 457, 294, 9953, 51, 284, 339, 9362, 13, 51114, 51114, 407, 286, 478, 445, 516, 281, 5055, 9163, 309, 294, 293, 1792, 291, 807, 309, 257, 707, 857, 13, 51264, 51264, 639, 307, 437, 309, 1542, 411, 13, 51364, 51364, 407, 321, 434, 516, 281, 974, 9953, 51, 284, 339, 293, 550, 321, 643, 281, 6964, 613, 2158, 6565, 411, 321, 362, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06540710981502089, "compression_ratio": 1.628158844765343, "no_speech_prob": 7.224272735584236e-07}, {"id": 1160, "seek": 597500, "start": 5990.0, "end": 5993.0, "text": " So I'm just going to copy paste it in and walk you through it a little bit.", "tokens": [50364, 457, 1228, 257, 4363, 2452, 18161, 3209, 6405, 411, 11, 337, 1365, 11, 50514, 50514, 9953, 51, 284, 339, 11, 597, 286, 600, 9810, 37140, 4532, 2771, 538, 13, 50764, 50764, 400, 370, 9953, 51, 284, 339, 307, 746, 291, 576, 764, 294, 4265, 13, 50914, 50914, 400, 286, 603, 855, 291, 577, 291, 393, 360, 264, 1900, 912, 551, 11, 457, 294, 9953, 51, 284, 339, 9362, 13, 51114, 51114, 407, 286, 478, 445, 516, 281, 5055, 9163, 309, 294, 293, 1792, 291, 807, 309, 257, 707, 857, 13, 51264, 51264, 639, 307, 437, 309, 1542, 411, 13, 51364, 51364, 407, 321, 434, 516, 281, 974, 9953, 51, 284, 339, 293, 550, 321, 643, 281, 6964, 613, 2158, 6565, 411, 321, 362, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06540710981502089, "compression_ratio": 1.628158844765343, "no_speech_prob": 7.224272735584236e-07}, {"id": 1161, "seek": 597500, "start": 5993.0, "end": 5995.0, "text": " This is what it looks like.", "tokens": [50364, 457, 1228, 257, 4363, 2452, 18161, 3209, 6405, 411, 11, 337, 1365, 11, 50514, 50514, 9953, 51, 284, 339, 11, 597, 286, 600, 9810, 37140, 4532, 2771, 538, 13, 50764, 50764, 400, 370, 9953, 51, 284, 339, 307, 746, 291, 576, 764, 294, 4265, 13, 50914, 50914, 400, 286, 603, 855, 291, 577, 291, 393, 360, 264, 1900, 912, 551, 11, 457, 294, 9953, 51, 284, 339, 9362, 13, 51114, 51114, 407, 286, 478, 445, 516, 281, 5055, 9163, 309, 294, 293, 1792, 291, 807, 309, 257, 707, 857, 13, 51264, 51264, 639, 307, 437, 309, 1542, 411, 13, 51364, 51364, 407, 321, 434, 516, 281, 974, 9953, 51, 284, 339, 293, 550, 321, 643, 281, 6964, 613, 2158, 6565, 411, 321, 362, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06540710981502089, "compression_ratio": 1.628158844765343, "no_speech_prob": 7.224272735584236e-07}, {"id": 1162, "seek": 597500, "start": 5995.0, "end": 6002.0, "text": " So we're going to import PyTorch and then we need to define these value objects like we have here.", "tokens": [50364, 457, 1228, 257, 4363, 2452, 18161, 3209, 6405, 411, 11, 337, 1365, 11, 50514, 50514, 9953, 51, 284, 339, 11, 597, 286, 600, 9810, 37140, 4532, 2771, 538, 13, 50764, 50764, 400, 370, 9953, 51, 284, 339, 307, 746, 291, 576, 764, 294, 4265, 13, 50914, 50914, 400, 286, 603, 855, 291, 577, 291, 393, 360, 264, 1900, 912, 551, 11, 457, 294, 9953, 51, 284, 339, 9362, 13, 51114, 51114, 407, 286, 478, 445, 516, 281, 5055, 9163, 309, 294, 293, 1792, 291, 807, 309, 257, 707, 857, 13, 51264, 51264, 639, 307, 437, 309, 1542, 411, 13, 51364, 51364, 407, 321, 434, 516, 281, 974, 9953, 51, 284, 339, 293, 550, 321, 643, 281, 6964, 613, 2158, 6565, 411, 321, 362, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06540710981502089, "compression_ratio": 1.628158844765343, "no_speech_prob": 7.224272735584236e-07}, {"id": 1163, "seek": 600200, "start": 6002.0, "end": 6005.0, "text": " Now micro grad is a scalar valued engine.", "tokens": [50364, 823, 4532, 2771, 307, 257, 39684, 22608, 2848, 13, 50514, 50514, 407, 321, 787, 362, 39684, 4190, 411, 568, 13, 15, 13, 50714, 50714, 583, 294, 9953, 51, 284, 339, 11, 1203, 307, 2361, 926, 10688, 830, 13, 50814, 50814, 400, 411, 286, 2835, 11, 10688, 830, 366, 445, 297, 18795, 41011, 295, 15664, 685, 13, 51064, 51064, 407, 300, 311, 983, 721, 483, 257, 707, 857, 544, 6179, 510, 13, 51214, 51214, 286, 445, 643, 257, 39684, 22608, 40863, 11, 257, 40863, 365, 445, 257, 2167, 4478, 13, 51414, 51414, 583, 538, 7576, 11, 562, 291, 589, 365, 9953, 51, 284, 339, 11, 291, 576, 764, 544, 6179, 10688, 830, 411, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0744208238892636, "compression_ratio": 1.7114624505928853, "no_speech_prob": 8.397892088396475e-06}, {"id": 1164, "seek": 600200, "start": 6005.0, "end": 6009.0, "text": " So we only have scalar values like 2.0.", "tokens": [50364, 823, 4532, 2771, 307, 257, 39684, 22608, 2848, 13, 50514, 50514, 407, 321, 787, 362, 39684, 4190, 411, 568, 13, 15, 13, 50714, 50714, 583, 294, 9953, 51, 284, 339, 11, 1203, 307, 2361, 926, 10688, 830, 13, 50814, 50814, 400, 411, 286, 2835, 11, 10688, 830, 366, 445, 297, 18795, 41011, 295, 15664, 685, 13, 51064, 51064, 407, 300, 311, 983, 721, 483, 257, 707, 857, 544, 6179, 510, 13, 51214, 51214, 286, 445, 643, 257, 39684, 22608, 40863, 11, 257, 40863, 365, 445, 257, 2167, 4478, 13, 51414, 51414, 583, 538, 7576, 11, 562, 291, 589, 365, 9953, 51, 284, 339, 11, 291, 576, 764, 544, 6179, 10688, 830, 411, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0744208238892636, "compression_ratio": 1.7114624505928853, "no_speech_prob": 8.397892088396475e-06}, {"id": 1165, "seek": 600200, "start": 6009.0, "end": 6011.0, "text": " But in PyTorch, everything is based around tensors.", "tokens": [50364, 823, 4532, 2771, 307, 257, 39684, 22608, 2848, 13, 50514, 50514, 407, 321, 787, 362, 39684, 4190, 411, 568, 13, 15, 13, 50714, 50714, 583, 294, 9953, 51, 284, 339, 11, 1203, 307, 2361, 926, 10688, 830, 13, 50814, 50814, 400, 411, 286, 2835, 11, 10688, 830, 366, 445, 297, 18795, 41011, 295, 15664, 685, 13, 51064, 51064, 407, 300, 311, 983, 721, 483, 257, 707, 857, 544, 6179, 510, 13, 51214, 51214, 286, 445, 643, 257, 39684, 22608, 40863, 11, 257, 40863, 365, 445, 257, 2167, 4478, 13, 51414, 51414, 583, 538, 7576, 11, 562, 291, 589, 365, 9953, 51, 284, 339, 11, 291, 576, 764, 544, 6179, 10688, 830, 411, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0744208238892636, "compression_ratio": 1.7114624505928853, "no_speech_prob": 8.397892088396475e-06}, {"id": 1166, "seek": 600200, "start": 6011.0, "end": 6016.0, "text": " And like I mentioned, tensors are just n dimensional arrays of scalars.", "tokens": [50364, 823, 4532, 2771, 307, 257, 39684, 22608, 2848, 13, 50514, 50514, 407, 321, 787, 362, 39684, 4190, 411, 568, 13, 15, 13, 50714, 50714, 583, 294, 9953, 51, 284, 339, 11, 1203, 307, 2361, 926, 10688, 830, 13, 50814, 50814, 400, 411, 286, 2835, 11, 10688, 830, 366, 445, 297, 18795, 41011, 295, 15664, 685, 13, 51064, 51064, 407, 300, 311, 983, 721, 483, 257, 707, 857, 544, 6179, 510, 13, 51214, 51214, 286, 445, 643, 257, 39684, 22608, 40863, 11, 257, 40863, 365, 445, 257, 2167, 4478, 13, 51414, 51414, 583, 538, 7576, 11, 562, 291, 589, 365, 9953, 51, 284, 339, 11, 291, 576, 764, 544, 6179, 10688, 830, 411, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0744208238892636, "compression_ratio": 1.7114624505928853, "no_speech_prob": 8.397892088396475e-06}, {"id": 1167, "seek": 600200, "start": 6016.0, "end": 6019.0, "text": " So that's why things get a little bit more complicated here.", "tokens": [50364, 823, 4532, 2771, 307, 257, 39684, 22608, 2848, 13, 50514, 50514, 407, 321, 787, 362, 39684, 4190, 411, 568, 13, 15, 13, 50714, 50714, 583, 294, 9953, 51, 284, 339, 11, 1203, 307, 2361, 926, 10688, 830, 13, 50814, 50814, 400, 411, 286, 2835, 11, 10688, 830, 366, 445, 297, 18795, 41011, 295, 15664, 685, 13, 51064, 51064, 407, 300, 311, 983, 721, 483, 257, 707, 857, 544, 6179, 510, 13, 51214, 51214, 286, 445, 643, 257, 39684, 22608, 40863, 11, 257, 40863, 365, 445, 257, 2167, 4478, 13, 51414, 51414, 583, 538, 7576, 11, 562, 291, 589, 365, 9953, 51, 284, 339, 11, 291, 576, 764, 544, 6179, 10688, 830, 411, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0744208238892636, "compression_ratio": 1.7114624505928853, "no_speech_prob": 8.397892088396475e-06}, {"id": 1168, "seek": 600200, "start": 6019.0, "end": 6023.0, "text": " I just need a scalar valued tensor, a tensor with just a single element.", "tokens": [50364, 823, 4532, 2771, 307, 257, 39684, 22608, 2848, 13, 50514, 50514, 407, 321, 787, 362, 39684, 4190, 411, 568, 13, 15, 13, 50714, 50714, 583, 294, 9953, 51, 284, 339, 11, 1203, 307, 2361, 926, 10688, 830, 13, 50814, 50814, 400, 411, 286, 2835, 11, 10688, 830, 366, 445, 297, 18795, 41011, 295, 15664, 685, 13, 51064, 51064, 407, 300, 311, 983, 721, 483, 257, 707, 857, 544, 6179, 510, 13, 51214, 51214, 286, 445, 643, 257, 39684, 22608, 40863, 11, 257, 40863, 365, 445, 257, 2167, 4478, 13, 51414, 51414, 583, 538, 7576, 11, 562, 291, 589, 365, 9953, 51, 284, 339, 11, 291, 576, 764, 544, 6179, 10688, 830, 411, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0744208238892636, "compression_ratio": 1.7114624505928853, "no_speech_prob": 8.397892088396475e-06}, {"id": 1169, "seek": 600200, "start": 6023.0, "end": 6030.0, "text": " But by default, when you work with PyTorch, you would use more complicated tensors like this.", "tokens": [50364, 823, 4532, 2771, 307, 257, 39684, 22608, 2848, 13, 50514, 50514, 407, 321, 787, 362, 39684, 4190, 411, 568, 13, 15, 13, 50714, 50714, 583, 294, 9953, 51, 284, 339, 11, 1203, 307, 2361, 926, 10688, 830, 13, 50814, 50814, 400, 411, 286, 2835, 11, 10688, 830, 366, 445, 297, 18795, 41011, 295, 15664, 685, 13, 51064, 51064, 407, 300, 311, 983, 721, 483, 257, 707, 857, 544, 6179, 510, 13, 51214, 51214, 286, 445, 643, 257, 39684, 22608, 40863, 11, 257, 40863, 365, 445, 257, 2167, 4478, 13, 51414, 51414, 583, 538, 7576, 11, 562, 291, 589, 365, 9953, 51, 284, 339, 11, 291, 576, 764, 544, 6179, 10688, 830, 411, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0744208238892636, "compression_ratio": 1.7114624505928853, "no_speech_prob": 8.397892088396475e-06}, {"id": 1170, "seek": 603000, "start": 6030.0, "end": 6036.0, "text": " So if I import PyTorch, then I can create tensors like this.", "tokens": [50364, 407, 498, 286, 974, 9953, 51, 284, 339, 11, 550, 286, 393, 1884, 10688, 830, 411, 341, 13, 50664, 50664, 400, 341, 40863, 11, 337, 1365, 11, 307, 257, 568, 538, 805, 10225, 295, 15664, 685, 294, 257, 2167, 14679, 10290, 13, 51114, 51114, 407, 321, 393, 1520, 1080, 3909, 13, 492, 536, 300, 309, 311, 257, 568, 538, 805, 10225, 293, 370, 322, 13, 51314, 51314, 407, 341, 307, 2673, 437, 291, 576, 589, 365, 294, 264, 3539, 15148, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06519762487972484, "compression_ratio": 1.4848484848484849, "no_speech_prob": 2.7693447464116616e-06}, {"id": 1171, "seek": 603000, "start": 6036.0, "end": 6045.0, "text": " And this tensor, for example, is a 2 by 3 array of scalars in a single compact representation.", "tokens": [50364, 407, 498, 286, 974, 9953, 51, 284, 339, 11, 550, 286, 393, 1884, 10688, 830, 411, 341, 13, 50664, 50664, 400, 341, 40863, 11, 337, 1365, 11, 307, 257, 568, 538, 805, 10225, 295, 15664, 685, 294, 257, 2167, 14679, 10290, 13, 51114, 51114, 407, 321, 393, 1520, 1080, 3909, 13, 492, 536, 300, 309, 311, 257, 568, 538, 805, 10225, 293, 370, 322, 13, 51314, 51314, 407, 341, 307, 2673, 437, 291, 576, 589, 365, 294, 264, 3539, 15148, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06519762487972484, "compression_ratio": 1.4848484848484849, "no_speech_prob": 2.7693447464116616e-06}, {"id": 1172, "seek": 603000, "start": 6045.0, "end": 6049.0, "text": " So we can check its shape. We see that it's a 2 by 3 array and so on.", "tokens": [50364, 407, 498, 286, 974, 9953, 51, 284, 339, 11, 550, 286, 393, 1884, 10688, 830, 411, 341, 13, 50664, 50664, 400, 341, 40863, 11, 337, 1365, 11, 307, 257, 568, 538, 805, 10225, 295, 15664, 685, 294, 257, 2167, 14679, 10290, 13, 51114, 51114, 407, 321, 393, 1520, 1080, 3909, 13, 492, 536, 300, 309, 311, 257, 568, 538, 805, 10225, 293, 370, 322, 13, 51314, 51314, 407, 341, 307, 2673, 437, 291, 576, 589, 365, 294, 264, 3539, 15148, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06519762487972484, "compression_ratio": 1.4848484848484849, "no_speech_prob": 2.7693447464116616e-06}, {"id": 1173, "seek": 603000, "start": 6049.0, "end": 6053.0, "text": " So this is usually what you would work with in the actual libraries.", "tokens": [50364, 407, 498, 286, 974, 9953, 51, 284, 339, 11, 550, 286, 393, 1884, 10688, 830, 411, 341, 13, 50664, 50664, 400, 341, 40863, 11, 337, 1365, 11, 307, 257, 568, 538, 805, 10225, 295, 15664, 685, 294, 257, 2167, 14679, 10290, 13, 51114, 51114, 407, 321, 393, 1520, 1080, 3909, 13, 492, 536, 300, 309, 311, 257, 568, 538, 805, 10225, 293, 370, 322, 13, 51314, 51314, 407, 341, 307, 2673, 437, 291, 576, 589, 365, 294, 264, 3539, 15148, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06519762487972484, "compression_ratio": 1.4848484848484849, "no_speech_prob": 2.7693447464116616e-06}, {"id": 1174, "seek": 605300, "start": 6053.0, "end": 6060.0, "text": " So here I'm creating a tensor that has only a single element 2.0.", "tokens": [50364, 407, 510, 286, 478, 4084, 257, 40863, 300, 575, 787, 257, 2167, 4478, 568, 13, 15, 13, 50714, 50714, 400, 550, 286, 478, 17301, 309, 281, 312, 3834, 570, 15329, 307, 538, 7576, 1228, 3834, 18356, 337, 1080, 12607, 935, 3547, 13, 51114, 51114, 407, 286, 1116, 411, 1203, 281, 312, 14800, 13, 51214, 51214, 3146, 7576, 11, 264, 1412, 2010, 295, 613, 10688, 830, 486, 312, 15706, 8858, 13, 51414, 51414, 407, 309, 311, 787, 1228, 257, 2167, 18356, 15706, 13, 51514, 51514, 407, 286, 478, 17301, 309, 281, 3834, 370, 300, 321, 362, 15706, 12145, 445, 411, 294, 15329, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06121071329656637, "compression_ratio": 1.7203389830508475, "no_speech_prob": 2.977033545903396e-05}, {"id": 1175, "seek": 605300, "start": 6060.0, "end": 6068.0, "text": " And then I'm casting it to be double because Python is by default using double precision for its floating point numbers.", "tokens": [50364, 407, 510, 286, 478, 4084, 257, 40863, 300, 575, 787, 257, 2167, 4478, 568, 13, 15, 13, 50714, 50714, 400, 550, 286, 478, 17301, 309, 281, 312, 3834, 570, 15329, 307, 538, 7576, 1228, 3834, 18356, 337, 1080, 12607, 935, 3547, 13, 51114, 51114, 407, 286, 1116, 411, 1203, 281, 312, 14800, 13, 51214, 51214, 3146, 7576, 11, 264, 1412, 2010, 295, 613, 10688, 830, 486, 312, 15706, 8858, 13, 51414, 51414, 407, 309, 311, 787, 1228, 257, 2167, 18356, 15706, 13, 51514, 51514, 407, 286, 478, 17301, 309, 281, 3834, 370, 300, 321, 362, 15706, 12145, 445, 411, 294, 15329, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06121071329656637, "compression_ratio": 1.7203389830508475, "no_speech_prob": 2.977033545903396e-05}, {"id": 1176, "seek": 605300, "start": 6068.0, "end": 6070.0, "text": " So I'd like everything to be identical.", "tokens": [50364, 407, 510, 286, 478, 4084, 257, 40863, 300, 575, 787, 257, 2167, 4478, 568, 13, 15, 13, 50714, 50714, 400, 550, 286, 478, 17301, 309, 281, 312, 3834, 570, 15329, 307, 538, 7576, 1228, 3834, 18356, 337, 1080, 12607, 935, 3547, 13, 51114, 51114, 407, 286, 1116, 411, 1203, 281, 312, 14800, 13, 51214, 51214, 3146, 7576, 11, 264, 1412, 2010, 295, 613, 10688, 830, 486, 312, 15706, 8858, 13, 51414, 51414, 407, 309, 311, 787, 1228, 257, 2167, 18356, 15706, 13, 51514, 51514, 407, 286, 478, 17301, 309, 281, 3834, 370, 300, 321, 362, 15706, 12145, 445, 411, 294, 15329, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06121071329656637, "compression_ratio": 1.7203389830508475, "no_speech_prob": 2.977033545903396e-05}, {"id": 1177, "seek": 605300, "start": 6070.0, "end": 6074.0, "text": " By default, the data type of these tensors will be float 32.", "tokens": [50364, 407, 510, 286, 478, 4084, 257, 40863, 300, 575, 787, 257, 2167, 4478, 568, 13, 15, 13, 50714, 50714, 400, 550, 286, 478, 17301, 309, 281, 312, 3834, 570, 15329, 307, 538, 7576, 1228, 3834, 18356, 337, 1080, 12607, 935, 3547, 13, 51114, 51114, 407, 286, 1116, 411, 1203, 281, 312, 14800, 13, 51214, 51214, 3146, 7576, 11, 264, 1412, 2010, 295, 613, 10688, 830, 486, 312, 15706, 8858, 13, 51414, 51414, 407, 309, 311, 787, 1228, 257, 2167, 18356, 15706, 13, 51514, 51514, 407, 286, 478, 17301, 309, 281, 3834, 370, 300, 321, 362, 15706, 12145, 445, 411, 294, 15329, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06121071329656637, "compression_ratio": 1.7203389830508475, "no_speech_prob": 2.977033545903396e-05}, {"id": 1178, "seek": 605300, "start": 6074.0, "end": 6076.0, "text": " So it's only using a single precision float.", "tokens": [50364, 407, 510, 286, 478, 4084, 257, 40863, 300, 575, 787, 257, 2167, 4478, 568, 13, 15, 13, 50714, 50714, 400, 550, 286, 478, 17301, 309, 281, 312, 3834, 570, 15329, 307, 538, 7576, 1228, 3834, 18356, 337, 1080, 12607, 935, 3547, 13, 51114, 51114, 407, 286, 1116, 411, 1203, 281, 312, 14800, 13, 51214, 51214, 3146, 7576, 11, 264, 1412, 2010, 295, 613, 10688, 830, 486, 312, 15706, 8858, 13, 51414, 51414, 407, 309, 311, 787, 1228, 257, 2167, 18356, 15706, 13, 51514, 51514, 407, 286, 478, 17301, 309, 281, 3834, 370, 300, 321, 362, 15706, 12145, 445, 411, 294, 15329, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06121071329656637, "compression_ratio": 1.7203389830508475, "no_speech_prob": 2.977033545903396e-05}, {"id": 1179, "seek": 605300, "start": 6076.0, "end": 6082.0, "text": " So I'm casting it to double so that we have float 64 just like in Python.", "tokens": [50364, 407, 510, 286, 478, 4084, 257, 40863, 300, 575, 787, 257, 2167, 4478, 568, 13, 15, 13, 50714, 50714, 400, 550, 286, 478, 17301, 309, 281, 312, 3834, 570, 15329, 307, 538, 7576, 1228, 3834, 18356, 337, 1080, 12607, 935, 3547, 13, 51114, 51114, 407, 286, 1116, 411, 1203, 281, 312, 14800, 13, 51214, 51214, 3146, 7576, 11, 264, 1412, 2010, 295, 613, 10688, 830, 486, 312, 15706, 8858, 13, 51414, 51414, 407, 309, 311, 787, 1228, 257, 2167, 18356, 15706, 13, 51514, 51514, 407, 286, 478, 17301, 309, 281, 3834, 370, 300, 321, 362, 15706, 12145, 445, 411, 294, 15329, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06121071329656637, "compression_ratio": 1.7203389830508475, "no_speech_prob": 2.977033545903396e-05}, {"id": 1180, "seek": 608200, "start": 6082.0, "end": 6088.0, "text": " So I'm casting to double and then we get something similar to value of 2.", "tokens": [50364, 407, 286, 478, 17301, 281, 3834, 293, 550, 321, 483, 746, 2531, 281, 2158, 295, 568, 13, 50664, 50664, 440, 958, 551, 286, 362, 281, 360, 307, 570, 613, 366, 10871, 13891, 11, 538, 7576, 11, 9953, 51, 284, 339, 37808, 300, 436, 360, 406, 3651, 2771, 2448, 13, 50964, 50964, 407, 286, 643, 281, 20803, 584, 300, 439, 295, 613, 13891, 3651, 2771, 2448, 13, 51164, 51164, 2264, 11, 370, 341, 307, 516, 281, 7690, 39684, 22608, 472, 4478, 10688, 830, 13, 51414, 51414, 4387, 988, 300, 9953, 51, 284, 339, 3255, 300, 436, 3651, 2771, 2448, 13, 51564, 51564, 823, 11, 538, 7576, 11, 613, 366, 992, 281, 7908, 11, 538, 264, 636, 11, 570, 295, 10493, 4112, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06723542785644532, "compression_ratio": 1.7777777777777777, "no_speech_prob": 7.411103524646023e-06}, {"id": 1181, "seek": 608200, "start": 6088.0, "end": 6094.0, "text": " The next thing I have to do is because these are leaf nodes, by default, PyTorch assumes that they do not require gradients.", "tokens": [50364, 407, 286, 478, 17301, 281, 3834, 293, 550, 321, 483, 746, 2531, 281, 2158, 295, 568, 13, 50664, 50664, 440, 958, 551, 286, 362, 281, 360, 307, 570, 613, 366, 10871, 13891, 11, 538, 7576, 11, 9953, 51, 284, 339, 37808, 300, 436, 360, 406, 3651, 2771, 2448, 13, 50964, 50964, 407, 286, 643, 281, 20803, 584, 300, 439, 295, 613, 13891, 3651, 2771, 2448, 13, 51164, 51164, 2264, 11, 370, 341, 307, 516, 281, 7690, 39684, 22608, 472, 4478, 10688, 830, 13, 51414, 51414, 4387, 988, 300, 9953, 51, 284, 339, 3255, 300, 436, 3651, 2771, 2448, 13, 51564, 51564, 823, 11, 538, 7576, 11, 613, 366, 992, 281, 7908, 11, 538, 264, 636, 11, 570, 295, 10493, 4112, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06723542785644532, "compression_ratio": 1.7777777777777777, "no_speech_prob": 7.411103524646023e-06}, {"id": 1182, "seek": 608200, "start": 6094.0, "end": 6098.0, "text": " So I need to explicitly say that all of these nodes require gradients.", "tokens": [50364, 407, 286, 478, 17301, 281, 3834, 293, 550, 321, 483, 746, 2531, 281, 2158, 295, 568, 13, 50664, 50664, 440, 958, 551, 286, 362, 281, 360, 307, 570, 613, 366, 10871, 13891, 11, 538, 7576, 11, 9953, 51, 284, 339, 37808, 300, 436, 360, 406, 3651, 2771, 2448, 13, 50964, 50964, 407, 286, 643, 281, 20803, 584, 300, 439, 295, 613, 13891, 3651, 2771, 2448, 13, 51164, 51164, 2264, 11, 370, 341, 307, 516, 281, 7690, 39684, 22608, 472, 4478, 10688, 830, 13, 51414, 51414, 4387, 988, 300, 9953, 51, 284, 339, 3255, 300, 436, 3651, 2771, 2448, 13, 51564, 51564, 823, 11, 538, 7576, 11, 613, 366, 992, 281, 7908, 11, 538, 264, 636, 11, 570, 295, 10493, 4112, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06723542785644532, "compression_ratio": 1.7777777777777777, "no_speech_prob": 7.411103524646023e-06}, {"id": 1183, "seek": 608200, "start": 6098.0, "end": 6103.0, "text": " OK, so this is going to construct scalar valued one element tensors.", "tokens": [50364, 407, 286, 478, 17301, 281, 3834, 293, 550, 321, 483, 746, 2531, 281, 2158, 295, 568, 13, 50664, 50664, 440, 958, 551, 286, 362, 281, 360, 307, 570, 613, 366, 10871, 13891, 11, 538, 7576, 11, 9953, 51, 284, 339, 37808, 300, 436, 360, 406, 3651, 2771, 2448, 13, 50964, 50964, 407, 286, 643, 281, 20803, 584, 300, 439, 295, 613, 13891, 3651, 2771, 2448, 13, 51164, 51164, 2264, 11, 370, 341, 307, 516, 281, 7690, 39684, 22608, 472, 4478, 10688, 830, 13, 51414, 51414, 4387, 988, 300, 9953, 51, 284, 339, 3255, 300, 436, 3651, 2771, 2448, 13, 51564, 51564, 823, 11, 538, 7576, 11, 613, 366, 992, 281, 7908, 11, 538, 264, 636, 11, 570, 295, 10493, 4112, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06723542785644532, "compression_ratio": 1.7777777777777777, "no_speech_prob": 7.411103524646023e-06}, {"id": 1184, "seek": 608200, "start": 6103.0, "end": 6106.0, "text": " Make sure that PyTorch knows that they require gradients.", "tokens": [50364, 407, 286, 478, 17301, 281, 3834, 293, 550, 321, 483, 746, 2531, 281, 2158, 295, 568, 13, 50664, 50664, 440, 958, 551, 286, 362, 281, 360, 307, 570, 613, 366, 10871, 13891, 11, 538, 7576, 11, 9953, 51, 284, 339, 37808, 300, 436, 360, 406, 3651, 2771, 2448, 13, 50964, 50964, 407, 286, 643, 281, 20803, 584, 300, 439, 295, 613, 13891, 3651, 2771, 2448, 13, 51164, 51164, 2264, 11, 370, 341, 307, 516, 281, 7690, 39684, 22608, 472, 4478, 10688, 830, 13, 51414, 51414, 4387, 988, 300, 9953, 51, 284, 339, 3255, 300, 436, 3651, 2771, 2448, 13, 51564, 51564, 823, 11, 538, 7576, 11, 613, 366, 992, 281, 7908, 11, 538, 264, 636, 11, 570, 295, 10493, 4112, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06723542785644532, "compression_ratio": 1.7777777777777777, "no_speech_prob": 7.411103524646023e-06}, {"id": 1185, "seek": 608200, "start": 6106.0, "end": 6110.0, "text": " Now, by default, these are set to false, by the way, because of efficiency reasons,", "tokens": [50364, 407, 286, 478, 17301, 281, 3834, 293, 550, 321, 483, 746, 2531, 281, 2158, 295, 568, 13, 50664, 50664, 440, 958, 551, 286, 362, 281, 360, 307, 570, 613, 366, 10871, 13891, 11, 538, 7576, 11, 9953, 51, 284, 339, 37808, 300, 436, 360, 406, 3651, 2771, 2448, 13, 50964, 50964, 407, 286, 643, 281, 20803, 584, 300, 439, 295, 613, 13891, 3651, 2771, 2448, 13, 51164, 51164, 2264, 11, 370, 341, 307, 516, 281, 7690, 39684, 22608, 472, 4478, 10688, 830, 13, 51414, 51414, 4387, 988, 300, 9953, 51, 284, 339, 3255, 300, 436, 3651, 2771, 2448, 13, 51564, 51564, 823, 11, 538, 7576, 11, 613, 366, 992, 281, 7908, 11, 538, 264, 636, 11, 570, 295, 10493, 4112, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06723542785644532, "compression_ratio": 1.7777777777777777, "no_speech_prob": 7.411103524646023e-06}, {"id": 1186, "seek": 611000, "start": 6110.0, "end": 6115.0, "text": " because usually you would not want gradients for leaf nodes like the inputs to the network.", "tokens": [50364, 570, 2673, 291, 576, 406, 528, 2771, 2448, 337, 10871, 13891, 411, 264, 15743, 281, 264, 3209, 13, 50614, 50614, 400, 341, 307, 445, 1382, 281, 312, 7148, 294, 264, 881, 2689, 3331, 13, 50814, 50814, 407, 1564, 321, 600, 7642, 439, 295, 527, 4190, 294, 9953, 51, 284, 339, 2117, 11, 321, 393, 2042, 42973, 445, 411, 321, 393, 510, 294, 4532, 2771, 2117, 13, 51164, 51164, 407, 341, 576, 445, 589, 13, 400, 550, 456, 311, 257, 27822, 5893, 1266, 71, 611, 13, 51364, 51364, 400, 562, 321, 483, 646, 382, 257, 40863, 797, 11, 293, 321, 393, 445, 411, 294, 4532, 2771, 11, 309, 311, 658, 257, 1412, 19667, 293, 309, 311, 658, 2771, 17212, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09630339707785505, "compression_ratio": 1.7381818181818183, "no_speech_prob": 3.089393885602476e-06}, {"id": 1187, "seek": 611000, "start": 6115.0, "end": 6119.0, "text": " And this is just trying to be efficient in the most common cases.", "tokens": [50364, 570, 2673, 291, 576, 406, 528, 2771, 2448, 337, 10871, 13891, 411, 264, 15743, 281, 264, 3209, 13, 50614, 50614, 400, 341, 307, 445, 1382, 281, 312, 7148, 294, 264, 881, 2689, 3331, 13, 50814, 50814, 407, 1564, 321, 600, 7642, 439, 295, 527, 4190, 294, 9953, 51, 284, 339, 2117, 11, 321, 393, 2042, 42973, 445, 411, 321, 393, 510, 294, 4532, 2771, 2117, 13, 51164, 51164, 407, 341, 576, 445, 589, 13, 400, 550, 456, 311, 257, 27822, 5893, 1266, 71, 611, 13, 51364, 51364, 400, 562, 321, 483, 646, 382, 257, 40863, 797, 11, 293, 321, 393, 445, 411, 294, 4532, 2771, 11, 309, 311, 658, 257, 1412, 19667, 293, 309, 311, 658, 2771, 17212, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09630339707785505, "compression_ratio": 1.7381818181818183, "no_speech_prob": 3.089393885602476e-06}, {"id": 1188, "seek": 611000, "start": 6119.0, "end": 6126.0, "text": " So once we've defined all of our values in PyTorch land, we can perform arithmetic just like we can here in micro grad land.", "tokens": [50364, 570, 2673, 291, 576, 406, 528, 2771, 2448, 337, 10871, 13891, 411, 264, 15743, 281, 264, 3209, 13, 50614, 50614, 400, 341, 307, 445, 1382, 281, 312, 7148, 294, 264, 881, 2689, 3331, 13, 50814, 50814, 407, 1564, 321, 600, 7642, 439, 295, 527, 4190, 294, 9953, 51, 284, 339, 2117, 11, 321, 393, 2042, 42973, 445, 411, 321, 393, 510, 294, 4532, 2771, 2117, 13, 51164, 51164, 407, 341, 576, 445, 589, 13, 400, 550, 456, 311, 257, 27822, 5893, 1266, 71, 611, 13, 51364, 51364, 400, 562, 321, 483, 646, 382, 257, 40863, 797, 11, 293, 321, 393, 445, 411, 294, 4532, 2771, 11, 309, 311, 658, 257, 1412, 19667, 293, 309, 311, 658, 2771, 17212, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09630339707785505, "compression_ratio": 1.7381818181818183, "no_speech_prob": 3.089393885602476e-06}, {"id": 1189, "seek": 611000, "start": 6126.0, "end": 6130.0, "text": " So this would just work. And then there's a torch dot 10h also.", "tokens": [50364, 570, 2673, 291, 576, 406, 528, 2771, 2448, 337, 10871, 13891, 411, 264, 15743, 281, 264, 3209, 13, 50614, 50614, 400, 341, 307, 445, 1382, 281, 312, 7148, 294, 264, 881, 2689, 3331, 13, 50814, 50814, 407, 1564, 321, 600, 7642, 439, 295, 527, 4190, 294, 9953, 51, 284, 339, 2117, 11, 321, 393, 2042, 42973, 445, 411, 321, 393, 510, 294, 4532, 2771, 2117, 13, 51164, 51164, 407, 341, 576, 445, 589, 13, 400, 550, 456, 311, 257, 27822, 5893, 1266, 71, 611, 13, 51364, 51364, 400, 562, 321, 483, 646, 382, 257, 40863, 797, 11, 293, 321, 393, 445, 411, 294, 4532, 2771, 11, 309, 311, 658, 257, 1412, 19667, 293, 309, 311, 658, 2771, 17212, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09630339707785505, "compression_ratio": 1.7381818181818183, "no_speech_prob": 3.089393885602476e-06}, {"id": 1190, "seek": 611000, "start": 6130.0, "end": 6138.0, "text": " And when we get back as a tensor again, and we can just like in micro grad, it's got a data attribute and it's got grad attributes.", "tokens": [50364, 570, 2673, 291, 576, 406, 528, 2771, 2448, 337, 10871, 13891, 411, 264, 15743, 281, 264, 3209, 13, 50614, 50614, 400, 341, 307, 445, 1382, 281, 312, 7148, 294, 264, 881, 2689, 3331, 13, 50814, 50814, 407, 1564, 321, 600, 7642, 439, 295, 527, 4190, 294, 9953, 51, 284, 339, 2117, 11, 321, 393, 2042, 42973, 445, 411, 321, 393, 510, 294, 4532, 2771, 2117, 13, 51164, 51164, 407, 341, 576, 445, 589, 13, 400, 550, 456, 311, 257, 27822, 5893, 1266, 71, 611, 13, 51364, 51364, 400, 562, 321, 483, 646, 382, 257, 40863, 797, 11, 293, 321, 393, 445, 411, 294, 4532, 2771, 11, 309, 311, 658, 257, 1412, 19667, 293, 309, 311, 658, 2771, 17212, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09630339707785505, "compression_ratio": 1.7381818181818183, "no_speech_prob": 3.089393885602476e-06}, {"id": 1191, "seek": 613800, "start": 6138.0, "end": 6143.0, "text": " So these tensor objects, just like in micro grad, have a dot data and a dot grad.", "tokens": [50364, 407, 613, 40863, 6565, 11, 445, 411, 294, 4532, 2771, 11, 362, 257, 5893, 1412, 293, 257, 5893, 2771, 13, 50614, 50614, 400, 264, 787, 2649, 510, 307, 300, 321, 643, 281, 818, 309, 5893, 3174, 570, 5911, 9953, 51, 284, 339, 5893, 3174, 1936, 2516, 257, 2167, 40863, 295, 472, 4478, 51164, 51164, 293, 309, 445, 11247, 300, 4478, 3575, 3759, 484, 264, 40863, 13, 51364, 51364, 407, 718, 385, 445, 1190, 341, 13, 400, 4696, 321, 366, 516, 281, 483, 341, 307, 516, 281, 4482, 264, 2128, 1320, 11, 597, 307, 935, 3407, 11, 3407, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10913329030953202, "compression_ratio": 1.6979591836734693, "no_speech_prob": 6.339033006952377e-06}, {"id": 1192, "seek": 613800, "start": 6143.0, "end": 6154.0, "text": " And the only difference here is that we need to call it dot item because otherwise PyTorch dot item basically takes a single tensor of one element", "tokens": [50364, 407, 613, 40863, 6565, 11, 445, 411, 294, 4532, 2771, 11, 362, 257, 5893, 1412, 293, 257, 5893, 2771, 13, 50614, 50614, 400, 264, 787, 2649, 510, 307, 300, 321, 643, 281, 818, 309, 5893, 3174, 570, 5911, 9953, 51, 284, 339, 5893, 3174, 1936, 2516, 257, 2167, 40863, 295, 472, 4478, 51164, 51164, 293, 309, 445, 11247, 300, 4478, 3575, 3759, 484, 264, 40863, 13, 51364, 51364, 407, 718, 385, 445, 1190, 341, 13, 400, 4696, 321, 366, 516, 281, 483, 341, 307, 516, 281, 4482, 264, 2128, 1320, 11, 597, 307, 935, 3407, 11, 3407, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10913329030953202, "compression_ratio": 1.6979591836734693, "no_speech_prob": 6.339033006952377e-06}, {"id": 1193, "seek": 613800, "start": 6154.0, "end": 6158.0, "text": " and it just returns that element stripping out the tensor.", "tokens": [50364, 407, 613, 40863, 6565, 11, 445, 411, 294, 4532, 2771, 11, 362, 257, 5893, 1412, 293, 257, 5893, 2771, 13, 50614, 50614, 400, 264, 787, 2649, 510, 307, 300, 321, 643, 281, 818, 309, 5893, 3174, 570, 5911, 9953, 51, 284, 339, 5893, 3174, 1936, 2516, 257, 2167, 40863, 295, 472, 4478, 51164, 51164, 293, 309, 445, 11247, 300, 4478, 3575, 3759, 484, 264, 40863, 13, 51364, 51364, 407, 718, 385, 445, 1190, 341, 13, 400, 4696, 321, 366, 516, 281, 483, 341, 307, 516, 281, 4482, 264, 2128, 1320, 11, 597, 307, 935, 3407, 11, 3407, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10913329030953202, "compression_ratio": 1.6979591836734693, "no_speech_prob": 6.339033006952377e-06}, {"id": 1194, "seek": 613800, "start": 6158.0, "end": 6165.0, "text": " So let me just run this. And hopefully we are going to get this is going to print the forward pass, which is point seven, seven.", "tokens": [50364, 407, 613, 40863, 6565, 11, 445, 411, 294, 4532, 2771, 11, 362, 257, 5893, 1412, 293, 257, 5893, 2771, 13, 50614, 50614, 400, 264, 787, 2649, 510, 307, 300, 321, 643, 281, 818, 309, 5893, 3174, 570, 5911, 9953, 51, 284, 339, 5893, 3174, 1936, 2516, 257, 2167, 40863, 295, 472, 4478, 51164, 51164, 293, 309, 445, 11247, 300, 4478, 3575, 3759, 484, 264, 40863, 13, 51364, 51364, 407, 718, 385, 445, 1190, 341, 13, 400, 4696, 321, 366, 516, 281, 483, 341, 307, 516, 281, 4482, 264, 2128, 1320, 11, 597, 307, 935, 3407, 11, 3407, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10913329030953202, "compression_ratio": 1.6979591836734693, "no_speech_prob": 6.339033006952377e-06}, {"id": 1195, "seek": 616500, "start": 6165.0, "end": 6171.0, "text": " And this will be the gradients, which hopefully are point five, zero, negative one point five and one.", "tokens": [50364, 400, 341, 486, 312, 264, 2771, 2448, 11, 597, 4696, 366, 935, 1732, 11, 4018, 11, 3671, 472, 935, 1732, 293, 472, 13, 50664, 50664, 407, 498, 321, 445, 1190, 341, 11, 456, 321, 352, 13, 12387, 3407, 13, 407, 264, 2128, 1320, 26383, 13, 50964, 50964, 400, 550, 935, 1732, 11, 4018, 11, 3671, 472, 935, 1732, 293, 472, 13, 407, 9953, 51, 284, 339, 26383, 365, 505, 13, 51214, 51214, 400, 445, 281, 855, 291, 510, 11, 1936, 11, 1954, 11, 510, 311, 257, 40863, 365, 257, 2167, 4478, 293, 309, 311, 257, 3834, 13, 51564, 51564, 400, 321, 393, 818, 300, 3174, 322, 309, 281, 445, 483, 264, 2167, 1230, 484, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.108189654951336, "compression_ratio": 1.8270042194092826, "no_speech_prob": 7.141480455175042e-05}, {"id": 1196, "seek": 616500, "start": 6171.0, "end": 6177.0, "text": " So if we just run this, there we go. Point seven. So the forward pass agrees.", "tokens": [50364, 400, 341, 486, 312, 264, 2771, 2448, 11, 597, 4696, 366, 935, 1732, 11, 4018, 11, 3671, 472, 935, 1732, 293, 472, 13, 50664, 50664, 407, 498, 321, 445, 1190, 341, 11, 456, 321, 352, 13, 12387, 3407, 13, 407, 264, 2128, 1320, 26383, 13, 50964, 50964, 400, 550, 935, 1732, 11, 4018, 11, 3671, 472, 935, 1732, 293, 472, 13, 407, 9953, 51, 284, 339, 26383, 365, 505, 13, 51214, 51214, 400, 445, 281, 855, 291, 510, 11, 1936, 11, 1954, 11, 510, 311, 257, 40863, 365, 257, 2167, 4478, 293, 309, 311, 257, 3834, 13, 51564, 51564, 400, 321, 393, 818, 300, 3174, 322, 309, 281, 445, 483, 264, 2167, 1230, 484, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.108189654951336, "compression_ratio": 1.8270042194092826, "no_speech_prob": 7.141480455175042e-05}, {"id": 1197, "seek": 616500, "start": 6177.0, "end": 6182.0, "text": " And then point five, zero, negative one point five and one. So PyTorch agrees with us.", "tokens": [50364, 400, 341, 486, 312, 264, 2771, 2448, 11, 597, 4696, 366, 935, 1732, 11, 4018, 11, 3671, 472, 935, 1732, 293, 472, 13, 50664, 50664, 407, 498, 321, 445, 1190, 341, 11, 456, 321, 352, 13, 12387, 3407, 13, 407, 264, 2128, 1320, 26383, 13, 50964, 50964, 400, 550, 935, 1732, 11, 4018, 11, 3671, 472, 935, 1732, 293, 472, 13, 407, 9953, 51, 284, 339, 26383, 365, 505, 13, 51214, 51214, 400, 445, 281, 855, 291, 510, 11, 1936, 11, 1954, 11, 510, 311, 257, 40863, 365, 257, 2167, 4478, 293, 309, 311, 257, 3834, 13, 51564, 51564, 400, 321, 393, 818, 300, 3174, 322, 309, 281, 445, 483, 264, 2167, 1230, 484, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.108189654951336, "compression_ratio": 1.8270042194092826, "no_speech_prob": 7.141480455175042e-05}, {"id": 1198, "seek": 616500, "start": 6182.0, "end": 6189.0, "text": " And just to show you here, basically, oh, here's a tensor with a single element and it's a double.", "tokens": [50364, 400, 341, 486, 312, 264, 2771, 2448, 11, 597, 4696, 366, 935, 1732, 11, 4018, 11, 3671, 472, 935, 1732, 293, 472, 13, 50664, 50664, 407, 498, 321, 445, 1190, 341, 11, 456, 321, 352, 13, 12387, 3407, 13, 407, 264, 2128, 1320, 26383, 13, 50964, 50964, 400, 550, 935, 1732, 11, 4018, 11, 3671, 472, 935, 1732, 293, 472, 13, 407, 9953, 51, 284, 339, 26383, 365, 505, 13, 51214, 51214, 400, 445, 281, 855, 291, 510, 11, 1936, 11, 1954, 11, 510, 311, 257, 40863, 365, 257, 2167, 4478, 293, 309, 311, 257, 3834, 13, 51564, 51564, 400, 321, 393, 818, 300, 3174, 322, 309, 281, 445, 483, 264, 2167, 1230, 484, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.108189654951336, "compression_ratio": 1.8270042194092826, "no_speech_prob": 7.141480455175042e-05}, {"id": 1199, "seek": 616500, "start": 6189.0, "end": 6194.0, "text": " And we can call that item on it to just get the single number out.", "tokens": [50364, 400, 341, 486, 312, 264, 2771, 2448, 11, 597, 4696, 366, 935, 1732, 11, 4018, 11, 3671, 472, 935, 1732, 293, 472, 13, 50664, 50664, 407, 498, 321, 445, 1190, 341, 11, 456, 321, 352, 13, 12387, 3407, 13, 407, 264, 2128, 1320, 26383, 13, 50964, 50964, 400, 550, 935, 1732, 11, 4018, 11, 3671, 472, 935, 1732, 293, 472, 13, 407, 9953, 51, 284, 339, 26383, 365, 505, 13, 51214, 51214, 400, 445, 281, 855, 291, 510, 11, 1936, 11, 1954, 11, 510, 311, 257, 40863, 365, 257, 2167, 4478, 293, 309, 311, 257, 3834, 13, 51564, 51564, 400, 321, 393, 818, 300, 3174, 322, 309, 281, 445, 483, 264, 2167, 1230, 484, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.108189654951336, "compression_ratio": 1.8270042194092826, "no_speech_prob": 7.141480455175042e-05}, {"id": 1200, "seek": 619400, "start": 6194.0, "end": 6202.0, "text": " So that's what item does. And O is a tensor object, like I mentioned, and it's got a backward function just like we've implemented.", "tokens": [50364, 407, 300, 311, 437, 3174, 775, 13, 400, 422, 307, 257, 40863, 2657, 11, 411, 286, 2835, 11, 293, 309, 311, 658, 257, 23897, 2445, 445, 411, 321, 600, 12270, 13, 50764, 50764, 400, 550, 439, 295, 613, 611, 362, 257, 5893, 2771, 13, 407, 411, 1783, 732, 11, 337, 1365, 11, 293, 264, 2771, 293, 309, 311, 257, 40863, 13, 51014, 51014, 400, 321, 393, 1665, 484, 264, 2609, 1230, 365, 5893, 3174, 13, 51214, 51214, 407, 1936, 11, 3930, 3781, 11, 3930, 3781, 393, 360, 437, 321, 630, 294, 4532, 2771, 382, 257, 2121, 1389, 562, 428, 10688, 830, 366, 439, 2167, 4478, 10688, 830, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1315514019557408, "compression_ratio": 1.692, "no_speech_prob": 1.5206061107164714e-05}, {"id": 1201, "seek": 619400, "start": 6202.0, "end": 6207.0, "text": " And then all of these also have a dot grad. So like X two, for example, and the grad and it's a tensor.", "tokens": [50364, 407, 300, 311, 437, 3174, 775, 13, 400, 422, 307, 257, 40863, 2657, 11, 411, 286, 2835, 11, 293, 309, 311, 658, 257, 23897, 2445, 445, 411, 321, 600, 12270, 13, 50764, 50764, 400, 550, 439, 295, 613, 611, 362, 257, 5893, 2771, 13, 407, 411, 1783, 732, 11, 337, 1365, 11, 293, 264, 2771, 293, 309, 311, 257, 40863, 13, 51014, 51014, 400, 321, 393, 1665, 484, 264, 2609, 1230, 365, 5893, 3174, 13, 51214, 51214, 407, 1936, 11, 3930, 3781, 11, 3930, 3781, 393, 360, 437, 321, 630, 294, 4532, 2771, 382, 257, 2121, 1389, 562, 428, 10688, 830, 366, 439, 2167, 4478, 10688, 830, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1315514019557408, "compression_ratio": 1.692, "no_speech_prob": 1.5206061107164714e-05}, {"id": 1202, "seek": 619400, "start": 6207.0, "end": 6211.0, "text": " And we can pop out the individual number with dot item.", "tokens": [50364, 407, 300, 311, 437, 3174, 775, 13, 400, 422, 307, 257, 40863, 2657, 11, 411, 286, 2835, 11, 293, 309, 311, 658, 257, 23897, 2445, 445, 411, 321, 600, 12270, 13, 50764, 50764, 400, 550, 439, 295, 613, 611, 362, 257, 5893, 2771, 13, 407, 411, 1783, 732, 11, 337, 1365, 11, 293, 264, 2771, 293, 309, 311, 257, 40863, 13, 51014, 51014, 400, 321, 393, 1665, 484, 264, 2609, 1230, 365, 5893, 3174, 13, 51214, 51214, 407, 1936, 11, 3930, 3781, 11, 3930, 3781, 393, 360, 437, 321, 630, 294, 4532, 2771, 382, 257, 2121, 1389, 562, 428, 10688, 830, 366, 439, 2167, 4478, 10688, 830, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1315514019557408, "compression_ratio": 1.692, "no_speech_prob": 1.5206061107164714e-05}, {"id": 1203, "seek": 619400, "start": 6211.0, "end": 6220.0, "text": " So basically, torches, torches can do what we did in micro grad as a special case when your tensors are all single element tensors.", "tokens": [50364, 407, 300, 311, 437, 3174, 775, 13, 400, 422, 307, 257, 40863, 2657, 11, 411, 286, 2835, 11, 293, 309, 311, 658, 257, 23897, 2445, 445, 411, 321, 600, 12270, 13, 50764, 50764, 400, 550, 439, 295, 613, 611, 362, 257, 5893, 2771, 13, 407, 411, 1783, 732, 11, 337, 1365, 11, 293, 264, 2771, 293, 309, 311, 257, 40863, 13, 51014, 51014, 400, 321, 393, 1665, 484, 264, 2609, 1230, 365, 5893, 3174, 13, 51214, 51214, 407, 1936, 11, 3930, 3781, 11, 3930, 3781, 393, 360, 437, 321, 630, 294, 4532, 2771, 382, 257, 2121, 1389, 562, 428, 10688, 830, 366, 439, 2167, 4478, 10688, 830, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1315514019557408, "compression_ratio": 1.692, "no_speech_prob": 1.5206061107164714e-05}, {"id": 1204, "seek": 622000, "start": 6220.0, "end": 6231.0, "text": " But the big deal with PyTorch is that everything is significantly more efficient because we are working with these tensor objects and we can do lots of operations in parallel on all of these tensors.", "tokens": [50364, 583, 264, 955, 2028, 365, 9953, 51, 284, 339, 307, 300, 1203, 307, 10591, 544, 7148, 570, 321, 366, 1364, 365, 613, 40863, 6565, 293, 321, 393, 360, 3195, 295, 7705, 294, 8952, 322, 439, 295, 613, 10688, 830, 13, 50914, 50914, 583, 5911, 11, 437, 321, 600, 3094, 588, 709, 26383, 365, 264, 9362, 295, 9953, 51, 284, 339, 13, 51114, 51114, 2264, 11, 370, 586, 300, 321, 362, 512, 27302, 281, 1322, 484, 1238, 6179, 18894, 15277, 11, 321, 393, 611, 722, 2390, 493, 18161, 36170, 13, 51464, 51464, 400, 382, 286, 2835, 11, 18161, 36170, 366, 445, 257, 2685, 1508, 295, 18894, 15277, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04890939781257698, "compression_ratio": 1.7064846416382253, "no_speech_prob": 4.936755885864841e-06}, {"id": 1205, "seek": 622000, "start": 6231.0, "end": 6235.0, "text": " But otherwise, what we've built very much agrees with the API of PyTorch.", "tokens": [50364, 583, 264, 955, 2028, 365, 9953, 51, 284, 339, 307, 300, 1203, 307, 10591, 544, 7148, 570, 321, 366, 1364, 365, 613, 40863, 6565, 293, 321, 393, 360, 3195, 295, 7705, 294, 8952, 322, 439, 295, 613, 10688, 830, 13, 50914, 50914, 583, 5911, 11, 437, 321, 600, 3094, 588, 709, 26383, 365, 264, 9362, 295, 9953, 51, 284, 339, 13, 51114, 51114, 2264, 11, 370, 586, 300, 321, 362, 512, 27302, 281, 1322, 484, 1238, 6179, 18894, 15277, 11, 321, 393, 611, 722, 2390, 493, 18161, 36170, 13, 51464, 51464, 400, 382, 286, 2835, 11, 18161, 36170, 366, 445, 257, 2685, 1508, 295, 18894, 15277, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04890939781257698, "compression_ratio": 1.7064846416382253, "no_speech_prob": 4.936755885864841e-06}, {"id": 1206, "seek": 622000, "start": 6235.0, "end": 6242.0, "text": " OK, so now that we have some machinery to build out pretty complicated mathematical expressions, we can also start building up neural nets.", "tokens": [50364, 583, 264, 955, 2028, 365, 9953, 51, 284, 339, 307, 300, 1203, 307, 10591, 544, 7148, 570, 321, 366, 1364, 365, 613, 40863, 6565, 293, 321, 393, 360, 3195, 295, 7705, 294, 8952, 322, 439, 295, 613, 10688, 830, 13, 50914, 50914, 583, 5911, 11, 437, 321, 600, 3094, 588, 709, 26383, 365, 264, 9362, 295, 9953, 51, 284, 339, 13, 51114, 51114, 2264, 11, 370, 586, 300, 321, 362, 512, 27302, 281, 1322, 484, 1238, 6179, 18894, 15277, 11, 321, 393, 611, 722, 2390, 493, 18161, 36170, 13, 51464, 51464, 400, 382, 286, 2835, 11, 18161, 36170, 366, 445, 257, 2685, 1508, 295, 18894, 15277, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04890939781257698, "compression_ratio": 1.7064846416382253, "no_speech_prob": 4.936755885864841e-06}, {"id": 1207, "seek": 622000, "start": 6242.0, "end": 6247.0, "text": " And as I mentioned, neural nets are just a specific class of mathematical expressions.", "tokens": [50364, 583, 264, 955, 2028, 365, 9953, 51, 284, 339, 307, 300, 1203, 307, 10591, 544, 7148, 570, 321, 366, 1364, 365, 613, 40863, 6565, 293, 321, 393, 360, 3195, 295, 7705, 294, 8952, 322, 439, 295, 613, 10688, 830, 13, 50914, 50914, 583, 5911, 11, 437, 321, 600, 3094, 588, 709, 26383, 365, 264, 9362, 295, 9953, 51, 284, 339, 13, 51114, 51114, 2264, 11, 370, 586, 300, 321, 362, 512, 27302, 281, 1322, 484, 1238, 6179, 18894, 15277, 11, 321, 393, 611, 722, 2390, 493, 18161, 36170, 13, 51464, 51464, 400, 382, 286, 2835, 11, 18161, 36170, 366, 445, 257, 2685, 1508, 295, 18894, 15277, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04890939781257698, "compression_ratio": 1.7064846416382253, "no_speech_prob": 4.936755885864841e-06}, {"id": 1208, "seek": 624700, "start": 6247.0, "end": 6254.0, "text": " So we're going to start building out a neural net piece by piece and eventually we'll build out a two layer multilayer layer perceptron, as it's called.", "tokens": [50364, 407, 321, 434, 516, 281, 722, 2390, 484, 257, 18161, 2533, 2522, 538, 2522, 293, 4728, 321, 603, 1322, 484, 257, 732, 4583, 2120, 388, 11167, 4583, 43276, 2044, 11, 382, 309, 311, 1219, 13, 50714, 50714, 400, 286, 603, 855, 291, 2293, 437, 300, 1355, 13, 961, 311, 722, 365, 257, 2167, 2609, 34090, 13, 50914, 50914, 492, 600, 12270, 472, 510, 11, 457, 510, 286, 478, 516, 281, 4445, 472, 300, 611, 2325, 6446, 281, 264, 9953, 51, 284, 339, 9362, 293, 577, 309, 11347, 1080, 18161, 3209, 16679, 13, 51364, 51364, 407, 445, 411, 321, 1866, 300, 321, 393, 411, 2995, 264, 9362, 295, 9953, 51, 284, 339, 322, 264, 1476, 664, 6206, 1252, 11, 321, 434, 516, 281, 853, 281, 360, 300, 322, 264, 18161, 3209, 16679, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0696406364440918, "compression_ratio": 1.7986577181208054, "no_speech_prob": 2.17789456655737e-05}, {"id": 1209, "seek": 624700, "start": 6254.0, "end": 6258.0, "text": " And I'll show you exactly what that means. Let's start with a single individual neuron.", "tokens": [50364, 407, 321, 434, 516, 281, 722, 2390, 484, 257, 18161, 2533, 2522, 538, 2522, 293, 4728, 321, 603, 1322, 484, 257, 732, 4583, 2120, 388, 11167, 4583, 43276, 2044, 11, 382, 309, 311, 1219, 13, 50714, 50714, 400, 286, 603, 855, 291, 2293, 437, 300, 1355, 13, 961, 311, 722, 365, 257, 2167, 2609, 34090, 13, 50914, 50914, 492, 600, 12270, 472, 510, 11, 457, 510, 286, 478, 516, 281, 4445, 472, 300, 611, 2325, 6446, 281, 264, 9953, 51, 284, 339, 9362, 293, 577, 309, 11347, 1080, 18161, 3209, 16679, 13, 51364, 51364, 407, 445, 411, 321, 1866, 300, 321, 393, 411, 2995, 264, 9362, 295, 9953, 51, 284, 339, 322, 264, 1476, 664, 6206, 1252, 11, 321, 434, 516, 281, 853, 281, 360, 300, 322, 264, 18161, 3209, 16679, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0696406364440918, "compression_ratio": 1.7986577181208054, "no_speech_prob": 2.17789456655737e-05}, {"id": 1210, "seek": 624700, "start": 6258.0, "end": 6267.0, "text": " We've implemented one here, but here I'm going to implement one that also subscribes to the PyTorch API and how it designs its neural network modules.", "tokens": [50364, 407, 321, 434, 516, 281, 722, 2390, 484, 257, 18161, 2533, 2522, 538, 2522, 293, 4728, 321, 603, 1322, 484, 257, 732, 4583, 2120, 388, 11167, 4583, 43276, 2044, 11, 382, 309, 311, 1219, 13, 50714, 50714, 400, 286, 603, 855, 291, 2293, 437, 300, 1355, 13, 961, 311, 722, 365, 257, 2167, 2609, 34090, 13, 50914, 50914, 492, 600, 12270, 472, 510, 11, 457, 510, 286, 478, 516, 281, 4445, 472, 300, 611, 2325, 6446, 281, 264, 9953, 51, 284, 339, 9362, 293, 577, 309, 11347, 1080, 18161, 3209, 16679, 13, 51364, 51364, 407, 445, 411, 321, 1866, 300, 321, 393, 411, 2995, 264, 9362, 295, 9953, 51, 284, 339, 322, 264, 1476, 664, 6206, 1252, 11, 321, 434, 516, 281, 853, 281, 360, 300, 322, 264, 18161, 3209, 16679, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0696406364440918, "compression_ratio": 1.7986577181208054, "no_speech_prob": 2.17789456655737e-05}, {"id": 1211, "seek": 624700, "start": 6267.0, "end": 6276.0, "text": " So just like we saw that we can like match the API of PyTorch on the autograd side, we're going to try to do that on the neural network modules.", "tokens": [50364, 407, 321, 434, 516, 281, 722, 2390, 484, 257, 18161, 2533, 2522, 538, 2522, 293, 4728, 321, 603, 1322, 484, 257, 732, 4583, 2120, 388, 11167, 4583, 43276, 2044, 11, 382, 309, 311, 1219, 13, 50714, 50714, 400, 286, 603, 855, 291, 2293, 437, 300, 1355, 13, 961, 311, 722, 365, 257, 2167, 2609, 34090, 13, 50914, 50914, 492, 600, 12270, 472, 510, 11, 457, 510, 286, 478, 516, 281, 4445, 472, 300, 611, 2325, 6446, 281, 264, 9953, 51, 284, 339, 9362, 293, 577, 309, 11347, 1080, 18161, 3209, 16679, 13, 51364, 51364, 407, 445, 411, 321, 1866, 300, 321, 393, 411, 2995, 264, 9362, 295, 9953, 51, 284, 339, 322, 264, 1476, 664, 6206, 1252, 11, 321, 434, 516, 281, 853, 281, 360, 300, 322, 264, 18161, 3209, 16679, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0696406364440918, "compression_ratio": 1.7986577181208054, "no_speech_prob": 2.17789456655737e-05}, {"id": 1212, "seek": 627600, "start": 6276.0, "end": 6285.0, "text": " So here's class neuron. And just for the sake of efficiency, I'm going to copy paste some sections that are relatively straightforward.", "tokens": [50364, 407, 510, 311, 1508, 34090, 13, 400, 445, 337, 264, 9717, 295, 10493, 11, 286, 478, 516, 281, 5055, 9163, 512, 10863, 300, 366, 7226, 15325, 13, 50814, 50814, 407, 264, 47479, 486, 747, 1230, 295, 15743, 281, 341, 34090, 11, 597, 307, 577, 867, 15743, 808, 281, 257, 34090, 13, 51164, 51164, 407, 341, 472, 11, 337, 1365, 11, 575, 1045, 15743, 13, 51314, 51314, 400, 550, 309, 311, 516, 281, 1884, 257, 3364, 300, 307, 512, 4974, 1230, 1296, 3671, 472, 293, 472, 337, 633, 472, 295, 729, 15743, 293, 257, 12577, 300, 9003, 264, 4787, 7875, 8324, 295, 341, 34090, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08482994856657805, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.00013978578499518335}, {"id": 1213, "seek": 627600, "start": 6285.0, "end": 6292.0, "text": " So the constructor will take number of inputs to this neuron, which is how many inputs come to a neuron.", "tokens": [50364, 407, 510, 311, 1508, 34090, 13, 400, 445, 337, 264, 9717, 295, 10493, 11, 286, 478, 516, 281, 5055, 9163, 512, 10863, 300, 366, 7226, 15325, 13, 50814, 50814, 407, 264, 47479, 486, 747, 1230, 295, 15743, 281, 341, 34090, 11, 597, 307, 577, 867, 15743, 808, 281, 257, 34090, 13, 51164, 51164, 407, 341, 472, 11, 337, 1365, 11, 575, 1045, 15743, 13, 51314, 51314, 400, 550, 309, 311, 516, 281, 1884, 257, 3364, 300, 307, 512, 4974, 1230, 1296, 3671, 472, 293, 472, 337, 633, 472, 295, 729, 15743, 293, 257, 12577, 300, 9003, 264, 4787, 7875, 8324, 295, 341, 34090, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08482994856657805, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.00013978578499518335}, {"id": 1214, "seek": 627600, "start": 6292.0, "end": 6295.0, "text": " So this one, for example, has three inputs.", "tokens": [50364, 407, 510, 311, 1508, 34090, 13, 400, 445, 337, 264, 9717, 295, 10493, 11, 286, 478, 516, 281, 5055, 9163, 512, 10863, 300, 366, 7226, 15325, 13, 50814, 50814, 407, 264, 47479, 486, 747, 1230, 295, 15743, 281, 341, 34090, 11, 597, 307, 577, 867, 15743, 808, 281, 257, 34090, 13, 51164, 51164, 407, 341, 472, 11, 337, 1365, 11, 575, 1045, 15743, 13, 51314, 51314, 400, 550, 309, 311, 516, 281, 1884, 257, 3364, 300, 307, 512, 4974, 1230, 1296, 3671, 472, 293, 472, 337, 633, 472, 295, 729, 15743, 293, 257, 12577, 300, 9003, 264, 4787, 7875, 8324, 295, 341, 34090, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08482994856657805, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.00013978578499518335}, {"id": 1215, "seek": 627600, "start": 6295.0, "end": 6305.0, "text": " And then it's going to create a weight that is some random number between negative one and one for every one of those inputs and a bias that controls the overall trigger happiness of this neuron.", "tokens": [50364, 407, 510, 311, 1508, 34090, 13, 400, 445, 337, 264, 9717, 295, 10493, 11, 286, 478, 516, 281, 5055, 9163, 512, 10863, 300, 366, 7226, 15325, 13, 50814, 50814, 407, 264, 47479, 486, 747, 1230, 295, 15743, 281, 341, 34090, 11, 597, 307, 577, 867, 15743, 808, 281, 257, 34090, 13, 51164, 51164, 407, 341, 472, 11, 337, 1365, 11, 575, 1045, 15743, 13, 51314, 51314, 400, 550, 309, 311, 516, 281, 1884, 257, 3364, 300, 307, 512, 4974, 1230, 1296, 3671, 472, 293, 472, 337, 633, 472, 295, 729, 15743, 293, 257, 12577, 300, 9003, 264, 4787, 7875, 8324, 295, 341, 34090, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08482994856657805, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.00013978578499518335}, {"id": 1216, "seek": 630500, "start": 6305.0, "end": 6314.0, "text": " And then we're going to implement a def underscore underscore call of self and X, some input X.", "tokens": [50364, 400, 550, 321, 434, 516, 281, 4445, 257, 1060, 37556, 37556, 818, 295, 2698, 293, 1783, 11, 512, 4846, 1783, 13, 50814, 50814, 400, 534, 437, 321, 500, 380, 360, 510, 307, 343, 1413, 1783, 1804, 363, 11, 689, 343, 1413, 1783, 510, 307, 257, 5893, 1674, 4682, 13, 51164, 51164, 823, 11, 498, 291, 2378, 380, 1612, 818, 11, 718, 385, 445, 2736, 1958, 13, 15, 510, 337, 586, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.14868985493977865, "compression_ratio": 1.469945355191257, "no_speech_prob": 7.03069890732877e-05}, {"id": 1217, "seek": 630500, "start": 6314.0, "end": 6321.0, "text": " And really what we don't do here is W times X plus B, where W times X here is a dot product specifically.", "tokens": [50364, 400, 550, 321, 434, 516, 281, 4445, 257, 1060, 37556, 37556, 818, 295, 2698, 293, 1783, 11, 512, 4846, 1783, 13, 50814, 50814, 400, 534, 437, 321, 500, 380, 360, 510, 307, 343, 1413, 1783, 1804, 363, 11, 689, 343, 1413, 1783, 510, 307, 257, 5893, 1674, 4682, 13, 51164, 51164, 823, 11, 498, 291, 2378, 380, 1612, 818, 11, 718, 385, 445, 2736, 1958, 13, 15, 510, 337, 586, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.14868985493977865, "compression_ratio": 1.469945355191257, "no_speech_prob": 7.03069890732877e-05}, {"id": 1218, "seek": 630500, "start": 6321.0, "end": 6326.0, "text": " Now, if you haven't seen call, let me just return 0.0 here for now.", "tokens": [50364, 400, 550, 321, 434, 516, 281, 4445, 257, 1060, 37556, 37556, 818, 295, 2698, 293, 1783, 11, 512, 4846, 1783, 13, 50814, 50814, 400, 534, 437, 321, 500, 380, 360, 510, 307, 343, 1413, 1783, 1804, 363, 11, 689, 343, 1413, 1783, 510, 307, 257, 5893, 1674, 4682, 13, 51164, 51164, 823, 11, 498, 291, 2378, 380, 1612, 818, 11, 718, 385, 445, 2736, 1958, 13, 15, 510, 337, 586, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.14868985493977865, "compression_ratio": 1.469945355191257, "no_speech_prob": 7.03069890732877e-05}, {"id": 1219, "seek": 632600, "start": 6326.0, "end": 6335.0, "text": " The way this works now is we can have an X, which is say like two point zero three point zero. Then we can initialize a neuron that is two dimensional because these are two numbers.", "tokens": [50364, 440, 636, 341, 1985, 586, 307, 321, 393, 362, 364, 1783, 11, 597, 307, 584, 411, 732, 935, 4018, 1045, 935, 4018, 13, 1396, 321, 393, 5883, 1125, 257, 34090, 300, 307, 732, 18795, 570, 613, 366, 732, 3547, 13, 50814, 50814, 400, 550, 321, 393, 3154, 729, 732, 3547, 666, 300, 34090, 281, 483, 364, 5598, 13, 51014, 51014, 400, 370, 562, 291, 764, 341, 24657, 293, 295, 1783, 11, 15329, 486, 764, 818, 13, 51314, 51314, 407, 4362, 11, 818, 445, 11247, 1958, 13, 15, 13, 51564, 51564, 823, 321, 1116, 411, 281, 767, 360, 264, 2128, 1320, 295, 341, 34090, 2602, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.13057575750788417, "compression_ratio": 1.6653696498054475, "no_speech_prob": 4.289071512175724e-06}, {"id": 1220, "seek": 632600, "start": 6335.0, "end": 6339.0, "text": " And then we can feed those two numbers into that neuron to get an output.", "tokens": [50364, 440, 636, 341, 1985, 586, 307, 321, 393, 362, 364, 1783, 11, 597, 307, 584, 411, 732, 935, 4018, 1045, 935, 4018, 13, 1396, 321, 393, 5883, 1125, 257, 34090, 300, 307, 732, 18795, 570, 613, 366, 732, 3547, 13, 50814, 50814, 400, 550, 321, 393, 3154, 729, 732, 3547, 666, 300, 34090, 281, 483, 364, 5598, 13, 51014, 51014, 400, 370, 562, 291, 764, 341, 24657, 293, 295, 1783, 11, 15329, 486, 764, 818, 13, 51314, 51314, 407, 4362, 11, 818, 445, 11247, 1958, 13, 15, 13, 51564, 51564, 823, 321, 1116, 411, 281, 767, 360, 264, 2128, 1320, 295, 341, 34090, 2602, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.13057575750788417, "compression_ratio": 1.6653696498054475, "no_speech_prob": 4.289071512175724e-06}, {"id": 1221, "seek": 632600, "start": 6339.0, "end": 6345.0, "text": " And so when you use this notation and of X, Python will use call.", "tokens": [50364, 440, 636, 341, 1985, 586, 307, 321, 393, 362, 364, 1783, 11, 597, 307, 584, 411, 732, 935, 4018, 1045, 935, 4018, 13, 1396, 321, 393, 5883, 1125, 257, 34090, 300, 307, 732, 18795, 570, 613, 366, 732, 3547, 13, 50814, 50814, 400, 550, 321, 393, 3154, 729, 732, 3547, 666, 300, 34090, 281, 483, 364, 5598, 13, 51014, 51014, 400, 370, 562, 291, 764, 341, 24657, 293, 295, 1783, 11, 15329, 486, 764, 818, 13, 51314, 51314, 407, 4362, 11, 818, 445, 11247, 1958, 13, 15, 13, 51564, 51564, 823, 321, 1116, 411, 281, 767, 360, 264, 2128, 1320, 295, 341, 34090, 2602, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.13057575750788417, "compression_ratio": 1.6653696498054475, "no_speech_prob": 4.289071512175724e-06}, {"id": 1222, "seek": 632600, "start": 6345.0, "end": 6350.0, "text": " So currently, call just returns 0.0.", "tokens": [50364, 440, 636, 341, 1985, 586, 307, 321, 393, 362, 364, 1783, 11, 597, 307, 584, 411, 732, 935, 4018, 1045, 935, 4018, 13, 1396, 321, 393, 5883, 1125, 257, 34090, 300, 307, 732, 18795, 570, 613, 366, 732, 3547, 13, 50814, 50814, 400, 550, 321, 393, 3154, 729, 732, 3547, 666, 300, 34090, 281, 483, 364, 5598, 13, 51014, 51014, 400, 370, 562, 291, 764, 341, 24657, 293, 295, 1783, 11, 15329, 486, 764, 818, 13, 51314, 51314, 407, 4362, 11, 818, 445, 11247, 1958, 13, 15, 13, 51564, 51564, 823, 321, 1116, 411, 281, 767, 360, 264, 2128, 1320, 295, 341, 34090, 2602, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.13057575750788417, "compression_ratio": 1.6653696498054475, "no_speech_prob": 4.289071512175724e-06}, {"id": 1223, "seek": 632600, "start": 6350.0, "end": 6355.0, "text": " Now we'd like to actually do the forward pass of this neuron instead.", "tokens": [50364, 440, 636, 341, 1985, 586, 307, 321, 393, 362, 364, 1783, 11, 597, 307, 584, 411, 732, 935, 4018, 1045, 935, 4018, 13, 1396, 321, 393, 5883, 1125, 257, 34090, 300, 307, 732, 18795, 570, 613, 366, 732, 3547, 13, 50814, 50814, 400, 550, 321, 393, 3154, 729, 732, 3547, 666, 300, 34090, 281, 483, 364, 5598, 13, 51014, 51014, 400, 370, 562, 291, 764, 341, 24657, 293, 295, 1783, 11, 15329, 486, 764, 818, 13, 51314, 51314, 407, 4362, 11, 818, 445, 11247, 1958, 13, 15, 13, 51564, 51564, 823, 321, 1116, 411, 281, 767, 360, 264, 2128, 1320, 295, 341, 34090, 2602, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.13057575750788417, "compression_ratio": 1.6653696498054475, "no_speech_prob": 4.289071512175724e-06}, {"id": 1224, "seek": 635500, "start": 6355.0, "end": 6362.0, "text": " So we're going to do here first is we need to basically multiply all of the elements of W with all of the elements of X pairwise.", "tokens": [50364, 407, 321, 434, 516, 281, 360, 510, 700, 307, 321, 643, 281, 1936, 12972, 439, 295, 264, 4959, 295, 343, 365, 439, 295, 264, 4959, 295, 1783, 6119, 3711, 13, 50714, 50714, 492, 643, 281, 12972, 552, 13, 50814, 50814, 407, 264, 700, 551, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 20730, 493, 11, 343, 293, 1783, 13, 51064, 51064, 400, 294, 15329, 20730, 2516, 732, 17138, 3391, 293, 309, 7829, 257, 777, 17138, 1639, 300, 17138, 1024, 670, 264, 48433, 904, 295, 641, 11760, 23041, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13971116694998234, "compression_ratio": 1.79, "no_speech_prob": 1.4284837561717723e-05}, {"id": 1225, "seek": 635500, "start": 6362.0, "end": 6364.0, "text": " We need to multiply them.", "tokens": [50364, 407, 321, 434, 516, 281, 360, 510, 700, 307, 321, 643, 281, 1936, 12972, 439, 295, 264, 4959, 295, 343, 365, 439, 295, 264, 4959, 295, 1783, 6119, 3711, 13, 50714, 50714, 492, 643, 281, 12972, 552, 13, 50814, 50814, 407, 264, 700, 551, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 20730, 493, 11, 343, 293, 1783, 13, 51064, 51064, 400, 294, 15329, 20730, 2516, 732, 17138, 3391, 293, 309, 7829, 257, 777, 17138, 1639, 300, 17138, 1024, 670, 264, 48433, 904, 295, 641, 11760, 23041, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13971116694998234, "compression_ratio": 1.79, "no_speech_prob": 1.4284837561717723e-05}, {"id": 1226, "seek": 635500, "start": 6364.0, "end": 6369.0, "text": " So the first thing we're going to do is we're going to zip up, W and X.", "tokens": [50364, 407, 321, 434, 516, 281, 360, 510, 700, 307, 321, 643, 281, 1936, 12972, 439, 295, 264, 4959, 295, 343, 365, 439, 295, 264, 4959, 295, 1783, 6119, 3711, 13, 50714, 50714, 492, 643, 281, 12972, 552, 13, 50814, 50814, 407, 264, 700, 551, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 20730, 493, 11, 343, 293, 1783, 13, 51064, 51064, 400, 294, 15329, 20730, 2516, 732, 17138, 3391, 293, 309, 7829, 257, 777, 17138, 1639, 300, 17138, 1024, 670, 264, 48433, 904, 295, 641, 11760, 23041, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13971116694998234, "compression_ratio": 1.79, "no_speech_prob": 1.4284837561717723e-05}, {"id": 1227, "seek": 635500, "start": 6369.0, "end": 6378.0, "text": " And in Python zip takes two iterators and it creates a new iterator that iterates over the topples of their corresponding entries.", "tokens": [50364, 407, 321, 434, 516, 281, 360, 510, 700, 307, 321, 643, 281, 1936, 12972, 439, 295, 264, 4959, 295, 343, 365, 439, 295, 264, 4959, 295, 1783, 6119, 3711, 13, 50714, 50714, 492, 643, 281, 12972, 552, 13, 50814, 50814, 407, 264, 700, 551, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 20730, 493, 11, 343, 293, 1783, 13, 51064, 51064, 400, 294, 15329, 20730, 2516, 732, 17138, 3391, 293, 309, 7829, 257, 777, 17138, 1639, 300, 17138, 1024, 670, 264, 48433, 904, 295, 641, 11760, 23041, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.13971116694998234, "compression_ratio": 1.79, "no_speech_prob": 1.4284837561717723e-05}, {"id": 1228, "seek": 637800, "start": 6378.0, "end": 6391.0, "text": " So, for example, just to show you, we can print this list and still return 0.0 here.", "tokens": [50364, 407, 11, 337, 1365, 11, 445, 281, 855, 291, 11, 321, 393, 4482, 341, 1329, 293, 920, 2736, 1958, 13, 15, 510, 13, 51014, 51014, 407, 321, 536, 300, 613, 343, 311, 366, 25699, 493, 365, 264, 1783, 311, 343, 365, 1783, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.16292184464474943, "compression_ratio": 1.2695652173913043, "no_speech_prob": 2.1443289369926788e-05}, {"id": 1229, "seek": 637800, "start": 6391.0, "end": 6401.0, "text": " So we see that these W's are paired up with the X's W with X.", "tokens": [50364, 407, 11, 337, 1365, 11, 445, 281, 855, 291, 11, 321, 393, 4482, 341, 1329, 293, 920, 2736, 1958, 13, 15, 510, 13, 51014, 51014, 407, 321, 536, 300, 613, 343, 311, 366, 25699, 493, 365, 264, 1783, 311, 343, 365, 1783, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.16292184464474943, "compression_ratio": 1.2695652173913043, "no_speech_prob": 2.1443289369926788e-05}, {"id": 1230, "seek": 640100, "start": 6401.0, "end": 6415.0, "text": " And now what we want to do is for WI, XI in, we want to multiply WI times XI.", "tokens": [50364, 400, 586, 437, 321, 528, 281, 360, 307, 337, 343, 40, 11, 1783, 40, 294, 11, 321, 528, 281, 12972, 343, 40, 1413, 1783, 40, 13, 51064, 51064, 400, 550, 321, 528, 281, 2408, 439, 295, 300, 1214, 281, 808, 493, 365, 364, 24433, 293, 909, 611, 318, 3158, 8241, 363, 322, 1192, 13, 51414, 51414, 407, 300, 311, 264, 8936, 24433, 13, 51514, 51514, 400, 550, 11, 295, 1164, 11, 321, 643, 281, 1320, 300, 807, 257, 2026, 1860, 13, 51614, 51614, 407, 437, 321, 434, 516, 281, 312, 12678, 307, 605, 13, 3279, 71, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19282869773335976, "compression_ratio": 1.5555555555555556, "no_speech_prob": 4.683496808866039e-05}, {"id": 1231, "seek": 640100, "start": 6415.0, "end": 6422.0, "text": " And then we want to sum all of that together to come up with an activation and add also SELTA B on top.", "tokens": [50364, 400, 586, 437, 321, 528, 281, 360, 307, 337, 343, 40, 11, 1783, 40, 294, 11, 321, 528, 281, 12972, 343, 40, 1413, 1783, 40, 13, 51064, 51064, 400, 550, 321, 528, 281, 2408, 439, 295, 300, 1214, 281, 808, 493, 365, 364, 24433, 293, 909, 611, 318, 3158, 8241, 363, 322, 1192, 13, 51414, 51414, 407, 300, 311, 264, 8936, 24433, 13, 51514, 51514, 400, 550, 11, 295, 1164, 11, 321, 643, 281, 1320, 300, 807, 257, 2026, 1860, 13, 51614, 51614, 407, 437, 321, 434, 516, 281, 312, 12678, 307, 605, 13, 3279, 71, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19282869773335976, "compression_ratio": 1.5555555555555556, "no_speech_prob": 4.683496808866039e-05}, {"id": 1232, "seek": 640100, "start": 6422.0, "end": 6424.0, "text": " So that's the raw activation.", "tokens": [50364, 400, 586, 437, 321, 528, 281, 360, 307, 337, 343, 40, 11, 1783, 40, 294, 11, 321, 528, 281, 12972, 343, 40, 1413, 1783, 40, 13, 51064, 51064, 400, 550, 321, 528, 281, 2408, 439, 295, 300, 1214, 281, 808, 493, 365, 364, 24433, 293, 909, 611, 318, 3158, 8241, 363, 322, 1192, 13, 51414, 51414, 407, 300, 311, 264, 8936, 24433, 13, 51514, 51514, 400, 550, 11, 295, 1164, 11, 321, 643, 281, 1320, 300, 807, 257, 2026, 1860, 13, 51614, 51614, 407, 437, 321, 434, 516, 281, 312, 12678, 307, 605, 13, 3279, 71, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19282869773335976, "compression_ratio": 1.5555555555555556, "no_speech_prob": 4.683496808866039e-05}, {"id": 1233, "seek": 640100, "start": 6424.0, "end": 6426.0, "text": " And then, of course, we need to pass that through a normality.", "tokens": [50364, 400, 586, 437, 321, 528, 281, 360, 307, 337, 343, 40, 11, 1783, 40, 294, 11, 321, 528, 281, 12972, 343, 40, 1413, 1783, 40, 13, 51064, 51064, 400, 550, 321, 528, 281, 2408, 439, 295, 300, 1214, 281, 808, 493, 365, 364, 24433, 293, 909, 611, 318, 3158, 8241, 363, 322, 1192, 13, 51414, 51414, 407, 300, 311, 264, 8936, 24433, 13, 51514, 51514, 400, 550, 11, 295, 1164, 11, 321, 643, 281, 1320, 300, 807, 257, 2026, 1860, 13, 51614, 51614, 407, 437, 321, 434, 516, 281, 312, 12678, 307, 605, 13, 3279, 71, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19282869773335976, "compression_ratio": 1.5555555555555556, "no_speech_prob": 4.683496808866039e-05}, {"id": 1234, "seek": 640100, "start": 6426.0, "end": 6430.0, "text": " So what we're going to be returning is act.10h.", "tokens": [50364, 400, 586, 437, 321, 528, 281, 360, 307, 337, 343, 40, 11, 1783, 40, 294, 11, 321, 528, 281, 12972, 343, 40, 1413, 1783, 40, 13, 51064, 51064, 400, 550, 321, 528, 281, 2408, 439, 295, 300, 1214, 281, 808, 493, 365, 364, 24433, 293, 909, 611, 318, 3158, 8241, 363, 322, 1192, 13, 51414, 51414, 407, 300, 311, 264, 8936, 24433, 13, 51514, 51514, 400, 550, 11, 295, 1164, 11, 321, 643, 281, 1320, 300, 807, 257, 2026, 1860, 13, 51614, 51614, 407, 437, 321, 434, 516, 281, 312, 12678, 307, 605, 13, 3279, 71, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19282869773335976, "compression_ratio": 1.5555555555555556, "no_speech_prob": 4.683496808866039e-05}, {"id": 1235, "seek": 643000, "start": 6430.0, "end": 6432.0, "text": " And here's out.", "tokens": [50364, 400, 510, 311, 484, 13, 50464, 50464, 407, 586, 321, 536, 300, 321, 366, 1242, 512, 23930, 293, 321, 483, 257, 819, 5598, 490, 257, 34090, 1184, 565, 570, 321, 366, 5883, 3319, 819, 17443, 293, 32152, 13, 50914, 50914, 400, 550, 281, 312, 257, 857, 544, 7148, 510, 11, 767, 11, 2408, 11, 538, 264, 636, 11, 2516, 257, 1150, 17312, 13075, 11, 597, 307, 264, 722, 13, 51314, 51314, 400, 538, 7576, 11, 264, 722, 307, 4018, 13, 51414, 51414, 407, 613, 4959, 295, 341, 2408, 486, 312, 3869, 322, 1192, 295, 4018, 281, 1841, 365, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09227915643488319, "compression_ratio": 1.6680327868852458, "no_speech_prob": 3.426612965995446e-05}, {"id": 1236, "seek": 643000, "start": 6432.0, "end": 6441.0, "text": " So now we see that we are getting some outputs and we get a different output from a neuron each time because we are initializing different weights and biases.", "tokens": [50364, 400, 510, 311, 484, 13, 50464, 50464, 407, 586, 321, 536, 300, 321, 366, 1242, 512, 23930, 293, 321, 483, 257, 819, 5598, 490, 257, 34090, 1184, 565, 570, 321, 366, 5883, 3319, 819, 17443, 293, 32152, 13, 50914, 50914, 400, 550, 281, 312, 257, 857, 544, 7148, 510, 11, 767, 11, 2408, 11, 538, 264, 636, 11, 2516, 257, 1150, 17312, 13075, 11, 597, 307, 264, 722, 13, 51314, 51314, 400, 538, 7576, 11, 264, 722, 307, 4018, 13, 51414, 51414, 407, 613, 4959, 295, 341, 2408, 486, 312, 3869, 322, 1192, 295, 4018, 281, 1841, 365, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09227915643488319, "compression_ratio": 1.6680327868852458, "no_speech_prob": 3.426612965995446e-05}, {"id": 1237, "seek": 643000, "start": 6441.0, "end": 6449.0, "text": " And then to be a bit more efficient here, actually, sum, by the way, takes a second optional parameter, which is the start.", "tokens": [50364, 400, 510, 311, 484, 13, 50464, 50464, 407, 586, 321, 536, 300, 321, 366, 1242, 512, 23930, 293, 321, 483, 257, 819, 5598, 490, 257, 34090, 1184, 565, 570, 321, 366, 5883, 3319, 819, 17443, 293, 32152, 13, 50914, 50914, 400, 550, 281, 312, 257, 857, 544, 7148, 510, 11, 767, 11, 2408, 11, 538, 264, 636, 11, 2516, 257, 1150, 17312, 13075, 11, 597, 307, 264, 722, 13, 51314, 51314, 400, 538, 7576, 11, 264, 722, 307, 4018, 13, 51414, 51414, 407, 613, 4959, 295, 341, 2408, 486, 312, 3869, 322, 1192, 295, 4018, 281, 1841, 365, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09227915643488319, "compression_ratio": 1.6680327868852458, "no_speech_prob": 3.426612965995446e-05}, {"id": 1238, "seek": 643000, "start": 6449.0, "end": 6451.0, "text": " And by default, the start is zero.", "tokens": [50364, 400, 510, 311, 484, 13, 50464, 50464, 407, 586, 321, 536, 300, 321, 366, 1242, 512, 23930, 293, 321, 483, 257, 819, 5598, 490, 257, 34090, 1184, 565, 570, 321, 366, 5883, 3319, 819, 17443, 293, 32152, 13, 50914, 50914, 400, 550, 281, 312, 257, 857, 544, 7148, 510, 11, 767, 11, 2408, 11, 538, 264, 636, 11, 2516, 257, 1150, 17312, 13075, 11, 597, 307, 264, 722, 13, 51314, 51314, 400, 538, 7576, 11, 264, 722, 307, 4018, 13, 51414, 51414, 407, 613, 4959, 295, 341, 2408, 486, 312, 3869, 322, 1192, 295, 4018, 281, 1841, 365, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09227915643488319, "compression_ratio": 1.6680327868852458, "no_speech_prob": 3.426612965995446e-05}, {"id": 1239, "seek": 643000, "start": 6451.0, "end": 6455.0, "text": " So these elements of this sum will be added on top of zero to begin with.", "tokens": [50364, 400, 510, 311, 484, 13, 50464, 50464, 407, 586, 321, 536, 300, 321, 366, 1242, 512, 23930, 293, 321, 483, 257, 819, 5598, 490, 257, 34090, 1184, 565, 570, 321, 366, 5883, 3319, 819, 17443, 293, 32152, 13, 50914, 50914, 400, 550, 281, 312, 257, 857, 544, 7148, 510, 11, 767, 11, 2408, 11, 538, 264, 636, 11, 2516, 257, 1150, 17312, 13075, 11, 597, 307, 264, 722, 13, 51314, 51314, 400, 538, 7576, 11, 264, 722, 307, 4018, 13, 51414, 51414, 407, 613, 4959, 295, 341, 2408, 486, 312, 3869, 322, 1192, 295, 4018, 281, 1841, 365, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09227915643488319, "compression_ratio": 1.6680327868852458, "no_speech_prob": 3.426612965995446e-05}, {"id": 1240, "seek": 645500, "start": 6455.0, "end": 6465.0, "text": " But actually, we can just start with SELTA B and then we just have an expression like this.", "tokens": [50364, 583, 767, 11, 321, 393, 445, 722, 365, 318, 3158, 8241, 363, 293, 550, 321, 445, 362, 364, 6114, 411, 341, 13, 50864, 50864, 400, 550, 264, 19265, 6114, 510, 1633, 312, 23350, 279, 1602, 294, 15329, 13, 51064, 51064, 821, 321, 352, 13, 51314, 51314, 407, 586, 321, 393, 2128, 257, 2167, 34090, 13, 51414, 51414, 3087, 493, 11, 321, 434, 516, 281, 6964, 257, 4583, 295, 22027, 13, 51564, 51564, 407, 510, 321, 362, 257, 44739, 337, 257, 21601, 47, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07179350140451014, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.5688779967604205e-05}, {"id": 1241, "seek": 645500, "start": 6465.0, "end": 6469.0, "text": " And then the generator expression here must be parenthesized in Python.", "tokens": [50364, 583, 767, 11, 321, 393, 445, 722, 365, 318, 3158, 8241, 363, 293, 550, 321, 445, 362, 364, 6114, 411, 341, 13, 50864, 50864, 400, 550, 264, 19265, 6114, 510, 1633, 312, 23350, 279, 1602, 294, 15329, 13, 51064, 51064, 821, 321, 352, 13, 51314, 51314, 407, 586, 321, 393, 2128, 257, 2167, 34090, 13, 51414, 51414, 3087, 493, 11, 321, 434, 516, 281, 6964, 257, 4583, 295, 22027, 13, 51564, 51564, 407, 510, 321, 362, 257, 44739, 337, 257, 21601, 47, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07179350140451014, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.5688779967604205e-05}, {"id": 1242, "seek": 645500, "start": 6469.0, "end": 6474.0, "text": " There we go.", "tokens": [50364, 583, 767, 11, 321, 393, 445, 722, 365, 318, 3158, 8241, 363, 293, 550, 321, 445, 362, 364, 6114, 411, 341, 13, 50864, 50864, 400, 550, 264, 19265, 6114, 510, 1633, 312, 23350, 279, 1602, 294, 15329, 13, 51064, 51064, 821, 321, 352, 13, 51314, 51314, 407, 586, 321, 393, 2128, 257, 2167, 34090, 13, 51414, 51414, 3087, 493, 11, 321, 434, 516, 281, 6964, 257, 4583, 295, 22027, 13, 51564, 51564, 407, 510, 321, 362, 257, 44739, 337, 257, 21601, 47, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07179350140451014, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.5688779967604205e-05}, {"id": 1243, "seek": 645500, "start": 6474.0, "end": 6476.0, "text": " So now we can forward a single neuron.", "tokens": [50364, 583, 767, 11, 321, 393, 445, 722, 365, 318, 3158, 8241, 363, 293, 550, 321, 445, 362, 364, 6114, 411, 341, 13, 50864, 50864, 400, 550, 264, 19265, 6114, 510, 1633, 312, 23350, 279, 1602, 294, 15329, 13, 51064, 51064, 821, 321, 352, 13, 51314, 51314, 407, 586, 321, 393, 2128, 257, 2167, 34090, 13, 51414, 51414, 3087, 493, 11, 321, 434, 516, 281, 6964, 257, 4583, 295, 22027, 13, 51564, 51564, 407, 510, 321, 362, 257, 44739, 337, 257, 21601, 47, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07179350140451014, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.5688779967604205e-05}, {"id": 1244, "seek": 645500, "start": 6476.0, "end": 6479.0, "text": " Next up, we're going to define a layer of neurons.", "tokens": [50364, 583, 767, 11, 321, 393, 445, 722, 365, 318, 3158, 8241, 363, 293, 550, 321, 445, 362, 364, 6114, 411, 341, 13, 50864, 50864, 400, 550, 264, 19265, 6114, 510, 1633, 312, 23350, 279, 1602, 294, 15329, 13, 51064, 51064, 821, 321, 352, 13, 51314, 51314, 407, 586, 321, 393, 2128, 257, 2167, 34090, 13, 51414, 51414, 3087, 493, 11, 321, 434, 516, 281, 6964, 257, 4583, 295, 22027, 13, 51564, 51564, 407, 510, 321, 362, 257, 44739, 337, 257, 21601, 47, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07179350140451014, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.5688779967604205e-05}, {"id": 1245, "seek": 645500, "start": 6479.0, "end": 6482.0, "text": " So here we have a schematic for a MLP.", "tokens": [50364, 583, 767, 11, 321, 393, 445, 722, 365, 318, 3158, 8241, 363, 293, 550, 321, 445, 362, 364, 6114, 411, 341, 13, 50864, 50864, 400, 550, 264, 19265, 6114, 510, 1633, 312, 23350, 279, 1602, 294, 15329, 13, 51064, 51064, 821, 321, 352, 13, 51314, 51314, 407, 586, 321, 393, 2128, 257, 2167, 34090, 13, 51414, 51414, 3087, 493, 11, 321, 434, 516, 281, 6964, 257, 4583, 295, 22027, 13, 51564, 51564, 407, 510, 321, 362, 257, 44739, 337, 257, 21601, 47, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07179350140451014, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.5688779967604205e-05}, {"id": 1246, "seek": 648200, "start": 6482.0, "end": 6491.0, "text": " So we see that these MLPs, each layer, this is one layer, has actually a number of neurons and they're not connected to each other, but all of them are fully connected to the input.", "tokens": [50364, 407, 321, 536, 300, 613, 21601, 23043, 11, 1184, 4583, 11, 341, 307, 472, 4583, 11, 575, 767, 257, 1230, 295, 22027, 293, 436, 434, 406, 4582, 281, 1184, 661, 11, 457, 439, 295, 552, 366, 4498, 4582, 281, 264, 4846, 13, 50814, 50814, 407, 437, 307, 257, 4583, 295, 22027, 30, 50914, 50914, 467, 311, 445, 309, 311, 445, 257, 992, 295, 22027, 25509, 21761, 13, 51064, 51064, 407, 294, 264, 1179, 295, 565, 11, 286, 478, 516, 281, 360, 746, 6457, 15325, 510, 13, 51414, 51414, 467, 311, 3736, 257, 4583, 13, 51564, 51564, 467, 311, 445, 257, 1329, 295, 22027, 13, 51714, 51714, 400, 550, 577, 867, 22027, 360, 321, 362, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06987231919745437, "compression_ratio": 1.7568627450980392, "no_speech_prob": 1.9221972252125852e-05}, {"id": 1247, "seek": 648200, "start": 6491.0, "end": 6493.0, "text": " So what is a layer of neurons?", "tokens": [50364, 407, 321, 536, 300, 613, 21601, 23043, 11, 1184, 4583, 11, 341, 307, 472, 4583, 11, 575, 767, 257, 1230, 295, 22027, 293, 436, 434, 406, 4582, 281, 1184, 661, 11, 457, 439, 295, 552, 366, 4498, 4582, 281, 264, 4846, 13, 50814, 50814, 407, 437, 307, 257, 4583, 295, 22027, 30, 50914, 50914, 467, 311, 445, 309, 311, 445, 257, 992, 295, 22027, 25509, 21761, 13, 51064, 51064, 407, 294, 264, 1179, 295, 565, 11, 286, 478, 516, 281, 360, 746, 6457, 15325, 510, 13, 51414, 51414, 467, 311, 3736, 257, 4583, 13, 51564, 51564, 467, 311, 445, 257, 1329, 295, 22027, 13, 51714, 51714, 400, 550, 577, 867, 22027, 360, 321, 362, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06987231919745437, "compression_ratio": 1.7568627450980392, "no_speech_prob": 1.9221972252125852e-05}, {"id": 1248, "seek": 648200, "start": 6493.0, "end": 6496.0, "text": " It's just it's just a set of neurons evaluated independently.", "tokens": [50364, 407, 321, 536, 300, 613, 21601, 23043, 11, 1184, 4583, 11, 341, 307, 472, 4583, 11, 575, 767, 257, 1230, 295, 22027, 293, 436, 434, 406, 4582, 281, 1184, 661, 11, 457, 439, 295, 552, 366, 4498, 4582, 281, 264, 4846, 13, 50814, 50814, 407, 437, 307, 257, 4583, 295, 22027, 30, 50914, 50914, 467, 311, 445, 309, 311, 445, 257, 992, 295, 22027, 25509, 21761, 13, 51064, 51064, 407, 294, 264, 1179, 295, 565, 11, 286, 478, 516, 281, 360, 746, 6457, 15325, 510, 13, 51414, 51414, 467, 311, 3736, 257, 4583, 13, 51564, 51564, 467, 311, 445, 257, 1329, 295, 22027, 13, 51714, 51714, 400, 550, 577, 867, 22027, 360, 321, 362, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06987231919745437, "compression_ratio": 1.7568627450980392, "no_speech_prob": 1.9221972252125852e-05}, {"id": 1249, "seek": 648200, "start": 6496.0, "end": 6503.0, "text": " So in the interest of time, I'm going to do something fairly straightforward here.", "tokens": [50364, 407, 321, 536, 300, 613, 21601, 23043, 11, 1184, 4583, 11, 341, 307, 472, 4583, 11, 575, 767, 257, 1230, 295, 22027, 293, 436, 434, 406, 4582, 281, 1184, 661, 11, 457, 439, 295, 552, 366, 4498, 4582, 281, 264, 4846, 13, 50814, 50814, 407, 437, 307, 257, 4583, 295, 22027, 30, 50914, 50914, 467, 311, 445, 309, 311, 445, 257, 992, 295, 22027, 25509, 21761, 13, 51064, 51064, 407, 294, 264, 1179, 295, 565, 11, 286, 478, 516, 281, 360, 746, 6457, 15325, 510, 13, 51414, 51414, 467, 311, 3736, 257, 4583, 13, 51564, 51564, 467, 311, 445, 257, 1329, 295, 22027, 13, 51714, 51714, 400, 550, 577, 867, 22027, 360, 321, 362, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06987231919745437, "compression_ratio": 1.7568627450980392, "no_speech_prob": 1.9221972252125852e-05}, {"id": 1250, "seek": 648200, "start": 6503.0, "end": 6506.0, "text": " It's literally a layer.", "tokens": [50364, 407, 321, 536, 300, 613, 21601, 23043, 11, 1184, 4583, 11, 341, 307, 472, 4583, 11, 575, 767, 257, 1230, 295, 22027, 293, 436, 434, 406, 4582, 281, 1184, 661, 11, 457, 439, 295, 552, 366, 4498, 4582, 281, 264, 4846, 13, 50814, 50814, 407, 437, 307, 257, 4583, 295, 22027, 30, 50914, 50914, 467, 311, 445, 309, 311, 445, 257, 992, 295, 22027, 25509, 21761, 13, 51064, 51064, 407, 294, 264, 1179, 295, 565, 11, 286, 478, 516, 281, 360, 746, 6457, 15325, 510, 13, 51414, 51414, 467, 311, 3736, 257, 4583, 13, 51564, 51564, 467, 311, 445, 257, 1329, 295, 22027, 13, 51714, 51714, 400, 550, 577, 867, 22027, 360, 321, 362, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06987231919745437, "compression_ratio": 1.7568627450980392, "no_speech_prob": 1.9221972252125852e-05}, {"id": 1251, "seek": 648200, "start": 6506.0, "end": 6509.0, "text": " It's just a list of neurons.", "tokens": [50364, 407, 321, 536, 300, 613, 21601, 23043, 11, 1184, 4583, 11, 341, 307, 472, 4583, 11, 575, 767, 257, 1230, 295, 22027, 293, 436, 434, 406, 4582, 281, 1184, 661, 11, 457, 439, 295, 552, 366, 4498, 4582, 281, 264, 4846, 13, 50814, 50814, 407, 437, 307, 257, 4583, 295, 22027, 30, 50914, 50914, 467, 311, 445, 309, 311, 445, 257, 992, 295, 22027, 25509, 21761, 13, 51064, 51064, 407, 294, 264, 1179, 295, 565, 11, 286, 478, 516, 281, 360, 746, 6457, 15325, 510, 13, 51414, 51414, 467, 311, 3736, 257, 4583, 13, 51564, 51564, 467, 311, 445, 257, 1329, 295, 22027, 13, 51714, 51714, 400, 550, 577, 867, 22027, 360, 321, 362, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06987231919745437, "compression_ratio": 1.7568627450980392, "no_speech_prob": 1.9221972252125852e-05}, {"id": 1252, "seek": 648200, "start": 6509.0, "end": 6510.0, "text": " And then how many neurons do we have?", "tokens": [50364, 407, 321, 536, 300, 613, 21601, 23043, 11, 1184, 4583, 11, 341, 307, 472, 4583, 11, 575, 767, 257, 1230, 295, 22027, 293, 436, 434, 406, 4582, 281, 1184, 661, 11, 457, 439, 295, 552, 366, 4498, 4582, 281, 264, 4846, 13, 50814, 50814, 407, 437, 307, 257, 4583, 295, 22027, 30, 50914, 50914, 467, 311, 445, 309, 311, 445, 257, 992, 295, 22027, 25509, 21761, 13, 51064, 51064, 407, 294, 264, 1179, 295, 565, 11, 286, 478, 516, 281, 360, 746, 6457, 15325, 510, 13, 51414, 51414, 467, 311, 3736, 257, 4583, 13, 51564, 51564, 467, 311, 445, 257, 1329, 295, 22027, 13, 51714, 51714, 400, 550, 577, 867, 22027, 360, 321, 362, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06987231919745437, "compression_ratio": 1.7568627450980392, "no_speech_prob": 1.9221972252125852e-05}, {"id": 1253, "seek": 651000, "start": 6510.0, "end": 6512.0, "text": " We take that as an input argument here.", "tokens": [50364, 492, 747, 300, 382, 364, 4846, 6770, 510, 13, 50464, 50464, 1012, 867, 22027, 360, 291, 528, 294, 428, 4583, 30, 50564, 50564, 5118, 295, 23930, 294, 341, 4583, 13, 50664, 50664, 400, 370, 321, 445, 5883, 1125, 2584, 6695, 22027, 365, 341, 2212, 10139, 1860, 13, 50914, 50914, 400, 562, 321, 818, 322, 309, 11, 321, 445, 21761, 13059, 552, 13, 51164, 51164, 407, 586, 2602, 295, 257, 34090, 11, 321, 393, 652, 257, 4583, 295, 22027, 13, 51314, 51314, 814, 366, 732, 18795, 22027, 293, 718, 311, 362, 1045, 295, 552, 13, 51464, 51464, 400, 586, 321, 536, 300, 321, 362, 1045, 6695, 43085, 295, 1045, 819, 22027, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07283524222995924, "compression_ratio": 1.7992424242424243, "no_speech_prob": 9.817837053560652e-06}, {"id": 1254, "seek": 651000, "start": 6512.0, "end": 6514.0, "text": " How many neurons do you want in your layer?", "tokens": [50364, 492, 747, 300, 382, 364, 4846, 6770, 510, 13, 50464, 50464, 1012, 867, 22027, 360, 291, 528, 294, 428, 4583, 30, 50564, 50564, 5118, 295, 23930, 294, 341, 4583, 13, 50664, 50664, 400, 370, 321, 445, 5883, 1125, 2584, 6695, 22027, 365, 341, 2212, 10139, 1860, 13, 50914, 50914, 400, 562, 321, 818, 322, 309, 11, 321, 445, 21761, 13059, 552, 13, 51164, 51164, 407, 586, 2602, 295, 257, 34090, 11, 321, 393, 652, 257, 4583, 295, 22027, 13, 51314, 51314, 814, 366, 732, 18795, 22027, 293, 718, 311, 362, 1045, 295, 552, 13, 51464, 51464, 400, 586, 321, 536, 300, 321, 362, 1045, 6695, 43085, 295, 1045, 819, 22027, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07283524222995924, "compression_ratio": 1.7992424242424243, "no_speech_prob": 9.817837053560652e-06}, {"id": 1255, "seek": 651000, "start": 6514.0, "end": 6516.0, "text": " Number of outputs in this layer.", "tokens": [50364, 492, 747, 300, 382, 364, 4846, 6770, 510, 13, 50464, 50464, 1012, 867, 22027, 360, 291, 528, 294, 428, 4583, 30, 50564, 50564, 5118, 295, 23930, 294, 341, 4583, 13, 50664, 50664, 400, 370, 321, 445, 5883, 1125, 2584, 6695, 22027, 365, 341, 2212, 10139, 1860, 13, 50914, 50914, 400, 562, 321, 818, 322, 309, 11, 321, 445, 21761, 13059, 552, 13, 51164, 51164, 407, 586, 2602, 295, 257, 34090, 11, 321, 393, 652, 257, 4583, 295, 22027, 13, 51314, 51314, 814, 366, 732, 18795, 22027, 293, 718, 311, 362, 1045, 295, 552, 13, 51464, 51464, 400, 586, 321, 536, 300, 321, 362, 1045, 6695, 43085, 295, 1045, 819, 22027, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07283524222995924, "compression_ratio": 1.7992424242424243, "no_speech_prob": 9.817837053560652e-06}, {"id": 1256, "seek": 651000, "start": 6516.0, "end": 6521.0, "text": " And so we just initialize completely independent neurons with this given dimensionality.", "tokens": [50364, 492, 747, 300, 382, 364, 4846, 6770, 510, 13, 50464, 50464, 1012, 867, 22027, 360, 291, 528, 294, 428, 4583, 30, 50564, 50564, 5118, 295, 23930, 294, 341, 4583, 13, 50664, 50664, 400, 370, 321, 445, 5883, 1125, 2584, 6695, 22027, 365, 341, 2212, 10139, 1860, 13, 50914, 50914, 400, 562, 321, 818, 322, 309, 11, 321, 445, 21761, 13059, 552, 13, 51164, 51164, 407, 586, 2602, 295, 257, 34090, 11, 321, 393, 652, 257, 4583, 295, 22027, 13, 51314, 51314, 814, 366, 732, 18795, 22027, 293, 718, 311, 362, 1045, 295, 552, 13, 51464, 51464, 400, 586, 321, 536, 300, 321, 362, 1045, 6695, 43085, 295, 1045, 819, 22027, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07283524222995924, "compression_ratio": 1.7992424242424243, "no_speech_prob": 9.817837053560652e-06}, {"id": 1257, "seek": 651000, "start": 6521.0, "end": 6526.0, "text": " And when we call on it, we just independently evaluate them.", "tokens": [50364, 492, 747, 300, 382, 364, 4846, 6770, 510, 13, 50464, 50464, 1012, 867, 22027, 360, 291, 528, 294, 428, 4583, 30, 50564, 50564, 5118, 295, 23930, 294, 341, 4583, 13, 50664, 50664, 400, 370, 321, 445, 5883, 1125, 2584, 6695, 22027, 365, 341, 2212, 10139, 1860, 13, 50914, 50914, 400, 562, 321, 818, 322, 309, 11, 321, 445, 21761, 13059, 552, 13, 51164, 51164, 407, 586, 2602, 295, 257, 34090, 11, 321, 393, 652, 257, 4583, 295, 22027, 13, 51314, 51314, 814, 366, 732, 18795, 22027, 293, 718, 311, 362, 1045, 295, 552, 13, 51464, 51464, 400, 586, 321, 536, 300, 321, 362, 1045, 6695, 43085, 295, 1045, 819, 22027, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07283524222995924, "compression_ratio": 1.7992424242424243, "no_speech_prob": 9.817837053560652e-06}, {"id": 1258, "seek": 651000, "start": 6526.0, "end": 6529.0, "text": " So now instead of a neuron, we can make a layer of neurons.", "tokens": [50364, 492, 747, 300, 382, 364, 4846, 6770, 510, 13, 50464, 50464, 1012, 867, 22027, 360, 291, 528, 294, 428, 4583, 30, 50564, 50564, 5118, 295, 23930, 294, 341, 4583, 13, 50664, 50664, 400, 370, 321, 445, 5883, 1125, 2584, 6695, 22027, 365, 341, 2212, 10139, 1860, 13, 50914, 50914, 400, 562, 321, 818, 322, 309, 11, 321, 445, 21761, 13059, 552, 13, 51164, 51164, 407, 586, 2602, 295, 257, 34090, 11, 321, 393, 652, 257, 4583, 295, 22027, 13, 51314, 51314, 814, 366, 732, 18795, 22027, 293, 718, 311, 362, 1045, 295, 552, 13, 51464, 51464, 400, 586, 321, 536, 300, 321, 362, 1045, 6695, 43085, 295, 1045, 819, 22027, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07283524222995924, "compression_ratio": 1.7992424242424243, "no_speech_prob": 9.817837053560652e-06}, {"id": 1259, "seek": 651000, "start": 6529.0, "end": 6532.0, "text": " They are two dimensional neurons and let's have three of them.", "tokens": [50364, 492, 747, 300, 382, 364, 4846, 6770, 510, 13, 50464, 50464, 1012, 867, 22027, 360, 291, 528, 294, 428, 4583, 30, 50564, 50564, 5118, 295, 23930, 294, 341, 4583, 13, 50664, 50664, 400, 370, 321, 445, 5883, 1125, 2584, 6695, 22027, 365, 341, 2212, 10139, 1860, 13, 50914, 50914, 400, 562, 321, 818, 322, 309, 11, 321, 445, 21761, 13059, 552, 13, 51164, 51164, 407, 586, 2602, 295, 257, 34090, 11, 321, 393, 652, 257, 4583, 295, 22027, 13, 51314, 51314, 814, 366, 732, 18795, 22027, 293, 718, 311, 362, 1045, 295, 552, 13, 51464, 51464, 400, 586, 321, 536, 300, 321, 362, 1045, 6695, 43085, 295, 1045, 819, 22027, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07283524222995924, "compression_ratio": 1.7992424242424243, "no_speech_prob": 9.817837053560652e-06}, {"id": 1260, "seek": 651000, "start": 6532.0, "end": 6537.0, "text": " And now we see that we have three independent evaluations of three different neurons.", "tokens": [50364, 492, 747, 300, 382, 364, 4846, 6770, 510, 13, 50464, 50464, 1012, 867, 22027, 360, 291, 528, 294, 428, 4583, 30, 50564, 50564, 5118, 295, 23930, 294, 341, 4583, 13, 50664, 50664, 400, 370, 321, 445, 5883, 1125, 2584, 6695, 22027, 365, 341, 2212, 10139, 1860, 13, 50914, 50914, 400, 562, 321, 818, 322, 309, 11, 321, 445, 21761, 13059, 552, 13, 51164, 51164, 407, 586, 2602, 295, 257, 34090, 11, 321, 393, 652, 257, 4583, 295, 22027, 13, 51314, 51314, 814, 366, 732, 18795, 22027, 293, 718, 311, 362, 1045, 295, 552, 13, 51464, 51464, 400, 586, 321, 536, 300, 321, 362, 1045, 6695, 43085, 295, 1045, 819, 22027, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07283524222995924, "compression_ratio": 1.7992424242424243, "no_speech_prob": 9.817837053560652e-06}, {"id": 1261, "seek": 653700, "start": 6537.0, "end": 6544.0, "text": " Right. OK. And finally, let's complete this picture and define an entire multilayer perceptron or MLP.", "tokens": [50364, 1779, 13, 2264, 13, 400, 2721, 11, 718, 311, 3566, 341, 3036, 293, 6964, 364, 2302, 2120, 388, 11167, 43276, 2044, 420, 21601, 47, 13, 50714, 50714, 400, 382, 321, 393, 536, 510, 294, 364, 21601, 47, 11, 613, 7914, 445, 3154, 666, 1184, 661, 5123, 3137, 13, 50964, 50964, 407, 718, 311, 808, 510, 293, 286, 478, 445, 516, 281, 5055, 264, 3089, 510, 294, 1179, 295, 565, 13, 51214, 51214, 407, 364, 21601, 47, 307, 588, 2531, 13, 51364, 51364, 492, 434, 1940, 264, 1230, 295, 15743, 382, 949, 13, 51514, 51514, 583, 586, 2602, 295, 1940, 257, 2167, 293, 484, 11, 597, 307, 1230, 295, 22027, 294, 257, 2167, 4583, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1187602059315827, "compression_ratio": 1.667953667953668, "no_speech_prob": 8.219858136726543e-05}, {"id": 1262, "seek": 653700, "start": 6544.0, "end": 6549.0, "text": " And as we can see here in an MLP, these layers just feed into each other sequentially.", "tokens": [50364, 1779, 13, 2264, 13, 400, 2721, 11, 718, 311, 3566, 341, 3036, 293, 6964, 364, 2302, 2120, 388, 11167, 43276, 2044, 420, 21601, 47, 13, 50714, 50714, 400, 382, 321, 393, 536, 510, 294, 364, 21601, 47, 11, 613, 7914, 445, 3154, 666, 1184, 661, 5123, 3137, 13, 50964, 50964, 407, 718, 311, 808, 510, 293, 286, 478, 445, 516, 281, 5055, 264, 3089, 510, 294, 1179, 295, 565, 13, 51214, 51214, 407, 364, 21601, 47, 307, 588, 2531, 13, 51364, 51364, 492, 434, 1940, 264, 1230, 295, 15743, 382, 949, 13, 51514, 51514, 583, 586, 2602, 295, 1940, 257, 2167, 293, 484, 11, 597, 307, 1230, 295, 22027, 294, 257, 2167, 4583, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1187602059315827, "compression_ratio": 1.667953667953668, "no_speech_prob": 8.219858136726543e-05}, {"id": 1263, "seek": 653700, "start": 6549.0, "end": 6554.0, "text": " So let's come here and I'm just going to copy the code here in interest of time.", "tokens": [50364, 1779, 13, 2264, 13, 400, 2721, 11, 718, 311, 3566, 341, 3036, 293, 6964, 364, 2302, 2120, 388, 11167, 43276, 2044, 420, 21601, 47, 13, 50714, 50714, 400, 382, 321, 393, 536, 510, 294, 364, 21601, 47, 11, 613, 7914, 445, 3154, 666, 1184, 661, 5123, 3137, 13, 50964, 50964, 407, 718, 311, 808, 510, 293, 286, 478, 445, 516, 281, 5055, 264, 3089, 510, 294, 1179, 295, 565, 13, 51214, 51214, 407, 364, 21601, 47, 307, 588, 2531, 13, 51364, 51364, 492, 434, 1940, 264, 1230, 295, 15743, 382, 949, 13, 51514, 51514, 583, 586, 2602, 295, 1940, 257, 2167, 293, 484, 11, 597, 307, 1230, 295, 22027, 294, 257, 2167, 4583, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1187602059315827, "compression_ratio": 1.667953667953668, "no_speech_prob": 8.219858136726543e-05}, {"id": 1264, "seek": 653700, "start": 6554.0, "end": 6557.0, "text": " So an MLP is very similar.", "tokens": [50364, 1779, 13, 2264, 13, 400, 2721, 11, 718, 311, 3566, 341, 3036, 293, 6964, 364, 2302, 2120, 388, 11167, 43276, 2044, 420, 21601, 47, 13, 50714, 50714, 400, 382, 321, 393, 536, 510, 294, 364, 21601, 47, 11, 613, 7914, 445, 3154, 666, 1184, 661, 5123, 3137, 13, 50964, 50964, 407, 718, 311, 808, 510, 293, 286, 478, 445, 516, 281, 5055, 264, 3089, 510, 294, 1179, 295, 565, 13, 51214, 51214, 407, 364, 21601, 47, 307, 588, 2531, 13, 51364, 51364, 492, 434, 1940, 264, 1230, 295, 15743, 382, 949, 13, 51514, 51514, 583, 586, 2602, 295, 1940, 257, 2167, 293, 484, 11, 597, 307, 1230, 295, 22027, 294, 257, 2167, 4583, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1187602059315827, "compression_ratio": 1.667953667953668, "no_speech_prob": 8.219858136726543e-05}, {"id": 1265, "seek": 653700, "start": 6557.0, "end": 6560.0, "text": " We're taking the number of inputs as before.", "tokens": [50364, 1779, 13, 2264, 13, 400, 2721, 11, 718, 311, 3566, 341, 3036, 293, 6964, 364, 2302, 2120, 388, 11167, 43276, 2044, 420, 21601, 47, 13, 50714, 50714, 400, 382, 321, 393, 536, 510, 294, 364, 21601, 47, 11, 613, 7914, 445, 3154, 666, 1184, 661, 5123, 3137, 13, 50964, 50964, 407, 718, 311, 808, 510, 293, 286, 478, 445, 516, 281, 5055, 264, 3089, 510, 294, 1179, 295, 565, 13, 51214, 51214, 407, 364, 21601, 47, 307, 588, 2531, 13, 51364, 51364, 492, 434, 1940, 264, 1230, 295, 15743, 382, 949, 13, 51514, 51514, 583, 586, 2602, 295, 1940, 257, 2167, 293, 484, 11, 597, 307, 1230, 295, 22027, 294, 257, 2167, 4583, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1187602059315827, "compression_ratio": 1.667953667953668, "no_speech_prob": 8.219858136726543e-05}, {"id": 1266, "seek": 653700, "start": 6560.0, "end": 6564.0, "text": " But now instead of taking a single and out, which is number of neurons in a single layer,", "tokens": [50364, 1779, 13, 2264, 13, 400, 2721, 11, 718, 311, 3566, 341, 3036, 293, 6964, 364, 2302, 2120, 388, 11167, 43276, 2044, 420, 21601, 47, 13, 50714, 50714, 400, 382, 321, 393, 536, 510, 294, 364, 21601, 47, 11, 613, 7914, 445, 3154, 666, 1184, 661, 5123, 3137, 13, 50964, 50964, 407, 718, 311, 808, 510, 293, 286, 478, 445, 516, 281, 5055, 264, 3089, 510, 294, 1179, 295, 565, 13, 51214, 51214, 407, 364, 21601, 47, 307, 588, 2531, 13, 51364, 51364, 492, 434, 1940, 264, 1230, 295, 15743, 382, 949, 13, 51514, 51514, 583, 586, 2602, 295, 1940, 257, 2167, 293, 484, 11, 597, 307, 1230, 295, 22027, 294, 257, 2167, 4583, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1187602059315827, "compression_ratio": 1.667953667953668, "no_speech_prob": 8.219858136726543e-05}, {"id": 1267, "seek": 656400, "start": 6564.0, "end": 6570.0, "text": " we're going to take a list of an outs and this list defines the sizes of all the layers that we want in our MLP.", "tokens": [50364, 321, 434, 516, 281, 747, 257, 1329, 295, 364, 14758, 293, 341, 1329, 23122, 264, 11602, 295, 439, 264, 7914, 300, 321, 528, 294, 527, 21601, 47, 13, 50664, 50664, 407, 510, 321, 445, 829, 552, 439, 1214, 293, 550, 44497, 670, 30497, 15494, 295, 613, 11602, 293, 1884, 4583, 6565, 337, 552, 13, 51064, 51064, 400, 550, 294, 264, 818, 2445, 11, 321, 366, 445, 5141, 552, 5123, 3137, 13, 51214, 51214, 407, 300, 311, 364, 21601, 47, 534, 13, 51314, 51314, 400, 718, 311, 767, 319, 12, 332, 43704, 341, 3036, 13, 51414, 51414, 407, 321, 528, 1045, 4846, 22027, 293, 550, 732, 7914, 295, 1451, 293, 364, 5598, 4985, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.060565540933201455, "compression_ratio": 1.7829457364341086, "no_speech_prob": 8.800873729342129e-06}, {"id": 1268, "seek": 656400, "start": 6570.0, "end": 6578.0, "text": " So here we just put them all together and then iterate over consecutive pairs of these sizes and create layer objects for them.", "tokens": [50364, 321, 434, 516, 281, 747, 257, 1329, 295, 364, 14758, 293, 341, 1329, 23122, 264, 11602, 295, 439, 264, 7914, 300, 321, 528, 294, 527, 21601, 47, 13, 50664, 50664, 407, 510, 321, 445, 829, 552, 439, 1214, 293, 550, 44497, 670, 30497, 15494, 295, 613, 11602, 293, 1884, 4583, 6565, 337, 552, 13, 51064, 51064, 400, 550, 294, 264, 818, 2445, 11, 321, 366, 445, 5141, 552, 5123, 3137, 13, 51214, 51214, 407, 300, 311, 364, 21601, 47, 534, 13, 51314, 51314, 400, 718, 311, 767, 319, 12, 332, 43704, 341, 3036, 13, 51414, 51414, 407, 321, 528, 1045, 4846, 22027, 293, 550, 732, 7914, 295, 1451, 293, 364, 5598, 4985, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.060565540933201455, "compression_ratio": 1.7829457364341086, "no_speech_prob": 8.800873729342129e-06}, {"id": 1269, "seek": 656400, "start": 6578.0, "end": 6581.0, "text": " And then in the call function, we are just calling them sequentially.", "tokens": [50364, 321, 434, 516, 281, 747, 257, 1329, 295, 364, 14758, 293, 341, 1329, 23122, 264, 11602, 295, 439, 264, 7914, 300, 321, 528, 294, 527, 21601, 47, 13, 50664, 50664, 407, 510, 321, 445, 829, 552, 439, 1214, 293, 550, 44497, 670, 30497, 15494, 295, 613, 11602, 293, 1884, 4583, 6565, 337, 552, 13, 51064, 51064, 400, 550, 294, 264, 818, 2445, 11, 321, 366, 445, 5141, 552, 5123, 3137, 13, 51214, 51214, 407, 300, 311, 364, 21601, 47, 534, 13, 51314, 51314, 400, 718, 311, 767, 319, 12, 332, 43704, 341, 3036, 13, 51414, 51414, 407, 321, 528, 1045, 4846, 22027, 293, 550, 732, 7914, 295, 1451, 293, 364, 5598, 4985, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.060565540933201455, "compression_ratio": 1.7829457364341086, "no_speech_prob": 8.800873729342129e-06}, {"id": 1270, "seek": 656400, "start": 6581.0, "end": 6583.0, "text": " So that's an MLP really.", "tokens": [50364, 321, 434, 516, 281, 747, 257, 1329, 295, 364, 14758, 293, 341, 1329, 23122, 264, 11602, 295, 439, 264, 7914, 300, 321, 528, 294, 527, 21601, 47, 13, 50664, 50664, 407, 510, 321, 445, 829, 552, 439, 1214, 293, 550, 44497, 670, 30497, 15494, 295, 613, 11602, 293, 1884, 4583, 6565, 337, 552, 13, 51064, 51064, 400, 550, 294, 264, 818, 2445, 11, 321, 366, 445, 5141, 552, 5123, 3137, 13, 51214, 51214, 407, 300, 311, 364, 21601, 47, 534, 13, 51314, 51314, 400, 718, 311, 767, 319, 12, 332, 43704, 341, 3036, 13, 51414, 51414, 407, 321, 528, 1045, 4846, 22027, 293, 550, 732, 7914, 295, 1451, 293, 364, 5598, 4985, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.060565540933201455, "compression_ratio": 1.7829457364341086, "no_speech_prob": 8.800873729342129e-06}, {"id": 1271, "seek": 656400, "start": 6583.0, "end": 6585.0, "text": " And let's actually re-implement this picture.", "tokens": [50364, 321, 434, 516, 281, 747, 257, 1329, 295, 364, 14758, 293, 341, 1329, 23122, 264, 11602, 295, 439, 264, 7914, 300, 321, 528, 294, 527, 21601, 47, 13, 50664, 50664, 407, 510, 321, 445, 829, 552, 439, 1214, 293, 550, 44497, 670, 30497, 15494, 295, 613, 11602, 293, 1884, 4583, 6565, 337, 552, 13, 51064, 51064, 400, 550, 294, 264, 818, 2445, 11, 321, 366, 445, 5141, 552, 5123, 3137, 13, 51214, 51214, 407, 300, 311, 364, 21601, 47, 534, 13, 51314, 51314, 400, 718, 311, 767, 319, 12, 332, 43704, 341, 3036, 13, 51414, 51414, 407, 321, 528, 1045, 4846, 22027, 293, 550, 732, 7914, 295, 1451, 293, 364, 5598, 4985, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.060565540933201455, "compression_ratio": 1.7829457364341086, "no_speech_prob": 8.800873729342129e-06}, {"id": 1272, "seek": 656400, "start": 6585.0, "end": 6590.0, "text": " So we want three input neurons and then two layers of four and an output unit.", "tokens": [50364, 321, 434, 516, 281, 747, 257, 1329, 295, 364, 14758, 293, 341, 1329, 23122, 264, 11602, 295, 439, 264, 7914, 300, 321, 528, 294, 527, 21601, 47, 13, 50664, 50664, 407, 510, 321, 445, 829, 552, 439, 1214, 293, 550, 44497, 670, 30497, 15494, 295, 613, 11602, 293, 1884, 4583, 6565, 337, 552, 13, 51064, 51064, 400, 550, 294, 264, 818, 2445, 11, 321, 366, 445, 5141, 552, 5123, 3137, 13, 51214, 51214, 407, 300, 311, 364, 21601, 47, 534, 13, 51314, 51314, 400, 718, 311, 767, 319, 12, 332, 43704, 341, 3036, 13, 51414, 51414, 407, 321, 528, 1045, 4846, 22027, 293, 550, 732, 7914, 295, 1451, 293, 364, 5598, 4985, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.060565540933201455, "compression_ratio": 1.7829457364341086, "no_speech_prob": 8.800873729342129e-06}, {"id": 1273, "seek": 659000, "start": 6590.0, "end": 6594.0, "text": " So we want a three dimensional input.", "tokens": [50364, 407, 321, 528, 257, 1045, 18795, 4846, 13, 50564, 50564, 6463, 341, 307, 364, 1365, 4846, 13, 50664, 50664, 492, 528, 1045, 15743, 666, 732, 7914, 295, 1451, 293, 472, 5598, 13, 50914, 50914, 400, 341, 11, 295, 1164, 11, 307, 364, 21601, 47, 13, 51064, 51064, 400, 456, 321, 352, 13, 51114, 51114, 663, 311, 257, 2128, 1320, 295, 364, 21601, 47, 13, 51214, 51214, 1407, 652, 341, 257, 707, 857, 22842, 11, 291, 536, 577, 321, 362, 445, 257, 2167, 4478, 11, 457, 309, 311, 14226, 294, 257, 1329, 570, 4583, 1009, 11247, 14511, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06561725918609317, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.3552021957584657e-05}, {"id": 1274, "seek": 659000, "start": 6594.0, "end": 6596.0, "text": " Say this is an example input.", "tokens": [50364, 407, 321, 528, 257, 1045, 18795, 4846, 13, 50564, 50564, 6463, 341, 307, 364, 1365, 4846, 13, 50664, 50664, 492, 528, 1045, 15743, 666, 732, 7914, 295, 1451, 293, 472, 5598, 13, 50914, 50914, 400, 341, 11, 295, 1164, 11, 307, 364, 21601, 47, 13, 51064, 51064, 400, 456, 321, 352, 13, 51114, 51114, 663, 311, 257, 2128, 1320, 295, 364, 21601, 47, 13, 51214, 51214, 1407, 652, 341, 257, 707, 857, 22842, 11, 291, 536, 577, 321, 362, 445, 257, 2167, 4478, 11, 457, 309, 311, 14226, 294, 257, 1329, 570, 4583, 1009, 11247, 14511, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06561725918609317, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.3552021957584657e-05}, {"id": 1275, "seek": 659000, "start": 6596.0, "end": 6601.0, "text": " We want three inputs into two layers of four and one output.", "tokens": [50364, 407, 321, 528, 257, 1045, 18795, 4846, 13, 50564, 50564, 6463, 341, 307, 364, 1365, 4846, 13, 50664, 50664, 492, 528, 1045, 15743, 666, 732, 7914, 295, 1451, 293, 472, 5598, 13, 50914, 50914, 400, 341, 11, 295, 1164, 11, 307, 364, 21601, 47, 13, 51064, 51064, 400, 456, 321, 352, 13, 51114, 51114, 663, 311, 257, 2128, 1320, 295, 364, 21601, 47, 13, 51214, 51214, 1407, 652, 341, 257, 707, 857, 22842, 11, 291, 536, 577, 321, 362, 445, 257, 2167, 4478, 11, 457, 309, 311, 14226, 294, 257, 1329, 570, 4583, 1009, 11247, 14511, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06561725918609317, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.3552021957584657e-05}, {"id": 1276, "seek": 659000, "start": 6601.0, "end": 6604.0, "text": " And this, of course, is an MLP.", "tokens": [50364, 407, 321, 528, 257, 1045, 18795, 4846, 13, 50564, 50564, 6463, 341, 307, 364, 1365, 4846, 13, 50664, 50664, 492, 528, 1045, 15743, 666, 732, 7914, 295, 1451, 293, 472, 5598, 13, 50914, 50914, 400, 341, 11, 295, 1164, 11, 307, 364, 21601, 47, 13, 51064, 51064, 400, 456, 321, 352, 13, 51114, 51114, 663, 311, 257, 2128, 1320, 295, 364, 21601, 47, 13, 51214, 51214, 1407, 652, 341, 257, 707, 857, 22842, 11, 291, 536, 577, 321, 362, 445, 257, 2167, 4478, 11, 457, 309, 311, 14226, 294, 257, 1329, 570, 4583, 1009, 11247, 14511, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06561725918609317, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.3552021957584657e-05}, {"id": 1277, "seek": 659000, "start": 6604.0, "end": 6605.0, "text": " And there we go.", "tokens": [50364, 407, 321, 528, 257, 1045, 18795, 4846, 13, 50564, 50564, 6463, 341, 307, 364, 1365, 4846, 13, 50664, 50664, 492, 528, 1045, 15743, 666, 732, 7914, 295, 1451, 293, 472, 5598, 13, 50914, 50914, 400, 341, 11, 295, 1164, 11, 307, 364, 21601, 47, 13, 51064, 51064, 400, 456, 321, 352, 13, 51114, 51114, 663, 311, 257, 2128, 1320, 295, 364, 21601, 47, 13, 51214, 51214, 1407, 652, 341, 257, 707, 857, 22842, 11, 291, 536, 577, 321, 362, 445, 257, 2167, 4478, 11, 457, 309, 311, 14226, 294, 257, 1329, 570, 4583, 1009, 11247, 14511, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06561725918609317, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.3552021957584657e-05}, {"id": 1278, "seek": 659000, "start": 6605.0, "end": 6607.0, "text": " That's a forward pass of an MLP.", "tokens": [50364, 407, 321, 528, 257, 1045, 18795, 4846, 13, 50564, 50564, 6463, 341, 307, 364, 1365, 4846, 13, 50664, 50664, 492, 528, 1045, 15743, 666, 732, 7914, 295, 1451, 293, 472, 5598, 13, 50914, 50914, 400, 341, 11, 295, 1164, 11, 307, 364, 21601, 47, 13, 51064, 51064, 400, 456, 321, 352, 13, 51114, 51114, 663, 311, 257, 2128, 1320, 295, 364, 21601, 47, 13, 51214, 51214, 1407, 652, 341, 257, 707, 857, 22842, 11, 291, 536, 577, 321, 362, 445, 257, 2167, 4478, 11, 457, 309, 311, 14226, 294, 257, 1329, 570, 4583, 1009, 11247, 14511, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06561725918609317, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.3552021957584657e-05}, {"id": 1279, "seek": 659000, "start": 6607.0, "end": 6614.0, "text": " To make this a little bit nicer, you see how we have just a single element, but it's wrapped in a list because layer always returns lists.", "tokens": [50364, 407, 321, 528, 257, 1045, 18795, 4846, 13, 50564, 50564, 6463, 341, 307, 364, 1365, 4846, 13, 50664, 50664, 492, 528, 1045, 15743, 666, 732, 7914, 295, 1451, 293, 472, 5598, 13, 50914, 50914, 400, 341, 11, 295, 1164, 11, 307, 364, 21601, 47, 13, 51064, 51064, 400, 456, 321, 352, 13, 51114, 51114, 663, 311, 257, 2128, 1320, 295, 364, 21601, 47, 13, 51214, 51214, 1407, 652, 341, 257, 707, 857, 22842, 11, 291, 536, 577, 321, 362, 445, 257, 2167, 4478, 11, 457, 309, 311, 14226, 294, 257, 1329, 570, 4583, 1009, 11247, 14511, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06561725918609317, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.3552021957584657e-05}, {"id": 1280, "seek": 661400, "start": 6614.0, "end": 6623.0, "text": " So for convenience, return outs at zero if len out is exactly a single element, else return fullest.", "tokens": [50364, 407, 337, 19283, 11, 2736, 14758, 412, 4018, 498, 40116, 484, 307, 2293, 257, 2167, 4478, 11, 1646, 2736, 45154, 13, 50814, 50814, 400, 341, 486, 2089, 505, 281, 445, 483, 257, 2167, 2158, 484, 412, 264, 1036, 4583, 300, 787, 575, 257, 2167, 34090, 13, 51064, 51064, 400, 2721, 11, 321, 820, 312, 1075, 281, 2642, 5893, 295, 297, 295, 2031, 13, 51264, 51264, 400, 382, 291, 1062, 3811, 11, 613, 15277, 366, 586, 1242, 7226, 3288, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10496877187705901, "compression_ratio": 1.572093023255814, "no_speech_prob": 5.7718139032658655e-06}, {"id": 1281, "seek": 661400, "start": 6623.0, "end": 6628.0, "text": " And this will allow us to just get a single value out at the last layer that only has a single neuron.", "tokens": [50364, 407, 337, 19283, 11, 2736, 14758, 412, 4018, 498, 40116, 484, 307, 2293, 257, 2167, 4478, 11, 1646, 2736, 45154, 13, 50814, 50814, 400, 341, 486, 2089, 505, 281, 445, 483, 257, 2167, 2158, 484, 412, 264, 1036, 4583, 300, 787, 575, 257, 2167, 34090, 13, 51064, 51064, 400, 2721, 11, 321, 820, 312, 1075, 281, 2642, 5893, 295, 297, 295, 2031, 13, 51264, 51264, 400, 382, 291, 1062, 3811, 11, 613, 15277, 366, 586, 1242, 7226, 3288, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10496877187705901, "compression_ratio": 1.572093023255814, "no_speech_prob": 5.7718139032658655e-06}, {"id": 1282, "seek": 661400, "start": 6628.0, "end": 6632.0, "text": " And finally, we should be able to draw dot of n of x.", "tokens": [50364, 407, 337, 19283, 11, 2736, 14758, 412, 4018, 498, 40116, 484, 307, 2293, 257, 2167, 4478, 11, 1646, 2736, 45154, 13, 50814, 50814, 400, 341, 486, 2089, 505, 281, 445, 483, 257, 2167, 2158, 484, 412, 264, 1036, 4583, 300, 787, 575, 257, 2167, 34090, 13, 51064, 51064, 400, 2721, 11, 321, 820, 312, 1075, 281, 2642, 5893, 295, 297, 295, 2031, 13, 51264, 51264, 400, 382, 291, 1062, 3811, 11, 613, 15277, 366, 586, 1242, 7226, 3288, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10496877187705901, "compression_ratio": 1.572093023255814, "no_speech_prob": 5.7718139032658655e-06}, {"id": 1283, "seek": 661400, "start": 6632.0, "end": 6639.0, "text": " And as you might imagine, these expressions are now getting relatively involved.", "tokens": [50364, 407, 337, 19283, 11, 2736, 14758, 412, 4018, 498, 40116, 484, 307, 2293, 257, 2167, 4478, 11, 1646, 2736, 45154, 13, 50814, 50814, 400, 341, 486, 2089, 505, 281, 445, 483, 257, 2167, 2158, 484, 412, 264, 1036, 4583, 300, 787, 575, 257, 2167, 34090, 13, 51064, 51064, 400, 2721, 11, 321, 820, 312, 1075, 281, 2642, 5893, 295, 297, 295, 2031, 13, 51264, 51264, 400, 382, 291, 1062, 3811, 11, 613, 15277, 366, 586, 1242, 7226, 3288, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10496877187705901, "compression_ratio": 1.572093023255814, "no_speech_prob": 5.7718139032658655e-06}, {"id": 1284, "seek": 663900, "start": 6639.0, "end": 6645.0, "text": " This is an entire MLP that we're defining now.", "tokens": [50364, 639, 307, 364, 2302, 21601, 47, 300, 321, 434, 17827, 586, 13, 50664, 50664, 1057, 264, 636, 1826, 257, 2167, 5598, 13, 50864, 50864, 400, 370, 2745, 11, 291, 576, 1128, 23203, 322, 3435, 293, 3035, 613, 15277, 13, 51114, 51114, 583, 365, 4532, 38, 6206, 11, 321, 486, 312, 1075, 281, 646, 48256, 439, 264, 636, 807, 341, 293, 646, 48256, 666, 613, 17443, 295, 439, 613, 22027, 13, 51564, 51564, 407, 718, 311, 536, 577, 300, 1985, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1172155879792713, "compression_ratio": 1.5205479452054795, "no_speech_prob": 1.4738593563379254e-05}, {"id": 1285, "seek": 663900, "start": 6645.0, "end": 6649.0, "text": " All the way until a single output.", "tokens": [50364, 639, 307, 364, 2302, 21601, 47, 300, 321, 434, 17827, 586, 13, 50664, 50664, 1057, 264, 636, 1826, 257, 2167, 5598, 13, 50864, 50864, 400, 370, 2745, 11, 291, 576, 1128, 23203, 322, 3435, 293, 3035, 613, 15277, 13, 51114, 51114, 583, 365, 4532, 38, 6206, 11, 321, 486, 312, 1075, 281, 646, 48256, 439, 264, 636, 807, 341, 293, 646, 48256, 666, 613, 17443, 295, 439, 613, 22027, 13, 51564, 51564, 407, 718, 311, 536, 577, 300, 1985, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1172155879792713, "compression_ratio": 1.5205479452054795, "no_speech_prob": 1.4738593563379254e-05}, {"id": 1286, "seek": 663900, "start": 6649.0, "end": 6654.0, "text": " And so obviously, you would never differentiate on pen and paper these expressions.", "tokens": [50364, 639, 307, 364, 2302, 21601, 47, 300, 321, 434, 17827, 586, 13, 50664, 50664, 1057, 264, 636, 1826, 257, 2167, 5598, 13, 50864, 50864, 400, 370, 2745, 11, 291, 576, 1128, 23203, 322, 3435, 293, 3035, 613, 15277, 13, 51114, 51114, 583, 365, 4532, 38, 6206, 11, 321, 486, 312, 1075, 281, 646, 48256, 439, 264, 636, 807, 341, 293, 646, 48256, 666, 613, 17443, 295, 439, 613, 22027, 13, 51564, 51564, 407, 718, 311, 536, 577, 300, 1985, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1172155879792713, "compression_ratio": 1.5205479452054795, "no_speech_prob": 1.4738593563379254e-05}, {"id": 1287, "seek": 663900, "start": 6654.0, "end": 6663.0, "text": " But with microGrad, we will be able to back propagate all the way through this and back propagate into these weights of all these neurons.", "tokens": [50364, 639, 307, 364, 2302, 21601, 47, 300, 321, 434, 17827, 586, 13, 50664, 50664, 1057, 264, 636, 1826, 257, 2167, 5598, 13, 50864, 50864, 400, 370, 2745, 11, 291, 576, 1128, 23203, 322, 3435, 293, 3035, 613, 15277, 13, 51114, 51114, 583, 365, 4532, 38, 6206, 11, 321, 486, 312, 1075, 281, 646, 48256, 439, 264, 636, 807, 341, 293, 646, 48256, 666, 613, 17443, 295, 439, 613, 22027, 13, 51564, 51564, 407, 718, 311, 536, 577, 300, 1985, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1172155879792713, "compression_ratio": 1.5205479452054795, "no_speech_prob": 1.4738593563379254e-05}, {"id": 1288, "seek": 663900, "start": 6663.0, "end": 6664.0, "text": " So let's see how that works.", "tokens": [50364, 639, 307, 364, 2302, 21601, 47, 300, 321, 434, 17827, 586, 13, 50664, 50664, 1057, 264, 636, 1826, 257, 2167, 5598, 13, 50864, 50864, 400, 370, 2745, 11, 291, 576, 1128, 23203, 322, 3435, 293, 3035, 613, 15277, 13, 51114, 51114, 583, 365, 4532, 38, 6206, 11, 321, 486, 312, 1075, 281, 646, 48256, 439, 264, 636, 807, 341, 293, 646, 48256, 666, 613, 17443, 295, 439, 613, 22027, 13, 51564, 51564, 407, 718, 311, 536, 577, 300, 1985, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1172155879792713, "compression_ratio": 1.5205479452054795, "no_speech_prob": 1.4738593563379254e-05}, {"id": 1289, "seek": 666400, "start": 6664.0, "end": 6669.0, "text": " OK, so let's create ourselves a very simple example data set here.", "tokens": [50364, 2264, 11, 370, 718, 311, 1884, 4175, 257, 588, 2199, 1365, 1412, 992, 510, 13, 50614, 50614, 407, 341, 1412, 992, 575, 1451, 5110, 13, 50714, 50714, 400, 370, 321, 362, 1451, 1944, 15743, 666, 264, 18161, 2533, 13, 50914, 50914, 400, 321, 362, 1451, 14721, 12911, 13, 51014, 51014, 407, 321, 1116, 411, 264, 18161, 2533, 281, 6269, 527, 5598, 502, 13, 15, 562, 309, 311, 4636, 341, 1365, 11, 3671, 472, 562, 309, 311, 4636, 613, 5110, 11, 293, 472, 562, 309, 311, 4636, 341, 1365, 13, 51564, 51564, 407, 309, 311, 257, 588, 2199, 17434, 1508, 9902, 18161, 2533, 1936, 300, 321, 576, 411, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08867987911258124, "compression_ratio": 1.933920704845815, "no_speech_prob": 4.3316988012520596e-05}, {"id": 1290, "seek": 666400, "start": 6669.0, "end": 6671.0, "text": " So this data set has four examples.", "tokens": [50364, 2264, 11, 370, 718, 311, 1884, 4175, 257, 588, 2199, 1365, 1412, 992, 510, 13, 50614, 50614, 407, 341, 1412, 992, 575, 1451, 5110, 13, 50714, 50714, 400, 370, 321, 362, 1451, 1944, 15743, 666, 264, 18161, 2533, 13, 50914, 50914, 400, 321, 362, 1451, 14721, 12911, 13, 51014, 51014, 407, 321, 1116, 411, 264, 18161, 2533, 281, 6269, 527, 5598, 502, 13, 15, 562, 309, 311, 4636, 341, 1365, 11, 3671, 472, 562, 309, 311, 4636, 613, 5110, 11, 293, 472, 562, 309, 311, 4636, 341, 1365, 13, 51564, 51564, 407, 309, 311, 257, 588, 2199, 17434, 1508, 9902, 18161, 2533, 1936, 300, 321, 576, 411, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08867987911258124, "compression_ratio": 1.933920704845815, "no_speech_prob": 4.3316988012520596e-05}, {"id": 1291, "seek": 666400, "start": 6671.0, "end": 6675.0, "text": " And so we have four possible inputs into the neural net.", "tokens": [50364, 2264, 11, 370, 718, 311, 1884, 4175, 257, 588, 2199, 1365, 1412, 992, 510, 13, 50614, 50614, 407, 341, 1412, 992, 575, 1451, 5110, 13, 50714, 50714, 400, 370, 321, 362, 1451, 1944, 15743, 666, 264, 18161, 2533, 13, 50914, 50914, 400, 321, 362, 1451, 14721, 12911, 13, 51014, 51014, 407, 321, 1116, 411, 264, 18161, 2533, 281, 6269, 527, 5598, 502, 13, 15, 562, 309, 311, 4636, 341, 1365, 11, 3671, 472, 562, 309, 311, 4636, 613, 5110, 11, 293, 472, 562, 309, 311, 4636, 341, 1365, 13, 51564, 51564, 407, 309, 311, 257, 588, 2199, 17434, 1508, 9902, 18161, 2533, 1936, 300, 321, 576, 411, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08867987911258124, "compression_ratio": 1.933920704845815, "no_speech_prob": 4.3316988012520596e-05}, {"id": 1292, "seek": 666400, "start": 6675.0, "end": 6677.0, "text": " And we have four desired targets.", "tokens": [50364, 2264, 11, 370, 718, 311, 1884, 4175, 257, 588, 2199, 1365, 1412, 992, 510, 13, 50614, 50614, 407, 341, 1412, 992, 575, 1451, 5110, 13, 50714, 50714, 400, 370, 321, 362, 1451, 1944, 15743, 666, 264, 18161, 2533, 13, 50914, 50914, 400, 321, 362, 1451, 14721, 12911, 13, 51014, 51014, 407, 321, 1116, 411, 264, 18161, 2533, 281, 6269, 527, 5598, 502, 13, 15, 562, 309, 311, 4636, 341, 1365, 11, 3671, 472, 562, 309, 311, 4636, 613, 5110, 11, 293, 472, 562, 309, 311, 4636, 341, 1365, 13, 51564, 51564, 407, 309, 311, 257, 588, 2199, 17434, 1508, 9902, 18161, 2533, 1936, 300, 321, 576, 411, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08867987911258124, "compression_ratio": 1.933920704845815, "no_speech_prob": 4.3316988012520596e-05}, {"id": 1293, "seek": 666400, "start": 6677.0, "end": 6688.0, "text": " So we'd like the neural net to assign our output 1.0 when it's fed this example, negative one when it's fed these examples, and one when it's fed this example.", "tokens": [50364, 2264, 11, 370, 718, 311, 1884, 4175, 257, 588, 2199, 1365, 1412, 992, 510, 13, 50614, 50614, 407, 341, 1412, 992, 575, 1451, 5110, 13, 50714, 50714, 400, 370, 321, 362, 1451, 1944, 15743, 666, 264, 18161, 2533, 13, 50914, 50914, 400, 321, 362, 1451, 14721, 12911, 13, 51014, 51014, 407, 321, 1116, 411, 264, 18161, 2533, 281, 6269, 527, 5598, 502, 13, 15, 562, 309, 311, 4636, 341, 1365, 11, 3671, 472, 562, 309, 311, 4636, 613, 5110, 11, 293, 472, 562, 309, 311, 4636, 341, 1365, 13, 51564, 51564, 407, 309, 311, 257, 588, 2199, 17434, 1508, 9902, 18161, 2533, 1936, 300, 321, 576, 411, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08867987911258124, "compression_ratio": 1.933920704845815, "no_speech_prob": 4.3316988012520596e-05}, {"id": 1294, "seek": 666400, "start": 6688.0, "end": 6692.0, "text": " So it's a very simple binary classifier neural net basically that we would like here.", "tokens": [50364, 2264, 11, 370, 718, 311, 1884, 4175, 257, 588, 2199, 1365, 1412, 992, 510, 13, 50614, 50614, 407, 341, 1412, 992, 575, 1451, 5110, 13, 50714, 50714, 400, 370, 321, 362, 1451, 1944, 15743, 666, 264, 18161, 2533, 13, 50914, 50914, 400, 321, 362, 1451, 14721, 12911, 13, 51014, 51014, 407, 321, 1116, 411, 264, 18161, 2533, 281, 6269, 527, 5598, 502, 13, 15, 562, 309, 311, 4636, 341, 1365, 11, 3671, 472, 562, 309, 311, 4636, 613, 5110, 11, 293, 472, 562, 309, 311, 4636, 341, 1365, 13, 51564, 51564, 407, 309, 311, 257, 588, 2199, 17434, 1508, 9902, 18161, 2533, 1936, 300, 321, 576, 411, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08867987911258124, "compression_ratio": 1.933920704845815, "no_speech_prob": 4.3316988012520596e-05}, {"id": 1295, "seek": 669200, "start": 6692.0, "end": 6696.0, "text": " Now, let's think what the neural net currently thinks about these four examples.", "tokens": [50364, 823, 11, 718, 311, 519, 437, 264, 18161, 2533, 4362, 7309, 466, 613, 1451, 5110, 13, 50564, 50564, 492, 393, 445, 483, 641, 21264, 13, 50664, 50664, 8537, 11, 321, 393, 445, 818, 297, 295, 2031, 337, 2031, 294, 9310, 13, 50864, 50864, 400, 550, 321, 393, 4482, 13, 51014, 51014, 407, 613, 366, 264, 23930, 295, 264, 18161, 2533, 322, 729, 1451, 5110, 13, 51214, 51214, 407, 264, 700, 472, 307, 935, 4949, 472, 13, 51364, 51364, 583, 321, 411, 309, 281, 312, 472, 13, 51464, 51464, 407, 321, 820, 2944, 341, 472, 2946, 13, 51564, 51564, 639, 472, 321, 528, 281, 312, 2946, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09770043113014915, "compression_ratio": 1.75, "no_speech_prob": 1.6441919797216542e-05}, {"id": 1296, "seek": 669200, "start": 6696.0, "end": 6698.0, "text": " We can just get their predictions.", "tokens": [50364, 823, 11, 718, 311, 519, 437, 264, 18161, 2533, 4362, 7309, 466, 613, 1451, 5110, 13, 50564, 50564, 492, 393, 445, 483, 641, 21264, 13, 50664, 50664, 8537, 11, 321, 393, 445, 818, 297, 295, 2031, 337, 2031, 294, 9310, 13, 50864, 50864, 400, 550, 321, 393, 4482, 13, 51014, 51014, 407, 613, 366, 264, 23930, 295, 264, 18161, 2533, 322, 729, 1451, 5110, 13, 51214, 51214, 407, 264, 700, 472, 307, 935, 4949, 472, 13, 51364, 51364, 583, 321, 411, 309, 281, 312, 472, 13, 51464, 51464, 407, 321, 820, 2944, 341, 472, 2946, 13, 51564, 51564, 639, 472, 321, 528, 281, 312, 2946, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09770043113014915, "compression_ratio": 1.75, "no_speech_prob": 1.6441919797216542e-05}, {"id": 1297, "seek": 669200, "start": 6698.0, "end": 6702.0, "text": " Basically, we can just call n of x for x in excess.", "tokens": [50364, 823, 11, 718, 311, 519, 437, 264, 18161, 2533, 4362, 7309, 466, 613, 1451, 5110, 13, 50564, 50564, 492, 393, 445, 483, 641, 21264, 13, 50664, 50664, 8537, 11, 321, 393, 445, 818, 297, 295, 2031, 337, 2031, 294, 9310, 13, 50864, 50864, 400, 550, 321, 393, 4482, 13, 51014, 51014, 407, 613, 366, 264, 23930, 295, 264, 18161, 2533, 322, 729, 1451, 5110, 13, 51214, 51214, 407, 264, 700, 472, 307, 935, 4949, 472, 13, 51364, 51364, 583, 321, 411, 309, 281, 312, 472, 13, 51464, 51464, 407, 321, 820, 2944, 341, 472, 2946, 13, 51564, 51564, 639, 472, 321, 528, 281, 312, 2946, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09770043113014915, "compression_ratio": 1.75, "no_speech_prob": 1.6441919797216542e-05}, {"id": 1298, "seek": 669200, "start": 6702.0, "end": 6705.0, "text": " And then we can print.", "tokens": [50364, 823, 11, 718, 311, 519, 437, 264, 18161, 2533, 4362, 7309, 466, 613, 1451, 5110, 13, 50564, 50564, 492, 393, 445, 483, 641, 21264, 13, 50664, 50664, 8537, 11, 321, 393, 445, 818, 297, 295, 2031, 337, 2031, 294, 9310, 13, 50864, 50864, 400, 550, 321, 393, 4482, 13, 51014, 51014, 407, 613, 366, 264, 23930, 295, 264, 18161, 2533, 322, 729, 1451, 5110, 13, 51214, 51214, 407, 264, 700, 472, 307, 935, 4949, 472, 13, 51364, 51364, 583, 321, 411, 309, 281, 312, 472, 13, 51464, 51464, 407, 321, 820, 2944, 341, 472, 2946, 13, 51564, 51564, 639, 472, 321, 528, 281, 312, 2946, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09770043113014915, "compression_ratio": 1.75, "no_speech_prob": 1.6441919797216542e-05}, {"id": 1299, "seek": 669200, "start": 6705.0, "end": 6709.0, "text": " So these are the outputs of the neural net on those four examples.", "tokens": [50364, 823, 11, 718, 311, 519, 437, 264, 18161, 2533, 4362, 7309, 466, 613, 1451, 5110, 13, 50564, 50564, 492, 393, 445, 483, 641, 21264, 13, 50664, 50664, 8537, 11, 321, 393, 445, 818, 297, 295, 2031, 337, 2031, 294, 9310, 13, 50864, 50864, 400, 550, 321, 393, 4482, 13, 51014, 51014, 407, 613, 366, 264, 23930, 295, 264, 18161, 2533, 322, 729, 1451, 5110, 13, 51214, 51214, 407, 264, 700, 472, 307, 935, 4949, 472, 13, 51364, 51364, 583, 321, 411, 309, 281, 312, 472, 13, 51464, 51464, 407, 321, 820, 2944, 341, 472, 2946, 13, 51564, 51564, 639, 472, 321, 528, 281, 312, 2946, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09770043113014915, "compression_ratio": 1.75, "no_speech_prob": 1.6441919797216542e-05}, {"id": 1300, "seek": 669200, "start": 6709.0, "end": 6712.0, "text": " So the first one is point nine one.", "tokens": [50364, 823, 11, 718, 311, 519, 437, 264, 18161, 2533, 4362, 7309, 466, 613, 1451, 5110, 13, 50564, 50564, 492, 393, 445, 483, 641, 21264, 13, 50664, 50664, 8537, 11, 321, 393, 445, 818, 297, 295, 2031, 337, 2031, 294, 9310, 13, 50864, 50864, 400, 550, 321, 393, 4482, 13, 51014, 51014, 407, 613, 366, 264, 23930, 295, 264, 18161, 2533, 322, 729, 1451, 5110, 13, 51214, 51214, 407, 264, 700, 472, 307, 935, 4949, 472, 13, 51364, 51364, 583, 321, 411, 309, 281, 312, 472, 13, 51464, 51464, 407, 321, 820, 2944, 341, 472, 2946, 13, 51564, 51564, 639, 472, 321, 528, 281, 312, 2946, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09770043113014915, "compression_ratio": 1.75, "no_speech_prob": 1.6441919797216542e-05}, {"id": 1301, "seek": 669200, "start": 6712.0, "end": 6714.0, "text": " But we like it to be one.", "tokens": [50364, 823, 11, 718, 311, 519, 437, 264, 18161, 2533, 4362, 7309, 466, 613, 1451, 5110, 13, 50564, 50564, 492, 393, 445, 483, 641, 21264, 13, 50664, 50664, 8537, 11, 321, 393, 445, 818, 297, 295, 2031, 337, 2031, 294, 9310, 13, 50864, 50864, 400, 550, 321, 393, 4482, 13, 51014, 51014, 407, 613, 366, 264, 23930, 295, 264, 18161, 2533, 322, 729, 1451, 5110, 13, 51214, 51214, 407, 264, 700, 472, 307, 935, 4949, 472, 13, 51364, 51364, 583, 321, 411, 309, 281, 312, 472, 13, 51464, 51464, 407, 321, 820, 2944, 341, 472, 2946, 13, 51564, 51564, 639, 472, 321, 528, 281, 312, 2946, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09770043113014915, "compression_ratio": 1.75, "no_speech_prob": 1.6441919797216542e-05}, {"id": 1302, "seek": 669200, "start": 6714.0, "end": 6716.0, "text": " So we should push this one higher.", "tokens": [50364, 823, 11, 718, 311, 519, 437, 264, 18161, 2533, 4362, 7309, 466, 613, 1451, 5110, 13, 50564, 50564, 492, 393, 445, 483, 641, 21264, 13, 50664, 50664, 8537, 11, 321, 393, 445, 818, 297, 295, 2031, 337, 2031, 294, 9310, 13, 50864, 50864, 400, 550, 321, 393, 4482, 13, 51014, 51014, 407, 613, 366, 264, 23930, 295, 264, 18161, 2533, 322, 729, 1451, 5110, 13, 51214, 51214, 407, 264, 700, 472, 307, 935, 4949, 472, 13, 51364, 51364, 583, 321, 411, 309, 281, 312, 472, 13, 51464, 51464, 407, 321, 820, 2944, 341, 472, 2946, 13, 51564, 51564, 639, 472, 321, 528, 281, 312, 2946, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09770043113014915, "compression_ratio": 1.75, "no_speech_prob": 1.6441919797216542e-05}, {"id": 1303, "seek": 669200, "start": 6716.0, "end": 6718.0, "text": " This one we want to be higher.", "tokens": [50364, 823, 11, 718, 311, 519, 437, 264, 18161, 2533, 4362, 7309, 466, 613, 1451, 5110, 13, 50564, 50564, 492, 393, 445, 483, 641, 21264, 13, 50664, 50664, 8537, 11, 321, 393, 445, 818, 297, 295, 2031, 337, 2031, 294, 9310, 13, 50864, 50864, 400, 550, 321, 393, 4482, 13, 51014, 51014, 407, 613, 366, 264, 23930, 295, 264, 18161, 2533, 322, 729, 1451, 5110, 13, 51214, 51214, 407, 264, 700, 472, 307, 935, 4949, 472, 13, 51364, 51364, 583, 321, 411, 309, 281, 312, 472, 13, 51464, 51464, 407, 321, 820, 2944, 341, 472, 2946, 13, 51564, 51564, 639, 472, 321, 528, 281, 312, 2946, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09770043113014915, "compression_ratio": 1.75, "no_speech_prob": 1.6441919797216542e-05}, {"id": 1304, "seek": 671800, "start": 6718.0, "end": 6722.0, "text": " This one says point eight eight, and we want this to be negative one.", "tokens": [50364, 639, 472, 1619, 935, 3180, 3180, 11, 293, 321, 528, 341, 281, 312, 3671, 472, 13, 50564, 50564, 639, 307, 935, 3180, 3180, 13, 50664, 50664, 492, 528, 309, 281, 312, 3671, 472, 13, 50714, 50714, 400, 341, 472, 307, 935, 3180, 3180, 13, 50764, 50764, 492, 528, 309, 281, 312, 472, 13, 50864, 50864, 407, 577, 360, 321, 652, 264, 18161, 2533, 293, 577, 360, 321, 10864, 264, 17443, 281, 1101, 6069, 264, 14721, 12911, 30, 51264, 51264, 400, 264, 4282, 1143, 294, 2452, 2539, 281, 4584, 341, 307, 281, 8873, 257, 2167, 1230, 300, 6063, 8000, 264, 3217, 3389, 295, 428, 18161, 2533, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06951239325783469, "compression_ratio": 1.9506726457399104, "no_speech_prob": 4.289250682631973e-06}, {"id": 1305, "seek": 671800, "start": 6722.0, "end": 6724.0, "text": " This is point eight eight.", "tokens": [50364, 639, 472, 1619, 935, 3180, 3180, 11, 293, 321, 528, 341, 281, 312, 3671, 472, 13, 50564, 50564, 639, 307, 935, 3180, 3180, 13, 50664, 50664, 492, 528, 309, 281, 312, 3671, 472, 13, 50714, 50714, 400, 341, 472, 307, 935, 3180, 3180, 13, 50764, 50764, 492, 528, 309, 281, 312, 472, 13, 50864, 50864, 407, 577, 360, 321, 652, 264, 18161, 2533, 293, 577, 360, 321, 10864, 264, 17443, 281, 1101, 6069, 264, 14721, 12911, 30, 51264, 51264, 400, 264, 4282, 1143, 294, 2452, 2539, 281, 4584, 341, 307, 281, 8873, 257, 2167, 1230, 300, 6063, 8000, 264, 3217, 3389, 295, 428, 18161, 2533, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06951239325783469, "compression_ratio": 1.9506726457399104, "no_speech_prob": 4.289250682631973e-06}, {"id": 1306, "seek": 671800, "start": 6724.0, "end": 6725.0, "text": " We want it to be negative one.", "tokens": [50364, 639, 472, 1619, 935, 3180, 3180, 11, 293, 321, 528, 341, 281, 312, 3671, 472, 13, 50564, 50564, 639, 307, 935, 3180, 3180, 13, 50664, 50664, 492, 528, 309, 281, 312, 3671, 472, 13, 50714, 50714, 400, 341, 472, 307, 935, 3180, 3180, 13, 50764, 50764, 492, 528, 309, 281, 312, 472, 13, 50864, 50864, 407, 577, 360, 321, 652, 264, 18161, 2533, 293, 577, 360, 321, 10864, 264, 17443, 281, 1101, 6069, 264, 14721, 12911, 30, 51264, 51264, 400, 264, 4282, 1143, 294, 2452, 2539, 281, 4584, 341, 307, 281, 8873, 257, 2167, 1230, 300, 6063, 8000, 264, 3217, 3389, 295, 428, 18161, 2533, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06951239325783469, "compression_ratio": 1.9506726457399104, "no_speech_prob": 4.289250682631973e-06}, {"id": 1307, "seek": 671800, "start": 6725.0, "end": 6726.0, "text": " And this one is point eight eight.", "tokens": [50364, 639, 472, 1619, 935, 3180, 3180, 11, 293, 321, 528, 341, 281, 312, 3671, 472, 13, 50564, 50564, 639, 307, 935, 3180, 3180, 13, 50664, 50664, 492, 528, 309, 281, 312, 3671, 472, 13, 50714, 50714, 400, 341, 472, 307, 935, 3180, 3180, 13, 50764, 50764, 492, 528, 309, 281, 312, 472, 13, 50864, 50864, 407, 577, 360, 321, 652, 264, 18161, 2533, 293, 577, 360, 321, 10864, 264, 17443, 281, 1101, 6069, 264, 14721, 12911, 30, 51264, 51264, 400, 264, 4282, 1143, 294, 2452, 2539, 281, 4584, 341, 307, 281, 8873, 257, 2167, 1230, 300, 6063, 8000, 264, 3217, 3389, 295, 428, 18161, 2533, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06951239325783469, "compression_ratio": 1.9506726457399104, "no_speech_prob": 4.289250682631973e-06}, {"id": 1308, "seek": 671800, "start": 6726.0, "end": 6728.0, "text": " We want it to be one.", "tokens": [50364, 639, 472, 1619, 935, 3180, 3180, 11, 293, 321, 528, 341, 281, 312, 3671, 472, 13, 50564, 50564, 639, 307, 935, 3180, 3180, 13, 50664, 50664, 492, 528, 309, 281, 312, 3671, 472, 13, 50714, 50714, 400, 341, 472, 307, 935, 3180, 3180, 13, 50764, 50764, 492, 528, 309, 281, 312, 472, 13, 50864, 50864, 407, 577, 360, 321, 652, 264, 18161, 2533, 293, 577, 360, 321, 10864, 264, 17443, 281, 1101, 6069, 264, 14721, 12911, 30, 51264, 51264, 400, 264, 4282, 1143, 294, 2452, 2539, 281, 4584, 341, 307, 281, 8873, 257, 2167, 1230, 300, 6063, 8000, 264, 3217, 3389, 295, 428, 18161, 2533, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06951239325783469, "compression_ratio": 1.9506726457399104, "no_speech_prob": 4.289250682631973e-06}, {"id": 1309, "seek": 671800, "start": 6728.0, "end": 6736.0, "text": " So how do we make the neural net and how do we tune the weights to better predict the desired targets?", "tokens": [50364, 639, 472, 1619, 935, 3180, 3180, 11, 293, 321, 528, 341, 281, 312, 3671, 472, 13, 50564, 50564, 639, 307, 935, 3180, 3180, 13, 50664, 50664, 492, 528, 309, 281, 312, 3671, 472, 13, 50714, 50714, 400, 341, 472, 307, 935, 3180, 3180, 13, 50764, 50764, 492, 528, 309, 281, 312, 472, 13, 50864, 50864, 407, 577, 360, 321, 652, 264, 18161, 2533, 293, 577, 360, 321, 10864, 264, 17443, 281, 1101, 6069, 264, 14721, 12911, 30, 51264, 51264, 400, 264, 4282, 1143, 294, 2452, 2539, 281, 4584, 341, 307, 281, 8873, 257, 2167, 1230, 300, 6063, 8000, 264, 3217, 3389, 295, 428, 18161, 2533, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06951239325783469, "compression_ratio": 1.9506726457399104, "no_speech_prob": 4.289250682631973e-06}, {"id": 1310, "seek": 671800, "start": 6736.0, "end": 6745.0, "text": " And the trick used in deep learning to achieve this is to calculate a single number that somehow measures the total performance of your neural net.", "tokens": [50364, 639, 472, 1619, 935, 3180, 3180, 11, 293, 321, 528, 341, 281, 312, 3671, 472, 13, 50564, 50564, 639, 307, 935, 3180, 3180, 13, 50664, 50664, 492, 528, 309, 281, 312, 3671, 472, 13, 50714, 50714, 400, 341, 472, 307, 935, 3180, 3180, 13, 50764, 50764, 492, 528, 309, 281, 312, 472, 13, 50864, 50864, 407, 577, 360, 321, 652, 264, 18161, 2533, 293, 577, 360, 321, 10864, 264, 17443, 281, 1101, 6069, 264, 14721, 12911, 30, 51264, 51264, 400, 264, 4282, 1143, 294, 2452, 2539, 281, 4584, 341, 307, 281, 8873, 257, 2167, 1230, 300, 6063, 8000, 264, 3217, 3389, 295, 428, 18161, 2533, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06951239325783469, "compression_ratio": 1.9506726457399104, "no_speech_prob": 4.289250682631973e-06}, {"id": 1311, "seek": 674500, "start": 6745.0, "end": 6748.0, "text": " We call the single number the loss.", "tokens": [50364, 492, 818, 264, 2167, 1230, 264, 4470, 13, 50514, 50514, 407, 264, 4470, 700, 307, 257, 2167, 1230, 300, 321, 434, 516, 281, 6964, 300, 1936, 8000, 577, 731, 264, 18161, 2533, 307, 10205, 13, 50914, 50914, 1779, 586, 11, 321, 362, 264, 21769, 2020, 300, 309, 311, 406, 10205, 588, 731, 570, 321, 434, 406, 588, 709, 1998, 281, 341, 13, 51164, 51164, 407, 264, 4470, 486, 312, 1090, 293, 321, 603, 528, 281, 17522, 264, 4470, 13, 51364, 51364, 407, 294, 1729, 11, 294, 341, 1389, 11, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 4445, 264, 914, 8889, 6713, 4470, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.04947211291338947, "compression_ratio": 1.9194915254237288, "no_speech_prob": 2.46820072788978e-05}, {"id": 1312, "seek": 674500, "start": 6748.0, "end": 6756.0, "text": " So the loss first is a single number that we're going to define that basically measures how well the neural net is performing.", "tokens": [50364, 492, 818, 264, 2167, 1230, 264, 4470, 13, 50514, 50514, 407, 264, 4470, 700, 307, 257, 2167, 1230, 300, 321, 434, 516, 281, 6964, 300, 1936, 8000, 577, 731, 264, 18161, 2533, 307, 10205, 13, 50914, 50914, 1779, 586, 11, 321, 362, 264, 21769, 2020, 300, 309, 311, 406, 10205, 588, 731, 570, 321, 434, 406, 588, 709, 1998, 281, 341, 13, 51164, 51164, 407, 264, 4470, 486, 312, 1090, 293, 321, 603, 528, 281, 17522, 264, 4470, 13, 51364, 51364, 407, 294, 1729, 11, 294, 341, 1389, 11, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 4445, 264, 914, 8889, 6713, 4470, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.04947211291338947, "compression_ratio": 1.9194915254237288, "no_speech_prob": 2.46820072788978e-05}, {"id": 1313, "seek": 674500, "start": 6756.0, "end": 6761.0, "text": " Right now, we have the intuitive sense that it's not performing very well because we're not very much close to this.", "tokens": [50364, 492, 818, 264, 2167, 1230, 264, 4470, 13, 50514, 50514, 407, 264, 4470, 700, 307, 257, 2167, 1230, 300, 321, 434, 516, 281, 6964, 300, 1936, 8000, 577, 731, 264, 18161, 2533, 307, 10205, 13, 50914, 50914, 1779, 586, 11, 321, 362, 264, 21769, 2020, 300, 309, 311, 406, 10205, 588, 731, 570, 321, 434, 406, 588, 709, 1998, 281, 341, 13, 51164, 51164, 407, 264, 4470, 486, 312, 1090, 293, 321, 603, 528, 281, 17522, 264, 4470, 13, 51364, 51364, 407, 294, 1729, 11, 294, 341, 1389, 11, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 4445, 264, 914, 8889, 6713, 4470, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.04947211291338947, "compression_ratio": 1.9194915254237288, "no_speech_prob": 2.46820072788978e-05}, {"id": 1314, "seek": 674500, "start": 6761.0, "end": 6765.0, "text": " So the loss will be high and we'll want to minimize the loss.", "tokens": [50364, 492, 818, 264, 2167, 1230, 264, 4470, 13, 50514, 50514, 407, 264, 4470, 700, 307, 257, 2167, 1230, 300, 321, 434, 516, 281, 6964, 300, 1936, 8000, 577, 731, 264, 18161, 2533, 307, 10205, 13, 50914, 50914, 1779, 586, 11, 321, 362, 264, 21769, 2020, 300, 309, 311, 406, 10205, 588, 731, 570, 321, 434, 406, 588, 709, 1998, 281, 341, 13, 51164, 51164, 407, 264, 4470, 486, 312, 1090, 293, 321, 603, 528, 281, 17522, 264, 4470, 13, 51364, 51364, 407, 294, 1729, 11, 294, 341, 1389, 11, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 4445, 264, 914, 8889, 6713, 4470, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.04947211291338947, "compression_ratio": 1.9194915254237288, "no_speech_prob": 2.46820072788978e-05}, {"id": 1315, "seek": 674500, "start": 6765.0, "end": 6770.0, "text": " So in particular, in this case, what we're going to do is we're going to implement the mean squared error loss.", "tokens": [50364, 492, 818, 264, 2167, 1230, 264, 4470, 13, 50514, 50514, 407, 264, 4470, 700, 307, 257, 2167, 1230, 300, 321, 434, 516, 281, 6964, 300, 1936, 8000, 577, 731, 264, 18161, 2533, 307, 10205, 13, 50914, 50914, 1779, 586, 11, 321, 362, 264, 21769, 2020, 300, 309, 311, 406, 10205, 588, 731, 570, 321, 434, 406, 588, 709, 1998, 281, 341, 13, 51164, 51164, 407, 264, 4470, 486, 312, 1090, 293, 321, 603, 528, 281, 17522, 264, 4470, 13, 51364, 51364, 407, 294, 1729, 11, 294, 341, 1389, 11, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 4445, 264, 914, 8889, 6713, 4470, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.04947211291338947, "compression_ratio": 1.9194915254237288, "no_speech_prob": 2.46820072788978e-05}, {"id": 1316, "seek": 677000, "start": 6770.0, "end": 6781.0, "text": " So this is doing is we're going to basically iterate for why ground truth and why output in zip of wise and my friend.", "tokens": [50364, 407, 341, 307, 884, 307, 321, 434, 516, 281, 1936, 44497, 337, 983, 2727, 3494, 293, 983, 5598, 294, 20730, 295, 10829, 293, 452, 1277, 13, 50914, 50914, 407, 321, 434, 516, 281, 6119, 493, 264, 2727, 30079, 365, 264, 21264, 293, 264, 20730, 17138, 1024, 670, 2604, 2622, 295, 552, 13, 51314, 51314, 400, 337, 1184, 11, 983, 2727, 3494, 293, 983, 5598, 11, 321, 434, 516, 281, 16390, 552, 293, 3732, 552, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.196328489086296, "compression_ratio": 1.8171428571428572, "no_speech_prob": 9.665849574957974e-06}, {"id": 1317, "seek": 677000, "start": 6781.0, "end": 6789.0, "text": " So we're going to pair up the ground truths with the predictions and the zip iterates over tuples of them.", "tokens": [50364, 407, 341, 307, 884, 307, 321, 434, 516, 281, 1936, 44497, 337, 983, 2727, 3494, 293, 983, 5598, 294, 20730, 295, 10829, 293, 452, 1277, 13, 50914, 50914, 407, 321, 434, 516, 281, 6119, 493, 264, 2727, 30079, 365, 264, 21264, 293, 264, 20730, 17138, 1024, 670, 2604, 2622, 295, 552, 13, 51314, 51314, 400, 337, 1184, 11, 983, 2727, 3494, 293, 983, 5598, 11, 321, 434, 516, 281, 16390, 552, 293, 3732, 552, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.196328489086296, "compression_ratio": 1.8171428571428572, "no_speech_prob": 9.665849574957974e-06}, {"id": 1318, "seek": 677000, "start": 6789.0, "end": 6799.0, "text": " And for each, why ground truth and why output, we're going to subtract them and square them.", "tokens": [50364, 407, 341, 307, 884, 307, 321, 434, 516, 281, 1936, 44497, 337, 983, 2727, 3494, 293, 983, 5598, 294, 20730, 295, 10829, 293, 452, 1277, 13, 50914, 50914, 407, 321, 434, 516, 281, 6119, 493, 264, 2727, 30079, 365, 264, 21264, 293, 264, 20730, 17138, 1024, 670, 2604, 2622, 295, 552, 13, 51314, 51314, 400, 337, 1184, 11, 983, 2727, 3494, 293, 983, 5598, 11, 321, 434, 516, 281, 16390, 552, 293, 3732, 552, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.196328489086296, "compression_ratio": 1.8171428571428572, "no_speech_prob": 9.665849574957974e-06}, {"id": 1319, "seek": 679900, "start": 6799.0, "end": 6803.0, "text": " So let's first see what these losses are. These are individual loss components.", "tokens": [50364, 407, 718, 311, 700, 536, 437, 613, 15352, 366, 13, 1981, 366, 2609, 4470, 6677, 13, 50564, 50564, 400, 370, 1936, 337, 1184, 472, 295, 264, 1451, 11, 321, 366, 1940, 264, 17630, 293, 264, 2727, 3494, 13, 50914, 50914, 492, 366, 16390, 278, 552, 293, 2339, 1921, 552, 13, 51064, 51064, 407, 570, 341, 472, 307, 370, 1998, 281, 1080, 3779, 11, 935, 4949, 472, 307, 1920, 472, 16390, 278, 552, 2709, 257, 588, 1359, 1230, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07435661408959365, "compression_ratio": 1.6, "no_speech_prob": 2.3551487174700014e-05}, {"id": 1320, "seek": 679900, "start": 6803.0, "end": 6810.0, "text": " And so basically for each one of the four, we are taking the prediction and the ground truth.", "tokens": [50364, 407, 718, 311, 700, 536, 437, 613, 15352, 366, 13, 1981, 366, 2609, 4470, 6677, 13, 50564, 50564, 400, 370, 1936, 337, 1184, 472, 295, 264, 1451, 11, 321, 366, 1940, 264, 17630, 293, 264, 2727, 3494, 13, 50914, 50914, 492, 366, 16390, 278, 552, 293, 2339, 1921, 552, 13, 51064, 51064, 407, 570, 341, 472, 307, 370, 1998, 281, 1080, 3779, 11, 935, 4949, 472, 307, 1920, 472, 16390, 278, 552, 2709, 257, 588, 1359, 1230, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07435661408959365, "compression_ratio": 1.6, "no_speech_prob": 2.3551487174700014e-05}, {"id": 1321, "seek": 679900, "start": 6810.0, "end": 6813.0, "text": " We are subtracting them and squaring them.", "tokens": [50364, 407, 718, 311, 700, 536, 437, 613, 15352, 366, 13, 1981, 366, 2609, 4470, 6677, 13, 50564, 50564, 400, 370, 1936, 337, 1184, 472, 295, 264, 1451, 11, 321, 366, 1940, 264, 17630, 293, 264, 2727, 3494, 13, 50914, 50914, 492, 366, 16390, 278, 552, 293, 2339, 1921, 552, 13, 51064, 51064, 407, 570, 341, 472, 307, 370, 1998, 281, 1080, 3779, 11, 935, 4949, 472, 307, 1920, 472, 16390, 278, 552, 2709, 257, 588, 1359, 1230, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07435661408959365, "compression_ratio": 1.6, "no_speech_prob": 2.3551487174700014e-05}, {"id": 1322, "seek": 679900, "start": 6813.0, "end": 6822.0, "text": " So because this one is so close to its target, point nine one is almost one subtracting them gives a very small number.", "tokens": [50364, 407, 718, 311, 700, 536, 437, 613, 15352, 366, 13, 1981, 366, 2609, 4470, 6677, 13, 50564, 50564, 400, 370, 1936, 337, 1184, 472, 295, 264, 1451, 11, 321, 366, 1940, 264, 17630, 293, 264, 2727, 3494, 13, 50914, 50914, 492, 366, 16390, 278, 552, 293, 2339, 1921, 552, 13, 51064, 51064, 407, 570, 341, 472, 307, 370, 1998, 281, 1080, 3779, 11, 935, 4949, 472, 307, 1920, 472, 16390, 278, 552, 2709, 257, 588, 1359, 1230, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07435661408959365, "compression_ratio": 1.6, "no_speech_prob": 2.3551487174700014e-05}, {"id": 1323, "seek": 682200, "start": 6822.0, "end": 6834.0, "text": " So here we would get like a negative point one and then squaring it just makes sure that regardless of whether we are more negative or more positive, we always get a positive number.", "tokens": [50364, 407, 510, 321, 576, 483, 411, 257, 3671, 935, 472, 293, 550, 2339, 1921, 309, 445, 1669, 988, 300, 10060, 295, 1968, 321, 366, 544, 3671, 420, 544, 3353, 11, 321, 1009, 483, 257, 3353, 1230, 13, 50964, 50964, 7156, 295, 2339, 1921, 11, 321, 820, 13, 492, 727, 611, 747, 11, 337, 1365, 11, 264, 8236, 2158, 13, 51114, 51114, 492, 643, 281, 31597, 264, 1465, 13, 400, 370, 291, 536, 300, 264, 6114, 307, 18721, 370, 300, 291, 787, 483, 4018, 2293, 562, 983, 484, 307, 2681, 281, 983, 2727, 3494, 13, 51564, 51564, 1133, 729, 732, 366, 2681, 11, 370, 428, 17630, 307, 2293, 264, 3779, 11, 291, 366, 516, 281, 483, 4018, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1137635964007417, "compression_ratio": 1.751700680272109, "no_speech_prob": 4.784989869222045e-06}, {"id": 1324, "seek": 682200, "start": 6834.0, "end": 6837.0, "text": " Instead of squaring, we should. We could also take, for example, the absolute value.", "tokens": [50364, 407, 510, 321, 576, 483, 411, 257, 3671, 935, 472, 293, 550, 2339, 1921, 309, 445, 1669, 988, 300, 10060, 295, 1968, 321, 366, 544, 3671, 420, 544, 3353, 11, 321, 1009, 483, 257, 3353, 1230, 13, 50964, 50964, 7156, 295, 2339, 1921, 11, 321, 820, 13, 492, 727, 611, 747, 11, 337, 1365, 11, 264, 8236, 2158, 13, 51114, 51114, 492, 643, 281, 31597, 264, 1465, 13, 400, 370, 291, 536, 300, 264, 6114, 307, 18721, 370, 300, 291, 787, 483, 4018, 2293, 562, 983, 484, 307, 2681, 281, 983, 2727, 3494, 13, 51564, 51564, 1133, 729, 732, 366, 2681, 11, 370, 428, 17630, 307, 2293, 264, 3779, 11, 291, 366, 516, 281, 483, 4018, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1137635964007417, "compression_ratio": 1.751700680272109, "no_speech_prob": 4.784989869222045e-06}, {"id": 1325, "seek": 682200, "start": 6837.0, "end": 6846.0, "text": " We need to discard the sign. And so you see that the expression is arranged so that you only get zero exactly when why out is equal to why ground truth.", "tokens": [50364, 407, 510, 321, 576, 483, 411, 257, 3671, 935, 472, 293, 550, 2339, 1921, 309, 445, 1669, 988, 300, 10060, 295, 1968, 321, 366, 544, 3671, 420, 544, 3353, 11, 321, 1009, 483, 257, 3353, 1230, 13, 50964, 50964, 7156, 295, 2339, 1921, 11, 321, 820, 13, 492, 727, 611, 747, 11, 337, 1365, 11, 264, 8236, 2158, 13, 51114, 51114, 492, 643, 281, 31597, 264, 1465, 13, 400, 370, 291, 536, 300, 264, 6114, 307, 18721, 370, 300, 291, 787, 483, 4018, 2293, 562, 983, 484, 307, 2681, 281, 983, 2727, 3494, 13, 51564, 51564, 1133, 729, 732, 366, 2681, 11, 370, 428, 17630, 307, 2293, 264, 3779, 11, 291, 366, 516, 281, 483, 4018, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1137635964007417, "compression_ratio": 1.751700680272109, "no_speech_prob": 4.784989869222045e-06}, {"id": 1326, "seek": 682200, "start": 6846.0, "end": 6851.0, "text": " When those two are equal, so your prediction is exactly the target, you are going to get zero.", "tokens": [50364, 407, 510, 321, 576, 483, 411, 257, 3671, 935, 472, 293, 550, 2339, 1921, 309, 445, 1669, 988, 300, 10060, 295, 1968, 321, 366, 544, 3671, 420, 544, 3353, 11, 321, 1009, 483, 257, 3353, 1230, 13, 50964, 50964, 7156, 295, 2339, 1921, 11, 321, 820, 13, 492, 727, 611, 747, 11, 337, 1365, 11, 264, 8236, 2158, 13, 51114, 51114, 492, 643, 281, 31597, 264, 1465, 13, 400, 370, 291, 536, 300, 264, 6114, 307, 18721, 370, 300, 291, 787, 483, 4018, 2293, 562, 983, 484, 307, 2681, 281, 983, 2727, 3494, 13, 51564, 51564, 1133, 729, 732, 366, 2681, 11, 370, 428, 17630, 307, 2293, 264, 3779, 11, 291, 366, 516, 281, 483, 4018, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1137635964007417, "compression_ratio": 1.751700680272109, "no_speech_prob": 4.784989869222045e-06}, {"id": 1327, "seek": 685100, "start": 6851.0, "end": 6855.0, "text": " And if your prediction is not the target, you are going to get some other number.", "tokens": [50364, 400, 498, 428, 17630, 307, 406, 264, 3779, 11, 291, 366, 516, 281, 483, 512, 661, 1230, 13, 50564, 50564, 407, 510, 11, 337, 1365, 11, 321, 366, 636, 766, 13, 400, 370, 300, 311, 983, 264, 4470, 307, 1596, 1090, 13, 50814, 50814, 400, 264, 544, 766, 321, 366, 11, 264, 5044, 264, 4470, 486, 312, 13, 407, 321, 500, 380, 528, 1090, 4470, 13, 51114, 51114, 492, 528, 2295, 4470, 13, 400, 370, 264, 2572, 4470, 510, 486, 312, 445, 264, 2408, 295, 439, 295, 613, 3547, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.03970302926733139, "compression_ratio": 1.6834170854271358, "no_speech_prob": 3.373538493178785e-05}, {"id": 1328, "seek": 685100, "start": 6855.0, "end": 6860.0, "text": " So here, for example, we are way off. And so that's why the loss is quite high.", "tokens": [50364, 400, 498, 428, 17630, 307, 406, 264, 3779, 11, 291, 366, 516, 281, 483, 512, 661, 1230, 13, 50564, 50564, 407, 510, 11, 337, 1365, 11, 321, 366, 636, 766, 13, 400, 370, 300, 311, 983, 264, 4470, 307, 1596, 1090, 13, 50814, 50814, 400, 264, 544, 766, 321, 366, 11, 264, 5044, 264, 4470, 486, 312, 13, 407, 321, 500, 380, 528, 1090, 4470, 13, 51114, 51114, 492, 528, 2295, 4470, 13, 400, 370, 264, 2572, 4470, 510, 486, 312, 445, 264, 2408, 295, 439, 295, 613, 3547, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.03970302926733139, "compression_ratio": 1.6834170854271358, "no_speech_prob": 3.373538493178785e-05}, {"id": 1329, "seek": 685100, "start": 6860.0, "end": 6866.0, "text": " And the more off we are, the greater the loss will be. So we don't want high loss.", "tokens": [50364, 400, 498, 428, 17630, 307, 406, 264, 3779, 11, 291, 366, 516, 281, 483, 512, 661, 1230, 13, 50564, 50564, 407, 510, 11, 337, 1365, 11, 321, 366, 636, 766, 13, 400, 370, 300, 311, 983, 264, 4470, 307, 1596, 1090, 13, 50814, 50814, 400, 264, 544, 766, 321, 366, 11, 264, 5044, 264, 4470, 486, 312, 13, 407, 321, 500, 380, 528, 1090, 4470, 13, 51114, 51114, 492, 528, 2295, 4470, 13, 400, 370, 264, 2572, 4470, 510, 486, 312, 445, 264, 2408, 295, 439, 295, 613, 3547, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.03970302926733139, "compression_ratio": 1.6834170854271358, "no_speech_prob": 3.373538493178785e-05}, {"id": 1330, "seek": 685100, "start": 6866.0, "end": 6874.0, "text": " We want low loss. And so the final loss here will be just the sum of all of these numbers.", "tokens": [50364, 400, 498, 428, 17630, 307, 406, 264, 3779, 11, 291, 366, 516, 281, 483, 512, 661, 1230, 13, 50564, 50564, 407, 510, 11, 337, 1365, 11, 321, 366, 636, 766, 13, 400, 370, 300, 311, 983, 264, 4470, 307, 1596, 1090, 13, 50814, 50814, 400, 264, 544, 766, 321, 366, 11, 264, 5044, 264, 4470, 486, 312, 13, 407, 321, 500, 380, 528, 1090, 4470, 13, 51114, 51114, 492, 528, 2295, 4470, 13, 400, 370, 264, 2572, 4470, 510, 486, 312, 445, 264, 2408, 295, 439, 295, 613, 3547, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.03970302926733139, "compression_ratio": 1.6834170854271358, "no_speech_prob": 3.373538493178785e-05}, {"id": 1331, "seek": 687400, "start": 6874.0, "end": 6885.0, "text": " So you see that this should be zero roughly plus zero roughly, but plus seven. So loss should be about seven here.", "tokens": [50364, 407, 291, 536, 300, 341, 820, 312, 4018, 9810, 1804, 4018, 9810, 11, 457, 1804, 3407, 13, 407, 4470, 820, 312, 466, 3407, 510, 13, 50914, 50914, 400, 586, 321, 528, 281, 17522, 264, 4470, 13, 492, 528, 264, 4470, 281, 312, 2295, 570, 498, 4470, 307, 2295, 11, 550, 633, 472, 295, 264, 21264, 307, 2681, 281, 1080, 3779, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11164350509643554, "compression_ratio": 1.5914634146341464, "no_speech_prob": 2.178173053835053e-05}, {"id": 1332, "seek": 687400, "start": 6885.0, "end": 6896.0, "text": " And now we want to minimize the loss. We want the loss to be low because if loss is low, then every one of the predictions is equal to its target.", "tokens": [50364, 407, 291, 536, 300, 341, 820, 312, 4018, 9810, 1804, 4018, 9810, 11, 457, 1804, 3407, 13, 407, 4470, 820, 312, 466, 3407, 510, 13, 50914, 50914, 400, 586, 321, 528, 281, 17522, 264, 4470, 13, 492, 528, 264, 4470, 281, 312, 2295, 570, 498, 4470, 307, 2295, 11, 550, 633, 472, 295, 264, 21264, 307, 2681, 281, 1080, 3779, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11164350509643554, "compression_ratio": 1.5914634146341464, "no_speech_prob": 2.178173053835053e-05}, {"id": 1333, "seek": 689600, "start": 6896.0, "end": 6904.0, "text": " So the loss, the lowest it can be is zero. And the greater it is, the worse off the neural net is predicting.", "tokens": [50364, 407, 264, 4470, 11, 264, 12437, 309, 393, 312, 307, 4018, 13, 400, 264, 5044, 309, 307, 11, 264, 5324, 766, 264, 18161, 2533, 307, 32884, 13, 50764, 50764, 407, 586, 11, 295, 1164, 11, 498, 321, 360, 4470, 23897, 11, 746, 12066, 2011, 562, 286, 2045, 3242, 13, 51064, 51064, 400, 264, 12066, 551, 11, 295, 1164, 11, 300, 2011, 307, 300, 321, 393, 574, 412, 426, 5893, 7914, 5893, 34090, 293, 300, 7914, 412, 11, 584, 11, 411, 264, 700, 4583, 300, 22027, 412, 4018, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14554910037828528, "compression_ratio": 1.7307692307692308, "no_speech_prob": 3.535528594511561e-05}, {"id": 1334, "seek": 689600, "start": 6904.0, "end": 6910.0, "text": " So now, of course, if we do loss backward, something magical happened when I hit enter.", "tokens": [50364, 407, 264, 4470, 11, 264, 12437, 309, 393, 312, 307, 4018, 13, 400, 264, 5044, 309, 307, 11, 264, 5324, 766, 264, 18161, 2533, 307, 32884, 13, 50764, 50764, 407, 586, 11, 295, 1164, 11, 498, 321, 360, 4470, 23897, 11, 746, 12066, 2011, 562, 286, 2045, 3242, 13, 51064, 51064, 400, 264, 12066, 551, 11, 295, 1164, 11, 300, 2011, 307, 300, 321, 393, 574, 412, 426, 5893, 7914, 5893, 34090, 293, 300, 7914, 412, 11, 584, 11, 411, 264, 700, 4583, 300, 22027, 412, 4018, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14554910037828528, "compression_ratio": 1.7307692307692308, "no_speech_prob": 3.535528594511561e-05}, {"id": 1335, "seek": 689600, "start": 6910.0, "end": 6922.0, "text": " And the magical thing, of course, that happened is that we can look at N dot layers dot neuron and that layers at, say, like the first layer that neurons at zero.", "tokens": [50364, 407, 264, 4470, 11, 264, 12437, 309, 393, 312, 307, 4018, 13, 400, 264, 5044, 309, 307, 11, 264, 5324, 766, 264, 18161, 2533, 307, 32884, 13, 50764, 50764, 407, 586, 11, 295, 1164, 11, 498, 321, 360, 4470, 23897, 11, 746, 12066, 2011, 562, 286, 2045, 3242, 13, 51064, 51064, 400, 264, 12066, 551, 11, 295, 1164, 11, 300, 2011, 307, 300, 321, 393, 574, 412, 426, 5893, 7914, 5893, 34090, 293, 300, 7914, 412, 11, 584, 11, 411, 264, 700, 4583, 300, 22027, 412, 4018, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14554910037828528, "compression_ratio": 1.7307692307692308, "no_speech_prob": 3.535528594511561e-05}, {"id": 1336, "seek": 692200, "start": 6922.0, "end": 6929.0, "text": " Because remember that MLP has the layers, which is a list, and each layer has a neurons, which is a list.", "tokens": [50364, 1436, 1604, 300, 21601, 47, 575, 264, 7914, 11, 597, 307, 257, 1329, 11, 293, 1184, 4583, 575, 257, 22027, 11, 597, 307, 257, 1329, 13, 50714, 50714, 400, 300, 2709, 505, 364, 2609, 34090, 13, 400, 550, 309, 311, 658, 512, 17443, 13, 50914, 50914, 400, 370, 321, 393, 11, 337, 1365, 11, 574, 412, 264, 17443, 412, 4018, 13, 21726, 11, 309, 311, 406, 1219, 17443, 13, 467, 311, 1219, 343, 13, 51464, 51464, 400, 300, 311, 257, 2158, 13, 583, 586, 341, 2158, 611, 575, 257, 2771, 570, 295, 264, 23897, 1320, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07757742881774903, "compression_ratio": 1.6820276497695852, "no_speech_prob": 1.7502663467894308e-05}, {"id": 1337, "seek": 692200, "start": 6929.0, "end": 6933.0, "text": " And that gives us an individual neuron. And then it's got some weights.", "tokens": [50364, 1436, 1604, 300, 21601, 47, 575, 264, 7914, 11, 597, 307, 257, 1329, 11, 293, 1184, 4583, 575, 257, 22027, 11, 597, 307, 257, 1329, 13, 50714, 50714, 400, 300, 2709, 505, 364, 2609, 34090, 13, 400, 550, 309, 311, 658, 512, 17443, 13, 50914, 50914, 400, 370, 321, 393, 11, 337, 1365, 11, 574, 412, 264, 17443, 412, 4018, 13, 21726, 11, 309, 311, 406, 1219, 17443, 13, 467, 311, 1219, 343, 13, 51464, 51464, 400, 300, 311, 257, 2158, 13, 583, 586, 341, 2158, 611, 575, 257, 2771, 570, 295, 264, 23897, 1320, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07757742881774903, "compression_ratio": 1.6820276497695852, "no_speech_prob": 1.7502663467894308e-05}, {"id": 1338, "seek": 692200, "start": 6933.0, "end": 6944.0, "text": " And so we can, for example, look at the weights at zero. Oops, it's not called weights. It's called W.", "tokens": [50364, 1436, 1604, 300, 21601, 47, 575, 264, 7914, 11, 597, 307, 257, 1329, 11, 293, 1184, 4583, 575, 257, 22027, 11, 597, 307, 257, 1329, 13, 50714, 50714, 400, 300, 2709, 505, 364, 2609, 34090, 13, 400, 550, 309, 311, 658, 512, 17443, 13, 50914, 50914, 400, 370, 321, 393, 11, 337, 1365, 11, 574, 412, 264, 17443, 412, 4018, 13, 21726, 11, 309, 311, 406, 1219, 17443, 13, 467, 311, 1219, 343, 13, 51464, 51464, 400, 300, 311, 257, 2158, 13, 583, 586, 341, 2158, 611, 575, 257, 2771, 570, 295, 264, 23897, 1320, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07757742881774903, "compression_ratio": 1.6820276497695852, "no_speech_prob": 1.7502663467894308e-05}, {"id": 1339, "seek": 692200, "start": 6944.0, "end": 6950.0, "text": " And that's a value. But now this value also has a grad because of the backward pass.", "tokens": [50364, 1436, 1604, 300, 21601, 47, 575, 264, 7914, 11, 597, 307, 257, 1329, 11, 293, 1184, 4583, 575, 257, 22027, 11, 597, 307, 257, 1329, 13, 50714, 50714, 400, 300, 2709, 505, 364, 2609, 34090, 13, 400, 550, 309, 311, 658, 512, 17443, 13, 50914, 50914, 400, 370, 321, 393, 11, 337, 1365, 11, 574, 412, 264, 17443, 412, 4018, 13, 21726, 11, 309, 311, 406, 1219, 17443, 13, 467, 311, 1219, 343, 13, 51464, 51464, 400, 300, 311, 257, 2158, 13, 583, 586, 341, 2158, 611, 575, 257, 2771, 570, 295, 264, 23897, 1320, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07757742881774903, "compression_ratio": 1.6820276497695852, "no_speech_prob": 1.7502663467894308e-05}, {"id": 1340, "seek": 695000, "start": 6950.0, "end": 6961.0, "text": " And so we see that because this gradient here on this particular weight of this particular neuron of this particular layer is negative, we see that its influence on the loss is also negative.", "tokens": [50364, 400, 370, 321, 536, 300, 570, 341, 16235, 510, 322, 341, 1729, 3364, 295, 341, 1729, 34090, 295, 341, 1729, 4583, 307, 3671, 11, 321, 536, 300, 1080, 6503, 322, 264, 4470, 307, 611, 3671, 13, 50914, 50914, 407, 4748, 5662, 341, 1729, 3364, 295, 341, 34090, 295, 341, 4583, 576, 652, 264, 4470, 352, 760, 13, 51314, 51314, 400, 321, 767, 362, 341, 1589, 337, 633, 2167, 472, 295, 527, 22027, 293, 439, 641, 9834, 13, 51514, 51514, 5135, 11, 309, 311, 3163, 1237, 412, 611, 264, 2642, 5893, 4470, 11, 538, 264, 636, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06976461410522461, "compression_ratio": 1.9372384937238494, "no_speech_prob": 9.080393283511512e-06}, {"id": 1341, "seek": 695000, "start": 6961.0, "end": 6969.0, "text": " So slightly increasing this particular weight of this neuron of this layer would make the loss go down.", "tokens": [50364, 400, 370, 321, 536, 300, 570, 341, 16235, 510, 322, 341, 1729, 3364, 295, 341, 1729, 34090, 295, 341, 1729, 4583, 307, 3671, 11, 321, 536, 300, 1080, 6503, 322, 264, 4470, 307, 611, 3671, 13, 50914, 50914, 407, 4748, 5662, 341, 1729, 3364, 295, 341, 34090, 295, 341, 4583, 576, 652, 264, 4470, 352, 760, 13, 51314, 51314, 400, 321, 767, 362, 341, 1589, 337, 633, 2167, 472, 295, 527, 22027, 293, 439, 641, 9834, 13, 51514, 51514, 5135, 11, 309, 311, 3163, 1237, 412, 611, 264, 2642, 5893, 4470, 11, 538, 264, 636, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06976461410522461, "compression_ratio": 1.9372384937238494, "no_speech_prob": 9.080393283511512e-06}, {"id": 1342, "seek": 695000, "start": 6969.0, "end": 6973.0, "text": " And we actually have this information for every single one of our neurons and all their parameters.", "tokens": [50364, 400, 370, 321, 536, 300, 570, 341, 16235, 510, 322, 341, 1729, 3364, 295, 341, 1729, 34090, 295, 341, 1729, 4583, 307, 3671, 11, 321, 536, 300, 1080, 6503, 322, 264, 4470, 307, 611, 3671, 13, 50914, 50914, 407, 4748, 5662, 341, 1729, 3364, 295, 341, 34090, 295, 341, 4583, 576, 652, 264, 4470, 352, 760, 13, 51314, 51314, 400, 321, 767, 362, 341, 1589, 337, 633, 2167, 472, 295, 527, 22027, 293, 439, 641, 9834, 13, 51514, 51514, 5135, 11, 309, 311, 3163, 1237, 412, 611, 264, 2642, 5893, 4470, 11, 538, 264, 636, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06976461410522461, "compression_ratio": 1.9372384937238494, "no_speech_prob": 9.080393283511512e-06}, {"id": 1343, "seek": 695000, "start": 6973.0, "end": 6978.0, "text": " Actually, it's worth looking at also the draw dot loss, by the way.", "tokens": [50364, 400, 370, 321, 536, 300, 570, 341, 16235, 510, 322, 341, 1729, 3364, 295, 341, 1729, 34090, 295, 341, 1729, 4583, 307, 3671, 11, 321, 536, 300, 1080, 6503, 322, 264, 4470, 307, 611, 3671, 13, 50914, 50914, 407, 4748, 5662, 341, 1729, 3364, 295, 341, 34090, 295, 341, 4583, 576, 652, 264, 4470, 352, 760, 13, 51314, 51314, 400, 321, 767, 362, 341, 1589, 337, 633, 2167, 472, 295, 527, 22027, 293, 439, 641, 9834, 13, 51514, 51514, 5135, 11, 309, 311, 3163, 1237, 412, 611, 264, 2642, 5893, 4470, 11, 538, 264, 636, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06976461410522461, "compression_ratio": 1.9372384937238494, "no_speech_prob": 9.080393283511512e-06}, {"id": 1344, "seek": 697800, "start": 6978.0, "end": 6984.0, "text": " So previously we looked at the draw dot of a single neuron forward pass, and that was already a large expression.", "tokens": [50364, 407, 8046, 321, 2956, 412, 264, 2642, 5893, 295, 257, 2167, 34090, 2128, 1320, 11, 293, 300, 390, 1217, 257, 2416, 6114, 13, 50664, 50664, 583, 437, 307, 341, 6114, 30, 492, 767, 2128, 292, 633, 472, 295, 729, 1451, 5110, 11, 293, 550, 321, 362, 264, 4470, 322, 1192, 295, 552, 365, 264, 914, 8889, 6713, 13, 51114, 51114, 400, 370, 341, 307, 257, 534, 5994, 4295, 570, 341, 4295, 300, 321, 600, 3094, 493, 586, 13, 51464, 51464, 876, 11, 452, 6502, 11, 341, 4295, 300, 321, 600, 3094, 493, 586, 11, 597, 307, 733, 295, 22704, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10061985712784988, "compression_ratio": 1.701195219123506, "no_speech_prob": 1.3006908375245985e-05}, {"id": 1345, "seek": 697800, "start": 6984.0, "end": 6993.0, "text": " But what is this expression? We actually forwarded every one of those four examples, and then we have the loss on top of them with the mean squared error.", "tokens": [50364, 407, 8046, 321, 2956, 412, 264, 2642, 5893, 295, 257, 2167, 34090, 2128, 1320, 11, 293, 300, 390, 1217, 257, 2416, 6114, 13, 50664, 50664, 583, 437, 307, 341, 6114, 30, 492, 767, 2128, 292, 633, 472, 295, 729, 1451, 5110, 11, 293, 550, 321, 362, 264, 4470, 322, 1192, 295, 552, 365, 264, 914, 8889, 6713, 13, 51114, 51114, 400, 370, 341, 307, 257, 534, 5994, 4295, 570, 341, 4295, 300, 321, 600, 3094, 493, 586, 13, 51464, 51464, 876, 11, 452, 6502, 11, 341, 4295, 300, 321, 600, 3094, 493, 586, 11, 597, 307, 733, 295, 22704, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10061985712784988, "compression_ratio": 1.701195219123506, "no_speech_prob": 1.3006908375245985e-05}, {"id": 1346, "seek": 697800, "start": 6993.0, "end": 7000.0, "text": " And so this is a really massive graph because this graph that we've built up now.", "tokens": [50364, 407, 8046, 321, 2956, 412, 264, 2642, 5893, 295, 257, 2167, 34090, 2128, 1320, 11, 293, 300, 390, 1217, 257, 2416, 6114, 13, 50664, 50664, 583, 437, 307, 341, 6114, 30, 492, 767, 2128, 292, 633, 472, 295, 729, 1451, 5110, 11, 293, 550, 321, 362, 264, 4470, 322, 1192, 295, 552, 365, 264, 914, 8889, 6713, 13, 51114, 51114, 400, 370, 341, 307, 257, 534, 5994, 4295, 570, 341, 4295, 300, 321, 600, 3094, 493, 586, 13, 51464, 51464, 876, 11, 452, 6502, 11, 341, 4295, 300, 321, 600, 3094, 493, 586, 11, 597, 307, 733, 295, 22704, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10061985712784988, "compression_ratio": 1.701195219123506, "no_speech_prob": 1.3006908375245985e-05}, {"id": 1347, "seek": 697800, "start": 7000.0, "end": 7005.0, "text": " Oh, my gosh, this graph that we've built up now, which is kind of excessive.", "tokens": [50364, 407, 8046, 321, 2956, 412, 264, 2642, 5893, 295, 257, 2167, 34090, 2128, 1320, 11, 293, 300, 390, 1217, 257, 2416, 6114, 13, 50664, 50664, 583, 437, 307, 341, 6114, 30, 492, 767, 2128, 292, 633, 472, 295, 729, 1451, 5110, 11, 293, 550, 321, 362, 264, 4470, 322, 1192, 295, 552, 365, 264, 914, 8889, 6713, 13, 51114, 51114, 400, 370, 341, 307, 257, 534, 5994, 4295, 570, 341, 4295, 300, 321, 600, 3094, 493, 586, 13, 51464, 51464, 876, 11, 452, 6502, 11, 341, 4295, 300, 321, 600, 3094, 493, 586, 11, 597, 307, 733, 295, 22704, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.10061985712784988, "compression_ratio": 1.701195219123506, "no_speech_prob": 1.3006908375245985e-05}, {"id": 1348, "seek": 700500, "start": 7005.0, "end": 7009.0, "text": " It's excessive because it has four forward passes of a neural net for every one of the examples.", "tokens": [50364, 467, 311, 22704, 570, 309, 575, 1451, 2128, 11335, 295, 257, 18161, 2533, 337, 633, 472, 295, 264, 5110, 13, 50564, 50564, 400, 550, 309, 575, 264, 4470, 322, 1192, 293, 309, 5314, 365, 264, 2158, 295, 264, 4470, 11, 597, 390, 1614, 13, 4762, 13, 50864, 50864, 400, 341, 4470, 486, 586, 646, 48256, 807, 439, 264, 1451, 2128, 11335, 439, 264, 636, 807, 445, 633, 2167, 19376, 2158, 295, 264, 18161, 2533, 11, 51264, 51264, 439, 264, 636, 646, 281, 11, 295, 1164, 11, 264, 9834, 295, 264, 17443, 11, 597, 366, 264, 4846, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09189372487587504, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.577946136421815e-06}, {"id": 1349, "seek": 700500, "start": 7009.0, "end": 7015.0, "text": " And then it has the loss on top and it ends with the value of the loss, which was 7.12.", "tokens": [50364, 467, 311, 22704, 570, 309, 575, 1451, 2128, 11335, 295, 257, 18161, 2533, 337, 633, 472, 295, 264, 5110, 13, 50564, 50564, 400, 550, 309, 575, 264, 4470, 322, 1192, 293, 309, 5314, 365, 264, 2158, 295, 264, 4470, 11, 597, 390, 1614, 13, 4762, 13, 50864, 50864, 400, 341, 4470, 486, 586, 646, 48256, 807, 439, 264, 1451, 2128, 11335, 439, 264, 636, 807, 445, 633, 2167, 19376, 2158, 295, 264, 18161, 2533, 11, 51264, 51264, 439, 264, 636, 646, 281, 11, 295, 1164, 11, 264, 9834, 295, 264, 17443, 11, 597, 366, 264, 4846, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09189372487587504, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.577946136421815e-06}, {"id": 1350, "seek": 700500, "start": 7015.0, "end": 7023.0, "text": " And this loss will now back propagate through all the four forward passes all the way through just every single intermediate value of the neural net,", "tokens": [50364, 467, 311, 22704, 570, 309, 575, 1451, 2128, 11335, 295, 257, 18161, 2533, 337, 633, 472, 295, 264, 5110, 13, 50564, 50564, 400, 550, 309, 575, 264, 4470, 322, 1192, 293, 309, 5314, 365, 264, 2158, 295, 264, 4470, 11, 597, 390, 1614, 13, 4762, 13, 50864, 50864, 400, 341, 4470, 486, 586, 646, 48256, 807, 439, 264, 1451, 2128, 11335, 439, 264, 636, 807, 445, 633, 2167, 19376, 2158, 295, 264, 18161, 2533, 11, 51264, 51264, 439, 264, 636, 646, 281, 11, 295, 1164, 11, 264, 9834, 295, 264, 17443, 11, 597, 366, 264, 4846, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09189372487587504, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.577946136421815e-06}, {"id": 1351, "seek": 700500, "start": 7023.0, "end": 7028.0, "text": " all the way back to, of course, the parameters of the weights, which are the input.", "tokens": [50364, 467, 311, 22704, 570, 309, 575, 1451, 2128, 11335, 295, 257, 18161, 2533, 337, 633, 472, 295, 264, 5110, 13, 50564, 50564, 400, 550, 309, 575, 264, 4470, 322, 1192, 293, 309, 5314, 365, 264, 2158, 295, 264, 4470, 11, 597, 390, 1614, 13, 4762, 13, 50864, 50864, 400, 341, 4470, 486, 586, 646, 48256, 807, 439, 264, 1451, 2128, 11335, 439, 264, 636, 807, 445, 633, 2167, 19376, 2158, 295, 264, 18161, 2533, 11, 51264, 51264, 439, 264, 636, 646, 281, 11, 295, 1164, 11, 264, 9834, 295, 264, 17443, 11, 597, 366, 264, 4846, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09189372487587504, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.577946136421815e-06}, {"id": 1352, "seek": 702800, "start": 7028.0, "end": 7037.0, "text": " So these weight parameters here are inputs to this neural net and these numbers here, these scalars are inputs to the neural net.", "tokens": [50364, 407, 613, 3364, 9834, 510, 366, 15743, 281, 341, 18161, 2533, 293, 613, 3547, 510, 11, 613, 15664, 685, 366, 15743, 281, 264, 18161, 2533, 13, 50814, 50814, 407, 498, 321, 1437, 926, 510, 11, 321, 486, 1391, 915, 512, 295, 613, 5110, 11, 341, 502, 13, 15, 11, 7263, 1310, 341, 502, 13, 15, 420, 512, 295, 264, 2357, 13, 51264, 51264, 400, 291, 603, 536, 300, 436, 439, 362, 2771, 2448, 382, 731, 13, 51414, 51414, 440, 551, 307, 11, 613, 2771, 2448, 322, 264, 4846, 1412, 366, 406, 300, 4420, 281, 505, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07292885780334472, "compression_ratio": 1.7647058823529411, "no_speech_prob": 6.144034614408156e-06}, {"id": 1353, "seek": 702800, "start": 7037.0, "end": 7046.0, "text": " So if we went around here, we will probably find some of these examples, this 1.0, potentially maybe this 1.0 or some of the others.", "tokens": [50364, 407, 613, 3364, 9834, 510, 366, 15743, 281, 341, 18161, 2533, 293, 613, 3547, 510, 11, 613, 15664, 685, 366, 15743, 281, 264, 18161, 2533, 13, 50814, 50814, 407, 498, 321, 1437, 926, 510, 11, 321, 486, 1391, 915, 512, 295, 613, 5110, 11, 341, 502, 13, 15, 11, 7263, 1310, 341, 502, 13, 15, 420, 512, 295, 264, 2357, 13, 51264, 51264, 400, 291, 603, 536, 300, 436, 439, 362, 2771, 2448, 382, 731, 13, 51414, 51414, 440, 551, 307, 11, 613, 2771, 2448, 322, 264, 4846, 1412, 366, 406, 300, 4420, 281, 505, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07292885780334472, "compression_ratio": 1.7647058823529411, "no_speech_prob": 6.144034614408156e-06}, {"id": 1354, "seek": 702800, "start": 7046.0, "end": 7049.0, "text": " And you'll see that they all have gradients as well.", "tokens": [50364, 407, 613, 3364, 9834, 510, 366, 15743, 281, 341, 18161, 2533, 293, 613, 3547, 510, 11, 613, 15664, 685, 366, 15743, 281, 264, 18161, 2533, 13, 50814, 50814, 407, 498, 321, 1437, 926, 510, 11, 321, 486, 1391, 915, 512, 295, 613, 5110, 11, 341, 502, 13, 15, 11, 7263, 1310, 341, 502, 13, 15, 420, 512, 295, 264, 2357, 13, 51264, 51264, 400, 291, 603, 536, 300, 436, 439, 362, 2771, 2448, 382, 731, 13, 51414, 51414, 440, 551, 307, 11, 613, 2771, 2448, 322, 264, 4846, 1412, 366, 406, 300, 4420, 281, 505, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07292885780334472, "compression_ratio": 1.7647058823529411, "no_speech_prob": 6.144034614408156e-06}, {"id": 1355, "seek": 702800, "start": 7049.0, "end": 7053.0, "text": " The thing is, these gradients on the input data are not that useful to us.", "tokens": [50364, 407, 613, 3364, 9834, 510, 366, 15743, 281, 341, 18161, 2533, 293, 613, 3547, 510, 11, 613, 15664, 685, 366, 15743, 281, 264, 18161, 2533, 13, 50814, 50814, 407, 498, 321, 1437, 926, 510, 11, 321, 486, 1391, 915, 512, 295, 613, 5110, 11, 341, 502, 13, 15, 11, 7263, 1310, 341, 502, 13, 15, 420, 512, 295, 264, 2357, 13, 51264, 51264, 400, 291, 603, 536, 300, 436, 439, 362, 2771, 2448, 382, 731, 13, 51414, 51414, 440, 551, 307, 11, 613, 2771, 2448, 322, 264, 4846, 1412, 366, 406, 300, 4420, 281, 505, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07292885780334472, "compression_ratio": 1.7647058823529411, "no_speech_prob": 6.144034614408156e-06}, {"id": 1356, "seek": 705300, "start": 7053.0, "end": 7058.0, "text": " And that's because the input data seems to be not changeable.", "tokens": [50364, 400, 300, 311, 570, 264, 4846, 1412, 2544, 281, 312, 406, 1319, 712, 13, 50614, 50614, 467, 311, 257, 2212, 281, 264, 1154, 13, 400, 370, 309, 311, 257, 6806, 4846, 13, 50764, 50764, 492, 434, 406, 516, 281, 312, 4473, 309, 420, 23258, 365, 309, 11, 754, 1673, 321, 360, 362, 2771, 2448, 337, 309, 13, 51014, 51014, 583, 512, 295, 613, 2771, 2448, 510, 486, 312, 337, 264, 18161, 3209, 9834, 11, 264, 343, 82, 293, 264, 363, 82, 13, 51364, 51364, 400, 729, 11, 295, 1164, 11, 321, 528, 281, 1319, 13, 51514, 51514, 2264, 11, 370, 586, 321, 434, 516, 281, 528, 512, 19283, 3089, 281, 5448, 493, 439, 264, 9834, 295, 264, 18161, 2533, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06249756929351062, "compression_ratio": 1.7413127413127414, "no_speech_prob": 5.9550170590227935e-06}, {"id": 1357, "seek": 705300, "start": 7058.0, "end": 7061.0, "text": " It's a given to the problem. And so it's a fixed input.", "tokens": [50364, 400, 300, 311, 570, 264, 4846, 1412, 2544, 281, 312, 406, 1319, 712, 13, 50614, 50614, 467, 311, 257, 2212, 281, 264, 1154, 13, 400, 370, 309, 311, 257, 6806, 4846, 13, 50764, 50764, 492, 434, 406, 516, 281, 312, 4473, 309, 420, 23258, 365, 309, 11, 754, 1673, 321, 360, 362, 2771, 2448, 337, 309, 13, 51014, 51014, 583, 512, 295, 613, 2771, 2448, 510, 486, 312, 337, 264, 18161, 3209, 9834, 11, 264, 343, 82, 293, 264, 363, 82, 13, 51364, 51364, 400, 729, 11, 295, 1164, 11, 321, 528, 281, 1319, 13, 51514, 51514, 2264, 11, 370, 586, 321, 434, 516, 281, 528, 512, 19283, 3089, 281, 5448, 493, 439, 264, 9834, 295, 264, 18161, 2533, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06249756929351062, "compression_ratio": 1.7413127413127414, "no_speech_prob": 5.9550170590227935e-06}, {"id": 1358, "seek": 705300, "start": 7061.0, "end": 7066.0, "text": " We're not going to be changing it or messing with it, even though we do have gradients for it.", "tokens": [50364, 400, 300, 311, 570, 264, 4846, 1412, 2544, 281, 312, 406, 1319, 712, 13, 50614, 50614, 467, 311, 257, 2212, 281, 264, 1154, 13, 400, 370, 309, 311, 257, 6806, 4846, 13, 50764, 50764, 492, 434, 406, 516, 281, 312, 4473, 309, 420, 23258, 365, 309, 11, 754, 1673, 321, 360, 362, 2771, 2448, 337, 309, 13, 51014, 51014, 583, 512, 295, 613, 2771, 2448, 510, 486, 312, 337, 264, 18161, 3209, 9834, 11, 264, 343, 82, 293, 264, 363, 82, 13, 51364, 51364, 400, 729, 11, 295, 1164, 11, 321, 528, 281, 1319, 13, 51514, 51514, 2264, 11, 370, 586, 321, 434, 516, 281, 528, 512, 19283, 3089, 281, 5448, 493, 439, 264, 9834, 295, 264, 18161, 2533, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06249756929351062, "compression_ratio": 1.7413127413127414, "no_speech_prob": 5.9550170590227935e-06}, {"id": 1359, "seek": 705300, "start": 7066.0, "end": 7073.0, "text": " But some of these gradients here will be for the neural network parameters, the Ws and the Bs.", "tokens": [50364, 400, 300, 311, 570, 264, 4846, 1412, 2544, 281, 312, 406, 1319, 712, 13, 50614, 50614, 467, 311, 257, 2212, 281, 264, 1154, 13, 400, 370, 309, 311, 257, 6806, 4846, 13, 50764, 50764, 492, 434, 406, 516, 281, 312, 4473, 309, 420, 23258, 365, 309, 11, 754, 1673, 321, 360, 362, 2771, 2448, 337, 309, 13, 51014, 51014, 583, 512, 295, 613, 2771, 2448, 510, 486, 312, 337, 264, 18161, 3209, 9834, 11, 264, 343, 82, 293, 264, 363, 82, 13, 51364, 51364, 400, 729, 11, 295, 1164, 11, 321, 528, 281, 1319, 13, 51514, 51514, 2264, 11, 370, 586, 321, 434, 516, 281, 528, 512, 19283, 3089, 281, 5448, 493, 439, 264, 9834, 295, 264, 18161, 2533, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06249756929351062, "compression_ratio": 1.7413127413127414, "no_speech_prob": 5.9550170590227935e-06}, {"id": 1360, "seek": 705300, "start": 7073.0, "end": 7076.0, "text": " And those, of course, we want to change.", "tokens": [50364, 400, 300, 311, 570, 264, 4846, 1412, 2544, 281, 312, 406, 1319, 712, 13, 50614, 50614, 467, 311, 257, 2212, 281, 264, 1154, 13, 400, 370, 309, 311, 257, 6806, 4846, 13, 50764, 50764, 492, 434, 406, 516, 281, 312, 4473, 309, 420, 23258, 365, 309, 11, 754, 1673, 321, 360, 362, 2771, 2448, 337, 309, 13, 51014, 51014, 583, 512, 295, 613, 2771, 2448, 510, 486, 312, 337, 264, 18161, 3209, 9834, 11, 264, 343, 82, 293, 264, 363, 82, 13, 51364, 51364, 400, 729, 11, 295, 1164, 11, 321, 528, 281, 1319, 13, 51514, 51514, 2264, 11, 370, 586, 321, 434, 516, 281, 528, 512, 19283, 3089, 281, 5448, 493, 439, 264, 9834, 295, 264, 18161, 2533, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06249756929351062, "compression_ratio": 1.7413127413127414, "no_speech_prob": 5.9550170590227935e-06}, {"id": 1361, "seek": 705300, "start": 7076.0, "end": 7081.0, "text": " OK, so now we're going to want some convenience code to gather up all the parameters of the neural net", "tokens": [50364, 400, 300, 311, 570, 264, 4846, 1412, 2544, 281, 312, 406, 1319, 712, 13, 50614, 50614, 467, 311, 257, 2212, 281, 264, 1154, 13, 400, 370, 309, 311, 257, 6806, 4846, 13, 50764, 50764, 492, 434, 406, 516, 281, 312, 4473, 309, 420, 23258, 365, 309, 11, 754, 1673, 321, 360, 362, 2771, 2448, 337, 309, 13, 51014, 51014, 583, 512, 295, 613, 2771, 2448, 510, 486, 312, 337, 264, 18161, 3209, 9834, 11, 264, 343, 82, 293, 264, 363, 82, 13, 51364, 51364, 400, 729, 11, 295, 1164, 11, 321, 528, 281, 1319, 13, 51514, 51514, 2264, 11, 370, 586, 321, 434, 516, 281, 528, 512, 19283, 3089, 281, 5448, 493, 439, 264, 9834, 295, 264, 18161, 2533, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06249756929351062, "compression_ratio": 1.7413127413127414, "no_speech_prob": 5.9550170590227935e-06}, {"id": 1362, "seek": 708100, "start": 7081.0, "end": 7084.0, "text": " so that we can operate on all of them simultaneously.", "tokens": [50364, 370, 300, 321, 393, 9651, 322, 439, 295, 552, 16561, 13, 50514, 50514, 400, 633, 472, 295, 552, 11, 321, 486, 297, 16032, 257, 5870, 2372, 2361, 322, 264, 16235, 1589, 13, 50814, 50814, 407, 718, 311, 2500, 264, 9834, 295, 264, 18161, 2533, 439, 294, 472, 10225, 13, 51014, 51014, 407, 718, 311, 1884, 257, 9834, 295, 2698, 300, 445, 11247, 2698, 13, 86, 11, 597, 307, 257, 1329, 11, 1588, 7186, 770, 365, 257, 1329, 295, 2698, 13, 65, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0848765539568524, "compression_ratio": 1.6313131313131313, "no_speech_prob": 2.144448626495432e-05}, {"id": 1363, "seek": 708100, "start": 7084.0, "end": 7090.0, "text": " And every one of them, we will nudge a tiny amount based on the gradient information.", "tokens": [50364, 370, 300, 321, 393, 9651, 322, 439, 295, 552, 16561, 13, 50514, 50514, 400, 633, 472, 295, 552, 11, 321, 486, 297, 16032, 257, 5870, 2372, 2361, 322, 264, 16235, 1589, 13, 50814, 50814, 407, 718, 311, 2500, 264, 9834, 295, 264, 18161, 2533, 439, 294, 472, 10225, 13, 51014, 51014, 407, 718, 311, 1884, 257, 9834, 295, 2698, 300, 445, 11247, 2698, 13, 86, 11, 597, 307, 257, 1329, 11, 1588, 7186, 770, 365, 257, 1329, 295, 2698, 13, 65, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0848765539568524, "compression_ratio": 1.6313131313131313, "no_speech_prob": 2.144448626495432e-05}, {"id": 1364, "seek": 708100, "start": 7090.0, "end": 7094.0, "text": " So let's collect the parameters of the neural net all in one array.", "tokens": [50364, 370, 300, 321, 393, 9651, 322, 439, 295, 552, 16561, 13, 50514, 50514, 400, 633, 472, 295, 552, 11, 321, 486, 297, 16032, 257, 5870, 2372, 2361, 322, 264, 16235, 1589, 13, 50814, 50814, 407, 718, 311, 2500, 264, 9834, 295, 264, 18161, 2533, 439, 294, 472, 10225, 13, 51014, 51014, 407, 718, 311, 1884, 257, 9834, 295, 2698, 300, 445, 11247, 2698, 13, 86, 11, 597, 307, 257, 1329, 11, 1588, 7186, 770, 365, 257, 1329, 295, 2698, 13, 65, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0848765539568524, "compression_ratio": 1.6313131313131313, "no_speech_prob": 2.144448626495432e-05}, {"id": 1365, "seek": 708100, "start": 7094.0, "end": 7107.0, "text": " So let's create a parameters of self that just returns self.w, which is a list, concatenated with a list of self.b.", "tokens": [50364, 370, 300, 321, 393, 9651, 322, 439, 295, 552, 16561, 13, 50514, 50514, 400, 633, 472, 295, 552, 11, 321, 486, 297, 16032, 257, 5870, 2372, 2361, 322, 264, 16235, 1589, 13, 50814, 50814, 407, 718, 311, 2500, 264, 9834, 295, 264, 18161, 2533, 439, 294, 472, 10225, 13, 51014, 51014, 407, 718, 311, 1884, 257, 9834, 295, 2698, 300, 445, 11247, 2698, 13, 86, 11, 597, 307, 257, 1329, 11, 1588, 7186, 770, 365, 257, 1329, 295, 2698, 13, 65, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0848765539568524, "compression_ratio": 1.6313131313131313, "no_speech_prob": 2.144448626495432e-05}, {"id": 1366, "seek": 710700, "start": 7107.0, "end": 7112.0, "text": " So this will just return a list. List plus list just gives you a list.", "tokens": [50364, 407, 341, 486, 445, 2736, 257, 1329, 13, 17668, 1804, 1329, 445, 2709, 291, 257, 1329, 13, 50614, 50614, 407, 300, 311, 9834, 295, 34090, 13, 50714, 50714, 400, 286, 478, 5141, 309, 341, 636, 570, 611, 9953, 51, 284, 339, 575, 9834, 322, 633, 2167, 426, 45, 10088, 13, 51014, 51014, 400, 309, 775, 2293, 437, 321, 434, 884, 510, 13, 51114, 51114, 467, 445, 11247, 264, 13075, 10688, 830, 337, 505, 382, 264, 13075, 15664, 685, 13, 51414, 51414, 823, 4583, 307, 611, 257, 10088, 11, 370, 309, 486, 362, 9834, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09445934879536531, "compression_ratio": 1.6805555555555556, "no_speech_prob": 6.240742095542373e-06}, {"id": 1367, "seek": 710700, "start": 7112.0, "end": 7114.0, "text": " So that's parameters of neuron.", "tokens": [50364, 407, 341, 486, 445, 2736, 257, 1329, 13, 17668, 1804, 1329, 445, 2709, 291, 257, 1329, 13, 50614, 50614, 407, 300, 311, 9834, 295, 34090, 13, 50714, 50714, 400, 286, 478, 5141, 309, 341, 636, 570, 611, 9953, 51, 284, 339, 575, 9834, 322, 633, 2167, 426, 45, 10088, 13, 51014, 51014, 400, 309, 775, 2293, 437, 321, 434, 884, 510, 13, 51114, 51114, 467, 445, 11247, 264, 13075, 10688, 830, 337, 505, 382, 264, 13075, 15664, 685, 13, 51414, 51414, 823, 4583, 307, 611, 257, 10088, 11, 370, 309, 486, 362, 9834, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09445934879536531, "compression_ratio": 1.6805555555555556, "no_speech_prob": 6.240742095542373e-06}, {"id": 1368, "seek": 710700, "start": 7114.0, "end": 7120.0, "text": " And I'm calling it this way because also PyTorch has parameters on every single NN module.", "tokens": [50364, 407, 341, 486, 445, 2736, 257, 1329, 13, 17668, 1804, 1329, 445, 2709, 291, 257, 1329, 13, 50614, 50614, 407, 300, 311, 9834, 295, 34090, 13, 50714, 50714, 400, 286, 478, 5141, 309, 341, 636, 570, 611, 9953, 51, 284, 339, 575, 9834, 322, 633, 2167, 426, 45, 10088, 13, 51014, 51014, 400, 309, 775, 2293, 437, 321, 434, 884, 510, 13, 51114, 51114, 467, 445, 11247, 264, 13075, 10688, 830, 337, 505, 382, 264, 13075, 15664, 685, 13, 51414, 51414, 823, 4583, 307, 611, 257, 10088, 11, 370, 309, 486, 362, 9834, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09445934879536531, "compression_ratio": 1.6805555555555556, "no_speech_prob": 6.240742095542373e-06}, {"id": 1369, "seek": 710700, "start": 7120.0, "end": 7122.0, "text": " And it does exactly what we're doing here.", "tokens": [50364, 407, 341, 486, 445, 2736, 257, 1329, 13, 17668, 1804, 1329, 445, 2709, 291, 257, 1329, 13, 50614, 50614, 407, 300, 311, 9834, 295, 34090, 13, 50714, 50714, 400, 286, 478, 5141, 309, 341, 636, 570, 611, 9953, 51, 284, 339, 575, 9834, 322, 633, 2167, 426, 45, 10088, 13, 51014, 51014, 400, 309, 775, 2293, 437, 321, 434, 884, 510, 13, 51114, 51114, 467, 445, 11247, 264, 13075, 10688, 830, 337, 505, 382, 264, 13075, 15664, 685, 13, 51414, 51414, 823, 4583, 307, 611, 257, 10088, 11, 370, 309, 486, 362, 9834, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09445934879536531, "compression_ratio": 1.6805555555555556, "no_speech_prob": 6.240742095542373e-06}, {"id": 1370, "seek": 710700, "start": 7122.0, "end": 7128.0, "text": " It just returns the parameter tensors for us as the parameter scalars.", "tokens": [50364, 407, 341, 486, 445, 2736, 257, 1329, 13, 17668, 1804, 1329, 445, 2709, 291, 257, 1329, 13, 50614, 50614, 407, 300, 311, 9834, 295, 34090, 13, 50714, 50714, 400, 286, 478, 5141, 309, 341, 636, 570, 611, 9953, 51, 284, 339, 575, 9834, 322, 633, 2167, 426, 45, 10088, 13, 51014, 51014, 400, 309, 775, 2293, 437, 321, 434, 884, 510, 13, 51114, 51114, 467, 445, 11247, 264, 13075, 10688, 830, 337, 505, 382, 264, 13075, 15664, 685, 13, 51414, 51414, 823, 4583, 307, 611, 257, 10088, 11, 370, 309, 486, 362, 9834, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09445934879536531, "compression_ratio": 1.6805555555555556, "no_speech_prob": 6.240742095542373e-06}, {"id": 1371, "seek": 710700, "start": 7128.0, "end": 7134.0, "text": " Now layer is also a module, so it will have parameters.", "tokens": [50364, 407, 341, 486, 445, 2736, 257, 1329, 13, 17668, 1804, 1329, 445, 2709, 291, 257, 1329, 13, 50614, 50614, 407, 300, 311, 9834, 295, 34090, 13, 50714, 50714, 400, 286, 478, 5141, 309, 341, 636, 570, 611, 9953, 51, 284, 339, 575, 9834, 322, 633, 2167, 426, 45, 10088, 13, 51014, 51014, 400, 309, 775, 2293, 437, 321, 434, 884, 510, 13, 51114, 51114, 467, 445, 11247, 264, 13075, 10688, 830, 337, 505, 382, 264, 13075, 15664, 685, 13, 51414, 51414, 823, 4583, 307, 611, 257, 10088, 11, 370, 309, 486, 362, 9834, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09445934879536531, "compression_ratio": 1.6805555555555556, "no_speech_prob": 6.240742095542373e-06}, {"id": 1372, "seek": 713400, "start": 7134.0, "end": 7140.0, "text": " And basically what we want to do here is something like this.", "tokens": [50364, 400, 1936, 437, 321, 528, 281, 360, 510, 307, 746, 411, 341, 13, 50664, 50664, 3457, 4070, 307, 510, 13, 400, 550, 337, 34090, 294, 5139, 13, 716, 374, 892, 11, 321, 528, 281, 483, 34090, 13, 2181, 335, 6202, 13, 51164, 51164, 400, 321, 528, 281, 971, 4070, 13, 3828, 521, 13, 51364, 51364, 407, 613, 366, 264, 9834, 295, 341, 34090, 13, 51514, 51514, 400, 550, 321, 528, 281, 829, 552, 322, 1192, 295, 971, 4070, 13, 407, 971, 4070, 13, 3828, 521, 295, 2522, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1334237119425898, "compression_ratio": 1.8271604938271604, "no_speech_prob": 3.647763514891267e-05}, {"id": 1373, "seek": 713400, "start": 7140.0, "end": 7150.0, "text": " Params is here. And then for neuron in salt.neurons, we want to get neuron.parameters.", "tokens": [50364, 400, 1936, 437, 321, 528, 281, 360, 510, 307, 746, 411, 341, 13, 50664, 50664, 3457, 4070, 307, 510, 13, 400, 550, 337, 34090, 294, 5139, 13, 716, 374, 892, 11, 321, 528, 281, 483, 34090, 13, 2181, 335, 6202, 13, 51164, 51164, 400, 321, 528, 281, 971, 4070, 13, 3828, 521, 13, 51364, 51364, 407, 613, 366, 264, 9834, 295, 341, 34090, 13, 51514, 51514, 400, 550, 321, 528, 281, 829, 552, 322, 1192, 295, 971, 4070, 13, 407, 971, 4070, 13, 3828, 521, 295, 2522, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1334237119425898, "compression_ratio": 1.8271604938271604, "no_speech_prob": 3.647763514891267e-05}, {"id": 1374, "seek": 713400, "start": 7150.0, "end": 7154.0, "text": " And we want to params.extend.", "tokens": [50364, 400, 1936, 437, 321, 528, 281, 360, 510, 307, 746, 411, 341, 13, 50664, 50664, 3457, 4070, 307, 510, 13, 400, 550, 337, 34090, 294, 5139, 13, 716, 374, 892, 11, 321, 528, 281, 483, 34090, 13, 2181, 335, 6202, 13, 51164, 51164, 400, 321, 528, 281, 971, 4070, 13, 3828, 521, 13, 51364, 51364, 407, 613, 366, 264, 9834, 295, 341, 34090, 13, 51514, 51514, 400, 550, 321, 528, 281, 829, 552, 322, 1192, 295, 971, 4070, 13, 407, 971, 4070, 13, 3828, 521, 295, 2522, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1334237119425898, "compression_ratio": 1.8271604938271604, "no_speech_prob": 3.647763514891267e-05}, {"id": 1375, "seek": 713400, "start": 7154.0, "end": 7157.0, "text": " So these are the parameters of this neuron.", "tokens": [50364, 400, 1936, 437, 321, 528, 281, 360, 510, 307, 746, 411, 341, 13, 50664, 50664, 3457, 4070, 307, 510, 13, 400, 550, 337, 34090, 294, 5139, 13, 716, 374, 892, 11, 321, 528, 281, 483, 34090, 13, 2181, 335, 6202, 13, 51164, 51164, 400, 321, 528, 281, 971, 4070, 13, 3828, 521, 13, 51364, 51364, 407, 613, 366, 264, 9834, 295, 341, 34090, 13, 51514, 51514, 400, 550, 321, 528, 281, 829, 552, 322, 1192, 295, 971, 4070, 13, 407, 971, 4070, 13, 3828, 521, 295, 2522, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1334237119425898, "compression_ratio": 1.8271604938271604, "no_speech_prob": 3.647763514891267e-05}, {"id": 1376, "seek": 713400, "start": 7157.0, "end": 7162.0, "text": " And then we want to put them on top of params. So params.extend of piece.", "tokens": [50364, 400, 1936, 437, 321, 528, 281, 360, 510, 307, 746, 411, 341, 13, 50664, 50664, 3457, 4070, 307, 510, 13, 400, 550, 337, 34090, 294, 5139, 13, 716, 374, 892, 11, 321, 528, 281, 483, 34090, 13, 2181, 335, 6202, 13, 51164, 51164, 400, 321, 528, 281, 971, 4070, 13, 3828, 521, 13, 51364, 51364, 407, 613, 366, 264, 9834, 295, 341, 34090, 13, 51514, 51514, 400, 550, 321, 528, 281, 829, 552, 322, 1192, 295, 971, 4070, 13, 407, 971, 4070, 13, 3828, 521, 295, 2522, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1334237119425898, "compression_ratio": 1.8271604938271604, "no_speech_prob": 3.647763514891267e-05}, {"id": 1377, "seek": 716200, "start": 7162.0, "end": 7165.0, "text": " And then when I return params.", "tokens": [50364, 400, 550, 562, 286, 2736, 971, 4070, 13, 50514, 50514, 407, 341, 307, 636, 886, 709, 3089, 13, 407, 767, 456, 311, 257, 636, 281, 20460, 341, 11, 597, 307, 2736, 280, 337, 34090, 294, 2698, 13, 716, 374, 892, 337, 280, 294, 34090, 13, 2181, 335, 6202, 13, 51514, 51514, 407, 309, 311, 257, 2167, 1329, 44991, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12656260293627541, "compression_ratio": 1.4861111111111112, "no_speech_prob": 1.8631351849762723e-05}, {"id": 1378, "seek": 716200, "start": 7165.0, "end": 7185.0, "text": " So this is way too much code. So actually there's a way to simplify this, which is return p for neuron in self.neurons for p in neuron.parameters.", "tokens": [50364, 400, 550, 562, 286, 2736, 971, 4070, 13, 50514, 50514, 407, 341, 307, 636, 886, 709, 3089, 13, 407, 767, 456, 311, 257, 636, 281, 20460, 341, 11, 597, 307, 2736, 280, 337, 34090, 294, 2698, 13, 716, 374, 892, 337, 280, 294, 34090, 13, 2181, 335, 6202, 13, 51514, 51514, 407, 309, 311, 257, 2167, 1329, 44991, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12656260293627541, "compression_ratio": 1.4861111111111112, "no_speech_prob": 1.8631351849762723e-05}, {"id": 1379, "seek": 716200, "start": 7185.0, "end": 7187.0, "text": " So it's a single list comprehension.", "tokens": [50364, 400, 550, 562, 286, 2736, 971, 4070, 13, 50514, 50514, 407, 341, 307, 636, 886, 709, 3089, 13, 407, 767, 456, 311, 257, 636, 281, 20460, 341, 11, 597, 307, 2736, 280, 337, 34090, 294, 2698, 13, 716, 374, 892, 337, 280, 294, 34090, 13, 2181, 335, 6202, 13, 51514, 51514, 407, 309, 311, 257, 2167, 1329, 44991, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12656260293627541, "compression_ratio": 1.4861111111111112, "no_speech_prob": 1.8631351849762723e-05}, {"id": 1380, "seek": 718700, "start": 7187.0, "end": 7195.0, "text": " In Python, you can sort of nest them like this and you can then create the desired array.", "tokens": [50364, 682, 15329, 11, 291, 393, 1333, 295, 15646, 552, 411, 341, 293, 291, 393, 550, 1884, 264, 14721, 10225, 13, 50764, 50764, 407, 613, 366, 14800, 13, 492, 393, 747, 341, 484, 13, 51014, 51014, 400, 550, 718, 311, 360, 264, 912, 510, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.09585060675938924, "compression_ratio": 1.3884297520661157, "no_speech_prob": 8.349010022357106e-05}, {"id": 1381, "seek": 718700, "start": 7195.0, "end": 7200.0, "text": " So these are identical. We can take this out.", "tokens": [50364, 682, 15329, 11, 291, 393, 1333, 295, 15646, 552, 411, 341, 293, 291, 393, 550, 1884, 264, 14721, 10225, 13, 50764, 50764, 407, 613, 366, 14800, 13, 492, 393, 747, 341, 484, 13, 51014, 51014, 400, 550, 718, 311, 360, 264, 912, 510, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.09585060675938924, "compression_ratio": 1.3884297520661157, "no_speech_prob": 8.349010022357106e-05}, {"id": 1382, "seek": 718700, "start": 7200.0, "end": 7205.0, "text": " And then let's do the same here.", "tokens": [50364, 682, 15329, 11, 291, 393, 1333, 295, 15646, 552, 411, 341, 293, 291, 393, 550, 1884, 264, 14721, 10225, 13, 50764, 50764, 407, 613, 366, 14800, 13, 492, 393, 747, 341, 484, 13, 51014, 51014, 400, 550, 718, 311, 360, 264, 912, 510, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.09585060675938924, "compression_ratio": 1.3884297520661157, "no_speech_prob": 8.349010022357106e-05}, {"id": 1383, "seek": 720500, "start": 7205.0, "end": 7221.0, "text": " So we have parameters. And return a parameter for layer in self.layers for p in layer.parameters.", "tokens": [50364, 407, 321, 362, 9834, 13, 400, 2736, 257, 13075, 337, 4583, 294, 2698, 13, 8376, 433, 337, 280, 294, 4583, 13, 2181, 335, 6202, 13, 51164, 51164, 400, 300, 820, 312, 665, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.3322157215427708, "compression_ratio": 1.3555555555555556, "no_speech_prob": 2.111151843564585e-05}, {"id": 1384, "seek": 720500, "start": 7221.0, "end": 7224.0, "text": " And that should be good.", "tokens": [50364, 407, 321, 362, 9834, 13, 400, 2736, 257, 13075, 337, 4583, 294, 2698, 13, 8376, 433, 337, 280, 294, 4583, 13, 2181, 335, 6202, 13, 51164, 51164, 400, 300, 820, 312, 665, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.3322157215427708, "compression_ratio": 1.3555555555555556, "no_speech_prob": 2.111151843564585e-05}, {"id": 1385, "seek": 722400, "start": 7224.0, "end": 7236.0, "text": " Now let me pop out this so we don't reinitialize our network because we need to reinitialize our...", "tokens": [50364, 823, 718, 385, 1665, 484, 341, 370, 321, 500, 380, 6561, 270, 831, 1125, 527, 3209, 570, 321, 643, 281, 6561, 270, 831, 1125, 527, 485, 50964, 50964, 2264, 11, 370, 7015, 321, 486, 362, 281, 1391, 6561, 270, 831, 1125, 264, 3209, 570, 321, 445, 632, 14980, 13, 51264, 51264, 1436, 341, 1508, 11, 295, 1164, 11, 286, 528, 281, 483, 439, 264, 917, 13, 2181, 335, 6202, 11, 457, 300, 311, 406, 516, 281, 589, 570, 341, 307, 264, 1331, 1508, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13932134888388895, "compression_ratio": 1.678391959798995, "no_speech_prob": 4.757526039611548e-05}, {"id": 1386, "seek": 722400, "start": 7236.0, "end": 7242.0, "text": " OK, so unfortunately we will have to probably reinitialize the network because we just had functionality.", "tokens": [50364, 823, 718, 385, 1665, 484, 341, 370, 321, 500, 380, 6561, 270, 831, 1125, 527, 3209, 570, 321, 643, 281, 6561, 270, 831, 1125, 527, 485, 50964, 50964, 2264, 11, 370, 7015, 321, 486, 362, 281, 1391, 6561, 270, 831, 1125, 264, 3209, 570, 321, 445, 632, 14980, 13, 51264, 51264, 1436, 341, 1508, 11, 295, 1164, 11, 286, 528, 281, 483, 439, 264, 917, 13, 2181, 335, 6202, 11, 457, 300, 311, 406, 516, 281, 589, 570, 341, 307, 264, 1331, 1508, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13932134888388895, "compression_ratio": 1.678391959798995, "no_speech_prob": 4.757526039611548e-05}, {"id": 1387, "seek": 722400, "start": 7242.0, "end": 7251.0, "text": " Because this class, of course, I want to get all the end.parameters, but that's not going to work because this is the old class.", "tokens": [50364, 823, 718, 385, 1665, 484, 341, 370, 321, 500, 380, 6561, 270, 831, 1125, 527, 3209, 570, 321, 643, 281, 6561, 270, 831, 1125, 527, 485, 50964, 50964, 2264, 11, 370, 7015, 321, 486, 362, 281, 1391, 6561, 270, 831, 1125, 264, 3209, 570, 321, 445, 632, 14980, 13, 51264, 51264, 1436, 341, 1508, 11, 295, 1164, 11, 286, 528, 281, 483, 439, 264, 917, 13, 2181, 335, 6202, 11, 457, 300, 311, 406, 516, 281, 589, 570, 341, 307, 264, 1331, 1508, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13932134888388895, "compression_ratio": 1.678391959798995, "no_speech_prob": 4.757526039611548e-05}, {"id": 1388, "seek": 725100, "start": 7251.0, "end": 7256.0, "text": " So unfortunately we do have to reinitialize the network, which will change some of the numbers.", "tokens": [50364, 407, 7015, 321, 360, 362, 281, 6561, 270, 831, 1125, 264, 3209, 11, 597, 486, 1319, 512, 295, 264, 3547, 13, 50614, 50614, 583, 718, 385, 360, 300, 370, 300, 321, 1888, 493, 264, 777, 9362, 13, 492, 393, 586, 360, 917, 13, 2181, 335, 6202, 13, 50814, 50814, 400, 613, 366, 439, 264, 17443, 293, 32152, 1854, 264, 2302, 18161, 2533, 13, 51114, 51114, 407, 294, 3217, 11, 341, 21601, 47, 575, 18173, 9834, 13, 51414, 51414, 400, 586, 321, 603, 312, 1075, 281, 1319, 552, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06372559588888417, "compression_ratio": 1.4954128440366972, "no_speech_prob": 4.8323970986530185e-05}, {"id": 1389, "seek": 725100, "start": 7256.0, "end": 7260.0, "text": " But let me do that so that we pick up the new API. We can now do end.parameters.", "tokens": [50364, 407, 7015, 321, 360, 362, 281, 6561, 270, 831, 1125, 264, 3209, 11, 597, 486, 1319, 512, 295, 264, 3547, 13, 50614, 50614, 583, 718, 385, 360, 300, 370, 300, 321, 1888, 493, 264, 777, 9362, 13, 492, 393, 586, 360, 917, 13, 2181, 335, 6202, 13, 50814, 50814, 400, 613, 366, 439, 264, 17443, 293, 32152, 1854, 264, 2302, 18161, 2533, 13, 51114, 51114, 407, 294, 3217, 11, 341, 21601, 47, 575, 18173, 9834, 13, 51414, 51414, 400, 586, 321, 603, 312, 1075, 281, 1319, 552, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06372559588888417, "compression_ratio": 1.4954128440366972, "no_speech_prob": 4.8323970986530185e-05}, {"id": 1390, "seek": 725100, "start": 7260.0, "end": 7266.0, "text": " And these are all the weights and biases inside the entire neural net.", "tokens": [50364, 407, 7015, 321, 360, 362, 281, 6561, 270, 831, 1125, 264, 3209, 11, 597, 486, 1319, 512, 295, 264, 3547, 13, 50614, 50614, 583, 718, 385, 360, 300, 370, 300, 321, 1888, 493, 264, 777, 9362, 13, 492, 393, 586, 360, 917, 13, 2181, 335, 6202, 13, 50814, 50814, 400, 613, 366, 439, 264, 17443, 293, 32152, 1854, 264, 2302, 18161, 2533, 13, 51114, 51114, 407, 294, 3217, 11, 341, 21601, 47, 575, 18173, 9834, 13, 51414, 51414, 400, 586, 321, 603, 312, 1075, 281, 1319, 552, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06372559588888417, "compression_ratio": 1.4954128440366972, "no_speech_prob": 4.8323970986530185e-05}, {"id": 1391, "seek": 725100, "start": 7266.0, "end": 7272.0, "text": " So in total, this MLP has 41 parameters.", "tokens": [50364, 407, 7015, 321, 360, 362, 281, 6561, 270, 831, 1125, 264, 3209, 11, 597, 486, 1319, 512, 295, 264, 3547, 13, 50614, 50614, 583, 718, 385, 360, 300, 370, 300, 321, 1888, 493, 264, 777, 9362, 13, 492, 393, 586, 360, 917, 13, 2181, 335, 6202, 13, 50814, 50814, 400, 613, 366, 439, 264, 17443, 293, 32152, 1854, 264, 2302, 18161, 2533, 13, 51114, 51114, 407, 294, 3217, 11, 341, 21601, 47, 575, 18173, 9834, 13, 51414, 51414, 400, 586, 321, 603, 312, 1075, 281, 1319, 552, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06372559588888417, "compression_ratio": 1.4954128440366972, "no_speech_prob": 4.8323970986530185e-05}, {"id": 1392, "seek": 725100, "start": 7272.0, "end": 7276.0, "text": " And now we'll be able to change them.", "tokens": [50364, 407, 7015, 321, 360, 362, 281, 6561, 270, 831, 1125, 264, 3209, 11, 597, 486, 1319, 512, 295, 264, 3547, 13, 50614, 50614, 583, 718, 385, 360, 300, 370, 300, 321, 1888, 493, 264, 777, 9362, 13, 492, 393, 586, 360, 917, 13, 2181, 335, 6202, 13, 50814, 50814, 400, 613, 366, 439, 264, 17443, 293, 32152, 1854, 264, 2302, 18161, 2533, 13, 51114, 51114, 407, 294, 3217, 11, 341, 21601, 47, 575, 18173, 9834, 13, 51414, 51414, 400, 586, 321, 603, 312, 1075, 281, 1319, 552, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06372559588888417, "compression_ratio": 1.4954128440366972, "no_speech_prob": 4.8323970986530185e-05}, {"id": 1393, "seek": 727600, "start": 7276.0, "end": 7287.0, "text": " So if we recalculate the loss here, we see that unfortunately we have slightly different predictions and slightly different loss.", "tokens": [50364, 407, 498, 321, 850, 304, 2444, 473, 264, 4470, 510, 11, 321, 536, 300, 7015, 321, 362, 4748, 819, 21264, 293, 4748, 819, 4470, 13, 50914, 50914, 583, 300, 311, 2264, 13, 51014, 51014, 2264, 11, 370, 321, 536, 300, 341, 22027, 16235, 307, 4748, 3671, 13, 51214, 51214, 492, 393, 611, 574, 412, 1080, 1412, 558, 586, 11, 597, 307, 1958, 13, 19287, 13, 51464, 51464, 407, 341, 307, 264, 2190, 2158, 295, 341, 34090, 293, 341, 307, 1080, 16235, 322, 264, 4470, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08385839355125856, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.834256727306638e-05}, {"id": 1394, "seek": 727600, "start": 7287.0, "end": 7289.0, "text": " But that's OK.", "tokens": [50364, 407, 498, 321, 850, 304, 2444, 473, 264, 4470, 510, 11, 321, 536, 300, 7015, 321, 362, 4748, 819, 21264, 293, 4748, 819, 4470, 13, 50914, 50914, 583, 300, 311, 2264, 13, 51014, 51014, 2264, 11, 370, 321, 536, 300, 341, 22027, 16235, 307, 4748, 3671, 13, 51214, 51214, 492, 393, 611, 574, 412, 1080, 1412, 558, 586, 11, 597, 307, 1958, 13, 19287, 13, 51464, 51464, 407, 341, 307, 264, 2190, 2158, 295, 341, 34090, 293, 341, 307, 1080, 16235, 322, 264, 4470, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08385839355125856, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.834256727306638e-05}, {"id": 1395, "seek": 727600, "start": 7289.0, "end": 7293.0, "text": " OK, so we see that this neurons gradient is slightly negative.", "tokens": [50364, 407, 498, 321, 850, 304, 2444, 473, 264, 4470, 510, 11, 321, 536, 300, 7015, 321, 362, 4748, 819, 21264, 293, 4748, 819, 4470, 13, 50914, 50914, 583, 300, 311, 2264, 13, 51014, 51014, 2264, 11, 370, 321, 536, 300, 341, 22027, 16235, 307, 4748, 3671, 13, 51214, 51214, 492, 393, 611, 574, 412, 1080, 1412, 558, 586, 11, 597, 307, 1958, 13, 19287, 13, 51464, 51464, 407, 341, 307, 264, 2190, 2158, 295, 341, 34090, 293, 341, 307, 1080, 16235, 322, 264, 4470, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08385839355125856, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.834256727306638e-05}, {"id": 1396, "seek": 727600, "start": 7293.0, "end": 7298.0, "text": " We can also look at its data right now, which is 0.85.", "tokens": [50364, 407, 498, 321, 850, 304, 2444, 473, 264, 4470, 510, 11, 321, 536, 300, 7015, 321, 362, 4748, 819, 21264, 293, 4748, 819, 4470, 13, 50914, 50914, 583, 300, 311, 2264, 13, 51014, 51014, 2264, 11, 370, 321, 536, 300, 341, 22027, 16235, 307, 4748, 3671, 13, 51214, 51214, 492, 393, 611, 574, 412, 1080, 1412, 558, 586, 11, 597, 307, 1958, 13, 19287, 13, 51464, 51464, 407, 341, 307, 264, 2190, 2158, 295, 341, 34090, 293, 341, 307, 1080, 16235, 322, 264, 4470, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08385839355125856, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.834256727306638e-05}, {"id": 1397, "seek": 727600, "start": 7298.0, "end": 7303.0, "text": " So this is the current value of this neuron and this is its gradient on the loss.", "tokens": [50364, 407, 498, 321, 850, 304, 2444, 473, 264, 4470, 510, 11, 321, 536, 300, 7015, 321, 362, 4748, 819, 21264, 293, 4748, 819, 4470, 13, 50914, 50914, 583, 300, 311, 2264, 13, 51014, 51014, 2264, 11, 370, 321, 536, 300, 341, 22027, 16235, 307, 4748, 3671, 13, 51214, 51214, 492, 393, 611, 574, 412, 1080, 1412, 558, 586, 11, 597, 307, 1958, 13, 19287, 13, 51464, 51464, 407, 341, 307, 264, 2190, 2158, 295, 341, 34090, 293, 341, 307, 1080, 16235, 322, 264, 4470, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08385839355125856, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.834256727306638e-05}, {"id": 1398, "seek": 730300, "start": 7303.0, "end": 7309.0, "text": " So what we want to do now is we want to iterate for every p in end.parameters.", "tokens": [50364, 407, 437, 321, 528, 281, 360, 586, 307, 321, 528, 281, 44497, 337, 633, 280, 294, 917, 13, 2181, 335, 6202, 13, 50664, 50664, 407, 337, 439, 264, 18173, 9834, 294, 341, 18161, 2533, 11, 321, 767, 528, 281, 1319, 280, 13, 67, 3274, 4748, 4650, 281, 264, 16235, 1589, 13, 51164, 51164, 2264, 11, 370, 5893, 5893, 5893, 281, 360, 510, 13, 51314, 51314, 583, 341, 486, 312, 1936, 257, 5870, 5623, 294, 341, 16235, 23475, 12232, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08682028644056206, "compression_ratio": 1.6185567010309279, "no_speech_prob": 2.144359314115718e-05}, {"id": 1399, "seek": 730300, "start": 7309.0, "end": 7319.0, "text": " So for all the 41 parameters in this neural net, we actually want to change p.data slightly according to the gradient information.", "tokens": [50364, 407, 437, 321, 528, 281, 360, 586, 307, 321, 528, 281, 44497, 337, 633, 280, 294, 917, 13, 2181, 335, 6202, 13, 50664, 50664, 407, 337, 439, 264, 18173, 9834, 294, 341, 18161, 2533, 11, 321, 767, 528, 281, 1319, 280, 13, 67, 3274, 4748, 4650, 281, 264, 16235, 1589, 13, 51164, 51164, 2264, 11, 370, 5893, 5893, 5893, 281, 360, 510, 13, 51314, 51314, 583, 341, 486, 312, 1936, 257, 5870, 5623, 294, 341, 16235, 23475, 12232, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08682028644056206, "compression_ratio": 1.6185567010309279, "no_speech_prob": 2.144359314115718e-05}, {"id": 1400, "seek": 730300, "start": 7319.0, "end": 7322.0, "text": " OK, so dot dot dot to do here.", "tokens": [50364, 407, 437, 321, 528, 281, 360, 586, 307, 321, 528, 281, 44497, 337, 633, 280, 294, 917, 13, 2181, 335, 6202, 13, 50664, 50664, 407, 337, 439, 264, 18173, 9834, 294, 341, 18161, 2533, 11, 321, 767, 528, 281, 1319, 280, 13, 67, 3274, 4748, 4650, 281, 264, 16235, 1589, 13, 51164, 51164, 2264, 11, 370, 5893, 5893, 5893, 281, 360, 510, 13, 51314, 51314, 583, 341, 486, 312, 1936, 257, 5870, 5623, 294, 341, 16235, 23475, 12232, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08682028644056206, "compression_ratio": 1.6185567010309279, "no_speech_prob": 2.144359314115718e-05}, {"id": 1401, "seek": 730300, "start": 7322.0, "end": 7328.0, "text": " But this will be basically a tiny update in this gradient descent scheme.", "tokens": [50364, 407, 437, 321, 528, 281, 360, 586, 307, 321, 528, 281, 44497, 337, 633, 280, 294, 917, 13, 2181, 335, 6202, 13, 50664, 50664, 407, 337, 439, 264, 18173, 9834, 294, 341, 18161, 2533, 11, 321, 767, 528, 281, 1319, 280, 13, 67, 3274, 4748, 4650, 281, 264, 16235, 1589, 13, 51164, 51164, 2264, 11, 370, 5893, 5893, 5893, 281, 360, 510, 13, 51314, 51314, 583, 341, 486, 312, 1936, 257, 5870, 5623, 294, 341, 16235, 23475, 12232, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08682028644056206, "compression_ratio": 1.6185567010309279, "no_speech_prob": 2.144359314115718e-05}, {"id": 1402, "seek": 732800, "start": 7328.0, "end": 7339.0, "text": " And gradient descent, we are thinking of the gradient as a vector pointing in the direction of increased loss.", "tokens": [50364, 400, 16235, 23475, 11, 321, 366, 1953, 295, 264, 16235, 382, 257, 8062, 12166, 294, 264, 3513, 295, 6505, 4470, 13, 50914, 50914, 400, 370, 294, 16235, 23475, 11, 321, 366, 42626, 280, 13, 67, 3274, 538, 257, 1359, 1823, 2744, 294, 264, 3513, 295, 264, 16235, 13, 51314, 51314, 407, 264, 1823, 2744, 11, 382, 364, 1365, 11, 727, 312, 411, 257, 588, 1359, 1230, 11, 411, 1958, 13, 10607, 307, 264, 1823, 2744, 11, 1413, 280, 13, 7165, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08156629450180951, "compression_ratio": 1.8066298342541436, "no_speech_prob": 1.5936055206111632e-05}, {"id": 1403, "seek": 732800, "start": 7339.0, "end": 7347.0, "text": " And so in gradient descent, we are modifying p.data by a small step size in the direction of the gradient.", "tokens": [50364, 400, 16235, 23475, 11, 321, 366, 1953, 295, 264, 16235, 382, 257, 8062, 12166, 294, 264, 3513, 295, 6505, 4470, 13, 50914, 50914, 400, 370, 294, 16235, 23475, 11, 321, 366, 42626, 280, 13, 67, 3274, 538, 257, 1359, 1823, 2744, 294, 264, 3513, 295, 264, 16235, 13, 51314, 51314, 407, 264, 1823, 2744, 11, 382, 364, 1365, 11, 727, 312, 411, 257, 588, 1359, 1230, 11, 411, 1958, 13, 10607, 307, 264, 1823, 2744, 11, 1413, 280, 13, 7165, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08156629450180951, "compression_ratio": 1.8066298342541436, "no_speech_prob": 1.5936055206111632e-05}, {"id": 1404, "seek": 732800, "start": 7347.0, "end": 7356.0, "text": " So the step size, as an example, could be like a very small number, like 0.01 is the step size, times p.grad.", "tokens": [50364, 400, 16235, 23475, 11, 321, 366, 1953, 295, 264, 16235, 382, 257, 8062, 12166, 294, 264, 3513, 295, 6505, 4470, 13, 50914, 50914, 400, 370, 294, 16235, 23475, 11, 321, 366, 42626, 280, 13, 67, 3274, 538, 257, 1359, 1823, 2744, 294, 264, 3513, 295, 264, 16235, 13, 51314, 51314, 407, 264, 1823, 2744, 11, 382, 364, 1365, 11, 727, 312, 411, 257, 588, 1359, 1230, 11, 411, 1958, 13, 10607, 307, 264, 1823, 2744, 11, 1413, 280, 13, 7165, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08156629450180951, "compression_ratio": 1.8066298342541436, "no_speech_prob": 1.5936055206111632e-05}, {"id": 1405, "seek": 735600, "start": 7356.0, "end": 7359.0, "text": " But we have to think through some of the signs here.", "tokens": [50364, 583, 321, 362, 281, 519, 807, 512, 295, 264, 7880, 510, 13, 50514, 50514, 407, 294, 1729, 11, 1364, 365, 341, 2685, 1365, 510, 11, 321, 536, 300, 498, 321, 445, 1411, 309, 411, 341, 11, 50914, 50914, 550, 341, 22027, 2158, 576, 312, 4362, 6505, 538, 257, 5870, 2372, 295, 264, 16235, 13, 51214, 51214, 440, 16235, 307, 3671, 13, 407, 341, 2158, 295, 341, 34090, 576, 352, 4748, 760, 13, 51464, 51464, 467, 576, 1813, 411, 1958, 13, 25494, 420, 746, 411, 300, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06732845306396484, "compression_ratio": 1.64, "no_speech_prob": 1.1659208212222438e-05}, {"id": 1406, "seek": 735600, "start": 7359.0, "end": 7367.0, "text": " So in particular, working with this specific example here, we see that if we just left it like this,", "tokens": [50364, 583, 321, 362, 281, 519, 807, 512, 295, 264, 7880, 510, 13, 50514, 50514, 407, 294, 1729, 11, 1364, 365, 341, 2685, 1365, 510, 11, 321, 536, 300, 498, 321, 445, 1411, 309, 411, 341, 11, 50914, 50914, 550, 341, 22027, 2158, 576, 312, 4362, 6505, 538, 257, 5870, 2372, 295, 264, 16235, 13, 51214, 51214, 440, 16235, 307, 3671, 13, 407, 341, 2158, 295, 341, 34090, 576, 352, 4748, 760, 13, 51464, 51464, 467, 576, 1813, 411, 1958, 13, 25494, 420, 746, 411, 300, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06732845306396484, "compression_ratio": 1.64, "no_speech_prob": 1.1659208212222438e-05}, {"id": 1407, "seek": 735600, "start": 7367.0, "end": 7373.0, "text": " then this neurons value would be currently increased by a tiny amount of the gradient.", "tokens": [50364, 583, 321, 362, 281, 519, 807, 512, 295, 264, 7880, 510, 13, 50514, 50514, 407, 294, 1729, 11, 1364, 365, 341, 2685, 1365, 510, 11, 321, 536, 300, 498, 321, 445, 1411, 309, 411, 341, 11, 50914, 50914, 550, 341, 22027, 2158, 576, 312, 4362, 6505, 538, 257, 5870, 2372, 295, 264, 16235, 13, 51214, 51214, 440, 16235, 307, 3671, 13, 407, 341, 2158, 295, 341, 34090, 576, 352, 4748, 760, 13, 51464, 51464, 467, 576, 1813, 411, 1958, 13, 25494, 420, 746, 411, 300, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06732845306396484, "compression_ratio": 1.64, "no_speech_prob": 1.1659208212222438e-05}, {"id": 1408, "seek": 735600, "start": 7373.0, "end": 7378.0, "text": " The gradient is negative. So this value of this neuron would go slightly down.", "tokens": [50364, 583, 321, 362, 281, 519, 807, 512, 295, 264, 7880, 510, 13, 50514, 50514, 407, 294, 1729, 11, 1364, 365, 341, 2685, 1365, 510, 11, 321, 536, 300, 498, 321, 445, 1411, 309, 411, 341, 11, 50914, 50914, 550, 341, 22027, 2158, 576, 312, 4362, 6505, 538, 257, 5870, 2372, 295, 264, 16235, 13, 51214, 51214, 440, 16235, 307, 3671, 13, 407, 341, 2158, 295, 341, 34090, 576, 352, 4748, 760, 13, 51464, 51464, 467, 576, 1813, 411, 1958, 13, 25494, 420, 746, 411, 300, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06732845306396484, "compression_ratio": 1.64, "no_speech_prob": 1.1659208212222438e-05}, {"id": 1409, "seek": 735600, "start": 7378.0, "end": 7383.0, "text": " It would become like 0.84 or something like that.", "tokens": [50364, 583, 321, 362, 281, 519, 807, 512, 295, 264, 7880, 510, 13, 50514, 50514, 407, 294, 1729, 11, 1364, 365, 341, 2685, 1365, 510, 11, 321, 536, 300, 498, 321, 445, 1411, 309, 411, 341, 11, 50914, 50914, 550, 341, 22027, 2158, 576, 312, 4362, 6505, 538, 257, 5870, 2372, 295, 264, 16235, 13, 51214, 51214, 440, 16235, 307, 3671, 13, 407, 341, 2158, 295, 341, 34090, 576, 352, 4748, 760, 13, 51464, 51464, 467, 576, 1813, 411, 1958, 13, 25494, 420, 746, 411, 300, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06732845306396484, "compression_ratio": 1.64, "no_speech_prob": 1.1659208212222438e-05}, {"id": 1410, "seek": 738300, "start": 7383.0, "end": 7391.0, "text": " But if this neurons value goes lower, that would actually increase the loss.", "tokens": [50364, 583, 498, 341, 22027, 2158, 1709, 3126, 11, 300, 576, 767, 3488, 264, 4470, 13, 50764, 50764, 663, 311, 570, 264, 13760, 295, 341, 34090, 307, 3671, 13, 50964, 50964, 407, 5662, 341, 1669, 264, 4470, 352, 760, 13, 51164, 51164, 407, 5662, 309, 307, 437, 321, 528, 281, 360, 2602, 295, 23223, 309, 13, 51364, 51364, 407, 1936, 437, 321, 434, 5361, 510, 307, 321, 434, 767, 5361, 257, 3671, 1465, 13, 51514, 51514, 400, 797, 11, 341, 661, 14174, 11, 293, 300, 311, 570, 321, 528, 281, 17522, 264, 4470, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055072032299238384, "compression_ratio": 1.911214953271028, "no_speech_prob": 5.682371920556761e-06}, {"id": 1411, "seek": 738300, "start": 7391.0, "end": 7395.0, "text": " That's because the derivative of this neuron is negative.", "tokens": [50364, 583, 498, 341, 22027, 2158, 1709, 3126, 11, 300, 576, 767, 3488, 264, 4470, 13, 50764, 50764, 663, 311, 570, 264, 13760, 295, 341, 34090, 307, 3671, 13, 50964, 50964, 407, 5662, 341, 1669, 264, 4470, 352, 760, 13, 51164, 51164, 407, 5662, 309, 307, 437, 321, 528, 281, 360, 2602, 295, 23223, 309, 13, 51364, 51364, 407, 1936, 437, 321, 434, 5361, 510, 307, 321, 434, 767, 5361, 257, 3671, 1465, 13, 51514, 51514, 400, 797, 11, 341, 661, 14174, 11, 293, 300, 311, 570, 321, 528, 281, 17522, 264, 4470, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055072032299238384, "compression_ratio": 1.911214953271028, "no_speech_prob": 5.682371920556761e-06}, {"id": 1412, "seek": 738300, "start": 7395.0, "end": 7399.0, "text": " So increasing this makes the loss go down.", "tokens": [50364, 583, 498, 341, 22027, 2158, 1709, 3126, 11, 300, 576, 767, 3488, 264, 4470, 13, 50764, 50764, 663, 311, 570, 264, 13760, 295, 341, 34090, 307, 3671, 13, 50964, 50964, 407, 5662, 341, 1669, 264, 4470, 352, 760, 13, 51164, 51164, 407, 5662, 309, 307, 437, 321, 528, 281, 360, 2602, 295, 23223, 309, 13, 51364, 51364, 407, 1936, 437, 321, 434, 5361, 510, 307, 321, 434, 767, 5361, 257, 3671, 1465, 13, 51514, 51514, 400, 797, 11, 341, 661, 14174, 11, 293, 300, 311, 570, 321, 528, 281, 17522, 264, 4470, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055072032299238384, "compression_ratio": 1.911214953271028, "no_speech_prob": 5.682371920556761e-06}, {"id": 1413, "seek": 738300, "start": 7399.0, "end": 7403.0, "text": " So increasing it is what we want to do instead of decreasing it.", "tokens": [50364, 583, 498, 341, 22027, 2158, 1709, 3126, 11, 300, 576, 767, 3488, 264, 4470, 13, 50764, 50764, 663, 311, 570, 264, 13760, 295, 341, 34090, 307, 3671, 13, 50964, 50964, 407, 5662, 341, 1669, 264, 4470, 352, 760, 13, 51164, 51164, 407, 5662, 309, 307, 437, 321, 528, 281, 360, 2602, 295, 23223, 309, 13, 51364, 51364, 407, 1936, 437, 321, 434, 5361, 510, 307, 321, 434, 767, 5361, 257, 3671, 1465, 13, 51514, 51514, 400, 797, 11, 341, 661, 14174, 11, 293, 300, 311, 570, 321, 528, 281, 17522, 264, 4470, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055072032299238384, "compression_ratio": 1.911214953271028, "no_speech_prob": 5.682371920556761e-06}, {"id": 1414, "seek": 738300, "start": 7403.0, "end": 7406.0, "text": " So basically what we're missing here is we're actually missing a negative sign.", "tokens": [50364, 583, 498, 341, 22027, 2158, 1709, 3126, 11, 300, 576, 767, 3488, 264, 4470, 13, 50764, 50764, 663, 311, 570, 264, 13760, 295, 341, 34090, 307, 3671, 13, 50964, 50964, 407, 5662, 341, 1669, 264, 4470, 352, 760, 13, 51164, 51164, 407, 5662, 309, 307, 437, 321, 528, 281, 360, 2602, 295, 23223, 309, 13, 51364, 51364, 407, 1936, 437, 321, 434, 5361, 510, 307, 321, 434, 767, 5361, 257, 3671, 1465, 13, 51514, 51514, 400, 797, 11, 341, 661, 14174, 11, 293, 300, 311, 570, 321, 528, 281, 17522, 264, 4470, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055072032299238384, "compression_ratio": 1.911214953271028, "no_speech_prob": 5.682371920556761e-06}, {"id": 1415, "seek": 738300, "start": 7406.0, "end": 7411.0, "text": " And again, this other interpretation, and that's because we want to minimize the loss.", "tokens": [50364, 583, 498, 341, 22027, 2158, 1709, 3126, 11, 300, 576, 767, 3488, 264, 4470, 13, 50764, 50764, 663, 311, 570, 264, 13760, 295, 341, 34090, 307, 3671, 13, 50964, 50964, 407, 5662, 341, 1669, 264, 4470, 352, 760, 13, 51164, 51164, 407, 5662, 309, 307, 437, 321, 528, 281, 360, 2602, 295, 23223, 309, 13, 51364, 51364, 407, 1936, 437, 321, 434, 5361, 510, 307, 321, 434, 767, 5361, 257, 3671, 1465, 13, 51514, 51514, 400, 797, 11, 341, 661, 14174, 11, 293, 300, 311, 570, 321, 528, 281, 17522, 264, 4470, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055072032299238384, "compression_ratio": 1.911214953271028, "no_speech_prob": 5.682371920556761e-06}, {"id": 1416, "seek": 741100, "start": 7411.0, "end": 7414.0, "text": " We don't want to maximize the loss. We want to decrease it.", "tokens": [50364, 492, 500, 380, 528, 281, 19874, 264, 4470, 13, 492, 528, 281, 11514, 309, 13, 50514, 50514, 400, 264, 661, 14174, 11, 382, 286, 2835, 11, 307, 291, 393, 519, 295, 264, 16235, 8062, 13, 50714, 50714, 407, 1936, 445, 264, 8062, 295, 439, 264, 2771, 2448, 382, 12166, 294, 264, 3513, 295, 5662, 264, 4470, 13, 51064, 51064, 583, 550, 321, 528, 281, 11514, 309, 13, 407, 321, 767, 528, 281, 352, 294, 264, 6182, 3513, 13, 51264, 51264, 400, 370, 291, 393, 13447, 1803, 300, 341, 775, 264, 558, 551, 510, 365, 257, 3671, 11, 570, 321, 528, 281, 17522, 264, 4470, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07748889048165138, "compression_ratio": 1.8658536585365855, "no_speech_prob": 4.3999381887260824e-05}, {"id": 1417, "seek": 741100, "start": 7414.0, "end": 7418.0, "text": " And the other interpretation, as I mentioned, is you can think of the gradient vector.", "tokens": [50364, 492, 500, 380, 528, 281, 19874, 264, 4470, 13, 492, 528, 281, 11514, 309, 13, 50514, 50514, 400, 264, 661, 14174, 11, 382, 286, 2835, 11, 307, 291, 393, 519, 295, 264, 16235, 8062, 13, 50714, 50714, 407, 1936, 445, 264, 8062, 295, 439, 264, 2771, 2448, 382, 12166, 294, 264, 3513, 295, 5662, 264, 4470, 13, 51064, 51064, 583, 550, 321, 528, 281, 11514, 309, 13, 407, 321, 767, 528, 281, 352, 294, 264, 6182, 3513, 13, 51264, 51264, 400, 370, 291, 393, 13447, 1803, 300, 341, 775, 264, 558, 551, 510, 365, 257, 3671, 11, 570, 321, 528, 281, 17522, 264, 4470, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07748889048165138, "compression_ratio": 1.8658536585365855, "no_speech_prob": 4.3999381887260824e-05}, {"id": 1418, "seek": 741100, "start": 7418.0, "end": 7425.0, "text": " So basically just the vector of all the gradients as pointing in the direction of increasing the loss.", "tokens": [50364, 492, 500, 380, 528, 281, 19874, 264, 4470, 13, 492, 528, 281, 11514, 309, 13, 50514, 50514, 400, 264, 661, 14174, 11, 382, 286, 2835, 11, 307, 291, 393, 519, 295, 264, 16235, 8062, 13, 50714, 50714, 407, 1936, 445, 264, 8062, 295, 439, 264, 2771, 2448, 382, 12166, 294, 264, 3513, 295, 5662, 264, 4470, 13, 51064, 51064, 583, 550, 321, 528, 281, 11514, 309, 13, 407, 321, 767, 528, 281, 352, 294, 264, 6182, 3513, 13, 51264, 51264, 400, 370, 291, 393, 13447, 1803, 300, 341, 775, 264, 558, 551, 510, 365, 257, 3671, 11, 570, 321, 528, 281, 17522, 264, 4470, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07748889048165138, "compression_ratio": 1.8658536585365855, "no_speech_prob": 4.3999381887260824e-05}, {"id": 1419, "seek": 741100, "start": 7425.0, "end": 7429.0, "text": " But then we want to decrease it. So we actually want to go in the opposite direction.", "tokens": [50364, 492, 500, 380, 528, 281, 19874, 264, 4470, 13, 492, 528, 281, 11514, 309, 13, 50514, 50514, 400, 264, 661, 14174, 11, 382, 286, 2835, 11, 307, 291, 393, 519, 295, 264, 16235, 8062, 13, 50714, 50714, 407, 1936, 445, 264, 8062, 295, 439, 264, 2771, 2448, 382, 12166, 294, 264, 3513, 295, 5662, 264, 4470, 13, 51064, 51064, 583, 550, 321, 528, 281, 11514, 309, 13, 407, 321, 767, 528, 281, 352, 294, 264, 6182, 3513, 13, 51264, 51264, 400, 370, 291, 393, 13447, 1803, 300, 341, 775, 264, 558, 551, 510, 365, 257, 3671, 11, 570, 321, 528, 281, 17522, 264, 4470, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07748889048165138, "compression_ratio": 1.8658536585365855, "no_speech_prob": 4.3999381887260824e-05}, {"id": 1420, "seek": 741100, "start": 7429.0, "end": 7435.0, "text": " And so you can convince yourself that this does the right thing here with a negative, because we want to minimize the loss.", "tokens": [50364, 492, 500, 380, 528, 281, 19874, 264, 4470, 13, 492, 528, 281, 11514, 309, 13, 50514, 50514, 400, 264, 661, 14174, 11, 382, 286, 2835, 11, 307, 291, 393, 519, 295, 264, 16235, 8062, 13, 50714, 50714, 407, 1936, 445, 264, 8062, 295, 439, 264, 2771, 2448, 382, 12166, 294, 264, 3513, 295, 5662, 264, 4470, 13, 51064, 51064, 583, 550, 321, 528, 281, 11514, 309, 13, 407, 321, 767, 528, 281, 352, 294, 264, 6182, 3513, 13, 51264, 51264, 400, 370, 291, 393, 13447, 1803, 300, 341, 775, 264, 558, 551, 510, 365, 257, 3671, 11, 570, 321, 528, 281, 17522, 264, 4470, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07748889048165138, "compression_ratio": 1.8658536585365855, "no_speech_prob": 4.3999381887260824e-05}, {"id": 1421, "seek": 743500, "start": 7435.0, "end": 7444.0, "text": " So if we notch all the parameters by tiny amount, then we'll see that this data will have changed a little bit.", "tokens": [50364, 407, 498, 321, 26109, 439, 264, 9834, 538, 5870, 2372, 11, 550, 321, 603, 536, 300, 341, 1412, 486, 362, 3105, 257, 707, 857, 13, 50814, 50814, 407, 586, 341, 34090, 307, 257, 5870, 2372, 5044, 2158, 13, 51064, 51064, 407, 1958, 13, 19287, 19, 1437, 281, 1958, 13, 23, 19004, 13, 51314, 51314, 400, 300, 311, 257, 665, 551, 11, 570, 4748, 5662, 341, 34090, 1412, 1669, 264, 4470, 352, 760, 4650, 281, 264, 16235, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08265263357280213, "compression_ratio": 1.5454545454545454, "no_speech_prob": 2.586682421679143e-05}, {"id": 1422, "seek": 743500, "start": 7444.0, "end": 7449.0, "text": " So now this neuron is a tiny amount greater value.", "tokens": [50364, 407, 498, 321, 26109, 439, 264, 9834, 538, 5870, 2372, 11, 550, 321, 603, 536, 300, 341, 1412, 486, 362, 3105, 257, 707, 857, 13, 50814, 50814, 407, 586, 341, 34090, 307, 257, 5870, 2372, 5044, 2158, 13, 51064, 51064, 407, 1958, 13, 19287, 19, 1437, 281, 1958, 13, 23, 19004, 13, 51314, 51314, 400, 300, 311, 257, 665, 551, 11, 570, 4748, 5662, 341, 34090, 1412, 1669, 264, 4470, 352, 760, 4650, 281, 264, 16235, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08265263357280213, "compression_ratio": 1.5454545454545454, "no_speech_prob": 2.586682421679143e-05}, {"id": 1423, "seek": 743500, "start": 7449.0, "end": 7454.0, "text": " So 0.854 went to 0.857.", "tokens": [50364, 407, 498, 321, 26109, 439, 264, 9834, 538, 5870, 2372, 11, 550, 321, 603, 536, 300, 341, 1412, 486, 362, 3105, 257, 707, 857, 13, 50814, 50814, 407, 586, 341, 34090, 307, 257, 5870, 2372, 5044, 2158, 13, 51064, 51064, 407, 1958, 13, 19287, 19, 1437, 281, 1958, 13, 23, 19004, 13, 51314, 51314, 400, 300, 311, 257, 665, 551, 11, 570, 4748, 5662, 341, 34090, 1412, 1669, 264, 4470, 352, 760, 4650, 281, 264, 16235, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08265263357280213, "compression_ratio": 1.5454545454545454, "no_speech_prob": 2.586682421679143e-05}, {"id": 1424, "seek": 743500, "start": 7454.0, "end": 7463.0, "text": " And that's a good thing, because slightly increasing this neuron data makes the loss go down according to the gradient.", "tokens": [50364, 407, 498, 321, 26109, 439, 264, 9834, 538, 5870, 2372, 11, 550, 321, 603, 536, 300, 341, 1412, 486, 362, 3105, 257, 707, 857, 13, 50814, 50814, 407, 586, 341, 34090, 307, 257, 5870, 2372, 5044, 2158, 13, 51064, 51064, 407, 1958, 13, 19287, 19, 1437, 281, 1958, 13, 23, 19004, 13, 51314, 51314, 400, 300, 311, 257, 665, 551, 11, 570, 4748, 5662, 341, 34090, 1412, 1669, 264, 4470, 352, 760, 4650, 281, 264, 16235, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08265263357280213, "compression_ratio": 1.5454545454545454, "no_speech_prob": 2.586682421679143e-05}, {"id": 1425, "seek": 746300, "start": 7463.0, "end": 7466.0, "text": " And so the correcting has happened sign-wise.", "tokens": [50364, 400, 370, 264, 47032, 575, 2011, 1465, 12, 3711, 13, 50514, 50514, 400, 370, 586, 437, 321, 576, 2066, 11, 295, 1164, 11, 307, 300, 570, 321, 600, 3105, 439, 613, 9834, 11, 321, 2066, 300, 264, 4470, 820, 362, 2780, 760, 257, 857, 13, 50964, 50964, 407, 321, 528, 281, 319, 12, 68, 3337, 10107, 264, 4470, 13, 961, 385, 1936, 485, 51214, 51214, 639, 307, 445, 257, 1412, 7123, 300, 6132, 380, 3105, 13, 51364, 51364, 583, 264, 2128, 1320, 510, 295, 264, 3209, 11, 321, 393, 850, 304, 2444, 473, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08447835883315728, "compression_ratio": 1.5555555555555556, "no_speech_prob": 9.223358574672602e-06}, {"id": 1426, "seek": 746300, "start": 7466.0, "end": 7475.0, "text": " And so now what we would expect, of course, is that because we've changed all these parameters, we expect that the loss should have gone down a bit.", "tokens": [50364, 400, 370, 264, 47032, 575, 2011, 1465, 12, 3711, 13, 50514, 50514, 400, 370, 586, 437, 321, 576, 2066, 11, 295, 1164, 11, 307, 300, 570, 321, 600, 3105, 439, 613, 9834, 11, 321, 2066, 300, 264, 4470, 820, 362, 2780, 760, 257, 857, 13, 50964, 50964, 407, 321, 528, 281, 319, 12, 68, 3337, 10107, 264, 4470, 13, 961, 385, 1936, 485, 51214, 51214, 639, 307, 445, 257, 1412, 7123, 300, 6132, 380, 3105, 13, 51364, 51364, 583, 264, 2128, 1320, 510, 295, 264, 3209, 11, 321, 393, 850, 304, 2444, 473, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08447835883315728, "compression_ratio": 1.5555555555555556, "no_speech_prob": 9.223358574672602e-06}, {"id": 1427, "seek": 746300, "start": 7475.0, "end": 7480.0, "text": " So we want to re-evaluate the loss. Let me basically...", "tokens": [50364, 400, 370, 264, 47032, 575, 2011, 1465, 12, 3711, 13, 50514, 50514, 400, 370, 586, 437, 321, 576, 2066, 11, 295, 1164, 11, 307, 300, 570, 321, 600, 3105, 439, 613, 9834, 11, 321, 2066, 300, 264, 4470, 820, 362, 2780, 760, 257, 857, 13, 50964, 50964, 407, 321, 528, 281, 319, 12, 68, 3337, 10107, 264, 4470, 13, 961, 385, 1936, 485, 51214, 51214, 639, 307, 445, 257, 1412, 7123, 300, 6132, 380, 3105, 13, 51364, 51364, 583, 264, 2128, 1320, 510, 295, 264, 3209, 11, 321, 393, 850, 304, 2444, 473, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08447835883315728, "compression_ratio": 1.5555555555555556, "no_speech_prob": 9.223358574672602e-06}, {"id": 1428, "seek": 746300, "start": 7480.0, "end": 7483.0, "text": " This is just a data definition that hasn't changed.", "tokens": [50364, 400, 370, 264, 47032, 575, 2011, 1465, 12, 3711, 13, 50514, 50514, 400, 370, 586, 437, 321, 576, 2066, 11, 295, 1164, 11, 307, 300, 570, 321, 600, 3105, 439, 613, 9834, 11, 321, 2066, 300, 264, 4470, 820, 362, 2780, 760, 257, 857, 13, 50964, 50964, 407, 321, 528, 281, 319, 12, 68, 3337, 10107, 264, 4470, 13, 961, 385, 1936, 485, 51214, 51214, 639, 307, 445, 257, 1412, 7123, 300, 6132, 380, 3105, 13, 51364, 51364, 583, 264, 2128, 1320, 510, 295, 264, 3209, 11, 321, 393, 850, 304, 2444, 473, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08447835883315728, "compression_ratio": 1.5555555555555556, "no_speech_prob": 9.223358574672602e-06}, {"id": 1429, "seek": 746300, "start": 7483.0, "end": 7490.0, "text": " But the forward pass here of the network, we can recalculate.", "tokens": [50364, 400, 370, 264, 47032, 575, 2011, 1465, 12, 3711, 13, 50514, 50514, 400, 370, 586, 437, 321, 576, 2066, 11, 295, 1164, 11, 307, 300, 570, 321, 600, 3105, 439, 613, 9834, 11, 321, 2066, 300, 264, 4470, 820, 362, 2780, 760, 257, 857, 13, 50964, 50964, 407, 321, 528, 281, 319, 12, 68, 3337, 10107, 264, 4470, 13, 961, 385, 1936, 485, 51214, 51214, 639, 307, 445, 257, 1412, 7123, 300, 6132, 380, 3105, 13, 51364, 51364, 583, 264, 2128, 1320, 510, 295, 264, 3209, 11, 321, 393, 850, 304, 2444, 473, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08447835883315728, "compression_ratio": 1.5555555555555556, "no_speech_prob": 9.223358574672602e-06}, {"id": 1430, "seek": 749000, "start": 7490.0, "end": 7494.0, "text": " And actually, let me do it outside here so that we can compare the two loss values.", "tokens": [50364, 400, 767, 11, 718, 385, 360, 309, 2380, 510, 370, 300, 321, 393, 6794, 264, 732, 4470, 4190, 13, 50564, 50564, 407, 510, 11, 498, 286, 850, 304, 2444, 473, 264, 4470, 11, 321, 1116, 2066, 264, 777, 4470, 586, 281, 312, 4748, 3126, 813, 341, 1230, 13, 50914, 50914, 407, 4696, 11, 437, 321, 434, 1242, 586, 307, 257, 5870, 857, 3126, 813, 1017, 13, 23, 16169, 13, 51264, 51264, 400, 1604, 11, 264, 636, 321, 600, 18721, 341, 307, 300, 2295, 4470, 1355, 300, 527, 21264, 366, 14324, 264, 12911, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08422652962281532, "compression_ratio": 1.593073593073593, "no_speech_prob": 1.0783054676721804e-05}, {"id": 1431, "seek": 749000, "start": 7494.0, "end": 7501.0, "text": " So here, if I recalculate the loss, we'd expect the new loss now to be slightly lower than this number.", "tokens": [50364, 400, 767, 11, 718, 385, 360, 309, 2380, 510, 370, 300, 321, 393, 6794, 264, 732, 4470, 4190, 13, 50564, 50564, 407, 510, 11, 498, 286, 850, 304, 2444, 473, 264, 4470, 11, 321, 1116, 2066, 264, 777, 4470, 586, 281, 312, 4748, 3126, 813, 341, 1230, 13, 50914, 50914, 407, 4696, 11, 437, 321, 434, 1242, 586, 307, 257, 5870, 857, 3126, 813, 1017, 13, 23, 16169, 13, 51264, 51264, 400, 1604, 11, 264, 636, 321, 600, 18721, 341, 307, 300, 2295, 4470, 1355, 300, 527, 21264, 366, 14324, 264, 12911, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08422652962281532, "compression_ratio": 1.593073593073593, "no_speech_prob": 1.0783054676721804e-05}, {"id": 1432, "seek": 749000, "start": 7501.0, "end": 7508.0, "text": " So hopefully, what we're getting now is a tiny bit lower than 4.846.", "tokens": [50364, 400, 767, 11, 718, 385, 360, 309, 2380, 510, 370, 300, 321, 393, 6794, 264, 732, 4470, 4190, 13, 50564, 50564, 407, 510, 11, 498, 286, 850, 304, 2444, 473, 264, 4470, 11, 321, 1116, 2066, 264, 777, 4470, 586, 281, 312, 4748, 3126, 813, 341, 1230, 13, 50914, 50914, 407, 4696, 11, 437, 321, 434, 1242, 586, 307, 257, 5870, 857, 3126, 813, 1017, 13, 23, 16169, 13, 51264, 51264, 400, 1604, 11, 264, 636, 321, 600, 18721, 341, 307, 300, 2295, 4470, 1355, 300, 527, 21264, 366, 14324, 264, 12911, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08422652962281532, "compression_ratio": 1.593073593073593, "no_speech_prob": 1.0783054676721804e-05}, {"id": 1433, "seek": 749000, "start": 7508.0, "end": 7515.0, "text": " And remember, the way we've arranged this is that low loss means that our predictions are matching the targets.", "tokens": [50364, 400, 767, 11, 718, 385, 360, 309, 2380, 510, 370, 300, 321, 393, 6794, 264, 732, 4470, 4190, 13, 50564, 50564, 407, 510, 11, 498, 286, 850, 304, 2444, 473, 264, 4470, 11, 321, 1116, 2066, 264, 777, 4470, 586, 281, 312, 4748, 3126, 813, 341, 1230, 13, 50914, 50914, 407, 4696, 11, 437, 321, 434, 1242, 586, 307, 257, 5870, 857, 3126, 813, 1017, 13, 23, 16169, 13, 51264, 51264, 400, 1604, 11, 264, 636, 321, 600, 18721, 341, 307, 300, 2295, 4470, 1355, 300, 527, 21264, 366, 14324, 264, 12911, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08422652962281532, "compression_ratio": 1.593073593073593, "no_speech_prob": 1.0783054676721804e-05}, {"id": 1434, "seek": 751500, "start": 7515.0, "end": 7520.0, "text": " So our predictions now are probably slightly closer to the targets.", "tokens": [50364, 407, 527, 21264, 586, 366, 1391, 4748, 4966, 281, 264, 12911, 13, 50614, 50614, 400, 586, 439, 321, 362, 281, 360, 307, 321, 362, 281, 44497, 341, 1399, 13, 50814, 50814, 407, 797, 11, 321, 600, 1096, 264, 2128, 1320, 11, 293, 341, 307, 264, 4470, 13, 51014, 51014, 823, 321, 393, 4470, 300, 23897, 13, 961, 385, 747, 613, 484, 13, 51214, 51214, 400, 321, 393, 360, 257, 1823, 2744, 13, 51314, 51314, 400, 586, 321, 820, 362, 257, 4748, 3126, 4470, 13, 1017, 13, 11309, 1709, 281, 805, 13, 24, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06906776329905716, "compression_ratio": 1.5876777251184835, "no_speech_prob": 1.3419323295238428e-05}, {"id": 1435, "seek": 751500, "start": 7520.0, "end": 7524.0, "text": " And now all we have to do is we have to iterate this process.", "tokens": [50364, 407, 527, 21264, 586, 366, 1391, 4748, 4966, 281, 264, 12911, 13, 50614, 50614, 400, 586, 439, 321, 362, 281, 360, 307, 321, 362, 281, 44497, 341, 1399, 13, 50814, 50814, 407, 797, 11, 321, 600, 1096, 264, 2128, 1320, 11, 293, 341, 307, 264, 4470, 13, 51014, 51014, 823, 321, 393, 4470, 300, 23897, 13, 961, 385, 747, 613, 484, 13, 51214, 51214, 400, 321, 393, 360, 257, 1823, 2744, 13, 51314, 51314, 400, 586, 321, 820, 362, 257, 4748, 3126, 4470, 13, 1017, 13, 11309, 1709, 281, 805, 13, 24, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06906776329905716, "compression_ratio": 1.5876777251184835, "no_speech_prob": 1.3419323295238428e-05}, {"id": 1436, "seek": 751500, "start": 7524.0, "end": 7528.0, "text": " So again, we've done the forward pass, and this is the loss.", "tokens": [50364, 407, 527, 21264, 586, 366, 1391, 4748, 4966, 281, 264, 12911, 13, 50614, 50614, 400, 586, 439, 321, 362, 281, 360, 307, 321, 362, 281, 44497, 341, 1399, 13, 50814, 50814, 407, 797, 11, 321, 600, 1096, 264, 2128, 1320, 11, 293, 341, 307, 264, 4470, 13, 51014, 51014, 823, 321, 393, 4470, 300, 23897, 13, 961, 385, 747, 613, 484, 13, 51214, 51214, 400, 321, 393, 360, 257, 1823, 2744, 13, 51314, 51314, 400, 586, 321, 820, 362, 257, 4748, 3126, 4470, 13, 1017, 13, 11309, 1709, 281, 805, 13, 24, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06906776329905716, "compression_ratio": 1.5876777251184835, "no_speech_prob": 1.3419323295238428e-05}, {"id": 1437, "seek": 751500, "start": 7528.0, "end": 7532.0, "text": " Now we can loss that backward. Let me take these out.", "tokens": [50364, 407, 527, 21264, 586, 366, 1391, 4748, 4966, 281, 264, 12911, 13, 50614, 50614, 400, 586, 439, 321, 362, 281, 360, 307, 321, 362, 281, 44497, 341, 1399, 13, 50814, 50814, 407, 797, 11, 321, 600, 1096, 264, 2128, 1320, 11, 293, 341, 307, 264, 4470, 13, 51014, 51014, 823, 321, 393, 4470, 300, 23897, 13, 961, 385, 747, 613, 484, 13, 51214, 51214, 400, 321, 393, 360, 257, 1823, 2744, 13, 51314, 51314, 400, 586, 321, 820, 362, 257, 4748, 3126, 4470, 13, 1017, 13, 11309, 1709, 281, 805, 13, 24, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06906776329905716, "compression_ratio": 1.5876777251184835, "no_speech_prob": 1.3419323295238428e-05}, {"id": 1438, "seek": 751500, "start": 7532.0, "end": 7534.0, "text": " And we can do a step size.", "tokens": [50364, 407, 527, 21264, 586, 366, 1391, 4748, 4966, 281, 264, 12911, 13, 50614, 50614, 400, 586, 439, 321, 362, 281, 360, 307, 321, 362, 281, 44497, 341, 1399, 13, 50814, 50814, 407, 797, 11, 321, 600, 1096, 264, 2128, 1320, 11, 293, 341, 307, 264, 4470, 13, 51014, 51014, 823, 321, 393, 4470, 300, 23897, 13, 961, 385, 747, 613, 484, 13, 51214, 51214, 400, 321, 393, 360, 257, 1823, 2744, 13, 51314, 51314, 400, 586, 321, 820, 362, 257, 4748, 3126, 4470, 13, 1017, 13, 11309, 1709, 281, 805, 13, 24, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06906776329905716, "compression_ratio": 1.5876777251184835, "no_speech_prob": 1.3419323295238428e-05}, {"id": 1439, "seek": 751500, "start": 7534.0, "end": 7540.0, "text": " And now we should have a slightly lower loss. 4.36 goes to 3.9.", "tokens": [50364, 407, 527, 21264, 586, 366, 1391, 4748, 4966, 281, 264, 12911, 13, 50614, 50614, 400, 586, 439, 321, 362, 281, 360, 307, 321, 362, 281, 44497, 341, 1399, 13, 50814, 50814, 407, 797, 11, 321, 600, 1096, 264, 2128, 1320, 11, 293, 341, 307, 264, 4470, 13, 51014, 51014, 823, 321, 393, 4470, 300, 23897, 13, 961, 385, 747, 613, 484, 13, 51214, 51214, 400, 321, 393, 360, 257, 1823, 2744, 13, 51314, 51314, 400, 586, 321, 820, 362, 257, 4748, 3126, 4470, 13, 1017, 13, 11309, 1709, 281, 805, 13, 24, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06906776329905716, "compression_ratio": 1.5876777251184835, "no_speech_prob": 1.3419323295238428e-05}, {"id": 1440, "seek": 754000, "start": 7540.0, "end": 7546.0, "text": " And OK, so we've done the forward pass. Here's the backward pass. Nudge.", "tokens": [50364, 400, 2264, 11, 370, 321, 600, 1096, 264, 2128, 1320, 13, 1692, 311, 264, 23897, 1320, 13, 426, 16032, 13, 50664, 50664, 400, 586, 264, 4470, 307, 805, 13, 15237, 13, 805, 13, 14060, 13, 400, 291, 483, 264, 1558, 13, 492, 445, 2354, 884, 341, 13, 51114, 51114, 400, 341, 307, 16235, 23475, 13, 492, 434, 445, 17138, 19020, 884, 2128, 1320, 11, 23897, 1320, 11, 5623, 13, 51414, 51414, 35524, 1320, 11, 23897, 1320, 11, 5623, 13, 400, 264, 18161, 2533, 307, 11470, 1080, 21264, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08161306381225586, "compression_ratio": 1.7106598984771573, "no_speech_prob": 3.340470811963314e-06}, {"id": 1441, "seek": 754000, "start": 7546.0, "end": 7555.0, "text": " And now the loss is 3.66. 3.47. And you get the idea. We just continue doing this.", "tokens": [50364, 400, 2264, 11, 370, 321, 600, 1096, 264, 2128, 1320, 13, 1692, 311, 264, 23897, 1320, 13, 426, 16032, 13, 50664, 50664, 400, 586, 264, 4470, 307, 805, 13, 15237, 13, 805, 13, 14060, 13, 400, 291, 483, 264, 1558, 13, 492, 445, 2354, 884, 341, 13, 51114, 51114, 400, 341, 307, 16235, 23475, 13, 492, 434, 445, 17138, 19020, 884, 2128, 1320, 11, 23897, 1320, 11, 5623, 13, 51414, 51414, 35524, 1320, 11, 23897, 1320, 11, 5623, 13, 400, 264, 18161, 2533, 307, 11470, 1080, 21264, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08161306381225586, "compression_ratio": 1.7106598984771573, "no_speech_prob": 3.340470811963314e-06}, {"id": 1442, "seek": 754000, "start": 7555.0, "end": 7561.0, "text": " And this is gradient descent. We're just iteratively doing forward pass, backward pass, update.", "tokens": [50364, 400, 2264, 11, 370, 321, 600, 1096, 264, 2128, 1320, 13, 1692, 311, 264, 23897, 1320, 13, 426, 16032, 13, 50664, 50664, 400, 586, 264, 4470, 307, 805, 13, 15237, 13, 805, 13, 14060, 13, 400, 291, 483, 264, 1558, 13, 492, 445, 2354, 884, 341, 13, 51114, 51114, 400, 341, 307, 16235, 23475, 13, 492, 434, 445, 17138, 19020, 884, 2128, 1320, 11, 23897, 1320, 11, 5623, 13, 51414, 51414, 35524, 1320, 11, 23897, 1320, 11, 5623, 13, 400, 264, 18161, 2533, 307, 11470, 1080, 21264, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08161306381225586, "compression_ratio": 1.7106598984771573, "no_speech_prob": 3.340470811963314e-06}, {"id": 1443, "seek": 754000, "start": 7561.0, "end": 7566.0, "text": " Forward pass, backward pass, update. And the neural net is improving its predictions.", "tokens": [50364, 400, 2264, 11, 370, 321, 600, 1096, 264, 2128, 1320, 13, 1692, 311, 264, 23897, 1320, 13, 426, 16032, 13, 50664, 50664, 400, 586, 264, 4470, 307, 805, 13, 15237, 13, 805, 13, 14060, 13, 400, 291, 483, 264, 1558, 13, 492, 445, 2354, 884, 341, 13, 51114, 51114, 400, 341, 307, 16235, 23475, 13, 492, 434, 445, 17138, 19020, 884, 2128, 1320, 11, 23897, 1320, 11, 5623, 13, 51414, 51414, 35524, 1320, 11, 23897, 1320, 11, 5623, 13, 400, 264, 18161, 2533, 307, 11470, 1080, 21264, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08161306381225586, "compression_ratio": 1.7106598984771573, "no_speech_prob": 3.340470811963314e-06}, {"id": 1444, "seek": 756600, "start": 7566.0, "end": 7577.0, "text": " So here, if we look at ypred now, we see that this value should be getting closer to 1.", "tokens": [50364, 407, 510, 11, 498, 321, 574, 412, 288, 79, 986, 586, 11, 321, 536, 300, 341, 2158, 820, 312, 1242, 4966, 281, 502, 13, 50914, 50914, 407, 341, 2158, 820, 312, 1242, 544, 3353, 13, 1981, 820, 312, 1242, 544, 3671, 13, 51064, 51064, 400, 341, 472, 820, 312, 611, 1242, 544, 3353, 13, 51164, 51164, 407, 498, 321, 445, 44497, 341, 257, 1326, 544, 1413, 11, 767, 11, 321, 815, 312, 1075, 281, 6157, 281, 352, 257, 857, 4663, 13, 51514, 51514, 961, 311, 853, 257, 4748, 2946, 15114, 3314, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08643420537312825, "compression_ratio": 1.83, "no_speech_prob": 1.0615959581627976e-05}, {"id": 1445, "seek": 756600, "start": 7577.0, "end": 7580.0, "text": " So this value should be getting more positive. These should be getting more negative.", "tokens": [50364, 407, 510, 11, 498, 321, 574, 412, 288, 79, 986, 586, 11, 321, 536, 300, 341, 2158, 820, 312, 1242, 4966, 281, 502, 13, 50914, 50914, 407, 341, 2158, 820, 312, 1242, 544, 3353, 13, 1981, 820, 312, 1242, 544, 3671, 13, 51064, 51064, 400, 341, 472, 820, 312, 611, 1242, 544, 3353, 13, 51164, 51164, 407, 498, 321, 445, 44497, 341, 257, 1326, 544, 1413, 11, 767, 11, 321, 815, 312, 1075, 281, 6157, 281, 352, 257, 857, 4663, 13, 51514, 51514, 961, 311, 853, 257, 4748, 2946, 15114, 3314, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08643420537312825, "compression_ratio": 1.83, "no_speech_prob": 1.0615959581627976e-05}, {"id": 1446, "seek": 756600, "start": 7580.0, "end": 7582.0, "text": " And this one should be also getting more positive.", "tokens": [50364, 407, 510, 11, 498, 321, 574, 412, 288, 79, 986, 586, 11, 321, 536, 300, 341, 2158, 820, 312, 1242, 4966, 281, 502, 13, 50914, 50914, 407, 341, 2158, 820, 312, 1242, 544, 3353, 13, 1981, 820, 312, 1242, 544, 3671, 13, 51064, 51064, 400, 341, 472, 820, 312, 611, 1242, 544, 3353, 13, 51164, 51164, 407, 498, 321, 445, 44497, 341, 257, 1326, 544, 1413, 11, 767, 11, 321, 815, 312, 1075, 281, 6157, 281, 352, 257, 857, 4663, 13, 51514, 51514, 961, 311, 853, 257, 4748, 2946, 15114, 3314, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08643420537312825, "compression_ratio": 1.83, "no_speech_prob": 1.0615959581627976e-05}, {"id": 1447, "seek": 756600, "start": 7582.0, "end": 7589.0, "text": " So if we just iterate this a few more times, actually, we may be able to afford to go a bit faster.", "tokens": [50364, 407, 510, 11, 498, 321, 574, 412, 288, 79, 986, 586, 11, 321, 536, 300, 341, 2158, 820, 312, 1242, 4966, 281, 502, 13, 50914, 50914, 407, 341, 2158, 820, 312, 1242, 544, 3353, 13, 1981, 820, 312, 1242, 544, 3671, 13, 51064, 51064, 400, 341, 472, 820, 312, 611, 1242, 544, 3353, 13, 51164, 51164, 407, 498, 321, 445, 44497, 341, 257, 1326, 544, 1413, 11, 767, 11, 321, 815, 312, 1075, 281, 6157, 281, 352, 257, 857, 4663, 13, 51514, 51514, 961, 311, 853, 257, 4748, 2946, 15114, 3314, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08643420537312825, "compression_ratio": 1.83, "no_speech_prob": 1.0615959581627976e-05}, {"id": 1448, "seek": 756600, "start": 7589.0, "end": 7594.0, "text": " Let's try a slightly higher loading rate.", "tokens": [50364, 407, 510, 11, 498, 321, 574, 412, 288, 79, 986, 586, 11, 321, 536, 300, 341, 2158, 820, 312, 1242, 4966, 281, 502, 13, 50914, 50914, 407, 341, 2158, 820, 312, 1242, 544, 3353, 13, 1981, 820, 312, 1242, 544, 3671, 13, 51064, 51064, 400, 341, 472, 820, 312, 611, 1242, 544, 3353, 13, 51164, 51164, 407, 498, 321, 445, 44497, 341, 257, 1326, 544, 1413, 11, 767, 11, 321, 815, 312, 1075, 281, 6157, 281, 352, 257, 857, 4663, 13, 51514, 51514, 961, 311, 853, 257, 4748, 2946, 15114, 3314, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08643420537312825, "compression_ratio": 1.83, "no_speech_prob": 1.0615959581627976e-05}, {"id": 1449, "seek": 759400, "start": 7594.0, "end": 7599.0, "text": " Oops. OK, there we go. So now we're at 0.31.", "tokens": [50364, 21726, 13, 2264, 11, 456, 321, 352, 13, 407, 586, 321, 434, 412, 1958, 13, 12967, 13, 50614, 50614, 759, 291, 352, 886, 2370, 11, 538, 264, 636, 11, 498, 291, 853, 281, 652, 309, 886, 955, 295, 257, 1823, 11, 291, 815, 767, 670, 16792, 13, 51014, 51014, 467, 311, 670, 47273, 13, 1436, 797, 11, 1604, 11, 321, 500, 380, 767, 458, 2293, 466, 264, 4470, 2445, 13, 51214, 51214, 440, 4470, 2445, 575, 439, 3685, 295, 3877, 13, 51314, 51314, 400, 321, 787, 458, 466, 264, 588, 2654, 31704, 295, 439, 613, 9834, 322, 264, 4470, 13, 51564, 51564, 583, 498, 321, 1823, 886, 1400, 11, 321, 815, 1823, 666, 257, 644, 295, 264, 4470, 300, 307, 2584, 819, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06832421670748492, "compression_ratio": 1.6774193548387097, "no_speech_prob": 1.9222885384806432e-05}, {"id": 1450, "seek": 759400, "start": 7599.0, "end": 7607.0, "text": " If you go too fast, by the way, if you try to make it too big of a step, you may actually overstep.", "tokens": [50364, 21726, 13, 2264, 11, 456, 321, 352, 13, 407, 586, 321, 434, 412, 1958, 13, 12967, 13, 50614, 50614, 759, 291, 352, 886, 2370, 11, 538, 264, 636, 11, 498, 291, 853, 281, 652, 309, 886, 955, 295, 257, 1823, 11, 291, 815, 767, 670, 16792, 13, 51014, 51014, 467, 311, 670, 47273, 13, 1436, 797, 11, 1604, 11, 321, 500, 380, 767, 458, 2293, 466, 264, 4470, 2445, 13, 51214, 51214, 440, 4470, 2445, 575, 439, 3685, 295, 3877, 13, 51314, 51314, 400, 321, 787, 458, 466, 264, 588, 2654, 31704, 295, 439, 613, 9834, 322, 264, 4470, 13, 51564, 51564, 583, 498, 321, 1823, 886, 1400, 11, 321, 815, 1823, 666, 257, 644, 295, 264, 4470, 300, 307, 2584, 819, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06832421670748492, "compression_ratio": 1.6774193548387097, "no_speech_prob": 1.9222885384806432e-05}, {"id": 1451, "seek": 759400, "start": 7607.0, "end": 7611.0, "text": " It's overconfidence. Because again, remember, we don't actually know exactly about the loss function.", "tokens": [50364, 21726, 13, 2264, 11, 456, 321, 352, 13, 407, 586, 321, 434, 412, 1958, 13, 12967, 13, 50614, 50614, 759, 291, 352, 886, 2370, 11, 538, 264, 636, 11, 498, 291, 853, 281, 652, 309, 886, 955, 295, 257, 1823, 11, 291, 815, 767, 670, 16792, 13, 51014, 51014, 467, 311, 670, 47273, 13, 1436, 797, 11, 1604, 11, 321, 500, 380, 767, 458, 2293, 466, 264, 4470, 2445, 13, 51214, 51214, 440, 4470, 2445, 575, 439, 3685, 295, 3877, 13, 51314, 51314, 400, 321, 787, 458, 466, 264, 588, 2654, 31704, 295, 439, 613, 9834, 322, 264, 4470, 13, 51564, 51564, 583, 498, 321, 1823, 886, 1400, 11, 321, 815, 1823, 666, 257, 644, 295, 264, 4470, 300, 307, 2584, 819, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06832421670748492, "compression_ratio": 1.6774193548387097, "no_speech_prob": 1.9222885384806432e-05}, {"id": 1452, "seek": 759400, "start": 7611.0, "end": 7613.0, "text": " The loss function has all kinds of structure.", "tokens": [50364, 21726, 13, 2264, 11, 456, 321, 352, 13, 407, 586, 321, 434, 412, 1958, 13, 12967, 13, 50614, 50614, 759, 291, 352, 886, 2370, 11, 538, 264, 636, 11, 498, 291, 853, 281, 652, 309, 886, 955, 295, 257, 1823, 11, 291, 815, 767, 670, 16792, 13, 51014, 51014, 467, 311, 670, 47273, 13, 1436, 797, 11, 1604, 11, 321, 500, 380, 767, 458, 2293, 466, 264, 4470, 2445, 13, 51214, 51214, 440, 4470, 2445, 575, 439, 3685, 295, 3877, 13, 51314, 51314, 400, 321, 787, 458, 466, 264, 588, 2654, 31704, 295, 439, 613, 9834, 322, 264, 4470, 13, 51564, 51564, 583, 498, 321, 1823, 886, 1400, 11, 321, 815, 1823, 666, 257, 644, 295, 264, 4470, 300, 307, 2584, 819, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06832421670748492, "compression_ratio": 1.6774193548387097, "no_speech_prob": 1.9222885384806432e-05}, {"id": 1453, "seek": 759400, "start": 7613.0, "end": 7618.0, "text": " And we only know about the very local dependence of all these parameters on the loss.", "tokens": [50364, 21726, 13, 2264, 11, 456, 321, 352, 13, 407, 586, 321, 434, 412, 1958, 13, 12967, 13, 50614, 50614, 759, 291, 352, 886, 2370, 11, 538, 264, 636, 11, 498, 291, 853, 281, 652, 309, 886, 955, 295, 257, 1823, 11, 291, 815, 767, 670, 16792, 13, 51014, 51014, 467, 311, 670, 47273, 13, 1436, 797, 11, 1604, 11, 321, 500, 380, 767, 458, 2293, 466, 264, 4470, 2445, 13, 51214, 51214, 440, 4470, 2445, 575, 439, 3685, 295, 3877, 13, 51314, 51314, 400, 321, 787, 458, 466, 264, 588, 2654, 31704, 295, 439, 613, 9834, 322, 264, 4470, 13, 51564, 51564, 583, 498, 321, 1823, 886, 1400, 11, 321, 815, 1823, 666, 257, 644, 295, 264, 4470, 300, 307, 2584, 819, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06832421670748492, "compression_ratio": 1.6774193548387097, "no_speech_prob": 1.9222885384806432e-05}, {"id": 1454, "seek": 759400, "start": 7618.0, "end": 7623.0, "text": " But if we step too far, we may step into a part of the loss that is completely different.", "tokens": [50364, 21726, 13, 2264, 11, 456, 321, 352, 13, 407, 586, 321, 434, 412, 1958, 13, 12967, 13, 50614, 50614, 759, 291, 352, 886, 2370, 11, 538, 264, 636, 11, 498, 291, 853, 281, 652, 309, 886, 955, 295, 257, 1823, 11, 291, 815, 767, 670, 16792, 13, 51014, 51014, 467, 311, 670, 47273, 13, 1436, 797, 11, 1604, 11, 321, 500, 380, 767, 458, 2293, 466, 264, 4470, 2445, 13, 51214, 51214, 440, 4470, 2445, 575, 439, 3685, 295, 3877, 13, 51314, 51314, 400, 321, 787, 458, 466, 264, 588, 2654, 31704, 295, 439, 613, 9834, 322, 264, 4470, 13, 51564, 51564, 583, 498, 321, 1823, 886, 1400, 11, 321, 815, 1823, 666, 257, 644, 295, 264, 4470, 300, 307, 2584, 819, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06832421670748492, "compression_ratio": 1.6774193548387097, "no_speech_prob": 1.9222885384806432e-05}, {"id": 1455, "seek": 762300, "start": 7623.0, "end": 7628.0, "text": " And that can destabilize training and make your loss actually blow up even.", "tokens": [50364, 400, 300, 393, 2677, 5177, 1125, 3097, 293, 652, 428, 4470, 767, 6327, 493, 754, 13, 50614, 50614, 407, 264, 4470, 307, 586, 1958, 13, 14565, 13, 407, 767, 11, 264, 21264, 820, 312, 534, 1596, 1998, 13, 50864, 50864, 961, 311, 747, 257, 574, 13, 50964, 50964, 407, 291, 536, 577, 341, 307, 1920, 502, 11, 1920, 3671, 502, 11, 1920, 502, 13, 51164, 51164, 492, 393, 2354, 516, 13, 407, 11, 18633, 11, 23897, 11, 5623, 13, 51514, 51514, 21726, 11, 456, 321, 352, 13, 407, 321, 1437, 636, 886, 2370, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10025251154996911, "compression_ratio": 1.518181818181818, "no_speech_prob": 1.40636757350876e-05}, {"id": 1456, "seek": 762300, "start": 7628.0, "end": 7633.0, "text": " So the loss is now 0.04. So actually, the predictions should be really quite close.", "tokens": [50364, 400, 300, 393, 2677, 5177, 1125, 3097, 293, 652, 428, 4470, 767, 6327, 493, 754, 13, 50614, 50614, 407, 264, 4470, 307, 586, 1958, 13, 14565, 13, 407, 767, 11, 264, 21264, 820, 312, 534, 1596, 1998, 13, 50864, 50864, 961, 311, 747, 257, 574, 13, 50964, 50964, 407, 291, 536, 577, 341, 307, 1920, 502, 11, 1920, 3671, 502, 11, 1920, 502, 13, 51164, 51164, 492, 393, 2354, 516, 13, 407, 11, 18633, 11, 23897, 11, 5623, 13, 51514, 51514, 21726, 11, 456, 321, 352, 13, 407, 321, 1437, 636, 886, 2370, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10025251154996911, "compression_ratio": 1.518181818181818, "no_speech_prob": 1.40636757350876e-05}, {"id": 1457, "seek": 762300, "start": 7633.0, "end": 7635.0, "text": " Let's take a look.", "tokens": [50364, 400, 300, 393, 2677, 5177, 1125, 3097, 293, 652, 428, 4470, 767, 6327, 493, 754, 13, 50614, 50614, 407, 264, 4470, 307, 586, 1958, 13, 14565, 13, 407, 767, 11, 264, 21264, 820, 312, 534, 1596, 1998, 13, 50864, 50864, 961, 311, 747, 257, 574, 13, 50964, 50964, 407, 291, 536, 577, 341, 307, 1920, 502, 11, 1920, 3671, 502, 11, 1920, 502, 13, 51164, 51164, 492, 393, 2354, 516, 13, 407, 11, 18633, 11, 23897, 11, 5623, 13, 51514, 51514, 21726, 11, 456, 321, 352, 13, 407, 321, 1437, 636, 886, 2370, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10025251154996911, "compression_ratio": 1.518181818181818, "no_speech_prob": 1.40636757350876e-05}, {"id": 1458, "seek": 762300, "start": 7635.0, "end": 7639.0, "text": " So you see how this is almost 1, almost negative 1, almost 1.", "tokens": [50364, 400, 300, 393, 2677, 5177, 1125, 3097, 293, 652, 428, 4470, 767, 6327, 493, 754, 13, 50614, 50614, 407, 264, 4470, 307, 586, 1958, 13, 14565, 13, 407, 767, 11, 264, 21264, 820, 312, 534, 1596, 1998, 13, 50864, 50864, 961, 311, 747, 257, 574, 13, 50964, 50964, 407, 291, 536, 577, 341, 307, 1920, 502, 11, 1920, 3671, 502, 11, 1920, 502, 13, 51164, 51164, 492, 393, 2354, 516, 13, 407, 11, 18633, 11, 23897, 11, 5623, 13, 51514, 51514, 21726, 11, 456, 321, 352, 13, 407, 321, 1437, 636, 886, 2370, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10025251154996911, "compression_ratio": 1.518181818181818, "no_speech_prob": 1.40636757350876e-05}, {"id": 1459, "seek": 762300, "start": 7639.0, "end": 7646.0, "text": " We can continue going. So, yep, backward, update.", "tokens": [50364, 400, 300, 393, 2677, 5177, 1125, 3097, 293, 652, 428, 4470, 767, 6327, 493, 754, 13, 50614, 50614, 407, 264, 4470, 307, 586, 1958, 13, 14565, 13, 407, 767, 11, 264, 21264, 820, 312, 534, 1596, 1998, 13, 50864, 50864, 961, 311, 747, 257, 574, 13, 50964, 50964, 407, 291, 536, 577, 341, 307, 1920, 502, 11, 1920, 3671, 502, 11, 1920, 502, 13, 51164, 51164, 492, 393, 2354, 516, 13, 407, 11, 18633, 11, 23897, 11, 5623, 13, 51514, 51514, 21726, 11, 456, 321, 352, 13, 407, 321, 1437, 636, 886, 2370, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10025251154996911, "compression_ratio": 1.518181818181818, "no_speech_prob": 1.40636757350876e-05}, {"id": 1460, "seek": 762300, "start": 7646.0, "end": 7648.0, "text": " Oops, there we go. So we went way too fast.", "tokens": [50364, 400, 300, 393, 2677, 5177, 1125, 3097, 293, 652, 428, 4470, 767, 6327, 493, 754, 13, 50614, 50614, 407, 264, 4470, 307, 586, 1958, 13, 14565, 13, 407, 767, 11, 264, 21264, 820, 312, 534, 1596, 1998, 13, 50864, 50864, 961, 311, 747, 257, 574, 13, 50964, 50964, 407, 291, 536, 577, 341, 307, 1920, 502, 11, 1920, 3671, 502, 11, 1920, 502, 13, 51164, 51164, 492, 393, 2354, 516, 13, 407, 11, 18633, 11, 23897, 11, 5623, 13, 51514, 51514, 21726, 11, 456, 321, 352, 13, 407, 321, 1437, 636, 886, 2370, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10025251154996911, "compression_ratio": 1.518181818181818, "no_speech_prob": 1.40636757350876e-05}, {"id": 1461, "seek": 764800, "start": 7648.0, "end": 7654.0, "text": " And we actually overstepped. So we got too eager.", "tokens": [50364, 400, 321, 767, 670, 2941, 3320, 13, 407, 321, 658, 886, 18259, 13, 50664, 50664, 2305, 366, 321, 586, 30, 21726, 13, 2264, 13, 1614, 68, 3671, 1722, 13, 407, 341, 307, 588, 11, 588, 2295, 4470, 13, 51014, 51014, 400, 264, 21264, 366, 1936, 2176, 13, 51214, 51214, 407, 6063, 321, 1936, 321, 645, 884, 636, 886, 955, 9205, 293, 321, 10515, 27049, 13, 51464, 51464, 583, 550, 6063, 321, 4590, 493, 1242, 666, 257, 534, 665, 4008, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10486388206481934, "compression_ratio": 1.5445544554455446, "no_speech_prob": 5.01460181112634e-06}, {"id": 1462, "seek": 764800, "start": 7654.0, "end": 7661.0, "text": " Where are we now? Oops. OK. 7e negative 9. So this is very, very low loss.", "tokens": [50364, 400, 321, 767, 670, 2941, 3320, 13, 407, 321, 658, 886, 18259, 13, 50664, 50664, 2305, 366, 321, 586, 30, 21726, 13, 2264, 13, 1614, 68, 3671, 1722, 13, 407, 341, 307, 588, 11, 588, 2295, 4470, 13, 51014, 51014, 400, 264, 21264, 366, 1936, 2176, 13, 51214, 51214, 407, 6063, 321, 1936, 321, 645, 884, 636, 886, 955, 9205, 293, 321, 10515, 27049, 13, 51464, 51464, 583, 550, 6063, 321, 4590, 493, 1242, 666, 257, 534, 665, 4008, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10486388206481934, "compression_ratio": 1.5445544554455446, "no_speech_prob": 5.01460181112634e-06}, {"id": 1463, "seek": 764800, "start": 7661.0, "end": 7665.0, "text": " And the predictions are basically perfect.", "tokens": [50364, 400, 321, 767, 670, 2941, 3320, 13, 407, 321, 658, 886, 18259, 13, 50664, 50664, 2305, 366, 321, 586, 30, 21726, 13, 2264, 13, 1614, 68, 3671, 1722, 13, 407, 341, 307, 588, 11, 588, 2295, 4470, 13, 51014, 51014, 400, 264, 21264, 366, 1936, 2176, 13, 51214, 51214, 407, 6063, 321, 1936, 321, 645, 884, 636, 886, 955, 9205, 293, 321, 10515, 27049, 13, 51464, 51464, 583, 550, 6063, 321, 4590, 493, 1242, 666, 257, 534, 665, 4008, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10486388206481934, "compression_ratio": 1.5445544554455446, "no_speech_prob": 5.01460181112634e-06}, {"id": 1464, "seek": 764800, "start": 7665.0, "end": 7670.0, "text": " So somehow we basically we were doing way too big updates and we briefly exploded.", "tokens": [50364, 400, 321, 767, 670, 2941, 3320, 13, 407, 321, 658, 886, 18259, 13, 50664, 50664, 2305, 366, 321, 586, 30, 21726, 13, 2264, 13, 1614, 68, 3671, 1722, 13, 407, 341, 307, 588, 11, 588, 2295, 4470, 13, 51014, 51014, 400, 264, 21264, 366, 1936, 2176, 13, 51214, 51214, 407, 6063, 321, 1936, 321, 645, 884, 636, 886, 955, 9205, 293, 321, 10515, 27049, 13, 51464, 51464, 583, 550, 6063, 321, 4590, 493, 1242, 666, 257, 534, 665, 4008, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10486388206481934, "compression_ratio": 1.5445544554455446, "no_speech_prob": 5.01460181112634e-06}, {"id": 1465, "seek": 764800, "start": 7670.0, "end": 7673.0, "text": " But then somehow we ended up getting into a really good spot.", "tokens": [50364, 400, 321, 767, 670, 2941, 3320, 13, 407, 321, 658, 886, 18259, 13, 50664, 50664, 2305, 366, 321, 586, 30, 21726, 13, 2264, 13, 1614, 68, 3671, 1722, 13, 407, 341, 307, 588, 11, 588, 2295, 4470, 13, 51014, 51014, 400, 264, 21264, 366, 1936, 2176, 13, 51214, 51214, 407, 6063, 321, 1936, 321, 645, 884, 636, 886, 955, 9205, 293, 321, 10515, 27049, 13, 51464, 51464, 583, 550, 6063, 321, 4590, 493, 1242, 666, 257, 534, 665, 4008, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10486388206481934, "compression_ratio": 1.5445544554455446, "no_speech_prob": 5.01460181112634e-06}, {"id": 1466, "seek": 767300, "start": 7673.0, "end": 7678.0, "text": " So usually this learning rate and the tuning of it is a subtle art.", "tokens": [50364, 407, 2673, 341, 2539, 3314, 293, 264, 15164, 295, 309, 307, 257, 13743, 1523, 13, 50614, 50614, 509, 528, 281, 992, 428, 2539, 3314, 13, 759, 309, 311, 886, 2295, 11, 291, 434, 516, 281, 747, 636, 886, 938, 281, 41881, 13, 50814, 50814, 583, 498, 309, 311, 886, 1090, 11, 264, 1379, 551, 2170, 23742, 293, 291, 1062, 767, 754, 21411, 264, 4470, 11, 5413, 322, 428, 4470, 2445, 13, 51164, 51164, 407, 5006, 264, 1823, 2744, 281, 312, 445, 558, 11, 309, 311, 257, 1238, 13743, 1523, 2171, 562, 291, 434, 1228, 1333, 295, 17528, 16235, 23475, 13, 51514, 51514, 583, 321, 1051, 281, 483, 666, 257, 665, 4008, 13, 492, 393, 574, 412, 297, 5893, 9834, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06995344161987305, "compression_ratio": 1.7241379310344827, "no_speech_prob": 9.22305480344221e-06}, {"id": 1467, "seek": 767300, "start": 7678.0, "end": 7682.0, "text": " You want to set your learning rate. If it's too low, you're going to take way too long to converge.", "tokens": [50364, 407, 2673, 341, 2539, 3314, 293, 264, 15164, 295, 309, 307, 257, 13743, 1523, 13, 50614, 50614, 509, 528, 281, 992, 428, 2539, 3314, 13, 759, 309, 311, 886, 2295, 11, 291, 434, 516, 281, 747, 636, 886, 938, 281, 41881, 13, 50814, 50814, 583, 498, 309, 311, 886, 1090, 11, 264, 1379, 551, 2170, 23742, 293, 291, 1062, 767, 754, 21411, 264, 4470, 11, 5413, 322, 428, 4470, 2445, 13, 51164, 51164, 407, 5006, 264, 1823, 2744, 281, 312, 445, 558, 11, 309, 311, 257, 1238, 13743, 1523, 2171, 562, 291, 434, 1228, 1333, 295, 17528, 16235, 23475, 13, 51514, 51514, 583, 321, 1051, 281, 483, 666, 257, 665, 4008, 13, 492, 393, 574, 412, 297, 5893, 9834, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06995344161987305, "compression_ratio": 1.7241379310344827, "no_speech_prob": 9.22305480344221e-06}, {"id": 1468, "seek": 767300, "start": 7682.0, "end": 7689.0, "text": " But if it's too high, the whole thing gets unstable and you might actually even explode the loss, depending on your loss function.", "tokens": [50364, 407, 2673, 341, 2539, 3314, 293, 264, 15164, 295, 309, 307, 257, 13743, 1523, 13, 50614, 50614, 509, 528, 281, 992, 428, 2539, 3314, 13, 759, 309, 311, 886, 2295, 11, 291, 434, 516, 281, 747, 636, 886, 938, 281, 41881, 13, 50814, 50814, 583, 498, 309, 311, 886, 1090, 11, 264, 1379, 551, 2170, 23742, 293, 291, 1062, 767, 754, 21411, 264, 4470, 11, 5413, 322, 428, 4470, 2445, 13, 51164, 51164, 407, 5006, 264, 1823, 2744, 281, 312, 445, 558, 11, 309, 311, 257, 1238, 13743, 1523, 2171, 562, 291, 434, 1228, 1333, 295, 17528, 16235, 23475, 13, 51514, 51514, 583, 321, 1051, 281, 483, 666, 257, 665, 4008, 13, 492, 393, 574, 412, 297, 5893, 9834, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06995344161987305, "compression_ratio": 1.7241379310344827, "no_speech_prob": 9.22305480344221e-06}, {"id": 1469, "seek": 767300, "start": 7689.0, "end": 7696.0, "text": " So finding the step size to be just right, it's a pretty subtle art sometimes when you're using sort of vanilla gradient descent.", "tokens": [50364, 407, 2673, 341, 2539, 3314, 293, 264, 15164, 295, 309, 307, 257, 13743, 1523, 13, 50614, 50614, 509, 528, 281, 992, 428, 2539, 3314, 13, 759, 309, 311, 886, 2295, 11, 291, 434, 516, 281, 747, 636, 886, 938, 281, 41881, 13, 50814, 50814, 583, 498, 309, 311, 886, 1090, 11, 264, 1379, 551, 2170, 23742, 293, 291, 1062, 767, 754, 21411, 264, 4470, 11, 5413, 322, 428, 4470, 2445, 13, 51164, 51164, 407, 5006, 264, 1823, 2744, 281, 312, 445, 558, 11, 309, 311, 257, 1238, 13743, 1523, 2171, 562, 291, 434, 1228, 1333, 295, 17528, 16235, 23475, 13, 51514, 51514, 583, 321, 1051, 281, 483, 666, 257, 665, 4008, 13, 492, 393, 574, 412, 297, 5893, 9834, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06995344161987305, "compression_ratio": 1.7241379310344827, "no_speech_prob": 9.22305480344221e-06}, {"id": 1470, "seek": 767300, "start": 7696.0, "end": 7702.0, "text": " But we happen to get into a good spot. We can look at n dot parameters.", "tokens": [50364, 407, 2673, 341, 2539, 3314, 293, 264, 15164, 295, 309, 307, 257, 13743, 1523, 13, 50614, 50614, 509, 528, 281, 992, 428, 2539, 3314, 13, 759, 309, 311, 886, 2295, 11, 291, 434, 516, 281, 747, 636, 886, 938, 281, 41881, 13, 50814, 50814, 583, 498, 309, 311, 886, 1090, 11, 264, 1379, 551, 2170, 23742, 293, 291, 1062, 767, 754, 21411, 264, 4470, 11, 5413, 322, 428, 4470, 2445, 13, 51164, 51164, 407, 5006, 264, 1823, 2744, 281, 312, 445, 558, 11, 309, 311, 257, 1238, 13743, 1523, 2171, 562, 291, 434, 1228, 1333, 295, 17528, 16235, 23475, 13, 51514, 51514, 583, 321, 1051, 281, 483, 666, 257, 665, 4008, 13, 492, 393, 574, 412, 297, 5893, 9834, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06995344161987305, "compression_ratio": 1.7241379310344827, "no_speech_prob": 9.22305480344221e-06}, {"id": 1471, "seek": 770200, "start": 7702.0, "end": 7713.0, "text": " So this is the setting of weights and biases that makes our network predict the desired targets very, very close.", "tokens": [50364, 407, 341, 307, 264, 3287, 295, 17443, 293, 32152, 300, 1669, 527, 3209, 6069, 264, 14721, 12911, 588, 11, 588, 1998, 13, 50914, 50914, 400, 1936, 321, 600, 10727, 8895, 257, 18161, 2533, 13, 51164, 51164, 2264, 11, 718, 311, 652, 341, 257, 5870, 857, 544, 44279, 293, 4445, 364, 3539, 3097, 6367, 293, 437, 300, 1542, 411, 13, 51414, 51414, 407, 341, 307, 264, 1412, 7123, 300, 4368, 341, 307, 264, 2128, 1320, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08870269075224671, "compression_ratio": 1.5936073059360731, "no_speech_prob": 3.966862550441874e-06}, {"id": 1472, "seek": 770200, "start": 7713.0, "end": 7718.0, "text": " And basically we've successfully trained a neural net.", "tokens": [50364, 407, 341, 307, 264, 3287, 295, 17443, 293, 32152, 300, 1669, 527, 3209, 6069, 264, 14721, 12911, 588, 11, 588, 1998, 13, 50914, 50914, 400, 1936, 321, 600, 10727, 8895, 257, 18161, 2533, 13, 51164, 51164, 2264, 11, 718, 311, 652, 341, 257, 5870, 857, 544, 44279, 293, 4445, 364, 3539, 3097, 6367, 293, 437, 300, 1542, 411, 13, 51414, 51414, 407, 341, 307, 264, 1412, 7123, 300, 4368, 341, 307, 264, 2128, 1320, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08870269075224671, "compression_ratio": 1.5936073059360731, "no_speech_prob": 3.966862550441874e-06}, {"id": 1473, "seek": 770200, "start": 7718.0, "end": 7723.0, "text": " OK, let's make this a tiny bit more respectable and implement an actual training loop and what that looks like.", "tokens": [50364, 407, 341, 307, 264, 3287, 295, 17443, 293, 32152, 300, 1669, 527, 3209, 6069, 264, 14721, 12911, 588, 11, 588, 1998, 13, 50914, 50914, 400, 1936, 321, 600, 10727, 8895, 257, 18161, 2533, 13, 51164, 51164, 2264, 11, 718, 311, 652, 341, 257, 5870, 857, 544, 44279, 293, 4445, 364, 3539, 3097, 6367, 293, 437, 300, 1542, 411, 13, 51414, 51414, 407, 341, 307, 264, 1412, 7123, 300, 4368, 341, 307, 264, 2128, 1320, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08870269075224671, "compression_ratio": 1.5936073059360731, "no_speech_prob": 3.966862550441874e-06}, {"id": 1474, "seek": 770200, "start": 7723.0, "end": 7728.0, "text": " So this is the data definition that states this is the forward pass.", "tokens": [50364, 407, 341, 307, 264, 3287, 295, 17443, 293, 32152, 300, 1669, 527, 3209, 6069, 264, 14721, 12911, 588, 11, 588, 1998, 13, 50914, 50914, 400, 1936, 321, 600, 10727, 8895, 257, 18161, 2533, 13, 51164, 51164, 2264, 11, 718, 311, 652, 341, 257, 5870, 857, 544, 44279, 293, 4445, 364, 3539, 3097, 6367, 293, 437, 300, 1542, 411, 13, 51414, 51414, 407, 341, 307, 264, 1412, 7123, 300, 4368, 341, 307, 264, 2128, 1320, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08870269075224671, "compression_ratio": 1.5936073059360731, "no_speech_prob": 3.966862550441874e-06}, {"id": 1475, "seek": 772800, "start": 7728.0, "end": 7737.0, "text": " So for K in range, you know, we're going to take a bunch of steps.", "tokens": [50364, 407, 337, 591, 294, 3613, 11, 291, 458, 11, 321, 434, 516, 281, 747, 257, 3840, 295, 4439, 13, 50814, 50814, 2386, 11, 321, 360, 264, 2128, 1320, 13, 492, 29562, 264, 4470, 13, 51164, 51164, 961, 311, 6561, 270, 831, 1125, 264, 18161, 2533, 490, 8459, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.1909629748417781, "compression_ratio": 1.2748091603053435, "no_speech_prob": 5.063892240286805e-05}, {"id": 1476, "seek": 772800, "start": 7737.0, "end": 7744.0, "text": " First, we do the forward pass. We validate the loss.", "tokens": [50364, 407, 337, 591, 294, 3613, 11, 291, 458, 11, 321, 434, 516, 281, 747, 257, 3840, 295, 4439, 13, 50814, 50814, 2386, 11, 321, 360, 264, 2128, 1320, 13, 492, 29562, 264, 4470, 13, 51164, 51164, 961, 311, 6561, 270, 831, 1125, 264, 18161, 2533, 490, 8459, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.1909629748417781, "compression_ratio": 1.2748091603053435, "no_speech_prob": 5.063892240286805e-05}, {"id": 1477, "seek": 772800, "start": 7744.0, "end": 7746.0, "text": " Let's reinitialize the neural net from scratch.", "tokens": [50364, 407, 337, 591, 294, 3613, 11, 291, 458, 11, 321, 434, 516, 281, 747, 257, 3840, 295, 4439, 13, 50814, 50814, 2386, 11, 321, 360, 264, 2128, 1320, 13, 492, 29562, 264, 4470, 13, 51164, 51164, 961, 311, 6561, 270, 831, 1125, 264, 18161, 2533, 490, 8459, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.1909629748417781, "compression_ratio": 1.2748091603053435, "no_speech_prob": 5.063892240286805e-05}, {"id": 1478, "seek": 774600, "start": 7746.0, "end": 7759.0, "text": " And here's the data. And we first do forward pass, then we do the backward pass.", "tokens": [50364, 400, 510, 311, 264, 1412, 13, 400, 321, 700, 360, 2128, 1320, 11, 550, 321, 360, 264, 23897, 1320, 13, 51014, 51014, 400, 550, 321, 360, 364, 5623, 13, 663, 311, 16235, 23475, 13, 51364, 51364, 400, 550, 321, 820, 312, 1075, 281, 44497, 341, 293, 321, 820, 312, 1075, 281, 4482, 264, 2190, 1823, 11, 264, 2190, 4470, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09066928178071976, "compression_ratio": 1.7092198581560283, "no_speech_prob": 6.339077117445413e-06}, {"id": 1479, "seek": 774600, "start": 7759.0, "end": 7766.0, "text": " And then we do an update. That's gradient descent.", "tokens": [50364, 400, 510, 311, 264, 1412, 13, 400, 321, 700, 360, 2128, 1320, 11, 550, 321, 360, 264, 23897, 1320, 13, 51014, 51014, 400, 550, 321, 360, 364, 5623, 13, 663, 311, 16235, 23475, 13, 51364, 51364, 400, 550, 321, 820, 312, 1075, 281, 44497, 341, 293, 321, 820, 312, 1075, 281, 4482, 264, 2190, 1823, 11, 264, 2190, 4470, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09066928178071976, "compression_ratio": 1.7092198581560283, "no_speech_prob": 6.339077117445413e-06}, {"id": 1480, "seek": 774600, "start": 7766.0, "end": 7772.0, "text": " And then we should be able to iterate this and we should be able to print the current step, the current loss.", "tokens": [50364, 400, 510, 311, 264, 1412, 13, 400, 321, 700, 360, 2128, 1320, 11, 550, 321, 360, 264, 23897, 1320, 13, 51014, 51014, 400, 550, 321, 360, 364, 5623, 13, 663, 311, 16235, 23475, 13, 51364, 51364, 400, 550, 321, 820, 312, 1075, 281, 44497, 341, 293, 321, 820, 312, 1075, 281, 4482, 264, 2190, 1823, 11, 264, 2190, 4470, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09066928178071976, "compression_ratio": 1.7092198581560283, "no_speech_prob": 6.339077117445413e-06}, {"id": 1481, "seek": 777200, "start": 7772.0, "end": 7777.0, "text": " Let's just print the sort of number of the loss.", "tokens": [50364, 961, 311, 445, 4482, 264, 1333, 295, 1230, 295, 264, 4470, 13, 50614, 50614, 400, 300, 820, 312, 309, 13, 400, 550, 264, 2539, 3314, 1958, 13, 10607, 307, 257, 707, 886, 1359, 13, 50914, 50914, 1958, 13, 16, 321, 1866, 307, 411, 257, 707, 857, 5795, 498, 291, 2256, 13, 51064, 51064, 961, 311, 352, 4079, 294, 1296, 293, 321, 603, 19719, 341, 337, 406, 1266, 4439, 11, 457, 718, 311, 352, 337, 11, 584, 11, 945, 4439, 13, 51464, 51464, 961, 385, 23525, 439, 295, 341, 19109, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12172944495018492, "compression_ratio": 1.5414634146341464, "no_speech_prob": 6.2407489167526364e-06}, {"id": 1482, "seek": 777200, "start": 7777.0, "end": 7783.0, "text": " And that should be it. And then the learning rate 0.01 is a little too small.", "tokens": [50364, 961, 311, 445, 4482, 264, 1333, 295, 1230, 295, 264, 4470, 13, 50614, 50614, 400, 300, 820, 312, 309, 13, 400, 550, 264, 2539, 3314, 1958, 13, 10607, 307, 257, 707, 886, 1359, 13, 50914, 50914, 1958, 13, 16, 321, 1866, 307, 411, 257, 707, 857, 5795, 498, 291, 2256, 13, 51064, 51064, 961, 311, 352, 4079, 294, 1296, 293, 321, 603, 19719, 341, 337, 406, 1266, 4439, 11, 457, 718, 311, 352, 337, 11, 584, 11, 945, 4439, 13, 51464, 51464, 961, 385, 23525, 439, 295, 341, 19109, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12172944495018492, "compression_ratio": 1.5414634146341464, "no_speech_prob": 6.2407489167526364e-06}, {"id": 1483, "seek": 777200, "start": 7783.0, "end": 7786.0, "text": " 0.1 we saw is like a little bit dangerous if you buy.", "tokens": [50364, 961, 311, 445, 4482, 264, 1333, 295, 1230, 295, 264, 4470, 13, 50614, 50614, 400, 300, 820, 312, 309, 13, 400, 550, 264, 2539, 3314, 1958, 13, 10607, 307, 257, 707, 886, 1359, 13, 50914, 50914, 1958, 13, 16, 321, 1866, 307, 411, 257, 707, 857, 5795, 498, 291, 2256, 13, 51064, 51064, 961, 311, 352, 4079, 294, 1296, 293, 321, 603, 19719, 341, 337, 406, 1266, 4439, 11, 457, 718, 311, 352, 337, 11, 584, 11, 945, 4439, 13, 51464, 51464, 961, 385, 23525, 439, 295, 341, 19109, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12172944495018492, "compression_ratio": 1.5414634146341464, "no_speech_prob": 6.2407489167526364e-06}, {"id": 1484, "seek": 777200, "start": 7786.0, "end": 7794.0, "text": " Let's go somewhere in between and we'll optimize this for not 10 steps, but let's go for, say, 20 steps.", "tokens": [50364, 961, 311, 445, 4482, 264, 1333, 295, 1230, 295, 264, 4470, 13, 50614, 50614, 400, 300, 820, 312, 309, 13, 400, 550, 264, 2539, 3314, 1958, 13, 10607, 307, 257, 707, 886, 1359, 13, 50914, 50914, 1958, 13, 16, 321, 1866, 307, 411, 257, 707, 857, 5795, 498, 291, 2256, 13, 51064, 51064, 961, 311, 352, 4079, 294, 1296, 293, 321, 603, 19719, 341, 337, 406, 1266, 4439, 11, 457, 718, 311, 352, 337, 11, 584, 11, 945, 4439, 13, 51464, 51464, 961, 385, 23525, 439, 295, 341, 19109, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12172944495018492, "compression_ratio": 1.5414634146341464, "no_speech_prob": 6.2407489167526364e-06}, {"id": 1485, "seek": 777200, "start": 7794.0, "end": 7799.0, "text": " Let me erase all of this junk.", "tokens": [50364, 961, 311, 445, 4482, 264, 1333, 295, 1230, 295, 264, 4470, 13, 50614, 50614, 400, 300, 820, 312, 309, 13, 400, 550, 264, 2539, 3314, 1958, 13, 10607, 307, 257, 707, 886, 1359, 13, 50914, 50914, 1958, 13, 16, 321, 1866, 307, 411, 257, 707, 857, 5795, 498, 291, 2256, 13, 51064, 51064, 961, 311, 352, 4079, 294, 1296, 293, 321, 603, 19719, 341, 337, 406, 1266, 4439, 11, 457, 718, 311, 352, 337, 11, 584, 11, 945, 4439, 13, 51464, 51464, 961, 385, 23525, 439, 295, 341, 19109, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12172944495018492, "compression_ratio": 1.5414634146341464, "no_speech_prob": 6.2407489167526364e-06}, {"id": 1486, "seek": 779900, "start": 7799.0, "end": 7803.0, "text": " And let's run the optimization.", "tokens": [50364, 400, 718, 311, 1190, 264, 19618, 13, 50564, 50564, 400, 291, 536, 577, 321, 600, 767, 9652, 3004, 14009, 294, 257, 544, 10164, 9060, 293, 658, 281, 257, 4470, 300, 307, 588, 2295, 13, 50964, 50964, 407, 286, 2066, 4874, 5961, 281, 312, 1596, 665, 13, 51214, 51214, 821, 321, 352, 13, 51514, 51514, 400, 300, 311, 309, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09639265423729307, "compression_ratio": 1.3766233766233766, "no_speech_prob": 1.4970732081565075e-05}, {"id": 1487, "seek": 779900, "start": 7803.0, "end": 7811.0, "text": " And you see how we've actually converged slower in a more controlled manner and got to a loss that is very low.", "tokens": [50364, 400, 718, 311, 1190, 264, 19618, 13, 50564, 50564, 400, 291, 536, 577, 321, 600, 767, 9652, 3004, 14009, 294, 257, 544, 10164, 9060, 293, 658, 281, 257, 4470, 300, 307, 588, 2295, 13, 50964, 50964, 407, 286, 2066, 4874, 5961, 281, 312, 1596, 665, 13, 51214, 51214, 821, 321, 352, 13, 51514, 51514, 400, 300, 311, 309, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09639265423729307, "compression_ratio": 1.3766233766233766, "no_speech_prob": 1.4970732081565075e-05}, {"id": 1488, "seek": 779900, "start": 7811.0, "end": 7816.0, "text": " So I expect wide bread to be quite good.", "tokens": [50364, 400, 718, 311, 1190, 264, 19618, 13, 50564, 50564, 400, 291, 536, 577, 321, 600, 767, 9652, 3004, 14009, 294, 257, 544, 10164, 9060, 293, 658, 281, 257, 4470, 300, 307, 588, 2295, 13, 50964, 50964, 407, 286, 2066, 4874, 5961, 281, 312, 1596, 665, 13, 51214, 51214, 821, 321, 352, 13, 51514, 51514, 400, 300, 311, 309, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09639265423729307, "compression_ratio": 1.3766233766233766, "no_speech_prob": 1.4970732081565075e-05}, {"id": 1489, "seek": 779900, "start": 7816.0, "end": 7822.0, "text": " There we go.", "tokens": [50364, 400, 718, 311, 1190, 264, 19618, 13, 50564, 50564, 400, 291, 536, 577, 321, 600, 767, 9652, 3004, 14009, 294, 257, 544, 10164, 9060, 293, 658, 281, 257, 4470, 300, 307, 588, 2295, 13, 50964, 50964, 407, 286, 2066, 4874, 5961, 281, 312, 1596, 665, 13, 51214, 51214, 821, 321, 352, 13, 51514, 51514, 400, 300, 311, 309, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09639265423729307, "compression_ratio": 1.3766233766233766, "no_speech_prob": 1.4970732081565075e-05}, {"id": 1490, "seek": 779900, "start": 7822.0, "end": 7824.0, "text": " And that's it.", "tokens": [50364, 400, 718, 311, 1190, 264, 19618, 13, 50564, 50564, 400, 291, 536, 577, 321, 600, 767, 9652, 3004, 14009, 294, 257, 544, 10164, 9060, 293, 658, 281, 257, 4470, 300, 307, 588, 2295, 13, 50964, 50964, 407, 286, 2066, 4874, 5961, 281, 312, 1596, 665, 13, 51214, 51214, 821, 321, 352, 13, 51514, 51514, 400, 300, 311, 309, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09639265423729307, "compression_ratio": 1.3766233766233766, "no_speech_prob": 1.4970732081565075e-05}, {"id": 1491, "seek": 782400, "start": 7824.0, "end": 7830.0, "text": " OK, so this is kind of embarrassing, but we actually have a really terrible bug in here and it's a subtle bug.", "tokens": [50364, 2264, 11, 370, 341, 307, 733, 295, 17299, 11, 457, 321, 767, 362, 257, 534, 6237, 7426, 294, 510, 293, 309, 311, 257, 13743, 7426, 13, 50664, 50664, 400, 309, 311, 257, 588, 2689, 7426, 13, 400, 286, 393, 380, 1697, 286, 600, 1096, 309, 337, 264, 945, 392, 565, 294, 452, 993, 11, 2318, 322, 2799, 13, 51014, 51014, 400, 286, 727, 362, 725, 12194, 264, 1379, 551, 11, 457, 286, 519, 309, 311, 1238, 4074, 13, 51164, 51164, 400, 11, 291, 458, 11, 291, 483, 281, 4449, 257, 857, 437, 1364, 365, 18161, 36170, 815, 312, 307, 411, 2171, 13, 51514, 51514, 492, 366, 12341, 295, 2689, 7426, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08331015213676121, "compression_ratio": 1.6206896551724137, "no_speech_prob": 8.091851486824453e-05}, {"id": 1492, "seek": 782400, "start": 7830.0, "end": 7837.0, "text": " And it's a very common bug. And I can't believe I've done it for the 20th time in my life, especially on camera.", "tokens": [50364, 2264, 11, 370, 341, 307, 733, 295, 17299, 11, 457, 321, 767, 362, 257, 534, 6237, 7426, 294, 510, 293, 309, 311, 257, 13743, 7426, 13, 50664, 50664, 400, 309, 311, 257, 588, 2689, 7426, 13, 400, 286, 393, 380, 1697, 286, 600, 1096, 309, 337, 264, 945, 392, 565, 294, 452, 993, 11, 2318, 322, 2799, 13, 51014, 51014, 400, 286, 727, 362, 725, 12194, 264, 1379, 551, 11, 457, 286, 519, 309, 311, 1238, 4074, 13, 51164, 51164, 400, 11, 291, 458, 11, 291, 483, 281, 4449, 257, 857, 437, 1364, 365, 18161, 36170, 815, 312, 307, 411, 2171, 13, 51514, 51514, 492, 366, 12341, 295, 2689, 7426, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08331015213676121, "compression_ratio": 1.6206896551724137, "no_speech_prob": 8.091851486824453e-05}, {"id": 1493, "seek": 782400, "start": 7837.0, "end": 7840.0, "text": " And I could have reshot the whole thing, but I think it's pretty funny.", "tokens": [50364, 2264, 11, 370, 341, 307, 733, 295, 17299, 11, 457, 321, 767, 362, 257, 534, 6237, 7426, 294, 510, 293, 309, 311, 257, 13743, 7426, 13, 50664, 50664, 400, 309, 311, 257, 588, 2689, 7426, 13, 400, 286, 393, 380, 1697, 286, 600, 1096, 309, 337, 264, 945, 392, 565, 294, 452, 993, 11, 2318, 322, 2799, 13, 51014, 51014, 400, 286, 727, 362, 725, 12194, 264, 1379, 551, 11, 457, 286, 519, 309, 311, 1238, 4074, 13, 51164, 51164, 400, 11, 291, 458, 11, 291, 483, 281, 4449, 257, 857, 437, 1364, 365, 18161, 36170, 815, 312, 307, 411, 2171, 13, 51514, 51514, 492, 366, 12341, 295, 2689, 7426, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08331015213676121, "compression_ratio": 1.6206896551724137, "no_speech_prob": 8.091851486824453e-05}, {"id": 1494, "seek": 782400, "start": 7840.0, "end": 7847.0, "text": " And, you know, you get to appreciate a bit what working with neural nets may be is like sometimes.", "tokens": [50364, 2264, 11, 370, 341, 307, 733, 295, 17299, 11, 457, 321, 767, 362, 257, 534, 6237, 7426, 294, 510, 293, 309, 311, 257, 13743, 7426, 13, 50664, 50664, 400, 309, 311, 257, 588, 2689, 7426, 13, 400, 286, 393, 380, 1697, 286, 600, 1096, 309, 337, 264, 945, 392, 565, 294, 452, 993, 11, 2318, 322, 2799, 13, 51014, 51014, 400, 286, 727, 362, 725, 12194, 264, 1379, 551, 11, 457, 286, 519, 309, 311, 1238, 4074, 13, 51164, 51164, 400, 11, 291, 458, 11, 291, 483, 281, 4449, 257, 857, 437, 1364, 365, 18161, 36170, 815, 312, 307, 411, 2171, 13, 51514, 51514, 492, 366, 12341, 295, 2689, 7426, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08331015213676121, "compression_ratio": 1.6206896551724137, "no_speech_prob": 8.091851486824453e-05}, {"id": 1495, "seek": 782400, "start": 7847.0, "end": 7851.0, "text": " We are guilty of common bug.", "tokens": [50364, 2264, 11, 370, 341, 307, 733, 295, 17299, 11, 457, 321, 767, 362, 257, 534, 6237, 7426, 294, 510, 293, 309, 311, 257, 13743, 7426, 13, 50664, 50664, 400, 309, 311, 257, 588, 2689, 7426, 13, 400, 286, 393, 380, 1697, 286, 600, 1096, 309, 337, 264, 945, 392, 565, 294, 452, 993, 11, 2318, 322, 2799, 13, 51014, 51014, 400, 286, 727, 362, 725, 12194, 264, 1379, 551, 11, 457, 286, 519, 309, 311, 1238, 4074, 13, 51164, 51164, 400, 11, 291, 458, 11, 291, 483, 281, 4449, 257, 857, 437, 1364, 365, 18161, 36170, 815, 312, 307, 411, 2171, 13, 51514, 51514, 492, 366, 12341, 295, 2689, 7426, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08331015213676121, "compression_ratio": 1.6206896551724137, "no_speech_prob": 8.091851486824453e-05}, {"id": 1496, "seek": 785100, "start": 7851.0, "end": 7857.0, "text": " I've actually tweeted the most common neural mistakes a long time ago now.", "tokens": [50364, 286, 600, 767, 25646, 264, 881, 2689, 18161, 8038, 257, 938, 565, 2057, 586, 13, 50664, 50664, 400, 286, 478, 406, 534, 516, 281, 2903, 604, 295, 613, 3993, 337, 321, 366, 12341, 295, 1230, 1045, 13, 50964, 50964, 509, 5298, 281, 4018, 2771, 949, 5893, 23897, 13, 51114, 51114, 708, 307, 300, 30, 51264, 51264, 8537, 11, 437, 311, 2737, 11, 293, 309, 311, 257, 13743, 7426, 293, 286, 478, 406, 988, 498, 291, 1866, 309, 11, 307, 300, 439, 295, 613, 17443, 510, 362, 257, 5893, 1412, 293, 257, 5893, 2771, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07697107353988959, "compression_ratio": 1.5738396624472575, "no_speech_prob": 5.422103640739806e-06}, {"id": 1497, "seek": 785100, "start": 7857.0, "end": 7863.0, "text": " And I'm not really going to explain any of these except for we are guilty of number three.", "tokens": [50364, 286, 600, 767, 25646, 264, 881, 2689, 18161, 8038, 257, 938, 565, 2057, 586, 13, 50664, 50664, 400, 286, 478, 406, 534, 516, 281, 2903, 604, 295, 613, 3993, 337, 321, 366, 12341, 295, 1230, 1045, 13, 50964, 50964, 509, 5298, 281, 4018, 2771, 949, 5893, 23897, 13, 51114, 51114, 708, 307, 300, 30, 51264, 51264, 8537, 11, 437, 311, 2737, 11, 293, 309, 311, 257, 13743, 7426, 293, 286, 478, 406, 988, 498, 291, 1866, 309, 11, 307, 300, 439, 295, 613, 17443, 510, 362, 257, 5893, 1412, 293, 257, 5893, 2771, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07697107353988959, "compression_ratio": 1.5738396624472575, "no_speech_prob": 5.422103640739806e-06}, {"id": 1498, "seek": 785100, "start": 7863.0, "end": 7866.0, "text": " You forgot to zero grad before dot backward.", "tokens": [50364, 286, 600, 767, 25646, 264, 881, 2689, 18161, 8038, 257, 938, 565, 2057, 586, 13, 50664, 50664, 400, 286, 478, 406, 534, 516, 281, 2903, 604, 295, 613, 3993, 337, 321, 366, 12341, 295, 1230, 1045, 13, 50964, 50964, 509, 5298, 281, 4018, 2771, 949, 5893, 23897, 13, 51114, 51114, 708, 307, 300, 30, 51264, 51264, 8537, 11, 437, 311, 2737, 11, 293, 309, 311, 257, 13743, 7426, 293, 286, 478, 406, 988, 498, 291, 1866, 309, 11, 307, 300, 439, 295, 613, 17443, 510, 362, 257, 5893, 1412, 293, 257, 5893, 2771, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07697107353988959, "compression_ratio": 1.5738396624472575, "no_speech_prob": 5.422103640739806e-06}, {"id": 1499, "seek": 785100, "start": 7866.0, "end": 7869.0, "text": " What is that?", "tokens": [50364, 286, 600, 767, 25646, 264, 881, 2689, 18161, 8038, 257, 938, 565, 2057, 586, 13, 50664, 50664, 400, 286, 478, 406, 534, 516, 281, 2903, 604, 295, 613, 3993, 337, 321, 366, 12341, 295, 1230, 1045, 13, 50964, 50964, 509, 5298, 281, 4018, 2771, 949, 5893, 23897, 13, 51114, 51114, 708, 307, 300, 30, 51264, 51264, 8537, 11, 437, 311, 2737, 11, 293, 309, 311, 257, 13743, 7426, 293, 286, 478, 406, 988, 498, 291, 1866, 309, 11, 307, 300, 439, 295, 613, 17443, 510, 362, 257, 5893, 1412, 293, 257, 5893, 2771, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07697107353988959, "compression_ratio": 1.5738396624472575, "no_speech_prob": 5.422103640739806e-06}, {"id": 1500, "seek": 785100, "start": 7869.0, "end": 7879.0, "text": " Basically, what's happening, and it's a subtle bug and I'm not sure if you saw it, is that all of these weights here have a dot data and a dot grad.", "tokens": [50364, 286, 600, 767, 25646, 264, 881, 2689, 18161, 8038, 257, 938, 565, 2057, 586, 13, 50664, 50664, 400, 286, 478, 406, 534, 516, 281, 2903, 604, 295, 613, 3993, 337, 321, 366, 12341, 295, 1230, 1045, 13, 50964, 50964, 509, 5298, 281, 4018, 2771, 949, 5893, 23897, 13, 51114, 51114, 708, 307, 300, 30, 51264, 51264, 8537, 11, 437, 311, 2737, 11, 293, 309, 311, 257, 13743, 7426, 293, 286, 478, 406, 988, 498, 291, 1866, 309, 11, 307, 300, 439, 295, 613, 17443, 510, 362, 257, 5893, 1412, 293, 257, 5893, 2771, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07697107353988959, "compression_ratio": 1.5738396624472575, "no_speech_prob": 5.422103640739806e-06}, {"id": 1501, "seek": 787900, "start": 7879.0, "end": 7882.0, "text": " And that grad starts at zero.", "tokens": [50364, 400, 300, 2771, 3719, 412, 4018, 13, 50514, 50514, 400, 550, 321, 360, 23897, 293, 321, 2836, 294, 264, 2771, 2448, 293, 550, 321, 360, 364, 5623, 322, 264, 1412, 11, 457, 321, 500, 380, 19568, 264, 2771, 13, 50864, 50864, 467, 10834, 456, 13, 50964, 50964, 407, 562, 321, 360, 264, 1150, 2128, 1320, 293, 321, 360, 23897, 797, 11, 1604, 300, 439, 264, 23897, 7705, 360, 257, 1804, 6915, 322, 264, 2771, 13, 51364, 51364, 400, 370, 613, 2771, 2448, 445, 909, 493, 293, 436, 1128, 483, 14322, 281, 4018, 13, 51614, 51614, 407, 1936, 11, 321, 994, 380, 4018, 2771, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06165480613708496, "compression_ratio": 1.8243243243243243, "no_speech_prob": 1.2878492725576507e-06}, {"id": 1502, "seek": 787900, "start": 7882.0, "end": 7889.0, "text": " And then we do backward and we fill in the gradients and then we do an update on the data, but we don't flush the grad.", "tokens": [50364, 400, 300, 2771, 3719, 412, 4018, 13, 50514, 50514, 400, 550, 321, 360, 23897, 293, 321, 2836, 294, 264, 2771, 2448, 293, 550, 321, 360, 364, 5623, 322, 264, 1412, 11, 457, 321, 500, 380, 19568, 264, 2771, 13, 50864, 50864, 467, 10834, 456, 13, 50964, 50964, 407, 562, 321, 360, 264, 1150, 2128, 1320, 293, 321, 360, 23897, 797, 11, 1604, 300, 439, 264, 23897, 7705, 360, 257, 1804, 6915, 322, 264, 2771, 13, 51364, 51364, 400, 370, 613, 2771, 2448, 445, 909, 493, 293, 436, 1128, 483, 14322, 281, 4018, 13, 51614, 51614, 407, 1936, 11, 321, 994, 380, 4018, 2771, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06165480613708496, "compression_ratio": 1.8243243243243243, "no_speech_prob": 1.2878492725576507e-06}, {"id": 1503, "seek": 787900, "start": 7889.0, "end": 7891.0, "text": " It stays there.", "tokens": [50364, 400, 300, 2771, 3719, 412, 4018, 13, 50514, 50514, 400, 550, 321, 360, 23897, 293, 321, 2836, 294, 264, 2771, 2448, 293, 550, 321, 360, 364, 5623, 322, 264, 1412, 11, 457, 321, 500, 380, 19568, 264, 2771, 13, 50864, 50864, 467, 10834, 456, 13, 50964, 50964, 407, 562, 321, 360, 264, 1150, 2128, 1320, 293, 321, 360, 23897, 797, 11, 1604, 300, 439, 264, 23897, 7705, 360, 257, 1804, 6915, 322, 264, 2771, 13, 51364, 51364, 400, 370, 613, 2771, 2448, 445, 909, 493, 293, 436, 1128, 483, 14322, 281, 4018, 13, 51614, 51614, 407, 1936, 11, 321, 994, 380, 4018, 2771, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06165480613708496, "compression_ratio": 1.8243243243243243, "no_speech_prob": 1.2878492725576507e-06}, {"id": 1504, "seek": 787900, "start": 7891.0, "end": 7899.0, "text": " So when we do the second forward pass and we do backward again, remember that all the backward operations do a plus equals on the grad.", "tokens": [50364, 400, 300, 2771, 3719, 412, 4018, 13, 50514, 50514, 400, 550, 321, 360, 23897, 293, 321, 2836, 294, 264, 2771, 2448, 293, 550, 321, 360, 364, 5623, 322, 264, 1412, 11, 457, 321, 500, 380, 19568, 264, 2771, 13, 50864, 50864, 467, 10834, 456, 13, 50964, 50964, 407, 562, 321, 360, 264, 1150, 2128, 1320, 293, 321, 360, 23897, 797, 11, 1604, 300, 439, 264, 23897, 7705, 360, 257, 1804, 6915, 322, 264, 2771, 13, 51364, 51364, 400, 370, 613, 2771, 2448, 445, 909, 493, 293, 436, 1128, 483, 14322, 281, 4018, 13, 51614, 51614, 407, 1936, 11, 321, 994, 380, 4018, 2771, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06165480613708496, "compression_ratio": 1.8243243243243243, "no_speech_prob": 1.2878492725576507e-06}, {"id": 1505, "seek": 787900, "start": 7899.0, "end": 7904.0, "text": " And so these gradients just add up and they never get reset to zero.", "tokens": [50364, 400, 300, 2771, 3719, 412, 4018, 13, 50514, 50514, 400, 550, 321, 360, 23897, 293, 321, 2836, 294, 264, 2771, 2448, 293, 550, 321, 360, 364, 5623, 322, 264, 1412, 11, 457, 321, 500, 380, 19568, 264, 2771, 13, 50864, 50864, 467, 10834, 456, 13, 50964, 50964, 407, 562, 321, 360, 264, 1150, 2128, 1320, 293, 321, 360, 23897, 797, 11, 1604, 300, 439, 264, 23897, 7705, 360, 257, 1804, 6915, 322, 264, 2771, 13, 51364, 51364, 400, 370, 613, 2771, 2448, 445, 909, 493, 293, 436, 1128, 483, 14322, 281, 4018, 13, 51614, 51614, 407, 1936, 11, 321, 994, 380, 4018, 2771, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06165480613708496, "compression_ratio": 1.8243243243243243, "no_speech_prob": 1.2878492725576507e-06}, {"id": 1506, "seek": 787900, "start": 7904.0, "end": 7907.0, "text": " So basically, we didn't zero grad.", "tokens": [50364, 400, 300, 2771, 3719, 412, 4018, 13, 50514, 50514, 400, 550, 321, 360, 23897, 293, 321, 2836, 294, 264, 2771, 2448, 293, 550, 321, 360, 364, 5623, 322, 264, 1412, 11, 457, 321, 500, 380, 19568, 264, 2771, 13, 50864, 50864, 467, 10834, 456, 13, 50964, 50964, 407, 562, 321, 360, 264, 1150, 2128, 1320, 293, 321, 360, 23897, 797, 11, 1604, 300, 439, 264, 23897, 7705, 360, 257, 1804, 6915, 322, 264, 2771, 13, 51364, 51364, 400, 370, 613, 2771, 2448, 445, 909, 493, 293, 436, 1128, 483, 14322, 281, 4018, 13, 51614, 51614, 407, 1936, 11, 321, 994, 380, 4018, 2771, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06165480613708496, "compression_ratio": 1.8243243243243243, "no_speech_prob": 1.2878492725576507e-06}, {"id": 1507, "seek": 790700, "start": 7907.0, "end": 7911.0, "text": " So here's how we zero grad before backward.", "tokens": [50364, 407, 510, 311, 577, 321, 4018, 2771, 949, 23897, 13, 50564, 50564, 492, 643, 281, 44497, 670, 439, 264, 9834, 293, 321, 643, 281, 652, 988, 300, 280, 5893, 2771, 307, 992, 281, 4018, 13, 50914, 50914, 492, 643, 281, 14322, 309, 281, 4018, 445, 411, 309, 307, 294, 264, 47479, 13, 51114, 51114, 407, 1604, 11, 439, 264, 636, 510, 337, 439, 613, 2158, 13891, 11, 2771, 307, 14322, 281, 4018, 13, 51364, 51364, 400, 550, 439, 613, 23897, 11335, 360, 257, 1804, 6915, 490, 300, 2771, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08985840376987252, "compression_ratio": 1.7661691542288558, "no_speech_prob": 3.5558873605623376e-06}, {"id": 1508, "seek": 790700, "start": 7911.0, "end": 7918.0, "text": " We need to iterate over all the parameters and we need to make sure that p dot grad is set to zero.", "tokens": [50364, 407, 510, 311, 577, 321, 4018, 2771, 949, 23897, 13, 50564, 50564, 492, 643, 281, 44497, 670, 439, 264, 9834, 293, 321, 643, 281, 652, 988, 300, 280, 5893, 2771, 307, 992, 281, 4018, 13, 50914, 50914, 492, 643, 281, 14322, 309, 281, 4018, 445, 411, 309, 307, 294, 264, 47479, 13, 51114, 51114, 407, 1604, 11, 439, 264, 636, 510, 337, 439, 613, 2158, 13891, 11, 2771, 307, 14322, 281, 4018, 13, 51364, 51364, 400, 550, 439, 613, 23897, 11335, 360, 257, 1804, 6915, 490, 300, 2771, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08985840376987252, "compression_ratio": 1.7661691542288558, "no_speech_prob": 3.5558873605623376e-06}, {"id": 1509, "seek": 790700, "start": 7918.0, "end": 7922.0, "text": " We need to reset it to zero just like it is in the constructor.", "tokens": [50364, 407, 510, 311, 577, 321, 4018, 2771, 949, 23897, 13, 50564, 50564, 492, 643, 281, 44497, 670, 439, 264, 9834, 293, 321, 643, 281, 652, 988, 300, 280, 5893, 2771, 307, 992, 281, 4018, 13, 50914, 50914, 492, 643, 281, 14322, 309, 281, 4018, 445, 411, 309, 307, 294, 264, 47479, 13, 51114, 51114, 407, 1604, 11, 439, 264, 636, 510, 337, 439, 613, 2158, 13891, 11, 2771, 307, 14322, 281, 4018, 13, 51364, 51364, 400, 550, 439, 613, 23897, 11335, 360, 257, 1804, 6915, 490, 300, 2771, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08985840376987252, "compression_ratio": 1.7661691542288558, "no_speech_prob": 3.5558873605623376e-06}, {"id": 1510, "seek": 790700, "start": 7922.0, "end": 7927.0, "text": " So remember, all the way here for all these value nodes, grad is reset to zero.", "tokens": [50364, 407, 510, 311, 577, 321, 4018, 2771, 949, 23897, 13, 50564, 50564, 492, 643, 281, 44497, 670, 439, 264, 9834, 293, 321, 643, 281, 652, 988, 300, 280, 5893, 2771, 307, 992, 281, 4018, 13, 50914, 50914, 492, 643, 281, 14322, 309, 281, 4018, 445, 411, 309, 307, 294, 264, 47479, 13, 51114, 51114, 407, 1604, 11, 439, 264, 636, 510, 337, 439, 613, 2158, 13891, 11, 2771, 307, 14322, 281, 4018, 13, 51364, 51364, 400, 550, 439, 613, 23897, 11335, 360, 257, 1804, 6915, 490, 300, 2771, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08985840376987252, "compression_ratio": 1.7661691542288558, "no_speech_prob": 3.5558873605623376e-06}, {"id": 1511, "seek": 790700, "start": 7927.0, "end": 7931.0, "text": " And then all these backward passes do a plus equals from that grad.", "tokens": [50364, 407, 510, 311, 577, 321, 4018, 2771, 949, 23897, 13, 50564, 50564, 492, 643, 281, 44497, 670, 439, 264, 9834, 293, 321, 643, 281, 652, 988, 300, 280, 5893, 2771, 307, 992, 281, 4018, 13, 50914, 50914, 492, 643, 281, 14322, 309, 281, 4018, 445, 411, 309, 307, 294, 264, 47479, 13, 51114, 51114, 407, 1604, 11, 439, 264, 636, 510, 337, 439, 613, 2158, 13891, 11, 2771, 307, 14322, 281, 4018, 13, 51364, 51364, 400, 550, 439, 613, 23897, 11335, 360, 257, 1804, 6915, 490, 300, 2771, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08985840376987252, "compression_ratio": 1.7661691542288558, "no_speech_prob": 3.5558873605623376e-06}, {"id": 1512, "seek": 793100, "start": 7931.0, "end": 7945.0, "text": " But we need to make sure that we reset these grads to zero so that when we do backward, all of them start at zero and the actual backward pass accumulates the loss derivatives into the grads.", "tokens": [50364, 583, 321, 643, 281, 652, 988, 300, 321, 14322, 613, 2771, 82, 281, 4018, 370, 300, 562, 321, 360, 23897, 11, 439, 295, 552, 722, 412, 4018, 293, 264, 3539, 23897, 1320, 12989, 26192, 264, 4470, 33733, 666, 264, 2771, 82, 13, 51064, 51064, 407, 341, 307, 4018, 2771, 294, 9953, 51, 284, 339, 13, 51264, 51264, 400, 321, 486, 483, 257, 4748, 819, 19618, 13, 51464, 51464, 961, 311, 14322, 264, 18161, 2533, 13, 51564, 51564, 440, 1412, 307, 264, 912, 13, 51614, 51614, 639, 307, 586, 11, 286, 519, 11, 3006, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07721243585859026, "compression_ratio": 1.575221238938053, "no_speech_prob": 4.425428414833732e-06}, {"id": 1513, "seek": 793100, "start": 7945.0, "end": 7949.0, "text": " So this is zero grad in PyTorch.", "tokens": [50364, 583, 321, 643, 281, 652, 988, 300, 321, 14322, 613, 2771, 82, 281, 4018, 370, 300, 562, 321, 360, 23897, 11, 439, 295, 552, 722, 412, 4018, 293, 264, 3539, 23897, 1320, 12989, 26192, 264, 4470, 33733, 666, 264, 2771, 82, 13, 51064, 51064, 407, 341, 307, 4018, 2771, 294, 9953, 51, 284, 339, 13, 51264, 51264, 400, 321, 486, 483, 257, 4748, 819, 19618, 13, 51464, 51464, 961, 311, 14322, 264, 18161, 2533, 13, 51564, 51564, 440, 1412, 307, 264, 912, 13, 51614, 51614, 639, 307, 586, 11, 286, 519, 11, 3006, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07721243585859026, "compression_ratio": 1.575221238938053, "no_speech_prob": 4.425428414833732e-06}, {"id": 1514, "seek": 793100, "start": 7949.0, "end": 7953.0, "text": " And we will get a slightly different optimization.", "tokens": [50364, 583, 321, 643, 281, 652, 988, 300, 321, 14322, 613, 2771, 82, 281, 4018, 370, 300, 562, 321, 360, 23897, 11, 439, 295, 552, 722, 412, 4018, 293, 264, 3539, 23897, 1320, 12989, 26192, 264, 4470, 33733, 666, 264, 2771, 82, 13, 51064, 51064, 407, 341, 307, 4018, 2771, 294, 9953, 51, 284, 339, 13, 51264, 51264, 400, 321, 486, 483, 257, 4748, 819, 19618, 13, 51464, 51464, 961, 311, 14322, 264, 18161, 2533, 13, 51564, 51564, 440, 1412, 307, 264, 912, 13, 51614, 51614, 639, 307, 586, 11, 286, 519, 11, 3006, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07721243585859026, "compression_ratio": 1.575221238938053, "no_speech_prob": 4.425428414833732e-06}, {"id": 1515, "seek": 793100, "start": 7953.0, "end": 7955.0, "text": " Let's reset the neural net.", "tokens": [50364, 583, 321, 643, 281, 652, 988, 300, 321, 14322, 613, 2771, 82, 281, 4018, 370, 300, 562, 321, 360, 23897, 11, 439, 295, 552, 722, 412, 4018, 293, 264, 3539, 23897, 1320, 12989, 26192, 264, 4470, 33733, 666, 264, 2771, 82, 13, 51064, 51064, 407, 341, 307, 4018, 2771, 294, 9953, 51, 284, 339, 13, 51264, 51264, 400, 321, 486, 483, 257, 4748, 819, 19618, 13, 51464, 51464, 961, 311, 14322, 264, 18161, 2533, 13, 51564, 51564, 440, 1412, 307, 264, 912, 13, 51614, 51614, 639, 307, 586, 11, 286, 519, 11, 3006, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07721243585859026, "compression_ratio": 1.575221238938053, "no_speech_prob": 4.425428414833732e-06}, {"id": 1516, "seek": 793100, "start": 7955.0, "end": 7956.0, "text": " The data is the same.", "tokens": [50364, 583, 321, 643, 281, 652, 988, 300, 321, 14322, 613, 2771, 82, 281, 4018, 370, 300, 562, 321, 360, 23897, 11, 439, 295, 552, 722, 412, 4018, 293, 264, 3539, 23897, 1320, 12989, 26192, 264, 4470, 33733, 666, 264, 2771, 82, 13, 51064, 51064, 407, 341, 307, 4018, 2771, 294, 9953, 51, 284, 339, 13, 51264, 51264, 400, 321, 486, 483, 257, 4748, 819, 19618, 13, 51464, 51464, 961, 311, 14322, 264, 18161, 2533, 13, 51564, 51564, 440, 1412, 307, 264, 912, 13, 51614, 51614, 639, 307, 586, 11, 286, 519, 11, 3006, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07721243585859026, "compression_ratio": 1.575221238938053, "no_speech_prob": 4.425428414833732e-06}, {"id": 1517, "seek": 793100, "start": 7956.0, "end": 7958.0, "text": " This is now, I think, correct.", "tokens": [50364, 583, 321, 643, 281, 652, 988, 300, 321, 14322, 613, 2771, 82, 281, 4018, 370, 300, 562, 321, 360, 23897, 11, 439, 295, 552, 722, 412, 4018, 293, 264, 3539, 23897, 1320, 12989, 26192, 264, 4470, 33733, 666, 264, 2771, 82, 13, 51064, 51064, 407, 341, 307, 4018, 2771, 294, 9953, 51, 284, 339, 13, 51264, 51264, 400, 321, 486, 483, 257, 4748, 819, 19618, 13, 51464, 51464, 961, 311, 14322, 264, 18161, 2533, 13, 51564, 51564, 440, 1412, 307, 264, 912, 13, 51614, 51614, 639, 307, 586, 11, 286, 519, 11, 3006, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07721243585859026, "compression_ratio": 1.575221238938053, "no_speech_prob": 4.425428414833732e-06}, {"id": 1518, "seek": 795800, "start": 7958.0, "end": 7964.0, "text": " And we get a much more, you know, we get a much more slower descent.", "tokens": [50364, 400, 321, 483, 257, 709, 544, 11, 291, 458, 11, 321, 483, 257, 709, 544, 14009, 23475, 13, 50664, 50664, 492, 920, 917, 493, 365, 1238, 665, 3542, 293, 321, 393, 2354, 281, 10315, 544, 281, 483, 760, 3126, 293, 3126, 293, 3126, 13, 51164, 51164, 865, 13, 51264, 51264, 407, 264, 787, 1778, 300, 264, 3894, 551, 2732, 11, 309, 311, 4664, 7426, 1480, 13, 51414, 51414, 440, 787, 1778, 300, 2732, 307, 300, 341, 307, 257, 588, 11, 588, 2199, 1154, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07948866215619174, "compression_ratio": 1.700507614213198, "no_speech_prob": 1.045132285071304e-05}, {"id": 1519, "seek": 795800, "start": 7964.0, "end": 7974.0, "text": " We still end up with pretty good results and we can continue to submit more to get down lower and lower and lower.", "tokens": [50364, 400, 321, 483, 257, 709, 544, 11, 291, 458, 11, 321, 483, 257, 709, 544, 14009, 23475, 13, 50664, 50664, 492, 920, 917, 493, 365, 1238, 665, 3542, 293, 321, 393, 2354, 281, 10315, 544, 281, 483, 760, 3126, 293, 3126, 293, 3126, 13, 51164, 51164, 865, 13, 51264, 51264, 407, 264, 787, 1778, 300, 264, 3894, 551, 2732, 11, 309, 311, 4664, 7426, 1480, 13, 51414, 51414, 440, 787, 1778, 300, 2732, 307, 300, 341, 307, 257, 588, 11, 588, 2199, 1154, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07948866215619174, "compression_ratio": 1.700507614213198, "no_speech_prob": 1.045132285071304e-05}, {"id": 1520, "seek": 795800, "start": 7974.0, "end": 7976.0, "text": " Yeah.", "tokens": [50364, 400, 321, 483, 257, 709, 544, 11, 291, 458, 11, 321, 483, 257, 709, 544, 14009, 23475, 13, 50664, 50664, 492, 920, 917, 493, 365, 1238, 665, 3542, 293, 321, 393, 2354, 281, 10315, 544, 281, 483, 760, 3126, 293, 3126, 293, 3126, 13, 51164, 51164, 865, 13, 51264, 51264, 407, 264, 787, 1778, 300, 264, 3894, 551, 2732, 11, 309, 311, 4664, 7426, 1480, 13, 51414, 51414, 440, 787, 1778, 300, 2732, 307, 300, 341, 307, 257, 588, 11, 588, 2199, 1154, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07948866215619174, "compression_ratio": 1.700507614213198, "no_speech_prob": 1.045132285071304e-05}, {"id": 1521, "seek": 795800, "start": 7976.0, "end": 7979.0, "text": " So the only reason that the previous thing worked, it's extremely buggy.", "tokens": [50364, 400, 321, 483, 257, 709, 544, 11, 291, 458, 11, 321, 483, 257, 709, 544, 14009, 23475, 13, 50664, 50664, 492, 920, 917, 493, 365, 1238, 665, 3542, 293, 321, 393, 2354, 281, 10315, 544, 281, 483, 760, 3126, 293, 3126, 293, 3126, 13, 51164, 51164, 865, 13, 51264, 51264, 407, 264, 787, 1778, 300, 264, 3894, 551, 2732, 11, 309, 311, 4664, 7426, 1480, 13, 51414, 51414, 440, 787, 1778, 300, 2732, 307, 300, 341, 307, 257, 588, 11, 588, 2199, 1154, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07948866215619174, "compression_ratio": 1.700507614213198, "no_speech_prob": 1.045132285071304e-05}, {"id": 1522, "seek": 795800, "start": 7979.0, "end": 7985.0, "text": " The only reason that worked is that this is a very, very simple problem.", "tokens": [50364, 400, 321, 483, 257, 709, 544, 11, 291, 458, 11, 321, 483, 257, 709, 544, 14009, 23475, 13, 50664, 50664, 492, 920, 917, 493, 365, 1238, 665, 3542, 293, 321, 393, 2354, 281, 10315, 544, 281, 483, 760, 3126, 293, 3126, 293, 3126, 13, 51164, 51164, 865, 13, 51264, 51264, 407, 264, 787, 1778, 300, 264, 3894, 551, 2732, 11, 309, 311, 4664, 7426, 1480, 13, 51414, 51414, 440, 787, 1778, 300, 2732, 307, 300, 341, 307, 257, 588, 11, 588, 2199, 1154, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07948866215619174, "compression_ratio": 1.700507614213198, "no_speech_prob": 1.045132285071304e-05}, {"id": 1523, "seek": 798500, "start": 7985.0, "end": 7989.0, "text": " And it's very easy for this neural net to fit this data.", "tokens": [50364, 400, 309, 311, 588, 1858, 337, 341, 18161, 2533, 281, 3318, 341, 1412, 13, 50564, 50564, 400, 370, 264, 2771, 82, 4590, 493, 12989, 12162, 293, 309, 8659, 2729, 505, 257, 5994, 1823, 2744, 293, 309, 1027, 505, 41881, 4664, 2370, 13, 51064, 51064, 583, 1936, 586, 321, 362, 281, 360, 544, 4439, 281, 483, 281, 588, 2295, 4190, 295, 4470, 293, 483, 343, 9139, 3850, 35, 281, 312, 534, 665, 13, 51414, 51414, 492, 393, 853, 281, 1823, 257, 857, 5044, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0710710711862849, "compression_ratio": 1.5352112676056338, "no_speech_prob": 9.817906175157987e-06}, {"id": 1524, "seek": 798500, "start": 7989.0, "end": 7999.0, "text": " And so the grads ended up accumulating and it effectively gave us a massive step size and it made us converge extremely fast.", "tokens": [50364, 400, 309, 311, 588, 1858, 337, 341, 18161, 2533, 281, 3318, 341, 1412, 13, 50564, 50564, 400, 370, 264, 2771, 82, 4590, 493, 12989, 12162, 293, 309, 8659, 2729, 505, 257, 5994, 1823, 2744, 293, 309, 1027, 505, 41881, 4664, 2370, 13, 51064, 51064, 583, 1936, 586, 321, 362, 281, 360, 544, 4439, 281, 483, 281, 588, 2295, 4190, 295, 4470, 293, 483, 343, 9139, 3850, 35, 281, 312, 534, 665, 13, 51414, 51414, 492, 393, 853, 281, 1823, 257, 857, 5044, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0710710711862849, "compression_ratio": 1.5352112676056338, "no_speech_prob": 9.817906175157987e-06}, {"id": 1525, "seek": 798500, "start": 7999.0, "end": 8006.0, "text": " But basically now we have to do more steps to get to very low values of loss and get WIPRED to be really good.", "tokens": [50364, 400, 309, 311, 588, 1858, 337, 341, 18161, 2533, 281, 3318, 341, 1412, 13, 50564, 50564, 400, 370, 264, 2771, 82, 4590, 493, 12989, 12162, 293, 309, 8659, 2729, 505, 257, 5994, 1823, 2744, 293, 309, 1027, 505, 41881, 4664, 2370, 13, 51064, 51064, 583, 1936, 586, 321, 362, 281, 360, 544, 4439, 281, 483, 281, 588, 2295, 4190, 295, 4470, 293, 483, 343, 9139, 3850, 35, 281, 312, 534, 665, 13, 51414, 51414, 492, 393, 853, 281, 1823, 257, 857, 5044, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0710710711862849, "compression_ratio": 1.5352112676056338, "no_speech_prob": 9.817906175157987e-06}, {"id": 1526, "seek": 798500, "start": 8006.0, "end": 8014.0, "text": " We can try to step a bit greater.", "tokens": [50364, 400, 309, 311, 588, 1858, 337, 341, 18161, 2533, 281, 3318, 341, 1412, 13, 50564, 50564, 400, 370, 264, 2771, 82, 4590, 493, 12989, 12162, 293, 309, 8659, 2729, 505, 257, 5994, 1823, 2744, 293, 309, 1027, 505, 41881, 4664, 2370, 13, 51064, 51064, 583, 1936, 586, 321, 362, 281, 360, 544, 4439, 281, 483, 281, 588, 2295, 4190, 295, 4470, 293, 483, 343, 9139, 3850, 35, 281, 312, 534, 665, 13, 51414, 51414, 492, 393, 853, 281, 1823, 257, 857, 5044, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0710710711862849, "compression_ratio": 1.5352112676056338, "no_speech_prob": 9.817906175157987e-06}, {"id": 1527, "seek": 801400, "start": 8014.0, "end": 8018.0, "text": " Yeah, we're going to get closer and closer to one minus one and one.", "tokens": [50364, 865, 11, 321, 434, 516, 281, 483, 4966, 293, 4966, 281, 472, 3175, 472, 293, 472, 13, 50564, 50564, 407, 1364, 365, 18161, 36170, 307, 2171, 12414, 570, 291, 815, 362, 3195, 295, 15120, 294, 264, 3089, 293, 428, 3209, 1062, 767, 589, 445, 411, 11896, 2732, 13, 51214, 51214, 583, 10486, 366, 307, 300, 498, 321, 632, 257, 544, 3997, 1154, 11, 550, 767, 341, 7426, 576, 362, 1027, 505, 406, 19719, 264, 4470, 588, 731, 13, 51564, 51564, 400, 321, 645, 787, 1075, 281, 483, 1314, 365, 309, 570, 264, 1154, 307, 588, 2199, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.060291186417683516, "compression_ratio": 1.6526717557251909, "no_speech_prob": 2.8129320526204538e-06}, {"id": 1528, "seek": 801400, "start": 8018.0, "end": 8031.0, "text": " So working with neural nets is sometimes tricky because you may have lots of bugs in the code and your network might actually work just like ours worked.", "tokens": [50364, 865, 11, 321, 434, 516, 281, 483, 4966, 293, 4966, 281, 472, 3175, 472, 293, 472, 13, 50564, 50564, 407, 1364, 365, 18161, 36170, 307, 2171, 12414, 570, 291, 815, 362, 3195, 295, 15120, 294, 264, 3089, 293, 428, 3209, 1062, 767, 589, 445, 411, 11896, 2732, 13, 51214, 51214, 583, 10486, 366, 307, 300, 498, 321, 632, 257, 544, 3997, 1154, 11, 550, 767, 341, 7426, 576, 362, 1027, 505, 406, 19719, 264, 4470, 588, 731, 13, 51564, 51564, 400, 321, 645, 787, 1075, 281, 483, 1314, 365, 309, 570, 264, 1154, 307, 588, 2199, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.060291186417683516, "compression_ratio": 1.6526717557251909, "no_speech_prob": 2.8129320526204538e-06}, {"id": 1529, "seek": 801400, "start": 8031.0, "end": 8038.0, "text": " But chances are is that if we had a more complex problem, then actually this bug would have made us not optimize the loss very well.", "tokens": [50364, 865, 11, 321, 434, 516, 281, 483, 4966, 293, 4966, 281, 472, 3175, 472, 293, 472, 13, 50564, 50564, 407, 1364, 365, 18161, 36170, 307, 2171, 12414, 570, 291, 815, 362, 3195, 295, 15120, 294, 264, 3089, 293, 428, 3209, 1062, 767, 589, 445, 411, 11896, 2732, 13, 51214, 51214, 583, 10486, 366, 307, 300, 498, 321, 632, 257, 544, 3997, 1154, 11, 550, 767, 341, 7426, 576, 362, 1027, 505, 406, 19719, 264, 4470, 588, 731, 13, 51564, 51564, 400, 321, 645, 787, 1075, 281, 483, 1314, 365, 309, 570, 264, 1154, 307, 588, 2199, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.060291186417683516, "compression_ratio": 1.6526717557251909, "no_speech_prob": 2.8129320526204538e-06}, {"id": 1530, "seek": 801400, "start": 8038.0, "end": 8043.0, "text": " And we were only able to get away with it because the problem is very simple.", "tokens": [50364, 865, 11, 321, 434, 516, 281, 483, 4966, 293, 4966, 281, 472, 3175, 472, 293, 472, 13, 50564, 50564, 407, 1364, 365, 18161, 36170, 307, 2171, 12414, 570, 291, 815, 362, 3195, 295, 15120, 294, 264, 3089, 293, 428, 3209, 1062, 767, 589, 445, 411, 11896, 2732, 13, 51214, 51214, 583, 10486, 366, 307, 300, 498, 321, 632, 257, 544, 3997, 1154, 11, 550, 767, 341, 7426, 576, 362, 1027, 505, 406, 19719, 264, 4470, 588, 731, 13, 51564, 51564, 400, 321, 645, 787, 1075, 281, 483, 1314, 365, 309, 570, 264, 1154, 307, 588, 2199, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.060291186417683516, "compression_ratio": 1.6526717557251909, "no_speech_prob": 2.8129320526204538e-06}, {"id": 1531, "seek": 804300, "start": 8043.0, "end": 8047.0, "text": " So let's now bring everything together and summarize what we learned.", "tokens": [50364, 407, 718, 311, 586, 1565, 1203, 1214, 293, 20858, 437, 321, 3264, 13, 50564, 50564, 708, 366, 18161, 36170, 30, 1734, 1807, 36170, 366, 613, 18894, 15277, 11, 6457, 2199, 18894, 15277, 294, 264, 1389, 295, 2120, 388, 11167, 43276, 2044, 11, 50964, 50964, 300, 747, 4846, 382, 264, 1412, 293, 436, 747, 4846, 11, 264, 17443, 293, 264, 9834, 295, 264, 18161, 2533, 11, 18894, 6114, 337, 264, 2128, 1320, 11, 51414, 51414, 6263, 538, 257, 4470, 2445, 293, 264, 4470, 2445, 9898, 281, 3481, 264, 14170, 295, 264, 21264, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10337957739830017, "compression_ratio": 1.8588709677419355, "no_speech_prob": 1.6440642866655253e-05}, {"id": 1532, "seek": 804300, "start": 8047.0, "end": 8055.0, "text": " What are neural nets? Neural nets are these mathematical expressions, fairly simple mathematical expressions in the case of multilayer perceptron,", "tokens": [50364, 407, 718, 311, 586, 1565, 1203, 1214, 293, 20858, 437, 321, 3264, 13, 50564, 50564, 708, 366, 18161, 36170, 30, 1734, 1807, 36170, 366, 613, 18894, 15277, 11, 6457, 2199, 18894, 15277, 294, 264, 1389, 295, 2120, 388, 11167, 43276, 2044, 11, 50964, 50964, 300, 747, 4846, 382, 264, 1412, 293, 436, 747, 4846, 11, 264, 17443, 293, 264, 9834, 295, 264, 18161, 2533, 11, 18894, 6114, 337, 264, 2128, 1320, 11, 51414, 51414, 6263, 538, 257, 4470, 2445, 293, 264, 4470, 2445, 9898, 281, 3481, 264, 14170, 295, 264, 21264, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10337957739830017, "compression_ratio": 1.8588709677419355, "no_speech_prob": 1.6440642866655253e-05}, {"id": 1533, "seek": 804300, "start": 8055.0, "end": 8064.0, "text": " that take input as the data and they take input, the weights and the parameters of the neural net, mathematical expression for the forward pass,", "tokens": [50364, 407, 718, 311, 586, 1565, 1203, 1214, 293, 20858, 437, 321, 3264, 13, 50564, 50564, 708, 366, 18161, 36170, 30, 1734, 1807, 36170, 366, 613, 18894, 15277, 11, 6457, 2199, 18894, 15277, 294, 264, 1389, 295, 2120, 388, 11167, 43276, 2044, 11, 50964, 50964, 300, 747, 4846, 382, 264, 1412, 293, 436, 747, 4846, 11, 264, 17443, 293, 264, 9834, 295, 264, 18161, 2533, 11, 18894, 6114, 337, 264, 2128, 1320, 11, 51414, 51414, 6263, 538, 257, 4470, 2445, 293, 264, 4470, 2445, 9898, 281, 3481, 264, 14170, 295, 264, 21264, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10337957739830017, "compression_ratio": 1.8588709677419355, "no_speech_prob": 1.6440642866655253e-05}, {"id": 1534, "seek": 804300, "start": 8064.0, "end": 8069.0, "text": " followed by a loss function and the loss function tries to measure the accuracy of the predictions.", "tokens": [50364, 407, 718, 311, 586, 1565, 1203, 1214, 293, 20858, 437, 321, 3264, 13, 50564, 50564, 708, 366, 18161, 36170, 30, 1734, 1807, 36170, 366, 613, 18894, 15277, 11, 6457, 2199, 18894, 15277, 294, 264, 1389, 295, 2120, 388, 11167, 43276, 2044, 11, 50964, 50964, 300, 747, 4846, 382, 264, 1412, 293, 436, 747, 4846, 11, 264, 17443, 293, 264, 9834, 295, 264, 18161, 2533, 11, 18894, 6114, 337, 264, 2128, 1320, 11, 51414, 51414, 6263, 538, 257, 4470, 2445, 293, 264, 4470, 2445, 9898, 281, 3481, 264, 14170, 295, 264, 21264, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10337957739830017, "compression_ratio": 1.8588709677419355, "no_speech_prob": 1.6440642866655253e-05}, {"id": 1535, "seek": 806900, "start": 8069.0, "end": 8076.0, "text": " And usually the loss will be low when your predictions are matching your targets or where the network is basically behaving well.", "tokens": [50364, 400, 2673, 264, 4470, 486, 312, 2295, 562, 428, 21264, 366, 14324, 428, 12911, 420, 689, 264, 3209, 307, 1936, 35263, 731, 13, 50714, 50714, 407, 321, 20459, 264, 4470, 2445, 370, 300, 562, 264, 4470, 307, 2295, 11, 264, 3209, 307, 884, 437, 291, 528, 309, 281, 360, 322, 428, 1154, 13, 51114, 51114, 400, 550, 321, 23897, 264, 4470, 11, 764, 646, 79, 1513, 559, 399, 281, 483, 264, 16235, 11, 293, 550, 321, 458, 577, 281, 10864, 439, 264, 9834, 281, 11514, 264, 4470, 16143, 13, 51564, 51564, 583, 550, 321, 362, 281, 44497, 300, 1399, 867, 1413, 294, 437, 311, 1219, 264, 16235, 23475, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.04909228434604881, "compression_ratio": 1.8507462686567164, "no_speech_prob": 8.1392554420745e-06}, {"id": 1536, "seek": 806900, "start": 8076.0, "end": 8084.0, "text": " So we manipulate the loss function so that when the loss is low, the network is doing what you want it to do on your problem.", "tokens": [50364, 400, 2673, 264, 4470, 486, 312, 2295, 562, 428, 21264, 366, 14324, 428, 12911, 420, 689, 264, 3209, 307, 1936, 35263, 731, 13, 50714, 50714, 407, 321, 20459, 264, 4470, 2445, 370, 300, 562, 264, 4470, 307, 2295, 11, 264, 3209, 307, 884, 437, 291, 528, 309, 281, 360, 322, 428, 1154, 13, 51114, 51114, 400, 550, 321, 23897, 264, 4470, 11, 764, 646, 79, 1513, 559, 399, 281, 483, 264, 16235, 11, 293, 550, 321, 458, 577, 281, 10864, 439, 264, 9834, 281, 11514, 264, 4470, 16143, 13, 51564, 51564, 583, 550, 321, 362, 281, 44497, 300, 1399, 867, 1413, 294, 437, 311, 1219, 264, 16235, 23475, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.04909228434604881, "compression_ratio": 1.8507462686567164, "no_speech_prob": 8.1392554420745e-06}, {"id": 1537, "seek": 806900, "start": 8084.0, "end": 8093.0, "text": " And then we backward the loss, use backpropagation to get the gradient, and then we know how to tune all the parameters to decrease the loss locally.", "tokens": [50364, 400, 2673, 264, 4470, 486, 312, 2295, 562, 428, 21264, 366, 14324, 428, 12911, 420, 689, 264, 3209, 307, 1936, 35263, 731, 13, 50714, 50714, 407, 321, 20459, 264, 4470, 2445, 370, 300, 562, 264, 4470, 307, 2295, 11, 264, 3209, 307, 884, 437, 291, 528, 309, 281, 360, 322, 428, 1154, 13, 51114, 51114, 400, 550, 321, 23897, 264, 4470, 11, 764, 646, 79, 1513, 559, 399, 281, 483, 264, 16235, 11, 293, 550, 321, 458, 577, 281, 10864, 439, 264, 9834, 281, 11514, 264, 4470, 16143, 13, 51564, 51564, 583, 550, 321, 362, 281, 44497, 300, 1399, 867, 1413, 294, 437, 311, 1219, 264, 16235, 23475, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.04909228434604881, "compression_ratio": 1.8507462686567164, "no_speech_prob": 8.1392554420745e-06}, {"id": 1538, "seek": 806900, "start": 8093.0, "end": 8097.0, "text": " But then we have to iterate that process many times in what's called the gradient descent.", "tokens": [50364, 400, 2673, 264, 4470, 486, 312, 2295, 562, 428, 21264, 366, 14324, 428, 12911, 420, 689, 264, 3209, 307, 1936, 35263, 731, 13, 50714, 50714, 407, 321, 20459, 264, 4470, 2445, 370, 300, 562, 264, 4470, 307, 2295, 11, 264, 3209, 307, 884, 437, 291, 528, 309, 281, 360, 322, 428, 1154, 13, 51114, 51114, 400, 550, 321, 23897, 264, 4470, 11, 764, 646, 79, 1513, 559, 399, 281, 483, 264, 16235, 11, 293, 550, 321, 458, 577, 281, 10864, 439, 264, 9834, 281, 11514, 264, 4470, 16143, 13, 51564, 51564, 583, 550, 321, 362, 281, 44497, 300, 1399, 867, 1413, 294, 437, 311, 1219, 264, 16235, 23475, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.04909228434604881, "compression_ratio": 1.8507462686567164, "no_speech_prob": 8.1392554420745e-06}, {"id": 1539, "seek": 809700, "start": 8097.0, "end": 8104.0, "text": " So we simply follow the gradient information and that minimizes the loss and the loss is arranged so that when the loss is minimized,", "tokens": [50364, 407, 321, 2935, 1524, 264, 16235, 1589, 293, 300, 4464, 5660, 264, 4470, 293, 264, 4470, 307, 18721, 370, 300, 562, 264, 4470, 307, 4464, 1602, 11, 50714, 50714, 264, 3209, 307, 884, 437, 291, 528, 309, 281, 360, 13, 50814, 50814, 400, 1338, 11, 370, 321, 445, 362, 257, 46115, 295, 18161, 1507, 293, 321, 393, 652, 309, 360, 23211, 721, 13, 51164, 51164, 400, 300, 311, 437, 2709, 18161, 36170, 641, 1347, 13, 51264, 51264, 467, 311, 11, 291, 458, 11, 341, 307, 257, 588, 5870, 3209, 365, 18173, 9834, 11, 51464, 51464, 457, 291, 393, 1322, 10591, 544, 6179, 18161, 36170, 365, 17375, 11, 412, 341, 935, 11, 1920, 504, 46279, 295, 9834, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08335205740179898, "compression_ratio": 1.7517482517482517, "no_speech_prob": 2.4824107640597504e-06}, {"id": 1540, "seek": 809700, "start": 8104.0, "end": 8106.0, "text": " the network is doing what you want it to do.", "tokens": [50364, 407, 321, 2935, 1524, 264, 16235, 1589, 293, 300, 4464, 5660, 264, 4470, 293, 264, 4470, 307, 18721, 370, 300, 562, 264, 4470, 307, 4464, 1602, 11, 50714, 50714, 264, 3209, 307, 884, 437, 291, 528, 309, 281, 360, 13, 50814, 50814, 400, 1338, 11, 370, 321, 445, 362, 257, 46115, 295, 18161, 1507, 293, 321, 393, 652, 309, 360, 23211, 721, 13, 51164, 51164, 400, 300, 311, 437, 2709, 18161, 36170, 641, 1347, 13, 51264, 51264, 467, 311, 11, 291, 458, 11, 341, 307, 257, 588, 5870, 3209, 365, 18173, 9834, 11, 51464, 51464, 457, 291, 393, 1322, 10591, 544, 6179, 18161, 36170, 365, 17375, 11, 412, 341, 935, 11, 1920, 504, 46279, 295, 9834, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08335205740179898, "compression_ratio": 1.7517482517482517, "no_speech_prob": 2.4824107640597504e-06}, {"id": 1541, "seek": 809700, "start": 8106.0, "end": 8113.0, "text": " And yeah, so we just have a blob of neural stuff and we can make it do arbitrary things.", "tokens": [50364, 407, 321, 2935, 1524, 264, 16235, 1589, 293, 300, 4464, 5660, 264, 4470, 293, 264, 4470, 307, 18721, 370, 300, 562, 264, 4470, 307, 4464, 1602, 11, 50714, 50714, 264, 3209, 307, 884, 437, 291, 528, 309, 281, 360, 13, 50814, 50814, 400, 1338, 11, 370, 321, 445, 362, 257, 46115, 295, 18161, 1507, 293, 321, 393, 652, 309, 360, 23211, 721, 13, 51164, 51164, 400, 300, 311, 437, 2709, 18161, 36170, 641, 1347, 13, 51264, 51264, 467, 311, 11, 291, 458, 11, 341, 307, 257, 588, 5870, 3209, 365, 18173, 9834, 11, 51464, 51464, 457, 291, 393, 1322, 10591, 544, 6179, 18161, 36170, 365, 17375, 11, 412, 341, 935, 11, 1920, 504, 46279, 295, 9834, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08335205740179898, "compression_ratio": 1.7517482517482517, "no_speech_prob": 2.4824107640597504e-06}, {"id": 1542, "seek": 809700, "start": 8113.0, "end": 8115.0, "text": " And that's what gives neural nets their power.", "tokens": [50364, 407, 321, 2935, 1524, 264, 16235, 1589, 293, 300, 4464, 5660, 264, 4470, 293, 264, 4470, 307, 18721, 370, 300, 562, 264, 4470, 307, 4464, 1602, 11, 50714, 50714, 264, 3209, 307, 884, 437, 291, 528, 309, 281, 360, 13, 50814, 50814, 400, 1338, 11, 370, 321, 445, 362, 257, 46115, 295, 18161, 1507, 293, 321, 393, 652, 309, 360, 23211, 721, 13, 51164, 51164, 400, 300, 311, 437, 2709, 18161, 36170, 641, 1347, 13, 51264, 51264, 467, 311, 11, 291, 458, 11, 341, 307, 257, 588, 5870, 3209, 365, 18173, 9834, 11, 51464, 51464, 457, 291, 393, 1322, 10591, 544, 6179, 18161, 36170, 365, 17375, 11, 412, 341, 935, 11, 1920, 504, 46279, 295, 9834, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08335205740179898, "compression_ratio": 1.7517482517482517, "no_speech_prob": 2.4824107640597504e-06}, {"id": 1543, "seek": 809700, "start": 8115.0, "end": 8119.0, "text": " It's, you know, this is a very tiny network with 41 parameters,", "tokens": [50364, 407, 321, 2935, 1524, 264, 16235, 1589, 293, 300, 4464, 5660, 264, 4470, 293, 264, 4470, 307, 18721, 370, 300, 562, 264, 4470, 307, 4464, 1602, 11, 50714, 50714, 264, 3209, 307, 884, 437, 291, 528, 309, 281, 360, 13, 50814, 50814, 400, 1338, 11, 370, 321, 445, 362, 257, 46115, 295, 18161, 1507, 293, 321, 393, 652, 309, 360, 23211, 721, 13, 51164, 51164, 400, 300, 311, 437, 2709, 18161, 36170, 641, 1347, 13, 51264, 51264, 467, 311, 11, 291, 458, 11, 341, 307, 257, 588, 5870, 3209, 365, 18173, 9834, 11, 51464, 51464, 457, 291, 393, 1322, 10591, 544, 6179, 18161, 36170, 365, 17375, 11, 412, 341, 935, 11, 1920, 504, 46279, 295, 9834, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08335205740179898, "compression_ratio": 1.7517482517482517, "no_speech_prob": 2.4824107640597504e-06}, {"id": 1544, "seek": 809700, "start": 8119.0, "end": 8126.0, "text": " but you can build significantly more complicated neural nets with billions, at this point, almost trillions of parameters.", "tokens": [50364, 407, 321, 2935, 1524, 264, 16235, 1589, 293, 300, 4464, 5660, 264, 4470, 293, 264, 4470, 307, 18721, 370, 300, 562, 264, 4470, 307, 4464, 1602, 11, 50714, 50714, 264, 3209, 307, 884, 437, 291, 528, 309, 281, 360, 13, 50814, 50814, 400, 1338, 11, 370, 321, 445, 362, 257, 46115, 295, 18161, 1507, 293, 321, 393, 652, 309, 360, 23211, 721, 13, 51164, 51164, 400, 300, 311, 437, 2709, 18161, 36170, 641, 1347, 13, 51264, 51264, 467, 311, 11, 291, 458, 11, 341, 307, 257, 588, 5870, 3209, 365, 18173, 9834, 11, 51464, 51464, 457, 291, 393, 1322, 10591, 544, 6179, 18161, 36170, 365, 17375, 11, 412, 341, 935, 11, 1920, 504, 46279, 295, 9834, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08335205740179898, "compression_ratio": 1.7517482517482517, "no_speech_prob": 2.4824107640597504e-06}, {"id": 1545, "seek": 812600, "start": 8126.0, "end": 8132.0, "text": " And it's a massive blob of neural tissue, simulated neural tissue, roughly speaking.", "tokens": [50364, 400, 309, 311, 257, 5994, 46115, 295, 18161, 12404, 11, 41713, 18161, 12404, 11, 9810, 4124, 13, 50664, 50664, 400, 291, 393, 652, 309, 360, 4664, 3997, 2740, 13, 50814, 50814, 400, 613, 18161, 36170, 550, 362, 439, 3685, 295, 588, 10343, 4345, 6930, 7221, 294, 562, 291, 853, 281, 652, 552, 360, 10591, 1152, 2740, 13, 51314, 51314, 1018, 294, 264, 1389, 295, 26039, 51, 11, 337, 1365, 11, 321, 362, 5994, 11663, 295, 2487, 490, 264, 7703, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06916164784204393, "compression_ratio": 1.6150442477876106, "no_speech_prob": 5.507347395905526e-06}, {"id": 1546, "seek": 812600, "start": 8132.0, "end": 8135.0, "text": " And you can make it do extremely complex problems.", "tokens": [50364, 400, 309, 311, 257, 5994, 46115, 295, 18161, 12404, 11, 41713, 18161, 12404, 11, 9810, 4124, 13, 50664, 50664, 400, 291, 393, 652, 309, 360, 4664, 3997, 2740, 13, 50814, 50814, 400, 613, 18161, 36170, 550, 362, 439, 3685, 295, 588, 10343, 4345, 6930, 7221, 294, 562, 291, 853, 281, 652, 552, 360, 10591, 1152, 2740, 13, 51314, 51314, 1018, 294, 264, 1389, 295, 26039, 51, 11, 337, 1365, 11, 321, 362, 5994, 11663, 295, 2487, 490, 264, 7703, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06916164784204393, "compression_ratio": 1.6150442477876106, "no_speech_prob": 5.507347395905526e-06}, {"id": 1547, "seek": 812600, "start": 8135.0, "end": 8145.0, "text": " And these neural nets then have all kinds of very fascinating emergent properties in when you try to make them do significantly hard problems.", "tokens": [50364, 400, 309, 311, 257, 5994, 46115, 295, 18161, 12404, 11, 41713, 18161, 12404, 11, 9810, 4124, 13, 50664, 50664, 400, 291, 393, 652, 309, 360, 4664, 3997, 2740, 13, 50814, 50814, 400, 613, 18161, 36170, 550, 362, 439, 3685, 295, 588, 10343, 4345, 6930, 7221, 294, 562, 291, 853, 281, 652, 552, 360, 10591, 1152, 2740, 13, 51314, 51314, 1018, 294, 264, 1389, 295, 26039, 51, 11, 337, 1365, 11, 321, 362, 5994, 11663, 295, 2487, 490, 264, 7703, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06916164784204393, "compression_ratio": 1.6150442477876106, "no_speech_prob": 5.507347395905526e-06}, {"id": 1548, "seek": 812600, "start": 8145.0, "end": 8150.0, "text": " As in the case of GPT, for example, we have massive amounts of text from the Internet.", "tokens": [50364, 400, 309, 311, 257, 5994, 46115, 295, 18161, 12404, 11, 41713, 18161, 12404, 11, 9810, 4124, 13, 50664, 50664, 400, 291, 393, 652, 309, 360, 4664, 3997, 2740, 13, 50814, 50814, 400, 613, 18161, 36170, 550, 362, 439, 3685, 295, 588, 10343, 4345, 6930, 7221, 294, 562, 291, 853, 281, 652, 552, 360, 10591, 1152, 2740, 13, 51314, 51314, 1018, 294, 264, 1389, 295, 26039, 51, 11, 337, 1365, 11, 321, 362, 5994, 11663, 295, 2487, 490, 264, 7703, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.06916164784204393, "compression_ratio": 1.6150442477876106, "no_speech_prob": 5.507347395905526e-06}, {"id": 1549, "seek": 815000, "start": 8150.0, "end": 8156.0, "text": " And we're trying to get neural nets to predict, to take like a few words and try to predict the next word in a sequence.", "tokens": [50364, 400, 321, 434, 1382, 281, 483, 18161, 36170, 281, 6069, 11, 281, 747, 411, 257, 1326, 2283, 293, 853, 281, 6069, 264, 958, 1349, 294, 257, 8310, 13, 50664, 50664, 663, 311, 264, 2539, 1154, 13, 50714, 50714, 400, 309, 4523, 484, 300, 562, 291, 3847, 341, 322, 439, 295, 7703, 11, 264, 18161, 2533, 767, 575, 411, 534, 12802, 4345, 6930, 7221, 13, 51014, 51014, 583, 300, 18161, 2533, 576, 362, 6779, 295, 17375, 295, 9834, 13, 51214, 51214, 583, 309, 1985, 322, 17879, 264, 1900, 912, 9156, 13, 51364, 51364, 440, 18161, 2533, 11, 295, 1164, 11, 486, 312, 257, 857, 544, 3997, 13, 51514, 51514, 583, 5911, 11, 264, 2158, 294, 264, 16235, 307, 456, 293, 486, 312, 14800, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07981980691744586, "compression_ratio": 1.7540983606557377, "no_speech_prob": 5.014358976040967e-06}, {"id": 1550, "seek": 815000, "start": 8156.0, "end": 8157.0, "text": " That's the learning problem.", "tokens": [50364, 400, 321, 434, 1382, 281, 483, 18161, 36170, 281, 6069, 11, 281, 747, 411, 257, 1326, 2283, 293, 853, 281, 6069, 264, 958, 1349, 294, 257, 8310, 13, 50664, 50664, 663, 311, 264, 2539, 1154, 13, 50714, 50714, 400, 309, 4523, 484, 300, 562, 291, 3847, 341, 322, 439, 295, 7703, 11, 264, 18161, 2533, 767, 575, 411, 534, 12802, 4345, 6930, 7221, 13, 51014, 51014, 583, 300, 18161, 2533, 576, 362, 6779, 295, 17375, 295, 9834, 13, 51214, 51214, 583, 309, 1985, 322, 17879, 264, 1900, 912, 9156, 13, 51364, 51364, 440, 18161, 2533, 11, 295, 1164, 11, 486, 312, 257, 857, 544, 3997, 13, 51514, 51514, 583, 5911, 11, 264, 2158, 294, 264, 16235, 307, 456, 293, 486, 312, 14800, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07981980691744586, "compression_ratio": 1.7540983606557377, "no_speech_prob": 5.014358976040967e-06}, {"id": 1551, "seek": 815000, "start": 8157.0, "end": 8163.0, "text": " And it turns out that when you train this on all of Internet, the neural net actually has like really remarkable emergent properties.", "tokens": [50364, 400, 321, 434, 1382, 281, 483, 18161, 36170, 281, 6069, 11, 281, 747, 411, 257, 1326, 2283, 293, 853, 281, 6069, 264, 958, 1349, 294, 257, 8310, 13, 50664, 50664, 663, 311, 264, 2539, 1154, 13, 50714, 50714, 400, 309, 4523, 484, 300, 562, 291, 3847, 341, 322, 439, 295, 7703, 11, 264, 18161, 2533, 767, 575, 411, 534, 12802, 4345, 6930, 7221, 13, 51014, 51014, 583, 300, 18161, 2533, 576, 362, 6779, 295, 17375, 295, 9834, 13, 51214, 51214, 583, 309, 1985, 322, 17879, 264, 1900, 912, 9156, 13, 51364, 51364, 440, 18161, 2533, 11, 295, 1164, 11, 486, 312, 257, 857, 544, 3997, 13, 51514, 51514, 583, 5911, 11, 264, 2158, 294, 264, 16235, 307, 456, 293, 486, 312, 14800, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07981980691744586, "compression_ratio": 1.7540983606557377, "no_speech_prob": 5.014358976040967e-06}, {"id": 1552, "seek": 815000, "start": 8163.0, "end": 8167.0, "text": " But that neural net would have hundreds of billions of parameters.", "tokens": [50364, 400, 321, 434, 1382, 281, 483, 18161, 36170, 281, 6069, 11, 281, 747, 411, 257, 1326, 2283, 293, 853, 281, 6069, 264, 958, 1349, 294, 257, 8310, 13, 50664, 50664, 663, 311, 264, 2539, 1154, 13, 50714, 50714, 400, 309, 4523, 484, 300, 562, 291, 3847, 341, 322, 439, 295, 7703, 11, 264, 18161, 2533, 767, 575, 411, 534, 12802, 4345, 6930, 7221, 13, 51014, 51014, 583, 300, 18161, 2533, 576, 362, 6779, 295, 17375, 295, 9834, 13, 51214, 51214, 583, 309, 1985, 322, 17879, 264, 1900, 912, 9156, 13, 51364, 51364, 440, 18161, 2533, 11, 295, 1164, 11, 486, 312, 257, 857, 544, 3997, 13, 51514, 51514, 583, 5911, 11, 264, 2158, 294, 264, 16235, 307, 456, 293, 486, 312, 14800, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07981980691744586, "compression_ratio": 1.7540983606557377, "no_speech_prob": 5.014358976040967e-06}, {"id": 1553, "seek": 815000, "start": 8167.0, "end": 8170.0, "text": " But it works on fundamentally the exact same principles.", "tokens": [50364, 400, 321, 434, 1382, 281, 483, 18161, 36170, 281, 6069, 11, 281, 747, 411, 257, 1326, 2283, 293, 853, 281, 6069, 264, 958, 1349, 294, 257, 8310, 13, 50664, 50664, 663, 311, 264, 2539, 1154, 13, 50714, 50714, 400, 309, 4523, 484, 300, 562, 291, 3847, 341, 322, 439, 295, 7703, 11, 264, 18161, 2533, 767, 575, 411, 534, 12802, 4345, 6930, 7221, 13, 51014, 51014, 583, 300, 18161, 2533, 576, 362, 6779, 295, 17375, 295, 9834, 13, 51214, 51214, 583, 309, 1985, 322, 17879, 264, 1900, 912, 9156, 13, 51364, 51364, 440, 18161, 2533, 11, 295, 1164, 11, 486, 312, 257, 857, 544, 3997, 13, 51514, 51514, 583, 5911, 11, 264, 2158, 294, 264, 16235, 307, 456, 293, 486, 312, 14800, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07981980691744586, "compression_ratio": 1.7540983606557377, "no_speech_prob": 5.014358976040967e-06}, {"id": 1554, "seek": 815000, "start": 8170.0, "end": 8173.0, "text": " The neural net, of course, will be a bit more complex.", "tokens": [50364, 400, 321, 434, 1382, 281, 483, 18161, 36170, 281, 6069, 11, 281, 747, 411, 257, 1326, 2283, 293, 853, 281, 6069, 264, 958, 1349, 294, 257, 8310, 13, 50664, 50664, 663, 311, 264, 2539, 1154, 13, 50714, 50714, 400, 309, 4523, 484, 300, 562, 291, 3847, 341, 322, 439, 295, 7703, 11, 264, 18161, 2533, 767, 575, 411, 534, 12802, 4345, 6930, 7221, 13, 51014, 51014, 583, 300, 18161, 2533, 576, 362, 6779, 295, 17375, 295, 9834, 13, 51214, 51214, 583, 309, 1985, 322, 17879, 264, 1900, 912, 9156, 13, 51364, 51364, 440, 18161, 2533, 11, 295, 1164, 11, 486, 312, 257, 857, 544, 3997, 13, 51514, 51514, 583, 5911, 11, 264, 2158, 294, 264, 16235, 307, 456, 293, 486, 312, 14800, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07981980691744586, "compression_ratio": 1.7540983606557377, "no_speech_prob": 5.014358976040967e-06}, {"id": 1555, "seek": 815000, "start": 8173.0, "end": 8178.0, "text": " But otherwise, the value in the gradient is there and will be identical.", "tokens": [50364, 400, 321, 434, 1382, 281, 483, 18161, 36170, 281, 6069, 11, 281, 747, 411, 257, 1326, 2283, 293, 853, 281, 6069, 264, 958, 1349, 294, 257, 8310, 13, 50664, 50664, 663, 311, 264, 2539, 1154, 13, 50714, 50714, 400, 309, 4523, 484, 300, 562, 291, 3847, 341, 322, 439, 295, 7703, 11, 264, 18161, 2533, 767, 575, 411, 534, 12802, 4345, 6930, 7221, 13, 51014, 51014, 583, 300, 18161, 2533, 576, 362, 6779, 295, 17375, 295, 9834, 13, 51214, 51214, 583, 309, 1985, 322, 17879, 264, 1900, 912, 9156, 13, 51364, 51364, 440, 18161, 2533, 11, 295, 1164, 11, 486, 312, 257, 857, 544, 3997, 13, 51514, 51514, 583, 5911, 11, 264, 2158, 294, 264, 16235, 307, 456, 293, 486, 312, 14800, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07981980691744586, "compression_ratio": 1.7540983606557377, "no_speech_prob": 5.014358976040967e-06}, {"id": 1556, "seek": 817800, "start": 8178.0, "end": 8182.0, "text": " And the gradient descent would be there and would be basically identical.", "tokens": [50364, 400, 264, 16235, 23475, 576, 312, 456, 293, 576, 312, 1936, 14800, 13, 50564, 50564, 583, 561, 2673, 764, 4748, 819, 9205, 13, 50664, 50664, 639, 307, 257, 588, 2199, 342, 8997, 2750, 16235, 23475, 5623, 13, 50914, 50914, 400, 264, 4470, 2445, 576, 406, 312, 914, 8889, 6713, 13, 51014, 51014, 814, 576, 312, 1228, 746, 1219, 264, 3278, 30867, 4470, 337, 32884, 264, 958, 14862, 13, 51214, 51214, 407, 456, 311, 257, 1326, 544, 4365, 13, 51264, 51264, 583, 17879, 11, 264, 18161, 3209, 8657, 293, 18161, 3209, 3097, 307, 14800, 293, 680, 39211, 13, 51514, 51514, 400, 586, 291, 1223, 46506, 577, 300, 1985, 833, 264, 13376, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06982329824696416, "compression_ratio": 1.797945205479452, "no_speech_prob": 6.643135748163331e-06}, {"id": 1557, "seek": 817800, "start": 8182.0, "end": 8184.0, "text": " But people usually use slightly different updates.", "tokens": [50364, 400, 264, 16235, 23475, 576, 312, 456, 293, 576, 312, 1936, 14800, 13, 50564, 50564, 583, 561, 2673, 764, 4748, 819, 9205, 13, 50664, 50664, 639, 307, 257, 588, 2199, 342, 8997, 2750, 16235, 23475, 5623, 13, 50914, 50914, 400, 264, 4470, 2445, 576, 406, 312, 914, 8889, 6713, 13, 51014, 51014, 814, 576, 312, 1228, 746, 1219, 264, 3278, 30867, 4470, 337, 32884, 264, 958, 14862, 13, 51214, 51214, 407, 456, 311, 257, 1326, 544, 4365, 13, 51264, 51264, 583, 17879, 11, 264, 18161, 3209, 8657, 293, 18161, 3209, 3097, 307, 14800, 293, 680, 39211, 13, 51514, 51514, 400, 586, 291, 1223, 46506, 577, 300, 1985, 833, 264, 13376, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06982329824696416, "compression_ratio": 1.797945205479452, "no_speech_prob": 6.643135748163331e-06}, {"id": 1558, "seek": 817800, "start": 8184.0, "end": 8189.0, "text": " This is a very simple stochastic gradient descent update.", "tokens": [50364, 400, 264, 16235, 23475, 576, 312, 456, 293, 576, 312, 1936, 14800, 13, 50564, 50564, 583, 561, 2673, 764, 4748, 819, 9205, 13, 50664, 50664, 639, 307, 257, 588, 2199, 342, 8997, 2750, 16235, 23475, 5623, 13, 50914, 50914, 400, 264, 4470, 2445, 576, 406, 312, 914, 8889, 6713, 13, 51014, 51014, 814, 576, 312, 1228, 746, 1219, 264, 3278, 30867, 4470, 337, 32884, 264, 958, 14862, 13, 51214, 51214, 407, 456, 311, 257, 1326, 544, 4365, 13, 51264, 51264, 583, 17879, 11, 264, 18161, 3209, 8657, 293, 18161, 3209, 3097, 307, 14800, 293, 680, 39211, 13, 51514, 51514, 400, 586, 291, 1223, 46506, 577, 300, 1985, 833, 264, 13376, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06982329824696416, "compression_ratio": 1.797945205479452, "no_speech_prob": 6.643135748163331e-06}, {"id": 1559, "seek": 817800, "start": 8189.0, "end": 8191.0, "text": " And the loss function would not be mean squared error.", "tokens": [50364, 400, 264, 16235, 23475, 576, 312, 456, 293, 576, 312, 1936, 14800, 13, 50564, 50564, 583, 561, 2673, 764, 4748, 819, 9205, 13, 50664, 50664, 639, 307, 257, 588, 2199, 342, 8997, 2750, 16235, 23475, 5623, 13, 50914, 50914, 400, 264, 4470, 2445, 576, 406, 312, 914, 8889, 6713, 13, 51014, 51014, 814, 576, 312, 1228, 746, 1219, 264, 3278, 30867, 4470, 337, 32884, 264, 958, 14862, 13, 51214, 51214, 407, 456, 311, 257, 1326, 544, 4365, 13, 51264, 51264, 583, 17879, 11, 264, 18161, 3209, 8657, 293, 18161, 3209, 3097, 307, 14800, 293, 680, 39211, 13, 51514, 51514, 400, 586, 291, 1223, 46506, 577, 300, 1985, 833, 264, 13376, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06982329824696416, "compression_ratio": 1.797945205479452, "no_speech_prob": 6.643135748163331e-06}, {"id": 1560, "seek": 817800, "start": 8191.0, "end": 8195.0, "text": " They would be using something called the cross entropy loss for predicting the next token.", "tokens": [50364, 400, 264, 16235, 23475, 576, 312, 456, 293, 576, 312, 1936, 14800, 13, 50564, 50564, 583, 561, 2673, 764, 4748, 819, 9205, 13, 50664, 50664, 639, 307, 257, 588, 2199, 342, 8997, 2750, 16235, 23475, 5623, 13, 50914, 50914, 400, 264, 4470, 2445, 576, 406, 312, 914, 8889, 6713, 13, 51014, 51014, 814, 576, 312, 1228, 746, 1219, 264, 3278, 30867, 4470, 337, 32884, 264, 958, 14862, 13, 51214, 51214, 407, 456, 311, 257, 1326, 544, 4365, 13, 51264, 51264, 583, 17879, 11, 264, 18161, 3209, 8657, 293, 18161, 3209, 3097, 307, 14800, 293, 680, 39211, 13, 51514, 51514, 400, 586, 291, 1223, 46506, 577, 300, 1985, 833, 264, 13376, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06982329824696416, "compression_ratio": 1.797945205479452, "no_speech_prob": 6.643135748163331e-06}, {"id": 1561, "seek": 817800, "start": 8195.0, "end": 8196.0, "text": " So there's a few more details.", "tokens": [50364, 400, 264, 16235, 23475, 576, 312, 456, 293, 576, 312, 1936, 14800, 13, 50564, 50564, 583, 561, 2673, 764, 4748, 819, 9205, 13, 50664, 50664, 639, 307, 257, 588, 2199, 342, 8997, 2750, 16235, 23475, 5623, 13, 50914, 50914, 400, 264, 4470, 2445, 576, 406, 312, 914, 8889, 6713, 13, 51014, 51014, 814, 576, 312, 1228, 746, 1219, 264, 3278, 30867, 4470, 337, 32884, 264, 958, 14862, 13, 51214, 51214, 407, 456, 311, 257, 1326, 544, 4365, 13, 51264, 51264, 583, 17879, 11, 264, 18161, 3209, 8657, 293, 18161, 3209, 3097, 307, 14800, 293, 680, 39211, 13, 51514, 51514, 400, 586, 291, 1223, 46506, 577, 300, 1985, 833, 264, 13376, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06982329824696416, "compression_ratio": 1.797945205479452, "no_speech_prob": 6.643135748163331e-06}, {"id": 1562, "seek": 817800, "start": 8196.0, "end": 8201.0, "text": " But fundamentally, the neural network setup and neural network training is identical and pervasive.", "tokens": [50364, 400, 264, 16235, 23475, 576, 312, 456, 293, 576, 312, 1936, 14800, 13, 50564, 50564, 583, 561, 2673, 764, 4748, 819, 9205, 13, 50664, 50664, 639, 307, 257, 588, 2199, 342, 8997, 2750, 16235, 23475, 5623, 13, 50914, 50914, 400, 264, 4470, 2445, 576, 406, 312, 914, 8889, 6713, 13, 51014, 51014, 814, 576, 312, 1228, 746, 1219, 264, 3278, 30867, 4470, 337, 32884, 264, 958, 14862, 13, 51214, 51214, 407, 456, 311, 257, 1326, 544, 4365, 13, 51264, 51264, 583, 17879, 11, 264, 18161, 3209, 8657, 293, 18161, 3209, 3097, 307, 14800, 293, 680, 39211, 13, 51514, 51514, 400, 586, 291, 1223, 46506, 577, 300, 1985, 833, 264, 13376, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06982329824696416, "compression_ratio": 1.797945205479452, "no_speech_prob": 6.643135748163331e-06}, {"id": 1563, "seek": 817800, "start": 8201.0, "end": 8205.0, "text": " And now you understand intuitively how that works under the hood.", "tokens": [50364, 400, 264, 16235, 23475, 576, 312, 456, 293, 576, 312, 1936, 14800, 13, 50564, 50564, 583, 561, 2673, 764, 4748, 819, 9205, 13, 50664, 50664, 639, 307, 257, 588, 2199, 342, 8997, 2750, 16235, 23475, 5623, 13, 50914, 50914, 400, 264, 4470, 2445, 576, 406, 312, 914, 8889, 6713, 13, 51014, 51014, 814, 576, 312, 1228, 746, 1219, 264, 3278, 30867, 4470, 337, 32884, 264, 958, 14862, 13, 51214, 51214, 407, 456, 311, 257, 1326, 544, 4365, 13, 51264, 51264, 583, 17879, 11, 264, 18161, 3209, 8657, 293, 18161, 3209, 3097, 307, 14800, 293, 680, 39211, 13, 51514, 51514, 400, 586, 291, 1223, 46506, 577, 300, 1985, 833, 264, 13376, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06982329824696416, "compression_ratio": 1.797945205479452, "no_speech_prob": 6.643135748163331e-06}, {"id": 1564, "seek": 820500, "start": 8205.0, "end": 8211.0, "text": " In the beginning of this video, I told you that by the end of it, you would understand everything in micro grad and we'd slowly build it up.", "tokens": [50364, 682, 264, 2863, 295, 341, 960, 11, 286, 1907, 291, 300, 538, 264, 917, 295, 309, 11, 291, 576, 1223, 1203, 294, 4532, 2771, 293, 321, 1116, 5692, 1322, 309, 493, 13, 50664, 50664, 961, 385, 10515, 7081, 300, 281, 291, 13, 50764, 50764, 407, 286, 478, 516, 281, 1823, 807, 439, 264, 3089, 300, 307, 294, 4532, 2771, 382, 295, 965, 13, 50964, 50964, 5135, 11, 7263, 512, 295, 264, 3089, 486, 1319, 538, 264, 565, 291, 1159, 341, 960, 570, 286, 19759, 281, 2354, 6416, 4532, 2771, 13, 51264, 51264, 583, 718, 311, 574, 412, 437, 321, 362, 370, 1400, 412, 1935, 13, 51364, 51364, 682, 309, 11, 300, 3895, 307, 6707, 13, 51464, 51464, 1133, 291, 352, 281, 2848, 300, 3895, 300, 575, 264, 2158, 11, 1203, 510, 291, 820, 5240, 5521, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1250692231314523, "compression_ratio": 1.7398119122257054, "no_speech_prob": 9.025879262480885e-05}, {"id": 1565, "seek": 820500, "start": 8211.0, "end": 8213.0, "text": " Let me briefly prove that to you.", "tokens": [50364, 682, 264, 2863, 295, 341, 960, 11, 286, 1907, 291, 300, 538, 264, 917, 295, 309, 11, 291, 576, 1223, 1203, 294, 4532, 2771, 293, 321, 1116, 5692, 1322, 309, 493, 13, 50664, 50664, 961, 385, 10515, 7081, 300, 281, 291, 13, 50764, 50764, 407, 286, 478, 516, 281, 1823, 807, 439, 264, 3089, 300, 307, 294, 4532, 2771, 382, 295, 965, 13, 50964, 50964, 5135, 11, 7263, 512, 295, 264, 3089, 486, 1319, 538, 264, 565, 291, 1159, 341, 960, 570, 286, 19759, 281, 2354, 6416, 4532, 2771, 13, 51264, 51264, 583, 718, 311, 574, 412, 437, 321, 362, 370, 1400, 412, 1935, 13, 51364, 51364, 682, 309, 11, 300, 3895, 307, 6707, 13, 51464, 51464, 1133, 291, 352, 281, 2848, 300, 3895, 300, 575, 264, 2158, 11, 1203, 510, 291, 820, 5240, 5521, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1250692231314523, "compression_ratio": 1.7398119122257054, "no_speech_prob": 9.025879262480885e-05}, {"id": 1566, "seek": 820500, "start": 8213.0, "end": 8217.0, "text": " So I'm going to step through all the code that is in micro grad as of today.", "tokens": [50364, 682, 264, 2863, 295, 341, 960, 11, 286, 1907, 291, 300, 538, 264, 917, 295, 309, 11, 291, 576, 1223, 1203, 294, 4532, 2771, 293, 321, 1116, 5692, 1322, 309, 493, 13, 50664, 50664, 961, 385, 10515, 7081, 300, 281, 291, 13, 50764, 50764, 407, 286, 478, 516, 281, 1823, 807, 439, 264, 3089, 300, 307, 294, 4532, 2771, 382, 295, 965, 13, 50964, 50964, 5135, 11, 7263, 512, 295, 264, 3089, 486, 1319, 538, 264, 565, 291, 1159, 341, 960, 570, 286, 19759, 281, 2354, 6416, 4532, 2771, 13, 51264, 51264, 583, 718, 311, 574, 412, 437, 321, 362, 370, 1400, 412, 1935, 13, 51364, 51364, 682, 309, 11, 300, 3895, 307, 6707, 13, 51464, 51464, 1133, 291, 352, 281, 2848, 300, 3895, 300, 575, 264, 2158, 11, 1203, 510, 291, 820, 5240, 5521, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1250692231314523, "compression_ratio": 1.7398119122257054, "no_speech_prob": 9.025879262480885e-05}, {"id": 1567, "seek": 820500, "start": 8217.0, "end": 8223.0, "text": " Actually, potentially some of the code will change by the time you watch this video because I intend to continue developing micro grad.", "tokens": [50364, 682, 264, 2863, 295, 341, 960, 11, 286, 1907, 291, 300, 538, 264, 917, 295, 309, 11, 291, 576, 1223, 1203, 294, 4532, 2771, 293, 321, 1116, 5692, 1322, 309, 493, 13, 50664, 50664, 961, 385, 10515, 7081, 300, 281, 291, 13, 50764, 50764, 407, 286, 478, 516, 281, 1823, 807, 439, 264, 3089, 300, 307, 294, 4532, 2771, 382, 295, 965, 13, 50964, 50964, 5135, 11, 7263, 512, 295, 264, 3089, 486, 1319, 538, 264, 565, 291, 1159, 341, 960, 570, 286, 19759, 281, 2354, 6416, 4532, 2771, 13, 51264, 51264, 583, 718, 311, 574, 412, 437, 321, 362, 370, 1400, 412, 1935, 13, 51364, 51364, 682, 309, 11, 300, 3895, 307, 6707, 13, 51464, 51464, 1133, 291, 352, 281, 2848, 300, 3895, 300, 575, 264, 2158, 11, 1203, 510, 291, 820, 5240, 5521, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1250692231314523, "compression_ratio": 1.7398119122257054, "no_speech_prob": 9.025879262480885e-05}, {"id": 1568, "seek": 820500, "start": 8223.0, "end": 8225.0, "text": " But let's look at what we have so far at least.", "tokens": [50364, 682, 264, 2863, 295, 341, 960, 11, 286, 1907, 291, 300, 538, 264, 917, 295, 309, 11, 291, 576, 1223, 1203, 294, 4532, 2771, 293, 321, 1116, 5692, 1322, 309, 493, 13, 50664, 50664, 961, 385, 10515, 7081, 300, 281, 291, 13, 50764, 50764, 407, 286, 478, 516, 281, 1823, 807, 439, 264, 3089, 300, 307, 294, 4532, 2771, 382, 295, 965, 13, 50964, 50964, 5135, 11, 7263, 512, 295, 264, 3089, 486, 1319, 538, 264, 565, 291, 1159, 341, 960, 570, 286, 19759, 281, 2354, 6416, 4532, 2771, 13, 51264, 51264, 583, 718, 311, 574, 412, 437, 321, 362, 370, 1400, 412, 1935, 13, 51364, 51364, 682, 309, 11, 300, 3895, 307, 6707, 13, 51464, 51464, 1133, 291, 352, 281, 2848, 300, 3895, 300, 575, 264, 2158, 11, 1203, 510, 291, 820, 5240, 5521, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1250692231314523, "compression_ratio": 1.7398119122257054, "no_speech_prob": 9.025879262480885e-05}, {"id": 1569, "seek": 820500, "start": 8225.0, "end": 8227.0, "text": " In it, that pi is empty.", "tokens": [50364, 682, 264, 2863, 295, 341, 960, 11, 286, 1907, 291, 300, 538, 264, 917, 295, 309, 11, 291, 576, 1223, 1203, 294, 4532, 2771, 293, 321, 1116, 5692, 1322, 309, 493, 13, 50664, 50664, 961, 385, 10515, 7081, 300, 281, 291, 13, 50764, 50764, 407, 286, 478, 516, 281, 1823, 807, 439, 264, 3089, 300, 307, 294, 4532, 2771, 382, 295, 965, 13, 50964, 50964, 5135, 11, 7263, 512, 295, 264, 3089, 486, 1319, 538, 264, 565, 291, 1159, 341, 960, 570, 286, 19759, 281, 2354, 6416, 4532, 2771, 13, 51264, 51264, 583, 718, 311, 574, 412, 437, 321, 362, 370, 1400, 412, 1935, 13, 51364, 51364, 682, 309, 11, 300, 3895, 307, 6707, 13, 51464, 51464, 1133, 291, 352, 281, 2848, 300, 3895, 300, 575, 264, 2158, 11, 1203, 510, 291, 820, 5240, 5521, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1250692231314523, "compression_ratio": 1.7398119122257054, "no_speech_prob": 9.025879262480885e-05}, {"id": 1570, "seek": 820500, "start": 8227.0, "end": 8232.0, "text": " When you go to engine that pi that has the value, everything here you should mostly recognize.", "tokens": [50364, 682, 264, 2863, 295, 341, 960, 11, 286, 1907, 291, 300, 538, 264, 917, 295, 309, 11, 291, 576, 1223, 1203, 294, 4532, 2771, 293, 321, 1116, 5692, 1322, 309, 493, 13, 50664, 50664, 961, 385, 10515, 7081, 300, 281, 291, 13, 50764, 50764, 407, 286, 478, 516, 281, 1823, 807, 439, 264, 3089, 300, 307, 294, 4532, 2771, 382, 295, 965, 13, 50964, 50964, 5135, 11, 7263, 512, 295, 264, 3089, 486, 1319, 538, 264, 565, 291, 1159, 341, 960, 570, 286, 19759, 281, 2354, 6416, 4532, 2771, 13, 51264, 51264, 583, 718, 311, 574, 412, 437, 321, 362, 370, 1400, 412, 1935, 13, 51364, 51364, 682, 309, 11, 300, 3895, 307, 6707, 13, 51464, 51464, 1133, 291, 352, 281, 2848, 300, 3895, 300, 575, 264, 2158, 11, 1203, 510, 291, 820, 5240, 5521, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1250692231314523, "compression_ratio": 1.7398119122257054, "no_speech_prob": 9.025879262480885e-05}, {"id": 1571, "seek": 823200, "start": 8232.0, "end": 8236.0, "text": " So we have the data data that grad attributes with the backward function.", "tokens": [50364, 407, 321, 362, 264, 1412, 1412, 300, 2771, 17212, 365, 264, 23897, 2445, 13, 50564, 50564, 492, 362, 264, 3894, 992, 295, 2227, 293, 264, 6916, 300, 7126, 341, 2158, 13, 50814, 50814, 492, 362, 4500, 11, 27290, 293, 11225, 281, 257, 39684, 1347, 13, 51014, 51014, 492, 362, 264, 1039, 84, 2107, 12, 1889, 17409, 11, 597, 307, 4748, 819, 2010, 295, 2107, 12, 1889, 17409, 813, 7603, 3205, 300, 321, 1143, 294, 341, 960, 13, 51314, 51314, 6767, 295, 552, 366, 2107, 12, 28263, 1088, 293, 31357, 7603, 3205, 307, 406, 767, 1974, 294, 4532, 2771, 382, 295, 558, 586, 13, 51564, 51564, 583, 286, 19759, 281, 909, 309, 1780, 365, 264, 23897, 11, 597, 307, 14800, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14444829571631648, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.132404501433484e-05}, {"id": 1572, "seek": 823200, "start": 8236.0, "end": 8241.0, "text": " We have the previous set of children and the operation that produced this value.", "tokens": [50364, 407, 321, 362, 264, 1412, 1412, 300, 2771, 17212, 365, 264, 23897, 2445, 13, 50564, 50564, 492, 362, 264, 3894, 992, 295, 2227, 293, 264, 6916, 300, 7126, 341, 2158, 13, 50814, 50814, 492, 362, 4500, 11, 27290, 293, 11225, 281, 257, 39684, 1347, 13, 51014, 51014, 492, 362, 264, 1039, 84, 2107, 12, 1889, 17409, 11, 597, 307, 4748, 819, 2010, 295, 2107, 12, 1889, 17409, 813, 7603, 3205, 300, 321, 1143, 294, 341, 960, 13, 51314, 51314, 6767, 295, 552, 366, 2107, 12, 28263, 1088, 293, 31357, 7603, 3205, 307, 406, 767, 1974, 294, 4532, 2771, 382, 295, 558, 586, 13, 51564, 51564, 583, 286, 19759, 281, 909, 309, 1780, 365, 264, 23897, 11, 597, 307, 14800, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14444829571631648, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.132404501433484e-05}, {"id": 1573, "seek": 823200, "start": 8241.0, "end": 8245.0, "text": " We have addition, multiplication and raising to a scalar power.", "tokens": [50364, 407, 321, 362, 264, 1412, 1412, 300, 2771, 17212, 365, 264, 23897, 2445, 13, 50564, 50564, 492, 362, 264, 3894, 992, 295, 2227, 293, 264, 6916, 300, 7126, 341, 2158, 13, 50814, 50814, 492, 362, 4500, 11, 27290, 293, 11225, 281, 257, 39684, 1347, 13, 51014, 51014, 492, 362, 264, 1039, 84, 2107, 12, 1889, 17409, 11, 597, 307, 4748, 819, 2010, 295, 2107, 12, 1889, 17409, 813, 7603, 3205, 300, 321, 1143, 294, 341, 960, 13, 51314, 51314, 6767, 295, 552, 366, 2107, 12, 28263, 1088, 293, 31357, 7603, 3205, 307, 406, 767, 1974, 294, 4532, 2771, 382, 295, 558, 586, 13, 51564, 51564, 583, 286, 19759, 281, 909, 309, 1780, 365, 264, 23897, 11, 597, 307, 14800, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14444829571631648, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.132404501433484e-05}, {"id": 1574, "seek": 823200, "start": 8245.0, "end": 8251.0, "text": " We have the relu non-linearity, which is slightly different type of non-linearity than tan age that we used in this video.", "tokens": [50364, 407, 321, 362, 264, 1412, 1412, 300, 2771, 17212, 365, 264, 23897, 2445, 13, 50564, 50564, 492, 362, 264, 3894, 992, 295, 2227, 293, 264, 6916, 300, 7126, 341, 2158, 13, 50814, 50814, 492, 362, 4500, 11, 27290, 293, 11225, 281, 257, 39684, 1347, 13, 51014, 51014, 492, 362, 264, 1039, 84, 2107, 12, 1889, 17409, 11, 597, 307, 4748, 819, 2010, 295, 2107, 12, 1889, 17409, 813, 7603, 3205, 300, 321, 1143, 294, 341, 960, 13, 51314, 51314, 6767, 295, 552, 366, 2107, 12, 28263, 1088, 293, 31357, 7603, 3205, 307, 406, 767, 1974, 294, 4532, 2771, 382, 295, 558, 586, 13, 51564, 51564, 583, 286, 19759, 281, 909, 309, 1780, 365, 264, 23897, 11, 597, 307, 14800, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14444829571631648, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.132404501433484e-05}, {"id": 1575, "seek": 823200, "start": 8251.0, "end": 8256.0, "text": " Both of them are non-linearities and notably tan age is not actually present in micro grad as of right now.", "tokens": [50364, 407, 321, 362, 264, 1412, 1412, 300, 2771, 17212, 365, 264, 23897, 2445, 13, 50564, 50564, 492, 362, 264, 3894, 992, 295, 2227, 293, 264, 6916, 300, 7126, 341, 2158, 13, 50814, 50814, 492, 362, 4500, 11, 27290, 293, 11225, 281, 257, 39684, 1347, 13, 51014, 51014, 492, 362, 264, 1039, 84, 2107, 12, 1889, 17409, 11, 597, 307, 4748, 819, 2010, 295, 2107, 12, 1889, 17409, 813, 7603, 3205, 300, 321, 1143, 294, 341, 960, 13, 51314, 51314, 6767, 295, 552, 366, 2107, 12, 28263, 1088, 293, 31357, 7603, 3205, 307, 406, 767, 1974, 294, 4532, 2771, 382, 295, 558, 586, 13, 51564, 51564, 583, 286, 19759, 281, 909, 309, 1780, 365, 264, 23897, 11, 597, 307, 14800, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14444829571631648, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.132404501433484e-05}, {"id": 1576, "seek": 823200, "start": 8256.0, "end": 8260.0, "text": " But I intend to add it later with the backward, which is identical.", "tokens": [50364, 407, 321, 362, 264, 1412, 1412, 300, 2771, 17212, 365, 264, 23897, 2445, 13, 50564, 50564, 492, 362, 264, 3894, 992, 295, 2227, 293, 264, 6916, 300, 7126, 341, 2158, 13, 50814, 50814, 492, 362, 4500, 11, 27290, 293, 11225, 281, 257, 39684, 1347, 13, 51014, 51014, 492, 362, 264, 1039, 84, 2107, 12, 1889, 17409, 11, 597, 307, 4748, 819, 2010, 295, 2107, 12, 1889, 17409, 813, 7603, 3205, 300, 321, 1143, 294, 341, 960, 13, 51314, 51314, 6767, 295, 552, 366, 2107, 12, 28263, 1088, 293, 31357, 7603, 3205, 307, 406, 767, 1974, 294, 4532, 2771, 382, 295, 558, 586, 13, 51564, 51564, 583, 286, 19759, 281, 909, 309, 1780, 365, 264, 23897, 11, 597, 307, 14800, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14444829571631648, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.132404501433484e-05}, {"id": 1577, "seek": 826000, "start": 8260.0, "end": 8265.0, "text": " And then all of these other operations, which are built up on top of operations here.", "tokens": [50364, 400, 550, 439, 295, 613, 661, 7705, 11, 597, 366, 3094, 493, 322, 1192, 295, 7705, 510, 13, 50614, 50614, 407, 4190, 820, 312, 588, 40757, 11, 3993, 337, 264, 2107, 12, 1889, 17409, 1143, 294, 341, 960, 13, 50864, 50864, 821, 311, 572, 5994, 2649, 1296, 1039, 84, 293, 7603, 3205, 293, 4556, 3280, 327, 293, 613, 661, 2107, 12, 28263, 1088, 13, 51114, 51114, 814, 434, 439, 9810, 10344, 293, 393, 312, 1143, 294, 21601, 23043, 13, 51214, 51214, 407, 286, 764, 7603, 3205, 570, 309, 311, 257, 857, 28640, 293, 570, 309, 311, 257, 707, 857, 544, 6179, 813, 1039, 84, 13, 51464, 51464, 400, 4412, 309, 311, 14471, 257, 707, 857, 544, 264, 2654, 2771, 2448, 293, 1364, 365, 729, 33733, 11, 597, 286, 1194, 576, 312, 4420, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0802791762526018, "compression_ratio": 1.7774294670846396, "no_speech_prob": 2.5865781935863197e-05}, {"id": 1578, "seek": 826000, "start": 8265.0, "end": 8270.0, "text": " So values should be very recognizable, except for the non-linearity used in this video.", "tokens": [50364, 400, 550, 439, 295, 613, 661, 7705, 11, 597, 366, 3094, 493, 322, 1192, 295, 7705, 510, 13, 50614, 50614, 407, 4190, 820, 312, 588, 40757, 11, 3993, 337, 264, 2107, 12, 1889, 17409, 1143, 294, 341, 960, 13, 50864, 50864, 821, 311, 572, 5994, 2649, 1296, 1039, 84, 293, 7603, 3205, 293, 4556, 3280, 327, 293, 613, 661, 2107, 12, 28263, 1088, 13, 51114, 51114, 814, 434, 439, 9810, 10344, 293, 393, 312, 1143, 294, 21601, 23043, 13, 51214, 51214, 407, 286, 764, 7603, 3205, 570, 309, 311, 257, 857, 28640, 293, 570, 309, 311, 257, 707, 857, 544, 6179, 813, 1039, 84, 13, 51464, 51464, 400, 4412, 309, 311, 14471, 257, 707, 857, 544, 264, 2654, 2771, 2448, 293, 1364, 365, 729, 33733, 11, 597, 286, 1194, 576, 312, 4420, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0802791762526018, "compression_ratio": 1.7774294670846396, "no_speech_prob": 2.5865781935863197e-05}, {"id": 1579, "seek": 826000, "start": 8270.0, "end": 8275.0, "text": " There's no massive difference between relu and tan age and sigmoid and these other non-linearities.", "tokens": [50364, 400, 550, 439, 295, 613, 661, 7705, 11, 597, 366, 3094, 493, 322, 1192, 295, 7705, 510, 13, 50614, 50614, 407, 4190, 820, 312, 588, 40757, 11, 3993, 337, 264, 2107, 12, 1889, 17409, 1143, 294, 341, 960, 13, 50864, 50864, 821, 311, 572, 5994, 2649, 1296, 1039, 84, 293, 7603, 3205, 293, 4556, 3280, 327, 293, 613, 661, 2107, 12, 28263, 1088, 13, 51114, 51114, 814, 434, 439, 9810, 10344, 293, 393, 312, 1143, 294, 21601, 23043, 13, 51214, 51214, 407, 286, 764, 7603, 3205, 570, 309, 311, 257, 857, 28640, 293, 570, 309, 311, 257, 707, 857, 544, 6179, 813, 1039, 84, 13, 51464, 51464, 400, 4412, 309, 311, 14471, 257, 707, 857, 544, 264, 2654, 2771, 2448, 293, 1364, 365, 729, 33733, 11, 597, 286, 1194, 576, 312, 4420, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0802791762526018, "compression_ratio": 1.7774294670846396, "no_speech_prob": 2.5865781935863197e-05}, {"id": 1580, "seek": 826000, "start": 8275.0, "end": 8277.0, "text": " They're all roughly equivalent and can be used in MLPs.", "tokens": [50364, 400, 550, 439, 295, 613, 661, 7705, 11, 597, 366, 3094, 493, 322, 1192, 295, 7705, 510, 13, 50614, 50614, 407, 4190, 820, 312, 588, 40757, 11, 3993, 337, 264, 2107, 12, 1889, 17409, 1143, 294, 341, 960, 13, 50864, 50864, 821, 311, 572, 5994, 2649, 1296, 1039, 84, 293, 7603, 3205, 293, 4556, 3280, 327, 293, 613, 661, 2107, 12, 28263, 1088, 13, 51114, 51114, 814, 434, 439, 9810, 10344, 293, 393, 312, 1143, 294, 21601, 23043, 13, 51214, 51214, 407, 286, 764, 7603, 3205, 570, 309, 311, 257, 857, 28640, 293, 570, 309, 311, 257, 707, 857, 544, 6179, 813, 1039, 84, 13, 51464, 51464, 400, 4412, 309, 311, 14471, 257, 707, 857, 544, 264, 2654, 2771, 2448, 293, 1364, 365, 729, 33733, 11, 597, 286, 1194, 576, 312, 4420, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0802791762526018, "compression_ratio": 1.7774294670846396, "no_speech_prob": 2.5865781935863197e-05}, {"id": 1581, "seek": 826000, "start": 8277.0, "end": 8282.0, "text": " So I use tan age because it's a bit smoother and because it's a little bit more complicated than relu.", "tokens": [50364, 400, 550, 439, 295, 613, 661, 7705, 11, 597, 366, 3094, 493, 322, 1192, 295, 7705, 510, 13, 50614, 50614, 407, 4190, 820, 312, 588, 40757, 11, 3993, 337, 264, 2107, 12, 1889, 17409, 1143, 294, 341, 960, 13, 50864, 50864, 821, 311, 572, 5994, 2649, 1296, 1039, 84, 293, 7603, 3205, 293, 4556, 3280, 327, 293, 613, 661, 2107, 12, 28263, 1088, 13, 51114, 51114, 814, 434, 439, 9810, 10344, 293, 393, 312, 1143, 294, 21601, 23043, 13, 51214, 51214, 407, 286, 764, 7603, 3205, 570, 309, 311, 257, 857, 28640, 293, 570, 309, 311, 257, 707, 857, 544, 6179, 813, 1039, 84, 13, 51464, 51464, 400, 4412, 309, 311, 14471, 257, 707, 857, 544, 264, 2654, 2771, 2448, 293, 1364, 365, 729, 33733, 11, 597, 286, 1194, 576, 312, 4420, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0802791762526018, "compression_ratio": 1.7774294670846396, "no_speech_prob": 2.5865781935863197e-05}, {"id": 1582, "seek": 826000, "start": 8282.0, "end": 8289.0, "text": " And therefore it's stressed a little bit more the local gradients and working with those derivatives, which I thought would be useful.", "tokens": [50364, 400, 550, 439, 295, 613, 661, 7705, 11, 597, 366, 3094, 493, 322, 1192, 295, 7705, 510, 13, 50614, 50614, 407, 4190, 820, 312, 588, 40757, 11, 3993, 337, 264, 2107, 12, 1889, 17409, 1143, 294, 341, 960, 13, 50864, 50864, 821, 311, 572, 5994, 2649, 1296, 1039, 84, 293, 7603, 3205, 293, 4556, 3280, 327, 293, 613, 661, 2107, 12, 28263, 1088, 13, 51114, 51114, 814, 434, 439, 9810, 10344, 293, 393, 312, 1143, 294, 21601, 23043, 13, 51214, 51214, 407, 286, 764, 7603, 3205, 570, 309, 311, 257, 857, 28640, 293, 570, 309, 311, 257, 707, 857, 544, 6179, 813, 1039, 84, 13, 51464, 51464, 400, 4412, 309, 311, 14471, 257, 707, 857, 544, 264, 2654, 2771, 2448, 293, 1364, 365, 729, 33733, 11, 597, 286, 1194, 576, 312, 4420, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0802791762526018, "compression_ratio": 1.7774294670846396, "no_speech_prob": 2.5865781935863197e-05}, {"id": 1583, "seek": 828900, "start": 8289.0, "end": 8293.0, "text": " Nn.py is the neural networks library, as I mentioned.", "tokens": [50364, 426, 77, 13, 8200, 307, 264, 18161, 9590, 6405, 11, 382, 286, 2835, 13, 50564, 50564, 407, 291, 820, 5521, 14800, 11420, 295, 1734, 1807, 11, 35166, 293, 21601, 47, 13, 50814, 50814, 1726, 1188, 11, 420, 406, 370, 709, 11, 321, 362, 257, 1508, 10088, 510, 13, 51014, 51014, 821, 311, 257, 2596, 1508, 295, 439, 613, 16679, 13, 51114, 51114, 286, 630, 300, 570, 456, 311, 364, 297, 77, 13, 8014, 2271, 1508, 294, 9953, 51, 284, 339, 13, 51314, 51314, 400, 370, 341, 2293, 10676, 300, 9362, 13, 51414, 51414, 400, 297, 77, 13, 8014, 2271, 294, 9953, 51, 284, 339, 575, 611, 257, 4018, 2771, 11, 597, 286, 1895, 578, 2769, 484, 510, 13, 51614, 51614, 407, 300, 311, 264, 917, 295, 4532, 2771, 11, 534, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11788717905680339, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.2122359951026738e-05}, {"id": 1584, "seek": 828900, "start": 8293.0, "end": 8298.0, "text": " So you should recognize identical implementation of Neural, Layer and MLP.", "tokens": [50364, 426, 77, 13, 8200, 307, 264, 18161, 9590, 6405, 11, 382, 286, 2835, 13, 50564, 50564, 407, 291, 820, 5521, 14800, 11420, 295, 1734, 1807, 11, 35166, 293, 21601, 47, 13, 50814, 50814, 1726, 1188, 11, 420, 406, 370, 709, 11, 321, 362, 257, 1508, 10088, 510, 13, 51014, 51014, 821, 311, 257, 2596, 1508, 295, 439, 613, 16679, 13, 51114, 51114, 286, 630, 300, 570, 456, 311, 364, 297, 77, 13, 8014, 2271, 1508, 294, 9953, 51, 284, 339, 13, 51314, 51314, 400, 370, 341, 2293, 10676, 300, 9362, 13, 51414, 51414, 400, 297, 77, 13, 8014, 2271, 294, 9953, 51, 284, 339, 575, 611, 257, 4018, 2771, 11, 597, 286, 1895, 578, 2769, 484, 510, 13, 51614, 51614, 407, 300, 311, 264, 917, 295, 4532, 2771, 11, 534, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11788717905680339, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.2122359951026738e-05}, {"id": 1585, "seek": 828900, "start": 8298.0, "end": 8302.0, "text": " Notably, or not so much, we have a class module here.", "tokens": [50364, 426, 77, 13, 8200, 307, 264, 18161, 9590, 6405, 11, 382, 286, 2835, 13, 50564, 50564, 407, 291, 820, 5521, 14800, 11420, 295, 1734, 1807, 11, 35166, 293, 21601, 47, 13, 50814, 50814, 1726, 1188, 11, 420, 406, 370, 709, 11, 321, 362, 257, 1508, 10088, 510, 13, 51014, 51014, 821, 311, 257, 2596, 1508, 295, 439, 613, 16679, 13, 51114, 51114, 286, 630, 300, 570, 456, 311, 364, 297, 77, 13, 8014, 2271, 1508, 294, 9953, 51, 284, 339, 13, 51314, 51314, 400, 370, 341, 2293, 10676, 300, 9362, 13, 51414, 51414, 400, 297, 77, 13, 8014, 2271, 294, 9953, 51, 284, 339, 575, 611, 257, 4018, 2771, 11, 597, 286, 1895, 578, 2769, 484, 510, 13, 51614, 51614, 407, 300, 311, 264, 917, 295, 4532, 2771, 11, 534, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11788717905680339, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.2122359951026738e-05}, {"id": 1586, "seek": 828900, "start": 8302.0, "end": 8304.0, "text": " There's a parent class of all these modules.", "tokens": [50364, 426, 77, 13, 8200, 307, 264, 18161, 9590, 6405, 11, 382, 286, 2835, 13, 50564, 50564, 407, 291, 820, 5521, 14800, 11420, 295, 1734, 1807, 11, 35166, 293, 21601, 47, 13, 50814, 50814, 1726, 1188, 11, 420, 406, 370, 709, 11, 321, 362, 257, 1508, 10088, 510, 13, 51014, 51014, 821, 311, 257, 2596, 1508, 295, 439, 613, 16679, 13, 51114, 51114, 286, 630, 300, 570, 456, 311, 364, 297, 77, 13, 8014, 2271, 1508, 294, 9953, 51, 284, 339, 13, 51314, 51314, 400, 370, 341, 2293, 10676, 300, 9362, 13, 51414, 51414, 400, 297, 77, 13, 8014, 2271, 294, 9953, 51, 284, 339, 575, 611, 257, 4018, 2771, 11, 597, 286, 1895, 578, 2769, 484, 510, 13, 51614, 51614, 407, 300, 311, 264, 917, 295, 4532, 2771, 11, 534, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11788717905680339, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.2122359951026738e-05}, {"id": 1587, "seek": 828900, "start": 8304.0, "end": 8308.0, "text": " I did that because there's an nn.module class in PyTorch.", "tokens": [50364, 426, 77, 13, 8200, 307, 264, 18161, 9590, 6405, 11, 382, 286, 2835, 13, 50564, 50564, 407, 291, 820, 5521, 14800, 11420, 295, 1734, 1807, 11, 35166, 293, 21601, 47, 13, 50814, 50814, 1726, 1188, 11, 420, 406, 370, 709, 11, 321, 362, 257, 1508, 10088, 510, 13, 51014, 51014, 821, 311, 257, 2596, 1508, 295, 439, 613, 16679, 13, 51114, 51114, 286, 630, 300, 570, 456, 311, 364, 297, 77, 13, 8014, 2271, 1508, 294, 9953, 51, 284, 339, 13, 51314, 51314, 400, 370, 341, 2293, 10676, 300, 9362, 13, 51414, 51414, 400, 297, 77, 13, 8014, 2271, 294, 9953, 51, 284, 339, 575, 611, 257, 4018, 2771, 11, 597, 286, 1895, 578, 2769, 484, 510, 13, 51614, 51614, 407, 300, 311, 264, 917, 295, 4532, 2771, 11, 534, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11788717905680339, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.2122359951026738e-05}, {"id": 1588, "seek": 828900, "start": 8308.0, "end": 8310.0, "text": " And so this exactly matches that API.", "tokens": [50364, 426, 77, 13, 8200, 307, 264, 18161, 9590, 6405, 11, 382, 286, 2835, 13, 50564, 50564, 407, 291, 820, 5521, 14800, 11420, 295, 1734, 1807, 11, 35166, 293, 21601, 47, 13, 50814, 50814, 1726, 1188, 11, 420, 406, 370, 709, 11, 321, 362, 257, 1508, 10088, 510, 13, 51014, 51014, 821, 311, 257, 2596, 1508, 295, 439, 613, 16679, 13, 51114, 51114, 286, 630, 300, 570, 456, 311, 364, 297, 77, 13, 8014, 2271, 1508, 294, 9953, 51, 284, 339, 13, 51314, 51314, 400, 370, 341, 2293, 10676, 300, 9362, 13, 51414, 51414, 400, 297, 77, 13, 8014, 2271, 294, 9953, 51, 284, 339, 575, 611, 257, 4018, 2771, 11, 597, 286, 1895, 578, 2769, 484, 510, 13, 51614, 51614, 407, 300, 311, 264, 917, 295, 4532, 2771, 11, 534, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11788717905680339, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.2122359951026738e-05}, {"id": 1589, "seek": 828900, "start": 8310.0, "end": 8314.0, "text": " And nn.module in PyTorch has also a zero grad, which I refactored out here.", "tokens": [50364, 426, 77, 13, 8200, 307, 264, 18161, 9590, 6405, 11, 382, 286, 2835, 13, 50564, 50564, 407, 291, 820, 5521, 14800, 11420, 295, 1734, 1807, 11, 35166, 293, 21601, 47, 13, 50814, 50814, 1726, 1188, 11, 420, 406, 370, 709, 11, 321, 362, 257, 1508, 10088, 510, 13, 51014, 51014, 821, 311, 257, 2596, 1508, 295, 439, 613, 16679, 13, 51114, 51114, 286, 630, 300, 570, 456, 311, 364, 297, 77, 13, 8014, 2271, 1508, 294, 9953, 51, 284, 339, 13, 51314, 51314, 400, 370, 341, 2293, 10676, 300, 9362, 13, 51414, 51414, 400, 297, 77, 13, 8014, 2271, 294, 9953, 51, 284, 339, 575, 611, 257, 4018, 2771, 11, 597, 286, 1895, 578, 2769, 484, 510, 13, 51614, 51614, 407, 300, 311, 264, 917, 295, 4532, 2771, 11, 534, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11788717905680339, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.2122359951026738e-05}, {"id": 1590, "seek": 828900, "start": 8314.0, "end": 8318.0, "text": " So that's the end of micro grad, really.", "tokens": [50364, 426, 77, 13, 8200, 307, 264, 18161, 9590, 6405, 11, 382, 286, 2835, 13, 50564, 50564, 407, 291, 820, 5521, 14800, 11420, 295, 1734, 1807, 11, 35166, 293, 21601, 47, 13, 50814, 50814, 1726, 1188, 11, 420, 406, 370, 709, 11, 321, 362, 257, 1508, 10088, 510, 13, 51014, 51014, 821, 311, 257, 2596, 1508, 295, 439, 613, 16679, 13, 51114, 51114, 286, 630, 300, 570, 456, 311, 364, 297, 77, 13, 8014, 2271, 1508, 294, 9953, 51, 284, 339, 13, 51314, 51314, 400, 370, 341, 2293, 10676, 300, 9362, 13, 51414, 51414, 400, 297, 77, 13, 8014, 2271, 294, 9953, 51, 284, 339, 575, 611, 257, 4018, 2771, 11, 597, 286, 1895, 578, 2769, 484, 510, 13, 51614, 51614, 407, 300, 311, 264, 917, 295, 4532, 2771, 11, 534, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11788717905680339, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.2122359951026738e-05}, {"id": 1591, "seek": 831800, "start": 8318.0, "end": 8327.0, "text": " Then there's a test, which you'll see basically creates two chunks of code, one in micro grad and one in PyTorch.", "tokens": [50364, 1396, 456, 311, 257, 1500, 11, 597, 291, 603, 536, 1936, 7829, 732, 24004, 295, 3089, 11, 472, 294, 4532, 2771, 293, 472, 294, 9953, 51, 284, 339, 13, 50814, 50814, 400, 321, 603, 652, 988, 300, 264, 2128, 293, 264, 23897, 14518, 3986, 2473, 984, 13, 50964, 50964, 1171, 257, 4748, 1570, 6179, 6114, 11, 257, 4748, 544, 6179, 6114, 11, 1203, 26383, 13, 51264, 51264, 407, 321, 3986, 365, 9953, 51, 284, 339, 322, 439, 295, 613, 7705, 13, 51414, 51414, 400, 2721, 11, 456, 311, 257, 10723, 300, 286, 8200, 88, 2504, 510, 13, 51514, 51514, 400, 309, 311, 257, 857, 544, 6179, 17434, 21538, 10723, 813, 264, 472, 286, 5343, 294, 341, 7991, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0855860710144043, "compression_ratio": 1.779783393501805, "no_speech_prob": 6.540243703057058e-06}, {"id": 1592, "seek": 831800, "start": 8327.0, "end": 8330.0, "text": " And we'll make sure that the forward and the backward paths agree identically.", "tokens": [50364, 1396, 456, 311, 257, 1500, 11, 597, 291, 603, 536, 1936, 7829, 732, 24004, 295, 3089, 11, 472, 294, 4532, 2771, 293, 472, 294, 9953, 51, 284, 339, 13, 50814, 50814, 400, 321, 603, 652, 988, 300, 264, 2128, 293, 264, 23897, 14518, 3986, 2473, 984, 13, 50964, 50964, 1171, 257, 4748, 1570, 6179, 6114, 11, 257, 4748, 544, 6179, 6114, 11, 1203, 26383, 13, 51264, 51264, 407, 321, 3986, 365, 9953, 51, 284, 339, 322, 439, 295, 613, 7705, 13, 51414, 51414, 400, 2721, 11, 456, 311, 257, 10723, 300, 286, 8200, 88, 2504, 510, 13, 51514, 51514, 400, 309, 311, 257, 857, 544, 6179, 17434, 21538, 10723, 813, 264, 472, 286, 5343, 294, 341, 7991, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0855860710144043, "compression_ratio": 1.779783393501805, "no_speech_prob": 6.540243703057058e-06}, {"id": 1593, "seek": 831800, "start": 8330.0, "end": 8336.0, "text": " For a slightly less complicated expression, a slightly more complicated expression, everything agrees.", "tokens": [50364, 1396, 456, 311, 257, 1500, 11, 597, 291, 603, 536, 1936, 7829, 732, 24004, 295, 3089, 11, 472, 294, 4532, 2771, 293, 472, 294, 9953, 51, 284, 339, 13, 50814, 50814, 400, 321, 603, 652, 988, 300, 264, 2128, 293, 264, 23897, 14518, 3986, 2473, 984, 13, 50964, 50964, 1171, 257, 4748, 1570, 6179, 6114, 11, 257, 4748, 544, 6179, 6114, 11, 1203, 26383, 13, 51264, 51264, 407, 321, 3986, 365, 9953, 51, 284, 339, 322, 439, 295, 613, 7705, 13, 51414, 51414, 400, 2721, 11, 456, 311, 257, 10723, 300, 286, 8200, 88, 2504, 510, 13, 51514, 51514, 400, 309, 311, 257, 857, 544, 6179, 17434, 21538, 10723, 813, 264, 472, 286, 5343, 294, 341, 7991, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0855860710144043, "compression_ratio": 1.779783393501805, "no_speech_prob": 6.540243703057058e-06}, {"id": 1594, "seek": 831800, "start": 8336.0, "end": 8339.0, "text": " So we agree with PyTorch on all of these operations.", "tokens": [50364, 1396, 456, 311, 257, 1500, 11, 597, 291, 603, 536, 1936, 7829, 732, 24004, 295, 3089, 11, 472, 294, 4532, 2771, 293, 472, 294, 9953, 51, 284, 339, 13, 50814, 50814, 400, 321, 603, 652, 988, 300, 264, 2128, 293, 264, 23897, 14518, 3986, 2473, 984, 13, 50964, 50964, 1171, 257, 4748, 1570, 6179, 6114, 11, 257, 4748, 544, 6179, 6114, 11, 1203, 26383, 13, 51264, 51264, 407, 321, 3986, 365, 9953, 51, 284, 339, 322, 439, 295, 613, 7705, 13, 51414, 51414, 400, 2721, 11, 456, 311, 257, 10723, 300, 286, 8200, 88, 2504, 510, 13, 51514, 51514, 400, 309, 311, 257, 857, 544, 6179, 17434, 21538, 10723, 813, 264, 472, 286, 5343, 294, 341, 7991, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0855860710144043, "compression_ratio": 1.779783393501805, "no_speech_prob": 6.540243703057058e-06}, {"id": 1595, "seek": 831800, "start": 8339.0, "end": 8341.0, "text": " And finally, there's a demo that Ipyymb here.", "tokens": [50364, 1396, 456, 311, 257, 1500, 11, 597, 291, 603, 536, 1936, 7829, 732, 24004, 295, 3089, 11, 472, 294, 4532, 2771, 293, 472, 294, 9953, 51, 284, 339, 13, 50814, 50814, 400, 321, 603, 652, 988, 300, 264, 2128, 293, 264, 23897, 14518, 3986, 2473, 984, 13, 50964, 50964, 1171, 257, 4748, 1570, 6179, 6114, 11, 257, 4748, 544, 6179, 6114, 11, 1203, 26383, 13, 51264, 51264, 407, 321, 3986, 365, 9953, 51, 284, 339, 322, 439, 295, 613, 7705, 13, 51414, 51414, 400, 2721, 11, 456, 311, 257, 10723, 300, 286, 8200, 88, 2504, 510, 13, 51514, 51514, 400, 309, 311, 257, 857, 544, 6179, 17434, 21538, 10723, 813, 264, 472, 286, 5343, 294, 341, 7991, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0855860710144043, "compression_ratio": 1.779783393501805, "no_speech_prob": 6.540243703057058e-06}, {"id": 1596, "seek": 831800, "start": 8341.0, "end": 8346.0, "text": " And it's a bit more complicated binary classification demo than the one I covered in this lecture.", "tokens": [50364, 1396, 456, 311, 257, 1500, 11, 597, 291, 603, 536, 1936, 7829, 732, 24004, 295, 3089, 11, 472, 294, 4532, 2771, 293, 472, 294, 9953, 51, 284, 339, 13, 50814, 50814, 400, 321, 603, 652, 988, 300, 264, 2128, 293, 264, 23897, 14518, 3986, 2473, 984, 13, 50964, 50964, 1171, 257, 4748, 1570, 6179, 6114, 11, 257, 4748, 544, 6179, 6114, 11, 1203, 26383, 13, 51264, 51264, 407, 321, 3986, 365, 9953, 51, 284, 339, 322, 439, 295, 613, 7705, 13, 51414, 51414, 400, 2721, 11, 456, 311, 257, 10723, 300, 286, 8200, 88, 2504, 510, 13, 51514, 51514, 400, 309, 311, 257, 857, 544, 6179, 17434, 21538, 10723, 813, 264, 472, 286, 5343, 294, 341, 7991, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0855860710144043, "compression_ratio": 1.779783393501805, "no_speech_prob": 6.540243703057058e-06}, {"id": 1597, "seek": 834600, "start": 8346.0, "end": 8349.0, "text": " So we only had a tiny data set of four examples.", "tokens": [50364, 407, 321, 787, 632, 257, 5870, 1412, 992, 295, 1451, 5110, 13, 50514, 50514, 1692, 321, 362, 257, 857, 544, 6179, 1365, 365, 3195, 295, 3344, 2793, 293, 3195, 295, 2182, 2793, 13, 50764, 50764, 400, 321, 434, 1382, 281, 11, 797, 11, 1322, 257, 17434, 1508, 9902, 281, 20206, 732, 18795, 2793, 382, 2182, 420, 3344, 13, 51064, 51064, 467, 311, 257, 857, 544, 6179, 21601, 47, 510, 365, 257, 3801, 21601, 47, 13, 51264, 51264, 440, 4470, 307, 257, 857, 544, 6179, 570, 309, 9346, 15245, 279, 13, 51514, 51514, 407, 570, 527, 1412, 992, 390, 370, 5870, 11, 321, 1009, 630, 257, 2128, 1320, 322, 264, 2302, 1412, 992, 295, 1451, 5110, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04999461571375529, "compression_ratio": 1.8521400778210118, "no_speech_prob": 1.2606284144567326e-05}, {"id": 1598, "seek": 834600, "start": 8349.0, "end": 8354.0, "text": " Here we have a bit more complicated example with lots of blue points and lots of red points.", "tokens": [50364, 407, 321, 787, 632, 257, 5870, 1412, 992, 295, 1451, 5110, 13, 50514, 50514, 1692, 321, 362, 257, 857, 544, 6179, 1365, 365, 3195, 295, 3344, 2793, 293, 3195, 295, 2182, 2793, 13, 50764, 50764, 400, 321, 434, 1382, 281, 11, 797, 11, 1322, 257, 17434, 1508, 9902, 281, 20206, 732, 18795, 2793, 382, 2182, 420, 3344, 13, 51064, 51064, 467, 311, 257, 857, 544, 6179, 21601, 47, 510, 365, 257, 3801, 21601, 47, 13, 51264, 51264, 440, 4470, 307, 257, 857, 544, 6179, 570, 309, 9346, 15245, 279, 13, 51514, 51514, 407, 570, 527, 1412, 992, 390, 370, 5870, 11, 321, 1009, 630, 257, 2128, 1320, 322, 264, 2302, 1412, 992, 295, 1451, 5110, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04999461571375529, "compression_ratio": 1.8521400778210118, "no_speech_prob": 1.2606284144567326e-05}, {"id": 1599, "seek": 834600, "start": 8354.0, "end": 8360.0, "text": " And we're trying to, again, build a binary classifier to distinguish two dimensional points as red or blue.", "tokens": [50364, 407, 321, 787, 632, 257, 5870, 1412, 992, 295, 1451, 5110, 13, 50514, 50514, 1692, 321, 362, 257, 857, 544, 6179, 1365, 365, 3195, 295, 3344, 2793, 293, 3195, 295, 2182, 2793, 13, 50764, 50764, 400, 321, 434, 1382, 281, 11, 797, 11, 1322, 257, 17434, 1508, 9902, 281, 20206, 732, 18795, 2793, 382, 2182, 420, 3344, 13, 51064, 51064, 467, 311, 257, 857, 544, 6179, 21601, 47, 510, 365, 257, 3801, 21601, 47, 13, 51264, 51264, 440, 4470, 307, 257, 857, 544, 6179, 570, 309, 9346, 15245, 279, 13, 51514, 51514, 407, 570, 527, 1412, 992, 390, 370, 5870, 11, 321, 1009, 630, 257, 2128, 1320, 322, 264, 2302, 1412, 992, 295, 1451, 5110, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04999461571375529, "compression_ratio": 1.8521400778210118, "no_speech_prob": 1.2606284144567326e-05}, {"id": 1600, "seek": 834600, "start": 8360.0, "end": 8364.0, "text": " It's a bit more complicated MLP here with a bigger MLP.", "tokens": [50364, 407, 321, 787, 632, 257, 5870, 1412, 992, 295, 1451, 5110, 13, 50514, 50514, 1692, 321, 362, 257, 857, 544, 6179, 1365, 365, 3195, 295, 3344, 2793, 293, 3195, 295, 2182, 2793, 13, 50764, 50764, 400, 321, 434, 1382, 281, 11, 797, 11, 1322, 257, 17434, 1508, 9902, 281, 20206, 732, 18795, 2793, 382, 2182, 420, 3344, 13, 51064, 51064, 467, 311, 257, 857, 544, 6179, 21601, 47, 510, 365, 257, 3801, 21601, 47, 13, 51264, 51264, 440, 4470, 307, 257, 857, 544, 6179, 570, 309, 9346, 15245, 279, 13, 51514, 51514, 407, 570, 527, 1412, 992, 390, 370, 5870, 11, 321, 1009, 630, 257, 2128, 1320, 322, 264, 2302, 1412, 992, 295, 1451, 5110, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04999461571375529, "compression_ratio": 1.8521400778210118, "no_speech_prob": 1.2606284144567326e-05}, {"id": 1601, "seek": 834600, "start": 8364.0, "end": 8369.0, "text": " The loss is a bit more complicated because it supports batches.", "tokens": [50364, 407, 321, 787, 632, 257, 5870, 1412, 992, 295, 1451, 5110, 13, 50514, 50514, 1692, 321, 362, 257, 857, 544, 6179, 1365, 365, 3195, 295, 3344, 2793, 293, 3195, 295, 2182, 2793, 13, 50764, 50764, 400, 321, 434, 1382, 281, 11, 797, 11, 1322, 257, 17434, 1508, 9902, 281, 20206, 732, 18795, 2793, 382, 2182, 420, 3344, 13, 51064, 51064, 467, 311, 257, 857, 544, 6179, 21601, 47, 510, 365, 257, 3801, 21601, 47, 13, 51264, 51264, 440, 4470, 307, 257, 857, 544, 6179, 570, 309, 9346, 15245, 279, 13, 51514, 51514, 407, 570, 527, 1412, 992, 390, 370, 5870, 11, 321, 1009, 630, 257, 2128, 1320, 322, 264, 2302, 1412, 992, 295, 1451, 5110, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04999461571375529, "compression_ratio": 1.8521400778210118, "no_speech_prob": 1.2606284144567326e-05}, {"id": 1602, "seek": 834600, "start": 8369.0, "end": 8375.0, "text": " So because our data set was so tiny, we always did a forward pass on the entire data set of four examples.", "tokens": [50364, 407, 321, 787, 632, 257, 5870, 1412, 992, 295, 1451, 5110, 13, 50514, 50514, 1692, 321, 362, 257, 857, 544, 6179, 1365, 365, 3195, 295, 3344, 2793, 293, 3195, 295, 2182, 2793, 13, 50764, 50764, 400, 321, 434, 1382, 281, 11, 797, 11, 1322, 257, 17434, 1508, 9902, 281, 20206, 732, 18795, 2793, 382, 2182, 420, 3344, 13, 51064, 51064, 467, 311, 257, 857, 544, 6179, 21601, 47, 510, 365, 257, 3801, 21601, 47, 13, 51264, 51264, 440, 4470, 307, 257, 857, 544, 6179, 570, 309, 9346, 15245, 279, 13, 51514, 51514, 407, 570, 527, 1412, 992, 390, 370, 5870, 11, 321, 1009, 630, 257, 2128, 1320, 322, 264, 2302, 1412, 992, 295, 1451, 5110, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04999461571375529, "compression_ratio": 1.8521400778210118, "no_speech_prob": 1.2606284144567326e-05}, {"id": 1603, "seek": 837500, "start": 8375.0, "end": 8382.0, "text": " But when your data set is like a million examples, what we usually do in practice is we basically pick out some random subset.", "tokens": [50364, 583, 562, 428, 1412, 992, 307, 411, 257, 2459, 5110, 11, 437, 321, 2673, 360, 294, 3124, 307, 321, 1936, 1888, 484, 512, 4974, 25993, 13, 50714, 50714, 492, 818, 300, 257, 15245, 13, 400, 550, 321, 787, 1399, 264, 15245, 2128, 11, 23897, 293, 5623, 13, 50964, 50964, 407, 321, 500, 380, 362, 281, 2128, 264, 2302, 3097, 992, 13, 51114, 51114, 407, 341, 9346, 15245, 278, 570, 456, 311, 257, 688, 544, 5110, 510, 13, 51314, 51314, 492, 360, 257, 2128, 1320, 13, 440, 4470, 307, 4748, 544, 819, 13, 51464, 51464, 639, 307, 257, 11469, 10270, 4470, 300, 286, 4445, 510, 13, 51614, 51614, 440, 472, 300, 321, 1143, 390, 264, 914, 8889, 6713, 4470, 570, 309, 311, 264, 22811, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04901485664899959, "compression_ratio": 1.7475083056478404, "no_speech_prob": 1.805709325708449e-05}, {"id": 1604, "seek": 837500, "start": 8382.0, "end": 8387.0, "text": " We call that a batch. And then we only process the batch forward, backward and update.", "tokens": [50364, 583, 562, 428, 1412, 992, 307, 411, 257, 2459, 5110, 11, 437, 321, 2673, 360, 294, 3124, 307, 321, 1936, 1888, 484, 512, 4974, 25993, 13, 50714, 50714, 492, 818, 300, 257, 15245, 13, 400, 550, 321, 787, 1399, 264, 15245, 2128, 11, 23897, 293, 5623, 13, 50964, 50964, 407, 321, 500, 380, 362, 281, 2128, 264, 2302, 3097, 992, 13, 51114, 51114, 407, 341, 9346, 15245, 278, 570, 456, 311, 257, 688, 544, 5110, 510, 13, 51314, 51314, 492, 360, 257, 2128, 1320, 13, 440, 4470, 307, 4748, 544, 819, 13, 51464, 51464, 639, 307, 257, 11469, 10270, 4470, 300, 286, 4445, 510, 13, 51614, 51614, 440, 472, 300, 321, 1143, 390, 264, 914, 8889, 6713, 4470, 570, 309, 311, 264, 22811, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04901485664899959, "compression_ratio": 1.7475083056478404, "no_speech_prob": 1.805709325708449e-05}, {"id": 1605, "seek": 837500, "start": 8387.0, "end": 8390.0, "text": " So we don't have to forward the entire training set.", "tokens": [50364, 583, 562, 428, 1412, 992, 307, 411, 257, 2459, 5110, 11, 437, 321, 2673, 360, 294, 3124, 307, 321, 1936, 1888, 484, 512, 4974, 25993, 13, 50714, 50714, 492, 818, 300, 257, 15245, 13, 400, 550, 321, 787, 1399, 264, 15245, 2128, 11, 23897, 293, 5623, 13, 50964, 50964, 407, 321, 500, 380, 362, 281, 2128, 264, 2302, 3097, 992, 13, 51114, 51114, 407, 341, 9346, 15245, 278, 570, 456, 311, 257, 688, 544, 5110, 510, 13, 51314, 51314, 492, 360, 257, 2128, 1320, 13, 440, 4470, 307, 4748, 544, 819, 13, 51464, 51464, 639, 307, 257, 11469, 10270, 4470, 300, 286, 4445, 510, 13, 51614, 51614, 440, 472, 300, 321, 1143, 390, 264, 914, 8889, 6713, 4470, 570, 309, 311, 264, 22811, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04901485664899959, "compression_ratio": 1.7475083056478404, "no_speech_prob": 1.805709325708449e-05}, {"id": 1606, "seek": 837500, "start": 8390.0, "end": 8394.0, "text": " So this supports batching because there's a lot more examples here.", "tokens": [50364, 583, 562, 428, 1412, 992, 307, 411, 257, 2459, 5110, 11, 437, 321, 2673, 360, 294, 3124, 307, 321, 1936, 1888, 484, 512, 4974, 25993, 13, 50714, 50714, 492, 818, 300, 257, 15245, 13, 400, 550, 321, 787, 1399, 264, 15245, 2128, 11, 23897, 293, 5623, 13, 50964, 50964, 407, 321, 500, 380, 362, 281, 2128, 264, 2302, 3097, 992, 13, 51114, 51114, 407, 341, 9346, 15245, 278, 570, 456, 311, 257, 688, 544, 5110, 510, 13, 51314, 51314, 492, 360, 257, 2128, 1320, 13, 440, 4470, 307, 4748, 544, 819, 13, 51464, 51464, 639, 307, 257, 11469, 10270, 4470, 300, 286, 4445, 510, 13, 51614, 51614, 440, 472, 300, 321, 1143, 390, 264, 914, 8889, 6713, 4470, 570, 309, 311, 264, 22811, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04901485664899959, "compression_ratio": 1.7475083056478404, "no_speech_prob": 1.805709325708449e-05}, {"id": 1607, "seek": 837500, "start": 8394.0, "end": 8397.0, "text": " We do a forward pass. The loss is slightly more different.", "tokens": [50364, 583, 562, 428, 1412, 992, 307, 411, 257, 2459, 5110, 11, 437, 321, 2673, 360, 294, 3124, 307, 321, 1936, 1888, 484, 512, 4974, 25993, 13, 50714, 50714, 492, 818, 300, 257, 15245, 13, 400, 550, 321, 787, 1399, 264, 15245, 2128, 11, 23897, 293, 5623, 13, 50964, 50964, 407, 321, 500, 380, 362, 281, 2128, 264, 2302, 3097, 992, 13, 51114, 51114, 407, 341, 9346, 15245, 278, 570, 456, 311, 257, 688, 544, 5110, 510, 13, 51314, 51314, 492, 360, 257, 2128, 1320, 13, 440, 4470, 307, 4748, 544, 819, 13, 51464, 51464, 639, 307, 257, 11469, 10270, 4470, 300, 286, 4445, 510, 13, 51614, 51614, 440, 472, 300, 321, 1143, 390, 264, 914, 8889, 6713, 4470, 570, 309, 311, 264, 22811, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04901485664899959, "compression_ratio": 1.7475083056478404, "no_speech_prob": 1.805709325708449e-05}, {"id": 1608, "seek": 837500, "start": 8397.0, "end": 8400.0, "text": " This is a max margin loss that I implement here.", "tokens": [50364, 583, 562, 428, 1412, 992, 307, 411, 257, 2459, 5110, 11, 437, 321, 2673, 360, 294, 3124, 307, 321, 1936, 1888, 484, 512, 4974, 25993, 13, 50714, 50714, 492, 818, 300, 257, 15245, 13, 400, 550, 321, 787, 1399, 264, 15245, 2128, 11, 23897, 293, 5623, 13, 50964, 50964, 407, 321, 500, 380, 362, 281, 2128, 264, 2302, 3097, 992, 13, 51114, 51114, 407, 341, 9346, 15245, 278, 570, 456, 311, 257, 688, 544, 5110, 510, 13, 51314, 51314, 492, 360, 257, 2128, 1320, 13, 440, 4470, 307, 4748, 544, 819, 13, 51464, 51464, 639, 307, 257, 11469, 10270, 4470, 300, 286, 4445, 510, 13, 51614, 51614, 440, 472, 300, 321, 1143, 390, 264, 914, 8889, 6713, 4470, 570, 309, 311, 264, 22811, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04901485664899959, "compression_ratio": 1.7475083056478404, "no_speech_prob": 1.805709325708449e-05}, {"id": 1609, "seek": 837500, "start": 8400.0, "end": 8404.0, "text": " The one that we used was the mean squared error loss because it's the simplest one.", "tokens": [50364, 583, 562, 428, 1412, 992, 307, 411, 257, 2459, 5110, 11, 437, 321, 2673, 360, 294, 3124, 307, 321, 1936, 1888, 484, 512, 4974, 25993, 13, 50714, 50714, 492, 818, 300, 257, 15245, 13, 400, 550, 321, 787, 1399, 264, 15245, 2128, 11, 23897, 293, 5623, 13, 50964, 50964, 407, 321, 500, 380, 362, 281, 2128, 264, 2302, 3097, 992, 13, 51114, 51114, 407, 341, 9346, 15245, 278, 570, 456, 311, 257, 688, 544, 5110, 510, 13, 51314, 51314, 492, 360, 257, 2128, 1320, 13, 440, 4470, 307, 4748, 544, 819, 13, 51464, 51464, 639, 307, 257, 11469, 10270, 4470, 300, 286, 4445, 510, 13, 51614, 51614, 440, 472, 300, 321, 1143, 390, 264, 914, 8889, 6713, 4470, 570, 309, 311, 264, 22811, 472, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04901485664899959, "compression_ratio": 1.7475083056478404, "no_speech_prob": 1.805709325708449e-05}, {"id": 1610, "seek": 840400, "start": 8404.0, "end": 8407.0, "text": " There's also the binary cross entropy loss.", "tokens": [50364, 821, 311, 611, 264, 17434, 3278, 30867, 4470, 13, 50514, 50514, 1057, 295, 552, 393, 312, 1143, 337, 17434, 21538, 293, 500, 380, 652, 886, 709, 295, 257, 2649, 294, 264, 2199, 5110, 300, 321, 2956, 412, 370, 1400, 13, 50814, 50814, 821, 311, 746, 1219, 441, 17, 3890, 2144, 1143, 510, 13, 50964, 50964, 639, 575, 281, 360, 365, 2674, 2144, 295, 264, 18161, 2533, 293, 9003, 264, 670, 69, 2414, 294, 3479, 2539, 3287, 13, 51264, 51264, 583, 286, 630, 406, 2060, 613, 10392, 294, 341, 960, 7263, 1780, 13, 51464, 51464, 400, 264, 3097, 6367, 291, 820, 5521, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.05845516132858564, "compression_ratio": 1.627659574468085, "no_speech_prob": 1.6700279957149178e-05}, {"id": 1611, "seek": 840400, "start": 8407.0, "end": 8413.0, "text": " All of them can be used for binary classification and don't make too much of a difference in the simple examples that we looked at so far.", "tokens": [50364, 821, 311, 611, 264, 17434, 3278, 30867, 4470, 13, 50514, 50514, 1057, 295, 552, 393, 312, 1143, 337, 17434, 21538, 293, 500, 380, 652, 886, 709, 295, 257, 2649, 294, 264, 2199, 5110, 300, 321, 2956, 412, 370, 1400, 13, 50814, 50814, 821, 311, 746, 1219, 441, 17, 3890, 2144, 1143, 510, 13, 50964, 50964, 639, 575, 281, 360, 365, 2674, 2144, 295, 264, 18161, 2533, 293, 9003, 264, 670, 69, 2414, 294, 3479, 2539, 3287, 13, 51264, 51264, 583, 286, 630, 406, 2060, 613, 10392, 294, 341, 960, 7263, 1780, 13, 51464, 51464, 400, 264, 3097, 6367, 291, 820, 5521, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.05845516132858564, "compression_ratio": 1.627659574468085, "no_speech_prob": 1.6700279957149178e-05}, {"id": 1612, "seek": 840400, "start": 8413.0, "end": 8416.0, "text": " There's something called L2 regularization used here.", "tokens": [50364, 821, 311, 611, 264, 17434, 3278, 30867, 4470, 13, 50514, 50514, 1057, 295, 552, 393, 312, 1143, 337, 17434, 21538, 293, 500, 380, 652, 886, 709, 295, 257, 2649, 294, 264, 2199, 5110, 300, 321, 2956, 412, 370, 1400, 13, 50814, 50814, 821, 311, 746, 1219, 441, 17, 3890, 2144, 1143, 510, 13, 50964, 50964, 639, 575, 281, 360, 365, 2674, 2144, 295, 264, 18161, 2533, 293, 9003, 264, 670, 69, 2414, 294, 3479, 2539, 3287, 13, 51264, 51264, 583, 286, 630, 406, 2060, 613, 10392, 294, 341, 960, 7263, 1780, 13, 51464, 51464, 400, 264, 3097, 6367, 291, 820, 5521, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.05845516132858564, "compression_ratio": 1.627659574468085, "no_speech_prob": 1.6700279957149178e-05}, {"id": 1613, "seek": 840400, "start": 8416.0, "end": 8422.0, "text": " This has to do with generalization of the neural net and controls the overfitting in machine learning setting.", "tokens": [50364, 821, 311, 611, 264, 17434, 3278, 30867, 4470, 13, 50514, 50514, 1057, 295, 552, 393, 312, 1143, 337, 17434, 21538, 293, 500, 380, 652, 886, 709, 295, 257, 2649, 294, 264, 2199, 5110, 300, 321, 2956, 412, 370, 1400, 13, 50814, 50814, 821, 311, 746, 1219, 441, 17, 3890, 2144, 1143, 510, 13, 50964, 50964, 639, 575, 281, 360, 365, 2674, 2144, 295, 264, 18161, 2533, 293, 9003, 264, 670, 69, 2414, 294, 3479, 2539, 3287, 13, 51264, 51264, 583, 286, 630, 406, 2060, 613, 10392, 294, 341, 960, 7263, 1780, 13, 51464, 51464, 400, 264, 3097, 6367, 291, 820, 5521, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.05845516132858564, "compression_ratio": 1.627659574468085, "no_speech_prob": 1.6700279957149178e-05}, {"id": 1614, "seek": 840400, "start": 8422.0, "end": 8426.0, "text": " But I did not cover these concepts in this video potentially later.", "tokens": [50364, 821, 311, 611, 264, 17434, 3278, 30867, 4470, 13, 50514, 50514, 1057, 295, 552, 393, 312, 1143, 337, 17434, 21538, 293, 500, 380, 652, 886, 709, 295, 257, 2649, 294, 264, 2199, 5110, 300, 321, 2956, 412, 370, 1400, 13, 50814, 50814, 821, 311, 746, 1219, 441, 17, 3890, 2144, 1143, 510, 13, 50964, 50964, 639, 575, 281, 360, 365, 2674, 2144, 295, 264, 18161, 2533, 293, 9003, 264, 670, 69, 2414, 294, 3479, 2539, 3287, 13, 51264, 51264, 583, 286, 630, 406, 2060, 613, 10392, 294, 341, 960, 7263, 1780, 13, 51464, 51464, 400, 264, 3097, 6367, 291, 820, 5521, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.05845516132858564, "compression_ratio": 1.627659574468085, "no_speech_prob": 1.6700279957149178e-05}, {"id": 1615, "seek": 840400, "start": 8426.0, "end": 8428.0, "text": " And the training loop you should recognize.", "tokens": [50364, 821, 311, 611, 264, 17434, 3278, 30867, 4470, 13, 50514, 50514, 1057, 295, 552, 393, 312, 1143, 337, 17434, 21538, 293, 500, 380, 652, 886, 709, 295, 257, 2649, 294, 264, 2199, 5110, 300, 321, 2956, 412, 370, 1400, 13, 50814, 50814, 821, 311, 746, 1219, 441, 17, 3890, 2144, 1143, 510, 13, 50964, 50964, 639, 575, 281, 360, 365, 2674, 2144, 295, 264, 18161, 2533, 293, 9003, 264, 670, 69, 2414, 294, 3479, 2539, 3287, 13, 51264, 51264, 583, 286, 630, 406, 2060, 613, 10392, 294, 341, 960, 7263, 1780, 13, 51464, 51464, 400, 264, 3097, 6367, 291, 820, 5521, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.05845516132858564, "compression_ratio": 1.627659574468085, "no_speech_prob": 1.6700279957149178e-05}, {"id": 1616, "seek": 842800, "start": 8428.0, "end": 8434.0, "text": " So forward, backward, with, zero grad, and update, and so on.", "tokens": [50364, 407, 2128, 11, 23897, 11, 365, 11, 4018, 2771, 11, 293, 5623, 11, 293, 370, 322, 13, 50664, 50664, 509, 603, 3449, 300, 294, 264, 5623, 510, 11, 264, 2539, 3314, 307, 36039, 382, 257, 2445, 295, 1230, 295, 36540, 293, 309, 9884, 16431, 13, 51014, 51014, 400, 341, 307, 746, 1219, 2539, 3314, 21039, 13, 51114, 51114, 407, 294, 264, 2863, 11, 291, 362, 257, 1090, 2539, 3314, 13, 51214, 51214, 400, 382, 264, 3209, 1333, 295, 11652, 5660, 2651, 264, 917, 11, 291, 1565, 760, 264, 2539, 3314, 281, 483, 512, 295, 264, 2489, 4365, 294, 264, 917, 13, 51564, 51564, 400, 294, 264, 917, 11, 321, 536, 264, 3537, 3753, 295, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07337273144331134, "compression_ratio": 1.8307692307692307, "no_speech_prob": 1.6439860701211728e-05}, {"id": 1617, "seek": 842800, "start": 8434.0, "end": 8441.0, "text": " You'll notice that in the update here, the learning rate is scaled as a function of number of iterations and it shrinks.", "tokens": [50364, 407, 2128, 11, 23897, 11, 365, 11, 4018, 2771, 11, 293, 5623, 11, 293, 370, 322, 13, 50664, 50664, 509, 603, 3449, 300, 294, 264, 5623, 510, 11, 264, 2539, 3314, 307, 36039, 382, 257, 2445, 295, 1230, 295, 36540, 293, 309, 9884, 16431, 13, 51014, 51014, 400, 341, 307, 746, 1219, 2539, 3314, 21039, 13, 51114, 51114, 407, 294, 264, 2863, 11, 291, 362, 257, 1090, 2539, 3314, 13, 51214, 51214, 400, 382, 264, 3209, 1333, 295, 11652, 5660, 2651, 264, 917, 11, 291, 1565, 760, 264, 2539, 3314, 281, 483, 512, 295, 264, 2489, 4365, 294, 264, 917, 13, 51564, 51564, 400, 294, 264, 917, 11, 321, 536, 264, 3537, 3753, 295, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07337273144331134, "compression_ratio": 1.8307692307692307, "no_speech_prob": 1.6439860701211728e-05}, {"id": 1618, "seek": 842800, "start": 8441.0, "end": 8443.0, "text": " And this is something called learning rate decay.", "tokens": [50364, 407, 2128, 11, 23897, 11, 365, 11, 4018, 2771, 11, 293, 5623, 11, 293, 370, 322, 13, 50664, 50664, 509, 603, 3449, 300, 294, 264, 5623, 510, 11, 264, 2539, 3314, 307, 36039, 382, 257, 2445, 295, 1230, 295, 36540, 293, 309, 9884, 16431, 13, 51014, 51014, 400, 341, 307, 746, 1219, 2539, 3314, 21039, 13, 51114, 51114, 407, 294, 264, 2863, 11, 291, 362, 257, 1090, 2539, 3314, 13, 51214, 51214, 400, 382, 264, 3209, 1333, 295, 11652, 5660, 2651, 264, 917, 11, 291, 1565, 760, 264, 2539, 3314, 281, 483, 512, 295, 264, 2489, 4365, 294, 264, 917, 13, 51564, 51564, 400, 294, 264, 917, 11, 321, 536, 264, 3537, 3753, 295, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07337273144331134, "compression_ratio": 1.8307692307692307, "no_speech_prob": 1.6439860701211728e-05}, {"id": 1619, "seek": 842800, "start": 8443.0, "end": 8445.0, "text": " So in the beginning, you have a high learning rate.", "tokens": [50364, 407, 2128, 11, 23897, 11, 365, 11, 4018, 2771, 11, 293, 5623, 11, 293, 370, 322, 13, 50664, 50664, 509, 603, 3449, 300, 294, 264, 5623, 510, 11, 264, 2539, 3314, 307, 36039, 382, 257, 2445, 295, 1230, 295, 36540, 293, 309, 9884, 16431, 13, 51014, 51014, 400, 341, 307, 746, 1219, 2539, 3314, 21039, 13, 51114, 51114, 407, 294, 264, 2863, 11, 291, 362, 257, 1090, 2539, 3314, 13, 51214, 51214, 400, 382, 264, 3209, 1333, 295, 11652, 5660, 2651, 264, 917, 11, 291, 1565, 760, 264, 2539, 3314, 281, 483, 512, 295, 264, 2489, 4365, 294, 264, 917, 13, 51564, 51564, 400, 294, 264, 917, 11, 321, 536, 264, 3537, 3753, 295, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07337273144331134, "compression_ratio": 1.8307692307692307, "no_speech_prob": 1.6439860701211728e-05}, {"id": 1620, "seek": 842800, "start": 8445.0, "end": 8452.0, "text": " And as the network sort of stabilizes near the end, you bring down the learning rate to get some of the fine details in the end.", "tokens": [50364, 407, 2128, 11, 23897, 11, 365, 11, 4018, 2771, 11, 293, 5623, 11, 293, 370, 322, 13, 50664, 50664, 509, 603, 3449, 300, 294, 264, 5623, 510, 11, 264, 2539, 3314, 307, 36039, 382, 257, 2445, 295, 1230, 295, 36540, 293, 309, 9884, 16431, 13, 51014, 51014, 400, 341, 307, 746, 1219, 2539, 3314, 21039, 13, 51114, 51114, 407, 294, 264, 2863, 11, 291, 362, 257, 1090, 2539, 3314, 13, 51214, 51214, 400, 382, 264, 3209, 1333, 295, 11652, 5660, 2651, 264, 917, 11, 291, 1565, 760, 264, 2539, 3314, 281, 483, 512, 295, 264, 2489, 4365, 294, 264, 917, 13, 51564, 51564, 400, 294, 264, 917, 11, 321, 536, 264, 3537, 3753, 295, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07337273144331134, "compression_ratio": 1.8307692307692307, "no_speech_prob": 1.6439860701211728e-05}, {"id": 1621, "seek": 842800, "start": 8452.0, "end": 8456.0, "text": " And in the end, we see the decision surface of the neural net.", "tokens": [50364, 407, 2128, 11, 23897, 11, 365, 11, 4018, 2771, 11, 293, 5623, 11, 293, 370, 322, 13, 50664, 50664, 509, 603, 3449, 300, 294, 264, 5623, 510, 11, 264, 2539, 3314, 307, 36039, 382, 257, 2445, 295, 1230, 295, 36540, 293, 309, 9884, 16431, 13, 51014, 51014, 400, 341, 307, 746, 1219, 2539, 3314, 21039, 13, 51114, 51114, 407, 294, 264, 2863, 11, 291, 362, 257, 1090, 2539, 3314, 13, 51214, 51214, 400, 382, 264, 3209, 1333, 295, 11652, 5660, 2651, 264, 917, 11, 291, 1565, 760, 264, 2539, 3314, 281, 483, 512, 295, 264, 2489, 4365, 294, 264, 917, 13, 51564, 51564, 400, 294, 264, 917, 11, 321, 536, 264, 3537, 3753, 295, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07337273144331134, "compression_ratio": 1.8307692307692307, "no_speech_prob": 1.6439860701211728e-05}, {"id": 1622, "seek": 845600, "start": 8456.0, "end": 8462.0, "text": " And we see that it learns to separate out the red and the blue area based on the data points.", "tokens": [50364, 400, 321, 536, 300, 309, 27152, 281, 4994, 484, 264, 2182, 293, 264, 3344, 1859, 2361, 322, 264, 1412, 2793, 13, 50664, 50664, 407, 300, 311, 264, 4748, 544, 6179, 1365, 294, 264, 10723, 10723, 300, 286, 2256, 472, 363, 300, 291, 434, 1737, 281, 352, 670, 13, 50914, 50914, 583, 1338, 11, 382, 295, 965, 11, 300, 307, 4532, 2771, 13, 51064, 51064, 286, 611, 1415, 281, 855, 291, 257, 707, 857, 295, 957, 1507, 370, 300, 291, 483, 281, 536, 577, 341, 307, 767, 12270, 294, 4265, 7204, 6405, 411, 538, 27822, 13, 51414, 51414, 407, 294, 1729, 11, 286, 1415, 281, 855, 286, 1415, 281, 915, 293, 855, 291, 264, 23897, 11524, 1266, 389, 294, 452, 27822, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15581437683105467, "compression_ratio": 1.7201365187713311, "no_speech_prob": 1.1842645108117722e-05}, {"id": 1623, "seek": 845600, "start": 8462.0, "end": 8467.0, "text": " So that's the slightly more complicated example in the demo demo that I buy one B that you're free to go over.", "tokens": [50364, 400, 321, 536, 300, 309, 27152, 281, 4994, 484, 264, 2182, 293, 264, 3344, 1859, 2361, 322, 264, 1412, 2793, 13, 50664, 50664, 407, 300, 311, 264, 4748, 544, 6179, 1365, 294, 264, 10723, 10723, 300, 286, 2256, 472, 363, 300, 291, 434, 1737, 281, 352, 670, 13, 50914, 50914, 583, 1338, 11, 382, 295, 965, 11, 300, 307, 4532, 2771, 13, 51064, 51064, 286, 611, 1415, 281, 855, 291, 257, 707, 857, 295, 957, 1507, 370, 300, 291, 483, 281, 536, 577, 341, 307, 767, 12270, 294, 4265, 7204, 6405, 411, 538, 27822, 13, 51414, 51414, 407, 294, 1729, 11, 286, 1415, 281, 855, 286, 1415, 281, 915, 293, 855, 291, 264, 23897, 11524, 1266, 389, 294, 452, 27822, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15581437683105467, "compression_ratio": 1.7201365187713311, "no_speech_prob": 1.1842645108117722e-05}, {"id": 1624, "seek": 845600, "start": 8467.0, "end": 8470.0, "text": " But yeah, as of today, that is micro grad.", "tokens": [50364, 400, 321, 536, 300, 309, 27152, 281, 4994, 484, 264, 2182, 293, 264, 3344, 1859, 2361, 322, 264, 1412, 2793, 13, 50664, 50664, 407, 300, 311, 264, 4748, 544, 6179, 1365, 294, 264, 10723, 10723, 300, 286, 2256, 472, 363, 300, 291, 434, 1737, 281, 352, 670, 13, 50914, 50914, 583, 1338, 11, 382, 295, 965, 11, 300, 307, 4532, 2771, 13, 51064, 51064, 286, 611, 1415, 281, 855, 291, 257, 707, 857, 295, 957, 1507, 370, 300, 291, 483, 281, 536, 577, 341, 307, 767, 12270, 294, 4265, 7204, 6405, 411, 538, 27822, 13, 51414, 51414, 407, 294, 1729, 11, 286, 1415, 281, 855, 286, 1415, 281, 915, 293, 855, 291, 264, 23897, 11524, 1266, 389, 294, 452, 27822, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15581437683105467, "compression_ratio": 1.7201365187713311, "no_speech_prob": 1.1842645108117722e-05}, {"id": 1625, "seek": 845600, "start": 8470.0, "end": 8477.0, "text": " I also wanted to show you a little bit of real stuff so that you get to see how this is actually implemented in production grade library like by torch.", "tokens": [50364, 400, 321, 536, 300, 309, 27152, 281, 4994, 484, 264, 2182, 293, 264, 3344, 1859, 2361, 322, 264, 1412, 2793, 13, 50664, 50664, 407, 300, 311, 264, 4748, 544, 6179, 1365, 294, 264, 10723, 10723, 300, 286, 2256, 472, 363, 300, 291, 434, 1737, 281, 352, 670, 13, 50914, 50914, 583, 1338, 11, 382, 295, 965, 11, 300, 307, 4532, 2771, 13, 51064, 51064, 286, 611, 1415, 281, 855, 291, 257, 707, 857, 295, 957, 1507, 370, 300, 291, 483, 281, 536, 577, 341, 307, 767, 12270, 294, 4265, 7204, 6405, 411, 538, 27822, 13, 51414, 51414, 407, 294, 1729, 11, 286, 1415, 281, 855, 286, 1415, 281, 915, 293, 855, 291, 264, 23897, 11524, 1266, 389, 294, 452, 27822, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15581437683105467, "compression_ratio": 1.7201365187713311, "no_speech_prob": 1.1842645108117722e-05}, {"id": 1626, "seek": 845600, "start": 8477.0, "end": 8483.0, "text": " So in particular, I wanted to show I wanted to find and show you the backward password 10 H in my torch.", "tokens": [50364, 400, 321, 536, 300, 309, 27152, 281, 4994, 484, 264, 2182, 293, 264, 3344, 1859, 2361, 322, 264, 1412, 2793, 13, 50664, 50664, 407, 300, 311, 264, 4748, 544, 6179, 1365, 294, 264, 10723, 10723, 300, 286, 2256, 472, 363, 300, 291, 434, 1737, 281, 352, 670, 13, 50914, 50914, 583, 1338, 11, 382, 295, 965, 11, 300, 307, 4532, 2771, 13, 51064, 51064, 286, 611, 1415, 281, 855, 291, 257, 707, 857, 295, 957, 1507, 370, 300, 291, 483, 281, 536, 577, 341, 307, 767, 12270, 294, 4265, 7204, 6405, 411, 538, 27822, 13, 51414, 51414, 407, 294, 1729, 11, 286, 1415, 281, 855, 286, 1415, 281, 915, 293, 855, 291, 264, 23897, 11524, 1266, 389, 294, 452, 27822, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15581437683105467, "compression_ratio": 1.7201365187713311, "no_speech_prob": 1.1842645108117722e-05}, {"id": 1627, "seek": 848300, "start": 8483.0, "end": 8495.0, "text": " So here in micro grad, we see that the backward password 10 H is one minus T square, where T is the output of the 10 H of X times of that grad, which is the chain rule.", "tokens": [50364, 407, 510, 294, 4532, 2771, 11, 321, 536, 300, 264, 23897, 11524, 1266, 389, 307, 472, 3175, 314, 3732, 11, 689, 314, 307, 264, 5598, 295, 264, 1266, 389, 295, 1783, 1413, 295, 300, 2771, 11, 597, 307, 264, 5021, 4978, 13, 50964, 50964, 407, 321, 434, 1237, 337, 746, 300, 1542, 411, 341, 13, 51114, 51114, 823, 11, 286, 1437, 281, 452, 27822, 11, 597, 575, 364, 1269, 4009, 23331, 3089, 3096, 11, 293, 286, 2956, 807, 257, 688, 295, 1080, 3089, 13, 51564, 51564, 400, 6095, 11, 286, 4418, 466, 2119, 2077, 293, 286, 2809, 380, 915, 1266, 389, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1286594552813836, "compression_ratio": 1.576, "no_speech_prob": 1.2606298696482554e-05}, {"id": 1628, "seek": 848300, "start": 8495.0, "end": 8498.0, "text": " So we're looking for something that looks like this.", "tokens": [50364, 407, 510, 294, 4532, 2771, 11, 321, 536, 300, 264, 23897, 11524, 1266, 389, 307, 472, 3175, 314, 3732, 11, 689, 314, 307, 264, 5598, 295, 264, 1266, 389, 295, 1783, 1413, 295, 300, 2771, 11, 597, 307, 264, 5021, 4978, 13, 50964, 50964, 407, 321, 434, 1237, 337, 746, 300, 1542, 411, 341, 13, 51114, 51114, 823, 11, 286, 1437, 281, 452, 27822, 11, 597, 575, 364, 1269, 4009, 23331, 3089, 3096, 11, 293, 286, 2956, 807, 257, 688, 295, 1080, 3089, 13, 51564, 51564, 400, 6095, 11, 286, 4418, 466, 2119, 2077, 293, 286, 2809, 380, 915, 1266, 389, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1286594552813836, "compression_ratio": 1.576, "no_speech_prob": 1.2606298696482554e-05}, {"id": 1629, "seek": 848300, "start": 8498.0, "end": 8507.0, "text": " Now, I went to my torch, which has an open source GitHub code base, and I looked through a lot of its code.", "tokens": [50364, 407, 510, 294, 4532, 2771, 11, 321, 536, 300, 264, 23897, 11524, 1266, 389, 307, 472, 3175, 314, 3732, 11, 689, 314, 307, 264, 5598, 295, 264, 1266, 389, 295, 1783, 1413, 295, 300, 2771, 11, 597, 307, 264, 5021, 4978, 13, 50964, 50964, 407, 321, 434, 1237, 337, 746, 300, 1542, 411, 341, 13, 51114, 51114, 823, 11, 286, 1437, 281, 452, 27822, 11, 597, 575, 364, 1269, 4009, 23331, 3089, 3096, 11, 293, 286, 2956, 807, 257, 688, 295, 1080, 3089, 13, 51564, 51564, 400, 6095, 11, 286, 4418, 466, 2119, 2077, 293, 286, 2809, 380, 915, 1266, 389, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1286594552813836, "compression_ratio": 1.576, "no_speech_prob": 1.2606298696482554e-05}, {"id": 1630, "seek": 848300, "start": 8507.0, "end": 8512.0, "text": " And honestly, I spent about 15 minutes and I couldn't find 10 H.", "tokens": [50364, 407, 510, 294, 4532, 2771, 11, 321, 536, 300, 264, 23897, 11524, 1266, 389, 307, 472, 3175, 314, 3732, 11, 689, 314, 307, 264, 5598, 295, 264, 1266, 389, 295, 1783, 1413, 295, 300, 2771, 11, 597, 307, 264, 5021, 4978, 13, 50964, 50964, 407, 321, 434, 1237, 337, 746, 300, 1542, 411, 341, 13, 51114, 51114, 823, 11, 286, 1437, 281, 452, 27822, 11, 597, 575, 364, 1269, 4009, 23331, 3089, 3096, 11, 293, 286, 2956, 807, 257, 688, 295, 1080, 3089, 13, 51564, 51564, 400, 6095, 11, 286, 4418, 466, 2119, 2077, 293, 286, 2809, 380, 915, 1266, 389, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1286594552813836, "compression_ratio": 1.576, "no_speech_prob": 1.2606298696482554e-05}, {"id": 1631, "seek": 851200, "start": 8512.0, "end": 8516.0, "text": " And that's because these libraries, unfortunately, they grow in size and entropy.", "tokens": [50364, 400, 300, 311, 570, 613, 15148, 11, 7015, 11, 436, 1852, 294, 2744, 293, 30867, 13, 50564, 50564, 400, 498, 291, 445, 3164, 337, 1266, 389, 11, 291, 483, 7970, 732, 4714, 3180, 3262, 3542, 293, 1451, 3262, 293, 1451, 3262, 293, 2309, 7098, 13, 50914, 50914, 407, 286, 500, 380, 458, 437, 613, 7098, 366, 884, 11, 6095, 13, 51164, 51164, 400, 983, 456, 366, 370, 867, 8000, 295, 1266, 389, 13, 583, 7015, 11, 613, 15148, 366, 1596, 3997, 13, 51364, 51364, 814, 434, 4140, 281, 312, 1143, 11, 406, 534, 1028, 10729, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07300710678100586, "compression_ratio": 1.6613545816733069, "no_speech_prob": 6.048738214303739e-06}, {"id": 1632, "seek": 851200, "start": 8516.0, "end": 8523.0, "text": " And if you just search for 10 H, you get apparently two thousand eight hundred results and four hundred and four hundred and six files.", "tokens": [50364, 400, 300, 311, 570, 613, 15148, 11, 7015, 11, 436, 1852, 294, 2744, 293, 30867, 13, 50564, 50564, 400, 498, 291, 445, 3164, 337, 1266, 389, 11, 291, 483, 7970, 732, 4714, 3180, 3262, 3542, 293, 1451, 3262, 293, 1451, 3262, 293, 2309, 7098, 13, 50914, 50914, 407, 286, 500, 380, 458, 437, 613, 7098, 366, 884, 11, 6095, 13, 51164, 51164, 400, 983, 456, 366, 370, 867, 8000, 295, 1266, 389, 13, 583, 7015, 11, 613, 15148, 366, 1596, 3997, 13, 51364, 51364, 814, 434, 4140, 281, 312, 1143, 11, 406, 534, 1028, 10729, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07300710678100586, "compression_ratio": 1.6613545816733069, "no_speech_prob": 6.048738214303739e-06}, {"id": 1633, "seek": 851200, "start": 8523.0, "end": 8528.0, "text": " So I don't know what these files are doing, honestly.", "tokens": [50364, 400, 300, 311, 570, 613, 15148, 11, 7015, 11, 436, 1852, 294, 2744, 293, 30867, 13, 50564, 50564, 400, 498, 291, 445, 3164, 337, 1266, 389, 11, 291, 483, 7970, 732, 4714, 3180, 3262, 3542, 293, 1451, 3262, 293, 1451, 3262, 293, 2309, 7098, 13, 50914, 50914, 407, 286, 500, 380, 458, 437, 613, 7098, 366, 884, 11, 6095, 13, 51164, 51164, 400, 983, 456, 366, 370, 867, 8000, 295, 1266, 389, 13, 583, 7015, 11, 613, 15148, 366, 1596, 3997, 13, 51364, 51364, 814, 434, 4140, 281, 312, 1143, 11, 406, 534, 1028, 10729, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07300710678100586, "compression_ratio": 1.6613545816733069, "no_speech_prob": 6.048738214303739e-06}, {"id": 1634, "seek": 851200, "start": 8528.0, "end": 8532.0, "text": " And why there are so many measures of 10 H. But unfortunately, these libraries are quite complex.", "tokens": [50364, 400, 300, 311, 570, 613, 15148, 11, 7015, 11, 436, 1852, 294, 2744, 293, 30867, 13, 50564, 50564, 400, 498, 291, 445, 3164, 337, 1266, 389, 11, 291, 483, 7970, 732, 4714, 3180, 3262, 3542, 293, 1451, 3262, 293, 1451, 3262, 293, 2309, 7098, 13, 50914, 50914, 407, 286, 500, 380, 458, 437, 613, 7098, 366, 884, 11, 6095, 13, 51164, 51164, 400, 983, 456, 366, 370, 867, 8000, 295, 1266, 389, 13, 583, 7015, 11, 613, 15148, 366, 1596, 3997, 13, 51364, 51364, 814, 434, 4140, 281, 312, 1143, 11, 406, 534, 1028, 10729, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07300710678100586, "compression_ratio": 1.6613545816733069, "no_speech_prob": 6.048738214303739e-06}, {"id": 1635, "seek": 851200, "start": 8532.0, "end": 8535.0, "text": " They're meant to be used, not really inspected.", "tokens": [50364, 400, 300, 311, 570, 613, 15148, 11, 7015, 11, 436, 1852, 294, 2744, 293, 30867, 13, 50564, 50564, 400, 498, 291, 445, 3164, 337, 1266, 389, 11, 291, 483, 7970, 732, 4714, 3180, 3262, 3542, 293, 1451, 3262, 293, 1451, 3262, 293, 2309, 7098, 13, 50914, 50914, 407, 286, 500, 380, 458, 437, 613, 7098, 366, 884, 11, 6095, 13, 51164, 51164, 400, 983, 456, 366, 370, 867, 8000, 295, 1266, 389, 13, 583, 7015, 11, 613, 15148, 366, 1596, 3997, 13, 51364, 51364, 814, 434, 4140, 281, 312, 1143, 11, 406, 534, 1028, 10729, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07300710678100586, "compression_ratio": 1.6613545816733069, "no_speech_prob": 6.048738214303739e-06}, {"id": 1636, "seek": 853500, "start": 8535.0, "end": 8542.0, "text": " Eventually, I did stumble on someone who tries to change the 10 H backward code for some reason.", "tokens": [50364, 17586, 11, 286, 630, 41302, 322, 1580, 567, 9898, 281, 1319, 264, 1266, 389, 23897, 3089, 337, 512, 1778, 13, 50714, 50714, 400, 1580, 510, 10932, 281, 264, 13199, 28256, 293, 264, 29777, 7509, 28256, 337, 1266, 389, 23897, 13, 50964, 50964, 407, 341, 1936, 5946, 322, 498, 291, 434, 1228, 9953, 51, 284, 339, 322, 257, 13199, 4302, 420, 322, 257, 18407, 11, 597, 613, 366, 819, 5759, 293, 286, 2378, 380, 5343, 341, 13, 51364, 51364, 583, 341, 307, 264, 1266, 389, 23897, 28256, 337, 13199, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1090849394439369, "compression_ratio": 1.6140350877192982, "no_speech_prob": 1.078294008038938e-05}, {"id": 1637, "seek": 853500, "start": 8542.0, "end": 8547.0, "text": " And someone here pointed to the CPU kernel and the CUDA kernel for 10 H backward.", "tokens": [50364, 17586, 11, 286, 630, 41302, 322, 1580, 567, 9898, 281, 1319, 264, 1266, 389, 23897, 3089, 337, 512, 1778, 13, 50714, 50714, 400, 1580, 510, 10932, 281, 264, 13199, 28256, 293, 264, 29777, 7509, 28256, 337, 1266, 389, 23897, 13, 50964, 50964, 407, 341, 1936, 5946, 322, 498, 291, 434, 1228, 9953, 51, 284, 339, 322, 257, 13199, 4302, 420, 322, 257, 18407, 11, 597, 613, 366, 819, 5759, 293, 286, 2378, 380, 5343, 341, 13, 51364, 51364, 583, 341, 307, 264, 1266, 389, 23897, 28256, 337, 13199, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1090849394439369, "compression_ratio": 1.6140350877192982, "no_speech_prob": 1.078294008038938e-05}, {"id": 1638, "seek": 853500, "start": 8547.0, "end": 8555.0, "text": " So this basically depends on if you're using PyTorch on a CPU device or on a GPU, which these are different devices and I haven't covered this.", "tokens": [50364, 17586, 11, 286, 630, 41302, 322, 1580, 567, 9898, 281, 1319, 264, 1266, 389, 23897, 3089, 337, 512, 1778, 13, 50714, 50714, 400, 1580, 510, 10932, 281, 264, 13199, 28256, 293, 264, 29777, 7509, 28256, 337, 1266, 389, 23897, 13, 50964, 50964, 407, 341, 1936, 5946, 322, 498, 291, 434, 1228, 9953, 51, 284, 339, 322, 257, 13199, 4302, 420, 322, 257, 18407, 11, 597, 613, 366, 819, 5759, 293, 286, 2378, 380, 5343, 341, 13, 51364, 51364, 583, 341, 307, 264, 1266, 389, 23897, 28256, 337, 13199, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1090849394439369, "compression_ratio": 1.6140350877192982, "no_speech_prob": 1.078294008038938e-05}, {"id": 1639, "seek": 853500, "start": 8555.0, "end": 8559.0, "text": " But this is the 10 H backward kernel for CPU.", "tokens": [50364, 17586, 11, 286, 630, 41302, 322, 1580, 567, 9898, 281, 1319, 264, 1266, 389, 23897, 3089, 337, 512, 1778, 13, 50714, 50714, 400, 1580, 510, 10932, 281, 264, 13199, 28256, 293, 264, 29777, 7509, 28256, 337, 1266, 389, 23897, 13, 50964, 50964, 407, 341, 1936, 5946, 322, 498, 291, 434, 1228, 9953, 51, 284, 339, 322, 257, 13199, 4302, 420, 322, 257, 18407, 11, 597, 613, 366, 819, 5759, 293, 286, 2378, 380, 5343, 341, 13, 51364, 51364, 583, 341, 307, 264, 1266, 389, 23897, 28256, 337, 13199, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1090849394439369, "compression_ratio": 1.6140350877192982, "no_speech_prob": 1.078294008038938e-05}, {"id": 1640, "seek": 855900, "start": 8559.0, "end": 8567.0, "text": " And the reason it's so large is that, number one, this is like if you're using a complex type, which we haven't even talked about.", "tokens": [50364, 400, 264, 1778, 309, 311, 370, 2416, 307, 300, 11, 1230, 472, 11, 341, 307, 411, 498, 291, 434, 1228, 257, 3997, 2010, 11, 597, 321, 2378, 380, 754, 2825, 466, 13, 50764, 50764, 759, 291, 434, 1228, 257, 2685, 1412, 2010, 295, 363, 15706, 3165, 11, 597, 321, 2378, 380, 2825, 466, 13, 51014, 51014, 400, 550, 498, 291, 434, 406, 11, 550, 341, 307, 264, 28256, 13, 51164, 51164, 400, 2452, 510, 11, 321, 536, 746, 300, 34433, 527, 23897, 1320, 13, 51364, 51364, 407, 436, 362, 3180, 1413, 472, 3175, 363, 3732, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09908822059631348, "compression_ratio": 1.6577777777777778, "no_speech_prob": 1.903337079056655e-06}, {"id": 1641, "seek": 855900, "start": 8567.0, "end": 8572.0, "text": " If you're using a specific data type of B float 16, which we haven't talked about.", "tokens": [50364, 400, 264, 1778, 309, 311, 370, 2416, 307, 300, 11, 1230, 472, 11, 341, 307, 411, 498, 291, 434, 1228, 257, 3997, 2010, 11, 597, 321, 2378, 380, 754, 2825, 466, 13, 50764, 50764, 759, 291, 434, 1228, 257, 2685, 1412, 2010, 295, 363, 15706, 3165, 11, 597, 321, 2378, 380, 2825, 466, 13, 51014, 51014, 400, 550, 498, 291, 434, 406, 11, 550, 341, 307, 264, 28256, 13, 51164, 51164, 400, 2452, 510, 11, 321, 536, 746, 300, 34433, 527, 23897, 1320, 13, 51364, 51364, 407, 436, 362, 3180, 1413, 472, 3175, 363, 3732, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09908822059631348, "compression_ratio": 1.6577777777777778, "no_speech_prob": 1.903337079056655e-06}, {"id": 1642, "seek": 855900, "start": 8572.0, "end": 8575.0, "text": " And then if you're not, then this is the kernel.", "tokens": [50364, 400, 264, 1778, 309, 311, 370, 2416, 307, 300, 11, 1230, 472, 11, 341, 307, 411, 498, 291, 434, 1228, 257, 3997, 2010, 11, 597, 321, 2378, 380, 754, 2825, 466, 13, 50764, 50764, 759, 291, 434, 1228, 257, 2685, 1412, 2010, 295, 363, 15706, 3165, 11, 597, 321, 2378, 380, 2825, 466, 13, 51014, 51014, 400, 550, 498, 291, 434, 406, 11, 550, 341, 307, 264, 28256, 13, 51164, 51164, 400, 2452, 510, 11, 321, 536, 746, 300, 34433, 527, 23897, 1320, 13, 51364, 51364, 407, 436, 362, 3180, 1413, 472, 3175, 363, 3732, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09908822059631348, "compression_ratio": 1.6577777777777778, "no_speech_prob": 1.903337079056655e-06}, {"id": 1643, "seek": 855900, "start": 8575.0, "end": 8579.0, "text": " And deep here, we see something that resembles our backward pass.", "tokens": [50364, 400, 264, 1778, 309, 311, 370, 2416, 307, 300, 11, 1230, 472, 11, 341, 307, 411, 498, 291, 434, 1228, 257, 3997, 2010, 11, 597, 321, 2378, 380, 754, 2825, 466, 13, 50764, 50764, 759, 291, 434, 1228, 257, 2685, 1412, 2010, 295, 363, 15706, 3165, 11, 597, 321, 2378, 380, 2825, 466, 13, 51014, 51014, 400, 550, 498, 291, 434, 406, 11, 550, 341, 307, 264, 28256, 13, 51164, 51164, 400, 2452, 510, 11, 321, 536, 746, 300, 34433, 527, 23897, 1320, 13, 51364, 51364, 407, 436, 362, 3180, 1413, 472, 3175, 363, 3732, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09908822059631348, "compression_ratio": 1.6577777777777778, "no_speech_prob": 1.903337079056655e-06}, {"id": 1644, "seek": 855900, "start": 8579.0, "end": 8583.0, "text": " So they have eight times one minus B square.", "tokens": [50364, 400, 264, 1778, 309, 311, 370, 2416, 307, 300, 11, 1230, 472, 11, 341, 307, 411, 498, 291, 434, 1228, 257, 3997, 2010, 11, 597, 321, 2378, 380, 754, 2825, 466, 13, 50764, 50764, 759, 291, 434, 1228, 257, 2685, 1412, 2010, 295, 363, 15706, 3165, 11, 597, 321, 2378, 380, 2825, 466, 13, 51014, 51014, 400, 550, 498, 291, 434, 406, 11, 550, 341, 307, 264, 28256, 13, 51164, 51164, 400, 2452, 510, 11, 321, 536, 746, 300, 34433, 527, 23897, 1320, 13, 51364, 51364, 407, 436, 362, 3180, 1413, 472, 3175, 363, 3732, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09908822059631348, "compression_ratio": 1.6577777777777778, "no_speech_prob": 1.903337079056655e-06}, {"id": 1645, "seek": 858300, "start": 8583.0, "end": 8589.0, "text": " So this B here must be the output of the 10 H and this is the out.grad.", "tokens": [50364, 407, 341, 363, 510, 1633, 312, 264, 5598, 295, 264, 1266, 389, 293, 341, 307, 264, 484, 13, 7165, 13, 50664, 50664, 407, 510, 321, 1352, 309, 2452, 1854, 9953, 51, 284, 339, 322, 341, 4914, 337, 512, 1778, 1854, 17434, 44663, 28256, 562, 1266, 389, 307, 406, 767, 17434, 999, 13, 51264, 51264, 400, 550, 341, 307, 264, 18407, 28256, 13, 51464, 51464, 492, 434, 406, 3997, 13, 492, 434, 510, 13, 51614, 51614, 400, 510, 321, 352, 365, 472, 1622, 295, 3089, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13180872027793628, "compression_ratio": 1.5958549222797926, "no_speech_prob": 8.990858191282314e-07}, {"id": 1646, "seek": 858300, "start": 8589.0, "end": 8601.0, "text": " So here we found it deep inside PyTorch on this location for some reason inside binary ops kernel when 10 H is not actually binary op.", "tokens": [50364, 407, 341, 363, 510, 1633, 312, 264, 5598, 295, 264, 1266, 389, 293, 341, 307, 264, 484, 13, 7165, 13, 50664, 50664, 407, 510, 321, 1352, 309, 2452, 1854, 9953, 51, 284, 339, 322, 341, 4914, 337, 512, 1778, 1854, 17434, 44663, 28256, 562, 1266, 389, 307, 406, 767, 17434, 999, 13, 51264, 51264, 400, 550, 341, 307, 264, 18407, 28256, 13, 51464, 51464, 492, 434, 406, 3997, 13, 492, 434, 510, 13, 51614, 51614, 400, 510, 321, 352, 365, 472, 1622, 295, 3089, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13180872027793628, "compression_ratio": 1.5958549222797926, "no_speech_prob": 8.990858191282314e-07}, {"id": 1647, "seek": 858300, "start": 8601.0, "end": 8605.0, "text": " And then this is the GPU kernel.", "tokens": [50364, 407, 341, 363, 510, 1633, 312, 264, 5598, 295, 264, 1266, 389, 293, 341, 307, 264, 484, 13, 7165, 13, 50664, 50664, 407, 510, 321, 1352, 309, 2452, 1854, 9953, 51, 284, 339, 322, 341, 4914, 337, 512, 1778, 1854, 17434, 44663, 28256, 562, 1266, 389, 307, 406, 767, 17434, 999, 13, 51264, 51264, 400, 550, 341, 307, 264, 18407, 28256, 13, 51464, 51464, 492, 434, 406, 3997, 13, 492, 434, 510, 13, 51614, 51614, 400, 510, 321, 352, 365, 472, 1622, 295, 3089, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13180872027793628, "compression_ratio": 1.5958549222797926, "no_speech_prob": 8.990858191282314e-07}, {"id": 1648, "seek": 858300, "start": 8605.0, "end": 8608.0, "text": " We're not complex. We're here.", "tokens": [50364, 407, 341, 363, 510, 1633, 312, 264, 5598, 295, 264, 1266, 389, 293, 341, 307, 264, 484, 13, 7165, 13, 50664, 50664, 407, 510, 321, 1352, 309, 2452, 1854, 9953, 51, 284, 339, 322, 341, 4914, 337, 512, 1778, 1854, 17434, 44663, 28256, 562, 1266, 389, 307, 406, 767, 17434, 999, 13, 51264, 51264, 400, 550, 341, 307, 264, 18407, 28256, 13, 51464, 51464, 492, 434, 406, 3997, 13, 492, 434, 510, 13, 51614, 51614, 400, 510, 321, 352, 365, 472, 1622, 295, 3089, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13180872027793628, "compression_ratio": 1.5958549222797926, "no_speech_prob": 8.990858191282314e-07}, {"id": 1649, "seek": 858300, "start": 8608.0, "end": 8610.0, "text": " And here we go with one line of code.", "tokens": [50364, 407, 341, 363, 510, 1633, 312, 264, 5598, 295, 264, 1266, 389, 293, 341, 307, 264, 484, 13, 7165, 13, 50664, 50664, 407, 510, 321, 1352, 309, 2452, 1854, 9953, 51, 284, 339, 322, 341, 4914, 337, 512, 1778, 1854, 17434, 44663, 28256, 562, 1266, 389, 307, 406, 767, 17434, 999, 13, 51264, 51264, 400, 550, 341, 307, 264, 18407, 28256, 13, 51464, 51464, 492, 434, 406, 3997, 13, 492, 434, 510, 13, 51614, 51614, 400, 510, 321, 352, 365, 472, 1622, 295, 3089, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13180872027793628, "compression_ratio": 1.5958549222797926, "no_speech_prob": 8.990858191282314e-07}, {"id": 1650, "seek": 861000, "start": 8610.0, "end": 8618.0, "text": " So we did find it, but basically, unfortunately, these code bases are very large and micro grad is very, very simple.", "tokens": [50364, 407, 321, 630, 915, 309, 11, 457, 1936, 11, 7015, 11, 613, 3089, 17949, 366, 588, 2416, 293, 4532, 2771, 307, 588, 11, 588, 2199, 13, 50764, 50764, 583, 498, 291, 767, 528, 281, 764, 957, 1507, 11, 5006, 264, 3089, 337, 309, 11, 291, 603, 767, 915, 300, 2252, 13, 51064, 51064, 286, 611, 1415, 281, 855, 291, 364, 1365, 510, 689, 9953, 51, 284, 339, 307, 4099, 291, 577, 291, 393, 7280, 257, 777, 2010, 295, 2445, 300, 291, 528, 281, 909, 281, 9953, 51, 284, 339, 382, 257, 28761, 2390, 3461, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07520836049860175, "compression_ratio": 1.6431535269709543, "no_speech_prob": 3.34041214955505e-06}, {"id": 1651, "seek": 861000, "start": 8618.0, "end": 8624.0, "text": " But if you actually want to use real stuff, finding the code for it, you'll actually find that difficult.", "tokens": [50364, 407, 321, 630, 915, 309, 11, 457, 1936, 11, 7015, 11, 613, 3089, 17949, 366, 588, 2416, 293, 4532, 2771, 307, 588, 11, 588, 2199, 13, 50764, 50764, 583, 498, 291, 767, 528, 281, 764, 957, 1507, 11, 5006, 264, 3089, 337, 309, 11, 291, 603, 767, 915, 300, 2252, 13, 51064, 51064, 286, 611, 1415, 281, 855, 291, 364, 1365, 510, 689, 9953, 51, 284, 339, 307, 4099, 291, 577, 291, 393, 7280, 257, 777, 2010, 295, 2445, 300, 291, 528, 281, 909, 281, 9953, 51, 284, 339, 382, 257, 28761, 2390, 3461, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07520836049860175, "compression_ratio": 1.6431535269709543, "no_speech_prob": 3.34041214955505e-06}, {"id": 1652, "seek": 861000, "start": 8624.0, "end": 8633.0, "text": " I also wanted to show you an example here where PyTorch is showing you how you can register a new type of function that you want to add to PyTorch as a Lego building block.", "tokens": [50364, 407, 321, 630, 915, 309, 11, 457, 1936, 11, 7015, 11, 613, 3089, 17949, 366, 588, 2416, 293, 4532, 2771, 307, 588, 11, 588, 2199, 13, 50764, 50764, 583, 498, 291, 767, 528, 281, 764, 957, 1507, 11, 5006, 264, 3089, 337, 309, 11, 291, 603, 767, 915, 300, 2252, 13, 51064, 51064, 286, 611, 1415, 281, 855, 291, 364, 1365, 510, 689, 9953, 51, 284, 339, 307, 4099, 291, 577, 291, 393, 7280, 257, 777, 2010, 295, 2445, 300, 291, 528, 281, 909, 281, 9953, 51, 284, 339, 382, 257, 28761, 2390, 3461, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07520836049860175, "compression_ratio": 1.6431535269709543, "no_speech_prob": 3.34041214955505e-06}, {"id": 1653, "seek": 863300, "start": 8633.0, "end": 8640.0, "text": " So here, if you want to, for example, add a Legendre polynomial three, here's how you could do it.", "tokens": [50364, 407, 510, 11, 498, 291, 528, 281, 11, 337, 1365, 11, 909, 257, 21480, 265, 26110, 1045, 11, 510, 311, 577, 291, 727, 360, 309, 13, 50714, 50714, 509, 486, 7280, 309, 382, 257, 1508, 300, 1422, 11665, 279, 3930, 339, 13, 346, 13, 7165, 300, 2445, 13, 51014, 51014, 400, 550, 291, 362, 281, 980, 9953, 51, 284, 339, 577, 281, 2128, 428, 777, 2445, 293, 577, 281, 23897, 807, 309, 13, 51314, 51314, 407, 382, 938, 382, 291, 393, 360, 264, 2128, 1320, 295, 341, 707, 2445, 2522, 300, 291, 528, 281, 909, 11, 293, 382, 938, 382, 291, 458, 264, 2654, 13760, 11, 264, 2654, 2771, 2448, 11, 597, 366, 12270, 294, 264, 23897, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13806469714055297, "compression_ratio": 1.7463235294117647, "no_speech_prob": 6.747830411768518e-06}, {"id": 1654, "seek": 863300, "start": 8640.0, "end": 8646.0, "text": " You will register it as a class that subclasses torch.out.grad that function.", "tokens": [50364, 407, 510, 11, 498, 291, 528, 281, 11, 337, 1365, 11, 909, 257, 21480, 265, 26110, 1045, 11, 510, 311, 577, 291, 727, 360, 309, 13, 50714, 50714, 509, 486, 7280, 309, 382, 257, 1508, 300, 1422, 11665, 279, 3930, 339, 13, 346, 13, 7165, 300, 2445, 13, 51014, 51014, 400, 550, 291, 362, 281, 980, 9953, 51, 284, 339, 577, 281, 2128, 428, 777, 2445, 293, 577, 281, 23897, 807, 309, 13, 51314, 51314, 407, 382, 938, 382, 291, 393, 360, 264, 2128, 1320, 295, 341, 707, 2445, 2522, 300, 291, 528, 281, 909, 11, 293, 382, 938, 382, 291, 458, 264, 2654, 13760, 11, 264, 2654, 2771, 2448, 11, 597, 366, 12270, 294, 264, 23897, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13806469714055297, "compression_ratio": 1.7463235294117647, "no_speech_prob": 6.747830411768518e-06}, {"id": 1655, "seek": 863300, "start": 8646.0, "end": 8652.0, "text": " And then you have to tell PyTorch how to forward your new function and how to backward through it.", "tokens": [50364, 407, 510, 11, 498, 291, 528, 281, 11, 337, 1365, 11, 909, 257, 21480, 265, 26110, 1045, 11, 510, 311, 577, 291, 727, 360, 309, 13, 50714, 50714, 509, 486, 7280, 309, 382, 257, 1508, 300, 1422, 11665, 279, 3930, 339, 13, 346, 13, 7165, 300, 2445, 13, 51014, 51014, 400, 550, 291, 362, 281, 980, 9953, 51, 284, 339, 577, 281, 2128, 428, 777, 2445, 293, 577, 281, 23897, 807, 309, 13, 51314, 51314, 407, 382, 938, 382, 291, 393, 360, 264, 2128, 1320, 295, 341, 707, 2445, 2522, 300, 291, 528, 281, 909, 11, 293, 382, 938, 382, 291, 458, 264, 2654, 13760, 11, 264, 2654, 2771, 2448, 11, 597, 366, 12270, 294, 264, 23897, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13806469714055297, "compression_ratio": 1.7463235294117647, "no_speech_prob": 6.747830411768518e-06}, {"id": 1656, "seek": 863300, "start": 8652.0, "end": 8661.0, "text": " So as long as you can do the forward pass of this little function piece that you want to add, and as long as you know the local derivative, the local gradients, which are implemented in the backward,", "tokens": [50364, 407, 510, 11, 498, 291, 528, 281, 11, 337, 1365, 11, 909, 257, 21480, 265, 26110, 1045, 11, 510, 311, 577, 291, 727, 360, 309, 13, 50714, 50714, 509, 486, 7280, 309, 382, 257, 1508, 300, 1422, 11665, 279, 3930, 339, 13, 346, 13, 7165, 300, 2445, 13, 51014, 51014, 400, 550, 291, 362, 281, 980, 9953, 51, 284, 339, 577, 281, 2128, 428, 777, 2445, 293, 577, 281, 23897, 807, 309, 13, 51314, 51314, 407, 382, 938, 382, 291, 393, 360, 264, 2128, 1320, 295, 341, 707, 2445, 2522, 300, 291, 528, 281, 909, 11, 293, 382, 938, 382, 291, 458, 264, 2654, 13760, 11, 264, 2654, 2771, 2448, 11, 597, 366, 12270, 294, 264, 23897, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.13806469714055297, "compression_ratio": 1.7463235294117647, "no_speech_prob": 6.747830411768518e-06}, {"id": 1657, "seek": 866100, "start": 8661.0, "end": 8664.0, "text": " PyTorch will be able to back propagate through your function.", "tokens": [50364, 9953, 51, 284, 339, 486, 312, 1075, 281, 646, 48256, 807, 428, 2445, 13, 50514, 50514, 400, 550, 291, 393, 764, 341, 382, 257, 28761, 3461, 294, 257, 4833, 28761, 14114, 295, 439, 264, 819, 28761, 8474, 300, 9953, 51, 284, 339, 1217, 575, 13, 50864, 50864, 400, 370, 300, 311, 264, 787, 551, 291, 362, 281, 980, 9953, 51, 284, 339, 293, 1203, 576, 445, 589, 13, 51014, 51014, 400, 291, 393, 7280, 777, 3467, 295, 6828, 294, 341, 636, 11, 3480, 341, 1365, 13, 51264, 51264, 400, 300, 307, 1203, 300, 286, 1415, 281, 2060, 294, 341, 7991, 13, 51364, 51364, 407, 286, 1454, 291, 4626, 2390, 484, 4532, 2771, 365, 385, 13, 51514, 51514, 286, 1454, 291, 915, 309, 1880, 11, 46401, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06887964835533729, "compression_ratio": 1.751700680272109, "no_speech_prob": 1.733004864945542e-06}, {"id": 1658, "seek": 866100, "start": 8664.0, "end": 8671.0, "text": " And then you can use this as a Lego block in a larger Lego castle of all the different Lego blocks that PyTorch already has.", "tokens": [50364, 9953, 51, 284, 339, 486, 312, 1075, 281, 646, 48256, 807, 428, 2445, 13, 50514, 50514, 400, 550, 291, 393, 764, 341, 382, 257, 28761, 3461, 294, 257, 4833, 28761, 14114, 295, 439, 264, 819, 28761, 8474, 300, 9953, 51, 284, 339, 1217, 575, 13, 50864, 50864, 400, 370, 300, 311, 264, 787, 551, 291, 362, 281, 980, 9953, 51, 284, 339, 293, 1203, 576, 445, 589, 13, 51014, 51014, 400, 291, 393, 7280, 777, 3467, 295, 6828, 294, 341, 636, 11, 3480, 341, 1365, 13, 51264, 51264, 400, 300, 307, 1203, 300, 286, 1415, 281, 2060, 294, 341, 7991, 13, 51364, 51364, 407, 286, 1454, 291, 4626, 2390, 484, 4532, 2771, 365, 385, 13, 51514, 51514, 286, 1454, 291, 915, 309, 1880, 11, 46401, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06887964835533729, "compression_ratio": 1.751700680272109, "no_speech_prob": 1.733004864945542e-06}, {"id": 1659, "seek": 866100, "start": 8671.0, "end": 8674.0, "text": " And so that's the only thing you have to tell PyTorch and everything would just work.", "tokens": [50364, 9953, 51, 284, 339, 486, 312, 1075, 281, 646, 48256, 807, 428, 2445, 13, 50514, 50514, 400, 550, 291, 393, 764, 341, 382, 257, 28761, 3461, 294, 257, 4833, 28761, 14114, 295, 439, 264, 819, 28761, 8474, 300, 9953, 51, 284, 339, 1217, 575, 13, 50864, 50864, 400, 370, 300, 311, 264, 787, 551, 291, 362, 281, 980, 9953, 51, 284, 339, 293, 1203, 576, 445, 589, 13, 51014, 51014, 400, 291, 393, 7280, 777, 3467, 295, 6828, 294, 341, 636, 11, 3480, 341, 1365, 13, 51264, 51264, 400, 300, 307, 1203, 300, 286, 1415, 281, 2060, 294, 341, 7991, 13, 51364, 51364, 407, 286, 1454, 291, 4626, 2390, 484, 4532, 2771, 365, 385, 13, 51514, 51514, 286, 1454, 291, 915, 309, 1880, 11, 46401, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06887964835533729, "compression_ratio": 1.751700680272109, "no_speech_prob": 1.733004864945542e-06}, {"id": 1660, "seek": 866100, "start": 8674.0, "end": 8679.0, "text": " And you can register new types of functions in this way, following this example.", "tokens": [50364, 9953, 51, 284, 339, 486, 312, 1075, 281, 646, 48256, 807, 428, 2445, 13, 50514, 50514, 400, 550, 291, 393, 764, 341, 382, 257, 28761, 3461, 294, 257, 4833, 28761, 14114, 295, 439, 264, 819, 28761, 8474, 300, 9953, 51, 284, 339, 1217, 575, 13, 50864, 50864, 400, 370, 300, 311, 264, 787, 551, 291, 362, 281, 980, 9953, 51, 284, 339, 293, 1203, 576, 445, 589, 13, 51014, 51014, 400, 291, 393, 7280, 777, 3467, 295, 6828, 294, 341, 636, 11, 3480, 341, 1365, 13, 51264, 51264, 400, 300, 307, 1203, 300, 286, 1415, 281, 2060, 294, 341, 7991, 13, 51364, 51364, 407, 286, 1454, 291, 4626, 2390, 484, 4532, 2771, 365, 385, 13, 51514, 51514, 286, 1454, 291, 915, 309, 1880, 11, 46401, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06887964835533729, "compression_ratio": 1.751700680272109, "no_speech_prob": 1.733004864945542e-06}, {"id": 1661, "seek": 866100, "start": 8679.0, "end": 8681.0, "text": " And that is everything that I wanted to cover in this lecture.", "tokens": [50364, 9953, 51, 284, 339, 486, 312, 1075, 281, 646, 48256, 807, 428, 2445, 13, 50514, 50514, 400, 550, 291, 393, 764, 341, 382, 257, 28761, 3461, 294, 257, 4833, 28761, 14114, 295, 439, 264, 819, 28761, 8474, 300, 9953, 51, 284, 339, 1217, 575, 13, 50864, 50864, 400, 370, 300, 311, 264, 787, 551, 291, 362, 281, 980, 9953, 51, 284, 339, 293, 1203, 576, 445, 589, 13, 51014, 51014, 400, 291, 393, 7280, 777, 3467, 295, 6828, 294, 341, 636, 11, 3480, 341, 1365, 13, 51264, 51264, 400, 300, 307, 1203, 300, 286, 1415, 281, 2060, 294, 341, 7991, 13, 51364, 51364, 407, 286, 1454, 291, 4626, 2390, 484, 4532, 2771, 365, 385, 13, 51514, 51514, 286, 1454, 291, 915, 309, 1880, 11, 46401, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06887964835533729, "compression_ratio": 1.751700680272109, "no_speech_prob": 1.733004864945542e-06}, {"id": 1662, "seek": 866100, "start": 8681.0, "end": 8684.0, "text": " So I hope you enjoyed building out micro grad with me.", "tokens": [50364, 9953, 51, 284, 339, 486, 312, 1075, 281, 646, 48256, 807, 428, 2445, 13, 50514, 50514, 400, 550, 291, 393, 764, 341, 382, 257, 28761, 3461, 294, 257, 4833, 28761, 14114, 295, 439, 264, 819, 28761, 8474, 300, 9953, 51, 284, 339, 1217, 575, 13, 50864, 50864, 400, 370, 300, 311, 264, 787, 551, 291, 362, 281, 980, 9953, 51, 284, 339, 293, 1203, 576, 445, 589, 13, 51014, 51014, 400, 291, 393, 7280, 777, 3467, 295, 6828, 294, 341, 636, 11, 3480, 341, 1365, 13, 51264, 51264, 400, 300, 307, 1203, 300, 286, 1415, 281, 2060, 294, 341, 7991, 13, 51364, 51364, 407, 286, 1454, 291, 4626, 2390, 484, 4532, 2771, 365, 385, 13, 51514, 51514, 286, 1454, 291, 915, 309, 1880, 11, 46401, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06887964835533729, "compression_ratio": 1.751700680272109, "no_speech_prob": 1.733004864945542e-06}, {"id": 1663, "seek": 866100, "start": 8684.0, "end": 8686.0, "text": " I hope you find it interesting, insightful.", "tokens": [50364, 9953, 51, 284, 339, 486, 312, 1075, 281, 646, 48256, 807, 428, 2445, 13, 50514, 50514, 400, 550, 291, 393, 764, 341, 382, 257, 28761, 3461, 294, 257, 4833, 28761, 14114, 295, 439, 264, 819, 28761, 8474, 300, 9953, 51, 284, 339, 1217, 575, 13, 50864, 50864, 400, 370, 300, 311, 264, 787, 551, 291, 362, 281, 980, 9953, 51, 284, 339, 293, 1203, 576, 445, 589, 13, 51014, 51014, 400, 291, 393, 7280, 777, 3467, 295, 6828, 294, 341, 636, 11, 3480, 341, 1365, 13, 51264, 51264, 400, 300, 307, 1203, 300, 286, 1415, 281, 2060, 294, 341, 7991, 13, 51364, 51364, 407, 286, 1454, 291, 4626, 2390, 484, 4532, 2771, 365, 385, 13, 51514, 51514, 286, 1454, 291, 915, 309, 1880, 11, 46401, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06887964835533729, "compression_ratio": 1.751700680272109, "no_speech_prob": 1.733004864945542e-06}, {"id": 1664, "seek": 868600, "start": 8686.0, "end": 8693.0, "text": " And yeah, I will post a lot of the links that are related to this video in the video description below.", "tokens": [50364, 400, 1338, 11, 286, 486, 2183, 257, 688, 295, 264, 6123, 300, 366, 4077, 281, 341, 960, 294, 264, 960, 3855, 2507, 13, 50714, 50714, 286, 486, 611, 1391, 2183, 257, 2113, 281, 257, 5017, 17542, 420, 5017, 1594, 689, 291, 393, 1029, 1651, 4077, 281, 341, 960, 13, 51064, 51064, 400, 550, 286, 393, 1867, 420, 1580, 1646, 393, 1867, 428, 1651, 13, 51214, 51214, 400, 286, 815, 611, 360, 257, 1524, 493, 960, 300, 6338, 512, 295, 264, 881, 2689, 1651, 13, 51464, 51464, 583, 337, 586, 11, 300, 311, 309, 13, 286, 1454, 291, 4626, 309, 13, 51564, 51564, 759, 291, 630, 11, 550, 1767, 411, 293, 3022, 370, 300, 3088, 3255, 281, 4111, 341, 960, 281, 544, 561, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04219434768196166, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.00012333301128819585}, {"id": 1665, "seek": 868600, "start": 8693.0, "end": 8700.0, "text": " I will also probably post a link to a discussion forum or discussion group where you can ask questions related to this video.", "tokens": [50364, 400, 1338, 11, 286, 486, 2183, 257, 688, 295, 264, 6123, 300, 366, 4077, 281, 341, 960, 294, 264, 960, 3855, 2507, 13, 50714, 50714, 286, 486, 611, 1391, 2183, 257, 2113, 281, 257, 5017, 17542, 420, 5017, 1594, 689, 291, 393, 1029, 1651, 4077, 281, 341, 960, 13, 51064, 51064, 400, 550, 286, 393, 1867, 420, 1580, 1646, 393, 1867, 428, 1651, 13, 51214, 51214, 400, 286, 815, 611, 360, 257, 1524, 493, 960, 300, 6338, 512, 295, 264, 881, 2689, 1651, 13, 51464, 51464, 583, 337, 586, 11, 300, 311, 309, 13, 286, 1454, 291, 4626, 309, 13, 51564, 51564, 759, 291, 630, 11, 550, 1767, 411, 293, 3022, 370, 300, 3088, 3255, 281, 4111, 341, 960, 281, 544, 561, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04219434768196166, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.00012333301128819585}, {"id": 1666, "seek": 868600, "start": 8700.0, "end": 8703.0, "text": " And then I can answer or someone else can answer your questions.", "tokens": [50364, 400, 1338, 11, 286, 486, 2183, 257, 688, 295, 264, 6123, 300, 366, 4077, 281, 341, 960, 294, 264, 960, 3855, 2507, 13, 50714, 50714, 286, 486, 611, 1391, 2183, 257, 2113, 281, 257, 5017, 17542, 420, 5017, 1594, 689, 291, 393, 1029, 1651, 4077, 281, 341, 960, 13, 51064, 51064, 400, 550, 286, 393, 1867, 420, 1580, 1646, 393, 1867, 428, 1651, 13, 51214, 51214, 400, 286, 815, 611, 360, 257, 1524, 493, 960, 300, 6338, 512, 295, 264, 881, 2689, 1651, 13, 51464, 51464, 583, 337, 586, 11, 300, 311, 309, 13, 286, 1454, 291, 4626, 309, 13, 51564, 51564, 759, 291, 630, 11, 550, 1767, 411, 293, 3022, 370, 300, 3088, 3255, 281, 4111, 341, 960, 281, 544, 561, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04219434768196166, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.00012333301128819585}, {"id": 1667, "seek": 868600, "start": 8703.0, "end": 8708.0, "text": " And I may also do a follow up video that answers some of the most common questions.", "tokens": [50364, 400, 1338, 11, 286, 486, 2183, 257, 688, 295, 264, 6123, 300, 366, 4077, 281, 341, 960, 294, 264, 960, 3855, 2507, 13, 50714, 50714, 286, 486, 611, 1391, 2183, 257, 2113, 281, 257, 5017, 17542, 420, 5017, 1594, 689, 291, 393, 1029, 1651, 4077, 281, 341, 960, 13, 51064, 51064, 400, 550, 286, 393, 1867, 420, 1580, 1646, 393, 1867, 428, 1651, 13, 51214, 51214, 400, 286, 815, 611, 360, 257, 1524, 493, 960, 300, 6338, 512, 295, 264, 881, 2689, 1651, 13, 51464, 51464, 583, 337, 586, 11, 300, 311, 309, 13, 286, 1454, 291, 4626, 309, 13, 51564, 51564, 759, 291, 630, 11, 550, 1767, 411, 293, 3022, 370, 300, 3088, 3255, 281, 4111, 341, 960, 281, 544, 561, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04219434768196166, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.00012333301128819585}, {"id": 1668, "seek": 868600, "start": 8708.0, "end": 8710.0, "text": " But for now, that's it. I hope you enjoyed it.", "tokens": [50364, 400, 1338, 11, 286, 486, 2183, 257, 688, 295, 264, 6123, 300, 366, 4077, 281, 341, 960, 294, 264, 960, 3855, 2507, 13, 50714, 50714, 286, 486, 611, 1391, 2183, 257, 2113, 281, 257, 5017, 17542, 420, 5017, 1594, 689, 291, 393, 1029, 1651, 4077, 281, 341, 960, 13, 51064, 51064, 400, 550, 286, 393, 1867, 420, 1580, 1646, 393, 1867, 428, 1651, 13, 51214, 51214, 400, 286, 815, 611, 360, 257, 1524, 493, 960, 300, 6338, 512, 295, 264, 881, 2689, 1651, 13, 51464, 51464, 583, 337, 586, 11, 300, 311, 309, 13, 286, 1454, 291, 4626, 309, 13, 51564, 51564, 759, 291, 630, 11, 550, 1767, 411, 293, 3022, 370, 300, 3088, 3255, 281, 4111, 341, 960, 281, 544, 561, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04219434768196166, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.00012333301128819585}, {"id": 1669, "seek": 868600, "start": 8710.0, "end": 8715.0, "text": " If you did, then please like and subscribe so that YouTube knows to feature this video to more people.", "tokens": [50364, 400, 1338, 11, 286, 486, 2183, 257, 688, 295, 264, 6123, 300, 366, 4077, 281, 341, 960, 294, 264, 960, 3855, 2507, 13, 50714, 50714, 286, 486, 611, 1391, 2183, 257, 2113, 281, 257, 5017, 17542, 420, 5017, 1594, 689, 291, 393, 1029, 1651, 4077, 281, 341, 960, 13, 51064, 51064, 400, 550, 286, 393, 1867, 420, 1580, 1646, 393, 1867, 428, 1651, 13, 51214, 51214, 400, 286, 815, 611, 360, 257, 1524, 493, 960, 300, 6338, 512, 295, 264, 881, 2689, 1651, 13, 51464, 51464, 583, 337, 586, 11, 300, 311, 309, 13, 286, 1454, 291, 4626, 309, 13, 51564, 51564, 759, 291, 630, 11, 550, 1767, 411, 293, 3022, 370, 300, 3088, 3255, 281, 4111, 341, 960, 281, 544, 561, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04219434768196166, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.00012333301128819585}, {"id": 1670, "seek": 871500, "start": 8715.0, "end": 8722.0, "text": " And that's it for now. I'll see you later.", "tokens": [50364, 400, 300, 311, 309, 337, 586, 13, 286, 603, 536, 291, 1780, 13, 50714, 50714, 823, 11, 510, 311, 264, 1154, 13, 50814, 50814, 492, 458, 413, 43, 538, 13, 3802, 11, 437, 307, 264, 1154, 30, 51214, 51214, 400, 300, 311, 1203, 286, 1415, 281, 2060, 294, 341, 7991, 13, 51314, 51314, 407, 286, 1454, 291, 4626, 505, 2390, 484, 4532, 2771, 4532, 2771, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12052839824131557, "compression_ratio": 1.41875, "no_speech_prob": 2.8405496777850203e-05}, {"id": 1671, "seek": 871500, "start": 8722.0, "end": 8724.0, "text": " Now, here's the problem.", "tokens": [50364, 400, 300, 311, 309, 337, 586, 13, 286, 603, 536, 291, 1780, 13, 50714, 50714, 823, 11, 510, 311, 264, 1154, 13, 50814, 50814, 492, 458, 413, 43, 538, 13, 3802, 11, 437, 307, 264, 1154, 30, 51214, 51214, 400, 300, 311, 1203, 286, 1415, 281, 2060, 294, 341, 7991, 13, 51314, 51314, 407, 286, 1454, 291, 4626, 505, 2390, 484, 4532, 2771, 4532, 2771, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12052839824131557, "compression_ratio": 1.41875, "no_speech_prob": 2.8405496777850203e-05}, {"id": 1672, "seek": 871500, "start": 8724.0, "end": 8732.0, "text": " We know DL by. Wait, what is the problem?", "tokens": [50364, 400, 300, 311, 309, 337, 586, 13, 286, 603, 536, 291, 1780, 13, 50714, 50714, 823, 11, 510, 311, 264, 1154, 13, 50814, 50814, 492, 458, 413, 43, 538, 13, 3802, 11, 437, 307, 264, 1154, 30, 51214, 51214, 400, 300, 311, 1203, 286, 1415, 281, 2060, 294, 341, 7991, 13, 51314, 51314, 407, 286, 1454, 291, 4626, 505, 2390, 484, 4532, 2771, 4532, 2771, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12052839824131557, "compression_ratio": 1.41875, "no_speech_prob": 2.8405496777850203e-05}, {"id": 1673, "seek": 871500, "start": 8732.0, "end": 8734.0, "text": " And that's everything I wanted to cover in this lecture.", "tokens": [50364, 400, 300, 311, 309, 337, 586, 13, 286, 603, 536, 291, 1780, 13, 50714, 50714, 823, 11, 510, 311, 264, 1154, 13, 50814, 50814, 492, 458, 413, 43, 538, 13, 3802, 11, 437, 307, 264, 1154, 30, 51214, 51214, 400, 300, 311, 1203, 286, 1415, 281, 2060, 294, 341, 7991, 13, 51314, 51314, 407, 286, 1454, 291, 4626, 505, 2390, 484, 4532, 2771, 4532, 2771, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12052839824131557, "compression_ratio": 1.41875, "no_speech_prob": 2.8405496777850203e-05}, {"id": 1674, "seek": 871500, "start": 8734.0, "end": 8742.0, "text": " So I hope you enjoyed us building out micro grad micro grad.", "tokens": [50364, 400, 300, 311, 309, 337, 586, 13, 286, 603, 536, 291, 1780, 13, 50714, 50714, 823, 11, 510, 311, 264, 1154, 13, 50814, 50814, 492, 458, 413, 43, 538, 13, 3802, 11, 437, 307, 264, 1154, 30, 51214, 51214, 400, 300, 311, 1203, 286, 1415, 281, 2060, 294, 341, 7991, 13, 51314, 51314, 407, 286, 1454, 291, 4626, 505, 2390, 484, 4532, 2771, 4532, 2771, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12052839824131557, "compression_ratio": 1.41875, "no_speech_prob": 2.8405496777850203e-05}, {"id": 1675, "seek": 874200, "start": 8742.0, "end": 8748.0, "text": " OK, now let's do the exact same thing from multiple because we can't do something like eight times to.", "tokens": [50364, 2264, 11, 586, 718, 311, 360, 264, 1900, 912, 551, 490, 3866, 570, 321, 393, 380, 360, 746, 411, 3180, 1413, 281, 13, 50664, 50664, 21726, 11, 286, 458, 437, 2011, 13, 50864], "temperature": 0.0, "avg_logprob": -0.32403842381068637, "compression_ratio": 1.1818181818181819, "no_speech_prob": 0.0006709565059281886}, {"id": 1676, "seek": 874800, "start": 8748.0, "end": 8776.0, "text": " Oops, I know what happened.", "tokens": [50364, 21726, 11, 286, 458, 437, 2011, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2593649625778198, "compression_ratio": 0.7714285714285715, "no_speech_prob": 0.000312059186398983}], "language": "en", "video_id": "VMj-3S1tku0", "entity": "Andrew Kaparthy"}}