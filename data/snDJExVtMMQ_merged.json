{"video_id": "snDJExVtMMQ", "title": "4.12 Neural network implementation in Python | General implementation of forward propagation-ML Ng", "description": "Second Course:\nAdvanced Learning Algorithms.\n\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 472, "views": 125, "publish_date": "11/04/2022", "timestamp": 1661385600, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " In the last video, you saw how to implement forward prop in Python, but by hard coding lines of code for every single neuron. Let's now take a look at the more general implementation of forward prop in Python. Similar to the previous video, my goal in this video is to show you the code so that when you see it again in the practice lab and the optional labs, you know how to interpret it. So as we walk through this example, don't worry about taking notes on every single line of code. If you can read through the code and understand it, that's definitely enough. So what you can do is write a function to implement a dense layer that is a single layer of a neural network. So I'm going to define the dense function, which takes as input the activation from the previous layer, as well as the parameters w and b for the neurons in a given layer. Using the example from the previous video, if layer one has three neurons, and if w1 and w2 and w3 are these, then what we'll do is stack all of these weight vectors into a matrix. This is going to be a two by three matrix, where the first column is the parameter w11, the second column is the parameter w12, and the third column is the parameter w13. And then in a similar way, if you have parameters b, b11 equals negative 1, b12 equals 1, and so on, then we're going to stack these three numbers into one D array b as follows, negative 1, 1, 2. So what the dense function will do is take as input the activation from the previous layer, and a here could be a0, which is equal to x, or the activation from a later layer, as well as the w parameters stacked in columns, like shown on the right, as well as the b parameters also stacked into one D array, like shown to the left over there. And what this function will do is input a, the activation from the previous layer, and will output the activations from the columns layer. So let's step through the code for doing this. Here's the code. First, units equals w dot shape one. So w here is a two by three matrix, and so the number of columns is three. That's equal to the number of units in this layer. So here, units would be equal to three. And looking at the shape of w, it's just a way of pulling out the number of hidden units, or the number of units in this layer. Next, we set a to be an array of zeros with as many elements as there are units. So in this example, we need to output three activation values. So this just initializes a to be zero, zero, zero, an array of three zeros. Next, we go through a for loop to compute the first, second, and third elements of a. So for j and range units, so j goes from zero to units minus one, so it goes from zero, one, two, indexing from zero and Python as usual. This command, w equals capital W colon comma j, this is how you pull out the j column of a matrix in Python. So the first time through this loop, this will pull out the first column of w, and so will pull out w one one. The second time through this loop, when you're computing the activation of the second unit, the pull out the second column corresponding to w one two, and so on for the third time through this loop. And then you compute z using the usual formula as a thought product between that parameter w and the activation that you had received plus b j. And then you compute the activation a j equals g sigmoid function applied to z. So three times through this loop and you computed the values for all three values of this vector of activations a, and then finally you return a. So what the dense function does is it inputs the activations from the previous layer, and given the parameters for the current layer, it returns the activations for the next layer. So given the dense function, here's how you can string together a few dense layers sequentially in order to implement forward prop in the neural network. Given the input features x, you can then compute the activations a one to be a one equals dens of x w one b one, where here w one b one are the parameters, sometimes also called the weights of the first hidden layer. Then you can compute a two as dens of now a one, which you just computed above, and w two b two, which are the parameters or weights of this second hidden layer, and then compute a three and a four. And if this is a neural network with four layers, then the final output f of x is just equal to a four. And so you return f of x. Notice that here I'm using a capital W, because one of the notational conventions from linear algebra is to use uppercase or a capital alphabets when it's referring to a matrix, and lowercase to refer to vectors and scalars. So because it's a matrix, this is capital W. So that's it. You now know how to implement forward prop yourself from scratch. And you get to see all this code and run it and practice it yourself in the practice lab coming after this as well. I think that even when you're using powerful libraries like TensorFlow, it's helpful to know how it works under the hood. Because in case something goes wrong, in case something runs really slowly, or you have a strange result, or it looks like there's a bug, your ability to understand what's actually going on will make you much more effective when debugging your code. When I run machine learning algorithms, a lot of the time, frankly, it doesn't work. Certainly not the first time. And so I find that my ability to debug my code, be it TensorFlow code or something else, is really important to being an effective machine learning engineer. So even when you're using TensorFlow or some other framework, I hope that you find this deeper understanding useful for your own applications and for debugging your own machine learning algorithms as well. So that's it. That's the last required video of this week with code in it. In the next video, I'd like to dive into what I think is a fun and fascinating topic, which is what is the relationship between neural networks and AI or AGI, artificial general intelligence? This is a controversial topic, but because it's been so widely discussed, I want to share with you some thoughts on this. So when you are asked, are neural networks at all on the path to human level intelligence, you have a framework for thinking about that question. Let's go take a look at that fun topic, I think, in the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.96, "text": " In the last video, you saw how to implement forward prop in Python, but by hard coding", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 1866, 577, 281, 4445, 2128, 2365, 294, 15329, 11, 457, 538, 1152, 17720, 50762, 50762, 3876, 295, 3089, 337, 633, 2167, 34090, 13, 50908, 50908, 961, 311, 586, 747, 257, 574, 412, 264, 544, 2674, 11420, 295, 2128, 2365, 294, 15329, 13, 51158, 51158, 10905, 281, 264, 3894, 960, 11, 452, 3387, 294, 341, 960, 307, 281, 855, 291, 264, 3089, 370, 300, 51380, 51380, 562, 291, 536, 309, 797, 294, 264, 3124, 2715, 293, 264, 17312, 20339, 11, 291, 458, 577, 281, 7302, 51652, 51652, 309, 13, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.1512383830790617, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.03019646741449833}, {"id": 1, "seek": 0, "start": 7.96, "end": 10.88, "text": " lines of code for every single neuron.", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 1866, 577, 281, 4445, 2128, 2365, 294, 15329, 11, 457, 538, 1152, 17720, 50762, 50762, 3876, 295, 3089, 337, 633, 2167, 34090, 13, 50908, 50908, 961, 311, 586, 747, 257, 574, 412, 264, 544, 2674, 11420, 295, 2128, 2365, 294, 15329, 13, 51158, 51158, 10905, 281, 264, 3894, 960, 11, 452, 3387, 294, 341, 960, 307, 281, 855, 291, 264, 3089, 370, 300, 51380, 51380, 562, 291, 536, 309, 797, 294, 264, 3124, 2715, 293, 264, 17312, 20339, 11, 291, 458, 577, 281, 7302, 51652, 51652, 309, 13, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.1512383830790617, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.03019646741449833}, {"id": 2, "seek": 0, "start": 10.88, "end": 15.88, "text": " Let's now take a look at the more general implementation of forward prop in Python.", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 1866, 577, 281, 4445, 2128, 2365, 294, 15329, 11, 457, 538, 1152, 17720, 50762, 50762, 3876, 295, 3089, 337, 633, 2167, 34090, 13, 50908, 50908, 961, 311, 586, 747, 257, 574, 412, 264, 544, 2674, 11420, 295, 2128, 2365, 294, 15329, 13, 51158, 51158, 10905, 281, 264, 3894, 960, 11, 452, 3387, 294, 341, 960, 307, 281, 855, 291, 264, 3089, 370, 300, 51380, 51380, 562, 291, 536, 309, 797, 294, 264, 3124, 2715, 293, 264, 17312, 20339, 11, 291, 458, 577, 281, 7302, 51652, 51652, 309, 13, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.1512383830790617, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.03019646741449833}, {"id": 3, "seek": 0, "start": 15.88, "end": 20.32, "text": " Similar to the previous video, my goal in this video is to show you the code so that", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 1866, 577, 281, 4445, 2128, 2365, 294, 15329, 11, 457, 538, 1152, 17720, 50762, 50762, 3876, 295, 3089, 337, 633, 2167, 34090, 13, 50908, 50908, 961, 311, 586, 747, 257, 574, 412, 264, 544, 2674, 11420, 295, 2128, 2365, 294, 15329, 13, 51158, 51158, 10905, 281, 264, 3894, 960, 11, 452, 3387, 294, 341, 960, 307, 281, 855, 291, 264, 3089, 370, 300, 51380, 51380, 562, 291, 536, 309, 797, 294, 264, 3124, 2715, 293, 264, 17312, 20339, 11, 291, 458, 577, 281, 7302, 51652, 51652, 309, 13, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.1512383830790617, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.03019646741449833}, {"id": 4, "seek": 0, "start": 20.32, "end": 25.76, "text": " when you see it again in the practice lab and the optional labs, you know how to interpret", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 1866, 577, 281, 4445, 2128, 2365, 294, 15329, 11, 457, 538, 1152, 17720, 50762, 50762, 3876, 295, 3089, 337, 633, 2167, 34090, 13, 50908, 50908, 961, 311, 586, 747, 257, 574, 412, 264, 544, 2674, 11420, 295, 2128, 2365, 294, 15329, 13, 51158, 51158, 10905, 281, 264, 3894, 960, 11, 452, 3387, 294, 341, 960, 307, 281, 855, 291, 264, 3089, 370, 300, 51380, 51380, 562, 291, 536, 309, 797, 294, 264, 3124, 2715, 293, 264, 17312, 20339, 11, 291, 458, 577, 281, 7302, 51652, 51652, 309, 13, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.1512383830790617, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.03019646741449833}, {"id": 5, "seek": 0, "start": 25.76, "end": 26.76, "text": " it.", "tokens": [50364, 682, 264, 1036, 960, 11, 291, 1866, 577, 281, 4445, 2128, 2365, 294, 15329, 11, 457, 538, 1152, 17720, 50762, 50762, 3876, 295, 3089, 337, 633, 2167, 34090, 13, 50908, 50908, 961, 311, 586, 747, 257, 574, 412, 264, 544, 2674, 11420, 295, 2128, 2365, 294, 15329, 13, 51158, 51158, 10905, 281, 264, 3894, 960, 11, 452, 3387, 294, 341, 960, 307, 281, 855, 291, 264, 3089, 370, 300, 51380, 51380, 562, 291, 536, 309, 797, 294, 264, 3124, 2715, 293, 264, 17312, 20339, 11, 291, 458, 577, 281, 7302, 51652, 51652, 309, 13, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.1512383830790617, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.03019646741449833}, {"id": 6, "seek": 2676, "start": 26.76, "end": 30.92, "text": " So as we walk through this example, don't worry about taking notes on every single line", "tokens": [50364, 407, 382, 321, 1792, 807, 341, 1365, 11, 500, 380, 3292, 466, 1940, 5570, 322, 633, 2167, 1622, 50572, 50572, 295, 3089, 13, 50632, 50632, 759, 291, 393, 1401, 807, 264, 3089, 293, 1223, 309, 11, 300, 311, 2138, 1547, 13, 50855, 50855, 407, 437, 291, 393, 360, 307, 2464, 257, 2445, 281, 4445, 257, 18011, 4583, 300, 307, 257, 2167, 4583, 51167, 51167, 295, 257, 18161, 3209, 13, 51282, 51282, 407, 286, 478, 516, 281, 6964, 264, 18011, 2445, 11, 597, 2516, 382, 4846, 264, 24433, 490, 264, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.10615142699210875, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.0001634369109524414}, {"id": 7, "seek": 2676, "start": 30.92, "end": 32.120000000000005, "text": " of code.", "tokens": [50364, 407, 382, 321, 1792, 807, 341, 1365, 11, 500, 380, 3292, 466, 1940, 5570, 322, 633, 2167, 1622, 50572, 50572, 295, 3089, 13, 50632, 50632, 759, 291, 393, 1401, 807, 264, 3089, 293, 1223, 309, 11, 300, 311, 2138, 1547, 13, 50855, 50855, 407, 437, 291, 393, 360, 307, 2464, 257, 2445, 281, 4445, 257, 18011, 4583, 300, 307, 257, 2167, 4583, 51167, 51167, 295, 257, 18161, 3209, 13, 51282, 51282, 407, 286, 478, 516, 281, 6964, 264, 18011, 2445, 11, 597, 2516, 382, 4846, 264, 24433, 490, 264, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.10615142699210875, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.0001634369109524414}, {"id": 8, "seek": 2676, "start": 32.120000000000005, "end": 36.58, "text": " If you can read through the code and understand it, that's definitely enough.", "tokens": [50364, 407, 382, 321, 1792, 807, 341, 1365, 11, 500, 380, 3292, 466, 1940, 5570, 322, 633, 2167, 1622, 50572, 50572, 295, 3089, 13, 50632, 50632, 759, 291, 393, 1401, 807, 264, 3089, 293, 1223, 309, 11, 300, 311, 2138, 1547, 13, 50855, 50855, 407, 437, 291, 393, 360, 307, 2464, 257, 2445, 281, 4445, 257, 18011, 4583, 300, 307, 257, 2167, 4583, 51167, 51167, 295, 257, 18161, 3209, 13, 51282, 51282, 407, 286, 478, 516, 281, 6964, 264, 18011, 2445, 11, 597, 2516, 382, 4846, 264, 24433, 490, 264, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.10615142699210875, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.0001634369109524414}, {"id": 9, "seek": 2676, "start": 36.58, "end": 42.82, "text": " So what you can do is write a function to implement a dense layer that is a single layer", "tokens": [50364, 407, 382, 321, 1792, 807, 341, 1365, 11, 500, 380, 3292, 466, 1940, 5570, 322, 633, 2167, 1622, 50572, 50572, 295, 3089, 13, 50632, 50632, 759, 291, 393, 1401, 807, 264, 3089, 293, 1223, 309, 11, 300, 311, 2138, 1547, 13, 50855, 50855, 407, 437, 291, 393, 360, 307, 2464, 257, 2445, 281, 4445, 257, 18011, 4583, 300, 307, 257, 2167, 4583, 51167, 51167, 295, 257, 18161, 3209, 13, 51282, 51282, 407, 286, 478, 516, 281, 6964, 264, 18011, 2445, 11, 597, 2516, 382, 4846, 264, 24433, 490, 264, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.10615142699210875, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.0001634369109524414}, {"id": 10, "seek": 2676, "start": 42.82, "end": 45.120000000000005, "text": " of a neural network.", "tokens": [50364, 407, 382, 321, 1792, 807, 341, 1365, 11, 500, 380, 3292, 466, 1940, 5570, 322, 633, 2167, 1622, 50572, 50572, 295, 3089, 13, 50632, 50632, 759, 291, 393, 1401, 807, 264, 3089, 293, 1223, 309, 11, 300, 311, 2138, 1547, 13, 50855, 50855, 407, 437, 291, 393, 360, 307, 2464, 257, 2445, 281, 4445, 257, 18011, 4583, 300, 307, 257, 2167, 4583, 51167, 51167, 295, 257, 18161, 3209, 13, 51282, 51282, 407, 286, 478, 516, 281, 6964, 264, 18011, 2445, 11, 597, 2516, 382, 4846, 264, 24433, 490, 264, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.10615142699210875, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.0001634369109524414}, {"id": 11, "seek": 2676, "start": 45.120000000000005, "end": 50.52, "text": " So I'm going to define the dense function, which takes as input the activation from the", "tokens": [50364, 407, 382, 321, 1792, 807, 341, 1365, 11, 500, 380, 3292, 466, 1940, 5570, 322, 633, 2167, 1622, 50572, 50572, 295, 3089, 13, 50632, 50632, 759, 291, 393, 1401, 807, 264, 3089, 293, 1223, 309, 11, 300, 311, 2138, 1547, 13, 50855, 50855, 407, 437, 291, 393, 360, 307, 2464, 257, 2445, 281, 4445, 257, 18011, 4583, 300, 307, 257, 2167, 4583, 51167, 51167, 295, 257, 18161, 3209, 13, 51282, 51282, 407, 286, 478, 516, 281, 6964, 264, 18011, 2445, 11, 597, 2516, 382, 4846, 264, 24433, 490, 264, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.10615142699210875, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.0001634369109524414}, {"id": 12, "seek": 5052, "start": 50.52, "end": 59.24, "text": " previous layer, as well as the parameters w and b for the neurons in a given layer.", "tokens": [50364, 3894, 4583, 11, 382, 731, 382, 264, 9834, 261, 293, 272, 337, 264, 22027, 294, 257, 2212, 4583, 13, 50800, 50800, 11142, 264, 1365, 490, 264, 3894, 960, 11, 498, 4583, 472, 575, 1045, 22027, 11, 293, 498, 261, 16, 51338, 51338, 293, 261, 17, 293, 261, 18, 366, 613, 11, 550, 437, 321, 603, 360, 307, 8630, 439, 295, 613, 3364, 18875, 666, 51782, 51782, 257, 8141, 13, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.127084784311791, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.8408689104253426e-05}, {"id": 13, "seek": 5052, "start": 59.24, "end": 70.0, "text": " Using the example from the previous video, if layer one has three neurons, and if w1", "tokens": [50364, 3894, 4583, 11, 382, 731, 382, 264, 9834, 261, 293, 272, 337, 264, 22027, 294, 257, 2212, 4583, 13, 50800, 50800, 11142, 264, 1365, 490, 264, 3894, 960, 11, 498, 4583, 472, 575, 1045, 22027, 11, 293, 498, 261, 16, 51338, 51338, 293, 261, 17, 293, 261, 18, 366, 613, 11, 550, 437, 321, 603, 360, 307, 8630, 439, 295, 613, 3364, 18875, 666, 51782, 51782, 257, 8141, 13, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.127084784311791, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.8408689104253426e-05}, {"id": 14, "seek": 5052, "start": 70.0, "end": 78.88, "text": " and w2 and w3 are these, then what we'll do is stack all of these weight vectors into", "tokens": [50364, 3894, 4583, 11, 382, 731, 382, 264, 9834, 261, 293, 272, 337, 264, 22027, 294, 257, 2212, 4583, 13, 50800, 50800, 11142, 264, 1365, 490, 264, 3894, 960, 11, 498, 4583, 472, 575, 1045, 22027, 11, 293, 498, 261, 16, 51338, 51338, 293, 261, 17, 293, 261, 18, 366, 613, 11, 550, 437, 321, 603, 360, 307, 8630, 439, 295, 613, 3364, 18875, 666, 51782, 51782, 257, 8141, 13, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.127084784311791, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.8408689104253426e-05}, {"id": 15, "seek": 5052, "start": 78.88, "end": 79.88, "text": " a matrix.", "tokens": [50364, 3894, 4583, 11, 382, 731, 382, 264, 9834, 261, 293, 272, 337, 264, 22027, 294, 257, 2212, 4583, 13, 50800, 50800, 11142, 264, 1365, 490, 264, 3894, 960, 11, 498, 4583, 472, 575, 1045, 22027, 11, 293, 498, 261, 16, 51338, 51338, 293, 261, 17, 293, 261, 18, 366, 613, 11, 550, 437, 321, 603, 360, 307, 8630, 439, 295, 613, 3364, 18875, 666, 51782, 51782, 257, 8141, 13, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.127084784311791, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.8408689104253426e-05}, {"id": 16, "seek": 7988, "start": 79.88, "end": 89.84, "text": " This is going to be a two by three matrix, where the first column is the parameter w11,", "tokens": [50364, 639, 307, 516, 281, 312, 257, 732, 538, 1045, 8141, 11, 689, 264, 700, 7738, 307, 264, 13075, 261, 5348, 11, 50862, 50862, 264, 1150, 7738, 307, 264, 13075, 261, 4762, 11, 293, 264, 2636, 7738, 307, 264, 13075, 261, 7668, 13, 51216, 51216, 400, 550, 294, 257, 2531, 636, 11, 498, 291, 362, 9834, 272, 11, 272, 5348, 6915, 3671, 502, 11, 272, 4762, 6915, 502, 11, 293, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11468206693048347, "compression_ratio": 1.7254901960784315, "no_speech_prob": 1.1125311175419483e-05}, {"id": 17, "seek": 7988, "start": 89.84, "end": 96.91999999999999, "text": " the second column is the parameter w12, and the third column is the parameter w13.", "tokens": [50364, 639, 307, 516, 281, 312, 257, 732, 538, 1045, 8141, 11, 689, 264, 700, 7738, 307, 264, 13075, 261, 5348, 11, 50862, 50862, 264, 1150, 7738, 307, 264, 13075, 261, 4762, 11, 293, 264, 2636, 7738, 307, 264, 13075, 261, 7668, 13, 51216, 51216, 400, 550, 294, 257, 2531, 636, 11, 498, 291, 362, 9834, 272, 11, 272, 5348, 6915, 3671, 502, 11, 272, 4762, 6915, 502, 11, 293, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11468206693048347, "compression_ratio": 1.7254901960784315, "no_speech_prob": 1.1125311175419483e-05}, {"id": 18, "seek": 7988, "start": 96.91999999999999, "end": 105.88, "text": " And then in a similar way, if you have parameters b, b11 equals negative 1, b12 equals 1, and", "tokens": [50364, 639, 307, 516, 281, 312, 257, 732, 538, 1045, 8141, 11, 689, 264, 700, 7738, 307, 264, 13075, 261, 5348, 11, 50862, 50862, 264, 1150, 7738, 307, 264, 13075, 261, 4762, 11, 293, 264, 2636, 7738, 307, 264, 13075, 261, 7668, 13, 51216, 51216, 400, 550, 294, 257, 2531, 636, 11, 498, 291, 362, 9834, 272, 11, 272, 5348, 6915, 3671, 502, 11, 272, 4762, 6915, 502, 11, 293, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11468206693048347, "compression_ratio": 1.7254901960784315, "no_speech_prob": 1.1125311175419483e-05}, {"id": 19, "seek": 10588, "start": 105.88, "end": 113.28, "text": " so on, then we're going to stack these three numbers into one D array b as follows, negative", "tokens": [50364, 370, 322, 11, 550, 321, 434, 516, 281, 8630, 613, 1045, 3547, 666, 472, 413, 10225, 272, 382, 10002, 11, 3671, 50734, 50734, 502, 11, 502, 11, 568, 13, 50818, 50818, 407, 437, 264, 18011, 2445, 486, 360, 307, 747, 382, 4846, 264, 24433, 490, 264, 3894, 51060, 51060, 4583, 11, 293, 257, 510, 727, 312, 257, 15, 11, 597, 307, 2681, 281, 2031, 11, 420, 264, 24433, 490, 257, 1780, 4583, 11, 51461, 51461, 382, 731, 382, 264, 261, 9834, 28867, 294, 13766, 11, 411, 4898, 322, 264, 558, 11, 382, 731, 382, 264, 272, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.13413488388061523, "compression_ratio": 1.6266666666666667, "no_speech_prob": 2.6015959520009346e-06}, {"id": 20, "seek": 10588, "start": 113.28, "end": 114.96, "text": " 1, 1, 2.", "tokens": [50364, 370, 322, 11, 550, 321, 434, 516, 281, 8630, 613, 1045, 3547, 666, 472, 413, 10225, 272, 382, 10002, 11, 3671, 50734, 50734, 502, 11, 502, 11, 568, 13, 50818, 50818, 407, 437, 264, 18011, 2445, 486, 360, 307, 747, 382, 4846, 264, 24433, 490, 264, 3894, 51060, 51060, 4583, 11, 293, 257, 510, 727, 312, 257, 15, 11, 597, 307, 2681, 281, 2031, 11, 420, 264, 24433, 490, 257, 1780, 4583, 11, 51461, 51461, 382, 731, 382, 264, 261, 9834, 28867, 294, 13766, 11, 411, 4898, 322, 264, 558, 11, 382, 731, 382, 264, 272, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.13413488388061523, "compression_ratio": 1.6266666666666667, "no_speech_prob": 2.6015959520009346e-06}, {"id": 21, "seek": 10588, "start": 114.96, "end": 119.8, "text": " So what the dense function will do is take as input the activation from the previous", "tokens": [50364, 370, 322, 11, 550, 321, 434, 516, 281, 8630, 613, 1045, 3547, 666, 472, 413, 10225, 272, 382, 10002, 11, 3671, 50734, 50734, 502, 11, 502, 11, 568, 13, 50818, 50818, 407, 437, 264, 18011, 2445, 486, 360, 307, 747, 382, 4846, 264, 24433, 490, 264, 3894, 51060, 51060, 4583, 11, 293, 257, 510, 727, 312, 257, 15, 11, 597, 307, 2681, 281, 2031, 11, 420, 264, 24433, 490, 257, 1780, 4583, 11, 51461, 51461, 382, 731, 382, 264, 261, 9834, 28867, 294, 13766, 11, 411, 4898, 322, 264, 558, 11, 382, 731, 382, 264, 272, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.13413488388061523, "compression_ratio": 1.6266666666666667, "no_speech_prob": 2.6015959520009346e-06}, {"id": 22, "seek": 10588, "start": 119.8, "end": 127.82, "text": " layer, and a here could be a0, which is equal to x, or the activation from a later layer,", "tokens": [50364, 370, 322, 11, 550, 321, 434, 516, 281, 8630, 613, 1045, 3547, 666, 472, 413, 10225, 272, 382, 10002, 11, 3671, 50734, 50734, 502, 11, 502, 11, 568, 13, 50818, 50818, 407, 437, 264, 18011, 2445, 486, 360, 307, 747, 382, 4846, 264, 24433, 490, 264, 3894, 51060, 51060, 4583, 11, 293, 257, 510, 727, 312, 257, 15, 11, 597, 307, 2681, 281, 2031, 11, 420, 264, 24433, 490, 257, 1780, 4583, 11, 51461, 51461, 382, 731, 382, 264, 261, 9834, 28867, 294, 13766, 11, 411, 4898, 322, 264, 558, 11, 382, 731, 382, 264, 272, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.13413488388061523, "compression_ratio": 1.6266666666666667, "no_speech_prob": 2.6015959520009346e-06}, {"id": 23, "seek": 10588, "start": 127.82, "end": 135.48, "text": " as well as the w parameters stacked in columns, like shown on the right, as well as the b", "tokens": [50364, 370, 322, 11, 550, 321, 434, 516, 281, 8630, 613, 1045, 3547, 666, 472, 413, 10225, 272, 382, 10002, 11, 3671, 50734, 50734, 502, 11, 502, 11, 568, 13, 50818, 50818, 407, 437, 264, 18011, 2445, 486, 360, 307, 747, 382, 4846, 264, 24433, 490, 264, 3894, 51060, 51060, 4583, 11, 293, 257, 510, 727, 312, 257, 15, 11, 597, 307, 2681, 281, 2031, 11, 420, 264, 24433, 490, 257, 1780, 4583, 11, 51461, 51461, 382, 731, 382, 264, 261, 9834, 28867, 294, 13766, 11, 411, 4898, 322, 264, 558, 11, 382, 731, 382, 264, 272, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.13413488388061523, "compression_ratio": 1.6266666666666667, "no_speech_prob": 2.6015959520009346e-06}, {"id": 24, "seek": 13548, "start": 135.48, "end": 143.32, "text": " parameters also stacked into one D array, like shown to the left over there.", "tokens": [50364, 9834, 611, 28867, 666, 472, 413, 10225, 11, 411, 4898, 281, 264, 1411, 670, 456, 13, 50756, 50756, 400, 437, 341, 2445, 486, 360, 307, 4846, 257, 11, 264, 24433, 490, 264, 3894, 4583, 11, 293, 51098, 51098, 486, 5598, 264, 2430, 763, 490, 264, 13766, 4583, 13, 51310, 51310, 407, 718, 311, 1823, 807, 264, 3089, 337, 884, 341, 13, 51466, 51466, 1692, 311, 264, 3089, 13, 51556, 51556, 2386, 11, 6815, 6915, 261, 5893, 3909, 472, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.19748876180993505, "compression_ratio": 1.572139303482587, "no_speech_prob": 2.6425693704368314e-06}, {"id": 25, "seek": 13548, "start": 143.32, "end": 150.16, "text": " And what this function will do is input a, the activation from the previous layer, and", "tokens": [50364, 9834, 611, 28867, 666, 472, 413, 10225, 11, 411, 4898, 281, 264, 1411, 670, 456, 13, 50756, 50756, 400, 437, 341, 2445, 486, 360, 307, 4846, 257, 11, 264, 24433, 490, 264, 3894, 4583, 11, 293, 51098, 51098, 486, 5598, 264, 2430, 763, 490, 264, 13766, 4583, 13, 51310, 51310, 407, 718, 311, 1823, 807, 264, 3089, 337, 884, 341, 13, 51466, 51466, 1692, 311, 264, 3089, 13, 51556, 51556, 2386, 11, 6815, 6915, 261, 5893, 3909, 472, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.19748876180993505, "compression_ratio": 1.572139303482587, "no_speech_prob": 2.6425693704368314e-06}, {"id": 26, "seek": 13548, "start": 150.16, "end": 154.39999999999998, "text": " will output the activations from the columns layer.", "tokens": [50364, 9834, 611, 28867, 666, 472, 413, 10225, 11, 411, 4898, 281, 264, 1411, 670, 456, 13, 50756, 50756, 400, 437, 341, 2445, 486, 360, 307, 4846, 257, 11, 264, 24433, 490, 264, 3894, 4583, 11, 293, 51098, 51098, 486, 5598, 264, 2430, 763, 490, 264, 13766, 4583, 13, 51310, 51310, 407, 718, 311, 1823, 807, 264, 3089, 337, 884, 341, 13, 51466, 51466, 1692, 311, 264, 3089, 13, 51556, 51556, 2386, 11, 6815, 6915, 261, 5893, 3909, 472, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.19748876180993505, "compression_ratio": 1.572139303482587, "no_speech_prob": 2.6425693704368314e-06}, {"id": 27, "seek": 13548, "start": 154.39999999999998, "end": 157.51999999999998, "text": " So let's step through the code for doing this.", "tokens": [50364, 9834, 611, 28867, 666, 472, 413, 10225, 11, 411, 4898, 281, 264, 1411, 670, 456, 13, 50756, 50756, 400, 437, 341, 2445, 486, 360, 307, 4846, 257, 11, 264, 24433, 490, 264, 3894, 4583, 11, 293, 51098, 51098, 486, 5598, 264, 2430, 763, 490, 264, 13766, 4583, 13, 51310, 51310, 407, 718, 311, 1823, 807, 264, 3089, 337, 884, 341, 13, 51466, 51466, 1692, 311, 264, 3089, 13, 51556, 51556, 2386, 11, 6815, 6915, 261, 5893, 3909, 472, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.19748876180993505, "compression_ratio": 1.572139303482587, "no_speech_prob": 2.6425693704368314e-06}, {"id": 28, "seek": 13548, "start": 157.51999999999998, "end": 159.32, "text": " Here's the code.", "tokens": [50364, 9834, 611, 28867, 666, 472, 413, 10225, 11, 411, 4898, 281, 264, 1411, 670, 456, 13, 50756, 50756, 400, 437, 341, 2445, 486, 360, 307, 4846, 257, 11, 264, 24433, 490, 264, 3894, 4583, 11, 293, 51098, 51098, 486, 5598, 264, 2430, 763, 490, 264, 13766, 4583, 13, 51310, 51310, 407, 718, 311, 1823, 807, 264, 3089, 337, 884, 341, 13, 51466, 51466, 1692, 311, 264, 3089, 13, 51556, 51556, 2386, 11, 6815, 6915, 261, 5893, 3909, 472, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.19748876180993505, "compression_ratio": 1.572139303482587, "no_speech_prob": 2.6425693704368314e-06}, {"id": 29, "seek": 13548, "start": 159.32, "end": 163.39999999999998, "text": " First, units equals w dot shape one.", "tokens": [50364, 9834, 611, 28867, 666, 472, 413, 10225, 11, 411, 4898, 281, 264, 1411, 670, 456, 13, 50756, 50756, 400, 437, 341, 2445, 486, 360, 307, 4846, 257, 11, 264, 24433, 490, 264, 3894, 4583, 11, 293, 51098, 51098, 486, 5598, 264, 2430, 763, 490, 264, 13766, 4583, 13, 51310, 51310, 407, 718, 311, 1823, 807, 264, 3089, 337, 884, 341, 13, 51466, 51466, 1692, 311, 264, 3089, 13, 51556, 51556, 2386, 11, 6815, 6915, 261, 5893, 3909, 472, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.19748876180993505, "compression_ratio": 1.572139303482587, "no_speech_prob": 2.6425693704368314e-06}, {"id": 30, "seek": 16340, "start": 163.4, "end": 170.72, "text": " So w here is a two by three matrix, and so the number of columns is three.", "tokens": [50364, 407, 261, 510, 307, 257, 732, 538, 1045, 8141, 11, 293, 370, 264, 1230, 295, 13766, 307, 1045, 13, 50730, 50730, 663, 311, 2681, 281, 264, 1230, 295, 6815, 294, 341, 4583, 13, 50879, 50879, 407, 510, 11, 6815, 576, 312, 2681, 281, 1045, 13, 51048, 51048, 400, 1237, 412, 264, 3909, 295, 261, 11, 309, 311, 445, 257, 636, 295, 8407, 484, 264, 1230, 295, 7633, 6815, 11, 51340, 51340, 420, 264, 1230, 295, 6815, 294, 341, 4583, 13, 51476, 51476, 3087, 11, 321, 992, 257, 281, 312, 364, 10225, 295, 35193, 365, 382, 867, 4959, 382, 456, 366, 6815, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1098648377184598, "compression_ratio": 1.7894736842105263, "no_speech_prob": 6.1440600802598055e-06}, {"id": 31, "seek": 16340, "start": 170.72, "end": 173.70000000000002, "text": " That's equal to the number of units in this layer.", "tokens": [50364, 407, 261, 510, 307, 257, 732, 538, 1045, 8141, 11, 293, 370, 264, 1230, 295, 13766, 307, 1045, 13, 50730, 50730, 663, 311, 2681, 281, 264, 1230, 295, 6815, 294, 341, 4583, 13, 50879, 50879, 407, 510, 11, 6815, 576, 312, 2681, 281, 1045, 13, 51048, 51048, 400, 1237, 412, 264, 3909, 295, 261, 11, 309, 311, 445, 257, 636, 295, 8407, 484, 264, 1230, 295, 7633, 6815, 11, 51340, 51340, 420, 264, 1230, 295, 6815, 294, 341, 4583, 13, 51476, 51476, 3087, 11, 321, 992, 257, 281, 312, 364, 10225, 295, 35193, 365, 382, 867, 4959, 382, 456, 366, 6815, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1098648377184598, "compression_ratio": 1.7894736842105263, "no_speech_prob": 6.1440600802598055e-06}, {"id": 32, "seek": 16340, "start": 173.70000000000002, "end": 177.08, "text": " So here, units would be equal to three.", "tokens": [50364, 407, 261, 510, 307, 257, 732, 538, 1045, 8141, 11, 293, 370, 264, 1230, 295, 13766, 307, 1045, 13, 50730, 50730, 663, 311, 2681, 281, 264, 1230, 295, 6815, 294, 341, 4583, 13, 50879, 50879, 407, 510, 11, 6815, 576, 312, 2681, 281, 1045, 13, 51048, 51048, 400, 1237, 412, 264, 3909, 295, 261, 11, 309, 311, 445, 257, 636, 295, 8407, 484, 264, 1230, 295, 7633, 6815, 11, 51340, 51340, 420, 264, 1230, 295, 6815, 294, 341, 4583, 13, 51476, 51476, 3087, 11, 321, 992, 257, 281, 312, 364, 10225, 295, 35193, 365, 382, 867, 4959, 382, 456, 366, 6815, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1098648377184598, "compression_ratio": 1.7894736842105263, "no_speech_prob": 6.1440600802598055e-06}, {"id": 33, "seek": 16340, "start": 177.08, "end": 182.92000000000002, "text": " And looking at the shape of w, it's just a way of pulling out the number of hidden units,", "tokens": [50364, 407, 261, 510, 307, 257, 732, 538, 1045, 8141, 11, 293, 370, 264, 1230, 295, 13766, 307, 1045, 13, 50730, 50730, 663, 311, 2681, 281, 264, 1230, 295, 6815, 294, 341, 4583, 13, 50879, 50879, 407, 510, 11, 6815, 576, 312, 2681, 281, 1045, 13, 51048, 51048, 400, 1237, 412, 264, 3909, 295, 261, 11, 309, 311, 445, 257, 636, 295, 8407, 484, 264, 1230, 295, 7633, 6815, 11, 51340, 51340, 420, 264, 1230, 295, 6815, 294, 341, 4583, 13, 51476, 51476, 3087, 11, 321, 992, 257, 281, 312, 364, 10225, 295, 35193, 365, 382, 867, 4959, 382, 456, 366, 6815, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1098648377184598, "compression_ratio": 1.7894736842105263, "no_speech_prob": 6.1440600802598055e-06}, {"id": 34, "seek": 16340, "start": 182.92000000000002, "end": 185.64000000000001, "text": " or the number of units in this layer.", "tokens": [50364, 407, 261, 510, 307, 257, 732, 538, 1045, 8141, 11, 293, 370, 264, 1230, 295, 13766, 307, 1045, 13, 50730, 50730, 663, 311, 2681, 281, 264, 1230, 295, 6815, 294, 341, 4583, 13, 50879, 50879, 407, 510, 11, 6815, 576, 312, 2681, 281, 1045, 13, 51048, 51048, 400, 1237, 412, 264, 3909, 295, 261, 11, 309, 311, 445, 257, 636, 295, 8407, 484, 264, 1230, 295, 7633, 6815, 11, 51340, 51340, 420, 264, 1230, 295, 6815, 294, 341, 4583, 13, 51476, 51476, 3087, 11, 321, 992, 257, 281, 312, 364, 10225, 295, 35193, 365, 382, 867, 4959, 382, 456, 366, 6815, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1098648377184598, "compression_ratio": 1.7894736842105263, "no_speech_prob": 6.1440600802598055e-06}, {"id": 35, "seek": 16340, "start": 185.64000000000001, "end": 192.20000000000002, "text": " Next, we set a to be an array of zeros with as many elements as there are units.", "tokens": [50364, 407, 261, 510, 307, 257, 732, 538, 1045, 8141, 11, 293, 370, 264, 1230, 295, 13766, 307, 1045, 13, 50730, 50730, 663, 311, 2681, 281, 264, 1230, 295, 6815, 294, 341, 4583, 13, 50879, 50879, 407, 510, 11, 6815, 576, 312, 2681, 281, 1045, 13, 51048, 51048, 400, 1237, 412, 264, 3909, 295, 261, 11, 309, 311, 445, 257, 636, 295, 8407, 484, 264, 1230, 295, 7633, 6815, 11, 51340, 51340, 420, 264, 1230, 295, 6815, 294, 341, 4583, 13, 51476, 51476, 3087, 11, 321, 992, 257, 281, 312, 364, 10225, 295, 35193, 365, 382, 867, 4959, 382, 456, 366, 6815, 13, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.1098648377184598, "compression_ratio": 1.7894736842105263, "no_speech_prob": 6.1440600802598055e-06}, {"id": 36, "seek": 19220, "start": 192.2, "end": 196.6, "text": " So in this example, we need to output three activation values.", "tokens": [50364, 407, 294, 341, 1365, 11, 321, 643, 281, 5598, 1045, 24433, 4190, 13, 50584, 50584, 407, 341, 445, 5883, 5660, 257, 281, 312, 4018, 11, 4018, 11, 4018, 11, 364, 10225, 295, 1045, 35193, 13, 50900, 50900, 3087, 11, 321, 352, 807, 257, 337, 6367, 281, 14722, 264, 700, 11, 1150, 11, 293, 2636, 4959, 295, 257, 13, 51204, 51204, 407, 337, 361, 293, 3613, 6815, 11, 370, 361, 1709, 490, 4018, 281, 6815, 3175, 472, 11, 370, 309, 1709, 490, 4018, 11, 51494, 51494, 472, 11, 732, 11, 8186, 278, 490, 4018, 293, 15329, 382, 7713, 13, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1395414296318503, "compression_ratio": 1.6759259259259258, "no_speech_prob": 1.5056854181239032e-06}, {"id": 37, "seek": 19220, "start": 196.6, "end": 202.92, "text": " So this just initializes a to be zero, zero, zero, an array of three zeros.", "tokens": [50364, 407, 294, 341, 1365, 11, 321, 643, 281, 5598, 1045, 24433, 4190, 13, 50584, 50584, 407, 341, 445, 5883, 5660, 257, 281, 312, 4018, 11, 4018, 11, 4018, 11, 364, 10225, 295, 1045, 35193, 13, 50900, 50900, 3087, 11, 321, 352, 807, 257, 337, 6367, 281, 14722, 264, 700, 11, 1150, 11, 293, 2636, 4959, 295, 257, 13, 51204, 51204, 407, 337, 361, 293, 3613, 6815, 11, 370, 361, 1709, 490, 4018, 281, 6815, 3175, 472, 11, 370, 309, 1709, 490, 4018, 11, 51494, 51494, 472, 11, 732, 11, 8186, 278, 490, 4018, 293, 15329, 382, 7713, 13, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1395414296318503, "compression_ratio": 1.6759259259259258, "no_speech_prob": 1.5056854181239032e-06}, {"id": 38, "seek": 19220, "start": 202.92, "end": 209.0, "text": " Next, we go through a for loop to compute the first, second, and third elements of a.", "tokens": [50364, 407, 294, 341, 1365, 11, 321, 643, 281, 5598, 1045, 24433, 4190, 13, 50584, 50584, 407, 341, 445, 5883, 5660, 257, 281, 312, 4018, 11, 4018, 11, 4018, 11, 364, 10225, 295, 1045, 35193, 13, 50900, 50900, 3087, 11, 321, 352, 807, 257, 337, 6367, 281, 14722, 264, 700, 11, 1150, 11, 293, 2636, 4959, 295, 257, 13, 51204, 51204, 407, 337, 361, 293, 3613, 6815, 11, 370, 361, 1709, 490, 4018, 281, 6815, 3175, 472, 11, 370, 309, 1709, 490, 4018, 11, 51494, 51494, 472, 11, 732, 11, 8186, 278, 490, 4018, 293, 15329, 382, 7713, 13, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1395414296318503, "compression_ratio": 1.6759259259259258, "no_speech_prob": 1.5056854181239032e-06}, {"id": 39, "seek": 19220, "start": 209.0, "end": 214.79999999999998, "text": " So for j and range units, so j goes from zero to units minus one, so it goes from zero,", "tokens": [50364, 407, 294, 341, 1365, 11, 321, 643, 281, 5598, 1045, 24433, 4190, 13, 50584, 50584, 407, 341, 445, 5883, 5660, 257, 281, 312, 4018, 11, 4018, 11, 4018, 11, 364, 10225, 295, 1045, 35193, 13, 50900, 50900, 3087, 11, 321, 352, 807, 257, 337, 6367, 281, 14722, 264, 700, 11, 1150, 11, 293, 2636, 4959, 295, 257, 13, 51204, 51204, 407, 337, 361, 293, 3613, 6815, 11, 370, 361, 1709, 490, 4018, 281, 6815, 3175, 472, 11, 370, 309, 1709, 490, 4018, 11, 51494, 51494, 472, 11, 732, 11, 8186, 278, 490, 4018, 293, 15329, 382, 7713, 13, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1395414296318503, "compression_ratio": 1.6759259259259258, "no_speech_prob": 1.5056854181239032e-06}, {"id": 40, "seek": 19220, "start": 214.79999999999998, "end": 218.28, "text": " one, two, indexing from zero and Python as usual.", "tokens": [50364, 407, 294, 341, 1365, 11, 321, 643, 281, 5598, 1045, 24433, 4190, 13, 50584, 50584, 407, 341, 445, 5883, 5660, 257, 281, 312, 4018, 11, 4018, 11, 4018, 11, 364, 10225, 295, 1045, 35193, 13, 50900, 50900, 3087, 11, 321, 352, 807, 257, 337, 6367, 281, 14722, 264, 700, 11, 1150, 11, 293, 2636, 4959, 295, 257, 13, 51204, 51204, 407, 337, 361, 293, 3613, 6815, 11, 370, 361, 1709, 490, 4018, 281, 6815, 3175, 472, 11, 370, 309, 1709, 490, 4018, 11, 51494, 51494, 472, 11, 732, 11, 8186, 278, 490, 4018, 293, 15329, 382, 7713, 13, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1395414296318503, "compression_ratio": 1.6759259259259258, "no_speech_prob": 1.5056854181239032e-06}, {"id": 41, "seek": 21828, "start": 218.28, "end": 227.56, "text": " This command, w equals capital W colon comma j, this is how you pull out the j column of", "tokens": [50364, 639, 5622, 11, 261, 6915, 4238, 343, 8255, 22117, 361, 11, 341, 307, 577, 291, 2235, 484, 264, 361, 7738, 295, 50828, 50828, 257, 8141, 294, 15329, 13, 50960, 50960, 407, 264, 700, 565, 807, 341, 6367, 11, 341, 486, 2235, 484, 264, 700, 7738, 295, 261, 11, 293, 370, 51250, 51250, 486, 2235, 484, 261, 472, 472, 13, 51406, 51406, 440, 1150, 565, 807, 341, 6367, 11, 562, 291, 434, 15866, 264, 24433, 295, 264, 1150, 4985, 11, 51628, 51628], "temperature": 0.0, "avg_logprob": -0.18009922303349138, "compression_ratio": 1.747191011235955, "no_speech_prob": 1.3006789231440052e-05}, {"id": 42, "seek": 21828, "start": 227.56, "end": 230.2, "text": " a matrix in Python.", "tokens": [50364, 639, 5622, 11, 261, 6915, 4238, 343, 8255, 22117, 361, 11, 341, 307, 577, 291, 2235, 484, 264, 361, 7738, 295, 50828, 50828, 257, 8141, 294, 15329, 13, 50960, 50960, 407, 264, 700, 565, 807, 341, 6367, 11, 341, 486, 2235, 484, 264, 700, 7738, 295, 261, 11, 293, 370, 51250, 51250, 486, 2235, 484, 261, 472, 472, 13, 51406, 51406, 440, 1150, 565, 807, 341, 6367, 11, 562, 291, 434, 15866, 264, 24433, 295, 264, 1150, 4985, 11, 51628, 51628], "temperature": 0.0, "avg_logprob": -0.18009922303349138, "compression_ratio": 1.747191011235955, "no_speech_prob": 1.3006789231440052e-05}, {"id": 43, "seek": 21828, "start": 230.2, "end": 236.0, "text": " So the first time through this loop, this will pull out the first column of w, and so", "tokens": [50364, 639, 5622, 11, 261, 6915, 4238, 343, 8255, 22117, 361, 11, 341, 307, 577, 291, 2235, 484, 264, 361, 7738, 295, 50828, 50828, 257, 8141, 294, 15329, 13, 50960, 50960, 407, 264, 700, 565, 807, 341, 6367, 11, 341, 486, 2235, 484, 264, 700, 7738, 295, 261, 11, 293, 370, 51250, 51250, 486, 2235, 484, 261, 472, 472, 13, 51406, 51406, 440, 1150, 565, 807, 341, 6367, 11, 562, 291, 434, 15866, 264, 24433, 295, 264, 1150, 4985, 11, 51628, 51628], "temperature": 0.0, "avg_logprob": -0.18009922303349138, "compression_ratio": 1.747191011235955, "no_speech_prob": 1.3006789231440052e-05}, {"id": 44, "seek": 21828, "start": 236.0, "end": 239.12, "text": " will pull out w one one.", "tokens": [50364, 639, 5622, 11, 261, 6915, 4238, 343, 8255, 22117, 361, 11, 341, 307, 577, 291, 2235, 484, 264, 361, 7738, 295, 50828, 50828, 257, 8141, 294, 15329, 13, 50960, 50960, 407, 264, 700, 565, 807, 341, 6367, 11, 341, 486, 2235, 484, 264, 700, 7738, 295, 261, 11, 293, 370, 51250, 51250, 486, 2235, 484, 261, 472, 472, 13, 51406, 51406, 440, 1150, 565, 807, 341, 6367, 11, 562, 291, 434, 15866, 264, 24433, 295, 264, 1150, 4985, 11, 51628, 51628], "temperature": 0.0, "avg_logprob": -0.18009922303349138, "compression_ratio": 1.747191011235955, "no_speech_prob": 1.3006789231440052e-05}, {"id": 45, "seek": 21828, "start": 239.12, "end": 243.56, "text": " The second time through this loop, when you're computing the activation of the second unit,", "tokens": [50364, 639, 5622, 11, 261, 6915, 4238, 343, 8255, 22117, 361, 11, 341, 307, 577, 291, 2235, 484, 264, 361, 7738, 295, 50828, 50828, 257, 8141, 294, 15329, 13, 50960, 50960, 407, 264, 700, 565, 807, 341, 6367, 11, 341, 486, 2235, 484, 264, 700, 7738, 295, 261, 11, 293, 370, 51250, 51250, 486, 2235, 484, 261, 472, 472, 13, 51406, 51406, 440, 1150, 565, 807, 341, 6367, 11, 562, 291, 434, 15866, 264, 24433, 295, 264, 1150, 4985, 11, 51628, 51628], "temperature": 0.0, "avg_logprob": -0.18009922303349138, "compression_ratio": 1.747191011235955, "no_speech_prob": 1.3006789231440052e-05}, {"id": 46, "seek": 24356, "start": 243.56, "end": 249.52, "text": " the pull out the second column corresponding to w one two, and so on for the third time", "tokens": [50364, 264, 2235, 484, 264, 1150, 7738, 11760, 281, 261, 472, 732, 11, 293, 370, 322, 337, 264, 2636, 565, 50662, 50662, 807, 341, 6367, 13, 50758, 50758, 400, 550, 291, 14722, 710, 1228, 264, 7713, 8513, 382, 257, 1194, 1674, 1296, 300, 13075, 51036, 51036, 261, 293, 264, 24433, 300, 291, 632, 4613, 1804, 272, 361, 13, 51348, 51348, 400, 550, 291, 14722, 264, 24433, 257, 361, 6915, 290, 4556, 3280, 327, 2445, 6456, 281, 710, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.21142293200080778, "compression_ratio": 1.6751269035532994, "no_speech_prob": 2.2602998797083274e-06}, {"id": 47, "seek": 24356, "start": 249.52, "end": 251.44, "text": " through this loop.", "tokens": [50364, 264, 2235, 484, 264, 1150, 7738, 11760, 281, 261, 472, 732, 11, 293, 370, 322, 337, 264, 2636, 565, 50662, 50662, 807, 341, 6367, 13, 50758, 50758, 400, 550, 291, 14722, 710, 1228, 264, 7713, 8513, 382, 257, 1194, 1674, 1296, 300, 13075, 51036, 51036, 261, 293, 264, 24433, 300, 291, 632, 4613, 1804, 272, 361, 13, 51348, 51348, 400, 550, 291, 14722, 264, 24433, 257, 361, 6915, 290, 4556, 3280, 327, 2445, 6456, 281, 710, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.21142293200080778, "compression_ratio": 1.6751269035532994, "no_speech_prob": 2.2602998797083274e-06}, {"id": 48, "seek": 24356, "start": 251.44, "end": 257.0, "text": " And then you compute z using the usual formula as a thought product between that parameter", "tokens": [50364, 264, 2235, 484, 264, 1150, 7738, 11760, 281, 261, 472, 732, 11, 293, 370, 322, 337, 264, 2636, 565, 50662, 50662, 807, 341, 6367, 13, 50758, 50758, 400, 550, 291, 14722, 710, 1228, 264, 7713, 8513, 382, 257, 1194, 1674, 1296, 300, 13075, 51036, 51036, 261, 293, 264, 24433, 300, 291, 632, 4613, 1804, 272, 361, 13, 51348, 51348, 400, 550, 291, 14722, 264, 24433, 257, 361, 6915, 290, 4556, 3280, 327, 2445, 6456, 281, 710, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.21142293200080778, "compression_ratio": 1.6751269035532994, "no_speech_prob": 2.2602998797083274e-06}, {"id": 49, "seek": 24356, "start": 257.0, "end": 263.24, "text": " w and the activation that you had received plus b j.", "tokens": [50364, 264, 2235, 484, 264, 1150, 7738, 11760, 281, 261, 472, 732, 11, 293, 370, 322, 337, 264, 2636, 565, 50662, 50662, 807, 341, 6367, 13, 50758, 50758, 400, 550, 291, 14722, 710, 1228, 264, 7713, 8513, 382, 257, 1194, 1674, 1296, 300, 13075, 51036, 51036, 261, 293, 264, 24433, 300, 291, 632, 4613, 1804, 272, 361, 13, 51348, 51348, 400, 550, 291, 14722, 264, 24433, 257, 361, 6915, 290, 4556, 3280, 327, 2445, 6456, 281, 710, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.21142293200080778, "compression_ratio": 1.6751269035532994, "no_speech_prob": 2.2602998797083274e-06}, {"id": 50, "seek": 24356, "start": 263.24, "end": 269.32, "text": " And then you compute the activation a j equals g sigmoid function applied to z.", "tokens": [50364, 264, 2235, 484, 264, 1150, 7738, 11760, 281, 261, 472, 732, 11, 293, 370, 322, 337, 264, 2636, 565, 50662, 50662, 807, 341, 6367, 13, 50758, 50758, 400, 550, 291, 14722, 710, 1228, 264, 7713, 8513, 382, 257, 1194, 1674, 1296, 300, 13075, 51036, 51036, 261, 293, 264, 24433, 300, 291, 632, 4613, 1804, 272, 361, 13, 51348, 51348, 400, 550, 291, 14722, 264, 24433, 257, 361, 6915, 290, 4556, 3280, 327, 2445, 6456, 281, 710, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.21142293200080778, "compression_ratio": 1.6751269035532994, "no_speech_prob": 2.2602998797083274e-06}, {"id": 51, "seek": 26932, "start": 269.32, "end": 274.44, "text": " So three times through this loop and you computed the values for all three values of this vector", "tokens": [50364, 407, 1045, 1413, 807, 341, 6367, 293, 291, 40610, 264, 4190, 337, 439, 1045, 4190, 295, 341, 8062, 50620, 50620, 295, 2430, 763, 257, 11, 293, 550, 2721, 291, 2736, 257, 13, 50870, 50870, 407, 437, 264, 18011, 2445, 775, 307, 309, 15743, 264, 2430, 763, 490, 264, 3894, 4583, 11, 293, 51136, 51136, 2212, 264, 9834, 337, 264, 2190, 4583, 11, 309, 11247, 264, 2430, 763, 337, 264, 958, 4583, 13, 51437, 51437, 407, 2212, 264, 18011, 2445, 11, 510, 311, 577, 291, 393, 6798, 1214, 257, 1326, 18011, 7914, 5123, 3137, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.12633594041018142, "compression_ratio": 1.8672566371681416, "no_speech_prob": 2.2033141533484013e-07}, {"id": 52, "seek": 26932, "start": 274.44, "end": 279.44, "text": " of activations a, and then finally you return a.", "tokens": [50364, 407, 1045, 1413, 807, 341, 6367, 293, 291, 40610, 264, 4190, 337, 439, 1045, 4190, 295, 341, 8062, 50620, 50620, 295, 2430, 763, 257, 11, 293, 550, 2721, 291, 2736, 257, 13, 50870, 50870, 407, 437, 264, 18011, 2445, 775, 307, 309, 15743, 264, 2430, 763, 490, 264, 3894, 4583, 11, 293, 51136, 51136, 2212, 264, 9834, 337, 264, 2190, 4583, 11, 309, 11247, 264, 2430, 763, 337, 264, 958, 4583, 13, 51437, 51437, 407, 2212, 264, 18011, 2445, 11, 510, 311, 577, 291, 393, 6798, 1214, 257, 1326, 18011, 7914, 5123, 3137, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.12633594041018142, "compression_ratio": 1.8672566371681416, "no_speech_prob": 2.2033141533484013e-07}, {"id": 53, "seek": 26932, "start": 279.44, "end": 284.76, "text": " So what the dense function does is it inputs the activations from the previous layer, and", "tokens": [50364, 407, 1045, 1413, 807, 341, 6367, 293, 291, 40610, 264, 4190, 337, 439, 1045, 4190, 295, 341, 8062, 50620, 50620, 295, 2430, 763, 257, 11, 293, 550, 2721, 291, 2736, 257, 13, 50870, 50870, 407, 437, 264, 18011, 2445, 775, 307, 309, 15743, 264, 2430, 763, 490, 264, 3894, 4583, 11, 293, 51136, 51136, 2212, 264, 9834, 337, 264, 2190, 4583, 11, 309, 11247, 264, 2430, 763, 337, 264, 958, 4583, 13, 51437, 51437, 407, 2212, 264, 18011, 2445, 11, 510, 311, 577, 291, 393, 6798, 1214, 257, 1326, 18011, 7914, 5123, 3137, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.12633594041018142, "compression_ratio": 1.8672566371681416, "no_speech_prob": 2.2033141533484013e-07}, {"id": 54, "seek": 26932, "start": 284.76, "end": 290.78, "text": " given the parameters for the current layer, it returns the activations for the next layer.", "tokens": [50364, 407, 1045, 1413, 807, 341, 6367, 293, 291, 40610, 264, 4190, 337, 439, 1045, 4190, 295, 341, 8062, 50620, 50620, 295, 2430, 763, 257, 11, 293, 550, 2721, 291, 2736, 257, 13, 50870, 50870, 407, 437, 264, 18011, 2445, 775, 307, 309, 15743, 264, 2430, 763, 490, 264, 3894, 4583, 11, 293, 51136, 51136, 2212, 264, 9834, 337, 264, 2190, 4583, 11, 309, 11247, 264, 2430, 763, 337, 264, 958, 4583, 13, 51437, 51437, 407, 2212, 264, 18011, 2445, 11, 510, 311, 577, 291, 393, 6798, 1214, 257, 1326, 18011, 7914, 5123, 3137, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.12633594041018142, "compression_ratio": 1.8672566371681416, "no_speech_prob": 2.2033141533484013e-07}, {"id": 55, "seek": 26932, "start": 290.78, "end": 296.84, "text": " So given the dense function, here's how you can string together a few dense layers sequentially", "tokens": [50364, 407, 1045, 1413, 807, 341, 6367, 293, 291, 40610, 264, 4190, 337, 439, 1045, 4190, 295, 341, 8062, 50620, 50620, 295, 2430, 763, 257, 11, 293, 550, 2721, 291, 2736, 257, 13, 50870, 50870, 407, 437, 264, 18011, 2445, 775, 307, 309, 15743, 264, 2430, 763, 490, 264, 3894, 4583, 11, 293, 51136, 51136, 2212, 264, 9834, 337, 264, 2190, 4583, 11, 309, 11247, 264, 2430, 763, 337, 264, 958, 4583, 13, 51437, 51437, 407, 2212, 264, 18011, 2445, 11, 510, 311, 577, 291, 393, 6798, 1214, 257, 1326, 18011, 7914, 5123, 3137, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.12633594041018142, "compression_ratio": 1.8672566371681416, "no_speech_prob": 2.2033141533484013e-07}, {"id": 56, "seek": 29684, "start": 296.84, "end": 300.96, "text": " in order to implement forward prop in the neural network.", "tokens": [50364, 294, 1668, 281, 4445, 2128, 2365, 294, 264, 18161, 3209, 13, 50570, 50570, 18600, 264, 4846, 4122, 2031, 11, 291, 393, 550, 14722, 264, 2430, 763, 257, 472, 281, 312, 257, 472, 6915, 24505, 51047, 51047, 295, 2031, 261, 472, 272, 472, 11, 689, 510, 261, 472, 272, 472, 366, 264, 9834, 11, 2171, 611, 1219, 264, 51408, 51408, 17443, 295, 264, 700, 7633, 4583, 13, 51597, 51597], "temperature": 0.0, "avg_logprob": -0.13144803728376117, "compression_ratio": 1.583815028901734, "no_speech_prob": 3.1875194963504327e-06}, {"id": 57, "seek": 29684, "start": 300.96, "end": 310.5, "text": " Given the input features x, you can then compute the activations a one to be a one equals dens", "tokens": [50364, 294, 1668, 281, 4445, 2128, 2365, 294, 264, 18161, 3209, 13, 50570, 50570, 18600, 264, 4846, 4122, 2031, 11, 291, 393, 550, 14722, 264, 2430, 763, 257, 472, 281, 312, 257, 472, 6915, 24505, 51047, 51047, 295, 2031, 261, 472, 272, 472, 11, 689, 510, 261, 472, 272, 472, 366, 264, 9834, 11, 2171, 611, 1219, 264, 51408, 51408, 17443, 295, 264, 700, 7633, 4583, 13, 51597, 51597], "temperature": 0.0, "avg_logprob": -0.13144803728376117, "compression_ratio": 1.583815028901734, "no_speech_prob": 3.1875194963504327e-06}, {"id": 58, "seek": 29684, "start": 310.5, "end": 317.71999999999997, "text": " of x w one b one, where here w one b one are the parameters, sometimes also called the", "tokens": [50364, 294, 1668, 281, 4445, 2128, 2365, 294, 264, 18161, 3209, 13, 50570, 50570, 18600, 264, 4846, 4122, 2031, 11, 291, 393, 550, 14722, 264, 2430, 763, 257, 472, 281, 312, 257, 472, 6915, 24505, 51047, 51047, 295, 2031, 261, 472, 272, 472, 11, 689, 510, 261, 472, 272, 472, 366, 264, 9834, 11, 2171, 611, 1219, 264, 51408, 51408, 17443, 295, 264, 700, 7633, 4583, 13, 51597, 51597], "temperature": 0.0, "avg_logprob": -0.13144803728376117, "compression_ratio": 1.583815028901734, "no_speech_prob": 3.1875194963504327e-06}, {"id": 59, "seek": 29684, "start": 317.71999999999997, "end": 321.5, "text": " weights of the first hidden layer.", "tokens": [50364, 294, 1668, 281, 4445, 2128, 2365, 294, 264, 18161, 3209, 13, 50570, 50570, 18600, 264, 4846, 4122, 2031, 11, 291, 393, 550, 14722, 264, 2430, 763, 257, 472, 281, 312, 257, 472, 6915, 24505, 51047, 51047, 295, 2031, 261, 472, 272, 472, 11, 689, 510, 261, 472, 272, 472, 366, 264, 9834, 11, 2171, 611, 1219, 264, 51408, 51408, 17443, 295, 264, 700, 7633, 4583, 13, 51597, 51597], "temperature": 0.0, "avg_logprob": -0.13144803728376117, "compression_ratio": 1.583815028901734, "no_speech_prob": 3.1875194963504327e-06}, {"id": 60, "seek": 32150, "start": 321.5, "end": 329.6, "text": " Then you can compute a two as dens of now a one, which you just computed above, and", "tokens": [50364, 1396, 291, 393, 14722, 257, 732, 382, 24505, 295, 586, 257, 472, 11, 597, 291, 445, 40610, 3673, 11, 293, 50769, 50769, 261, 732, 272, 732, 11, 597, 366, 264, 9834, 420, 17443, 295, 341, 1150, 7633, 4583, 11, 293, 550, 14722, 51093, 51093, 257, 1045, 293, 257, 1451, 13, 51227, 51227, 400, 498, 341, 307, 257, 18161, 3209, 365, 1451, 7914, 11, 550, 264, 2572, 5598, 283, 295, 2031, 307, 445, 51559, 51559, 2681, 281, 257, 1451, 13, 51629, 51629, 400, 370, 291, 2736, 283, 295, 2031, 13, 51811, 51811], "temperature": 0.0, "avg_logprob": -0.1347949222851825, "compression_ratio": 1.6994818652849741, "no_speech_prob": 1.034848310155212e-06}, {"id": 61, "seek": 32150, "start": 329.6, "end": 336.08, "text": " w two b two, which are the parameters or weights of this second hidden layer, and then compute", "tokens": [50364, 1396, 291, 393, 14722, 257, 732, 382, 24505, 295, 586, 257, 472, 11, 597, 291, 445, 40610, 3673, 11, 293, 50769, 50769, 261, 732, 272, 732, 11, 597, 366, 264, 9834, 420, 17443, 295, 341, 1150, 7633, 4583, 11, 293, 550, 14722, 51093, 51093, 257, 1045, 293, 257, 1451, 13, 51227, 51227, 400, 498, 341, 307, 257, 18161, 3209, 365, 1451, 7914, 11, 550, 264, 2572, 5598, 283, 295, 2031, 307, 445, 51559, 51559, 2681, 281, 257, 1451, 13, 51629, 51629, 400, 370, 291, 2736, 283, 295, 2031, 13, 51811, 51811], "temperature": 0.0, "avg_logprob": -0.1347949222851825, "compression_ratio": 1.6994818652849741, "no_speech_prob": 1.034848310155212e-06}, {"id": 62, "seek": 32150, "start": 336.08, "end": 338.76, "text": " a three and a four.", "tokens": [50364, 1396, 291, 393, 14722, 257, 732, 382, 24505, 295, 586, 257, 472, 11, 597, 291, 445, 40610, 3673, 11, 293, 50769, 50769, 261, 732, 272, 732, 11, 597, 366, 264, 9834, 420, 17443, 295, 341, 1150, 7633, 4583, 11, 293, 550, 14722, 51093, 51093, 257, 1045, 293, 257, 1451, 13, 51227, 51227, 400, 498, 341, 307, 257, 18161, 3209, 365, 1451, 7914, 11, 550, 264, 2572, 5598, 283, 295, 2031, 307, 445, 51559, 51559, 2681, 281, 257, 1451, 13, 51629, 51629, 400, 370, 291, 2736, 283, 295, 2031, 13, 51811, 51811], "temperature": 0.0, "avg_logprob": -0.1347949222851825, "compression_ratio": 1.6994818652849741, "no_speech_prob": 1.034848310155212e-06}, {"id": 63, "seek": 32150, "start": 338.76, "end": 345.4, "text": " And if this is a neural network with four layers, then the final output f of x is just", "tokens": [50364, 1396, 291, 393, 14722, 257, 732, 382, 24505, 295, 586, 257, 472, 11, 597, 291, 445, 40610, 3673, 11, 293, 50769, 50769, 261, 732, 272, 732, 11, 597, 366, 264, 9834, 420, 17443, 295, 341, 1150, 7633, 4583, 11, 293, 550, 14722, 51093, 51093, 257, 1045, 293, 257, 1451, 13, 51227, 51227, 400, 498, 341, 307, 257, 18161, 3209, 365, 1451, 7914, 11, 550, 264, 2572, 5598, 283, 295, 2031, 307, 445, 51559, 51559, 2681, 281, 257, 1451, 13, 51629, 51629, 400, 370, 291, 2736, 283, 295, 2031, 13, 51811, 51811], "temperature": 0.0, "avg_logprob": -0.1347949222851825, "compression_ratio": 1.6994818652849741, "no_speech_prob": 1.034848310155212e-06}, {"id": 64, "seek": 32150, "start": 345.4, "end": 346.8, "text": " equal to a four.", "tokens": [50364, 1396, 291, 393, 14722, 257, 732, 382, 24505, 295, 586, 257, 472, 11, 597, 291, 445, 40610, 3673, 11, 293, 50769, 50769, 261, 732, 272, 732, 11, 597, 366, 264, 9834, 420, 17443, 295, 341, 1150, 7633, 4583, 11, 293, 550, 14722, 51093, 51093, 257, 1045, 293, 257, 1451, 13, 51227, 51227, 400, 498, 341, 307, 257, 18161, 3209, 365, 1451, 7914, 11, 550, 264, 2572, 5598, 283, 295, 2031, 307, 445, 51559, 51559, 2681, 281, 257, 1451, 13, 51629, 51629, 400, 370, 291, 2736, 283, 295, 2031, 13, 51811, 51811], "temperature": 0.0, "avg_logprob": -0.1347949222851825, "compression_ratio": 1.6994818652849741, "no_speech_prob": 1.034848310155212e-06}, {"id": 65, "seek": 32150, "start": 346.8, "end": 350.44, "text": " And so you return f of x.", "tokens": [50364, 1396, 291, 393, 14722, 257, 732, 382, 24505, 295, 586, 257, 472, 11, 597, 291, 445, 40610, 3673, 11, 293, 50769, 50769, 261, 732, 272, 732, 11, 597, 366, 264, 9834, 420, 17443, 295, 341, 1150, 7633, 4583, 11, 293, 550, 14722, 51093, 51093, 257, 1045, 293, 257, 1451, 13, 51227, 51227, 400, 498, 341, 307, 257, 18161, 3209, 365, 1451, 7914, 11, 550, 264, 2572, 5598, 283, 295, 2031, 307, 445, 51559, 51559, 2681, 281, 257, 1451, 13, 51629, 51629, 400, 370, 291, 2736, 283, 295, 2031, 13, 51811, 51811], "temperature": 0.0, "avg_logprob": -0.1347949222851825, "compression_ratio": 1.6994818652849741, "no_speech_prob": 1.034848310155212e-06}, {"id": 66, "seek": 35044, "start": 350.44, "end": 356.28, "text": " Notice that here I'm using a capital W, because one of the notational conventions from linear", "tokens": [50364, 13428, 300, 510, 286, 478, 1228, 257, 4238, 343, 11, 570, 472, 295, 264, 406, 1478, 33520, 490, 8213, 50656, 50656, 21989, 307, 281, 764, 11775, 2869, 651, 420, 257, 4238, 419, 950, 455, 1385, 562, 309, 311, 13761, 281, 257, 8141, 11, 293, 3126, 9765, 50984, 50984, 281, 2864, 281, 18875, 293, 15664, 685, 13, 51138, 51138, 407, 570, 309, 311, 257, 8141, 11, 341, 307, 4238, 343, 13, 51316, 51316, 407, 300, 311, 309, 13, 51366, 51366, 509, 586, 458, 577, 281, 4445, 2128, 2365, 1803, 490, 8459, 13, 51548, 51548, 400, 291, 483, 281, 536, 439, 341, 3089, 293, 1190, 309, 293, 3124, 309, 1803, 294, 264, 3124, 2715, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.1318854052444984, "compression_ratio": 1.7109375, "no_speech_prob": 2.8409027436282486e-05}, {"id": 67, "seek": 35044, "start": 356.28, "end": 362.84, "text": " algebra is to use uppercase or a capital alphabets when it's referring to a matrix, and lowercase", "tokens": [50364, 13428, 300, 510, 286, 478, 1228, 257, 4238, 343, 11, 570, 472, 295, 264, 406, 1478, 33520, 490, 8213, 50656, 50656, 21989, 307, 281, 764, 11775, 2869, 651, 420, 257, 4238, 419, 950, 455, 1385, 562, 309, 311, 13761, 281, 257, 8141, 11, 293, 3126, 9765, 50984, 50984, 281, 2864, 281, 18875, 293, 15664, 685, 13, 51138, 51138, 407, 570, 309, 311, 257, 8141, 11, 341, 307, 4238, 343, 13, 51316, 51316, 407, 300, 311, 309, 13, 51366, 51366, 509, 586, 458, 577, 281, 4445, 2128, 2365, 1803, 490, 8459, 13, 51548, 51548, 400, 291, 483, 281, 536, 439, 341, 3089, 293, 1190, 309, 293, 3124, 309, 1803, 294, 264, 3124, 2715, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.1318854052444984, "compression_ratio": 1.7109375, "no_speech_prob": 2.8409027436282486e-05}, {"id": 68, "seek": 35044, "start": 362.84, "end": 365.92, "text": " to refer to vectors and scalars.", "tokens": [50364, 13428, 300, 510, 286, 478, 1228, 257, 4238, 343, 11, 570, 472, 295, 264, 406, 1478, 33520, 490, 8213, 50656, 50656, 21989, 307, 281, 764, 11775, 2869, 651, 420, 257, 4238, 419, 950, 455, 1385, 562, 309, 311, 13761, 281, 257, 8141, 11, 293, 3126, 9765, 50984, 50984, 281, 2864, 281, 18875, 293, 15664, 685, 13, 51138, 51138, 407, 570, 309, 311, 257, 8141, 11, 341, 307, 4238, 343, 13, 51316, 51316, 407, 300, 311, 309, 13, 51366, 51366, 509, 586, 458, 577, 281, 4445, 2128, 2365, 1803, 490, 8459, 13, 51548, 51548, 400, 291, 483, 281, 536, 439, 341, 3089, 293, 1190, 309, 293, 3124, 309, 1803, 294, 264, 3124, 2715, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.1318854052444984, "compression_ratio": 1.7109375, "no_speech_prob": 2.8409027436282486e-05}, {"id": 69, "seek": 35044, "start": 365.92, "end": 369.48, "text": " So because it's a matrix, this is capital W.", "tokens": [50364, 13428, 300, 510, 286, 478, 1228, 257, 4238, 343, 11, 570, 472, 295, 264, 406, 1478, 33520, 490, 8213, 50656, 50656, 21989, 307, 281, 764, 11775, 2869, 651, 420, 257, 4238, 419, 950, 455, 1385, 562, 309, 311, 13761, 281, 257, 8141, 11, 293, 3126, 9765, 50984, 50984, 281, 2864, 281, 18875, 293, 15664, 685, 13, 51138, 51138, 407, 570, 309, 311, 257, 8141, 11, 341, 307, 4238, 343, 13, 51316, 51316, 407, 300, 311, 309, 13, 51366, 51366, 509, 586, 458, 577, 281, 4445, 2128, 2365, 1803, 490, 8459, 13, 51548, 51548, 400, 291, 483, 281, 536, 439, 341, 3089, 293, 1190, 309, 293, 3124, 309, 1803, 294, 264, 3124, 2715, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.1318854052444984, "compression_ratio": 1.7109375, "no_speech_prob": 2.8409027436282486e-05}, {"id": 70, "seek": 35044, "start": 369.48, "end": 370.48, "text": " So that's it.", "tokens": [50364, 13428, 300, 510, 286, 478, 1228, 257, 4238, 343, 11, 570, 472, 295, 264, 406, 1478, 33520, 490, 8213, 50656, 50656, 21989, 307, 281, 764, 11775, 2869, 651, 420, 257, 4238, 419, 950, 455, 1385, 562, 309, 311, 13761, 281, 257, 8141, 11, 293, 3126, 9765, 50984, 50984, 281, 2864, 281, 18875, 293, 15664, 685, 13, 51138, 51138, 407, 570, 309, 311, 257, 8141, 11, 341, 307, 4238, 343, 13, 51316, 51316, 407, 300, 311, 309, 13, 51366, 51366, 509, 586, 458, 577, 281, 4445, 2128, 2365, 1803, 490, 8459, 13, 51548, 51548, 400, 291, 483, 281, 536, 439, 341, 3089, 293, 1190, 309, 293, 3124, 309, 1803, 294, 264, 3124, 2715, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.1318854052444984, "compression_ratio": 1.7109375, "no_speech_prob": 2.8409027436282486e-05}, {"id": 71, "seek": 35044, "start": 370.48, "end": 374.12, "text": " You now know how to implement forward prop yourself from scratch.", "tokens": [50364, 13428, 300, 510, 286, 478, 1228, 257, 4238, 343, 11, 570, 472, 295, 264, 406, 1478, 33520, 490, 8213, 50656, 50656, 21989, 307, 281, 764, 11775, 2869, 651, 420, 257, 4238, 419, 950, 455, 1385, 562, 309, 311, 13761, 281, 257, 8141, 11, 293, 3126, 9765, 50984, 50984, 281, 2864, 281, 18875, 293, 15664, 685, 13, 51138, 51138, 407, 570, 309, 311, 257, 8141, 11, 341, 307, 4238, 343, 13, 51316, 51316, 407, 300, 311, 309, 13, 51366, 51366, 509, 586, 458, 577, 281, 4445, 2128, 2365, 1803, 490, 8459, 13, 51548, 51548, 400, 291, 483, 281, 536, 439, 341, 3089, 293, 1190, 309, 293, 3124, 309, 1803, 294, 264, 3124, 2715, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.1318854052444984, "compression_ratio": 1.7109375, "no_speech_prob": 2.8409027436282486e-05}, {"id": 72, "seek": 35044, "start": 374.12, "end": 378.48, "text": " And you get to see all this code and run it and practice it yourself in the practice lab", "tokens": [50364, 13428, 300, 510, 286, 478, 1228, 257, 4238, 343, 11, 570, 472, 295, 264, 406, 1478, 33520, 490, 8213, 50656, 50656, 21989, 307, 281, 764, 11775, 2869, 651, 420, 257, 4238, 419, 950, 455, 1385, 562, 309, 311, 13761, 281, 257, 8141, 11, 293, 3126, 9765, 50984, 50984, 281, 2864, 281, 18875, 293, 15664, 685, 13, 51138, 51138, 407, 570, 309, 311, 257, 8141, 11, 341, 307, 4238, 343, 13, 51316, 51316, 407, 300, 311, 309, 13, 51366, 51366, 509, 586, 458, 577, 281, 4445, 2128, 2365, 1803, 490, 8459, 13, 51548, 51548, 400, 291, 483, 281, 536, 439, 341, 3089, 293, 1190, 309, 293, 3124, 309, 1803, 294, 264, 3124, 2715, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.1318854052444984, "compression_ratio": 1.7109375, "no_speech_prob": 2.8409027436282486e-05}, {"id": 73, "seek": 37848, "start": 378.48, "end": 381.20000000000005, "text": " coming after this as well.", "tokens": [50364, 1348, 934, 341, 382, 731, 13, 50500, 50500, 286, 519, 300, 754, 562, 291, 434, 1228, 4005, 15148, 411, 37624, 11, 309, 311, 4961, 281, 50734, 50734, 458, 577, 309, 1985, 833, 264, 13376, 13, 50856, 50856, 1436, 294, 1389, 746, 1709, 2085, 11, 294, 1389, 746, 6676, 534, 5692, 11, 420, 291, 362, 51074, 51074, 257, 5861, 1874, 11, 420, 309, 1542, 411, 456, 311, 257, 7426, 11, 428, 3485, 281, 1223, 437, 311, 767, 51344, 51344, 516, 322, 486, 652, 291, 709, 544, 4942, 562, 45592, 428, 3089, 13, 51552, 51552, 1133, 286, 1190, 3479, 2539, 14642, 11, 257, 688, 295, 264, 565, 11, 11939, 11, 309, 1177, 380, 589, 13, 51781, 51781], "temperature": 0.0, "avg_logprob": -0.08258716876690204, "compression_ratio": 1.6317567567567568, "no_speech_prob": 3.480553277768195e-05}, {"id": 74, "seek": 37848, "start": 381.20000000000005, "end": 385.88, "text": " I think that even when you're using powerful libraries like TensorFlow, it's helpful to", "tokens": [50364, 1348, 934, 341, 382, 731, 13, 50500, 50500, 286, 519, 300, 754, 562, 291, 434, 1228, 4005, 15148, 411, 37624, 11, 309, 311, 4961, 281, 50734, 50734, 458, 577, 309, 1985, 833, 264, 13376, 13, 50856, 50856, 1436, 294, 1389, 746, 1709, 2085, 11, 294, 1389, 746, 6676, 534, 5692, 11, 420, 291, 362, 51074, 51074, 257, 5861, 1874, 11, 420, 309, 1542, 411, 456, 311, 257, 7426, 11, 428, 3485, 281, 1223, 437, 311, 767, 51344, 51344, 516, 322, 486, 652, 291, 709, 544, 4942, 562, 45592, 428, 3089, 13, 51552, 51552, 1133, 286, 1190, 3479, 2539, 14642, 11, 257, 688, 295, 264, 565, 11, 11939, 11, 309, 1177, 380, 589, 13, 51781, 51781], "temperature": 0.0, "avg_logprob": -0.08258716876690204, "compression_ratio": 1.6317567567567568, "no_speech_prob": 3.480553277768195e-05}, {"id": 75, "seek": 37848, "start": 385.88, "end": 388.32, "text": " know how it works under the hood.", "tokens": [50364, 1348, 934, 341, 382, 731, 13, 50500, 50500, 286, 519, 300, 754, 562, 291, 434, 1228, 4005, 15148, 411, 37624, 11, 309, 311, 4961, 281, 50734, 50734, 458, 577, 309, 1985, 833, 264, 13376, 13, 50856, 50856, 1436, 294, 1389, 746, 1709, 2085, 11, 294, 1389, 746, 6676, 534, 5692, 11, 420, 291, 362, 51074, 51074, 257, 5861, 1874, 11, 420, 309, 1542, 411, 456, 311, 257, 7426, 11, 428, 3485, 281, 1223, 437, 311, 767, 51344, 51344, 516, 322, 486, 652, 291, 709, 544, 4942, 562, 45592, 428, 3089, 13, 51552, 51552, 1133, 286, 1190, 3479, 2539, 14642, 11, 257, 688, 295, 264, 565, 11, 11939, 11, 309, 1177, 380, 589, 13, 51781, 51781], "temperature": 0.0, "avg_logprob": -0.08258716876690204, "compression_ratio": 1.6317567567567568, "no_speech_prob": 3.480553277768195e-05}, {"id": 76, "seek": 37848, "start": 388.32, "end": 392.68, "text": " Because in case something goes wrong, in case something runs really slowly, or you have", "tokens": [50364, 1348, 934, 341, 382, 731, 13, 50500, 50500, 286, 519, 300, 754, 562, 291, 434, 1228, 4005, 15148, 411, 37624, 11, 309, 311, 4961, 281, 50734, 50734, 458, 577, 309, 1985, 833, 264, 13376, 13, 50856, 50856, 1436, 294, 1389, 746, 1709, 2085, 11, 294, 1389, 746, 6676, 534, 5692, 11, 420, 291, 362, 51074, 51074, 257, 5861, 1874, 11, 420, 309, 1542, 411, 456, 311, 257, 7426, 11, 428, 3485, 281, 1223, 437, 311, 767, 51344, 51344, 516, 322, 486, 652, 291, 709, 544, 4942, 562, 45592, 428, 3089, 13, 51552, 51552, 1133, 286, 1190, 3479, 2539, 14642, 11, 257, 688, 295, 264, 565, 11, 11939, 11, 309, 1177, 380, 589, 13, 51781, 51781], "temperature": 0.0, "avg_logprob": -0.08258716876690204, "compression_ratio": 1.6317567567567568, "no_speech_prob": 3.480553277768195e-05}, {"id": 77, "seek": 37848, "start": 392.68, "end": 398.08000000000004, "text": " a strange result, or it looks like there's a bug, your ability to understand what's actually", "tokens": [50364, 1348, 934, 341, 382, 731, 13, 50500, 50500, 286, 519, 300, 754, 562, 291, 434, 1228, 4005, 15148, 411, 37624, 11, 309, 311, 4961, 281, 50734, 50734, 458, 577, 309, 1985, 833, 264, 13376, 13, 50856, 50856, 1436, 294, 1389, 746, 1709, 2085, 11, 294, 1389, 746, 6676, 534, 5692, 11, 420, 291, 362, 51074, 51074, 257, 5861, 1874, 11, 420, 309, 1542, 411, 456, 311, 257, 7426, 11, 428, 3485, 281, 1223, 437, 311, 767, 51344, 51344, 516, 322, 486, 652, 291, 709, 544, 4942, 562, 45592, 428, 3089, 13, 51552, 51552, 1133, 286, 1190, 3479, 2539, 14642, 11, 257, 688, 295, 264, 565, 11, 11939, 11, 309, 1177, 380, 589, 13, 51781, 51781], "temperature": 0.0, "avg_logprob": -0.08258716876690204, "compression_ratio": 1.6317567567567568, "no_speech_prob": 3.480553277768195e-05}, {"id": 78, "seek": 37848, "start": 398.08000000000004, "end": 402.24, "text": " going on will make you much more effective when debugging your code.", "tokens": [50364, 1348, 934, 341, 382, 731, 13, 50500, 50500, 286, 519, 300, 754, 562, 291, 434, 1228, 4005, 15148, 411, 37624, 11, 309, 311, 4961, 281, 50734, 50734, 458, 577, 309, 1985, 833, 264, 13376, 13, 50856, 50856, 1436, 294, 1389, 746, 1709, 2085, 11, 294, 1389, 746, 6676, 534, 5692, 11, 420, 291, 362, 51074, 51074, 257, 5861, 1874, 11, 420, 309, 1542, 411, 456, 311, 257, 7426, 11, 428, 3485, 281, 1223, 437, 311, 767, 51344, 51344, 516, 322, 486, 652, 291, 709, 544, 4942, 562, 45592, 428, 3089, 13, 51552, 51552, 1133, 286, 1190, 3479, 2539, 14642, 11, 257, 688, 295, 264, 565, 11, 11939, 11, 309, 1177, 380, 589, 13, 51781, 51781], "temperature": 0.0, "avg_logprob": -0.08258716876690204, "compression_ratio": 1.6317567567567568, "no_speech_prob": 3.480553277768195e-05}, {"id": 79, "seek": 37848, "start": 402.24, "end": 406.82, "text": " When I run machine learning algorithms, a lot of the time, frankly, it doesn't work.", "tokens": [50364, 1348, 934, 341, 382, 731, 13, 50500, 50500, 286, 519, 300, 754, 562, 291, 434, 1228, 4005, 15148, 411, 37624, 11, 309, 311, 4961, 281, 50734, 50734, 458, 577, 309, 1985, 833, 264, 13376, 13, 50856, 50856, 1436, 294, 1389, 746, 1709, 2085, 11, 294, 1389, 746, 6676, 534, 5692, 11, 420, 291, 362, 51074, 51074, 257, 5861, 1874, 11, 420, 309, 1542, 411, 456, 311, 257, 7426, 11, 428, 3485, 281, 1223, 437, 311, 767, 51344, 51344, 516, 322, 486, 652, 291, 709, 544, 4942, 562, 45592, 428, 3089, 13, 51552, 51552, 1133, 286, 1190, 3479, 2539, 14642, 11, 257, 688, 295, 264, 565, 11, 11939, 11, 309, 1177, 380, 589, 13, 51781, 51781], "temperature": 0.0, "avg_logprob": -0.08258716876690204, "compression_ratio": 1.6317567567567568, "no_speech_prob": 3.480553277768195e-05}, {"id": 80, "seek": 40682, "start": 406.82, "end": 408.62, "text": " Certainly not the first time.", "tokens": [50364, 16628, 406, 264, 700, 565, 13, 50454, 50454, 400, 370, 286, 915, 300, 452, 3485, 281, 24083, 452, 3089, 11, 312, 309, 37624, 3089, 420, 746, 1646, 11, 50731, 50731, 307, 534, 1021, 281, 885, 364, 4942, 3479, 2539, 11403, 13, 50941, 50941, 407, 754, 562, 291, 434, 1228, 37624, 420, 512, 661, 8388, 11, 286, 1454, 300, 291, 915, 341, 51211, 51211, 7731, 3701, 4420, 337, 428, 1065, 5821, 293, 337, 45592, 428, 1065, 3479, 2539, 51519, 51519, 14642, 382, 731, 13, 51651, 51651, 407, 300, 311, 309, 13, 51737, 51737], "temperature": 0.0, "avg_logprob": -0.09014687639601687, "compression_ratio": 1.6345381526104417, "no_speech_prob": 4.289184744266095e-06}, {"id": 81, "seek": 40682, "start": 408.62, "end": 414.15999999999997, "text": " And so I find that my ability to debug my code, be it TensorFlow code or something else,", "tokens": [50364, 16628, 406, 264, 700, 565, 13, 50454, 50454, 400, 370, 286, 915, 300, 452, 3485, 281, 24083, 452, 3089, 11, 312, 309, 37624, 3089, 420, 746, 1646, 11, 50731, 50731, 307, 534, 1021, 281, 885, 364, 4942, 3479, 2539, 11403, 13, 50941, 50941, 407, 754, 562, 291, 434, 1228, 37624, 420, 512, 661, 8388, 11, 286, 1454, 300, 291, 915, 341, 51211, 51211, 7731, 3701, 4420, 337, 428, 1065, 5821, 293, 337, 45592, 428, 1065, 3479, 2539, 51519, 51519, 14642, 382, 731, 13, 51651, 51651, 407, 300, 311, 309, 13, 51737, 51737], "temperature": 0.0, "avg_logprob": -0.09014687639601687, "compression_ratio": 1.6345381526104417, "no_speech_prob": 4.289184744266095e-06}, {"id": 82, "seek": 40682, "start": 414.15999999999997, "end": 418.36, "text": " is really important to being an effective machine learning engineer.", "tokens": [50364, 16628, 406, 264, 700, 565, 13, 50454, 50454, 400, 370, 286, 915, 300, 452, 3485, 281, 24083, 452, 3089, 11, 312, 309, 37624, 3089, 420, 746, 1646, 11, 50731, 50731, 307, 534, 1021, 281, 885, 364, 4942, 3479, 2539, 11403, 13, 50941, 50941, 407, 754, 562, 291, 434, 1228, 37624, 420, 512, 661, 8388, 11, 286, 1454, 300, 291, 915, 341, 51211, 51211, 7731, 3701, 4420, 337, 428, 1065, 5821, 293, 337, 45592, 428, 1065, 3479, 2539, 51519, 51519, 14642, 382, 731, 13, 51651, 51651, 407, 300, 311, 309, 13, 51737, 51737], "temperature": 0.0, "avg_logprob": -0.09014687639601687, "compression_ratio": 1.6345381526104417, "no_speech_prob": 4.289184744266095e-06}, {"id": 83, "seek": 40682, "start": 418.36, "end": 423.76, "text": " So even when you're using TensorFlow or some other framework, I hope that you find this", "tokens": [50364, 16628, 406, 264, 700, 565, 13, 50454, 50454, 400, 370, 286, 915, 300, 452, 3485, 281, 24083, 452, 3089, 11, 312, 309, 37624, 3089, 420, 746, 1646, 11, 50731, 50731, 307, 534, 1021, 281, 885, 364, 4942, 3479, 2539, 11403, 13, 50941, 50941, 407, 754, 562, 291, 434, 1228, 37624, 420, 512, 661, 8388, 11, 286, 1454, 300, 291, 915, 341, 51211, 51211, 7731, 3701, 4420, 337, 428, 1065, 5821, 293, 337, 45592, 428, 1065, 3479, 2539, 51519, 51519, 14642, 382, 731, 13, 51651, 51651, 407, 300, 311, 309, 13, 51737, 51737], "temperature": 0.0, "avg_logprob": -0.09014687639601687, "compression_ratio": 1.6345381526104417, "no_speech_prob": 4.289184744266095e-06}, {"id": 84, "seek": 40682, "start": 423.76, "end": 429.92, "text": " deeper understanding useful for your own applications and for debugging your own machine learning", "tokens": [50364, 16628, 406, 264, 700, 565, 13, 50454, 50454, 400, 370, 286, 915, 300, 452, 3485, 281, 24083, 452, 3089, 11, 312, 309, 37624, 3089, 420, 746, 1646, 11, 50731, 50731, 307, 534, 1021, 281, 885, 364, 4942, 3479, 2539, 11403, 13, 50941, 50941, 407, 754, 562, 291, 434, 1228, 37624, 420, 512, 661, 8388, 11, 286, 1454, 300, 291, 915, 341, 51211, 51211, 7731, 3701, 4420, 337, 428, 1065, 5821, 293, 337, 45592, 428, 1065, 3479, 2539, 51519, 51519, 14642, 382, 731, 13, 51651, 51651, 407, 300, 311, 309, 13, 51737, 51737], "temperature": 0.0, "avg_logprob": -0.09014687639601687, "compression_ratio": 1.6345381526104417, "no_speech_prob": 4.289184744266095e-06}, {"id": 85, "seek": 40682, "start": 429.92, "end": 432.56, "text": " algorithms as well.", "tokens": [50364, 16628, 406, 264, 700, 565, 13, 50454, 50454, 400, 370, 286, 915, 300, 452, 3485, 281, 24083, 452, 3089, 11, 312, 309, 37624, 3089, 420, 746, 1646, 11, 50731, 50731, 307, 534, 1021, 281, 885, 364, 4942, 3479, 2539, 11403, 13, 50941, 50941, 407, 754, 562, 291, 434, 1228, 37624, 420, 512, 661, 8388, 11, 286, 1454, 300, 291, 915, 341, 51211, 51211, 7731, 3701, 4420, 337, 428, 1065, 5821, 293, 337, 45592, 428, 1065, 3479, 2539, 51519, 51519, 14642, 382, 731, 13, 51651, 51651, 407, 300, 311, 309, 13, 51737, 51737], "temperature": 0.0, "avg_logprob": -0.09014687639601687, "compression_ratio": 1.6345381526104417, "no_speech_prob": 4.289184744266095e-06}, {"id": 86, "seek": 40682, "start": 432.56, "end": 434.28, "text": " So that's it.", "tokens": [50364, 16628, 406, 264, 700, 565, 13, 50454, 50454, 400, 370, 286, 915, 300, 452, 3485, 281, 24083, 452, 3089, 11, 312, 309, 37624, 3089, 420, 746, 1646, 11, 50731, 50731, 307, 534, 1021, 281, 885, 364, 4942, 3479, 2539, 11403, 13, 50941, 50941, 407, 754, 562, 291, 434, 1228, 37624, 420, 512, 661, 8388, 11, 286, 1454, 300, 291, 915, 341, 51211, 51211, 7731, 3701, 4420, 337, 428, 1065, 5821, 293, 337, 45592, 428, 1065, 3479, 2539, 51519, 51519, 14642, 382, 731, 13, 51651, 51651, 407, 300, 311, 309, 13, 51737, 51737], "temperature": 0.0, "avg_logprob": -0.09014687639601687, "compression_ratio": 1.6345381526104417, "no_speech_prob": 4.289184744266095e-06}, {"id": 87, "seek": 43428, "start": 434.28, "end": 438.08, "text": " That's the last required video of this week with code in it.", "tokens": [50364, 663, 311, 264, 1036, 4739, 960, 295, 341, 1243, 365, 3089, 294, 309, 13, 50554, 50554, 682, 264, 958, 960, 11, 286, 1116, 411, 281, 9192, 666, 437, 286, 519, 307, 257, 1019, 293, 10343, 4829, 11, 597, 50778, 50778, 307, 437, 307, 264, 2480, 1296, 18161, 9590, 293, 7318, 420, 316, 26252, 11, 11677, 2674, 51099, 51099, 7599, 30, 51182, 51182, 639, 307, 257, 17323, 4829, 11, 457, 570, 309, 311, 668, 370, 13371, 7152, 11, 286, 528, 281, 2073, 51408, 51408, 365, 291, 512, 4598, 322, 341, 13, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.09380708856785551, "compression_ratio": 1.5518672199170125, "no_speech_prob": 1.8628736143000424e-05}, {"id": 88, "seek": 43428, "start": 438.08, "end": 442.55999999999995, "text": " In the next video, I'd like to dive into what I think is a fun and fascinating topic, which", "tokens": [50364, 663, 311, 264, 1036, 4739, 960, 295, 341, 1243, 365, 3089, 294, 309, 13, 50554, 50554, 682, 264, 958, 960, 11, 286, 1116, 411, 281, 9192, 666, 437, 286, 519, 307, 257, 1019, 293, 10343, 4829, 11, 597, 50778, 50778, 307, 437, 307, 264, 2480, 1296, 18161, 9590, 293, 7318, 420, 316, 26252, 11, 11677, 2674, 51099, 51099, 7599, 30, 51182, 51182, 639, 307, 257, 17323, 4829, 11, 457, 570, 309, 311, 668, 370, 13371, 7152, 11, 286, 528, 281, 2073, 51408, 51408, 365, 291, 512, 4598, 322, 341, 13, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.09380708856785551, "compression_ratio": 1.5518672199170125, "no_speech_prob": 1.8628736143000424e-05}, {"id": 89, "seek": 43428, "start": 442.55999999999995, "end": 448.97999999999996, "text": " is what is the relationship between neural networks and AI or AGI, artificial general", "tokens": [50364, 663, 311, 264, 1036, 4739, 960, 295, 341, 1243, 365, 3089, 294, 309, 13, 50554, 50554, 682, 264, 958, 960, 11, 286, 1116, 411, 281, 9192, 666, 437, 286, 519, 307, 257, 1019, 293, 10343, 4829, 11, 597, 50778, 50778, 307, 437, 307, 264, 2480, 1296, 18161, 9590, 293, 7318, 420, 316, 26252, 11, 11677, 2674, 51099, 51099, 7599, 30, 51182, 51182, 639, 307, 257, 17323, 4829, 11, 457, 570, 309, 311, 668, 370, 13371, 7152, 11, 286, 528, 281, 2073, 51408, 51408, 365, 291, 512, 4598, 322, 341, 13, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.09380708856785551, "compression_ratio": 1.5518672199170125, "no_speech_prob": 1.8628736143000424e-05}, {"id": 90, "seek": 43428, "start": 448.97999999999996, "end": 450.64, "text": " intelligence?", "tokens": [50364, 663, 311, 264, 1036, 4739, 960, 295, 341, 1243, 365, 3089, 294, 309, 13, 50554, 50554, 682, 264, 958, 960, 11, 286, 1116, 411, 281, 9192, 666, 437, 286, 519, 307, 257, 1019, 293, 10343, 4829, 11, 597, 50778, 50778, 307, 437, 307, 264, 2480, 1296, 18161, 9590, 293, 7318, 420, 316, 26252, 11, 11677, 2674, 51099, 51099, 7599, 30, 51182, 51182, 639, 307, 257, 17323, 4829, 11, 457, 570, 309, 311, 668, 370, 13371, 7152, 11, 286, 528, 281, 2073, 51408, 51408, 365, 291, 512, 4598, 322, 341, 13, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.09380708856785551, "compression_ratio": 1.5518672199170125, "no_speech_prob": 1.8628736143000424e-05}, {"id": 91, "seek": 43428, "start": 450.64, "end": 455.15999999999997, "text": " This is a controversial topic, but because it's been so widely discussed, I want to share", "tokens": [50364, 663, 311, 264, 1036, 4739, 960, 295, 341, 1243, 365, 3089, 294, 309, 13, 50554, 50554, 682, 264, 958, 960, 11, 286, 1116, 411, 281, 9192, 666, 437, 286, 519, 307, 257, 1019, 293, 10343, 4829, 11, 597, 50778, 50778, 307, 437, 307, 264, 2480, 1296, 18161, 9590, 293, 7318, 420, 316, 26252, 11, 11677, 2674, 51099, 51099, 7599, 30, 51182, 51182, 639, 307, 257, 17323, 4829, 11, 457, 570, 309, 311, 668, 370, 13371, 7152, 11, 286, 528, 281, 2073, 51408, 51408, 365, 291, 512, 4598, 322, 341, 13, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.09380708856785551, "compression_ratio": 1.5518672199170125, "no_speech_prob": 1.8628736143000424e-05}, {"id": 92, "seek": 43428, "start": 455.15999999999997, "end": 457.59999999999997, "text": " with you some thoughts on this.", "tokens": [50364, 663, 311, 264, 1036, 4739, 960, 295, 341, 1243, 365, 3089, 294, 309, 13, 50554, 50554, 682, 264, 958, 960, 11, 286, 1116, 411, 281, 9192, 666, 437, 286, 519, 307, 257, 1019, 293, 10343, 4829, 11, 597, 50778, 50778, 307, 437, 307, 264, 2480, 1296, 18161, 9590, 293, 7318, 420, 316, 26252, 11, 11677, 2674, 51099, 51099, 7599, 30, 51182, 51182, 639, 307, 257, 17323, 4829, 11, 457, 570, 309, 311, 668, 370, 13371, 7152, 11, 286, 528, 281, 2073, 51408, 51408, 365, 291, 512, 4598, 322, 341, 13, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.09380708856785551, "compression_ratio": 1.5518672199170125, "no_speech_prob": 1.8628736143000424e-05}, {"id": 93, "seek": 45760, "start": 457.6, "end": 464.54, "text": " So when you are asked, are neural networks at all on the path to human level intelligence,", "tokens": [50364, 407, 562, 291, 366, 2351, 11, 366, 18161, 9590, 412, 439, 322, 264, 3100, 281, 1952, 1496, 7599, 11, 50711, 50711, 291, 362, 257, 8388, 337, 1953, 466, 300, 1168, 13, 50886, 50886, 961, 311, 352, 747, 257, 574, 412, 300, 1019, 4829, 11, 286, 519, 11, 294, 264, 958, 960, 13, 51060], "temperature": 0.0, "avg_logprob": -0.12668848037719727, "compression_ratio": 1.392156862745098, "no_speech_prob": 7.179203294072067e-06}, {"id": 94, "seek": 45760, "start": 464.54, "end": 468.04, "text": " you have a framework for thinking about that question.", "tokens": [50364, 407, 562, 291, 366, 2351, 11, 366, 18161, 9590, 412, 439, 322, 264, 3100, 281, 1952, 1496, 7599, 11, 50711, 50711, 291, 362, 257, 8388, 337, 1953, 466, 300, 1168, 13, 50886, 50886, 961, 311, 352, 747, 257, 574, 412, 300, 1019, 4829, 11, 286, 519, 11, 294, 264, 958, 960, 13, 51060], "temperature": 0.0, "avg_logprob": -0.12668848037719727, "compression_ratio": 1.392156862745098, "no_speech_prob": 7.179203294072067e-06}, {"id": 95, "seek": 46804, "start": 468.04, "end": 488.44, "text": " Let's go take a look at that fun topic, I think, in the next video.", "tokens": [50364, 961, 311, 352, 747, 257, 574, 412, 300, 1019, 4829, 11, 286, 519, 11, 294, 264, 958, 960, 13, 51384], "temperature": 0.0, "avg_logprob": -0.27772758223793725, "compression_ratio": 0.9571428571428572, "no_speech_prob": 3.1121944630285725e-05}], "language": "en", "video_id": "snDJExVtMMQ", "entity": "ML Specialization, Andrew Ng (2022)"}}