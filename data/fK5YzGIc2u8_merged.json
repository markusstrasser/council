{"video_id": "fK5YzGIc2u8", "title": "5.5 Activation Functions | Why do we need activation functions? --[Machine Learning | Andrew Ng]", "description": "Second Course:\nAdvanced Learning Algorithms.\n\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 331, "views": 84, "publish_date": "11/04/2022", "timestamp": 1661472000, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " Let's take a look at why neural networks need activation functions and why they just don't work if we were to use the linear activation function in every neuron in the neural network. Recall this demand prediction example. What would happen if we were to use a linear activation function for all of the nodes in this neural network? It turns out that this big neural network will become no different than just linear regression. And so this would defeat the entire purpose of using a neural network because it would then just not be able to fit anything more complex than the linear regression model that we learned about in the first course. Let's illustrate this with a simpler example. Let's look at the example of a neural network where the input x is just a number and we have one hidden unit with parameters w1 and b1 that outputs a1, which is here just a number. And then the second layer is the output layer and it has also just one output unit with parameters w2 and b2 and that outputs a2, which is also just a number, just a scalar, which is the output of the neural network f of x. Let's see what this neural network would do if we were to use the linear activation function g of z equals z everywhere. So to compute a1 as a function of x, the neural network would use a1 equals g of w1 times x plus b1, but g of z is equal to z, so this is just w1 times x plus b1. Then a2 is equal to w2 times a1 plus b2 because g of z equals z. And let me take this expression for a1 and substitute it in there. So that becomes w2 times w1 x plus b1 plus b2. And if we simplify, this becomes w2 w1 times x plus w2 b1 plus b2. And it turns out that if I were to set w equals w2 times w1 and set b equals this quantity over here, then what we've just shown is that a2 is equal to wx plus b. So w is just a linear function of the input x. And rather than using a neural network with one hidden layer and one output layer, we might as well have just used a linear regression model. If you're familiar with linear algebra, this result comes from the fact that a linear function of a linear function is itself a linear function. And this is why having multiple layers in a neural network doesn't let the neural network compute any more complex features or learn anything more complex than just a linear function. So in the general case, if you had a neural network with multiple layers like this, and say you were to use a linear activation function for all the hidden layers, and also use a linear activation function for the output layer, then it turns out this model will compute an output that is completely equivalent to linear regression. The output a4 can be expressed as a linear function of the input features x plus b. Or alternatively, if we were to still use a linear activation function for all the hidden layers, for these three hidden layers here, but we were to use a logistic activation function for the output layer, then it turns out you can show that this model becomes equivalent to logistic regression. And a4 in this case can be expressed as 1 over 1 plus e to the negative wx plus b for some values of w and b. And so this big neural network doesn't do anything that you can't also do with logistic regression. That's why a common rule of thumb is don't use the linear activation function in the hidden layers of your neural network. And in fact, I recommend typically using the ReLU activation function should do just fine. So that's why a neural network needs activation functions other than just the linear activation function everywhere. So far, you've learned to build neural networks for binary classification problems, where y is either 0 or 1, as well as for regression problems, where y can take negative or positive values or maybe just positive and non-negative values. In the next video, I'd like to share with you a generalization of what you've seen so far for classification. In particular, when y doesn't just take on two values, but may take on three or four or 10 or even more categorical values. Let's take a look at how you can build a neural network for that type of classification problem.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.700000000000001, "text": " Let's take a look at why neural networks need activation functions and why they just don't", "tokens": [50364, 961, 311, 747, 257, 574, 412, 983, 18161, 9590, 643, 24433, 6828, 293, 983, 436, 445, 500, 380, 50799, 50799, 589, 498, 321, 645, 281, 764, 264, 8213, 24433, 2445, 294, 633, 34090, 294, 264, 18161, 3209, 13, 51157, 51157, 9647, 336, 341, 4733, 17630, 1365, 13, 51317, 51317, 708, 576, 1051, 498, 321, 645, 281, 764, 257, 8213, 24433, 2445, 337, 439, 295, 264, 13891, 294, 51605, 51605, 341, 18161, 3209, 30, 51677, 51677], "temperature": 0.0, "avg_logprob": -0.12152879888361151, "compression_ratio": 1.8757062146892656, "no_speech_prob": 0.020952213555574417}, {"id": 1, "seek": 0, "start": 8.700000000000001, "end": 15.860000000000001, "text": " work if we were to use the linear activation function in every neuron in the neural network.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 983, 18161, 9590, 643, 24433, 6828, 293, 983, 436, 445, 500, 380, 50799, 50799, 589, 498, 321, 645, 281, 764, 264, 8213, 24433, 2445, 294, 633, 34090, 294, 264, 18161, 3209, 13, 51157, 51157, 9647, 336, 341, 4733, 17630, 1365, 13, 51317, 51317, 708, 576, 1051, 498, 321, 645, 281, 764, 257, 8213, 24433, 2445, 337, 439, 295, 264, 13891, 294, 51605, 51605, 341, 18161, 3209, 30, 51677, 51677], "temperature": 0.0, "avg_logprob": -0.12152879888361151, "compression_ratio": 1.8757062146892656, "no_speech_prob": 0.020952213555574417}, {"id": 2, "seek": 0, "start": 15.860000000000001, "end": 19.06, "text": " Recall this demand prediction example.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 983, 18161, 9590, 643, 24433, 6828, 293, 983, 436, 445, 500, 380, 50799, 50799, 589, 498, 321, 645, 281, 764, 264, 8213, 24433, 2445, 294, 633, 34090, 294, 264, 18161, 3209, 13, 51157, 51157, 9647, 336, 341, 4733, 17630, 1365, 13, 51317, 51317, 708, 576, 1051, 498, 321, 645, 281, 764, 257, 8213, 24433, 2445, 337, 439, 295, 264, 13891, 294, 51605, 51605, 341, 18161, 3209, 30, 51677, 51677], "temperature": 0.0, "avg_logprob": -0.12152879888361151, "compression_ratio": 1.8757062146892656, "no_speech_prob": 0.020952213555574417}, {"id": 3, "seek": 0, "start": 19.06, "end": 24.82, "text": " What would happen if we were to use a linear activation function for all of the nodes in", "tokens": [50364, 961, 311, 747, 257, 574, 412, 983, 18161, 9590, 643, 24433, 6828, 293, 983, 436, 445, 500, 380, 50799, 50799, 589, 498, 321, 645, 281, 764, 264, 8213, 24433, 2445, 294, 633, 34090, 294, 264, 18161, 3209, 13, 51157, 51157, 9647, 336, 341, 4733, 17630, 1365, 13, 51317, 51317, 708, 576, 1051, 498, 321, 645, 281, 764, 257, 8213, 24433, 2445, 337, 439, 295, 264, 13891, 294, 51605, 51605, 341, 18161, 3209, 30, 51677, 51677], "temperature": 0.0, "avg_logprob": -0.12152879888361151, "compression_ratio": 1.8757062146892656, "no_speech_prob": 0.020952213555574417}, {"id": 4, "seek": 0, "start": 24.82, "end": 26.26, "text": " this neural network?", "tokens": [50364, 961, 311, 747, 257, 574, 412, 983, 18161, 9590, 643, 24433, 6828, 293, 983, 436, 445, 500, 380, 50799, 50799, 589, 498, 321, 645, 281, 764, 264, 8213, 24433, 2445, 294, 633, 34090, 294, 264, 18161, 3209, 13, 51157, 51157, 9647, 336, 341, 4733, 17630, 1365, 13, 51317, 51317, 708, 576, 1051, 498, 321, 645, 281, 764, 257, 8213, 24433, 2445, 337, 439, 295, 264, 13891, 294, 51605, 51605, 341, 18161, 3209, 30, 51677, 51677], "temperature": 0.0, "avg_logprob": -0.12152879888361151, "compression_ratio": 1.8757062146892656, "no_speech_prob": 0.020952213555574417}, {"id": 5, "seek": 2626, "start": 26.26, "end": 31.44, "text": " It turns out that this big neural network will become no different than just linear", "tokens": [50364, 467, 4523, 484, 300, 341, 955, 18161, 3209, 486, 1813, 572, 819, 813, 445, 8213, 50623, 50623, 24590, 13, 50692, 50692, 400, 370, 341, 576, 11785, 264, 2302, 4334, 295, 1228, 257, 18161, 3209, 570, 309, 576, 50937, 50937, 550, 445, 406, 312, 1075, 281, 3318, 1340, 544, 3997, 813, 264, 8213, 24590, 2316, 300, 51253, 51253, 321, 3264, 466, 294, 264, 700, 1164, 13, 51400, 51400, 961, 311, 23221, 341, 365, 257, 18587, 1365, 13, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.14727551937103273, "compression_ratio": 1.6985645933014355, "no_speech_prob": 6.0487695918709505e-06}, {"id": 6, "seek": 2626, "start": 31.44, "end": 32.82, "text": " regression.", "tokens": [50364, 467, 4523, 484, 300, 341, 955, 18161, 3209, 486, 1813, 572, 819, 813, 445, 8213, 50623, 50623, 24590, 13, 50692, 50692, 400, 370, 341, 576, 11785, 264, 2302, 4334, 295, 1228, 257, 18161, 3209, 570, 309, 576, 50937, 50937, 550, 445, 406, 312, 1075, 281, 3318, 1340, 544, 3997, 813, 264, 8213, 24590, 2316, 300, 51253, 51253, 321, 3264, 466, 294, 264, 700, 1164, 13, 51400, 51400, 961, 311, 23221, 341, 365, 257, 18587, 1365, 13, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.14727551937103273, "compression_ratio": 1.6985645933014355, "no_speech_prob": 6.0487695918709505e-06}, {"id": 7, "seek": 2626, "start": 32.82, "end": 37.72, "text": " And so this would defeat the entire purpose of using a neural network because it would", "tokens": [50364, 467, 4523, 484, 300, 341, 955, 18161, 3209, 486, 1813, 572, 819, 813, 445, 8213, 50623, 50623, 24590, 13, 50692, 50692, 400, 370, 341, 576, 11785, 264, 2302, 4334, 295, 1228, 257, 18161, 3209, 570, 309, 576, 50937, 50937, 550, 445, 406, 312, 1075, 281, 3318, 1340, 544, 3997, 813, 264, 8213, 24590, 2316, 300, 51253, 51253, 321, 3264, 466, 294, 264, 700, 1164, 13, 51400, 51400, 961, 311, 23221, 341, 365, 257, 18587, 1365, 13, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.14727551937103273, "compression_ratio": 1.6985645933014355, "no_speech_prob": 6.0487695918709505e-06}, {"id": 8, "seek": 2626, "start": 37.72, "end": 44.040000000000006, "text": " then just not be able to fit anything more complex than the linear regression model that", "tokens": [50364, 467, 4523, 484, 300, 341, 955, 18161, 3209, 486, 1813, 572, 819, 813, 445, 8213, 50623, 50623, 24590, 13, 50692, 50692, 400, 370, 341, 576, 11785, 264, 2302, 4334, 295, 1228, 257, 18161, 3209, 570, 309, 576, 50937, 50937, 550, 445, 406, 312, 1075, 281, 3318, 1340, 544, 3997, 813, 264, 8213, 24590, 2316, 300, 51253, 51253, 321, 3264, 466, 294, 264, 700, 1164, 13, 51400, 51400, 961, 311, 23221, 341, 365, 257, 18587, 1365, 13, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.14727551937103273, "compression_ratio": 1.6985645933014355, "no_speech_prob": 6.0487695918709505e-06}, {"id": 9, "seek": 2626, "start": 44.040000000000006, "end": 46.980000000000004, "text": " we learned about in the first course.", "tokens": [50364, 467, 4523, 484, 300, 341, 955, 18161, 3209, 486, 1813, 572, 819, 813, 445, 8213, 50623, 50623, 24590, 13, 50692, 50692, 400, 370, 341, 576, 11785, 264, 2302, 4334, 295, 1228, 257, 18161, 3209, 570, 309, 576, 50937, 50937, 550, 445, 406, 312, 1075, 281, 3318, 1340, 544, 3997, 813, 264, 8213, 24590, 2316, 300, 51253, 51253, 321, 3264, 466, 294, 264, 700, 1164, 13, 51400, 51400, 961, 311, 23221, 341, 365, 257, 18587, 1365, 13, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.14727551937103273, "compression_ratio": 1.6985645933014355, "no_speech_prob": 6.0487695918709505e-06}, {"id": 10, "seek": 2626, "start": 46.980000000000004, "end": 51.44, "text": " Let's illustrate this with a simpler example.", "tokens": [50364, 467, 4523, 484, 300, 341, 955, 18161, 3209, 486, 1813, 572, 819, 813, 445, 8213, 50623, 50623, 24590, 13, 50692, 50692, 400, 370, 341, 576, 11785, 264, 2302, 4334, 295, 1228, 257, 18161, 3209, 570, 309, 576, 50937, 50937, 550, 445, 406, 312, 1075, 281, 3318, 1340, 544, 3997, 813, 264, 8213, 24590, 2316, 300, 51253, 51253, 321, 3264, 466, 294, 264, 700, 1164, 13, 51400, 51400, 961, 311, 23221, 341, 365, 257, 18587, 1365, 13, 51623, 51623], "temperature": 0.0, "avg_logprob": -0.14727551937103273, "compression_ratio": 1.6985645933014355, "no_speech_prob": 6.0487695918709505e-06}, {"id": 11, "seek": 5144, "start": 51.44, "end": 57.08, "text": " Let's look at the example of a neural network where the input x is just a number and we", "tokens": [50364, 961, 311, 574, 412, 264, 1365, 295, 257, 18161, 3209, 689, 264, 4846, 2031, 307, 445, 257, 1230, 293, 321, 50646, 50646, 362, 472, 7633, 4985, 365, 9834, 261, 16, 293, 272, 16, 300, 23930, 257, 16, 11, 597, 307, 510, 445, 257, 1230, 13, 51219, 51219, 400, 550, 264, 1150, 4583, 307, 264, 5598, 4583, 293, 309, 575, 611, 445, 472, 5598, 4985, 365, 51539, 51539], "temperature": 0.0, "avg_logprob": -0.11305507715197577, "compression_ratio": 1.5857988165680474, "no_speech_prob": 8.267655175586697e-06}, {"id": 12, "seek": 5144, "start": 57.08, "end": 68.53999999999999, "text": " have one hidden unit with parameters w1 and b1 that outputs a1, which is here just a number.", "tokens": [50364, 961, 311, 574, 412, 264, 1365, 295, 257, 18161, 3209, 689, 264, 4846, 2031, 307, 445, 257, 1230, 293, 321, 50646, 50646, 362, 472, 7633, 4985, 365, 9834, 261, 16, 293, 272, 16, 300, 23930, 257, 16, 11, 597, 307, 510, 445, 257, 1230, 13, 51219, 51219, 400, 550, 264, 1150, 4583, 307, 264, 5598, 4583, 293, 309, 575, 611, 445, 472, 5598, 4985, 365, 51539, 51539], "temperature": 0.0, "avg_logprob": -0.11305507715197577, "compression_ratio": 1.5857988165680474, "no_speech_prob": 8.267655175586697e-06}, {"id": 13, "seek": 5144, "start": 68.53999999999999, "end": 74.94, "text": " And then the second layer is the output layer and it has also just one output unit with", "tokens": [50364, 961, 311, 574, 412, 264, 1365, 295, 257, 18161, 3209, 689, 264, 4846, 2031, 307, 445, 257, 1230, 293, 321, 50646, 50646, 362, 472, 7633, 4985, 365, 9834, 261, 16, 293, 272, 16, 300, 23930, 257, 16, 11, 597, 307, 510, 445, 257, 1230, 13, 51219, 51219, 400, 550, 264, 1150, 4583, 307, 264, 5598, 4583, 293, 309, 575, 611, 445, 472, 5598, 4985, 365, 51539, 51539], "temperature": 0.0, "avg_logprob": -0.11305507715197577, "compression_ratio": 1.5857988165680474, "no_speech_prob": 8.267655175586697e-06}, {"id": 14, "seek": 7494, "start": 74.94, "end": 81.72, "text": " parameters w2 and b2 and that outputs a2, which is also just a number, just a scalar,", "tokens": [50364, 9834, 261, 17, 293, 272, 17, 293, 300, 23930, 257, 17, 11, 597, 307, 611, 445, 257, 1230, 11, 445, 257, 39684, 11, 50703, 50703, 597, 307, 264, 5598, 295, 264, 18161, 3209, 283, 295, 2031, 13, 50895, 50895, 961, 311, 536, 437, 341, 18161, 3209, 576, 360, 498, 321, 645, 281, 764, 264, 8213, 24433, 2445, 51157, 51157, 290, 295, 710, 6915, 710, 5315, 13, 51385, 51385], "temperature": 0.0, "avg_logprob": -0.11742401123046875, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.8448101855028654e-06}, {"id": 15, "seek": 7494, "start": 81.72, "end": 85.56, "text": " which is the output of the neural network f of x.", "tokens": [50364, 9834, 261, 17, 293, 272, 17, 293, 300, 23930, 257, 17, 11, 597, 307, 611, 445, 257, 1230, 11, 445, 257, 39684, 11, 50703, 50703, 597, 307, 264, 5598, 295, 264, 18161, 3209, 283, 295, 2031, 13, 50895, 50895, 961, 311, 536, 437, 341, 18161, 3209, 576, 360, 498, 321, 645, 281, 764, 264, 8213, 24433, 2445, 51157, 51157, 290, 295, 710, 6915, 710, 5315, 13, 51385, 51385], "temperature": 0.0, "avg_logprob": -0.11742401123046875, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.8448101855028654e-06}, {"id": 16, "seek": 7494, "start": 85.56, "end": 90.8, "text": " Let's see what this neural network would do if we were to use the linear activation function", "tokens": [50364, 9834, 261, 17, 293, 272, 17, 293, 300, 23930, 257, 17, 11, 597, 307, 611, 445, 257, 1230, 11, 445, 257, 39684, 11, 50703, 50703, 597, 307, 264, 5598, 295, 264, 18161, 3209, 283, 295, 2031, 13, 50895, 50895, 961, 311, 536, 437, 341, 18161, 3209, 576, 360, 498, 321, 645, 281, 764, 264, 8213, 24433, 2445, 51157, 51157, 290, 295, 710, 6915, 710, 5315, 13, 51385, 51385], "temperature": 0.0, "avg_logprob": -0.11742401123046875, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.8448101855028654e-06}, {"id": 17, "seek": 7494, "start": 90.8, "end": 95.36, "text": " g of z equals z everywhere.", "tokens": [50364, 9834, 261, 17, 293, 272, 17, 293, 300, 23930, 257, 17, 11, 597, 307, 611, 445, 257, 1230, 11, 445, 257, 39684, 11, 50703, 50703, 597, 307, 264, 5598, 295, 264, 18161, 3209, 283, 295, 2031, 13, 50895, 50895, 961, 311, 536, 437, 341, 18161, 3209, 576, 360, 498, 321, 645, 281, 764, 264, 8213, 24433, 2445, 51157, 51157, 290, 295, 710, 6915, 710, 5315, 13, 51385, 51385], "temperature": 0.0, "avg_logprob": -0.11742401123046875, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.8448101855028654e-06}, {"id": 18, "seek": 9536, "start": 95.36, "end": 105.2, "text": " So to compute a1 as a function of x, the neural network would use a1 equals g of w1 times", "tokens": [50364, 407, 281, 14722, 257, 16, 382, 257, 2445, 295, 2031, 11, 264, 18161, 3209, 576, 764, 257, 16, 6915, 290, 295, 261, 16, 1413, 50856, 50856, 2031, 1804, 272, 16, 11, 457, 290, 295, 710, 307, 2681, 281, 710, 11, 370, 341, 307, 445, 261, 16, 1413, 2031, 1804, 272, 16, 13, 51372, 51372, 1396, 257, 17, 307, 2681, 281, 261, 17, 1413, 257, 16, 1804, 272, 17, 570, 290, 295, 710, 6915, 710, 13, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.10815473146076444, "compression_ratio": 1.5874125874125875, "no_speech_prob": 2.2602978333452484e-06}, {"id": 19, "seek": 9536, "start": 105.2, "end": 115.52, "text": " x plus b1, but g of z is equal to z, so this is just w1 times x plus b1.", "tokens": [50364, 407, 281, 14722, 257, 16, 382, 257, 2445, 295, 2031, 11, 264, 18161, 3209, 576, 764, 257, 16, 6915, 290, 295, 261, 16, 1413, 50856, 50856, 2031, 1804, 272, 16, 11, 457, 290, 295, 710, 307, 2681, 281, 710, 11, 370, 341, 307, 445, 261, 16, 1413, 2031, 1804, 272, 16, 13, 51372, 51372, 1396, 257, 17, 307, 2681, 281, 261, 17, 1413, 257, 16, 1804, 272, 17, 570, 290, 295, 710, 6915, 710, 13, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.10815473146076444, "compression_ratio": 1.5874125874125875, "no_speech_prob": 2.2602978333452484e-06}, {"id": 20, "seek": 9536, "start": 115.52, "end": 123.96000000000001, "text": " Then a2 is equal to w2 times a1 plus b2 because g of z equals z.", "tokens": [50364, 407, 281, 14722, 257, 16, 382, 257, 2445, 295, 2031, 11, 264, 18161, 3209, 576, 764, 257, 16, 6915, 290, 295, 261, 16, 1413, 50856, 50856, 2031, 1804, 272, 16, 11, 457, 290, 295, 710, 307, 2681, 281, 710, 11, 370, 341, 307, 445, 261, 16, 1413, 2031, 1804, 272, 16, 13, 51372, 51372, 1396, 257, 17, 307, 2681, 281, 261, 17, 1413, 257, 16, 1804, 272, 17, 570, 290, 295, 710, 6915, 710, 13, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.10815473146076444, "compression_ratio": 1.5874125874125875, "no_speech_prob": 2.2602978333452484e-06}, {"id": 21, "seek": 12396, "start": 123.96, "end": 129.68, "text": " And let me take this expression for a1 and substitute it in there.", "tokens": [50364, 400, 718, 385, 747, 341, 6114, 337, 257, 16, 293, 15802, 309, 294, 456, 13, 50650, 50650, 407, 300, 3643, 261, 17, 1413, 261, 16, 2031, 1804, 272, 16, 1804, 272, 17, 13, 51161, 51161, 400, 498, 321, 20460, 11, 341, 3643, 261, 17, 261, 16, 1413, 2031, 1804, 261, 17, 272, 16, 1804, 272, 17, 13, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.09180255014388287, "compression_ratio": 1.4516129032258065, "no_speech_prob": 8.397820238315035e-06}, {"id": 22, "seek": 12396, "start": 129.68, "end": 139.9, "text": " So that becomes w2 times w1 x plus b1 plus b2.", "tokens": [50364, 400, 718, 385, 747, 341, 6114, 337, 257, 16, 293, 15802, 309, 294, 456, 13, 50650, 50650, 407, 300, 3643, 261, 17, 1413, 261, 16, 2031, 1804, 272, 16, 1804, 272, 17, 13, 51161, 51161, 400, 498, 321, 20460, 11, 341, 3643, 261, 17, 261, 16, 1413, 2031, 1804, 261, 17, 272, 16, 1804, 272, 17, 13, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.09180255014388287, "compression_ratio": 1.4516129032258065, "no_speech_prob": 8.397820238315035e-06}, {"id": 23, "seek": 12396, "start": 139.9, "end": 152.84, "text": " And if we simplify, this becomes w2 w1 times x plus w2 b1 plus b2.", "tokens": [50364, 400, 718, 385, 747, 341, 6114, 337, 257, 16, 293, 15802, 309, 294, 456, 13, 50650, 50650, 407, 300, 3643, 261, 17, 1413, 261, 16, 2031, 1804, 272, 16, 1804, 272, 17, 13, 51161, 51161, 400, 498, 321, 20460, 11, 341, 3643, 261, 17, 261, 16, 1413, 2031, 1804, 261, 17, 272, 16, 1804, 272, 17, 13, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.09180255014388287, "compression_ratio": 1.4516129032258065, "no_speech_prob": 8.397820238315035e-06}, {"id": 24, "seek": 15284, "start": 152.84, "end": 161.56, "text": " And it turns out that if I were to set w equals w2 times w1 and set b equals this quantity", "tokens": [50364, 400, 309, 4523, 484, 300, 498, 286, 645, 281, 992, 261, 6915, 261, 17, 1413, 261, 16, 293, 992, 272, 6915, 341, 11275, 50800, 50800, 670, 510, 11, 550, 437, 321, 600, 445, 4898, 307, 300, 257, 17, 307, 2681, 281, 261, 87, 1804, 272, 13, 51132, 51132, 407, 261, 307, 445, 257, 8213, 2445, 295, 264, 4846, 2031, 13, 51438, 51438, 400, 2831, 813, 1228, 257, 18161, 3209, 365, 472, 7633, 4583, 293, 472, 5598, 4583, 11, 321, 51622, 51622, 1062, 382, 731, 362, 445, 1143, 257, 8213, 24590, 2316, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08561576406160991, "compression_ratio": 1.5810810810810811, "no_speech_prob": 5.954968401056249e-06}, {"id": 25, "seek": 15284, "start": 161.56, "end": 168.2, "text": " over here, then what we've just shown is that a2 is equal to wx plus b.", "tokens": [50364, 400, 309, 4523, 484, 300, 498, 286, 645, 281, 992, 261, 6915, 261, 17, 1413, 261, 16, 293, 992, 272, 6915, 341, 11275, 50800, 50800, 670, 510, 11, 550, 437, 321, 600, 445, 4898, 307, 300, 257, 17, 307, 2681, 281, 261, 87, 1804, 272, 13, 51132, 51132, 407, 261, 307, 445, 257, 8213, 2445, 295, 264, 4846, 2031, 13, 51438, 51438, 400, 2831, 813, 1228, 257, 18161, 3209, 365, 472, 7633, 4583, 293, 472, 5598, 4583, 11, 321, 51622, 51622, 1062, 382, 731, 362, 445, 1143, 257, 8213, 24590, 2316, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08561576406160991, "compression_ratio": 1.5810810810810811, "no_speech_prob": 5.954968401056249e-06}, {"id": 26, "seek": 15284, "start": 168.2, "end": 174.32, "text": " So w is just a linear function of the input x.", "tokens": [50364, 400, 309, 4523, 484, 300, 498, 286, 645, 281, 992, 261, 6915, 261, 17, 1413, 261, 16, 293, 992, 272, 6915, 341, 11275, 50800, 50800, 670, 510, 11, 550, 437, 321, 600, 445, 4898, 307, 300, 257, 17, 307, 2681, 281, 261, 87, 1804, 272, 13, 51132, 51132, 407, 261, 307, 445, 257, 8213, 2445, 295, 264, 4846, 2031, 13, 51438, 51438, 400, 2831, 813, 1228, 257, 18161, 3209, 365, 472, 7633, 4583, 293, 472, 5598, 4583, 11, 321, 51622, 51622, 1062, 382, 731, 362, 445, 1143, 257, 8213, 24590, 2316, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08561576406160991, "compression_ratio": 1.5810810810810811, "no_speech_prob": 5.954968401056249e-06}, {"id": 27, "seek": 15284, "start": 174.32, "end": 178.0, "text": " And rather than using a neural network with one hidden layer and one output layer, we", "tokens": [50364, 400, 309, 4523, 484, 300, 498, 286, 645, 281, 992, 261, 6915, 261, 17, 1413, 261, 16, 293, 992, 272, 6915, 341, 11275, 50800, 50800, 670, 510, 11, 550, 437, 321, 600, 445, 4898, 307, 300, 257, 17, 307, 2681, 281, 261, 87, 1804, 272, 13, 51132, 51132, 407, 261, 307, 445, 257, 8213, 2445, 295, 264, 4846, 2031, 13, 51438, 51438, 400, 2831, 813, 1228, 257, 18161, 3209, 365, 472, 7633, 4583, 293, 472, 5598, 4583, 11, 321, 51622, 51622, 1062, 382, 731, 362, 445, 1143, 257, 8213, 24590, 2316, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08561576406160991, "compression_ratio": 1.5810810810810811, "no_speech_prob": 5.954968401056249e-06}, {"id": 28, "seek": 15284, "start": 178.0, "end": 181.84, "text": " might as well have just used a linear regression model.", "tokens": [50364, 400, 309, 4523, 484, 300, 498, 286, 645, 281, 992, 261, 6915, 261, 17, 1413, 261, 16, 293, 992, 272, 6915, 341, 11275, 50800, 50800, 670, 510, 11, 550, 437, 321, 600, 445, 4898, 307, 300, 257, 17, 307, 2681, 281, 261, 87, 1804, 272, 13, 51132, 51132, 407, 261, 307, 445, 257, 8213, 2445, 295, 264, 4846, 2031, 13, 51438, 51438, 400, 2831, 813, 1228, 257, 18161, 3209, 365, 472, 7633, 4583, 293, 472, 5598, 4583, 11, 321, 51622, 51622, 1062, 382, 731, 362, 445, 1143, 257, 8213, 24590, 2316, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08561576406160991, "compression_ratio": 1.5810810810810811, "no_speech_prob": 5.954968401056249e-06}, {"id": 29, "seek": 18184, "start": 181.84, "end": 187.8, "text": " If you're familiar with linear algebra, this result comes from the fact that a linear function", "tokens": [50364, 759, 291, 434, 4963, 365, 8213, 21989, 11, 341, 1874, 1487, 490, 264, 1186, 300, 257, 8213, 2445, 50662, 50662, 295, 257, 8213, 2445, 307, 2564, 257, 8213, 2445, 13, 50828, 50828, 400, 341, 307, 983, 1419, 3866, 7914, 294, 257, 18161, 3209, 1177, 380, 718, 264, 18161, 3209, 51038, 51038, 14722, 604, 544, 3997, 4122, 420, 1466, 1340, 544, 3997, 813, 445, 257, 8213, 2445, 13, 51408, 51408, 407, 294, 264, 2674, 1389, 11, 498, 291, 632, 257, 18161, 3209, 365, 3866, 7914, 411, 341, 11, 293, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.08264091222182564, "compression_ratio": 1.908256880733945, "no_speech_prob": 7.646402082173154e-06}, {"id": 30, "seek": 18184, "start": 187.8, "end": 191.12, "text": " of a linear function is itself a linear function.", "tokens": [50364, 759, 291, 434, 4963, 365, 8213, 21989, 11, 341, 1874, 1487, 490, 264, 1186, 300, 257, 8213, 2445, 50662, 50662, 295, 257, 8213, 2445, 307, 2564, 257, 8213, 2445, 13, 50828, 50828, 400, 341, 307, 983, 1419, 3866, 7914, 294, 257, 18161, 3209, 1177, 380, 718, 264, 18161, 3209, 51038, 51038, 14722, 604, 544, 3997, 4122, 420, 1466, 1340, 544, 3997, 813, 445, 257, 8213, 2445, 13, 51408, 51408, 407, 294, 264, 2674, 1389, 11, 498, 291, 632, 257, 18161, 3209, 365, 3866, 7914, 411, 341, 11, 293, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.08264091222182564, "compression_ratio": 1.908256880733945, "no_speech_prob": 7.646402082173154e-06}, {"id": 31, "seek": 18184, "start": 191.12, "end": 195.32, "text": " And this is why having multiple layers in a neural network doesn't let the neural network", "tokens": [50364, 759, 291, 434, 4963, 365, 8213, 21989, 11, 341, 1874, 1487, 490, 264, 1186, 300, 257, 8213, 2445, 50662, 50662, 295, 257, 8213, 2445, 307, 2564, 257, 8213, 2445, 13, 50828, 50828, 400, 341, 307, 983, 1419, 3866, 7914, 294, 257, 18161, 3209, 1177, 380, 718, 264, 18161, 3209, 51038, 51038, 14722, 604, 544, 3997, 4122, 420, 1466, 1340, 544, 3997, 813, 445, 257, 8213, 2445, 13, 51408, 51408, 407, 294, 264, 2674, 1389, 11, 498, 291, 632, 257, 18161, 3209, 365, 3866, 7914, 411, 341, 11, 293, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.08264091222182564, "compression_ratio": 1.908256880733945, "no_speech_prob": 7.646402082173154e-06}, {"id": 32, "seek": 18184, "start": 195.32, "end": 202.72, "text": " compute any more complex features or learn anything more complex than just a linear function.", "tokens": [50364, 759, 291, 434, 4963, 365, 8213, 21989, 11, 341, 1874, 1487, 490, 264, 1186, 300, 257, 8213, 2445, 50662, 50662, 295, 257, 8213, 2445, 307, 2564, 257, 8213, 2445, 13, 50828, 50828, 400, 341, 307, 983, 1419, 3866, 7914, 294, 257, 18161, 3209, 1177, 380, 718, 264, 18161, 3209, 51038, 51038, 14722, 604, 544, 3997, 4122, 420, 1466, 1340, 544, 3997, 813, 445, 257, 8213, 2445, 13, 51408, 51408, 407, 294, 264, 2674, 1389, 11, 498, 291, 632, 257, 18161, 3209, 365, 3866, 7914, 411, 341, 11, 293, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.08264091222182564, "compression_ratio": 1.908256880733945, "no_speech_prob": 7.646402082173154e-06}, {"id": 33, "seek": 18184, "start": 202.72, "end": 210.12, "text": " So in the general case, if you had a neural network with multiple layers like this, and", "tokens": [50364, 759, 291, 434, 4963, 365, 8213, 21989, 11, 341, 1874, 1487, 490, 264, 1186, 300, 257, 8213, 2445, 50662, 50662, 295, 257, 8213, 2445, 307, 2564, 257, 8213, 2445, 13, 50828, 50828, 400, 341, 307, 983, 1419, 3866, 7914, 294, 257, 18161, 3209, 1177, 380, 718, 264, 18161, 3209, 51038, 51038, 14722, 604, 544, 3997, 4122, 420, 1466, 1340, 544, 3997, 813, 445, 257, 8213, 2445, 13, 51408, 51408, 407, 294, 264, 2674, 1389, 11, 498, 291, 632, 257, 18161, 3209, 365, 3866, 7914, 411, 341, 11, 293, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.08264091222182564, "compression_ratio": 1.908256880733945, "no_speech_prob": 7.646402082173154e-06}, {"id": 34, "seek": 21012, "start": 210.12, "end": 214.68, "text": " say you were to use a linear activation function for all the hidden layers, and also use a", "tokens": [50364, 584, 291, 645, 281, 764, 257, 8213, 24433, 2445, 337, 439, 264, 7633, 7914, 11, 293, 611, 764, 257, 50592, 50592, 8213, 24433, 2445, 337, 264, 5598, 4583, 11, 550, 309, 4523, 484, 341, 2316, 486, 14722, 50922, 50922, 364, 5598, 300, 307, 2584, 10344, 281, 8213, 24590, 13, 51132, 51132, 440, 5598, 257, 19, 393, 312, 12675, 382, 257, 8213, 2445, 295, 264, 4846, 4122, 2031, 1804, 272, 13, 51552, 51552, 1610, 8535, 356, 11, 498, 321, 645, 281, 920, 764, 257, 8213, 24433, 2445, 337, 439, 264, 7633, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.0963372271111671, "compression_ratio": 2.0242718446601944, "no_speech_prob": 1.0783135621750262e-05}, {"id": 35, "seek": 21012, "start": 214.68, "end": 221.28, "text": " linear activation function for the output layer, then it turns out this model will compute", "tokens": [50364, 584, 291, 645, 281, 764, 257, 8213, 24433, 2445, 337, 439, 264, 7633, 7914, 11, 293, 611, 764, 257, 50592, 50592, 8213, 24433, 2445, 337, 264, 5598, 4583, 11, 550, 309, 4523, 484, 341, 2316, 486, 14722, 50922, 50922, 364, 5598, 300, 307, 2584, 10344, 281, 8213, 24590, 13, 51132, 51132, 440, 5598, 257, 19, 393, 312, 12675, 382, 257, 8213, 2445, 295, 264, 4846, 4122, 2031, 1804, 272, 13, 51552, 51552, 1610, 8535, 356, 11, 498, 321, 645, 281, 920, 764, 257, 8213, 24433, 2445, 337, 439, 264, 7633, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.0963372271111671, "compression_ratio": 2.0242718446601944, "no_speech_prob": 1.0783135621750262e-05}, {"id": 36, "seek": 21012, "start": 221.28, "end": 225.48000000000002, "text": " an output that is completely equivalent to linear regression.", "tokens": [50364, 584, 291, 645, 281, 764, 257, 8213, 24433, 2445, 337, 439, 264, 7633, 7914, 11, 293, 611, 764, 257, 50592, 50592, 8213, 24433, 2445, 337, 264, 5598, 4583, 11, 550, 309, 4523, 484, 341, 2316, 486, 14722, 50922, 50922, 364, 5598, 300, 307, 2584, 10344, 281, 8213, 24590, 13, 51132, 51132, 440, 5598, 257, 19, 393, 312, 12675, 382, 257, 8213, 2445, 295, 264, 4846, 4122, 2031, 1804, 272, 13, 51552, 51552, 1610, 8535, 356, 11, 498, 321, 645, 281, 920, 764, 257, 8213, 24433, 2445, 337, 439, 264, 7633, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.0963372271111671, "compression_ratio": 2.0242718446601944, "no_speech_prob": 1.0783135621750262e-05}, {"id": 37, "seek": 21012, "start": 225.48000000000002, "end": 233.88, "text": " The output a4 can be expressed as a linear function of the input features x plus b.", "tokens": [50364, 584, 291, 645, 281, 764, 257, 8213, 24433, 2445, 337, 439, 264, 7633, 7914, 11, 293, 611, 764, 257, 50592, 50592, 8213, 24433, 2445, 337, 264, 5598, 4583, 11, 550, 309, 4523, 484, 341, 2316, 486, 14722, 50922, 50922, 364, 5598, 300, 307, 2584, 10344, 281, 8213, 24590, 13, 51132, 51132, 440, 5598, 257, 19, 393, 312, 12675, 382, 257, 8213, 2445, 295, 264, 4846, 4122, 2031, 1804, 272, 13, 51552, 51552, 1610, 8535, 356, 11, 498, 321, 645, 281, 920, 764, 257, 8213, 24433, 2445, 337, 439, 264, 7633, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.0963372271111671, "compression_ratio": 2.0242718446601944, "no_speech_prob": 1.0783135621750262e-05}, {"id": 38, "seek": 21012, "start": 233.88, "end": 239.36, "text": " Or alternatively, if we were to still use a linear activation function for all the hidden", "tokens": [50364, 584, 291, 645, 281, 764, 257, 8213, 24433, 2445, 337, 439, 264, 7633, 7914, 11, 293, 611, 764, 257, 50592, 50592, 8213, 24433, 2445, 337, 264, 5598, 4583, 11, 550, 309, 4523, 484, 341, 2316, 486, 14722, 50922, 50922, 364, 5598, 300, 307, 2584, 10344, 281, 8213, 24590, 13, 51132, 51132, 440, 5598, 257, 19, 393, 312, 12675, 382, 257, 8213, 2445, 295, 264, 4846, 4122, 2031, 1804, 272, 13, 51552, 51552, 1610, 8535, 356, 11, 498, 321, 645, 281, 920, 764, 257, 8213, 24433, 2445, 337, 439, 264, 7633, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.0963372271111671, "compression_ratio": 2.0242718446601944, "no_speech_prob": 1.0783135621750262e-05}, {"id": 39, "seek": 23936, "start": 239.36, "end": 245.32000000000002, "text": " layers, for these three hidden layers here, but we were to use a logistic activation function", "tokens": [50364, 7914, 11, 337, 613, 1045, 7633, 7914, 510, 11, 457, 321, 645, 281, 764, 257, 3565, 3142, 24433, 2445, 50662, 50662, 337, 264, 5598, 4583, 11, 550, 309, 4523, 484, 291, 393, 855, 300, 341, 2316, 3643, 10344, 50950, 50950, 281, 3565, 3142, 24590, 13, 51080, 51080, 400, 257, 19, 294, 341, 1389, 393, 312, 12675, 382, 502, 670, 502, 1804, 308, 281, 264, 3671, 261, 87, 1804, 272, 337, 51456, 51456, 512, 4190, 295, 261, 293, 272, 13, 51566, 51566, 400, 370, 341, 955, 18161, 3209, 1177, 380, 360, 1340, 300, 291, 393, 380, 611, 360, 365, 3565, 3142, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.10039039758535531, "compression_ratio": 1.6932773109243697, "no_speech_prob": 4.425398856255924e-06}, {"id": 40, "seek": 23936, "start": 245.32000000000002, "end": 251.08, "text": " for the output layer, then it turns out you can show that this model becomes equivalent", "tokens": [50364, 7914, 11, 337, 613, 1045, 7633, 7914, 510, 11, 457, 321, 645, 281, 764, 257, 3565, 3142, 24433, 2445, 50662, 50662, 337, 264, 5598, 4583, 11, 550, 309, 4523, 484, 291, 393, 855, 300, 341, 2316, 3643, 10344, 50950, 50950, 281, 3565, 3142, 24590, 13, 51080, 51080, 400, 257, 19, 294, 341, 1389, 393, 312, 12675, 382, 502, 670, 502, 1804, 308, 281, 264, 3671, 261, 87, 1804, 272, 337, 51456, 51456, 512, 4190, 295, 261, 293, 272, 13, 51566, 51566, 400, 370, 341, 955, 18161, 3209, 1177, 380, 360, 1340, 300, 291, 393, 380, 611, 360, 365, 3565, 3142, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.10039039758535531, "compression_ratio": 1.6932773109243697, "no_speech_prob": 4.425398856255924e-06}, {"id": 41, "seek": 23936, "start": 251.08, "end": 253.68, "text": " to logistic regression.", "tokens": [50364, 7914, 11, 337, 613, 1045, 7633, 7914, 510, 11, 457, 321, 645, 281, 764, 257, 3565, 3142, 24433, 2445, 50662, 50662, 337, 264, 5598, 4583, 11, 550, 309, 4523, 484, 291, 393, 855, 300, 341, 2316, 3643, 10344, 50950, 50950, 281, 3565, 3142, 24590, 13, 51080, 51080, 400, 257, 19, 294, 341, 1389, 393, 312, 12675, 382, 502, 670, 502, 1804, 308, 281, 264, 3671, 261, 87, 1804, 272, 337, 51456, 51456, 512, 4190, 295, 261, 293, 272, 13, 51566, 51566, 400, 370, 341, 955, 18161, 3209, 1177, 380, 360, 1340, 300, 291, 393, 380, 611, 360, 365, 3565, 3142, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.10039039758535531, "compression_ratio": 1.6932773109243697, "no_speech_prob": 4.425398856255924e-06}, {"id": 42, "seek": 23936, "start": 253.68, "end": 261.2, "text": " And a4 in this case can be expressed as 1 over 1 plus e to the negative wx plus b for", "tokens": [50364, 7914, 11, 337, 613, 1045, 7633, 7914, 510, 11, 457, 321, 645, 281, 764, 257, 3565, 3142, 24433, 2445, 50662, 50662, 337, 264, 5598, 4583, 11, 550, 309, 4523, 484, 291, 393, 855, 300, 341, 2316, 3643, 10344, 50950, 50950, 281, 3565, 3142, 24590, 13, 51080, 51080, 400, 257, 19, 294, 341, 1389, 393, 312, 12675, 382, 502, 670, 502, 1804, 308, 281, 264, 3671, 261, 87, 1804, 272, 337, 51456, 51456, 512, 4190, 295, 261, 293, 272, 13, 51566, 51566, 400, 370, 341, 955, 18161, 3209, 1177, 380, 360, 1340, 300, 291, 393, 380, 611, 360, 365, 3565, 3142, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.10039039758535531, "compression_ratio": 1.6932773109243697, "no_speech_prob": 4.425398856255924e-06}, {"id": 43, "seek": 23936, "start": 261.2, "end": 263.40000000000003, "text": " some values of w and b.", "tokens": [50364, 7914, 11, 337, 613, 1045, 7633, 7914, 510, 11, 457, 321, 645, 281, 764, 257, 3565, 3142, 24433, 2445, 50662, 50662, 337, 264, 5598, 4583, 11, 550, 309, 4523, 484, 291, 393, 855, 300, 341, 2316, 3643, 10344, 50950, 50950, 281, 3565, 3142, 24590, 13, 51080, 51080, 400, 257, 19, 294, 341, 1389, 393, 312, 12675, 382, 502, 670, 502, 1804, 308, 281, 264, 3671, 261, 87, 1804, 272, 337, 51456, 51456, 512, 4190, 295, 261, 293, 272, 13, 51566, 51566, 400, 370, 341, 955, 18161, 3209, 1177, 380, 360, 1340, 300, 291, 393, 380, 611, 360, 365, 3565, 3142, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.10039039758535531, "compression_ratio": 1.6932773109243697, "no_speech_prob": 4.425398856255924e-06}, {"id": 44, "seek": 23936, "start": 263.40000000000003, "end": 268.84000000000003, "text": " And so this big neural network doesn't do anything that you can't also do with logistic", "tokens": [50364, 7914, 11, 337, 613, 1045, 7633, 7914, 510, 11, 457, 321, 645, 281, 764, 257, 3565, 3142, 24433, 2445, 50662, 50662, 337, 264, 5598, 4583, 11, 550, 309, 4523, 484, 291, 393, 855, 300, 341, 2316, 3643, 10344, 50950, 50950, 281, 3565, 3142, 24590, 13, 51080, 51080, 400, 257, 19, 294, 341, 1389, 393, 312, 12675, 382, 502, 670, 502, 1804, 308, 281, 264, 3671, 261, 87, 1804, 272, 337, 51456, 51456, 512, 4190, 295, 261, 293, 272, 13, 51566, 51566, 400, 370, 341, 955, 18161, 3209, 1177, 380, 360, 1340, 300, 291, 393, 380, 611, 360, 365, 3565, 3142, 51838, 51838], "temperature": 0.0, "avg_logprob": -0.10039039758535531, "compression_ratio": 1.6932773109243697, "no_speech_prob": 4.425398856255924e-06}, {"id": 45, "seek": 26884, "start": 268.84, "end": 269.96, "text": " regression.", "tokens": [50364, 24590, 13, 50420, 50420, 663, 311, 983, 257, 2689, 4978, 295, 9298, 307, 500, 380, 764, 264, 8213, 24433, 2445, 294, 264, 50610, 50610, 7633, 7914, 295, 428, 18161, 3209, 13, 50726, 50726, 400, 294, 1186, 11, 286, 2748, 5850, 1228, 264, 1300, 43, 52, 24433, 2445, 820, 360, 445, 2489, 13, 51062, 51062, 407, 300, 311, 983, 257, 18161, 3209, 2203, 24433, 6828, 661, 813, 445, 264, 8213, 24433, 51362, 51362, 2445, 5315, 13, 51469, 51469, 407, 1400, 11, 291, 600, 3264, 281, 1322, 18161, 9590, 337, 17434, 21538, 2740, 11, 689, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14438165094434602, "compression_ratio": 1.8305084745762712, "no_speech_prob": 5.6823473641998135e-06}, {"id": 46, "seek": 26884, "start": 269.96, "end": 273.76, "text": " That's why a common rule of thumb is don't use the linear activation function in the", "tokens": [50364, 24590, 13, 50420, 50420, 663, 311, 983, 257, 2689, 4978, 295, 9298, 307, 500, 380, 764, 264, 8213, 24433, 2445, 294, 264, 50610, 50610, 7633, 7914, 295, 428, 18161, 3209, 13, 50726, 50726, 400, 294, 1186, 11, 286, 2748, 5850, 1228, 264, 1300, 43, 52, 24433, 2445, 820, 360, 445, 2489, 13, 51062, 51062, 407, 300, 311, 983, 257, 18161, 3209, 2203, 24433, 6828, 661, 813, 445, 264, 8213, 24433, 51362, 51362, 2445, 5315, 13, 51469, 51469, 407, 1400, 11, 291, 600, 3264, 281, 1322, 18161, 9590, 337, 17434, 21538, 2740, 11, 689, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14438165094434602, "compression_ratio": 1.8305084745762712, "no_speech_prob": 5.6823473641998135e-06}, {"id": 47, "seek": 26884, "start": 273.76, "end": 276.08, "text": " hidden layers of your neural network.", "tokens": [50364, 24590, 13, 50420, 50420, 663, 311, 983, 257, 2689, 4978, 295, 9298, 307, 500, 380, 764, 264, 8213, 24433, 2445, 294, 264, 50610, 50610, 7633, 7914, 295, 428, 18161, 3209, 13, 50726, 50726, 400, 294, 1186, 11, 286, 2748, 5850, 1228, 264, 1300, 43, 52, 24433, 2445, 820, 360, 445, 2489, 13, 51062, 51062, 407, 300, 311, 983, 257, 18161, 3209, 2203, 24433, 6828, 661, 813, 445, 264, 8213, 24433, 51362, 51362, 2445, 5315, 13, 51469, 51469, 407, 1400, 11, 291, 600, 3264, 281, 1322, 18161, 9590, 337, 17434, 21538, 2740, 11, 689, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14438165094434602, "compression_ratio": 1.8305084745762712, "no_speech_prob": 5.6823473641998135e-06}, {"id": 48, "seek": 26884, "start": 276.08, "end": 282.79999999999995, "text": " And in fact, I recommend typically using the ReLU activation function should do just fine.", "tokens": [50364, 24590, 13, 50420, 50420, 663, 311, 983, 257, 2689, 4978, 295, 9298, 307, 500, 380, 764, 264, 8213, 24433, 2445, 294, 264, 50610, 50610, 7633, 7914, 295, 428, 18161, 3209, 13, 50726, 50726, 400, 294, 1186, 11, 286, 2748, 5850, 1228, 264, 1300, 43, 52, 24433, 2445, 820, 360, 445, 2489, 13, 51062, 51062, 407, 300, 311, 983, 257, 18161, 3209, 2203, 24433, 6828, 661, 813, 445, 264, 8213, 24433, 51362, 51362, 2445, 5315, 13, 51469, 51469, 407, 1400, 11, 291, 600, 3264, 281, 1322, 18161, 9590, 337, 17434, 21538, 2740, 11, 689, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14438165094434602, "compression_ratio": 1.8305084745762712, "no_speech_prob": 5.6823473641998135e-06}, {"id": 49, "seek": 26884, "start": 282.79999999999995, "end": 288.79999999999995, "text": " So that's why a neural network needs activation functions other than just the linear activation", "tokens": [50364, 24590, 13, 50420, 50420, 663, 311, 983, 257, 2689, 4978, 295, 9298, 307, 500, 380, 764, 264, 8213, 24433, 2445, 294, 264, 50610, 50610, 7633, 7914, 295, 428, 18161, 3209, 13, 50726, 50726, 400, 294, 1186, 11, 286, 2748, 5850, 1228, 264, 1300, 43, 52, 24433, 2445, 820, 360, 445, 2489, 13, 51062, 51062, 407, 300, 311, 983, 257, 18161, 3209, 2203, 24433, 6828, 661, 813, 445, 264, 8213, 24433, 51362, 51362, 2445, 5315, 13, 51469, 51469, 407, 1400, 11, 291, 600, 3264, 281, 1322, 18161, 9590, 337, 17434, 21538, 2740, 11, 689, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14438165094434602, "compression_ratio": 1.8305084745762712, "no_speech_prob": 5.6823473641998135e-06}, {"id": 50, "seek": 26884, "start": 288.79999999999995, "end": 290.94, "text": " function everywhere.", "tokens": [50364, 24590, 13, 50420, 50420, 663, 311, 983, 257, 2689, 4978, 295, 9298, 307, 500, 380, 764, 264, 8213, 24433, 2445, 294, 264, 50610, 50610, 7633, 7914, 295, 428, 18161, 3209, 13, 50726, 50726, 400, 294, 1186, 11, 286, 2748, 5850, 1228, 264, 1300, 43, 52, 24433, 2445, 820, 360, 445, 2489, 13, 51062, 51062, 407, 300, 311, 983, 257, 18161, 3209, 2203, 24433, 6828, 661, 813, 445, 264, 8213, 24433, 51362, 51362, 2445, 5315, 13, 51469, 51469, 407, 1400, 11, 291, 600, 3264, 281, 1322, 18161, 9590, 337, 17434, 21538, 2740, 11, 689, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14438165094434602, "compression_ratio": 1.8305084745762712, "no_speech_prob": 5.6823473641998135e-06}, {"id": 51, "seek": 26884, "start": 290.94, "end": 295.84, "text": " So far, you've learned to build neural networks for binary classification problems, where", "tokens": [50364, 24590, 13, 50420, 50420, 663, 311, 983, 257, 2689, 4978, 295, 9298, 307, 500, 380, 764, 264, 8213, 24433, 2445, 294, 264, 50610, 50610, 7633, 7914, 295, 428, 18161, 3209, 13, 50726, 50726, 400, 294, 1186, 11, 286, 2748, 5850, 1228, 264, 1300, 43, 52, 24433, 2445, 820, 360, 445, 2489, 13, 51062, 51062, 407, 300, 311, 983, 257, 18161, 3209, 2203, 24433, 6828, 661, 813, 445, 264, 8213, 24433, 51362, 51362, 2445, 5315, 13, 51469, 51469, 407, 1400, 11, 291, 600, 3264, 281, 1322, 18161, 9590, 337, 17434, 21538, 2740, 11, 689, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14438165094434602, "compression_ratio": 1.8305084745762712, "no_speech_prob": 5.6823473641998135e-06}, {"id": 52, "seek": 29584, "start": 295.84, "end": 303.0, "text": " y is either 0 or 1, as well as for regression problems, where y can take negative or positive", "tokens": [50364, 288, 307, 2139, 1958, 420, 502, 11, 382, 731, 382, 337, 24590, 2740, 11, 689, 288, 393, 747, 3671, 420, 3353, 50722, 50722, 4190, 420, 1310, 445, 3353, 293, 2107, 12, 28561, 1166, 4190, 13, 50954, 50954, 682, 264, 958, 960, 11, 286, 1116, 411, 281, 2073, 365, 291, 257, 2674, 2144, 295, 437, 291, 600, 1612, 51165, 51165, 370, 1400, 337, 21538, 13, 51332, 51332, 682, 1729, 11, 562, 288, 1177, 380, 445, 747, 322, 732, 4190, 11, 457, 815, 747, 322, 1045, 420, 1451, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.10518745846218533, "compression_ratio": 1.6103286384976525, "no_speech_prob": 8.530124432581943e-06}, {"id": 53, "seek": 29584, "start": 303.0, "end": 307.64, "text": " values or maybe just positive and non-negative values.", "tokens": [50364, 288, 307, 2139, 1958, 420, 502, 11, 382, 731, 382, 337, 24590, 2740, 11, 689, 288, 393, 747, 3671, 420, 3353, 50722, 50722, 4190, 420, 1310, 445, 3353, 293, 2107, 12, 28561, 1166, 4190, 13, 50954, 50954, 682, 264, 958, 960, 11, 286, 1116, 411, 281, 2073, 365, 291, 257, 2674, 2144, 295, 437, 291, 600, 1612, 51165, 51165, 370, 1400, 337, 21538, 13, 51332, 51332, 682, 1729, 11, 562, 288, 1177, 380, 445, 747, 322, 732, 4190, 11, 457, 815, 747, 322, 1045, 420, 1451, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.10518745846218533, "compression_ratio": 1.6103286384976525, "no_speech_prob": 8.530124432581943e-06}, {"id": 54, "seek": 29584, "start": 307.64, "end": 311.85999999999996, "text": " In the next video, I'd like to share with you a generalization of what you've seen", "tokens": [50364, 288, 307, 2139, 1958, 420, 502, 11, 382, 731, 382, 337, 24590, 2740, 11, 689, 288, 393, 747, 3671, 420, 3353, 50722, 50722, 4190, 420, 1310, 445, 3353, 293, 2107, 12, 28561, 1166, 4190, 13, 50954, 50954, 682, 264, 958, 960, 11, 286, 1116, 411, 281, 2073, 365, 291, 257, 2674, 2144, 295, 437, 291, 600, 1612, 51165, 51165, 370, 1400, 337, 21538, 13, 51332, 51332, 682, 1729, 11, 562, 288, 1177, 380, 445, 747, 322, 732, 4190, 11, 457, 815, 747, 322, 1045, 420, 1451, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.10518745846218533, "compression_ratio": 1.6103286384976525, "no_speech_prob": 8.530124432581943e-06}, {"id": 55, "seek": 29584, "start": 311.85999999999996, "end": 315.2, "text": " so far for classification.", "tokens": [50364, 288, 307, 2139, 1958, 420, 502, 11, 382, 731, 382, 337, 24590, 2740, 11, 689, 288, 393, 747, 3671, 420, 3353, 50722, 50722, 4190, 420, 1310, 445, 3353, 293, 2107, 12, 28561, 1166, 4190, 13, 50954, 50954, 682, 264, 958, 960, 11, 286, 1116, 411, 281, 2073, 365, 291, 257, 2674, 2144, 295, 437, 291, 600, 1612, 51165, 51165, 370, 1400, 337, 21538, 13, 51332, 51332, 682, 1729, 11, 562, 288, 1177, 380, 445, 747, 322, 732, 4190, 11, 457, 815, 747, 322, 1045, 420, 1451, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.10518745846218533, "compression_ratio": 1.6103286384976525, "no_speech_prob": 8.530124432581943e-06}, {"id": 56, "seek": 29584, "start": 315.2, "end": 321.96, "text": " In particular, when y doesn't just take on two values, but may take on three or four", "tokens": [50364, 288, 307, 2139, 1958, 420, 502, 11, 382, 731, 382, 337, 24590, 2740, 11, 689, 288, 393, 747, 3671, 420, 3353, 50722, 50722, 4190, 420, 1310, 445, 3353, 293, 2107, 12, 28561, 1166, 4190, 13, 50954, 50954, 682, 264, 958, 960, 11, 286, 1116, 411, 281, 2073, 365, 291, 257, 2674, 2144, 295, 437, 291, 600, 1612, 51165, 51165, 370, 1400, 337, 21538, 13, 51332, 51332, 682, 1729, 11, 562, 288, 1177, 380, 445, 747, 322, 732, 4190, 11, 457, 815, 747, 322, 1045, 420, 1451, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.10518745846218533, "compression_ratio": 1.6103286384976525, "no_speech_prob": 8.530124432581943e-06}, {"id": 57, "seek": 32196, "start": 321.96, "end": 326.08, "text": " or 10 or even more categorical values.", "tokens": [50364, 420, 1266, 420, 754, 544, 19250, 804, 4190, 13, 50570, 50570, 961, 311, 747, 257, 574, 412, 577, 291, 393, 1322, 257, 18161, 3209, 337, 300, 2010, 295, 21538, 1154, 13, 50796], "temperature": 0.0, "avg_logprob": -0.12102145307204303, "compression_ratio": 1.238532110091743, "no_speech_prob": 6.636745638388675e-06}, {"id": 58, "seek": 32608, "start": 326.08, "end": 353.08, "text": " Let's take a look at how you can build a neural network for that type of classification problem.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 577, 291, 393, 1322, 257, 18161, 3209, 337, 300, 2010, 295, 21538, 1154, 13, 51714], "temperature": 0.0, "avg_logprob": -0.22768406246019446, "compression_ratio": 1.103448275862069, "no_speech_prob": 2.850639020834933e-06}], "language": "en", "video_id": "fK5YzGIc2u8", "entity": "ML Specialization, Andrew Ng (2022)"}}