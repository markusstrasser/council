{"video_id": "P6sfmUTpUmc", "title": "Building makemore Part 3: Activations & Gradients, BatchNorm", "description": "We dive into some of the internals of MLPs with multiple layers and scrutinize the statistics of the forward pass activations, backward pass gradients, and some of the pitfalls when they are improperly scaled. We also look at the typical diagnostic tools and visualizations you'd want to use to understand the health of your deep network. We learn why training deep neural nets can be fragile and introduce the first modern innovation that made doing so much easier: Batch Normalization. Residual connections and the Adam optimizer remain notable todos for later video.\n\nLinks:\n- makemore on github: https://github.com/karpathy/makemore\n- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part3_bn.ipynb\n- collab notebook: https://colab.research.google.com/drive/1H5CSy-OnisagUgDUXhHwo1ng2pjKHYSN?usp=sharing\n- my website: https://karpathy.ai\n- my twitter: https://twitter.com/karpathy\n- Discord channel: https://discord.gg/Hp2m3kheJn\n\nUseful links:\n- \"Kaiming init\" paper: https://arxiv.org/abs/1502.01852\n- BatchNorm paper: https://arxiv.org/abs/1502.03167\n- Bengio et al. 2003 MLP language model paper (pdf): https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n\nExercises:\n- E01: I did not get around to seeing what happens when you initialize all weights and biases to zero. Try this and train the neural net. You might think either that 1) the network trains just fine or 2) the network doesn't train at all, but actually it is 3) the network trains but only partially, and achieves a pretty bad final performance. Inspect the gradients and activations to figure out what is happening and why the network is only partially training, and what part is being trained exactly.\n- E02: BatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the batchnorm gamma/beta can be \"folded into\" the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time. Set up a small 3-layer MLP with batchnorms, train the network, then \"fold\" the batchnorm gamma/beta into the preceeding Linear layer's W,b by creating a new W2, b2 and erasing the batch norm. Verify that this gives the same forward pass during inference. i.e. we see that the batchnorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool.\n\nChapters:\n00:00:00 intro\n00:01:22 starter code\n00:04:19 fixing the initial loss \n00:12:59 fixing the saturated tanh\n00:27:53 calculating the init scale: \u201cKaiming init\u201d\n00:40:40 batch normalization\n01:03:07 batch normalization: summary\n01:04:50 real example: resnet50 walkthrough\n01:14:10 summary of the lecture\n01:18:35 just kidding: part2: PyTorch-ifying the code\n01:26:51 viz #1: forward pass activations statistics\n01:30:54 viz #2: backward pass gradient statistics\n01:32:07 the fully linear case of no non-linearities\n01:36:15 viz #3: parameter activation and gradient statistics\n01:39:55 viz #4: update:data ratio over time\n01:46:04 bringing back batchnorm, looking at the visualizations\n01:51:34 summary of the lecture for real this time", "author": "Andrej Karpathy", "keywords": ["neural network", "deep learning", "makemore", "batchnorm", "batch normalization", "pytorch"], "channel_url": "https://www.youtube.com/channel/UCXUPKJO5MZQN11PqgIvyuvQ", "length": 6957, "views": 29754, "publish_date": "11/02/2022", "timestamp": 1664841600, "entity": "Andrew Kaparthy", "transcript": {"text": " Hi everyone. Today we are continuing our implementation of MakeMore. Now in the last lecture we implemented the Multilayer perceptron along the lines of Benjotel 2003 for character-level language modeling. So we followed this paper, took in a few characters in the past, and used an MLP to predict the next character in a sequence. So what we'd like to do now is we'd like to move on to more complex and larger neural networks, like recurrent neural networks and their variations like the GRU, LSTM, and so on. Now before we do that though, we have to stick around the level of Multilayer perceptron for a bit longer. And I'd like to do this because I would like us to have a very good intuitive understanding of the activations in the neural net during training, and especially the gradients that are flowing backwards, and how they behave and what they look like. And this is going to be very important to understand the history of the development of these architectures, because we'll see that recurrent neural networks, while they are very expressive in that they are a universal approximator, they can in principle implement all the algorithms. We'll see that they are not very easily optimizable with the first-order gradient-based techniques that we have available to us and that we use all the time. And the key to understanding why they are not optimizable easily is to understand the activations and the gradients and how they behave during training. And we'll see that a lot of the variants since recurrent neural networks have tried to improve that situation. And so that's the path that we have to take, and let's go start it. So the starting code for this lecture is largely the code from before, but I've cleaned it up a little bit. So you'll see that we are importing all the torch and mathplotlib utilities. We're reading in the words just like before. These are eight example words. There's a total of 32,000 of them. Here's a vocabulary of all the lowercase letters and the special dot token. Here we are reading the data set and processing it and creating three splits, the train, dev, and the test split. Now in MLP, this is the identical same MLP, except you see that I removed a bunch of magic numbers that we had here. And instead we have the dimensionality of the embedding space of the characters and the number of hidden units in the hidden layer. And so I pulled them outside here so that we don't have to go and change all these magic numbers all the time. With the same neural net with 11,000 parameters that we optimize now over 200,000 steps with a batch size of 32. And you'll see that I refactored the code here a little bit, but there are no functional changes. I just created a few extra variables, a few more comments, and I removed all the magic numbers. And otherwise it's the exact same thing. Then when we optimize, we saw that our loss looked something like this. We saw that the train and val loss were about 2.16 and so on. Here I refactored the code a little bit for the evaluation of arbitrary splits. So you pass in a string of which split you'd like to evaluate. And then here, depending on train, val, or test, I index in and I get the correct split. And then this is the forward pass of the network and evaluation of the loss and printing it. So just making it nicer. One thing that you'll notice here is I'm using a decorator torch.nograd, which you can also look up and read documentation of. Basically, what this decorator does on top of a function is that whatever happens in this function is assumed by torch to never require any gradients. So it will not do any of the bookkeeping that it does to keep track of all the gradients in anticipation of an eventual backward pass. It's almost as if all the tensors that get created here have a requires grad of false. And so it just makes everything much more efficient because you're telling torch that I will not call that backward on any of this computation. And you don't need to maintain the graph under the hood. So that's what this does. And you can also use a context manager with torch.nograd and you can look those up. Then here we have the sampling from a model. Just as before, just a forward pass of a neural net, getting the distribution, sampling from it, adjusting the context window and repeating until we get the special end token. And we see that we are starting to get much nicer looking words sampled from the model. It's still not amazing and they're still not fully name like, but it's much better than when we had to go with the bigram model. So that's our starting point. Now, the first thing I would like to scrutinize is the initialization. I can tell that our network is very improperly configured at initialization and there's multiple things wrong with it. But let's just start with the first one. Look here on the zero iteration, the very first iteration, we are recording a loss of 27 and this rapidly comes down to roughly one or two or so. So I can tell that the initialization is all messed up because this is way too high. In training of neural nets, it is almost always the case that you will have a rough idea for what loss to expect at initialization. And that just depends on the loss function and the problem set up. In this case, I do not expect 27. I expect a much lower number and we can calculate it together. Basically, at initialization, what we'd like is that there's 27 characters that could come next for any one training example. At initialization, we have no reason to believe any characters to be much more likely than others. And so we'd expect that the probability distribution that comes out initially is a uniform distribution assigning about equal probability to all the 27 characters. So basically what we'd like is the probability for any character would be roughly one over 27. That is the probability we should record. And then the loss is the negative log probability. So let's wrap this in a tensor. And then then we can take the log of it. And then the negative log probability is the loss we would expect, which is 3.29, much, much lower than 27. And so what's happening right now is that at initialization, the neural net is creating probability distributions that are all messed up. Some characters are very confident and some characters are very not confident. And then basically what's happening is that the network is very confidently wrong. And that's what makes it record very high loss. So here's a smaller four dimensional example of the issue. Let's say we only have four characters and then we have logits that come out of the neural net and they are very, very close to zero. Then when we take the softmax of all zeros, we get probabilities there are a diffuse distribution. So sums to one and is exactly uniform. And then in this case, if the label is, say, two, it doesn't actually matter if this if the label is two or three or one or zero, because it's a uniform distribution, we're recording the exact same loss in this case, 1.38. So this is the loss we would expect for a four dimensional example. And I can see, of course, that as we start to manipulate these logits, we're going to be changing the loss here. So it could be that we lock out and by chance this could be a very high number like five or something like that. Then in that case, we'll record a very low loss because we're assigning the correct probability at initialization by chance to the correct label. Much more likely it is that some other dimension will have a high logit. And then what will happen is we start to record much higher loss. And what can come what can happen is basically the logits come out like something like this, you know, and they take on extreme values and we record really high loss. For example, if we have torched out random of four, these are uniform. So these are normally distributed numbers for them. And here we can also print the logits probabilities that come out of it and loss. And so because these logits are near zero, for the most part, loss that comes out is is OK. But suppose this is like times 10 now. You see how because these are more extreme values, it's very unlikely that you're going to be guessing the correct bucket. And then you're confidently wrong and recording very high loss. If your logits are coming up even more extreme, you might get extremely same losses like infinity even at initialization. So basically, this is not good. And we want the logits to be roughly zero when the network is initialized. In fact, the logits can don't have to be just zero. They just have to be equal. So, for example, if all the logits are one, then because of the normalization inside the softmax, this will actually come out OK. But by symmetry, we don't want it to be any arbitrary positive or negative number. We just want it to be all zeros and record the loss that we expect at initialization. So let's now concretely see where things go wrong in our example. Here we have the initialization. Let me reinitialize the neural net. And here let me break after the very first iteration. So we only see the initial loss, which is 27. So that's way too high. And intuitively, now we can expect the variables involved. And we see that the logits here, if we just print some of these, we just print the first row. We see that the logits take on quite extreme values. And that's what's creating the fake confidence and incorrect answers and makes the loss get very, very high. So these logits should be much, much closer to zero. So now let's think through how we can achieve logits coming out of this neural net to be more closer to zero. You see here that logits are calculated as the hidden states multiplied by W2 plus B2. So first of all, currently we're initializing B2 as random values of the right size. But because we want roughly zero, we don't actually want to be adding a bias of random numbers. So in fact, I'm going to add a times zero here to make sure that B2 is just basically zero at initialization. And second, this is H multiplied by W2. So if we want logits to be very, very small, then we would be multiplying W2 and making that smaller. So, for example, if we scale down W2 by 0.1, all the elements, then if I do again just the very first iteration, you see that we are getting much closer to what we expect. So roughly what we want is about 3.29. This is 4.2. I can make this maybe even smaller. 3.32. OK, so we're getting closer and closer. Now you're probably wondering, can we just set this to zero? Then we get, of course, exactly what we're looking for at initialization. And the reason I don't usually do this is because I'm very nervous. And I'll show you in a second why you don't want to be setting Ws or weights of a neural net exactly to zero. You usually want it to be small numbers instead of exactly zero. For this output layer in this specific case, I think it would be fine. But I'll show you in a second where things go wrong very quickly if you do that. So let's just go with 0.01. In that case, our loss is close enough, but has some entropy. It's not exactly zero. It's got some little entropy. And that's used for symmetry breaking, as we'll see in a second. The logits are now coming out much closer to zero and everything is well and good. So if I just erase these and I now take away the break statement, we can run the optimization with this new initialization. And let's just see what losses we record. OK, so I let it run and you see that we started off good. And then we came down a bit. The plot of the loss now doesn't have this hockey shape appearance because basically what's happening in the hockey stick, the very first few iterations of the loss, what's happening during the optimization is the optimization is just squashing down the logits and then it's rearranging the logits. So basically we took away this easy part of the loss function where just the weights were just being shrunk down. And so therefore we don't get these easy gains in the beginning. And we're just getting some of the hard gains of training the actual neural net. And so there's no hockey stick appearance. So good things are happening in that both number one loss initialization is what we expect. And the loss doesn't look like a hockey stick. And this is true for any neural net you might train and something to look out for. And second, the loss that came out is actually quite a bit improved. Unfortunately, I erased what we had here before. I believe this was two point one two and this was this was two point one six. So we get a slightly improved result. And the reason for that is because we're spending more cycles, more time optimizing the neural net actually, instead of just spending the first several thousand iterations probably just squashing down the weights because they are so way too high in the beginning of the initialization. So something to look out for. And that's number one. Now let's look at the second problem. Let me reinitialize our neural net and let me reintroduce the break statement. So we have a reasonable initial loss. So even though everything is looking good on the level of loss and we get something that we expect, there's still a deeper problem lurking inside this neural net and its initialization. So the logits are now OK. The problem now is with the values of H, the activations of the hidden states. Now, if we just visualize this vector, sorry, this tensor H, it's kind of hard to see. But the problem here, roughly speaking, is you see how many of the elements are one or negative one. Now, recall that torch dot 10H, the 10H function is a squashing function. It makes arbitrary numbers and it squashes them into a range of negative one and one, and it does so smoothly. So let's look at the histogram of H to get a better idea of the distribution of the values inside this tensor. We can do this first. Well, we can see that H is 32 examples and 200 activations in each example. We can view it as negative one to stretch it out into one large vector. And we can then call to list to convert this into one large Python list of floats. And then we can pass this into PLT dot hist for histogram. And we say we want 50 bins and a semicolon to suppress a bunch of output we don't want. So we see this histogram and we see that most of the values by far take on value of negative one and one. So this 10H is very, very active. And we can also look at basically why that is. We can look at the preactivations that feed into the 10H. And we can see that the distribution of the preactivations are is very, very broad. These take numbers between negative 15 and 15. And that's why in the torch dot 10H, everything is being squashed and capped to be in the range of negative one and one. And lots of numbers here take on very extreme values. Now, if you are new to neural networks, you might not actually see this as an issue. But if you're well versed in the dark arts of backpropagation and have an intuitive sense of how these gradients flow through a neural net, you are looking at your distribution of 10H activations here and you are sweating. So let me show you why. We have to keep in mind that during backpropagation, just like we saw in micrograd, we are doing backward pass starting at the loss and flowing through the network backwards. In particular, we're going to back propagate through this torch dot 10H. And this layer here is made up of 200 neurons for each one of these examples. And it implements an element twice 10H. So let's look at what happens in 10H in the backward pass. We can actually go back to our previous micrograd code in the very first lecture and see how we implemented 10H. We saw that the input here was X and then we calculate T, which is the 10H of X. So that's T. And T is between negative one and one. It's the output of the 10H. And then in the backward pass, how do we back propagate through a 10H? We take out dot grad and then we multiply it. This is the chain rule with the local gradient, which took the form of one minus T squared. So what happens if the outputs of your 10H are very close to negative one or one? If you plug in T equals one here, you're going to get a zero, multiplying out dot grad. No matter what out dot grad is, we are killing the gradient and we're stopping effectively the back propagation through this 10H unit. Similarly, when T is negative one, this will again become zero and out dot grad just stops. And intuitively this makes sense because this is a 10H neuron. And what's happening is if its output is very close to one, then we are in the tail of this 10H. And so changing basically the input is not going to impact the output of the 10H too much because it's in a flat region of the 10H. And so therefore there's no impact on the loss. And so indeed the weights and the biases along with this 10H neuron do not impact the loss because the output of this 10H unit is in the flat region of the 10H and there's no influence. We can be changing them however we want and the loss is not impacted. So that's another way to justify that indeed the gradient would be basically zero. It vanishes. Indeed, when T equals zero, we get one times out dot grad. So when the 10H takes on exactly value of zero, then out dot grad is just passed through. So basically what this is doing, right, is if T is equal to zero, then this 10H unit is sort of inactive and gradient just passes through. But the more you are in the flat tails, the more the gradient is squashed. So in fact you'll see that the gradient flowing through 10H can only ever decrease and the amount that it decreases is proportional through a square here depending on how far you are in the flat tails of this 10H. And so that's kind of what's happening here. And through this, the concern here is that if all of these outputs H are in the flat regions of negative 1 and 1, then the gradients that are flowing through the network will just get destroyed at this layer. Now there is some redeeming quality here and that we can actually get a sense of the problem here as follows. I wrote some code here. And basically what we want to do here is we want to take a look at H, take the absolute value, and see how often it is in the flat region. So say greater than 0.99. And what you get is the following. And this is a Boolean tensor. So in the Boolean tensor you get a white if this is true and a black if this is false. And so basically what we have here is the 32 examples and the 200 hidden neurons. And we see that a lot of this is white. And what that's telling us is that all these 10H neurons were very, very active and they're in the flat tail. And so in all these cases, the backward gradient would get destroyed. Now we would be in a lot of trouble if for any one of these 200 neurons, if it was the case that the entire column is white, because in that case we have what's called a dead neuron. And this could be a 10H neuron where the initialization of the weights and the biases could be such that no single example ever activates this 10H in the sort of active part of the 10H. If all the examples land in the tail, then this neuron will never learn. It is a dead neuron. And so just scrutinizing this and looking for columns of completely white, we see that this is not the case. So I don't see a single neuron that is all of, you know, white. And so therefore it is the case that for every one of these 10H neurons, we do have some examples that activate them in the active part of the 10H. And so some gradients will flow through and this neuron will learn and the neuron will change and it will move and it will do something. But you can sometimes get yourself in cases where you have dead neurons. And the way this manifests is that for a 10H neuron, this would be when no matter what inputs you plug in from your data set, this 10H neuron always fires completely one or completely negative one. And then it will just not learn because all the gradients will be just zeroed out. This is true not just for 10H, but for a lot of other nonlinearities that people use in neural networks. So we certainly use 10H a lot, but sigmoid will have the exact same issue because it is a squashing neuron. And so the same will be true for sigmoid. But basically the same will actually apply to sigmoid. The same will also apply to ReLU. So ReLU has a completely flat region here below zero. So if you have a ReLU neuron, then it is a pass through if it is positive. And if the pre-activation is negative, it will just shut it off. Since the region here is completely flat, then during backpropagation, this would be exactly zeroing out the gradient. Like all of the gradient would be set exactly to zero instead of just like a very, very small number, depending on how positive or negative t is. And so you can get, for example, a dead ReLU neuron. And a dead ReLU neuron would basically look like... Basically what it is, is if a neuron with a ReLU nonlinearity never activates, so for any examples that you plug in in the dataset, it never turns on, it is always in this flat region, then this ReLU neuron is a dead neuron. Its weights and bias will never learn. They will never get a gradient because the neuron never activated. And this can sometimes happen at initialization because the weights and the biases just make it so that by chance some neurons are just forever dead. But it can also happen during optimization. If you have like a too high of a learning rate, for example, sometimes you have these neurons that get too much of a gradient and they get knocked out off the data manifold. And what happens is that from then on, no example ever activates this neuron. So this neuron remains dead forever. So it's kind of like a permanent brain damage in a mind of a network. And so sometimes what can happen is if your learning rate is very high, for example, and you have a neural net with ReLU neurons, you train the neural net and you get some last loss. But then actually what you do is you go through the entire training set and you forward your examples and you can find neurons that never activate. They are dead neurons in your network. And so those neurons will never turn on. And usually what happens is that during training, these ReLU neurons are changing, moving, et cetera. And then because of a high gradient somewhere by chance, they get knocked off and then nothing ever activates them. And from then on, they are just dead. So that's kind of like a permanent brain damage that can happen to some of these neurons. These other nonlinearities like Leaky ReLU will not suffer from this issue as much because you can see that it doesn't have flat tails. You'll almost always get gradients. And Elu is also fairly frequently used. It also might suffer from this issue because it has flat parts. So that's just something to be aware of and something to be concerned about. And in this case, we have way too many activations H that take on extreme values. And because there's no column of white, I think we will be OK. And indeed, the network optimizes and gives us a pretty decent loss. But it's just not optimal. And this is not something you want, especially during initialization. And so basically what's happening is that this H preactivation that's flowing to 10H, it's too extreme. It's too large. It's creating a distribution that is too saturated in both sides of the 10H. And it's not something you want because it means that there's less training for these neurons because they update less frequently. So how do we fix this? Well, H preactivation is MCAT, which comes from C. So these are uniform Gaussian. But then it's multiplied by W1 plus B1. And H preact is too far off from zero, and that's causing the issue. So we want this reactivation to be closer to zero, very similar to what we had with logits. So here we want actually something very, very similar. Now, it's OK to set the biases to a very small number. We can either multiply it by 0, 0, 1 to get like a little bit of entropy. I sometimes like to do that just so that there's like a little bit of variation and diversity in the original initialization of these 10H neurons. And I find in practice that that can help optimization a little bit. And then the weights, we can also just like squash. So let's multiply everything by 0.1. Let's rerun the first batch. And now let's look at this. And well, first, let's look here. You see now, because we multiply W by 0.1, we have a much better histogram. And that's because the reactivations are now between negative 1.5 and 1.5. And this we expect much, much less white. OK, there's no white. So basically, that's because there are no neurons that saturated above 0.99 in either direction. So it's actually a pretty decent place to be. Maybe we can go up a little bit. Sorry, am I changing W1 here? So maybe we can go to 0.2. OK, so maybe something like this is a nice distribution. So maybe this is what our initialization should be. So let me now erase these. And let me, starting with initialization, let me run the full optimization without the break. And let's see what we get. OK, so the optimization finished. And I rerun the loss. And this is the result that we get. And then just as a reminder, I put down all the losses that we saw previously in this lecture. So we see that we actually do get an improvement here. And just as a reminder, we started off with a validation loss of 2.17 when we started. By fixing the softmax being confidently wrong, we came down to 2.13. And by fixing the 10H layer being way too saturated, we came down to 2.10. And the reason this is happening, of course, is because our initialization is better. And so we're spending more time doing productive training instead of not very productive training, because our gradients are set to 0. And we have to learn very simple things like the overconfidence of the softmax in the beginning. And we're spending cycles just like squashing down the weight matrix. So this is illustrating basically initialization and its impacts on performance just by being aware of the internals of these neural nets and their activations and their gradients. Now, we're working with a very small network. This is just one layer multilayer perception. So because the network is so shallow, the optimization problem is actually quite easy and very forgiving. So even though our initialization was terrible, the network still learned eventually. It just got a bit worse result. This is not the case in general, though. Once we actually start working with much deeper networks that have, say, 50 layers, things can get much more complicated and these problems stack up. And so you can actually get into a place where the network is basically not training at all if your initialization is bad enough. And the deeper your network is and the more complex it is, the less forgiving it is to some of these errors. And so something to be definitely aware of and something to scrutinize, something to plot and something to be careful with. And yeah. OK, so that's great that that worked for us. But what we have here now is all these magic numbers like point two. Like, where do I come up with this and how am I supposed to set these if I have a large neural net with lots and lots of layers? And so obviously no one does this by hand. There's actually some relatively principled ways of setting these scales that I would like to introduce to you now. So let me paste some code here that I prepared just to motivate the discussion of this. So what I'm doing here is we have some random input here X that is drawn from a Gaussian. And there's one thousand examples that are ten dimensional. And then we have a weight layer here that is also initialized using Gaussian, just like we did here. And we these neurons in the hidden layer look at ten inputs and there are 200 neurons in this layer. And then we have here, just like here in this case, the multiplication X multiplied by W to get the pre activations of these neurons. And basically the analysis here looks at OK, suppose these are uniform Gaussian and these weights are uniform Gaussian. If I do X times W and we forget for now the bias and the nonlinearity, then what is the mean and the standard deviation of these gaussians? So in the beginning here, the input is just a normal Gaussian distribution mean zero and the standard deviation is one. And the standard deviation again is just a measure of a spread of the Gaussian. But then once we multiply here and we look at the histogram of Y, we see that the mean, of course, stays the same. It's about zero because this is a symmetric operation. But we see here that the standard deviation has expanded to three. So the input standard deviation was one. But now we've grown to three. And so what you're seeing in the histogram is that this Gaussian is expanding. And so we're expanding this Gaussian from the input. And we don't want that. We want most of the neural nets to have relatively similar activations. So unit Gaussian roughly throughout the neural net. And so the question is, how do we scale these W's to preserve the to preserve this distribution to remain a Gaussian? And so intuitively, if I multiply here, these elements of W by a larger number, let's say by five, then this Gaussian grows and grows in standard deviation. So now we're at 15. So basically, these numbers here in the output Y take on more and more extreme values. But if we scale it down, let's say point two, then conversely, this Gaussian is getting smaller and smaller and it's shrinking. And you can see that the standard deviation is point six. And so the question is, what do I multiply by here to exactly preserve the standard deviation to be one? And it turns out that the correct answer mathematically when you work out through the variance of this multiplication here is that you are supposed to divide by the square root of the fan in the fan in is the basically the number of input elements here. Ten. So we are supposed to divide by ten square root. And this is one way to do the square root. You raise it to a power of point five. And that's the same as doing a square root. So when you divide by the square root of ten, then we see that the output Gaussian, it has exactly standard deviation of one. Now, unsurprisingly, a number of papers have looked into how to best initialize neural networks. And in the case of multi-layer perceptrons, we can have fairly deep networks that have these nonlinearities in between. And we want to make sure that the activations are well behaved and they don't expand to infinity or shrink all the way to zero. And the question is, how do we initialize the weights so that these activations take on reasonable values throughout the network? Now, one paper that has studied this in quite a bit detail that is often referenced is this paper by Kaming Hettel called Delving Deep Interactifiers. Now, in this case, they actually study convolutional neural networks and they study especially the ReLU nonlinearity and the P-ReLU nonlinearity instead of a 10H nonlinearity. But the analysis is very similar. And basically what happens here is for them, the ReLU nonlinearity that they care about quite a bit here is a squashing function where all the negative numbers are simply clamped to zero. So the positive numbers are a pass-through, but everything negative is just set to zero. And because you are basically throwing away half of the distribution, they find in their analysis of the forward activations in the neural net that you have to compensate for that with a gain. And so here they find that basically when they initialize their weights, they have to do it with a zero-mean Gaussian whose standard deviation is square root of 2 over the fan-in. What we have here is we are initializing the Gaussian with the square root of fan-in. This nl here is the fan-in. So what we have is square root of 1 over the fan-in because we have the division here. Now, they have to add this factor of 2 because of the ReLU, which basically discards half of the distribution and clamps it at zero. And so that's where you get an initial factor. Now, in addition to that, this paper also studies not just the sort of behavior of the activations in the forward pass of the neural net, but it also studies the back propagation. And we have to make sure that the gradients also are well behaved. And so because ultimately they end up updating our parameters. And what they find here through a lot of the analysis that I invite you to read through, but it's not exactly approachable. What they find is basically if you properly initialize the forward pass, the backward pass is also approximately initialized up to a constant factor that has to do with the size of the number of hidden neurons in an early and late layer. And but basically they find empirically that this is not a choice that matters too much. Now, this timing initialization is also implemented in PyTorch. So if you go to torch.enend.init documentation, you'll find timing normal. And in my opinion, this is probably the most common way of initializing neural networks now. And it takes a few keyword arguments here. So number one, it wants to know the mode. Would you like to normalize the activations or would you like to normalize the gradients to be always Gaussian with zero mean and a unit or one standard deviation? And because they find the paper that this doesn't matter too much, most of the people just leave it as the default, which is fan in. And then second, passing the non-linearity that you are using, because depending on the non-linearity, we need to calculate a slightly different gain. And so if your non-linearity is just linear, so there's no non-linearity, then the gain here will be one. And we have the exact same kind of formula that we've got here. But if the non-linearity is something else, we're going to get a slightly different gain. And so if we come up here to the top, we see that, for example, in the case of Brelu, this gain is a square root of two. And the reason it's a square root, because in this paper, you see how the two is inside of the square root. So the gain is a square root of two. In a case of linear or identity, we just get a gain of one. In a case of 10H, which is what we're using here, the advised gain is a five over three. And intuitively, why do we need a gain on top of the initialization? It's because 10H, just like Brelu, is a contractive transformation. So what that means is you're taking the output distribution from this matrix multiplication, and then you are squashing it in some way. Now, Brelu squashes it by taking everything below zero and clamping it to zero. 10H also squashes it because it's a contractive operation. It will take the tails and it will squeeze them in. And so in order to fight the squeezing in, we need to boost the weights a little bit so that we renormalize everything back to unit standard deviation. So that's why there's a little bit of a gain that comes out. Now, I'm skipping through this section a little bit quickly, and I'm doing that actually intentionally. And the reason for that is because about seven years ago when this paper was written, you had to actually be extremely careful with the activations and the gradients and their ranges and their histograms. And you have to be very careful with the precise setting of gains and the scrutinizing of the nonlinearities used and so on. And everything was very finicky and very fragile and very properly arranged for the neural net to train, especially if your neural net was very deep. But there are a number of modern innovations that have made everything significantly more stable and more well behaved. And it's become less important to initialize these networks exactly right. And some of those modern innovations, for example, are residual connections, which we will cover in the future. The use of a number of normalization layers, like, for example, batch normalization, layer normalization, group normalization. We're going to go into a lot of these as well. And number three, much better optimizers, not just stochastic gradient descent, the simple optimizer we're basically using here, but slightly more complex optimizers like RMSProp and especially Adam. And so all of these modern innovations make it less important for you to precisely calibrate the initialization of the neural net. All that being said, in practice, what should we do in practice when I initialize these neural nets? I basically just normalize my weights by the square root of the fan in. So basically, roughly what we did here is what I do. Now, if we want to be exactly accurate here, we can go by in it of kind of normal. This is how we would implement it. We want to set the standard deviation to be gained over the square root of fan in. Right. So to set the standard deviation of our weights, we will proceed as follows. Basically, when we have torshtart-Rannon, and let's say I just create a thousand numbers, we can look at the standard deviation of this. And of course, that's one. That's the amount of spread. Let's make this a bit bigger so it's closer to one. So that's the spread of the Gaussian of zero mean and unit standard deviation. Now, basically, when you take these and you multiply by, say, point two, that basically scales down the Gaussian and that makes its standard deviation point two. So basically, the number that you multiply by here ends up being the standard deviation of this Gaussian. So here, this is a standard deviation point two Gaussian here when we sample our W one. But we want to set the standard deviation to gain over square root of fan mode, which is fan in. So in other words, we want to multiply by gain, which for 10 H is five over three. Five over three is the gain and then times. I guess I divide square root of the fan in. And in this example here, the fan in was 10. And I just noticed that actually here the fan in for W one is actually an embed times block size, which, as you will recall, is actually 30. And that's because each character is 10 dimensional. But then we have three of them and we concatenate them. So actually, the fan in here was 30 and I should have used 30 here probably. But basically, we want 30 square root. So this is the number. This is what our standard deviation we want to be. And this number turns out to be point three. Whereas here, just by fiddling with it and looking at the distribution and making sure it looks OK, we came up with point two. And so instead, what we want to do here is we want to make the standard deviation be five over three, which is our gain divide this amount times point two square root. And these brackets here are not that necessary, but I'll just put them here for clarity. This is basically what we want. This is the timing in it. In our case, for a 10H non-linearity. And this is how we would initialize the neural net. And so we're multiplying by point three instead of multiplying by point two. And so we can we can initialize this way and then we can train the neural net and see what we got. OK, so I trained the neural net and we end up in roughly the same spot. So looking at the validation loss, we now get two point one zero. And previously we also had two point one zero. And there's a little bit of a difference, but that's just the randomness of the process, I suspect. But the big deal, of course, is we get to the same spot. But we did not have to introduce any magic numbers that we got from just looking at histograms and guessing, checking. We have something that is semi principled and will scale us to much bigger networks and something that we can sort of use as a guide. So I mentioned that the precise setting of these initializations is not as important today due to some modern innovations. And I think now is a pretty good time to introduce one of those modern innovations, and that is batch normalization. So batch normalization came out in 2015 from a team at Google. And it was an extremely impactful paper because it made it possible to train very deep neural nets quite reliably and it basically just worked. So here's what batch normalization does and let's implement it. Basically, we have these hidden states H preact, right? And we were talking about how we don't want these these pre activation states to be way too small because then the 10H is not doing anything. But we don't want them to be too large because then the 10H is saturated. In fact, we want them to be roughly, roughly Gaussian. So zero mean and a unit or one standard deviation, at least at initialization. So the insight from the batch normalization paper is, OK, you have these hidden states and you'd like them to be roughly Gaussian. Then why not take the hidden states and just normalize them to be Gaussian? And it sounds kind of crazy, but you can just do that because standardizing hidden states so that their unit Gaussian is a perfectly differentiable operation, as we'll soon see. And so that was kind of like the big insight in this paper. And when I first read it, my mind was blown because you can just normalize these hidden states. And if you'd like unit Gaussian states in your network, at least initialization, you can just normalize them to be Gaussian. So let's see how that works. So we're going to scroll to our pre activations here just before they enter into the 10H. Now, the idea, again, is, remember, we're trying to make these roughly Gaussian. And that's because if these are way too small numbers, then the 10H here is kind of inactive. But if these are very large numbers, then the 10H is way too saturated and great in the flow. So we'd like this to be roughly Gaussian. So the insight in batch normalization, again, is that we can just standardize these activations so they are exactly Gaussian. So here, Hpreact has a shape of 32 by 200, 32 examples by 200 neurons in the hidden layer. So basically what we can do is we can take Hpreact and we can just calculate the mean. And the mean we want to calculate across the zero dimension. And we want to also keep them as true so that we can easily broadcast this. So the shape of this is 1 by 200. In other words, we are doing the mean over all the elements in the batch. And similarly, we can calculate the standard deviation of these activations. And that will also be 1 by 200. Now, in this paper, they have the sort of prescription here. And see here, we are calculating the mean, which is just taking the average value of any neurons activation. And then the standard deviation is basically kind of like the measure of the spread that we've been using, which is the distance of every one of these values away from the mean and that squared and averaged. That's the variance. And then if you want to take the standard deviation, you would square root the variance to get the standard deviation. So these are the two that we're calculating. And now we're going to normalize or standardize these x's by subtracting the mean and dividing by the standard deviation. So basically, we're taking entropy act and we subtract the mean. And then we divide by the standard deviation. And this is exactly what these two STD and mean are calculating. Sorry, this is the mean and this is the variance. You see how the sigma is the standard deviation usually. So this is sigma square, which is variance is the square of the standard deviation. So this is how you standardize these values. And what this will do is that every single neuron now and its firing rate will be exactly unit Gaussian on these 32 examples, at least, of this batch. That's why it's called batch normalization. So we're normalizing these batches. And then we could, in principle, train this. Notice that calculating the mean and standard deviation, these are just mathematical formulas. They're perfectly differentiable. All this is perfectly differentiable. And we can just train this. The problem is you actually won't achieve a very good result with this. And the reason for that is we want these to be roughly Gaussian, but only at initialization. So we want to be forced to be Gaussian always. We'd like to allow the neural nets to move this around to potentially make it more diffuse, to make it more sharp, to make some 10H neurons maybe more trigger happy or less trigger happy. So we'd like this distribution to move around. And we'd like the back propagation to tell us how the distribution should move around. In addition to this idea of standardizing the activations at any point in the network, we have to also introduce this additional component in the paper here, described as scale and shift. And so basically what we're doing is we're taking these normalized inputs and we are additionally scaling them by some gain and offsetting them by some bias to get our final output from this layer. And so what that amounts to is the following. We are going to allow a batch normalization gain to be initialized at just once. And the once will be in the shape of one by n hidden. And then we also will have a bn bias, which will be torched at zeros. And it will also be of the shape n by one by n hidden. And then here the bn gain will multiply this and the bn bias will offset it here. So because this is initialized to one and this to zero, at initialization, each neuron's firing values in this batch will be exactly unit Gaussian and will have nice numbers. No matter what the distribution of the HB act is coming in, coming out, it will be unit Gaussian for each neuron. And that's roughly what we want, at least at initialization. And then during optimization, we'll be able to back propagate to bn gain and bn bias and change them. So the network is given the full ability to do with this whatever it wants internally. Here we just have to make sure that we include these in the parameters of the neural net because they will be trained with back propagation. So let's initialize this and then we should be able to train. And then we're going to also copy this line, which is the batch normalization layer here on a single line of code. And we're going to swing down here and we're also going to do the exact same thing at test time here. So similar to train time, we're going to normalize and then scale. And that's going to give us our train and validation loss. And we'll see in a second that we're actually going to change this a little bit, but for now, I'm going to keep it this way. So I'm just going to wait for this to converge. OK, so I allowed the neural nets to converge here and when we scroll down, we see that our validation loss here is two point one zero roughly, which I wrote down here. And we see that this is actually kind of comparable to some of the results that we've achieved previously. Now, I'm not actually expecting an improvement in this case, and that's because we are dealing with a very simple neural net that has just a single hidden layer. So, in fact, in this very simple case of just one hidden layer, we were able to actually calculate what the scale of W should be to make these pre activations already have a roughly Gaussian shape. So the batch normalization is not doing much here, but you might imagine that once you have a much deeper neural net that has lots of different types of operations. And there's also, for example, residual connections, which will cover and so on. It will become basically very, very difficult to tune the scales of your weight matrices, such that all the activations throughout the neural net are roughly Gaussian. And so that's going to become very quickly intractable. But compared to that, it's going to be much, much easier to sprinkle batch normalization layers throughout the neural net. So in particular, it's common to look at every single linear layer like this one. This is a linear layer multiplying by weight matrix and adding the bias. Or, for example, convolutions, which we'll cover later and also perform basically a multiplication with the weight matrix, but in a more spatially structured format. It's customary to take these linear layer or convolutional layer and append a batch normalization layer right after it to control the scale of these activations at every point in the neural net. So we'd be adding these batch normal layers throughout the neural net. And then this controls the scale of these activations throughout the neural net. It doesn't require us to do perfect mathematics and care about the activation distributions for all these different types of neural network, Lego building blocks that you might want to introduce into your neural net. And it significantly stabilizes the train. And that's why these layers are quite popular. Now, the stability offered by batch normalization actually comes at a terrible cost. And that cost is that if you think about what's happening here, something terribly strange and unnatural is happening. It could be that we have a single example feeding into a neural net and then we calculate this activations and its logits. And this is a deterministic sort of process. So you arrive at some logits for this example. And then because of efficiency of training, we suddenly started to use batches of examples. But those batches of examples were processed independently and it was just an efficiency thing. But now suddenly in batch normalization, because of the normalization through the batch, we are coupling these examples mathematically and in the forward pass and the backward pass of the neural net. So now the hidden state activations, Hpreact and your logits for any one input example are not just a function of that example and its input, but they're also a function of all the other examples that happen to come for a ride in that batch. And these examples are sampled randomly. And so what's happening is, for example, when you look at Hpreact that's going to feed into H, the hidden state activations, for example, for any one of these input examples, is going to actually change slightly depending on what other examples there are in the batch. And depending on what other examples happen to come for a ride, H is going to change suddenly and it's going to like jitter, if you imagine sampling different examples, because the statistics of the mean and the standard deviation are going to be impacted. And so you'll get a jitter for H and you'll get a jitter for logits. And you think that this would be a bug or something undesirable, but in a very strange way, this actually turns out to be good in neural network training as a side effect. And the reason for that is that you can think of this as kind of like a regularizer, because what's happening is you have your input and you get your H, and then depending on the other examples, this is jittering a bit. And so what that does is that it's effectively padding out any one of these input examples and it's introducing a little bit of entropy. And because of the padding out, it's actually kind of like a form of data augmentation, which we'll cover in the future. And it's kind of like augmenting the input a little bit and jittering it. And that makes it harder for the neural nets to overfit these concrete specific examples. So by introducing all this noise, it actually like pads out the examples and it regularizes the neural net. And that's one of the reasons why the seemingly as a second order effect, this is actually a regularizer. And that has made it harder for us to remove the use of batch normalization, because basically no one likes this property that the examples in the batch are coupled mathematically and in the forward pass. And it leads to all kinds of like strange results. We'll go into some of that in a second as well. And it leads to a lot of bugs and so on. And so no one likes this property. And so people have tried to deprecate the use of batch normalization and move to other normalization techniques that do not couple the examples of a batch. Examples are layer normalization, instance normalization, group normalization, and so on. And we'll cover some of these later. But basically, long story short, batch normalization was the first kind of normalization layer to be introduced. It worked extremely well. It happens to have this regularizing effect. It stabilized training. And people have been trying to remove it and move to some of the other normalization techniques. But it's been hard because it just works quite well. And some of the reason that it works quite well is, again, because of this regularizing effect and because it is quite effective at controlling the activations and their distributions. So that's kind of like the brief story of batch normalization. And I'd like to show you one of the other weird sort of outcomes of this coupling. So here's one of the strange outcomes that I only glossed over previously when I was evaluating the loss on the validation set. Basically, once we've trained a neural net, we'd like to deploy it in some kind of a setting. And we'd like to be able to feed in a single individual example and get a prediction out from our neural net. But how do we do that when our neural net now in the forward pass estimates the statistics of the mean and standard deviation of a batch? The neural net expects batches as an input now. So how do we feed in a single example and get sensible results out? And so the proposal in the batch normalization paper is the following. What we would like to do here is we would like to basically have a step after training that calculates and sets the batch mean and standard deviation a single time over the training set. And so I wrote this code here in the interest of time. And we're going to call what's called calibrate the batch norm statistics. And basically what we do is torch not no grad telling PyTorch that none of this we will call the dot backward on. And it's going to be a bit more efficient. We're going to take the training set, get the preactivations for every single training example, and then one single time estimate the mean and standard deviation over the entire training set. And then we're going to get B and mean and B and standard deviation. And now these are fixed numbers estimating over the entire training set. And here, instead of estimating it dynamically, we are going to instead here use B and mean. And here we're just going to use B and standard deviation. And so at test time, we are going to fix these, clamp them and use them during inference. And now you see that we get basically identical result. But the benefit that we've gained is that we can now also forward a single example because the mean and standard deviation are now fixed sort of tensors. That said, nobody actually wants to estimate this mean and standard deviation as a second stage after neural network training because everyone is lazy. And so this batch normalization paper actually introduced one more idea, which is that we can estimate the mean and standard deviation in a running manner during training of the neural net. And then we can simply just have a single stage of training. And on the side of that training, we are estimating the running mean and standard deviation. So let's see what that would look like. Let me basically take the mean here that we are estimating on the batch. And let me call this B and mean on the i-th iteration. And then here this is B and STD. B and STD at i. And the mean comes here and the STD comes here. So so far I've done nothing. I've just moved around and I created these extra variables for the mean and standard deviation. And I've put them here. So so far nothing has changed. But what we're going to do now is we're going to keep a running mean of both of these values during training. So let me swing up here and let me create a BN mean underscore running. And I'm going to initialize it at zeros and then B and STD running, which are initialized at once. Because in the beginning, because of the way we initialized W1 and B1, HpAX will be roughly unit Gaussian. So the mean will be roughly zero and the standard deviation roughly one. So I'm going to initialize these that way. But then here I'm going to update these. And in PyTorch, these mean and standard deviation that are running, they're not actually part of the gradient based optimization. We're never going to derive gradients with respect to them. They're they're updated on the side of training. And so what we're going to do here is we're going to say with torch.nograd telling PyTorch that the update here is not supposed to be building out a graph because there will be no dot backward. But this running mean is basically going to be zero point nine nine nine times the current value plus zero point zero zero one times the this value, this new mean. And in the same way, BN STD running will be mostly what it used to be. But it will receive a small update in the direction of what the current standard deviation is. And as you're seeing here, this update is outside and on the side of the gradient based optimization. And it's simply being updated, not using gradient descent. It's just being updated using a Genki like smooth sort of running mean manner. And so while the network is training and these pre activations are sort of changing and shifting around during during back propagation, we are keeping track of the typical mean and standard deviation. And we're estimating them once. And when I run this, now I'm keeping track of this in a running manner. And what we're hoping for, of course, is that the BN mean underscore running and BN mean underscore STD are going to be very similar to the ones that we calculated here before. And that way we don't need a second stage because we've sort of combined the two stages and we've put them on the side of each other. If you want to look at it that way. And this is how this is also implemented in the batch normalization layer in PyTorch. So during training, the exact same thing will happen. And then later when you're using inference, it will use the estimated running mean of both the mean and standard deviation of those hidden states. So let's wait for the optimization to converge and hopefully the running mean and standard deviation are roughly equal to these two. And then we can simply use it here. And we don't need this stage of explicit calibration at the end. OK, so the optimization finished. I'll rerun the explicit estimation. And then the BN mean from the explicit estimation is here. And BN mean from the running estimation during the during the optimization, you can see is very, very similar. It's not identical, but it's pretty close. And the same way BNSTD is this and BNSTD running is this. As you can see that once again, they are fairly similar values, not identical, but pretty close. And so then here instead of BN mean, we can use the BN mean running instead of BNSTD. We can use BNSTD running. And hopefully the validation loss will not be impacted too much. So basically identical. And this way we've eliminated the need for this explicit stage of calibration because we are doing it in line over here. OK, so we're almost done with batch normalization. There are only two more notes that I'd like to make. Number one, I've skipped a discussion over what is this plus epsilon doing here. This epsilon is usually like some small fixed number, for example, 1e negative 5 by default. And what it's doing is that it's basically preventing a division by zero. In the case that the variance over your batch is exactly zero. In that case, here we normally have a division by zero. But because of the plus epsilon, this is going to become a small number in the denominator instead. And things will be more well behaved. So feel free to also add a plus epsilon here of a very small number. It doesn't actually substantially change the result. I'm going to skip it in our case just because this is unlikely to happen in our very simple example here. And the second thing I want you to notice is that we're being wasteful here. And it's very subtle. But right here where we are adding the bias into Hpreact, these biases now are actually useless because we're adding them to the Hpreact. But then we are calculating the mean for every one of these neurons and subtracting it. So whatever bias you add here is going to get subtracted right here. And so these biases are not doing anything. In fact, they're being subtracted out and they don't impact the rest of the calculation. So if you look at b1.grad, it's actually going to be zero because it's being subtracted out and doesn't actually have any effect. And so whenever you're using batch normalization layers, then if you have any weight layers before, like a linear or a conv or something like that, you're better off coming here and just like not using bias. So you don't want to use bias. And then here you don't want to add it because that's spurious. Instead, we have this batch normalization bias here. And that batch normalization bias is now in charge of the biasing of this distribution instead of this b1 that we had here originally. And so basically, the batch normalization layer has its own bias. And there's no need to have a bias in the layer before it because that bias is going to be subtracted out anyway. So that's the other small detail to be careful with sometimes. It's not going to do anything catastrophic. This b1 will just be useless. It will never get any gradient. It will not learn. It will stay constant and it's just wasteful, but it doesn't actually really impact anything otherwise. OK, so I rearranged the code a little bit with comments. And I just wanted to give a very quick summary of the batch normalization layer. We are using batch normalization to control the statistics of activations in the neural net. It is common to sprinkle batch normalization layer across the neural net. And usually we will place it after layers that have multiplications, for example, a linear layer or a convolutional layer, which we may cover in the future. Now, the batch normalization internally has parameters for the gain and the bias. And these are trained using backpropagation. It also has two buffers. The buffers are the mean and the standard deviation, the running mean and the running mean of the standard deviation. And these are not trained using backpropagation. These are trained using this janky update of kind of like a running mean update. So these are sort of the parameters and the buffers of the batch normalization layer. And then really what it's doing is it's calculating the mean and the standard deviation of the activations that are feeding into the batch normalization layer over that batch. Then it's centering that batch to be unit Gaussian. And then it's offsetting and scaling it by the learned bias and gain. And then on top of that, it's keeping track of the mean and standard deviation of the inputs. And it's maintaining this running mean and standard deviation. And this will later be used at inference so that we don't have to re-estimate the mean and standard deviation all the time. And in addition, that allows us to basically forward individual examples at test time. So that's the batch normalization layer. It's a fairly complicated layer, but this is what it's doing internally. Now, I wanted to show you a little bit of a real example. So you can search ResNet, which is a residual neural network. And these are contacts of neural networks used for image classification. And of course, we haven't come to ResNet in detail, so I'm not going to explain all the pieces of it. But for now, just note that the image feeds into a ResNet on the top here. And there's many, many layers with repeating structure all the way to predictions of what's inside that image. This repeating structure is made up of these blocks. And these blocks are just sequentially stacked up in this deep neural network. Now, the code for this, the block basically that's used and repeated sequentially in series, is called this bottleneck block. And there's a lot here. This is all PyTorch. And of course, we haven't covered all of it, but I want to point out some small pieces of it. Here in the init is where we initialize the neural net. So this code block here is basically the kind of stuff we're doing here. We're initializing all the layers. And in the forward, we are specifying how the neural net acts once you actually have the input. So this code here is along the lines of what we're doing here. And now these blocks are replicated and stacked up serially, and that's what a residual network would be. And so notice what's happening here. Conv1, these are convolution layers. And these convolution layers, basically, they're the same thing as a linear layer, except convolution layers don't apply. Convolution layers are used for images. And so they have spatial structure, and basically this linear multiplication and bias offset are done on patches instead of the full input. So because these images have structure, spatial structure, convolutions just basically do wx plus b, but they do it on overlapping patches of the input. But otherwise, it's wx plus b. Then we have the normal layer, which by default here is initialized to be a batch norm in 2D, so a two dimensional batch normalization layer. And then we have a non-linearity like ReLU. So instead of here they use ReLU, we are using 10h in this case. But both are just non-linearities, and you can just use them relatively interchangeably. For very deep networks, ReLU typically empirically work a bit better. So see the motif that's being repeated here. We have convolution, batch normalization, ReLU, convolution, batch normalization, ReLU, et cetera. And then here this is a residual connection that we haven't covered yet. But basically that's the exact same pattern we have here. We have a weight layer, like a convolution or like a linear layer, batch normalization, and then 10h, which is a non-linearity. But basically a weight layer, a normalization layer, and non-linearity. And that's the motif that you would be stacking up when you create these deep neural networks, exactly as it's done here. And one more thing I'd like you to notice is that here when they are initializing the comp layers, like comp one by one, the depth for that is right here. And so it's initializing an nn.conf2d, which is a convolution layer in PyTorch. And there's a bunch of keyword arguments here that I'm not going to explain yet. But you see how there's bias equals false. The bias equals false is exactly for the same reason as bias is not used in our case. You see how I erase the use of bias. And the use of bias is spurious because after this weight layer, there's a batch normalization. And the batch normalization subtracts that bias and then has its own bias. So there's no need to introduce these spurious parameters. It wouldn't hurt performance. It's just useless. And so because they have this motif of conv, batch, and relu, they don't need a bias here, because there's a bias inside here. So, by the way, this example here is very easy to find. Just do ResNet PyTorch. And it's this example here. So this is kind of like the stock implementation of a residual neural network in PyTorch. And you can find that here. But of course, I haven't covered many of these parts yet. And I would also like to briefly descend into the definitions of these PyTorch layers and the parameters that they take. Now, instead of a convolutional layer, we're going to look at a linear layer because that's the one that we're using here. This is a linear layer. And I haven't covered convolutions yet. But as I mentioned, convolutions are basically linear layers except on patches. So a linear layer performs a wx plus b, except here they're calling the w a transpose. So the conv is wx plus b, very much like we did here. To initialize this layer, you need to know the fan in, the fan out. And that's so that they can initialize this w. This is the fan in and the fan out. So they know how big the weight matrix should be. You need to also pass in whether or not you want a bias. And if you set it to false, then no bias will be inside this layer. And you may want to do that exactly like in our case if your layer is followed by a normalization layer such as batch norm. So this allows you to basically disable bias. In terms of the initialization, if we swing down here, this is reporting the variables used inside this linear layer. And our linear layer here has two parameters, the weight and the bias. In the same way, they have a weight and a bias. And they're talking about how they initialize it by default. So by default, PyTorch will initialize your weights by taking the fan in and then doing one over fan in square root. And then instead of a normal distribution, they are using a uniform distribution. So it's very much the same thing, but they are using a one instead of five over three. So there's no gain being calculated here. It's just one, but otherwise it's exactly one over the square root of fan in exactly as we have here. So one over the square root of k is the scale of the weights. But when they are drawing the numbers, they're not using a Gaussian by default. They're using a uniform distribution by default. And so they draw uniformly from negative square root of k to square root of k. But it's the exact same thing and the same motivation with respect to what we've seen in this lecture. And the reason they're doing this is if you have a roughly Gaussian input, this will ensure that out of this layer, you will have a roughly Gaussian output. And you basically achieve that by scaling the weights by one over the square root of fan in. So that's what this is doing. And then the second thing is the batch normalization layer. So let's look at what that looks like in PyTorch. So here we have a one dimensional batch normalization layer exactly as we are using here. And there are a number of keyword arguments going into it as well. So we need to know the number of features. For us, that is 200. And that is needed so that we can initialize these parameters here. The gain, the bias, and the buffers for the running mean and standard deviation. Then they need to know the value of epsilon here. And by default, this is one negative five. So we don't typically change this too much. Then they need to know the momentum. And the momentum here, as they explain, is basically used for these running mean and running standard deviation. So by default, the momentum here is 0.1. The momentum we are using here in this example is 0.001. And basically, you may want to change this sometimes. And roughly speaking, if you have a very large batch size, then typically what you'll see is that when you estimate the mean and the standard deviation, for every single batch size, if it's large enough, you're going to get roughly the same result. And so therefore, you can use slightly higher momentum, like 0.1. But for a batch size as small as 32, the mean and standard deviation here might take on slightly different numbers, because there's only 32 examples we are using to estimate the mean and standard deviation. So the value is changing around a lot. And if your momentum is 0.1, that might not be good enough for this value to settle. And converge to the actual mean and standard deviation over the entire training set. And so basically, if your batch size is very small, momentum of 0.1 is potentially dangerous. And it might make it so that the running mean and standard deviation is thrashing too much during training, and it's not actually converging properly. affine equals true determines whether this batch normalization layer has these learnable affine parameters, the gain and the bias. This is almost always kept true. I'm not actually sure why you would want to change this to false. Then track running stats is determining whether or not batch normalization layer of PyTorch will be doing this. And one reason you may want to skip the running stats is because you may want to, for example, estimate them at the end as a stage two, like this. And in that case, you don't want the batch normalization layer to be doing all this extra compute that you're not going to use. And finally, we need to know which device we're going to run this batch normalization on, a CPU or a GPU, and what the data type should be, half precision, single precision, double precision, and so on. So that's the batch normalization layer. Otherwise, they link to the paper. It's the same formula we've implemented, and everything is the same exactly as we've done here. OK, so that's everything that I wanted to cover for this lecture. Really, what I wanted to talk about is the importance of understanding the activations and the gradients and their statistics in neural networks. And this becomes increasingly important, especially as you make your neural networks bigger, larger, and deeper. We looked at the distributions basically at the output layer, and we saw that if you have two confident mispredictions because the activations are too messed up at the last layer, you can end up with these hockey stick losses. And if you fix this, you get a better loss at the end of training because your training is not doing wasteful work. Then we also saw that we need to control the activations. We don't want them to squash to zero or explode to infinity, because that you can run into a lot of trouble with all of these nonlinearities in these neural nets. And basically, you want everything to be fairly homogeneous throughout the neural net. You want roughly Gaussian activations throughout the neural net. Then we talked about, OK, if we want roughly Gaussian activations, how do we scale these weight matrices and biases during initialization of the neural net so that we don't get, you know, so everything is as controlled as possible. So that gave us a large boost in improvement. And then I talked about how that strategy is not actually possible for much, much deeper neural nets, because when you have much deeper neural nets with lots of different types of layers, it becomes really, really hard to precisely set the weights and the biases in such a way that the activations are roughly uniform throughout the neural net. So then I introduced the notion of the normalization layer. Now, there are many normalization layers that people use in practice. Bash normalization, layer normalization, instance normalization, group normalization. We haven't covered most of them, but I've introduced the first one. And also the one that I believe came out first, and that's called bash normalization. And we saw how bash normalization works. This is a layer that you can sprinkle throughout your deep neural net. And the basic idea is if you want roughly Gaussian activations, well, then take your activations and take the mean and standard deviation and center your data. And you can do that because the centering operation is differentiable. But on top of that, we actually had to add a lot of bells and whistles, and that gave you a sense of the complexities of the bash normalization layer, because now we're centering the data, that's great, but suddenly we need the gain and the bias. And now those are trainable. And then because we are coupling all the training examples, now suddenly the question is how do you do the inference? Where to do the inference, we need to now estimate these mean and standard deviation once over the entire training set and then use those at inference. But then no one likes to do stage two. So instead we fold everything into the bash normalization layer during training and try to estimate these in the running manner so that everything is a bit simpler. And that gives us the bash normalization layer. And as I mentioned, no one likes this layer. It causes a huge amount of bugs. And intuitively it's because it is coupling examples in the forward pass of the neural net. And I've shot myself in the foot with this layer over and over again in my life, and I don't want you to suffer the same. So basically try to avoid it as much as possible. Some of the other alternatives to these layers are, for example, group normalization or layer normalization, and those have become more common in more recent deep learning, but we haven't covered those yet. But definitely bash normalization was very influential at the time when it came out in roughly 2015 because it was kind of the first time that you could train reliably much deeper neural nets. And fundamentally the reason for that is because this layer was very effective at controlling the statistics of the activations in the neural net. So that's the story so far, and that's all I wanted to cover. And in the future lectures, hopefully we can start going into recurrent neural nets. And recurrent neural nets, as we'll see, are just very, very deep networks because you unroll the loop when you actually optimize these neural nets. And that's where a lot of this analysis around the activation statistics and all these normalization layers will become very, very important for good performance. So we'll see that next time. Bye. And I would like us to do one more summary here as a bonus. And I think it's useful as to have one more summary of everything I've presented in this lecture. But also I would like us to start by torchifying our code a little bit. So it looks much more like what you would encounter in PyTorch. So you'll see that I will structure our code into these modules, like a linear module and a batch form module. And I'm putting the code inside these modules so that we can construct neural networks very much like we would construct them in PyTorch. And I will go through this in detail. So we'll create our neural net. Then we will do the optimization loop as we did before. And then the one more thing that I want to do here is I want to look at the activation statistics, both in the forward pass and in the backward pass. And then here we have the evaluation and sampling just like before. So let me rewind all the way up here and go a little bit slower. So here I am creating a linear layer. You'll notice that torch.nn has lots of different types of layers. And one of those layers is the linear layer. Torch.nn.linear takes a number of input features, output features, whether or not we should have bias, and then the device that we want to place this layer on, and the data type. So I will omit these two, but otherwise we have the exact same thing. We have the fan in, which is the number of inputs, fan out, the number of outputs, and whether or not we want to use a bias. And internally inside this layer, there's a weight and a bias, if you'd like it. It is typical to initialize the weight using, say, random numbers drawn from a Gaussian. And then here's the timing initialization that we discussed already in this lecture. And that's a good default and also the default that I believe PyTorch uses. And by default, the bias is usually initialized to zeros. Now, when you call this module, this will basically calculate w times x plus b, if you have nb. And then when you also call the parameters on this module, it will return the tensors that are the parameters of this layer. Now, next we have the BatchNormalization layer. So I've written that here. And this is very similar to PyTorch.nn.batchNormal1d layer, as shown here. So I'm kind of taking these three parameters here, the dimensionality, the epsilon that we will use in the division, and the momentum that we will use in keeping track of these running stats, the running mean and the running variance. Now, PyTorch actually takes quite a few more things, but I'm assuming some of their settings. So for us, I'll find will be true. That means that we will be using a gamma and beta after the normalization. The track running stats will be true. So we will be keeping track of the running mean and the running variance in the BatchNormal. Our device by default is the CPU, and the data type by default is float32. So those are the defaults. Otherwise, we are taking all the same parameters in this BatchNormal layer. So first, I'm just saving them. Now, here's something new. There's a.training, which by default is true. And PyTorch.nn modules also have this attribute.training. And that's because many modules, and BatchNormal is included in that, have a different behavior, whether you are training your enrollment or whether you are running it in an evaluation mode and calculating your evaluation laws or using it for inference on some test examples. And BatchNormal is an example of this, because when we are training, we are going to be using the mean and the variance estimated from the current batch. But during inference, we are using the running mean and running variance. And so also, if we are training, we are updating mean and variance. But if we are testing, then these are not being updated. They are kept fixed. And so this flag is necessary and by default true, just like in PyTorch. Now, the parameters of BatchNormal 1D are the gamma and the beta here. And then the running mean and running variance are called buffers in PyTorch nomenclature. And these buffers are trained using exponential moving average here explicitly. And they are not part of the back propagation and stochastic gradient descent. So they are not sort of like parameters of this layer. And that's why when we have parameters here, we only return gamma and beta. We do not return the mean and the variance. This is trained sort of like internally here every forward pass using exponential moving average. So that's the initialization. Now, in a forward pass, if we are training, then we use the mean and the variance estimated by the batch. Let me pull up the paper here. We calculate the mean and the variance. Now, up above, I was estimating the standard deviation and keeping track of the standard deviation here in the running standard deviation instead of running variance. But let's follow the paper exactly. Here they calculate the variance, which is the standard deviation squared. And that's what's kept track of in the running variance instead of a running standard deviation. But those two would be very, very similar, I believe. If we are not training, then we use the running mean and variance. We normalize. And then here I am calculating the output of this layer. And I'm also assigning it to an attribute called dot out. Now, dot out is something that I'm using in our modules here. This is not what you would find in PyTorch. We are slightly deviating from it. I'm creating a dot out because I would like to very easily maintain all those variables so that we can create statistics of them and plot them. But PyTorch and modules will not have a dot out attribute. And finally, here we are updating the buffers using, again, as I mentioned, exponential moving average, given the provided momentum. And importantly, you'll notice that I'm using the torch.nograd context manager. And I'm doing this because if we don't use this, then PyTorch will start building out an entire computational graph out of these tensors because it is expecting that we will eventually call it dot backward. But we are never going to be calling dot backward on anything that includes running mean and running variance. So that's why we need to use this context manager so that we are not sort of maintaining them using all this additional memory. So this will make it more efficient. And it's just telling PyTorch that there will be no backward. We just have a bunch of tensors. We want to update them. That's it. And then we return. OK, now scrolling down, we have the 10H layer. This is very, very similar to torch.10H. And it doesn't do too much. It just calculates 10H, as you might expect. So that's torch.10H. And there's no parameters in this layer. But because these are layers, it now becomes very easy to sort of like stack them up into basically just a list. And we can do all the initializations that we're used to. So we have the initial sort of embedding matrix. We have our layers and we can call them sequentially. And then again, with torch.nograd, there's some initializations here. So we want to make the output softmax a bit less confident, like we saw. And in addition to that, because we are using a six-layer multilayer perceptron here, so you see how I'm stacking linear, 10H, linear, 10H, et cetera, I'm going to be using the gain here. And I'm going to play with this in a second. So you'll see how when we change this, what happens to the statistics. Finally, the parameters are basically the embedding matrix and all the parameters in all the layers. And notice here, I'm using a double list comprehension, if you want to call it that. But for every layer in layers and for every parameter in each of those layers, we are just stacking up all those piece, all those parameters. Now, in total, we have 46,000 parameters. And I'm telling PyTorch that all of them require gradient. Then here, we have everything here we are actually mostly used to. We are sampling batch. We are doing a forward pass. The forward pass now is just a linear application of all the layers in order, followed by the cross entropy. And then in the backward pass, you'll notice that for every single layer, I now iterate over all the outputs, and I'm telling PyTorch to retain the gradient of them. And then here, we are already used to all the gradients set to none. Do the backward to fill in the gradients, do an update using stochastic gradient send, and then track some statistics. And then I am going to break after a single iteration. Now, here in this cell, in this diagram, I'm visualizing the histograms of the forward pass activations, and I'm specifically doing it at the 10 each layers. So iterating over all the layers, except for the very last one, which is basically just the softmax layer. If it is a 10 H layer, and I'm using a 10 H layer just because they have a finite output, negative one to one. And so it's very easy to visualize here. So you see negative one to one. It's a finite range and easy to work with. I take the out tensor from that layer into T, and then I'm calculating the mean, the standard deviation and the percent saturation of T. And the way I define the percent saturation is that T dot absolute value is greater than point nine seven. So that means we are here at the tails of the 10 H. And remember that when we are in the tails of the 10 H, that will actually stop gradients. So we don't want this to be too high. Now, here I'm calling torch dot histogram, and then I am plotting this histogram. So basically what this is doing is that every different type of layer and they all have a different color. We are looking at how many values in these testers take on any of the values below on this axis here. So the first layer is fairly saturated here at 20 percent. So you can see that it's got tails here, but then everything sort of stabilizes. And if we had more layers here, it would actually just stabilize it around the standard deviation of about point six five and the saturation would be roughly five percent. And the reason that this stabilizes and gives us a nice distribution here is because gain is set to five over three. Now, here, this game, you see that by default, we initialize with one over square root of fan in. But then here during initialization, I come in and I iterate over all the layers. And if it's a linear layer, I boost that by the game. Now, we saw that one. So basically, if we just do not use a game, then what happens if I redraw this, you will see that the standard deviation is shrinking and the saturation is coming to zero. And basically what's happening is the first layer is, you know, pretty decent. But then further layers are just kind of like shrinking down to zero. And it's happening slowly, but it's shrinking to zero. And the reason for that is when you just have a sandwich of linear layers alone, then a then initializing our weights in this manner we saw previously would have conserved the standard deviation of one. But because we have this interspersed ten each layers in there, the standard layers are squashing functions. And so they take your distribution and they slightly squash it. And so some game is necessary to keep expanding it to fight the squashing. So it just turns out that five over three is a good value. So if we have something too small, like one, we saw that things will come towards zero. But if it's something too high, let's do two. Then here we see that. Well, let me do something a bit more extreme because so it's a bit more visible. Let's try three. OK, so we see here that the saturations are going to be way too large. OK, so three would create way too saturated activations. So five over three is a good setting for a sandwich of linear layers with ten each activations. And it roughly stabilizes the standard deviation at a reasonable point. Now, honestly, I have no idea where five or three came from in my torch. When we were looking at the coming initialization, I see empirically that it stabilizes the sandwich of linear and ten each and that the saturation is in a good range. But I don't actually know if this came out of some math formula. I tried searching briefly for where this comes from, but I wasn't able to find anything. But certainly we see that empirically, these are very nice ranges. Our saturation is roughly five percent, which is a pretty good number. And this is a good setting of the gain in this context. Similarly, we can do the exact same thing with the gradients. So here is a very same loop if it's a ten each. But instead of taking the layer that out, I'm taking the grad. And then I'm also showing the mean and the standard deviation. And I'm plotting the histogram of these values. And so you'll see that the gradient distribution is fairly reasonable. And in particular, what we're looking for is that all the different layers in this sandwich has roughly the same gradient. Things are not shrinking or exploding. So we can, for example, come here and we can take a look at what happens if this game was way too small. So this was point five. Then you see the first of all, the activations are shrinking to zero, but also the gradients are doing something weird. The gradients started out here and then now they're like expanding out. And similarly, if we, for example, have a too high of a gain, so like three, then we see that also the gradients have there's some asymmetry going on where as you go into deeper and deeper layers, the activations are also changing. And so that's not what we want. And in this case, we saw that without the use of bathroom as we are going through right now, we have to very carefully set those gains to get nice activations in both the forward pass and the backward pass. Now, before we move on to passion realization, I would also like to take a look at what happens when we have no teenage units here. So erasing all the teenage nonlinearities, but keeping the game at five over three, we now have just a giant linear sandwich. So let's see what happens to the activations. As we saw before, the correct game here is one that is the standard deviation preserving game. So one point six, six, seven is too high. And so what's going to happen now is the following. I have to change this to be linear. So we are because there's no more teenage players. And let me change this to linear as well. So we're seeing is the activations started out on the blue and have by layer four become very diffuse. So what's happening to the activations is this. And with the gradients on the top layer, the activation, the gradient statistics are the purple and then they diminish as you go down deeper in the layers. And so basically you have an asymmetry like in the neural net. And you might imagine that if you have a very deep neural networks, say like 50 layers or something like that, this just this is not a good place to be. So that's why before that normalization, this was incredibly tricky to to set in particular. If this is too large of a game, this happens. And if it's too little of a game, then this happens. So the opposite of that basically happens. Here we have a shrinking and a diffusion depending on which direction you look at it from. And so certainly this is not what you want. And in this case, the correct setting of the game is exactly one, just like we're doing at initialization. And then we see that the statistics for the forward and backward pass are well behaved. And so the reason I want to show you this is that basically like getting your own to train before these normalization layers and before the use of advanced optimizers like Adam, which we still have to cover and residual connections and so on. Training neural nets basically look like this. It's like a total balancing act. You have to make sure that everything is precisely orchestrated and you have to care about the activations and the gradients and their statistics. And then maybe you can train something. But it was basically impossible to train very deep networks. And this is fundamentally the reason for that. You'd have to be very, very careful with your initialization. The other point here is you might be asking yourself, by the way, I'm not sure if I covered this. Why do we need these 10H layers at all? Why do we include them and then have to worry about the game? And the reason for that, of course, is that if you just have a stack of linear layers, then certainly we're getting very easily nice activations and so on. But this is just a massive linear sandwich. And it turns out that it collapses to a single linear layer in terms of its representation power. So if you were to plot the output as a function of the input, you're just getting a linear function. No matter how many linear layers you stack up, you still just end up with a linear transformation. All the Wx plus b's just collapse into a large Wx plus b with slightly different Ws and slightly different b. But interestingly, even though the forward pass collapses to just a linear layer, because of backpropagation and the dynamics of the backward pass, the optimization is really is not identical. You actually end up with all kinds of interesting dynamics in the backward pass because of the way the chain rule is calculating it. And so optimizing a linear layer by itself and optimizing a sandwich of 10 linear layers, in both cases, those are just a linear transformation in the forward pass. But the training dynamics will be different. And there's entire papers that analyze, in fact, like infinitely layered linear layers and so on. And so there's a lot of things to that you can play with there. But basically, the 10-inch nonlinearities allow us to turn this sandwich from just a linear function into a neural network that can, in principle, approximate any arbitrary function. OK, so now I've reset the code to use the linear 10-inch sandwich like before, and I've reset everything. So the gain is five over three. We can run a single step of optimization and we can look at the activation statistics of the forward pass and the backward pass. But I've added one more plot here that I think is really important to look at when you're training your neural nets and to consider. And ultimately, what we're doing is we're updating the parameters of the neural net. So we care about the parameters and their values and their gradients. So here what I'm doing is I'm actually iterating over all the parameters available and then I'm only restricting it to the two dimensional parameters, which are basically the weights of these linear layers. And I'm skipping the biases and I'm skipping the gammas and the betas and the bastro just for simplicity. But you can also take a look at those as well. But what's happening with the weights is instructive by itself. So here we have all the different weights, their shapes. So this is the embedding layer, the first linear layer all the way to the very last linear layer. And then we have the mean, the standard deviation of all these parameters, the histogram. And you can see that it actually doesn't look that amazing. So there's some trouble in paradise, even though these gradients look OK. There's something weird going on here. I'll get to that in a second. And the last thing here is the gradient to data ratio. So sometimes I like to visualize this as well, because what this gives you a sense of is what is the scale of the gradient compared to the scale of the actual values. And this is important because we're going to end up taking a step update that is the learning rate times the gradient onto the data. And so the gradient has too large of magnitude. If the numbers in there are too large compared to the numbers in data, then you'd be in trouble. But in this case, the gradient to data is our low numbers. So the values inside grad are 1000 times smaller than the values inside data in these weights, most of them. Now, notably, that is not true about the last layer. And so the last layer actually here, the output layer, is a bit of a troublemaker in the way that this is currently arranged, because you can see that the last layer here in pink takes on values that are much larger than some of the values inside the neural net. So the standard deviations are roughly wanting negative three throughout, except for the last layer, which actually has roughly wanting negative two standard deviation of gradients. And so the gradients on the last layer are currently about 100 times greater, sorry, 10 times greater than all the other weights inside the neural net. And so that's problematic because in the simple stochastic gradient in the sense setup, you would be training this last layer about 10 times faster than you would be training the other layers at initialization. Now, this actually like kind of fixes itself a little bit if you train for a bit longer. So, for example, if I agree, then 1000 only then do a break. Let me initialize and then let me do it 1000 steps. And after 1000 steps, we can look at the forward pass. OK, so you see how the neurons are a bit are saturating a bit. And we can also look at the backward pass. But otherwise, they look good. They're about equal. And there's no shrinking to zero or exploding to infinities. And you can see that here in the weights, things are also stabilizing a little bit. So the tails of the last pink layer are actually coming down, coming in during the optimization. But certainly this is like a little bit troubling, especially if you are using a very simple update rule like stochastic gradient descent instead of a modern optimizer like Adam. Now I'd like to show you one more plot that I usually look at when I train neural networks. And basically, the gradient to data ratio is not actually that informative because what matters at the end is not the gradient to data ratio, but the update to the data ratio, because that is the amount by which we will actually change the data in these tensors. So coming up here, what I'd like to do is I'd like to introduce a new update to data ratio. It's going to be lost. And we're going to build it out every single iteration. And here I'd like to keep track of basically the ratio every single iteration. So without any gradients, I'm comparing the update, which is learning rate times the gradient. That is the update that we're going to apply to every parameter. So see, I'm iterating over all the parameters and then I'm taking the basically standard deviation of the update we're going to apply and divided by the actual content, the data of that parameter and its standard deviation. So this is the ratio of basically how great are the updates to the values in these tensors. Then we're going to take a log of it. And actually, I'd like to take a log 10 just so it's a nicer visualization. So we're going to be basically looking at the exponents of the division here and then that item to pop out the float. And we're going to be keeping track of this for all the parameters and adding it to these UD tensor. So now let me reinitialize and run a thousand iterations. We can look at the activations, the gradients and the parameter gradients as we did before. But now I have one more plot here to introduce. What's happening here is where every interval of the parameters and I'm constraining it again, like I did here, to just the weights. So the number of dimensions in these sensors is two. And then I'm basically plotting all of these update ratios over time. So when I plot this, I plot those ratios and you can see that they evolve over time during initialization to take on certain values. And then these updates are like start stabilizing usually during training. Then the other thing that I'm plotting here is I'm plotting here like an approximate value that is a rough guide for what it roughly should be. And it should be like roughly one in negative three. And so that means that basically there's some values in this tensor and they take on certain values and the updates to them at every single iteration are no more than roughly one thousandth of the actual magnitude in those tensors. If this was much larger, like, for example, if this was if the log of this was like, say, negative one, this is actually updating those values quite a lot. They're undergoing a lot of change. But the reason that the final rate, the final layer here is an outlier is because this layer was artificially shrunk down to keep the softmax unconfident. So here you see how we multiply the weight by point one in the initialization to make the last layer prediction less confident. That made that artificially made the values inside that tensor way too low. And that's why we're getting temporarily a very high ratio. But you see that that stabilizes over time once that weight starts to learn starts to learn. But basically, I like to look at the evolution of this update ratio for all my parameters usually. And I like to make sure that it's not too much above one in negative three roughly. So around negative three on this log plot. If it's below negative three, usually that means that the parameters are not training fast enough. So if our learning rate was very low, let's do that experiment. Let's initialize and then let's actually do a learning rate of, say, one in negative three here. So zero point zero zero one. If your learning rate is way too low. This plot will typically reveal it. So you see how all of these updates are way too small. So the size of the update is basically 10,000 times in magnitude to the size of the numbers in that tensor in the first place. So this is a symptom of training way too slow. So this is another way to sometimes set the learning rate and to get a sense of what that learning rate should be. And ultimately, this is something that you would keep track of. If anything, the learning rate here is a little bit on the higher side because you see that we're above the black line of negative three. We're somewhere around negative two point five. It's like, OK, and but everything is like somewhat stabilizing. And so this looks like a pretty decent setting of of learning rates and so on. But this is something to look at. And when things are miscalibrated, you will you will see very quickly. So, for example, everything looks pretty well behaved. But just as a comparison, when things are not properly calibrated, what does that look like? Let me come up here and let's say that, for example, what do we do? Let's say that we forgot to apply this fan in normalization. So the weights inside the linear layers are just a sample from a Gaussian in all the stages. What happens to our how do we notice that something's off? Well, the activation plot will tell you, whoa, your neurons are way too saturated. The gradients are going to be all messed up. The histogram for these weights are going to be all messed up as well. And there's a lot of asymmetry. And then if we look here, I suspect it's all going to be also pretty messed up. So you see, there's a lot of discrepancy in how fast these layers are learning. And some of them are learning way too fast. So native one native one point five. Those are very large numbers in terms of this ratio. Again, you should be somewhere around negative three and not much more about that. So this is how miscalibrations of your neural nets are going to manifest. And these kinds of plots here are a good way of sort of bringing those miscalibrations sort of to your attention. And so you can address them. OK, so so far we've seen that when we have this linear 10H sandwich, we can actually precisely calibrate the gains and make the activations, the gradients and the parameters and the updates all look pretty decent. But it definitely feels a little bit like balancing of a pencil on your finger. And that's because this game has to be very precisely calibrated. So now let's introduce batch formalization layers into the fix into the mix. And let's let's see how that helps fix the problem. So here I'm going to take the bathroom one D class and I'm going to start placing it inside. And as I mentioned before, the standard typical place you would place it is between the linear layer. So right after it, but before the non-linearity. But people have definitely played with that. And in fact, you can get very similar results even if you place it after the non-linearity. And the other thing that I wanted to mention is it's totally fine to also place it at the end after the last linear layer and before the loss function. So this is potentially fine as well. And in this case, this would be up, but it would be woke up size. Now, because the last layer is best room, we would not be changing to wait to make the softmax less confident. We'd be changing the gamma because gamma, remember, in the bathroom is the variable that multiplicatively interacts with the output of that normalization. So we can initialize this sandwich now. We can train and we can see that the activations are going to, of course, look very good. And they are going to necessarily look good because now before every single 10 H layer, there is a normalization in the bachelor. So this is unsurprisingly all looks pretty good. It's going to be standard deviation of roughly point six five two percent and roughly equal standard deviation throughout the entire layers. So everything looks very homogeneous. The gradients look good. The weights look good and their distributions and then the updates also look pretty reasonable. We're going above negative three a little bit, but not by too much. So all the parameters are training in roughly the same rate here. But now what we've gained is we are going to be slightly less brittle with respect to the gain of these. So, for example, I can make the game be say point two here, which was much slower than what we had with the 10 H. But as we'll see, the activations will actually be exactly unaffected. And that's because of, again, this explicit normalization. The gradients are going to look OK, the weight gradients are going to look OK, but actually the updates will change. And so even though the forward and backward pass to a very large extent look OK because of the backward pass of the bachelor and how the scale of the incoming activations interacts in the bachelor and its backward pass, this is actually changing the scale of the updates on these parameters. So the gradients of these weights are affected. So we still don't get a completely free pass to pass an arbitrary weights here. But everything else is significantly more robust in terms of the forward, backward and the weight gradients. It's just that you may have to retune your learning rate if you are changing sufficiently the scale of the activations that are coming into the bachelors. So here, for example, we changed the gains of these linear layers to be greater and we're seeing that the updates are coming out lower as a result. And then finally, we can also if we are using bachelors, we don't actually need to necessarily let me reset this to one. So there's no gain. We don't necessarily even have to normalize by fan in sometimes. So if I take out the fan in, so these are just now random Gaussian, we'll see that because of bachelor, this will actually be relatively well behaved. So this is look, of course, in the forward pass look good. The gradients look good. The backward weight updates look OK. A little bit of fat tails on some of the layers. And this looks OK as well. But as you as you can see, we're significantly below negative three. So we'd have to bump up the learning rate of this bachelor so that we are training more properly. And in particular, looking at this roughly looks like we have to 10x the learning rate to get to about one negative three. So we come here and we would change this to be update of one point zero. And if I initialize. Then we'll see that everything still, of course, looks good. And now we are roughly here and we expect this to be an OK training run. So long story short, we are significantly more robust to the gain of these linear layers, whether or not we have to apply the fan in. And then we can change the game, but we actually do have to worry a little bit about the update scales and making sure that the learning rate is properly calibrated here. But the activations of the forward backward pass and the updates are all are looking significantly more well behaved, except for the global scale that is potentially being adjusted here. OK, so now let me summarize. There are three things I was hoping to achieve with this section. Number one, I wanted to introduce you to bash normalization, which is one of the first modern innovations that we're looking into that helped stabilize very deep neural networks and their training. And I hope you understand how the bash normalization works and how it would be used in a neural network. Number two, I was hoping to pytorchify some of our code and wrap it up into these modules. So like linear bash arm on the 10 age, etc. These are layers or modules and they can be stacked up into neural nets like Lego building blocks. And these layers actually exist in pytorch. And if you import torch and then then you can actually the way I've constructed it, you can simply just use pytorch by prepending and endowed to all these different layers. And actually everything will just work because the API that I've developed here is identical to the API that pytorch uses. And the implementation also is basically, as far as I'm aware, identical to the one in pytorch. And number three, I tried to introduce you to the diagnostic tools that you would use to understand whether your neural network is in a good state dynamically. So we are looking at the statistics and histograms and activation of the forward pass activation activations, the backward pass gradients. And then also we're looking at the ways that are going to be updated as part of stochastic gradient ascent. And we're looking at their means, standard deviations and also the ratio of gradients to data or even better, the updates to data. And we saw that typically we don't actually look at it as a single snapshot frozen in time at some particular iteration. Typically, people look at this as over time, just like I've done here. And they look at these update to data ratios and they make sure everything looks OK. And in particular, I said that wanting negative three or basically negative three on the log scale is a good rough heuristic for what you want this ratio to be. And if it's way too high, then probably the learning rate or the updates are a little too big. And if it's way too small, that the learning rate is probably too small. So that's just some of the things that you may want to play with when you try to get your neural network to work very well. Now, there's a number of things I did not try to achieve. I did not try to beat our previous performance, as an example, by introducing the BatchNorm layer. Actually, I did try and I found that I used the learning rate finding mechanism that I've described before. I tried to train the BatchNorm layer, BatchNorm neural net. And I actually ended up with results that are very, very similar to what we've obtained before. And that's because our performance now is not bottlenecked by the optimization, which is what BatchNorm is helping with. The performance at this stage is bottlenecked by what I suspect is the context length of our context. So currently we are taking three characters to predict the fourth one. And I think we need to go beyond that. And we need to look at more powerful architectures for our like recurrent neural networks and transformers in order to further push the log probabilities that we're achieving on this data set. And I also did not try to have a full explanation of all of these activations, the gradients and the backward pass and the statistics of all these gradients. And so you may have found some of the parts here unintuitive and maybe you're slightly confused about, OK, if I change the game here, how come that we need a different learning rate? And I didn't go into the full detail because you'd have to actually look at the backward pass of all these different layers and get an intuitive understanding of how that works. And I did not go into that in this lecture. The purpose really was just to introduce you to the diagnostic tools and what they look like. But there's still a lot of work remaining on the intuitive level to understand the initialization, the backward pass and how all of that interacts. But you shouldn't feel too bad because honestly, we are getting to the cutting edge of where the field is. We certainly haven't, I would say, solved initialization and we haven't solved backpropagation. And these are still very much an active area of research. People are still trying to figure out what is the best way to initialize these networks? What is the best update rule to use? And so on. So none of this is really solved. And we don't really have all the answers to all the to all these cases. But at least we're making progress and at least we have some tools to tell us whether or not things are on the right track for now. So I think we've made positive progress in this lecture, and I hope you enjoyed that. And I will see you next time.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.0, "text": " Hi everyone. Today we are continuing our implementation of MakeMore.", "tokens": [50364, 2421, 1518, 13, 2692, 321, 366, 9289, 527, 11420, 295, 4387, 33986, 13, 50564, 50564, 823, 294, 264, 1036, 7991, 321, 12270, 264, 14665, 388, 11167, 43276, 2044, 2051, 264, 3876, 295, 3964, 73, 310, 338, 16416, 337, 2517, 12, 12418, 2856, 15983, 13, 50864, 50864, 407, 321, 6263, 341, 3035, 11, 1890, 294, 257, 1326, 4342, 294, 264, 1791, 11, 293, 1143, 364, 21601, 47, 281, 6069, 264, 958, 2517, 294, 257, 8310, 13, 51214, 51214, 407, 437, 321, 1116, 411, 281, 360, 586, 307, 321, 1116, 411, 281, 1286, 322, 281, 544, 3997, 293, 4833, 18161, 9590, 11, 411, 18680, 1753, 18161, 9590, 293, 641, 17840, 411, 264, 10903, 52, 11, 441, 6840, 44, 11, 293, 370, 322, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.17048147583007814, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.0045287152752280235}, {"id": 1, "seek": 0, "start": 4.0, "end": 10.0, "text": " Now in the last lecture we implemented the Multilayer perceptron along the lines of Benjotel 2003 for character-level language modeling.", "tokens": [50364, 2421, 1518, 13, 2692, 321, 366, 9289, 527, 11420, 295, 4387, 33986, 13, 50564, 50564, 823, 294, 264, 1036, 7991, 321, 12270, 264, 14665, 388, 11167, 43276, 2044, 2051, 264, 3876, 295, 3964, 73, 310, 338, 16416, 337, 2517, 12, 12418, 2856, 15983, 13, 50864, 50864, 407, 321, 6263, 341, 3035, 11, 1890, 294, 257, 1326, 4342, 294, 264, 1791, 11, 293, 1143, 364, 21601, 47, 281, 6069, 264, 958, 2517, 294, 257, 8310, 13, 51214, 51214, 407, 437, 321, 1116, 411, 281, 360, 586, 307, 321, 1116, 411, 281, 1286, 322, 281, 544, 3997, 293, 4833, 18161, 9590, 11, 411, 18680, 1753, 18161, 9590, 293, 641, 17840, 411, 264, 10903, 52, 11, 441, 6840, 44, 11, 293, 370, 322, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.17048147583007814, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.0045287152752280235}, {"id": 2, "seek": 0, "start": 10.0, "end": 17.0, "text": " So we followed this paper, took in a few characters in the past, and used an MLP to predict the next character in a sequence.", "tokens": [50364, 2421, 1518, 13, 2692, 321, 366, 9289, 527, 11420, 295, 4387, 33986, 13, 50564, 50564, 823, 294, 264, 1036, 7991, 321, 12270, 264, 14665, 388, 11167, 43276, 2044, 2051, 264, 3876, 295, 3964, 73, 310, 338, 16416, 337, 2517, 12, 12418, 2856, 15983, 13, 50864, 50864, 407, 321, 6263, 341, 3035, 11, 1890, 294, 257, 1326, 4342, 294, 264, 1791, 11, 293, 1143, 364, 21601, 47, 281, 6069, 264, 958, 2517, 294, 257, 8310, 13, 51214, 51214, 407, 437, 321, 1116, 411, 281, 360, 586, 307, 321, 1116, 411, 281, 1286, 322, 281, 544, 3997, 293, 4833, 18161, 9590, 11, 411, 18680, 1753, 18161, 9590, 293, 641, 17840, 411, 264, 10903, 52, 11, 441, 6840, 44, 11, 293, 370, 322, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.17048147583007814, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.0045287152752280235}, {"id": 3, "seek": 0, "start": 17.0, "end": 26.0, "text": " So what we'd like to do now is we'd like to move on to more complex and larger neural networks, like recurrent neural networks and their variations like the GRU, LSTM, and so on.", "tokens": [50364, 2421, 1518, 13, 2692, 321, 366, 9289, 527, 11420, 295, 4387, 33986, 13, 50564, 50564, 823, 294, 264, 1036, 7991, 321, 12270, 264, 14665, 388, 11167, 43276, 2044, 2051, 264, 3876, 295, 3964, 73, 310, 338, 16416, 337, 2517, 12, 12418, 2856, 15983, 13, 50864, 50864, 407, 321, 6263, 341, 3035, 11, 1890, 294, 257, 1326, 4342, 294, 264, 1791, 11, 293, 1143, 364, 21601, 47, 281, 6069, 264, 958, 2517, 294, 257, 8310, 13, 51214, 51214, 407, 437, 321, 1116, 411, 281, 360, 586, 307, 321, 1116, 411, 281, 1286, 322, 281, 544, 3997, 293, 4833, 18161, 9590, 11, 411, 18680, 1753, 18161, 9590, 293, 641, 17840, 411, 264, 10903, 52, 11, 441, 6840, 44, 11, 293, 370, 322, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.17048147583007814, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.0045287152752280235}, {"id": 4, "seek": 2600, "start": 26.0, "end": 31.0, "text": " Now before we do that though, we have to stick around the level of Multilayer perceptron for a bit longer.", "tokens": [50364, 823, 949, 321, 360, 300, 1673, 11, 321, 362, 281, 2897, 926, 264, 1496, 295, 14665, 388, 11167, 43276, 2044, 337, 257, 857, 2854, 13, 50614, 50614, 400, 286, 1116, 411, 281, 360, 341, 570, 286, 576, 411, 505, 281, 362, 257, 588, 665, 21769, 3701, 295, 264, 2430, 763, 294, 264, 18161, 2533, 1830, 3097, 11, 50964, 50964, 293, 2318, 264, 2771, 2448, 300, 366, 13974, 12204, 11, 293, 577, 436, 15158, 293, 437, 436, 574, 411, 13, 51214, 51214, 400, 341, 307, 516, 281, 312, 588, 1021, 281, 1223, 264, 2503, 295, 264, 3250, 295, 613, 6331, 1303, 11, 51464, 51464, 570, 321, 603, 536, 300, 18680, 1753, 18161, 9590, 11, 1339, 436, 366, 588, 40189, 294, 300, 436, 366, 257, 11455, 8542, 1639, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0715778656588256, "compression_ratio": 1.7920489296636086, "no_speech_prob": 0.0001397437008563429}, {"id": 5, "seek": 2600, "start": 31.0, "end": 38.0, "text": " And I'd like to do this because I would like us to have a very good intuitive understanding of the activations in the neural net during training,", "tokens": [50364, 823, 949, 321, 360, 300, 1673, 11, 321, 362, 281, 2897, 926, 264, 1496, 295, 14665, 388, 11167, 43276, 2044, 337, 257, 857, 2854, 13, 50614, 50614, 400, 286, 1116, 411, 281, 360, 341, 570, 286, 576, 411, 505, 281, 362, 257, 588, 665, 21769, 3701, 295, 264, 2430, 763, 294, 264, 18161, 2533, 1830, 3097, 11, 50964, 50964, 293, 2318, 264, 2771, 2448, 300, 366, 13974, 12204, 11, 293, 577, 436, 15158, 293, 437, 436, 574, 411, 13, 51214, 51214, 400, 341, 307, 516, 281, 312, 588, 1021, 281, 1223, 264, 2503, 295, 264, 3250, 295, 613, 6331, 1303, 11, 51464, 51464, 570, 321, 603, 536, 300, 18680, 1753, 18161, 9590, 11, 1339, 436, 366, 588, 40189, 294, 300, 436, 366, 257, 11455, 8542, 1639, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0715778656588256, "compression_ratio": 1.7920489296636086, "no_speech_prob": 0.0001397437008563429}, {"id": 6, "seek": 2600, "start": 38.0, "end": 43.0, "text": " and especially the gradients that are flowing backwards, and how they behave and what they look like.", "tokens": [50364, 823, 949, 321, 360, 300, 1673, 11, 321, 362, 281, 2897, 926, 264, 1496, 295, 14665, 388, 11167, 43276, 2044, 337, 257, 857, 2854, 13, 50614, 50614, 400, 286, 1116, 411, 281, 360, 341, 570, 286, 576, 411, 505, 281, 362, 257, 588, 665, 21769, 3701, 295, 264, 2430, 763, 294, 264, 18161, 2533, 1830, 3097, 11, 50964, 50964, 293, 2318, 264, 2771, 2448, 300, 366, 13974, 12204, 11, 293, 577, 436, 15158, 293, 437, 436, 574, 411, 13, 51214, 51214, 400, 341, 307, 516, 281, 312, 588, 1021, 281, 1223, 264, 2503, 295, 264, 3250, 295, 613, 6331, 1303, 11, 51464, 51464, 570, 321, 603, 536, 300, 18680, 1753, 18161, 9590, 11, 1339, 436, 366, 588, 40189, 294, 300, 436, 366, 257, 11455, 8542, 1639, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0715778656588256, "compression_ratio": 1.7920489296636086, "no_speech_prob": 0.0001397437008563429}, {"id": 7, "seek": 2600, "start": 43.0, "end": 48.0, "text": " And this is going to be very important to understand the history of the development of these architectures,", "tokens": [50364, 823, 949, 321, 360, 300, 1673, 11, 321, 362, 281, 2897, 926, 264, 1496, 295, 14665, 388, 11167, 43276, 2044, 337, 257, 857, 2854, 13, 50614, 50614, 400, 286, 1116, 411, 281, 360, 341, 570, 286, 576, 411, 505, 281, 362, 257, 588, 665, 21769, 3701, 295, 264, 2430, 763, 294, 264, 18161, 2533, 1830, 3097, 11, 50964, 50964, 293, 2318, 264, 2771, 2448, 300, 366, 13974, 12204, 11, 293, 577, 436, 15158, 293, 437, 436, 574, 411, 13, 51214, 51214, 400, 341, 307, 516, 281, 312, 588, 1021, 281, 1223, 264, 2503, 295, 264, 3250, 295, 613, 6331, 1303, 11, 51464, 51464, 570, 321, 603, 536, 300, 18680, 1753, 18161, 9590, 11, 1339, 436, 366, 588, 40189, 294, 300, 436, 366, 257, 11455, 8542, 1639, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0715778656588256, "compression_ratio": 1.7920489296636086, "no_speech_prob": 0.0001397437008563429}, {"id": 8, "seek": 2600, "start": 48.0, "end": 54.0, "text": " because we'll see that recurrent neural networks, while they are very expressive in that they are a universal approximator,", "tokens": [50364, 823, 949, 321, 360, 300, 1673, 11, 321, 362, 281, 2897, 926, 264, 1496, 295, 14665, 388, 11167, 43276, 2044, 337, 257, 857, 2854, 13, 50614, 50614, 400, 286, 1116, 411, 281, 360, 341, 570, 286, 576, 411, 505, 281, 362, 257, 588, 665, 21769, 3701, 295, 264, 2430, 763, 294, 264, 18161, 2533, 1830, 3097, 11, 50964, 50964, 293, 2318, 264, 2771, 2448, 300, 366, 13974, 12204, 11, 293, 577, 436, 15158, 293, 437, 436, 574, 411, 13, 51214, 51214, 400, 341, 307, 516, 281, 312, 588, 1021, 281, 1223, 264, 2503, 295, 264, 3250, 295, 613, 6331, 1303, 11, 51464, 51464, 570, 321, 603, 536, 300, 18680, 1753, 18161, 9590, 11, 1339, 436, 366, 588, 40189, 294, 300, 436, 366, 257, 11455, 8542, 1639, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0715778656588256, "compression_ratio": 1.7920489296636086, "no_speech_prob": 0.0001397437008563429}, {"id": 9, "seek": 5400, "start": 54.0, "end": 57.0, "text": " they can in principle implement all the algorithms.", "tokens": [50364, 436, 393, 294, 8665, 4445, 439, 264, 14642, 13, 50514, 50514, 492, 603, 536, 300, 436, 366, 406, 588, 3612, 5028, 22395, 365, 264, 700, 12, 4687, 16235, 12, 6032, 7512, 300, 321, 362, 2435, 281, 505, 293, 300, 321, 764, 439, 264, 565, 13, 50864, 50864, 400, 264, 2141, 281, 3701, 983, 436, 366, 406, 5028, 22395, 3612, 307, 281, 1223, 264, 2430, 763, 293, 264, 2771, 2448, 293, 577, 436, 15158, 1830, 3097, 13, 51264, 51264, 400, 321, 603, 536, 300, 257, 688, 295, 264, 21669, 1670, 18680, 1753, 18161, 9590, 362, 3031, 281, 3470, 300, 2590, 13, 51614, 51614, 400, 370, 300, 311, 264, 3100, 300, 321, 362, 281, 747, 11, 293, 718, 311, 352, 722, 309, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06812510681152344, "compression_ratio": 1.9049295774647887, "no_speech_prob": 1.0450649824633729e-05}, {"id": 10, "seek": 5400, "start": 57.0, "end": 64.0, "text": " We'll see that they are not very easily optimizable with the first-order gradient-based techniques that we have available to us and that we use all the time.", "tokens": [50364, 436, 393, 294, 8665, 4445, 439, 264, 14642, 13, 50514, 50514, 492, 603, 536, 300, 436, 366, 406, 588, 3612, 5028, 22395, 365, 264, 700, 12, 4687, 16235, 12, 6032, 7512, 300, 321, 362, 2435, 281, 505, 293, 300, 321, 764, 439, 264, 565, 13, 50864, 50864, 400, 264, 2141, 281, 3701, 983, 436, 366, 406, 5028, 22395, 3612, 307, 281, 1223, 264, 2430, 763, 293, 264, 2771, 2448, 293, 577, 436, 15158, 1830, 3097, 13, 51264, 51264, 400, 321, 603, 536, 300, 257, 688, 295, 264, 21669, 1670, 18680, 1753, 18161, 9590, 362, 3031, 281, 3470, 300, 2590, 13, 51614, 51614, 400, 370, 300, 311, 264, 3100, 300, 321, 362, 281, 747, 11, 293, 718, 311, 352, 722, 309, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06812510681152344, "compression_ratio": 1.9049295774647887, "no_speech_prob": 1.0450649824633729e-05}, {"id": 11, "seek": 5400, "start": 64.0, "end": 72.0, "text": " And the key to understanding why they are not optimizable easily is to understand the activations and the gradients and how they behave during training.", "tokens": [50364, 436, 393, 294, 8665, 4445, 439, 264, 14642, 13, 50514, 50514, 492, 603, 536, 300, 436, 366, 406, 588, 3612, 5028, 22395, 365, 264, 700, 12, 4687, 16235, 12, 6032, 7512, 300, 321, 362, 2435, 281, 505, 293, 300, 321, 764, 439, 264, 565, 13, 50864, 50864, 400, 264, 2141, 281, 3701, 983, 436, 366, 406, 5028, 22395, 3612, 307, 281, 1223, 264, 2430, 763, 293, 264, 2771, 2448, 293, 577, 436, 15158, 1830, 3097, 13, 51264, 51264, 400, 321, 603, 536, 300, 257, 688, 295, 264, 21669, 1670, 18680, 1753, 18161, 9590, 362, 3031, 281, 3470, 300, 2590, 13, 51614, 51614, 400, 370, 300, 311, 264, 3100, 300, 321, 362, 281, 747, 11, 293, 718, 311, 352, 722, 309, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06812510681152344, "compression_ratio": 1.9049295774647887, "no_speech_prob": 1.0450649824633729e-05}, {"id": 12, "seek": 5400, "start": 72.0, "end": 79.0, "text": " And we'll see that a lot of the variants since recurrent neural networks have tried to improve that situation.", "tokens": [50364, 436, 393, 294, 8665, 4445, 439, 264, 14642, 13, 50514, 50514, 492, 603, 536, 300, 436, 366, 406, 588, 3612, 5028, 22395, 365, 264, 700, 12, 4687, 16235, 12, 6032, 7512, 300, 321, 362, 2435, 281, 505, 293, 300, 321, 764, 439, 264, 565, 13, 50864, 50864, 400, 264, 2141, 281, 3701, 983, 436, 366, 406, 5028, 22395, 3612, 307, 281, 1223, 264, 2430, 763, 293, 264, 2771, 2448, 293, 577, 436, 15158, 1830, 3097, 13, 51264, 51264, 400, 321, 603, 536, 300, 257, 688, 295, 264, 21669, 1670, 18680, 1753, 18161, 9590, 362, 3031, 281, 3470, 300, 2590, 13, 51614, 51614, 400, 370, 300, 311, 264, 3100, 300, 321, 362, 281, 747, 11, 293, 718, 311, 352, 722, 309, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06812510681152344, "compression_ratio": 1.9049295774647887, "no_speech_prob": 1.0450649824633729e-05}, {"id": 13, "seek": 5400, "start": 79.0, "end": 83.0, "text": " And so that's the path that we have to take, and let's go start it.", "tokens": [50364, 436, 393, 294, 8665, 4445, 439, 264, 14642, 13, 50514, 50514, 492, 603, 536, 300, 436, 366, 406, 588, 3612, 5028, 22395, 365, 264, 700, 12, 4687, 16235, 12, 6032, 7512, 300, 321, 362, 2435, 281, 505, 293, 300, 321, 764, 439, 264, 565, 13, 50864, 50864, 400, 264, 2141, 281, 3701, 983, 436, 366, 406, 5028, 22395, 3612, 307, 281, 1223, 264, 2430, 763, 293, 264, 2771, 2448, 293, 577, 436, 15158, 1830, 3097, 13, 51264, 51264, 400, 321, 603, 536, 300, 257, 688, 295, 264, 21669, 1670, 18680, 1753, 18161, 9590, 362, 3031, 281, 3470, 300, 2590, 13, 51614, 51614, 400, 370, 300, 311, 264, 3100, 300, 321, 362, 281, 747, 11, 293, 718, 311, 352, 722, 309, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06812510681152344, "compression_ratio": 1.9049295774647887, "no_speech_prob": 1.0450649824633729e-05}, {"id": 14, "seek": 8300, "start": 83.0, "end": 88.0, "text": " So the starting code for this lecture is largely the code from before, but I've cleaned it up a little bit.", "tokens": [50364, 407, 264, 2891, 3089, 337, 341, 7991, 307, 11611, 264, 3089, 490, 949, 11, 457, 286, 600, 16146, 309, 493, 257, 707, 857, 13, 50614, 50614, 407, 291, 603, 536, 300, 321, 366, 43866, 439, 264, 27822, 293, 5221, 564, 310, 38270, 30482, 13, 50864, 50864, 492, 434, 3760, 294, 264, 2283, 445, 411, 949, 13, 1981, 366, 3180, 1365, 2283, 13, 51064, 51064, 821, 311, 257, 3217, 295, 8858, 11, 1360, 295, 552, 13, 1692, 311, 257, 19864, 295, 439, 264, 3126, 9765, 7825, 293, 264, 2121, 5893, 14862, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07539231149773849, "compression_ratio": 1.5720338983050848, "no_speech_prob": 7.249286863952875e-05}, {"id": 15, "seek": 8300, "start": 88.0, "end": 93.0, "text": " So you'll see that we are importing all the torch and mathplotlib utilities.", "tokens": [50364, 407, 264, 2891, 3089, 337, 341, 7991, 307, 11611, 264, 3089, 490, 949, 11, 457, 286, 600, 16146, 309, 493, 257, 707, 857, 13, 50614, 50614, 407, 291, 603, 536, 300, 321, 366, 43866, 439, 264, 27822, 293, 5221, 564, 310, 38270, 30482, 13, 50864, 50864, 492, 434, 3760, 294, 264, 2283, 445, 411, 949, 13, 1981, 366, 3180, 1365, 2283, 13, 51064, 51064, 821, 311, 257, 3217, 295, 8858, 11, 1360, 295, 552, 13, 1692, 311, 257, 19864, 295, 439, 264, 3126, 9765, 7825, 293, 264, 2121, 5893, 14862, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07539231149773849, "compression_ratio": 1.5720338983050848, "no_speech_prob": 7.249286863952875e-05}, {"id": 16, "seek": 8300, "start": 93.0, "end": 97.0, "text": " We're reading in the words just like before. These are eight example words.", "tokens": [50364, 407, 264, 2891, 3089, 337, 341, 7991, 307, 11611, 264, 3089, 490, 949, 11, 457, 286, 600, 16146, 309, 493, 257, 707, 857, 13, 50614, 50614, 407, 291, 603, 536, 300, 321, 366, 43866, 439, 264, 27822, 293, 5221, 564, 310, 38270, 30482, 13, 50864, 50864, 492, 434, 3760, 294, 264, 2283, 445, 411, 949, 13, 1981, 366, 3180, 1365, 2283, 13, 51064, 51064, 821, 311, 257, 3217, 295, 8858, 11, 1360, 295, 552, 13, 1692, 311, 257, 19864, 295, 439, 264, 3126, 9765, 7825, 293, 264, 2121, 5893, 14862, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07539231149773849, "compression_ratio": 1.5720338983050848, "no_speech_prob": 7.249286863952875e-05}, {"id": 17, "seek": 8300, "start": 97.0, "end": 104.0, "text": " There's a total of 32,000 of them. Here's a vocabulary of all the lowercase letters and the special dot token.", "tokens": [50364, 407, 264, 2891, 3089, 337, 341, 7991, 307, 11611, 264, 3089, 490, 949, 11, 457, 286, 600, 16146, 309, 493, 257, 707, 857, 13, 50614, 50614, 407, 291, 603, 536, 300, 321, 366, 43866, 439, 264, 27822, 293, 5221, 564, 310, 38270, 30482, 13, 50864, 50864, 492, 434, 3760, 294, 264, 2283, 445, 411, 949, 13, 1981, 366, 3180, 1365, 2283, 13, 51064, 51064, 821, 311, 257, 3217, 295, 8858, 11, 1360, 295, 552, 13, 1692, 311, 257, 19864, 295, 439, 264, 3126, 9765, 7825, 293, 264, 2121, 5893, 14862, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07539231149773849, "compression_ratio": 1.5720338983050848, "no_speech_prob": 7.249286863952875e-05}, {"id": 18, "seek": 10400, "start": 104.0, "end": 113.0, "text": " Here we are reading the data set and processing it and creating three splits, the train, dev, and the test split.", "tokens": [50364, 1692, 321, 366, 3760, 264, 1412, 992, 293, 9007, 309, 293, 4084, 1045, 37741, 11, 264, 3847, 11, 1905, 11, 293, 264, 1500, 7472, 13, 50814, 50814, 823, 294, 21601, 47, 11, 341, 307, 264, 14800, 912, 21601, 47, 11, 3993, 291, 536, 300, 286, 7261, 257, 3840, 295, 5585, 3547, 300, 321, 632, 510, 13, 51114, 51114, 400, 2602, 321, 362, 264, 10139, 1860, 295, 264, 12240, 3584, 1901, 295, 264, 4342, 293, 264, 1230, 295, 7633, 6815, 294, 264, 7633, 4583, 13, 51464, 51464, 400, 370, 286, 7373, 552, 2380, 510, 370, 300, 321, 500, 380, 362, 281, 352, 293, 1319, 439, 613, 5585, 3547, 439, 264, 565, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06378393173217774, "compression_ratio": 1.7781954887218046, "no_speech_prob": 8.938871360442135e-06}, {"id": 19, "seek": 10400, "start": 113.0, "end": 119.0, "text": " Now in MLP, this is the identical same MLP, except you see that I removed a bunch of magic numbers that we had here.", "tokens": [50364, 1692, 321, 366, 3760, 264, 1412, 992, 293, 9007, 309, 293, 4084, 1045, 37741, 11, 264, 3847, 11, 1905, 11, 293, 264, 1500, 7472, 13, 50814, 50814, 823, 294, 21601, 47, 11, 341, 307, 264, 14800, 912, 21601, 47, 11, 3993, 291, 536, 300, 286, 7261, 257, 3840, 295, 5585, 3547, 300, 321, 632, 510, 13, 51114, 51114, 400, 2602, 321, 362, 264, 10139, 1860, 295, 264, 12240, 3584, 1901, 295, 264, 4342, 293, 264, 1230, 295, 7633, 6815, 294, 264, 7633, 4583, 13, 51464, 51464, 400, 370, 286, 7373, 552, 2380, 510, 370, 300, 321, 500, 380, 362, 281, 352, 293, 1319, 439, 613, 5585, 3547, 439, 264, 565, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06378393173217774, "compression_ratio": 1.7781954887218046, "no_speech_prob": 8.938871360442135e-06}, {"id": 20, "seek": 10400, "start": 119.0, "end": 126.0, "text": " And instead we have the dimensionality of the embedding space of the characters and the number of hidden units in the hidden layer.", "tokens": [50364, 1692, 321, 366, 3760, 264, 1412, 992, 293, 9007, 309, 293, 4084, 1045, 37741, 11, 264, 3847, 11, 1905, 11, 293, 264, 1500, 7472, 13, 50814, 50814, 823, 294, 21601, 47, 11, 341, 307, 264, 14800, 912, 21601, 47, 11, 3993, 291, 536, 300, 286, 7261, 257, 3840, 295, 5585, 3547, 300, 321, 632, 510, 13, 51114, 51114, 400, 2602, 321, 362, 264, 10139, 1860, 295, 264, 12240, 3584, 1901, 295, 264, 4342, 293, 264, 1230, 295, 7633, 6815, 294, 264, 7633, 4583, 13, 51464, 51464, 400, 370, 286, 7373, 552, 2380, 510, 370, 300, 321, 500, 380, 362, 281, 352, 293, 1319, 439, 613, 5585, 3547, 439, 264, 565, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06378393173217774, "compression_ratio": 1.7781954887218046, "no_speech_prob": 8.938871360442135e-06}, {"id": 21, "seek": 10400, "start": 126.0, "end": 131.0, "text": " And so I pulled them outside here so that we don't have to go and change all these magic numbers all the time.", "tokens": [50364, 1692, 321, 366, 3760, 264, 1412, 992, 293, 9007, 309, 293, 4084, 1045, 37741, 11, 264, 3847, 11, 1905, 11, 293, 264, 1500, 7472, 13, 50814, 50814, 823, 294, 21601, 47, 11, 341, 307, 264, 14800, 912, 21601, 47, 11, 3993, 291, 536, 300, 286, 7261, 257, 3840, 295, 5585, 3547, 300, 321, 632, 510, 13, 51114, 51114, 400, 2602, 321, 362, 264, 10139, 1860, 295, 264, 12240, 3584, 1901, 295, 264, 4342, 293, 264, 1230, 295, 7633, 6815, 294, 264, 7633, 4583, 13, 51464, 51464, 400, 370, 286, 7373, 552, 2380, 510, 370, 300, 321, 500, 380, 362, 281, 352, 293, 1319, 439, 613, 5585, 3547, 439, 264, 565, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06378393173217774, "compression_ratio": 1.7781954887218046, "no_speech_prob": 8.938871360442135e-06}, {"id": 22, "seek": 13100, "start": 131.0, "end": 138.0, "text": " With the same neural net with 11,000 parameters that we optimize now over 200,000 steps with a batch size of 32.", "tokens": [50364, 2022, 264, 912, 18161, 2533, 365, 2975, 11, 1360, 9834, 300, 321, 19719, 586, 670, 2331, 11, 1360, 4439, 365, 257, 15245, 2744, 295, 8858, 13, 50714, 50714, 400, 291, 603, 536, 300, 286, 1895, 578, 2769, 264, 3089, 510, 257, 707, 857, 11, 457, 456, 366, 572, 11745, 2962, 13, 50964, 50964, 286, 445, 2942, 257, 1326, 2857, 9102, 11, 257, 1326, 544, 3053, 11, 293, 286, 7261, 439, 264, 5585, 3547, 13, 51264, 51264, 400, 5911, 309, 311, 264, 1900, 912, 551, 13, 1396, 562, 321, 19719, 11, 321, 1866, 300, 527, 4470, 2956, 746, 411, 341, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0904306906920213, "compression_ratio": 1.5811320754716982, "no_speech_prob": 4.331581658334471e-05}, {"id": 23, "seek": 13100, "start": 138.0, "end": 143.0, "text": " And you'll see that I refactored the code here a little bit, but there are no functional changes.", "tokens": [50364, 2022, 264, 912, 18161, 2533, 365, 2975, 11, 1360, 9834, 300, 321, 19719, 586, 670, 2331, 11, 1360, 4439, 365, 257, 15245, 2744, 295, 8858, 13, 50714, 50714, 400, 291, 603, 536, 300, 286, 1895, 578, 2769, 264, 3089, 510, 257, 707, 857, 11, 457, 456, 366, 572, 11745, 2962, 13, 50964, 50964, 286, 445, 2942, 257, 1326, 2857, 9102, 11, 257, 1326, 544, 3053, 11, 293, 286, 7261, 439, 264, 5585, 3547, 13, 51264, 51264, 400, 5911, 309, 311, 264, 1900, 912, 551, 13, 1396, 562, 321, 19719, 11, 321, 1866, 300, 527, 4470, 2956, 746, 411, 341, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0904306906920213, "compression_ratio": 1.5811320754716982, "no_speech_prob": 4.331581658334471e-05}, {"id": 24, "seek": 13100, "start": 143.0, "end": 149.0, "text": " I just created a few extra variables, a few more comments, and I removed all the magic numbers.", "tokens": [50364, 2022, 264, 912, 18161, 2533, 365, 2975, 11, 1360, 9834, 300, 321, 19719, 586, 670, 2331, 11, 1360, 4439, 365, 257, 15245, 2744, 295, 8858, 13, 50714, 50714, 400, 291, 603, 536, 300, 286, 1895, 578, 2769, 264, 3089, 510, 257, 707, 857, 11, 457, 456, 366, 572, 11745, 2962, 13, 50964, 50964, 286, 445, 2942, 257, 1326, 2857, 9102, 11, 257, 1326, 544, 3053, 11, 293, 286, 7261, 439, 264, 5585, 3547, 13, 51264, 51264, 400, 5911, 309, 311, 264, 1900, 912, 551, 13, 1396, 562, 321, 19719, 11, 321, 1866, 300, 527, 4470, 2956, 746, 411, 341, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0904306906920213, "compression_ratio": 1.5811320754716982, "no_speech_prob": 4.331581658334471e-05}, {"id": 25, "seek": 13100, "start": 149.0, "end": 155.0, "text": " And otherwise it's the exact same thing. Then when we optimize, we saw that our loss looked something like this.", "tokens": [50364, 2022, 264, 912, 18161, 2533, 365, 2975, 11, 1360, 9834, 300, 321, 19719, 586, 670, 2331, 11, 1360, 4439, 365, 257, 15245, 2744, 295, 8858, 13, 50714, 50714, 400, 291, 603, 536, 300, 286, 1895, 578, 2769, 264, 3089, 510, 257, 707, 857, 11, 457, 456, 366, 572, 11745, 2962, 13, 50964, 50964, 286, 445, 2942, 257, 1326, 2857, 9102, 11, 257, 1326, 544, 3053, 11, 293, 286, 7261, 439, 264, 5585, 3547, 13, 51264, 51264, 400, 5911, 309, 311, 264, 1900, 912, 551, 13, 1396, 562, 321, 19719, 11, 321, 1866, 300, 527, 4470, 2956, 746, 411, 341, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.0904306906920213, "compression_ratio": 1.5811320754716982, "no_speech_prob": 4.331581658334471e-05}, {"id": 26, "seek": 15500, "start": 155.0, "end": 161.0, "text": " We saw that the train and val loss were about 2.16 and so on.", "tokens": [50364, 492, 1866, 300, 264, 3847, 293, 1323, 4470, 645, 466, 568, 13, 6866, 293, 370, 322, 13, 50664, 50664, 1692, 286, 1895, 578, 2769, 264, 3089, 257, 707, 857, 337, 264, 13344, 295, 23211, 37741, 13, 50914, 50914, 407, 291, 1320, 294, 257, 6798, 295, 597, 7472, 291, 1116, 411, 281, 13059, 13, 51064, 51064, 400, 550, 510, 11, 5413, 322, 3847, 11, 1323, 11, 420, 1500, 11, 286, 8186, 294, 293, 286, 483, 264, 3006, 7472, 13, 51364, 51364, 400, 550, 341, 307, 264, 2128, 1320, 295, 264, 3209, 293, 13344, 295, 264, 4470, 293, 14699, 309, 13, 51564, 51564, 407, 445, 1455, 309, 22842, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06345457644075961, "compression_ratio": 1.6983471074380165, "no_speech_prob": 2.857213985407725e-06}, {"id": 27, "seek": 15500, "start": 161.0, "end": 166.0, "text": " Here I refactored the code a little bit for the evaluation of arbitrary splits.", "tokens": [50364, 492, 1866, 300, 264, 3847, 293, 1323, 4470, 645, 466, 568, 13, 6866, 293, 370, 322, 13, 50664, 50664, 1692, 286, 1895, 578, 2769, 264, 3089, 257, 707, 857, 337, 264, 13344, 295, 23211, 37741, 13, 50914, 50914, 407, 291, 1320, 294, 257, 6798, 295, 597, 7472, 291, 1116, 411, 281, 13059, 13, 51064, 51064, 400, 550, 510, 11, 5413, 322, 3847, 11, 1323, 11, 420, 1500, 11, 286, 8186, 294, 293, 286, 483, 264, 3006, 7472, 13, 51364, 51364, 400, 550, 341, 307, 264, 2128, 1320, 295, 264, 3209, 293, 13344, 295, 264, 4470, 293, 14699, 309, 13, 51564, 51564, 407, 445, 1455, 309, 22842, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06345457644075961, "compression_ratio": 1.6983471074380165, "no_speech_prob": 2.857213985407725e-06}, {"id": 28, "seek": 15500, "start": 166.0, "end": 169.0, "text": " So you pass in a string of which split you'd like to evaluate.", "tokens": [50364, 492, 1866, 300, 264, 3847, 293, 1323, 4470, 645, 466, 568, 13, 6866, 293, 370, 322, 13, 50664, 50664, 1692, 286, 1895, 578, 2769, 264, 3089, 257, 707, 857, 337, 264, 13344, 295, 23211, 37741, 13, 50914, 50914, 407, 291, 1320, 294, 257, 6798, 295, 597, 7472, 291, 1116, 411, 281, 13059, 13, 51064, 51064, 400, 550, 510, 11, 5413, 322, 3847, 11, 1323, 11, 420, 1500, 11, 286, 8186, 294, 293, 286, 483, 264, 3006, 7472, 13, 51364, 51364, 400, 550, 341, 307, 264, 2128, 1320, 295, 264, 3209, 293, 13344, 295, 264, 4470, 293, 14699, 309, 13, 51564, 51564, 407, 445, 1455, 309, 22842, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06345457644075961, "compression_ratio": 1.6983471074380165, "no_speech_prob": 2.857213985407725e-06}, {"id": 29, "seek": 15500, "start": 169.0, "end": 175.0, "text": " And then here, depending on train, val, or test, I index in and I get the correct split.", "tokens": [50364, 492, 1866, 300, 264, 3847, 293, 1323, 4470, 645, 466, 568, 13, 6866, 293, 370, 322, 13, 50664, 50664, 1692, 286, 1895, 578, 2769, 264, 3089, 257, 707, 857, 337, 264, 13344, 295, 23211, 37741, 13, 50914, 50914, 407, 291, 1320, 294, 257, 6798, 295, 597, 7472, 291, 1116, 411, 281, 13059, 13, 51064, 51064, 400, 550, 510, 11, 5413, 322, 3847, 11, 1323, 11, 420, 1500, 11, 286, 8186, 294, 293, 286, 483, 264, 3006, 7472, 13, 51364, 51364, 400, 550, 341, 307, 264, 2128, 1320, 295, 264, 3209, 293, 13344, 295, 264, 4470, 293, 14699, 309, 13, 51564, 51564, 407, 445, 1455, 309, 22842, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06345457644075961, "compression_ratio": 1.6983471074380165, "no_speech_prob": 2.857213985407725e-06}, {"id": 30, "seek": 15500, "start": 175.0, "end": 179.0, "text": " And then this is the forward pass of the network and evaluation of the loss and printing it.", "tokens": [50364, 492, 1866, 300, 264, 3847, 293, 1323, 4470, 645, 466, 568, 13, 6866, 293, 370, 322, 13, 50664, 50664, 1692, 286, 1895, 578, 2769, 264, 3089, 257, 707, 857, 337, 264, 13344, 295, 23211, 37741, 13, 50914, 50914, 407, 291, 1320, 294, 257, 6798, 295, 597, 7472, 291, 1116, 411, 281, 13059, 13, 51064, 51064, 400, 550, 510, 11, 5413, 322, 3847, 11, 1323, 11, 420, 1500, 11, 286, 8186, 294, 293, 286, 483, 264, 3006, 7472, 13, 51364, 51364, 400, 550, 341, 307, 264, 2128, 1320, 295, 264, 3209, 293, 13344, 295, 264, 4470, 293, 14699, 309, 13, 51564, 51564, 407, 445, 1455, 309, 22842, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06345457644075961, "compression_ratio": 1.6983471074380165, "no_speech_prob": 2.857213985407725e-06}, {"id": 31, "seek": 15500, "start": 179.0, "end": 182.0, "text": " So just making it nicer.", "tokens": [50364, 492, 1866, 300, 264, 3847, 293, 1323, 4470, 645, 466, 568, 13, 6866, 293, 370, 322, 13, 50664, 50664, 1692, 286, 1895, 578, 2769, 264, 3089, 257, 707, 857, 337, 264, 13344, 295, 23211, 37741, 13, 50914, 50914, 407, 291, 1320, 294, 257, 6798, 295, 597, 7472, 291, 1116, 411, 281, 13059, 13, 51064, 51064, 400, 550, 510, 11, 5413, 322, 3847, 11, 1323, 11, 420, 1500, 11, 286, 8186, 294, 293, 286, 483, 264, 3006, 7472, 13, 51364, 51364, 400, 550, 341, 307, 264, 2128, 1320, 295, 264, 3209, 293, 13344, 295, 264, 4470, 293, 14699, 309, 13, 51564, 51564, 407, 445, 1455, 309, 22842, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06345457644075961, "compression_ratio": 1.6983471074380165, "no_speech_prob": 2.857213985407725e-06}, {"id": 32, "seek": 18200, "start": 182.0, "end": 191.0, "text": " One thing that you'll notice here is I'm using a decorator torch.nograd, which you can also look up and read documentation of.", "tokens": [50364, 1485, 551, 300, 291, 603, 3449, 510, 307, 286, 478, 1228, 257, 7919, 1639, 27822, 13, 1771, 7165, 11, 597, 291, 393, 611, 574, 493, 293, 1401, 14333, 295, 13, 50814, 50814, 8537, 11, 437, 341, 7919, 1639, 775, 322, 1192, 295, 257, 2445, 307, 300, 2035, 2314, 294, 341, 2445, 307, 15895, 538, 27822, 281, 1128, 3651, 604, 2771, 2448, 13, 51364, 51364, 407, 309, 486, 406, 360, 604, 295, 264, 1446, 25769, 300, 309, 775, 281, 1066, 2837, 295, 439, 264, 2771, 2448, 294, 35979, 295, 364, 33160, 23897, 1320, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07371082010957383, "compression_ratio": 1.6954732510288066, "no_speech_prob": 1.9525121388141997e-05}, {"id": 33, "seek": 18200, "start": 191.0, "end": 202.0, "text": " Basically, what this decorator does on top of a function is that whatever happens in this function is assumed by torch to never require any gradients.", "tokens": [50364, 1485, 551, 300, 291, 603, 3449, 510, 307, 286, 478, 1228, 257, 7919, 1639, 27822, 13, 1771, 7165, 11, 597, 291, 393, 611, 574, 493, 293, 1401, 14333, 295, 13, 50814, 50814, 8537, 11, 437, 341, 7919, 1639, 775, 322, 1192, 295, 257, 2445, 307, 300, 2035, 2314, 294, 341, 2445, 307, 15895, 538, 27822, 281, 1128, 3651, 604, 2771, 2448, 13, 51364, 51364, 407, 309, 486, 406, 360, 604, 295, 264, 1446, 25769, 300, 309, 775, 281, 1066, 2837, 295, 439, 264, 2771, 2448, 294, 35979, 295, 364, 33160, 23897, 1320, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07371082010957383, "compression_ratio": 1.6954732510288066, "no_speech_prob": 1.9525121388141997e-05}, {"id": 34, "seek": 18200, "start": 202.0, "end": 209.0, "text": " So it will not do any of the bookkeeping that it does to keep track of all the gradients in anticipation of an eventual backward pass.", "tokens": [50364, 1485, 551, 300, 291, 603, 3449, 510, 307, 286, 478, 1228, 257, 7919, 1639, 27822, 13, 1771, 7165, 11, 597, 291, 393, 611, 574, 493, 293, 1401, 14333, 295, 13, 50814, 50814, 8537, 11, 437, 341, 7919, 1639, 775, 322, 1192, 295, 257, 2445, 307, 300, 2035, 2314, 294, 341, 2445, 307, 15895, 538, 27822, 281, 1128, 3651, 604, 2771, 2448, 13, 51364, 51364, 407, 309, 486, 406, 360, 604, 295, 264, 1446, 25769, 300, 309, 775, 281, 1066, 2837, 295, 439, 264, 2771, 2448, 294, 35979, 295, 364, 33160, 23897, 1320, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07371082010957383, "compression_ratio": 1.6954732510288066, "no_speech_prob": 1.9525121388141997e-05}, {"id": 35, "seek": 20900, "start": 209.0, "end": 214.0, "text": " It's almost as if all the tensors that get created here have a requires grad of false.", "tokens": [50364, 467, 311, 1920, 382, 498, 439, 264, 10688, 830, 300, 483, 2942, 510, 362, 257, 7029, 2771, 295, 7908, 13, 50614, 50614, 400, 370, 309, 445, 1669, 1203, 709, 544, 7148, 570, 291, 434, 3585, 27822, 300, 286, 486, 406, 818, 300, 23897, 322, 604, 295, 341, 24903, 13, 50914, 50914, 400, 291, 500, 380, 643, 281, 6909, 264, 4295, 833, 264, 13376, 13, 51064, 51064, 407, 300, 311, 437, 341, 775, 13, 51164, 51164, 400, 291, 393, 611, 764, 257, 4319, 6598, 365, 27822, 13, 1771, 7165, 293, 291, 393, 574, 729, 493, 13, 51514, 51514, 1396, 510, 321, 362, 264, 21179, 490, 257, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07150671503565333, "compression_ratio": 1.637037037037037, "no_speech_prob": 2.190746499763918e-06}, {"id": 36, "seek": 20900, "start": 214.0, "end": 220.0, "text": " And so it just makes everything much more efficient because you're telling torch that I will not call that backward on any of this computation.", "tokens": [50364, 467, 311, 1920, 382, 498, 439, 264, 10688, 830, 300, 483, 2942, 510, 362, 257, 7029, 2771, 295, 7908, 13, 50614, 50614, 400, 370, 309, 445, 1669, 1203, 709, 544, 7148, 570, 291, 434, 3585, 27822, 300, 286, 486, 406, 818, 300, 23897, 322, 604, 295, 341, 24903, 13, 50914, 50914, 400, 291, 500, 380, 643, 281, 6909, 264, 4295, 833, 264, 13376, 13, 51064, 51064, 407, 300, 311, 437, 341, 775, 13, 51164, 51164, 400, 291, 393, 611, 764, 257, 4319, 6598, 365, 27822, 13, 1771, 7165, 293, 291, 393, 574, 729, 493, 13, 51514, 51514, 1396, 510, 321, 362, 264, 21179, 490, 257, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07150671503565333, "compression_ratio": 1.637037037037037, "no_speech_prob": 2.190746499763918e-06}, {"id": 37, "seek": 20900, "start": 220.0, "end": 223.0, "text": " And you don't need to maintain the graph under the hood.", "tokens": [50364, 467, 311, 1920, 382, 498, 439, 264, 10688, 830, 300, 483, 2942, 510, 362, 257, 7029, 2771, 295, 7908, 13, 50614, 50614, 400, 370, 309, 445, 1669, 1203, 709, 544, 7148, 570, 291, 434, 3585, 27822, 300, 286, 486, 406, 818, 300, 23897, 322, 604, 295, 341, 24903, 13, 50914, 50914, 400, 291, 500, 380, 643, 281, 6909, 264, 4295, 833, 264, 13376, 13, 51064, 51064, 407, 300, 311, 437, 341, 775, 13, 51164, 51164, 400, 291, 393, 611, 764, 257, 4319, 6598, 365, 27822, 13, 1771, 7165, 293, 291, 393, 574, 729, 493, 13, 51514, 51514, 1396, 510, 321, 362, 264, 21179, 490, 257, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07150671503565333, "compression_ratio": 1.637037037037037, "no_speech_prob": 2.190746499763918e-06}, {"id": 38, "seek": 20900, "start": 223.0, "end": 225.0, "text": " So that's what this does.", "tokens": [50364, 467, 311, 1920, 382, 498, 439, 264, 10688, 830, 300, 483, 2942, 510, 362, 257, 7029, 2771, 295, 7908, 13, 50614, 50614, 400, 370, 309, 445, 1669, 1203, 709, 544, 7148, 570, 291, 434, 3585, 27822, 300, 286, 486, 406, 818, 300, 23897, 322, 604, 295, 341, 24903, 13, 50914, 50914, 400, 291, 500, 380, 643, 281, 6909, 264, 4295, 833, 264, 13376, 13, 51064, 51064, 407, 300, 311, 437, 341, 775, 13, 51164, 51164, 400, 291, 393, 611, 764, 257, 4319, 6598, 365, 27822, 13, 1771, 7165, 293, 291, 393, 574, 729, 493, 13, 51514, 51514, 1396, 510, 321, 362, 264, 21179, 490, 257, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07150671503565333, "compression_ratio": 1.637037037037037, "no_speech_prob": 2.190746499763918e-06}, {"id": 39, "seek": 20900, "start": 225.0, "end": 232.0, "text": " And you can also use a context manager with torch.nograd and you can look those up.", "tokens": [50364, 467, 311, 1920, 382, 498, 439, 264, 10688, 830, 300, 483, 2942, 510, 362, 257, 7029, 2771, 295, 7908, 13, 50614, 50614, 400, 370, 309, 445, 1669, 1203, 709, 544, 7148, 570, 291, 434, 3585, 27822, 300, 286, 486, 406, 818, 300, 23897, 322, 604, 295, 341, 24903, 13, 50914, 50914, 400, 291, 500, 380, 643, 281, 6909, 264, 4295, 833, 264, 13376, 13, 51064, 51064, 407, 300, 311, 437, 341, 775, 13, 51164, 51164, 400, 291, 393, 611, 764, 257, 4319, 6598, 365, 27822, 13, 1771, 7165, 293, 291, 393, 574, 729, 493, 13, 51514, 51514, 1396, 510, 321, 362, 264, 21179, 490, 257, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07150671503565333, "compression_ratio": 1.637037037037037, "no_speech_prob": 2.190746499763918e-06}, {"id": 40, "seek": 20900, "start": 232.0, "end": 235.0, "text": " Then here we have the sampling from a model.", "tokens": [50364, 467, 311, 1920, 382, 498, 439, 264, 10688, 830, 300, 483, 2942, 510, 362, 257, 7029, 2771, 295, 7908, 13, 50614, 50614, 400, 370, 309, 445, 1669, 1203, 709, 544, 7148, 570, 291, 434, 3585, 27822, 300, 286, 486, 406, 818, 300, 23897, 322, 604, 295, 341, 24903, 13, 50914, 50914, 400, 291, 500, 380, 643, 281, 6909, 264, 4295, 833, 264, 13376, 13, 51064, 51064, 407, 300, 311, 437, 341, 775, 13, 51164, 51164, 400, 291, 393, 611, 764, 257, 4319, 6598, 365, 27822, 13, 1771, 7165, 293, 291, 393, 574, 729, 493, 13, 51514, 51514, 1396, 510, 321, 362, 264, 21179, 490, 257, 2316, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07150671503565333, "compression_ratio": 1.637037037037037, "no_speech_prob": 2.190746499763918e-06}, {"id": 41, "seek": 23500, "start": 235.0, "end": 244.0, "text": " Just as before, just a forward pass of a neural net, getting the distribution, sampling from it, adjusting the context window and repeating until we get the special end token.", "tokens": [50364, 1449, 382, 949, 11, 445, 257, 2128, 1320, 295, 257, 18161, 2533, 11, 1242, 264, 7316, 11, 21179, 490, 309, 11, 23559, 264, 4319, 4910, 293, 18617, 1826, 321, 483, 264, 2121, 917, 14862, 13, 50814, 50814, 400, 321, 536, 300, 321, 366, 2891, 281, 483, 709, 22842, 1237, 2283, 3247, 15551, 490, 264, 2316, 13, 51064, 51064, 467, 311, 920, 406, 2243, 293, 436, 434, 920, 406, 4498, 1315, 411, 11, 457, 309, 311, 709, 1101, 813, 562, 321, 632, 281, 352, 365, 264, 955, 2356, 2316, 13, 51464, 51464, 407, 300, 311, 527, 2891, 935, 13, 51564, 51564, 823, 11, 264, 700, 551, 286, 576, 411, 281, 28949, 259, 1125, 307, 264, 5883, 2144, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11079909584738991, "compression_ratio": 1.6941580756013745, "no_speech_prob": 7.071453637763625e-06}, {"id": 42, "seek": 23500, "start": 244.0, "end": 249.0, "text": " And we see that we are starting to get much nicer looking words sampled from the model.", "tokens": [50364, 1449, 382, 949, 11, 445, 257, 2128, 1320, 295, 257, 18161, 2533, 11, 1242, 264, 7316, 11, 21179, 490, 309, 11, 23559, 264, 4319, 4910, 293, 18617, 1826, 321, 483, 264, 2121, 917, 14862, 13, 50814, 50814, 400, 321, 536, 300, 321, 366, 2891, 281, 483, 709, 22842, 1237, 2283, 3247, 15551, 490, 264, 2316, 13, 51064, 51064, 467, 311, 920, 406, 2243, 293, 436, 434, 920, 406, 4498, 1315, 411, 11, 457, 309, 311, 709, 1101, 813, 562, 321, 632, 281, 352, 365, 264, 955, 2356, 2316, 13, 51464, 51464, 407, 300, 311, 527, 2891, 935, 13, 51564, 51564, 823, 11, 264, 700, 551, 286, 576, 411, 281, 28949, 259, 1125, 307, 264, 5883, 2144, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11079909584738991, "compression_ratio": 1.6941580756013745, "no_speech_prob": 7.071453637763625e-06}, {"id": 43, "seek": 23500, "start": 249.0, "end": 257.0, "text": " It's still not amazing and they're still not fully name like, but it's much better than when we had to go with the bigram model.", "tokens": [50364, 1449, 382, 949, 11, 445, 257, 2128, 1320, 295, 257, 18161, 2533, 11, 1242, 264, 7316, 11, 21179, 490, 309, 11, 23559, 264, 4319, 4910, 293, 18617, 1826, 321, 483, 264, 2121, 917, 14862, 13, 50814, 50814, 400, 321, 536, 300, 321, 366, 2891, 281, 483, 709, 22842, 1237, 2283, 3247, 15551, 490, 264, 2316, 13, 51064, 51064, 467, 311, 920, 406, 2243, 293, 436, 434, 920, 406, 4498, 1315, 411, 11, 457, 309, 311, 709, 1101, 813, 562, 321, 632, 281, 352, 365, 264, 955, 2356, 2316, 13, 51464, 51464, 407, 300, 311, 527, 2891, 935, 13, 51564, 51564, 823, 11, 264, 700, 551, 286, 576, 411, 281, 28949, 259, 1125, 307, 264, 5883, 2144, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11079909584738991, "compression_ratio": 1.6941580756013745, "no_speech_prob": 7.071453637763625e-06}, {"id": 44, "seek": 23500, "start": 257.0, "end": 259.0, "text": " So that's our starting point.", "tokens": [50364, 1449, 382, 949, 11, 445, 257, 2128, 1320, 295, 257, 18161, 2533, 11, 1242, 264, 7316, 11, 21179, 490, 309, 11, 23559, 264, 4319, 4910, 293, 18617, 1826, 321, 483, 264, 2121, 917, 14862, 13, 50814, 50814, 400, 321, 536, 300, 321, 366, 2891, 281, 483, 709, 22842, 1237, 2283, 3247, 15551, 490, 264, 2316, 13, 51064, 51064, 467, 311, 920, 406, 2243, 293, 436, 434, 920, 406, 4498, 1315, 411, 11, 457, 309, 311, 709, 1101, 813, 562, 321, 632, 281, 352, 365, 264, 955, 2356, 2316, 13, 51464, 51464, 407, 300, 311, 527, 2891, 935, 13, 51564, 51564, 823, 11, 264, 700, 551, 286, 576, 411, 281, 28949, 259, 1125, 307, 264, 5883, 2144, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11079909584738991, "compression_ratio": 1.6941580756013745, "no_speech_prob": 7.071453637763625e-06}, {"id": 45, "seek": 23500, "start": 259.0, "end": 262.0, "text": " Now, the first thing I would like to scrutinize is the initialization.", "tokens": [50364, 1449, 382, 949, 11, 445, 257, 2128, 1320, 295, 257, 18161, 2533, 11, 1242, 264, 7316, 11, 21179, 490, 309, 11, 23559, 264, 4319, 4910, 293, 18617, 1826, 321, 483, 264, 2121, 917, 14862, 13, 50814, 50814, 400, 321, 536, 300, 321, 366, 2891, 281, 483, 709, 22842, 1237, 2283, 3247, 15551, 490, 264, 2316, 13, 51064, 51064, 467, 311, 920, 406, 2243, 293, 436, 434, 920, 406, 4498, 1315, 411, 11, 457, 309, 311, 709, 1101, 813, 562, 321, 632, 281, 352, 365, 264, 955, 2356, 2316, 13, 51464, 51464, 407, 300, 311, 527, 2891, 935, 13, 51564, 51564, 823, 11, 264, 700, 551, 286, 576, 411, 281, 28949, 259, 1125, 307, 264, 5883, 2144, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11079909584738991, "compression_ratio": 1.6941580756013745, "no_speech_prob": 7.071453637763625e-06}, {"id": 46, "seek": 26200, "start": 262.0, "end": 269.0, "text": " I can tell that our network is very improperly configured at initialization and there's multiple things wrong with it.", "tokens": [50364, 286, 393, 980, 300, 527, 3209, 307, 588, 40651, 356, 30538, 412, 5883, 2144, 293, 456, 311, 3866, 721, 2085, 365, 309, 13, 50714, 50714, 583, 718, 311, 445, 722, 365, 264, 700, 472, 13, 50814, 50814, 2053, 510, 322, 264, 4018, 24784, 11, 264, 588, 700, 24784, 11, 321, 366, 6613, 257, 4470, 295, 7634, 293, 341, 12910, 1487, 760, 281, 9810, 472, 420, 732, 420, 370, 13, 51264, 51264, 407, 286, 393, 980, 300, 264, 5883, 2144, 307, 439, 16507, 493, 570, 341, 307, 636, 886, 1090, 13, 51464, 51464, 682, 3097, 295, 18161, 36170, 11, 309, 307, 1920, 1009, 264, 1389, 300, 291, 486, 362, 257, 5903, 1558, 337, 437, 4470, 281, 2066, 412, 5883, 2144, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06883423559127315, "compression_ratio": 1.7516778523489933, "no_speech_prob": 1.473767861170927e-05}, {"id": 47, "seek": 26200, "start": 269.0, "end": 271.0, "text": " But let's just start with the first one.", "tokens": [50364, 286, 393, 980, 300, 527, 3209, 307, 588, 40651, 356, 30538, 412, 5883, 2144, 293, 456, 311, 3866, 721, 2085, 365, 309, 13, 50714, 50714, 583, 718, 311, 445, 722, 365, 264, 700, 472, 13, 50814, 50814, 2053, 510, 322, 264, 4018, 24784, 11, 264, 588, 700, 24784, 11, 321, 366, 6613, 257, 4470, 295, 7634, 293, 341, 12910, 1487, 760, 281, 9810, 472, 420, 732, 420, 370, 13, 51264, 51264, 407, 286, 393, 980, 300, 264, 5883, 2144, 307, 439, 16507, 493, 570, 341, 307, 636, 886, 1090, 13, 51464, 51464, 682, 3097, 295, 18161, 36170, 11, 309, 307, 1920, 1009, 264, 1389, 300, 291, 486, 362, 257, 5903, 1558, 337, 437, 4470, 281, 2066, 412, 5883, 2144, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06883423559127315, "compression_ratio": 1.7516778523489933, "no_speech_prob": 1.473767861170927e-05}, {"id": 48, "seek": 26200, "start": 271.0, "end": 280.0, "text": " Look here on the zero iteration, the very first iteration, we are recording a loss of 27 and this rapidly comes down to roughly one or two or so.", "tokens": [50364, 286, 393, 980, 300, 527, 3209, 307, 588, 40651, 356, 30538, 412, 5883, 2144, 293, 456, 311, 3866, 721, 2085, 365, 309, 13, 50714, 50714, 583, 718, 311, 445, 722, 365, 264, 700, 472, 13, 50814, 50814, 2053, 510, 322, 264, 4018, 24784, 11, 264, 588, 700, 24784, 11, 321, 366, 6613, 257, 4470, 295, 7634, 293, 341, 12910, 1487, 760, 281, 9810, 472, 420, 732, 420, 370, 13, 51264, 51264, 407, 286, 393, 980, 300, 264, 5883, 2144, 307, 439, 16507, 493, 570, 341, 307, 636, 886, 1090, 13, 51464, 51464, 682, 3097, 295, 18161, 36170, 11, 309, 307, 1920, 1009, 264, 1389, 300, 291, 486, 362, 257, 5903, 1558, 337, 437, 4470, 281, 2066, 412, 5883, 2144, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06883423559127315, "compression_ratio": 1.7516778523489933, "no_speech_prob": 1.473767861170927e-05}, {"id": 49, "seek": 26200, "start": 280.0, "end": 284.0, "text": " So I can tell that the initialization is all messed up because this is way too high.", "tokens": [50364, 286, 393, 980, 300, 527, 3209, 307, 588, 40651, 356, 30538, 412, 5883, 2144, 293, 456, 311, 3866, 721, 2085, 365, 309, 13, 50714, 50714, 583, 718, 311, 445, 722, 365, 264, 700, 472, 13, 50814, 50814, 2053, 510, 322, 264, 4018, 24784, 11, 264, 588, 700, 24784, 11, 321, 366, 6613, 257, 4470, 295, 7634, 293, 341, 12910, 1487, 760, 281, 9810, 472, 420, 732, 420, 370, 13, 51264, 51264, 407, 286, 393, 980, 300, 264, 5883, 2144, 307, 439, 16507, 493, 570, 341, 307, 636, 886, 1090, 13, 51464, 51464, 682, 3097, 295, 18161, 36170, 11, 309, 307, 1920, 1009, 264, 1389, 300, 291, 486, 362, 257, 5903, 1558, 337, 437, 4470, 281, 2066, 412, 5883, 2144, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06883423559127315, "compression_ratio": 1.7516778523489933, "no_speech_prob": 1.473767861170927e-05}, {"id": 50, "seek": 26200, "start": 284.0, "end": 290.0, "text": " In training of neural nets, it is almost always the case that you will have a rough idea for what loss to expect at initialization.", "tokens": [50364, 286, 393, 980, 300, 527, 3209, 307, 588, 40651, 356, 30538, 412, 5883, 2144, 293, 456, 311, 3866, 721, 2085, 365, 309, 13, 50714, 50714, 583, 718, 311, 445, 722, 365, 264, 700, 472, 13, 50814, 50814, 2053, 510, 322, 264, 4018, 24784, 11, 264, 588, 700, 24784, 11, 321, 366, 6613, 257, 4470, 295, 7634, 293, 341, 12910, 1487, 760, 281, 9810, 472, 420, 732, 420, 370, 13, 51264, 51264, 407, 286, 393, 980, 300, 264, 5883, 2144, 307, 439, 16507, 493, 570, 341, 307, 636, 886, 1090, 13, 51464, 51464, 682, 3097, 295, 18161, 36170, 11, 309, 307, 1920, 1009, 264, 1389, 300, 291, 486, 362, 257, 5903, 1558, 337, 437, 4470, 281, 2066, 412, 5883, 2144, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06883423559127315, "compression_ratio": 1.7516778523489933, "no_speech_prob": 1.473767861170927e-05}, {"id": 51, "seek": 29000, "start": 290.0, "end": 294.0, "text": " And that just depends on the loss function and the problem set up.", "tokens": [50364, 400, 300, 445, 5946, 322, 264, 4470, 2445, 293, 264, 1154, 992, 493, 13, 50564, 50564, 682, 341, 1389, 11, 286, 360, 406, 2066, 7634, 13, 50714, 50714, 286, 2066, 257, 709, 3126, 1230, 293, 321, 393, 8873, 309, 1214, 13, 50864, 50864, 8537, 11, 412, 5883, 2144, 11, 437, 321, 1116, 411, 307, 300, 456, 311, 7634, 4342, 300, 727, 808, 958, 337, 604, 472, 3097, 1365, 13, 51314, 51314, 1711, 5883, 2144, 11, 321, 362, 572, 1778, 281, 1697, 604, 4342, 281, 312, 709, 544, 3700, 813, 2357, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06194680866442229, "compression_ratio": 1.6234309623430963, "no_speech_prob": 8.267430530395359e-06}, {"id": 52, "seek": 29000, "start": 294.0, "end": 297.0, "text": " In this case, I do not expect 27.", "tokens": [50364, 400, 300, 445, 5946, 322, 264, 4470, 2445, 293, 264, 1154, 992, 493, 13, 50564, 50564, 682, 341, 1389, 11, 286, 360, 406, 2066, 7634, 13, 50714, 50714, 286, 2066, 257, 709, 3126, 1230, 293, 321, 393, 8873, 309, 1214, 13, 50864, 50864, 8537, 11, 412, 5883, 2144, 11, 437, 321, 1116, 411, 307, 300, 456, 311, 7634, 4342, 300, 727, 808, 958, 337, 604, 472, 3097, 1365, 13, 51314, 51314, 1711, 5883, 2144, 11, 321, 362, 572, 1778, 281, 1697, 604, 4342, 281, 312, 709, 544, 3700, 813, 2357, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06194680866442229, "compression_ratio": 1.6234309623430963, "no_speech_prob": 8.267430530395359e-06}, {"id": 53, "seek": 29000, "start": 297.0, "end": 300.0, "text": " I expect a much lower number and we can calculate it together.", "tokens": [50364, 400, 300, 445, 5946, 322, 264, 4470, 2445, 293, 264, 1154, 992, 493, 13, 50564, 50564, 682, 341, 1389, 11, 286, 360, 406, 2066, 7634, 13, 50714, 50714, 286, 2066, 257, 709, 3126, 1230, 293, 321, 393, 8873, 309, 1214, 13, 50864, 50864, 8537, 11, 412, 5883, 2144, 11, 437, 321, 1116, 411, 307, 300, 456, 311, 7634, 4342, 300, 727, 808, 958, 337, 604, 472, 3097, 1365, 13, 51314, 51314, 1711, 5883, 2144, 11, 321, 362, 572, 1778, 281, 1697, 604, 4342, 281, 312, 709, 544, 3700, 813, 2357, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06194680866442229, "compression_ratio": 1.6234309623430963, "no_speech_prob": 8.267430530395359e-06}, {"id": 54, "seek": 29000, "start": 300.0, "end": 309.0, "text": " Basically, at initialization, what we'd like is that there's 27 characters that could come next for any one training example.", "tokens": [50364, 400, 300, 445, 5946, 322, 264, 4470, 2445, 293, 264, 1154, 992, 493, 13, 50564, 50564, 682, 341, 1389, 11, 286, 360, 406, 2066, 7634, 13, 50714, 50714, 286, 2066, 257, 709, 3126, 1230, 293, 321, 393, 8873, 309, 1214, 13, 50864, 50864, 8537, 11, 412, 5883, 2144, 11, 437, 321, 1116, 411, 307, 300, 456, 311, 7634, 4342, 300, 727, 808, 958, 337, 604, 472, 3097, 1365, 13, 51314, 51314, 1711, 5883, 2144, 11, 321, 362, 572, 1778, 281, 1697, 604, 4342, 281, 312, 709, 544, 3700, 813, 2357, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06194680866442229, "compression_ratio": 1.6234309623430963, "no_speech_prob": 8.267430530395359e-06}, {"id": 55, "seek": 29000, "start": 309.0, "end": 313.0, "text": " At initialization, we have no reason to believe any characters to be much more likely than others.", "tokens": [50364, 400, 300, 445, 5946, 322, 264, 4470, 2445, 293, 264, 1154, 992, 493, 13, 50564, 50564, 682, 341, 1389, 11, 286, 360, 406, 2066, 7634, 13, 50714, 50714, 286, 2066, 257, 709, 3126, 1230, 293, 321, 393, 8873, 309, 1214, 13, 50864, 50864, 8537, 11, 412, 5883, 2144, 11, 437, 321, 1116, 411, 307, 300, 456, 311, 7634, 4342, 300, 727, 808, 958, 337, 604, 472, 3097, 1365, 13, 51314, 51314, 1711, 5883, 2144, 11, 321, 362, 572, 1778, 281, 1697, 604, 4342, 281, 312, 709, 544, 3700, 813, 2357, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06194680866442229, "compression_ratio": 1.6234309623430963, "no_speech_prob": 8.267430530395359e-06}, {"id": 56, "seek": 31300, "start": 313.0, "end": 322.0, "text": " And so we'd expect that the probability distribution that comes out initially is a uniform distribution assigning about equal probability to all the 27 characters.", "tokens": [50364, 400, 370, 321, 1116, 2066, 300, 264, 8482, 7316, 300, 1487, 484, 9105, 307, 257, 9452, 7316, 49602, 466, 2681, 8482, 281, 439, 264, 7634, 4342, 13, 50814, 50814, 407, 1936, 437, 321, 1116, 411, 307, 264, 8482, 337, 604, 2517, 576, 312, 9810, 472, 670, 7634, 13, 51214, 51214, 663, 307, 264, 8482, 321, 820, 2136, 13, 51364, 51364, 400, 550, 264, 4470, 307, 264, 3671, 3565, 8482, 13, 51514, 51514, 407, 718, 311, 7019, 341, 294, 257, 40863, 13, 51664, 51664, 400, 550, 550, 321, 393, 747, 264, 3565, 295, 309, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09505487948047872, "compression_ratio": 1.8197424892703862, "no_speech_prob": 5.0935045692313e-06}, {"id": 57, "seek": 31300, "start": 322.0, "end": 330.0, "text": " So basically what we'd like is the probability for any character would be roughly one over 27.", "tokens": [50364, 400, 370, 321, 1116, 2066, 300, 264, 8482, 7316, 300, 1487, 484, 9105, 307, 257, 9452, 7316, 49602, 466, 2681, 8482, 281, 439, 264, 7634, 4342, 13, 50814, 50814, 407, 1936, 437, 321, 1116, 411, 307, 264, 8482, 337, 604, 2517, 576, 312, 9810, 472, 670, 7634, 13, 51214, 51214, 663, 307, 264, 8482, 321, 820, 2136, 13, 51364, 51364, 400, 550, 264, 4470, 307, 264, 3671, 3565, 8482, 13, 51514, 51514, 407, 718, 311, 7019, 341, 294, 257, 40863, 13, 51664, 51664, 400, 550, 550, 321, 393, 747, 264, 3565, 295, 309, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09505487948047872, "compression_ratio": 1.8197424892703862, "no_speech_prob": 5.0935045692313e-06}, {"id": 58, "seek": 31300, "start": 330.0, "end": 333.0, "text": " That is the probability we should record.", "tokens": [50364, 400, 370, 321, 1116, 2066, 300, 264, 8482, 7316, 300, 1487, 484, 9105, 307, 257, 9452, 7316, 49602, 466, 2681, 8482, 281, 439, 264, 7634, 4342, 13, 50814, 50814, 407, 1936, 437, 321, 1116, 411, 307, 264, 8482, 337, 604, 2517, 576, 312, 9810, 472, 670, 7634, 13, 51214, 51214, 663, 307, 264, 8482, 321, 820, 2136, 13, 51364, 51364, 400, 550, 264, 4470, 307, 264, 3671, 3565, 8482, 13, 51514, 51514, 407, 718, 311, 7019, 341, 294, 257, 40863, 13, 51664, 51664, 400, 550, 550, 321, 393, 747, 264, 3565, 295, 309, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09505487948047872, "compression_ratio": 1.8197424892703862, "no_speech_prob": 5.0935045692313e-06}, {"id": 59, "seek": 31300, "start": 333.0, "end": 336.0, "text": " And then the loss is the negative log probability.", "tokens": [50364, 400, 370, 321, 1116, 2066, 300, 264, 8482, 7316, 300, 1487, 484, 9105, 307, 257, 9452, 7316, 49602, 466, 2681, 8482, 281, 439, 264, 7634, 4342, 13, 50814, 50814, 407, 1936, 437, 321, 1116, 411, 307, 264, 8482, 337, 604, 2517, 576, 312, 9810, 472, 670, 7634, 13, 51214, 51214, 663, 307, 264, 8482, 321, 820, 2136, 13, 51364, 51364, 400, 550, 264, 4470, 307, 264, 3671, 3565, 8482, 13, 51514, 51514, 407, 718, 311, 7019, 341, 294, 257, 40863, 13, 51664, 51664, 400, 550, 550, 321, 393, 747, 264, 3565, 295, 309, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09505487948047872, "compression_ratio": 1.8197424892703862, "no_speech_prob": 5.0935045692313e-06}, {"id": 60, "seek": 31300, "start": 336.0, "end": 339.0, "text": " So let's wrap this in a tensor.", "tokens": [50364, 400, 370, 321, 1116, 2066, 300, 264, 8482, 7316, 300, 1487, 484, 9105, 307, 257, 9452, 7316, 49602, 466, 2681, 8482, 281, 439, 264, 7634, 4342, 13, 50814, 50814, 407, 1936, 437, 321, 1116, 411, 307, 264, 8482, 337, 604, 2517, 576, 312, 9810, 472, 670, 7634, 13, 51214, 51214, 663, 307, 264, 8482, 321, 820, 2136, 13, 51364, 51364, 400, 550, 264, 4470, 307, 264, 3671, 3565, 8482, 13, 51514, 51514, 407, 718, 311, 7019, 341, 294, 257, 40863, 13, 51664, 51664, 400, 550, 550, 321, 393, 747, 264, 3565, 295, 309, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09505487948047872, "compression_ratio": 1.8197424892703862, "no_speech_prob": 5.0935045692313e-06}, {"id": 61, "seek": 31300, "start": 339.0, "end": 341.0, "text": " And then then we can take the log of it.", "tokens": [50364, 400, 370, 321, 1116, 2066, 300, 264, 8482, 7316, 300, 1487, 484, 9105, 307, 257, 9452, 7316, 49602, 466, 2681, 8482, 281, 439, 264, 7634, 4342, 13, 50814, 50814, 407, 1936, 437, 321, 1116, 411, 307, 264, 8482, 337, 604, 2517, 576, 312, 9810, 472, 670, 7634, 13, 51214, 51214, 663, 307, 264, 8482, 321, 820, 2136, 13, 51364, 51364, 400, 550, 264, 4470, 307, 264, 3671, 3565, 8482, 13, 51514, 51514, 407, 718, 311, 7019, 341, 294, 257, 40863, 13, 51664, 51664, 400, 550, 550, 321, 393, 747, 264, 3565, 295, 309, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09505487948047872, "compression_ratio": 1.8197424892703862, "no_speech_prob": 5.0935045692313e-06}, {"id": 62, "seek": 34100, "start": 341.0, "end": 349.0, "text": " And then the negative log probability is the loss we would expect, which is 3.29, much, much lower than 27.", "tokens": [50364, 400, 550, 264, 3671, 3565, 8482, 307, 264, 4470, 321, 576, 2066, 11, 597, 307, 805, 13, 11871, 11, 709, 11, 709, 3126, 813, 7634, 13, 50764, 50764, 400, 370, 437, 311, 2737, 558, 586, 307, 300, 412, 5883, 2144, 11, 264, 18161, 2533, 307, 4084, 8482, 37870, 300, 366, 439, 16507, 493, 13, 51114, 51114, 2188, 4342, 366, 588, 6679, 293, 512, 4342, 366, 588, 406, 6679, 13, 51314, 51314, 400, 550, 1936, 437, 311, 2737, 307, 300, 264, 3209, 307, 588, 41956, 2085, 13, 51564, 51564, 400, 300, 311, 437, 1669, 309, 2136, 588, 1090, 4470, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05136963927630082, "compression_ratio": 1.82, "no_speech_prob": 1.2029102435917594e-05}, {"id": 63, "seek": 34100, "start": 349.0, "end": 356.0, "text": " And so what's happening right now is that at initialization, the neural net is creating probability distributions that are all messed up.", "tokens": [50364, 400, 550, 264, 3671, 3565, 8482, 307, 264, 4470, 321, 576, 2066, 11, 597, 307, 805, 13, 11871, 11, 709, 11, 709, 3126, 813, 7634, 13, 50764, 50764, 400, 370, 437, 311, 2737, 558, 586, 307, 300, 412, 5883, 2144, 11, 264, 18161, 2533, 307, 4084, 8482, 37870, 300, 366, 439, 16507, 493, 13, 51114, 51114, 2188, 4342, 366, 588, 6679, 293, 512, 4342, 366, 588, 406, 6679, 13, 51314, 51314, 400, 550, 1936, 437, 311, 2737, 307, 300, 264, 3209, 307, 588, 41956, 2085, 13, 51564, 51564, 400, 300, 311, 437, 1669, 309, 2136, 588, 1090, 4470, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05136963927630082, "compression_ratio": 1.82, "no_speech_prob": 1.2029102435917594e-05}, {"id": 64, "seek": 34100, "start": 356.0, "end": 360.0, "text": " Some characters are very confident and some characters are very not confident.", "tokens": [50364, 400, 550, 264, 3671, 3565, 8482, 307, 264, 4470, 321, 576, 2066, 11, 597, 307, 805, 13, 11871, 11, 709, 11, 709, 3126, 813, 7634, 13, 50764, 50764, 400, 370, 437, 311, 2737, 558, 586, 307, 300, 412, 5883, 2144, 11, 264, 18161, 2533, 307, 4084, 8482, 37870, 300, 366, 439, 16507, 493, 13, 51114, 51114, 2188, 4342, 366, 588, 6679, 293, 512, 4342, 366, 588, 406, 6679, 13, 51314, 51314, 400, 550, 1936, 437, 311, 2737, 307, 300, 264, 3209, 307, 588, 41956, 2085, 13, 51564, 51564, 400, 300, 311, 437, 1669, 309, 2136, 588, 1090, 4470, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05136963927630082, "compression_ratio": 1.82, "no_speech_prob": 1.2029102435917594e-05}, {"id": 65, "seek": 34100, "start": 360.0, "end": 365.0, "text": " And then basically what's happening is that the network is very confidently wrong.", "tokens": [50364, 400, 550, 264, 3671, 3565, 8482, 307, 264, 4470, 321, 576, 2066, 11, 597, 307, 805, 13, 11871, 11, 709, 11, 709, 3126, 813, 7634, 13, 50764, 50764, 400, 370, 437, 311, 2737, 558, 586, 307, 300, 412, 5883, 2144, 11, 264, 18161, 2533, 307, 4084, 8482, 37870, 300, 366, 439, 16507, 493, 13, 51114, 51114, 2188, 4342, 366, 588, 6679, 293, 512, 4342, 366, 588, 406, 6679, 13, 51314, 51314, 400, 550, 1936, 437, 311, 2737, 307, 300, 264, 3209, 307, 588, 41956, 2085, 13, 51564, 51564, 400, 300, 311, 437, 1669, 309, 2136, 588, 1090, 4470, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05136963927630082, "compression_ratio": 1.82, "no_speech_prob": 1.2029102435917594e-05}, {"id": 66, "seek": 34100, "start": 365.0, "end": 370.0, "text": " And that's what makes it record very high loss.", "tokens": [50364, 400, 550, 264, 3671, 3565, 8482, 307, 264, 4470, 321, 576, 2066, 11, 597, 307, 805, 13, 11871, 11, 709, 11, 709, 3126, 813, 7634, 13, 50764, 50764, 400, 370, 437, 311, 2737, 558, 586, 307, 300, 412, 5883, 2144, 11, 264, 18161, 2533, 307, 4084, 8482, 37870, 300, 366, 439, 16507, 493, 13, 51114, 51114, 2188, 4342, 366, 588, 6679, 293, 512, 4342, 366, 588, 406, 6679, 13, 51314, 51314, 400, 550, 1936, 437, 311, 2737, 307, 300, 264, 3209, 307, 588, 41956, 2085, 13, 51564, 51564, 400, 300, 311, 437, 1669, 309, 2136, 588, 1090, 4470, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05136963927630082, "compression_ratio": 1.82, "no_speech_prob": 1.2029102435917594e-05}, {"id": 67, "seek": 37000, "start": 370.0, "end": 373.0, "text": " So here's a smaller four dimensional example of the issue.", "tokens": [50364, 407, 510, 311, 257, 4356, 1451, 18795, 1365, 295, 264, 2734, 13, 50514, 50514, 961, 311, 584, 321, 787, 362, 1451, 4342, 293, 550, 321, 362, 3565, 1208, 300, 808, 484, 295, 264, 18161, 2533, 293, 436, 366, 588, 11, 588, 1998, 281, 4018, 13, 50864, 50864, 1396, 562, 321, 747, 264, 2787, 41167, 295, 439, 35193, 11, 321, 483, 33783, 456, 366, 257, 42165, 7316, 13, 51214, 51214, 407, 34499, 281, 472, 293, 307, 2293, 9452, 13, 51414, 51414, 400, 550, 294, 341, 1389, 11, 498, 264, 7645, 307, 11, 584, 11, 732, 11, 309, 1177, 380, 767, 1871, 498, 341, 498, 264, 7645, 307, 732, 420, 1045, 420, 472, 420, 4018, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07367932594428628, "compression_ratio": 1.6974169741697418, "no_speech_prob": 2.601566848170478e-06}, {"id": 68, "seek": 37000, "start": 373.0, "end": 380.0, "text": " Let's say we only have four characters and then we have logits that come out of the neural net and they are very, very close to zero.", "tokens": [50364, 407, 510, 311, 257, 4356, 1451, 18795, 1365, 295, 264, 2734, 13, 50514, 50514, 961, 311, 584, 321, 787, 362, 1451, 4342, 293, 550, 321, 362, 3565, 1208, 300, 808, 484, 295, 264, 18161, 2533, 293, 436, 366, 588, 11, 588, 1998, 281, 4018, 13, 50864, 50864, 1396, 562, 321, 747, 264, 2787, 41167, 295, 439, 35193, 11, 321, 483, 33783, 456, 366, 257, 42165, 7316, 13, 51214, 51214, 407, 34499, 281, 472, 293, 307, 2293, 9452, 13, 51414, 51414, 400, 550, 294, 341, 1389, 11, 498, 264, 7645, 307, 11, 584, 11, 732, 11, 309, 1177, 380, 767, 1871, 498, 341, 498, 264, 7645, 307, 732, 420, 1045, 420, 472, 420, 4018, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07367932594428628, "compression_ratio": 1.6974169741697418, "no_speech_prob": 2.601566848170478e-06}, {"id": 69, "seek": 37000, "start": 380.0, "end": 387.0, "text": " Then when we take the softmax of all zeros, we get probabilities there are a diffuse distribution.", "tokens": [50364, 407, 510, 311, 257, 4356, 1451, 18795, 1365, 295, 264, 2734, 13, 50514, 50514, 961, 311, 584, 321, 787, 362, 1451, 4342, 293, 550, 321, 362, 3565, 1208, 300, 808, 484, 295, 264, 18161, 2533, 293, 436, 366, 588, 11, 588, 1998, 281, 4018, 13, 50864, 50864, 1396, 562, 321, 747, 264, 2787, 41167, 295, 439, 35193, 11, 321, 483, 33783, 456, 366, 257, 42165, 7316, 13, 51214, 51214, 407, 34499, 281, 472, 293, 307, 2293, 9452, 13, 51414, 51414, 400, 550, 294, 341, 1389, 11, 498, 264, 7645, 307, 11, 584, 11, 732, 11, 309, 1177, 380, 767, 1871, 498, 341, 498, 264, 7645, 307, 732, 420, 1045, 420, 472, 420, 4018, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07367932594428628, "compression_ratio": 1.6974169741697418, "no_speech_prob": 2.601566848170478e-06}, {"id": 70, "seek": 37000, "start": 387.0, "end": 391.0, "text": " So sums to one and is exactly uniform.", "tokens": [50364, 407, 510, 311, 257, 4356, 1451, 18795, 1365, 295, 264, 2734, 13, 50514, 50514, 961, 311, 584, 321, 787, 362, 1451, 4342, 293, 550, 321, 362, 3565, 1208, 300, 808, 484, 295, 264, 18161, 2533, 293, 436, 366, 588, 11, 588, 1998, 281, 4018, 13, 50864, 50864, 1396, 562, 321, 747, 264, 2787, 41167, 295, 439, 35193, 11, 321, 483, 33783, 456, 366, 257, 42165, 7316, 13, 51214, 51214, 407, 34499, 281, 472, 293, 307, 2293, 9452, 13, 51414, 51414, 400, 550, 294, 341, 1389, 11, 498, 264, 7645, 307, 11, 584, 11, 732, 11, 309, 1177, 380, 767, 1871, 498, 341, 498, 264, 7645, 307, 732, 420, 1045, 420, 472, 420, 4018, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07367932594428628, "compression_ratio": 1.6974169741697418, "no_speech_prob": 2.601566848170478e-06}, {"id": 71, "seek": 37000, "start": 391.0, "end": 398.0, "text": " And then in this case, if the label is, say, two, it doesn't actually matter if this if the label is two or three or one or zero,", "tokens": [50364, 407, 510, 311, 257, 4356, 1451, 18795, 1365, 295, 264, 2734, 13, 50514, 50514, 961, 311, 584, 321, 787, 362, 1451, 4342, 293, 550, 321, 362, 3565, 1208, 300, 808, 484, 295, 264, 18161, 2533, 293, 436, 366, 588, 11, 588, 1998, 281, 4018, 13, 50864, 50864, 1396, 562, 321, 747, 264, 2787, 41167, 295, 439, 35193, 11, 321, 483, 33783, 456, 366, 257, 42165, 7316, 13, 51214, 51214, 407, 34499, 281, 472, 293, 307, 2293, 9452, 13, 51414, 51414, 400, 550, 294, 341, 1389, 11, 498, 264, 7645, 307, 11, 584, 11, 732, 11, 309, 1177, 380, 767, 1871, 498, 341, 498, 264, 7645, 307, 732, 420, 1045, 420, 472, 420, 4018, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07367932594428628, "compression_ratio": 1.6974169741697418, "no_speech_prob": 2.601566848170478e-06}, {"id": 72, "seek": 39800, "start": 398.0, "end": 403.0, "text": " because it's a uniform distribution, we're recording the exact same loss in this case, 1.38.", "tokens": [50364, 570, 309, 311, 257, 9452, 7316, 11, 321, 434, 6613, 264, 1900, 912, 4470, 294, 341, 1389, 11, 502, 13, 12625, 13, 50614, 50614, 407, 341, 307, 264, 4470, 321, 576, 2066, 337, 257, 1451, 18795, 1365, 13, 50764, 50764, 400, 286, 393, 536, 11, 295, 1164, 11, 300, 382, 321, 722, 281, 20459, 613, 3565, 1208, 11, 321, 434, 516, 281, 312, 4473, 264, 4470, 510, 13, 51064, 51064, 407, 309, 727, 312, 300, 321, 4017, 484, 293, 538, 2931, 341, 727, 312, 257, 588, 1090, 1230, 411, 1732, 420, 746, 411, 300, 13, 51414, 51414, 1396, 294, 300, 1389, 11, 321, 603, 2136, 257, 588, 2295, 4470, 570, 321, 434, 49602, 264, 3006, 8482, 412, 5883, 2144, 538, 2931, 281, 264, 3006, 7645, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06874445401705229, "compression_ratio": 1.7557755775577557, "no_speech_prob": 7.296069270523731e-06}, {"id": 73, "seek": 39800, "start": 403.0, "end": 406.0, "text": " So this is the loss we would expect for a four dimensional example.", "tokens": [50364, 570, 309, 311, 257, 9452, 7316, 11, 321, 434, 6613, 264, 1900, 912, 4470, 294, 341, 1389, 11, 502, 13, 12625, 13, 50614, 50614, 407, 341, 307, 264, 4470, 321, 576, 2066, 337, 257, 1451, 18795, 1365, 13, 50764, 50764, 400, 286, 393, 536, 11, 295, 1164, 11, 300, 382, 321, 722, 281, 20459, 613, 3565, 1208, 11, 321, 434, 516, 281, 312, 4473, 264, 4470, 510, 13, 51064, 51064, 407, 309, 727, 312, 300, 321, 4017, 484, 293, 538, 2931, 341, 727, 312, 257, 588, 1090, 1230, 411, 1732, 420, 746, 411, 300, 13, 51414, 51414, 1396, 294, 300, 1389, 11, 321, 603, 2136, 257, 588, 2295, 4470, 570, 321, 434, 49602, 264, 3006, 8482, 412, 5883, 2144, 538, 2931, 281, 264, 3006, 7645, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06874445401705229, "compression_ratio": 1.7557755775577557, "no_speech_prob": 7.296069270523731e-06}, {"id": 74, "seek": 39800, "start": 406.0, "end": 412.0, "text": " And I can see, of course, that as we start to manipulate these logits, we're going to be changing the loss here.", "tokens": [50364, 570, 309, 311, 257, 9452, 7316, 11, 321, 434, 6613, 264, 1900, 912, 4470, 294, 341, 1389, 11, 502, 13, 12625, 13, 50614, 50614, 407, 341, 307, 264, 4470, 321, 576, 2066, 337, 257, 1451, 18795, 1365, 13, 50764, 50764, 400, 286, 393, 536, 11, 295, 1164, 11, 300, 382, 321, 722, 281, 20459, 613, 3565, 1208, 11, 321, 434, 516, 281, 312, 4473, 264, 4470, 510, 13, 51064, 51064, 407, 309, 727, 312, 300, 321, 4017, 484, 293, 538, 2931, 341, 727, 312, 257, 588, 1090, 1230, 411, 1732, 420, 746, 411, 300, 13, 51414, 51414, 1396, 294, 300, 1389, 11, 321, 603, 2136, 257, 588, 2295, 4470, 570, 321, 434, 49602, 264, 3006, 8482, 412, 5883, 2144, 538, 2931, 281, 264, 3006, 7645, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06874445401705229, "compression_ratio": 1.7557755775577557, "no_speech_prob": 7.296069270523731e-06}, {"id": 75, "seek": 39800, "start": 412.0, "end": 419.0, "text": " So it could be that we lock out and by chance this could be a very high number like five or something like that.", "tokens": [50364, 570, 309, 311, 257, 9452, 7316, 11, 321, 434, 6613, 264, 1900, 912, 4470, 294, 341, 1389, 11, 502, 13, 12625, 13, 50614, 50614, 407, 341, 307, 264, 4470, 321, 576, 2066, 337, 257, 1451, 18795, 1365, 13, 50764, 50764, 400, 286, 393, 536, 11, 295, 1164, 11, 300, 382, 321, 722, 281, 20459, 613, 3565, 1208, 11, 321, 434, 516, 281, 312, 4473, 264, 4470, 510, 13, 51064, 51064, 407, 309, 727, 312, 300, 321, 4017, 484, 293, 538, 2931, 341, 727, 312, 257, 588, 1090, 1230, 411, 1732, 420, 746, 411, 300, 13, 51414, 51414, 1396, 294, 300, 1389, 11, 321, 603, 2136, 257, 588, 2295, 4470, 570, 321, 434, 49602, 264, 3006, 8482, 412, 5883, 2144, 538, 2931, 281, 264, 3006, 7645, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06874445401705229, "compression_ratio": 1.7557755775577557, "no_speech_prob": 7.296069270523731e-06}, {"id": 76, "seek": 39800, "start": 419.0, "end": 426.0, "text": " Then in that case, we'll record a very low loss because we're assigning the correct probability at initialization by chance to the correct label.", "tokens": [50364, 570, 309, 311, 257, 9452, 7316, 11, 321, 434, 6613, 264, 1900, 912, 4470, 294, 341, 1389, 11, 502, 13, 12625, 13, 50614, 50614, 407, 341, 307, 264, 4470, 321, 576, 2066, 337, 257, 1451, 18795, 1365, 13, 50764, 50764, 400, 286, 393, 536, 11, 295, 1164, 11, 300, 382, 321, 722, 281, 20459, 613, 3565, 1208, 11, 321, 434, 516, 281, 312, 4473, 264, 4470, 510, 13, 51064, 51064, 407, 309, 727, 312, 300, 321, 4017, 484, 293, 538, 2931, 341, 727, 312, 257, 588, 1090, 1230, 411, 1732, 420, 746, 411, 300, 13, 51414, 51414, 1396, 294, 300, 1389, 11, 321, 603, 2136, 257, 588, 2295, 4470, 570, 321, 434, 49602, 264, 3006, 8482, 412, 5883, 2144, 538, 2931, 281, 264, 3006, 7645, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06874445401705229, "compression_ratio": 1.7557755775577557, "no_speech_prob": 7.296069270523731e-06}, {"id": 77, "seek": 42600, "start": 426.0, "end": 434.0, "text": " Much more likely it is that some other dimension will have a high logit.", "tokens": [50364, 12313, 544, 3700, 309, 307, 300, 512, 661, 10139, 486, 362, 257, 1090, 3565, 270, 13, 50764, 50764, 400, 550, 437, 486, 1051, 307, 321, 722, 281, 2136, 709, 2946, 4470, 13, 50914, 50914, 400, 437, 393, 808, 437, 393, 1051, 307, 1936, 264, 3565, 1208, 808, 484, 411, 746, 411, 341, 11, 291, 458, 11, 293, 436, 747, 322, 8084, 4190, 293, 321, 2136, 534, 1090, 4470, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09605541621168999, "compression_ratio": 1.6850828729281768, "no_speech_prob": 4.289218850317411e-06}, {"id": 78, "seek": 42600, "start": 434.0, "end": 437.0, "text": " And then what will happen is we start to record much higher loss.", "tokens": [50364, 12313, 544, 3700, 309, 307, 300, 512, 661, 10139, 486, 362, 257, 1090, 3565, 270, 13, 50764, 50764, 400, 550, 437, 486, 1051, 307, 321, 722, 281, 2136, 709, 2946, 4470, 13, 50914, 50914, 400, 437, 393, 808, 437, 393, 1051, 307, 1936, 264, 3565, 1208, 808, 484, 411, 746, 411, 341, 11, 291, 458, 11, 293, 436, 747, 322, 8084, 4190, 293, 321, 2136, 534, 1090, 4470, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09605541621168999, "compression_ratio": 1.6850828729281768, "no_speech_prob": 4.289218850317411e-06}, {"id": 79, "seek": 42600, "start": 437.0, "end": 448.0, "text": " And what can come what can happen is basically the logits come out like something like this, you know, and they take on extreme values and we record really high loss.", "tokens": [50364, 12313, 544, 3700, 309, 307, 300, 512, 661, 10139, 486, 362, 257, 1090, 3565, 270, 13, 50764, 50764, 400, 550, 437, 486, 1051, 307, 321, 722, 281, 2136, 709, 2946, 4470, 13, 50914, 50914, 400, 437, 393, 808, 437, 393, 1051, 307, 1936, 264, 3565, 1208, 808, 484, 411, 746, 411, 341, 11, 291, 458, 11, 293, 436, 747, 322, 8084, 4190, 293, 321, 2136, 534, 1090, 4470, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09605541621168999, "compression_ratio": 1.6850828729281768, "no_speech_prob": 4.289218850317411e-06}, {"id": 80, "seek": 44800, "start": 448.0, "end": 460.0, "text": " For example, if we have torched out random of four, these are uniform. So these are normally distributed numbers for them.", "tokens": [50364, 1171, 1365, 11, 498, 321, 362, 3930, 19318, 484, 4974, 295, 1451, 11, 613, 366, 9452, 13, 407, 613, 366, 5646, 12631, 3547, 337, 552, 13, 50964, 50964, 400, 510, 321, 393, 611, 4482, 264, 3565, 1208, 33783, 300, 808, 484, 295, 309, 293, 4470, 13, 51314, 51314, 400, 370, 570, 613, 3565, 1208, 366, 2651, 4018, 11, 337, 264, 881, 644, 11, 4470, 300, 1487, 484, 307, 307, 2264, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14926966349283854, "compression_ratio": 1.5578947368421052, "no_speech_prob": 6.961972758290358e-06}, {"id": 81, "seek": 44800, "start": 460.0, "end": 467.0, "text": " And here we can also print the logits probabilities that come out of it and loss.", "tokens": [50364, 1171, 1365, 11, 498, 321, 362, 3930, 19318, 484, 4974, 295, 1451, 11, 613, 366, 9452, 13, 407, 613, 366, 5646, 12631, 3547, 337, 552, 13, 50964, 50964, 400, 510, 321, 393, 611, 4482, 264, 3565, 1208, 33783, 300, 808, 484, 295, 309, 293, 4470, 13, 51314, 51314, 400, 370, 570, 613, 3565, 1208, 366, 2651, 4018, 11, 337, 264, 881, 644, 11, 4470, 300, 1487, 484, 307, 307, 2264, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14926966349283854, "compression_ratio": 1.5578947368421052, "no_speech_prob": 6.961972758290358e-06}, {"id": 82, "seek": 44800, "start": 467.0, "end": 474.0, "text": " And so because these logits are near zero, for the most part, loss that comes out is is OK.", "tokens": [50364, 1171, 1365, 11, 498, 321, 362, 3930, 19318, 484, 4974, 295, 1451, 11, 613, 366, 9452, 13, 407, 613, 366, 5646, 12631, 3547, 337, 552, 13, 50964, 50964, 400, 510, 321, 393, 611, 4482, 264, 3565, 1208, 33783, 300, 808, 484, 295, 309, 293, 4470, 13, 51314, 51314, 400, 370, 570, 613, 3565, 1208, 366, 2651, 4018, 11, 337, 264, 881, 644, 11, 4470, 300, 1487, 484, 307, 307, 2264, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.14926966349283854, "compression_ratio": 1.5578947368421052, "no_speech_prob": 6.961972758290358e-06}, {"id": 83, "seek": 47400, "start": 474.0, "end": 479.0, "text": " But suppose this is like times 10 now.", "tokens": [50364, 583, 7297, 341, 307, 411, 1413, 1266, 586, 13, 50614, 50614, 509, 536, 577, 570, 613, 366, 544, 8084, 4190, 11, 309, 311, 588, 17518, 300, 291, 434, 516, 281, 312, 17939, 264, 3006, 13058, 13, 50964, 50964, 400, 550, 291, 434, 41956, 2085, 293, 6613, 588, 1090, 4470, 13, 51164, 51164, 759, 428, 3565, 1208, 366, 1348, 493, 754, 544, 8084, 11, 291, 1062, 483, 4664, 912, 15352, 411, 13202, 754, 412, 5883, 2144, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0931367814540863, "compression_ratio": 1.5701357466063348, "no_speech_prob": 2.8128858957643388e-06}, {"id": 84, "seek": 47400, "start": 479.0, "end": 486.0, "text": " You see how because these are more extreme values, it's very unlikely that you're going to be guessing the correct bucket.", "tokens": [50364, 583, 7297, 341, 307, 411, 1413, 1266, 586, 13, 50614, 50614, 509, 536, 577, 570, 613, 366, 544, 8084, 4190, 11, 309, 311, 588, 17518, 300, 291, 434, 516, 281, 312, 17939, 264, 3006, 13058, 13, 50964, 50964, 400, 550, 291, 434, 41956, 2085, 293, 6613, 588, 1090, 4470, 13, 51164, 51164, 759, 428, 3565, 1208, 366, 1348, 493, 754, 544, 8084, 11, 291, 1062, 483, 4664, 912, 15352, 411, 13202, 754, 412, 5883, 2144, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0931367814540863, "compression_ratio": 1.5701357466063348, "no_speech_prob": 2.8128858957643388e-06}, {"id": 85, "seek": 47400, "start": 486.0, "end": 490.0, "text": " And then you're confidently wrong and recording very high loss.", "tokens": [50364, 583, 7297, 341, 307, 411, 1413, 1266, 586, 13, 50614, 50614, 509, 536, 577, 570, 613, 366, 544, 8084, 4190, 11, 309, 311, 588, 17518, 300, 291, 434, 516, 281, 312, 17939, 264, 3006, 13058, 13, 50964, 50964, 400, 550, 291, 434, 41956, 2085, 293, 6613, 588, 1090, 4470, 13, 51164, 51164, 759, 428, 3565, 1208, 366, 1348, 493, 754, 544, 8084, 11, 291, 1062, 483, 4664, 912, 15352, 411, 13202, 754, 412, 5883, 2144, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0931367814540863, "compression_ratio": 1.5701357466063348, "no_speech_prob": 2.8128858957643388e-06}, {"id": 86, "seek": 47400, "start": 490.0, "end": 500.0, "text": " If your logits are coming up even more extreme, you might get extremely same losses like infinity even at initialization.", "tokens": [50364, 583, 7297, 341, 307, 411, 1413, 1266, 586, 13, 50614, 50614, 509, 536, 577, 570, 613, 366, 544, 8084, 4190, 11, 309, 311, 588, 17518, 300, 291, 434, 516, 281, 312, 17939, 264, 3006, 13058, 13, 50964, 50964, 400, 550, 291, 434, 41956, 2085, 293, 6613, 588, 1090, 4470, 13, 51164, 51164, 759, 428, 3565, 1208, 366, 1348, 493, 754, 544, 8084, 11, 291, 1062, 483, 4664, 912, 15352, 411, 13202, 754, 412, 5883, 2144, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0931367814540863, "compression_ratio": 1.5701357466063348, "no_speech_prob": 2.8128858957643388e-06}, {"id": 87, "seek": 50000, "start": 500.0, "end": 507.0, "text": " So basically, this is not good. And we want the logits to be roughly zero when the network is initialized.", "tokens": [50364, 407, 1936, 11, 341, 307, 406, 665, 13, 400, 321, 528, 264, 3565, 1208, 281, 312, 9810, 4018, 562, 264, 3209, 307, 5883, 1602, 13, 50714, 50714, 682, 1186, 11, 264, 3565, 1208, 393, 500, 380, 362, 281, 312, 445, 4018, 13, 814, 445, 362, 281, 312, 2681, 13, 50914, 50914, 407, 11, 337, 1365, 11, 498, 439, 264, 3565, 1208, 366, 472, 11, 550, 570, 295, 264, 2710, 2144, 1854, 264, 2787, 41167, 11, 341, 486, 767, 808, 484, 2264, 13, 51264, 51264, 583, 538, 25440, 11, 321, 500, 380, 528, 309, 281, 312, 604, 23211, 3353, 420, 3671, 1230, 13, 51464, 51464, 492, 445, 528, 309, 281, 312, 439, 35193, 293, 2136, 264, 4470, 300, 321, 2066, 412, 5883, 2144, 13, 51664, 51664, 407, 718, 311, 586, 39481, 736, 536, 689, 721, 352, 2085, 294, 527, 1365, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06457710266113281, "compression_ratio": 1.7272727272727273, "no_speech_prob": 6.643239430559333e-06}, {"id": 88, "seek": 50000, "start": 507.0, "end": 511.0, "text": " In fact, the logits can don't have to be just zero. They just have to be equal.", "tokens": [50364, 407, 1936, 11, 341, 307, 406, 665, 13, 400, 321, 528, 264, 3565, 1208, 281, 312, 9810, 4018, 562, 264, 3209, 307, 5883, 1602, 13, 50714, 50714, 682, 1186, 11, 264, 3565, 1208, 393, 500, 380, 362, 281, 312, 445, 4018, 13, 814, 445, 362, 281, 312, 2681, 13, 50914, 50914, 407, 11, 337, 1365, 11, 498, 439, 264, 3565, 1208, 366, 472, 11, 550, 570, 295, 264, 2710, 2144, 1854, 264, 2787, 41167, 11, 341, 486, 767, 808, 484, 2264, 13, 51264, 51264, 583, 538, 25440, 11, 321, 500, 380, 528, 309, 281, 312, 604, 23211, 3353, 420, 3671, 1230, 13, 51464, 51464, 492, 445, 528, 309, 281, 312, 439, 35193, 293, 2136, 264, 4470, 300, 321, 2066, 412, 5883, 2144, 13, 51664, 51664, 407, 718, 311, 586, 39481, 736, 536, 689, 721, 352, 2085, 294, 527, 1365, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06457710266113281, "compression_ratio": 1.7272727272727273, "no_speech_prob": 6.643239430559333e-06}, {"id": 89, "seek": 50000, "start": 511.0, "end": 518.0, "text": " So, for example, if all the logits are one, then because of the normalization inside the softmax, this will actually come out OK.", "tokens": [50364, 407, 1936, 11, 341, 307, 406, 665, 13, 400, 321, 528, 264, 3565, 1208, 281, 312, 9810, 4018, 562, 264, 3209, 307, 5883, 1602, 13, 50714, 50714, 682, 1186, 11, 264, 3565, 1208, 393, 500, 380, 362, 281, 312, 445, 4018, 13, 814, 445, 362, 281, 312, 2681, 13, 50914, 50914, 407, 11, 337, 1365, 11, 498, 439, 264, 3565, 1208, 366, 472, 11, 550, 570, 295, 264, 2710, 2144, 1854, 264, 2787, 41167, 11, 341, 486, 767, 808, 484, 2264, 13, 51264, 51264, 583, 538, 25440, 11, 321, 500, 380, 528, 309, 281, 312, 604, 23211, 3353, 420, 3671, 1230, 13, 51464, 51464, 492, 445, 528, 309, 281, 312, 439, 35193, 293, 2136, 264, 4470, 300, 321, 2066, 412, 5883, 2144, 13, 51664, 51664, 407, 718, 311, 586, 39481, 736, 536, 689, 721, 352, 2085, 294, 527, 1365, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06457710266113281, "compression_ratio": 1.7272727272727273, "no_speech_prob": 6.643239430559333e-06}, {"id": 90, "seek": 50000, "start": 518.0, "end": 522.0, "text": " But by symmetry, we don't want it to be any arbitrary positive or negative number.", "tokens": [50364, 407, 1936, 11, 341, 307, 406, 665, 13, 400, 321, 528, 264, 3565, 1208, 281, 312, 9810, 4018, 562, 264, 3209, 307, 5883, 1602, 13, 50714, 50714, 682, 1186, 11, 264, 3565, 1208, 393, 500, 380, 362, 281, 312, 445, 4018, 13, 814, 445, 362, 281, 312, 2681, 13, 50914, 50914, 407, 11, 337, 1365, 11, 498, 439, 264, 3565, 1208, 366, 472, 11, 550, 570, 295, 264, 2710, 2144, 1854, 264, 2787, 41167, 11, 341, 486, 767, 808, 484, 2264, 13, 51264, 51264, 583, 538, 25440, 11, 321, 500, 380, 528, 309, 281, 312, 604, 23211, 3353, 420, 3671, 1230, 13, 51464, 51464, 492, 445, 528, 309, 281, 312, 439, 35193, 293, 2136, 264, 4470, 300, 321, 2066, 412, 5883, 2144, 13, 51664, 51664, 407, 718, 311, 586, 39481, 736, 536, 689, 721, 352, 2085, 294, 527, 1365, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06457710266113281, "compression_ratio": 1.7272727272727273, "no_speech_prob": 6.643239430559333e-06}, {"id": 91, "seek": 50000, "start": 522.0, "end": 526.0, "text": " We just want it to be all zeros and record the loss that we expect at initialization.", "tokens": [50364, 407, 1936, 11, 341, 307, 406, 665, 13, 400, 321, 528, 264, 3565, 1208, 281, 312, 9810, 4018, 562, 264, 3209, 307, 5883, 1602, 13, 50714, 50714, 682, 1186, 11, 264, 3565, 1208, 393, 500, 380, 362, 281, 312, 445, 4018, 13, 814, 445, 362, 281, 312, 2681, 13, 50914, 50914, 407, 11, 337, 1365, 11, 498, 439, 264, 3565, 1208, 366, 472, 11, 550, 570, 295, 264, 2710, 2144, 1854, 264, 2787, 41167, 11, 341, 486, 767, 808, 484, 2264, 13, 51264, 51264, 583, 538, 25440, 11, 321, 500, 380, 528, 309, 281, 312, 604, 23211, 3353, 420, 3671, 1230, 13, 51464, 51464, 492, 445, 528, 309, 281, 312, 439, 35193, 293, 2136, 264, 4470, 300, 321, 2066, 412, 5883, 2144, 13, 51664, 51664, 407, 718, 311, 586, 39481, 736, 536, 689, 721, 352, 2085, 294, 527, 1365, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06457710266113281, "compression_ratio": 1.7272727272727273, "no_speech_prob": 6.643239430559333e-06}, {"id": 92, "seek": 50000, "start": 526.0, "end": 529.0, "text": " So let's now concretely see where things go wrong in our example.", "tokens": [50364, 407, 1936, 11, 341, 307, 406, 665, 13, 400, 321, 528, 264, 3565, 1208, 281, 312, 9810, 4018, 562, 264, 3209, 307, 5883, 1602, 13, 50714, 50714, 682, 1186, 11, 264, 3565, 1208, 393, 500, 380, 362, 281, 312, 445, 4018, 13, 814, 445, 362, 281, 312, 2681, 13, 50914, 50914, 407, 11, 337, 1365, 11, 498, 439, 264, 3565, 1208, 366, 472, 11, 550, 570, 295, 264, 2710, 2144, 1854, 264, 2787, 41167, 11, 341, 486, 767, 808, 484, 2264, 13, 51264, 51264, 583, 538, 25440, 11, 321, 500, 380, 528, 309, 281, 312, 604, 23211, 3353, 420, 3671, 1230, 13, 51464, 51464, 492, 445, 528, 309, 281, 312, 439, 35193, 293, 2136, 264, 4470, 300, 321, 2066, 412, 5883, 2144, 13, 51664, 51664, 407, 718, 311, 586, 39481, 736, 536, 689, 721, 352, 2085, 294, 527, 1365, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06457710266113281, "compression_ratio": 1.7272727272727273, "no_speech_prob": 6.643239430559333e-06}, {"id": 93, "seek": 52900, "start": 529.0, "end": 533.0, "text": " Here we have the initialization. Let me reinitialize the neural net.", "tokens": [50364, 1692, 321, 362, 264, 5883, 2144, 13, 961, 385, 6561, 270, 831, 1125, 264, 18161, 2533, 13, 50564, 50564, 400, 510, 718, 385, 1821, 934, 264, 588, 700, 24784, 13, 50714, 50714, 407, 321, 787, 536, 264, 5883, 4470, 11, 597, 307, 7634, 13, 407, 300, 311, 636, 886, 1090, 13, 50964, 50964, 400, 46506, 11, 586, 321, 393, 2066, 264, 9102, 3288, 13, 51114, 51114, 400, 321, 536, 300, 264, 3565, 1208, 510, 11, 498, 321, 445, 4482, 512, 295, 613, 11, 321, 445, 4482, 264, 700, 5386, 13, 51464, 51464, 492, 536, 300, 264, 3565, 1208, 747, 322, 1596, 8084, 4190, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.090639626538312, "compression_ratio": 1.7304347826086957, "no_speech_prob": 8.939467988966499e-06}, {"id": 94, "seek": 52900, "start": 533.0, "end": 536.0, "text": " And here let me break after the very first iteration.", "tokens": [50364, 1692, 321, 362, 264, 5883, 2144, 13, 961, 385, 6561, 270, 831, 1125, 264, 18161, 2533, 13, 50564, 50564, 400, 510, 718, 385, 1821, 934, 264, 588, 700, 24784, 13, 50714, 50714, 407, 321, 787, 536, 264, 5883, 4470, 11, 597, 307, 7634, 13, 407, 300, 311, 636, 886, 1090, 13, 50964, 50964, 400, 46506, 11, 586, 321, 393, 2066, 264, 9102, 3288, 13, 51114, 51114, 400, 321, 536, 300, 264, 3565, 1208, 510, 11, 498, 321, 445, 4482, 512, 295, 613, 11, 321, 445, 4482, 264, 700, 5386, 13, 51464, 51464, 492, 536, 300, 264, 3565, 1208, 747, 322, 1596, 8084, 4190, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.090639626538312, "compression_ratio": 1.7304347826086957, "no_speech_prob": 8.939467988966499e-06}, {"id": 95, "seek": 52900, "start": 536.0, "end": 541.0, "text": " So we only see the initial loss, which is 27. So that's way too high.", "tokens": [50364, 1692, 321, 362, 264, 5883, 2144, 13, 961, 385, 6561, 270, 831, 1125, 264, 18161, 2533, 13, 50564, 50564, 400, 510, 718, 385, 1821, 934, 264, 588, 700, 24784, 13, 50714, 50714, 407, 321, 787, 536, 264, 5883, 4470, 11, 597, 307, 7634, 13, 407, 300, 311, 636, 886, 1090, 13, 50964, 50964, 400, 46506, 11, 586, 321, 393, 2066, 264, 9102, 3288, 13, 51114, 51114, 400, 321, 536, 300, 264, 3565, 1208, 510, 11, 498, 321, 445, 4482, 512, 295, 613, 11, 321, 445, 4482, 264, 700, 5386, 13, 51464, 51464, 492, 536, 300, 264, 3565, 1208, 747, 322, 1596, 8084, 4190, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.090639626538312, "compression_ratio": 1.7304347826086957, "no_speech_prob": 8.939467988966499e-06}, {"id": 96, "seek": 52900, "start": 541.0, "end": 544.0, "text": " And intuitively, now we can expect the variables involved.", "tokens": [50364, 1692, 321, 362, 264, 5883, 2144, 13, 961, 385, 6561, 270, 831, 1125, 264, 18161, 2533, 13, 50564, 50564, 400, 510, 718, 385, 1821, 934, 264, 588, 700, 24784, 13, 50714, 50714, 407, 321, 787, 536, 264, 5883, 4470, 11, 597, 307, 7634, 13, 407, 300, 311, 636, 886, 1090, 13, 50964, 50964, 400, 46506, 11, 586, 321, 393, 2066, 264, 9102, 3288, 13, 51114, 51114, 400, 321, 536, 300, 264, 3565, 1208, 510, 11, 498, 321, 445, 4482, 512, 295, 613, 11, 321, 445, 4482, 264, 700, 5386, 13, 51464, 51464, 492, 536, 300, 264, 3565, 1208, 747, 322, 1596, 8084, 4190, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.090639626538312, "compression_ratio": 1.7304347826086957, "no_speech_prob": 8.939467988966499e-06}, {"id": 97, "seek": 52900, "start": 544.0, "end": 551.0, "text": " And we see that the logits here, if we just print some of these, we just print the first row.", "tokens": [50364, 1692, 321, 362, 264, 5883, 2144, 13, 961, 385, 6561, 270, 831, 1125, 264, 18161, 2533, 13, 50564, 50564, 400, 510, 718, 385, 1821, 934, 264, 588, 700, 24784, 13, 50714, 50714, 407, 321, 787, 536, 264, 5883, 4470, 11, 597, 307, 7634, 13, 407, 300, 311, 636, 886, 1090, 13, 50964, 50964, 400, 46506, 11, 586, 321, 393, 2066, 264, 9102, 3288, 13, 51114, 51114, 400, 321, 536, 300, 264, 3565, 1208, 510, 11, 498, 321, 445, 4482, 512, 295, 613, 11, 321, 445, 4482, 264, 700, 5386, 13, 51464, 51464, 492, 536, 300, 264, 3565, 1208, 747, 322, 1596, 8084, 4190, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.090639626538312, "compression_ratio": 1.7304347826086957, "no_speech_prob": 8.939467988966499e-06}, {"id": 98, "seek": 52900, "start": 551.0, "end": 553.0, "text": " We see that the logits take on quite extreme values.", "tokens": [50364, 1692, 321, 362, 264, 5883, 2144, 13, 961, 385, 6561, 270, 831, 1125, 264, 18161, 2533, 13, 50564, 50564, 400, 510, 718, 385, 1821, 934, 264, 588, 700, 24784, 13, 50714, 50714, 407, 321, 787, 536, 264, 5883, 4470, 11, 597, 307, 7634, 13, 407, 300, 311, 636, 886, 1090, 13, 50964, 50964, 400, 46506, 11, 586, 321, 393, 2066, 264, 9102, 3288, 13, 51114, 51114, 400, 321, 536, 300, 264, 3565, 1208, 510, 11, 498, 321, 445, 4482, 512, 295, 613, 11, 321, 445, 4482, 264, 700, 5386, 13, 51464, 51464, 492, 536, 300, 264, 3565, 1208, 747, 322, 1596, 8084, 4190, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.090639626538312, "compression_ratio": 1.7304347826086957, "no_speech_prob": 8.939467988966499e-06}, {"id": 99, "seek": 55300, "start": 553.0, "end": 562.0, "text": " And that's what's creating the fake confidence and incorrect answers and makes the loss get very, very high.", "tokens": [50364, 400, 300, 311, 437, 311, 4084, 264, 7592, 6687, 293, 18424, 6338, 293, 1669, 264, 4470, 483, 588, 11, 588, 1090, 13, 50814, 50814, 407, 613, 3565, 1208, 820, 312, 709, 11, 709, 4966, 281, 4018, 13, 50964, 50964, 407, 586, 718, 311, 519, 807, 577, 321, 393, 4584, 3565, 1208, 1348, 484, 295, 341, 18161, 2533, 281, 312, 544, 4966, 281, 4018, 13, 51314, 51314, 509, 536, 510, 300, 3565, 1208, 366, 15598, 382, 264, 7633, 4368, 17207, 538, 343, 17, 1804, 363, 17, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04041228559282091, "compression_ratio": 1.6053811659192825, "no_speech_prob": 5.955007964075776e-06}, {"id": 100, "seek": 55300, "start": 562.0, "end": 565.0, "text": " So these logits should be much, much closer to zero.", "tokens": [50364, 400, 300, 311, 437, 311, 4084, 264, 7592, 6687, 293, 18424, 6338, 293, 1669, 264, 4470, 483, 588, 11, 588, 1090, 13, 50814, 50814, 407, 613, 3565, 1208, 820, 312, 709, 11, 709, 4966, 281, 4018, 13, 50964, 50964, 407, 586, 718, 311, 519, 807, 577, 321, 393, 4584, 3565, 1208, 1348, 484, 295, 341, 18161, 2533, 281, 312, 544, 4966, 281, 4018, 13, 51314, 51314, 509, 536, 510, 300, 3565, 1208, 366, 15598, 382, 264, 7633, 4368, 17207, 538, 343, 17, 1804, 363, 17, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04041228559282091, "compression_ratio": 1.6053811659192825, "no_speech_prob": 5.955007964075776e-06}, {"id": 101, "seek": 55300, "start": 565.0, "end": 572.0, "text": " So now let's think through how we can achieve logits coming out of this neural net to be more closer to zero.", "tokens": [50364, 400, 300, 311, 437, 311, 4084, 264, 7592, 6687, 293, 18424, 6338, 293, 1669, 264, 4470, 483, 588, 11, 588, 1090, 13, 50814, 50814, 407, 613, 3565, 1208, 820, 312, 709, 11, 709, 4966, 281, 4018, 13, 50964, 50964, 407, 586, 718, 311, 519, 807, 577, 321, 393, 4584, 3565, 1208, 1348, 484, 295, 341, 18161, 2533, 281, 312, 544, 4966, 281, 4018, 13, 51314, 51314, 509, 536, 510, 300, 3565, 1208, 366, 15598, 382, 264, 7633, 4368, 17207, 538, 343, 17, 1804, 363, 17, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04041228559282091, "compression_ratio": 1.6053811659192825, "no_speech_prob": 5.955007964075776e-06}, {"id": 102, "seek": 55300, "start": 572.0, "end": 577.0, "text": " You see here that logits are calculated as the hidden states multiplied by W2 plus B2.", "tokens": [50364, 400, 300, 311, 437, 311, 4084, 264, 7592, 6687, 293, 18424, 6338, 293, 1669, 264, 4470, 483, 588, 11, 588, 1090, 13, 50814, 50814, 407, 613, 3565, 1208, 820, 312, 709, 11, 709, 4966, 281, 4018, 13, 50964, 50964, 407, 586, 718, 311, 519, 807, 577, 321, 393, 4584, 3565, 1208, 1348, 484, 295, 341, 18161, 2533, 281, 312, 544, 4966, 281, 4018, 13, 51314, 51314, 509, 536, 510, 300, 3565, 1208, 366, 15598, 382, 264, 7633, 4368, 17207, 538, 343, 17, 1804, 363, 17, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04041228559282091, "compression_ratio": 1.6053811659192825, "no_speech_prob": 5.955007964075776e-06}, {"id": 103, "seek": 57700, "start": 577.0, "end": 584.0, "text": " So first of all, currently we're initializing B2 as random values of the right size.", "tokens": [50364, 407, 700, 295, 439, 11, 4362, 321, 434, 5883, 3319, 363, 17, 382, 4974, 4190, 295, 264, 558, 2744, 13, 50714, 50714, 583, 570, 321, 528, 9810, 4018, 11, 321, 500, 380, 767, 528, 281, 312, 5127, 257, 12577, 295, 4974, 3547, 13, 50964, 50964, 407, 294, 1186, 11, 286, 478, 516, 281, 909, 257, 1413, 4018, 510, 281, 652, 988, 300, 363, 17, 307, 445, 1936, 4018, 412, 5883, 2144, 13, 51364, 51364, 400, 1150, 11, 341, 307, 389, 17207, 538, 343, 17, 13, 51514, 51514, 407, 498, 321, 528, 3565, 1208, 281, 312, 588, 11, 588, 1359, 11, 550, 321, 576, 312, 30955, 343, 17, 293, 1455, 300, 4356, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.044930042891666806, "compression_ratio": 1.6179775280898876, "no_speech_prob": 1.9946771772083594e-06}, {"id": 104, "seek": 57700, "start": 584.0, "end": 589.0, "text": " But because we want roughly zero, we don't actually want to be adding a bias of random numbers.", "tokens": [50364, 407, 700, 295, 439, 11, 4362, 321, 434, 5883, 3319, 363, 17, 382, 4974, 4190, 295, 264, 558, 2744, 13, 50714, 50714, 583, 570, 321, 528, 9810, 4018, 11, 321, 500, 380, 767, 528, 281, 312, 5127, 257, 12577, 295, 4974, 3547, 13, 50964, 50964, 407, 294, 1186, 11, 286, 478, 516, 281, 909, 257, 1413, 4018, 510, 281, 652, 988, 300, 363, 17, 307, 445, 1936, 4018, 412, 5883, 2144, 13, 51364, 51364, 400, 1150, 11, 341, 307, 389, 17207, 538, 343, 17, 13, 51514, 51514, 407, 498, 321, 528, 3565, 1208, 281, 312, 588, 11, 588, 1359, 11, 550, 321, 576, 312, 30955, 343, 17, 293, 1455, 300, 4356, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.044930042891666806, "compression_ratio": 1.6179775280898876, "no_speech_prob": 1.9946771772083594e-06}, {"id": 105, "seek": 57700, "start": 589.0, "end": 597.0, "text": " So in fact, I'm going to add a times zero here to make sure that B2 is just basically zero at initialization.", "tokens": [50364, 407, 700, 295, 439, 11, 4362, 321, 434, 5883, 3319, 363, 17, 382, 4974, 4190, 295, 264, 558, 2744, 13, 50714, 50714, 583, 570, 321, 528, 9810, 4018, 11, 321, 500, 380, 767, 528, 281, 312, 5127, 257, 12577, 295, 4974, 3547, 13, 50964, 50964, 407, 294, 1186, 11, 286, 478, 516, 281, 909, 257, 1413, 4018, 510, 281, 652, 988, 300, 363, 17, 307, 445, 1936, 4018, 412, 5883, 2144, 13, 51364, 51364, 400, 1150, 11, 341, 307, 389, 17207, 538, 343, 17, 13, 51514, 51514, 407, 498, 321, 528, 3565, 1208, 281, 312, 588, 11, 588, 1359, 11, 550, 321, 576, 312, 30955, 343, 17, 293, 1455, 300, 4356, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.044930042891666806, "compression_ratio": 1.6179775280898876, "no_speech_prob": 1.9946771772083594e-06}, {"id": 106, "seek": 57700, "start": 597.0, "end": 600.0, "text": " And second, this is H multiplied by W2.", "tokens": [50364, 407, 700, 295, 439, 11, 4362, 321, 434, 5883, 3319, 363, 17, 382, 4974, 4190, 295, 264, 558, 2744, 13, 50714, 50714, 583, 570, 321, 528, 9810, 4018, 11, 321, 500, 380, 767, 528, 281, 312, 5127, 257, 12577, 295, 4974, 3547, 13, 50964, 50964, 407, 294, 1186, 11, 286, 478, 516, 281, 909, 257, 1413, 4018, 510, 281, 652, 988, 300, 363, 17, 307, 445, 1936, 4018, 412, 5883, 2144, 13, 51364, 51364, 400, 1150, 11, 341, 307, 389, 17207, 538, 343, 17, 13, 51514, 51514, 407, 498, 321, 528, 3565, 1208, 281, 312, 588, 11, 588, 1359, 11, 550, 321, 576, 312, 30955, 343, 17, 293, 1455, 300, 4356, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.044930042891666806, "compression_ratio": 1.6179775280898876, "no_speech_prob": 1.9946771772083594e-06}, {"id": 107, "seek": 57700, "start": 600.0, "end": 606.0, "text": " So if we want logits to be very, very small, then we would be multiplying W2 and making that smaller.", "tokens": [50364, 407, 700, 295, 439, 11, 4362, 321, 434, 5883, 3319, 363, 17, 382, 4974, 4190, 295, 264, 558, 2744, 13, 50714, 50714, 583, 570, 321, 528, 9810, 4018, 11, 321, 500, 380, 767, 528, 281, 312, 5127, 257, 12577, 295, 4974, 3547, 13, 50964, 50964, 407, 294, 1186, 11, 286, 478, 516, 281, 909, 257, 1413, 4018, 510, 281, 652, 988, 300, 363, 17, 307, 445, 1936, 4018, 412, 5883, 2144, 13, 51364, 51364, 400, 1150, 11, 341, 307, 389, 17207, 538, 343, 17, 13, 51514, 51514, 407, 498, 321, 528, 3565, 1208, 281, 312, 588, 11, 588, 1359, 11, 550, 321, 576, 312, 30955, 343, 17, 293, 1455, 300, 4356, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.044930042891666806, "compression_ratio": 1.6179775280898876, "no_speech_prob": 1.9946771772083594e-06}, {"id": 108, "seek": 60600, "start": 606.0, "end": 614.0, "text": " So, for example, if we scale down W2 by 0.1, all the elements, then if I do again just the very first iteration,", "tokens": [50364, 407, 11, 337, 1365, 11, 498, 321, 4373, 760, 343, 17, 538, 1958, 13, 16, 11, 439, 264, 4959, 11, 550, 498, 286, 360, 797, 445, 264, 588, 700, 24784, 11, 50764, 50764, 291, 536, 300, 321, 366, 1242, 709, 4966, 281, 437, 321, 2066, 13, 50914, 50914, 407, 9810, 437, 321, 528, 307, 466, 805, 13, 11871, 13, 51064, 51064, 639, 307, 1017, 13, 17, 13, 286, 393, 652, 341, 1310, 754, 4356, 13, 51264, 51264, 805, 13, 11440, 13, 2264, 11, 370, 321, 434, 1242, 4966, 293, 4966, 13, 51464, 51464, 823, 291, 434, 1391, 6359, 11, 393, 321, 445, 992, 341, 281, 4018, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10577442409755948, "compression_ratio": 1.493877551020408, "no_speech_prob": 9.368305654788855e-06}, {"id": 109, "seek": 60600, "start": 614.0, "end": 617.0, "text": " you see that we are getting much closer to what we expect.", "tokens": [50364, 407, 11, 337, 1365, 11, 498, 321, 4373, 760, 343, 17, 538, 1958, 13, 16, 11, 439, 264, 4959, 11, 550, 498, 286, 360, 797, 445, 264, 588, 700, 24784, 11, 50764, 50764, 291, 536, 300, 321, 366, 1242, 709, 4966, 281, 437, 321, 2066, 13, 50914, 50914, 407, 9810, 437, 321, 528, 307, 466, 805, 13, 11871, 13, 51064, 51064, 639, 307, 1017, 13, 17, 13, 286, 393, 652, 341, 1310, 754, 4356, 13, 51264, 51264, 805, 13, 11440, 13, 2264, 11, 370, 321, 434, 1242, 4966, 293, 4966, 13, 51464, 51464, 823, 291, 434, 1391, 6359, 11, 393, 321, 445, 992, 341, 281, 4018, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10577442409755948, "compression_ratio": 1.493877551020408, "no_speech_prob": 9.368305654788855e-06}, {"id": 110, "seek": 60600, "start": 617.0, "end": 620.0, "text": " So roughly what we want is about 3.29.", "tokens": [50364, 407, 11, 337, 1365, 11, 498, 321, 4373, 760, 343, 17, 538, 1958, 13, 16, 11, 439, 264, 4959, 11, 550, 498, 286, 360, 797, 445, 264, 588, 700, 24784, 11, 50764, 50764, 291, 536, 300, 321, 366, 1242, 709, 4966, 281, 437, 321, 2066, 13, 50914, 50914, 407, 9810, 437, 321, 528, 307, 466, 805, 13, 11871, 13, 51064, 51064, 639, 307, 1017, 13, 17, 13, 286, 393, 652, 341, 1310, 754, 4356, 13, 51264, 51264, 805, 13, 11440, 13, 2264, 11, 370, 321, 434, 1242, 4966, 293, 4966, 13, 51464, 51464, 823, 291, 434, 1391, 6359, 11, 393, 321, 445, 992, 341, 281, 4018, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10577442409755948, "compression_ratio": 1.493877551020408, "no_speech_prob": 9.368305654788855e-06}, {"id": 111, "seek": 60600, "start": 620.0, "end": 624.0, "text": " This is 4.2. I can make this maybe even smaller.", "tokens": [50364, 407, 11, 337, 1365, 11, 498, 321, 4373, 760, 343, 17, 538, 1958, 13, 16, 11, 439, 264, 4959, 11, 550, 498, 286, 360, 797, 445, 264, 588, 700, 24784, 11, 50764, 50764, 291, 536, 300, 321, 366, 1242, 709, 4966, 281, 437, 321, 2066, 13, 50914, 50914, 407, 9810, 437, 321, 528, 307, 466, 805, 13, 11871, 13, 51064, 51064, 639, 307, 1017, 13, 17, 13, 286, 393, 652, 341, 1310, 754, 4356, 13, 51264, 51264, 805, 13, 11440, 13, 2264, 11, 370, 321, 434, 1242, 4966, 293, 4966, 13, 51464, 51464, 823, 291, 434, 1391, 6359, 11, 393, 321, 445, 992, 341, 281, 4018, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10577442409755948, "compression_ratio": 1.493877551020408, "no_speech_prob": 9.368305654788855e-06}, {"id": 112, "seek": 60600, "start": 624.0, "end": 628.0, "text": " 3.32. OK, so we're getting closer and closer.", "tokens": [50364, 407, 11, 337, 1365, 11, 498, 321, 4373, 760, 343, 17, 538, 1958, 13, 16, 11, 439, 264, 4959, 11, 550, 498, 286, 360, 797, 445, 264, 588, 700, 24784, 11, 50764, 50764, 291, 536, 300, 321, 366, 1242, 709, 4966, 281, 437, 321, 2066, 13, 50914, 50914, 407, 9810, 437, 321, 528, 307, 466, 805, 13, 11871, 13, 51064, 51064, 639, 307, 1017, 13, 17, 13, 286, 393, 652, 341, 1310, 754, 4356, 13, 51264, 51264, 805, 13, 11440, 13, 2264, 11, 370, 321, 434, 1242, 4966, 293, 4966, 13, 51464, 51464, 823, 291, 434, 1391, 6359, 11, 393, 321, 445, 992, 341, 281, 4018, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10577442409755948, "compression_ratio": 1.493877551020408, "no_speech_prob": 9.368305654788855e-06}, {"id": 113, "seek": 60600, "start": 628.0, "end": 632.0, "text": " Now you're probably wondering, can we just set this to zero?", "tokens": [50364, 407, 11, 337, 1365, 11, 498, 321, 4373, 760, 343, 17, 538, 1958, 13, 16, 11, 439, 264, 4959, 11, 550, 498, 286, 360, 797, 445, 264, 588, 700, 24784, 11, 50764, 50764, 291, 536, 300, 321, 366, 1242, 709, 4966, 281, 437, 321, 2066, 13, 50914, 50914, 407, 9810, 437, 321, 528, 307, 466, 805, 13, 11871, 13, 51064, 51064, 639, 307, 1017, 13, 17, 13, 286, 393, 652, 341, 1310, 754, 4356, 13, 51264, 51264, 805, 13, 11440, 13, 2264, 11, 370, 321, 434, 1242, 4966, 293, 4966, 13, 51464, 51464, 823, 291, 434, 1391, 6359, 11, 393, 321, 445, 992, 341, 281, 4018, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10577442409755948, "compression_ratio": 1.493877551020408, "no_speech_prob": 9.368305654788855e-06}, {"id": 114, "seek": 63200, "start": 632.0, "end": 637.0, "text": " Then we get, of course, exactly what we're looking for at initialization.", "tokens": [50364, 1396, 321, 483, 11, 295, 1164, 11, 2293, 437, 321, 434, 1237, 337, 412, 5883, 2144, 13, 50614, 50614, 400, 264, 1778, 286, 500, 380, 2673, 360, 341, 307, 570, 286, 478, 588, 6296, 13, 50864, 50864, 400, 286, 603, 855, 291, 294, 257, 1150, 983, 291, 500, 380, 528, 281, 312, 3287, 343, 82, 420, 17443, 295, 257, 18161, 2533, 2293, 281, 4018, 13, 51164, 51164, 509, 2673, 528, 309, 281, 312, 1359, 3547, 2602, 295, 2293, 4018, 13, 51364, 51364, 1171, 341, 5598, 4583, 294, 341, 2685, 1389, 11, 286, 519, 309, 576, 312, 2489, 13, 51614, 51614, 583, 286, 603, 855, 291, 294, 257, 1150, 689, 721, 352, 2085, 588, 2661, 498, 291, 360, 300, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.050992264011041906, "compression_ratio": 1.6895306859205776, "no_speech_prob": 1.2029360732412897e-05}, {"id": 115, "seek": 63200, "start": 637.0, "end": 642.0, "text": " And the reason I don't usually do this is because I'm very nervous.", "tokens": [50364, 1396, 321, 483, 11, 295, 1164, 11, 2293, 437, 321, 434, 1237, 337, 412, 5883, 2144, 13, 50614, 50614, 400, 264, 1778, 286, 500, 380, 2673, 360, 341, 307, 570, 286, 478, 588, 6296, 13, 50864, 50864, 400, 286, 603, 855, 291, 294, 257, 1150, 983, 291, 500, 380, 528, 281, 312, 3287, 343, 82, 420, 17443, 295, 257, 18161, 2533, 2293, 281, 4018, 13, 51164, 51164, 509, 2673, 528, 309, 281, 312, 1359, 3547, 2602, 295, 2293, 4018, 13, 51364, 51364, 1171, 341, 5598, 4583, 294, 341, 2685, 1389, 11, 286, 519, 309, 576, 312, 2489, 13, 51614, 51614, 583, 286, 603, 855, 291, 294, 257, 1150, 689, 721, 352, 2085, 588, 2661, 498, 291, 360, 300, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.050992264011041906, "compression_ratio": 1.6895306859205776, "no_speech_prob": 1.2029360732412897e-05}, {"id": 116, "seek": 63200, "start": 642.0, "end": 648.0, "text": " And I'll show you in a second why you don't want to be setting Ws or weights of a neural net exactly to zero.", "tokens": [50364, 1396, 321, 483, 11, 295, 1164, 11, 2293, 437, 321, 434, 1237, 337, 412, 5883, 2144, 13, 50614, 50614, 400, 264, 1778, 286, 500, 380, 2673, 360, 341, 307, 570, 286, 478, 588, 6296, 13, 50864, 50864, 400, 286, 603, 855, 291, 294, 257, 1150, 983, 291, 500, 380, 528, 281, 312, 3287, 343, 82, 420, 17443, 295, 257, 18161, 2533, 2293, 281, 4018, 13, 51164, 51164, 509, 2673, 528, 309, 281, 312, 1359, 3547, 2602, 295, 2293, 4018, 13, 51364, 51364, 1171, 341, 5598, 4583, 294, 341, 2685, 1389, 11, 286, 519, 309, 576, 312, 2489, 13, 51614, 51614, 583, 286, 603, 855, 291, 294, 257, 1150, 689, 721, 352, 2085, 588, 2661, 498, 291, 360, 300, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.050992264011041906, "compression_ratio": 1.6895306859205776, "no_speech_prob": 1.2029360732412897e-05}, {"id": 117, "seek": 63200, "start": 648.0, "end": 652.0, "text": " You usually want it to be small numbers instead of exactly zero.", "tokens": [50364, 1396, 321, 483, 11, 295, 1164, 11, 2293, 437, 321, 434, 1237, 337, 412, 5883, 2144, 13, 50614, 50614, 400, 264, 1778, 286, 500, 380, 2673, 360, 341, 307, 570, 286, 478, 588, 6296, 13, 50864, 50864, 400, 286, 603, 855, 291, 294, 257, 1150, 983, 291, 500, 380, 528, 281, 312, 3287, 343, 82, 420, 17443, 295, 257, 18161, 2533, 2293, 281, 4018, 13, 51164, 51164, 509, 2673, 528, 309, 281, 312, 1359, 3547, 2602, 295, 2293, 4018, 13, 51364, 51364, 1171, 341, 5598, 4583, 294, 341, 2685, 1389, 11, 286, 519, 309, 576, 312, 2489, 13, 51614, 51614, 583, 286, 603, 855, 291, 294, 257, 1150, 689, 721, 352, 2085, 588, 2661, 498, 291, 360, 300, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.050992264011041906, "compression_ratio": 1.6895306859205776, "no_speech_prob": 1.2029360732412897e-05}, {"id": 118, "seek": 63200, "start": 652.0, "end": 657.0, "text": " For this output layer in this specific case, I think it would be fine.", "tokens": [50364, 1396, 321, 483, 11, 295, 1164, 11, 2293, 437, 321, 434, 1237, 337, 412, 5883, 2144, 13, 50614, 50614, 400, 264, 1778, 286, 500, 380, 2673, 360, 341, 307, 570, 286, 478, 588, 6296, 13, 50864, 50864, 400, 286, 603, 855, 291, 294, 257, 1150, 983, 291, 500, 380, 528, 281, 312, 3287, 343, 82, 420, 17443, 295, 257, 18161, 2533, 2293, 281, 4018, 13, 51164, 51164, 509, 2673, 528, 309, 281, 312, 1359, 3547, 2602, 295, 2293, 4018, 13, 51364, 51364, 1171, 341, 5598, 4583, 294, 341, 2685, 1389, 11, 286, 519, 309, 576, 312, 2489, 13, 51614, 51614, 583, 286, 603, 855, 291, 294, 257, 1150, 689, 721, 352, 2085, 588, 2661, 498, 291, 360, 300, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.050992264011041906, "compression_ratio": 1.6895306859205776, "no_speech_prob": 1.2029360732412897e-05}, {"id": 119, "seek": 63200, "start": 657.0, "end": 660.0, "text": " But I'll show you in a second where things go wrong very quickly if you do that.", "tokens": [50364, 1396, 321, 483, 11, 295, 1164, 11, 2293, 437, 321, 434, 1237, 337, 412, 5883, 2144, 13, 50614, 50614, 400, 264, 1778, 286, 500, 380, 2673, 360, 341, 307, 570, 286, 478, 588, 6296, 13, 50864, 50864, 400, 286, 603, 855, 291, 294, 257, 1150, 983, 291, 500, 380, 528, 281, 312, 3287, 343, 82, 420, 17443, 295, 257, 18161, 2533, 2293, 281, 4018, 13, 51164, 51164, 509, 2673, 528, 309, 281, 312, 1359, 3547, 2602, 295, 2293, 4018, 13, 51364, 51364, 1171, 341, 5598, 4583, 294, 341, 2685, 1389, 11, 286, 519, 309, 576, 312, 2489, 13, 51614, 51614, 583, 286, 603, 855, 291, 294, 257, 1150, 689, 721, 352, 2085, 588, 2661, 498, 291, 360, 300, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.050992264011041906, "compression_ratio": 1.6895306859205776, "no_speech_prob": 1.2029360732412897e-05}, {"id": 120, "seek": 66000, "start": 660.0, "end": 666.0, "text": " So let's just go with 0.01. In that case, our loss is close enough, but has some entropy.", "tokens": [50364, 407, 718, 311, 445, 352, 365, 1958, 13, 10607, 13, 682, 300, 1389, 11, 527, 4470, 307, 1998, 1547, 11, 457, 575, 512, 30867, 13, 50664, 50664, 467, 311, 406, 2293, 4018, 13, 467, 311, 658, 512, 707, 30867, 13, 50864, 50864, 400, 300, 311, 1143, 337, 25440, 7697, 11, 382, 321, 603, 536, 294, 257, 1150, 13, 51014, 51014, 440, 3565, 1208, 366, 586, 1348, 484, 709, 4966, 281, 4018, 293, 1203, 307, 731, 293, 665, 13, 51264, 51264, 407, 498, 286, 445, 23525, 613, 293, 286, 586, 747, 1314, 264, 1821, 5629, 11, 51614, 51614, 321, 393, 1190, 264, 19618, 365, 341, 777, 5883, 2144, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.076076226575034, "compression_ratio": 1.6046511627906976, "no_speech_prob": 1.1125312084914185e-05}, {"id": 121, "seek": 66000, "start": 666.0, "end": 670.0, "text": " It's not exactly zero. It's got some little entropy.", "tokens": [50364, 407, 718, 311, 445, 352, 365, 1958, 13, 10607, 13, 682, 300, 1389, 11, 527, 4470, 307, 1998, 1547, 11, 457, 575, 512, 30867, 13, 50664, 50664, 467, 311, 406, 2293, 4018, 13, 467, 311, 658, 512, 707, 30867, 13, 50864, 50864, 400, 300, 311, 1143, 337, 25440, 7697, 11, 382, 321, 603, 536, 294, 257, 1150, 13, 51014, 51014, 440, 3565, 1208, 366, 586, 1348, 484, 709, 4966, 281, 4018, 293, 1203, 307, 731, 293, 665, 13, 51264, 51264, 407, 498, 286, 445, 23525, 613, 293, 286, 586, 747, 1314, 264, 1821, 5629, 11, 51614, 51614, 321, 393, 1190, 264, 19618, 365, 341, 777, 5883, 2144, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.076076226575034, "compression_ratio": 1.6046511627906976, "no_speech_prob": 1.1125312084914185e-05}, {"id": 122, "seek": 66000, "start": 670.0, "end": 673.0, "text": " And that's used for symmetry breaking, as we'll see in a second.", "tokens": [50364, 407, 718, 311, 445, 352, 365, 1958, 13, 10607, 13, 682, 300, 1389, 11, 527, 4470, 307, 1998, 1547, 11, 457, 575, 512, 30867, 13, 50664, 50664, 467, 311, 406, 2293, 4018, 13, 467, 311, 658, 512, 707, 30867, 13, 50864, 50864, 400, 300, 311, 1143, 337, 25440, 7697, 11, 382, 321, 603, 536, 294, 257, 1150, 13, 51014, 51014, 440, 3565, 1208, 366, 586, 1348, 484, 709, 4966, 281, 4018, 293, 1203, 307, 731, 293, 665, 13, 51264, 51264, 407, 498, 286, 445, 23525, 613, 293, 286, 586, 747, 1314, 264, 1821, 5629, 11, 51614, 51614, 321, 393, 1190, 264, 19618, 365, 341, 777, 5883, 2144, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.076076226575034, "compression_ratio": 1.6046511627906976, "no_speech_prob": 1.1125312084914185e-05}, {"id": 123, "seek": 66000, "start": 673.0, "end": 678.0, "text": " The logits are now coming out much closer to zero and everything is well and good.", "tokens": [50364, 407, 718, 311, 445, 352, 365, 1958, 13, 10607, 13, 682, 300, 1389, 11, 527, 4470, 307, 1998, 1547, 11, 457, 575, 512, 30867, 13, 50664, 50664, 467, 311, 406, 2293, 4018, 13, 467, 311, 658, 512, 707, 30867, 13, 50864, 50864, 400, 300, 311, 1143, 337, 25440, 7697, 11, 382, 321, 603, 536, 294, 257, 1150, 13, 51014, 51014, 440, 3565, 1208, 366, 586, 1348, 484, 709, 4966, 281, 4018, 293, 1203, 307, 731, 293, 665, 13, 51264, 51264, 407, 498, 286, 445, 23525, 613, 293, 286, 586, 747, 1314, 264, 1821, 5629, 11, 51614, 51614, 321, 393, 1190, 264, 19618, 365, 341, 777, 5883, 2144, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.076076226575034, "compression_ratio": 1.6046511627906976, "no_speech_prob": 1.1125312084914185e-05}, {"id": 124, "seek": 66000, "start": 678.0, "end": 685.0, "text": " So if I just erase these and I now take away the break statement,", "tokens": [50364, 407, 718, 311, 445, 352, 365, 1958, 13, 10607, 13, 682, 300, 1389, 11, 527, 4470, 307, 1998, 1547, 11, 457, 575, 512, 30867, 13, 50664, 50664, 467, 311, 406, 2293, 4018, 13, 467, 311, 658, 512, 707, 30867, 13, 50864, 50864, 400, 300, 311, 1143, 337, 25440, 7697, 11, 382, 321, 603, 536, 294, 257, 1150, 13, 51014, 51014, 440, 3565, 1208, 366, 586, 1348, 484, 709, 4966, 281, 4018, 293, 1203, 307, 731, 293, 665, 13, 51264, 51264, 407, 498, 286, 445, 23525, 613, 293, 286, 586, 747, 1314, 264, 1821, 5629, 11, 51614, 51614, 321, 393, 1190, 264, 19618, 365, 341, 777, 5883, 2144, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.076076226575034, "compression_ratio": 1.6046511627906976, "no_speech_prob": 1.1125312084914185e-05}, {"id": 125, "seek": 66000, "start": 685.0, "end": 688.0, "text": " we can run the optimization with this new initialization.", "tokens": [50364, 407, 718, 311, 445, 352, 365, 1958, 13, 10607, 13, 682, 300, 1389, 11, 527, 4470, 307, 1998, 1547, 11, 457, 575, 512, 30867, 13, 50664, 50664, 467, 311, 406, 2293, 4018, 13, 467, 311, 658, 512, 707, 30867, 13, 50864, 50864, 400, 300, 311, 1143, 337, 25440, 7697, 11, 382, 321, 603, 536, 294, 257, 1150, 13, 51014, 51014, 440, 3565, 1208, 366, 586, 1348, 484, 709, 4966, 281, 4018, 293, 1203, 307, 731, 293, 665, 13, 51264, 51264, 407, 498, 286, 445, 23525, 613, 293, 286, 586, 747, 1314, 264, 1821, 5629, 11, 51614, 51614, 321, 393, 1190, 264, 19618, 365, 341, 777, 5883, 2144, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.076076226575034, "compression_ratio": 1.6046511627906976, "no_speech_prob": 1.1125312084914185e-05}, {"id": 126, "seek": 68800, "start": 688.0, "end": 692.0, "text": " And let's just see what losses we record.", "tokens": [50364, 400, 718, 311, 445, 536, 437, 15352, 321, 2136, 13, 50564, 50564, 2264, 11, 370, 286, 718, 309, 1190, 293, 291, 536, 300, 321, 1409, 766, 665, 13, 50714, 50714, 400, 550, 321, 1361, 760, 257, 857, 13, 50864, 50864, 440, 7542, 295, 264, 4470, 586, 1177, 380, 362, 341, 22449, 3909, 8967, 51114, 51114, 570, 1936, 437, 311, 2737, 294, 264, 22449, 2897, 11, 264, 588, 700, 1326, 36540, 295, 264, 4470, 11, 51364, 51364, 437, 311, 2737, 1830, 264, 19618, 307, 264, 19618, 307, 445, 2339, 11077, 760, 264, 3565, 1208, 293, 550, 309, 311, 29875, 9741, 264, 3565, 1208, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07415415416254062, "compression_ratio": 1.7357723577235773, "no_speech_prob": 3.1200626835925505e-05}, {"id": 127, "seek": 68800, "start": 692.0, "end": 695.0, "text": " OK, so I let it run and you see that we started off good.", "tokens": [50364, 400, 718, 311, 445, 536, 437, 15352, 321, 2136, 13, 50564, 50564, 2264, 11, 370, 286, 718, 309, 1190, 293, 291, 536, 300, 321, 1409, 766, 665, 13, 50714, 50714, 400, 550, 321, 1361, 760, 257, 857, 13, 50864, 50864, 440, 7542, 295, 264, 4470, 586, 1177, 380, 362, 341, 22449, 3909, 8967, 51114, 51114, 570, 1936, 437, 311, 2737, 294, 264, 22449, 2897, 11, 264, 588, 700, 1326, 36540, 295, 264, 4470, 11, 51364, 51364, 437, 311, 2737, 1830, 264, 19618, 307, 264, 19618, 307, 445, 2339, 11077, 760, 264, 3565, 1208, 293, 550, 309, 311, 29875, 9741, 264, 3565, 1208, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07415415416254062, "compression_ratio": 1.7357723577235773, "no_speech_prob": 3.1200626835925505e-05}, {"id": 128, "seek": 68800, "start": 695.0, "end": 698.0, "text": " And then we came down a bit.", "tokens": [50364, 400, 718, 311, 445, 536, 437, 15352, 321, 2136, 13, 50564, 50564, 2264, 11, 370, 286, 718, 309, 1190, 293, 291, 536, 300, 321, 1409, 766, 665, 13, 50714, 50714, 400, 550, 321, 1361, 760, 257, 857, 13, 50864, 50864, 440, 7542, 295, 264, 4470, 586, 1177, 380, 362, 341, 22449, 3909, 8967, 51114, 51114, 570, 1936, 437, 311, 2737, 294, 264, 22449, 2897, 11, 264, 588, 700, 1326, 36540, 295, 264, 4470, 11, 51364, 51364, 437, 311, 2737, 1830, 264, 19618, 307, 264, 19618, 307, 445, 2339, 11077, 760, 264, 3565, 1208, 293, 550, 309, 311, 29875, 9741, 264, 3565, 1208, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07415415416254062, "compression_ratio": 1.7357723577235773, "no_speech_prob": 3.1200626835925505e-05}, {"id": 129, "seek": 68800, "start": 698.0, "end": 703.0, "text": " The plot of the loss now doesn't have this hockey shape appearance", "tokens": [50364, 400, 718, 311, 445, 536, 437, 15352, 321, 2136, 13, 50564, 50564, 2264, 11, 370, 286, 718, 309, 1190, 293, 291, 536, 300, 321, 1409, 766, 665, 13, 50714, 50714, 400, 550, 321, 1361, 760, 257, 857, 13, 50864, 50864, 440, 7542, 295, 264, 4470, 586, 1177, 380, 362, 341, 22449, 3909, 8967, 51114, 51114, 570, 1936, 437, 311, 2737, 294, 264, 22449, 2897, 11, 264, 588, 700, 1326, 36540, 295, 264, 4470, 11, 51364, 51364, 437, 311, 2737, 1830, 264, 19618, 307, 264, 19618, 307, 445, 2339, 11077, 760, 264, 3565, 1208, 293, 550, 309, 311, 29875, 9741, 264, 3565, 1208, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07415415416254062, "compression_ratio": 1.7357723577235773, "no_speech_prob": 3.1200626835925505e-05}, {"id": 130, "seek": 68800, "start": 703.0, "end": 708.0, "text": " because basically what's happening in the hockey stick, the very first few iterations of the loss,", "tokens": [50364, 400, 718, 311, 445, 536, 437, 15352, 321, 2136, 13, 50564, 50564, 2264, 11, 370, 286, 718, 309, 1190, 293, 291, 536, 300, 321, 1409, 766, 665, 13, 50714, 50714, 400, 550, 321, 1361, 760, 257, 857, 13, 50864, 50864, 440, 7542, 295, 264, 4470, 586, 1177, 380, 362, 341, 22449, 3909, 8967, 51114, 51114, 570, 1936, 437, 311, 2737, 294, 264, 22449, 2897, 11, 264, 588, 700, 1326, 36540, 295, 264, 4470, 11, 51364, 51364, 437, 311, 2737, 1830, 264, 19618, 307, 264, 19618, 307, 445, 2339, 11077, 760, 264, 3565, 1208, 293, 550, 309, 311, 29875, 9741, 264, 3565, 1208, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07415415416254062, "compression_ratio": 1.7357723577235773, "no_speech_prob": 3.1200626835925505e-05}, {"id": 131, "seek": 68800, "start": 708.0, "end": 715.0, "text": " what's happening during the optimization is the optimization is just squashing down the logits and then it's rearranging the logits.", "tokens": [50364, 400, 718, 311, 445, 536, 437, 15352, 321, 2136, 13, 50564, 50564, 2264, 11, 370, 286, 718, 309, 1190, 293, 291, 536, 300, 321, 1409, 766, 665, 13, 50714, 50714, 400, 550, 321, 1361, 760, 257, 857, 13, 50864, 50864, 440, 7542, 295, 264, 4470, 586, 1177, 380, 362, 341, 22449, 3909, 8967, 51114, 51114, 570, 1936, 437, 311, 2737, 294, 264, 22449, 2897, 11, 264, 588, 700, 1326, 36540, 295, 264, 4470, 11, 51364, 51364, 437, 311, 2737, 1830, 264, 19618, 307, 264, 19618, 307, 445, 2339, 11077, 760, 264, 3565, 1208, 293, 550, 309, 311, 29875, 9741, 264, 3565, 1208, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07415415416254062, "compression_ratio": 1.7357723577235773, "no_speech_prob": 3.1200626835925505e-05}, {"id": 132, "seek": 71500, "start": 715.0, "end": 722.0, "text": " So basically we took away this easy part of the loss function where just the weights were just being shrunk down.", "tokens": [50364, 407, 1936, 321, 1890, 1314, 341, 1858, 644, 295, 264, 4470, 2445, 689, 445, 264, 17443, 645, 445, 885, 9884, 3197, 760, 13, 50714, 50714, 400, 370, 4412, 321, 500, 380, 483, 613, 1858, 16823, 294, 264, 2863, 13, 50914, 50914, 400, 321, 434, 445, 1242, 512, 295, 264, 1152, 16823, 295, 3097, 264, 3539, 18161, 2533, 13, 51064, 51064, 400, 370, 456, 311, 572, 22449, 2897, 8967, 13, 51214, 51214, 407, 665, 721, 366, 2737, 294, 300, 1293, 1230, 472, 4470, 5883, 2144, 307, 437, 321, 2066, 13, 51464, 51464, 400, 264, 4470, 1177, 380, 574, 411, 257, 22449, 2897, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0987523006943037, "compression_ratio": 1.764, "no_speech_prob": 7.410861599055352e-06}, {"id": 133, "seek": 71500, "start": 722.0, "end": 726.0, "text": " And so therefore we don't get these easy gains in the beginning.", "tokens": [50364, 407, 1936, 321, 1890, 1314, 341, 1858, 644, 295, 264, 4470, 2445, 689, 445, 264, 17443, 645, 445, 885, 9884, 3197, 760, 13, 50714, 50714, 400, 370, 4412, 321, 500, 380, 483, 613, 1858, 16823, 294, 264, 2863, 13, 50914, 50914, 400, 321, 434, 445, 1242, 512, 295, 264, 1152, 16823, 295, 3097, 264, 3539, 18161, 2533, 13, 51064, 51064, 400, 370, 456, 311, 572, 22449, 2897, 8967, 13, 51214, 51214, 407, 665, 721, 366, 2737, 294, 300, 1293, 1230, 472, 4470, 5883, 2144, 307, 437, 321, 2066, 13, 51464, 51464, 400, 264, 4470, 1177, 380, 574, 411, 257, 22449, 2897, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0987523006943037, "compression_ratio": 1.764, "no_speech_prob": 7.410861599055352e-06}, {"id": 134, "seek": 71500, "start": 726.0, "end": 729.0, "text": " And we're just getting some of the hard gains of training the actual neural net.", "tokens": [50364, 407, 1936, 321, 1890, 1314, 341, 1858, 644, 295, 264, 4470, 2445, 689, 445, 264, 17443, 645, 445, 885, 9884, 3197, 760, 13, 50714, 50714, 400, 370, 4412, 321, 500, 380, 483, 613, 1858, 16823, 294, 264, 2863, 13, 50914, 50914, 400, 321, 434, 445, 1242, 512, 295, 264, 1152, 16823, 295, 3097, 264, 3539, 18161, 2533, 13, 51064, 51064, 400, 370, 456, 311, 572, 22449, 2897, 8967, 13, 51214, 51214, 407, 665, 721, 366, 2737, 294, 300, 1293, 1230, 472, 4470, 5883, 2144, 307, 437, 321, 2066, 13, 51464, 51464, 400, 264, 4470, 1177, 380, 574, 411, 257, 22449, 2897, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0987523006943037, "compression_ratio": 1.764, "no_speech_prob": 7.410861599055352e-06}, {"id": 135, "seek": 71500, "start": 729.0, "end": 732.0, "text": " And so there's no hockey stick appearance.", "tokens": [50364, 407, 1936, 321, 1890, 1314, 341, 1858, 644, 295, 264, 4470, 2445, 689, 445, 264, 17443, 645, 445, 885, 9884, 3197, 760, 13, 50714, 50714, 400, 370, 4412, 321, 500, 380, 483, 613, 1858, 16823, 294, 264, 2863, 13, 50914, 50914, 400, 321, 434, 445, 1242, 512, 295, 264, 1152, 16823, 295, 3097, 264, 3539, 18161, 2533, 13, 51064, 51064, 400, 370, 456, 311, 572, 22449, 2897, 8967, 13, 51214, 51214, 407, 665, 721, 366, 2737, 294, 300, 1293, 1230, 472, 4470, 5883, 2144, 307, 437, 321, 2066, 13, 51464, 51464, 400, 264, 4470, 1177, 380, 574, 411, 257, 22449, 2897, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0987523006943037, "compression_ratio": 1.764, "no_speech_prob": 7.410861599055352e-06}, {"id": 136, "seek": 71500, "start": 732.0, "end": 737.0, "text": " So good things are happening in that both number one loss initialization is what we expect.", "tokens": [50364, 407, 1936, 321, 1890, 1314, 341, 1858, 644, 295, 264, 4470, 2445, 689, 445, 264, 17443, 645, 445, 885, 9884, 3197, 760, 13, 50714, 50714, 400, 370, 4412, 321, 500, 380, 483, 613, 1858, 16823, 294, 264, 2863, 13, 50914, 50914, 400, 321, 434, 445, 1242, 512, 295, 264, 1152, 16823, 295, 3097, 264, 3539, 18161, 2533, 13, 51064, 51064, 400, 370, 456, 311, 572, 22449, 2897, 8967, 13, 51214, 51214, 407, 665, 721, 366, 2737, 294, 300, 1293, 1230, 472, 4470, 5883, 2144, 307, 437, 321, 2066, 13, 51464, 51464, 400, 264, 4470, 1177, 380, 574, 411, 257, 22449, 2897, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0987523006943037, "compression_ratio": 1.764, "no_speech_prob": 7.410861599055352e-06}, {"id": 137, "seek": 71500, "start": 737.0, "end": 741.0, "text": " And the loss doesn't look like a hockey stick.", "tokens": [50364, 407, 1936, 321, 1890, 1314, 341, 1858, 644, 295, 264, 4470, 2445, 689, 445, 264, 17443, 645, 445, 885, 9884, 3197, 760, 13, 50714, 50714, 400, 370, 4412, 321, 500, 380, 483, 613, 1858, 16823, 294, 264, 2863, 13, 50914, 50914, 400, 321, 434, 445, 1242, 512, 295, 264, 1152, 16823, 295, 3097, 264, 3539, 18161, 2533, 13, 51064, 51064, 400, 370, 456, 311, 572, 22449, 2897, 8967, 13, 51214, 51214, 407, 665, 721, 366, 2737, 294, 300, 1293, 1230, 472, 4470, 5883, 2144, 307, 437, 321, 2066, 13, 51464, 51464, 400, 264, 4470, 1177, 380, 574, 411, 257, 22449, 2897, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0987523006943037, "compression_ratio": 1.764, "no_speech_prob": 7.410861599055352e-06}, {"id": 138, "seek": 74100, "start": 741.0, "end": 746.0, "text": " And this is true for any neural net you might train and something to look out for.", "tokens": [50364, 400, 341, 307, 2074, 337, 604, 18161, 2533, 291, 1062, 3847, 293, 746, 281, 574, 484, 337, 13, 50614, 50614, 400, 1150, 11, 264, 4470, 300, 1361, 484, 307, 767, 1596, 257, 857, 9689, 13, 50814, 50814, 8590, 11, 286, 38359, 437, 321, 632, 510, 949, 13, 50914, 50914, 286, 1697, 341, 390, 732, 935, 472, 732, 293, 341, 390, 341, 390, 732, 935, 472, 2309, 13, 51164, 51164, 407, 321, 483, 257, 4748, 9689, 1874, 13, 51314, 51314, 400, 264, 1778, 337, 300, 307, 570, 321, 434, 6434, 544, 17796, 11, 544, 565, 40425, 264, 18161, 2533, 767, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09223444645221417, "compression_ratio": 1.6932270916334662, "no_speech_prob": 2.2602634999202564e-06}, {"id": 139, "seek": 74100, "start": 746.0, "end": 750.0, "text": " And second, the loss that came out is actually quite a bit improved.", "tokens": [50364, 400, 341, 307, 2074, 337, 604, 18161, 2533, 291, 1062, 3847, 293, 746, 281, 574, 484, 337, 13, 50614, 50614, 400, 1150, 11, 264, 4470, 300, 1361, 484, 307, 767, 1596, 257, 857, 9689, 13, 50814, 50814, 8590, 11, 286, 38359, 437, 321, 632, 510, 949, 13, 50914, 50914, 286, 1697, 341, 390, 732, 935, 472, 732, 293, 341, 390, 341, 390, 732, 935, 472, 2309, 13, 51164, 51164, 407, 321, 483, 257, 4748, 9689, 1874, 13, 51314, 51314, 400, 264, 1778, 337, 300, 307, 570, 321, 434, 6434, 544, 17796, 11, 544, 565, 40425, 264, 18161, 2533, 767, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09223444645221417, "compression_ratio": 1.6932270916334662, "no_speech_prob": 2.2602634999202564e-06}, {"id": 140, "seek": 74100, "start": 750.0, "end": 752.0, "text": " Unfortunately, I erased what we had here before.", "tokens": [50364, 400, 341, 307, 2074, 337, 604, 18161, 2533, 291, 1062, 3847, 293, 746, 281, 574, 484, 337, 13, 50614, 50614, 400, 1150, 11, 264, 4470, 300, 1361, 484, 307, 767, 1596, 257, 857, 9689, 13, 50814, 50814, 8590, 11, 286, 38359, 437, 321, 632, 510, 949, 13, 50914, 50914, 286, 1697, 341, 390, 732, 935, 472, 732, 293, 341, 390, 341, 390, 732, 935, 472, 2309, 13, 51164, 51164, 407, 321, 483, 257, 4748, 9689, 1874, 13, 51314, 51314, 400, 264, 1778, 337, 300, 307, 570, 321, 434, 6434, 544, 17796, 11, 544, 565, 40425, 264, 18161, 2533, 767, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09223444645221417, "compression_ratio": 1.6932270916334662, "no_speech_prob": 2.2602634999202564e-06}, {"id": 141, "seek": 74100, "start": 752.0, "end": 757.0, "text": " I believe this was two point one two and this was this was two point one six.", "tokens": [50364, 400, 341, 307, 2074, 337, 604, 18161, 2533, 291, 1062, 3847, 293, 746, 281, 574, 484, 337, 13, 50614, 50614, 400, 1150, 11, 264, 4470, 300, 1361, 484, 307, 767, 1596, 257, 857, 9689, 13, 50814, 50814, 8590, 11, 286, 38359, 437, 321, 632, 510, 949, 13, 50914, 50914, 286, 1697, 341, 390, 732, 935, 472, 732, 293, 341, 390, 341, 390, 732, 935, 472, 2309, 13, 51164, 51164, 407, 321, 483, 257, 4748, 9689, 1874, 13, 51314, 51314, 400, 264, 1778, 337, 300, 307, 570, 321, 434, 6434, 544, 17796, 11, 544, 565, 40425, 264, 18161, 2533, 767, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09223444645221417, "compression_ratio": 1.6932270916334662, "no_speech_prob": 2.2602634999202564e-06}, {"id": 142, "seek": 74100, "start": 757.0, "end": 760.0, "text": " So we get a slightly improved result.", "tokens": [50364, 400, 341, 307, 2074, 337, 604, 18161, 2533, 291, 1062, 3847, 293, 746, 281, 574, 484, 337, 13, 50614, 50614, 400, 1150, 11, 264, 4470, 300, 1361, 484, 307, 767, 1596, 257, 857, 9689, 13, 50814, 50814, 8590, 11, 286, 38359, 437, 321, 632, 510, 949, 13, 50914, 50914, 286, 1697, 341, 390, 732, 935, 472, 732, 293, 341, 390, 341, 390, 732, 935, 472, 2309, 13, 51164, 51164, 407, 321, 483, 257, 4748, 9689, 1874, 13, 51314, 51314, 400, 264, 1778, 337, 300, 307, 570, 321, 434, 6434, 544, 17796, 11, 544, 565, 40425, 264, 18161, 2533, 767, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09223444645221417, "compression_ratio": 1.6932270916334662, "no_speech_prob": 2.2602634999202564e-06}, {"id": 143, "seek": 74100, "start": 760.0, "end": 767.0, "text": " And the reason for that is because we're spending more cycles, more time optimizing the neural net actually,", "tokens": [50364, 400, 341, 307, 2074, 337, 604, 18161, 2533, 291, 1062, 3847, 293, 746, 281, 574, 484, 337, 13, 50614, 50614, 400, 1150, 11, 264, 4470, 300, 1361, 484, 307, 767, 1596, 257, 857, 9689, 13, 50814, 50814, 8590, 11, 286, 38359, 437, 321, 632, 510, 949, 13, 50914, 50914, 286, 1697, 341, 390, 732, 935, 472, 732, 293, 341, 390, 341, 390, 732, 935, 472, 2309, 13, 51164, 51164, 407, 321, 483, 257, 4748, 9689, 1874, 13, 51314, 51314, 400, 264, 1778, 337, 300, 307, 570, 321, 434, 6434, 544, 17796, 11, 544, 565, 40425, 264, 18161, 2533, 767, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09223444645221417, "compression_ratio": 1.6932270916334662, "no_speech_prob": 2.2602634999202564e-06}, {"id": 144, "seek": 76700, "start": 767.0, "end": 773.0, "text": " instead of just spending the first several thousand iterations probably just squashing down the weights", "tokens": [50364, 2602, 295, 445, 6434, 264, 700, 2940, 4714, 36540, 1391, 445, 2339, 11077, 760, 264, 17443, 50664, 50664, 570, 436, 366, 370, 636, 886, 1090, 294, 264, 2863, 295, 264, 5883, 2144, 13, 50864, 50864, 407, 746, 281, 574, 484, 337, 13, 400, 300, 311, 1230, 472, 13, 51014, 51014, 823, 718, 311, 574, 412, 264, 1150, 1154, 13, 51114, 51114, 961, 385, 6561, 270, 831, 1125, 527, 18161, 2533, 293, 718, 385, 319, 38132, 384, 264, 1821, 5629, 13, 51314, 51314, 407, 321, 362, 257, 10585, 5883, 4470, 13, 51464, 51464, 407, 754, 1673, 1203, 307, 1237, 665, 322, 264, 1496, 295, 4470, 293, 321, 483, 746, 300, 321, 2066, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07657938167966645, "compression_ratio": 1.7347670250896057, "no_speech_prob": 3.5007562928512925e-06}, {"id": 145, "seek": 76700, "start": 773.0, "end": 777.0, "text": " because they are so way too high in the beginning of the initialization.", "tokens": [50364, 2602, 295, 445, 6434, 264, 700, 2940, 4714, 36540, 1391, 445, 2339, 11077, 760, 264, 17443, 50664, 50664, 570, 436, 366, 370, 636, 886, 1090, 294, 264, 2863, 295, 264, 5883, 2144, 13, 50864, 50864, 407, 746, 281, 574, 484, 337, 13, 400, 300, 311, 1230, 472, 13, 51014, 51014, 823, 718, 311, 574, 412, 264, 1150, 1154, 13, 51114, 51114, 961, 385, 6561, 270, 831, 1125, 527, 18161, 2533, 293, 718, 385, 319, 38132, 384, 264, 1821, 5629, 13, 51314, 51314, 407, 321, 362, 257, 10585, 5883, 4470, 13, 51464, 51464, 407, 754, 1673, 1203, 307, 1237, 665, 322, 264, 1496, 295, 4470, 293, 321, 483, 746, 300, 321, 2066, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07657938167966645, "compression_ratio": 1.7347670250896057, "no_speech_prob": 3.5007562928512925e-06}, {"id": 146, "seek": 76700, "start": 777.0, "end": 780.0, "text": " So something to look out for. And that's number one.", "tokens": [50364, 2602, 295, 445, 6434, 264, 700, 2940, 4714, 36540, 1391, 445, 2339, 11077, 760, 264, 17443, 50664, 50664, 570, 436, 366, 370, 636, 886, 1090, 294, 264, 2863, 295, 264, 5883, 2144, 13, 50864, 50864, 407, 746, 281, 574, 484, 337, 13, 400, 300, 311, 1230, 472, 13, 51014, 51014, 823, 718, 311, 574, 412, 264, 1150, 1154, 13, 51114, 51114, 961, 385, 6561, 270, 831, 1125, 527, 18161, 2533, 293, 718, 385, 319, 38132, 384, 264, 1821, 5629, 13, 51314, 51314, 407, 321, 362, 257, 10585, 5883, 4470, 13, 51464, 51464, 407, 754, 1673, 1203, 307, 1237, 665, 322, 264, 1496, 295, 4470, 293, 321, 483, 746, 300, 321, 2066, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07657938167966645, "compression_ratio": 1.7347670250896057, "no_speech_prob": 3.5007562928512925e-06}, {"id": 147, "seek": 76700, "start": 780.0, "end": 782.0, "text": " Now let's look at the second problem.", "tokens": [50364, 2602, 295, 445, 6434, 264, 700, 2940, 4714, 36540, 1391, 445, 2339, 11077, 760, 264, 17443, 50664, 50664, 570, 436, 366, 370, 636, 886, 1090, 294, 264, 2863, 295, 264, 5883, 2144, 13, 50864, 50864, 407, 746, 281, 574, 484, 337, 13, 400, 300, 311, 1230, 472, 13, 51014, 51014, 823, 718, 311, 574, 412, 264, 1150, 1154, 13, 51114, 51114, 961, 385, 6561, 270, 831, 1125, 527, 18161, 2533, 293, 718, 385, 319, 38132, 384, 264, 1821, 5629, 13, 51314, 51314, 407, 321, 362, 257, 10585, 5883, 4470, 13, 51464, 51464, 407, 754, 1673, 1203, 307, 1237, 665, 322, 264, 1496, 295, 4470, 293, 321, 483, 746, 300, 321, 2066, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07657938167966645, "compression_ratio": 1.7347670250896057, "no_speech_prob": 3.5007562928512925e-06}, {"id": 148, "seek": 76700, "start": 782.0, "end": 786.0, "text": " Let me reinitialize our neural net and let me reintroduce the break statement.", "tokens": [50364, 2602, 295, 445, 6434, 264, 700, 2940, 4714, 36540, 1391, 445, 2339, 11077, 760, 264, 17443, 50664, 50664, 570, 436, 366, 370, 636, 886, 1090, 294, 264, 2863, 295, 264, 5883, 2144, 13, 50864, 50864, 407, 746, 281, 574, 484, 337, 13, 400, 300, 311, 1230, 472, 13, 51014, 51014, 823, 718, 311, 574, 412, 264, 1150, 1154, 13, 51114, 51114, 961, 385, 6561, 270, 831, 1125, 527, 18161, 2533, 293, 718, 385, 319, 38132, 384, 264, 1821, 5629, 13, 51314, 51314, 407, 321, 362, 257, 10585, 5883, 4470, 13, 51464, 51464, 407, 754, 1673, 1203, 307, 1237, 665, 322, 264, 1496, 295, 4470, 293, 321, 483, 746, 300, 321, 2066, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07657938167966645, "compression_ratio": 1.7347670250896057, "no_speech_prob": 3.5007562928512925e-06}, {"id": 149, "seek": 76700, "start": 786.0, "end": 789.0, "text": " So we have a reasonable initial loss.", "tokens": [50364, 2602, 295, 445, 6434, 264, 700, 2940, 4714, 36540, 1391, 445, 2339, 11077, 760, 264, 17443, 50664, 50664, 570, 436, 366, 370, 636, 886, 1090, 294, 264, 2863, 295, 264, 5883, 2144, 13, 50864, 50864, 407, 746, 281, 574, 484, 337, 13, 400, 300, 311, 1230, 472, 13, 51014, 51014, 823, 718, 311, 574, 412, 264, 1150, 1154, 13, 51114, 51114, 961, 385, 6561, 270, 831, 1125, 527, 18161, 2533, 293, 718, 385, 319, 38132, 384, 264, 1821, 5629, 13, 51314, 51314, 407, 321, 362, 257, 10585, 5883, 4470, 13, 51464, 51464, 407, 754, 1673, 1203, 307, 1237, 665, 322, 264, 1496, 295, 4470, 293, 321, 483, 746, 300, 321, 2066, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07657938167966645, "compression_ratio": 1.7347670250896057, "no_speech_prob": 3.5007562928512925e-06}, {"id": 150, "seek": 76700, "start": 789.0, "end": 793.0, "text": " So even though everything is looking good on the level of loss and we get something that we expect,", "tokens": [50364, 2602, 295, 445, 6434, 264, 700, 2940, 4714, 36540, 1391, 445, 2339, 11077, 760, 264, 17443, 50664, 50664, 570, 436, 366, 370, 636, 886, 1090, 294, 264, 2863, 295, 264, 5883, 2144, 13, 50864, 50864, 407, 746, 281, 574, 484, 337, 13, 400, 300, 311, 1230, 472, 13, 51014, 51014, 823, 718, 311, 574, 412, 264, 1150, 1154, 13, 51114, 51114, 961, 385, 6561, 270, 831, 1125, 527, 18161, 2533, 293, 718, 385, 319, 38132, 384, 264, 1821, 5629, 13, 51314, 51314, 407, 321, 362, 257, 10585, 5883, 4470, 13, 51464, 51464, 407, 754, 1673, 1203, 307, 1237, 665, 322, 264, 1496, 295, 4470, 293, 321, 483, 746, 300, 321, 2066, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07657938167966645, "compression_ratio": 1.7347670250896057, "no_speech_prob": 3.5007562928512925e-06}, {"id": 151, "seek": 79300, "start": 793.0, "end": 798.0, "text": " there's still a deeper problem lurking inside this neural net and its initialization.", "tokens": [50364, 456, 311, 920, 257, 7731, 1154, 35583, 5092, 1854, 341, 18161, 2533, 293, 1080, 5883, 2144, 13, 50614, 50614, 407, 264, 3565, 1208, 366, 586, 2264, 13, 50714, 50714, 440, 1154, 586, 307, 365, 264, 4190, 295, 389, 11, 264, 2430, 763, 295, 264, 7633, 4368, 13, 51014, 51014, 823, 11, 498, 321, 445, 23273, 341, 8062, 11, 2597, 11, 341, 40863, 389, 11, 309, 311, 733, 295, 1152, 281, 536, 13, 51264, 51264, 583, 264, 1154, 510, 11, 9810, 4124, 11, 307, 291, 536, 577, 867, 295, 264, 4959, 366, 472, 420, 3671, 472, 13, 51514, 51514, 823, 11, 9901, 300, 27822, 5893, 1266, 39, 11, 264, 1266, 39, 2445, 307, 257, 2339, 11077, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11251901989140786, "compression_ratio": 1.6496350364963503, "no_speech_prob": 2.0580077944032382e-06}, {"id": 152, "seek": 79300, "start": 798.0, "end": 800.0, "text": " So the logits are now OK.", "tokens": [50364, 456, 311, 920, 257, 7731, 1154, 35583, 5092, 1854, 341, 18161, 2533, 293, 1080, 5883, 2144, 13, 50614, 50614, 407, 264, 3565, 1208, 366, 586, 2264, 13, 50714, 50714, 440, 1154, 586, 307, 365, 264, 4190, 295, 389, 11, 264, 2430, 763, 295, 264, 7633, 4368, 13, 51014, 51014, 823, 11, 498, 321, 445, 23273, 341, 8062, 11, 2597, 11, 341, 40863, 389, 11, 309, 311, 733, 295, 1152, 281, 536, 13, 51264, 51264, 583, 264, 1154, 510, 11, 9810, 4124, 11, 307, 291, 536, 577, 867, 295, 264, 4959, 366, 472, 420, 3671, 472, 13, 51514, 51514, 823, 11, 9901, 300, 27822, 5893, 1266, 39, 11, 264, 1266, 39, 2445, 307, 257, 2339, 11077, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11251901989140786, "compression_ratio": 1.6496350364963503, "no_speech_prob": 2.0580077944032382e-06}, {"id": 153, "seek": 79300, "start": 800.0, "end": 806.0, "text": " The problem now is with the values of H, the activations of the hidden states.", "tokens": [50364, 456, 311, 920, 257, 7731, 1154, 35583, 5092, 1854, 341, 18161, 2533, 293, 1080, 5883, 2144, 13, 50614, 50614, 407, 264, 3565, 1208, 366, 586, 2264, 13, 50714, 50714, 440, 1154, 586, 307, 365, 264, 4190, 295, 389, 11, 264, 2430, 763, 295, 264, 7633, 4368, 13, 51014, 51014, 823, 11, 498, 321, 445, 23273, 341, 8062, 11, 2597, 11, 341, 40863, 389, 11, 309, 311, 733, 295, 1152, 281, 536, 13, 51264, 51264, 583, 264, 1154, 510, 11, 9810, 4124, 11, 307, 291, 536, 577, 867, 295, 264, 4959, 366, 472, 420, 3671, 472, 13, 51514, 51514, 823, 11, 9901, 300, 27822, 5893, 1266, 39, 11, 264, 1266, 39, 2445, 307, 257, 2339, 11077, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11251901989140786, "compression_ratio": 1.6496350364963503, "no_speech_prob": 2.0580077944032382e-06}, {"id": 154, "seek": 79300, "start": 806.0, "end": 811.0, "text": " Now, if we just visualize this vector, sorry, this tensor H, it's kind of hard to see.", "tokens": [50364, 456, 311, 920, 257, 7731, 1154, 35583, 5092, 1854, 341, 18161, 2533, 293, 1080, 5883, 2144, 13, 50614, 50614, 407, 264, 3565, 1208, 366, 586, 2264, 13, 50714, 50714, 440, 1154, 586, 307, 365, 264, 4190, 295, 389, 11, 264, 2430, 763, 295, 264, 7633, 4368, 13, 51014, 51014, 823, 11, 498, 321, 445, 23273, 341, 8062, 11, 2597, 11, 341, 40863, 389, 11, 309, 311, 733, 295, 1152, 281, 536, 13, 51264, 51264, 583, 264, 1154, 510, 11, 9810, 4124, 11, 307, 291, 536, 577, 867, 295, 264, 4959, 366, 472, 420, 3671, 472, 13, 51514, 51514, 823, 11, 9901, 300, 27822, 5893, 1266, 39, 11, 264, 1266, 39, 2445, 307, 257, 2339, 11077, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11251901989140786, "compression_ratio": 1.6496350364963503, "no_speech_prob": 2.0580077944032382e-06}, {"id": 155, "seek": 79300, "start": 811.0, "end": 816.0, "text": " But the problem here, roughly speaking, is you see how many of the elements are one or negative one.", "tokens": [50364, 456, 311, 920, 257, 7731, 1154, 35583, 5092, 1854, 341, 18161, 2533, 293, 1080, 5883, 2144, 13, 50614, 50614, 407, 264, 3565, 1208, 366, 586, 2264, 13, 50714, 50714, 440, 1154, 586, 307, 365, 264, 4190, 295, 389, 11, 264, 2430, 763, 295, 264, 7633, 4368, 13, 51014, 51014, 823, 11, 498, 321, 445, 23273, 341, 8062, 11, 2597, 11, 341, 40863, 389, 11, 309, 311, 733, 295, 1152, 281, 536, 13, 51264, 51264, 583, 264, 1154, 510, 11, 9810, 4124, 11, 307, 291, 536, 577, 867, 295, 264, 4959, 366, 472, 420, 3671, 472, 13, 51514, 51514, 823, 11, 9901, 300, 27822, 5893, 1266, 39, 11, 264, 1266, 39, 2445, 307, 257, 2339, 11077, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11251901989140786, "compression_ratio": 1.6496350364963503, "no_speech_prob": 2.0580077944032382e-06}, {"id": 156, "seek": 79300, "start": 816.0, "end": 821.0, "text": " Now, recall that torch dot 10H, the 10H function is a squashing function.", "tokens": [50364, 456, 311, 920, 257, 7731, 1154, 35583, 5092, 1854, 341, 18161, 2533, 293, 1080, 5883, 2144, 13, 50614, 50614, 407, 264, 3565, 1208, 366, 586, 2264, 13, 50714, 50714, 440, 1154, 586, 307, 365, 264, 4190, 295, 389, 11, 264, 2430, 763, 295, 264, 7633, 4368, 13, 51014, 51014, 823, 11, 498, 321, 445, 23273, 341, 8062, 11, 2597, 11, 341, 40863, 389, 11, 309, 311, 733, 295, 1152, 281, 536, 13, 51264, 51264, 583, 264, 1154, 510, 11, 9810, 4124, 11, 307, 291, 536, 577, 867, 295, 264, 4959, 366, 472, 420, 3671, 472, 13, 51514, 51514, 823, 11, 9901, 300, 27822, 5893, 1266, 39, 11, 264, 1266, 39, 2445, 307, 257, 2339, 11077, 2445, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11251901989140786, "compression_ratio": 1.6496350364963503, "no_speech_prob": 2.0580077944032382e-06}, {"id": 157, "seek": 82100, "start": 821.0, "end": 826.0, "text": " It makes arbitrary numbers and it squashes them into a range of negative one and one, and it does so smoothly.", "tokens": [50364, 467, 1669, 23211, 3547, 293, 309, 2339, 12808, 552, 666, 257, 3613, 295, 3671, 472, 293, 472, 11, 293, 309, 775, 370, 19565, 13, 50614, 50614, 407, 718, 311, 574, 412, 264, 49816, 295, 389, 281, 483, 257, 1101, 1558, 295, 264, 7316, 295, 264, 4190, 1854, 341, 40863, 13, 50914, 50914, 492, 393, 360, 341, 700, 13, 51064, 51064, 1042, 11, 321, 393, 536, 300, 389, 307, 8858, 5110, 293, 2331, 2430, 763, 294, 1184, 1365, 13, 51364, 51364, 492, 393, 1910, 309, 382, 3671, 472, 281, 5985, 309, 484, 666, 472, 2416, 8062, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07582294464111328, "compression_ratio": 1.649789029535865, "no_speech_prob": 3.90537843486527e-06}, {"id": 158, "seek": 82100, "start": 826.0, "end": 832.0, "text": " So let's look at the histogram of H to get a better idea of the distribution of the values inside this tensor.", "tokens": [50364, 467, 1669, 23211, 3547, 293, 309, 2339, 12808, 552, 666, 257, 3613, 295, 3671, 472, 293, 472, 11, 293, 309, 775, 370, 19565, 13, 50614, 50614, 407, 718, 311, 574, 412, 264, 49816, 295, 389, 281, 483, 257, 1101, 1558, 295, 264, 7316, 295, 264, 4190, 1854, 341, 40863, 13, 50914, 50914, 492, 393, 360, 341, 700, 13, 51064, 51064, 1042, 11, 321, 393, 536, 300, 389, 307, 8858, 5110, 293, 2331, 2430, 763, 294, 1184, 1365, 13, 51364, 51364, 492, 393, 1910, 309, 382, 3671, 472, 281, 5985, 309, 484, 666, 472, 2416, 8062, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07582294464111328, "compression_ratio": 1.649789029535865, "no_speech_prob": 3.90537843486527e-06}, {"id": 159, "seek": 82100, "start": 832.0, "end": 835.0, "text": " We can do this first.", "tokens": [50364, 467, 1669, 23211, 3547, 293, 309, 2339, 12808, 552, 666, 257, 3613, 295, 3671, 472, 293, 472, 11, 293, 309, 775, 370, 19565, 13, 50614, 50614, 407, 718, 311, 574, 412, 264, 49816, 295, 389, 281, 483, 257, 1101, 1558, 295, 264, 7316, 295, 264, 4190, 1854, 341, 40863, 13, 50914, 50914, 492, 393, 360, 341, 700, 13, 51064, 51064, 1042, 11, 321, 393, 536, 300, 389, 307, 8858, 5110, 293, 2331, 2430, 763, 294, 1184, 1365, 13, 51364, 51364, 492, 393, 1910, 309, 382, 3671, 472, 281, 5985, 309, 484, 666, 472, 2416, 8062, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07582294464111328, "compression_ratio": 1.649789029535865, "no_speech_prob": 3.90537843486527e-06}, {"id": 160, "seek": 82100, "start": 835.0, "end": 841.0, "text": " Well, we can see that H is 32 examples and 200 activations in each example.", "tokens": [50364, 467, 1669, 23211, 3547, 293, 309, 2339, 12808, 552, 666, 257, 3613, 295, 3671, 472, 293, 472, 11, 293, 309, 775, 370, 19565, 13, 50614, 50614, 407, 718, 311, 574, 412, 264, 49816, 295, 389, 281, 483, 257, 1101, 1558, 295, 264, 7316, 295, 264, 4190, 1854, 341, 40863, 13, 50914, 50914, 492, 393, 360, 341, 700, 13, 51064, 51064, 1042, 11, 321, 393, 536, 300, 389, 307, 8858, 5110, 293, 2331, 2430, 763, 294, 1184, 1365, 13, 51364, 51364, 492, 393, 1910, 309, 382, 3671, 472, 281, 5985, 309, 484, 666, 472, 2416, 8062, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07582294464111328, "compression_ratio": 1.649789029535865, "no_speech_prob": 3.90537843486527e-06}, {"id": 161, "seek": 82100, "start": 841.0, "end": 846.0, "text": " We can view it as negative one to stretch it out into one large vector.", "tokens": [50364, 467, 1669, 23211, 3547, 293, 309, 2339, 12808, 552, 666, 257, 3613, 295, 3671, 472, 293, 472, 11, 293, 309, 775, 370, 19565, 13, 50614, 50614, 407, 718, 311, 574, 412, 264, 49816, 295, 389, 281, 483, 257, 1101, 1558, 295, 264, 7316, 295, 264, 4190, 1854, 341, 40863, 13, 50914, 50914, 492, 393, 360, 341, 700, 13, 51064, 51064, 1042, 11, 321, 393, 536, 300, 389, 307, 8858, 5110, 293, 2331, 2430, 763, 294, 1184, 1365, 13, 51364, 51364, 492, 393, 1910, 309, 382, 3671, 472, 281, 5985, 309, 484, 666, 472, 2416, 8062, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07582294464111328, "compression_ratio": 1.649789029535865, "no_speech_prob": 3.90537843486527e-06}, {"id": 162, "seek": 84600, "start": 846.0, "end": 853.0, "text": " And we can then call to list to convert this into one large Python list of floats.", "tokens": [50364, 400, 321, 393, 550, 818, 281, 1329, 281, 7620, 341, 666, 472, 2416, 15329, 1329, 295, 37878, 13, 50714, 50714, 400, 550, 321, 393, 1320, 341, 666, 6999, 51, 5893, 1758, 337, 49816, 13, 50914, 50914, 400, 321, 584, 321, 528, 2625, 41275, 293, 257, 27515, 38780, 281, 26835, 257, 3840, 295, 5598, 321, 500, 380, 528, 13, 51264, 51264, 407, 321, 536, 341, 49816, 293, 321, 536, 300, 881, 295, 264, 4190, 538, 1400, 747, 322, 2158, 295, 3671, 472, 293, 472, 13, 51564, 51564, 407, 341, 1266, 39, 307, 588, 11, 588, 4967, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08416006088256836, "compression_ratio": 1.64, "no_speech_prob": 1.3845189641870093e-05}, {"id": 163, "seek": 84600, "start": 853.0, "end": 857.0, "text": " And then we can pass this into PLT dot hist for histogram.", "tokens": [50364, 400, 321, 393, 550, 818, 281, 1329, 281, 7620, 341, 666, 472, 2416, 15329, 1329, 295, 37878, 13, 50714, 50714, 400, 550, 321, 393, 1320, 341, 666, 6999, 51, 5893, 1758, 337, 49816, 13, 50914, 50914, 400, 321, 584, 321, 528, 2625, 41275, 293, 257, 27515, 38780, 281, 26835, 257, 3840, 295, 5598, 321, 500, 380, 528, 13, 51264, 51264, 407, 321, 536, 341, 49816, 293, 321, 536, 300, 881, 295, 264, 4190, 538, 1400, 747, 322, 2158, 295, 3671, 472, 293, 472, 13, 51564, 51564, 407, 341, 1266, 39, 307, 588, 11, 588, 4967, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08416006088256836, "compression_ratio": 1.64, "no_speech_prob": 1.3845189641870093e-05}, {"id": 164, "seek": 84600, "start": 857.0, "end": 864.0, "text": " And we say we want 50 bins and a semicolon to suppress a bunch of output we don't want.", "tokens": [50364, 400, 321, 393, 550, 818, 281, 1329, 281, 7620, 341, 666, 472, 2416, 15329, 1329, 295, 37878, 13, 50714, 50714, 400, 550, 321, 393, 1320, 341, 666, 6999, 51, 5893, 1758, 337, 49816, 13, 50914, 50914, 400, 321, 584, 321, 528, 2625, 41275, 293, 257, 27515, 38780, 281, 26835, 257, 3840, 295, 5598, 321, 500, 380, 528, 13, 51264, 51264, 407, 321, 536, 341, 49816, 293, 321, 536, 300, 881, 295, 264, 4190, 538, 1400, 747, 322, 2158, 295, 3671, 472, 293, 472, 13, 51564, 51564, 407, 341, 1266, 39, 307, 588, 11, 588, 4967, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08416006088256836, "compression_ratio": 1.64, "no_speech_prob": 1.3845189641870093e-05}, {"id": 165, "seek": 84600, "start": 864.0, "end": 870.0, "text": " So we see this histogram and we see that most of the values by far take on value of negative one and one.", "tokens": [50364, 400, 321, 393, 550, 818, 281, 1329, 281, 7620, 341, 666, 472, 2416, 15329, 1329, 295, 37878, 13, 50714, 50714, 400, 550, 321, 393, 1320, 341, 666, 6999, 51, 5893, 1758, 337, 49816, 13, 50914, 50914, 400, 321, 584, 321, 528, 2625, 41275, 293, 257, 27515, 38780, 281, 26835, 257, 3840, 295, 5598, 321, 500, 380, 528, 13, 51264, 51264, 407, 321, 536, 341, 49816, 293, 321, 536, 300, 881, 295, 264, 4190, 538, 1400, 747, 322, 2158, 295, 3671, 472, 293, 472, 13, 51564, 51564, 407, 341, 1266, 39, 307, 588, 11, 588, 4967, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08416006088256836, "compression_ratio": 1.64, "no_speech_prob": 1.3845189641870093e-05}, {"id": 166, "seek": 84600, "start": 870.0, "end": 873.0, "text": " So this 10H is very, very active.", "tokens": [50364, 400, 321, 393, 550, 818, 281, 1329, 281, 7620, 341, 666, 472, 2416, 15329, 1329, 295, 37878, 13, 50714, 50714, 400, 550, 321, 393, 1320, 341, 666, 6999, 51, 5893, 1758, 337, 49816, 13, 50914, 50914, 400, 321, 584, 321, 528, 2625, 41275, 293, 257, 27515, 38780, 281, 26835, 257, 3840, 295, 5598, 321, 500, 380, 528, 13, 51264, 51264, 407, 321, 536, 341, 49816, 293, 321, 536, 300, 881, 295, 264, 4190, 538, 1400, 747, 322, 2158, 295, 3671, 472, 293, 472, 13, 51564, 51564, 407, 341, 1266, 39, 307, 588, 11, 588, 4967, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08416006088256836, "compression_ratio": 1.64, "no_speech_prob": 1.3845189641870093e-05}, {"id": 167, "seek": 87300, "start": 873.0, "end": 878.0, "text": " And we can also look at basically why that is.", "tokens": [50364, 400, 321, 393, 611, 574, 412, 1936, 983, 300, 307, 13, 50614, 50614, 492, 393, 574, 412, 264, 659, 23397, 763, 300, 3154, 666, 264, 1266, 39, 13, 50864, 50864, 400, 321, 393, 536, 300, 264, 7316, 295, 264, 659, 23397, 763, 366, 307, 588, 11, 588, 4152, 13, 51064, 51064, 1981, 747, 3547, 1296, 3671, 2119, 293, 2119, 13, 51214, 51214, 400, 300, 311, 983, 294, 264, 27822, 5893, 1266, 39, 11, 1203, 307, 885, 2339, 12219, 293, 1335, 3320, 281, 312, 294, 264, 3613, 295, 3671, 472, 293, 472, 13, 51464, 51464, 400, 3195, 295, 3547, 510, 747, 322, 588, 8084, 4190, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07667929973077336, "compression_ratio": 1.715481171548117, "no_speech_prob": 1.1658964467642363e-05}, {"id": 168, "seek": 87300, "start": 878.0, "end": 883.0, "text": " We can look at the preactivations that feed into the 10H.", "tokens": [50364, 400, 321, 393, 611, 574, 412, 1936, 983, 300, 307, 13, 50614, 50614, 492, 393, 574, 412, 264, 659, 23397, 763, 300, 3154, 666, 264, 1266, 39, 13, 50864, 50864, 400, 321, 393, 536, 300, 264, 7316, 295, 264, 659, 23397, 763, 366, 307, 588, 11, 588, 4152, 13, 51064, 51064, 1981, 747, 3547, 1296, 3671, 2119, 293, 2119, 13, 51214, 51214, 400, 300, 311, 983, 294, 264, 27822, 5893, 1266, 39, 11, 1203, 307, 885, 2339, 12219, 293, 1335, 3320, 281, 312, 294, 264, 3613, 295, 3671, 472, 293, 472, 13, 51464, 51464, 400, 3195, 295, 3547, 510, 747, 322, 588, 8084, 4190, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07667929973077336, "compression_ratio": 1.715481171548117, "no_speech_prob": 1.1658964467642363e-05}, {"id": 169, "seek": 87300, "start": 883.0, "end": 887.0, "text": " And we can see that the distribution of the preactivations are is very, very broad.", "tokens": [50364, 400, 321, 393, 611, 574, 412, 1936, 983, 300, 307, 13, 50614, 50614, 492, 393, 574, 412, 264, 659, 23397, 763, 300, 3154, 666, 264, 1266, 39, 13, 50864, 50864, 400, 321, 393, 536, 300, 264, 7316, 295, 264, 659, 23397, 763, 366, 307, 588, 11, 588, 4152, 13, 51064, 51064, 1981, 747, 3547, 1296, 3671, 2119, 293, 2119, 13, 51214, 51214, 400, 300, 311, 983, 294, 264, 27822, 5893, 1266, 39, 11, 1203, 307, 885, 2339, 12219, 293, 1335, 3320, 281, 312, 294, 264, 3613, 295, 3671, 472, 293, 472, 13, 51464, 51464, 400, 3195, 295, 3547, 510, 747, 322, 588, 8084, 4190, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07667929973077336, "compression_ratio": 1.715481171548117, "no_speech_prob": 1.1658964467642363e-05}, {"id": 170, "seek": 87300, "start": 887.0, "end": 890.0, "text": " These take numbers between negative 15 and 15.", "tokens": [50364, 400, 321, 393, 611, 574, 412, 1936, 983, 300, 307, 13, 50614, 50614, 492, 393, 574, 412, 264, 659, 23397, 763, 300, 3154, 666, 264, 1266, 39, 13, 50864, 50864, 400, 321, 393, 536, 300, 264, 7316, 295, 264, 659, 23397, 763, 366, 307, 588, 11, 588, 4152, 13, 51064, 51064, 1981, 747, 3547, 1296, 3671, 2119, 293, 2119, 13, 51214, 51214, 400, 300, 311, 983, 294, 264, 27822, 5893, 1266, 39, 11, 1203, 307, 885, 2339, 12219, 293, 1335, 3320, 281, 312, 294, 264, 3613, 295, 3671, 472, 293, 472, 13, 51464, 51464, 400, 3195, 295, 3547, 510, 747, 322, 588, 8084, 4190, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07667929973077336, "compression_ratio": 1.715481171548117, "no_speech_prob": 1.1658964467642363e-05}, {"id": 171, "seek": 87300, "start": 890.0, "end": 895.0, "text": " And that's why in the torch dot 10H, everything is being squashed and capped to be in the range of negative one and one.", "tokens": [50364, 400, 321, 393, 611, 574, 412, 1936, 983, 300, 307, 13, 50614, 50614, 492, 393, 574, 412, 264, 659, 23397, 763, 300, 3154, 666, 264, 1266, 39, 13, 50864, 50864, 400, 321, 393, 536, 300, 264, 7316, 295, 264, 659, 23397, 763, 366, 307, 588, 11, 588, 4152, 13, 51064, 51064, 1981, 747, 3547, 1296, 3671, 2119, 293, 2119, 13, 51214, 51214, 400, 300, 311, 983, 294, 264, 27822, 5893, 1266, 39, 11, 1203, 307, 885, 2339, 12219, 293, 1335, 3320, 281, 312, 294, 264, 3613, 295, 3671, 472, 293, 472, 13, 51464, 51464, 400, 3195, 295, 3547, 510, 747, 322, 588, 8084, 4190, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07667929973077336, "compression_ratio": 1.715481171548117, "no_speech_prob": 1.1658964467642363e-05}, {"id": 172, "seek": 87300, "start": 895.0, "end": 899.0, "text": " And lots of numbers here take on very extreme values.", "tokens": [50364, 400, 321, 393, 611, 574, 412, 1936, 983, 300, 307, 13, 50614, 50614, 492, 393, 574, 412, 264, 659, 23397, 763, 300, 3154, 666, 264, 1266, 39, 13, 50864, 50864, 400, 321, 393, 536, 300, 264, 7316, 295, 264, 659, 23397, 763, 366, 307, 588, 11, 588, 4152, 13, 51064, 51064, 1981, 747, 3547, 1296, 3671, 2119, 293, 2119, 13, 51214, 51214, 400, 300, 311, 983, 294, 264, 27822, 5893, 1266, 39, 11, 1203, 307, 885, 2339, 12219, 293, 1335, 3320, 281, 312, 294, 264, 3613, 295, 3671, 472, 293, 472, 13, 51464, 51464, 400, 3195, 295, 3547, 510, 747, 322, 588, 8084, 4190, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07667929973077336, "compression_ratio": 1.715481171548117, "no_speech_prob": 1.1658964467642363e-05}, {"id": 173, "seek": 89900, "start": 899.0, "end": 903.0, "text": " Now, if you are new to neural networks, you might not actually see this as an issue.", "tokens": [50364, 823, 11, 498, 291, 366, 777, 281, 18161, 9590, 11, 291, 1062, 406, 767, 536, 341, 382, 364, 2734, 13, 50564, 50564, 583, 498, 291, 434, 731, 1774, 292, 294, 264, 2877, 8609, 295, 646, 79, 1513, 559, 399, 293, 362, 364, 21769, 2020, 295, 577, 613, 2771, 2448, 3095, 807, 257, 18161, 2533, 11, 50914, 50914, 291, 366, 1237, 412, 428, 7316, 295, 1266, 39, 2430, 763, 510, 293, 291, 366, 25438, 13, 51164, 51164, 407, 718, 385, 855, 291, 983, 13, 51214, 51214, 492, 362, 281, 1066, 294, 1575, 300, 1830, 646, 79, 1513, 559, 399, 11, 445, 411, 321, 1866, 294, 4532, 7165, 11, 51414, 51414, 321, 366, 884, 23897, 1320, 2891, 412, 264, 4470, 293, 13974, 807, 264, 3209, 12204, 13, 51614, 51614, 682, 1729, 11, 321, 434, 516, 281, 646, 48256, 807, 341, 27822, 5893, 1266, 39, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07400231134323847, "compression_ratio": 1.7925696594427245, "no_speech_prob": 1.4969253243179992e-05}, {"id": 174, "seek": 89900, "start": 903.0, "end": 910.0, "text": " But if you're well versed in the dark arts of backpropagation and have an intuitive sense of how these gradients flow through a neural net,", "tokens": [50364, 823, 11, 498, 291, 366, 777, 281, 18161, 9590, 11, 291, 1062, 406, 767, 536, 341, 382, 364, 2734, 13, 50564, 50564, 583, 498, 291, 434, 731, 1774, 292, 294, 264, 2877, 8609, 295, 646, 79, 1513, 559, 399, 293, 362, 364, 21769, 2020, 295, 577, 613, 2771, 2448, 3095, 807, 257, 18161, 2533, 11, 50914, 50914, 291, 366, 1237, 412, 428, 7316, 295, 1266, 39, 2430, 763, 510, 293, 291, 366, 25438, 13, 51164, 51164, 407, 718, 385, 855, 291, 983, 13, 51214, 51214, 492, 362, 281, 1066, 294, 1575, 300, 1830, 646, 79, 1513, 559, 399, 11, 445, 411, 321, 1866, 294, 4532, 7165, 11, 51414, 51414, 321, 366, 884, 23897, 1320, 2891, 412, 264, 4470, 293, 13974, 807, 264, 3209, 12204, 13, 51614, 51614, 682, 1729, 11, 321, 434, 516, 281, 646, 48256, 807, 341, 27822, 5893, 1266, 39, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07400231134323847, "compression_ratio": 1.7925696594427245, "no_speech_prob": 1.4969253243179992e-05}, {"id": 175, "seek": 89900, "start": 910.0, "end": 915.0, "text": " you are looking at your distribution of 10H activations here and you are sweating.", "tokens": [50364, 823, 11, 498, 291, 366, 777, 281, 18161, 9590, 11, 291, 1062, 406, 767, 536, 341, 382, 364, 2734, 13, 50564, 50564, 583, 498, 291, 434, 731, 1774, 292, 294, 264, 2877, 8609, 295, 646, 79, 1513, 559, 399, 293, 362, 364, 21769, 2020, 295, 577, 613, 2771, 2448, 3095, 807, 257, 18161, 2533, 11, 50914, 50914, 291, 366, 1237, 412, 428, 7316, 295, 1266, 39, 2430, 763, 510, 293, 291, 366, 25438, 13, 51164, 51164, 407, 718, 385, 855, 291, 983, 13, 51214, 51214, 492, 362, 281, 1066, 294, 1575, 300, 1830, 646, 79, 1513, 559, 399, 11, 445, 411, 321, 1866, 294, 4532, 7165, 11, 51414, 51414, 321, 366, 884, 23897, 1320, 2891, 412, 264, 4470, 293, 13974, 807, 264, 3209, 12204, 13, 51614, 51614, 682, 1729, 11, 321, 434, 516, 281, 646, 48256, 807, 341, 27822, 5893, 1266, 39, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07400231134323847, "compression_ratio": 1.7925696594427245, "no_speech_prob": 1.4969253243179992e-05}, {"id": 176, "seek": 89900, "start": 915.0, "end": 916.0, "text": " So let me show you why.", "tokens": [50364, 823, 11, 498, 291, 366, 777, 281, 18161, 9590, 11, 291, 1062, 406, 767, 536, 341, 382, 364, 2734, 13, 50564, 50564, 583, 498, 291, 434, 731, 1774, 292, 294, 264, 2877, 8609, 295, 646, 79, 1513, 559, 399, 293, 362, 364, 21769, 2020, 295, 577, 613, 2771, 2448, 3095, 807, 257, 18161, 2533, 11, 50914, 50914, 291, 366, 1237, 412, 428, 7316, 295, 1266, 39, 2430, 763, 510, 293, 291, 366, 25438, 13, 51164, 51164, 407, 718, 385, 855, 291, 983, 13, 51214, 51214, 492, 362, 281, 1066, 294, 1575, 300, 1830, 646, 79, 1513, 559, 399, 11, 445, 411, 321, 1866, 294, 4532, 7165, 11, 51414, 51414, 321, 366, 884, 23897, 1320, 2891, 412, 264, 4470, 293, 13974, 807, 264, 3209, 12204, 13, 51614, 51614, 682, 1729, 11, 321, 434, 516, 281, 646, 48256, 807, 341, 27822, 5893, 1266, 39, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07400231134323847, "compression_ratio": 1.7925696594427245, "no_speech_prob": 1.4969253243179992e-05}, {"id": 177, "seek": 89900, "start": 916.0, "end": 920.0, "text": " We have to keep in mind that during backpropagation, just like we saw in micrograd,", "tokens": [50364, 823, 11, 498, 291, 366, 777, 281, 18161, 9590, 11, 291, 1062, 406, 767, 536, 341, 382, 364, 2734, 13, 50564, 50564, 583, 498, 291, 434, 731, 1774, 292, 294, 264, 2877, 8609, 295, 646, 79, 1513, 559, 399, 293, 362, 364, 21769, 2020, 295, 577, 613, 2771, 2448, 3095, 807, 257, 18161, 2533, 11, 50914, 50914, 291, 366, 1237, 412, 428, 7316, 295, 1266, 39, 2430, 763, 510, 293, 291, 366, 25438, 13, 51164, 51164, 407, 718, 385, 855, 291, 983, 13, 51214, 51214, 492, 362, 281, 1066, 294, 1575, 300, 1830, 646, 79, 1513, 559, 399, 11, 445, 411, 321, 1866, 294, 4532, 7165, 11, 51414, 51414, 321, 366, 884, 23897, 1320, 2891, 412, 264, 4470, 293, 13974, 807, 264, 3209, 12204, 13, 51614, 51614, 682, 1729, 11, 321, 434, 516, 281, 646, 48256, 807, 341, 27822, 5893, 1266, 39, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07400231134323847, "compression_ratio": 1.7925696594427245, "no_speech_prob": 1.4969253243179992e-05}, {"id": 178, "seek": 89900, "start": 920.0, "end": 924.0, "text": " we are doing backward pass starting at the loss and flowing through the network backwards.", "tokens": [50364, 823, 11, 498, 291, 366, 777, 281, 18161, 9590, 11, 291, 1062, 406, 767, 536, 341, 382, 364, 2734, 13, 50564, 50564, 583, 498, 291, 434, 731, 1774, 292, 294, 264, 2877, 8609, 295, 646, 79, 1513, 559, 399, 293, 362, 364, 21769, 2020, 295, 577, 613, 2771, 2448, 3095, 807, 257, 18161, 2533, 11, 50914, 50914, 291, 366, 1237, 412, 428, 7316, 295, 1266, 39, 2430, 763, 510, 293, 291, 366, 25438, 13, 51164, 51164, 407, 718, 385, 855, 291, 983, 13, 51214, 51214, 492, 362, 281, 1066, 294, 1575, 300, 1830, 646, 79, 1513, 559, 399, 11, 445, 411, 321, 1866, 294, 4532, 7165, 11, 51414, 51414, 321, 366, 884, 23897, 1320, 2891, 412, 264, 4470, 293, 13974, 807, 264, 3209, 12204, 13, 51614, 51614, 682, 1729, 11, 321, 434, 516, 281, 646, 48256, 807, 341, 27822, 5893, 1266, 39, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07400231134323847, "compression_ratio": 1.7925696594427245, "no_speech_prob": 1.4969253243179992e-05}, {"id": 179, "seek": 89900, "start": 924.0, "end": 928.0, "text": " In particular, we're going to back propagate through this torch dot 10H.", "tokens": [50364, 823, 11, 498, 291, 366, 777, 281, 18161, 9590, 11, 291, 1062, 406, 767, 536, 341, 382, 364, 2734, 13, 50564, 50564, 583, 498, 291, 434, 731, 1774, 292, 294, 264, 2877, 8609, 295, 646, 79, 1513, 559, 399, 293, 362, 364, 21769, 2020, 295, 577, 613, 2771, 2448, 3095, 807, 257, 18161, 2533, 11, 50914, 50914, 291, 366, 1237, 412, 428, 7316, 295, 1266, 39, 2430, 763, 510, 293, 291, 366, 25438, 13, 51164, 51164, 407, 718, 385, 855, 291, 983, 13, 51214, 51214, 492, 362, 281, 1066, 294, 1575, 300, 1830, 646, 79, 1513, 559, 399, 11, 445, 411, 321, 1866, 294, 4532, 7165, 11, 51414, 51414, 321, 366, 884, 23897, 1320, 2891, 412, 264, 4470, 293, 13974, 807, 264, 3209, 12204, 13, 51614, 51614, 682, 1729, 11, 321, 434, 516, 281, 646, 48256, 807, 341, 27822, 5893, 1266, 39, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07400231134323847, "compression_ratio": 1.7925696594427245, "no_speech_prob": 1.4969253243179992e-05}, {"id": 180, "seek": 92800, "start": 928.0, "end": 933.0, "text": " And this layer here is made up of 200 neurons for each one of these examples.", "tokens": [50364, 400, 341, 4583, 510, 307, 1027, 493, 295, 2331, 22027, 337, 1184, 472, 295, 613, 5110, 13, 50614, 50614, 400, 309, 704, 17988, 364, 4478, 6091, 1266, 39, 13, 50764, 50764, 407, 718, 311, 574, 412, 437, 2314, 294, 1266, 39, 294, 264, 23897, 1320, 13, 50914, 50914, 492, 393, 767, 352, 646, 281, 527, 3894, 4532, 7165, 3089, 294, 264, 588, 700, 7991, 293, 536, 577, 321, 12270, 1266, 39, 13, 51264, 51264, 492, 1866, 300, 264, 4846, 510, 390, 1783, 293, 550, 321, 8873, 314, 11, 597, 307, 264, 1266, 39, 295, 1783, 13, 51564, 51564, 407, 300, 311, 314, 13, 400, 314, 307, 1296, 3671, 472, 293, 472, 13, 51664, 51664, 467, 311, 264, 5598, 295, 264, 1266, 39, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06508287294643132, "compression_ratio": 1.618705035971223, "no_speech_prob": 3.187482661815011e-06}, {"id": 181, "seek": 92800, "start": 933.0, "end": 936.0, "text": " And it implements an element twice 10H.", "tokens": [50364, 400, 341, 4583, 510, 307, 1027, 493, 295, 2331, 22027, 337, 1184, 472, 295, 613, 5110, 13, 50614, 50614, 400, 309, 704, 17988, 364, 4478, 6091, 1266, 39, 13, 50764, 50764, 407, 718, 311, 574, 412, 437, 2314, 294, 1266, 39, 294, 264, 23897, 1320, 13, 50914, 50914, 492, 393, 767, 352, 646, 281, 527, 3894, 4532, 7165, 3089, 294, 264, 588, 700, 7991, 293, 536, 577, 321, 12270, 1266, 39, 13, 51264, 51264, 492, 1866, 300, 264, 4846, 510, 390, 1783, 293, 550, 321, 8873, 314, 11, 597, 307, 264, 1266, 39, 295, 1783, 13, 51564, 51564, 407, 300, 311, 314, 13, 400, 314, 307, 1296, 3671, 472, 293, 472, 13, 51664, 51664, 467, 311, 264, 5598, 295, 264, 1266, 39, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06508287294643132, "compression_ratio": 1.618705035971223, "no_speech_prob": 3.187482661815011e-06}, {"id": 182, "seek": 92800, "start": 936.0, "end": 939.0, "text": " So let's look at what happens in 10H in the backward pass.", "tokens": [50364, 400, 341, 4583, 510, 307, 1027, 493, 295, 2331, 22027, 337, 1184, 472, 295, 613, 5110, 13, 50614, 50614, 400, 309, 704, 17988, 364, 4478, 6091, 1266, 39, 13, 50764, 50764, 407, 718, 311, 574, 412, 437, 2314, 294, 1266, 39, 294, 264, 23897, 1320, 13, 50914, 50914, 492, 393, 767, 352, 646, 281, 527, 3894, 4532, 7165, 3089, 294, 264, 588, 700, 7991, 293, 536, 577, 321, 12270, 1266, 39, 13, 51264, 51264, 492, 1866, 300, 264, 4846, 510, 390, 1783, 293, 550, 321, 8873, 314, 11, 597, 307, 264, 1266, 39, 295, 1783, 13, 51564, 51564, 407, 300, 311, 314, 13, 400, 314, 307, 1296, 3671, 472, 293, 472, 13, 51664, 51664, 467, 311, 264, 5598, 295, 264, 1266, 39, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06508287294643132, "compression_ratio": 1.618705035971223, "no_speech_prob": 3.187482661815011e-06}, {"id": 183, "seek": 92800, "start": 939.0, "end": 946.0, "text": " We can actually go back to our previous micrograd code in the very first lecture and see how we implemented 10H.", "tokens": [50364, 400, 341, 4583, 510, 307, 1027, 493, 295, 2331, 22027, 337, 1184, 472, 295, 613, 5110, 13, 50614, 50614, 400, 309, 704, 17988, 364, 4478, 6091, 1266, 39, 13, 50764, 50764, 407, 718, 311, 574, 412, 437, 2314, 294, 1266, 39, 294, 264, 23897, 1320, 13, 50914, 50914, 492, 393, 767, 352, 646, 281, 527, 3894, 4532, 7165, 3089, 294, 264, 588, 700, 7991, 293, 536, 577, 321, 12270, 1266, 39, 13, 51264, 51264, 492, 1866, 300, 264, 4846, 510, 390, 1783, 293, 550, 321, 8873, 314, 11, 597, 307, 264, 1266, 39, 295, 1783, 13, 51564, 51564, 407, 300, 311, 314, 13, 400, 314, 307, 1296, 3671, 472, 293, 472, 13, 51664, 51664, 467, 311, 264, 5598, 295, 264, 1266, 39, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06508287294643132, "compression_ratio": 1.618705035971223, "no_speech_prob": 3.187482661815011e-06}, {"id": 184, "seek": 92800, "start": 946.0, "end": 952.0, "text": " We saw that the input here was X and then we calculate T, which is the 10H of X.", "tokens": [50364, 400, 341, 4583, 510, 307, 1027, 493, 295, 2331, 22027, 337, 1184, 472, 295, 613, 5110, 13, 50614, 50614, 400, 309, 704, 17988, 364, 4478, 6091, 1266, 39, 13, 50764, 50764, 407, 718, 311, 574, 412, 437, 2314, 294, 1266, 39, 294, 264, 23897, 1320, 13, 50914, 50914, 492, 393, 767, 352, 646, 281, 527, 3894, 4532, 7165, 3089, 294, 264, 588, 700, 7991, 293, 536, 577, 321, 12270, 1266, 39, 13, 51264, 51264, 492, 1866, 300, 264, 4846, 510, 390, 1783, 293, 550, 321, 8873, 314, 11, 597, 307, 264, 1266, 39, 295, 1783, 13, 51564, 51564, 407, 300, 311, 314, 13, 400, 314, 307, 1296, 3671, 472, 293, 472, 13, 51664, 51664, 467, 311, 264, 5598, 295, 264, 1266, 39, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06508287294643132, "compression_ratio": 1.618705035971223, "no_speech_prob": 3.187482661815011e-06}, {"id": 185, "seek": 92800, "start": 952.0, "end": 954.0, "text": " So that's T. And T is between negative one and one.", "tokens": [50364, 400, 341, 4583, 510, 307, 1027, 493, 295, 2331, 22027, 337, 1184, 472, 295, 613, 5110, 13, 50614, 50614, 400, 309, 704, 17988, 364, 4478, 6091, 1266, 39, 13, 50764, 50764, 407, 718, 311, 574, 412, 437, 2314, 294, 1266, 39, 294, 264, 23897, 1320, 13, 50914, 50914, 492, 393, 767, 352, 646, 281, 527, 3894, 4532, 7165, 3089, 294, 264, 588, 700, 7991, 293, 536, 577, 321, 12270, 1266, 39, 13, 51264, 51264, 492, 1866, 300, 264, 4846, 510, 390, 1783, 293, 550, 321, 8873, 314, 11, 597, 307, 264, 1266, 39, 295, 1783, 13, 51564, 51564, 407, 300, 311, 314, 13, 400, 314, 307, 1296, 3671, 472, 293, 472, 13, 51664, 51664, 467, 311, 264, 5598, 295, 264, 1266, 39, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06508287294643132, "compression_ratio": 1.618705035971223, "no_speech_prob": 3.187482661815011e-06}, {"id": 186, "seek": 92800, "start": 954.0, "end": 956.0, "text": " It's the output of the 10H.", "tokens": [50364, 400, 341, 4583, 510, 307, 1027, 493, 295, 2331, 22027, 337, 1184, 472, 295, 613, 5110, 13, 50614, 50614, 400, 309, 704, 17988, 364, 4478, 6091, 1266, 39, 13, 50764, 50764, 407, 718, 311, 574, 412, 437, 2314, 294, 1266, 39, 294, 264, 23897, 1320, 13, 50914, 50914, 492, 393, 767, 352, 646, 281, 527, 3894, 4532, 7165, 3089, 294, 264, 588, 700, 7991, 293, 536, 577, 321, 12270, 1266, 39, 13, 51264, 51264, 492, 1866, 300, 264, 4846, 510, 390, 1783, 293, 550, 321, 8873, 314, 11, 597, 307, 264, 1266, 39, 295, 1783, 13, 51564, 51564, 407, 300, 311, 314, 13, 400, 314, 307, 1296, 3671, 472, 293, 472, 13, 51664, 51664, 467, 311, 264, 5598, 295, 264, 1266, 39, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06508287294643132, "compression_ratio": 1.618705035971223, "no_speech_prob": 3.187482661815011e-06}, {"id": 187, "seek": 95600, "start": 956.0, "end": 960.0, "text": " And then in the backward pass, how do we back propagate through a 10H?", "tokens": [50364, 400, 550, 294, 264, 23897, 1320, 11, 577, 360, 321, 646, 48256, 807, 257, 1266, 39, 30, 50564, 50564, 492, 747, 484, 5893, 2771, 293, 550, 321, 12972, 309, 13, 50764, 50764, 639, 307, 264, 5021, 4978, 365, 264, 2654, 16235, 11, 597, 1890, 264, 1254, 295, 472, 3175, 314, 8889, 13, 51014, 51014, 407, 437, 2314, 498, 264, 23930, 295, 428, 1266, 39, 366, 588, 1998, 281, 3671, 472, 420, 472, 30, 51264, 51264, 759, 291, 5452, 294, 314, 6915, 472, 510, 11, 291, 434, 516, 281, 483, 257, 4018, 11, 30955, 484, 5893, 2771, 13, 51514, 51514, 883, 1871, 437, 484, 5893, 2771, 307, 11, 321, 366, 8011, 264, 16235, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06883354022585113, "compression_ratio": 1.6404494382022472, "no_speech_prob": 8.267652447102591e-06}, {"id": 188, "seek": 95600, "start": 960.0, "end": 964.0, "text": " We take out dot grad and then we multiply it.", "tokens": [50364, 400, 550, 294, 264, 23897, 1320, 11, 577, 360, 321, 646, 48256, 807, 257, 1266, 39, 30, 50564, 50564, 492, 747, 484, 5893, 2771, 293, 550, 321, 12972, 309, 13, 50764, 50764, 639, 307, 264, 5021, 4978, 365, 264, 2654, 16235, 11, 597, 1890, 264, 1254, 295, 472, 3175, 314, 8889, 13, 51014, 51014, 407, 437, 2314, 498, 264, 23930, 295, 428, 1266, 39, 366, 588, 1998, 281, 3671, 472, 420, 472, 30, 51264, 51264, 759, 291, 5452, 294, 314, 6915, 472, 510, 11, 291, 434, 516, 281, 483, 257, 4018, 11, 30955, 484, 5893, 2771, 13, 51514, 51514, 883, 1871, 437, 484, 5893, 2771, 307, 11, 321, 366, 8011, 264, 16235, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06883354022585113, "compression_ratio": 1.6404494382022472, "no_speech_prob": 8.267652447102591e-06}, {"id": 189, "seek": 95600, "start": 964.0, "end": 969.0, "text": " This is the chain rule with the local gradient, which took the form of one minus T squared.", "tokens": [50364, 400, 550, 294, 264, 23897, 1320, 11, 577, 360, 321, 646, 48256, 807, 257, 1266, 39, 30, 50564, 50564, 492, 747, 484, 5893, 2771, 293, 550, 321, 12972, 309, 13, 50764, 50764, 639, 307, 264, 5021, 4978, 365, 264, 2654, 16235, 11, 597, 1890, 264, 1254, 295, 472, 3175, 314, 8889, 13, 51014, 51014, 407, 437, 2314, 498, 264, 23930, 295, 428, 1266, 39, 366, 588, 1998, 281, 3671, 472, 420, 472, 30, 51264, 51264, 759, 291, 5452, 294, 314, 6915, 472, 510, 11, 291, 434, 516, 281, 483, 257, 4018, 11, 30955, 484, 5893, 2771, 13, 51514, 51514, 883, 1871, 437, 484, 5893, 2771, 307, 11, 321, 366, 8011, 264, 16235, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06883354022585113, "compression_ratio": 1.6404494382022472, "no_speech_prob": 8.267652447102591e-06}, {"id": 190, "seek": 95600, "start": 969.0, "end": 974.0, "text": " So what happens if the outputs of your 10H are very close to negative one or one?", "tokens": [50364, 400, 550, 294, 264, 23897, 1320, 11, 577, 360, 321, 646, 48256, 807, 257, 1266, 39, 30, 50564, 50564, 492, 747, 484, 5893, 2771, 293, 550, 321, 12972, 309, 13, 50764, 50764, 639, 307, 264, 5021, 4978, 365, 264, 2654, 16235, 11, 597, 1890, 264, 1254, 295, 472, 3175, 314, 8889, 13, 51014, 51014, 407, 437, 2314, 498, 264, 23930, 295, 428, 1266, 39, 366, 588, 1998, 281, 3671, 472, 420, 472, 30, 51264, 51264, 759, 291, 5452, 294, 314, 6915, 472, 510, 11, 291, 434, 516, 281, 483, 257, 4018, 11, 30955, 484, 5893, 2771, 13, 51514, 51514, 883, 1871, 437, 484, 5893, 2771, 307, 11, 321, 366, 8011, 264, 16235, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06883354022585113, "compression_ratio": 1.6404494382022472, "no_speech_prob": 8.267652447102591e-06}, {"id": 191, "seek": 95600, "start": 974.0, "end": 979.0, "text": " If you plug in T equals one here, you're going to get a zero, multiplying out dot grad.", "tokens": [50364, 400, 550, 294, 264, 23897, 1320, 11, 577, 360, 321, 646, 48256, 807, 257, 1266, 39, 30, 50564, 50564, 492, 747, 484, 5893, 2771, 293, 550, 321, 12972, 309, 13, 50764, 50764, 639, 307, 264, 5021, 4978, 365, 264, 2654, 16235, 11, 597, 1890, 264, 1254, 295, 472, 3175, 314, 8889, 13, 51014, 51014, 407, 437, 2314, 498, 264, 23930, 295, 428, 1266, 39, 366, 588, 1998, 281, 3671, 472, 420, 472, 30, 51264, 51264, 759, 291, 5452, 294, 314, 6915, 472, 510, 11, 291, 434, 516, 281, 483, 257, 4018, 11, 30955, 484, 5893, 2771, 13, 51514, 51514, 883, 1871, 437, 484, 5893, 2771, 307, 11, 321, 366, 8011, 264, 16235, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06883354022585113, "compression_ratio": 1.6404494382022472, "no_speech_prob": 8.267652447102591e-06}, {"id": 192, "seek": 95600, "start": 979.0, "end": 983.0, "text": " No matter what out dot grad is, we are killing the gradient", "tokens": [50364, 400, 550, 294, 264, 23897, 1320, 11, 577, 360, 321, 646, 48256, 807, 257, 1266, 39, 30, 50564, 50564, 492, 747, 484, 5893, 2771, 293, 550, 321, 12972, 309, 13, 50764, 50764, 639, 307, 264, 5021, 4978, 365, 264, 2654, 16235, 11, 597, 1890, 264, 1254, 295, 472, 3175, 314, 8889, 13, 51014, 51014, 407, 437, 2314, 498, 264, 23930, 295, 428, 1266, 39, 366, 588, 1998, 281, 3671, 472, 420, 472, 30, 51264, 51264, 759, 291, 5452, 294, 314, 6915, 472, 510, 11, 291, 434, 516, 281, 483, 257, 4018, 11, 30955, 484, 5893, 2771, 13, 51514, 51514, 883, 1871, 437, 484, 5893, 2771, 307, 11, 321, 366, 8011, 264, 16235, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06883354022585113, "compression_ratio": 1.6404494382022472, "no_speech_prob": 8.267652447102591e-06}, {"id": 193, "seek": 98300, "start": 983.0, "end": 987.0, "text": " and we're stopping effectively the back propagation through this 10H unit.", "tokens": [50364, 293, 321, 434, 12767, 8659, 264, 646, 38377, 807, 341, 1266, 39, 4985, 13, 50564, 50564, 13157, 11, 562, 314, 307, 3671, 472, 11, 341, 486, 797, 1813, 4018, 293, 484, 5893, 2771, 445, 10094, 13, 50864, 50864, 400, 46506, 341, 1669, 2020, 570, 341, 307, 257, 1266, 39, 34090, 13, 51064, 51064, 400, 437, 311, 2737, 307, 498, 1080, 5598, 307, 588, 1998, 281, 472, 11, 550, 321, 366, 294, 264, 6838, 295, 341, 1266, 39, 13, 51414, 51414, 400, 370, 4473, 1936, 264, 4846, 307, 406, 516, 281, 2712, 264, 5598, 295, 264, 1266, 39, 886, 709, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.051897460974535896, "compression_ratio": 1.6442687747035574, "no_speech_prob": 4.092863491678145e-06}, {"id": 194, "seek": 98300, "start": 987.0, "end": 993.0, "text": " Similarly, when T is negative one, this will again become zero and out dot grad just stops.", "tokens": [50364, 293, 321, 434, 12767, 8659, 264, 646, 38377, 807, 341, 1266, 39, 4985, 13, 50564, 50564, 13157, 11, 562, 314, 307, 3671, 472, 11, 341, 486, 797, 1813, 4018, 293, 484, 5893, 2771, 445, 10094, 13, 50864, 50864, 400, 46506, 341, 1669, 2020, 570, 341, 307, 257, 1266, 39, 34090, 13, 51064, 51064, 400, 437, 311, 2737, 307, 498, 1080, 5598, 307, 588, 1998, 281, 472, 11, 550, 321, 366, 294, 264, 6838, 295, 341, 1266, 39, 13, 51414, 51414, 400, 370, 4473, 1936, 264, 4846, 307, 406, 516, 281, 2712, 264, 5598, 295, 264, 1266, 39, 886, 709, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.051897460974535896, "compression_ratio": 1.6442687747035574, "no_speech_prob": 4.092863491678145e-06}, {"id": 195, "seek": 98300, "start": 993.0, "end": 997.0, "text": " And intuitively this makes sense because this is a 10H neuron.", "tokens": [50364, 293, 321, 434, 12767, 8659, 264, 646, 38377, 807, 341, 1266, 39, 4985, 13, 50564, 50564, 13157, 11, 562, 314, 307, 3671, 472, 11, 341, 486, 797, 1813, 4018, 293, 484, 5893, 2771, 445, 10094, 13, 50864, 50864, 400, 46506, 341, 1669, 2020, 570, 341, 307, 257, 1266, 39, 34090, 13, 51064, 51064, 400, 437, 311, 2737, 307, 498, 1080, 5598, 307, 588, 1998, 281, 472, 11, 550, 321, 366, 294, 264, 6838, 295, 341, 1266, 39, 13, 51414, 51414, 400, 370, 4473, 1936, 264, 4846, 307, 406, 516, 281, 2712, 264, 5598, 295, 264, 1266, 39, 886, 709, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.051897460974535896, "compression_ratio": 1.6442687747035574, "no_speech_prob": 4.092863491678145e-06}, {"id": 196, "seek": 98300, "start": 997.0, "end": 1004.0, "text": " And what's happening is if its output is very close to one, then we are in the tail of this 10H.", "tokens": [50364, 293, 321, 434, 12767, 8659, 264, 646, 38377, 807, 341, 1266, 39, 4985, 13, 50564, 50564, 13157, 11, 562, 314, 307, 3671, 472, 11, 341, 486, 797, 1813, 4018, 293, 484, 5893, 2771, 445, 10094, 13, 50864, 50864, 400, 46506, 341, 1669, 2020, 570, 341, 307, 257, 1266, 39, 34090, 13, 51064, 51064, 400, 437, 311, 2737, 307, 498, 1080, 5598, 307, 588, 1998, 281, 472, 11, 550, 321, 366, 294, 264, 6838, 295, 341, 1266, 39, 13, 51414, 51414, 400, 370, 4473, 1936, 264, 4846, 307, 406, 516, 281, 2712, 264, 5598, 295, 264, 1266, 39, 886, 709, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.051897460974535896, "compression_ratio": 1.6442687747035574, "no_speech_prob": 4.092863491678145e-06}, {"id": 197, "seek": 98300, "start": 1004.0, "end": 1012.0, "text": " And so changing basically the input is not going to impact the output of the 10H too much", "tokens": [50364, 293, 321, 434, 12767, 8659, 264, 646, 38377, 807, 341, 1266, 39, 4985, 13, 50564, 50564, 13157, 11, 562, 314, 307, 3671, 472, 11, 341, 486, 797, 1813, 4018, 293, 484, 5893, 2771, 445, 10094, 13, 50864, 50864, 400, 46506, 341, 1669, 2020, 570, 341, 307, 257, 1266, 39, 34090, 13, 51064, 51064, 400, 437, 311, 2737, 307, 498, 1080, 5598, 307, 588, 1998, 281, 472, 11, 550, 321, 366, 294, 264, 6838, 295, 341, 1266, 39, 13, 51414, 51414, 400, 370, 4473, 1936, 264, 4846, 307, 406, 516, 281, 2712, 264, 5598, 295, 264, 1266, 39, 886, 709, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.051897460974535896, "compression_ratio": 1.6442687747035574, "no_speech_prob": 4.092863491678145e-06}, {"id": 198, "seek": 101200, "start": 1012.0, "end": 1015.0, "text": " because it's in a flat region of the 10H.", "tokens": [50364, 570, 309, 311, 294, 257, 4962, 4458, 295, 264, 1266, 39, 13, 50514, 50514, 400, 370, 4412, 456, 311, 572, 2712, 322, 264, 4470, 13, 50664, 50664, 400, 370, 6451, 264, 17443, 293, 264, 32152, 2051, 365, 341, 1266, 39, 34090, 360, 406, 2712, 264, 4470, 51014, 51014, 570, 264, 5598, 295, 341, 1266, 39, 4985, 307, 294, 264, 4962, 4458, 295, 264, 1266, 39, 293, 456, 311, 572, 6503, 13, 51214, 51214, 492, 393, 312, 4473, 552, 4461, 321, 528, 293, 264, 4470, 307, 406, 15653, 13, 51464, 51464, 407, 300, 311, 1071, 636, 281, 20833, 300, 6451, 264, 16235, 576, 312, 1936, 4018, 13, 467, 3161, 16423, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07934412203337017, "compression_ratio": 1.8073770491803278, "no_speech_prob": 1.241096560988808e-05}, {"id": 199, "seek": 101200, "start": 1015.0, "end": 1018.0, "text": " And so therefore there's no impact on the loss.", "tokens": [50364, 570, 309, 311, 294, 257, 4962, 4458, 295, 264, 1266, 39, 13, 50514, 50514, 400, 370, 4412, 456, 311, 572, 2712, 322, 264, 4470, 13, 50664, 50664, 400, 370, 6451, 264, 17443, 293, 264, 32152, 2051, 365, 341, 1266, 39, 34090, 360, 406, 2712, 264, 4470, 51014, 51014, 570, 264, 5598, 295, 341, 1266, 39, 4985, 307, 294, 264, 4962, 4458, 295, 264, 1266, 39, 293, 456, 311, 572, 6503, 13, 51214, 51214, 492, 393, 312, 4473, 552, 4461, 321, 528, 293, 264, 4470, 307, 406, 15653, 13, 51464, 51464, 407, 300, 311, 1071, 636, 281, 20833, 300, 6451, 264, 16235, 576, 312, 1936, 4018, 13, 467, 3161, 16423, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07934412203337017, "compression_ratio": 1.8073770491803278, "no_speech_prob": 1.241096560988808e-05}, {"id": 200, "seek": 101200, "start": 1018.0, "end": 1025.0, "text": " And so indeed the weights and the biases along with this 10H neuron do not impact the loss", "tokens": [50364, 570, 309, 311, 294, 257, 4962, 4458, 295, 264, 1266, 39, 13, 50514, 50514, 400, 370, 4412, 456, 311, 572, 2712, 322, 264, 4470, 13, 50664, 50664, 400, 370, 6451, 264, 17443, 293, 264, 32152, 2051, 365, 341, 1266, 39, 34090, 360, 406, 2712, 264, 4470, 51014, 51014, 570, 264, 5598, 295, 341, 1266, 39, 4985, 307, 294, 264, 4962, 4458, 295, 264, 1266, 39, 293, 456, 311, 572, 6503, 13, 51214, 51214, 492, 393, 312, 4473, 552, 4461, 321, 528, 293, 264, 4470, 307, 406, 15653, 13, 51464, 51464, 407, 300, 311, 1071, 636, 281, 20833, 300, 6451, 264, 16235, 576, 312, 1936, 4018, 13, 467, 3161, 16423, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07934412203337017, "compression_ratio": 1.8073770491803278, "no_speech_prob": 1.241096560988808e-05}, {"id": 201, "seek": 101200, "start": 1025.0, "end": 1029.0, "text": " because the output of this 10H unit is in the flat region of the 10H and there's no influence.", "tokens": [50364, 570, 309, 311, 294, 257, 4962, 4458, 295, 264, 1266, 39, 13, 50514, 50514, 400, 370, 4412, 456, 311, 572, 2712, 322, 264, 4470, 13, 50664, 50664, 400, 370, 6451, 264, 17443, 293, 264, 32152, 2051, 365, 341, 1266, 39, 34090, 360, 406, 2712, 264, 4470, 51014, 51014, 570, 264, 5598, 295, 341, 1266, 39, 4985, 307, 294, 264, 4962, 4458, 295, 264, 1266, 39, 293, 456, 311, 572, 6503, 13, 51214, 51214, 492, 393, 312, 4473, 552, 4461, 321, 528, 293, 264, 4470, 307, 406, 15653, 13, 51464, 51464, 407, 300, 311, 1071, 636, 281, 20833, 300, 6451, 264, 16235, 576, 312, 1936, 4018, 13, 467, 3161, 16423, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07934412203337017, "compression_ratio": 1.8073770491803278, "no_speech_prob": 1.241096560988808e-05}, {"id": 202, "seek": 101200, "start": 1029.0, "end": 1034.0, "text": " We can be changing them however we want and the loss is not impacted.", "tokens": [50364, 570, 309, 311, 294, 257, 4962, 4458, 295, 264, 1266, 39, 13, 50514, 50514, 400, 370, 4412, 456, 311, 572, 2712, 322, 264, 4470, 13, 50664, 50664, 400, 370, 6451, 264, 17443, 293, 264, 32152, 2051, 365, 341, 1266, 39, 34090, 360, 406, 2712, 264, 4470, 51014, 51014, 570, 264, 5598, 295, 341, 1266, 39, 4985, 307, 294, 264, 4962, 4458, 295, 264, 1266, 39, 293, 456, 311, 572, 6503, 13, 51214, 51214, 492, 393, 312, 4473, 552, 4461, 321, 528, 293, 264, 4470, 307, 406, 15653, 13, 51464, 51464, 407, 300, 311, 1071, 636, 281, 20833, 300, 6451, 264, 16235, 576, 312, 1936, 4018, 13, 467, 3161, 16423, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07934412203337017, "compression_ratio": 1.8073770491803278, "no_speech_prob": 1.241096560988808e-05}, {"id": 203, "seek": 101200, "start": 1034.0, "end": 1040.0, "text": " So that's another way to justify that indeed the gradient would be basically zero. It vanishes.", "tokens": [50364, 570, 309, 311, 294, 257, 4962, 4458, 295, 264, 1266, 39, 13, 50514, 50514, 400, 370, 4412, 456, 311, 572, 2712, 322, 264, 4470, 13, 50664, 50664, 400, 370, 6451, 264, 17443, 293, 264, 32152, 2051, 365, 341, 1266, 39, 34090, 360, 406, 2712, 264, 4470, 51014, 51014, 570, 264, 5598, 295, 341, 1266, 39, 4985, 307, 294, 264, 4962, 4458, 295, 264, 1266, 39, 293, 456, 311, 572, 6503, 13, 51214, 51214, 492, 393, 312, 4473, 552, 4461, 321, 528, 293, 264, 4470, 307, 406, 15653, 13, 51464, 51464, 407, 300, 311, 1071, 636, 281, 20833, 300, 6451, 264, 16235, 576, 312, 1936, 4018, 13, 467, 3161, 16423, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07934412203337017, "compression_ratio": 1.8073770491803278, "no_speech_prob": 1.241096560988808e-05}, {"id": 204, "seek": 104000, "start": 1040.0, "end": 1047.0, "text": " Indeed, when T equals zero, we get one times out dot grad.", "tokens": [50364, 15061, 11, 562, 314, 6915, 4018, 11, 321, 483, 472, 1413, 484, 5893, 2771, 13, 50714, 50714, 407, 562, 264, 1266, 39, 2516, 322, 2293, 2158, 295, 4018, 11, 550, 484, 5893, 2771, 307, 445, 4678, 807, 13, 51064, 51064, 407, 1936, 437, 341, 307, 884, 11, 558, 11, 307, 498, 314, 307, 2681, 281, 4018, 11, 550, 341, 1266, 39, 4985, 307, 1333, 295, 294, 12596, 51464, 51464, 293, 16235, 445, 11335, 807, 13, 51564, 51564, 583, 264, 544, 291, 366, 294, 264, 4962, 28537, 11, 264, 544, 264, 16235, 307, 2339, 12219, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08830120587589765, "compression_ratio": 1.7075471698113207, "no_speech_prob": 8.397165402129758e-06}, {"id": 205, "seek": 104000, "start": 1047.0, "end": 1054.0, "text": " So when the 10H takes on exactly value of zero, then out dot grad is just passed through.", "tokens": [50364, 15061, 11, 562, 314, 6915, 4018, 11, 321, 483, 472, 1413, 484, 5893, 2771, 13, 50714, 50714, 407, 562, 264, 1266, 39, 2516, 322, 2293, 2158, 295, 4018, 11, 550, 484, 5893, 2771, 307, 445, 4678, 807, 13, 51064, 51064, 407, 1936, 437, 341, 307, 884, 11, 558, 11, 307, 498, 314, 307, 2681, 281, 4018, 11, 550, 341, 1266, 39, 4985, 307, 1333, 295, 294, 12596, 51464, 51464, 293, 16235, 445, 11335, 807, 13, 51564, 51564, 583, 264, 544, 291, 366, 294, 264, 4962, 28537, 11, 264, 544, 264, 16235, 307, 2339, 12219, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08830120587589765, "compression_ratio": 1.7075471698113207, "no_speech_prob": 8.397165402129758e-06}, {"id": 206, "seek": 104000, "start": 1054.0, "end": 1062.0, "text": " So basically what this is doing, right, is if T is equal to zero, then this 10H unit is sort of inactive", "tokens": [50364, 15061, 11, 562, 314, 6915, 4018, 11, 321, 483, 472, 1413, 484, 5893, 2771, 13, 50714, 50714, 407, 562, 264, 1266, 39, 2516, 322, 2293, 2158, 295, 4018, 11, 550, 484, 5893, 2771, 307, 445, 4678, 807, 13, 51064, 51064, 407, 1936, 437, 341, 307, 884, 11, 558, 11, 307, 498, 314, 307, 2681, 281, 4018, 11, 550, 341, 1266, 39, 4985, 307, 1333, 295, 294, 12596, 51464, 51464, 293, 16235, 445, 11335, 807, 13, 51564, 51564, 583, 264, 544, 291, 366, 294, 264, 4962, 28537, 11, 264, 544, 264, 16235, 307, 2339, 12219, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08830120587589765, "compression_ratio": 1.7075471698113207, "no_speech_prob": 8.397165402129758e-06}, {"id": 207, "seek": 104000, "start": 1062.0, "end": 1064.0, "text": " and gradient just passes through.", "tokens": [50364, 15061, 11, 562, 314, 6915, 4018, 11, 321, 483, 472, 1413, 484, 5893, 2771, 13, 50714, 50714, 407, 562, 264, 1266, 39, 2516, 322, 2293, 2158, 295, 4018, 11, 550, 484, 5893, 2771, 307, 445, 4678, 807, 13, 51064, 51064, 407, 1936, 437, 341, 307, 884, 11, 558, 11, 307, 498, 314, 307, 2681, 281, 4018, 11, 550, 341, 1266, 39, 4985, 307, 1333, 295, 294, 12596, 51464, 51464, 293, 16235, 445, 11335, 807, 13, 51564, 51564, 583, 264, 544, 291, 366, 294, 264, 4962, 28537, 11, 264, 544, 264, 16235, 307, 2339, 12219, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08830120587589765, "compression_ratio": 1.7075471698113207, "no_speech_prob": 8.397165402129758e-06}, {"id": 208, "seek": 104000, "start": 1064.0, "end": 1069.0, "text": " But the more you are in the flat tails, the more the gradient is squashed.", "tokens": [50364, 15061, 11, 562, 314, 6915, 4018, 11, 321, 483, 472, 1413, 484, 5893, 2771, 13, 50714, 50714, 407, 562, 264, 1266, 39, 2516, 322, 2293, 2158, 295, 4018, 11, 550, 484, 5893, 2771, 307, 445, 4678, 807, 13, 51064, 51064, 407, 1936, 437, 341, 307, 884, 11, 558, 11, 307, 498, 314, 307, 2681, 281, 4018, 11, 550, 341, 1266, 39, 4985, 307, 1333, 295, 294, 12596, 51464, 51464, 293, 16235, 445, 11335, 807, 13, 51564, 51564, 583, 264, 544, 291, 366, 294, 264, 4962, 28537, 11, 264, 544, 264, 16235, 307, 2339, 12219, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08830120587589765, "compression_ratio": 1.7075471698113207, "no_speech_prob": 8.397165402129758e-06}, {"id": 209, "seek": 106900, "start": 1069.0, "end": 1074.0, "text": " So in fact you'll see that the gradient flowing through 10H can only ever decrease", "tokens": [50364, 407, 294, 1186, 291, 603, 536, 300, 264, 16235, 13974, 807, 1266, 39, 393, 787, 1562, 11514, 50614, 50614, 293, 264, 2372, 300, 309, 24108, 307, 24969, 807, 257, 3732, 510, 50964, 50964, 5413, 322, 577, 1400, 291, 366, 294, 264, 4962, 28537, 295, 341, 1266, 39, 13, 51164, 51164, 400, 370, 300, 311, 733, 295, 437, 311, 2737, 510, 13, 51264, 51264, 400, 807, 341, 11, 264, 3136, 510, 307, 300, 498, 439, 295, 613, 23930, 389, 366, 294, 264, 4962, 10682, 295, 3671, 502, 293, 502, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07797779575470955, "compression_ratio": 1.6756756756756757, "no_speech_prob": 5.014613179810112e-06}, {"id": 210, "seek": 106900, "start": 1074.0, "end": 1081.0, "text": " and the amount that it decreases is proportional through a square here", "tokens": [50364, 407, 294, 1186, 291, 603, 536, 300, 264, 16235, 13974, 807, 1266, 39, 393, 787, 1562, 11514, 50614, 50614, 293, 264, 2372, 300, 309, 24108, 307, 24969, 807, 257, 3732, 510, 50964, 50964, 5413, 322, 577, 1400, 291, 366, 294, 264, 4962, 28537, 295, 341, 1266, 39, 13, 51164, 51164, 400, 370, 300, 311, 733, 295, 437, 311, 2737, 510, 13, 51264, 51264, 400, 807, 341, 11, 264, 3136, 510, 307, 300, 498, 439, 295, 613, 23930, 389, 366, 294, 264, 4962, 10682, 295, 3671, 502, 293, 502, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07797779575470955, "compression_ratio": 1.6756756756756757, "no_speech_prob": 5.014613179810112e-06}, {"id": 211, "seek": 106900, "start": 1081.0, "end": 1085.0, "text": " depending on how far you are in the flat tails of this 10H.", "tokens": [50364, 407, 294, 1186, 291, 603, 536, 300, 264, 16235, 13974, 807, 1266, 39, 393, 787, 1562, 11514, 50614, 50614, 293, 264, 2372, 300, 309, 24108, 307, 24969, 807, 257, 3732, 510, 50964, 50964, 5413, 322, 577, 1400, 291, 366, 294, 264, 4962, 28537, 295, 341, 1266, 39, 13, 51164, 51164, 400, 370, 300, 311, 733, 295, 437, 311, 2737, 510, 13, 51264, 51264, 400, 807, 341, 11, 264, 3136, 510, 307, 300, 498, 439, 295, 613, 23930, 389, 366, 294, 264, 4962, 10682, 295, 3671, 502, 293, 502, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07797779575470955, "compression_ratio": 1.6756756756756757, "no_speech_prob": 5.014613179810112e-06}, {"id": 212, "seek": 106900, "start": 1085.0, "end": 1087.0, "text": " And so that's kind of what's happening here.", "tokens": [50364, 407, 294, 1186, 291, 603, 536, 300, 264, 16235, 13974, 807, 1266, 39, 393, 787, 1562, 11514, 50614, 50614, 293, 264, 2372, 300, 309, 24108, 307, 24969, 807, 257, 3732, 510, 50964, 50964, 5413, 322, 577, 1400, 291, 366, 294, 264, 4962, 28537, 295, 341, 1266, 39, 13, 51164, 51164, 400, 370, 300, 311, 733, 295, 437, 311, 2737, 510, 13, 51264, 51264, 400, 807, 341, 11, 264, 3136, 510, 307, 300, 498, 439, 295, 613, 23930, 389, 366, 294, 264, 4962, 10682, 295, 3671, 502, 293, 502, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07797779575470955, "compression_ratio": 1.6756756756756757, "no_speech_prob": 5.014613179810112e-06}, {"id": 213, "seek": 106900, "start": 1087.0, "end": 1095.0, "text": " And through this, the concern here is that if all of these outputs H are in the flat regions of negative 1 and 1,", "tokens": [50364, 407, 294, 1186, 291, 603, 536, 300, 264, 16235, 13974, 807, 1266, 39, 393, 787, 1562, 11514, 50614, 50614, 293, 264, 2372, 300, 309, 24108, 307, 24969, 807, 257, 3732, 510, 50964, 50964, 5413, 322, 577, 1400, 291, 366, 294, 264, 4962, 28537, 295, 341, 1266, 39, 13, 51164, 51164, 400, 370, 300, 311, 733, 295, 437, 311, 2737, 510, 13, 51264, 51264, 400, 807, 341, 11, 264, 3136, 510, 307, 300, 498, 439, 295, 613, 23930, 389, 366, 294, 264, 4962, 10682, 295, 3671, 502, 293, 502, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07797779575470955, "compression_ratio": 1.6756756756756757, "no_speech_prob": 5.014613179810112e-06}, {"id": 214, "seek": 109500, "start": 1095.0, "end": 1101.0, "text": " then the gradients that are flowing through the network will just get destroyed at this layer.", "tokens": [50364, 550, 264, 2771, 2448, 300, 366, 13974, 807, 264, 3209, 486, 445, 483, 8937, 412, 341, 4583, 13, 50664, 50664, 823, 456, 307, 512, 37715, 278, 3125, 510, 293, 300, 321, 393, 767, 483, 257, 2020, 295, 264, 1154, 510, 382, 10002, 13, 51014, 51014, 286, 4114, 512, 3089, 510, 13, 51064, 51064, 400, 1936, 437, 321, 528, 281, 360, 510, 307, 321, 528, 281, 747, 257, 574, 412, 389, 11, 747, 264, 8236, 2158, 11, 51364, 51364, 293, 536, 577, 2049, 309, 307, 294, 264, 4962, 4458, 13, 407, 584, 5044, 813, 1958, 13, 8494, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08458261206598565, "compression_ratio": 1.6307053941908713, "no_speech_prob": 3.0894391329638893e-06}, {"id": 215, "seek": 109500, "start": 1101.0, "end": 1108.0, "text": " Now there is some redeeming quality here and that we can actually get a sense of the problem here as follows.", "tokens": [50364, 550, 264, 2771, 2448, 300, 366, 13974, 807, 264, 3209, 486, 445, 483, 8937, 412, 341, 4583, 13, 50664, 50664, 823, 456, 307, 512, 37715, 278, 3125, 510, 293, 300, 321, 393, 767, 483, 257, 2020, 295, 264, 1154, 510, 382, 10002, 13, 51014, 51014, 286, 4114, 512, 3089, 510, 13, 51064, 51064, 400, 1936, 437, 321, 528, 281, 360, 510, 307, 321, 528, 281, 747, 257, 574, 412, 389, 11, 747, 264, 8236, 2158, 11, 51364, 51364, 293, 536, 577, 2049, 309, 307, 294, 264, 4962, 4458, 13, 407, 584, 5044, 813, 1958, 13, 8494, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08458261206598565, "compression_ratio": 1.6307053941908713, "no_speech_prob": 3.0894391329638893e-06}, {"id": 216, "seek": 109500, "start": 1108.0, "end": 1109.0, "text": " I wrote some code here.", "tokens": [50364, 550, 264, 2771, 2448, 300, 366, 13974, 807, 264, 3209, 486, 445, 483, 8937, 412, 341, 4583, 13, 50664, 50664, 823, 456, 307, 512, 37715, 278, 3125, 510, 293, 300, 321, 393, 767, 483, 257, 2020, 295, 264, 1154, 510, 382, 10002, 13, 51014, 51014, 286, 4114, 512, 3089, 510, 13, 51064, 51064, 400, 1936, 437, 321, 528, 281, 360, 510, 307, 321, 528, 281, 747, 257, 574, 412, 389, 11, 747, 264, 8236, 2158, 11, 51364, 51364, 293, 536, 577, 2049, 309, 307, 294, 264, 4962, 4458, 13, 407, 584, 5044, 813, 1958, 13, 8494, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08458261206598565, "compression_ratio": 1.6307053941908713, "no_speech_prob": 3.0894391329638893e-06}, {"id": 217, "seek": 109500, "start": 1109.0, "end": 1115.0, "text": " And basically what we want to do here is we want to take a look at H, take the absolute value,", "tokens": [50364, 550, 264, 2771, 2448, 300, 366, 13974, 807, 264, 3209, 486, 445, 483, 8937, 412, 341, 4583, 13, 50664, 50664, 823, 456, 307, 512, 37715, 278, 3125, 510, 293, 300, 321, 393, 767, 483, 257, 2020, 295, 264, 1154, 510, 382, 10002, 13, 51014, 51014, 286, 4114, 512, 3089, 510, 13, 51064, 51064, 400, 1936, 437, 321, 528, 281, 360, 510, 307, 321, 528, 281, 747, 257, 574, 412, 389, 11, 747, 264, 8236, 2158, 11, 51364, 51364, 293, 536, 577, 2049, 309, 307, 294, 264, 4962, 4458, 13, 407, 584, 5044, 813, 1958, 13, 8494, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08458261206598565, "compression_ratio": 1.6307053941908713, "no_speech_prob": 3.0894391329638893e-06}, {"id": 218, "seek": 109500, "start": 1115.0, "end": 1122.0, "text": " and see how often it is in the flat region. So say greater than 0.99.", "tokens": [50364, 550, 264, 2771, 2448, 300, 366, 13974, 807, 264, 3209, 486, 445, 483, 8937, 412, 341, 4583, 13, 50664, 50664, 823, 456, 307, 512, 37715, 278, 3125, 510, 293, 300, 321, 393, 767, 483, 257, 2020, 295, 264, 1154, 510, 382, 10002, 13, 51014, 51014, 286, 4114, 512, 3089, 510, 13, 51064, 51064, 400, 1936, 437, 321, 528, 281, 360, 510, 307, 321, 528, 281, 747, 257, 574, 412, 389, 11, 747, 264, 8236, 2158, 11, 51364, 51364, 293, 536, 577, 2049, 309, 307, 294, 264, 4962, 4458, 13, 407, 584, 5044, 813, 1958, 13, 8494, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08458261206598565, "compression_ratio": 1.6307053941908713, "no_speech_prob": 3.0894391329638893e-06}, {"id": 219, "seek": 112200, "start": 1122.0, "end": 1126.0, "text": " And what you get is the following. And this is a Boolean tensor.", "tokens": [50364, 400, 437, 291, 483, 307, 264, 3480, 13, 400, 341, 307, 257, 23351, 28499, 40863, 13, 50564, 50564, 407, 294, 264, 23351, 28499, 40863, 291, 483, 257, 2418, 498, 341, 307, 2074, 293, 257, 2211, 498, 341, 307, 7908, 13, 50864, 50864, 400, 370, 1936, 437, 321, 362, 510, 307, 264, 8858, 5110, 293, 264, 2331, 7633, 22027, 13, 51114, 51114, 400, 321, 536, 300, 257, 688, 295, 341, 307, 2418, 13, 51264, 51264, 400, 437, 300, 311, 3585, 505, 307, 300, 439, 613, 1266, 39, 22027, 645, 588, 11, 588, 4967, 293, 436, 434, 294, 264, 4962, 6838, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05670863389968872, "compression_ratio": 1.7488584474885844, "no_speech_prob": 1.777755096554756e-05}, {"id": 220, "seek": 112200, "start": 1126.0, "end": 1132.0, "text": " So in the Boolean tensor you get a white if this is true and a black if this is false.", "tokens": [50364, 400, 437, 291, 483, 307, 264, 3480, 13, 400, 341, 307, 257, 23351, 28499, 40863, 13, 50564, 50564, 407, 294, 264, 23351, 28499, 40863, 291, 483, 257, 2418, 498, 341, 307, 2074, 293, 257, 2211, 498, 341, 307, 7908, 13, 50864, 50864, 400, 370, 1936, 437, 321, 362, 510, 307, 264, 8858, 5110, 293, 264, 2331, 7633, 22027, 13, 51114, 51114, 400, 321, 536, 300, 257, 688, 295, 341, 307, 2418, 13, 51264, 51264, 400, 437, 300, 311, 3585, 505, 307, 300, 439, 613, 1266, 39, 22027, 645, 588, 11, 588, 4967, 293, 436, 434, 294, 264, 4962, 6838, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05670863389968872, "compression_ratio": 1.7488584474885844, "no_speech_prob": 1.777755096554756e-05}, {"id": 221, "seek": 112200, "start": 1132.0, "end": 1137.0, "text": " And so basically what we have here is the 32 examples and the 200 hidden neurons.", "tokens": [50364, 400, 437, 291, 483, 307, 264, 3480, 13, 400, 341, 307, 257, 23351, 28499, 40863, 13, 50564, 50564, 407, 294, 264, 23351, 28499, 40863, 291, 483, 257, 2418, 498, 341, 307, 2074, 293, 257, 2211, 498, 341, 307, 7908, 13, 50864, 50864, 400, 370, 1936, 437, 321, 362, 510, 307, 264, 8858, 5110, 293, 264, 2331, 7633, 22027, 13, 51114, 51114, 400, 321, 536, 300, 257, 688, 295, 341, 307, 2418, 13, 51264, 51264, 400, 437, 300, 311, 3585, 505, 307, 300, 439, 613, 1266, 39, 22027, 645, 588, 11, 588, 4967, 293, 436, 434, 294, 264, 4962, 6838, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05670863389968872, "compression_ratio": 1.7488584474885844, "no_speech_prob": 1.777755096554756e-05}, {"id": 222, "seek": 112200, "start": 1137.0, "end": 1140.0, "text": " And we see that a lot of this is white.", "tokens": [50364, 400, 437, 291, 483, 307, 264, 3480, 13, 400, 341, 307, 257, 23351, 28499, 40863, 13, 50564, 50564, 407, 294, 264, 23351, 28499, 40863, 291, 483, 257, 2418, 498, 341, 307, 2074, 293, 257, 2211, 498, 341, 307, 7908, 13, 50864, 50864, 400, 370, 1936, 437, 321, 362, 510, 307, 264, 8858, 5110, 293, 264, 2331, 7633, 22027, 13, 51114, 51114, 400, 321, 536, 300, 257, 688, 295, 341, 307, 2418, 13, 51264, 51264, 400, 437, 300, 311, 3585, 505, 307, 300, 439, 613, 1266, 39, 22027, 645, 588, 11, 588, 4967, 293, 436, 434, 294, 264, 4962, 6838, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05670863389968872, "compression_ratio": 1.7488584474885844, "no_speech_prob": 1.777755096554756e-05}, {"id": 223, "seek": 112200, "start": 1140.0, "end": 1149.0, "text": " And what that's telling us is that all these 10H neurons were very, very active and they're in the flat tail.", "tokens": [50364, 400, 437, 291, 483, 307, 264, 3480, 13, 400, 341, 307, 257, 23351, 28499, 40863, 13, 50564, 50564, 407, 294, 264, 23351, 28499, 40863, 291, 483, 257, 2418, 498, 341, 307, 2074, 293, 257, 2211, 498, 341, 307, 7908, 13, 50864, 50864, 400, 370, 1936, 437, 321, 362, 510, 307, 264, 8858, 5110, 293, 264, 2331, 7633, 22027, 13, 51114, 51114, 400, 321, 536, 300, 257, 688, 295, 341, 307, 2418, 13, 51264, 51264, 400, 437, 300, 311, 3585, 505, 307, 300, 439, 613, 1266, 39, 22027, 645, 588, 11, 588, 4967, 293, 436, 434, 294, 264, 4962, 6838, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05670863389968872, "compression_ratio": 1.7488584474885844, "no_speech_prob": 1.777755096554756e-05}, {"id": 224, "seek": 114900, "start": 1149.0, "end": 1156.0, "text": " And so in all these cases, the backward gradient would get destroyed.", "tokens": [50364, 400, 370, 294, 439, 613, 3331, 11, 264, 23897, 16235, 576, 483, 8937, 13, 50714, 50714, 823, 321, 576, 312, 294, 257, 688, 295, 5253, 498, 337, 604, 472, 295, 613, 2331, 22027, 11, 51014, 51014, 498, 309, 390, 264, 1389, 300, 264, 2302, 7738, 307, 2418, 11, 570, 294, 300, 1389, 321, 362, 437, 311, 1219, 257, 3116, 34090, 13, 51364, 51364, 400, 341, 727, 312, 257, 1266, 39, 34090, 689, 264, 5883, 2144, 295, 264, 17443, 293, 264, 32152, 727, 312, 1270, 300, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07945506492357575, "compression_ratio": 1.6376146788990826, "no_speech_prob": 4.63777951154043e-06}, {"id": 225, "seek": 114900, "start": 1156.0, "end": 1162.0, "text": " Now we would be in a lot of trouble if for any one of these 200 neurons,", "tokens": [50364, 400, 370, 294, 439, 613, 3331, 11, 264, 23897, 16235, 576, 483, 8937, 13, 50714, 50714, 823, 321, 576, 312, 294, 257, 688, 295, 5253, 498, 337, 604, 472, 295, 613, 2331, 22027, 11, 51014, 51014, 498, 309, 390, 264, 1389, 300, 264, 2302, 7738, 307, 2418, 11, 570, 294, 300, 1389, 321, 362, 437, 311, 1219, 257, 3116, 34090, 13, 51364, 51364, 400, 341, 727, 312, 257, 1266, 39, 34090, 689, 264, 5883, 2144, 295, 264, 17443, 293, 264, 32152, 727, 312, 1270, 300, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07945506492357575, "compression_ratio": 1.6376146788990826, "no_speech_prob": 4.63777951154043e-06}, {"id": 226, "seek": 114900, "start": 1162.0, "end": 1169.0, "text": " if it was the case that the entire column is white, because in that case we have what's called a dead neuron.", "tokens": [50364, 400, 370, 294, 439, 613, 3331, 11, 264, 23897, 16235, 576, 483, 8937, 13, 50714, 50714, 823, 321, 576, 312, 294, 257, 688, 295, 5253, 498, 337, 604, 472, 295, 613, 2331, 22027, 11, 51014, 51014, 498, 309, 390, 264, 1389, 300, 264, 2302, 7738, 307, 2418, 11, 570, 294, 300, 1389, 321, 362, 437, 311, 1219, 257, 3116, 34090, 13, 51364, 51364, 400, 341, 727, 312, 257, 1266, 39, 34090, 689, 264, 5883, 2144, 295, 264, 17443, 293, 264, 32152, 727, 312, 1270, 300, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07945506492357575, "compression_ratio": 1.6376146788990826, "no_speech_prob": 4.63777951154043e-06}, {"id": 227, "seek": 114900, "start": 1169.0, "end": 1173.0, "text": " And this could be a 10H neuron where the initialization of the weights and the biases could be such that", "tokens": [50364, 400, 370, 294, 439, 613, 3331, 11, 264, 23897, 16235, 576, 483, 8937, 13, 50714, 50714, 823, 321, 576, 312, 294, 257, 688, 295, 5253, 498, 337, 604, 472, 295, 613, 2331, 22027, 11, 51014, 51014, 498, 309, 390, 264, 1389, 300, 264, 2302, 7738, 307, 2418, 11, 570, 294, 300, 1389, 321, 362, 437, 311, 1219, 257, 3116, 34090, 13, 51364, 51364, 400, 341, 727, 312, 257, 1266, 39, 34090, 689, 264, 5883, 2144, 295, 264, 17443, 293, 264, 32152, 727, 312, 1270, 300, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.07945506492357575, "compression_ratio": 1.6376146788990826, "no_speech_prob": 4.63777951154043e-06}, {"id": 228, "seek": 117300, "start": 1173.0, "end": 1180.0, "text": " no single example ever activates this 10H in the sort of active part of the 10H.", "tokens": [50364, 572, 2167, 1365, 1562, 43869, 341, 1266, 39, 294, 264, 1333, 295, 4967, 644, 295, 264, 1266, 39, 13, 50714, 50714, 759, 439, 264, 5110, 2117, 294, 264, 6838, 11, 550, 341, 34090, 486, 1128, 1466, 13, 467, 307, 257, 3116, 34090, 13, 51014, 51014, 400, 370, 445, 28949, 259, 3319, 341, 293, 1237, 337, 13766, 295, 2584, 2418, 11, 321, 536, 300, 341, 307, 406, 264, 1389, 13, 51414, 51414, 407, 286, 500, 380, 536, 257, 2167, 34090, 300, 307, 439, 295, 11, 291, 458, 11, 2418, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04237015016617313, "compression_ratio": 1.6445497630331753, "no_speech_prob": 6.747936822648626e-06}, {"id": 229, "seek": 117300, "start": 1180.0, "end": 1186.0, "text": " If all the examples land in the tail, then this neuron will never learn. It is a dead neuron.", "tokens": [50364, 572, 2167, 1365, 1562, 43869, 341, 1266, 39, 294, 264, 1333, 295, 4967, 644, 295, 264, 1266, 39, 13, 50714, 50714, 759, 439, 264, 5110, 2117, 294, 264, 6838, 11, 550, 341, 34090, 486, 1128, 1466, 13, 467, 307, 257, 3116, 34090, 13, 51014, 51014, 400, 370, 445, 28949, 259, 3319, 341, 293, 1237, 337, 13766, 295, 2584, 2418, 11, 321, 536, 300, 341, 307, 406, 264, 1389, 13, 51414, 51414, 407, 286, 500, 380, 536, 257, 2167, 34090, 300, 307, 439, 295, 11, 291, 458, 11, 2418, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04237015016617313, "compression_ratio": 1.6445497630331753, "no_speech_prob": 6.747936822648626e-06}, {"id": 230, "seek": 117300, "start": 1186.0, "end": 1194.0, "text": " And so just scrutinizing this and looking for columns of completely white, we see that this is not the case.", "tokens": [50364, 572, 2167, 1365, 1562, 43869, 341, 1266, 39, 294, 264, 1333, 295, 4967, 644, 295, 264, 1266, 39, 13, 50714, 50714, 759, 439, 264, 5110, 2117, 294, 264, 6838, 11, 550, 341, 34090, 486, 1128, 1466, 13, 467, 307, 257, 3116, 34090, 13, 51014, 51014, 400, 370, 445, 28949, 259, 3319, 341, 293, 1237, 337, 13766, 295, 2584, 2418, 11, 321, 536, 300, 341, 307, 406, 264, 1389, 13, 51414, 51414, 407, 286, 500, 380, 536, 257, 2167, 34090, 300, 307, 439, 295, 11, 291, 458, 11, 2418, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04237015016617313, "compression_ratio": 1.6445497630331753, "no_speech_prob": 6.747936822648626e-06}, {"id": 231, "seek": 117300, "start": 1194.0, "end": 1199.0, "text": " So I don't see a single neuron that is all of, you know, white.", "tokens": [50364, 572, 2167, 1365, 1562, 43869, 341, 1266, 39, 294, 264, 1333, 295, 4967, 644, 295, 264, 1266, 39, 13, 50714, 50714, 759, 439, 264, 5110, 2117, 294, 264, 6838, 11, 550, 341, 34090, 486, 1128, 1466, 13, 467, 307, 257, 3116, 34090, 13, 51014, 51014, 400, 370, 445, 28949, 259, 3319, 341, 293, 1237, 337, 13766, 295, 2584, 2418, 11, 321, 536, 300, 341, 307, 406, 264, 1389, 13, 51414, 51414, 407, 286, 500, 380, 536, 257, 2167, 34090, 300, 307, 439, 295, 11, 291, 458, 11, 2418, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04237015016617313, "compression_ratio": 1.6445497630331753, "no_speech_prob": 6.747936822648626e-06}, {"id": 232, "seek": 119900, "start": 1199.0, "end": 1204.0, "text": " And so therefore it is the case that for every one of these 10H neurons,", "tokens": [50364, 400, 370, 4412, 309, 307, 264, 1389, 300, 337, 633, 472, 295, 613, 1266, 39, 22027, 11, 50614, 50614, 321, 360, 362, 512, 5110, 300, 13615, 552, 294, 264, 4967, 644, 295, 264, 1266, 39, 13, 50864, 50864, 400, 370, 512, 2771, 2448, 486, 3095, 807, 293, 341, 34090, 486, 1466, 293, 264, 34090, 486, 1319, 293, 309, 486, 1286, 293, 309, 486, 360, 746, 13, 51214, 51214, 583, 291, 393, 2171, 483, 1803, 294, 3331, 689, 291, 362, 3116, 22027, 13, 51414, 51414, 400, 264, 636, 341, 50252, 307, 300, 337, 257, 1266, 39, 34090, 11, 341, 576, 312, 562, 572, 1871, 437, 15743, 291, 5452, 294, 490, 428, 1412, 992, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05746864661192282, "compression_ratio": 1.8505747126436782, "no_speech_prob": 4.78500805911608e-06}, {"id": 233, "seek": 119900, "start": 1204.0, "end": 1209.0, "text": " we do have some examples that activate them in the active part of the 10H.", "tokens": [50364, 400, 370, 4412, 309, 307, 264, 1389, 300, 337, 633, 472, 295, 613, 1266, 39, 22027, 11, 50614, 50614, 321, 360, 362, 512, 5110, 300, 13615, 552, 294, 264, 4967, 644, 295, 264, 1266, 39, 13, 50864, 50864, 400, 370, 512, 2771, 2448, 486, 3095, 807, 293, 341, 34090, 486, 1466, 293, 264, 34090, 486, 1319, 293, 309, 486, 1286, 293, 309, 486, 360, 746, 13, 51214, 51214, 583, 291, 393, 2171, 483, 1803, 294, 3331, 689, 291, 362, 3116, 22027, 13, 51414, 51414, 400, 264, 636, 341, 50252, 307, 300, 337, 257, 1266, 39, 34090, 11, 341, 576, 312, 562, 572, 1871, 437, 15743, 291, 5452, 294, 490, 428, 1412, 992, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05746864661192282, "compression_ratio": 1.8505747126436782, "no_speech_prob": 4.78500805911608e-06}, {"id": 234, "seek": 119900, "start": 1209.0, "end": 1216.0, "text": " And so some gradients will flow through and this neuron will learn and the neuron will change and it will move and it will do something.", "tokens": [50364, 400, 370, 4412, 309, 307, 264, 1389, 300, 337, 633, 472, 295, 613, 1266, 39, 22027, 11, 50614, 50614, 321, 360, 362, 512, 5110, 300, 13615, 552, 294, 264, 4967, 644, 295, 264, 1266, 39, 13, 50864, 50864, 400, 370, 512, 2771, 2448, 486, 3095, 807, 293, 341, 34090, 486, 1466, 293, 264, 34090, 486, 1319, 293, 309, 486, 1286, 293, 309, 486, 360, 746, 13, 51214, 51214, 583, 291, 393, 2171, 483, 1803, 294, 3331, 689, 291, 362, 3116, 22027, 13, 51414, 51414, 400, 264, 636, 341, 50252, 307, 300, 337, 257, 1266, 39, 34090, 11, 341, 576, 312, 562, 572, 1871, 437, 15743, 291, 5452, 294, 490, 428, 1412, 992, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05746864661192282, "compression_ratio": 1.8505747126436782, "no_speech_prob": 4.78500805911608e-06}, {"id": 235, "seek": 119900, "start": 1216.0, "end": 1220.0, "text": " But you can sometimes get yourself in cases where you have dead neurons.", "tokens": [50364, 400, 370, 4412, 309, 307, 264, 1389, 300, 337, 633, 472, 295, 613, 1266, 39, 22027, 11, 50614, 50614, 321, 360, 362, 512, 5110, 300, 13615, 552, 294, 264, 4967, 644, 295, 264, 1266, 39, 13, 50864, 50864, 400, 370, 512, 2771, 2448, 486, 3095, 807, 293, 341, 34090, 486, 1466, 293, 264, 34090, 486, 1319, 293, 309, 486, 1286, 293, 309, 486, 360, 746, 13, 51214, 51214, 583, 291, 393, 2171, 483, 1803, 294, 3331, 689, 291, 362, 3116, 22027, 13, 51414, 51414, 400, 264, 636, 341, 50252, 307, 300, 337, 257, 1266, 39, 34090, 11, 341, 576, 312, 562, 572, 1871, 437, 15743, 291, 5452, 294, 490, 428, 1412, 992, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05746864661192282, "compression_ratio": 1.8505747126436782, "no_speech_prob": 4.78500805911608e-06}, {"id": 236, "seek": 119900, "start": 1220.0, "end": 1227.0, "text": " And the way this manifests is that for a 10H neuron, this would be when no matter what inputs you plug in from your data set,", "tokens": [50364, 400, 370, 4412, 309, 307, 264, 1389, 300, 337, 633, 472, 295, 613, 1266, 39, 22027, 11, 50614, 50614, 321, 360, 362, 512, 5110, 300, 13615, 552, 294, 264, 4967, 644, 295, 264, 1266, 39, 13, 50864, 50864, 400, 370, 512, 2771, 2448, 486, 3095, 807, 293, 341, 34090, 486, 1466, 293, 264, 34090, 486, 1319, 293, 309, 486, 1286, 293, 309, 486, 360, 746, 13, 51214, 51214, 583, 291, 393, 2171, 483, 1803, 294, 3331, 689, 291, 362, 3116, 22027, 13, 51414, 51414, 400, 264, 636, 341, 50252, 307, 300, 337, 257, 1266, 39, 34090, 11, 341, 576, 312, 562, 572, 1871, 437, 15743, 291, 5452, 294, 490, 428, 1412, 992, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05746864661192282, "compression_ratio": 1.8505747126436782, "no_speech_prob": 4.78500805911608e-06}, {"id": 237, "seek": 122700, "start": 1227.0, "end": 1231.0, "text": " this 10H neuron always fires completely one or completely negative one.", "tokens": [50364, 341, 1266, 39, 34090, 1009, 15044, 2584, 472, 420, 2584, 3671, 472, 13, 50564, 50564, 400, 550, 309, 486, 445, 406, 1466, 570, 439, 264, 2771, 2448, 486, 312, 445, 4018, 292, 484, 13, 50814, 50814, 639, 307, 2074, 406, 445, 337, 1266, 39, 11, 457, 337, 257, 688, 295, 661, 2107, 28263, 1088, 300, 561, 764, 294, 18161, 9590, 13, 51064, 51064, 407, 321, 3297, 764, 1266, 39, 257, 688, 11, 457, 4556, 3280, 327, 486, 362, 264, 1900, 912, 2734, 570, 309, 307, 257, 2339, 11077, 34090, 13, 51364, 51364, 400, 370, 264, 912, 486, 312, 2074, 337, 4556, 3280, 327, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05776083910906756, "compression_ratio": 1.683127572016461, "no_speech_prob": 7.183085926953936e-06}, {"id": 238, "seek": 122700, "start": 1231.0, "end": 1236.0, "text": " And then it will just not learn because all the gradients will be just zeroed out.", "tokens": [50364, 341, 1266, 39, 34090, 1009, 15044, 2584, 472, 420, 2584, 3671, 472, 13, 50564, 50564, 400, 550, 309, 486, 445, 406, 1466, 570, 439, 264, 2771, 2448, 486, 312, 445, 4018, 292, 484, 13, 50814, 50814, 639, 307, 2074, 406, 445, 337, 1266, 39, 11, 457, 337, 257, 688, 295, 661, 2107, 28263, 1088, 300, 561, 764, 294, 18161, 9590, 13, 51064, 51064, 407, 321, 3297, 764, 1266, 39, 257, 688, 11, 457, 4556, 3280, 327, 486, 362, 264, 1900, 912, 2734, 570, 309, 307, 257, 2339, 11077, 34090, 13, 51364, 51364, 400, 370, 264, 912, 486, 312, 2074, 337, 4556, 3280, 327, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05776083910906756, "compression_ratio": 1.683127572016461, "no_speech_prob": 7.183085926953936e-06}, {"id": 239, "seek": 122700, "start": 1236.0, "end": 1241.0, "text": " This is true not just for 10H, but for a lot of other nonlinearities that people use in neural networks.", "tokens": [50364, 341, 1266, 39, 34090, 1009, 15044, 2584, 472, 420, 2584, 3671, 472, 13, 50564, 50564, 400, 550, 309, 486, 445, 406, 1466, 570, 439, 264, 2771, 2448, 486, 312, 445, 4018, 292, 484, 13, 50814, 50814, 639, 307, 2074, 406, 445, 337, 1266, 39, 11, 457, 337, 257, 688, 295, 661, 2107, 28263, 1088, 300, 561, 764, 294, 18161, 9590, 13, 51064, 51064, 407, 321, 3297, 764, 1266, 39, 257, 688, 11, 457, 4556, 3280, 327, 486, 362, 264, 1900, 912, 2734, 570, 309, 307, 257, 2339, 11077, 34090, 13, 51364, 51364, 400, 370, 264, 912, 486, 312, 2074, 337, 4556, 3280, 327, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05776083910906756, "compression_ratio": 1.683127572016461, "no_speech_prob": 7.183085926953936e-06}, {"id": 240, "seek": 122700, "start": 1241.0, "end": 1247.0, "text": " So we certainly use 10H a lot, but sigmoid will have the exact same issue because it is a squashing neuron.", "tokens": [50364, 341, 1266, 39, 34090, 1009, 15044, 2584, 472, 420, 2584, 3671, 472, 13, 50564, 50564, 400, 550, 309, 486, 445, 406, 1466, 570, 439, 264, 2771, 2448, 486, 312, 445, 4018, 292, 484, 13, 50814, 50814, 639, 307, 2074, 406, 445, 337, 1266, 39, 11, 457, 337, 257, 688, 295, 661, 2107, 28263, 1088, 300, 561, 764, 294, 18161, 9590, 13, 51064, 51064, 407, 321, 3297, 764, 1266, 39, 257, 688, 11, 457, 4556, 3280, 327, 486, 362, 264, 1900, 912, 2734, 570, 309, 307, 257, 2339, 11077, 34090, 13, 51364, 51364, 400, 370, 264, 912, 486, 312, 2074, 337, 4556, 3280, 327, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05776083910906756, "compression_ratio": 1.683127572016461, "no_speech_prob": 7.183085926953936e-06}, {"id": 241, "seek": 122700, "start": 1247.0, "end": 1250.0, "text": " And so the same will be true for sigmoid.", "tokens": [50364, 341, 1266, 39, 34090, 1009, 15044, 2584, 472, 420, 2584, 3671, 472, 13, 50564, 50564, 400, 550, 309, 486, 445, 406, 1466, 570, 439, 264, 2771, 2448, 486, 312, 445, 4018, 292, 484, 13, 50814, 50814, 639, 307, 2074, 406, 445, 337, 1266, 39, 11, 457, 337, 257, 688, 295, 661, 2107, 28263, 1088, 300, 561, 764, 294, 18161, 9590, 13, 51064, 51064, 407, 321, 3297, 764, 1266, 39, 257, 688, 11, 457, 4556, 3280, 327, 486, 362, 264, 1900, 912, 2734, 570, 309, 307, 257, 2339, 11077, 34090, 13, 51364, 51364, 400, 370, 264, 912, 486, 312, 2074, 337, 4556, 3280, 327, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.05776083910906756, "compression_ratio": 1.683127572016461, "no_speech_prob": 7.183085926953936e-06}, {"id": 242, "seek": 125000, "start": 1250.0, "end": 1257.0, "text": " But basically the same will actually apply to sigmoid.", "tokens": [50364, 583, 1936, 264, 912, 486, 767, 3079, 281, 4556, 3280, 327, 13, 50714, 50714, 440, 912, 486, 611, 3079, 281, 1300, 43, 52, 13, 50814, 50814, 407, 1300, 43, 52, 575, 257, 2584, 4962, 4458, 510, 2507, 4018, 13, 51014, 51014, 407, 498, 291, 362, 257, 1300, 43, 52, 34090, 11, 550, 309, 307, 257, 1320, 807, 498, 309, 307, 3353, 13, 51264, 51264, 400, 498, 264, 659, 12, 23397, 399, 307, 3671, 11, 309, 486, 445, 5309, 309, 766, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09254392175113453, "compression_ratio": 1.5243243243243243, "no_speech_prob": 4.710518169304123e-06}, {"id": 243, "seek": 125000, "start": 1257.0, "end": 1259.0, "text": " The same will also apply to ReLU.", "tokens": [50364, 583, 1936, 264, 912, 486, 767, 3079, 281, 4556, 3280, 327, 13, 50714, 50714, 440, 912, 486, 611, 3079, 281, 1300, 43, 52, 13, 50814, 50814, 407, 1300, 43, 52, 575, 257, 2584, 4962, 4458, 510, 2507, 4018, 13, 51014, 51014, 407, 498, 291, 362, 257, 1300, 43, 52, 34090, 11, 550, 309, 307, 257, 1320, 807, 498, 309, 307, 3353, 13, 51264, 51264, 400, 498, 264, 659, 12, 23397, 399, 307, 3671, 11, 309, 486, 445, 5309, 309, 766, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09254392175113453, "compression_ratio": 1.5243243243243243, "no_speech_prob": 4.710518169304123e-06}, {"id": 244, "seek": 125000, "start": 1259.0, "end": 1263.0, "text": " So ReLU has a completely flat region here below zero.", "tokens": [50364, 583, 1936, 264, 912, 486, 767, 3079, 281, 4556, 3280, 327, 13, 50714, 50714, 440, 912, 486, 611, 3079, 281, 1300, 43, 52, 13, 50814, 50814, 407, 1300, 43, 52, 575, 257, 2584, 4962, 4458, 510, 2507, 4018, 13, 51014, 51014, 407, 498, 291, 362, 257, 1300, 43, 52, 34090, 11, 550, 309, 307, 257, 1320, 807, 498, 309, 307, 3353, 13, 51264, 51264, 400, 498, 264, 659, 12, 23397, 399, 307, 3671, 11, 309, 486, 445, 5309, 309, 766, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09254392175113453, "compression_ratio": 1.5243243243243243, "no_speech_prob": 4.710518169304123e-06}, {"id": 245, "seek": 125000, "start": 1263.0, "end": 1268.0, "text": " So if you have a ReLU neuron, then it is a pass through if it is positive.", "tokens": [50364, 583, 1936, 264, 912, 486, 767, 3079, 281, 4556, 3280, 327, 13, 50714, 50714, 440, 912, 486, 611, 3079, 281, 1300, 43, 52, 13, 50814, 50814, 407, 1300, 43, 52, 575, 257, 2584, 4962, 4458, 510, 2507, 4018, 13, 51014, 51014, 407, 498, 291, 362, 257, 1300, 43, 52, 34090, 11, 550, 309, 307, 257, 1320, 807, 498, 309, 307, 3353, 13, 51264, 51264, 400, 498, 264, 659, 12, 23397, 399, 307, 3671, 11, 309, 486, 445, 5309, 309, 766, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09254392175113453, "compression_ratio": 1.5243243243243243, "no_speech_prob": 4.710518169304123e-06}, {"id": 246, "seek": 125000, "start": 1268.0, "end": 1272.0, "text": " And if the pre-activation is negative, it will just shut it off.", "tokens": [50364, 583, 1936, 264, 912, 486, 767, 3079, 281, 4556, 3280, 327, 13, 50714, 50714, 440, 912, 486, 611, 3079, 281, 1300, 43, 52, 13, 50814, 50814, 407, 1300, 43, 52, 575, 257, 2584, 4962, 4458, 510, 2507, 4018, 13, 51014, 51014, 407, 498, 291, 362, 257, 1300, 43, 52, 34090, 11, 550, 309, 307, 257, 1320, 807, 498, 309, 307, 3353, 13, 51264, 51264, 400, 498, 264, 659, 12, 23397, 399, 307, 3671, 11, 309, 486, 445, 5309, 309, 766, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09254392175113453, "compression_ratio": 1.5243243243243243, "no_speech_prob": 4.710518169304123e-06}, {"id": 247, "seek": 127200, "start": 1272.0, "end": 1280.0, "text": " Since the region here is completely flat, then during backpropagation, this would be exactly zeroing out the gradient.", "tokens": [50364, 4162, 264, 4458, 510, 307, 2584, 4962, 11, 550, 1830, 646, 79, 1513, 559, 399, 11, 341, 576, 312, 2293, 4018, 278, 484, 264, 16235, 13, 50764, 50764, 1743, 439, 295, 264, 16235, 576, 312, 992, 2293, 281, 4018, 2602, 295, 445, 411, 257, 588, 11, 588, 1359, 1230, 11, 5413, 322, 577, 3353, 420, 3671, 256, 307, 13, 51164, 51164, 400, 370, 291, 393, 483, 11, 337, 1365, 11, 257, 3116, 1300, 43, 52, 34090, 13, 51314, 51314, 400, 257, 3116, 1300, 43, 52, 34090, 576, 1936, 574, 411, 485, 51514, 51514, 8537, 437, 309, 307, 11, 307, 498, 257, 34090, 365, 257, 1300, 43, 52, 2107, 1889, 17409, 1128, 43869, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10587122501471104, "compression_ratio": 1.7230769230769232, "no_speech_prob": 2.2602548597205896e-06}, {"id": 248, "seek": 127200, "start": 1280.0, "end": 1288.0, "text": " Like all of the gradient would be set exactly to zero instead of just like a very, very small number, depending on how positive or negative t is.", "tokens": [50364, 4162, 264, 4458, 510, 307, 2584, 4962, 11, 550, 1830, 646, 79, 1513, 559, 399, 11, 341, 576, 312, 2293, 4018, 278, 484, 264, 16235, 13, 50764, 50764, 1743, 439, 295, 264, 16235, 576, 312, 992, 2293, 281, 4018, 2602, 295, 445, 411, 257, 588, 11, 588, 1359, 1230, 11, 5413, 322, 577, 3353, 420, 3671, 256, 307, 13, 51164, 51164, 400, 370, 291, 393, 483, 11, 337, 1365, 11, 257, 3116, 1300, 43, 52, 34090, 13, 51314, 51314, 400, 257, 3116, 1300, 43, 52, 34090, 576, 1936, 574, 411, 485, 51514, 51514, 8537, 437, 309, 307, 11, 307, 498, 257, 34090, 365, 257, 1300, 43, 52, 2107, 1889, 17409, 1128, 43869, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10587122501471104, "compression_ratio": 1.7230769230769232, "no_speech_prob": 2.2602548597205896e-06}, {"id": 249, "seek": 127200, "start": 1288.0, "end": 1291.0, "text": " And so you can get, for example, a dead ReLU neuron.", "tokens": [50364, 4162, 264, 4458, 510, 307, 2584, 4962, 11, 550, 1830, 646, 79, 1513, 559, 399, 11, 341, 576, 312, 2293, 4018, 278, 484, 264, 16235, 13, 50764, 50764, 1743, 439, 295, 264, 16235, 576, 312, 992, 2293, 281, 4018, 2602, 295, 445, 411, 257, 588, 11, 588, 1359, 1230, 11, 5413, 322, 577, 3353, 420, 3671, 256, 307, 13, 51164, 51164, 400, 370, 291, 393, 483, 11, 337, 1365, 11, 257, 3116, 1300, 43, 52, 34090, 13, 51314, 51314, 400, 257, 3116, 1300, 43, 52, 34090, 576, 1936, 574, 411, 485, 51514, 51514, 8537, 437, 309, 307, 11, 307, 498, 257, 34090, 365, 257, 1300, 43, 52, 2107, 1889, 17409, 1128, 43869, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10587122501471104, "compression_ratio": 1.7230769230769232, "no_speech_prob": 2.2602548597205896e-06}, {"id": 250, "seek": 127200, "start": 1291.0, "end": 1295.0, "text": " And a dead ReLU neuron would basically look like...", "tokens": [50364, 4162, 264, 4458, 510, 307, 2584, 4962, 11, 550, 1830, 646, 79, 1513, 559, 399, 11, 341, 576, 312, 2293, 4018, 278, 484, 264, 16235, 13, 50764, 50764, 1743, 439, 295, 264, 16235, 576, 312, 992, 2293, 281, 4018, 2602, 295, 445, 411, 257, 588, 11, 588, 1359, 1230, 11, 5413, 322, 577, 3353, 420, 3671, 256, 307, 13, 51164, 51164, 400, 370, 291, 393, 483, 11, 337, 1365, 11, 257, 3116, 1300, 43, 52, 34090, 13, 51314, 51314, 400, 257, 3116, 1300, 43, 52, 34090, 576, 1936, 574, 411, 485, 51514, 51514, 8537, 437, 309, 307, 11, 307, 498, 257, 34090, 365, 257, 1300, 43, 52, 2107, 1889, 17409, 1128, 43869, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10587122501471104, "compression_ratio": 1.7230769230769232, "no_speech_prob": 2.2602548597205896e-06}, {"id": 251, "seek": 127200, "start": 1295.0, "end": 1301.0, "text": " Basically what it is, is if a neuron with a ReLU nonlinearity never activates,", "tokens": [50364, 4162, 264, 4458, 510, 307, 2584, 4962, 11, 550, 1830, 646, 79, 1513, 559, 399, 11, 341, 576, 312, 2293, 4018, 278, 484, 264, 16235, 13, 50764, 50764, 1743, 439, 295, 264, 16235, 576, 312, 992, 2293, 281, 4018, 2602, 295, 445, 411, 257, 588, 11, 588, 1359, 1230, 11, 5413, 322, 577, 3353, 420, 3671, 256, 307, 13, 51164, 51164, 400, 370, 291, 393, 483, 11, 337, 1365, 11, 257, 3116, 1300, 43, 52, 34090, 13, 51314, 51314, 400, 257, 3116, 1300, 43, 52, 34090, 576, 1936, 574, 411, 485, 51514, 51514, 8537, 437, 309, 307, 11, 307, 498, 257, 34090, 365, 257, 1300, 43, 52, 2107, 1889, 17409, 1128, 43869, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10587122501471104, "compression_ratio": 1.7230769230769232, "no_speech_prob": 2.2602548597205896e-06}, {"id": 252, "seek": 130100, "start": 1301.0, "end": 1307.0, "text": " so for any examples that you plug in in the dataset, it never turns on, it is always in this flat region,", "tokens": [50364, 370, 337, 604, 5110, 300, 291, 5452, 294, 294, 264, 28872, 11, 309, 1128, 4523, 322, 11, 309, 307, 1009, 294, 341, 4962, 4458, 11, 50664, 50664, 550, 341, 1300, 43, 52, 34090, 307, 257, 3116, 34090, 13, 50764, 50764, 6953, 17443, 293, 12577, 486, 1128, 1466, 13, 50914, 50914, 814, 486, 1128, 483, 257, 16235, 570, 264, 34090, 1128, 18157, 13, 51064, 51064, 400, 341, 393, 2171, 1051, 412, 5883, 2144, 570, 264, 17443, 293, 264, 32152, 445, 652, 309, 370, 300, 538, 2931, 512, 22027, 366, 445, 5680, 3116, 13, 51414, 51414, 583, 309, 393, 611, 1051, 1830, 19618, 13, 51564, 51564, 759, 291, 362, 411, 257, 886, 1090, 295, 257, 2539, 3314, 11, 337, 1365, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08444950444911553, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.2410835552145727e-05}, {"id": 253, "seek": 130100, "start": 1307.0, "end": 1309.0, "text": " then this ReLU neuron is a dead neuron.", "tokens": [50364, 370, 337, 604, 5110, 300, 291, 5452, 294, 294, 264, 28872, 11, 309, 1128, 4523, 322, 11, 309, 307, 1009, 294, 341, 4962, 4458, 11, 50664, 50664, 550, 341, 1300, 43, 52, 34090, 307, 257, 3116, 34090, 13, 50764, 50764, 6953, 17443, 293, 12577, 486, 1128, 1466, 13, 50914, 50914, 814, 486, 1128, 483, 257, 16235, 570, 264, 34090, 1128, 18157, 13, 51064, 51064, 400, 341, 393, 2171, 1051, 412, 5883, 2144, 570, 264, 17443, 293, 264, 32152, 445, 652, 309, 370, 300, 538, 2931, 512, 22027, 366, 445, 5680, 3116, 13, 51414, 51414, 583, 309, 393, 611, 1051, 1830, 19618, 13, 51564, 51564, 759, 291, 362, 411, 257, 886, 1090, 295, 257, 2539, 3314, 11, 337, 1365, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08444950444911553, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.2410835552145727e-05}, {"id": 254, "seek": 130100, "start": 1309.0, "end": 1312.0, "text": " Its weights and bias will never learn.", "tokens": [50364, 370, 337, 604, 5110, 300, 291, 5452, 294, 294, 264, 28872, 11, 309, 1128, 4523, 322, 11, 309, 307, 1009, 294, 341, 4962, 4458, 11, 50664, 50664, 550, 341, 1300, 43, 52, 34090, 307, 257, 3116, 34090, 13, 50764, 50764, 6953, 17443, 293, 12577, 486, 1128, 1466, 13, 50914, 50914, 814, 486, 1128, 483, 257, 16235, 570, 264, 34090, 1128, 18157, 13, 51064, 51064, 400, 341, 393, 2171, 1051, 412, 5883, 2144, 570, 264, 17443, 293, 264, 32152, 445, 652, 309, 370, 300, 538, 2931, 512, 22027, 366, 445, 5680, 3116, 13, 51414, 51414, 583, 309, 393, 611, 1051, 1830, 19618, 13, 51564, 51564, 759, 291, 362, 411, 257, 886, 1090, 295, 257, 2539, 3314, 11, 337, 1365, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08444950444911553, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.2410835552145727e-05}, {"id": 255, "seek": 130100, "start": 1312.0, "end": 1315.0, "text": " They will never get a gradient because the neuron never activated.", "tokens": [50364, 370, 337, 604, 5110, 300, 291, 5452, 294, 294, 264, 28872, 11, 309, 1128, 4523, 322, 11, 309, 307, 1009, 294, 341, 4962, 4458, 11, 50664, 50664, 550, 341, 1300, 43, 52, 34090, 307, 257, 3116, 34090, 13, 50764, 50764, 6953, 17443, 293, 12577, 486, 1128, 1466, 13, 50914, 50914, 814, 486, 1128, 483, 257, 16235, 570, 264, 34090, 1128, 18157, 13, 51064, 51064, 400, 341, 393, 2171, 1051, 412, 5883, 2144, 570, 264, 17443, 293, 264, 32152, 445, 652, 309, 370, 300, 538, 2931, 512, 22027, 366, 445, 5680, 3116, 13, 51414, 51414, 583, 309, 393, 611, 1051, 1830, 19618, 13, 51564, 51564, 759, 291, 362, 411, 257, 886, 1090, 295, 257, 2539, 3314, 11, 337, 1365, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08444950444911553, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.2410835552145727e-05}, {"id": 256, "seek": 130100, "start": 1315.0, "end": 1322.0, "text": " And this can sometimes happen at initialization because the weights and the biases just make it so that by chance some neurons are just forever dead.", "tokens": [50364, 370, 337, 604, 5110, 300, 291, 5452, 294, 294, 264, 28872, 11, 309, 1128, 4523, 322, 11, 309, 307, 1009, 294, 341, 4962, 4458, 11, 50664, 50664, 550, 341, 1300, 43, 52, 34090, 307, 257, 3116, 34090, 13, 50764, 50764, 6953, 17443, 293, 12577, 486, 1128, 1466, 13, 50914, 50914, 814, 486, 1128, 483, 257, 16235, 570, 264, 34090, 1128, 18157, 13, 51064, 51064, 400, 341, 393, 2171, 1051, 412, 5883, 2144, 570, 264, 17443, 293, 264, 32152, 445, 652, 309, 370, 300, 538, 2931, 512, 22027, 366, 445, 5680, 3116, 13, 51414, 51414, 583, 309, 393, 611, 1051, 1830, 19618, 13, 51564, 51564, 759, 291, 362, 411, 257, 886, 1090, 295, 257, 2539, 3314, 11, 337, 1365, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08444950444911553, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.2410835552145727e-05}, {"id": 257, "seek": 130100, "start": 1322.0, "end": 1325.0, "text": " But it can also happen during optimization.", "tokens": [50364, 370, 337, 604, 5110, 300, 291, 5452, 294, 294, 264, 28872, 11, 309, 1128, 4523, 322, 11, 309, 307, 1009, 294, 341, 4962, 4458, 11, 50664, 50664, 550, 341, 1300, 43, 52, 34090, 307, 257, 3116, 34090, 13, 50764, 50764, 6953, 17443, 293, 12577, 486, 1128, 1466, 13, 50914, 50914, 814, 486, 1128, 483, 257, 16235, 570, 264, 34090, 1128, 18157, 13, 51064, 51064, 400, 341, 393, 2171, 1051, 412, 5883, 2144, 570, 264, 17443, 293, 264, 32152, 445, 652, 309, 370, 300, 538, 2931, 512, 22027, 366, 445, 5680, 3116, 13, 51414, 51414, 583, 309, 393, 611, 1051, 1830, 19618, 13, 51564, 51564, 759, 291, 362, 411, 257, 886, 1090, 295, 257, 2539, 3314, 11, 337, 1365, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08444950444911553, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.2410835552145727e-05}, {"id": 258, "seek": 130100, "start": 1325.0, "end": 1327.0, "text": " If you have like a too high of a learning rate, for example,", "tokens": [50364, 370, 337, 604, 5110, 300, 291, 5452, 294, 294, 264, 28872, 11, 309, 1128, 4523, 322, 11, 309, 307, 1009, 294, 341, 4962, 4458, 11, 50664, 50664, 550, 341, 1300, 43, 52, 34090, 307, 257, 3116, 34090, 13, 50764, 50764, 6953, 17443, 293, 12577, 486, 1128, 1466, 13, 50914, 50914, 814, 486, 1128, 483, 257, 16235, 570, 264, 34090, 1128, 18157, 13, 51064, 51064, 400, 341, 393, 2171, 1051, 412, 5883, 2144, 570, 264, 17443, 293, 264, 32152, 445, 652, 309, 370, 300, 538, 2931, 512, 22027, 366, 445, 5680, 3116, 13, 51414, 51414, 583, 309, 393, 611, 1051, 1830, 19618, 13, 51564, 51564, 759, 291, 362, 411, 257, 886, 1090, 295, 257, 2539, 3314, 11, 337, 1365, 11, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08444950444911553, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.2410835552145727e-05}, {"id": 259, "seek": 132700, "start": 1327.0, "end": 1333.0, "text": " sometimes you have these neurons that get too much of a gradient and they get knocked out off the data manifold.", "tokens": [50364, 2171, 291, 362, 613, 22027, 300, 483, 886, 709, 295, 257, 16235, 293, 436, 483, 16914, 484, 766, 264, 1412, 47138, 13, 50664, 50664, 400, 437, 2314, 307, 300, 490, 550, 322, 11, 572, 1365, 1562, 43869, 341, 34090, 13, 50914, 50914, 407, 341, 34090, 7023, 3116, 5680, 13, 50964, 50964, 407, 309, 311, 733, 295, 411, 257, 10996, 3567, 4344, 294, 257, 1575, 295, 257, 3209, 13, 51164, 51164, 400, 370, 2171, 437, 393, 1051, 307, 498, 428, 2539, 3314, 307, 588, 1090, 11, 337, 1365, 11, 51364, 51364, 293, 291, 362, 257, 18161, 2533, 365, 1300, 43, 52, 22027, 11, 291, 3847, 264, 18161, 2533, 293, 291, 483, 512, 1036, 4470, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06752335419089107, "compression_ratio": 1.7582417582417582, "no_speech_prob": 4.710764187620953e-06}, {"id": 260, "seek": 132700, "start": 1333.0, "end": 1338.0, "text": " And what happens is that from then on, no example ever activates this neuron.", "tokens": [50364, 2171, 291, 362, 613, 22027, 300, 483, 886, 709, 295, 257, 16235, 293, 436, 483, 16914, 484, 766, 264, 1412, 47138, 13, 50664, 50664, 400, 437, 2314, 307, 300, 490, 550, 322, 11, 572, 1365, 1562, 43869, 341, 34090, 13, 50914, 50914, 407, 341, 34090, 7023, 3116, 5680, 13, 50964, 50964, 407, 309, 311, 733, 295, 411, 257, 10996, 3567, 4344, 294, 257, 1575, 295, 257, 3209, 13, 51164, 51164, 400, 370, 2171, 437, 393, 1051, 307, 498, 428, 2539, 3314, 307, 588, 1090, 11, 337, 1365, 11, 51364, 51364, 293, 291, 362, 257, 18161, 2533, 365, 1300, 43, 52, 22027, 11, 291, 3847, 264, 18161, 2533, 293, 291, 483, 512, 1036, 4470, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06752335419089107, "compression_ratio": 1.7582417582417582, "no_speech_prob": 4.710764187620953e-06}, {"id": 261, "seek": 132700, "start": 1338.0, "end": 1339.0, "text": " So this neuron remains dead forever.", "tokens": [50364, 2171, 291, 362, 613, 22027, 300, 483, 886, 709, 295, 257, 16235, 293, 436, 483, 16914, 484, 766, 264, 1412, 47138, 13, 50664, 50664, 400, 437, 2314, 307, 300, 490, 550, 322, 11, 572, 1365, 1562, 43869, 341, 34090, 13, 50914, 50914, 407, 341, 34090, 7023, 3116, 5680, 13, 50964, 50964, 407, 309, 311, 733, 295, 411, 257, 10996, 3567, 4344, 294, 257, 1575, 295, 257, 3209, 13, 51164, 51164, 400, 370, 2171, 437, 393, 1051, 307, 498, 428, 2539, 3314, 307, 588, 1090, 11, 337, 1365, 11, 51364, 51364, 293, 291, 362, 257, 18161, 2533, 365, 1300, 43, 52, 22027, 11, 291, 3847, 264, 18161, 2533, 293, 291, 483, 512, 1036, 4470, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06752335419089107, "compression_ratio": 1.7582417582417582, "no_speech_prob": 4.710764187620953e-06}, {"id": 262, "seek": 132700, "start": 1339.0, "end": 1343.0, "text": " So it's kind of like a permanent brain damage in a mind of a network.", "tokens": [50364, 2171, 291, 362, 613, 22027, 300, 483, 886, 709, 295, 257, 16235, 293, 436, 483, 16914, 484, 766, 264, 1412, 47138, 13, 50664, 50664, 400, 437, 2314, 307, 300, 490, 550, 322, 11, 572, 1365, 1562, 43869, 341, 34090, 13, 50914, 50914, 407, 341, 34090, 7023, 3116, 5680, 13, 50964, 50964, 407, 309, 311, 733, 295, 411, 257, 10996, 3567, 4344, 294, 257, 1575, 295, 257, 3209, 13, 51164, 51164, 400, 370, 2171, 437, 393, 1051, 307, 498, 428, 2539, 3314, 307, 588, 1090, 11, 337, 1365, 11, 51364, 51364, 293, 291, 362, 257, 18161, 2533, 365, 1300, 43, 52, 22027, 11, 291, 3847, 264, 18161, 2533, 293, 291, 483, 512, 1036, 4470, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06752335419089107, "compression_ratio": 1.7582417582417582, "no_speech_prob": 4.710764187620953e-06}, {"id": 263, "seek": 132700, "start": 1343.0, "end": 1347.0, "text": " And so sometimes what can happen is if your learning rate is very high, for example,", "tokens": [50364, 2171, 291, 362, 613, 22027, 300, 483, 886, 709, 295, 257, 16235, 293, 436, 483, 16914, 484, 766, 264, 1412, 47138, 13, 50664, 50664, 400, 437, 2314, 307, 300, 490, 550, 322, 11, 572, 1365, 1562, 43869, 341, 34090, 13, 50914, 50914, 407, 341, 34090, 7023, 3116, 5680, 13, 50964, 50964, 407, 309, 311, 733, 295, 411, 257, 10996, 3567, 4344, 294, 257, 1575, 295, 257, 3209, 13, 51164, 51164, 400, 370, 2171, 437, 393, 1051, 307, 498, 428, 2539, 3314, 307, 588, 1090, 11, 337, 1365, 11, 51364, 51364, 293, 291, 362, 257, 18161, 2533, 365, 1300, 43, 52, 22027, 11, 291, 3847, 264, 18161, 2533, 293, 291, 483, 512, 1036, 4470, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06752335419089107, "compression_ratio": 1.7582417582417582, "no_speech_prob": 4.710764187620953e-06}, {"id": 264, "seek": 132700, "start": 1347.0, "end": 1352.0, "text": " and you have a neural net with ReLU neurons, you train the neural net and you get some last loss.", "tokens": [50364, 2171, 291, 362, 613, 22027, 300, 483, 886, 709, 295, 257, 16235, 293, 436, 483, 16914, 484, 766, 264, 1412, 47138, 13, 50664, 50664, 400, 437, 2314, 307, 300, 490, 550, 322, 11, 572, 1365, 1562, 43869, 341, 34090, 13, 50914, 50914, 407, 341, 34090, 7023, 3116, 5680, 13, 50964, 50964, 407, 309, 311, 733, 295, 411, 257, 10996, 3567, 4344, 294, 257, 1575, 295, 257, 3209, 13, 51164, 51164, 400, 370, 2171, 437, 393, 1051, 307, 498, 428, 2539, 3314, 307, 588, 1090, 11, 337, 1365, 11, 51364, 51364, 293, 291, 362, 257, 18161, 2533, 365, 1300, 43, 52, 22027, 11, 291, 3847, 264, 18161, 2533, 293, 291, 483, 512, 1036, 4470, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06752335419089107, "compression_ratio": 1.7582417582417582, "no_speech_prob": 4.710764187620953e-06}, {"id": 265, "seek": 135200, "start": 1352.0, "end": 1362.0, "text": " But then actually what you do is you go through the entire training set and you forward your examples and you can find neurons that never activate.", "tokens": [50364, 583, 550, 767, 437, 291, 360, 307, 291, 352, 807, 264, 2302, 3097, 992, 293, 291, 2128, 428, 5110, 293, 291, 393, 915, 22027, 300, 1128, 13615, 13, 50864, 50864, 814, 366, 3116, 22027, 294, 428, 3209, 13, 50964, 50964, 400, 370, 729, 22027, 486, 1128, 1261, 322, 13, 51064, 51064, 400, 2673, 437, 2314, 307, 300, 1830, 3097, 11, 613, 1300, 43, 52, 22027, 366, 4473, 11, 2684, 11, 1030, 11458, 13, 51264, 51264, 400, 550, 570, 295, 257, 1090, 16235, 4079, 538, 2931, 11, 436, 483, 16914, 766, 293, 550, 1825, 1562, 43869, 552, 13, 51564, 51564, 400, 490, 550, 322, 11, 436, 366, 445, 3116, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05884331306525036, "compression_ratio": 1.8295454545454546, "no_speech_prob": 9.817962563829497e-06}, {"id": 266, "seek": 135200, "start": 1362.0, "end": 1364.0, "text": " They are dead neurons in your network.", "tokens": [50364, 583, 550, 767, 437, 291, 360, 307, 291, 352, 807, 264, 2302, 3097, 992, 293, 291, 2128, 428, 5110, 293, 291, 393, 915, 22027, 300, 1128, 13615, 13, 50864, 50864, 814, 366, 3116, 22027, 294, 428, 3209, 13, 50964, 50964, 400, 370, 729, 22027, 486, 1128, 1261, 322, 13, 51064, 51064, 400, 2673, 437, 2314, 307, 300, 1830, 3097, 11, 613, 1300, 43, 52, 22027, 366, 4473, 11, 2684, 11, 1030, 11458, 13, 51264, 51264, 400, 550, 570, 295, 257, 1090, 16235, 4079, 538, 2931, 11, 436, 483, 16914, 766, 293, 550, 1825, 1562, 43869, 552, 13, 51564, 51564, 400, 490, 550, 322, 11, 436, 366, 445, 3116, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05884331306525036, "compression_ratio": 1.8295454545454546, "no_speech_prob": 9.817962563829497e-06}, {"id": 267, "seek": 135200, "start": 1364.0, "end": 1366.0, "text": " And so those neurons will never turn on.", "tokens": [50364, 583, 550, 767, 437, 291, 360, 307, 291, 352, 807, 264, 2302, 3097, 992, 293, 291, 2128, 428, 5110, 293, 291, 393, 915, 22027, 300, 1128, 13615, 13, 50864, 50864, 814, 366, 3116, 22027, 294, 428, 3209, 13, 50964, 50964, 400, 370, 729, 22027, 486, 1128, 1261, 322, 13, 51064, 51064, 400, 2673, 437, 2314, 307, 300, 1830, 3097, 11, 613, 1300, 43, 52, 22027, 366, 4473, 11, 2684, 11, 1030, 11458, 13, 51264, 51264, 400, 550, 570, 295, 257, 1090, 16235, 4079, 538, 2931, 11, 436, 483, 16914, 766, 293, 550, 1825, 1562, 43869, 552, 13, 51564, 51564, 400, 490, 550, 322, 11, 436, 366, 445, 3116, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05884331306525036, "compression_ratio": 1.8295454545454546, "no_speech_prob": 9.817962563829497e-06}, {"id": 268, "seek": 135200, "start": 1366.0, "end": 1370.0, "text": " And usually what happens is that during training, these ReLU neurons are changing, moving, et cetera.", "tokens": [50364, 583, 550, 767, 437, 291, 360, 307, 291, 352, 807, 264, 2302, 3097, 992, 293, 291, 2128, 428, 5110, 293, 291, 393, 915, 22027, 300, 1128, 13615, 13, 50864, 50864, 814, 366, 3116, 22027, 294, 428, 3209, 13, 50964, 50964, 400, 370, 729, 22027, 486, 1128, 1261, 322, 13, 51064, 51064, 400, 2673, 437, 2314, 307, 300, 1830, 3097, 11, 613, 1300, 43, 52, 22027, 366, 4473, 11, 2684, 11, 1030, 11458, 13, 51264, 51264, 400, 550, 570, 295, 257, 1090, 16235, 4079, 538, 2931, 11, 436, 483, 16914, 766, 293, 550, 1825, 1562, 43869, 552, 13, 51564, 51564, 400, 490, 550, 322, 11, 436, 366, 445, 3116, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05884331306525036, "compression_ratio": 1.8295454545454546, "no_speech_prob": 9.817962563829497e-06}, {"id": 269, "seek": 135200, "start": 1370.0, "end": 1376.0, "text": " And then because of a high gradient somewhere by chance, they get knocked off and then nothing ever activates them.", "tokens": [50364, 583, 550, 767, 437, 291, 360, 307, 291, 352, 807, 264, 2302, 3097, 992, 293, 291, 2128, 428, 5110, 293, 291, 393, 915, 22027, 300, 1128, 13615, 13, 50864, 50864, 814, 366, 3116, 22027, 294, 428, 3209, 13, 50964, 50964, 400, 370, 729, 22027, 486, 1128, 1261, 322, 13, 51064, 51064, 400, 2673, 437, 2314, 307, 300, 1830, 3097, 11, 613, 1300, 43, 52, 22027, 366, 4473, 11, 2684, 11, 1030, 11458, 13, 51264, 51264, 400, 550, 570, 295, 257, 1090, 16235, 4079, 538, 2931, 11, 436, 483, 16914, 766, 293, 550, 1825, 1562, 43869, 552, 13, 51564, 51564, 400, 490, 550, 322, 11, 436, 366, 445, 3116, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05884331306525036, "compression_ratio": 1.8295454545454546, "no_speech_prob": 9.817962563829497e-06}, {"id": 270, "seek": 135200, "start": 1376.0, "end": 1379.0, "text": " And from then on, they are just dead.", "tokens": [50364, 583, 550, 767, 437, 291, 360, 307, 291, 352, 807, 264, 2302, 3097, 992, 293, 291, 2128, 428, 5110, 293, 291, 393, 915, 22027, 300, 1128, 13615, 13, 50864, 50864, 814, 366, 3116, 22027, 294, 428, 3209, 13, 50964, 50964, 400, 370, 729, 22027, 486, 1128, 1261, 322, 13, 51064, 51064, 400, 2673, 437, 2314, 307, 300, 1830, 3097, 11, 613, 1300, 43, 52, 22027, 366, 4473, 11, 2684, 11, 1030, 11458, 13, 51264, 51264, 400, 550, 570, 295, 257, 1090, 16235, 4079, 538, 2931, 11, 436, 483, 16914, 766, 293, 550, 1825, 1562, 43869, 552, 13, 51564, 51564, 400, 490, 550, 322, 11, 436, 366, 445, 3116, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05884331306525036, "compression_ratio": 1.8295454545454546, "no_speech_prob": 9.817962563829497e-06}, {"id": 271, "seek": 137900, "start": 1379.0, "end": 1383.0, "text": " So that's kind of like a permanent brain damage that can happen to some of these neurons.", "tokens": [50364, 407, 300, 311, 733, 295, 411, 257, 10996, 3567, 4344, 300, 393, 1051, 281, 512, 295, 613, 22027, 13, 50564, 50564, 1981, 661, 2107, 28263, 1088, 411, 1456, 15681, 1300, 43, 52, 486, 406, 9753, 490, 341, 2734, 382, 709, 570, 291, 393, 536, 300, 309, 1177, 380, 362, 4962, 28537, 13, 50914, 50914, 509, 603, 1920, 1009, 483, 2771, 2448, 13, 51064, 51064, 400, 2699, 84, 307, 611, 6457, 10374, 1143, 13, 51214, 51214, 467, 611, 1062, 9753, 490, 341, 2734, 570, 309, 575, 4962, 3166, 13, 51414, 51414, 407, 300, 311, 445, 746, 281, 312, 3650, 295, 293, 746, 281, 312, 5922, 466, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06835447224703702, "compression_ratio": 1.6616541353383458, "no_speech_prob": 3.071464379900135e-05}, {"id": 272, "seek": 137900, "start": 1383.0, "end": 1390.0, "text": " These other nonlinearities like Leaky ReLU will not suffer from this issue as much because you can see that it doesn't have flat tails.", "tokens": [50364, 407, 300, 311, 733, 295, 411, 257, 10996, 3567, 4344, 300, 393, 1051, 281, 512, 295, 613, 22027, 13, 50564, 50564, 1981, 661, 2107, 28263, 1088, 411, 1456, 15681, 1300, 43, 52, 486, 406, 9753, 490, 341, 2734, 382, 709, 570, 291, 393, 536, 300, 309, 1177, 380, 362, 4962, 28537, 13, 50914, 50914, 509, 603, 1920, 1009, 483, 2771, 2448, 13, 51064, 51064, 400, 2699, 84, 307, 611, 6457, 10374, 1143, 13, 51214, 51214, 467, 611, 1062, 9753, 490, 341, 2734, 570, 309, 575, 4962, 3166, 13, 51414, 51414, 407, 300, 311, 445, 746, 281, 312, 3650, 295, 293, 746, 281, 312, 5922, 466, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06835447224703702, "compression_ratio": 1.6616541353383458, "no_speech_prob": 3.071464379900135e-05}, {"id": 273, "seek": 137900, "start": 1390.0, "end": 1393.0, "text": " You'll almost always get gradients.", "tokens": [50364, 407, 300, 311, 733, 295, 411, 257, 10996, 3567, 4344, 300, 393, 1051, 281, 512, 295, 613, 22027, 13, 50564, 50564, 1981, 661, 2107, 28263, 1088, 411, 1456, 15681, 1300, 43, 52, 486, 406, 9753, 490, 341, 2734, 382, 709, 570, 291, 393, 536, 300, 309, 1177, 380, 362, 4962, 28537, 13, 50914, 50914, 509, 603, 1920, 1009, 483, 2771, 2448, 13, 51064, 51064, 400, 2699, 84, 307, 611, 6457, 10374, 1143, 13, 51214, 51214, 467, 611, 1062, 9753, 490, 341, 2734, 570, 309, 575, 4962, 3166, 13, 51414, 51414, 407, 300, 311, 445, 746, 281, 312, 3650, 295, 293, 746, 281, 312, 5922, 466, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06835447224703702, "compression_ratio": 1.6616541353383458, "no_speech_prob": 3.071464379900135e-05}, {"id": 274, "seek": 137900, "start": 1393.0, "end": 1396.0, "text": " And Elu is also fairly frequently used.", "tokens": [50364, 407, 300, 311, 733, 295, 411, 257, 10996, 3567, 4344, 300, 393, 1051, 281, 512, 295, 613, 22027, 13, 50564, 50564, 1981, 661, 2107, 28263, 1088, 411, 1456, 15681, 1300, 43, 52, 486, 406, 9753, 490, 341, 2734, 382, 709, 570, 291, 393, 536, 300, 309, 1177, 380, 362, 4962, 28537, 13, 50914, 50914, 509, 603, 1920, 1009, 483, 2771, 2448, 13, 51064, 51064, 400, 2699, 84, 307, 611, 6457, 10374, 1143, 13, 51214, 51214, 467, 611, 1062, 9753, 490, 341, 2734, 570, 309, 575, 4962, 3166, 13, 51414, 51414, 407, 300, 311, 445, 746, 281, 312, 3650, 295, 293, 746, 281, 312, 5922, 466, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06835447224703702, "compression_ratio": 1.6616541353383458, "no_speech_prob": 3.071464379900135e-05}, {"id": 275, "seek": 137900, "start": 1396.0, "end": 1400.0, "text": " It also might suffer from this issue because it has flat parts.", "tokens": [50364, 407, 300, 311, 733, 295, 411, 257, 10996, 3567, 4344, 300, 393, 1051, 281, 512, 295, 613, 22027, 13, 50564, 50564, 1981, 661, 2107, 28263, 1088, 411, 1456, 15681, 1300, 43, 52, 486, 406, 9753, 490, 341, 2734, 382, 709, 570, 291, 393, 536, 300, 309, 1177, 380, 362, 4962, 28537, 13, 50914, 50914, 509, 603, 1920, 1009, 483, 2771, 2448, 13, 51064, 51064, 400, 2699, 84, 307, 611, 6457, 10374, 1143, 13, 51214, 51214, 467, 611, 1062, 9753, 490, 341, 2734, 570, 309, 575, 4962, 3166, 13, 51414, 51414, 407, 300, 311, 445, 746, 281, 312, 3650, 295, 293, 746, 281, 312, 5922, 466, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06835447224703702, "compression_ratio": 1.6616541353383458, "no_speech_prob": 3.071464379900135e-05}, {"id": 276, "seek": 137900, "start": 1400.0, "end": 1404.0, "text": " So that's just something to be aware of and something to be concerned about.", "tokens": [50364, 407, 300, 311, 733, 295, 411, 257, 10996, 3567, 4344, 300, 393, 1051, 281, 512, 295, 613, 22027, 13, 50564, 50564, 1981, 661, 2107, 28263, 1088, 411, 1456, 15681, 1300, 43, 52, 486, 406, 9753, 490, 341, 2734, 382, 709, 570, 291, 393, 536, 300, 309, 1177, 380, 362, 4962, 28537, 13, 50914, 50914, 509, 603, 1920, 1009, 483, 2771, 2448, 13, 51064, 51064, 400, 2699, 84, 307, 611, 6457, 10374, 1143, 13, 51214, 51214, 467, 611, 1062, 9753, 490, 341, 2734, 570, 309, 575, 4962, 3166, 13, 51414, 51414, 407, 300, 311, 445, 746, 281, 312, 3650, 295, 293, 746, 281, 312, 5922, 466, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06835447224703702, "compression_ratio": 1.6616541353383458, "no_speech_prob": 3.071464379900135e-05}, {"id": 277, "seek": 140400, "start": 1404.0, "end": 1410.0, "text": " And in this case, we have way too many activations H that take on extreme values.", "tokens": [50364, 400, 294, 341, 1389, 11, 321, 362, 636, 886, 867, 2430, 763, 389, 300, 747, 322, 8084, 4190, 13, 50664, 50664, 400, 570, 456, 311, 572, 7738, 295, 2418, 11, 286, 519, 321, 486, 312, 2264, 13, 50864, 50864, 400, 6451, 11, 264, 3209, 5028, 5660, 293, 2709, 505, 257, 1238, 8681, 4470, 13, 51014, 51014, 583, 309, 311, 445, 406, 16252, 13, 51114, 51114, 400, 341, 307, 406, 746, 291, 528, 11, 2318, 1830, 5883, 2144, 13, 51264, 51264, 400, 370, 1936, 437, 311, 2737, 307, 300, 341, 389, 659, 23397, 399, 300, 311, 13974, 281, 1266, 39, 11, 309, 311, 886, 8084, 13, 51664, 51664, 467, 311, 886, 2416, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07053884966620083, "compression_ratio": 1.628787878787879, "no_speech_prob": 3.426653711358085e-05}, {"id": 278, "seek": 140400, "start": 1410.0, "end": 1414.0, "text": " And because there's no column of white, I think we will be OK.", "tokens": [50364, 400, 294, 341, 1389, 11, 321, 362, 636, 886, 867, 2430, 763, 389, 300, 747, 322, 8084, 4190, 13, 50664, 50664, 400, 570, 456, 311, 572, 7738, 295, 2418, 11, 286, 519, 321, 486, 312, 2264, 13, 50864, 50864, 400, 6451, 11, 264, 3209, 5028, 5660, 293, 2709, 505, 257, 1238, 8681, 4470, 13, 51014, 51014, 583, 309, 311, 445, 406, 16252, 13, 51114, 51114, 400, 341, 307, 406, 746, 291, 528, 11, 2318, 1830, 5883, 2144, 13, 51264, 51264, 400, 370, 1936, 437, 311, 2737, 307, 300, 341, 389, 659, 23397, 399, 300, 311, 13974, 281, 1266, 39, 11, 309, 311, 886, 8084, 13, 51664, 51664, 467, 311, 886, 2416, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07053884966620083, "compression_ratio": 1.628787878787879, "no_speech_prob": 3.426653711358085e-05}, {"id": 279, "seek": 140400, "start": 1414.0, "end": 1417.0, "text": " And indeed, the network optimizes and gives us a pretty decent loss.", "tokens": [50364, 400, 294, 341, 1389, 11, 321, 362, 636, 886, 867, 2430, 763, 389, 300, 747, 322, 8084, 4190, 13, 50664, 50664, 400, 570, 456, 311, 572, 7738, 295, 2418, 11, 286, 519, 321, 486, 312, 2264, 13, 50864, 50864, 400, 6451, 11, 264, 3209, 5028, 5660, 293, 2709, 505, 257, 1238, 8681, 4470, 13, 51014, 51014, 583, 309, 311, 445, 406, 16252, 13, 51114, 51114, 400, 341, 307, 406, 746, 291, 528, 11, 2318, 1830, 5883, 2144, 13, 51264, 51264, 400, 370, 1936, 437, 311, 2737, 307, 300, 341, 389, 659, 23397, 399, 300, 311, 13974, 281, 1266, 39, 11, 309, 311, 886, 8084, 13, 51664, 51664, 467, 311, 886, 2416, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07053884966620083, "compression_ratio": 1.628787878787879, "no_speech_prob": 3.426653711358085e-05}, {"id": 280, "seek": 140400, "start": 1417.0, "end": 1419.0, "text": " But it's just not optimal.", "tokens": [50364, 400, 294, 341, 1389, 11, 321, 362, 636, 886, 867, 2430, 763, 389, 300, 747, 322, 8084, 4190, 13, 50664, 50664, 400, 570, 456, 311, 572, 7738, 295, 2418, 11, 286, 519, 321, 486, 312, 2264, 13, 50864, 50864, 400, 6451, 11, 264, 3209, 5028, 5660, 293, 2709, 505, 257, 1238, 8681, 4470, 13, 51014, 51014, 583, 309, 311, 445, 406, 16252, 13, 51114, 51114, 400, 341, 307, 406, 746, 291, 528, 11, 2318, 1830, 5883, 2144, 13, 51264, 51264, 400, 370, 1936, 437, 311, 2737, 307, 300, 341, 389, 659, 23397, 399, 300, 311, 13974, 281, 1266, 39, 11, 309, 311, 886, 8084, 13, 51664, 51664, 467, 311, 886, 2416, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07053884966620083, "compression_ratio": 1.628787878787879, "no_speech_prob": 3.426653711358085e-05}, {"id": 281, "seek": 140400, "start": 1419.0, "end": 1422.0, "text": " And this is not something you want, especially during initialization.", "tokens": [50364, 400, 294, 341, 1389, 11, 321, 362, 636, 886, 867, 2430, 763, 389, 300, 747, 322, 8084, 4190, 13, 50664, 50664, 400, 570, 456, 311, 572, 7738, 295, 2418, 11, 286, 519, 321, 486, 312, 2264, 13, 50864, 50864, 400, 6451, 11, 264, 3209, 5028, 5660, 293, 2709, 505, 257, 1238, 8681, 4470, 13, 51014, 51014, 583, 309, 311, 445, 406, 16252, 13, 51114, 51114, 400, 341, 307, 406, 746, 291, 528, 11, 2318, 1830, 5883, 2144, 13, 51264, 51264, 400, 370, 1936, 437, 311, 2737, 307, 300, 341, 389, 659, 23397, 399, 300, 311, 13974, 281, 1266, 39, 11, 309, 311, 886, 8084, 13, 51664, 51664, 467, 311, 886, 2416, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07053884966620083, "compression_ratio": 1.628787878787879, "no_speech_prob": 3.426653711358085e-05}, {"id": 282, "seek": 140400, "start": 1422.0, "end": 1430.0, "text": " And so basically what's happening is that this H preactivation that's flowing to 10H, it's too extreme.", "tokens": [50364, 400, 294, 341, 1389, 11, 321, 362, 636, 886, 867, 2430, 763, 389, 300, 747, 322, 8084, 4190, 13, 50664, 50664, 400, 570, 456, 311, 572, 7738, 295, 2418, 11, 286, 519, 321, 486, 312, 2264, 13, 50864, 50864, 400, 6451, 11, 264, 3209, 5028, 5660, 293, 2709, 505, 257, 1238, 8681, 4470, 13, 51014, 51014, 583, 309, 311, 445, 406, 16252, 13, 51114, 51114, 400, 341, 307, 406, 746, 291, 528, 11, 2318, 1830, 5883, 2144, 13, 51264, 51264, 400, 370, 1936, 437, 311, 2737, 307, 300, 341, 389, 659, 23397, 399, 300, 311, 13974, 281, 1266, 39, 11, 309, 311, 886, 8084, 13, 51664, 51664, 467, 311, 886, 2416, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07053884966620083, "compression_ratio": 1.628787878787879, "no_speech_prob": 3.426653711358085e-05}, {"id": 283, "seek": 140400, "start": 1430.0, "end": 1431.0, "text": " It's too large.", "tokens": [50364, 400, 294, 341, 1389, 11, 321, 362, 636, 886, 867, 2430, 763, 389, 300, 747, 322, 8084, 4190, 13, 50664, 50664, 400, 570, 456, 311, 572, 7738, 295, 2418, 11, 286, 519, 321, 486, 312, 2264, 13, 50864, 50864, 400, 6451, 11, 264, 3209, 5028, 5660, 293, 2709, 505, 257, 1238, 8681, 4470, 13, 51014, 51014, 583, 309, 311, 445, 406, 16252, 13, 51114, 51114, 400, 341, 307, 406, 746, 291, 528, 11, 2318, 1830, 5883, 2144, 13, 51264, 51264, 400, 370, 1936, 437, 311, 2737, 307, 300, 341, 389, 659, 23397, 399, 300, 311, 13974, 281, 1266, 39, 11, 309, 311, 886, 8084, 13, 51664, 51664, 467, 311, 886, 2416, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07053884966620083, "compression_ratio": 1.628787878787879, "no_speech_prob": 3.426653711358085e-05}, {"id": 284, "seek": 143100, "start": 1431.0, "end": 1437.0, "text": " It's creating a distribution that is too saturated in both sides of the 10H.", "tokens": [50364, 467, 311, 4084, 257, 7316, 300, 307, 886, 25408, 294, 1293, 4881, 295, 264, 1266, 39, 13, 50664, 50664, 400, 309, 311, 406, 746, 291, 528, 570, 309, 1355, 300, 456, 311, 1570, 3097, 337, 613, 22027, 570, 436, 5623, 1570, 10374, 13, 51064, 51064, 407, 577, 360, 321, 3191, 341, 30, 51164, 51164, 1042, 11, 389, 659, 23397, 399, 307, 8797, 2218, 11, 597, 1487, 490, 383, 13, 51414, 51414, 407, 613, 366, 9452, 39148, 13, 51514, 51514, 583, 550, 309, 311, 17207, 538, 343, 16, 1804, 363, 16, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07175293972617701, "compression_ratio": 1.4789915966386555, "no_speech_prob": 5.422107278718613e-06}, {"id": 285, "seek": 143100, "start": 1437.0, "end": 1445.0, "text": " And it's not something you want because it means that there's less training for these neurons because they update less frequently.", "tokens": [50364, 467, 311, 4084, 257, 7316, 300, 307, 886, 25408, 294, 1293, 4881, 295, 264, 1266, 39, 13, 50664, 50664, 400, 309, 311, 406, 746, 291, 528, 570, 309, 1355, 300, 456, 311, 1570, 3097, 337, 613, 22027, 570, 436, 5623, 1570, 10374, 13, 51064, 51064, 407, 577, 360, 321, 3191, 341, 30, 51164, 51164, 1042, 11, 389, 659, 23397, 399, 307, 8797, 2218, 11, 597, 1487, 490, 383, 13, 51414, 51414, 407, 613, 366, 9452, 39148, 13, 51514, 51514, 583, 550, 309, 311, 17207, 538, 343, 16, 1804, 363, 16, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07175293972617701, "compression_ratio": 1.4789915966386555, "no_speech_prob": 5.422107278718613e-06}, {"id": 286, "seek": 143100, "start": 1445.0, "end": 1447.0, "text": " So how do we fix this?", "tokens": [50364, 467, 311, 4084, 257, 7316, 300, 307, 886, 25408, 294, 1293, 4881, 295, 264, 1266, 39, 13, 50664, 50664, 400, 309, 311, 406, 746, 291, 528, 570, 309, 1355, 300, 456, 311, 1570, 3097, 337, 613, 22027, 570, 436, 5623, 1570, 10374, 13, 51064, 51064, 407, 577, 360, 321, 3191, 341, 30, 51164, 51164, 1042, 11, 389, 659, 23397, 399, 307, 8797, 2218, 11, 597, 1487, 490, 383, 13, 51414, 51414, 407, 613, 366, 9452, 39148, 13, 51514, 51514, 583, 550, 309, 311, 17207, 538, 343, 16, 1804, 363, 16, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07175293972617701, "compression_ratio": 1.4789915966386555, "no_speech_prob": 5.422107278718613e-06}, {"id": 287, "seek": 143100, "start": 1447.0, "end": 1452.0, "text": " Well, H preactivation is MCAT, which comes from C.", "tokens": [50364, 467, 311, 4084, 257, 7316, 300, 307, 886, 25408, 294, 1293, 4881, 295, 264, 1266, 39, 13, 50664, 50664, 400, 309, 311, 406, 746, 291, 528, 570, 309, 1355, 300, 456, 311, 1570, 3097, 337, 613, 22027, 570, 436, 5623, 1570, 10374, 13, 51064, 51064, 407, 577, 360, 321, 3191, 341, 30, 51164, 51164, 1042, 11, 389, 659, 23397, 399, 307, 8797, 2218, 11, 597, 1487, 490, 383, 13, 51414, 51414, 407, 613, 366, 9452, 39148, 13, 51514, 51514, 583, 550, 309, 311, 17207, 538, 343, 16, 1804, 363, 16, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07175293972617701, "compression_ratio": 1.4789915966386555, "no_speech_prob": 5.422107278718613e-06}, {"id": 288, "seek": 143100, "start": 1452.0, "end": 1454.0, "text": " So these are uniform Gaussian.", "tokens": [50364, 467, 311, 4084, 257, 7316, 300, 307, 886, 25408, 294, 1293, 4881, 295, 264, 1266, 39, 13, 50664, 50664, 400, 309, 311, 406, 746, 291, 528, 570, 309, 1355, 300, 456, 311, 1570, 3097, 337, 613, 22027, 570, 436, 5623, 1570, 10374, 13, 51064, 51064, 407, 577, 360, 321, 3191, 341, 30, 51164, 51164, 1042, 11, 389, 659, 23397, 399, 307, 8797, 2218, 11, 597, 1487, 490, 383, 13, 51414, 51414, 407, 613, 366, 9452, 39148, 13, 51514, 51514, 583, 550, 309, 311, 17207, 538, 343, 16, 1804, 363, 16, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07175293972617701, "compression_ratio": 1.4789915966386555, "no_speech_prob": 5.422107278718613e-06}, {"id": 289, "seek": 143100, "start": 1454.0, "end": 1457.0, "text": " But then it's multiplied by W1 plus B1.", "tokens": [50364, 467, 311, 4084, 257, 7316, 300, 307, 886, 25408, 294, 1293, 4881, 295, 264, 1266, 39, 13, 50664, 50664, 400, 309, 311, 406, 746, 291, 528, 570, 309, 1355, 300, 456, 311, 1570, 3097, 337, 613, 22027, 570, 436, 5623, 1570, 10374, 13, 51064, 51064, 407, 577, 360, 321, 3191, 341, 30, 51164, 51164, 1042, 11, 389, 659, 23397, 399, 307, 8797, 2218, 11, 597, 1487, 490, 383, 13, 51414, 51414, 407, 613, 366, 9452, 39148, 13, 51514, 51514, 583, 550, 309, 311, 17207, 538, 343, 16, 1804, 363, 16, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07175293972617701, "compression_ratio": 1.4789915966386555, "no_speech_prob": 5.422107278718613e-06}, {"id": 290, "seek": 145700, "start": 1457.0, "end": 1461.0, "text": " And H preact is too far off from zero, and that's causing the issue.", "tokens": [50364, 400, 389, 659, 578, 307, 886, 1400, 766, 490, 4018, 11, 293, 300, 311, 9853, 264, 2734, 13, 50564, 50564, 407, 321, 528, 341, 4515, 592, 399, 281, 312, 4966, 281, 4018, 11, 588, 2531, 281, 437, 321, 632, 365, 3565, 1208, 13, 50864, 50864, 407, 510, 321, 528, 767, 746, 588, 11, 588, 2531, 13, 51064, 51064, 823, 11, 309, 311, 2264, 281, 992, 264, 32152, 281, 257, 588, 1359, 1230, 13, 51264, 51264, 492, 393, 2139, 12972, 309, 538, 1958, 11, 1958, 11, 502, 281, 483, 411, 257, 707, 857, 295, 30867, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08868009393865411, "compression_ratio": 1.5426008968609866, "no_speech_prob": 3.3404432997485856e-06}, {"id": 291, "seek": 145700, "start": 1461.0, "end": 1467.0, "text": " So we want this reactivation to be closer to zero, very similar to what we had with logits.", "tokens": [50364, 400, 389, 659, 578, 307, 886, 1400, 766, 490, 4018, 11, 293, 300, 311, 9853, 264, 2734, 13, 50564, 50564, 407, 321, 528, 341, 4515, 592, 399, 281, 312, 4966, 281, 4018, 11, 588, 2531, 281, 437, 321, 632, 365, 3565, 1208, 13, 50864, 50864, 407, 510, 321, 528, 767, 746, 588, 11, 588, 2531, 13, 51064, 51064, 823, 11, 309, 311, 2264, 281, 992, 264, 32152, 281, 257, 588, 1359, 1230, 13, 51264, 51264, 492, 393, 2139, 12972, 309, 538, 1958, 11, 1958, 11, 502, 281, 483, 411, 257, 707, 857, 295, 30867, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08868009393865411, "compression_ratio": 1.5426008968609866, "no_speech_prob": 3.3404432997485856e-06}, {"id": 292, "seek": 145700, "start": 1467.0, "end": 1471.0, "text": " So here we want actually something very, very similar.", "tokens": [50364, 400, 389, 659, 578, 307, 886, 1400, 766, 490, 4018, 11, 293, 300, 311, 9853, 264, 2734, 13, 50564, 50564, 407, 321, 528, 341, 4515, 592, 399, 281, 312, 4966, 281, 4018, 11, 588, 2531, 281, 437, 321, 632, 365, 3565, 1208, 13, 50864, 50864, 407, 510, 321, 528, 767, 746, 588, 11, 588, 2531, 13, 51064, 51064, 823, 11, 309, 311, 2264, 281, 992, 264, 32152, 281, 257, 588, 1359, 1230, 13, 51264, 51264, 492, 393, 2139, 12972, 309, 538, 1958, 11, 1958, 11, 502, 281, 483, 411, 257, 707, 857, 295, 30867, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08868009393865411, "compression_ratio": 1.5426008968609866, "no_speech_prob": 3.3404432997485856e-06}, {"id": 293, "seek": 145700, "start": 1471.0, "end": 1475.0, "text": " Now, it's OK to set the biases to a very small number.", "tokens": [50364, 400, 389, 659, 578, 307, 886, 1400, 766, 490, 4018, 11, 293, 300, 311, 9853, 264, 2734, 13, 50564, 50564, 407, 321, 528, 341, 4515, 592, 399, 281, 312, 4966, 281, 4018, 11, 588, 2531, 281, 437, 321, 632, 365, 3565, 1208, 13, 50864, 50864, 407, 510, 321, 528, 767, 746, 588, 11, 588, 2531, 13, 51064, 51064, 823, 11, 309, 311, 2264, 281, 992, 264, 32152, 281, 257, 588, 1359, 1230, 13, 51264, 51264, 492, 393, 2139, 12972, 309, 538, 1958, 11, 1958, 11, 502, 281, 483, 411, 257, 707, 857, 295, 30867, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08868009393865411, "compression_ratio": 1.5426008968609866, "no_speech_prob": 3.3404432997485856e-06}, {"id": 294, "seek": 145700, "start": 1475.0, "end": 1479.0, "text": " We can either multiply it by 0, 0, 1 to get like a little bit of entropy.", "tokens": [50364, 400, 389, 659, 578, 307, 886, 1400, 766, 490, 4018, 11, 293, 300, 311, 9853, 264, 2734, 13, 50564, 50564, 407, 321, 528, 341, 4515, 592, 399, 281, 312, 4966, 281, 4018, 11, 588, 2531, 281, 437, 321, 632, 365, 3565, 1208, 13, 50864, 50864, 407, 510, 321, 528, 767, 746, 588, 11, 588, 2531, 13, 51064, 51064, 823, 11, 309, 311, 2264, 281, 992, 264, 32152, 281, 257, 588, 1359, 1230, 13, 51264, 51264, 492, 393, 2139, 12972, 309, 538, 1958, 11, 1958, 11, 502, 281, 483, 411, 257, 707, 857, 295, 30867, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08868009393865411, "compression_ratio": 1.5426008968609866, "no_speech_prob": 3.3404432997485856e-06}, {"id": 295, "seek": 147900, "start": 1479.0, "end": 1489.0, "text": " I sometimes like to do that just so that there's like a little bit of variation and diversity in the original initialization of these 10H neurons.", "tokens": [50364, 286, 2171, 411, 281, 360, 300, 445, 370, 300, 456, 311, 411, 257, 707, 857, 295, 12990, 293, 8811, 294, 264, 3380, 5883, 2144, 295, 613, 1266, 39, 22027, 13, 50864, 50864, 400, 286, 915, 294, 3124, 300, 300, 393, 854, 19618, 257, 707, 857, 13, 51064, 51064, 400, 550, 264, 17443, 11, 321, 393, 611, 445, 411, 30725, 13, 51214, 51214, 407, 718, 311, 12972, 1203, 538, 1958, 13, 16, 13, 51364, 51364, 961, 311, 43819, 409, 264, 700, 15245, 13, 51464, 51464, 400, 586, 718, 311, 574, 412, 341, 13, 51564, 51564, 400, 731, 11, 700, 11, 718, 311, 574, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07618324844925492, "compression_ratio": 1.6808510638297873, "no_speech_prob": 4.029339379485464e-06}, {"id": 296, "seek": 147900, "start": 1489.0, "end": 1493.0, "text": " And I find in practice that that can help optimization a little bit.", "tokens": [50364, 286, 2171, 411, 281, 360, 300, 445, 370, 300, 456, 311, 411, 257, 707, 857, 295, 12990, 293, 8811, 294, 264, 3380, 5883, 2144, 295, 613, 1266, 39, 22027, 13, 50864, 50864, 400, 286, 915, 294, 3124, 300, 300, 393, 854, 19618, 257, 707, 857, 13, 51064, 51064, 400, 550, 264, 17443, 11, 321, 393, 611, 445, 411, 30725, 13, 51214, 51214, 407, 718, 311, 12972, 1203, 538, 1958, 13, 16, 13, 51364, 51364, 961, 311, 43819, 409, 264, 700, 15245, 13, 51464, 51464, 400, 586, 718, 311, 574, 412, 341, 13, 51564, 51564, 400, 731, 11, 700, 11, 718, 311, 574, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07618324844925492, "compression_ratio": 1.6808510638297873, "no_speech_prob": 4.029339379485464e-06}, {"id": 297, "seek": 147900, "start": 1493.0, "end": 1496.0, "text": " And then the weights, we can also just like squash.", "tokens": [50364, 286, 2171, 411, 281, 360, 300, 445, 370, 300, 456, 311, 411, 257, 707, 857, 295, 12990, 293, 8811, 294, 264, 3380, 5883, 2144, 295, 613, 1266, 39, 22027, 13, 50864, 50864, 400, 286, 915, 294, 3124, 300, 300, 393, 854, 19618, 257, 707, 857, 13, 51064, 51064, 400, 550, 264, 17443, 11, 321, 393, 611, 445, 411, 30725, 13, 51214, 51214, 407, 718, 311, 12972, 1203, 538, 1958, 13, 16, 13, 51364, 51364, 961, 311, 43819, 409, 264, 700, 15245, 13, 51464, 51464, 400, 586, 718, 311, 574, 412, 341, 13, 51564, 51564, 400, 731, 11, 700, 11, 718, 311, 574, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07618324844925492, "compression_ratio": 1.6808510638297873, "no_speech_prob": 4.029339379485464e-06}, {"id": 298, "seek": 147900, "start": 1496.0, "end": 1499.0, "text": " So let's multiply everything by 0.1.", "tokens": [50364, 286, 2171, 411, 281, 360, 300, 445, 370, 300, 456, 311, 411, 257, 707, 857, 295, 12990, 293, 8811, 294, 264, 3380, 5883, 2144, 295, 613, 1266, 39, 22027, 13, 50864, 50864, 400, 286, 915, 294, 3124, 300, 300, 393, 854, 19618, 257, 707, 857, 13, 51064, 51064, 400, 550, 264, 17443, 11, 321, 393, 611, 445, 411, 30725, 13, 51214, 51214, 407, 718, 311, 12972, 1203, 538, 1958, 13, 16, 13, 51364, 51364, 961, 311, 43819, 409, 264, 700, 15245, 13, 51464, 51464, 400, 586, 718, 311, 574, 412, 341, 13, 51564, 51564, 400, 731, 11, 700, 11, 718, 311, 574, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07618324844925492, "compression_ratio": 1.6808510638297873, "no_speech_prob": 4.029339379485464e-06}, {"id": 299, "seek": 147900, "start": 1499.0, "end": 1501.0, "text": " Let's rerun the first batch.", "tokens": [50364, 286, 2171, 411, 281, 360, 300, 445, 370, 300, 456, 311, 411, 257, 707, 857, 295, 12990, 293, 8811, 294, 264, 3380, 5883, 2144, 295, 613, 1266, 39, 22027, 13, 50864, 50864, 400, 286, 915, 294, 3124, 300, 300, 393, 854, 19618, 257, 707, 857, 13, 51064, 51064, 400, 550, 264, 17443, 11, 321, 393, 611, 445, 411, 30725, 13, 51214, 51214, 407, 718, 311, 12972, 1203, 538, 1958, 13, 16, 13, 51364, 51364, 961, 311, 43819, 409, 264, 700, 15245, 13, 51464, 51464, 400, 586, 718, 311, 574, 412, 341, 13, 51564, 51564, 400, 731, 11, 700, 11, 718, 311, 574, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07618324844925492, "compression_ratio": 1.6808510638297873, "no_speech_prob": 4.029339379485464e-06}, {"id": 300, "seek": 147900, "start": 1501.0, "end": 1503.0, "text": " And now let's look at this.", "tokens": [50364, 286, 2171, 411, 281, 360, 300, 445, 370, 300, 456, 311, 411, 257, 707, 857, 295, 12990, 293, 8811, 294, 264, 3380, 5883, 2144, 295, 613, 1266, 39, 22027, 13, 50864, 50864, 400, 286, 915, 294, 3124, 300, 300, 393, 854, 19618, 257, 707, 857, 13, 51064, 51064, 400, 550, 264, 17443, 11, 321, 393, 611, 445, 411, 30725, 13, 51214, 51214, 407, 718, 311, 12972, 1203, 538, 1958, 13, 16, 13, 51364, 51364, 961, 311, 43819, 409, 264, 700, 15245, 13, 51464, 51464, 400, 586, 718, 311, 574, 412, 341, 13, 51564, 51564, 400, 731, 11, 700, 11, 718, 311, 574, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07618324844925492, "compression_ratio": 1.6808510638297873, "no_speech_prob": 4.029339379485464e-06}, {"id": 301, "seek": 147900, "start": 1503.0, "end": 1507.0, "text": " And well, first, let's look here.", "tokens": [50364, 286, 2171, 411, 281, 360, 300, 445, 370, 300, 456, 311, 411, 257, 707, 857, 295, 12990, 293, 8811, 294, 264, 3380, 5883, 2144, 295, 613, 1266, 39, 22027, 13, 50864, 50864, 400, 286, 915, 294, 3124, 300, 300, 393, 854, 19618, 257, 707, 857, 13, 51064, 51064, 400, 550, 264, 17443, 11, 321, 393, 611, 445, 411, 30725, 13, 51214, 51214, 407, 718, 311, 12972, 1203, 538, 1958, 13, 16, 13, 51364, 51364, 961, 311, 43819, 409, 264, 700, 15245, 13, 51464, 51464, 400, 586, 718, 311, 574, 412, 341, 13, 51564, 51564, 400, 731, 11, 700, 11, 718, 311, 574, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07618324844925492, "compression_ratio": 1.6808510638297873, "no_speech_prob": 4.029339379485464e-06}, {"id": 302, "seek": 150700, "start": 1507.0, "end": 1511.0, "text": " You see now, because we multiply W by 0.1, we have a much better histogram.", "tokens": [50364, 509, 536, 586, 11, 570, 321, 12972, 343, 538, 1958, 13, 16, 11, 321, 362, 257, 709, 1101, 49816, 13, 50564, 50564, 400, 300, 311, 570, 264, 4515, 592, 763, 366, 586, 1296, 3671, 502, 13, 20, 293, 502, 13, 20, 13, 50764, 50764, 400, 341, 321, 2066, 709, 11, 709, 1570, 2418, 13, 50914, 50914, 2264, 11, 456, 311, 572, 2418, 13, 51064, 51064, 407, 1936, 11, 300, 311, 570, 456, 366, 572, 22027, 300, 25408, 3673, 1958, 13, 8494, 294, 2139, 3513, 13, 51414, 51414, 407, 309, 311, 767, 257, 1238, 8681, 1081, 281, 312, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08193373212627336, "compression_ratio": 1.5726872246696035, "no_speech_prob": 2.7968548238277435e-05}, {"id": 303, "seek": 150700, "start": 1511.0, "end": 1515.0, "text": " And that's because the reactivations are now between negative 1.5 and 1.5.", "tokens": [50364, 509, 536, 586, 11, 570, 321, 12972, 343, 538, 1958, 13, 16, 11, 321, 362, 257, 709, 1101, 49816, 13, 50564, 50564, 400, 300, 311, 570, 264, 4515, 592, 763, 366, 586, 1296, 3671, 502, 13, 20, 293, 502, 13, 20, 13, 50764, 50764, 400, 341, 321, 2066, 709, 11, 709, 1570, 2418, 13, 50914, 50914, 2264, 11, 456, 311, 572, 2418, 13, 51064, 51064, 407, 1936, 11, 300, 311, 570, 456, 366, 572, 22027, 300, 25408, 3673, 1958, 13, 8494, 294, 2139, 3513, 13, 51414, 51414, 407, 309, 311, 767, 257, 1238, 8681, 1081, 281, 312, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08193373212627336, "compression_ratio": 1.5726872246696035, "no_speech_prob": 2.7968548238277435e-05}, {"id": 304, "seek": 150700, "start": 1515.0, "end": 1518.0, "text": " And this we expect much, much less white.", "tokens": [50364, 509, 536, 586, 11, 570, 321, 12972, 343, 538, 1958, 13, 16, 11, 321, 362, 257, 709, 1101, 49816, 13, 50564, 50564, 400, 300, 311, 570, 264, 4515, 592, 763, 366, 586, 1296, 3671, 502, 13, 20, 293, 502, 13, 20, 13, 50764, 50764, 400, 341, 321, 2066, 709, 11, 709, 1570, 2418, 13, 50914, 50914, 2264, 11, 456, 311, 572, 2418, 13, 51064, 51064, 407, 1936, 11, 300, 311, 570, 456, 366, 572, 22027, 300, 25408, 3673, 1958, 13, 8494, 294, 2139, 3513, 13, 51414, 51414, 407, 309, 311, 767, 257, 1238, 8681, 1081, 281, 312, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08193373212627336, "compression_ratio": 1.5726872246696035, "no_speech_prob": 2.7968548238277435e-05}, {"id": 305, "seek": 150700, "start": 1518.0, "end": 1521.0, "text": " OK, there's no white.", "tokens": [50364, 509, 536, 586, 11, 570, 321, 12972, 343, 538, 1958, 13, 16, 11, 321, 362, 257, 709, 1101, 49816, 13, 50564, 50564, 400, 300, 311, 570, 264, 4515, 592, 763, 366, 586, 1296, 3671, 502, 13, 20, 293, 502, 13, 20, 13, 50764, 50764, 400, 341, 321, 2066, 709, 11, 709, 1570, 2418, 13, 50914, 50914, 2264, 11, 456, 311, 572, 2418, 13, 51064, 51064, 407, 1936, 11, 300, 311, 570, 456, 366, 572, 22027, 300, 25408, 3673, 1958, 13, 8494, 294, 2139, 3513, 13, 51414, 51414, 407, 309, 311, 767, 257, 1238, 8681, 1081, 281, 312, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08193373212627336, "compression_ratio": 1.5726872246696035, "no_speech_prob": 2.7968548238277435e-05}, {"id": 306, "seek": 150700, "start": 1521.0, "end": 1528.0, "text": " So basically, that's because there are no neurons that saturated above 0.99 in either direction.", "tokens": [50364, 509, 536, 586, 11, 570, 321, 12972, 343, 538, 1958, 13, 16, 11, 321, 362, 257, 709, 1101, 49816, 13, 50564, 50564, 400, 300, 311, 570, 264, 4515, 592, 763, 366, 586, 1296, 3671, 502, 13, 20, 293, 502, 13, 20, 13, 50764, 50764, 400, 341, 321, 2066, 709, 11, 709, 1570, 2418, 13, 50914, 50914, 2264, 11, 456, 311, 572, 2418, 13, 51064, 51064, 407, 1936, 11, 300, 311, 570, 456, 366, 572, 22027, 300, 25408, 3673, 1958, 13, 8494, 294, 2139, 3513, 13, 51414, 51414, 407, 309, 311, 767, 257, 1238, 8681, 1081, 281, 312, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08193373212627336, "compression_ratio": 1.5726872246696035, "no_speech_prob": 2.7968548238277435e-05}, {"id": 307, "seek": 150700, "start": 1528.0, "end": 1532.0, "text": " So it's actually a pretty decent place to be.", "tokens": [50364, 509, 536, 586, 11, 570, 321, 12972, 343, 538, 1958, 13, 16, 11, 321, 362, 257, 709, 1101, 49816, 13, 50564, 50564, 400, 300, 311, 570, 264, 4515, 592, 763, 366, 586, 1296, 3671, 502, 13, 20, 293, 502, 13, 20, 13, 50764, 50764, 400, 341, 321, 2066, 709, 11, 709, 1570, 2418, 13, 50914, 50914, 2264, 11, 456, 311, 572, 2418, 13, 51064, 51064, 407, 1936, 11, 300, 311, 570, 456, 366, 572, 22027, 300, 25408, 3673, 1958, 13, 8494, 294, 2139, 3513, 13, 51414, 51414, 407, 309, 311, 767, 257, 1238, 8681, 1081, 281, 312, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08193373212627336, "compression_ratio": 1.5726872246696035, "no_speech_prob": 2.7968548238277435e-05}, {"id": 308, "seek": 153200, "start": 1532.0, "end": 1537.0, "text": " Maybe we can go up a little bit.", "tokens": [50364, 2704, 321, 393, 352, 493, 257, 707, 857, 13, 50614, 50614, 4919, 11, 669, 286, 4473, 343, 16, 510, 30, 50714, 50714, 407, 1310, 321, 393, 352, 281, 1958, 13, 17, 13, 50864, 50864, 2264, 11, 370, 1310, 746, 411, 341, 307, 257, 1481, 7316, 13, 51064, 51064, 407, 1310, 341, 307, 437, 527, 5883, 2144, 820, 312, 13, 51214, 51214, 407, 718, 385, 586, 23525, 613, 13, 51414, 51414, 400, 718, 385, 11, 2891, 365, 5883, 2144, 11, 718, 385, 1190, 264, 1577, 19618, 1553, 264, 1821, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0584437616409794, "compression_ratio": 1.6275510204081634, "no_speech_prob": 3.5559330626711017e-06}, {"id": 309, "seek": 153200, "start": 1537.0, "end": 1539.0, "text": " Sorry, am I changing W1 here?", "tokens": [50364, 2704, 321, 393, 352, 493, 257, 707, 857, 13, 50614, 50614, 4919, 11, 669, 286, 4473, 343, 16, 510, 30, 50714, 50714, 407, 1310, 321, 393, 352, 281, 1958, 13, 17, 13, 50864, 50864, 2264, 11, 370, 1310, 746, 411, 341, 307, 257, 1481, 7316, 13, 51064, 51064, 407, 1310, 341, 307, 437, 527, 5883, 2144, 820, 312, 13, 51214, 51214, 407, 718, 385, 586, 23525, 613, 13, 51414, 51414, 400, 718, 385, 11, 2891, 365, 5883, 2144, 11, 718, 385, 1190, 264, 1577, 19618, 1553, 264, 1821, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0584437616409794, "compression_ratio": 1.6275510204081634, "no_speech_prob": 3.5559330626711017e-06}, {"id": 310, "seek": 153200, "start": 1539.0, "end": 1542.0, "text": " So maybe we can go to 0.2.", "tokens": [50364, 2704, 321, 393, 352, 493, 257, 707, 857, 13, 50614, 50614, 4919, 11, 669, 286, 4473, 343, 16, 510, 30, 50714, 50714, 407, 1310, 321, 393, 352, 281, 1958, 13, 17, 13, 50864, 50864, 2264, 11, 370, 1310, 746, 411, 341, 307, 257, 1481, 7316, 13, 51064, 51064, 407, 1310, 341, 307, 437, 527, 5883, 2144, 820, 312, 13, 51214, 51214, 407, 718, 385, 586, 23525, 613, 13, 51414, 51414, 400, 718, 385, 11, 2891, 365, 5883, 2144, 11, 718, 385, 1190, 264, 1577, 19618, 1553, 264, 1821, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0584437616409794, "compression_ratio": 1.6275510204081634, "no_speech_prob": 3.5559330626711017e-06}, {"id": 311, "seek": 153200, "start": 1542.0, "end": 1546.0, "text": " OK, so maybe something like this is a nice distribution.", "tokens": [50364, 2704, 321, 393, 352, 493, 257, 707, 857, 13, 50614, 50614, 4919, 11, 669, 286, 4473, 343, 16, 510, 30, 50714, 50714, 407, 1310, 321, 393, 352, 281, 1958, 13, 17, 13, 50864, 50864, 2264, 11, 370, 1310, 746, 411, 341, 307, 257, 1481, 7316, 13, 51064, 51064, 407, 1310, 341, 307, 437, 527, 5883, 2144, 820, 312, 13, 51214, 51214, 407, 718, 385, 586, 23525, 613, 13, 51414, 51414, 400, 718, 385, 11, 2891, 365, 5883, 2144, 11, 718, 385, 1190, 264, 1577, 19618, 1553, 264, 1821, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0584437616409794, "compression_ratio": 1.6275510204081634, "no_speech_prob": 3.5559330626711017e-06}, {"id": 312, "seek": 153200, "start": 1546.0, "end": 1549.0, "text": " So maybe this is what our initialization should be.", "tokens": [50364, 2704, 321, 393, 352, 493, 257, 707, 857, 13, 50614, 50614, 4919, 11, 669, 286, 4473, 343, 16, 510, 30, 50714, 50714, 407, 1310, 321, 393, 352, 281, 1958, 13, 17, 13, 50864, 50864, 2264, 11, 370, 1310, 746, 411, 341, 307, 257, 1481, 7316, 13, 51064, 51064, 407, 1310, 341, 307, 437, 527, 5883, 2144, 820, 312, 13, 51214, 51214, 407, 718, 385, 586, 23525, 613, 13, 51414, 51414, 400, 718, 385, 11, 2891, 365, 5883, 2144, 11, 718, 385, 1190, 264, 1577, 19618, 1553, 264, 1821, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0584437616409794, "compression_ratio": 1.6275510204081634, "no_speech_prob": 3.5559330626711017e-06}, {"id": 313, "seek": 153200, "start": 1549.0, "end": 1553.0, "text": " So let me now erase these.", "tokens": [50364, 2704, 321, 393, 352, 493, 257, 707, 857, 13, 50614, 50614, 4919, 11, 669, 286, 4473, 343, 16, 510, 30, 50714, 50714, 407, 1310, 321, 393, 352, 281, 1958, 13, 17, 13, 50864, 50864, 2264, 11, 370, 1310, 746, 411, 341, 307, 257, 1481, 7316, 13, 51064, 51064, 407, 1310, 341, 307, 437, 527, 5883, 2144, 820, 312, 13, 51214, 51214, 407, 718, 385, 586, 23525, 613, 13, 51414, 51414, 400, 718, 385, 11, 2891, 365, 5883, 2144, 11, 718, 385, 1190, 264, 1577, 19618, 1553, 264, 1821, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0584437616409794, "compression_ratio": 1.6275510204081634, "no_speech_prob": 3.5559330626711017e-06}, {"id": 314, "seek": 153200, "start": 1553.0, "end": 1560.0, "text": " And let me, starting with initialization, let me run the full optimization without the break.", "tokens": [50364, 2704, 321, 393, 352, 493, 257, 707, 857, 13, 50614, 50614, 4919, 11, 669, 286, 4473, 343, 16, 510, 30, 50714, 50714, 407, 1310, 321, 393, 352, 281, 1958, 13, 17, 13, 50864, 50864, 2264, 11, 370, 1310, 746, 411, 341, 307, 257, 1481, 7316, 13, 51064, 51064, 407, 1310, 341, 307, 437, 527, 5883, 2144, 820, 312, 13, 51214, 51214, 407, 718, 385, 586, 23525, 613, 13, 51414, 51414, 400, 718, 385, 11, 2891, 365, 5883, 2144, 11, 718, 385, 1190, 264, 1577, 19618, 1553, 264, 1821, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0584437616409794, "compression_ratio": 1.6275510204081634, "no_speech_prob": 3.5559330626711017e-06}, {"id": 315, "seek": 156000, "start": 1560.0, "end": 1563.0, "text": " And let's see what we get.", "tokens": [50364, 400, 718, 311, 536, 437, 321, 483, 13, 50514, 50514, 2264, 11, 370, 264, 19618, 4335, 13, 50614, 50614, 400, 286, 43819, 409, 264, 4470, 13, 50664, 50664, 400, 341, 307, 264, 1874, 300, 321, 483, 13, 50764, 50764, 400, 550, 445, 382, 257, 13548, 11, 286, 829, 760, 439, 264, 15352, 300, 321, 1866, 8046, 294, 341, 7991, 13, 50964, 50964, 407, 321, 536, 300, 321, 767, 360, 483, 364, 10444, 510, 13, 51114, 51114, 400, 445, 382, 257, 13548, 11, 321, 1409, 766, 365, 257, 24071, 4470, 295, 568, 13, 7773, 562, 321, 1409, 13, 51364, 51364, 3146, 19442, 264, 2787, 41167, 885, 41956, 2085, 11, 321, 1361, 760, 281, 568, 13, 7668, 13, 51564, 51564, 400, 538, 19442, 264, 1266, 39, 4583, 885, 636, 886, 25408, 11, 321, 1361, 760, 281, 568, 13, 3279, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.03887599958500392, "compression_ratio": 1.7695035460992907, "no_speech_prob": 1.5206452189886477e-05}, {"id": 316, "seek": 156000, "start": 1563.0, "end": 1565.0, "text": " OK, so the optimization finished.", "tokens": [50364, 400, 718, 311, 536, 437, 321, 483, 13, 50514, 50514, 2264, 11, 370, 264, 19618, 4335, 13, 50614, 50614, 400, 286, 43819, 409, 264, 4470, 13, 50664, 50664, 400, 341, 307, 264, 1874, 300, 321, 483, 13, 50764, 50764, 400, 550, 445, 382, 257, 13548, 11, 286, 829, 760, 439, 264, 15352, 300, 321, 1866, 8046, 294, 341, 7991, 13, 50964, 50964, 407, 321, 536, 300, 321, 767, 360, 483, 364, 10444, 510, 13, 51114, 51114, 400, 445, 382, 257, 13548, 11, 321, 1409, 766, 365, 257, 24071, 4470, 295, 568, 13, 7773, 562, 321, 1409, 13, 51364, 51364, 3146, 19442, 264, 2787, 41167, 885, 41956, 2085, 11, 321, 1361, 760, 281, 568, 13, 7668, 13, 51564, 51564, 400, 538, 19442, 264, 1266, 39, 4583, 885, 636, 886, 25408, 11, 321, 1361, 760, 281, 568, 13, 3279, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.03887599958500392, "compression_ratio": 1.7695035460992907, "no_speech_prob": 1.5206452189886477e-05}, {"id": 317, "seek": 156000, "start": 1565.0, "end": 1566.0, "text": " And I rerun the loss.", "tokens": [50364, 400, 718, 311, 536, 437, 321, 483, 13, 50514, 50514, 2264, 11, 370, 264, 19618, 4335, 13, 50614, 50614, 400, 286, 43819, 409, 264, 4470, 13, 50664, 50664, 400, 341, 307, 264, 1874, 300, 321, 483, 13, 50764, 50764, 400, 550, 445, 382, 257, 13548, 11, 286, 829, 760, 439, 264, 15352, 300, 321, 1866, 8046, 294, 341, 7991, 13, 50964, 50964, 407, 321, 536, 300, 321, 767, 360, 483, 364, 10444, 510, 13, 51114, 51114, 400, 445, 382, 257, 13548, 11, 321, 1409, 766, 365, 257, 24071, 4470, 295, 568, 13, 7773, 562, 321, 1409, 13, 51364, 51364, 3146, 19442, 264, 2787, 41167, 885, 41956, 2085, 11, 321, 1361, 760, 281, 568, 13, 7668, 13, 51564, 51564, 400, 538, 19442, 264, 1266, 39, 4583, 885, 636, 886, 25408, 11, 321, 1361, 760, 281, 568, 13, 3279, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.03887599958500392, "compression_ratio": 1.7695035460992907, "no_speech_prob": 1.5206452189886477e-05}, {"id": 318, "seek": 156000, "start": 1566.0, "end": 1568.0, "text": " And this is the result that we get.", "tokens": [50364, 400, 718, 311, 536, 437, 321, 483, 13, 50514, 50514, 2264, 11, 370, 264, 19618, 4335, 13, 50614, 50614, 400, 286, 43819, 409, 264, 4470, 13, 50664, 50664, 400, 341, 307, 264, 1874, 300, 321, 483, 13, 50764, 50764, 400, 550, 445, 382, 257, 13548, 11, 286, 829, 760, 439, 264, 15352, 300, 321, 1866, 8046, 294, 341, 7991, 13, 50964, 50964, 407, 321, 536, 300, 321, 767, 360, 483, 364, 10444, 510, 13, 51114, 51114, 400, 445, 382, 257, 13548, 11, 321, 1409, 766, 365, 257, 24071, 4470, 295, 568, 13, 7773, 562, 321, 1409, 13, 51364, 51364, 3146, 19442, 264, 2787, 41167, 885, 41956, 2085, 11, 321, 1361, 760, 281, 568, 13, 7668, 13, 51564, 51564, 400, 538, 19442, 264, 1266, 39, 4583, 885, 636, 886, 25408, 11, 321, 1361, 760, 281, 568, 13, 3279, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.03887599958500392, "compression_ratio": 1.7695035460992907, "no_speech_prob": 1.5206452189886477e-05}, {"id": 319, "seek": 156000, "start": 1568.0, "end": 1572.0, "text": " And then just as a reminder, I put down all the losses that we saw previously in this lecture.", "tokens": [50364, 400, 718, 311, 536, 437, 321, 483, 13, 50514, 50514, 2264, 11, 370, 264, 19618, 4335, 13, 50614, 50614, 400, 286, 43819, 409, 264, 4470, 13, 50664, 50664, 400, 341, 307, 264, 1874, 300, 321, 483, 13, 50764, 50764, 400, 550, 445, 382, 257, 13548, 11, 286, 829, 760, 439, 264, 15352, 300, 321, 1866, 8046, 294, 341, 7991, 13, 50964, 50964, 407, 321, 536, 300, 321, 767, 360, 483, 364, 10444, 510, 13, 51114, 51114, 400, 445, 382, 257, 13548, 11, 321, 1409, 766, 365, 257, 24071, 4470, 295, 568, 13, 7773, 562, 321, 1409, 13, 51364, 51364, 3146, 19442, 264, 2787, 41167, 885, 41956, 2085, 11, 321, 1361, 760, 281, 568, 13, 7668, 13, 51564, 51564, 400, 538, 19442, 264, 1266, 39, 4583, 885, 636, 886, 25408, 11, 321, 1361, 760, 281, 568, 13, 3279, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.03887599958500392, "compression_ratio": 1.7695035460992907, "no_speech_prob": 1.5206452189886477e-05}, {"id": 320, "seek": 156000, "start": 1572.0, "end": 1575.0, "text": " So we see that we actually do get an improvement here.", "tokens": [50364, 400, 718, 311, 536, 437, 321, 483, 13, 50514, 50514, 2264, 11, 370, 264, 19618, 4335, 13, 50614, 50614, 400, 286, 43819, 409, 264, 4470, 13, 50664, 50664, 400, 341, 307, 264, 1874, 300, 321, 483, 13, 50764, 50764, 400, 550, 445, 382, 257, 13548, 11, 286, 829, 760, 439, 264, 15352, 300, 321, 1866, 8046, 294, 341, 7991, 13, 50964, 50964, 407, 321, 536, 300, 321, 767, 360, 483, 364, 10444, 510, 13, 51114, 51114, 400, 445, 382, 257, 13548, 11, 321, 1409, 766, 365, 257, 24071, 4470, 295, 568, 13, 7773, 562, 321, 1409, 13, 51364, 51364, 3146, 19442, 264, 2787, 41167, 885, 41956, 2085, 11, 321, 1361, 760, 281, 568, 13, 7668, 13, 51564, 51564, 400, 538, 19442, 264, 1266, 39, 4583, 885, 636, 886, 25408, 11, 321, 1361, 760, 281, 568, 13, 3279, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.03887599958500392, "compression_ratio": 1.7695035460992907, "no_speech_prob": 1.5206452189886477e-05}, {"id": 321, "seek": 156000, "start": 1575.0, "end": 1580.0, "text": " And just as a reminder, we started off with a validation loss of 2.17 when we started.", "tokens": [50364, 400, 718, 311, 536, 437, 321, 483, 13, 50514, 50514, 2264, 11, 370, 264, 19618, 4335, 13, 50614, 50614, 400, 286, 43819, 409, 264, 4470, 13, 50664, 50664, 400, 341, 307, 264, 1874, 300, 321, 483, 13, 50764, 50764, 400, 550, 445, 382, 257, 13548, 11, 286, 829, 760, 439, 264, 15352, 300, 321, 1866, 8046, 294, 341, 7991, 13, 50964, 50964, 407, 321, 536, 300, 321, 767, 360, 483, 364, 10444, 510, 13, 51114, 51114, 400, 445, 382, 257, 13548, 11, 321, 1409, 766, 365, 257, 24071, 4470, 295, 568, 13, 7773, 562, 321, 1409, 13, 51364, 51364, 3146, 19442, 264, 2787, 41167, 885, 41956, 2085, 11, 321, 1361, 760, 281, 568, 13, 7668, 13, 51564, 51564, 400, 538, 19442, 264, 1266, 39, 4583, 885, 636, 886, 25408, 11, 321, 1361, 760, 281, 568, 13, 3279, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.03887599958500392, "compression_ratio": 1.7695035460992907, "no_speech_prob": 1.5206452189886477e-05}, {"id": 322, "seek": 156000, "start": 1580.0, "end": 1584.0, "text": " By fixing the softmax being confidently wrong, we came down to 2.13.", "tokens": [50364, 400, 718, 311, 536, 437, 321, 483, 13, 50514, 50514, 2264, 11, 370, 264, 19618, 4335, 13, 50614, 50614, 400, 286, 43819, 409, 264, 4470, 13, 50664, 50664, 400, 341, 307, 264, 1874, 300, 321, 483, 13, 50764, 50764, 400, 550, 445, 382, 257, 13548, 11, 286, 829, 760, 439, 264, 15352, 300, 321, 1866, 8046, 294, 341, 7991, 13, 50964, 50964, 407, 321, 536, 300, 321, 767, 360, 483, 364, 10444, 510, 13, 51114, 51114, 400, 445, 382, 257, 13548, 11, 321, 1409, 766, 365, 257, 24071, 4470, 295, 568, 13, 7773, 562, 321, 1409, 13, 51364, 51364, 3146, 19442, 264, 2787, 41167, 885, 41956, 2085, 11, 321, 1361, 760, 281, 568, 13, 7668, 13, 51564, 51564, 400, 538, 19442, 264, 1266, 39, 4583, 885, 636, 886, 25408, 11, 321, 1361, 760, 281, 568, 13, 3279, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.03887599958500392, "compression_ratio": 1.7695035460992907, "no_speech_prob": 1.5206452189886477e-05}, {"id": 323, "seek": 156000, "start": 1584.0, "end": 1589.0, "text": " And by fixing the 10H layer being way too saturated, we came down to 2.10.", "tokens": [50364, 400, 718, 311, 536, 437, 321, 483, 13, 50514, 50514, 2264, 11, 370, 264, 19618, 4335, 13, 50614, 50614, 400, 286, 43819, 409, 264, 4470, 13, 50664, 50664, 400, 341, 307, 264, 1874, 300, 321, 483, 13, 50764, 50764, 400, 550, 445, 382, 257, 13548, 11, 286, 829, 760, 439, 264, 15352, 300, 321, 1866, 8046, 294, 341, 7991, 13, 50964, 50964, 407, 321, 536, 300, 321, 767, 360, 483, 364, 10444, 510, 13, 51114, 51114, 400, 445, 382, 257, 13548, 11, 321, 1409, 766, 365, 257, 24071, 4470, 295, 568, 13, 7773, 562, 321, 1409, 13, 51364, 51364, 3146, 19442, 264, 2787, 41167, 885, 41956, 2085, 11, 321, 1361, 760, 281, 568, 13, 7668, 13, 51564, 51564, 400, 538, 19442, 264, 1266, 39, 4583, 885, 636, 886, 25408, 11, 321, 1361, 760, 281, 568, 13, 3279, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.03887599958500392, "compression_ratio": 1.7695035460992907, "no_speech_prob": 1.5206452189886477e-05}, {"id": 324, "seek": 158900, "start": 1589.0, "end": 1592.0, "text": " And the reason this is happening, of course, is because our initialization is better.", "tokens": [50364, 400, 264, 1778, 341, 307, 2737, 11, 295, 1164, 11, 307, 570, 527, 5883, 2144, 307, 1101, 13, 50514, 50514, 400, 370, 321, 434, 6434, 544, 565, 884, 13304, 3097, 2602, 295, 406, 588, 13304, 3097, 11, 50814, 50814, 570, 527, 2771, 2448, 366, 992, 281, 1958, 13, 50914, 50914, 400, 321, 362, 281, 1466, 588, 2199, 721, 411, 264, 670, 47273, 295, 264, 2787, 41167, 294, 264, 2863, 13, 51164, 51164, 400, 321, 434, 6434, 17796, 445, 411, 2339, 11077, 760, 264, 3364, 8141, 13, 51364, 51364, 407, 341, 307, 8490, 8754, 1936, 5883, 2144, 293, 1080, 11606, 322, 3389, 445, 538, 885, 3650, 295, 264, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06098714175525012, "compression_ratio": 1.7963636363636364, "no_speech_prob": 8.397380042879377e-06}, {"id": 325, "seek": 158900, "start": 1592.0, "end": 1598.0, "text": " And so we're spending more time doing productive training instead of not very productive training,", "tokens": [50364, 400, 264, 1778, 341, 307, 2737, 11, 295, 1164, 11, 307, 570, 527, 5883, 2144, 307, 1101, 13, 50514, 50514, 400, 370, 321, 434, 6434, 544, 565, 884, 13304, 3097, 2602, 295, 406, 588, 13304, 3097, 11, 50814, 50814, 570, 527, 2771, 2448, 366, 992, 281, 1958, 13, 50914, 50914, 400, 321, 362, 281, 1466, 588, 2199, 721, 411, 264, 670, 47273, 295, 264, 2787, 41167, 294, 264, 2863, 13, 51164, 51164, 400, 321, 434, 6434, 17796, 445, 411, 2339, 11077, 760, 264, 3364, 8141, 13, 51364, 51364, 407, 341, 307, 8490, 8754, 1936, 5883, 2144, 293, 1080, 11606, 322, 3389, 445, 538, 885, 3650, 295, 264, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06098714175525012, "compression_ratio": 1.7963636363636364, "no_speech_prob": 8.397380042879377e-06}, {"id": 326, "seek": 158900, "start": 1598.0, "end": 1600.0, "text": " because our gradients are set to 0.", "tokens": [50364, 400, 264, 1778, 341, 307, 2737, 11, 295, 1164, 11, 307, 570, 527, 5883, 2144, 307, 1101, 13, 50514, 50514, 400, 370, 321, 434, 6434, 544, 565, 884, 13304, 3097, 2602, 295, 406, 588, 13304, 3097, 11, 50814, 50814, 570, 527, 2771, 2448, 366, 992, 281, 1958, 13, 50914, 50914, 400, 321, 362, 281, 1466, 588, 2199, 721, 411, 264, 670, 47273, 295, 264, 2787, 41167, 294, 264, 2863, 13, 51164, 51164, 400, 321, 434, 6434, 17796, 445, 411, 2339, 11077, 760, 264, 3364, 8141, 13, 51364, 51364, 407, 341, 307, 8490, 8754, 1936, 5883, 2144, 293, 1080, 11606, 322, 3389, 445, 538, 885, 3650, 295, 264, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06098714175525012, "compression_ratio": 1.7963636363636364, "no_speech_prob": 8.397380042879377e-06}, {"id": 327, "seek": 158900, "start": 1600.0, "end": 1605.0, "text": " And we have to learn very simple things like the overconfidence of the softmax in the beginning.", "tokens": [50364, 400, 264, 1778, 341, 307, 2737, 11, 295, 1164, 11, 307, 570, 527, 5883, 2144, 307, 1101, 13, 50514, 50514, 400, 370, 321, 434, 6434, 544, 565, 884, 13304, 3097, 2602, 295, 406, 588, 13304, 3097, 11, 50814, 50814, 570, 527, 2771, 2448, 366, 992, 281, 1958, 13, 50914, 50914, 400, 321, 362, 281, 1466, 588, 2199, 721, 411, 264, 670, 47273, 295, 264, 2787, 41167, 294, 264, 2863, 13, 51164, 51164, 400, 321, 434, 6434, 17796, 445, 411, 2339, 11077, 760, 264, 3364, 8141, 13, 51364, 51364, 407, 341, 307, 8490, 8754, 1936, 5883, 2144, 293, 1080, 11606, 322, 3389, 445, 538, 885, 3650, 295, 264, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06098714175525012, "compression_ratio": 1.7963636363636364, "no_speech_prob": 8.397380042879377e-06}, {"id": 328, "seek": 158900, "start": 1605.0, "end": 1609.0, "text": " And we're spending cycles just like squashing down the weight matrix.", "tokens": [50364, 400, 264, 1778, 341, 307, 2737, 11, 295, 1164, 11, 307, 570, 527, 5883, 2144, 307, 1101, 13, 50514, 50514, 400, 370, 321, 434, 6434, 544, 565, 884, 13304, 3097, 2602, 295, 406, 588, 13304, 3097, 11, 50814, 50814, 570, 527, 2771, 2448, 366, 992, 281, 1958, 13, 50914, 50914, 400, 321, 362, 281, 1466, 588, 2199, 721, 411, 264, 670, 47273, 295, 264, 2787, 41167, 294, 264, 2863, 13, 51164, 51164, 400, 321, 434, 6434, 17796, 445, 411, 2339, 11077, 760, 264, 3364, 8141, 13, 51364, 51364, 407, 341, 307, 8490, 8754, 1936, 5883, 2144, 293, 1080, 11606, 322, 3389, 445, 538, 885, 3650, 295, 264, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06098714175525012, "compression_ratio": 1.7963636363636364, "no_speech_prob": 8.397380042879377e-06}, {"id": 329, "seek": 158900, "start": 1609.0, "end": 1617.0, "text": " So this is illustrating basically initialization and its impacts on performance just by being aware of the", "tokens": [50364, 400, 264, 1778, 341, 307, 2737, 11, 295, 1164, 11, 307, 570, 527, 5883, 2144, 307, 1101, 13, 50514, 50514, 400, 370, 321, 434, 6434, 544, 565, 884, 13304, 3097, 2602, 295, 406, 588, 13304, 3097, 11, 50814, 50814, 570, 527, 2771, 2448, 366, 992, 281, 1958, 13, 50914, 50914, 400, 321, 362, 281, 1466, 588, 2199, 721, 411, 264, 670, 47273, 295, 264, 2787, 41167, 294, 264, 2863, 13, 51164, 51164, 400, 321, 434, 6434, 17796, 445, 411, 2339, 11077, 760, 264, 3364, 8141, 13, 51364, 51364, 407, 341, 307, 8490, 8754, 1936, 5883, 2144, 293, 1080, 11606, 322, 3389, 445, 538, 885, 3650, 295, 264, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06098714175525012, "compression_ratio": 1.7963636363636364, "no_speech_prob": 8.397380042879377e-06}, {"id": 330, "seek": 161700, "start": 1617.0, "end": 1620.0, "text": " internals of these neural nets and their activations and their gradients.", "tokens": [50364, 2154, 1124, 295, 613, 18161, 36170, 293, 641, 2430, 763, 293, 641, 2771, 2448, 13, 50514, 50514, 823, 11, 321, 434, 1364, 365, 257, 588, 1359, 3209, 13, 50664, 50664, 639, 307, 445, 472, 4583, 2120, 388, 11167, 12860, 13, 50764, 50764, 407, 570, 264, 3209, 307, 370, 20488, 11, 264, 19618, 1154, 307, 767, 1596, 1858, 293, 588, 37701, 13, 51064, 51064, 407, 754, 1673, 527, 5883, 2144, 390, 6237, 11, 264, 3209, 920, 3264, 4728, 13, 51264, 51264, 467, 445, 658, 257, 857, 5324, 1874, 13, 51364, 51364, 639, 307, 406, 264, 1389, 294, 2674, 11, 1673, 13, 51464, 51464, 3443, 321, 767, 722, 1364, 365, 709, 7731, 9590, 300, 362, 11, 584, 11, 2625, 7914, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05256179483925424, "compression_ratio": 1.7423728813559323, "no_speech_prob": 3.373391882632859e-05}, {"id": 331, "seek": 161700, "start": 1620.0, "end": 1623.0, "text": " Now, we're working with a very small network.", "tokens": [50364, 2154, 1124, 295, 613, 18161, 36170, 293, 641, 2430, 763, 293, 641, 2771, 2448, 13, 50514, 50514, 823, 11, 321, 434, 1364, 365, 257, 588, 1359, 3209, 13, 50664, 50664, 639, 307, 445, 472, 4583, 2120, 388, 11167, 12860, 13, 50764, 50764, 407, 570, 264, 3209, 307, 370, 20488, 11, 264, 19618, 1154, 307, 767, 1596, 1858, 293, 588, 37701, 13, 51064, 51064, 407, 754, 1673, 527, 5883, 2144, 390, 6237, 11, 264, 3209, 920, 3264, 4728, 13, 51264, 51264, 467, 445, 658, 257, 857, 5324, 1874, 13, 51364, 51364, 639, 307, 406, 264, 1389, 294, 2674, 11, 1673, 13, 51464, 51464, 3443, 321, 767, 722, 1364, 365, 709, 7731, 9590, 300, 362, 11, 584, 11, 2625, 7914, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05256179483925424, "compression_ratio": 1.7423728813559323, "no_speech_prob": 3.373391882632859e-05}, {"id": 332, "seek": 161700, "start": 1623.0, "end": 1625.0, "text": " This is just one layer multilayer perception.", "tokens": [50364, 2154, 1124, 295, 613, 18161, 36170, 293, 641, 2430, 763, 293, 641, 2771, 2448, 13, 50514, 50514, 823, 11, 321, 434, 1364, 365, 257, 588, 1359, 3209, 13, 50664, 50664, 639, 307, 445, 472, 4583, 2120, 388, 11167, 12860, 13, 50764, 50764, 407, 570, 264, 3209, 307, 370, 20488, 11, 264, 19618, 1154, 307, 767, 1596, 1858, 293, 588, 37701, 13, 51064, 51064, 407, 754, 1673, 527, 5883, 2144, 390, 6237, 11, 264, 3209, 920, 3264, 4728, 13, 51264, 51264, 467, 445, 658, 257, 857, 5324, 1874, 13, 51364, 51364, 639, 307, 406, 264, 1389, 294, 2674, 11, 1673, 13, 51464, 51464, 3443, 321, 767, 722, 1364, 365, 709, 7731, 9590, 300, 362, 11, 584, 11, 2625, 7914, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05256179483925424, "compression_ratio": 1.7423728813559323, "no_speech_prob": 3.373391882632859e-05}, {"id": 333, "seek": 161700, "start": 1625.0, "end": 1631.0, "text": " So because the network is so shallow, the optimization problem is actually quite easy and very forgiving.", "tokens": [50364, 2154, 1124, 295, 613, 18161, 36170, 293, 641, 2430, 763, 293, 641, 2771, 2448, 13, 50514, 50514, 823, 11, 321, 434, 1364, 365, 257, 588, 1359, 3209, 13, 50664, 50664, 639, 307, 445, 472, 4583, 2120, 388, 11167, 12860, 13, 50764, 50764, 407, 570, 264, 3209, 307, 370, 20488, 11, 264, 19618, 1154, 307, 767, 1596, 1858, 293, 588, 37701, 13, 51064, 51064, 407, 754, 1673, 527, 5883, 2144, 390, 6237, 11, 264, 3209, 920, 3264, 4728, 13, 51264, 51264, 467, 445, 658, 257, 857, 5324, 1874, 13, 51364, 51364, 639, 307, 406, 264, 1389, 294, 2674, 11, 1673, 13, 51464, 51464, 3443, 321, 767, 722, 1364, 365, 709, 7731, 9590, 300, 362, 11, 584, 11, 2625, 7914, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05256179483925424, "compression_ratio": 1.7423728813559323, "no_speech_prob": 3.373391882632859e-05}, {"id": 334, "seek": 161700, "start": 1631.0, "end": 1635.0, "text": " So even though our initialization was terrible, the network still learned eventually.", "tokens": [50364, 2154, 1124, 295, 613, 18161, 36170, 293, 641, 2430, 763, 293, 641, 2771, 2448, 13, 50514, 50514, 823, 11, 321, 434, 1364, 365, 257, 588, 1359, 3209, 13, 50664, 50664, 639, 307, 445, 472, 4583, 2120, 388, 11167, 12860, 13, 50764, 50764, 407, 570, 264, 3209, 307, 370, 20488, 11, 264, 19618, 1154, 307, 767, 1596, 1858, 293, 588, 37701, 13, 51064, 51064, 407, 754, 1673, 527, 5883, 2144, 390, 6237, 11, 264, 3209, 920, 3264, 4728, 13, 51264, 51264, 467, 445, 658, 257, 857, 5324, 1874, 13, 51364, 51364, 639, 307, 406, 264, 1389, 294, 2674, 11, 1673, 13, 51464, 51464, 3443, 321, 767, 722, 1364, 365, 709, 7731, 9590, 300, 362, 11, 584, 11, 2625, 7914, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05256179483925424, "compression_ratio": 1.7423728813559323, "no_speech_prob": 3.373391882632859e-05}, {"id": 335, "seek": 161700, "start": 1635.0, "end": 1637.0, "text": " It just got a bit worse result.", "tokens": [50364, 2154, 1124, 295, 613, 18161, 36170, 293, 641, 2430, 763, 293, 641, 2771, 2448, 13, 50514, 50514, 823, 11, 321, 434, 1364, 365, 257, 588, 1359, 3209, 13, 50664, 50664, 639, 307, 445, 472, 4583, 2120, 388, 11167, 12860, 13, 50764, 50764, 407, 570, 264, 3209, 307, 370, 20488, 11, 264, 19618, 1154, 307, 767, 1596, 1858, 293, 588, 37701, 13, 51064, 51064, 407, 754, 1673, 527, 5883, 2144, 390, 6237, 11, 264, 3209, 920, 3264, 4728, 13, 51264, 51264, 467, 445, 658, 257, 857, 5324, 1874, 13, 51364, 51364, 639, 307, 406, 264, 1389, 294, 2674, 11, 1673, 13, 51464, 51464, 3443, 321, 767, 722, 1364, 365, 709, 7731, 9590, 300, 362, 11, 584, 11, 2625, 7914, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05256179483925424, "compression_ratio": 1.7423728813559323, "no_speech_prob": 3.373391882632859e-05}, {"id": 336, "seek": 161700, "start": 1637.0, "end": 1639.0, "text": " This is not the case in general, though.", "tokens": [50364, 2154, 1124, 295, 613, 18161, 36170, 293, 641, 2430, 763, 293, 641, 2771, 2448, 13, 50514, 50514, 823, 11, 321, 434, 1364, 365, 257, 588, 1359, 3209, 13, 50664, 50664, 639, 307, 445, 472, 4583, 2120, 388, 11167, 12860, 13, 50764, 50764, 407, 570, 264, 3209, 307, 370, 20488, 11, 264, 19618, 1154, 307, 767, 1596, 1858, 293, 588, 37701, 13, 51064, 51064, 407, 754, 1673, 527, 5883, 2144, 390, 6237, 11, 264, 3209, 920, 3264, 4728, 13, 51264, 51264, 467, 445, 658, 257, 857, 5324, 1874, 13, 51364, 51364, 639, 307, 406, 264, 1389, 294, 2674, 11, 1673, 13, 51464, 51464, 3443, 321, 767, 722, 1364, 365, 709, 7731, 9590, 300, 362, 11, 584, 11, 2625, 7914, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05256179483925424, "compression_ratio": 1.7423728813559323, "no_speech_prob": 3.373391882632859e-05}, {"id": 337, "seek": 161700, "start": 1639.0, "end": 1644.0, "text": " Once we actually start working with much deeper networks that have, say, 50 layers,", "tokens": [50364, 2154, 1124, 295, 613, 18161, 36170, 293, 641, 2430, 763, 293, 641, 2771, 2448, 13, 50514, 50514, 823, 11, 321, 434, 1364, 365, 257, 588, 1359, 3209, 13, 50664, 50664, 639, 307, 445, 472, 4583, 2120, 388, 11167, 12860, 13, 50764, 50764, 407, 570, 264, 3209, 307, 370, 20488, 11, 264, 19618, 1154, 307, 767, 1596, 1858, 293, 588, 37701, 13, 51064, 51064, 407, 754, 1673, 527, 5883, 2144, 390, 6237, 11, 264, 3209, 920, 3264, 4728, 13, 51264, 51264, 467, 445, 658, 257, 857, 5324, 1874, 13, 51364, 51364, 639, 307, 406, 264, 1389, 294, 2674, 11, 1673, 13, 51464, 51464, 3443, 321, 767, 722, 1364, 365, 709, 7731, 9590, 300, 362, 11, 584, 11, 2625, 7914, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05256179483925424, "compression_ratio": 1.7423728813559323, "no_speech_prob": 3.373391882632859e-05}, {"id": 338, "seek": 164400, "start": 1644.0, "end": 1650.0, "text": " things can get much more complicated and these problems stack up.", "tokens": [50364, 721, 393, 483, 709, 544, 6179, 293, 613, 2740, 8630, 493, 13, 50664, 50664, 400, 370, 291, 393, 767, 483, 666, 257, 1081, 689, 264, 3209, 307, 1936, 406, 3097, 412, 439, 498, 428, 5883, 2144, 307, 1578, 1547, 13, 51014, 51014, 400, 264, 7731, 428, 3209, 307, 293, 264, 544, 3997, 309, 307, 11, 264, 1570, 37701, 309, 307, 281, 512, 295, 613, 13603, 13, 51314, 51314, 400, 370, 746, 281, 312, 2138, 3650, 295, 293, 746, 281, 28949, 259, 1125, 11, 746, 281, 7542, 293, 746, 281, 312, 5026, 365, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06593628519589138, "compression_ratio": 1.829059829059829, "no_speech_prob": 1.2804452126147225e-05}, {"id": 339, "seek": 164400, "start": 1650.0, "end": 1657.0, "text": " And so you can actually get into a place where the network is basically not training at all if your initialization is bad enough.", "tokens": [50364, 721, 393, 483, 709, 544, 6179, 293, 613, 2740, 8630, 493, 13, 50664, 50664, 400, 370, 291, 393, 767, 483, 666, 257, 1081, 689, 264, 3209, 307, 1936, 406, 3097, 412, 439, 498, 428, 5883, 2144, 307, 1578, 1547, 13, 51014, 51014, 400, 264, 7731, 428, 3209, 307, 293, 264, 544, 3997, 309, 307, 11, 264, 1570, 37701, 309, 307, 281, 512, 295, 613, 13603, 13, 51314, 51314, 400, 370, 746, 281, 312, 2138, 3650, 295, 293, 746, 281, 28949, 259, 1125, 11, 746, 281, 7542, 293, 746, 281, 312, 5026, 365, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06593628519589138, "compression_ratio": 1.829059829059829, "no_speech_prob": 1.2804452126147225e-05}, {"id": 340, "seek": 164400, "start": 1657.0, "end": 1663.0, "text": " And the deeper your network is and the more complex it is, the less forgiving it is to some of these errors.", "tokens": [50364, 721, 393, 483, 709, 544, 6179, 293, 613, 2740, 8630, 493, 13, 50664, 50664, 400, 370, 291, 393, 767, 483, 666, 257, 1081, 689, 264, 3209, 307, 1936, 406, 3097, 412, 439, 498, 428, 5883, 2144, 307, 1578, 1547, 13, 51014, 51014, 400, 264, 7731, 428, 3209, 307, 293, 264, 544, 3997, 309, 307, 11, 264, 1570, 37701, 309, 307, 281, 512, 295, 613, 13603, 13, 51314, 51314, 400, 370, 746, 281, 312, 2138, 3650, 295, 293, 746, 281, 28949, 259, 1125, 11, 746, 281, 7542, 293, 746, 281, 312, 5026, 365, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06593628519589138, "compression_ratio": 1.829059829059829, "no_speech_prob": 1.2804452126147225e-05}, {"id": 341, "seek": 164400, "start": 1663.0, "end": 1671.0, "text": " And so something to be definitely aware of and something to scrutinize, something to plot and something to be careful with.", "tokens": [50364, 721, 393, 483, 709, 544, 6179, 293, 613, 2740, 8630, 493, 13, 50664, 50664, 400, 370, 291, 393, 767, 483, 666, 257, 1081, 689, 264, 3209, 307, 1936, 406, 3097, 412, 439, 498, 428, 5883, 2144, 307, 1578, 1547, 13, 51014, 51014, 400, 264, 7731, 428, 3209, 307, 293, 264, 544, 3997, 309, 307, 11, 264, 1570, 37701, 309, 307, 281, 512, 295, 613, 13603, 13, 51314, 51314, 400, 370, 746, 281, 312, 2138, 3650, 295, 293, 746, 281, 28949, 259, 1125, 11, 746, 281, 7542, 293, 746, 281, 312, 5026, 365, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06593628519589138, "compression_ratio": 1.829059829059829, "no_speech_prob": 1.2804452126147225e-05}, {"id": 342, "seek": 167100, "start": 1671.0, "end": 1675.0, "text": " And yeah. OK, so that's great that that worked for us.", "tokens": [50364, 400, 1338, 13, 2264, 11, 370, 300, 311, 869, 300, 300, 2732, 337, 505, 13, 50564, 50564, 583, 437, 321, 362, 510, 586, 307, 439, 613, 5585, 3547, 411, 935, 732, 13, 50764, 50764, 1743, 11, 689, 360, 286, 808, 493, 365, 341, 293, 577, 669, 286, 3442, 281, 992, 613, 498, 286, 362, 257, 2416, 18161, 2533, 365, 3195, 293, 3195, 295, 7914, 30, 51064, 51064, 400, 370, 2745, 572, 472, 775, 341, 538, 1011, 13, 51164, 51164, 821, 311, 767, 512, 7226, 3681, 15551, 2098, 295, 3287, 613, 17408, 300, 286, 576, 411, 281, 5366, 281, 291, 586, 13, 51514, 51514, 407, 718, 385, 9163, 512, 3089, 510, 300, 286, 4927, 445, 281, 28497, 264, 5017, 295, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08534099578857422, "compression_ratio": 1.646864686468647, "no_speech_prob": 3.169029514538124e-05}, {"id": 343, "seek": 167100, "start": 1675.0, "end": 1679.0, "text": " But what we have here now is all these magic numbers like point two.", "tokens": [50364, 400, 1338, 13, 2264, 11, 370, 300, 311, 869, 300, 300, 2732, 337, 505, 13, 50564, 50564, 583, 437, 321, 362, 510, 586, 307, 439, 613, 5585, 3547, 411, 935, 732, 13, 50764, 50764, 1743, 11, 689, 360, 286, 808, 493, 365, 341, 293, 577, 669, 286, 3442, 281, 992, 613, 498, 286, 362, 257, 2416, 18161, 2533, 365, 3195, 293, 3195, 295, 7914, 30, 51064, 51064, 400, 370, 2745, 572, 472, 775, 341, 538, 1011, 13, 51164, 51164, 821, 311, 767, 512, 7226, 3681, 15551, 2098, 295, 3287, 613, 17408, 300, 286, 576, 411, 281, 5366, 281, 291, 586, 13, 51514, 51514, 407, 718, 385, 9163, 512, 3089, 510, 300, 286, 4927, 445, 281, 28497, 264, 5017, 295, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08534099578857422, "compression_ratio": 1.646864686468647, "no_speech_prob": 3.169029514538124e-05}, {"id": 344, "seek": 167100, "start": 1679.0, "end": 1685.0, "text": " Like, where do I come up with this and how am I supposed to set these if I have a large neural net with lots and lots of layers?", "tokens": [50364, 400, 1338, 13, 2264, 11, 370, 300, 311, 869, 300, 300, 2732, 337, 505, 13, 50564, 50564, 583, 437, 321, 362, 510, 586, 307, 439, 613, 5585, 3547, 411, 935, 732, 13, 50764, 50764, 1743, 11, 689, 360, 286, 808, 493, 365, 341, 293, 577, 669, 286, 3442, 281, 992, 613, 498, 286, 362, 257, 2416, 18161, 2533, 365, 3195, 293, 3195, 295, 7914, 30, 51064, 51064, 400, 370, 2745, 572, 472, 775, 341, 538, 1011, 13, 51164, 51164, 821, 311, 767, 512, 7226, 3681, 15551, 2098, 295, 3287, 613, 17408, 300, 286, 576, 411, 281, 5366, 281, 291, 586, 13, 51514, 51514, 407, 718, 385, 9163, 512, 3089, 510, 300, 286, 4927, 445, 281, 28497, 264, 5017, 295, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08534099578857422, "compression_ratio": 1.646864686468647, "no_speech_prob": 3.169029514538124e-05}, {"id": 345, "seek": 167100, "start": 1685.0, "end": 1687.0, "text": " And so obviously no one does this by hand.", "tokens": [50364, 400, 1338, 13, 2264, 11, 370, 300, 311, 869, 300, 300, 2732, 337, 505, 13, 50564, 50564, 583, 437, 321, 362, 510, 586, 307, 439, 613, 5585, 3547, 411, 935, 732, 13, 50764, 50764, 1743, 11, 689, 360, 286, 808, 493, 365, 341, 293, 577, 669, 286, 3442, 281, 992, 613, 498, 286, 362, 257, 2416, 18161, 2533, 365, 3195, 293, 3195, 295, 7914, 30, 51064, 51064, 400, 370, 2745, 572, 472, 775, 341, 538, 1011, 13, 51164, 51164, 821, 311, 767, 512, 7226, 3681, 15551, 2098, 295, 3287, 613, 17408, 300, 286, 576, 411, 281, 5366, 281, 291, 586, 13, 51514, 51514, 407, 718, 385, 9163, 512, 3089, 510, 300, 286, 4927, 445, 281, 28497, 264, 5017, 295, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08534099578857422, "compression_ratio": 1.646864686468647, "no_speech_prob": 3.169029514538124e-05}, {"id": 346, "seek": 167100, "start": 1687.0, "end": 1694.0, "text": " There's actually some relatively principled ways of setting these scales that I would like to introduce to you now.", "tokens": [50364, 400, 1338, 13, 2264, 11, 370, 300, 311, 869, 300, 300, 2732, 337, 505, 13, 50564, 50564, 583, 437, 321, 362, 510, 586, 307, 439, 613, 5585, 3547, 411, 935, 732, 13, 50764, 50764, 1743, 11, 689, 360, 286, 808, 493, 365, 341, 293, 577, 669, 286, 3442, 281, 992, 613, 498, 286, 362, 257, 2416, 18161, 2533, 365, 3195, 293, 3195, 295, 7914, 30, 51064, 51064, 400, 370, 2745, 572, 472, 775, 341, 538, 1011, 13, 51164, 51164, 821, 311, 767, 512, 7226, 3681, 15551, 2098, 295, 3287, 613, 17408, 300, 286, 576, 411, 281, 5366, 281, 291, 586, 13, 51514, 51514, 407, 718, 385, 9163, 512, 3089, 510, 300, 286, 4927, 445, 281, 28497, 264, 5017, 295, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08534099578857422, "compression_ratio": 1.646864686468647, "no_speech_prob": 3.169029514538124e-05}, {"id": 347, "seek": 167100, "start": 1694.0, "end": 1699.0, "text": " So let me paste some code here that I prepared just to motivate the discussion of this.", "tokens": [50364, 400, 1338, 13, 2264, 11, 370, 300, 311, 869, 300, 300, 2732, 337, 505, 13, 50564, 50564, 583, 437, 321, 362, 510, 586, 307, 439, 613, 5585, 3547, 411, 935, 732, 13, 50764, 50764, 1743, 11, 689, 360, 286, 808, 493, 365, 341, 293, 577, 669, 286, 3442, 281, 992, 613, 498, 286, 362, 257, 2416, 18161, 2533, 365, 3195, 293, 3195, 295, 7914, 30, 51064, 51064, 400, 370, 2745, 572, 472, 775, 341, 538, 1011, 13, 51164, 51164, 821, 311, 767, 512, 7226, 3681, 15551, 2098, 295, 3287, 613, 17408, 300, 286, 576, 411, 281, 5366, 281, 291, 586, 13, 51514, 51514, 407, 718, 385, 9163, 512, 3089, 510, 300, 286, 4927, 445, 281, 28497, 264, 5017, 295, 341, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08534099578857422, "compression_ratio": 1.646864686468647, "no_speech_prob": 3.169029514538124e-05}, {"id": 348, "seek": 169900, "start": 1699.0, "end": 1705.0, "text": " So what I'm doing here is we have some random input here X that is drawn from a Gaussian.", "tokens": [50364, 407, 437, 286, 478, 884, 510, 307, 321, 362, 512, 4974, 4846, 510, 1783, 300, 307, 10117, 490, 257, 39148, 13, 50664, 50664, 400, 456, 311, 472, 4714, 5110, 300, 366, 2064, 18795, 13, 50814, 50814, 400, 550, 321, 362, 257, 3364, 4583, 510, 300, 307, 611, 5883, 1602, 1228, 39148, 11, 445, 411, 321, 630, 510, 13, 51114, 51114, 400, 321, 613, 22027, 294, 264, 7633, 4583, 574, 412, 2064, 15743, 293, 456, 366, 2331, 22027, 294, 341, 4583, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.13530475392061123, "compression_ratio": 1.6714285714285715, "no_speech_prob": 2.546482028265018e-05}, {"id": 349, "seek": 169900, "start": 1705.0, "end": 1708.0, "text": " And there's one thousand examples that are ten dimensional.", "tokens": [50364, 407, 437, 286, 478, 884, 510, 307, 321, 362, 512, 4974, 4846, 510, 1783, 300, 307, 10117, 490, 257, 39148, 13, 50664, 50664, 400, 456, 311, 472, 4714, 5110, 300, 366, 2064, 18795, 13, 50814, 50814, 400, 550, 321, 362, 257, 3364, 4583, 510, 300, 307, 611, 5883, 1602, 1228, 39148, 11, 445, 411, 321, 630, 510, 13, 51114, 51114, 400, 321, 613, 22027, 294, 264, 7633, 4583, 574, 412, 2064, 15743, 293, 456, 366, 2331, 22027, 294, 341, 4583, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.13530475392061123, "compression_ratio": 1.6714285714285715, "no_speech_prob": 2.546482028265018e-05}, {"id": 350, "seek": 169900, "start": 1708.0, "end": 1714.0, "text": " And then we have a weight layer here that is also initialized using Gaussian, just like we did here.", "tokens": [50364, 407, 437, 286, 478, 884, 510, 307, 321, 362, 512, 4974, 4846, 510, 1783, 300, 307, 10117, 490, 257, 39148, 13, 50664, 50664, 400, 456, 311, 472, 4714, 5110, 300, 366, 2064, 18795, 13, 50814, 50814, 400, 550, 321, 362, 257, 3364, 4583, 510, 300, 307, 611, 5883, 1602, 1228, 39148, 11, 445, 411, 321, 630, 510, 13, 51114, 51114, 400, 321, 613, 22027, 294, 264, 7633, 4583, 574, 412, 2064, 15743, 293, 456, 366, 2331, 22027, 294, 341, 4583, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.13530475392061123, "compression_ratio": 1.6714285714285715, "no_speech_prob": 2.546482028265018e-05}, {"id": 351, "seek": 169900, "start": 1714.0, "end": 1721.0, "text": " And we these neurons in the hidden layer look at ten inputs and there are 200 neurons in this layer.", "tokens": [50364, 407, 437, 286, 478, 884, 510, 307, 321, 362, 512, 4974, 4846, 510, 1783, 300, 307, 10117, 490, 257, 39148, 13, 50664, 50664, 400, 456, 311, 472, 4714, 5110, 300, 366, 2064, 18795, 13, 50814, 50814, 400, 550, 321, 362, 257, 3364, 4583, 510, 300, 307, 611, 5883, 1602, 1228, 39148, 11, 445, 411, 321, 630, 510, 13, 51114, 51114, 400, 321, 613, 22027, 294, 264, 7633, 4583, 574, 412, 2064, 15743, 293, 456, 366, 2331, 22027, 294, 341, 4583, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.13530475392061123, "compression_ratio": 1.6714285714285715, "no_speech_prob": 2.546482028265018e-05}, {"id": 352, "seek": 172100, "start": 1721.0, "end": 1730.0, "text": " And then we have here, just like here in this case, the multiplication X multiplied by W to get the pre activations of these neurons.", "tokens": [50364, 400, 550, 321, 362, 510, 11, 445, 411, 510, 294, 341, 1389, 11, 264, 27290, 1783, 17207, 538, 343, 281, 483, 264, 659, 2430, 763, 295, 613, 22027, 13, 50814, 50814, 400, 1936, 264, 5215, 510, 1542, 412, 2264, 11, 7297, 613, 366, 9452, 39148, 293, 613, 17443, 366, 9452, 39148, 13, 51164, 51164, 759, 286, 360, 1783, 1413, 343, 293, 321, 2870, 337, 586, 264, 12577, 293, 264, 2107, 1889, 17409, 11, 550, 437, 307, 264, 914, 293, 264, 3832, 25163, 295, 613, 5959, 2023, 2567, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10732756490292757, "compression_ratio": 1.7012987012987013, "no_speech_prob": 2.225239995823358e-06}, {"id": 353, "seek": 172100, "start": 1730.0, "end": 1737.0, "text": " And basically the analysis here looks at OK, suppose these are uniform Gaussian and these weights are uniform Gaussian.", "tokens": [50364, 400, 550, 321, 362, 510, 11, 445, 411, 510, 294, 341, 1389, 11, 264, 27290, 1783, 17207, 538, 343, 281, 483, 264, 659, 2430, 763, 295, 613, 22027, 13, 50814, 50814, 400, 1936, 264, 5215, 510, 1542, 412, 2264, 11, 7297, 613, 366, 9452, 39148, 293, 613, 17443, 366, 9452, 39148, 13, 51164, 51164, 759, 286, 360, 1783, 1413, 343, 293, 321, 2870, 337, 586, 264, 12577, 293, 264, 2107, 1889, 17409, 11, 550, 437, 307, 264, 914, 293, 264, 3832, 25163, 295, 613, 5959, 2023, 2567, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10732756490292757, "compression_ratio": 1.7012987012987013, "no_speech_prob": 2.225239995823358e-06}, {"id": 354, "seek": 172100, "start": 1737.0, "end": 1747.0, "text": " If I do X times W and we forget for now the bias and the nonlinearity, then what is the mean and the standard deviation of these gaussians?", "tokens": [50364, 400, 550, 321, 362, 510, 11, 445, 411, 510, 294, 341, 1389, 11, 264, 27290, 1783, 17207, 538, 343, 281, 483, 264, 659, 2430, 763, 295, 613, 22027, 13, 50814, 50814, 400, 1936, 264, 5215, 510, 1542, 412, 2264, 11, 7297, 613, 366, 9452, 39148, 293, 613, 17443, 366, 9452, 39148, 13, 51164, 51164, 759, 286, 360, 1783, 1413, 343, 293, 321, 2870, 337, 586, 264, 12577, 293, 264, 2107, 1889, 17409, 11, 550, 437, 307, 264, 914, 293, 264, 3832, 25163, 295, 613, 5959, 2023, 2567, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10732756490292757, "compression_ratio": 1.7012987012987013, "no_speech_prob": 2.225239995823358e-06}, {"id": 355, "seek": 174700, "start": 1747.0, "end": 1753.0, "text": " So in the beginning here, the input is just a normal Gaussian distribution mean zero and the standard deviation is one.", "tokens": [50364, 407, 294, 264, 2863, 510, 11, 264, 4846, 307, 445, 257, 2710, 39148, 7316, 914, 4018, 293, 264, 3832, 25163, 307, 472, 13, 50664, 50664, 400, 264, 3832, 25163, 797, 307, 445, 257, 3481, 295, 257, 3974, 295, 264, 39148, 13, 50914, 50914, 583, 550, 1564, 321, 12972, 510, 293, 321, 574, 412, 264, 49816, 295, 398, 11, 321, 536, 300, 264, 914, 11, 295, 1164, 11, 10834, 264, 912, 13, 51264, 51264, 467, 311, 466, 4018, 570, 341, 307, 257, 32330, 6916, 13, 51414, 51414, 583, 321, 536, 510, 300, 264, 3832, 25163, 575, 14342, 281, 1045, 13, 51614, 51614, 407, 264, 4846, 3832, 25163, 390, 472, 13, 51714, 51714, 583, 586, 321, 600, 7709, 281, 1045, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09456620565274866, "compression_ratio": 1.8847583643122676, "no_speech_prob": 5.507427431439282e-06}, {"id": 356, "seek": 174700, "start": 1753.0, "end": 1758.0, "text": " And the standard deviation again is just a measure of a spread of the Gaussian.", "tokens": [50364, 407, 294, 264, 2863, 510, 11, 264, 4846, 307, 445, 257, 2710, 39148, 7316, 914, 4018, 293, 264, 3832, 25163, 307, 472, 13, 50664, 50664, 400, 264, 3832, 25163, 797, 307, 445, 257, 3481, 295, 257, 3974, 295, 264, 39148, 13, 50914, 50914, 583, 550, 1564, 321, 12972, 510, 293, 321, 574, 412, 264, 49816, 295, 398, 11, 321, 536, 300, 264, 914, 11, 295, 1164, 11, 10834, 264, 912, 13, 51264, 51264, 467, 311, 466, 4018, 570, 341, 307, 257, 32330, 6916, 13, 51414, 51414, 583, 321, 536, 510, 300, 264, 3832, 25163, 575, 14342, 281, 1045, 13, 51614, 51614, 407, 264, 4846, 3832, 25163, 390, 472, 13, 51714, 51714, 583, 586, 321, 600, 7709, 281, 1045, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09456620565274866, "compression_ratio": 1.8847583643122676, "no_speech_prob": 5.507427431439282e-06}, {"id": 357, "seek": 174700, "start": 1758.0, "end": 1765.0, "text": " But then once we multiply here and we look at the histogram of Y, we see that the mean, of course, stays the same.", "tokens": [50364, 407, 294, 264, 2863, 510, 11, 264, 4846, 307, 445, 257, 2710, 39148, 7316, 914, 4018, 293, 264, 3832, 25163, 307, 472, 13, 50664, 50664, 400, 264, 3832, 25163, 797, 307, 445, 257, 3481, 295, 257, 3974, 295, 264, 39148, 13, 50914, 50914, 583, 550, 1564, 321, 12972, 510, 293, 321, 574, 412, 264, 49816, 295, 398, 11, 321, 536, 300, 264, 914, 11, 295, 1164, 11, 10834, 264, 912, 13, 51264, 51264, 467, 311, 466, 4018, 570, 341, 307, 257, 32330, 6916, 13, 51414, 51414, 583, 321, 536, 510, 300, 264, 3832, 25163, 575, 14342, 281, 1045, 13, 51614, 51614, 407, 264, 4846, 3832, 25163, 390, 472, 13, 51714, 51714, 583, 586, 321, 600, 7709, 281, 1045, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09456620565274866, "compression_ratio": 1.8847583643122676, "no_speech_prob": 5.507427431439282e-06}, {"id": 358, "seek": 174700, "start": 1765.0, "end": 1768.0, "text": " It's about zero because this is a symmetric operation.", "tokens": [50364, 407, 294, 264, 2863, 510, 11, 264, 4846, 307, 445, 257, 2710, 39148, 7316, 914, 4018, 293, 264, 3832, 25163, 307, 472, 13, 50664, 50664, 400, 264, 3832, 25163, 797, 307, 445, 257, 3481, 295, 257, 3974, 295, 264, 39148, 13, 50914, 50914, 583, 550, 1564, 321, 12972, 510, 293, 321, 574, 412, 264, 49816, 295, 398, 11, 321, 536, 300, 264, 914, 11, 295, 1164, 11, 10834, 264, 912, 13, 51264, 51264, 467, 311, 466, 4018, 570, 341, 307, 257, 32330, 6916, 13, 51414, 51414, 583, 321, 536, 510, 300, 264, 3832, 25163, 575, 14342, 281, 1045, 13, 51614, 51614, 407, 264, 4846, 3832, 25163, 390, 472, 13, 51714, 51714, 583, 586, 321, 600, 7709, 281, 1045, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09456620565274866, "compression_ratio": 1.8847583643122676, "no_speech_prob": 5.507427431439282e-06}, {"id": 359, "seek": 174700, "start": 1768.0, "end": 1772.0, "text": " But we see here that the standard deviation has expanded to three.", "tokens": [50364, 407, 294, 264, 2863, 510, 11, 264, 4846, 307, 445, 257, 2710, 39148, 7316, 914, 4018, 293, 264, 3832, 25163, 307, 472, 13, 50664, 50664, 400, 264, 3832, 25163, 797, 307, 445, 257, 3481, 295, 257, 3974, 295, 264, 39148, 13, 50914, 50914, 583, 550, 1564, 321, 12972, 510, 293, 321, 574, 412, 264, 49816, 295, 398, 11, 321, 536, 300, 264, 914, 11, 295, 1164, 11, 10834, 264, 912, 13, 51264, 51264, 467, 311, 466, 4018, 570, 341, 307, 257, 32330, 6916, 13, 51414, 51414, 583, 321, 536, 510, 300, 264, 3832, 25163, 575, 14342, 281, 1045, 13, 51614, 51614, 407, 264, 4846, 3832, 25163, 390, 472, 13, 51714, 51714, 583, 586, 321, 600, 7709, 281, 1045, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09456620565274866, "compression_ratio": 1.8847583643122676, "no_speech_prob": 5.507427431439282e-06}, {"id": 360, "seek": 174700, "start": 1772.0, "end": 1774.0, "text": " So the input standard deviation was one.", "tokens": [50364, 407, 294, 264, 2863, 510, 11, 264, 4846, 307, 445, 257, 2710, 39148, 7316, 914, 4018, 293, 264, 3832, 25163, 307, 472, 13, 50664, 50664, 400, 264, 3832, 25163, 797, 307, 445, 257, 3481, 295, 257, 3974, 295, 264, 39148, 13, 50914, 50914, 583, 550, 1564, 321, 12972, 510, 293, 321, 574, 412, 264, 49816, 295, 398, 11, 321, 536, 300, 264, 914, 11, 295, 1164, 11, 10834, 264, 912, 13, 51264, 51264, 467, 311, 466, 4018, 570, 341, 307, 257, 32330, 6916, 13, 51414, 51414, 583, 321, 536, 510, 300, 264, 3832, 25163, 575, 14342, 281, 1045, 13, 51614, 51614, 407, 264, 4846, 3832, 25163, 390, 472, 13, 51714, 51714, 583, 586, 321, 600, 7709, 281, 1045, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09456620565274866, "compression_ratio": 1.8847583643122676, "no_speech_prob": 5.507427431439282e-06}, {"id": 361, "seek": 174700, "start": 1774.0, "end": 1776.0, "text": " But now we've grown to three.", "tokens": [50364, 407, 294, 264, 2863, 510, 11, 264, 4846, 307, 445, 257, 2710, 39148, 7316, 914, 4018, 293, 264, 3832, 25163, 307, 472, 13, 50664, 50664, 400, 264, 3832, 25163, 797, 307, 445, 257, 3481, 295, 257, 3974, 295, 264, 39148, 13, 50914, 50914, 583, 550, 1564, 321, 12972, 510, 293, 321, 574, 412, 264, 49816, 295, 398, 11, 321, 536, 300, 264, 914, 11, 295, 1164, 11, 10834, 264, 912, 13, 51264, 51264, 467, 311, 466, 4018, 570, 341, 307, 257, 32330, 6916, 13, 51414, 51414, 583, 321, 536, 510, 300, 264, 3832, 25163, 575, 14342, 281, 1045, 13, 51614, 51614, 407, 264, 4846, 3832, 25163, 390, 472, 13, 51714, 51714, 583, 586, 321, 600, 7709, 281, 1045, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09456620565274866, "compression_ratio": 1.8847583643122676, "no_speech_prob": 5.507427431439282e-06}, {"id": 362, "seek": 177600, "start": 1776.0, "end": 1781.0, "text": " And so what you're seeing in the histogram is that this Gaussian is expanding.", "tokens": [50364, 400, 370, 437, 291, 434, 2577, 294, 264, 49816, 307, 300, 341, 39148, 307, 14702, 13, 50614, 50614, 400, 370, 321, 434, 14702, 341, 39148, 490, 264, 4846, 13, 50814, 50814, 400, 321, 500, 380, 528, 300, 13, 492, 528, 881, 295, 264, 18161, 36170, 281, 362, 7226, 2531, 2430, 763, 13, 51064, 51064, 407, 4985, 39148, 9810, 3710, 264, 18161, 2533, 13, 51264, 51264, 400, 370, 264, 1168, 307, 11, 577, 360, 321, 4373, 613, 343, 311, 281, 15665, 264, 281, 15665, 341, 7316, 281, 6222, 257, 39148, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09338311946138422, "compression_ratio": 1.7802690582959642, "no_speech_prob": 8.800738214631565e-06}, {"id": 363, "seek": 177600, "start": 1781.0, "end": 1785.0, "text": " And so we're expanding this Gaussian from the input.", "tokens": [50364, 400, 370, 437, 291, 434, 2577, 294, 264, 49816, 307, 300, 341, 39148, 307, 14702, 13, 50614, 50614, 400, 370, 321, 434, 14702, 341, 39148, 490, 264, 4846, 13, 50814, 50814, 400, 321, 500, 380, 528, 300, 13, 492, 528, 881, 295, 264, 18161, 36170, 281, 362, 7226, 2531, 2430, 763, 13, 51064, 51064, 407, 4985, 39148, 9810, 3710, 264, 18161, 2533, 13, 51264, 51264, 400, 370, 264, 1168, 307, 11, 577, 360, 321, 4373, 613, 343, 311, 281, 15665, 264, 281, 15665, 341, 7316, 281, 6222, 257, 39148, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09338311946138422, "compression_ratio": 1.7802690582959642, "no_speech_prob": 8.800738214631565e-06}, {"id": 364, "seek": 177600, "start": 1785.0, "end": 1790.0, "text": " And we don't want that. We want most of the neural nets to have relatively similar activations.", "tokens": [50364, 400, 370, 437, 291, 434, 2577, 294, 264, 49816, 307, 300, 341, 39148, 307, 14702, 13, 50614, 50614, 400, 370, 321, 434, 14702, 341, 39148, 490, 264, 4846, 13, 50814, 50814, 400, 321, 500, 380, 528, 300, 13, 492, 528, 881, 295, 264, 18161, 36170, 281, 362, 7226, 2531, 2430, 763, 13, 51064, 51064, 407, 4985, 39148, 9810, 3710, 264, 18161, 2533, 13, 51264, 51264, 400, 370, 264, 1168, 307, 11, 577, 360, 321, 4373, 613, 343, 311, 281, 15665, 264, 281, 15665, 341, 7316, 281, 6222, 257, 39148, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09338311946138422, "compression_ratio": 1.7802690582959642, "no_speech_prob": 8.800738214631565e-06}, {"id": 365, "seek": 177600, "start": 1790.0, "end": 1794.0, "text": " So unit Gaussian roughly throughout the neural net.", "tokens": [50364, 400, 370, 437, 291, 434, 2577, 294, 264, 49816, 307, 300, 341, 39148, 307, 14702, 13, 50614, 50614, 400, 370, 321, 434, 14702, 341, 39148, 490, 264, 4846, 13, 50814, 50814, 400, 321, 500, 380, 528, 300, 13, 492, 528, 881, 295, 264, 18161, 36170, 281, 362, 7226, 2531, 2430, 763, 13, 51064, 51064, 407, 4985, 39148, 9810, 3710, 264, 18161, 2533, 13, 51264, 51264, 400, 370, 264, 1168, 307, 11, 577, 360, 321, 4373, 613, 343, 311, 281, 15665, 264, 281, 15665, 341, 7316, 281, 6222, 257, 39148, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09338311946138422, "compression_ratio": 1.7802690582959642, "no_speech_prob": 8.800738214631565e-06}, {"id": 366, "seek": 177600, "start": 1794.0, "end": 1803.0, "text": " And so the question is, how do we scale these W's to preserve the to preserve this distribution to remain a Gaussian?", "tokens": [50364, 400, 370, 437, 291, 434, 2577, 294, 264, 49816, 307, 300, 341, 39148, 307, 14702, 13, 50614, 50614, 400, 370, 321, 434, 14702, 341, 39148, 490, 264, 4846, 13, 50814, 50814, 400, 321, 500, 380, 528, 300, 13, 492, 528, 881, 295, 264, 18161, 36170, 281, 362, 7226, 2531, 2430, 763, 13, 51064, 51064, 407, 4985, 39148, 9810, 3710, 264, 18161, 2533, 13, 51264, 51264, 400, 370, 264, 1168, 307, 11, 577, 360, 321, 4373, 613, 343, 311, 281, 15665, 264, 281, 15665, 341, 7316, 281, 6222, 257, 39148, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09338311946138422, "compression_ratio": 1.7802690582959642, "no_speech_prob": 8.800738214631565e-06}, {"id": 367, "seek": 180300, "start": 1803.0, "end": 1816.0, "text": " And so intuitively, if I multiply here, these elements of W by a larger number, let's say by five, then this Gaussian grows and grows in standard deviation.", "tokens": [50364, 400, 370, 46506, 11, 498, 286, 12972, 510, 11, 613, 4959, 295, 343, 538, 257, 4833, 1230, 11, 718, 311, 584, 538, 1732, 11, 550, 341, 39148, 13156, 293, 13156, 294, 3832, 25163, 13, 51014, 51014, 407, 586, 321, 434, 412, 2119, 13, 407, 1936, 11, 613, 3547, 510, 294, 264, 5598, 398, 747, 322, 544, 293, 544, 8084, 4190, 13, 51314, 51314, 583, 498, 321, 4373, 309, 760, 11, 718, 311, 584, 935, 732, 11, 550, 2615, 736, 11, 341, 39148, 307, 1242, 4356, 293, 4356, 293, 309, 311, 41684, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0962824821472168, "compression_ratio": 1.6359832635983265, "no_speech_prob": 3.966916210629279e-06}, {"id": 368, "seek": 180300, "start": 1816.0, "end": 1822.0, "text": " So now we're at 15. So basically, these numbers here in the output Y take on more and more extreme values.", "tokens": [50364, 400, 370, 46506, 11, 498, 286, 12972, 510, 11, 613, 4959, 295, 343, 538, 257, 4833, 1230, 11, 718, 311, 584, 538, 1732, 11, 550, 341, 39148, 13156, 293, 13156, 294, 3832, 25163, 13, 51014, 51014, 407, 586, 321, 434, 412, 2119, 13, 407, 1936, 11, 613, 3547, 510, 294, 264, 5598, 398, 747, 322, 544, 293, 544, 8084, 4190, 13, 51314, 51314, 583, 498, 321, 4373, 309, 760, 11, 718, 311, 584, 935, 732, 11, 550, 2615, 736, 11, 341, 39148, 307, 1242, 4356, 293, 4356, 293, 309, 311, 41684, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0962824821472168, "compression_ratio": 1.6359832635983265, "no_speech_prob": 3.966916210629279e-06}, {"id": 369, "seek": 180300, "start": 1822.0, "end": 1831.0, "text": " But if we scale it down, let's say point two, then conversely, this Gaussian is getting smaller and smaller and it's shrinking.", "tokens": [50364, 400, 370, 46506, 11, 498, 286, 12972, 510, 11, 613, 4959, 295, 343, 538, 257, 4833, 1230, 11, 718, 311, 584, 538, 1732, 11, 550, 341, 39148, 13156, 293, 13156, 294, 3832, 25163, 13, 51014, 51014, 407, 586, 321, 434, 412, 2119, 13, 407, 1936, 11, 613, 3547, 510, 294, 264, 5598, 398, 747, 322, 544, 293, 544, 8084, 4190, 13, 51314, 51314, 583, 498, 321, 4373, 309, 760, 11, 718, 311, 584, 935, 732, 11, 550, 2615, 736, 11, 341, 39148, 307, 1242, 4356, 293, 4356, 293, 309, 311, 41684, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0962824821472168, "compression_ratio": 1.6359832635983265, "no_speech_prob": 3.966916210629279e-06}, {"id": 370, "seek": 183100, "start": 1831.0, "end": 1834.0, "text": " And you can see that the standard deviation is point six.", "tokens": [50364, 400, 291, 393, 536, 300, 264, 3832, 25163, 307, 935, 2309, 13, 50514, 50514, 400, 370, 264, 1168, 307, 11, 437, 360, 286, 12972, 538, 510, 281, 2293, 15665, 264, 3832, 25163, 281, 312, 472, 30, 50864, 50864, 400, 309, 4523, 484, 300, 264, 3006, 1867, 44003, 562, 291, 589, 484, 807, 264, 21977, 295, 341, 27290, 510, 307, 300, 291, 366, 3442, 281, 9845, 538, 264, 3732, 5593, 295, 264, 3429, 294, 264, 3429, 294, 307, 264, 1936, 264, 1230, 295, 4846, 4959, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08643987973531088, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.2958906811254565e-06}, {"id": 371, "seek": 183100, "start": 1834.0, "end": 1841.0, "text": " And so the question is, what do I multiply by here to exactly preserve the standard deviation to be one?", "tokens": [50364, 400, 291, 393, 536, 300, 264, 3832, 25163, 307, 935, 2309, 13, 50514, 50514, 400, 370, 264, 1168, 307, 11, 437, 360, 286, 12972, 538, 510, 281, 2293, 15665, 264, 3832, 25163, 281, 312, 472, 30, 50864, 50864, 400, 309, 4523, 484, 300, 264, 3006, 1867, 44003, 562, 291, 589, 484, 807, 264, 21977, 295, 341, 27290, 510, 307, 300, 291, 366, 3442, 281, 9845, 538, 264, 3732, 5593, 295, 264, 3429, 294, 264, 3429, 294, 307, 264, 1936, 264, 1230, 295, 4846, 4959, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08643987973531088, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.2958906811254565e-06}, {"id": 372, "seek": 183100, "start": 1841.0, "end": 1857.0, "text": " And it turns out that the correct answer mathematically when you work out through the variance of this multiplication here is that you are supposed to divide by the square root of the fan in the fan in is the basically the number of input elements here.", "tokens": [50364, 400, 291, 393, 536, 300, 264, 3832, 25163, 307, 935, 2309, 13, 50514, 50514, 400, 370, 264, 1168, 307, 11, 437, 360, 286, 12972, 538, 510, 281, 2293, 15665, 264, 3832, 25163, 281, 312, 472, 30, 50864, 50864, 400, 309, 4523, 484, 300, 264, 3006, 1867, 44003, 562, 291, 589, 484, 807, 264, 21977, 295, 341, 27290, 510, 307, 300, 291, 366, 3442, 281, 9845, 538, 264, 3732, 5593, 295, 264, 3429, 294, 264, 3429, 294, 307, 264, 1936, 264, 1230, 295, 4846, 4959, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08643987973531088, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.2958906811254565e-06}, {"id": 373, "seek": 185700, "start": 1857.0, "end": 1862.0, "text": " Ten. So we are supposed to divide by ten square root. And this is one way to do the square root.", "tokens": [50364, 9380, 13, 407, 321, 366, 3442, 281, 9845, 538, 2064, 3732, 5593, 13, 400, 341, 307, 472, 636, 281, 360, 264, 3732, 5593, 13, 50614, 50614, 509, 5300, 309, 281, 257, 1347, 295, 935, 1732, 13, 400, 300, 311, 264, 912, 382, 884, 257, 3732, 5593, 13, 50864, 50864, 407, 562, 291, 9845, 538, 264, 3732, 5593, 295, 2064, 11, 550, 321, 536, 300, 264, 5598, 39148, 11, 309, 575, 2293, 3832, 25163, 295, 472, 13, 51364, 51364, 823, 11, 2693, 374, 34408, 11, 257, 1230, 295, 10577, 362, 2956, 666, 577, 281, 1151, 5883, 1125, 18161, 9590, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07566981639676881, "compression_ratio": 1.6611570247933884, "no_speech_prob": 8.579103223382845e-07}, {"id": 374, "seek": 185700, "start": 1862.0, "end": 1867.0, "text": " You raise it to a power of point five. And that's the same as doing a square root.", "tokens": [50364, 9380, 13, 407, 321, 366, 3442, 281, 9845, 538, 2064, 3732, 5593, 13, 400, 341, 307, 472, 636, 281, 360, 264, 3732, 5593, 13, 50614, 50614, 509, 5300, 309, 281, 257, 1347, 295, 935, 1732, 13, 400, 300, 311, 264, 912, 382, 884, 257, 3732, 5593, 13, 50864, 50864, 407, 562, 291, 9845, 538, 264, 3732, 5593, 295, 2064, 11, 550, 321, 536, 300, 264, 5598, 39148, 11, 309, 575, 2293, 3832, 25163, 295, 472, 13, 51364, 51364, 823, 11, 2693, 374, 34408, 11, 257, 1230, 295, 10577, 362, 2956, 666, 577, 281, 1151, 5883, 1125, 18161, 9590, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07566981639676881, "compression_ratio": 1.6611570247933884, "no_speech_prob": 8.579103223382845e-07}, {"id": 375, "seek": 185700, "start": 1867.0, "end": 1877.0, "text": " So when you divide by the square root of ten, then we see that the output Gaussian, it has exactly standard deviation of one.", "tokens": [50364, 9380, 13, 407, 321, 366, 3442, 281, 9845, 538, 2064, 3732, 5593, 13, 400, 341, 307, 472, 636, 281, 360, 264, 3732, 5593, 13, 50614, 50614, 509, 5300, 309, 281, 257, 1347, 295, 935, 1732, 13, 400, 300, 311, 264, 912, 382, 884, 257, 3732, 5593, 13, 50864, 50864, 407, 562, 291, 9845, 538, 264, 3732, 5593, 295, 2064, 11, 550, 321, 536, 300, 264, 5598, 39148, 11, 309, 575, 2293, 3832, 25163, 295, 472, 13, 51364, 51364, 823, 11, 2693, 374, 34408, 11, 257, 1230, 295, 10577, 362, 2956, 666, 577, 281, 1151, 5883, 1125, 18161, 9590, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07566981639676881, "compression_ratio": 1.6611570247933884, "no_speech_prob": 8.579103223382845e-07}, {"id": 376, "seek": 185700, "start": 1877.0, "end": 1883.0, "text": " Now, unsurprisingly, a number of papers have looked into how to best initialize neural networks.", "tokens": [50364, 9380, 13, 407, 321, 366, 3442, 281, 9845, 538, 2064, 3732, 5593, 13, 400, 341, 307, 472, 636, 281, 360, 264, 3732, 5593, 13, 50614, 50614, 509, 5300, 309, 281, 257, 1347, 295, 935, 1732, 13, 400, 300, 311, 264, 912, 382, 884, 257, 3732, 5593, 13, 50864, 50864, 407, 562, 291, 9845, 538, 264, 3732, 5593, 295, 2064, 11, 550, 321, 536, 300, 264, 5598, 39148, 11, 309, 575, 2293, 3832, 25163, 295, 472, 13, 51364, 51364, 823, 11, 2693, 374, 34408, 11, 257, 1230, 295, 10577, 362, 2956, 666, 577, 281, 1151, 5883, 1125, 18161, 9590, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07566981639676881, "compression_ratio": 1.6611570247933884, "no_speech_prob": 8.579103223382845e-07}, {"id": 377, "seek": 188300, "start": 1883.0, "end": 1889.0, "text": " And in the case of multi-layer perceptrons, we can have fairly deep networks that have these nonlinearities in between.", "tokens": [50364, 400, 294, 264, 1389, 295, 4825, 12, 8376, 260, 43276, 13270, 11, 321, 393, 362, 6457, 2452, 9590, 300, 362, 613, 2107, 28263, 1088, 294, 1296, 13, 50664, 50664, 400, 321, 528, 281, 652, 988, 300, 264, 2430, 763, 366, 731, 48249, 293, 436, 500, 380, 5268, 281, 13202, 420, 23060, 439, 264, 636, 281, 4018, 13, 50964, 50964, 400, 264, 1168, 307, 11, 577, 360, 321, 5883, 1125, 264, 17443, 370, 300, 613, 2430, 763, 747, 322, 10585, 4190, 3710, 264, 3209, 30, 51264, 51264, 823, 11, 472, 3035, 300, 575, 9454, 341, 294, 1596, 257, 857, 2607, 300, 307, 2049, 32734, 307, 341, 3035, 538, 591, 5184, 389, 3093, 338, 1219, 5831, 798, 14895, 5751, 578, 23463, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12803858326327416, "compression_ratio": 1.6923076923076923, "no_speech_prob": 1.9220020476495847e-05}, {"id": 378, "seek": 188300, "start": 1889.0, "end": 1895.0, "text": " And we want to make sure that the activations are well behaved and they don't expand to infinity or shrink all the way to zero.", "tokens": [50364, 400, 294, 264, 1389, 295, 4825, 12, 8376, 260, 43276, 13270, 11, 321, 393, 362, 6457, 2452, 9590, 300, 362, 613, 2107, 28263, 1088, 294, 1296, 13, 50664, 50664, 400, 321, 528, 281, 652, 988, 300, 264, 2430, 763, 366, 731, 48249, 293, 436, 500, 380, 5268, 281, 13202, 420, 23060, 439, 264, 636, 281, 4018, 13, 50964, 50964, 400, 264, 1168, 307, 11, 577, 360, 321, 5883, 1125, 264, 17443, 370, 300, 613, 2430, 763, 747, 322, 10585, 4190, 3710, 264, 3209, 30, 51264, 51264, 823, 11, 472, 3035, 300, 575, 9454, 341, 294, 1596, 257, 857, 2607, 300, 307, 2049, 32734, 307, 341, 3035, 538, 591, 5184, 389, 3093, 338, 1219, 5831, 798, 14895, 5751, 578, 23463, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12803858326327416, "compression_ratio": 1.6923076923076923, "no_speech_prob": 1.9220020476495847e-05}, {"id": 379, "seek": 188300, "start": 1895.0, "end": 1901.0, "text": " And the question is, how do we initialize the weights so that these activations take on reasonable values throughout the network?", "tokens": [50364, 400, 294, 264, 1389, 295, 4825, 12, 8376, 260, 43276, 13270, 11, 321, 393, 362, 6457, 2452, 9590, 300, 362, 613, 2107, 28263, 1088, 294, 1296, 13, 50664, 50664, 400, 321, 528, 281, 652, 988, 300, 264, 2430, 763, 366, 731, 48249, 293, 436, 500, 380, 5268, 281, 13202, 420, 23060, 439, 264, 636, 281, 4018, 13, 50964, 50964, 400, 264, 1168, 307, 11, 577, 360, 321, 5883, 1125, 264, 17443, 370, 300, 613, 2430, 763, 747, 322, 10585, 4190, 3710, 264, 3209, 30, 51264, 51264, 823, 11, 472, 3035, 300, 575, 9454, 341, 294, 1596, 257, 857, 2607, 300, 307, 2049, 32734, 307, 341, 3035, 538, 591, 5184, 389, 3093, 338, 1219, 5831, 798, 14895, 5751, 578, 23463, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12803858326327416, "compression_ratio": 1.6923076923076923, "no_speech_prob": 1.9220020476495847e-05}, {"id": 380, "seek": 188300, "start": 1901.0, "end": 1909.0, "text": " Now, one paper that has studied this in quite a bit detail that is often referenced is this paper by Kaming Hettel called Delving Deep Interactifiers.", "tokens": [50364, 400, 294, 264, 1389, 295, 4825, 12, 8376, 260, 43276, 13270, 11, 321, 393, 362, 6457, 2452, 9590, 300, 362, 613, 2107, 28263, 1088, 294, 1296, 13, 50664, 50664, 400, 321, 528, 281, 652, 988, 300, 264, 2430, 763, 366, 731, 48249, 293, 436, 500, 380, 5268, 281, 13202, 420, 23060, 439, 264, 636, 281, 4018, 13, 50964, 50964, 400, 264, 1168, 307, 11, 577, 360, 321, 5883, 1125, 264, 17443, 370, 300, 613, 2430, 763, 747, 322, 10585, 4190, 3710, 264, 3209, 30, 51264, 51264, 823, 11, 472, 3035, 300, 575, 9454, 341, 294, 1596, 257, 857, 2607, 300, 307, 2049, 32734, 307, 341, 3035, 538, 591, 5184, 389, 3093, 338, 1219, 5831, 798, 14895, 5751, 578, 23463, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12803858326327416, "compression_ratio": 1.6923076923076923, "no_speech_prob": 1.9220020476495847e-05}, {"id": 381, "seek": 190900, "start": 1909.0, "end": 1920.0, "text": " Now, in this case, they actually study convolutional neural networks and they study especially the ReLU nonlinearity and the P-ReLU nonlinearity instead of a 10H nonlinearity.", "tokens": [50364, 823, 11, 294, 341, 1389, 11, 436, 767, 2979, 45216, 304, 18161, 9590, 293, 436, 2979, 2318, 264, 1300, 43, 52, 2107, 1889, 17409, 293, 264, 430, 12, 8524, 43, 52, 2107, 1889, 17409, 2602, 295, 257, 1266, 39, 2107, 1889, 17409, 13, 50914, 50914, 583, 264, 5215, 307, 588, 2531, 13, 400, 1936, 437, 2314, 510, 307, 337, 552, 11, 264, 1300, 43, 52, 2107, 1889, 17409, 300, 436, 1127, 466, 1596, 257, 857, 510, 307, 257, 2339, 11077, 2445, 689, 439, 264, 3671, 3547, 366, 2935, 17690, 292, 281, 4018, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09544935914659008, "compression_ratio": 1.6751054852320675, "no_speech_prob": 6.747796760464553e-06}, {"id": 382, "seek": 190900, "start": 1920.0, "end": 1936.0, "text": " But the analysis is very similar. And basically what happens here is for them, the ReLU nonlinearity that they care about quite a bit here is a squashing function where all the negative numbers are simply clamped to zero.", "tokens": [50364, 823, 11, 294, 341, 1389, 11, 436, 767, 2979, 45216, 304, 18161, 9590, 293, 436, 2979, 2318, 264, 1300, 43, 52, 2107, 1889, 17409, 293, 264, 430, 12, 8524, 43, 52, 2107, 1889, 17409, 2602, 295, 257, 1266, 39, 2107, 1889, 17409, 13, 50914, 50914, 583, 264, 5215, 307, 588, 2531, 13, 400, 1936, 437, 2314, 510, 307, 337, 552, 11, 264, 1300, 43, 52, 2107, 1889, 17409, 300, 436, 1127, 466, 1596, 257, 857, 510, 307, 257, 2339, 11077, 2445, 689, 439, 264, 3671, 3547, 366, 2935, 17690, 292, 281, 4018, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09544935914659008, "compression_ratio": 1.6751054852320675, "no_speech_prob": 6.747796760464553e-06}, {"id": 383, "seek": 193600, "start": 1936.0, "end": 1940.0, "text": " So the positive numbers are a pass-through, but everything negative is just set to zero.", "tokens": [50364, 407, 264, 3353, 3547, 366, 257, 1320, 12, 11529, 11, 457, 1203, 3671, 307, 445, 992, 281, 4018, 13, 50564, 50564, 400, 570, 291, 366, 1936, 10238, 1314, 1922, 295, 264, 7316, 11, 436, 915, 294, 641, 5215, 295, 264, 2128, 2430, 763, 294, 264, 18161, 2533, 300, 291, 362, 281, 29458, 337, 300, 365, 257, 6052, 13, 51164, 51164, 400, 370, 510, 436, 915, 300, 1936, 562, 436, 5883, 1125, 641, 17443, 11, 436, 362, 281, 360, 309, 365, 257, 4018, 12, 1398, 282, 39148, 6104, 3832, 25163, 307, 3732, 5593, 295, 568, 670, 264, 3429, 12, 259, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07338402362970206, "compression_ratio": 1.7074074074074075, "no_speech_prob": 4.222512416163227e-06}, {"id": 384, "seek": 193600, "start": 1940.0, "end": 1952.0, "text": " And because you are basically throwing away half of the distribution, they find in their analysis of the forward activations in the neural net that you have to compensate for that with a gain.", "tokens": [50364, 407, 264, 3353, 3547, 366, 257, 1320, 12, 11529, 11, 457, 1203, 3671, 307, 445, 992, 281, 4018, 13, 50564, 50564, 400, 570, 291, 366, 1936, 10238, 1314, 1922, 295, 264, 7316, 11, 436, 915, 294, 641, 5215, 295, 264, 2128, 2430, 763, 294, 264, 18161, 2533, 300, 291, 362, 281, 29458, 337, 300, 365, 257, 6052, 13, 51164, 51164, 400, 370, 510, 436, 915, 300, 1936, 562, 436, 5883, 1125, 641, 17443, 11, 436, 362, 281, 360, 309, 365, 257, 4018, 12, 1398, 282, 39148, 6104, 3832, 25163, 307, 3732, 5593, 295, 568, 670, 264, 3429, 12, 259, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07338402362970206, "compression_ratio": 1.7074074074074075, "no_speech_prob": 4.222512416163227e-06}, {"id": 385, "seek": 193600, "start": 1952.0, "end": 1963.0, "text": " And so here they find that basically when they initialize their weights, they have to do it with a zero-mean Gaussian whose standard deviation is square root of 2 over the fan-in.", "tokens": [50364, 407, 264, 3353, 3547, 366, 257, 1320, 12, 11529, 11, 457, 1203, 3671, 307, 445, 992, 281, 4018, 13, 50564, 50564, 400, 570, 291, 366, 1936, 10238, 1314, 1922, 295, 264, 7316, 11, 436, 915, 294, 641, 5215, 295, 264, 2128, 2430, 763, 294, 264, 18161, 2533, 300, 291, 362, 281, 29458, 337, 300, 365, 257, 6052, 13, 51164, 51164, 400, 370, 510, 436, 915, 300, 1936, 562, 436, 5883, 1125, 641, 17443, 11, 436, 362, 281, 360, 309, 365, 257, 4018, 12, 1398, 282, 39148, 6104, 3832, 25163, 307, 3732, 5593, 295, 568, 670, 264, 3429, 12, 259, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07338402362970206, "compression_ratio": 1.7074074074074075, "no_speech_prob": 4.222512416163227e-06}, {"id": 386, "seek": 196300, "start": 1963.0, "end": 1970.0, "text": " What we have here is we are initializing the Gaussian with the square root of fan-in. This nl here is the fan-in.", "tokens": [50364, 708, 321, 362, 510, 307, 321, 366, 5883, 3319, 264, 39148, 365, 264, 3732, 5593, 295, 3429, 12, 259, 13, 639, 297, 75, 510, 307, 264, 3429, 12, 259, 13, 50714, 50714, 407, 437, 321, 362, 307, 3732, 5593, 295, 502, 670, 264, 3429, 12, 259, 570, 321, 362, 264, 10044, 510, 13, 51114, 51114, 823, 11, 436, 362, 281, 909, 341, 5952, 295, 568, 570, 295, 264, 1300, 43, 52, 11, 597, 1936, 2983, 2287, 1922, 295, 264, 7316, 293, 44423, 309, 412, 4018, 13, 51464, 51464, 400, 370, 300, 311, 689, 291, 483, 364, 5883, 5952, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07136370371846319, "compression_ratio": 1.6964285714285714, "no_speech_prob": 9.36845663090935e-06}, {"id": 387, "seek": 196300, "start": 1970.0, "end": 1978.0, "text": " So what we have is square root of 1 over the fan-in because we have the division here.", "tokens": [50364, 708, 321, 362, 510, 307, 321, 366, 5883, 3319, 264, 39148, 365, 264, 3732, 5593, 295, 3429, 12, 259, 13, 639, 297, 75, 510, 307, 264, 3429, 12, 259, 13, 50714, 50714, 407, 437, 321, 362, 307, 3732, 5593, 295, 502, 670, 264, 3429, 12, 259, 570, 321, 362, 264, 10044, 510, 13, 51114, 51114, 823, 11, 436, 362, 281, 909, 341, 5952, 295, 568, 570, 295, 264, 1300, 43, 52, 11, 597, 1936, 2983, 2287, 1922, 295, 264, 7316, 293, 44423, 309, 412, 4018, 13, 51464, 51464, 400, 370, 300, 311, 689, 291, 483, 364, 5883, 5952, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07136370371846319, "compression_ratio": 1.6964285714285714, "no_speech_prob": 9.36845663090935e-06}, {"id": 388, "seek": 196300, "start": 1978.0, "end": 1985.0, "text": " Now, they have to add this factor of 2 because of the ReLU, which basically discards half of the distribution and clamps it at zero.", "tokens": [50364, 708, 321, 362, 510, 307, 321, 366, 5883, 3319, 264, 39148, 365, 264, 3732, 5593, 295, 3429, 12, 259, 13, 639, 297, 75, 510, 307, 264, 3429, 12, 259, 13, 50714, 50714, 407, 437, 321, 362, 307, 3732, 5593, 295, 502, 670, 264, 3429, 12, 259, 570, 321, 362, 264, 10044, 510, 13, 51114, 51114, 823, 11, 436, 362, 281, 909, 341, 5952, 295, 568, 570, 295, 264, 1300, 43, 52, 11, 597, 1936, 2983, 2287, 1922, 295, 264, 7316, 293, 44423, 309, 412, 4018, 13, 51464, 51464, 400, 370, 300, 311, 689, 291, 483, 364, 5883, 5952, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07136370371846319, "compression_ratio": 1.6964285714285714, "no_speech_prob": 9.36845663090935e-06}, {"id": 389, "seek": 196300, "start": 1985.0, "end": 1988.0, "text": " And so that's where you get an initial factor.", "tokens": [50364, 708, 321, 362, 510, 307, 321, 366, 5883, 3319, 264, 39148, 365, 264, 3732, 5593, 295, 3429, 12, 259, 13, 639, 297, 75, 510, 307, 264, 3429, 12, 259, 13, 50714, 50714, 407, 437, 321, 362, 307, 3732, 5593, 295, 502, 670, 264, 3429, 12, 259, 570, 321, 362, 264, 10044, 510, 13, 51114, 51114, 823, 11, 436, 362, 281, 909, 341, 5952, 295, 568, 570, 295, 264, 1300, 43, 52, 11, 597, 1936, 2983, 2287, 1922, 295, 264, 7316, 293, 44423, 309, 412, 4018, 13, 51464, 51464, 400, 370, 300, 311, 689, 291, 483, 364, 5883, 5952, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07136370371846319, "compression_ratio": 1.6964285714285714, "no_speech_prob": 9.36845663090935e-06}, {"id": 390, "seek": 198800, "start": 1988.0, "end": 1998.0, "text": " Now, in addition to that, this paper also studies not just the sort of behavior of the activations in the forward pass of the neural net, but it also studies the back propagation.", "tokens": [50364, 823, 11, 294, 4500, 281, 300, 11, 341, 3035, 611, 5313, 406, 445, 264, 1333, 295, 5223, 295, 264, 2430, 763, 294, 264, 2128, 1320, 295, 264, 18161, 2533, 11, 457, 309, 611, 5313, 264, 646, 38377, 13, 50864, 50864, 400, 321, 362, 281, 652, 988, 300, 264, 2771, 2448, 611, 366, 731, 48249, 13, 51014, 51014, 400, 370, 570, 6284, 436, 917, 493, 25113, 527, 9834, 13, 51264, 51264, 400, 437, 436, 915, 510, 807, 257, 688, 295, 264, 5215, 300, 286, 7980, 291, 281, 1401, 807, 11, 457, 309, 311, 406, 2293, 3109, 712, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08406611716393197, "compression_ratio": 1.704724409448819, "no_speech_prob": 1.147843613580335e-05}, {"id": 391, "seek": 198800, "start": 1998.0, "end": 2001.0, "text": " And we have to make sure that the gradients also are well behaved.", "tokens": [50364, 823, 11, 294, 4500, 281, 300, 11, 341, 3035, 611, 5313, 406, 445, 264, 1333, 295, 5223, 295, 264, 2430, 763, 294, 264, 2128, 1320, 295, 264, 18161, 2533, 11, 457, 309, 611, 5313, 264, 646, 38377, 13, 50864, 50864, 400, 321, 362, 281, 652, 988, 300, 264, 2771, 2448, 611, 366, 731, 48249, 13, 51014, 51014, 400, 370, 570, 6284, 436, 917, 493, 25113, 527, 9834, 13, 51264, 51264, 400, 437, 436, 915, 510, 807, 257, 688, 295, 264, 5215, 300, 286, 7980, 291, 281, 1401, 807, 11, 457, 309, 311, 406, 2293, 3109, 712, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08406611716393197, "compression_ratio": 1.704724409448819, "no_speech_prob": 1.147843613580335e-05}, {"id": 392, "seek": 198800, "start": 2001.0, "end": 2006.0, "text": " And so because ultimately they end up updating our parameters.", "tokens": [50364, 823, 11, 294, 4500, 281, 300, 11, 341, 3035, 611, 5313, 406, 445, 264, 1333, 295, 5223, 295, 264, 2430, 763, 294, 264, 2128, 1320, 295, 264, 18161, 2533, 11, 457, 309, 611, 5313, 264, 646, 38377, 13, 50864, 50864, 400, 321, 362, 281, 652, 988, 300, 264, 2771, 2448, 611, 366, 731, 48249, 13, 51014, 51014, 400, 370, 570, 6284, 436, 917, 493, 25113, 527, 9834, 13, 51264, 51264, 400, 437, 436, 915, 510, 807, 257, 688, 295, 264, 5215, 300, 286, 7980, 291, 281, 1401, 807, 11, 457, 309, 311, 406, 2293, 3109, 712, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08406611716393197, "compression_ratio": 1.704724409448819, "no_speech_prob": 1.147843613580335e-05}, {"id": 393, "seek": 198800, "start": 2006.0, "end": 2012.0, "text": " And what they find here through a lot of the analysis that I invite you to read through, but it's not exactly approachable.", "tokens": [50364, 823, 11, 294, 4500, 281, 300, 11, 341, 3035, 611, 5313, 406, 445, 264, 1333, 295, 5223, 295, 264, 2430, 763, 294, 264, 2128, 1320, 295, 264, 18161, 2533, 11, 457, 309, 611, 5313, 264, 646, 38377, 13, 50864, 50864, 400, 321, 362, 281, 652, 988, 300, 264, 2771, 2448, 611, 366, 731, 48249, 13, 51014, 51014, 400, 370, 570, 6284, 436, 917, 493, 25113, 527, 9834, 13, 51264, 51264, 400, 437, 436, 915, 510, 807, 257, 688, 295, 264, 5215, 300, 286, 7980, 291, 281, 1401, 807, 11, 457, 309, 311, 406, 2293, 3109, 712, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08406611716393197, "compression_ratio": 1.704724409448819, "no_speech_prob": 1.147843613580335e-05}, {"id": 394, "seek": 201200, "start": 2012.0, "end": 2029.0, "text": " What they find is basically if you properly initialize the forward pass, the backward pass is also approximately initialized up to a constant factor that has to do with the size of the number of hidden neurons in an early and late layer.", "tokens": [50364, 708, 436, 915, 307, 1936, 498, 291, 6108, 5883, 1125, 264, 2128, 1320, 11, 264, 23897, 1320, 307, 611, 10447, 5883, 1602, 493, 281, 257, 5754, 5952, 300, 575, 281, 360, 365, 264, 2744, 295, 264, 1230, 295, 7633, 22027, 294, 364, 2440, 293, 3469, 4583, 13, 51214, 51214, 400, 457, 1936, 436, 915, 25790, 984, 300, 341, 307, 406, 257, 3922, 300, 7001, 886, 709, 13, 51464, 51464, 823, 11, 341, 10822, 5883, 2144, 307, 611, 12270, 294, 9953, 51, 284, 339, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09142743457447398, "compression_ratio": 1.703056768558952, "no_speech_prob": 5.3380581448436715e-06}, {"id": 395, "seek": 201200, "start": 2029.0, "end": 2034.0, "text": " And but basically they find empirically that this is not a choice that matters too much.", "tokens": [50364, 708, 436, 915, 307, 1936, 498, 291, 6108, 5883, 1125, 264, 2128, 1320, 11, 264, 23897, 1320, 307, 611, 10447, 5883, 1602, 493, 281, 257, 5754, 5952, 300, 575, 281, 360, 365, 264, 2744, 295, 264, 1230, 295, 7633, 22027, 294, 364, 2440, 293, 3469, 4583, 13, 51214, 51214, 400, 457, 1936, 436, 915, 25790, 984, 300, 341, 307, 406, 257, 3922, 300, 7001, 886, 709, 13, 51464, 51464, 823, 11, 341, 10822, 5883, 2144, 307, 611, 12270, 294, 9953, 51, 284, 339, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09142743457447398, "compression_ratio": 1.703056768558952, "no_speech_prob": 5.3380581448436715e-06}, {"id": 396, "seek": 201200, "start": 2034.0, "end": 2038.0, "text": " Now, this timing initialization is also implemented in PyTorch.", "tokens": [50364, 708, 436, 915, 307, 1936, 498, 291, 6108, 5883, 1125, 264, 2128, 1320, 11, 264, 23897, 1320, 307, 611, 10447, 5883, 1602, 493, 281, 257, 5754, 5952, 300, 575, 281, 360, 365, 264, 2744, 295, 264, 1230, 295, 7633, 22027, 294, 364, 2440, 293, 3469, 4583, 13, 51214, 51214, 400, 457, 1936, 436, 915, 25790, 984, 300, 341, 307, 406, 257, 3922, 300, 7001, 886, 709, 13, 51464, 51464, 823, 11, 341, 10822, 5883, 2144, 307, 611, 12270, 294, 9953, 51, 284, 339, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09142743457447398, "compression_ratio": 1.703056768558952, "no_speech_prob": 5.3380581448436715e-06}, {"id": 397, "seek": 203800, "start": 2038.0, "end": 2042.0, "text": " So if you go to torch.enend.init documentation, you'll find timing normal.", "tokens": [50364, 407, 498, 291, 352, 281, 27822, 13, 268, 521, 13, 259, 270, 14333, 11, 291, 603, 915, 10822, 2710, 13, 50564, 50564, 400, 294, 452, 4800, 11, 341, 307, 1391, 264, 881, 2689, 636, 295, 5883, 3319, 18161, 9590, 586, 13, 50814, 50814, 400, 309, 2516, 257, 1326, 20428, 12869, 510, 13, 50914, 50914, 407, 1230, 472, 11, 309, 2738, 281, 458, 264, 4391, 13, 51064, 51064, 6068, 291, 411, 281, 2710, 1125, 264, 2430, 763, 420, 576, 291, 411, 281, 2710, 1125, 264, 2771, 2448, 281, 312, 1009, 39148, 365, 4018, 914, 293, 257, 4985, 420, 472, 3832, 25163, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10343359084356399, "compression_ratio": 1.6774193548387097, "no_speech_prob": 8.013023034436628e-06}, {"id": 398, "seek": 203800, "start": 2042.0, "end": 2047.0, "text": " And in my opinion, this is probably the most common way of initializing neural networks now.", "tokens": [50364, 407, 498, 291, 352, 281, 27822, 13, 268, 521, 13, 259, 270, 14333, 11, 291, 603, 915, 10822, 2710, 13, 50564, 50564, 400, 294, 452, 4800, 11, 341, 307, 1391, 264, 881, 2689, 636, 295, 5883, 3319, 18161, 9590, 586, 13, 50814, 50814, 400, 309, 2516, 257, 1326, 20428, 12869, 510, 13, 50914, 50914, 407, 1230, 472, 11, 309, 2738, 281, 458, 264, 4391, 13, 51064, 51064, 6068, 291, 411, 281, 2710, 1125, 264, 2430, 763, 420, 576, 291, 411, 281, 2710, 1125, 264, 2771, 2448, 281, 312, 1009, 39148, 365, 4018, 914, 293, 257, 4985, 420, 472, 3832, 25163, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10343359084356399, "compression_ratio": 1.6774193548387097, "no_speech_prob": 8.013023034436628e-06}, {"id": 399, "seek": 203800, "start": 2047.0, "end": 2049.0, "text": " And it takes a few keyword arguments here.", "tokens": [50364, 407, 498, 291, 352, 281, 27822, 13, 268, 521, 13, 259, 270, 14333, 11, 291, 603, 915, 10822, 2710, 13, 50564, 50564, 400, 294, 452, 4800, 11, 341, 307, 1391, 264, 881, 2689, 636, 295, 5883, 3319, 18161, 9590, 586, 13, 50814, 50814, 400, 309, 2516, 257, 1326, 20428, 12869, 510, 13, 50914, 50914, 407, 1230, 472, 11, 309, 2738, 281, 458, 264, 4391, 13, 51064, 51064, 6068, 291, 411, 281, 2710, 1125, 264, 2430, 763, 420, 576, 291, 411, 281, 2710, 1125, 264, 2771, 2448, 281, 312, 1009, 39148, 365, 4018, 914, 293, 257, 4985, 420, 472, 3832, 25163, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10343359084356399, "compression_ratio": 1.6774193548387097, "no_speech_prob": 8.013023034436628e-06}, {"id": 400, "seek": 203800, "start": 2049.0, "end": 2052.0, "text": " So number one, it wants to know the mode.", "tokens": [50364, 407, 498, 291, 352, 281, 27822, 13, 268, 521, 13, 259, 270, 14333, 11, 291, 603, 915, 10822, 2710, 13, 50564, 50564, 400, 294, 452, 4800, 11, 341, 307, 1391, 264, 881, 2689, 636, 295, 5883, 3319, 18161, 9590, 586, 13, 50814, 50814, 400, 309, 2516, 257, 1326, 20428, 12869, 510, 13, 50914, 50914, 407, 1230, 472, 11, 309, 2738, 281, 458, 264, 4391, 13, 51064, 51064, 6068, 291, 411, 281, 2710, 1125, 264, 2430, 763, 420, 576, 291, 411, 281, 2710, 1125, 264, 2771, 2448, 281, 312, 1009, 39148, 365, 4018, 914, 293, 257, 4985, 420, 472, 3832, 25163, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10343359084356399, "compression_ratio": 1.6774193548387097, "no_speech_prob": 8.013023034436628e-06}, {"id": 401, "seek": 203800, "start": 2052.0, "end": 2062.0, "text": " Would you like to normalize the activations or would you like to normalize the gradients to be always Gaussian with zero mean and a unit or one standard deviation?", "tokens": [50364, 407, 498, 291, 352, 281, 27822, 13, 268, 521, 13, 259, 270, 14333, 11, 291, 603, 915, 10822, 2710, 13, 50564, 50564, 400, 294, 452, 4800, 11, 341, 307, 1391, 264, 881, 2689, 636, 295, 5883, 3319, 18161, 9590, 586, 13, 50814, 50814, 400, 309, 2516, 257, 1326, 20428, 12869, 510, 13, 50914, 50914, 407, 1230, 472, 11, 309, 2738, 281, 458, 264, 4391, 13, 51064, 51064, 6068, 291, 411, 281, 2710, 1125, 264, 2430, 763, 420, 576, 291, 411, 281, 2710, 1125, 264, 2771, 2448, 281, 312, 1009, 39148, 365, 4018, 914, 293, 257, 4985, 420, 472, 3832, 25163, 30, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10343359084356399, "compression_ratio": 1.6774193548387097, "no_speech_prob": 8.013023034436628e-06}, {"id": 402, "seek": 206200, "start": 2062.0, "end": 2068.0, "text": " And because they find the paper that this doesn't matter too much, most of the people just leave it as the default, which is fan in.", "tokens": [50364, 400, 570, 436, 915, 264, 3035, 300, 341, 1177, 380, 1871, 886, 709, 11, 881, 295, 264, 561, 445, 1856, 309, 382, 264, 7576, 11, 597, 307, 3429, 294, 13, 50664, 50664, 400, 550, 1150, 11, 8437, 264, 2107, 12, 1889, 17409, 300, 291, 366, 1228, 11, 570, 5413, 322, 264, 2107, 12, 1889, 17409, 11, 321, 643, 281, 8873, 257, 4748, 819, 6052, 13, 51064, 51064, 400, 370, 498, 428, 2107, 12, 1889, 17409, 307, 445, 8213, 11, 370, 456, 311, 572, 2107, 12, 1889, 17409, 11, 550, 264, 6052, 510, 486, 312, 472, 13, 51364, 51364, 400, 321, 362, 264, 1900, 912, 733, 295, 8513, 300, 321, 600, 658, 510, 13, 51564, 51564, 583, 498, 264, 2107, 12, 1889, 17409, 307, 746, 1646, 11, 321, 434, 516, 281, 483, 257, 4748, 819, 6052, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08293347358703614, "compression_ratio": 1.925531914893617, "no_speech_prob": 4.157272542215651e-06}, {"id": 403, "seek": 206200, "start": 2068.0, "end": 2076.0, "text": " And then second, passing the non-linearity that you are using, because depending on the non-linearity, we need to calculate a slightly different gain.", "tokens": [50364, 400, 570, 436, 915, 264, 3035, 300, 341, 1177, 380, 1871, 886, 709, 11, 881, 295, 264, 561, 445, 1856, 309, 382, 264, 7576, 11, 597, 307, 3429, 294, 13, 50664, 50664, 400, 550, 1150, 11, 8437, 264, 2107, 12, 1889, 17409, 300, 291, 366, 1228, 11, 570, 5413, 322, 264, 2107, 12, 1889, 17409, 11, 321, 643, 281, 8873, 257, 4748, 819, 6052, 13, 51064, 51064, 400, 370, 498, 428, 2107, 12, 1889, 17409, 307, 445, 8213, 11, 370, 456, 311, 572, 2107, 12, 1889, 17409, 11, 550, 264, 6052, 510, 486, 312, 472, 13, 51364, 51364, 400, 321, 362, 264, 1900, 912, 733, 295, 8513, 300, 321, 600, 658, 510, 13, 51564, 51564, 583, 498, 264, 2107, 12, 1889, 17409, 307, 746, 1646, 11, 321, 434, 516, 281, 483, 257, 4748, 819, 6052, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08293347358703614, "compression_ratio": 1.925531914893617, "no_speech_prob": 4.157272542215651e-06}, {"id": 404, "seek": 206200, "start": 2076.0, "end": 2082.0, "text": " And so if your non-linearity is just linear, so there's no non-linearity, then the gain here will be one.", "tokens": [50364, 400, 570, 436, 915, 264, 3035, 300, 341, 1177, 380, 1871, 886, 709, 11, 881, 295, 264, 561, 445, 1856, 309, 382, 264, 7576, 11, 597, 307, 3429, 294, 13, 50664, 50664, 400, 550, 1150, 11, 8437, 264, 2107, 12, 1889, 17409, 300, 291, 366, 1228, 11, 570, 5413, 322, 264, 2107, 12, 1889, 17409, 11, 321, 643, 281, 8873, 257, 4748, 819, 6052, 13, 51064, 51064, 400, 370, 498, 428, 2107, 12, 1889, 17409, 307, 445, 8213, 11, 370, 456, 311, 572, 2107, 12, 1889, 17409, 11, 550, 264, 6052, 510, 486, 312, 472, 13, 51364, 51364, 400, 321, 362, 264, 1900, 912, 733, 295, 8513, 300, 321, 600, 658, 510, 13, 51564, 51564, 583, 498, 264, 2107, 12, 1889, 17409, 307, 746, 1646, 11, 321, 434, 516, 281, 483, 257, 4748, 819, 6052, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08293347358703614, "compression_ratio": 1.925531914893617, "no_speech_prob": 4.157272542215651e-06}, {"id": 405, "seek": 206200, "start": 2082.0, "end": 2086.0, "text": " And we have the exact same kind of formula that we've got here.", "tokens": [50364, 400, 570, 436, 915, 264, 3035, 300, 341, 1177, 380, 1871, 886, 709, 11, 881, 295, 264, 561, 445, 1856, 309, 382, 264, 7576, 11, 597, 307, 3429, 294, 13, 50664, 50664, 400, 550, 1150, 11, 8437, 264, 2107, 12, 1889, 17409, 300, 291, 366, 1228, 11, 570, 5413, 322, 264, 2107, 12, 1889, 17409, 11, 321, 643, 281, 8873, 257, 4748, 819, 6052, 13, 51064, 51064, 400, 370, 498, 428, 2107, 12, 1889, 17409, 307, 445, 8213, 11, 370, 456, 311, 572, 2107, 12, 1889, 17409, 11, 550, 264, 6052, 510, 486, 312, 472, 13, 51364, 51364, 400, 321, 362, 264, 1900, 912, 733, 295, 8513, 300, 321, 600, 658, 510, 13, 51564, 51564, 583, 498, 264, 2107, 12, 1889, 17409, 307, 746, 1646, 11, 321, 434, 516, 281, 483, 257, 4748, 819, 6052, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08293347358703614, "compression_ratio": 1.925531914893617, "no_speech_prob": 4.157272542215651e-06}, {"id": 406, "seek": 206200, "start": 2086.0, "end": 2089.0, "text": " But if the non-linearity is something else, we're going to get a slightly different gain.", "tokens": [50364, 400, 570, 436, 915, 264, 3035, 300, 341, 1177, 380, 1871, 886, 709, 11, 881, 295, 264, 561, 445, 1856, 309, 382, 264, 7576, 11, 597, 307, 3429, 294, 13, 50664, 50664, 400, 550, 1150, 11, 8437, 264, 2107, 12, 1889, 17409, 300, 291, 366, 1228, 11, 570, 5413, 322, 264, 2107, 12, 1889, 17409, 11, 321, 643, 281, 8873, 257, 4748, 819, 6052, 13, 51064, 51064, 400, 370, 498, 428, 2107, 12, 1889, 17409, 307, 445, 8213, 11, 370, 456, 311, 572, 2107, 12, 1889, 17409, 11, 550, 264, 6052, 510, 486, 312, 472, 13, 51364, 51364, 400, 321, 362, 264, 1900, 912, 733, 295, 8513, 300, 321, 600, 658, 510, 13, 51564, 51564, 583, 498, 264, 2107, 12, 1889, 17409, 307, 746, 1646, 11, 321, 434, 516, 281, 483, 257, 4748, 819, 6052, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08293347358703614, "compression_ratio": 1.925531914893617, "no_speech_prob": 4.157272542215651e-06}, {"id": 407, "seek": 208900, "start": 2089.0, "end": 2096.0, "text": " And so if we come up here to the top, we see that, for example, in the case of Brelu, this gain is a square root of two.", "tokens": [50364, 400, 370, 498, 321, 808, 493, 510, 281, 264, 1192, 11, 321, 536, 300, 11, 337, 1365, 11, 294, 264, 1389, 295, 363, 4419, 84, 11, 341, 6052, 307, 257, 3732, 5593, 295, 732, 13, 50714, 50714, 400, 264, 1778, 309, 311, 257, 3732, 5593, 11, 570, 294, 341, 3035, 11, 291, 536, 577, 264, 732, 307, 1854, 295, 264, 3732, 5593, 13, 51214, 51214, 407, 264, 6052, 307, 257, 3732, 5593, 295, 732, 13, 51364, 51364, 682, 257, 1389, 295, 8213, 420, 6575, 11, 321, 445, 483, 257, 6052, 295, 472, 13, 51564, 51564, 682, 257, 1389, 295, 1266, 39, 11, 597, 307, 437, 321, 434, 1228, 510, 11, 264, 26269, 6052, 307, 257, 1732, 670, 1045, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09157575436723911, "compression_ratio": 1.7922077922077921, "no_speech_prob": 8.13931728771422e-06}, {"id": 408, "seek": 208900, "start": 2096.0, "end": 2106.0, "text": " And the reason it's a square root, because in this paper, you see how the two is inside of the square root.", "tokens": [50364, 400, 370, 498, 321, 808, 493, 510, 281, 264, 1192, 11, 321, 536, 300, 11, 337, 1365, 11, 294, 264, 1389, 295, 363, 4419, 84, 11, 341, 6052, 307, 257, 3732, 5593, 295, 732, 13, 50714, 50714, 400, 264, 1778, 309, 311, 257, 3732, 5593, 11, 570, 294, 341, 3035, 11, 291, 536, 577, 264, 732, 307, 1854, 295, 264, 3732, 5593, 13, 51214, 51214, 407, 264, 6052, 307, 257, 3732, 5593, 295, 732, 13, 51364, 51364, 682, 257, 1389, 295, 8213, 420, 6575, 11, 321, 445, 483, 257, 6052, 295, 472, 13, 51564, 51564, 682, 257, 1389, 295, 1266, 39, 11, 597, 307, 437, 321, 434, 1228, 510, 11, 264, 26269, 6052, 307, 257, 1732, 670, 1045, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09157575436723911, "compression_ratio": 1.7922077922077921, "no_speech_prob": 8.13931728771422e-06}, {"id": 409, "seek": 208900, "start": 2106.0, "end": 2109.0, "text": " So the gain is a square root of two.", "tokens": [50364, 400, 370, 498, 321, 808, 493, 510, 281, 264, 1192, 11, 321, 536, 300, 11, 337, 1365, 11, 294, 264, 1389, 295, 363, 4419, 84, 11, 341, 6052, 307, 257, 3732, 5593, 295, 732, 13, 50714, 50714, 400, 264, 1778, 309, 311, 257, 3732, 5593, 11, 570, 294, 341, 3035, 11, 291, 536, 577, 264, 732, 307, 1854, 295, 264, 3732, 5593, 13, 51214, 51214, 407, 264, 6052, 307, 257, 3732, 5593, 295, 732, 13, 51364, 51364, 682, 257, 1389, 295, 8213, 420, 6575, 11, 321, 445, 483, 257, 6052, 295, 472, 13, 51564, 51564, 682, 257, 1389, 295, 1266, 39, 11, 597, 307, 437, 321, 434, 1228, 510, 11, 264, 26269, 6052, 307, 257, 1732, 670, 1045, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09157575436723911, "compression_ratio": 1.7922077922077921, "no_speech_prob": 8.13931728771422e-06}, {"id": 410, "seek": 208900, "start": 2109.0, "end": 2113.0, "text": " In a case of linear or identity, we just get a gain of one.", "tokens": [50364, 400, 370, 498, 321, 808, 493, 510, 281, 264, 1192, 11, 321, 536, 300, 11, 337, 1365, 11, 294, 264, 1389, 295, 363, 4419, 84, 11, 341, 6052, 307, 257, 3732, 5593, 295, 732, 13, 50714, 50714, 400, 264, 1778, 309, 311, 257, 3732, 5593, 11, 570, 294, 341, 3035, 11, 291, 536, 577, 264, 732, 307, 1854, 295, 264, 3732, 5593, 13, 51214, 51214, 407, 264, 6052, 307, 257, 3732, 5593, 295, 732, 13, 51364, 51364, 682, 257, 1389, 295, 8213, 420, 6575, 11, 321, 445, 483, 257, 6052, 295, 472, 13, 51564, 51564, 682, 257, 1389, 295, 1266, 39, 11, 597, 307, 437, 321, 434, 1228, 510, 11, 264, 26269, 6052, 307, 257, 1732, 670, 1045, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09157575436723911, "compression_ratio": 1.7922077922077921, "no_speech_prob": 8.13931728771422e-06}, {"id": 411, "seek": 208900, "start": 2113.0, "end": 2118.0, "text": " In a case of 10H, which is what we're using here, the advised gain is a five over three.", "tokens": [50364, 400, 370, 498, 321, 808, 493, 510, 281, 264, 1192, 11, 321, 536, 300, 11, 337, 1365, 11, 294, 264, 1389, 295, 363, 4419, 84, 11, 341, 6052, 307, 257, 3732, 5593, 295, 732, 13, 50714, 50714, 400, 264, 1778, 309, 311, 257, 3732, 5593, 11, 570, 294, 341, 3035, 11, 291, 536, 577, 264, 732, 307, 1854, 295, 264, 3732, 5593, 13, 51214, 51214, 407, 264, 6052, 307, 257, 3732, 5593, 295, 732, 13, 51364, 51364, 682, 257, 1389, 295, 8213, 420, 6575, 11, 321, 445, 483, 257, 6052, 295, 472, 13, 51564, 51564, 682, 257, 1389, 295, 1266, 39, 11, 597, 307, 437, 321, 434, 1228, 510, 11, 264, 26269, 6052, 307, 257, 1732, 670, 1045, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09157575436723911, "compression_ratio": 1.7922077922077921, "no_speech_prob": 8.13931728771422e-06}, {"id": 412, "seek": 211800, "start": 2118.0, "end": 2122.0, "text": " And intuitively, why do we need a gain on top of the initialization?", "tokens": [50364, 400, 46506, 11, 983, 360, 321, 643, 257, 6052, 322, 1192, 295, 264, 5883, 2144, 30, 50564, 50564, 467, 311, 570, 1266, 39, 11, 445, 411, 363, 4419, 84, 11, 307, 257, 4364, 488, 9887, 13, 50814, 50814, 407, 437, 300, 1355, 307, 291, 434, 1940, 264, 5598, 7316, 490, 341, 8141, 27290, 11, 293, 550, 291, 366, 2339, 11077, 309, 294, 512, 636, 13, 51114, 51114, 823, 11, 363, 4419, 84, 2339, 12808, 309, 538, 1940, 1203, 2507, 4018, 293, 17690, 278, 309, 281, 4018, 13, 51314, 51314, 1266, 39, 611, 2339, 12808, 309, 570, 309, 311, 257, 4364, 488, 6916, 13, 51464, 51464, 467, 486, 747, 264, 28537, 293, 309, 486, 13578, 552, 294, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04436868872524293, "compression_ratio": 1.6775362318840579, "no_speech_prob": 2.1111798560013995e-05}, {"id": 413, "seek": 211800, "start": 2122.0, "end": 2127.0, "text": " It's because 10H, just like Brelu, is a contractive transformation.", "tokens": [50364, 400, 46506, 11, 983, 360, 321, 643, 257, 6052, 322, 1192, 295, 264, 5883, 2144, 30, 50564, 50564, 467, 311, 570, 1266, 39, 11, 445, 411, 363, 4419, 84, 11, 307, 257, 4364, 488, 9887, 13, 50814, 50814, 407, 437, 300, 1355, 307, 291, 434, 1940, 264, 5598, 7316, 490, 341, 8141, 27290, 11, 293, 550, 291, 366, 2339, 11077, 309, 294, 512, 636, 13, 51114, 51114, 823, 11, 363, 4419, 84, 2339, 12808, 309, 538, 1940, 1203, 2507, 4018, 293, 17690, 278, 309, 281, 4018, 13, 51314, 51314, 1266, 39, 611, 2339, 12808, 309, 570, 309, 311, 257, 4364, 488, 6916, 13, 51464, 51464, 467, 486, 747, 264, 28537, 293, 309, 486, 13578, 552, 294, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04436868872524293, "compression_ratio": 1.6775362318840579, "no_speech_prob": 2.1111798560013995e-05}, {"id": 414, "seek": 211800, "start": 2127.0, "end": 2133.0, "text": " So what that means is you're taking the output distribution from this matrix multiplication, and then you are squashing it in some way.", "tokens": [50364, 400, 46506, 11, 983, 360, 321, 643, 257, 6052, 322, 1192, 295, 264, 5883, 2144, 30, 50564, 50564, 467, 311, 570, 1266, 39, 11, 445, 411, 363, 4419, 84, 11, 307, 257, 4364, 488, 9887, 13, 50814, 50814, 407, 437, 300, 1355, 307, 291, 434, 1940, 264, 5598, 7316, 490, 341, 8141, 27290, 11, 293, 550, 291, 366, 2339, 11077, 309, 294, 512, 636, 13, 51114, 51114, 823, 11, 363, 4419, 84, 2339, 12808, 309, 538, 1940, 1203, 2507, 4018, 293, 17690, 278, 309, 281, 4018, 13, 51314, 51314, 1266, 39, 611, 2339, 12808, 309, 570, 309, 311, 257, 4364, 488, 6916, 13, 51464, 51464, 467, 486, 747, 264, 28537, 293, 309, 486, 13578, 552, 294, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04436868872524293, "compression_ratio": 1.6775362318840579, "no_speech_prob": 2.1111798560013995e-05}, {"id": 415, "seek": 211800, "start": 2133.0, "end": 2137.0, "text": " Now, Brelu squashes it by taking everything below zero and clamping it to zero.", "tokens": [50364, 400, 46506, 11, 983, 360, 321, 643, 257, 6052, 322, 1192, 295, 264, 5883, 2144, 30, 50564, 50564, 467, 311, 570, 1266, 39, 11, 445, 411, 363, 4419, 84, 11, 307, 257, 4364, 488, 9887, 13, 50814, 50814, 407, 437, 300, 1355, 307, 291, 434, 1940, 264, 5598, 7316, 490, 341, 8141, 27290, 11, 293, 550, 291, 366, 2339, 11077, 309, 294, 512, 636, 13, 51114, 51114, 823, 11, 363, 4419, 84, 2339, 12808, 309, 538, 1940, 1203, 2507, 4018, 293, 17690, 278, 309, 281, 4018, 13, 51314, 51314, 1266, 39, 611, 2339, 12808, 309, 570, 309, 311, 257, 4364, 488, 6916, 13, 51464, 51464, 467, 486, 747, 264, 28537, 293, 309, 486, 13578, 552, 294, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04436868872524293, "compression_ratio": 1.6775362318840579, "no_speech_prob": 2.1111798560013995e-05}, {"id": 416, "seek": 211800, "start": 2137.0, "end": 2140.0, "text": " 10H also squashes it because it's a contractive operation.", "tokens": [50364, 400, 46506, 11, 983, 360, 321, 643, 257, 6052, 322, 1192, 295, 264, 5883, 2144, 30, 50564, 50564, 467, 311, 570, 1266, 39, 11, 445, 411, 363, 4419, 84, 11, 307, 257, 4364, 488, 9887, 13, 50814, 50814, 407, 437, 300, 1355, 307, 291, 434, 1940, 264, 5598, 7316, 490, 341, 8141, 27290, 11, 293, 550, 291, 366, 2339, 11077, 309, 294, 512, 636, 13, 51114, 51114, 823, 11, 363, 4419, 84, 2339, 12808, 309, 538, 1940, 1203, 2507, 4018, 293, 17690, 278, 309, 281, 4018, 13, 51314, 51314, 1266, 39, 611, 2339, 12808, 309, 570, 309, 311, 257, 4364, 488, 6916, 13, 51464, 51464, 467, 486, 747, 264, 28537, 293, 309, 486, 13578, 552, 294, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04436868872524293, "compression_ratio": 1.6775362318840579, "no_speech_prob": 2.1111798560013995e-05}, {"id": 417, "seek": 211800, "start": 2140.0, "end": 2144.0, "text": " It will take the tails and it will squeeze them in.", "tokens": [50364, 400, 46506, 11, 983, 360, 321, 643, 257, 6052, 322, 1192, 295, 264, 5883, 2144, 30, 50564, 50564, 467, 311, 570, 1266, 39, 11, 445, 411, 363, 4419, 84, 11, 307, 257, 4364, 488, 9887, 13, 50814, 50814, 407, 437, 300, 1355, 307, 291, 434, 1940, 264, 5598, 7316, 490, 341, 8141, 27290, 11, 293, 550, 291, 366, 2339, 11077, 309, 294, 512, 636, 13, 51114, 51114, 823, 11, 363, 4419, 84, 2339, 12808, 309, 538, 1940, 1203, 2507, 4018, 293, 17690, 278, 309, 281, 4018, 13, 51314, 51314, 1266, 39, 611, 2339, 12808, 309, 570, 309, 311, 257, 4364, 488, 6916, 13, 51464, 51464, 467, 486, 747, 264, 28537, 293, 309, 486, 13578, 552, 294, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04436868872524293, "compression_ratio": 1.6775362318840579, "no_speech_prob": 2.1111798560013995e-05}, {"id": 418, "seek": 214400, "start": 2144.0, "end": 2153.0, "text": " And so in order to fight the squeezing in, we need to boost the weights a little bit so that we renormalize everything back to unit standard deviation.", "tokens": [50364, 400, 370, 294, 1668, 281, 2092, 264, 36645, 294, 11, 321, 643, 281, 9194, 264, 17443, 257, 707, 857, 370, 300, 321, 8124, 24440, 1125, 1203, 646, 281, 4985, 3832, 25163, 13, 50814, 50814, 407, 300, 311, 983, 456, 311, 257, 707, 857, 295, 257, 6052, 300, 1487, 484, 13, 50964, 50964, 823, 11, 286, 478, 31533, 807, 341, 3541, 257, 707, 857, 2661, 11, 293, 286, 478, 884, 300, 767, 22062, 13, 51214, 51214, 400, 264, 1778, 337, 300, 307, 570, 466, 3407, 924, 2057, 562, 341, 3035, 390, 3720, 11, 51464, 51464, 291, 632, 281, 767, 312, 4664, 5026, 365, 264, 2430, 763, 293, 264, 2771, 2448, 293, 641, 22526, 293, 641, 49816, 82, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05179336248350538, "compression_ratio": 1.7366666666666666, "no_speech_prob": 2.1111187379574403e-05}, {"id": 419, "seek": 214400, "start": 2153.0, "end": 2156.0, "text": " So that's why there's a little bit of a gain that comes out.", "tokens": [50364, 400, 370, 294, 1668, 281, 2092, 264, 36645, 294, 11, 321, 643, 281, 9194, 264, 17443, 257, 707, 857, 370, 300, 321, 8124, 24440, 1125, 1203, 646, 281, 4985, 3832, 25163, 13, 50814, 50814, 407, 300, 311, 983, 456, 311, 257, 707, 857, 295, 257, 6052, 300, 1487, 484, 13, 50964, 50964, 823, 11, 286, 478, 31533, 807, 341, 3541, 257, 707, 857, 2661, 11, 293, 286, 478, 884, 300, 767, 22062, 13, 51214, 51214, 400, 264, 1778, 337, 300, 307, 570, 466, 3407, 924, 2057, 562, 341, 3035, 390, 3720, 11, 51464, 51464, 291, 632, 281, 767, 312, 4664, 5026, 365, 264, 2430, 763, 293, 264, 2771, 2448, 293, 641, 22526, 293, 641, 49816, 82, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05179336248350538, "compression_ratio": 1.7366666666666666, "no_speech_prob": 2.1111187379574403e-05}, {"id": 420, "seek": 214400, "start": 2156.0, "end": 2161.0, "text": " Now, I'm skipping through this section a little bit quickly, and I'm doing that actually intentionally.", "tokens": [50364, 400, 370, 294, 1668, 281, 2092, 264, 36645, 294, 11, 321, 643, 281, 9194, 264, 17443, 257, 707, 857, 370, 300, 321, 8124, 24440, 1125, 1203, 646, 281, 4985, 3832, 25163, 13, 50814, 50814, 407, 300, 311, 983, 456, 311, 257, 707, 857, 295, 257, 6052, 300, 1487, 484, 13, 50964, 50964, 823, 11, 286, 478, 31533, 807, 341, 3541, 257, 707, 857, 2661, 11, 293, 286, 478, 884, 300, 767, 22062, 13, 51214, 51214, 400, 264, 1778, 337, 300, 307, 570, 466, 3407, 924, 2057, 562, 341, 3035, 390, 3720, 11, 51464, 51464, 291, 632, 281, 767, 312, 4664, 5026, 365, 264, 2430, 763, 293, 264, 2771, 2448, 293, 641, 22526, 293, 641, 49816, 82, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05179336248350538, "compression_ratio": 1.7366666666666666, "no_speech_prob": 2.1111187379574403e-05}, {"id": 421, "seek": 214400, "start": 2161.0, "end": 2166.0, "text": " And the reason for that is because about seven years ago when this paper was written,", "tokens": [50364, 400, 370, 294, 1668, 281, 2092, 264, 36645, 294, 11, 321, 643, 281, 9194, 264, 17443, 257, 707, 857, 370, 300, 321, 8124, 24440, 1125, 1203, 646, 281, 4985, 3832, 25163, 13, 50814, 50814, 407, 300, 311, 983, 456, 311, 257, 707, 857, 295, 257, 6052, 300, 1487, 484, 13, 50964, 50964, 823, 11, 286, 478, 31533, 807, 341, 3541, 257, 707, 857, 2661, 11, 293, 286, 478, 884, 300, 767, 22062, 13, 51214, 51214, 400, 264, 1778, 337, 300, 307, 570, 466, 3407, 924, 2057, 562, 341, 3035, 390, 3720, 11, 51464, 51464, 291, 632, 281, 767, 312, 4664, 5026, 365, 264, 2430, 763, 293, 264, 2771, 2448, 293, 641, 22526, 293, 641, 49816, 82, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05179336248350538, "compression_ratio": 1.7366666666666666, "no_speech_prob": 2.1111187379574403e-05}, {"id": 422, "seek": 214400, "start": 2166.0, "end": 2172.0, "text": " you had to actually be extremely careful with the activations and the gradients and their ranges and their histograms.", "tokens": [50364, 400, 370, 294, 1668, 281, 2092, 264, 36645, 294, 11, 321, 643, 281, 9194, 264, 17443, 257, 707, 857, 370, 300, 321, 8124, 24440, 1125, 1203, 646, 281, 4985, 3832, 25163, 13, 50814, 50814, 407, 300, 311, 983, 456, 311, 257, 707, 857, 295, 257, 6052, 300, 1487, 484, 13, 50964, 50964, 823, 11, 286, 478, 31533, 807, 341, 3541, 257, 707, 857, 2661, 11, 293, 286, 478, 884, 300, 767, 22062, 13, 51214, 51214, 400, 264, 1778, 337, 300, 307, 570, 466, 3407, 924, 2057, 562, 341, 3035, 390, 3720, 11, 51464, 51464, 291, 632, 281, 767, 312, 4664, 5026, 365, 264, 2430, 763, 293, 264, 2771, 2448, 293, 641, 22526, 293, 641, 49816, 82, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05179336248350538, "compression_ratio": 1.7366666666666666, "no_speech_prob": 2.1111187379574403e-05}, {"id": 423, "seek": 217200, "start": 2172.0, "end": 2177.0, "text": " And you have to be very careful with the precise setting of gains and the scrutinizing of the nonlinearities used and so on.", "tokens": [50364, 400, 291, 362, 281, 312, 588, 5026, 365, 264, 13600, 3287, 295, 16823, 293, 264, 28949, 259, 3319, 295, 264, 2107, 28263, 1088, 1143, 293, 370, 322, 13, 50614, 50614, 400, 1203, 390, 588, 962, 20539, 293, 588, 23847, 293, 588, 6108, 18721, 337, 264, 18161, 2533, 281, 3847, 11, 2318, 498, 428, 18161, 2533, 390, 588, 2452, 13, 51014, 51014, 583, 456, 366, 257, 1230, 295, 4363, 24283, 300, 362, 1027, 1203, 10591, 544, 8351, 293, 544, 731, 48249, 13, 51264, 51264, 400, 309, 311, 1813, 1570, 1021, 281, 5883, 1125, 613, 9590, 2293, 558, 13, 51464, 51464, 400, 512, 295, 729, 4363, 24283, 11, 337, 1365, 11, 366, 27980, 9271, 11, 597, 321, 486, 2060, 294, 264, 2027, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05157738876342773, "compression_ratio": 1.809968847352025, "no_speech_prob": 1.3844237400917336e-05}, {"id": 424, "seek": 217200, "start": 2177.0, "end": 2185.0, "text": " And everything was very finicky and very fragile and very properly arranged for the neural net to train, especially if your neural net was very deep.", "tokens": [50364, 400, 291, 362, 281, 312, 588, 5026, 365, 264, 13600, 3287, 295, 16823, 293, 264, 28949, 259, 3319, 295, 264, 2107, 28263, 1088, 1143, 293, 370, 322, 13, 50614, 50614, 400, 1203, 390, 588, 962, 20539, 293, 588, 23847, 293, 588, 6108, 18721, 337, 264, 18161, 2533, 281, 3847, 11, 2318, 498, 428, 18161, 2533, 390, 588, 2452, 13, 51014, 51014, 583, 456, 366, 257, 1230, 295, 4363, 24283, 300, 362, 1027, 1203, 10591, 544, 8351, 293, 544, 731, 48249, 13, 51264, 51264, 400, 309, 311, 1813, 1570, 1021, 281, 5883, 1125, 613, 9590, 2293, 558, 13, 51464, 51464, 400, 512, 295, 729, 4363, 24283, 11, 337, 1365, 11, 366, 27980, 9271, 11, 597, 321, 486, 2060, 294, 264, 2027, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05157738876342773, "compression_ratio": 1.809968847352025, "no_speech_prob": 1.3844237400917336e-05}, {"id": 425, "seek": 217200, "start": 2185.0, "end": 2190.0, "text": " But there are a number of modern innovations that have made everything significantly more stable and more well behaved.", "tokens": [50364, 400, 291, 362, 281, 312, 588, 5026, 365, 264, 13600, 3287, 295, 16823, 293, 264, 28949, 259, 3319, 295, 264, 2107, 28263, 1088, 1143, 293, 370, 322, 13, 50614, 50614, 400, 1203, 390, 588, 962, 20539, 293, 588, 23847, 293, 588, 6108, 18721, 337, 264, 18161, 2533, 281, 3847, 11, 2318, 498, 428, 18161, 2533, 390, 588, 2452, 13, 51014, 51014, 583, 456, 366, 257, 1230, 295, 4363, 24283, 300, 362, 1027, 1203, 10591, 544, 8351, 293, 544, 731, 48249, 13, 51264, 51264, 400, 309, 311, 1813, 1570, 1021, 281, 5883, 1125, 613, 9590, 2293, 558, 13, 51464, 51464, 400, 512, 295, 729, 4363, 24283, 11, 337, 1365, 11, 366, 27980, 9271, 11, 597, 321, 486, 2060, 294, 264, 2027, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05157738876342773, "compression_ratio": 1.809968847352025, "no_speech_prob": 1.3844237400917336e-05}, {"id": 426, "seek": 217200, "start": 2190.0, "end": 2194.0, "text": " And it's become less important to initialize these networks exactly right.", "tokens": [50364, 400, 291, 362, 281, 312, 588, 5026, 365, 264, 13600, 3287, 295, 16823, 293, 264, 28949, 259, 3319, 295, 264, 2107, 28263, 1088, 1143, 293, 370, 322, 13, 50614, 50614, 400, 1203, 390, 588, 962, 20539, 293, 588, 23847, 293, 588, 6108, 18721, 337, 264, 18161, 2533, 281, 3847, 11, 2318, 498, 428, 18161, 2533, 390, 588, 2452, 13, 51014, 51014, 583, 456, 366, 257, 1230, 295, 4363, 24283, 300, 362, 1027, 1203, 10591, 544, 8351, 293, 544, 731, 48249, 13, 51264, 51264, 400, 309, 311, 1813, 1570, 1021, 281, 5883, 1125, 613, 9590, 2293, 558, 13, 51464, 51464, 400, 512, 295, 729, 4363, 24283, 11, 337, 1365, 11, 366, 27980, 9271, 11, 597, 321, 486, 2060, 294, 264, 2027, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05157738876342773, "compression_ratio": 1.809968847352025, "no_speech_prob": 1.3844237400917336e-05}, {"id": 427, "seek": 217200, "start": 2194.0, "end": 2199.0, "text": " And some of those modern innovations, for example, are residual connections, which we will cover in the future.", "tokens": [50364, 400, 291, 362, 281, 312, 588, 5026, 365, 264, 13600, 3287, 295, 16823, 293, 264, 28949, 259, 3319, 295, 264, 2107, 28263, 1088, 1143, 293, 370, 322, 13, 50614, 50614, 400, 1203, 390, 588, 962, 20539, 293, 588, 23847, 293, 588, 6108, 18721, 337, 264, 18161, 2533, 281, 3847, 11, 2318, 498, 428, 18161, 2533, 390, 588, 2452, 13, 51014, 51014, 583, 456, 366, 257, 1230, 295, 4363, 24283, 300, 362, 1027, 1203, 10591, 544, 8351, 293, 544, 731, 48249, 13, 51264, 51264, 400, 309, 311, 1813, 1570, 1021, 281, 5883, 1125, 613, 9590, 2293, 558, 13, 51464, 51464, 400, 512, 295, 729, 4363, 24283, 11, 337, 1365, 11, 366, 27980, 9271, 11, 597, 321, 486, 2060, 294, 264, 2027, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05157738876342773, "compression_ratio": 1.809968847352025, "no_speech_prob": 1.3844237400917336e-05}, {"id": 428, "seek": 219900, "start": 2199.0, "end": 2207.0, "text": " The use of a number of normalization layers, like, for example, batch normalization, layer normalization, group normalization.", "tokens": [50364, 440, 764, 295, 257, 1230, 295, 2710, 2144, 7914, 11, 411, 11, 337, 1365, 11, 15245, 2710, 2144, 11, 4583, 2710, 2144, 11, 1594, 2710, 2144, 13, 50764, 50764, 492, 434, 516, 281, 352, 666, 257, 688, 295, 613, 382, 731, 13, 50864, 50864, 400, 1230, 1045, 11, 709, 1101, 5028, 22525, 11, 406, 445, 342, 8997, 2750, 16235, 23475, 11, 264, 2199, 5028, 6545, 321, 434, 1936, 1228, 510, 11, 51164, 51164, 457, 4748, 544, 3997, 5028, 22525, 411, 497, 10288, 47, 1513, 293, 2318, 7938, 13, 51364, 51364, 400, 370, 439, 295, 613, 4363, 24283, 652, 309, 1570, 1021, 337, 291, 281, 13402, 21583, 4404, 264, 5883, 2144, 295, 264, 18161, 2533, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06929432844915309, "compression_ratio": 1.7622377622377623, "no_speech_prob": 5.507056357600959e-06}, {"id": 429, "seek": 219900, "start": 2207.0, "end": 2209.0, "text": " We're going to go into a lot of these as well.", "tokens": [50364, 440, 764, 295, 257, 1230, 295, 2710, 2144, 7914, 11, 411, 11, 337, 1365, 11, 15245, 2710, 2144, 11, 4583, 2710, 2144, 11, 1594, 2710, 2144, 13, 50764, 50764, 492, 434, 516, 281, 352, 666, 257, 688, 295, 613, 382, 731, 13, 50864, 50864, 400, 1230, 1045, 11, 709, 1101, 5028, 22525, 11, 406, 445, 342, 8997, 2750, 16235, 23475, 11, 264, 2199, 5028, 6545, 321, 434, 1936, 1228, 510, 11, 51164, 51164, 457, 4748, 544, 3997, 5028, 22525, 411, 497, 10288, 47, 1513, 293, 2318, 7938, 13, 51364, 51364, 400, 370, 439, 295, 613, 4363, 24283, 652, 309, 1570, 1021, 337, 291, 281, 13402, 21583, 4404, 264, 5883, 2144, 295, 264, 18161, 2533, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06929432844915309, "compression_ratio": 1.7622377622377623, "no_speech_prob": 5.507056357600959e-06}, {"id": 430, "seek": 219900, "start": 2209.0, "end": 2215.0, "text": " And number three, much better optimizers, not just stochastic gradient descent, the simple optimizer we're basically using here,", "tokens": [50364, 440, 764, 295, 257, 1230, 295, 2710, 2144, 7914, 11, 411, 11, 337, 1365, 11, 15245, 2710, 2144, 11, 4583, 2710, 2144, 11, 1594, 2710, 2144, 13, 50764, 50764, 492, 434, 516, 281, 352, 666, 257, 688, 295, 613, 382, 731, 13, 50864, 50864, 400, 1230, 1045, 11, 709, 1101, 5028, 22525, 11, 406, 445, 342, 8997, 2750, 16235, 23475, 11, 264, 2199, 5028, 6545, 321, 434, 1936, 1228, 510, 11, 51164, 51164, 457, 4748, 544, 3997, 5028, 22525, 411, 497, 10288, 47, 1513, 293, 2318, 7938, 13, 51364, 51364, 400, 370, 439, 295, 613, 4363, 24283, 652, 309, 1570, 1021, 337, 291, 281, 13402, 21583, 4404, 264, 5883, 2144, 295, 264, 18161, 2533, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06929432844915309, "compression_ratio": 1.7622377622377623, "no_speech_prob": 5.507056357600959e-06}, {"id": 431, "seek": 219900, "start": 2215.0, "end": 2219.0, "text": " but slightly more complex optimizers like RMSProp and especially Adam.", "tokens": [50364, 440, 764, 295, 257, 1230, 295, 2710, 2144, 7914, 11, 411, 11, 337, 1365, 11, 15245, 2710, 2144, 11, 4583, 2710, 2144, 11, 1594, 2710, 2144, 13, 50764, 50764, 492, 434, 516, 281, 352, 666, 257, 688, 295, 613, 382, 731, 13, 50864, 50864, 400, 1230, 1045, 11, 709, 1101, 5028, 22525, 11, 406, 445, 342, 8997, 2750, 16235, 23475, 11, 264, 2199, 5028, 6545, 321, 434, 1936, 1228, 510, 11, 51164, 51164, 457, 4748, 544, 3997, 5028, 22525, 411, 497, 10288, 47, 1513, 293, 2318, 7938, 13, 51364, 51364, 400, 370, 439, 295, 613, 4363, 24283, 652, 309, 1570, 1021, 337, 291, 281, 13402, 21583, 4404, 264, 5883, 2144, 295, 264, 18161, 2533, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06929432844915309, "compression_ratio": 1.7622377622377623, "no_speech_prob": 5.507056357600959e-06}, {"id": 432, "seek": 219900, "start": 2219.0, "end": 2226.0, "text": " And so all of these modern innovations make it less important for you to precisely calibrate the initialization of the neural net.", "tokens": [50364, 440, 764, 295, 257, 1230, 295, 2710, 2144, 7914, 11, 411, 11, 337, 1365, 11, 15245, 2710, 2144, 11, 4583, 2710, 2144, 11, 1594, 2710, 2144, 13, 50764, 50764, 492, 434, 516, 281, 352, 666, 257, 688, 295, 613, 382, 731, 13, 50864, 50864, 400, 1230, 1045, 11, 709, 1101, 5028, 22525, 11, 406, 445, 342, 8997, 2750, 16235, 23475, 11, 264, 2199, 5028, 6545, 321, 434, 1936, 1228, 510, 11, 51164, 51164, 457, 4748, 544, 3997, 5028, 22525, 411, 497, 10288, 47, 1513, 293, 2318, 7938, 13, 51364, 51364, 400, 370, 439, 295, 613, 4363, 24283, 652, 309, 1570, 1021, 337, 291, 281, 13402, 21583, 4404, 264, 5883, 2144, 295, 264, 18161, 2533, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06929432844915309, "compression_ratio": 1.7622377622377623, "no_speech_prob": 5.507056357600959e-06}, {"id": 433, "seek": 222600, "start": 2226.0, "end": 2231.0, "text": " All that being said, in practice, what should we do in practice when I initialize these neural nets?", "tokens": [50364, 1057, 300, 885, 848, 11, 294, 3124, 11, 437, 820, 321, 360, 294, 3124, 562, 286, 5883, 1125, 613, 18161, 36170, 30, 50614, 50614, 286, 1936, 445, 2710, 1125, 452, 17443, 538, 264, 3732, 5593, 295, 264, 3429, 294, 13, 50814, 50814, 407, 1936, 11, 9810, 437, 321, 630, 510, 307, 437, 286, 360, 13, 51064, 51064, 823, 11, 498, 321, 528, 281, 312, 2293, 8559, 510, 11, 321, 393, 352, 538, 294, 309, 295, 733, 295, 2710, 13, 51414, 51414, 639, 307, 577, 321, 576, 4445, 309, 13, 51514, 51514, 492, 528, 281, 992, 264, 3832, 25163, 281, 312, 12634, 670, 264, 3732, 5593, 295, 3429, 294, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10647258927336836, "compression_ratio": 1.697211155378486, "no_speech_prob": 9.514950761513319e-06}, {"id": 434, "seek": 222600, "start": 2231.0, "end": 2235.0, "text": " I basically just normalize my weights by the square root of the fan in.", "tokens": [50364, 1057, 300, 885, 848, 11, 294, 3124, 11, 437, 820, 321, 360, 294, 3124, 562, 286, 5883, 1125, 613, 18161, 36170, 30, 50614, 50614, 286, 1936, 445, 2710, 1125, 452, 17443, 538, 264, 3732, 5593, 295, 264, 3429, 294, 13, 50814, 50814, 407, 1936, 11, 9810, 437, 321, 630, 510, 307, 437, 286, 360, 13, 51064, 51064, 823, 11, 498, 321, 528, 281, 312, 2293, 8559, 510, 11, 321, 393, 352, 538, 294, 309, 295, 733, 295, 2710, 13, 51414, 51414, 639, 307, 577, 321, 576, 4445, 309, 13, 51514, 51514, 492, 528, 281, 992, 264, 3832, 25163, 281, 312, 12634, 670, 264, 3732, 5593, 295, 3429, 294, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10647258927336836, "compression_ratio": 1.697211155378486, "no_speech_prob": 9.514950761513319e-06}, {"id": 435, "seek": 222600, "start": 2235.0, "end": 2240.0, "text": " So basically, roughly what we did here is what I do.", "tokens": [50364, 1057, 300, 885, 848, 11, 294, 3124, 11, 437, 820, 321, 360, 294, 3124, 562, 286, 5883, 1125, 613, 18161, 36170, 30, 50614, 50614, 286, 1936, 445, 2710, 1125, 452, 17443, 538, 264, 3732, 5593, 295, 264, 3429, 294, 13, 50814, 50814, 407, 1936, 11, 9810, 437, 321, 630, 510, 307, 437, 286, 360, 13, 51064, 51064, 823, 11, 498, 321, 528, 281, 312, 2293, 8559, 510, 11, 321, 393, 352, 538, 294, 309, 295, 733, 295, 2710, 13, 51414, 51414, 639, 307, 577, 321, 576, 4445, 309, 13, 51514, 51514, 492, 528, 281, 992, 264, 3832, 25163, 281, 312, 12634, 670, 264, 3732, 5593, 295, 3429, 294, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10647258927336836, "compression_ratio": 1.697211155378486, "no_speech_prob": 9.514950761513319e-06}, {"id": 436, "seek": 222600, "start": 2240.0, "end": 2247.0, "text": " Now, if we want to be exactly accurate here, we can go by in it of kind of normal.", "tokens": [50364, 1057, 300, 885, 848, 11, 294, 3124, 11, 437, 820, 321, 360, 294, 3124, 562, 286, 5883, 1125, 613, 18161, 36170, 30, 50614, 50614, 286, 1936, 445, 2710, 1125, 452, 17443, 538, 264, 3732, 5593, 295, 264, 3429, 294, 13, 50814, 50814, 407, 1936, 11, 9810, 437, 321, 630, 510, 307, 437, 286, 360, 13, 51064, 51064, 823, 11, 498, 321, 528, 281, 312, 2293, 8559, 510, 11, 321, 393, 352, 538, 294, 309, 295, 733, 295, 2710, 13, 51414, 51414, 639, 307, 577, 321, 576, 4445, 309, 13, 51514, 51514, 492, 528, 281, 992, 264, 3832, 25163, 281, 312, 12634, 670, 264, 3732, 5593, 295, 3429, 294, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10647258927336836, "compression_ratio": 1.697211155378486, "no_speech_prob": 9.514950761513319e-06}, {"id": 437, "seek": 222600, "start": 2247.0, "end": 2249.0, "text": " This is how we would implement it.", "tokens": [50364, 1057, 300, 885, 848, 11, 294, 3124, 11, 437, 820, 321, 360, 294, 3124, 562, 286, 5883, 1125, 613, 18161, 36170, 30, 50614, 50614, 286, 1936, 445, 2710, 1125, 452, 17443, 538, 264, 3732, 5593, 295, 264, 3429, 294, 13, 50814, 50814, 407, 1936, 11, 9810, 437, 321, 630, 510, 307, 437, 286, 360, 13, 51064, 51064, 823, 11, 498, 321, 528, 281, 312, 2293, 8559, 510, 11, 321, 393, 352, 538, 294, 309, 295, 733, 295, 2710, 13, 51414, 51414, 639, 307, 577, 321, 576, 4445, 309, 13, 51514, 51514, 492, 528, 281, 992, 264, 3832, 25163, 281, 312, 12634, 670, 264, 3732, 5593, 295, 3429, 294, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10647258927336836, "compression_ratio": 1.697211155378486, "no_speech_prob": 9.514950761513319e-06}, {"id": 438, "seek": 222600, "start": 2249.0, "end": 2254.0, "text": " We want to set the standard deviation to be gained over the square root of fan in.", "tokens": [50364, 1057, 300, 885, 848, 11, 294, 3124, 11, 437, 820, 321, 360, 294, 3124, 562, 286, 5883, 1125, 613, 18161, 36170, 30, 50614, 50614, 286, 1936, 445, 2710, 1125, 452, 17443, 538, 264, 3732, 5593, 295, 264, 3429, 294, 13, 50814, 50814, 407, 1936, 11, 9810, 437, 321, 630, 510, 307, 437, 286, 360, 13, 51064, 51064, 823, 11, 498, 321, 528, 281, 312, 2293, 8559, 510, 11, 321, 393, 352, 538, 294, 309, 295, 733, 295, 2710, 13, 51414, 51414, 639, 307, 577, 321, 576, 4445, 309, 13, 51514, 51514, 492, 528, 281, 992, 264, 3832, 25163, 281, 312, 12634, 670, 264, 3732, 5593, 295, 3429, 294, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10647258927336836, "compression_ratio": 1.697211155378486, "no_speech_prob": 9.514950761513319e-06}, {"id": 439, "seek": 225400, "start": 2254.0, "end": 2261.0, "text": " Right. So to set the standard deviation of our weights, we will proceed as follows.", "tokens": [50364, 1779, 13, 407, 281, 992, 264, 3832, 25163, 295, 527, 17443, 11, 321, 486, 8991, 382, 10002, 13, 50714, 50714, 8537, 11, 562, 321, 362, 3930, 82, 357, 446, 12, 49, 16138, 11, 293, 718, 311, 584, 286, 445, 1884, 257, 4714, 3547, 11, 321, 393, 574, 412, 264, 3832, 25163, 295, 341, 13, 51014, 51014, 400, 295, 1164, 11, 300, 311, 472, 13, 663, 311, 264, 2372, 295, 3974, 13, 51164, 51164, 961, 311, 652, 341, 257, 857, 3801, 370, 309, 311, 4966, 281, 472, 13, 51264, 51264, 407, 300, 311, 264, 3974, 295, 264, 39148, 295, 4018, 914, 293, 4985, 3832, 25163, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13092558099589216, "compression_ratio": 1.6818181818181819, "no_speech_prob": 1.0129573638550937e-05}, {"id": 440, "seek": 225400, "start": 2261.0, "end": 2267.0, "text": " Basically, when we have torshtart-Rannon, and let's say I just create a thousand numbers, we can look at the standard deviation of this.", "tokens": [50364, 1779, 13, 407, 281, 992, 264, 3832, 25163, 295, 527, 17443, 11, 321, 486, 8991, 382, 10002, 13, 50714, 50714, 8537, 11, 562, 321, 362, 3930, 82, 357, 446, 12, 49, 16138, 11, 293, 718, 311, 584, 286, 445, 1884, 257, 4714, 3547, 11, 321, 393, 574, 412, 264, 3832, 25163, 295, 341, 13, 51014, 51014, 400, 295, 1164, 11, 300, 311, 472, 13, 663, 311, 264, 2372, 295, 3974, 13, 51164, 51164, 961, 311, 652, 341, 257, 857, 3801, 370, 309, 311, 4966, 281, 472, 13, 51264, 51264, 407, 300, 311, 264, 3974, 295, 264, 39148, 295, 4018, 914, 293, 4985, 3832, 25163, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13092558099589216, "compression_ratio": 1.6818181818181819, "no_speech_prob": 1.0129573638550937e-05}, {"id": 441, "seek": 225400, "start": 2267.0, "end": 2270.0, "text": " And of course, that's one. That's the amount of spread.", "tokens": [50364, 1779, 13, 407, 281, 992, 264, 3832, 25163, 295, 527, 17443, 11, 321, 486, 8991, 382, 10002, 13, 50714, 50714, 8537, 11, 562, 321, 362, 3930, 82, 357, 446, 12, 49, 16138, 11, 293, 718, 311, 584, 286, 445, 1884, 257, 4714, 3547, 11, 321, 393, 574, 412, 264, 3832, 25163, 295, 341, 13, 51014, 51014, 400, 295, 1164, 11, 300, 311, 472, 13, 663, 311, 264, 2372, 295, 3974, 13, 51164, 51164, 961, 311, 652, 341, 257, 857, 3801, 370, 309, 311, 4966, 281, 472, 13, 51264, 51264, 407, 300, 311, 264, 3974, 295, 264, 39148, 295, 4018, 914, 293, 4985, 3832, 25163, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13092558099589216, "compression_ratio": 1.6818181818181819, "no_speech_prob": 1.0129573638550937e-05}, {"id": 442, "seek": 225400, "start": 2270.0, "end": 2272.0, "text": " Let's make this a bit bigger so it's closer to one.", "tokens": [50364, 1779, 13, 407, 281, 992, 264, 3832, 25163, 295, 527, 17443, 11, 321, 486, 8991, 382, 10002, 13, 50714, 50714, 8537, 11, 562, 321, 362, 3930, 82, 357, 446, 12, 49, 16138, 11, 293, 718, 311, 584, 286, 445, 1884, 257, 4714, 3547, 11, 321, 393, 574, 412, 264, 3832, 25163, 295, 341, 13, 51014, 51014, 400, 295, 1164, 11, 300, 311, 472, 13, 663, 311, 264, 2372, 295, 3974, 13, 51164, 51164, 961, 311, 652, 341, 257, 857, 3801, 370, 309, 311, 4966, 281, 472, 13, 51264, 51264, 407, 300, 311, 264, 3974, 295, 264, 39148, 295, 4018, 914, 293, 4985, 3832, 25163, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13092558099589216, "compression_ratio": 1.6818181818181819, "no_speech_prob": 1.0129573638550937e-05}, {"id": 443, "seek": 225400, "start": 2272.0, "end": 2278.0, "text": " So that's the spread of the Gaussian of zero mean and unit standard deviation.", "tokens": [50364, 1779, 13, 407, 281, 992, 264, 3832, 25163, 295, 527, 17443, 11, 321, 486, 8991, 382, 10002, 13, 50714, 50714, 8537, 11, 562, 321, 362, 3930, 82, 357, 446, 12, 49, 16138, 11, 293, 718, 311, 584, 286, 445, 1884, 257, 4714, 3547, 11, 321, 393, 574, 412, 264, 3832, 25163, 295, 341, 13, 51014, 51014, 400, 295, 1164, 11, 300, 311, 472, 13, 663, 311, 264, 2372, 295, 3974, 13, 51164, 51164, 961, 311, 652, 341, 257, 857, 3801, 370, 309, 311, 4966, 281, 472, 13, 51264, 51264, 407, 300, 311, 264, 3974, 295, 264, 39148, 295, 4018, 914, 293, 4985, 3832, 25163, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.13092558099589216, "compression_ratio": 1.6818181818181819, "no_speech_prob": 1.0129573638550937e-05}, {"id": 444, "seek": 227800, "start": 2278.0, "end": 2287.0, "text": " Now, basically, when you take these and you multiply by, say, point two, that basically scales down the Gaussian and that makes its standard deviation point two.", "tokens": [50364, 823, 11, 1936, 11, 562, 291, 747, 613, 293, 291, 12972, 538, 11, 584, 11, 935, 732, 11, 300, 1936, 17408, 760, 264, 39148, 293, 300, 1669, 1080, 3832, 25163, 935, 732, 13, 50814, 50814, 407, 1936, 11, 264, 1230, 300, 291, 12972, 538, 510, 5314, 493, 885, 264, 3832, 25163, 295, 341, 39148, 13, 51064, 51064, 407, 510, 11, 341, 307, 257, 3832, 25163, 935, 732, 39148, 510, 562, 321, 6889, 527, 343, 472, 13, 51414, 51414, 583, 321, 528, 281, 992, 264, 3832, 25163, 281, 6052, 670, 3732, 5593, 295, 3429, 4391, 11, 597, 307, 3429, 294, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10687379653637226, "compression_ratio": 1.9567099567099566, "no_speech_prob": 4.637746314983815e-06}, {"id": 445, "seek": 227800, "start": 2287.0, "end": 2292.0, "text": " So basically, the number that you multiply by here ends up being the standard deviation of this Gaussian.", "tokens": [50364, 823, 11, 1936, 11, 562, 291, 747, 613, 293, 291, 12972, 538, 11, 584, 11, 935, 732, 11, 300, 1936, 17408, 760, 264, 39148, 293, 300, 1669, 1080, 3832, 25163, 935, 732, 13, 50814, 50814, 407, 1936, 11, 264, 1230, 300, 291, 12972, 538, 510, 5314, 493, 885, 264, 3832, 25163, 295, 341, 39148, 13, 51064, 51064, 407, 510, 11, 341, 307, 257, 3832, 25163, 935, 732, 39148, 510, 562, 321, 6889, 527, 343, 472, 13, 51414, 51414, 583, 321, 528, 281, 992, 264, 3832, 25163, 281, 6052, 670, 3732, 5593, 295, 3429, 4391, 11, 597, 307, 3429, 294, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10687379653637226, "compression_ratio": 1.9567099567099566, "no_speech_prob": 4.637746314983815e-06}, {"id": 446, "seek": 227800, "start": 2292.0, "end": 2299.0, "text": " So here, this is a standard deviation point two Gaussian here when we sample our W one.", "tokens": [50364, 823, 11, 1936, 11, 562, 291, 747, 613, 293, 291, 12972, 538, 11, 584, 11, 935, 732, 11, 300, 1936, 17408, 760, 264, 39148, 293, 300, 1669, 1080, 3832, 25163, 935, 732, 13, 50814, 50814, 407, 1936, 11, 264, 1230, 300, 291, 12972, 538, 510, 5314, 493, 885, 264, 3832, 25163, 295, 341, 39148, 13, 51064, 51064, 407, 510, 11, 341, 307, 257, 3832, 25163, 935, 732, 39148, 510, 562, 321, 6889, 527, 343, 472, 13, 51414, 51414, 583, 321, 528, 281, 992, 264, 3832, 25163, 281, 6052, 670, 3732, 5593, 295, 3429, 4391, 11, 597, 307, 3429, 294, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10687379653637226, "compression_ratio": 1.9567099567099566, "no_speech_prob": 4.637746314983815e-06}, {"id": 447, "seek": 227800, "start": 2299.0, "end": 2306.0, "text": " But we want to set the standard deviation to gain over square root of fan mode, which is fan in.", "tokens": [50364, 823, 11, 1936, 11, 562, 291, 747, 613, 293, 291, 12972, 538, 11, 584, 11, 935, 732, 11, 300, 1936, 17408, 760, 264, 39148, 293, 300, 1669, 1080, 3832, 25163, 935, 732, 13, 50814, 50814, 407, 1936, 11, 264, 1230, 300, 291, 12972, 538, 510, 5314, 493, 885, 264, 3832, 25163, 295, 341, 39148, 13, 51064, 51064, 407, 510, 11, 341, 307, 257, 3832, 25163, 935, 732, 39148, 510, 562, 321, 6889, 527, 343, 472, 13, 51414, 51414, 583, 321, 528, 281, 992, 264, 3832, 25163, 281, 6052, 670, 3732, 5593, 295, 3429, 4391, 11, 597, 307, 3429, 294, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10687379653637226, "compression_ratio": 1.9567099567099566, "no_speech_prob": 4.637746314983815e-06}, {"id": 448, "seek": 230600, "start": 2306.0, "end": 2314.0, "text": " So in other words, we want to multiply by gain, which for 10 H is five over three.", "tokens": [50364, 407, 294, 661, 2283, 11, 321, 528, 281, 12972, 538, 6052, 11, 597, 337, 1266, 389, 307, 1732, 670, 1045, 13, 50764, 50764, 9436, 670, 1045, 307, 264, 6052, 293, 550, 1413, 13, 51264, 51264, 286, 2041, 286, 9845, 3732, 5593, 295, 264, 3429, 294, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.24310869216918946, "compression_ratio": 1.328125, "no_speech_prob": 1.834229078667704e-05}, {"id": 449, "seek": 230600, "start": 2314.0, "end": 2324.0, "text": " Five over three is the gain and then times.", "tokens": [50364, 407, 294, 661, 2283, 11, 321, 528, 281, 12972, 538, 6052, 11, 597, 337, 1266, 389, 307, 1732, 670, 1045, 13, 50764, 50764, 9436, 670, 1045, 307, 264, 6052, 293, 550, 1413, 13, 51264, 51264, 286, 2041, 286, 9845, 3732, 5593, 295, 264, 3429, 294, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.24310869216918946, "compression_ratio": 1.328125, "no_speech_prob": 1.834229078667704e-05}, {"id": 450, "seek": 230600, "start": 2324.0, "end": 2331.0, "text": " I guess I divide square root of the fan in.", "tokens": [50364, 407, 294, 661, 2283, 11, 321, 528, 281, 12972, 538, 6052, 11, 597, 337, 1266, 389, 307, 1732, 670, 1045, 13, 50764, 50764, 9436, 670, 1045, 307, 264, 6052, 293, 550, 1413, 13, 51264, 51264, 286, 2041, 286, 9845, 3732, 5593, 295, 264, 3429, 294, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.24310869216918946, "compression_ratio": 1.328125, "no_speech_prob": 1.834229078667704e-05}, {"id": 451, "seek": 233100, "start": 2331.0, "end": 2342.0, "text": " And in this example here, the fan in was 10. And I just noticed that actually here the fan in for W one is actually an embed times block size, which, as you will recall, is actually 30.", "tokens": [50364, 400, 294, 341, 1365, 510, 11, 264, 3429, 294, 390, 1266, 13, 400, 286, 445, 5694, 300, 767, 510, 264, 3429, 294, 337, 343, 472, 307, 767, 364, 12240, 1413, 3461, 2744, 11, 597, 11, 382, 291, 486, 9901, 11, 307, 767, 2217, 13, 50914, 50914, 400, 300, 311, 570, 1184, 2517, 307, 1266, 18795, 13, 583, 550, 321, 362, 1045, 295, 552, 293, 321, 1588, 7186, 473, 552, 13, 51114, 51114, 407, 767, 11, 264, 3429, 294, 510, 390, 2217, 293, 286, 820, 362, 1143, 2217, 510, 1391, 13, 51314, 51314, 583, 1936, 11, 321, 528, 2217, 3732, 5593, 13, 407, 341, 307, 264, 1230, 13, 51514, 51514, 639, 307, 437, 527, 3832, 25163, 321, 528, 281, 312, 13, 400, 341, 1230, 4523, 484, 281, 312, 935, 1045, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10680335315305796, "compression_ratio": 1.787162162162162, "no_speech_prob": 4.157276634941809e-06}, {"id": 452, "seek": 233100, "start": 2342.0, "end": 2346.0, "text": " And that's because each character is 10 dimensional. But then we have three of them and we concatenate them.", "tokens": [50364, 400, 294, 341, 1365, 510, 11, 264, 3429, 294, 390, 1266, 13, 400, 286, 445, 5694, 300, 767, 510, 264, 3429, 294, 337, 343, 472, 307, 767, 364, 12240, 1413, 3461, 2744, 11, 597, 11, 382, 291, 486, 9901, 11, 307, 767, 2217, 13, 50914, 50914, 400, 300, 311, 570, 1184, 2517, 307, 1266, 18795, 13, 583, 550, 321, 362, 1045, 295, 552, 293, 321, 1588, 7186, 473, 552, 13, 51114, 51114, 407, 767, 11, 264, 3429, 294, 510, 390, 2217, 293, 286, 820, 362, 1143, 2217, 510, 1391, 13, 51314, 51314, 583, 1936, 11, 321, 528, 2217, 3732, 5593, 13, 407, 341, 307, 264, 1230, 13, 51514, 51514, 639, 307, 437, 527, 3832, 25163, 321, 528, 281, 312, 13, 400, 341, 1230, 4523, 484, 281, 312, 935, 1045, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10680335315305796, "compression_ratio": 1.787162162162162, "no_speech_prob": 4.157276634941809e-06}, {"id": 453, "seek": 233100, "start": 2346.0, "end": 2350.0, "text": " So actually, the fan in here was 30 and I should have used 30 here probably.", "tokens": [50364, 400, 294, 341, 1365, 510, 11, 264, 3429, 294, 390, 1266, 13, 400, 286, 445, 5694, 300, 767, 510, 264, 3429, 294, 337, 343, 472, 307, 767, 364, 12240, 1413, 3461, 2744, 11, 597, 11, 382, 291, 486, 9901, 11, 307, 767, 2217, 13, 50914, 50914, 400, 300, 311, 570, 1184, 2517, 307, 1266, 18795, 13, 583, 550, 321, 362, 1045, 295, 552, 293, 321, 1588, 7186, 473, 552, 13, 51114, 51114, 407, 767, 11, 264, 3429, 294, 510, 390, 2217, 293, 286, 820, 362, 1143, 2217, 510, 1391, 13, 51314, 51314, 583, 1936, 11, 321, 528, 2217, 3732, 5593, 13, 407, 341, 307, 264, 1230, 13, 51514, 51514, 639, 307, 437, 527, 3832, 25163, 321, 528, 281, 312, 13, 400, 341, 1230, 4523, 484, 281, 312, 935, 1045, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10680335315305796, "compression_ratio": 1.787162162162162, "no_speech_prob": 4.157276634941809e-06}, {"id": 454, "seek": 233100, "start": 2350.0, "end": 2354.0, "text": " But basically, we want 30 square root. So this is the number.", "tokens": [50364, 400, 294, 341, 1365, 510, 11, 264, 3429, 294, 390, 1266, 13, 400, 286, 445, 5694, 300, 767, 510, 264, 3429, 294, 337, 343, 472, 307, 767, 364, 12240, 1413, 3461, 2744, 11, 597, 11, 382, 291, 486, 9901, 11, 307, 767, 2217, 13, 50914, 50914, 400, 300, 311, 570, 1184, 2517, 307, 1266, 18795, 13, 583, 550, 321, 362, 1045, 295, 552, 293, 321, 1588, 7186, 473, 552, 13, 51114, 51114, 407, 767, 11, 264, 3429, 294, 510, 390, 2217, 293, 286, 820, 362, 1143, 2217, 510, 1391, 13, 51314, 51314, 583, 1936, 11, 321, 528, 2217, 3732, 5593, 13, 407, 341, 307, 264, 1230, 13, 51514, 51514, 639, 307, 437, 527, 3832, 25163, 321, 528, 281, 312, 13, 400, 341, 1230, 4523, 484, 281, 312, 935, 1045, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10680335315305796, "compression_ratio": 1.787162162162162, "no_speech_prob": 4.157276634941809e-06}, {"id": 455, "seek": 233100, "start": 2354.0, "end": 2359.0, "text": " This is what our standard deviation we want to be. And this number turns out to be point three.", "tokens": [50364, 400, 294, 341, 1365, 510, 11, 264, 3429, 294, 390, 1266, 13, 400, 286, 445, 5694, 300, 767, 510, 264, 3429, 294, 337, 343, 472, 307, 767, 364, 12240, 1413, 3461, 2744, 11, 597, 11, 382, 291, 486, 9901, 11, 307, 767, 2217, 13, 50914, 50914, 400, 300, 311, 570, 1184, 2517, 307, 1266, 18795, 13, 583, 550, 321, 362, 1045, 295, 552, 293, 321, 1588, 7186, 473, 552, 13, 51114, 51114, 407, 767, 11, 264, 3429, 294, 510, 390, 2217, 293, 286, 820, 362, 1143, 2217, 510, 1391, 13, 51314, 51314, 583, 1936, 11, 321, 528, 2217, 3732, 5593, 13, 407, 341, 307, 264, 1230, 13, 51514, 51514, 639, 307, 437, 527, 3832, 25163, 321, 528, 281, 312, 13, 400, 341, 1230, 4523, 484, 281, 312, 935, 1045, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10680335315305796, "compression_ratio": 1.787162162162162, "no_speech_prob": 4.157276634941809e-06}, {"id": 456, "seek": 235900, "start": 2359.0, "end": 2366.0, "text": " Whereas here, just by fiddling with it and looking at the distribution and making sure it looks OK, we came up with point two.", "tokens": [50364, 13813, 510, 11, 445, 538, 283, 14273, 1688, 365, 309, 293, 1237, 412, 264, 7316, 293, 1455, 988, 309, 1542, 2264, 11, 321, 1361, 493, 365, 935, 732, 13, 50714, 50714, 400, 370, 2602, 11, 437, 321, 528, 281, 360, 510, 307, 321, 528, 281, 652, 264, 3832, 25163, 312, 1732, 670, 1045, 11, 597, 307, 527, 6052, 9845, 341, 2372, 1413, 935, 732, 3732, 5593, 13, 51464, 51464, 400, 613, 26179, 510, 366, 406, 300, 4818, 11, 457, 286, 603, 445, 829, 552, 510, 337, 16992, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06490048636560855, "compression_ratio": 1.6367521367521367, "no_speech_prob": 2.3320360469369916e-06}, {"id": 457, "seek": 235900, "start": 2366.0, "end": 2381.0, "text": " And so instead, what we want to do here is we want to make the standard deviation be five over three, which is our gain divide this amount times point two square root.", "tokens": [50364, 13813, 510, 11, 445, 538, 283, 14273, 1688, 365, 309, 293, 1237, 412, 264, 7316, 293, 1455, 988, 309, 1542, 2264, 11, 321, 1361, 493, 365, 935, 732, 13, 50714, 50714, 400, 370, 2602, 11, 437, 321, 528, 281, 360, 510, 307, 321, 528, 281, 652, 264, 3832, 25163, 312, 1732, 670, 1045, 11, 597, 307, 527, 6052, 9845, 341, 2372, 1413, 935, 732, 3732, 5593, 13, 51464, 51464, 400, 613, 26179, 510, 366, 406, 300, 4818, 11, 457, 286, 603, 445, 829, 552, 510, 337, 16992, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06490048636560855, "compression_ratio": 1.6367521367521367, "no_speech_prob": 2.3320360469369916e-06}, {"id": 458, "seek": 235900, "start": 2381.0, "end": 2386.0, "text": " And these brackets here are not that necessary, but I'll just put them here for clarity.", "tokens": [50364, 13813, 510, 11, 445, 538, 283, 14273, 1688, 365, 309, 293, 1237, 412, 264, 7316, 293, 1455, 988, 309, 1542, 2264, 11, 321, 1361, 493, 365, 935, 732, 13, 50714, 50714, 400, 370, 2602, 11, 437, 321, 528, 281, 360, 510, 307, 321, 528, 281, 652, 264, 3832, 25163, 312, 1732, 670, 1045, 11, 597, 307, 527, 6052, 9845, 341, 2372, 1413, 935, 732, 3732, 5593, 13, 51464, 51464, 400, 613, 26179, 510, 366, 406, 300, 4818, 11, 457, 286, 603, 445, 829, 552, 510, 337, 16992, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06490048636560855, "compression_ratio": 1.6367521367521367, "no_speech_prob": 2.3320360469369916e-06}, {"id": 459, "seek": 238600, "start": 2386.0, "end": 2392.0, "text": " This is basically what we want. This is the timing in it. In our case, for a 10H non-linearity.", "tokens": [50364, 639, 307, 1936, 437, 321, 528, 13, 639, 307, 264, 10822, 294, 309, 13, 682, 527, 1389, 11, 337, 257, 1266, 39, 2107, 12, 1889, 17409, 13, 50664, 50664, 400, 341, 307, 577, 321, 576, 5883, 1125, 264, 18161, 2533, 13, 400, 370, 321, 434, 30955, 538, 935, 1045, 2602, 295, 30955, 538, 935, 732, 13, 51114, 51114, 400, 370, 321, 393, 321, 393, 5883, 1125, 341, 636, 293, 550, 321, 393, 3847, 264, 18161, 2533, 293, 536, 437, 321, 658, 13, 51464, 51464, 2264, 11, 370, 286, 8895, 264, 18161, 2533, 293, 321, 917, 493, 294, 9810, 264, 912, 4008, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11290170561592534, "compression_ratio": 1.7954545454545454, "no_speech_prob": 8.013133083295543e-06}, {"id": 460, "seek": 238600, "start": 2392.0, "end": 2401.0, "text": " And this is how we would initialize the neural net. And so we're multiplying by point three instead of multiplying by point two.", "tokens": [50364, 639, 307, 1936, 437, 321, 528, 13, 639, 307, 264, 10822, 294, 309, 13, 682, 527, 1389, 11, 337, 257, 1266, 39, 2107, 12, 1889, 17409, 13, 50664, 50664, 400, 341, 307, 577, 321, 576, 5883, 1125, 264, 18161, 2533, 13, 400, 370, 321, 434, 30955, 538, 935, 1045, 2602, 295, 30955, 538, 935, 732, 13, 51114, 51114, 400, 370, 321, 393, 321, 393, 5883, 1125, 341, 636, 293, 550, 321, 393, 3847, 264, 18161, 2533, 293, 536, 437, 321, 658, 13, 51464, 51464, 2264, 11, 370, 286, 8895, 264, 18161, 2533, 293, 321, 917, 493, 294, 9810, 264, 912, 4008, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11290170561592534, "compression_ratio": 1.7954545454545454, "no_speech_prob": 8.013133083295543e-06}, {"id": 461, "seek": 238600, "start": 2401.0, "end": 2408.0, "text": " And so we can we can initialize this way and then we can train the neural net and see what we got.", "tokens": [50364, 639, 307, 1936, 437, 321, 528, 13, 639, 307, 264, 10822, 294, 309, 13, 682, 527, 1389, 11, 337, 257, 1266, 39, 2107, 12, 1889, 17409, 13, 50664, 50664, 400, 341, 307, 577, 321, 576, 5883, 1125, 264, 18161, 2533, 13, 400, 370, 321, 434, 30955, 538, 935, 1045, 2602, 295, 30955, 538, 935, 732, 13, 51114, 51114, 400, 370, 321, 393, 321, 393, 5883, 1125, 341, 636, 293, 550, 321, 393, 3847, 264, 18161, 2533, 293, 536, 437, 321, 658, 13, 51464, 51464, 2264, 11, 370, 286, 8895, 264, 18161, 2533, 293, 321, 917, 493, 294, 9810, 264, 912, 4008, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11290170561592534, "compression_ratio": 1.7954545454545454, "no_speech_prob": 8.013133083295543e-06}, {"id": 462, "seek": 238600, "start": 2408.0, "end": 2412.0, "text": " OK, so I trained the neural net and we end up in roughly the same spot.", "tokens": [50364, 639, 307, 1936, 437, 321, 528, 13, 639, 307, 264, 10822, 294, 309, 13, 682, 527, 1389, 11, 337, 257, 1266, 39, 2107, 12, 1889, 17409, 13, 50664, 50664, 400, 341, 307, 577, 321, 576, 5883, 1125, 264, 18161, 2533, 13, 400, 370, 321, 434, 30955, 538, 935, 1045, 2602, 295, 30955, 538, 935, 732, 13, 51114, 51114, 400, 370, 321, 393, 321, 393, 5883, 1125, 341, 636, 293, 550, 321, 393, 3847, 264, 18161, 2533, 293, 536, 437, 321, 658, 13, 51464, 51464, 2264, 11, 370, 286, 8895, 264, 18161, 2533, 293, 321, 917, 493, 294, 9810, 264, 912, 4008, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.11290170561592534, "compression_ratio": 1.7954545454545454, "no_speech_prob": 8.013133083295543e-06}, {"id": 463, "seek": 241200, "start": 2412.0, "end": 2417.0, "text": " So looking at the validation loss, we now get two point one zero. And previously we also had two point one zero.", "tokens": [50364, 407, 1237, 412, 264, 24071, 4470, 11, 321, 586, 483, 732, 935, 472, 4018, 13, 400, 8046, 321, 611, 632, 732, 935, 472, 4018, 13, 50614, 50614, 400, 456, 311, 257, 707, 857, 295, 257, 2649, 11, 457, 300, 311, 445, 264, 4974, 1287, 295, 264, 1399, 11, 286, 9091, 13, 50814, 50814, 583, 264, 955, 2028, 11, 295, 1164, 11, 307, 321, 483, 281, 264, 912, 4008, 13, 50964, 50964, 583, 321, 630, 406, 362, 281, 5366, 604, 5585, 3547, 300, 321, 658, 490, 445, 1237, 412, 49816, 82, 293, 17939, 11, 8568, 13, 51364, 51364, 492, 362, 746, 300, 307, 12909, 3681, 15551, 293, 486, 4373, 505, 281, 709, 3801, 9590, 293, 746, 300, 321, 393, 1333, 295, 764, 382, 257, 5934, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0652951528859693, "compression_ratio": 1.73421926910299, "no_speech_prob": 1.3211186342232395e-05}, {"id": 464, "seek": 241200, "start": 2417.0, "end": 2421.0, "text": " And there's a little bit of a difference, but that's just the randomness of the process, I suspect.", "tokens": [50364, 407, 1237, 412, 264, 24071, 4470, 11, 321, 586, 483, 732, 935, 472, 4018, 13, 400, 8046, 321, 611, 632, 732, 935, 472, 4018, 13, 50614, 50614, 400, 456, 311, 257, 707, 857, 295, 257, 2649, 11, 457, 300, 311, 445, 264, 4974, 1287, 295, 264, 1399, 11, 286, 9091, 13, 50814, 50814, 583, 264, 955, 2028, 11, 295, 1164, 11, 307, 321, 483, 281, 264, 912, 4008, 13, 50964, 50964, 583, 321, 630, 406, 362, 281, 5366, 604, 5585, 3547, 300, 321, 658, 490, 445, 1237, 412, 49816, 82, 293, 17939, 11, 8568, 13, 51364, 51364, 492, 362, 746, 300, 307, 12909, 3681, 15551, 293, 486, 4373, 505, 281, 709, 3801, 9590, 293, 746, 300, 321, 393, 1333, 295, 764, 382, 257, 5934, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0652951528859693, "compression_ratio": 1.73421926910299, "no_speech_prob": 1.3211186342232395e-05}, {"id": 465, "seek": 241200, "start": 2421.0, "end": 2424.0, "text": " But the big deal, of course, is we get to the same spot.", "tokens": [50364, 407, 1237, 412, 264, 24071, 4470, 11, 321, 586, 483, 732, 935, 472, 4018, 13, 400, 8046, 321, 611, 632, 732, 935, 472, 4018, 13, 50614, 50614, 400, 456, 311, 257, 707, 857, 295, 257, 2649, 11, 457, 300, 311, 445, 264, 4974, 1287, 295, 264, 1399, 11, 286, 9091, 13, 50814, 50814, 583, 264, 955, 2028, 11, 295, 1164, 11, 307, 321, 483, 281, 264, 912, 4008, 13, 50964, 50964, 583, 321, 630, 406, 362, 281, 5366, 604, 5585, 3547, 300, 321, 658, 490, 445, 1237, 412, 49816, 82, 293, 17939, 11, 8568, 13, 51364, 51364, 492, 362, 746, 300, 307, 12909, 3681, 15551, 293, 486, 4373, 505, 281, 709, 3801, 9590, 293, 746, 300, 321, 393, 1333, 295, 764, 382, 257, 5934, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0652951528859693, "compression_ratio": 1.73421926910299, "no_speech_prob": 1.3211186342232395e-05}, {"id": 466, "seek": 241200, "start": 2424.0, "end": 2432.0, "text": " But we did not have to introduce any magic numbers that we got from just looking at histograms and guessing, checking.", "tokens": [50364, 407, 1237, 412, 264, 24071, 4470, 11, 321, 586, 483, 732, 935, 472, 4018, 13, 400, 8046, 321, 611, 632, 732, 935, 472, 4018, 13, 50614, 50614, 400, 456, 311, 257, 707, 857, 295, 257, 2649, 11, 457, 300, 311, 445, 264, 4974, 1287, 295, 264, 1399, 11, 286, 9091, 13, 50814, 50814, 583, 264, 955, 2028, 11, 295, 1164, 11, 307, 321, 483, 281, 264, 912, 4008, 13, 50964, 50964, 583, 321, 630, 406, 362, 281, 5366, 604, 5585, 3547, 300, 321, 658, 490, 445, 1237, 412, 49816, 82, 293, 17939, 11, 8568, 13, 51364, 51364, 492, 362, 746, 300, 307, 12909, 3681, 15551, 293, 486, 4373, 505, 281, 709, 3801, 9590, 293, 746, 300, 321, 393, 1333, 295, 764, 382, 257, 5934, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0652951528859693, "compression_ratio": 1.73421926910299, "no_speech_prob": 1.3211186342232395e-05}, {"id": 467, "seek": 241200, "start": 2432.0, "end": 2440.0, "text": " We have something that is semi principled and will scale us to much bigger networks and something that we can sort of use as a guide.", "tokens": [50364, 407, 1237, 412, 264, 24071, 4470, 11, 321, 586, 483, 732, 935, 472, 4018, 13, 400, 8046, 321, 611, 632, 732, 935, 472, 4018, 13, 50614, 50614, 400, 456, 311, 257, 707, 857, 295, 257, 2649, 11, 457, 300, 311, 445, 264, 4974, 1287, 295, 264, 1399, 11, 286, 9091, 13, 50814, 50814, 583, 264, 955, 2028, 11, 295, 1164, 11, 307, 321, 483, 281, 264, 912, 4008, 13, 50964, 50964, 583, 321, 630, 406, 362, 281, 5366, 604, 5585, 3547, 300, 321, 658, 490, 445, 1237, 412, 49816, 82, 293, 17939, 11, 8568, 13, 51364, 51364, 492, 362, 746, 300, 307, 12909, 3681, 15551, 293, 486, 4373, 505, 281, 709, 3801, 9590, 293, 746, 300, 321, 393, 1333, 295, 764, 382, 257, 5934, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0652951528859693, "compression_ratio": 1.73421926910299, "no_speech_prob": 1.3211186342232395e-05}, {"id": 468, "seek": 244000, "start": 2440.0, "end": 2446.0, "text": " So I mentioned that the precise setting of these initializations is not as important today due to some modern innovations.", "tokens": [50364, 407, 286, 2835, 300, 264, 13600, 3287, 295, 613, 5883, 14455, 307, 406, 382, 1021, 965, 3462, 281, 512, 4363, 24283, 13, 50664, 50664, 400, 286, 519, 586, 307, 257, 1238, 665, 565, 281, 5366, 472, 295, 729, 4363, 24283, 11, 293, 300, 307, 15245, 2710, 2144, 13, 50914, 50914, 407, 15245, 2710, 2144, 1361, 484, 294, 7546, 490, 257, 1469, 412, 3329, 13, 51164, 51164, 400, 309, 390, 364, 4664, 30842, 3035, 570, 309, 1027, 309, 1944, 281, 3847, 588, 2452, 18161, 36170, 1596, 49927, 293, 309, 1936, 445, 2732, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07224571208159129, "compression_ratio": 1.6830188679245284, "no_speech_prob": 7.295903742488008e-06}, {"id": 469, "seek": 244000, "start": 2446.0, "end": 2451.0, "text": " And I think now is a pretty good time to introduce one of those modern innovations, and that is batch normalization.", "tokens": [50364, 407, 286, 2835, 300, 264, 13600, 3287, 295, 613, 5883, 14455, 307, 406, 382, 1021, 965, 3462, 281, 512, 4363, 24283, 13, 50664, 50664, 400, 286, 519, 586, 307, 257, 1238, 665, 565, 281, 5366, 472, 295, 729, 4363, 24283, 11, 293, 300, 307, 15245, 2710, 2144, 13, 50914, 50914, 407, 15245, 2710, 2144, 1361, 484, 294, 7546, 490, 257, 1469, 412, 3329, 13, 51164, 51164, 400, 309, 390, 364, 4664, 30842, 3035, 570, 309, 1027, 309, 1944, 281, 3847, 588, 2452, 18161, 36170, 1596, 49927, 293, 309, 1936, 445, 2732, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07224571208159129, "compression_ratio": 1.6830188679245284, "no_speech_prob": 7.295903742488008e-06}, {"id": 470, "seek": 244000, "start": 2451.0, "end": 2456.0, "text": " So batch normalization came out in 2015 from a team at Google.", "tokens": [50364, 407, 286, 2835, 300, 264, 13600, 3287, 295, 613, 5883, 14455, 307, 406, 382, 1021, 965, 3462, 281, 512, 4363, 24283, 13, 50664, 50664, 400, 286, 519, 586, 307, 257, 1238, 665, 565, 281, 5366, 472, 295, 729, 4363, 24283, 11, 293, 300, 307, 15245, 2710, 2144, 13, 50914, 50914, 407, 15245, 2710, 2144, 1361, 484, 294, 7546, 490, 257, 1469, 412, 3329, 13, 51164, 51164, 400, 309, 390, 364, 4664, 30842, 3035, 570, 309, 1027, 309, 1944, 281, 3847, 588, 2452, 18161, 36170, 1596, 49927, 293, 309, 1936, 445, 2732, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07224571208159129, "compression_ratio": 1.6830188679245284, "no_speech_prob": 7.295903742488008e-06}, {"id": 471, "seek": 244000, "start": 2456.0, "end": 2465.0, "text": " And it was an extremely impactful paper because it made it possible to train very deep neural nets quite reliably and it basically just worked.", "tokens": [50364, 407, 286, 2835, 300, 264, 13600, 3287, 295, 613, 5883, 14455, 307, 406, 382, 1021, 965, 3462, 281, 512, 4363, 24283, 13, 50664, 50664, 400, 286, 519, 586, 307, 257, 1238, 665, 565, 281, 5366, 472, 295, 729, 4363, 24283, 11, 293, 300, 307, 15245, 2710, 2144, 13, 50914, 50914, 407, 15245, 2710, 2144, 1361, 484, 294, 7546, 490, 257, 1469, 412, 3329, 13, 51164, 51164, 400, 309, 390, 364, 4664, 30842, 3035, 570, 309, 1027, 309, 1944, 281, 3847, 588, 2452, 18161, 36170, 1596, 49927, 293, 309, 1936, 445, 2732, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07224571208159129, "compression_ratio": 1.6830188679245284, "no_speech_prob": 7.295903742488008e-06}, {"id": 472, "seek": 246500, "start": 2465.0, "end": 2470.0, "text": " So here's what batch normalization does and let's implement it.", "tokens": [50364, 407, 510, 311, 437, 15245, 2710, 2144, 775, 293, 718, 311, 4445, 309, 13, 50614, 50614, 8537, 11, 321, 362, 613, 7633, 4368, 389, 659, 578, 11, 558, 30, 50814, 50814, 400, 321, 645, 1417, 466, 577, 321, 500, 380, 528, 613, 613, 659, 24433, 4368, 281, 312, 636, 886, 1359, 570, 550, 264, 1266, 39, 307, 406, 884, 1340, 13, 51314, 51314, 583, 321, 500, 380, 528, 552, 281, 312, 886, 2416, 570, 550, 264, 1266, 39, 307, 25408, 13, 51464, 51464, 682, 1186, 11, 321, 528, 552, 281, 312, 9810, 11, 9810, 39148, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12693912506103516, "compression_ratio": 1.7180616740088106, "no_speech_prob": 4.0292320591106545e-06}, {"id": 473, "seek": 246500, "start": 2470.0, "end": 2474.0, "text": " Basically, we have these hidden states H preact, right?", "tokens": [50364, 407, 510, 311, 437, 15245, 2710, 2144, 775, 293, 718, 311, 4445, 309, 13, 50614, 50614, 8537, 11, 321, 362, 613, 7633, 4368, 389, 659, 578, 11, 558, 30, 50814, 50814, 400, 321, 645, 1417, 466, 577, 321, 500, 380, 528, 613, 613, 659, 24433, 4368, 281, 312, 636, 886, 1359, 570, 550, 264, 1266, 39, 307, 406, 884, 1340, 13, 51314, 51314, 583, 321, 500, 380, 528, 552, 281, 312, 886, 2416, 570, 550, 264, 1266, 39, 307, 25408, 13, 51464, 51464, 682, 1186, 11, 321, 528, 552, 281, 312, 9810, 11, 9810, 39148, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12693912506103516, "compression_ratio": 1.7180616740088106, "no_speech_prob": 4.0292320591106545e-06}, {"id": 474, "seek": 246500, "start": 2474.0, "end": 2484.0, "text": " And we were talking about how we don't want these these pre activation states to be way too small because then the 10H is not doing anything.", "tokens": [50364, 407, 510, 311, 437, 15245, 2710, 2144, 775, 293, 718, 311, 4445, 309, 13, 50614, 50614, 8537, 11, 321, 362, 613, 7633, 4368, 389, 659, 578, 11, 558, 30, 50814, 50814, 400, 321, 645, 1417, 466, 577, 321, 500, 380, 528, 613, 613, 659, 24433, 4368, 281, 312, 636, 886, 1359, 570, 550, 264, 1266, 39, 307, 406, 884, 1340, 13, 51314, 51314, 583, 321, 500, 380, 528, 552, 281, 312, 886, 2416, 570, 550, 264, 1266, 39, 307, 25408, 13, 51464, 51464, 682, 1186, 11, 321, 528, 552, 281, 312, 9810, 11, 9810, 39148, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12693912506103516, "compression_ratio": 1.7180616740088106, "no_speech_prob": 4.0292320591106545e-06}, {"id": 475, "seek": 246500, "start": 2484.0, "end": 2487.0, "text": " But we don't want them to be too large because then the 10H is saturated.", "tokens": [50364, 407, 510, 311, 437, 15245, 2710, 2144, 775, 293, 718, 311, 4445, 309, 13, 50614, 50614, 8537, 11, 321, 362, 613, 7633, 4368, 389, 659, 578, 11, 558, 30, 50814, 50814, 400, 321, 645, 1417, 466, 577, 321, 500, 380, 528, 613, 613, 659, 24433, 4368, 281, 312, 636, 886, 1359, 570, 550, 264, 1266, 39, 307, 406, 884, 1340, 13, 51314, 51314, 583, 321, 500, 380, 528, 552, 281, 312, 886, 2416, 570, 550, 264, 1266, 39, 307, 25408, 13, 51464, 51464, 682, 1186, 11, 321, 528, 552, 281, 312, 9810, 11, 9810, 39148, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12693912506103516, "compression_ratio": 1.7180616740088106, "no_speech_prob": 4.0292320591106545e-06}, {"id": 476, "seek": 246500, "start": 2487.0, "end": 2490.0, "text": " In fact, we want them to be roughly, roughly Gaussian.", "tokens": [50364, 407, 510, 311, 437, 15245, 2710, 2144, 775, 293, 718, 311, 4445, 309, 13, 50614, 50614, 8537, 11, 321, 362, 613, 7633, 4368, 389, 659, 578, 11, 558, 30, 50814, 50814, 400, 321, 645, 1417, 466, 577, 321, 500, 380, 528, 613, 613, 659, 24433, 4368, 281, 312, 636, 886, 1359, 570, 550, 264, 1266, 39, 307, 406, 884, 1340, 13, 51314, 51314, 583, 321, 500, 380, 528, 552, 281, 312, 886, 2416, 570, 550, 264, 1266, 39, 307, 25408, 13, 51464, 51464, 682, 1186, 11, 321, 528, 552, 281, 312, 9810, 11, 9810, 39148, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.12693912506103516, "compression_ratio": 1.7180616740088106, "no_speech_prob": 4.0292320591106545e-06}, {"id": 477, "seek": 249000, "start": 2490.0, "end": 2495.0, "text": " So zero mean and a unit or one standard deviation, at least at initialization.", "tokens": [50364, 407, 4018, 914, 293, 257, 4985, 420, 472, 3832, 25163, 11, 412, 1935, 412, 5883, 2144, 13, 50614, 50614, 407, 264, 11269, 490, 264, 15245, 2710, 2144, 3035, 307, 11, 2264, 11, 291, 362, 613, 7633, 4368, 293, 291, 1116, 411, 552, 281, 312, 9810, 39148, 13, 51014, 51014, 1396, 983, 406, 747, 264, 7633, 4368, 293, 445, 2710, 1125, 552, 281, 312, 39148, 30, 51264, 51264, 400, 309, 3263, 733, 295, 3219, 11, 457, 291, 393, 445, 360, 300, 570, 3832, 3319, 7633, 4368, 370, 300, 641, 4985, 39148, 307, 257, 6239, 819, 9364, 6916, 11, 382, 321, 603, 2321, 536, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07763803785092363, "compression_ratio": 1.7406015037593985, "no_speech_prob": 3.726515842572553e-06}, {"id": 478, "seek": 249000, "start": 2495.0, "end": 2503.0, "text": " So the insight from the batch normalization paper is, OK, you have these hidden states and you'd like them to be roughly Gaussian.", "tokens": [50364, 407, 4018, 914, 293, 257, 4985, 420, 472, 3832, 25163, 11, 412, 1935, 412, 5883, 2144, 13, 50614, 50614, 407, 264, 11269, 490, 264, 15245, 2710, 2144, 3035, 307, 11, 2264, 11, 291, 362, 613, 7633, 4368, 293, 291, 1116, 411, 552, 281, 312, 9810, 39148, 13, 51014, 51014, 1396, 983, 406, 747, 264, 7633, 4368, 293, 445, 2710, 1125, 552, 281, 312, 39148, 30, 51264, 51264, 400, 309, 3263, 733, 295, 3219, 11, 457, 291, 393, 445, 360, 300, 570, 3832, 3319, 7633, 4368, 370, 300, 641, 4985, 39148, 307, 257, 6239, 819, 9364, 6916, 11, 382, 321, 603, 2321, 536, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07763803785092363, "compression_ratio": 1.7406015037593985, "no_speech_prob": 3.726515842572553e-06}, {"id": 479, "seek": 249000, "start": 2503.0, "end": 2508.0, "text": " Then why not take the hidden states and just normalize them to be Gaussian?", "tokens": [50364, 407, 4018, 914, 293, 257, 4985, 420, 472, 3832, 25163, 11, 412, 1935, 412, 5883, 2144, 13, 50614, 50614, 407, 264, 11269, 490, 264, 15245, 2710, 2144, 3035, 307, 11, 2264, 11, 291, 362, 613, 7633, 4368, 293, 291, 1116, 411, 552, 281, 312, 9810, 39148, 13, 51014, 51014, 1396, 983, 406, 747, 264, 7633, 4368, 293, 445, 2710, 1125, 552, 281, 312, 39148, 30, 51264, 51264, 400, 309, 3263, 733, 295, 3219, 11, 457, 291, 393, 445, 360, 300, 570, 3832, 3319, 7633, 4368, 370, 300, 641, 4985, 39148, 307, 257, 6239, 819, 9364, 6916, 11, 382, 321, 603, 2321, 536, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07763803785092363, "compression_ratio": 1.7406015037593985, "no_speech_prob": 3.726515842572553e-06}, {"id": 480, "seek": 249000, "start": 2508.0, "end": 2519.0, "text": " And it sounds kind of crazy, but you can just do that because standardizing hidden states so that their unit Gaussian is a perfectly differentiable operation, as we'll soon see.", "tokens": [50364, 407, 4018, 914, 293, 257, 4985, 420, 472, 3832, 25163, 11, 412, 1935, 412, 5883, 2144, 13, 50614, 50614, 407, 264, 11269, 490, 264, 15245, 2710, 2144, 3035, 307, 11, 2264, 11, 291, 362, 613, 7633, 4368, 293, 291, 1116, 411, 552, 281, 312, 9810, 39148, 13, 51014, 51014, 1396, 983, 406, 747, 264, 7633, 4368, 293, 445, 2710, 1125, 552, 281, 312, 39148, 30, 51264, 51264, 400, 309, 3263, 733, 295, 3219, 11, 457, 291, 393, 445, 360, 300, 570, 3832, 3319, 7633, 4368, 370, 300, 641, 4985, 39148, 307, 257, 6239, 819, 9364, 6916, 11, 382, 321, 603, 2321, 536, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07763803785092363, "compression_ratio": 1.7406015037593985, "no_speech_prob": 3.726515842572553e-06}, {"id": 481, "seek": 251900, "start": 2519.0, "end": 2522.0, "text": " And so that was kind of like the big insight in this paper.", "tokens": [50364, 400, 370, 300, 390, 733, 295, 411, 264, 955, 11269, 294, 341, 3035, 13, 50514, 50514, 400, 562, 286, 700, 1401, 309, 11, 452, 1575, 390, 16479, 570, 291, 393, 445, 2710, 1125, 613, 7633, 4368, 13, 50714, 50714, 400, 498, 291, 1116, 411, 4985, 39148, 4368, 294, 428, 3209, 11, 412, 1935, 5883, 2144, 11, 291, 393, 445, 2710, 1125, 552, 281, 312, 39148, 13, 51114, 51114, 407, 718, 311, 536, 577, 300, 1985, 13, 51214, 51214, 407, 321, 434, 516, 281, 11369, 281, 527, 659, 2430, 763, 510, 445, 949, 436, 3242, 666, 264, 1266, 39, 13, 51464, 51464, 823, 11, 264, 1558, 11, 797, 11, 307, 11, 1604, 11, 321, 434, 1382, 281, 652, 613, 9810, 39148, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.058089279174804685, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.6280240515698097e-06}, {"id": 482, "seek": 251900, "start": 2522.0, "end": 2526.0, "text": " And when I first read it, my mind was blown because you can just normalize these hidden states.", "tokens": [50364, 400, 370, 300, 390, 733, 295, 411, 264, 955, 11269, 294, 341, 3035, 13, 50514, 50514, 400, 562, 286, 700, 1401, 309, 11, 452, 1575, 390, 16479, 570, 291, 393, 445, 2710, 1125, 613, 7633, 4368, 13, 50714, 50714, 400, 498, 291, 1116, 411, 4985, 39148, 4368, 294, 428, 3209, 11, 412, 1935, 5883, 2144, 11, 291, 393, 445, 2710, 1125, 552, 281, 312, 39148, 13, 51114, 51114, 407, 718, 311, 536, 577, 300, 1985, 13, 51214, 51214, 407, 321, 434, 516, 281, 11369, 281, 527, 659, 2430, 763, 510, 445, 949, 436, 3242, 666, 264, 1266, 39, 13, 51464, 51464, 823, 11, 264, 1558, 11, 797, 11, 307, 11, 1604, 11, 321, 434, 1382, 281, 652, 613, 9810, 39148, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.058089279174804685, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.6280240515698097e-06}, {"id": 483, "seek": 251900, "start": 2526.0, "end": 2534.0, "text": " And if you'd like unit Gaussian states in your network, at least initialization, you can just normalize them to be Gaussian.", "tokens": [50364, 400, 370, 300, 390, 733, 295, 411, 264, 955, 11269, 294, 341, 3035, 13, 50514, 50514, 400, 562, 286, 700, 1401, 309, 11, 452, 1575, 390, 16479, 570, 291, 393, 445, 2710, 1125, 613, 7633, 4368, 13, 50714, 50714, 400, 498, 291, 1116, 411, 4985, 39148, 4368, 294, 428, 3209, 11, 412, 1935, 5883, 2144, 11, 291, 393, 445, 2710, 1125, 552, 281, 312, 39148, 13, 51114, 51114, 407, 718, 311, 536, 577, 300, 1985, 13, 51214, 51214, 407, 321, 434, 516, 281, 11369, 281, 527, 659, 2430, 763, 510, 445, 949, 436, 3242, 666, 264, 1266, 39, 13, 51464, 51464, 823, 11, 264, 1558, 11, 797, 11, 307, 11, 1604, 11, 321, 434, 1382, 281, 652, 613, 9810, 39148, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.058089279174804685, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.6280240515698097e-06}, {"id": 484, "seek": 251900, "start": 2534.0, "end": 2536.0, "text": " So let's see how that works.", "tokens": [50364, 400, 370, 300, 390, 733, 295, 411, 264, 955, 11269, 294, 341, 3035, 13, 50514, 50514, 400, 562, 286, 700, 1401, 309, 11, 452, 1575, 390, 16479, 570, 291, 393, 445, 2710, 1125, 613, 7633, 4368, 13, 50714, 50714, 400, 498, 291, 1116, 411, 4985, 39148, 4368, 294, 428, 3209, 11, 412, 1935, 5883, 2144, 11, 291, 393, 445, 2710, 1125, 552, 281, 312, 39148, 13, 51114, 51114, 407, 718, 311, 536, 577, 300, 1985, 13, 51214, 51214, 407, 321, 434, 516, 281, 11369, 281, 527, 659, 2430, 763, 510, 445, 949, 436, 3242, 666, 264, 1266, 39, 13, 51464, 51464, 823, 11, 264, 1558, 11, 797, 11, 307, 11, 1604, 11, 321, 434, 1382, 281, 652, 613, 9810, 39148, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.058089279174804685, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.6280240515698097e-06}, {"id": 485, "seek": 251900, "start": 2536.0, "end": 2541.0, "text": " So we're going to scroll to our pre activations here just before they enter into the 10H.", "tokens": [50364, 400, 370, 300, 390, 733, 295, 411, 264, 955, 11269, 294, 341, 3035, 13, 50514, 50514, 400, 562, 286, 700, 1401, 309, 11, 452, 1575, 390, 16479, 570, 291, 393, 445, 2710, 1125, 613, 7633, 4368, 13, 50714, 50714, 400, 498, 291, 1116, 411, 4985, 39148, 4368, 294, 428, 3209, 11, 412, 1935, 5883, 2144, 11, 291, 393, 445, 2710, 1125, 552, 281, 312, 39148, 13, 51114, 51114, 407, 718, 311, 536, 577, 300, 1985, 13, 51214, 51214, 407, 321, 434, 516, 281, 11369, 281, 527, 659, 2430, 763, 510, 445, 949, 436, 3242, 666, 264, 1266, 39, 13, 51464, 51464, 823, 11, 264, 1558, 11, 797, 11, 307, 11, 1604, 11, 321, 434, 1382, 281, 652, 613, 9810, 39148, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.058089279174804685, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.6280240515698097e-06}, {"id": 486, "seek": 251900, "start": 2541.0, "end": 2545.0, "text": " Now, the idea, again, is, remember, we're trying to make these roughly Gaussian.", "tokens": [50364, 400, 370, 300, 390, 733, 295, 411, 264, 955, 11269, 294, 341, 3035, 13, 50514, 50514, 400, 562, 286, 700, 1401, 309, 11, 452, 1575, 390, 16479, 570, 291, 393, 445, 2710, 1125, 613, 7633, 4368, 13, 50714, 50714, 400, 498, 291, 1116, 411, 4985, 39148, 4368, 294, 428, 3209, 11, 412, 1935, 5883, 2144, 11, 291, 393, 445, 2710, 1125, 552, 281, 312, 39148, 13, 51114, 51114, 407, 718, 311, 536, 577, 300, 1985, 13, 51214, 51214, 407, 321, 434, 516, 281, 11369, 281, 527, 659, 2430, 763, 510, 445, 949, 436, 3242, 666, 264, 1266, 39, 13, 51464, 51464, 823, 11, 264, 1558, 11, 797, 11, 307, 11, 1604, 11, 321, 434, 1382, 281, 652, 613, 9810, 39148, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.058089279174804685, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.6280240515698097e-06}, {"id": 487, "seek": 254500, "start": 2545.0, "end": 2550.0, "text": " And that's because if these are way too small numbers, then the 10H here is kind of inactive.", "tokens": [50364, 400, 300, 311, 570, 498, 613, 366, 636, 886, 1359, 3547, 11, 550, 264, 1266, 39, 510, 307, 733, 295, 294, 12596, 13, 50614, 50614, 583, 498, 613, 366, 588, 2416, 3547, 11, 550, 264, 1266, 39, 307, 636, 886, 25408, 293, 869, 294, 264, 3095, 13, 50914, 50914, 407, 321, 1116, 411, 341, 281, 312, 9810, 39148, 13, 51064, 51064, 407, 264, 11269, 294, 15245, 2710, 2144, 11, 797, 11, 307, 300, 321, 393, 445, 3832, 1125, 613, 2430, 763, 370, 436, 366, 2293, 39148, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08011910155579284, "compression_ratio": 1.658878504672897, "no_speech_prob": 1.844794837779773e-06}, {"id": 488, "seek": 254500, "start": 2550.0, "end": 2556.0, "text": " But if these are very large numbers, then the 10H is way too saturated and great in the flow.", "tokens": [50364, 400, 300, 311, 570, 498, 613, 366, 636, 886, 1359, 3547, 11, 550, 264, 1266, 39, 510, 307, 733, 295, 294, 12596, 13, 50614, 50614, 583, 498, 613, 366, 588, 2416, 3547, 11, 550, 264, 1266, 39, 307, 636, 886, 25408, 293, 869, 294, 264, 3095, 13, 50914, 50914, 407, 321, 1116, 411, 341, 281, 312, 9810, 39148, 13, 51064, 51064, 407, 264, 11269, 294, 15245, 2710, 2144, 11, 797, 11, 307, 300, 321, 393, 445, 3832, 1125, 613, 2430, 763, 370, 436, 366, 2293, 39148, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08011910155579284, "compression_ratio": 1.658878504672897, "no_speech_prob": 1.844794837779773e-06}, {"id": 489, "seek": 254500, "start": 2556.0, "end": 2559.0, "text": " So we'd like this to be roughly Gaussian.", "tokens": [50364, 400, 300, 311, 570, 498, 613, 366, 636, 886, 1359, 3547, 11, 550, 264, 1266, 39, 510, 307, 733, 295, 294, 12596, 13, 50614, 50614, 583, 498, 613, 366, 588, 2416, 3547, 11, 550, 264, 1266, 39, 307, 636, 886, 25408, 293, 869, 294, 264, 3095, 13, 50914, 50914, 407, 321, 1116, 411, 341, 281, 312, 9810, 39148, 13, 51064, 51064, 407, 264, 11269, 294, 15245, 2710, 2144, 11, 797, 11, 307, 300, 321, 393, 445, 3832, 1125, 613, 2430, 763, 370, 436, 366, 2293, 39148, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08011910155579284, "compression_ratio": 1.658878504672897, "no_speech_prob": 1.844794837779773e-06}, {"id": 490, "seek": 254500, "start": 2559.0, "end": 2567.0, "text": " So the insight in batch normalization, again, is that we can just standardize these activations so they are exactly Gaussian.", "tokens": [50364, 400, 300, 311, 570, 498, 613, 366, 636, 886, 1359, 3547, 11, 550, 264, 1266, 39, 510, 307, 733, 295, 294, 12596, 13, 50614, 50614, 583, 498, 613, 366, 588, 2416, 3547, 11, 550, 264, 1266, 39, 307, 636, 886, 25408, 293, 869, 294, 264, 3095, 13, 50914, 50914, 407, 321, 1116, 411, 341, 281, 312, 9810, 39148, 13, 51064, 51064, 407, 264, 11269, 294, 15245, 2710, 2144, 11, 797, 11, 307, 300, 321, 393, 445, 3832, 1125, 613, 2430, 763, 370, 436, 366, 2293, 39148, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.08011910155579284, "compression_ratio": 1.658878504672897, "no_speech_prob": 1.844794837779773e-06}, {"id": 491, "seek": 256700, "start": 2567.0, "end": 2576.0, "text": " So here, Hpreact has a shape of 32 by 200, 32 examples by 200 neurons in the hidden layer.", "tokens": [50364, 407, 510, 11, 389, 3712, 578, 575, 257, 3909, 295, 8858, 538, 2331, 11, 8858, 5110, 538, 2331, 22027, 294, 264, 7633, 4583, 13, 50814, 50814, 407, 1936, 437, 321, 393, 360, 307, 321, 393, 747, 389, 3712, 578, 293, 321, 393, 445, 8873, 264, 914, 13, 51064, 51064, 400, 264, 914, 321, 528, 281, 8873, 2108, 264, 4018, 10139, 13, 51264, 51264, 400, 321, 528, 281, 611, 1066, 552, 382, 2074, 370, 300, 321, 393, 3612, 9975, 341, 13, 51564, 51564, 407, 264, 3909, 295, 341, 307, 502, 538, 2331, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08075372874736786, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.611875399656128e-06}, {"id": 492, "seek": 256700, "start": 2576.0, "end": 2581.0, "text": " So basically what we can do is we can take Hpreact and we can just calculate the mean.", "tokens": [50364, 407, 510, 11, 389, 3712, 578, 575, 257, 3909, 295, 8858, 538, 2331, 11, 8858, 5110, 538, 2331, 22027, 294, 264, 7633, 4583, 13, 50814, 50814, 407, 1936, 437, 321, 393, 360, 307, 321, 393, 747, 389, 3712, 578, 293, 321, 393, 445, 8873, 264, 914, 13, 51064, 51064, 400, 264, 914, 321, 528, 281, 8873, 2108, 264, 4018, 10139, 13, 51264, 51264, 400, 321, 528, 281, 611, 1066, 552, 382, 2074, 370, 300, 321, 393, 3612, 9975, 341, 13, 51564, 51564, 407, 264, 3909, 295, 341, 307, 502, 538, 2331, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08075372874736786, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.611875399656128e-06}, {"id": 493, "seek": 256700, "start": 2581.0, "end": 2585.0, "text": " And the mean we want to calculate across the zero dimension.", "tokens": [50364, 407, 510, 11, 389, 3712, 578, 575, 257, 3909, 295, 8858, 538, 2331, 11, 8858, 5110, 538, 2331, 22027, 294, 264, 7633, 4583, 13, 50814, 50814, 407, 1936, 437, 321, 393, 360, 307, 321, 393, 747, 389, 3712, 578, 293, 321, 393, 445, 8873, 264, 914, 13, 51064, 51064, 400, 264, 914, 321, 528, 281, 8873, 2108, 264, 4018, 10139, 13, 51264, 51264, 400, 321, 528, 281, 611, 1066, 552, 382, 2074, 370, 300, 321, 393, 3612, 9975, 341, 13, 51564, 51564, 407, 264, 3909, 295, 341, 307, 502, 538, 2331, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08075372874736786, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.611875399656128e-06}, {"id": 494, "seek": 256700, "start": 2585.0, "end": 2591.0, "text": " And we want to also keep them as true so that we can easily broadcast this.", "tokens": [50364, 407, 510, 11, 389, 3712, 578, 575, 257, 3909, 295, 8858, 538, 2331, 11, 8858, 5110, 538, 2331, 22027, 294, 264, 7633, 4583, 13, 50814, 50814, 407, 1936, 437, 321, 393, 360, 307, 321, 393, 747, 389, 3712, 578, 293, 321, 393, 445, 8873, 264, 914, 13, 51064, 51064, 400, 264, 914, 321, 528, 281, 8873, 2108, 264, 4018, 10139, 13, 51264, 51264, 400, 321, 528, 281, 611, 1066, 552, 382, 2074, 370, 300, 321, 393, 3612, 9975, 341, 13, 51564, 51564, 407, 264, 3909, 295, 341, 307, 502, 538, 2331, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08075372874736786, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.611875399656128e-06}, {"id": 495, "seek": 256700, "start": 2591.0, "end": 2595.0, "text": " So the shape of this is 1 by 200.", "tokens": [50364, 407, 510, 11, 389, 3712, 578, 575, 257, 3909, 295, 8858, 538, 2331, 11, 8858, 5110, 538, 2331, 22027, 294, 264, 7633, 4583, 13, 50814, 50814, 407, 1936, 437, 321, 393, 360, 307, 321, 393, 747, 389, 3712, 578, 293, 321, 393, 445, 8873, 264, 914, 13, 51064, 51064, 400, 264, 914, 321, 528, 281, 8873, 2108, 264, 4018, 10139, 13, 51264, 51264, 400, 321, 528, 281, 611, 1066, 552, 382, 2074, 370, 300, 321, 393, 3612, 9975, 341, 13, 51564, 51564, 407, 264, 3909, 295, 341, 307, 502, 538, 2331, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08075372874736786, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.611875399656128e-06}, {"id": 496, "seek": 259500, "start": 2595.0, "end": 2601.0, "text": " In other words, we are doing the mean over all the elements in the batch.", "tokens": [50364, 682, 661, 2283, 11, 321, 366, 884, 264, 914, 670, 439, 264, 4959, 294, 264, 15245, 13, 50664, 50664, 400, 14138, 11, 321, 393, 8873, 264, 3832, 25163, 295, 613, 2430, 763, 13, 50964, 50964, 400, 300, 486, 611, 312, 502, 538, 2331, 13, 51064, 51064, 823, 11, 294, 341, 3035, 11, 436, 362, 264, 1333, 295, 22456, 510, 13, 51364, 51364, 400, 536, 510, 11, 321, 366, 28258, 264, 914, 11, 597, 307, 445, 1940, 264, 4274, 2158, 295, 604, 22027, 24433, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05626184831966053, "compression_ratio": 1.6682464454976302, "no_speech_prob": 8.01325495558558e-06}, {"id": 497, "seek": 259500, "start": 2601.0, "end": 2607.0, "text": " And similarly, we can calculate the standard deviation of these activations.", "tokens": [50364, 682, 661, 2283, 11, 321, 366, 884, 264, 914, 670, 439, 264, 4959, 294, 264, 15245, 13, 50664, 50664, 400, 14138, 11, 321, 393, 8873, 264, 3832, 25163, 295, 613, 2430, 763, 13, 50964, 50964, 400, 300, 486, 611, 312, 502, 538, 2331, 13, 51064, 51064, 823, 11, 294, 341, 3035, 11, 436, 362, 264, 1333, 295, 22456, 510, 13, 51364, 51364, 400, 536, 510, 11, 321, 366, 28258, 264, 914, 11, 597, 307, 445, 1940, 264, 4274, 2158, 295, 604, 22027, 24433, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05626184831966053, "compression_ratio": 1.6682464454976302, "no_speech_prob": 8.01325495558558e-06}, {"id": 498, "seek": 259500, "start": 2607.0, "end": 2609.0, "text": " And that will also be 1 by 200.", "tokens": [50364, 682, 661, 2283, 11, 321, 366, 884, 264, 914, 670, 439, 264, 4959, 294, 264, 15245, 13, 50664, 50664, 400, 14138, 11, 321, 393, 8873, 264, 3832, 25163, 295, 613, 2430, 763, 13, 50964, 50964, 400, 300, 486, 611, 312, 502, 538, 2331, 13, 51064, 51064, 823, 11, 294, 341, 3035, 11, 436, 362, 264, 1333, 295, 22456, 510, 13, 51364, 51364, 400, 536, 510, 11, 321, 366, 28258, 264, 914, 11, 597, 307, 445, 1940, 264, 4274, 2158, 295, 604, 22027, 24433, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05626184831966053, "compression_ratio": 1.6682464454976302, "no_speech_prob": 8.01325495558558e-06}, {"id": 499, "seek": 259500, "start": 2609.0, "end": 2615.0, "text": " Now, in this paper, they have the sort of prescription here.", "tokens": [50364, 682, 661, 2283, 11, 321, 366, 884, 264, 914, 670, 439, 264, 4959, 294, 264, 15245, 13, 50664, 50664, 400, 14138, 11, 321, 393, 8873, 264, 3832, 25163, 295, 613, 2430, 763, 13, 50964, 50964, 400, 300, 486, 611, 312, 502, 538, 2331, 13, 51064, 51064, 823, 11, 294, 341, 3035, 11, 436, 362, 264, 1333, 295, 22456, 510, 13, 51364, 51364, 400, 536, 510, 11, 321, 366, 28258, 264, 914, 11, 597, 307, 445, 1940, 264, 4274, 2158, 295, 604, 22027, 24433, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05626184831966053, "compression_ratio": 1.6682464454976302, "no_speech_prob": 8.01325495558558e-06}, {"id": 500, "seek": 259500, "start": 2615.0, "end": 2624.0, "text": " And see here, we are calculating the mean, which is just taking the average value of any neurons activation.", "tokens": [50364, 682, 661, 2283, 11, 321, 366, 884, 264, 914, 670, 439, 264, 4959, 294, 264, 15245, 13, 50664, 50664, 400, 14138, 11, 321, 393, 8873, 264, 3832, 25163, 295, 613, 2430, 763, 13, 50964, 50964, 400, 300, 486, 611, 312, 502, 538, 2331, 13, 51064, 51064, 823, 11, 294, 341, 3035, 11, 436, 362, 264, 1333, 295, 22456, 510, 13, 51364, 51364, 400, 536, 510, 11, 321, 366, 28258, 264, 914, 11, 597, 307, 445, 1940, 264, 4274, 2158, 295, 604, 22027, 24433, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05626184831966053, "compression_ratio": 1.6682464454976302, "no_speech_prob": 8.01325495558558e-06}, {"id": 501, "seek": 262400, "start": 2624.0, "end": 2630.0, "text": " And then the standard deviation is basically kind of like the measure of the spread that we've been using,", "tokens": [50364, 400, 550, 264, 3832, 25163, 307, 1936, 733, 295, 411, 264, 3481, 295, 264, 3974, 300, 321, 600, 668, 1228, 11, 50664, 50664, 597, 307, 264, 4560, 295, 633, 472, 295, 613, 4190, 1314, 490, 264, 914, 293, 300, 8889, 293, 18247, 2980, 13, 51114, 51114, 663, 311, 264, 21977, 13, 51264, 51264, 400, 550, 498, 291, 528, 281, 747, 264, 3832, 25163, 11, 291, 576, 3732, 5593, 264, 21977, 281, 483, 264, 3832, 25163, 13, 51564, 51564, 407, 613, 366, 264, 732, 300, 321, 434, 28258, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.050277419712232506, "compression_ratio": 1.84037558685446, "no_speech_prob": 5.77171340410132e-06}, {"id": 502, "seek": 262400, "start": 2630.0, "end": 2639.0, "text": " which is the distance of every one of these values away from the mean and that squared and averaged.", "tokens": [50364, 400, 550, 264, 3832, 25163, 307, 1936, 733, 295, 411, 264, 3481, 295, 264, 3974, 300, 321, 600, 668, 1228, 11, 50664, 50664, 597, 307, 264, 4560, 295, 633, 472, 295, 613, 4190, 1314, 490, 264, 914, 293, 300, 8889, 293, 18247, 2980, 13, 51114, 51114, 663, 311, 264, 21977, 13, 51264, 51264, 400, 550, 498, 291, 528, 281, 747, 264, 3832, 25163, 11, 291, 576, 3732, 5593, 264, 21977, 281, 483, 264, 3832, 25163, 13, 51564, 51564, 407, 613, 366, 264, 732, 300, 321, 434, 28258, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.050277419712232506, "compression_ratio": 1.84037558685446, "no_speech_prob": 5.77171340410132e-06}, {"id": 503, "seek": 262400, "start": 2639.0, "end": 2642.0, "text": " That's the variance.", "tokens": [50364, 400, 550, 264, 3832, 25163, 307, 1936, 733, 295, 411, 264, 3481, 295, 264, 3974, 300, 321, 600, 668, 1228, 11, 50664, 50664, 597, 307, 264, 4560, 295, 633, 472, 295, 613, 4190, 1314, 490, 264, 914, 293, 300, 8889, 293, 18247, 2980, 13, 51114, 51114, 663, 311, 264, 21977, 13, 51264, 51264, 400, 550, 498, 291, 528, 281, 747, 264, 3832, 25163, 11, 291, 576, 3732, 5593, 264, 21977, 281, 483, 264, 3832, 25163, 13, 51564, 51564, 407, 613, 366, 264, 732, 300, 321, 434, 28258, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.050277419712232506, "compression_ratio": 1.84037558685446, "no_speech_prob": 5.77171340410132e-06}, {"id": 504, "seek": 262400, "start": 2642.0, "end": 2648.0, "text": " And then if you want to take the standard deviation, you would square root the variance to get the standard deviation.", "tokens": [50364, 400, 550, 264, 3832, 25163, 307, 1936, 733, 295, 411, 264, 3481, 295, 264, 3974, 300, 321, 600, 668, 1228, 11, 50664, 50664, 597, 307, 264, 4560, 295, 633, 472, 295, 613, 4190, 1314, 490, 264, 914, 293, 300, 8889, 293, 18247, 2980, 13, 51114, 51114, 663, 311, 264, 21977, 13, 51264, 51264, 400, 550, 498, 291, 528, 281, 747, 264, 3832, 25163, 11, 291, 576, 3732, 5593, 264, 21977, 281, 483, 264, 3832, 25163, 13, 51564, 51564, 407, 613, 366, 264, 732, 300, 321, 434, 28258, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.050277419712232506, "compression_ratio": 1.84037558685446, "no_speech_prob": 5.77171340410132e-06}, {"id": 505, "seek": 262400, "start": 2648.0, "end": 2650.0, "text": " So these are the two that we're calculating.", "tokens": [50364, 400, 550, 264, 3832, 25163, 307, 1936, 733, 295, 411, 264, 3481, 295, 264, 3974, 300, 321, 600, 668, 1228, 11, 50664, 50664, 597, 307, 264, 4560, 295, 633, 472, 295, 613, 4190, 1314, 490, 264, 914, 293, 300, 8889, 293, 18247, 2980, 13, 51114, 51114, 663, 311, 264, 21977, 13, 51264, 51264, 400, 550, 498, 291, 528, 281, 747, 264, 3832, 25163, 11, 291, 576, 3732, 5593, 264, 21977, 281, 483, 264, 3832, 25163, 13, 51564, 51564, 407, 613, 366, 264, 732, 300, 321, 434, 28258, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.050277419712232506, "compression_ratio": 1.84037558685446, "no_speech_prob": 5.77171340410132e-06}, {"id": 506, "seek": 265000, "start": 2650.0, "end": 2658.0, "text": " And now we're going to normalize or standardize these x's by subtracting the mean and dividing by the standard deviation.", "tokens": [50364, 400, 586, 321, 434, 516, 281, 2710, 1125, 420, 3832, 1125, 613, 2031, 311, 538, 16390, 278, 264, 914, 293, 26764, 538, 264, 3832, 25163, 13, 50764, 50764, 407, 1936, 11, 321, 434, 1940, 30867, 605, 293, 321, 16390, 264, 914, 13, 51364, 51364, 400, 550, 321, 9845, 538, 264, 3832, 25163, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10457406127661989, "compression_ratio": 1.6571428571428573, "no_speech_prob": 1.4510206710838247e-05}, {"id": 507, "seek": 265000, "start": 2658.0, "end": 2670.0, "text": " So basically, we're taking entropy act and we subtract the mean.", "tokens": [50364, 400, 586, 321, 434, 516, 281, 2710, 1125, 420, 3832, 1125, 613, 2031, 311, 538, 16390, 278, 264, 914, 293, 26764, 538, 264, 3832, 25163, 13, 50764, 50764, 407, 1936, 11, 321, 434, 1940, 30867, 605, 293, 321, 16390, 264, 914, 13, 51364, 51364, 400, 550, 321, 9845, 538, 264, 3832, 25163, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10457406127661989, "compression_ratio": 1.6571428571428573, "no_speech_prob": 1.4510206710838247e-05}, {"id": 508, "seek": 265000, "start": 2670.0, "end": 2675.0, "text": " And then we divide by the standard deviation.", "tokens": [50364, 400, 586, 321, 434, 516, 281, 2710, 1125, 420, 3832, 1125, 613, 2031, 311, 538, 16390, 278, 264, 914, 293, 26764, 538, 264, 3832, 25163, 13, 50764, 50764, 407, 1936, 11, 321, 434, 1940, 30867, 605, 293, 321, 16390, 264, 914, 13, 51364, 51364, 400, 550, 321, 9845, 538, 264, 3832, 25163, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10457406127661989, "compression_ratio": 1.6571428571428573, "no_speech_prob": 1.4510206710838247e-05}, {"id": 509, "seek": 267500, "start": 2675.0, "end": 2681.0, "text": " And this is exactly what these two STD and mean are calculating.", "tokens": [50364, 400, 341, 307, 2293, 437, 613, 732, 4904, 35, 293, 914, 366, 28258, 13, 50664, 50664, 4919, 11, 341, 307, 264, 914, 293, 341, 307, 264, 21977, 13, 50814, 50814, 509, 536, 577, 264, 12771, 307, 264, 3832, 25163, 2673, 13, 50914, 50914, 407, 341, 307, 12771, 3732, 11, 597, 307, 21977, 307, 264, 3732, 295, 264, 3832, 25163, 13, 51164, 51164, 407, 341, 307, 577, 291, 3832, 1125, 613, 4190, 13, 51314, 51314, 400, 437, 341, 486, 360, 307, 300, 633, 2167, 34090, 586, 293, 1080, 16045, 3314, 486, 312, 2293, 4985, 39148, 322, 613, 8858, 5110, 11, 412, 1935, 11, 295, 341, 15245, 13, 51714, 51714, 663, 311, 983, 309, 311, 1219, 15245, 2710, 2144, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09934076715688236, "compression_ratio": 1.8198529411764706, "no_speech_prob": 3.1692394259152934e-05}, {"id": 510, "seek": 267500, "start": 2681.0, "end": 2684.0, "text": " Sorry, this is the mean and this is the variance.", "tokens": [50364, 400, 341, 307, 2293, 437, 613, 732, 4904, 35, 293, 914, 366, 28258, 13, 50664, 50664, 4919, 11, 341, 307, 264, 914, 293, 341, 307, 264, 21977, 13, 50814, 50814, 509, 536, 577, 264, 12771, 307, 264, 3832, 25163, 2673, 13, 50914, 50914, 407, 341, 307, 12771, 3732, 11, 597, 307, 21977, 307, 264, 3732, 295, 264, 3832, 25163, 13, 51164, 51164, 407, 341, 307, 577, 291, 3832, 1125, 613, 4190, 13, 51314, 51314, 400, 437, 341, 486, 360, 307, 300, 633, 2167, 34090, 586, 293, 1080, 16045, 3314, 486, 312, 2293, 4985, 39148, 322, 613, 8858, 5110, 11, 412, 1935, 11, 295, 341, 15245, 13, 51714, 51714, 663, 311, 983, 309, 311, 1219, 15245, 2710, 2144, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09934076715688236, "compression_ratio": 1.8198529411764706, "no_speech_prob": 3.1692394259152934e-05}, {"id": 511, "seek": 267500, "start": 2684.0, "end": 2686.0, "text": " You see how the sigma is the standard deviation usually.", "tokens": [50364, 400, 341, 307, 2293, 437, 613, 732, 4904, 35, 293, 914, 366, 28258, 13, 50664, 50664, 4919, 11, 341, 307, 264, 914, 293, 341, 307, 264, 21977, 13, 50814, 50814, 509, 536, 577, 264, 12771, 307, 264, 3832, 25163, 2673, 13, 50914, 50914, 407, 341, 307, 12771, 3732, 11, 597, 307, 21977, 307, 264, 3732, 295, 264, 3832, 25163, 13, 51164, 51164, 407, 341, 307, 577, 291, 3832, 1125, 613, 4190, 13, 51314, 51314, 400, 437, 341, 486, 360, 307, 300, 633, 2167, 34090, 586, 293, 1080, 16045, 3314, 486, 312, 2293, 4985, 39148, 322, 613, 8858, 5110, 11, 412, 1935, 11, 295, 341, 15245, 13, 51714, 51714, 663, 311, 983, 309, 311, 1219, 15245, 2710, 2144, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09934076715688236, "compression_ratio": 1.8198529411764706, "no_speech_prob": 3.1692394259152934e-05}, {"id": 512, "seek": 267500, "start": 2686.0, "end": 2691.0, "text": " So this is sigma square, which is variance is the square of the standard deviation.", "tokens": [50364, 400, 341, 307, 2293, 437, 613, 732, 4904, 35, 293, 914, 366, 28258, 13, 50664, 50664, 4919, 11, 341, 307, 264, 914, 293, 341, 307, 264, 21977, 13, 50814, 50814, 509, 536, 577, 264, 12771, 307, 264, 3832, 25163, 2673, 13, 50914, 50914, 407, 341, 307, 12771, 3732, 11, 597, 307, 21977, 307, 264, 3732, 295, 264, 3832, 25163, 13, 51164, 51164, 407, 341, 307, 577, 291, 3832, 1125, 613, 4190, 13, 51314, 51314, 400, 437, 341, 486, 360, 307, 300, 633, 2167, 34090, 586, 293, 1080, 16045, 3314, 486, 312, 2293, 4985, 39148, 322, 613, 8858, 5110, 11, 412, 1935, 11, 295, 341, 15245, 13, 51714, 51714, 663, 311, 983, 309, 311, 1219, 15245, 2710, 2144, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09934076715688236, "compression_ratio": 1.8198529411764706, "no_speech_prob": 3.1692394259152934e-05}, {"id": 513, "seek": 267500, "start": 2691.0, "end": 2694.0, "text": " So this is how you standardize these values.", "tokens": [50364, 400, 341, 307, 2293, 437, 613, 732, 4904, 35, 293, 914, 366, 28258, 13, 50664, 50664, 4919, 11, 341, 307, 264, 914, 293, 341, 307, 264, 21977, 13, 50814, 50814, 509, 536, 577, 264, 12771, 307, 264, 3832, 25163, 2673, 13, 50914, 50914, 407, 341, 307, 12771, 3732, 11, 597, 307, 21977, 307, 264, 3732, 295, 264, 3832, 25163, 13, 51164, 51164, 407, 341, 307, 577, 291, 3832, 1125, 613, 4190, 13, 51314, 51314, 400, 437, 341, 486, 360, 307, 300, 633, 2167, 34090, 586, 293, 1080, 16045, 3314, 486, 312, 2293, 4985, 39148, 322, 613, 8858, 5110, 11, 412, 1935, 11, 295, 341, 15245, 13, 51714, 51714, 663, 311, 983, 309, 311, 1219, 15245, 2710, 2144, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09934076715688236, "compression_ratio": 1.8198529411764706, "no_speech_prob": 3.1692394259152934e-05}, {"id": 514, "seek": 267500, "start": 2694.0, "end": 2702.0, "text": " And what this will do is that every single neuron now and its firing rate will be exactly unit Gaussian on these 32 examples, at least, of this batch.", "tokens": [50364, 400, 341, 307, 2293, 437, 613, 732, 4904, 35, 293, 914, 366, 28258, 13, 50664, 50664, 4919, 11, 341, 307, 264, 914, 293, 341, 307, 264, 21977, 13, 50814, 50814, 509, 536, 577, 264, 12771, 307, 264, 3832, 25163, 2673, 13, 50914, 50914, 407, 341, 307, 12771, 3732, 11, 597, 307, 21977, 307, 264, 3732, 295, 264, 3832, 25163, 13, 51164, 51164, 407, 341, 307, 577, 291, 3832, 1125, 613, 4190, 13, 51314, 51314, 400, 437, 341, 486, 360, 307, 300, 633, 2167, 34090, 586, 293, 1080, 16045, 3314, 486, 312, 2293, 4985, 39148, 322, 613, 8858, 5110, 11, 412, 1935, 11, 295, 341, 15245, 13, 51714, 51714, 663, 311, 983, 309, 311, 1219, 15245, 2710, 2144, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09934076715688236, "compression_ratio": 1.8198529411764706, "no_speech_prob": 3.1692394259152934e-05}, {"id": 515, "seek": 267500, "start": 2702.0, "end": 2704.0, "text": " That's why it's called batch normalization.", "tokens": [50364, 400, 341, 307, 2293, 437, 613, 732, 4904, 35, 293, 914, 366, 28258, 13, 50664, 50664, 4919, 11, 341, 307, 264, 914, 293, 341, 307, 264, 21977, 13, 50814, 50814, 509, 536, 577, 264, 12771, 307, 264, 3832, 25163, 2673, 13, 50914, 50914, 407, 341, 307, 12771, 3732, 11, 597, 307, 21977, 307, 264, 3732, 295, 264, 3832, 25163, 13, 51164, 51164, 407, 341, 307, 577, 291, 3832, 1125, 613, 4190, 13, 51314, 51314, 400, 437, 341, 486, 360, 307, 300, 633, 2167, 34090, 586, 293, 1080, 16045, 3314, 486, 312, 2293, 4985, 39148, 322, 613, 8858, 5110, 11, 412, 1935, 11, 295, 341, 15245, 13, 51714, 51714, 663, 311, 983, 309, 311, 1219, 15245, 2710, 2144, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09934076715688236, "compression_ratio": 1.8198529411764706, "no_speech_prob": 3.1692394259152934e-05}, {"id": 516, "seek": 270400, "start": 2704.0, "end": 2708.0, "text": " So we're normalizing these batches.", "tokens": [50364, 407, 321, 434, 2710, 3319, 613, 15245, 279, 13, 50564, 50564, 400, 550, 321, 727, 11, 294, 8665, 11, 3847, 341, 13, 50714, 50714, 13428, 300, 28258, 264, 914, 293, 3832, 25163, 11, 613, 366, 445, 18894, 30546, 13, 50914, 50914, 814, 434, 6239, 819, 9364, 13, 50964, 50964, 1057, 341, 307, 6239, 819, 9364, 13, 51064, 51064, 400, 321, 393, 445, 3847, 341, 13, 51164, 51164, 440, 1154, 307, 291, 767, 1582, 380, 4584, 257, 588, 665, 1874, 365, 341, 13, 51364, 51364, 400, 264, 1778, 337, 300, 307, 321, 528, 613, 281, 312, 9810, 39148, 11, 457, 787, 412, 5883, 2144, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11554642076845523, "compression_ratio": 1.6988416988416988, "no_speech_prob": 2.144446079910267e-05}, {"id": 517, "seek": 270400, "start": 2708.0, "end": 2711.0, "text": " And then we could, in principle, train this.", "tokens": [50364, 407, 321, 434, 2710, 3319, 613, 15245, 279, 13, 50564, 50564, 400, 550, 321, 727, 11, 294, 8665, 11, 3847, 341, 13, 50714, 50714, 13428, 300, 28258, 264, 914, 293, 3832, 25163, 11, 613, 366, 445, 18894, 30546, 13, 50914, 50914, 814, 434, 6239, 819, 9364, 13, 50964, 50964, 1057, 341, 307, 6239, 819, 9364, 13, 51064, 51064, 400, 321, 393, 445, 3847, 341, 13, 51164, 51164, 440, 1154, 307, 291, 767, 1582, 380, 4584, 257, 588, 665, 1874, 365, 341, 13, 51364, 51364, 400, 264, 1778, 337, 300, 307, 321, 528, 613, 281, 312, 9810, 39148, 11, 457, 787, 412, 5883, 2144, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11554642076845523, "compression_ratio": 1.6988416988416988, "no_speech_prob": 2.144446079910267e-05}, {"id": 518, "seek": 270400, "start": 2711.0, "end": 2715.0, "text": " Notice that calculating the mean and standard deviation, these are just mathematical formulas.", "tokens": [50364, 407, 321, 434, 2710, 3319, 613, 15245, 279, 13, 50564, 50564, 400, 550, 321, 727, 11, 294, 8665, 11, 3847, 341, 13, 50714, 50714, 13428, 300, 28258, 264, 914, 293, 3832, 25163, 11, 613, 366, 445, 18894, 30546, 13, 50914, 50914, 814, 434, 6239, 819, 9364, 13, 50964, 50964, 1057, 341, 307, 6239, 819, 9364, 13, 51064, 51064, 400, 321, 393, 445, 3847, 341, 13, 51164, 51164, 440, 1154, 307, 291, 767, 1582, 380, 4584, 257, 588, 665, 1874, 365, 341, 13, 51364, 51364, 400, 264, 1778, 337, 300, 307, 321, 528, 613, 281, 312, 9810, 39148, 11, 457, 787, 412, 5883, 2144, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11554642076845523, "compression_ratio": 1.6988416988416988, "no_speech_prob": 2.144446079910267e-05}, {"id": 519, "seek": 270400, "start": 2715.0, "end": 2716.0, "text": " They're perfectly differentiable.", "tokens": [50364, 407, 321, 434, 2710, 3319, 613, 15245, 279, 13, 50564, 50564, 400, 550, 321, 727, 11, 294, 8665, 11, 3847, 341, 13, 50714, 50714, 13428, 300, 28258, 264, 914, 293, 3832, 25163, 11, 613, 366, 445, 18894, 30546, 13, 50914, 50914, 814, 434, 6239, 819, 9364, 13, 50964, 50964, 1057, 341, 307, 6239, 819, 9364, 13, 51064, 51064, 400, 321, 393, 445, 3847, 341, 13, 51164, 51164, 440, 1154, 307, 291, 767, 1582, 380, 4584, 257, 588, 665, 1874, 365, 341, 13, 51364, 51364, 400, 264, 1778, 337, 300, 307, 321, 528, 613, 281, 312, 9810, 39148, 11, 457, 787, 412, 5883, 2144, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11554642076845523, "compression_ratio": 1.6988416988416988, "no_speech_prob": 2.144446079910267e-05}, {"id": 520, "seek": 270400, "start": 2716.0, "end": 2718.0, "text": " All this is perfectly differentiable.", "tokens": [50364, 407, 321, 434, 2710, 3319, 613, 15245, 279, 13, 50564, 50564, 400, 550, 321, 727, 11, 294, 8665, 11, 3847, 341, 13, 50714, 50714, 13428, 300, 28258, 264, 914, 293, 3832, 25163, 11, 613, 366, 445, 18894, 30546, 13, 50914, 50914, 814, 434, 6239, 819, 9364, 13, 50964, 50964, 1057, 341, 307, 6239, 819, 9364, 13, 51064, 51064, 400, 321, 393, 445, 3847, 341, 13, 51164, 51164, 440, 1154, 307, 291, 767, 1582, 380, 4584, 257, 588, 665, 1874, 365, 341, 13, 51364, 51364, 400, 264, 1778, 337, 300, 307, 321, 528, 613, 281, 312, 9810, 39148, 11, 457, 787, 412, 5883, 2144, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11554642076845523, "compression_ratio": 1.6988416988416988, "no_speech_prob": 2.144446079910267e-05}, {"id": 521, "seek": 270400, "start": 2718.0, "end": 2720.0, "text": " And we can just train this.", "tokens": [50364, 407, 321, 434, 2710, 3319, 613, 15245, 279, 13, 50564, 50564, 400, 550, 321, 727, 11, 294, 8665, 11, 3847, 341, 13, 50714, 50714, 13428, 300, 28258, 264, 914, 293, 3832, 25163, 11, 613, 366, 445, 18894, 30546, 13, 50914, 50914, 814, 434, 6239, 819, 9364, 13, 50964, 50964, 1057, 341, 307, 6239, 819, 9364, 13, 51064, 51064, 400, 321, 393, 445, 3847, 341, 13, 51164, 51164, 440, 1154, 307, 291, 767, 1582, 380, 4584, 257, 588, 665, 1874, 365, 341, 13, 51364, 51364, 400, 264, 1778, 337, 300, 307, 321, 528, 613, 281, 312, 9810, 39148, 11, 457, 787, 412, 5883, 2144, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11554642076845523, "compression_ratio": 1.6988416988416988, "no_speech_prob": 2.144446079910267e-05}, {"id": 522, "seek": 270400, "start": 2720.0, "end": 2724.0, "text": " The problem is you actually won't achieve a very good result with this.", "tokens": [50364, 407, 321, 434, 2710, 3319, 613, 15245, 279, 13, 50564, 50564, 400, 550, 321, 727, 11, 294, 8665, 11, 3847, 341, 13, 50714, 50714, 13428, 300, 28258, 264, 914, 293, 3832, 25163, 11, 613, 366, 445, 18894, 30546, 13, 50914, 50914, 814, 434, 6239, 819, 9364, 13, 50964, 50964, 1057, 341, 307, 6239, 819, 9364, 13, 51064, 51064, 400, 321, 393, 445, 3847, 341, 13, 51164, 51164, 440, 1154, 307, 291, 767, 1582, 380, 4584, 257, 588, 665, 1874, 365, 341, 13, 51364, 51364, 400, 264, 1778, 337, 300, 307, 321, 528, 613, 281, 312, 9810, 39148, 11, 457, 787, 412, 5883, 2144, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11554642076845523, "compression_ratio": 1.6988416988416988, "no_speech_prob": 2.144446079910267e-05}, {"id": 523, "seek": 270400, "start": 2724.0, "end": 2731.0, "text": " And the reason for that is we want these to be roughly Gaussian, but only at initialization.", "tokens": [50364, 407, 321, 434, 2710, 3319, 613, 15245, 279, 13, 50564, 50564, 400, 550, 321, 727, 11, 294, 8665, 11, 3847, 341, 13, 50714, 50714, 13428, 300, 28258, 264, 914, 293, 3832, 25163, 11, 613, 366, 445, 18894, 30546, 13, 50914, 50914, 814, 434, 6239, 819, 9364, 13, 50964, 50964, 1057, 341, 307, 6239, 819, 9364, 13, 51064, 51064, 400, 321, 393, 445, 3847, 341, 13, 51164, 51164, 440, 1154, 307, 291, 767, 1582, 380, 4584, 257, 588, 665, 1874, 365, 341, 13, 51364, 51364, 400, 264, 1778, 337, 300, 307, 321, 528, 613, 281, 312, 9810, 39148, 11, 457, 787, 412, 5883, 2144, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11554642076845523, "compression_ratio": 1.6988416988416988, "no_speech_prob": 2.144446079910267e-05}, {"id": 524, "seek": 273100, "start": 2731.0, "end": 2735.0, "text": " So we want to be forced to be Gaussian always.", "tokens": [50364, 407, 321, 528, 281, 312, 7579, 281, 312, 39148, 1009, 13, 50564, 50564, 492, 1116, 411, 281, 2089, 264, 18161, 36170, 281, 1286, 341, 926, 281, 7263, 652, 309, 544, 42165, 11, 281, 652, 309, 544, 8199, 11, 281, 652, 512, 1266, 39, 22027, 1310, 544, 7875, 2055, 420, 1570, 7875, 2055, 13, 51114, 51114, 407, 321, 1116, 411, 341, 7316, 281, 1286, 926, 13, 51214, 51214, 400, 321, 1116, 411, 264, 646, 38377, 281, 980, 505, 577, 264, 7316, 820, 1286, 926, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.17413746226917615, "compression_ratio": 1.7692307692307692, "no_speech_prob": 3.0238124963943847e-05}, {"id": 525, "seek": 273100, "start": 2735.0, "end": 2746.0, "text": " We'd like to allow the neural nets to move this around to potentially make it more diffuse, to make it more sharp, to make some 10H neurons maybe more trigger happy or less trigger happy.", "tokens": [50364, 407, 321, 528, 281, 312, 7579, 281, 312, 39148, 1009, 13, 50564, 50564, 492, 1116, 411, 281, 2089, 264, 18161, 36170, 281, 1286, 341, 926, 281, 7263, 652, 309, 544, 42165, 11, 281, 652, 309, 544, 8199, 11, 281, 652, 512, 1266, 39, 22027, 1310, 544, 7875, 2055, 420, 1570, 7875, 2055, 13, 51114, 51114, 407, 321, 1116, 411, 341, 7316, 281, 1286, 926, 13, 51214, 51214, 400, 321, 1116, 411, 264, 646, 38377, 281, 980, 505, 577, 264, 7316, 820, 1286, 926, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.17413746226917615, "compression_ratio": 1.7692307692307692, "no_speech_prob": 3.0238124963943847e-05}, {"id": 526, "seek": 273100, "start": 2746.0, "end": 2748.0, "text": " So we'd like this distribution to move around.", "tokens": [50364, 407, 321, 528, 281, 312, 7579, 281, 312, 39148, 1009, 13, 50564, 50564, 492, 1116, 411, 281, 2089, 264, 18161, 36170, 281, 1286, 341, 926, 281, 7263, 652, 309, 544, 42165, 11, 281, 652, 309, 544, 8199, 11, 281, 652, 512, 1266, 39, 22027, 1310, 544, 7875, 2055, 420, 1570, 7875, 2055, 13, 51114, 51114, 407, 321, 1116, 411, 341, 7316, 281, 1286, 926, 13, 51214, 51214, 400, 321, 1116, 411, 264, 646, 38377, 281, 980, 505, 577, 264, 7316, 820, 1286, 926, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.17413746226917615, "compression_ratio": 1.7692307692307692, "no_speech_prob": 3.0238124963943847e-05}, {"id": 527, "seek": 273100, "start": 2748.0, "end": 2753.0, "text": " And we'd like the back propagation to tell us how the distribution should move around.", "tokens": [50364, 407, 321, 528, 281, 312, 7579, 281, 312, 39148, 1009, 13, 50564, 50564, 492, 1116, 411, 281, 2089, 264, 18161, 36170, 281, 1286, 341, 926, 281, 7263, 652, 309, 544, 42165, 11, 281, 652, 309, 544, 8199, 11, 281, 652, 512, 1266, 39, 22027, 1310, 544, 7875, 2055, 420, 1570, 7875, 2055, 13, 51114, 51114, 407, 321, 1116, 411, 341, 7316, 281, 1286, 926, 13, 51214, 51214, 400, 321, 1116, 411, 264, 646, 38377, 281, 980, 505, 577, 264, 7316, 820, 1286, 926, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.17413746226917615, "compression_ratio": 1.7692307692307692, "no_speech_prob": 3.0238124963943847e-05}, {"id": 528, "seek": 275300, "start": 2753.0, "end": 2766.0, "text": " In addition to this idea of standardizing the activations at any point in the network, we have to also introduce this additional component in the paper here, described as scale and shift.", "tokens": [50364, 682, 4500, 281, 341, 1558, 295, 3832, 3319, 264, 2430, 763, 412, 604, 935, 294, 264, 3209, 11, 321, 362, 281, 611, 5366, 341, 4497, 6542, 294, 264, 3035, 510, 11, 7619, 382, 4373, 293, 5513, 13, 51014, 51014, 400, 370, 1936, 437, 321, 434, 884, 307, 321, 434, 1940, 613, 48704, 15743, 293, 321, 366, 43181, 21589, 552, 538, 512, 6052, 293, 18687, 783, 552, 538, 512, 12577, 281, 483, 527, 2572, 5598, 490, 341, 4583, 13, 51614, 51614, 400, 370, 437, 300, 11663, 281, 307, 264, 3480, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04837625077430238, "compression_ratio": 1.724, "no_speech_prob": 1.4823208402958699e-06}, {"id": 529, "seek": 275300, "start": 2766.0, "end": 2778.0, "text": " And so basically what we're doing is we're taking these normalized inputs and we are additionally scaling them by some gain and offsetting them by some bias to get our final output from this layer.", "tokens": [50364, 682, 4500, 281, 341, 1558, 295, 3832, 3319, 264, 2430, 763, 412, 604, 935, 294, 264, 3209, 11, 321, 362, 281, 611, 5366, 341, 4497, 6542, 294, 264, 3035, 510, 11, 7619, 382, 4373, 293, 5513, 13, 51014, 51014, 400, 370, 1936, 437, 321, 434, 884, 307, 321, 434, 1940, 613, 48704, 15743, 293, 321, 366, 43181, 21589, 552, 538, 512, 6052, 293, 18687, 783, 552, 538, 512, 12577, 281, 483, 527, 2572, 5598, 490, 341, 4583, 13, 51614, 51614, 400, 370, 437, 300, 11663, 281, 307, 264, 3480, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04837625077430238, "compression_ratio": 1.724, "no_speech_prob": 1.4823208402958699e-06}, {"id": 530, "seek": 275300, "start": 2778.0, "end": 2780.0, "text": " And so what that amounts to is the following.", "tokens": [50364, 682, 4500, 281, 341, 1558, 295, 3832, 3319, 264, 2430, 763, 412, 604, 935, 294, 264, 3209, 11, 321, 362, 281, 611, 5366, 341, 4497, 6542, 294, 264, 3035, 510, 11, 7619, 382, 4373, 293, 5513, 13, 51014, 51014, 400, 370, 1936, 437, 321, 434, 884, 307, 321, 434, 1940, 613, 48704, 15743, 293, 321, 366, 43181, 21589, 552, 538, 512, 6052, 293, 18687, 783, 552, 538, 512, 12577, 281, 483, 527, 2572, 5598, 490, 341, 4583, 13, 51614, 51614, 400, 370, 437, 300, 11663, 281, 307, 264, 3480, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04837625077430238, "compression_ratio": 1.724, "no_speech_prob": 1.4823208402958699e-06}, {"id": 531, "seek": 278000, "start": 2780.0, "end": 2787.0, "text": " We are going to allow a batch normalization gain to be initialized at just once.", "tokens": [50364, 492, 366, 516, 281, 2089, 257, 15245, 2710, 2144, 6052, 281, 312, 5883, 1602, 412, 445, 1564, 13, 50714, 50714, 400, 264, 1564, 486, 312, 294, 264, 3909, 295, 472, 538, 297, 7633, 13, 50964, 50964, 400, 550, 321, 611, 486, 362, 257, 272, 77, 12577, 11, 597, 486, 312, 3930, 19318, 412, 35193, 13, 51214, 51214, 400, 309, 486, 611, 312, 295, 264, 3909, 297, 538, 472, 538, 297, 7633, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.12338845353377492, "compression_ratio": 1.7039473684210527, "no_speech_prob": 3.219056088710204e-05}, {"id": 532, "seek": 278000, "start": 2787.0, "end": 2792.0, "text": " And the once will be in the shape of one by n hidden.", "tokens": [50364, 492, 366, 516, 281, 2089, 257, 15245, 2710, 2144, 6052, 281, 312, 5883, 1602, 412, 445, 1564, 13, 50714, 50714, 400, 264, 1564, 486, 312, 294, 264, 3909, 295, 472, 538, 297, 7633, 13, 50964, 50964, 400, 550, 321, 611, 486, 362, 257, 272, 77, 12577, 11, 597, 486, 312, 3930, 19318, 412, 35193, 13, 51214, 51214, 400, 309, 486, 611, 312, 295, 264, 3909, 297, 538, 472, 538, 297, 7633, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.12338845353377492, "compression_ratio": 1.7039473684210527, "no_speech_prob": 3.219056088710204e-05}, {"id": 533, "seek": 278000, "start": 2792.0, "end": 2797.0, "text": " And then we also will have a bn bias, which will be torched at zeros.", "tokens": [50364, 492, 366, 516, 281, 2089, 257, 15245, 2710, 2144, 6052, 281, 312, 5883, 1602, 412, 445, 1564, 13, 50714, 50714, 400, 264, 1564, 486, 312, 294, 264, 3909, 295, 472, 538, 297, 7633, 13, 50964, 50964, 400, 550, 321, 611, 486, 362, 257, 272, 77, 12577, 11, 597, 486, 312, 3930, 19318, 412, 35193, 13, 51214, 51214, 400, 309, 486, 611, 312, 295, 264, 3909, 297, 538, 472, 538, 297, 7633, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.12338845353377492, "compression_ratio": 1.7039473684210527, "no_speech_prob": 3.219056088710204e-05}, {"id": 534, "seek": 278000, "start": 2797.0, "end": 2802.0, "text": " And it will also be of the shape n by one by n hidden.", "tokens": [50364, 492, 366, 516, 281, 2089, 257, 15245, 2710, 2144, 6052, 281, 312, 5883, 1602, 412, 445, 1564, 13, 50714, 50714, 400, 264, 1564, 486, 312, 294, 264, 3909, 295, 472, 538, 297, 7633, 13, 50964, 50964, 400, 550, 321, 611, 486, 362, 257, 272, 77, 12577, 11, 597, 486, 312, 3930, 19318, 412, 35193, 13, 51214, 51214, 400, 309, 486, 611, 312, 295, 264, 3909, 297, 538, 472, 538, 297, 7633, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.12338845353377492, "compression_ratio": 1.7039473684210527, "no_speech_prob": 3.219056088710204e-05}, {"id": 535, "seek": 280200, "start": 2802.0, "end": 2811.0, "text": " And then here the bn gain will multiply this and the bn bias will offset it here.", "tokens": [50364, 400, 550, 510, 264, 272, 77, 6052, 486, 12972, 341, 293, 264, 272, 77, 12577, 486, 18687, 309, 510, 13, 50814, 50814, 407, 570, 341, 307, 5883, 1602, 281, 472, 293, 341, 281, 4018, 11, 412, 5883, 2144, 11, 1184, 34090, 311, 16045, 4190, 294, 341, 15245, 486, 312, 2293, 4985, 39148, 293, 486, 362, 1481, 3547, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.10005630985383064, "compression_ratio": 1.532934131736527, "no_speech_prob": 8.664420420245733e-06}, {"id": 536, "seek": 280200, "start": 2811.0, "end": 2823.0, "text": " So because this is initialized to one and this to zero, at initialization, each neuron's firing values in this batch will be exactly unit Gaussian and will have nice numbers.", "tokens": [50364, 400, 550, 510, 264, 272, 77, 6052, 486, 12972, 341, 293, 264, 272, 77, 12577, 486, 18687, 309, 510, 13, 50814, 50814, 407, 570, 341, 307, 5883, 1602, 281, 472, 293, 341, 281, 4018, 11, 412, 5883, 2144, 11, 1184, 34090, 311, 16045, 4190, 294, 341, 15245, 486, 312, 2293, 4985, 39148, 293, 486, 362, 1481, 3547, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.10005630985383064, "compression_ratio": 1.532934131736527, "no_speech_prob": 8.664420420245733e-06}, {"id": 537, "seek": 282300, "start": 2823.0, "end": 2833.0, "text": " No matter what the distribution of the HB act is coming in, coming out, it will be unit Gaussian for each neuron. And that's roughly what we want, at least at initialization.", "tokens": [50364, 883, 1871, 437, 264, 7316, 295, 264, 389, 33, 605, 307, 1348, 294, 11, 1348, 484, 11, 309, 486, 312, 4985, 39148, 337, 1184, 34090, 13, 400, 300, 311, 9810, 437, 321, 528, 11, 412, 1935, 412, 5883, 2144, 13, 50864, 50864, 400, 550, 1830, 19618, 11, 321, 603, 312, 1075, 281, 646, 48256, 281, 272, 77, 6052, 293, 272, 77, 12577, 293, 1319, 552, 13, 51164, 51164, 407, 264, 3209, 307, 2212, 264, 1577, 3485, 281, 360, 365, 341, 2035, 309, 2738, 19501, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.12747093800748333, "compression_ratio": 1.5646551724137931, "no_speech_prob": 1.679701767898223e-06}, {"id": 538, "seek": 282300, "start": 2833.0, "end": 2839.0, "text": " And then during optimization, we'll be able to back propagate to bn gain and bn bias and change them.", "tokens": [50364, 883, 1871, 437, 264, 7316, 295, 264, 389, 33, 605, 307, 1348, 294, 11, 1348, 484, 11, 309, 486, 312, 4985, 39148, 337, 1184, 34090, 13, 400, 300, 311, 9810, 437, 321, 528, 11, 412, 1935, 412, 5883, 2144, 13, 50864, 50864, 400, 550, 1830, 19618, 11, 321, 603, 312, 1075, 281, 646, 48256, 281, 272, 77, 6052, 293, 272, 77, 12577, 293, 1319, 552, 13, 51164, 51164, 407, 264, 3209, 307, 2212, 264, 1577, 3485, 281, 360, 365, 341, 2035, 309, 2738, 19501, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.12747093800748333, "compression_ratio": 1.5646551724137931, "no_speech_prob": 1.679701767898223e-06}, {"id": 539, "seek": 282300, "start": 2839.0, "end": 2845.0, "text": " So the network is given the full ability to do with this whatever it wants internally.", "tokens": [50364, 883, 1871, 437, 264, 7316, 295, 264, 389, 33, 605, 307, 1348, 294, 11, 1348, 484, 11, 309, 486, 312, 4985, 39148, 337, 1184, 34090, 13, 400, 300, 311, 9810, 437, 321, 528, 11, 412, 1935, 412, 5883, 2144, 13, 50864, 50864, 400, 550, 1830, 19618, 11, 321, 603, 312, 1075, 281, 646, 48256, 281, 272, 77, 6052, 293, 272, 77, 12577, 293, 1319, 552, 13, 51164, 51164, 407, 264, 3209, 307, 2212, 264, 1577, 3485, 281, 360, 365, 341, 2035, 309, 2738, 19501, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.12747093800748333, "compression_ratio": 1.5646551724137931, "no_speech_prob": 1.679701767898223e-06}, {"id": 540, "seek": 284500, "start": 2845.0, "end": 2855.0, "text": " Here we just have to make sure that we include these in the parameters of the neural net because they will be trained with back propagation.", "tokens": [50364, 1692, 321, 445, 362, 281, 652, 988, 300, 321, 4090, 613, 294, 264, 9834, 295, 264, 18161, 2533, 570, 436, 486, 312, 8895, 365, 646, 38377, 13, 50864, 50864, 407, 718, 311, 5883, 1125, 341, 293, 550, 321, 820, 312, 1075, 281, 3847, 13, 51364, 51364, 400, 550, 321, 434, 516, 281, 611, 5055, 341, 1622, 11, 597, 307, 264, 15245, 2710, 2144, 4583, 510, 322, 257, 2167, 1622, 295, 3089, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07833206653594971, "compression_ratio": 1.5771144278606966, "no_speech_prob": 5.173841145733604e-06}, {"id": 541, "seek": 284500, "start": 2855.0, "end": 2865.0, "text": " So let's initialize this and then we should be able to train.", "tokens": [50364, 1692, 321, 445, 362, 281, 652, 988, 300, 321, 4090, 613, 294, 264, 9834, 295, 264, 18161, 2533, 570, 436, 486, 312, 8895, 365, 646, 38377, 13, 50864, 50864, 407, 718, 311, 5883, 1125, 341, 293, 550, 321, 820, 312, 1075, 281, 3847, 13, 51364, 51364, 400, 550, 321, 434, 516, 281, 611, 5055, 341, 1622, 11, 597, 307, 264, 15245, 2710, 2144, 4583, 510, 322, 257, 2167, 1622, 295, 3089, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07833206653594971, "compression_ratio": 1.5771144278606966, "no_speech_prob": 5.173841145733604e-06}, {"id": 542, "seek": 284500, "start": 2865.0, "end": 2873.0, "text": " And then we're going to also copy this line, which is the batch normalization layer here on a single line of code.", "tokens": [50364, 1692, 321, 445, 362, 281, 652, 988, 300, 321, 4090, 613, 294, 264, 9834, 295, 264, 18161, 2533, 570, 436, 486, 312, 8895, 365, 646, 38377, 13, 50864, 50864, 407, 718, 311, 5883, 1125, 341, 293, 550, 321, 820, 312, 1075, 281, 3847, 13, 51364, 51364, 400, 550, 321, 434, 516, 281, 611, 5055, 341, 1622, 11, 597, 307, 264, 15245, 2710, 2144, 4583, 510, 322, 257, 2167, 1622, 295, 3089, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07833206653594971, "compression_ratio": 1.5771144278606966, "no_speech_prob": 5.173841145733604e-06}, {"id": 543, "seek": 287300, "start": 2873.0, "end": 2881.0, "text": " And we're going to swing down here and we're also going to do the exact same thing at test time here.", "tokens": [50364, 400, 321, 434, 516, 281, 11173, 760, 510, 293, 321, 434, 611, 516, 281, 360, 264, 1900, 912, 551, 412, 1500, 565, 510, 13, 50764, 50764, 407, 2531, 281, 3847, 565, 11, 321, 434, 516, 281, 2710, 1125, 293, 550, 4373, 13, 51014, 51014, 400, 300, 311, 516, 281, 976, 505, 527, 3847, 293, 24071, 4470, 13, 51214, 51214, 400, 321, 603, 536, 294, 257, 1150, 300, 321, 434, 767, 516, 281, 1319, 341, 257, 707, 857, 11, 457, 337, 586, 11, 286, 478, 516, 281, 1066, 309, 341, 636, 13, 51464, 51464, 407, 286, 478, 445, 516, 281, 1699, 337, 341, 281, 41881, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04482784621212461, "compression_ratio": 1.8099547511312217, "no_speech_prob": 2.3687512111791875e-06}, {"id": 544, "seek": 287300, "start": 2881.0, "end": 2886.0, "text": " So similar to train time, we're going to normalize and then scale.", "tokens": [50364, 400, 321, 434, 516, 281, 11173, 760, 510, 293, 321, 434, 611, 516, 281, 360, 264, 1900, 912, 551, 412, 1500, 565, 510, 13, 50764, 50764, 407, 2531, 281, 3847, 565, 11, 321, 434, 516, 281, 2710, 1125, 293, 550, 4373, 13, 51014, 51014, 400, 300, 311, 516, 281, 976, 505, 527, 3847, 293, 24071, 4470, 13, 51214, 51214, 400, 321, 603, 536, 294, 257, 1150, 300, 321, 434, 767, 516, 281, 1319, 341, 257, 707, 857, 11, 457, 337, 586, 11, 286, 478, 516, 281, 1066, 309, 341, 636, 13, 51464, 51464, 407, 286, 478, 445, 516, 281, 1699, 337, 341, 281, 41881, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04482784621212461, "compression_ratio": 1.8099547511312217, "no_speech_prob": 2.3687512111791875e-06}, {"id": 545, "seek": 287300, "start": 2886.0, "end": 2890.0, "text": " And that's going to give us our train and validation loss.", "tokens": [50364, 400, 321, 434, 516, 281, 11173, 760, 510, 293, 321, 434, 611, 516, 281, 360, 264, 1900, 912, 551, 412, 1500, 565, 510, 13, 50764, 50764, 407, 2531, 281, 3847, 565, 11, 321, 434, 516, 281, 2710, 1125, 293, 550, 4373, 13, 51014, 51014, 400, 300, 311, 516, 281, 976, 505, 527, 3847, 293, 24071, 4470, 13, 51214, 51214, 400, 321, 603, 536, 294, 257, 1150, 300, 321, 434, 767, 516, 281, 1319, 341, 257, 707, 857, 11, 457, 337, 586, 11, 286, 478, 516, 281, 1066, 309, 341, 636, 13, 51464, 51464, 407, 286, 478, 445, 516, 281, 1699, 337, 341, 281, 41881, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04482784621212461, "compression_ratio": 1.8099547511312217, "no_speech_prob": 2.3687512111791875e-06}, {"id": 546, "seek": 287300, "start": 2890.0, "end": 2895.0, "text": " And we'll see in a second that we're actually going to change this a little bit, but for now, I'm going to keep it this way.", "tokens": [50364, 400, 321, 434, 516, 281, 11173, 760, 510, 293, 321, 434, 611, 516, 281, 360, 264, 1900, 912, 551, 412, 1500, 565, 510, 13, 50764, 50764, 407, 2531, 281, 3847, 565, 11, 321, 434, 516, 281, 2710, 1125, 293, 550, 4373, 13, 51014, 51014, 400, 300, 311, 516, 281, 976, 505, 527, 3847, 293, 24071, 4470, 13, 51214, 51214, 400, 321, 603, 536, 294, 257, 1150, 300, 321, 434, 767, 516, 281, 1319, 341, 257, 707, 857, 11, 457, 337, 586, 11, 286, 478, 516, 281, 1066, 309, 341, 636, 13, 51464, 51464, 407, 286, 478, 445, 516, 281, 1699, 337, 341, 281, 41881, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04482784621212461, "compression_ratio": 1.8099547511312217, "no_speech_prob": 2.3687512111791875e-06}, {"id": 547, "seek": 287300, "start": 2895.0, "end": 2897.0, "text": " So I'm just going to wait for this to converge.", "tokens": [50364, 400, 321, 434, 516, 281, 11173, 760, 510, 293, 321, 434, 611, 516, 281, 360, 264, 1900, 912, 551, 412, 1500, 565, 510, 13, 50764, 50764, 407, 2531, 281, 3847, 565, 11, 321, 434, 516, 281, 2710, 1125, 293, 550, 4373, 13, 51014, 51014, 400, 300, 311, 516, 281, 976, 505, 527, 3847, 293, 24071, 4470, 13, 51214, 51214, 400, 321, 603, 536, 294, 257, 1150, 300, 321, 434, 767, 516, 281, 1319, 341, 257, 707, 857, 11, 457, 337, 586, 11, 286, 478, 516, 281, 1066, 309, 341, 636, 13, 51464, 51464, 407, 286, 478, 445, 516, 281, 1699, 337, 341, 281, 41881, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04482784621212461, "compression_ratio": 1.8099547511312217, "no_speech_prob": 2.3687512111791875e-06}, {"id": 548, "seek": 289700, "start": 2897.0, "end": 2906.0, "text": " OK, so I allowed the neural nets to converge here and when we scroll down, we see that our validation loss here is two point one zero roughly, which I wrote down here.", "tokens": [50364, 2264, 11, 370, 286, 4350, 264, 18161, 36170, 281, 41881, 510, 293, 562, 321, 11369, 760, 11, 321, 536, 300, 527, 24071, 4470, 510, 307, 732, 935, 472, 4018, 9810, 11, 597, 286, 4114, 760, 510, 13, 50814, 50814, 400, 321, 536, 300, 341, 307, 767, 733, 295, 25323, 281, 512, 295, 264, 3542, 300, 321, 600, 11042, 8046, 13, 51064, 51064, 823, 11, 286, 478, 406, 767, 9650, 364, 10444, 294, 341, 1389, 11, 293, 300, 311, 570, 321, 366, 6260, 365, 257, 588, 2199, 18161, 2533, 300, 575, 445, 257, 2167, 7633, 4583, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.07723119258880615, "compression_ratio": 1.620817843866171, "no_speech_prob": 9.080070412892383e-06}, {"id": 549, "seek": 289700, "start": 2906.0, "end": 2911.0, "text": " And we see that this is actually kind of comparable to some of the results that we've achieved previously.", "tokens": [50364, 2264, 11, 370, 286, 4350, 264, 18161, 36170, 281, 41881, 510, 293, 562, 321, 11369, 760, 11, 321, 536, 300, 527, 24071, 4470, 510, 307, 732, 935, 472, 4018, 9810, 11, 597, 286, 4114, 760, 510, 13, 50814, 50814, 400, 321, 536, 300, 341, 307, 767, 733, 295, 25323, 281, 512, 295, 264, 3542, 300, 321, 600, 11042, 8046, 13, 51064, 51064, 823, 11, 286, 478, 406, 767, 9650, 364, 10444, 294, 341, 1389, 11, 293, 300, 311, 570, 321, 366, 6260, 365, 257, 588, 2199, 18161, 2533, 300, 575, 445, 257, 2167, 7633, 4583, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.07723119258880615, "compression_ratio": 1.620817843866171, "no_speech_prob": 9.080070412892383e-06}, {"id": 550, "seek": 289700, "start": 2911.0, "end": 2919.0, "text": " Now, I'm not actually expecting an improvement in this case, and that's because we are dealing with a very simple neural net that has just a single hidden layer.", "tokens": [50364, 2264, 11, 370, 286, 4350, 264, 18161, 36170, 281, 41881, 510, 293, 562, 321, 11369, 760, 11, 321, 536, 300, 527, 24071, 4470, 510, 307, 732, 935, 472, 4018, 9810, 11, 597, 286, 4114, 760, 510, 13, 50814, 50814, 400, 321, 536, 300, 341, 307, 767, 733, 295, 25323, 281, 512, 295, 264, 3542, 300, 321, 600, 11042, 8046, 13, 51064, 51064, 823, 11, 286, 478, 406, 767, 9650, 364, 10444, 294, 341, 1389, 11, 293, 300, 311, 570, 321, 366, 6260, 365, 257, 588, 2199, 18161, 2533, 300, 575, 445, 257, 2167, 7633, 4583, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.07723119258880615, "compression_ratio": 1.620817843866171, "no_speech_prob": 9.080070412892383e-06}, {"id": 551, "seek": 291900, "start": 2919.0, "end": 2930.0, "text": " So, in fact, in this very simple case of just one hidden layer, we were able to actually calculate what the scale of W should be to make these pre activations already have a roughly Gaussian shape.", "tokens": [50364, 407, 11, 294, 1186, 11, 294, 341, 588, 2199, 1389, 295, 445, 472, 7633, 4583, 11, 321, 645, 1075, 281, 767, 8873, 437, 264, 4373, 295, 343, 820, 312, 281, 652, 613, 659, 2430, 763, 1217, 362, 257, 9810, 39148, 3909, 13, 50914, 50914, 407, 264, 15245, 2710, 2144, 307, 406, 884, 709, 510, 11, 457, 291, 1062, 3811, 300, 1564, 291, 362, 257, 709, 7731, 18161, 2533, 300, 575, 3195, 295, 819, 3467, 295, 7705, 13, 51364, 51364, 400, 456, 311, 611, 11, 337, 1365, 11, 27980, 9271, 11, 597, 486, 2060, 293, 370, 322, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08126516625432685, "compression_ratio": 1.5935251798561152, "no_speech_prob": 5.337962647899985e-06}, {"id": 552, "seek": 291900, "start": 2930.0, "end": 2939.0, "text": " So the batch normalization is not doing much here, but you might imagine that once you have a much deeper neural net that has lots of different types of operations.", "tokens": [50364, 407, 11, 294, 1186, 11, 294, 341, 588, 2199, 1389, 295, 445, 472, 7633, 4583, 11, 321, 645, 1075, 281, 767, 8873, 437, 264, 4373, 295, 343, 820, 312, 281, 652, 613, 659, 2430, 763, 1217, 362, 257, 9810, 39148, 3909, 13, 50914, 50914, 407, 264, 15245, 2710, 2144, 307, 406, 884, 709, 510, 11, 457, 291, 1062, 3811, 300, 1564, 291, 362, 257, 709, 7731, 18161, 2533, 300, 575, 3195, 295, 819, 3467, 295, 7705, 13, 51364, 51364, 400, 456, 311, 611, 11, 337, 1365, 11, 27980, 9271, 11, 597, 486, 2060, 293, 370, 322, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08126516625432685, "compression_ratio": 1.5935251798561152, "no_speech_prob": 5.337962647899985e-06}, {"id": 553, "seek": 291900, "start": 2939.0, "end": 2943.0, "text": " And there's also, for example, residual connections, which will cover and so on.", "tokens": [50364, 407, 11, 294, 1186, 11, 294, 341, 588, 2199, 1389, 295, 445, 472, 7633, 4583, 11, 321, 645, 1075, 281, 767, 8873, 437, 264, 4373, 295, 343, 820, 312, 281, 652, 613, 659, 2430, 763, 1217, 362, 257, 9810, 39148, 3909, 13, 50914, 50914, 407, 264, 15245, 2710, 2144, 307, 406, 884, 709, 510, 11, 457, 291, 1062, 3811, 300, 1564, 291, 362, 257, 709, 7731, 18161, 2533, 300, 575, 3195, 295, 819, 3467, 295, 7705, 13, 51364, 51364, 400, 456, 311, 611, 11, 337, 1365, 11, 27980, 9271, 11, 597, 486, 2060, 293, 370, 322, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.08126516625432685, "compression_ratio": 1.5935251798561152, "no_speech_prob": 5.337962647899985e-06}, {"id": 554, "seek": 294300, "start": 2943.0, "end": 2953.0, "text": " It will become basically very, very difficult to tune the scales of your weight matrices, such that all the activations throughout the neural net are roughly Gaussian.", "tokens": [50364, 467, 486, 1813, 1936, 588, 11, 588, 2252, 281, 10864, 264, 17408, 295, 428, 3364, 32284, 11, 1270, 300, 439, 264, 2430, 763, 3710, 264, 18161, 2533, 366, 9810, 39148, 13, 50864, 50864, 400, 370, 300, 311, 516, 281, 1813, 588, 2661, 560, 1897, 712, 13, 51014, 51014, 583, 5347, 281, 300, 11, 309, 311, 516, 281, 312, 709, 11, 709, 3571, 281, 24745, 15245, 2710, 2144, 7914, 3710, 264, 18161, 2533, 13, 51314, 51314, 407, 294, 1729, 11, 309, 311, 2689, 281, 574, 412, 633, 2167, 8213, 4583, 411, 341, 472, 13, 51564, 51564, 639, 307, 257, 8213, 4583, 30955, 538, 3364, 8141, 293, 5127, 264, 12577, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07091924785512739, "compression_ratio": 1.7578947368421052, "no_speech_prob": 6.048592240404105e-06}, {"id": 555, "seek": 294300, "start": 2953.0, "end": 2956.0, "text": " And so that's going to become very quickly intractable.", "tokens": [50364, 467, 486, 1813, 1936, 588, 11, 588, 2252, 281, 10864, 264, 17408, 295, 428, 3364, 32284, 11, 1270, 300, 439, 264, 2430, 763, 3710, 264, 18161, 2533, 366, 9810, 39148, 13, 50864, 50864, 400, 370, 300, 311, 516, 281, 1813, 588, 2661, 560, 1897, 712, 13, 51014, 51014, 583, 5347, 281, 300, 11, 309, 311, 516, 281, 312, 709, 11, 709, 3571, 281, 24745, 15245, 2710, 2144, 7914, 3710, 264, 18161, 2533, 13, 51314, 51314, 407, 294, 1729, 11, 309, 311, 2689, 281, 574, 412, 633, 2167, 8213, 4583, 411, 341, 472, 13, 51564, 51564, 639, 307, 257, 8213, 4583, 30955, 538, 3364, 8141, 293, 5127, 264, 12577, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07091924785512739, "compression_ratio": 1.7578947368421052, "no_speech_prob": 6.048592240404105e-06}, {"id": 556, "seek": 294300, "start": 2956.0, "end": 2962.0, "text": " But compared to that, it's going to be much, much easier to sprinkle batch normalization layers throughout the neural net.", "tokens": [50364, 467, 486, 1813, 1936, 588, 11, 588, 2252, 281, 10864, 264, 17408, 295, 428, 3364, 32284, 11, 1270, 300, 439, 264, 2430, 763, 3710, 264, 18161, 2533, 366, 9810, 39148, 13, 50864, 50864, 400, 370, 300, 311, 516, 281, 1813, 588, 2661, 560, 1897, 712, 13, 51014, 51014, 583, 5347, 281, 300, 11, 309, 311, 516, 281, 312, 709, 11, 709, 3571, 281, 24745, 15245, 2710, 2144, 7914, 3710, 264, 18161, 2533, 13, 51314, 51314, 407, 294, 1729, 11, 309, 311, 2689, 281, 574, 412, 633, 2167, 8213, 4583, 411, 341, 472, 13, 51564, 51564, 639, 307, 257, 8213, 4583, 30955, 538, 3364, 8141, 293, 5127, 264, 12577, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07091924785512739, "compression_ratio": 1.7578947368421052, "no_speech_prob": 6.048592240404105e-06}, {"id": 557, "seek": 294300, "start": 2962.0, "end": 2967.0, "text": " So in particular, it's common to look at every single linear layer like this one.", "tokens": [50364, 467, 486, 1813, 1936, 588, 11, 588, 2252, 281, 10864, 264, 17408, 295, 428, 3364, 32284, 11, 1270, 300, 439, 264, 2430, 763, 3710, 264, 18161, 2533, 366, 9810, 39148, 13, 50864, 50864, 400, 370, 300, 311, 516, 281, 1813, 588, 2661, 560, 1897, 712, 13, 51014, 51014, 583, 5347, 281, 300, 11, 309, 311, 516, 281, 312, 709, 11, 709, 3571, 281, 24745, 15245, 2710, 2144, 7914, 3710, 264, 18161, 2533, 13, 51314, 51314, 407, 294, 1729, 11, 309, 311, 2689, 281, 574, 412, 633, 2167, 8213, 4583, 411, 341, 472, 13, 51564, 51564, 639, 307, 257, 8213, 4583, 30955, 538, 3364, 8141, 293, 5127, 264, 12577, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07091924785512739, "compression_ratio": 1.7578947368421052, "no_speech_prob": 6.048592240404105e-06}, {"id": 558, "seek": 294300, "start": 2967.0, "end": 2971.0, "text": " This is a linear layer multiplying by weight matrix and adding the bias.", "tokens": [50364, 467, 486, 1813, 1936, 588, 11, 588, 2252, 281, 10864, 264, 17408, 295, 428, 3364, 32284, 11, 1270, 300, 439, 264, 2430, 763, 3710, 264, 18161, 2533, 366, 9810, 39148, 13, 50864, 50864, 400, 370, 300, 311, 516, 281, 1813, 588, 2661, 560, 1897, 712, 13, 51014, 51014, 583, 5347, 281, 300, 11, 309, 311, 516, 281, 312, 709, 11, 709, 3571, 281, 24745, 15245, 2710, 2144, 7914, 3710, 264, 18161, 2533, 13, 51314, 51314, 407, 294, 1729, 11, 309, 311, 2689, 281, 574, 412, 633, 2167, 8213, 4583, 411, 341, 472, 13, 51564, 51564, 639, 307, 257, 8213, 4583, 30955, 538, 3364, 8141, 293, 5127, 264, 12577, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07091924785512739, "compression_ratio": 1.7578947368421052, "no_speech_prob": 6.048592240404105e-06}, {"id": 559, "seek": 297100, "start": 2971.0, "end": 2980.0, "text": " Or, for example, convolutions, which we'll cover later and also perform basically a multiplication with the weight matrix, but in a more spatially structured format.", "tokens": [50364, 1610, 11, 337, 1365, 11, 3754, 15892, 11, 597, 321, 603, 2060, 1780, 293, 611, 2042, 1936, 257, 27290, 365, 264, 3364, 8141, 11, 457, 294, 257, 544, 15000, 2270, 18519, 7877, 13, 50814, 50814, 467, 311, 2375, 822, 281, 747, 613, 8213, 4583, 420, 45216, 304, 4583, 293, 34116, 257, 15245, 2710, 2144, 4583, 558, 934, 309, 281, 1969, 264, 4373, 295, 613, 2430, 763, 412, 633, 935, 294, 264, 18161, 2533, 13, 51414, 51414, 407, 321, 1116, 312, 5127, 613, 15245, 2710, 7914, 3710, 264, 18161, 2533, 13, 51564, 51564, 400, 550, 341, 9003, 264, 4373, 295, 613, 2430, 763, 3710, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08909993558316617, "compression_ratio": 1.8892988929889298, "no_speech_prob": 5.093410436529666e-06}, {"id": 560, "seek": 297100, "start": 2980.0, "end": 2992.0, "text": " It's customary to take these linear layer or convolutional layer and append a batch normalization layer right after it to control the scale of these activations at every point in the neural net.", "tokens": [50364, 1610, 11, 337, 1365, 11, 3754, 15892, 11, 597, 321, 603, 2060, 1780, 293, 611, 2042, 1936, 257, 27290, 365, 264, 3364, 8141, 11, 457, 294, 257, 544, 15000, 2270, 18519, 7877, 13, 50814, 50814, 467, 311, 2375, 822, 281, 747, 613, 8213, 4583, 420, 45216, 304, 4583, 293, 34116, 257, 15245, 2710, 2144, 4583, 558, 934, 309, 281, 1969, 264, 4373, 295, 613, 2430, 763, 412, 633, 935, 294, 264, 18161, 2533, 13, 51414, 51414, 407, 321, 1116, 312, 5127, 613, 15245, 2710, 7914, 3710, 264, 18161, 2533, 13, 51564, 51564, 400, 550, 341, 9003, 264, 4373, 295, 613, 2430, 763, 3710, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08909993558316617, "compression_ratio": 1.8892988929889298, "no_speech_prob": 5.093410436529666e-06}, {"id": 561, "seek": 297100, "start": 2992.0, "end": 2995.0, "text": " So we'd be adding these batch normal layers throughout the neural net.", "tokens": [50364, 1610, 11, 337, 1365, 11, 3754, 15892, 11, 597, 321, 603, 2060, 1780, 293, 611, 2042, 1936, 257, 27290, 365, 264, 3364, 8141, 11, 457, 294, 257, 544, 15000, 2270, 18519, 7877, 13, 50814, 50814, 467, 311, 2375, 822, 281, 747, 613, 8213, 4583, 420, 45216, 304, 4583, 293, 34116, 257, 15245, 2710, 2144, 4583, 558, 934, 309, 281, 1969, 264, 4373, 295, 613, 2430, 763, 412, 633, 935, 294, 264, 18161, 2533, 13, 51414, 51414, 407, 321, 1116, 312, 5127, 613, 15245, 2710, 7914, 3710, 264, 18161, 2533, 13, 51564, 51564, 400, 550, 341, 9003, 264, 4373, 295, 613, 2430, 763, 3710, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08909993558316617, "compression_ratio": 1.8892988929889298, "no_speech_prob": 5.093410436529666e-06}, {"id": 562, "seek": 297100, "start": 2995.0, "end": 2999.0, "text": " And then this controls the scale of these activations throughout the neural net.", "tokens": [50364, 1610, 11, 337, 1365, 11, 3754, 15892, 11, 597, 321, 603, 2060, 1780, 293, 611, 2042, 1936, 257, 27290, 365, 264, 3364, 8141, 11, 457, 294, 257, 544, 15000, 2270, 18519, 7877, 13, 50814, 50814, 467, 311, 2375, 822, 281, 747, 613, 8213, 4583, 420, 45216, 304, 4583, 293, 34116, 257, 15245, 2710, 2144, 4583, 558, 934, 309, 281, 1969, 264, 4373, 295, 613, 2430, 763, 412, 633, 935, 294, 264, 18161, 2533, 13, 51414, 51414, 407, 321, 1116, 312, 5127, 613, 15245, 2710, 7914, 3710, 264, 18161, 2533, 13, 51564, 51564, 400, 550, 341, 9003, 264, 4373, 295, 613, 2430, 763, 3710, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08909993558316617, "compression_ratio": 1.8892988929889298, "no_speech_prob": 5.093410436529666e-06}, {"id": 563, "seek": 299900, "start": 2999.0, "end": 3007.0, "text": " It doesn't require us to do perfect mathematics and care about the activation distributions for all these different types of neural network,", "tokens": [50364, 467, 1177, 380, 3651, 505, 281, 360, 2176, 18666, 293, 1127, 466, 264, 24433, 37870, 337, 439, 613, 819, 3467, 295, 18161, 3209, 11, 50764, 50764, 28761, 2390, 8474, 300, 291, 1062, 528, 281, 5366, 666, 428, 18161, 2533, 13, 50914, 50914, 400, 309, 10591, 11652, 5660, 264, 3847, 13, 51064, 51064, 400, 300, 311, 983, 613, 7914, 366, 1596, 3743, 13, 51164, 51164, 823, 11, 264, 11826, 8059, 538, 15245, 2710, 2144, 767, 1487, 412, 257, 6237, 2063, 13, 51364, 51364, 400, 300, 2063, 307, 300, 498, 291, 519, 466, 437, 311, 2737, 510, 11, 746, 22903, 5861, 293, 43470, 307, 2737, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08431072588320132, "compression_ratio": 1.705685618729097, "no_speech_prob": 9.665412108006421e-06}, {"id": 564, "seek": 299900, "start": 3007.0, "end": 3010.0, "text": " Lego building blocks that you might want to introduce into your neural net.", "tokens": [50364, 467, 1177, 380, 3651, 505, 281, 360, 2176, 18666, 293, 1127, 466, 264, 24433, 37870, 337, 439, 613, 819, 3467, 295, 18161, 3209, 11, 50764, 50764, 28761, 2390, 8474, 300, 291, 1062, 528, 281, 5366, 666, 428, 18161, 2533, 13, 50914, 50914, 400, 309, 10591, 11652, 5660, 264, 3847, 13, 51064, 51064, 400, 300, 311, 983, 613, 7914, 366, 1596, 3743, 13, 51164, 51164, 823, 11, 264, 11826, 8059, 538, 15245, 2710, 2144, 767, 1487, 412, 257, 6237, 2063, 13, 51364, 51364, 400, 300, 2063, 307, 300, 498, 291, 519, 466, 437, 311, 2737, 510, 11, 746, 22903, 5861, 293, 43470, 307, 2737, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08431072588320132, "compression_ratio": 1.705685618729097, "no_speech_prob": 9.665412108006421e-06}, {"id": 565, "seek": 299900, "start": 3010.0, "end": 3013.0, "text": " And it significantly stabilizes the train.", "tokens": [50364, 467, 1177, 380, 3651, 505, 281, 360, 2176, 18666, 293, 1127, 466, 264, 24433, 37870, 337, 439, 613, 819, 3467, 295, 18161, 3209, 11, 50764, 50764, 28761, 2390, 8474, 300, 291, 1062, 528, 281, 5366, 666, 428, 18161, 2533, 13, 50914, 50914, 400, 309, 10591, 11652, 5660, 264, 3847, 13, 51064, 51064, 400, 300, 311, 983, 613, 7914, 366, 1596, 3743, 13, 51164, 51164, 823, 11, 264, 11826, 8059, 538, 15245, 2710, 2144, 767, 1487, 412, 257, 6237, 2063, 13, 51364, 51364, 400, 300, 2063, 307, 300, 498, 291, 519, 466, 437, 311, 2737, 510, 11, 746, 22903, 5861, 293, 43470, 307, 2737, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08431072588320132, "compression_ratio": 1.705685618729097, "no_speech_prob": 9.665412108006421e-06}, {"id": 566, "seek": 299900, "start": 3013.0, "end": 3015.0, "text": " And that's why these layers are quite popular.", "tokens": [50364, 467, 1177, 380, 3651, 505, 281, 360, 2176, 18666, 293, 1127, 466, 264, 24433, 37870, 337, 439, 613, 819, 3467, 295, 18161, 3209, 11, 50764, 50764, 28761, 2390, 8474, 300, 291, 1062, 528, 281, 5366, 666, 428, 18161, 2533, 13, 50914, 50914, 400, 309, 10591, 11652, 5660, 264, 3847, 13, 51064, 51064, 400, 300, 311, 983, 613, 7914, 366, 1596, 3743, 13, 51164, 51164, 823, 11, 264, 11826, 8059, 538, 15245, 2710, 2144, 767, 1487, 412, 257, 6237, 2063, 13, 51364, 51364, 400, 300, 2063, 307, 300, 498, 291, 519, 466, 437, 311, 2737, 510, 11, 746, 22903, 5861, 293, 43470, 307, 2737, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08431072588320132, "compression_ratio": 1.705685618729097, "no_speech_prob": 9.665412108006421e-06}, {"id": 567, "seek": 299900, "start": 3015.0, "end": 3019.0, "text": " Now, the stability offered by batch normalization actually comes at a terrible cost.", "tokens": [50364, 467, 1177, 380, 3651, 505, 281, 360, 2176, 18666, 293, 1127, 466, 264, 24433, 37870, 337, 439, 613, 819, 3467, 295, 18161, 3209, 11, 50764, 50764, 28761, 2390, 8474, 300, 291, 1062, 528, 281, 5366, 666, 428, 18161, 2533, 13, 50914, 50914, 400, 309, 10591, 11652, 5660, 264, 3847, 13, 51064, 51064, 400, 300, 311, 983, 613, 7914, 366, 1596, 3743, 13, 51164, 51164, 823, 11, 264, 11826, 8059, 538, 15245, 2710, 2144, 767, 1487, 412, 257, 6237, 2063, 13, 51364, 51364, 400, 300, 2063, 307, 300, 498, 291, 519, 466, 437, 311, 2737, 510, 11, 746, 22903, 5861, 293, 43470, 307, 2737, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08431072588320132, "compression_ratio": 1.705685618729097, "no_speech_prob": 9.665412108006421e-06}, {"id": 568, "seek": 299900, "start": 3019.0, "end": 3027.0, "text": " And that cost is that if you think about what's happening here, something terribly strange and unnatural is happening.", "tokens": [50364, 467, 1177, 380, 3651, 505, 281, 360, 2176, 18666, 293, 1127, 466, 264, 24433, 37870, 337, 439, 613, 819, 3467, 295, 18161, 3209, 11, 50764, 50764, 28761, 2390, 8474, 300, 291, 1062, 528, 281, 5366, 666, 428, 18161, 2533, 13, 50914, 50914, 400, 309, 10591, 11652, 5660, 264, 3847, 13, 51064, 51064, 400, 300, 311, 983, 613, 7914, 366, 1596, 3743, 13, 51164, 51164, 823, 11, 264, 11826, 8059, 538, 15245, 2710, 2144, 767, 1487, 412, 257, 6237, 2063, 13, 51364, 51364, 400, 300, 2063, 307, 300, 498, 291, 519, 466, 437, 311, 2737, 510, 11, 746, 22903, 5861, 293, 43470, 307, 2737, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08431072588320132, "compression_ratio": 1.705685618729097, "no_speech_prob": 9.665412108006421e-06}, {"id": 569, "seek": 302700, "start": 3027.0, "end": 3035.0, "text": " It could be that we have a single example feeding into a neural net and then we calculate this activations and its logits.", "tokens": [50364, 467, 727, 312, 300, 321, 362, 257, 2167, 1365, 12919, 666, 257, 18161, 2533, 293, 550, 321, 8873, 341, 2430, 763, 293, 1080, 3565, 1208, 13, 50764, 50764, 400, 341, 307, 257, 15957, 3142, 1333, 295, 1399, 13, 50914, 50914, 407, 291, 8881, 412, 512, 3565, 1208, 337, 341, 1365, 13, 51014, 51014, 400, 550, 570, 295, 10493, 295, 3097, 11, 321, 5800, 1409, 281, 764, 15245, 279, 295, 5110, 13, 51264, 51264, 583, 729, 15245, 279, 295, 5110, 645, 18846, 21761, 293, 309, 390, 445, 364, 10493, 551, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.10565095252179085, "compression_ratio": 1.7253218884120172, "no_speech_prob": 1.1842455933219753e-05}, {"id": 570, "seek": 302700, "start": 3035.0, "end": 3038.0, "text": " And this is a deterministic sort of process.", "tokens": [50364, 467, 727, 312, 300, 321, 362, 257, 2167, 1365, 12919, 666, 257, 18161, 2533, 293, 550, 321, 8873, 341, 2430, 763, 293, 1080, 3565, 1208, 13, 50764, 50764, 400, 341, 307, 257, 15957, 3142, 1333, 295, 1399, 13, 50914, 50914, 407, 291, 8881, 412, 512, 3565, 1208, 337, 341, 1365, 13, 51014, 51014, 400, 550, 570, 295, 10493, 295, 3097, 11, 321, 5800, 1409, 281, 764, 15245, 279, 295, 5110, 13, 51264, 51264, 583, 729, 15245, 279, 295, 5110, 645, 18846, 21761, 293, 309, 390, 445, 364, 10493, 551, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.10565095252179085, "compression_ratio": 1.7253218884120172, "no_speech_prob": 1.1842455933219753e-05}, {"id": 571, "seek": 302700, "start": 3038.0, "end": 3040.0, "text": " So you arrive at some logits for this example.", "tokens": [50364, 467, 727, 312, 300, 321, 362, 257, 2167, 1365, 12919, 666, 257, 18161, 2533, 293, 550, 321, 8873, 341, 2430, 763, 293, 1080, 3565, 1208, 13, 50764, 50764, 400, 341, 307, 257, 15957, 3142, 1333, 295, 1399, 13, 50914, 50914, 407, 291, 8881, 412, 512, 3565, 1208, 337, 341, 1365, 13, 51014, 51014, 400, 550, 570, 295, 10493, 295, 3097, 11, 321, 5800, 1409, 281, 764, 15245, 279, 295, 5110, 13, 51264, 51264, 583, 729, 15245, 279, 295, 5110, 645, 18846, 21761, 293, 309, 390, 445, 364, 10493, 551, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.10565095252179085, "compression_ratio": 1.7253218884120172, "no_speech_prob": 1.1842455933219753e-05}, {"id": 572, "seek": 302700, "start": 3040.0, "end": 3045.0, "text": " And then because of efficiency of training, we suddenly started to use batches of examples.", "tokens": [50364, 467, 727, 312, 300, 321, 362, 257, 2167, 1365, 12919, 666, 257, 18161, 2533, 293, 550, 321, 8873, 341, 2430, 763, 293, 1080, 3565, 1208, 13, 50764, 50764, 400, 341, 307, 257, 15957, 3142, 1333, 295, 1399, 13, 50914, 50914, 407, 291, 8881, 412, 512, 3565, 1208, 337, 341, 1365, 13, 51014, 51014, 400, 550, 570, 295, 10493, 295, 3097, 11, 321, 5800, 1409, 281, 764, 15245, 279, 295, 5110, 13, 51264, 51264, 583, 729, 15245, 279, 295, 5110, 645, 18846, 21761, 293, 309, 390, 445, 364, 10493, 551, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.10565095252179085, "compression_ratio": 1.7253218884120172, "no_speech_prob": 1.1842455933219753e-05}, {"id": 573, "seek": 302700, "start": 3045.0, "end": 3050.0, "text": " But those batches of examples were processed independently and it was just an efficiency thing.", "tokens": [50364, 467, 727, 312, 300, 321, 362, 257, 2167, 1365, 12919, 666, 257, 18161, 2533, 293, 550, 321, 8873, 341, 2430, 763, 293, 1080, 3565, 1208, 13, 50764, 50764, 400, 341, 307, 257, 15957, 3142, 1333, 295, 1399, 13, 50914, 50914, 407, 291, 8881, 412, 512, 3565, 1208, 337, 341, 1365, 13, 51014, 51014, 400, 550, 570, 295, 10493, 295, 3097, 11, 321, 5800, 1409, 281, 764, 15245, 279, 295, 5110, 13, 51264, 51264, 583, 729, 15245, 279, 295, 5110, 645, 18846, 21761, 293, 309, 390, 445, 364, 10493, 551, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.10565095252179085, "compression_ratio": 1.7253218884120172, "no_speech_prob": 1.1842455933219753e-05}, {"id": 574, "seek": 305000, "start": 3050.0, "end": 3059.0, "text": " But now suddenly in batch normalization, because of the normalization through the batch, we are coupling these examples mathematically and in the forward pass and the backward pass of the neural net.", "tokens": [50364, 583, 586, 5800, 294, 15245, 2710, 2144, 11, 570, 295, 264, 2710, 2144, 807, 264, 15245, 11, 321, 366, 37447, 613, 5110, 44003, 293, 294, 264, 2128, 1320, 293, 264, 23897, 1320, 295, 264, 18161, 2533, 13, 50814, 50814, 407, 586, 264, 7633, 1785, 2430, 763, 11, 389, 3712, 578, 293, 428, 3565, 1208, 337, 604, 472, 4846, 1365, 366, 406, 445, 257, 2445, 295, 300, 1365, 293, 1080, 4846, 11, 51264, 51264, 457, 436, 434, 611, 257, 2445, 295, 439, 264, 661, 5110, 300, 1051, 281, 808, 337, 257, 5077, 294, 300, 15245, 13, 51564, 51564, 400, 613, 5110, 366, 3247, 15551, 16979, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07867638999169026, "compression_ratio": 1.8467432950191571, "no_speech_prob": 2.812848151734215e-06}, {"id": 575, "seek": 305000, "start": 3059.0, "end": 3068.0, "text": " So now the hidden state activations, Hpreact and your logits for any one input example are not just a function of that example and its input,", "tokens": [50364, 583, 586, 5800, 294, 15245, 2710, 2144, 11, 570, 295, 264, 2710, 2144, 807, 264, 15245, 11, 321, 366, 37447, 613, 5110, 44003, 293, 294, 264, 2128, 1320, 293, 264, 23897, 1320, 295, 264, 18161, 2533, 13, 50814, 50814, 407, 586, 264, 7633, 1785, 2430, 763, 11, 389, 3712, 578, 293, 428, 3565, 1208, 337, 604, 472, 4846, 1365, 366, 406, 445, 257, 2445, 295, 300, 1365, 293, 1080, 4846, 11, 51264, 51264, 457, 436, 434, 611, 257, 2445, 295, 439, 264, 661, 5110, 300, 1051, 281, 808, 337, 257, 5077, 294, 300, 15245, 13, 51564, 51564, 400, 613, 5110, 366, 3247, 15551, 16979, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07867638999169026, "compression_ratio": 1.8467432950191571, "no_speech_prob": 2.812848151734215e-06}, {"id": 576, "seek": 305000, "start": 3068.0, "end": 3074.0, "text": " but they're also a function of all the other examples that happen to come for a ride in that batch.", "tokens": [50364, 583, 586, 5800, 294, 15245, 2710, 2144, 11, 570, 295, 264, 2710, 2144, 807, 264, 15245, 11, 321, 366, 37447, 613, 5110, 44003, 293, 294, 264, 2128, 1320, 293, 264, 23897, 1320, 295, 264, 18161, 2533, 13, 50814, 50814, 407, 586, 264, 7633, 1785, 2430, 763, 11, 389, 3712, 578, 293, 428, 3565, 1208, 337, 604, 472, 4846, 1365, 366, 406, 445, 257, 2445, 295, 300, 1365, 293, 1080, 4846, 11, 51264, 51264, 457, 436, 434, 611, 257, 2445, 295, 439, 264, 661, 5110, 300, 1051, 281, 808, 337, 257, 5077, 294, 300, 15245, 13, 51564, 51564, 400, 613, 5110, 366, 3247, 15551, 16979, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07867638999169026, "compression_ratio": 1.8467432950191571, "no_speech_prob": 2.812848151734215e-06}, {"id": 577, "seek": 305000, "start": 3074.0, "end": 3076.0, "text": " And these examples are sampled randomly.", "tokens": [50364, 583, 586, 5800, 294, 15245, 2710, 2144, 11, 570, 295, 264, 2710, 2144, 807, 264, 15245, 11, 321, 366, 37447, 613, 5110, 44003, 293, 294, 264, 2128, 1320, 293, 264, 23897, 1320, 295, 264, 18161, 2533, 13, 50814, 50814, 407, 586, 264, 7633, 1785, 2430, 763, 11, 389, 3712, 578, 293, 428, 3565, 1208, 337, 604, 472, 4846, 1365, 366, 406, 445, 257, 2445, 295, 300, 1365, 293, 1080, 4846, 11, 51264, 51264, 457, 436, 434, 611, 257, 2445, 295, 439, 264, 661, 5110, 300, 1051, 281, 808, 337, 257, 5077, 294, 300, 15245, 13, 51564, 51564, 400, 613, 5110, 366, 3247, 15551, 16979, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07867638999169026, "compression_ratio": 1.8467432950191571, "no_speech_prob": 2.812848151734215e-06}, {"id": 578, "seek": 307600, "start": 3076.0, "end": 3085.0, "text": " And so what's happening is, for example, when you look at Hpreact that's going to feed into H, the hidden state activations, for example, for any one of these input examples,", "tokens": [50364, 400, 370, 437, 311, 2737, 307, 11, 337, 1365, 11, 562, 291, 574, 412, 389, 3712, 578, 300, 311, 516, 281, 3154, 666, 389, 11, 264, 7633, 1785, 2430, 763, 11, 337, 1365, 11, 337, 604, 472, 295, 613, 4846, 5110, 11, 50814, 50814, 307, 516, 281, 767, 1319, 4748, 5413, 322, 437, 661, 5110, 456, 366, 294, 264, 15245, 13, 51064, 51064, 400, 5413, 322, 437, 661, 5110, 1051, 281, 808, 337, 257, 5077, 11, 389, 307, 516, 281, 1319, 5800, 293, 309, 311, 516, 281, 411, 361, 3904, 11, 498, 291, 3811, 21179, 819, 5110, 11, 51514, 51514, 570, 264, 12523, 295, 264, 914, 293, 264, 3832, 25163, 366, 516, 281, 312, 15653, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09236374696095785, "compression_ratio": 1.9516728624535316, "no_speech_prob": 9.368183782498818e-06}, {"id": 579, "seek": 307600, "start": 3085.0, "end": 3090.0, "text": " is going to actually change slightly depending on what other examples there are in the batch.", "tokens": [50364, 400, 370, 437, 311, 2737, 307, 11, 337, 1365, 11, 562, 291, 574, 412, 389, 3712, 578, 300, 311, 516, 281, 3154, 666, 389, 11, 264, 7633, 1785, 2430, 763, 11, 337, 1365, 11, 337, 604, 472, 295, 613, 4846, 5110, 11, 50814, 50814, 307, 516, 281, 767, 1319, 4748, 5413, 322, 437, 661, 5110, 456, 366, 294, 264, 15245, 13, 51064, 51064, 400, 5413, 322, 437, 661, 5110, 1051, 281, 808, 337, 257, 5077, 11, 389, 307, 516, 281, 1319, 5800, 293, 309, 311, 516, 281, 411, 361, 3904, 11, 498, 291, 3811, 21179, 819, 5110, 11, 51514, 51514, 570, 264, 12523, 295, 264, 914, 293, 264, 3832, 25163, 366, 516, 281, 312, 15653, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09236374696095785, "compression_ratio": 1.9516728624535316, "no_speech_prob": 9.368183782498818e-06}, {"id": 580, "seek": 307600, "start": 3090.0, "end": 3099.0, "text": " And depending on what other examples happen to come for a ride, H is going to change suddenly and it's going to like jitter, if you imagine sampling different examples,", "tokens": [50364, 400, 370, 437, 311, 2737, 307, 11, 337, 1365, 11, 562, 291, 574, 412, 389, 3712, 578, 300, 311, 516, 281, 3154, 666, 389, 11, 264, 7633, 1785, 2430, 763, 11, 337, 1365, 11, 337, 604, 472, 295, 613, 4846, 5110, 11, 50814, 50814, 307, 516, 281, 767, 1319, 4748, 5413, 322, 437, 661, 5110, 456, 366, 294, 264, 15245, 13, 51064, 51064, 400, 5413, 322, 437, 661, 5110, 1051, 281, 808, 337, 257, 5077, 11, 389, 307, 516, 281, 1319, 5800, 293, 309, 311, 516, 281, 411, 361, 3904, 11, 498, 291, 3811, 21179, 819, 5110, 11, 51514, 51514, 570, 264, 12523, 295, 264, 914, 293, 264, 3832, 25163, 366, 516, 281, 312, 15653, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09236374696095785, "compression_ratio": 1.9516728624535316, "no_speech_prob": 9.368183782498818e-06}, {"id": 581, "seek": 307600, "start": 3099.0, "end": 3104.0, "text": " because the statistics of the mean and the standard deviation are going to be impacted.", "tokens": [50364, 400, 370, 437, 311, 2737, 307, 11, 337, 1365, 11, 562, 291, 574, 412, 389, 3712, 578, 300, 311, 516, 281, 3154, 666, 389, 11, 264, 7633, 1785, 2430, 763, 11, 337, 1365, 11, 337, 604, 472, 295, 613, 4846, 5110, 11, 50814, 50814, 307, 516, 281, 767, 1319, 4748, 5413, 322, 437, 661, 5110, 456, 366, 294, 264, 15245, 13, 51064, 51064, 400, 5413, 322, 437, 661, 5110, 1051, 281, 808, 337, 257, 5077, 11, 389, 307, 516, 281, 1319, 5800, 293, 309, 311, 516, 281, 411, 361, 3904, 11, 498, 291, 3811, 21179, 819, 5110, 11, 51514, 51514, 570, 264, 12523, 295, 264, 914, 293, 264, 3832, 25163, 366, 516, 281, 312, 15653, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09236374696095785, "compression_ratio": 1.9516728624535316, "no_speech_prob": 9.368183782498818e-06}, {"id": 582, "seek": 310400, "start": 3104.0, "end": 3108.0, "text": " And so you'll get a jitter for H and you'll get a jitter for logits.", "tokens": [50364, 400, 370, 291, 603, 483, 257, 361, 3904, 337, 389, 293, 291, 603, 483, 257, 361, 3904, 337, 3565, 1208, 13, 50564, 50564, 400, 291, 519, 300, 341, 576, 312, 257, 7426, 420, 746, 45667, 21493, 11, 457, 294, 257, 588, 5861, 636, 11, 341, 767, 4523, 484, 281, 312, 665, 294, 18161, 3209, 3097, 382, 257, 1252, 1802, 13, 51114, 51114, 400, 264, 1778, 337, 300, 307, 300, 291, 393, 519, 295, 341, 382, 733, 295, 411, 257, 3890, 6545, 11, 570, 437, 311, 2737, 307, 291, 362, 428, 4846, 293, 291, 483, 428, 389, 11, 51464, 51464, 293, 550, 5413, 322, 264, 661, 5110, 11, 341, 307, 361, 3904, 278, 257, 857, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0747673852103097, "compression_ratio": 1.7490494296577948, "no_speech_prob": 7.182712579378858e-06}, {"id": 583, "seek": 310400, "start": 3108.0, "end": 3119.0, "text": " And you think that this would be a bug or something undesirable, but in a very strange way, this actually turns out to be good in neural network training as a side effect.", "tokens": [50364, 400, 370, 291, 603, 483, 257, 361, 3904, 337, 389, 293, 291, 603, 483, 257, 361, 3904, 337, 3565, 1208, 13, 50564, 50564, 400, 291, 519, 300, 341, 576, 312, 257, 7426, 420, 746, 45667, 21493, 11, 457, 294, 257, 588, 5861, 636, 11, 341, 767, 4523, 484, 281, 312, 665, 294, 18161, 3209, 3097, 382, 257, 1252, 1802, 13, 51114, 51114, 400, 264, 1778, 337, 300, 307, 300, 291, 393, 519, 295, 341, 382, 733, 295, 411, 257, 3890, 6545, 11, 570, 437, 311, 2737, 307, 291, 362, 428, 4846, 293, 291, 483, 428, 389, 11, 51464, 51464, 293, 550, 5413, 322, 264, 661, 5110, 11, 341, 307, 361, 3904, 278, 257, 857, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0747673852103097, "compression_ratio": 1.7490494296577948, "no_speech_prob": 7.182712579378858e-06}, {"id": 584, "seek": 310400, "start": 3119.0, "end": 3126.0, "text": " And the reason for that is that you can think of this as kind of like a regularizer, because what's happening is you have your input and you get your H,", "tokens": [50364, 400, 370, 291, 603, 483, 257, 361, 3904, 337, 389, 293, 291, 603, 483, 257, 361, 3904, 337, 3565, 1208, 13, 50564, 50564, 400, 291, 519, 300, 341, 576, 312, 257, 7426, 420, 746, 45667, 21493, 11, 457, 294, 257, 588, 5861, 636, 11, 341, 767, 4523, 484, 281, 312, 665, 294, 18161, 3209, 3097, 382, 257, 1252, 1802, 13, 51114, 51114, 400, 264, 1778, 337, 300, 307, 300, 291, 393, 519, 295, 341, 382, 733, 295, 411, 257, 3890, 6545, 11, 570, 437, 311, 2737, 307, 291, 362, 428, 4846, 293, 291, 483, 428, 389, 11, 51464, 51464, 293, 550, 5413, 322, 264, 661, 5110, 11, 341, 307, 361, 3904, 278, 257, 857, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0747673852103097, "compression_ratio": 1.7490494296577948, "no_speech_prob": 7.182712579378858e-06}, {"id": 585, "seek": 310400, "start": 3126.0, "end": 3130.0, "text": " and then depending on the other examples, this is jittering a bit.", "tokens": [50364, 400, 370, 291, 603, 483, 257, 361, 3904, 337, 389, 293, 291, 603, 483, 257, 361, 3904, 337, 3565, 1208, 13, 50564, 50564, 400, 291, 519, 300, 341, 576, 312, 257, 7426, 420, 746, 45667, 21493, 11, 457, 294, 257, 588, 5861, 636, 11, 341, 767, 4523, 484, 281, 312, 665, 294, 18161, 3209, 3097, 382, 257, 1252, 1802, 13, 51114, 51114, 400, 264, 1778, 337, 300, 307, 300, 291, 393, 519, 295, 341, 382, 733, 295, 411, 257, 3890, 6545, 11, 570, 437, 311, 2737, 307, 291, 362, 428, 4846, 293, 291, 483, 428, 389, 11, 51464, 51464, 293, 550, 5413, 322, 264, 661, 5110, 11, 341, 307, 361, 3904, 278, 257, 857, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0747673852103097, "compression_ratio": 1.7490494296577948, "no_speech_prob": 7.182712579378858e-06}, {"id": 586, "seek": 313000, "start": 3130.0, "end": 3136.0, "text": " And so what that does is that it's effectively padding out any one of these input examples and it's introducing a little bit of entropy.", "tokens": [50364, 400, 370, 437, 300, 775, 307, 300, 309, 311, 8659, 39562, 484, 604, 472, 295, 613, 4846, 5110, 293, 309, 311, 15424, 257, 707, 857, 295, 30867, 13, 50664, 50664, 400, 570, 295, 264, 39562, 484, 11, 309, 311, 767, 733, 295, 411, 257, 1254, 295, 1412, 14501, 19631, 11, 597, 321, 603, 2060, 294, 264, 2027, 13, 51014, 51014, 400, 309, 311, 733, 295, 411, 29919, 278, 264, 4846, 257, 707, 857, 293, 361, 3904, 278, 309, 13, 51214, 51214, 400, 300, 1669, 309, 6081, 337, 264, 18161, 36170, 281, 670, 6845, 613, 9859, 2685, 5110, 13, 51464, 51464, 407, 538, 15424, 439, 341, 5658, 11, 309, 767, 411, 19179, 484, 264, 5110, 293, 309, 3890, 5660, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.054803954230414495, "compression_ratio": 1.9028776978417266, "no_speech_prob": 1.3005671462451573e-05}, {"id": 587, "seek": 313000, "start": 3136.0, "end": 3143.0, "text": " And because of the padding out, it's actually kind of like a form of data augmentation, which we'll cover in the future.", "tokens": [50364, 400, 370, 437, 300, 775, 307, 300, 309, 311, 8659, 39562, 484, 604, 472, 295, 613, 4846, 5110, 293, 309, 311, 15424, 257, 707, 857, 295, 30867, 13, 50664, 50664, 400, 570, 295, 264, 39562, 484, 11, 309, 311, 767, 733, 295, 411, 257, 1254, 295, 1412, 14501, 19631, 11, 597, 321, 603, 2060, 294, 264, 2027, 13, 51014, 51014, 400, 309, 311, 733, 295, 411, 29919, 278, 264, 4846, 257, 707, 857, 293, 361, 3904, 278, 309, 13, 51214, 51214, 400, 300, 1669, 309, 6081, 337, 264, 18161, 36170, 281, 670, 6845, 613, 9859, 2685, 5110, 13, 51464, 51464, 407, 538, 15424, 439, 341, 5658, 11, 309, 767, 411, 19179, 484, 264, 5110, 293, 309, 3890, 5660, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.054803954230414495, "compression_ratio": 1.9028776978417266, "no_speech_prob": 1.3005671462451573e-05}, {"id": 588, "seek": 313000, "start": 3143.0, "end": 3147.0, "text": " And it's kind of like augmenting the input a little bit and jittering it.", "tokens": [50364, 400, 370, 437, 300, 775, 307, 300, 309, 311, 8659, 39562, 484, 604, 472, 295, 613, 4846, 5110, 293, 309, 311, 15424, 257, 707, 857, 295, 30867, 13, 50664, 50664, 400, 570, 295, 264, 39562, 484, 11, 309, 311, 767, 733, 295, 411, 257, 1254, 295, 1412, 14501, 19631, 11, 597, 321, 603, 2060, 294, 264, 2027, 13, 51014, 51014, 400, 309, 311, 733, 295, 411, 29919, 278, 264, 4846, 257, 707, 857, 293, 361, 3904, 278, 309, 13, 51214, 51214, 400, 300, 1669, 309, 6081, 337, 264, 18161, 36170, 281, 670, 6845, 613, 9859, 2685, 5110, 13, 51464, 51464, 407, 538, 15424, 439, 341, 5658, 11, 309, 767, 411, 19179, 484, 264, 5110, 293, 309, 3890, 5660, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.054803954230414495, "compression_ratio": 1.9028776978417266, "no_speech_prob": 1.3005671462451573e-05}, {"id": 589, "seek": 313000, "start": 3147.0, "end": 3152.0, "text": " And that makes it harder for the neural nets to overfit these concrete specific examples.", "tokens": [50364, 400, 370, 437, 300, 775, 307, 300, 309, 311, 8659, 39562, 484, 604, 472, 295, 613, 4846, 5110, 293, 309, 311, 15424, 257, 707, 857, 295, 30867, 13, 50664, 50664, 400, 570, 295, 264, 39562, 484, 11, 309, 311, 767, 733, 295, 411, 257, 1254, 295, 1412, 14501, 19631, 11, 597, 321, 603, 2060, 294, 264, 2027, 13, 51014, 51014, 400, 309, 311, 733, 295, 411, 29919, 278, 264, 4846, 257, 707, 857, 293, 361, 3904, 278, 309, 13, 51214, 51214, 400, 300, 1669, 309, 6081, 337, 264, 18161, 36170, 281, 670, 6845, 613, 9859, 2685, 5110, 13, 51464, 51464, 407, 538, 15424, 439, 341, 5658, 11, 309, 767, 411, 19179, 484, 264, 5110, 293, 309, 3890, 5660, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.054803954230414495, "compression_ratio": 1.9028776978417266, "no_speech_prob": 1.3005671462451573e-05}, {"id": 590, "seek": 313000, "start": 3152.0, "end": 3158.0, "text": " So by introducing all this noise, it actually like pads out the examples and it regularizes the neural net.", "tokens": [50364, 400, 370, 437, 300, 775, 307, 300, 309, 311, 8659, 39562, 484, 604, 472, 295, 613, 4846, 5110, 293, 309, 311, 15424, 257, 707, 857, 295, 30867, 13, 50664, 50664, 400, 570, 295, 264, 39562, 484, 11, 309, 311, 767, 733, 295, 411, 257, 1254, 295, 1412, 14501, 19631, 11, 597, 321, 603, 2060, 294, 264, 2027, 13, 51014, 51014, 400, 309, 311, 733, 295, 411, 29919, 278, 264, 4846, 257, 707, 857, 293, 361, 3904, 278, 309, 13, 51214, 51214, 400, 300, 1669, 309, 6081, 337, 264, 18161, 36170, 281, 670, 6845, 613, 9859, 2685, 5110, 13, 51464, 51464, 407, 538, 15424, 439, 341, 5658, 11, 309, 767, 411, 19179, 484, 264, 5110, 293, 309, 3890, 5660, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.054803954230414495, "compression_ratio": 1.9028776978417266, "no_speech_prob": 1.3005671462451573e-05}, {"id": 591, "seek": 315800, "start": 3158.0, "end": 3164.0, "text": " And that's one of the reasons why the seemingly as a second order effect, this is actually a regularizer.", "tokens": [50364, 400, 300, 311, 472, 295, 264, 4112, 983, 264, 18709, 382, 257, 1150, 1668, 1802, 11, 341, 307, 767, 257, 3890, 6545, 13, 50664, 50664, 400, 300, 575, 1027, 309, 6081, 337, 505, 281, 4159, 264, 764, 295, 15245, 2710, 2144, 11, 50914, 50914, 570, 1936, 572, 472, 5902, 341, 4707, 300, 264, 5110, 294, 264, 15245, 366, 29482, 44003, 293, 294, 264, 2128, 1320, 13, 51264, 51264, 400, 309, 6689, 281, 439, 3685, 295, 411, 5861, 3542, 13, 51414, 51414, 492, 603, 352, 666, 512, 295, 300, 294, 257, 1150, 382, 731, 13, 51564, 51564, 400, 309, 6689, 281, 257, 688, 295, 15120, 293, 370, 322, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09558534622192383, "compression_ratio": 1.744186046511628, "no_speech_prob": 3.23763970300206e-06}, {"id": 592, "seek": 315800, "start": 3164.0, "end": 3169.0, "text": " And that has made it harder for us to remove the use of batch normalization,", "tokens": [50364, 400, 300, 311, 472, 295, 264, 4112, 983, 264, 18709, 382, 257, 1150, 1668, 1802, 11, 341, 307, 767, 257, 3890, 6545, 13, 50664, 50664, 400, 300, 575, 1027, 309, 6081, 337, 505, 281, 4159, 264, 764, 295, 15245, 2710, 2144, 11, 50914, 50914, 570, 1936, 572, 472, 5902, 341, 4707, 300, 264, 5110, 294, 264, 15245, 366, 29482, 44003, 293, 294, 264, 2128, 1320, 13, 51264, 51264, 400, 309, 6689, 281, 439, 3685, 295, 411, 5861, 3542, 13, 51414, 51414, 492, 603, 352, 666, 512, 295, 300, 294, 257, 1150, 382, 731, 13, 51564, 51564, 400, 309, 6689, 281, 257, 688, 295, 15120, 293, 370, 322, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09558534622192383, "compression_ratio": 1.744186046511628, "no_speech_prob": 3.23763970300206e-06}, {"id": 593, "seek": 315800, "start": 3169.0, "end": 3176.0, "text": " because basically no one likes this property that the examples in the batch are coupled mathematically and in the forward pass.", "tokens": [50364, 400, 300, 311, 472, 295, 264, 4112, 983, 264, 18709, 382, 257, 1150, 1668, 1802, 11, 341, 307, 767, 257, 3890, 6545, 13, 50664, 50664, 400, 300, 575, 1027, 309, 6081, 337, 505, 281, 4159, 264, 764, 295, 15245, 2710, 2144, 11, 50914, 50914, 570, 1936, 572, 472, 5902, 341, 4707, 300, 264, 5110, 294, 264, 15245, 366, 29482, 44003, 293, 294, 264, 2128, 1320, 13, 51264, 51264, 400, 309, 6689, 281, 439, 3685, 295, 411, 5861, 3542, 13, 51414, 51414, 492, 603, 352, 666, 512, 295, 300, 294, 257, 1150, 382, 731, 13, 51564, 51564, 400, 309, 6689, 281, 257, 688, 295, 15120, 293, 370, 322, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09558534622192383, "compression_ratio": 1.744186046511628, "no_speech_prob": 3.23763970300206e-06}, {"id": 594, "seek": 315800, "start": 3176.0, "end": 3179.0, "text": " And it leads to all kinds of like strange results.", "tokens": [50364, 400, 300, 311, 472, 295, 264, 4112, 983, 264, 18709, 382, 257, 1150, 1668, 1802, 11, 341, 307, 767, 257, 3890, 6545, 13, 50664, 50664, 400, 300, 575, 1027, 309, 6081, 337, 505, 281, 4159, 264, 764, 295, 15245, 2710, 2144, 11, 50914, 50914, 570, 1936, 572, 472, 5902, 341, 4707, 300, 264, 5110, 294, 264, 15245, 366, 29482, 44003, 293, 294, 264, 2128, 1320, 13, 51264, 51264, 400, 309, 6689, 281, 439, 3685, 295, 411, 5861, 3542, 13, 51414, 51414, 492, 603, 352, 666, 512, 295, 300, 294, 257, 1150, 382, 731, 13, 51564, 51564, 400, 309, 6689, 281, 257, 688, 295, 15120, 293, 370, 322, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09558534622192383, "compression_ratio": 1.744186046511628, "no_speech_prob": 3.23763970300206e-06}, {"id": 595, "seek": 315800, "start": 3179.0, "end": 3182.0, "text": " We'll go into some of that in a second as well.", "tokens": [50364, 400, 300, 311, 472, 295, 264, 4112, 983, 264, 18709, 382, 257, 1150, 1668, 1802, 11, 341, 307, 767, 257, 3890, 6545, 13, 50664, 50664, 400, 300, 575, 1027, 309, 6081, 337, 505, 281, 4159, 264, 764, 295, 15245, 2710, 2144, 11, 50914, 50914, 570, 1936, 572, 472, 5902, 341, 4707, 300, 264, 5110, 294, 264, 15245, 366, 29482, 44003, 293, 294, 264, 2128, 1320, 13, 51264, 51264, 400, 309, 6689, 281, 439, 3685, 295, 411, 5861, 3542, 13, 51414, 51414, 492, 603, 352, 666, 512, 295, 300, 294, 257, 1150, 382, 731, 13, 51564, 51564, 400, 309, 6689, 281, 257, 688, 295, 15120, 293, 370, 322, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09558534622192383, "compression_ratio": 1.744186046511628, "no_speech_prob": 3.23763970300206e-06}, {"id": 596, "seek": 315800, "start": 3182.0, "end": 3185.0, "text": " And it leads to a lot of bugs and so on.", "tokens": [50364, 400, 300, 311, 472, 295, 264, 4112, 983, 264, 18709, 382, 257, 1150, 1668, 1802, 11, 341, 307, 767, 257, 3890, 6545, 13, 50664, 50664, 400, 300, 575, 1027, 309, 6081, 337, 505, 281, 4159, 264, 764, 295, 15245, 2710, 2144, 11, 50914, 50914, 570, 1936, 572, 472, 5902, 341, 4707, 300, 264, 5110, 294, 264, 15245, 366, 29482, 44003, 293, 294, 264, 2128, 1320, 13, 51264, 51264, 400, 309, 6689, 281, 439, 3685, 295, 411, 5861, 3542, 13, 51414, 51414, 492, 603, 352, 666, 512, 295, 300, 294, 257, 1150, 382, 731, 13, 51564, 51564, 400, 309, 6689, 281, 257, 688, 295, 15120, 293, 370, 322, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09558534622192383, "compression_ratio": 1.744186046511628, "no_speech_prob": 3.23763970300206e-06}, {"id": 597, "seek": 318500, "start": 3185.0, "end": 3191.0, "text": " And so no one likes this property. And so people have tried to deprecate the use of batch normalization", "tokens": [50364, 400, 370, 572, 472, 5902, 341, 4707, 13, 400, 370, 561, 362, 3031, 281, 1367, 13867, 473, 264, 764, 295, 15245, 2710, 2144, 50664, 50664, 293, 1286, 281, 661, 2710, 2144, 7512, 300, 360, 406, 1916, 264, 5110, 295, 257, 15245, 13, 50864, 50864, 48591, 366, 4583, 2710, 2144, 11, 5197, 2710, 2144, 11, 1594, 2710, 2144, 11, 293, 370, 322, 13, 51114, 51114, 400, 321, 603, 2060, 512, 295, 613, 1780, 13, 51314, 51314, 583, 1936, 11, 938, 1657, 2099, 11, 15245, 2710, 2144, 390, 264, 700, 733, 295, 2710, 2144, 4583, 281, 312, 7268, 13, 51564, 51564, 467, 2732, 4664, 731, 13, 467, 2314, 281, 362, 341, 3890, 3319, 1802, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05006363860562316, "compression_ratio": 1.8555555555555556, "no_speech_prob": 1.0288869816577062e-05}, {"id": 598, "seek": 318500, "start": 3191.0, "end": 3195.0, "text": " and move to other normalization techniques that do not couple the examples of a batch.", "tokens": [50364, 400, 370, 572, 472, 5902, 341, 4707, 13, 400, 370, 561, 362, 3031, 281, 1367, 13867, 473, 264, 764, 295, 15245, 2710, 2144, 50664, 50664, 293, 1286, 281, 661, 2710, 2144, 7512, 300, 360, 406, 1916, 264, 5110, 295, 257, 15245, 13, 50864, 50864, 48591, 366, 4583, 2710, 2144, 11, 5197, 2710, 2144, 11, 1594, 2710, 2144, 11, 293, 370, 322, 13, 51114, 51114, 400, 321, 603, 2060, 512, 295, 613, 1780, 13, 51314, 51314, 583, 1936, 11, 938, 1657, 2099, 11, 15245, 2710, 2144, 390, 264, 700, 733, 295, 2710, 2144, 4583, 281, 312, 7268, 13, 51564, 51564, 467, 2732, 4664, 731, 13, 467, 2314, 281, 362, 341, 3890, 3319, 1802, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05006363860562316, "compression_ratio": 1.8555555555555556, "no_speech_prob": 1.0288869816577062e-05}, {"id": 599, "seek": 318500, "start": 3195.0, "end": 3200.0, "text": " Examples are layer normalization, instance normalization, group normalization, and so on.", "tokens": [50364, 400, 370, 572, 472, 5902, 341, 4707, 13, 400, 370, 561, 362, 3031, 281, 1367, 13867, 473, 264, 764, 295, 15245, 2710, 2144, 50664, 50664, 293, 1286, 281, 661, 2710, 2144, 7512, 300, 360, 406, 1916, 264, 5110, 295, 257, 15245, 13, 50864, 50864, 48591, 366, 4583, 2710, 2144, 11, 5197, 2710, 2144, 11, 1594, 2710, 2144, 11, 293, 370, 322, 13, 51114, 51114, 400, 321, 603, 2060, 512, 295, 613, 1780, 13, 51314, 51314, 583, 1936, 11, 938, 1657, 2099, 11, 15245, 2710, 2144, 390, 264, 700, 733, 295, 2710, 2144, 4583, 281, 312, 7268, 13, 51564, 51564, 467, 2732, 4664, 731, 13, 467, 2314, 281, 362, 341, 3890, 3319, 1802, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05006363860562316, "compression_ratio": 1.8555555555555556, "no_speech_prob": 1.0288869816577062e-05}, {"id": 600, "seek": 318500, "start": 3200.0, "end": 3204.0, "text": " And we'll cover some of these later.", "tokens": [50364, 400, 370, 572, 472, 5902, 341, 4707, 13, 400, 370, 561, 362, 3031, 281, 1367, 13867, 473, 264, 764, 295, 15245, 2710, 2144, 50664, 50664, 293, 1286, 281, 661, 2710, 2144, 7512, 300, 360, 406, 1916, 264, 5110, 295, 257, 15245, 13, 50864, 50864, 48591, 366, 4583, 2710, 2144, 11, 5197, 2710, 2144, 11, 1594, 2710, 2144, 11, 293, 370, 322, 13, 51114, 51114, 400, 321, 603, 2060, 512, 295, 613, 1780, 13, 51314, 51314, 583, 1936, 11, 938, 1657, 2099, 11, 15245, 2710, 2144, 390, 264, 700, 733, 295, 2710, 2144, 4583, 281, 312, 7268, 13, 51564, 51564, 467, 2732, 4664, 731, 13, 467, 2314, 281, 362, 341, 3890, 3319, 1802, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05006363860562316, "compression_ratio": 1.8555555555555556, "no_speech_prob": 1.0288869816577062e-05}, {"id": 601, "seek": 318500, "start": 3204.0, "end": 3209.0, "text": " But basically, long story short, batch normalization was the first kind of normalization layer to be introduced.", "tokens": [50364, 400, 370, 572, 472, 5902, 341, 4707, 13, 400, 370, 561, 362, 3031, 281, 1367, 13867, 473, 264, 764, 295, 15245, 2710, 2144, 50664, 50664, 293, 1286, 281, 661, 2710, 2144, 7512, 300, 360, 406, 1916, 264, 5110, 295, 257, 15245, 13, 50864, 50864, 48591, 366, 4583, 2710, 2144, 11, 5197, 2710, 2144, 11, 1594, 2710, 2144, 11, 293, 370, 322, 13, 51114, 51114, 400, 321, 603, 2060, 512, 295, 613, 1780, 13, 51314, 51314, 583, 1936, 11, 938, 1657, 2099, 11, 15245, 2710, 2144, 390, 264, 700, 733, 295, 2710, 2144, 4583, 281, 312, 7268, 13, 51564, 51564, 467, 2732, 4664, 731, 13, 467, 2314, 281, 362, 341, 3890, 3319, 1802, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05006363860562316, "compression_ratio": 1.8555555555555556, "no_speech_prob": 1.0288869816577062e-05}, {"id": 602, "seek": 318500, "start": 3209.0, "end": 3213.0, "text": " It worked extremely well. It happens to have this regularizing effect.", "tokens": [50364, 400, 370, 572, 472, 5902, 341, 4707, 13, 400, 370, 561, 362, 3031, 281, 1367, 13867, 473, 264, 764, 295, 15245, 2710, 2144, 50664, 50664, 293, 1286, 281, 661, 2710, 2144, 7512, 300, 360, 406, 1916, 264, 5110, 295, 257, 15245, 13, 50864, 50864, 48591, 366, 4583, 2710, 2144, 11, 5197, 2710, 2144, 11, 1594, 2710, 2144, 11, 293, 370, 322, 13, 51114, 51114, 400, 321, 603, 2060, 512, 295, 613, 1780, 13, 51314, 51314, 583, 1936, 11, 938, 1657, 2099, 11, 15245, 2710, 2144, 390, 264, 700, 733, 295, 2710, 2144, 4583, 281, 312, 7268, 13, 51564, 51564, 467, 2732, 4664, 731, 13, 467, 2314, 281, 362, 341, 3890, 3319, 1802, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05006363860562316, "compression_ratio": 1.8555555555555556, "no_speech_prob": 1.0288869816577062e-05}, {"id": 603, "seek": 321300, "start": 3213.0, "end": 3216.0, "text": " It stabilized training.", "tokens": [50364, 467, 48384, 3097, 13, 50514, 50514, 400, 561, 362, 668, 1382, 281, 4159, 309, 293, 1286, 281, 512, 295, 264, 661, 2710, 2144, 7512, 13, 50764, 50764, 583, 309, 311, 668, 1152, 570, 309, 445, 1985, 1596, 731, 13, 50914, 50914, 400, 512, 295, 264, 1778, 300, 309, 1985, 1596, 731, 307, 11, 797, 11, 570, 295, 341, 3890, 3319, 1802, 51114, 51114, 293, 570, 309, 307, 1596, 4942, 412, 14905, 264, 2430, 763, 293, 641, 37870, 13, 51414, 51414, 407, 300, 311, 733, 295, 411, 264, 5353, 1657, 295, 15245, 2710, 2144, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04698404973866988, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.7776710592443123e-05}, {"id": 604, "seek": 321300, "start": 3216.0, "end": 3221.0, "text": " And people have been trying to remove it and move to some of the other normalization techniques.", "tokens": [50364, 467, 48384, 3097, 13, 50514, 50514, 400, 561, 362, 668, 1382, 281, 4159, 309, 293, 1286, 281, 512, 295, 264, 661, 2710, 2144, 7512, 13, 50764, 50764, 583, 309, 311, 668, 1152, 570, 309, 445, 1985, 1596, 731, 13, 50914, 50914, 400, 512, 295, 264, 1778, 300, 309, 1985, 1596, 731, 307, 11, 797, 11, 570, 295, 341, 3890, 3319, 1802, 51114, 51114, 293, 570, 309, 307, 1596, 4942, 412, 14905, 264, 2430, 763, 293, 641, 37870, 13, 51414, 51414, 407, 300, 311, 733, 295, 411, 264, 5353, 1657, 295, 15245, 2710, 2144, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04698404973866988, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.7776710592443123e-05}, {"id": 605, "seek": 321300, "start": 3221.0, "end": 3224.0, "text": " But it's been hard because it just works quite well.", "tokens": [50364, 467, 48384, 3097, 13, 50514, 50514, 400, 561, 362, 668, 1382, 281, 4159, 309, 293, 1286, 281, 512, 295, 264, 661, 2710, 2144, 7512, 13, 50764, 50764, 583, 309, 311, 668, 1152, 570, 309, 445, 1985, 1596, 731, 13, 50914, 50914, 400, 512, 295, 264, 1778, 300, 309, 1985, 1596, 731, 307, 11, 797, 11, 570, 295, 341, 3890, 3319, 1802, 51114, 51114, 293, 570, 309, 307, 1596, 4942, 412, 14905, 264, 2430, 763, 293, 641, 37870, 13, 51414, 51414, 407, 300, 311, 733, 295, 411, 264, 5353, 1657, 295, 15245, 2710, 2144, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04698404973866988, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.7776710592443123e-05}, {"id": 606, "seek": 321300, "start": 3224.0, "end": 3228.0, "text": " And some of the reason that it works quite well is, again, because of this regularizing effect", "tokens": [50364, 467, 48384, 3097, 13, 50514, 50514, 400, 561, 362, 668, 1382, 281, 4159, 309, 293, 1286, 281, 512, 295, 264, 661, 2710, 2144, 7512, 13, 50764, 50764, 583, 309, 311, 668, 1152, 570, 309, 445, 1985, 1596, 731, 13, 50914, 50914, 400, 512, 295, 264, 1778, 300, 309, 1985, 1596, 731, 307, 11, 797, 11, 570, 295, 341, 3890, 3319, 1802, 51114, 51114, 293, 570, 309, 307, 1596, 4942, 412, 14905, 264, 2430, 763, 293, 641, 37870, 13, 51414, 51414, 407, 300, 311, 733, 295, 411, 264, 5353, 1657, 295, 15245, 2710, 2144, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04698404973866988, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.7776710592443123e-05}, {"id": 607, "seek": 321300, "start": 3228.0, "end": 3234.0, "text": " and because it is quite effective at controlling the activations and their distributions.", "tokens": [50364, 467, 48384, 3097, 13, 50514, 50514, 400, 561, 362, 668, 1382, 281, 4159, 309, 293, 1286, 281, 512, 295, 264, 661, 2710, 2144, 7512, 13, 50764, 50764, 583, 309, 311, 668, 1152, 570, 309, 445, 1985, 1596, 731, 13, 50914, 50914, 400, 512, 295, 264, 1778, 300, 309, 1985, 1596, 731, 307, 11, 797, 11, 570, 295, 341, 3890, 3319, 1802, 51114, 51114, 293, 570, 309, 307, 1596, 4942, 412, 14905, 264, 2430, 763, 293, 641, 37870, 13, 51414, 51414, 407, 300, 311, 733, 295, 411, 264, 5353, 1657, 295, 15245, 2710, 2144, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04698404973866988, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.7776710592443123e-05}, {"id": 608, "seek": 321300, "start": 3234.0, "end": 3237.0, "text": " So that's kind of like the brief story of batch normalization.", "tokens": [50364, 467, 48384, 3097, 13, 50514, 50514, 400, 561, 362, 668, 1382, 281, 4159, 309, 293, 1286, 281, 512, 295, 264, 661, 2710, 2144, 7512, 13, 50764, 50764, 583, 309, 311, 668, 1152, 570, 309, 445, 1985, 1596, 731, 13, 50914, 50914, 400, 512, 295, 264, 1778, 300, 309, 1985, 1596, 731, 307, 11, 797, 11, 570, 295, 341, 3890, 3319, 1802, 51114, 51114, 293, 570, 309, 307, 1596, 4942, 412, 14905, 264, 2430, 763, 293, 641, 37870, 13, 51414, 51414, 407, 300, 311, 733, 295, 411, 264, 5353, 1657, 295, 15245, 2710, 2144, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.04698404973866988, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.7776710592443123e-05}, {"id": 609, "seek": 323700, "start": 3237.0, "end": 3243.0, "text": " And I'd like to show you one of the other weird sort of outcomes of this coupling.", "tokens": [50364, 400, 286, 1116, 411, 281, 855, 291, 472, 295, 264, 661, 3657, 1333, 295, 10070, 295, 341, 37447, 13, 50664, 50664, 407, 510, 311, 472, 295, 264, 5861, 10070, 300, 286, 787, 19574, 292, 670, 8046, 50864, 50864, 562, 286, 390, 27479, 264, 4470, 322, 264, 24071, 992, 13, 51014, 51014, 8537, 11, 1564, 321, 600, 8895, 257, 18161, 2533, 11, 321, 1116, 411, 281, 7274, 309, 294, 512, 733, 295, 257, 3287, 13, 51264, 51264, 400, 321, 1116, 411, 281, 312, 1075, 281, 3154, 294, 257, 2167, 2609, 1365, 293, 483, 257, 17630, 484, 490, 527, 18161, 2533, 13, 51564, 51564, 583, 577, 360, 321, 360, 300, 562, 527, 18161, 2533, 586, 294, 264, 2128, 1320, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04351021632675297, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.1110907255206257e-05}, {"id": 610, "seek": 323700, "start": 3243.0, "end": 3247.0, "text": " So here's one of the strange outcomes that I only glossed over previously", "tokens": [50364, 400, 286, 1116, 411, 281, 855, 291, 472, 295, 264, 661, 3657, 1333, 295, 10070, 295, 341, 37447, 13, 50664, 50664, 407, 510, 311, 472, 295, 264, 5861, 10070, 300, 286, 787, 19574, 292, 670, 8046, 50864, 50864, 562, 286, 390, 27479, 264, 4470, 322, 264, 24071, 992, 13, 51014, 51014, 8537, 11, 1564, 321, 600, 8895, 257, 18161, 2533, 11, 321, 1116, 411, 281, 7274, 309, 294, 512, 733, 295, 257, 3287, 13, 51264, 51264, 400, 321, 1116, 411, 281, 312, 1075, 281, 3154, 294, 257, 2167, 2609, 1365, 293, 483, 257, 17630, 484, 490, 527, 18161, 2533, 13, 51564, 51564, 583, 577, 360, 321, 360, 300, 562, 527, 18161, 2533, 586, 294, 264, 2128, 1320, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04351021632675297, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.1110907255206257e-05}, {"id": 611, "seek": 323700, "start": 3247.0, "end": 3250.0, "text": " when I was evaluating the loss on the validation set.", "tokens": [50364, 400, 286, 1116, 411, 281, 855, 291, 472, 295, 264, 661, 3657, 1333, 295, 10070, 295, 341, 37447, 13, 50664, 50664, 407, 510, 311, 472, 295, 264, 5861, 10070, 300, 286, 787, 19574, 292, 670, 8046, 50864, 50864, 562, 286, 390, 27479, 264, 4470, 322, 264, 24071, 992, 13, 51014, 51014, 8537, 11, 1564, 321, 600, 8895, 257, 18161, 2533, 11, 321, 1116, 411, 281, 7274, 309, 294, 512, 733, 295, 257, 3287, 13, 51264, 51264, 400, 321, 1116, 411, 281, 312, 1075, 281, 3154, 294, 257, 2167, 2609, 1365, 293, 483, 257, 17630, 484, 490, 527, 18161, 2533, 13, 51564, 51564, 583, 577, 360, 321, 360, 300, 562, 527, 18161, 2533, 586, 294, 264, 2128, 1320, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04351021632675297, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.1110907255206257e-05}, {"id": 612, "seek": 323700, "start": 3250.0, "end": 3255.0, "text": " Basically, once we've trained a neural net, we'd like to deploy it in some kind of a setting.", "tokens": [50364, 400, 286, 1116, 411, 281, 855, 291, 472, 295, 264, 661, 3657, 1333, 295, 10070, 295, 341, 37447, 13, 50664, 50664, 407, 510, 311, 472, 295, 264, 5861, 10070, 300, 286, 787, 19574, 292, 670, 8046, 50864, 50864, 562, 286, 390, 27479, 264, 4470, 322, 264, 24071, 992, 13, 51014, 51014, 8537, 11, 1564, 321, 600, 8895, 257, 18161, 2533, 11, 321, 1116, 411, 281, 7274, 309, 294, 512, 733, 295, 257, 3287, 13, 51264, 51264, 400, 321, 1116, 411, 281, 312, 1075, 281, 3154, 294, 257, 2167, 2609, 1365, 293, 483, 257, 17630, 484, 490, 527, 18161, 2533, 13, 51564, 51564, 583, 577, 360, 321, 360, 300, 562, 527, 18161, 2533, 586, 294, 264, 2128, 1320, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04351021632675297, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.1110907255206257e-05}, {"id": 613, "seek": 323700, "start": 3255.0, "end": 3261.0, "text": " And we'd like to be able to feed in a single individual example and get a prediction out from our neural net.", "tokens": [50364, 400, 286, 1116, 411, 281, 855, 291, 472, 295, 264, 661, 3657, 1333, 295, 10070, 295, 341, 37447, 13, 50664, 50664, 407, 510, 311, 472, 295, 264, 5861, 10070, 300, 286, 787, 19574, 292, 670, 8046, 50864, 50864, 562, 286, 390, 27479, 264, 4470, 322, 264, 24071, 992, 13, 51014, 51014, 8537, 11, 1564, 321, 600, 8895, 257, 18161, 2533, 11, 321, 1116, 411, 281, 7274, 309, 294, 512, 733, 295, 257, 3287, 13, 51264, 51264, 400, 321, 1116, 411, 281, 312, 1075, 281, 3154, 294, 257, 2167, 2609, 1365, 293, 483, 257, 17630, 484, 490, 527, 18161, 2533, 13, 51564, 51564, 583, 577, 360, 321, 360, 300, 562, 527, 18161, 2533, 586, 294, 264, 2128, 1320, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04351021632675297, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.1110907255206257e-05}, {"id": 614, "seek": 323700, "start": 3261.0, "end": 3264.0, "text": " But how do we do that when our neural net now in the forward pass", "tokens": [50364, 400, 286, 1116, 411, 281, 855, 291, 472, 295, 264, 661, 3657, 1333, 295, 10070, 295, 341, 37447, 13, 50664, 50664, 407, 510, 311, 472, 295, 264, 5861, 10070, 300, 286, 787, 19574, 292, 670, 8046, 50864, 50864, 562, 286, 390, 27479, 264, 4470, 322, 264, 24071, 992, 13, 51014, 51014, 8537, 11, 1564, 321, 600, 8895, 257, 18161, 2533, 11, 321, 1116, 411, 281, 7274, 309, 294, 512, 733, 295, 257, 3287, 13, 51264, 51264, 400, 321, 1116, 411, 281, 312, 1075, 281, 3154, 294, 257, 2167, 2609, 1365, 293, 483, 257, 17630, 484, 490, 527, 18161, 2533, 13, 51564, 51564, 583, 577, 360, 321, 360, 300, 562, 527, 18161, 2533, 586, 294, 264, 2128, 1320, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04351021632675297, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.1110907255206257e-05}, {"id": 615, "seek": 326400, "start": 3264.0, "end": 3268.0, "text": " estimates the statistics of the mean and standard deviation of a batch?", "tokens": [50364, 20561, 264, 12523, 295, 264, 914, 293, 3832, 25163, 295, 257, 15245, 30, 50564, 50564, 440, 18161, 2533, 33280, 15245, 279, 382, 364, 4846, 586, 13, 50664, 50664, 407, 577, 360, 321, 3154, 294, 257, 2167, 1365, 293, 483, 25380, 3542, 484, 30, 50864, 50864, 400, 370, 264, 11494, 294, 264, 15245, 2710, 2144, 3035, 307, 264, 3480, 13, 51064, 51064, 708, 321, 576, 411, 281, 360, 510, 307, 321, 576, 411, 281, 1936, 362, 257, 1823, 934, 3097, 51364, 51364, 300, 4322, 1024, 293, 6352, 264, 15245, 914, 293, 3832, 25163, 257, 2167, 565, 670, 264, 3097, 992, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05338342831684993, "compression_ratio": 1.78, "no_speech_prob": 2.6424900170241017e-06}, {"id": 616, "seek": 326400, "start": 3268.0, "end": 3270.0, "text": " The neural net expects batches as an input now.", "tokens": [50364, 20561, 264, 12523, 295, 264, 914, 293, 3832, 25163, 295, 257, 15245, 30, 50564, 50564, 440, 18161, 2533, 33280, 15245, 279, 382, 364, 4846, 586, 13, 50664, 50664, 407, 577, 360, 321, 3154, 294, 257, 2167, 1365, 293, 483, 25380, 3542, 484, 30, 50864, 50864, 400, 370, 264, 11494, 294, 264, 15245, 2710, 2144, 3035, 307, 264, 3480, 13, 51064, 51064, 708, 321, 576, 411, 281, 360, 510, 307, 321, 576, 411, 281, 1936, 362, 257, 1823, 934, 3097, 51364, 51364, 300, 4322, 1024, 293, 6352, 264, 15245, 914, 293, 3832, 25163, 257, 2167, 565, 670, 264, 3097, 992, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05338342831684993, "compression_ratio": 1.78, "no_speech_prob": 2.6424900170241017e-06}, {"id": 617, "seek": 326400, "start": 3270.0, "end": 3274.0, "text": " So how do we feed in a single example and get sensible results out?", "tokens": [50364, 20561, 264, 12523, 295, 264, 914, 293, 3832, 25163, 295, 257, 15245, 30, 50564, 50564, 440, 18161, 2533, 33280, 15245, 279, 382, 364, 4846, 586, 13, 50664, 50664, 407, 577, 360, 321, 3154, 294, 257, 2167, 1365, 293, 483, 25380, 3542, 484, 30, 50864, 50864, 400, 370, 264, 11494, 294, 264, 15245, 2710, 2144, 3035, 307, 264, 3480, 13, 51064, 51064, 708, 321, 576, 411, 281, 360, 510, 307, 321, 576, 411, 281, 1936, 362, 257, 1823, 934, 3097, 51364, 51364, 300, 4322, 1024, 293, 6352, 264, 15245, 914, 293, 3832, 25163, 257, 2167, 565, 670, 264, 3097, 992, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05338342831684993, "compression_ratio": 1.78, "no_speech_prob": 2.6424900170241017e-06}, {"id": 618, "seek": 326400, "start": 3274.0, "end": 3278.0, "text": " And so the proposal in the batch normalization paper is the following.", "tokens": [50364, 20561, 264, 12523, 295, 264, 914, 293, 3832, 25163, 295, 257, 15245, 30, 50564, 50564, 440, 18161, 2533, 33280, 15245, 279, 382, 364, 4846, 586, 13, 50664, 50664, 407, 577, 360, 321, 3154, 294, 257, 2167, 1365, 293, 483, 25380, 3542, 484, 30, 50864, 50864, 400, 370, 264, 11494, 294, 264, 15245, 2710, 2144, 3035, 307, 264, 3480, 13, 51064, 51064, 708, 321, 576, 411, 281, 360, 510, 307, 321, 576, 411, 281, 1936, 362, 257, 1823, 934, 3097, 51364, 51364, 300, 4322, 1024, 293, 6352, 264, 15245, 914, 293, 3832, 25163, 257, 2167, 565, 670, 264, 3097, 992, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05338342831684993, "compression_ratio": 1.78, "no_speech_prob": 2.6424900170241017e-06}, {"id": 619, "seek": 326400, "start": 3278.0, "end": 3284.0, "text": " What we would like to do here is we would like to basically have a step after training", "tokens": [50364, 20561, 264, 12523, 295, 264, 914, 293, 3832, 25163, 295, 257, 15245, 30, 50564, 50564, 440, 18161, 2533, 33280, 15245, 279, 382, 364, 4846, 586, 13, 50664, 50664, 407, 577, 360, 321, 3154, 294, 257, 2167, 1365, 293, 483, 25380, 3542, 484, 30, 50864, 50864, 400, 370, 264, 11494, 294, 264, 15245, 2710, 2144, 3035, 307, 264, 3480, 13, 51064, 51064, 708, 321, 576, 411, 281, 360, 510, 307, 321, 576, 411, 281, 1936, 362, 257, 1823, 934, 3097, 51364, 51364, 300, 4322, 1024, 293, 6352, 264, 15245, 914, 293, 3832, 25163, 257, 2167, 565, 670, 264, 3097, 992, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05338342831684993, "compression_ratio": 1.78, "no_speech_prob": 2.6424900170241017e-06}, {"id": 620, "seek": 326400, "start": 3284.0, "end": 3292.0, "text": " that calculates and sets the batch mean and standard deviation a single time over the training set.", "tokens": [50364, 20561, 264, 12523, 295, 264, 914, 293, 3832, 25163, 295, 257, 15245, 30, 50564, 50564, 440, 18161, 2533, 33280, 15245, 279, 382, 364, 4846, 586, 13, 50664, 50664, 407, 577, 360, 321, 3154, 294, 257, 2167, 1365, 293, 483, 25380, 3542, 484, 30, 50864, 50864, 400, 370, 264, 11494, 294, 264, 15245, 2710, 2144, 3035, 307, 264, 3480, 13, 51064, 51064, 708, 321, 576, 411, 281, 360, 510, 307, 321, 576, 411, 281, 1936, 362, 257, 1823, 934, 3097, 51364, 51364, 300, 4322, 1024, 293, 6352, 264, 15245, 914, 293, 3832, 25163, 257, 2167, 565, 670, 264, 3097, 992, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.05338342831684993, "compression_ratio": 1.78, "no_speech_prob": 2.6424900170241017e-06}, {"id": 621, "seek": 329200, "start": 3292.0, "end": 3295.0, "text": " And so I wrote this code here in the interest of time.", "tokens": [50364, 400, 370, 286, 4114, 341, 3089, 510, 294, 264, 1179, 295, 565, 13, 50514, 50514, 400, 321, 434, 516, 281, 818, 437, 311, 1219, 21583, 4404, 264, 15245, 2026, 12523, 13, 50714, 50714, 400, 1936, 437, 321, 360, 307, 27822, 406, 572, 2771, 3585, 9953, 51, 284, 339, 300, 6022, 295, 341, 321, 486, 818, 264, 5893, 23897, 322, 13, 51064, 51064, 400, 309, 311, 516, 281, 312, 257, 857, 544, 7148, 13, 51164, 51164, 492, 434, 516, 281, 747, 264, 3097, 992, 11, 483, 264, 659, 23397, 763, 337, 633, 2167, 3097, 1365, 11, 51414, 51414, 293, 550, 472, 2167, 565, 12539, 264, 914, 293, 3832, 25163, 670, 264, 2302, 3097, 992, 13, 51664, 51664, 400, 550, 321, 434, 516, 281, 483, 363, 293, 914, 293, 363, 293, 3832, 25163, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11209872189690084, "compression_ratio": 1.9125874125874125, "no_speech_prob": 2.0462002794374712e-05}, {"id": 622, "seek": 329200, "start": 3295.0, "end": 3299.0, "text": " And we're going to call what's called calibrate the batch norm statistics.", "tokens": [50364, 400, 370, 286, 4114, 341, 3089, 510, 294, 264, 1179, 295, 565, 13, 50514, 50514, 400, 321, 434, 516, 281, 818, 437, 311, 1219, 21583, 4404, 264, 15245, 2026, 12523, 13, 50714, 50714, 400, 1936, 437, 321, 360, 307, 27822, 406, 572, 2771, 3585, 9953, 51, 284, 339, 300, 6022, 295, 341, 321, 486, 818, 264, 5893, 23897, 322, 13, 51064, 51064, 400, 309, 311, 516, 281, 312, 257, 857, 544, 7148, 13, 51164, 51164, 492, 434, 516, 281, 747, 264, 3097, 992, 11, 483, 264, 659, 23397, 763, 337, 633, 2167, 3097, 1365, 11, 51414, 51414, 293, 550, 472, 2167, 565, 12539, 264, 914, 293, 3832, 25163, 670, 264, 2302, 3097, 992, 13, 51664, 51664, 400, 550, 321, 434, 516, 281, 483, 363, 293, 914, 293, 363, 293, 3832, 25163, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11209872189690084, "compression_ratio": 1.9125874125874125, "no_speech_prob": 2.0462002794374712e-05}, {"id": 623, "seek": 329200, "start": 3299.0, "end": 3306.0, "text": " And basically what we do is torch not no grad telling PyTorch that none of this we will call the dot backward on.", "tokens": [50364, 400, 370, 286, 4114, 341, 3089, 510, 294, 264, 1179, 295, 565, 13, 50514, 50514, 400, 321, 434, 516, 281, 818, 437, 311, 1219, 21583, 4404, 264, 15245, 2026, 12523, 13, 50714, 50714, 400, 1936, 437, 321, 360, 307, 27822, 406, 572, 2771, 3585, 9953, 51, 284, 339, 300, 6022, 295, 341, 321, 486, 818, 264, 5893, 23897, 322, 13, 51064, 51064, 400, 309, 311, 516, 281, 312, 257, 857, 544, 7148, 13, 51164, 51164, 492, 434, 516, 281, 747, 264, 3097, 992, 11, 483, 264, 659, 23397, 763, 337, 633, 2167, 3097, 1365, 11, 51414, 51414, 293, 550, 472, 2167, 565, 12539, 264, 914, 293, 3832, 25163, 670, 264, 2302, 3097, 992, 13, 51664, 51664, 400, 550, 321, 434, 516, 281, 483, 363, 293, 914, 293, 363, 293, 3832, 25163, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11209872189690084, "compression_ratio": 1.9125874125874125, "no_speech_prob": 2.0462002794374712e-05}, {"id": 624, "seek": 329200, "start": 3306.0, "end": 3308.0, "text": " And it's going to be a bit more efficient.", "tokens": [50364, 400, 370, 286, 4114, 341, 3089, 510, 294, 264, 1179, 295, 565, 13, 50514, 50514, 400, 321, 434, 516, 281, 818, 437, 311, 1219, 21583, 4404, 264, 15245, 2026, 12523, 13, 50714, 50714, 400, 1936, 437, 321, 360, 307, 27822, 406, 572, 2771, 3585, 9953, 51, 284, 339, 300, 6022, 295, 341, 321, 486, 818, 264, 5893, 23897, 322, 13, 51064, 51064, 400, 309, 311, 516, 281, 312, 257, 857, 544, 7148, 13, 51164, 51164, 492, 434, 516, 281, 747, 264, 3097, 992, 11, 483, 264, 659, 23397, 763, 337, 633, 2167, 3097, 1365, 11, 51414, 51414, 293, 550, 472, 2167, 565, 12539, 264, 914, 293, 3832, 25163, 670, 264, 2302, 3097, 992, 13, 51664, 51664, 400, 550, 321, 434, 516, 281, 483, 363, 293, 914, 293, 363, 293, 3832, 25163, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11209872189690084, "compression_ratio": 1.9125874125874125, "no_speech_prob": 2.0462002794374712e-05}, {"id": 625, "seek": 329200, "start": 3308.0, "end": 3313.0, "text": " We're going to take the training set, get the preactivations for every single training example,", "tokens": [50364, 400, 370, 286, 4114, 341, 3089, 510, 294, 264, 1179, 295, 565, 13, 50514, 50514, 400, 321, 434, 516, 281, 818, 437, 311, 1219, 21583, 4404, 264, 15245, 2026, 12523, 13, 50714, 50714, 400, 1936, 437, 321, 360, 307, 27822, 406, 572, 2771, 3585, 9953, 51, 284, 339, 300, 6022, 295, 341, 321, 486, 818, 264, 5893, 23897, 322, 13, 51064, 51064, 400, 309, 311, 516, 281, 312, 257, 857, 544, 7148, 13, 51164, 51164, 492, 434, 516, 281, 747, 264, 3097, 992, 11, 483, 264, 659, 23397, 763, 337, 633, 2167, 3097, 1365, 11, 51414, 51414, 293, 550, 472, 2167, 565, 12539, 264, 914, 293, 3832, 25163, 670, 264, 2302, 3097, 992, 13, 51664, 51664, 400, 550, 321, 434, 516, 281, 483, 363, 293, 914, 293, 363, 293, 3832, 25163, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11209872189690084, "compression_ratio": 1.9125874125874125, "no_speech_prob": 2.0462002794374712e-05}, {"id": 626, "seek": 329200, "start": 3313.0, "end": 3318.0, "text": " and then one single time estimate the mean and standard deviation over the entire training set.", "tokens": [50364, 400, 370, 286, 4114, 341, 3089, 510, 294, 264, 1179, 295, 565, 13, 50514, 50514, 400, 321, 434, 516, 281, 818, 437, 311, 1219, 21583, 4404, 264, 15245, 2026, 12523, 13, 50714, 50714, 400, 1936, 437, 321, 360, 307, 27822, 406, 572, 2771, 3585, 9953, 51, 284, 339, 300, 6022, 295, 341, 321, 486, 818, 264, 5893, 23897, 322, 13, 51064, 51064, 400, 309, 311, 516, 281, 312, 257, 857, 544, 7148, 13, 51164, 51164, 492, 434, 516, 281, 747, 264, 3097, 992, 11, 483, 264, 659, 23397, 763, 337, 633, 2167, 3097, 1365, 11, 51414, 51414, 293, 550, 472, 2167, 565, 12539, 264, 914, 293, 3832, 25163, 670, 264, 2302, 3097, 992, 13, 51664, 51664, 400, 550, 321, 434, 516, 281, 483, 363, 293, 914, 293, 363, 293, 3832, 25163, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11209872189690084, "compression_ratio": 1.9125874125874125, "no_speech_prob": 2.0462002794374712e-05}, {"id": 627, "seek": 329200, "start": 3318.0, "end": 3321.0, "text": " And then we're going to get B and mean and B and standard deviation.", "tokens": [50364, 400, 370, 286, 4114, 341, 3089, 510, 294, 264, 1179, 295, 565, 13, 50514, 50514, 400, 321, 434, 516, 281, 818, 437, 311, 1219, 21583, 4404, 264, 15245, 2026, 12523, 13, 50714, 50714, 400, 1936, 437, 321, 360, 307, 27822, 406, 572, 2771, 3585, 9953, 51, 284, 339, 300, 6022, 295, 341, 321, 486, 818, 264, 5893, 23897, 322, 13, 51064, 51064, 400, 309, 311, 516, 281, 312, 257, 857, 544, 7148, 13, 51164, 51164, 492, 434, 516, 281, 747, 264, 3097, 992, 11, 483, 264, 659, 23397, 763, 337, 633, 2167, 3097, 1365, 11, 51414, 51414, 293, 550, 472, 2167, 565, 12539, 264, 914, 293, 3832, 25163, 670, 264, 2302, 3097, 992, 13, 51664, 51664, 400, 550, 321, 434, 516, 281, 483, 363, 293, 914, 293, 363, 293, 3832, 25163, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11209872189690084, "compression_ratio": 1.9125874125874125, "no_speech_prob": 2.0462002794374712e-05}, {"id": 628, "seek": 332100, "start": 3321.0, "end": 3325.0, "text": " And now these are fixed numbers estimating over the entire training set.", "tokens": [50364, 400, 586, 613, 366, 6806, 3547, 8017, 990, 670, 264, 2302, 3097, 992, 13, 50564, 50564, 400, 510, 11, 2602, 295, 8017, 990, 309, 43492, 11, 321, 366, 516, 281, 2602, 510, 764, 363, 293, 914, 13, 51014, 51014, 400, 510, 321, 434, 445, 516, 281, 764, 363, 293, 3832, 25163, 13, 51214, 51214, 400, 370, 412, 1500, 565, 11, 321, 366, 516, 281, 3191, 613, 11, 17690, 552, 293, 764, 552, 1830, 38253, 13, 51464, 51464, 400, 586, 291, 536, 300, 321, 483, 1936, 14800, 1874, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07847597288048785, "compression_ratio": 1.78743961352657, "no_speech_prob": 2.260234623463475e-06}, {"id": 629, "seek": 332100, "start": 3325.0, "end": 3334.0, "text": " And here, instead of estimating it dynamically, we are going to instead here use B and mean.", "tokens": [50364, 400, 586, 613, 366, 6806, 3547, 8017, 990, 670, 264, 2302, 3097, 992, 13, 50564, 50564, 400, 510, 11, 2602, 295, 8017, 990, 309, 43492, 11, 321, 366, 516, 281, 2602, 510, 764, 363, 293, 914, 13, 51014, 51014, 400, 510, 321, 434, 445, 516, 281, 764, 363, 293, 3832, 25163, 13, 51214, 51214, 400, 370, 412, 1500, 565, 11, 321, 366, 516, 281, 3191, 613, 11, 17690, 552, 293, 764, 552, 1830, 38253, 13, 51464, 51464, 400, 586, 291, 536, 300, 321, 483, 1936, 14800, 1874, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07847597288048785, "compression_ratio": 1.78743961352657, "no_speech_prob": 2.260234623463475e-06}, {"id": 630, "seek": 332100, "start": 3334.0, "end": 3338.0, "text": " And here we're just going to use B and standard deviation.", "tokens": [50364, 400, 586, 613, 366, 6806, 3547, 8017, 990, 670, 264, 2302, 3097, 992, 13, 50564, 50564, 400, 510, 11, 2602, 295, 8017, 990, 309, 43492, 11, 321, 366, 516, 281, 2602, 510, 764, 363, 293, 914, 13, 51014, 51014, 400, 510, 321, 434, 445, 516, 281, 764, 363, 293, 3832, 25163, 13, 51214, 51214, 400, 370, 412, 1500, 565, 11, 321, 366, 516, 281, 3191, 613, 11, 17690, 552, 293, 764, 552, 1830, 38253, 13, 51464, 51464, 400, 586, 291, 536, 300, 321, 483, 1936, 14800, 1874, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07847597288048785, "compression_ratio": 1.78743961352657, "no_speech_prob": 2.260234623463475e-06}, {"id": 631, "seek": 332100, "start": 3338.0, "end": 3343.0, "text": " And so at test time, we are going to fix these, clamp them and use them during inference.", "tokens": [50364, 400, 586, 613, 366, 6806, 3547, 8017, 990, 670, 264, 2302, 3097, 992, 13, 50564, 50564, 400, 510, 11, 2602, 295, 8017, 990, 309, 43492, 11, 321, 366, 516, 281, 2602, 510, 764, 363, 293, 914, 13, 51014, 51014, 400, 510, 321, 434, 445, 516, 281, 764, 363, 293, 3832, 25163, 13, 51214, 51214, 400, 370, 412, 1500, 565, 11, 321, 366, 516, 281, 3191, 613, 11, 17690, 552, 293, 764, 552, 1830, 38253, 13, 51464, 51464, 400, 586, 291, 536, 300, 321, 483, 1936, 14800, 1874, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07847597288048785, "compression_ratio": 1.78743961352657, "no_speech_prob": 2.260234623463475e-06}, {"id": 632, "seek": 332100, "start": 3343.0, "end": 3349.0, "text": " And now you see that we get basically identical result.", "tokens": [50364, 400, 586, 613, 366, 6806, 3547, 8017, 990, 670, 264, 2302, 3097, 992, 13, 50564, 50564, 400, 510, 11, 2602, 295, 8017, 990, 309, 43492, 11, 321, 366, 516, 281, 2602, 510, 764, 363, 293, 914, 13, 51014, 51014, 400, 510, 321, 434, 445, 516, 281, 764, 363, 293, 3832, 25163, 13, 51214, 51214, 400, 370, 412, 1500, 565, 11, 321, 366, 516, 281, 3191, 613, 11, 17690, 552, 293, 764, 552, 1830, 38253, 13, 51464, 51464, 400, 586, 291, 536, 300, 321, 483, 1936, 14800, 1874, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07847597288048785, "compression_ratio": 1.78743961352657, "no_speech_prob": 2.260234623463475e-06}, {"id": 633, "seek": 334900, "start": 3349.0, "end": 3353.0, "text": " But the benefit that we've gained is that we can now also forward a single example", "tokens": [50364, 583, 264, 5121, 300, 321, 600, 12634, 307, 300, 321, 393, 586, 611, 2128, 257, 2167, 1365, 50564, 50564, 570, 264, 914, 293, 3832, 25163, 366, 586, 6806, 1333, 295, 10688, 830, 13, 50764, 50764, 663, 848, 11, 5079, 767, 2738, 281, 12539, 341, 914, 293, 3832, 25163, 382, 257, 1150, 3233, 934, 18161, 3209, 3097, 51114, 51114, 570, 1518, 307, 14847, 13, 51214, 51214, 400, 370, 341, 15245, 2710, 2144, 3035, 767, 7268, 472, 544, 1558, 11, 51364, 51364, 597, 307, 300, 321, 393, 12539, 264, 914, 293, 3832, 25163, 294, 257, 2614, 9060, 1830, 3097, 295, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0506765752468469, "compression_ratio": 1.9186046511627908, "no_speech_prob": 1.706045622995589e-06}, {"id": 634, "seek": 334900, "start": 3353.0, "end": 3357.0, "text": " because the mean and standard deviation are now fixed sort of tensors.", "tokens": [50364, 583, 264, 5121, 300, 321, 600, 12634, 307, 300, 321, 393, 586, 611, 2128, 257, 2167, 1365, 50564, 50564, 570, 264, 914, 293, 3832, 25163, 366, 586, 6806, 1333, 295, 10688, 830, 13, 50764, 50764, 663, 848, 11, 5079, 767, 2738, 281, 12539, 341, 914, 293, 3832, 25163, 382, 257, 1150, 3233, 934, 18161, 3209, 3097, 51114, 51114, 570, 1518, 307, 14847, 13, 51214, 51214, 400, 370, 341, 15245, 2710, 2144, 3035, 767, 7268, 472, 544, 1558, 11, 51364, 51364, 597, 307, 300, 321, 393, 12539, 264, 914, 293, 3832, 25163, 294, 257, 2614, 9060, 1830, 3097, 295, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0506765752468469, "compression_ratio": 1.9186046511627908, "no_speech_prob": 1.706045622995589e-06}, {"id": 635, "seek": 334900, "start": 3357.0, "end": 3364.0, "text": " That said, nobody actually wants to estimate this mean and standard deviation as a second stage after neural network training", "tokens": [50364, 583, 264, 5121, 300, 321, 600, 12634, 307, 300, 321, 393, 586, 611, 2128, 257, 2167, 1365, 50564, 50564, 570, 264, 914, 293, 3832, 25163, 366, 586, 6806, 1333, 295, 10688, 830, 13, 50764, 50764, 663, 848, 11, 5079, 767, 2738, 281, 12539, 341, 914, 293, 3832, 25163, 382, 257, 1150, 3233, 934, 18161, 3209, 3097, 51114, 51114, 570, 1518, 307, 14847, 13, 51214, 51214, 400, 370, 341, 15245, 2710, 2144, 3035, 767, 7268, 472, 544, 1558, 11, 51364, 51364, 597, 307, 300, 321, 393, 12539, 264, 914, 293, 3832, 25163, 294, 257, 2614, 9060, 1830, 3097, 295, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0506765752468469, "compression_ratio": 1.9186046511627908, "no_speech_prob": 1.706045622995589e-06}, {"id": 636, "seek": 334900, "start": 3364.0, "end": 3366.0, "text": " because everyone is lazy.", "tokens": [50364, 583, 264, 5121, 300, 321, 600, 12634, 307, 300, 321, 393, 586, 611, 2128, 257, 2167, 1365, 50564, 50564, 570, 264, 914, 293, 3832, 25163, 366, 586, 6806, 1333, 295, 10688, 830, 13, 50764, 50764, 663, 848, 11, 5079, 767, 2738, 281, 12539, 341, 914, 293, 3832, 25163, 382, 257, 1150, 3233, 934, 18161, 3209, 3097, 51114, 51114, 570, 1518, 307, 14847, 13, 51214, 51214, 400, 370, 341, 15245, 2710, 2144, 3035, 767, 7268, 472, 544, 1558, 11, 51364, 51364, 597, 307, 300, 321, 393, 12539, 264, 914, 293, 3832, 25163, 294, 257, 2614, 9060, 1830, 3097, 295, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0506765752468469, "compression_ratio": 1.9186046511627908, "no_speech_prob": 1.706045622995589e-06}, {"id": 637, "seek": 334900, "start": 3366.0, "end": 3369.0, "text": " And so this batch normalization paper actually introduced one more idea,", "tokens": [50364, 583, 264, 5121, 300, 321, 600, 12634, 307, 300, 321, 393, 586, 611, 2128, 257, 2167, 1365, 50564, 50564, 570, 264, 914, 293, 3832, 25163, 366, 586, 6806, 1333, 295, 10688, 830, 13, 50764, 50764, 663, 848, 11, 5079, 767, 2738, 281, 12539, 341, 914, 293, 3832, 25163, 382, 257, 1150, 3233, 934, 18161, 3209, 3097, 51114, 51114, 570, 1518, 307, 14847, 13, 51214, 51214, 400, 370, 341, 15245, 2710, 2144, 3035, 767, 7268, 472, 544, 1558, 11, 51364, 51364, 597, 307, 300, 321, 393, 12539, 264, 914, 293, 3832, 25163, 294, 257, 2614, 9060, 1830, 3097, 295, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0506765752468469, "compression_ratio": 1.9186046511627908, "no_speech_prob": 1.706045622995589e-06}, {"id": 638, "seek": 334900, "start": 3369.0, "end": 3377.0, "text": " which is that we can estimate the mean and standard deviation in a running manner during training of the neural net.", "tokens": [50364, 583, 264, 5121, 300, 321, 600, 12634, 307, 300, 321, 393, 586, 611, 2128, 257, 2167, 1365, 50564, 50564, 570, 264, 914, 293, 3832, 25163, 366, 586, 6806, 1333, 295, 10688, 830, 13, 50764, 50764, 663, 848, 11, 5079, 767, 2738, 281, 12539, 341, 914, 293, 3832, 25163, 382, 257, 1150, 3233, 934, 18161, 3209, 3097, 51114, 51114, 570, 1518, 307, 14847, 13, 51214, 51214, 400, 370, 341, 15245, 2710, 2144, 3035, 767, 7268, 472, 544, 1558, 11, 51364, 51364, 597, 307, 300, 321, 393, 12539, 264, 914, 293, 3832, 25163, 294, 257, 2614, 9060, 1830, 3097, 295, 264, 18161, 2533, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0506765752468469, "compression_ratio": 1.9186046511627908, "no_speech_prob": 1.706045622995589e-06}, {"id": 639, "seek": 337700, "start": 3377.0, "end": 3380.0, "text": " And then we can simply just have a single stage of training.", "tokens": [50364, 400, 550, 321, 393, 2935, 445, 362, 257, 2167, 3233, 295, 3097, 13, 50514, 50514, 400, 322, 264, 1252, 295, 300, 3097, 11, 321, 366, 8017, 990, 264, 2614, 914, 293, 3832, 25163, 13, 50714, 50714, 407, 718, 311, 536, 437, 300, 576, 574, 411, 13, 50814, 50814, 961, 385, 1936, 747, 264, 914, 510, 300, 321, 366, 8017, 990, 322, 264, 15245, 13, 51014, 51014, 400, 718, 385, 818, 341, 363, 293, 914, 322, 264, 741, 12, 392, 24784, 13, 51264, 51264, 400, 550, 510, 341, 307, 363, 293, 4904, 35, 13, 51564, 51564, 363, 293, 4904, 35, 412, 741, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10623091571735886, "compression_ratio": 1.7452830188679245, "no_speech_prob": 7.0716882873966824e-06}, {"id": 640, "seek": 337700, "start": 3380.0, "end": 3384.0, "text": " And on the side of that training, we are estimating the running mean and standard deviation.", "tokens": [50364, 400, 550, 321, 393, 2935, 445, 362, 257, 2167, 3233, 295, 3097, 13, 50514, 50514, 400, 322, 264, 1252, 295, 300, 3097, 11, 321, 366, 8017, 990, 264, 2614, 914, 293, 3832, 25163, 13, 50714, 50714, 407, 718, 311, 536, 437, 300, 576, 574, 411, 13, 50814, 50814, 961, 385, 1936, 747, 264, 914, 510, 300, 321, 366, 8017, 990, 322, 264, 15245, 13, 51014, 51014, 400, 718, 385, 818, 341, 363, 293, 914, 322, 264, 741, 12, 392, 24784, 13, 51264, 51264, 400, 550, 510, 341, 307, 363, 293, 4904, 35, 13, 51564, 51564, 363, 293, 4904, 35, 412, 741, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10623091571735886, "compression_ratio": 1.7452830188679245, "no_speech_prob": 7.0716882873966824e-06}, {"id": 641, "seek": 337700, "start": 3384.0, "end": 3386.0, "text": " So let's see what that would look like.", "tokens": [50364, 400, 550, 321, 393, 2935, 445, 362, 257, 2167, 3233, 295, 3097, 13, 50514, 50514, 400, 322, 264, 1252, 295, 300, 3097, 11, 321, 366, 8017, 990, 264, 2614, 914, 293, 3832, 25163, 13, 50714, 50714, 407, 718, 311, 536, 437, 300, 576, 574, 411, 13, 50814, 50814, 961, 385, 1936, 747, 264, 914, 510, 300, 321, 366, 8017, 990, 322, 264, 15245, 13, 51014, 51014, 400, 718, 385, 818, 341, 363, 293, 914, 322, 264, 741, 12, 392, 24784, 13, 51264, 51264, 400, 550, 510, 341, 307, 363, 293, 4904, 35, 13, 51564, 51564, 363, 293, 4904, 35, 412, 741, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10623091571735886, "compression_ratio": 1.7452830188679245, "no_speech_prob": 7.0716882873966824e-06}, {"id": 642, "seek": 337700, "start": 3386.0, "end": 3390.0, "text": " Let me basically take the mean here that we are estimating on the batch.", "tokens": [50364, 400, 550, 321, 393, 2935, 445, 362, 257, 2167, 3233, 295, 3097, 13, 50514, 50514, 400, 322, 264, 1252, 295, 300, 3097, 11, 321, 366, 8017, 990, 264, 2614, 914, 293, 3832, 25163, 13, 50714, 50714, 407, 718, 311, 536, 437, 300, 576, 574, 411, 13, 50814, 50814, 961, 385, 1936, 747, 264, 914, 510, 300, 321, 366, 8017, 990, 322, 264, 15245, 13, 51014, 51014, 400, 718, 385, 818, 341, 363, 293, 914, 322, 264, 741, 12, 392, 24784, 13, 51264, 51264, 400, 550, 510, 341, 307, 363, 293, 4904, 35, 13, 51564, 51564, 363, 293, 4904, 35, 412, 741, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10623091571735886, "compression_ratio": 1.7452830188679245, "no_speech_prob": 7.0716882873966824e-06}, {"id": 643, "seek": 337700, "start": 3390.0, "end": 3395.0, "text": " And let me call this B and mean on the i-th iteration.", "tokens": [50364, 400, 550, 321, 393, 2935, 445, 362, 257, 2167, 3233, 295, 3097, 13, 50514, 50514, 400, 322, 264, 1252, 295, 300, 3097, 11, 321, 366, 8017, 990, 264, 2614, 914, 293, 3832, 25163, 13, 50714, 50714, 407, 718, 311, 536, 437, 300, 576, 574, 411, 13, 50814, 50814, 961, 385, 1936, 747, 264, 914, 510, 300, 321, 366, 8017, 990, 322, 264, 15245, 13, 51014, 51014, 400, 718, 385, 818, 341, 363, 293, 914, 322, 264, 741, 12, 392, 24784, 13, 51264, 51264, 400, 550, 510, 341, 307, 363, 293, 4904, 35, 13, 51564, 51564, 363, 293, 4904, 35, 412, 741, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10623091571735886, "compression_ratio": 1.7452830188679245, "no_speech_prob": 7.0716882873966824e-06}, {"id": 644, "seek": 337700, "start": 3395.0, "end": 3401.0, "text": " And then here this is B and STD.", "tokens": [50364, 400, 550, 321, 393, 2935, 445, 362, 257, 2167, 3233, 295, 3097, 13, 50514, 50514, 400, 322, 264, 1252, 295, 300, 3097, 11, 321, 366, 8017, 990, 264, 2614, 914, 293, 3832, 25163, 13, 50714, 50714, 407, 718, 311, 536, 437, 300, 576, 574, 411, 13, 50814, 50814, 961, 385, 1936, 747, 264, 914, 510, 300, 321, 366, 8017, 990, 322, 264, 15245, 13, 51014, 51014, 400, 718, 385, 818, 341, 363, 293, 914, 322, 264, 741, 12, 392, 24784, 13, 51264, 51264, 400, 550, 510, 341, 307, 363, 293, 4904, 35, 13, 51564, 51564, 363, 293, 4904, 35, 412, 741, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10623091571735886, "compression_ratio": 1.7452830188679245, "no_speech_prob": 7.0716882873966824e-06}, {"id": 645, "seek": 337700, "start": 3401.0, "end": 3403.0, "text": " B and STD at i.", "tokens": [50364, 400, 550, 321, 393, 2935, 445, 362, 257, 2167, 3233, 295, 3097, 13, 50514, 50514, 400, 322, 264, 1252, 295, 300, 3097, 11, 321, 366, 8017, 990, 264, 2614, 914, 293, 3832, 25163, 13, 50714, 50714, 407, 718, 311, 536, 437, 300, 576, 574, 411, 13, 50814, 50814, 961, 385, 1936, 747, 264, 914, 510, 300, 321, 366, 8017, 990, 322, 264, 15245, 13, 51014, 51014, 400, 718, 385, 818, 341, 363, 293, 914, 322, 264, 741, 12, 392, 24784, 13, 51264, 51264, 400, 550, 510, 341, 307, 363, 293, 4904, 35, 13, 51564, 51564, 363, 293, 4904, 35, 412, 741, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10623091571735886, "compression_ratio": 1.7452830188679245, "no_speech_prob": 7.0716882873966824e-06}, {"id": 646, "seek": 340300, "start": 3403.0, "end": 3413.0, "text": " And the mean comes here and the STD comes here.", "tokens": [50364, 400, 264, 914, 1487, 510, 293, 264, 4904, 35, 1487, 510, 13, 50864, 50864, 407, 370, 1400, 286, 600, 1096, 1825, 13, 50914, 50914, 286, 600, 445, 4259, 926, 293, 286, 2942, 613, 2857, 9102, 337, 264, 914, 293, 3832, 25163, 13, 51114, 51114, 400, 286, 600, 829, 552, 510, 13, 51164, 51164, 407, 370, 1400, 1825, 575, 3105, 13, 51264, 51264, 583, 437, 321, 434, 516, 281, 360, 586, 307, 321, 434, 516, 281, 1066, 257, 2614, 914, 295, 1293, 295, 613, 4190, 1830, 3097, 13, 51514, 51514, 407, 718, 385, 11173, 493, 510, 293, 718, 385, 1884, 257, 363, 45, 914, 37556, 2614, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1011295491998846, "compression_ratio": 1.7705627705627707, "no_speech_prob": 1.3006406334170606e-05}, {"id": 647, "seek": 340300, "start": 3413.0, "end": 3414.0, "text": " So so far I've done nothing.", "tokens": [50364, 400, 264, 914, 1487, 510, 293, 264, 4904, 35, 1487, 510, 13, 50864, 50864, 407, 370, 1400, 286, 600, 1096, 1825, 13, 50914, 50914, 286, 600, 445, 4259, 926, 293, 286, 2942, 613, 2857, 9102, 337, 264, 914, 293, 3832, 25163, 13, 51114, 51114, 400, 286, 600, 829, 552, 510, 13, 51164, 51164, 407, 370, 1400, 1825, 575, 3105, 13, 51264, 51264, 583, 437, 321, 434, 516, 281, 360, 586, 307, 321, 434, 516, 281, 1066, 257, 2614, 914, 295, 1293, 295, 613, 4190, 1830, 3097, 13, 51514, 51514, 407, 718, 385, 11173, 493, 510, 293, 718, 385, 1884, 257, 363, 45, 914, 37556, 2614, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1011295491998846, "compression_ratio": 1.7705627705627707, "no_speech_prob": 1.3006406334170606e-05}, {"id": 648, "seek": 340300, "start": 3414.0, "end": 3418.0, "text": " I've just moved around and I created these extra variables for the mean and standard deviation.", "tokens": [50364, 400, 264, 914, 1487, 510, 293, 264, 4904, 35, 1487, 510, 13, 50864, 50864, 407, 370, 1400, 286, 600, 1096, 1825, 13, 50914, 50914, 286, 600, 445, 4259, 926, 293, 286, 2942, 613, 2857, 9102, 337, 264, 914, 293, 3832, 25163, 13, 51114, 51114, 400, 286, 600, 829, 552, 510, 13, 51164, 51164, 407, 370, 1400, 1825, 575, 3105, 13, 51264, 51264, 583, 437, 321, 434, 516, 281, 360, 586, 307, 321, 434, 516, 281, 1066, 257, 2614, 914, 295, 1293, 295, 613, 4190, 1830, 3097, 13, 51514, 51514, 407, 718, 385, 11173, 493, 510, 293, 718, 385, 1884, 257, 363, 45, 914, 37556, 2614, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1011295491998846, "compression_ratio": 1.7705627705627707, "no_speech_prob": 1.3006406334170606e-05}, {"id": 649, "seek": 340300, "start": 3418.0, "end": 3419.0, "text": " And I've put them here.", "tokens": [50364, 400, 264, 914, 1487, 510, 293, 264, 4904, 35, 1487, 510, 13, 50864, 50864, 407, 370, 1400, 286, 600, 1096, 1825, 13, 50914, 50914, 286, 600, 445, 4259, 926, 293, 286, 2942, 613, 2857, 9102, 337, 264, 914, 293, 3832, 25163, 13, 51114, 51114, 400, 286, 600, 829, 552, 510, 13, 51164, 51164, 407, 370, 1400, 1825, 575, 3105, 13, 51264, 51264, 583, 437, 321, 434, 516, 281, 360, 586, 307, 321, 434, 516, 281, 1066, 257, 2614, 914, 295, 1293, 295, 613, 4190, 1830, 3097, 13, 51514, 51514, 407, 718, 385, 11173, 493, 510, 293, 718, 385, 1884, 257, 363, 45, 914, 37556, 2614, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1011295491998846, "compression_ratio": 1.7705627705627707, "no_speech_prob": 1.3006406334170606e-05}, {"id": 650, "seek": 340300, "start": 3419.0, "end": 3421.0, "text": " So so far nothing has changed.", "tokens": [50364, 400, 264, 914, 1487, 510, 293, 264, 4904, 35, 1487, 510, 13, 50864, 50864, 407, 370, 1400, 286, 600, 1096, 1825, 13, 50914, 50914, 286, 600, 445, 4259, 926, 293, 286, 2942, 613, 2857, 9102, 337, 264, 914, 293, 3832, 25163, 13, 51114, 51114, 400, 286, 600, 829, 552, 510, 13, 51164, 51164, 407, 370, 1400, 1825, 575, 3105, 13, 51264, 51264, 583, 437, 321, 434, 516, 281, 360, 586, 307, 321, 434, 516, 281, 1066, 257, 2614, 914, 295, 1293, 295, 613, 4190, 1830, 3097, 13, 51514, 51514, 407, 718, 385, 11173, 493, 510, 293, 718, 385, 1884, 257, 363, 45, 914, 37556, 2614, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1011295491998846, "compression_ratio": 1.7705627705627707, "no_speech_prob": 1.3006406334170606e-05}, {"id": 651, "seek": 340300, "start": 3421.0, "end": 3426.0, "text": " But what we're going to do now is we're going to keep a running mean of both of these values during training.", "tokens": [50364, 400, 264, 914, 1487, 510, 293, 264, 4904, 35, 1487, 510, 13, 50864, 50864, 407, 370, 1400, 286, 600, 1096, 1825, 13, 50914, 50914, 286, 600, 445, 4259, 926, 293, 286, 2942, 613, 2857, 9102, 337, 264, 914, 293, 3832, 25163, 13, 51114, 51114, 400, 286, 600, 829, 552, 510, 13, 51164, 51164, 407, 370, 1400, 1825, 575, 3105, 13, 51264, 51264, 583, 437, 321, 434, 516, 281, 360, 586, 307, 321, 434, 516, 281, 1066, 257, 2614, 914, 295, 1293, 295, 613, 4190, 1830, 3097, 13, 51514, 51514, 407, 718, 385, 11173, 493, 510, 293, 718, 385, 1884, 257, 363, 45, 914, 37556, 2614, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1011295491998846, "compression_ratio": 1.7705627705627707, "no_speech_prob": 1.3006406334170606e-05}, {"id": 652, "seek": 340300, "start": 3426.0, "end": 3431.0, "text": " So let me swing up here and let me create a BN mean underscore running.", "tokens": [50364, 400, 264, 914, 1487, 510, 293, 264, 4904, 35, 1487, 510, 13, 50864, 50864, 407, 370, 1400, 286, 600, 1096, 1825, 13, 50914, 50914, 286, 600, 445, 4259, 926, 293, 286, 2942, 613, 2857, 9102, 337, 264, 914, 293, 3832, 25163, 13, 51114, 51114, 400, 286, 600, 829, 552, 510, 13, 51164, 51164, 407, 370, 1400, 1825, 575, 3105, 13, 51264, 51264, 583, 437, 321, 434, 516, 281, 360, 586, 307, 321, 434, 516, 281, 1066, 257, 2614, 914, 295, 1293, 295, 613, 4190, 1830, 3097, 13, 51514, 51514, 407, 718, 385, 11173, 493, 510, 293, 718, 385, 1884, 257, 363, 45, 914, 37556, 2614, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1011295491998846, "compression_ratio": 1.7705627705627707, "no_speech_prob": 1.3006406334170606e-05}, {"id": 653, "seek": 343100, "start": 3431.0, "end": 3443.0, "text": " And I'm going to initialize it at zeros and then B and STD running, which are initialized at once.", "tokens": [50364, 400, 286, 478, 516, 281, 5883, 1125, 309, 412, 35193, 293, 550, 363, 293, 4904, 35, 2614, 11, 597, 366, 5883, 1602, 412, 1564, 13, 50964, 50964, 1436, 294, 264, 2863, 11, 570, 295, 264, 636, 321, 5883, 1602, 343, 16, 293, 363, 16, 11, 389, 79, 32, 55, 486, 312, 9810, 4985, 39148, 13, 51364, 51364, 407, 264, 914, 486, 312, 9810, 4018, 293, 264, 3832, 25163, 9810, 472, 13, 51514, 51514, 407, 286, 478, 516, 281, 5883, 1125, 613, 300, 636, 13, 51664, 51664, 583, 550, 510, 286, 478, 516, 281, 5623, 613, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12358170509338379, "compression_ratio": 1.7658536585365854, "no_speech_prob": 9.367975508212112e-06}, {"id": 654, "seek": 343100, "start": 3443.0, "end": 3451.0, "text": " Because in the beginning, because of the way we initialized W1 and B1, HpAX will be roughly unit Gaussian.", "tokens": [50364, 400, 286, 478, 516, 281, 5883, 1125, 309, 412, 35193, 293, 550, 363, 293, 4904, 35, 2614, 11, 597, 366, 5883, 1602, 412, 1564, 13, 50964, 50964, 1436, 294, 264, 2863, 11, 570, 295, 264, 636, 321, 5883, 1602, 343, 16, 293, 363, 16, 11, 389, 79, 32, 55, 486, 312, 9810, 4985, 39148, 13, 51364, 51364, 407, 264, 914, 486, 312, 9810, 4018, 293, 264, 3832, 25163, 9810, 472, 13, 51514, 51514, 407, 286, 478, 516, 281, 5883, 1125, 613, 300, 636, 13, 51664, 51664, 583, 550, 510, 286, 478, 516, 281, 5623, 613, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12358170509338379, "compression_ratio": 1.7658536585365854, "no_speech_prob": 9.367975508212112e-06}, {"id": 655, "seek": 343100, "start": 3451.0, "end": 3454.0, "text": " So the mean will be roughly zero and the standard deviation roughly one.", "tokens": [50364, 400, 286, 478, 516, 281, 5883, 1125, 309, 412, 35193, 293, 550, 363, 293, 4904, 35, 2614, 11, 597, 366, 5883, 1602, 412, 1564, 13, 50964, 50964, 1436, 294, 264, 2863, 11, 570, 295, 264, 636, 321, 5883, 1602, 343, 16, 293, 363, 16, 11, 389, 79, 32, 55, 486, 312, 9810, 4985, 39148, 13, 51364, 51364, 407, 264, 914, 486, 312, 9810, 4018, 293, 264, 3832, 25163, 9810, 472, 13, 51514, 51514, 407, 286, 478, 516, 281, 5883, 1125, 613, 300, 636, 13, 51664, 51664, 583, 550, 510, 286, 478, 516, 281, 5623, 613, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12358170509338379, "compression_ratio": 1.7658536585365854, "no_speech_prob": 9.367975508212112e-06}, {"id": 656, "seek": 343100, "start": 3454.0, "end": 3457.0, "text": " So I'm going to initialize these that way.", "tokens": [50364, 400, 286, 478, 516, 281, 5883, 1125, 309, 412, 35193, 293, 550, 363, 293, 4904, 35, 2614, 11, 597, 366, 5883, 1602, 412, 1564, 13, 50964, 50964, 1436, 294, 264, 2863, 11, 570, 295, 264, 636, 321, 5883, 1602, 343, 16, 293, 363, 16, 11, 389, 79, 32, 55, 486, 312, 9810, 4985, 39148, 13, 51364, 51364, 407, 264, 914, 486, 312, 9810, 4018, 293, 264, 3832, 25163, 9810, 472, 13, 51514, 51514, 407, 286, 478, 516, 281, 5883, 1125, 613, 300, 636, 13, 51664, 51664, 583, 550, 510, 286, 478, 516, 281, 5623, 613, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12358170509338379, "compression_ratio": 1.7658536585365854, "no_speech_prob": 9.367975508212112e-06}, {"id": 657, "seek": 343100, "start": 3457.0, "end": 3459.0, "text": " But then here I'm going to update these.", "tokens": [50364, 400, 286, 478, 516, 281, 5883, 1125, 309, 412, 35193, 293, 550, 363, 293, 4904, 35, 2614, 11, 597, 366, 5883, 1602, 412, 1564, 13, 50964, 50964, 1436, 294, 264, 2863, 11, 570, 295, 264, 636, 321, 5883, 1602, 343, 16, 293, 363, 16, 11, 389, 79, 32, 55, 486, 312, 9810, 4985, 39148, 13, 51364, 51364, 407, 264, 914, 486, 312, 9810, 4018, 293, 264, 3832, 25163, 9810, 472, 13, 51514, 51514, 407, 286, 478, 516, 281, 5883, 1125, 613, 300, 636, 13, 51664, 51664, 583, 550, 510, 286, 478, 516, 281, 5623, 613, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12358170509338379, "compression_ratio": 1.7658536585365854, "no_speech_prob": 9.367975508212112e-06}, {"id": 658, "seek": 345900, "start": 3459.0, "end": 3468.0, "text": " And in PyTorch, these mean and standard deviation that are running, they're not actually part of the gradient based optimization.", "tokens": [50364, 400, 294, 9953, 51, 284, 339, 11, 613, 914, 293, 3832, 25163, 300, 366, 2614, 11, 436, 434, 406, 767, 644, 295, 264, 16235, 2361, 19618, 13, 50814, 50814, 492, 434, 1128, 516, 281, 28446, 2771, 2448, 365, 3104, 281, 552, 13, 50914, 50914, 814, 434, 436, 434, 10588, 322, 264, 1252, 295, 3097, 13, 51064, 51064, 400, 370, 437, 321, 434, 516, 281, 360, 510, 307, 321, 434, 516, 281, 584, 365, 27822, 13, 77, 664, 6206, 3585, 9953, 51, 284, 339, 300, 264, 5623, 510, 307, 406, 3442, 281, 312, 2390, 484, 257, 4295, 570, 456, 486, 312, 572, 5893, 23897, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08693233242741337, "compression_ratio": 1.7489878542510122, "no_speech_prob": 5.6822709666448645e-06}, {"id": 659, "seek": 345900, "start": 3468.0, "end": 3470.0, "text": " We're never going to derive gradients with respect to them.", "tokens": [50364, 400, 294, 9953, 51, 284, 339, 11, 613, 914, 293, 3832, 25163, 300, 366, 2614, 11, 436, 434, 406, 767, 644, 295, 264, 16235, 2361, 19618, 13, 50814, 50814, 492, 434, 1128, 516, 281, 28446, 2771, 2448, 365, 3104, 281, 552, 13, 50914, 50914, 814, 434, 436, 434, 10588, 322, 264, 1252, 295, 3097, 13, 51064, 51064, 400, 370, 437, 321, 434, 516, 281, 360, 510, 307, 321, 434, 516, 281, 584, 365, 27822, 13, 77, 664, 6206, 3585, 9953, 51, 284, 339, 300, 264, 5623, 510, 307, 406, 3442, 281, 312, 2390, 484, 257, 4295, 570, 456, 486, 312, 572, 5893, 23897, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08693233242741337, "compression_ratio": 1.7489878542510122, "no_speech_prob": 5.6822709666448645e-06}, {"id": 660, "seek": 345900, "start": 3470.0, "end": 3473.0, "text": " They're they're updated on the side of training.", "tokens": [50364, 400, 294, 9953, 51, 284, 339, 11, 613, 914, 293, 3832, 25163, 300, 366, 2614, 11, 436, 434, 406, 767, 644, 295, 264, 16235, 2361, 19618, 13, 50814, 50814, 492, 434, 1128, 516, 281, 28446, 2771, 2448, 365, 3104, 281, 552, 13, 50914, 50914, 814, 434, 436, 434, 10588, 322, 264, 1252, 295, 3097, 13, 51064, 51064, 400, 370, 437, 321, 434, 516, 281, 360, 510, 307, 321, 434, 516, 281, 584, 365, 27822, 13, 77, 664, 6206, 3585, 9953, 51, 284, 339, 300, 264, 5623, 510, 307, 406, 3442, 281, 312, 2390, 484, 257, 4295, 570, 456, 486, 312, 572, 5893, 23897, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08693233242741337, "compression_ratio": 1.7489878542510122, "no_speech_prob": 5.6822709666448645e-06}, {"id": 661, "seek": 345900, "start": 3473.0, "end": 3485.0, "text": " And so what we're going to do here is we're going to say with torch.nograd telling PyTorch that the update here is not supposed to be building out a graph because there will be no dot backward.", "tokens": [50364, 400, 294, 9953, 51, 284, 339, 11, 613, 914, 293, 3832, 25163, 300, 366, 2614, 11, 436, 434, 406, 767, 644, 295, 264, 16235, 2361, 19618, 13, 50814, 50814, 492, 434, 1128, 516, 281, 28446, 2771, 2448, 365, 3104, 281, 552, 13, 50914, 50914, 814, 434, 436, 434, 10588, 322, 264, 1252, 295, 3097, 13, 51064, 51064, 400, 370, 437, 321, 434, 516, 281, 360, 510, 307, 321, 434, 516, 281, 584, 365, 27822, 13, 77, 664, 6206, 3585, 9953, 51, 284, 339, 300, 264, 5623, 510, 307, 406, 3442, 281, 312, 2390, 484, 257, 4295, 570, 456, 486, 312, 572, 5893, 23897, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08693233242741337, "compression_ratio": 1.7489878542510122, "no_speech_prob": 5.6822709666448645e-06}, {"id": 662, "seek": 348500, "start": 3485.0, "end": 3500.0, "text": " But this running mean is basically going to be zero point nine nine nine times the current value plus zero point zero zero one times the this value, this new mean.", "tokens": [50364, 583, 341, 2614, 914, 307, 1936, 516, 281, 312, 4018, 935, 4949, 4949, 4949, 1413, 264, 2190, 2158, 1804, 4018, 935, 4018, 4018, 472, 1413, 264, 341, 2158, 11, 341, 777, 914, 13, 51114, 51114, 400, 294, 264, 912, 636, 11, 363, 45, 4904, 35, 2614, 486, 312, 5240, 437, 309, 1143, 281, 312, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12134146286269366, "compression_ratio": 1.5918367346938775, "no_speech_prob": 7.527803063567262e-06}, {"id": 663, "seek": 348500, "start": 3500.0, "end": 3508.0, "text": " And in the same way, BN STD running will be mostly what it used to be.", "tokens": [50364, 583, 341, 2614, 914, 307, 1936, 516, 281, 312, 4018, 935, 4949, 4949, 4949, 1413, 264, 2190, 2158, 1804, 4018, 935, 4018, 4018, 472, 1413, 264, 341, 2158, 11, 341, 777, 914, 13, 51114, 51114, 400, 294, 264, 912, 636, 11, 363, 45, 4904, 35, 2614, 486, 312, 5240, 437, 309, 1143, 281, 312, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.12134146286269366, "compression_ratio": 1.5918367346938775, "no_speech_prob": 7.527803063567262e-06}, {"id": 664, "seek": 350800, "start": 3508.0, "end": 3515.0, "text": " But it will receive a small update in the direction of what the current standard deviation is.", "tokens": [50364, 583, 309, 486, 4774, 257, 1359, 5623, 294, 264, 3513, 295, 437, 264, 2190, 3832, 25163, 307, 13, 50714, 50714, 400, 382, 291, 434, 2577, 510, 11, 341, 5623, 307, 2380, 293, 322, 264, 1252, 295, 264, 16235, 2361, 19618, 13, 51014, 51014, 400, 309, 311, 2935, 885, 10588, 11, 406, 1228, 16235, 23475, 13, 51164, 51164, 467, 311, 445, 885, 10588, 1228, 257, 3632, 2984, 411, 5508, 1333, 295, 2614, 914, 9060, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0819226167140863, "compression_ratio": 1.645320197044335, "no_speech_prob": 8.990238598016731e-07}, {"id": 665, "seek": 350800, "start": 3515.0, "end": 3521.0, "text": " And as you're seeing here, this update is outside and on the side of the gradient based optimization.", "tokens": [50364, 583, 309, 486, 4774, 257, 1359, 5623, 294, 264, 3513, 295, 437, 264, 2190, 3832, 25163, 307, 13, 50714, 50714, 400, 382, 291, 434, 2577, 510, 11, 341, 5623, 307, 2380, 293, 322, 264, 1252, 295, 264, 16235, 2361, 19618, 13, 51014, 51014, 400, 309, 311, 2935, 885, 10588, 11, 406, 1228, 16235, 23475, 13, 51164, 51164, 467, 311, 445, 885, 10588, 1228, 257, 3632, 2984, 411, 5508, 1333, 295, 2614, 914, 9060, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0819226167140863, "compression_ratio": 1.645320197044335, "no_speech_prob": 8.990238598016731e-07}, {"id": 666, "seek": 350800, "start": 3521.0, "end": 3524.0, "text": " And it's simply being updated, not using gradient descent.", "tokens": [50364, 583, 309, 486, 4774, 257, 1359, 5623, 294, 264, 3513, 295, 437, 264, 2190, 3832, 25163, 307, 13, 50714, 50714, 400, 382, 291, 434, 2577, 510, 11, 341, 5623, 307, 2380, 293, 322, 264, 1252, 295, 264, 16235, 2361, 19618, 13, 51014, 51014, 400, 309, 311, 2935, 885, 10588, 11, 406, 1228, 16235, 23475, 13, 51164, 51164, 467, 311, 445, 885, 10588, 1228, 257, 3632, 2984, 411, 5508, 1333, 295, 2614, 914, 9060, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0819226167140863, "compression_ratio": 1.645320197044335, "no_speech_prob": 8.990238598016731e-07}, {"id": 667, "seek": 350800, "start": 3524.0, "end": 3533.0, "text": " It's just being updated using a Genki like smooth sort of running mean manner.", "tokens": [50364, 583, 309, 486, 4774, 257, 1359, 5623, 294, 264, 3513, 295, 437, 264, 2190, 3832, 25163, 307, 13, 50714, 50714, 400, 382, 291, 434, 2577, 510, 11, 341, 5623, 307, 2380, 293, 322, 264, 1252, 295, 264, 16235, 2361, 19618, 13, 51014, 51014, 400, 309, 311, 2935, 885, 10588, 11, 406, 1228, 16235, 23475, 13, 51164, 51164, 467, 311, 445, 885, 10588, 1228, 257, 3632, 2984, 411, 5508, 1333, 295, 2614, 914, 9060, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0819226167140863, "compression_ratio": 1.645320197044335, "no_speech_prob": 8.990238598016731e-07}, {"id": 668, "seek": 353300, "start": 3533.0, "end": 3543.0, "text": " And so while the network is training and these pre activations are sort of changing and shifting around during during back propagation, we are keeping track of the typical mean and standard deviation.", "tokens": [50364, 400, 370, 1339, 264, 3209, 307, 3097, 293, 613, 659, 2430, 763, 366, 1333, 295, 4473, 293, 17573, 926, 1830, 1830, 646, 38377, 11, 321, 366, 5145, 2837, 295, 264, 7476, 914, 293, 3832, 25163, 13, 50864, 50864, 400, 321, 434, 8017, 990, 552, 1564, 13, 50964, 50964, 400, 562, 286, 1190, 341, 11, 586, 286, 478, 5145, 2837, 295, 341, 294, 257, 2614, 9060, 13, 51314, 51314, 400, 437, 321, 434, 7159, 337, 11, 295, 1164, 11, 307, 300, 264, 363, 45, 914, 37556, 2614, 293, 363, 45, 914, 37556, 4904, 35, 366, 516, 281, 312, 588, 2531, 281, 264, 2306, 300, 321, 15598, 510, 949, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0915174228804452, "compression_ratio": 1.801498127340824, "no_speech_prob": 1.8161381376557983e-06}, {"id": 669, "seek": 353300, "start": 3543.0, "end": 3545.0, "text": " And we're estimating them once.", "tokens": [50364, 400, 370, 1339, 264, 3209, 307, 3097, 293, 613, 659, 2430, 763, 366, 1333, 295, 4473, 293, 17573, 926, 1830, 1830, 646, 38377, 11, 321, 366, 5145, 2837, 295, 264, 7476, 914, 293, 3832, 25163, 13, 50864, 50864, 400, 321, 434, 8017, 990, 552, 1564, 13, 50964, 50964, 400, 562, 286, 1190, 341, 11, 586, 286, 478, 5145, 2837, 295, 341, 294, 257, 2614, 9060, 13, 51314, 51314, 400, 437, 321, 434, 7159, 337, 11, 295, 1164, 11, 307, 300, 264, 363, 45, 914, 37556, 2614, 293, 363, 45, 914, 37556, 4904, 35, 366, 516, 281, 312, 588, 2531, 281, 264, 2306, 300, 321, 15598, 510, 949, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0915174228804452, "compression_ratio": 1.801498127340824, "no_speech_prob": 1.8161381376557983e-06}, {"id": 670, "seek": 353300, "start": 3545.0, "end": 3552.0, "text": " And when I run this, now I'm keeping track of this in a running manner.", "tokens": [50364, 400, 370, 1339, 264, 3209, 307, 3097, 293, 613, 659, 2430, 763, 366, 1333, 295, 4473, 293, 17573, 926, 1830, 1830, 646, 38377, 11, 321, 366, 5145, 2837, 295, 264, 7476, 914, 293, 3832, 25163, 13, 50864, 50864, 400, 321, 434, 8017, 990, 552, 1564, 13, 50964, 50964, 400, 562, 286, 1190, 341, 11, 586, 286, 478, 5145, 2837, 295, 341, 294, 257, 2614, 9060, 13, 51314, 51314, 400, 437, 321, 434, 7159, 337, 11, 295, 1164, 11, 307, 300, 264, 363, 45, 914, 37556, 2614, 293, 363, 45, 914, 37556, 4904, 35, 366, 516, 281, 312, 588, 2531, 281, 264, 2306, 300, 321, 15598, 510, 949, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0915174228804452, "compression_ratio": 1.801498127340824, "no_speech_prob": 1.8161381376557983e-06}, {"id": 671, "seek": 353300, "start": 3552.0, "end": 3562.0, "text": " And what we're hoping for, of course, is that the BN mean underscore running and BN mean underscore STD are going to be very similar to the ones that we calculated here before.", "tokens": [50364, 400, 370, 1339, 264, 3209, 307, 3097, 293, 613, 659, 2430, 763, 366, 1333, 295, 4473, 293, 17573, 926, 1830, 1830, 646, 38377, 11, 321, 366, 5145, 2837, 295, 264, 7476, 914, 293, 3832, 25163, 13, 50864, 50864, 400, 321, 434, 8017, 990, 552, 1564, 13, 50964, 50964, 400, 562, 286, 1190, 341, 11, 586, 286, 478, 5145, 2837, 295, 341, 294, 257, 2614, 9060, 13, 51314, 51314, 400, 437, 321, 434, 7159, 337, 11, 295, 1164, 11, 307, 300, 264, 363, 45, 914, 37556, 2614, 293, 363, 45, 914, 37556, 4904, 35, 366, 516, 281, 312, 588, 2531, 281, 264, 2306, 300, 321, 15598, 510, 949, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0915174228804452, "compression_ratio": 1.801498127340824, "no_speech_prob": 1.8161381376557983e-06}, {"id": 672, "seek": 356200, "start": 3562.0, "end": 3568.0, "text": " And that way we don't need a second stage because we've sort of combined the two stages and we've put them on the side of each other.", "tokens": [50364, 400, 300, 636, 321, 500, 380, 643, 257, 1150, 3233, 570, 321, 600, 1333, 295, 9354, 264, 732, 10232, 293, 321, 600, 829, 552, 322, 264, 1252, 295, 1184, 661, 13, 50664, 50664, 759, 291, 528, 281, 574, 412, 309, 300, 636, 13, 50764, 50764, 400, 341, 307, 577, 341, 307, 611, 12270, 294, 264, 15245, 2710, 2144, 4583, 294, 9953, 51, 284, 339, 13, 51014, 51014, 407, 1830, 3097, 11, 264, 1900, 912, 551, 486, 1051, 13, 51214, 51214, 400, 550, 1780, 562, 291, 434, 1228, 38253, 11, 309, 486, 764, 264, 14109, 2614, 914, 295, 1293, 264, 914, 293, 3832, 25163, 295, 729, 7633, 4368, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06259826251438685, "compression_ratio": 1.6703296703296704, "no_speech_prob": 1.3209273674874566e-05}, {"id": 673, "seek": 356200, "start": 3568.0, "end": 3570.0, "text": " If you want to look at it that way.", "tokens": [50364, 400, 300, 636, 321, 500, 380, 643, 257, 1150, 3233, 570, 321, 600, 1333, 295, 9354, 264, 732, 10232, 293, 321, 600, 829, 552, 322, 264, 1252, 295, 1184, 661, 13, 50664, 50664, 759, 291, 528, 281, 574, 412, 309, 300, 636, 13, 50764, 50764, 400, 341, 307, 577, 341, 307, 611, 12270, 294, 264, 15245, 2710, 2144, 4583, 294, 9953, 51, 284, 339, 13, 51014, 51014, 407, 1830, 3097, 11, 264, 1900, 912, 551, 486, 1051, 13, 51214, 51214, 400, 550, 1780, 562, 291, 434, 1228, 38253, 11, 309, 486, 764, 264, 14109, 2614, 914, 295, 1293, 264, 914, 293, 3832, 25163, 295, 729, 7633, 4368, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06259826251438685, "compression_ratio": 1.6703296703296704, "no_speech_prob": 1.3209273674874566e-05}, {"id": 674, "seek": 356200, "start": 3570.0, "end": 3575.0, "text": " And this is how this is also implemented in the batch normalization layer in PyTorch.", "tokens": [50364, 400, 300, 636, 321, 500, 380, 643, 257, 1150, 3233, 570, 321, 600, 1333, 295, 9354, 264, 732, 10232, 293, 321, 600, 829, 552, 322, 264, 1252, 295, 1184, 661, 13, 50664, 50664, 759, 291, 528, 281, 574, 412, 309, 300, 636, 13, 50764, 50764, 400, 341, 307, 577, 341, 307, 611, 12270, 294, 264, 15245, 2710, 2144, 4583, 294, 9953, 51, 284, 339, 13, 51014, 51014, 407, 1830, 3097, 11, 264, 1900, 912, 551, 486, 1051, 13, 51214, 51214, 400, 550, 1780, 562, 291, 434, 1228, 38253, 11, 309, 486, 764, 264, 14109, 2614, 914, 295, 1293, 264, 914, 293, 3832, 25163, 295, 729, 7633, 4368, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06259826251438685, "compression_ratio": 1.6703296703296704, "no_speech_prob": 1.3209273674874566e-05}, {"id": 675, "seek": 356200, "start": 3575.0, "end": 3579.0, "text": " So during training, the exact same thing will happen.", "tokens": [50364, 400, 300, 636, 321, 500, 380, 643, 257, 1150, 3233, 570, 321, 600, 1333, 295, 9354, 264, 732, 10232, 293, 321, 600, 829, 552, 322, 264, 1252, 295, 1184, 661, 13, 50664, 50664, 759, 291, 528, 281, 574, 412, 309, 300, 636, 13, 50764, 50764, 400, 341, 307, 577, 341, 307, 611, 12270, 294, 264, 15245, 2710, 2144, 4583, 294, 9953, 51, 284, 339, 13, 51014, 51014, 407, 1830, 3097, 11, 264, 1900, 912, 551, 486, 1051, 13, 51214, 51214, 400, 550, 1780, 562, 291, 434, 1228, 38253, 11, 309, 486, 764, 264, 14109, 2614, 914, 295, 1293, 264, 914, 293, 3832, 25163, 295, 729, 7633, 4368, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06259826251438685, "compression_ratio": 1.6703296703296704, "no_speech_prob": 1.3209273674874566e-05}, {"id": 676, "seek": 356200, "start": 3579.0, "end": 3587.0, "text": " And then later when you're using inference, it will use the estimated running mean of both the mean and standard deviation of those hidden states.", "tokens": [50364, 400, 300, 636, 321, 500, 380, 643, 257, 1150, 3233, 570, 321, 600, 1333, 295, 9354, 264, 732, 10232, 293, 321, 600, 829, 552, 322, 264, 1252, 295, 1184, 661, 13, 50664, 50664, 759, 291, 528, 281, 574, 412, 309, 300, 636, 13, 50764, 50764, 400, 341, 307, 577, 341, 307, 611, 12270, 294, 264, 15245, 2710, 2144, 4583, 294, 9953, 51, 284, 339, 13, 51014, 51014, 407, 1830, 3097, 11, 264, 1900, 912, 551, 486, 1051, 13, 51214, 51214, 400, 550, 1780, 562, 291, 434, 1228, 38253, 11, 309, 486, 764, 264, 14109, 2614, 914, 295, 1293, 264, 914, 293, 3832, 25163, 295, 729, 7633, 4368, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06259826251438685, "compression_ratio": 1.6703296703296704, "no_speech_prob": 1.3209273674874566e-05}, {"id": 677, "seek": 358700, "start": 3587.0, "end": 3593.0, "text": " So let's wait for the optimization to converge and hopefully the running mean and standard deviation are roughly equal to these two.", "tokens": [50364, 407, 718, 311, 1699, 337, 264, 19618, 281, 41881, 293, 4696, 264, 2614, 914, 293, 3832, 25163, 366, 9810, 2681, 281, 613, 732, 13, 50664, 50664, 400, 550, 321, 393, 2935, 764, 309, 510, 13, 50764, 50764, 400, 321, 500, 380, 643, 341, 3233, 295, 13691, 38732, 412, 264, 917, 13, 50964, 50964, 2264, 11, 370, 264, 19618, 4335, 13, 51064, 51064, 286, 603, 43819, 409, 264, 13691, 35701, 13, 51164, 51164, 400, 550, 264, 363, 45, 914, 490, 264, 13691, 35701, 307, 510, 13, 51364, 51364, 400, 363, 45, 914, 490, 264, 2614, 35701, 1830, 264, 1830, 264, 19618, 11, 291, 393, 536, 307, 588, 11, 588, 2531, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08011499204133686, "compression_ratio": 1.907258064516129, "no_speech_prob": 3.6687395095214015e-06}, {"id": 678, "seek": 358700, "start": 3593.0, "end": 3595.0, "text": " And then we can simply use it here.", "tokens": [50364, 407, 718, 311, 1699, 337, 264, 19618, 281, 41881, 293, 4696, 264, 2614, 914, 293, 3832, 25163, 366, 9810, 2681, 281, 613, 732, 13, 50664, 50664, 400, 550, 321, 393, 2935, 764, 309, 510, 13, 50764, 50764, 400, 321, 500, 380, 643, 341, 3233, 295, 13691, 38732, 412, 264, 917, 13, 50964, 50964, 2264, 11, 370, 264, 19618, 4335, 13, 51064, 51064, 286, 603, 43819, 409, 264, 13691, 35701, 13, 51164, 51164, 400, 550, 264, 363, 45, 914, 490, 264, 13691, 35701, 307, 510, 13, 51364, 51364, 400, 363, 45, 914, 490, 264, 2614, 35701, 1830, 264, 1830, 264, 19618, 11, 291, 393, 536, 307, 588, 11, 588, 2531, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08011499204133686, "compression_ratio": 1.907258064516129, "no_speech_prob": 3.6687395095214015e-06}, {"id": 679, "seek": 358700, "start": 3595.0, "end": 3599.0, "text": " And we don't need this stage of explicit calibration at the end.", "tokens": [50364, 407, 718, 311, 1699, 337, 264, 19618, 281, 41881, 293, 4696, 264, 2614, 914, 293, 3832, 25163, 366, 9810, 2681, 281, 613, 732, 13, 50664, 50664, 400, 550, 321, 393, 2935, 764, 309, 510, 13, 50764, 50764, 400, 321, 500, 380, 643, 341, 3233, 295, 13691, 38732, 412, 264, 917, 13, 50964, 50964, 2264, 11, 370, 264, 19618, 4335, 13, 51064, 51064, 286, 603, 43819, 409, 264, 13691, 35701, 13, 51164, 51164, 400, 550, 264, 363, 45, 914, 490, 264, 13691, 35701, 307, 510, 13, 51364, 51364, 400, 363, 45, 914, 490, 264, 2614, 35701, 1830, 264, 1830, 264, 19618, 11, 291, 393, 536, 307, 588, 11, 588, 2531, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08011499204133686, "compression_ratio": 1.907258064516129, "no_speech_prob": 3.6687395095214015e-06}, {"id": 680, "seek": 358700, "start": 3599.0, "end": 3601.0, "text": " OK, so the optimization finished.", "tokens": [50364, 407, 718, 311, 1699, 337, 264, 19618, 281, 41881, 293, 4696, 264, 2614, 914, 293, 3832, 25163, 366, 9810, 2681, 281, 613, 732, 13, 50664, 50664, 400, 550, 321, 393, 2935, 764, 309, 510, 13, 50764, 50764, 400, 321, 500, 380, 643, 341, 3233, 295, 13691, 38732, 412, 264, 917, 13, 50964, 50964, 2264, 11, 370, 264, 19618, 4335, 13, 51064, 51064, 286, 603, 43819, 409, 264, 13691, 35701, 13, 51164, 51164, 400, 550, 264, 363, 45, 914, 490, 264, 13691, 35701, 307, 510, 13, 51364, 51364, 400, 363, 45, 914, 490, 264, 2614, 35701, 1830, 264, 1830, 264, 19618, 11, 291, 393, 536, 307, 588, 11, 588, 2531, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08011499204133686, "compression_ratio": 1.907258064516129, "no_speech_prob": 3.6687395095214015e-06}, {"id": 681, "seek": 358700, "start": 3601.0, "end": 3603.0, "text": " I'll rerun the explicit estimation.", "tokens": [50364, 407, 718, 311, 1699, 337, 264, 19618, 281, 41881, 293, 4696, 264, 2614, 914, 293, 3832, 25163, 366, 9810, 2681, 281, 613, 732, 13, 50664, 50664, 400, 550, 321, 393, 2935, 764, 309, 510, 13, 50764, 50764, 400, 321, 500, 380, 643, 341, 3233, 295, 13691, 38732, 412, 264, 917, 13, 50964, 50964, 2264, 11, 370, 264, 19618, 4335, 13, 51064, 51064, 286, 603, 43819, 409, 264, 13691, 35701, 13, 51164, 51164, 400, 550, 264, 363, 45, 914, 490, 264, 13691, 35701, 307, 510, 13, 51364, 51364, 400, 363, 45, 914, 490, 264, 2614, 35701, 1830, 264, 1830, 264, 19618, 11, 291, 393, 536, 307, 588, 11, 588, 2531, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08011499204133686, "compression_ratio": 1.907258064516129, "no_speech_prob": 3.6687395095214015e-06}, {"id": 682, "seek": 358700, "start": 3603.0, "end": 3607.0, "text": " And then the BN mean from the explicit estimation is here.", "tokens": [50364, 407, 718, 311, 1699, 337, 264, 19618, 281, 41881, 293, 4696, 264, 2614, 914, 293, 3832, 25163, 366, 9810, 2681, 281, 613, 732, 13, 50664, 50664, 400, 550, 321, 393, 2935, 764, 309, 510, 13, 50764, 50764, 400, 321, 500, 380, 643, 341, 3233, 295, 13691, 38732, 412, 264, 917, 13, 50964, 50964, 2264, 11, 370, 264, 19618, 4335, 13, 51064, 51064, 286, 603, 43819, 409, 264, 13691, 35701, 13, 51164, 51164, 400, 550, 264, 363, 45, 914, 490, 264, 13691, 35701, 307, 510, 13, 51364, 51364, 400, 363, 45, 914, 490, 264, 2614, 35701, 1830, 264, 1830, 264, 19618, 11, 291, 393, 536, 307, 588, 11, 588, 2531, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08011499204133686, "compression_ratio": 1.907258064516129, "no_speech_prob": 3.6687395095214015e-06}, {"id": 683, "seek": 358700, "start": 3607.0, "end": 3615.0, "text": " And BN mean from the running estimation during the during the optimization, you can see is very, very similar.", "tokens": [50364, 407, 718, 311, 1699, 337, 264, 19618, 281, 41881, 293, 4696, 264, 2614, 914, 293, 3832, 25163, 366, 9810, 2681, 281, 613, 732, 13, 50664, 50664, 400, 550, 321, 393, 2935, 764, 309, 510, 13, 50764, 50764, 400, 321, 500, 380, 643, 341, 3233, 295, 13691, 38732, 412, 264, 917, 13, 50964, 50964, 2264, 11, 370, 264, 19618, 4335, 13, 51064, 51064, 286, 603, 43819, 409, 264, 13691, 35701, 13, 51164, 51164, 400, 550, 264, 363, 45, 914, 490, 264, 13691, 35701, 307, 510, 13, 51364, 51364, 400, 363, 45, 914, 490, 264, 2614, 35701, 1830, 264, 1830, 264, 19618, 11, 291, 393, 536, 307, 588, 11, 588, 2531, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08011499204133686, "compression_ratio": 1.907258064516129, "no_speech_prob": 3.6687395095214015e-06}, {"id": 684, "seek": 361500, "start": 3615.0, "end": 3618.0, "text": " It's not identical, but it's pretty close.", "tokens": [50364, 467, 311, 406, 14800, 11, 457, 309, 311, 1238, 1998, 13, 50514, 50514, 400, 264, 912, 636, 363, 45, 6840, 35, 307, 341, 293, 363, 45, 6840, 35, 2614, 307, 341, 13, 50864, 50864, 1018, 291, 393, 536, 300, 1564, 797, 11, 436, 366, 6457, 2531, 4190, 11, 406, 14800, 11, 457, 1238, 1998, 13, 51164, 51164, 400, 370, 550, 510, 2602, 295, 363, 45, 914, 11, 321, 393, 764, 264, 363, 45, 914, 2614, 2602, 295, 363, 45, 6840, 35, 13, 51414, 51414, 492, 393, 764, 363, 45, 6840, 35, 2614, 13, 51564, 51564, 400, 4696, 264, 24071, 4470, 486, 406, 312, 15653, 886, 709, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10808227298496005, "compression_ratio": 1.7395348837209301, "no_speech_prob": 1.0451125490362756e-05}, {"id": 685, "seek": 361500, "start": 3618.0, "end": 3625.0, "text": " And the same way BNSTD is this and BNSTD running is this.", "tokens": [50364, 467, 311, 406, 14800, 11, 457, 309, 311, 1238, 1998, 13, 50514, 50514, 400, 264, 912, 636, 363, 45, 6840, 35, 307, 341, 293, 363, 45, 6840, 35, 2614, 307, 341, 13, 50864, 50864, 1018, 291, 393, 536, 300, 1564, 797, 11, 436, 366, 6457, 2531, 4190, 11, 406, 14800, 11, 457, 1238, 1998, 13, 51164, 51164, 400, 370, 550, 510, 2602, 295, 363, 45, 914, 11, 321, 393, 764, 264, 363, 45, 914, 2614, 2602, 295, 363, 45, 6840, 35, 13, 51414, 51414, 492, 393, 764, 363, 45, 6840, 35, 2614, 13, 51564, 51564, 400, 4696, 264, 24071, 4470, 486, 406, 312, 15653, 886, 709, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10808227298496005, "compression_ratio": 1.7395348837209301, "no_speech_prob": 1.0451125490362756e-05}, {"id": 686, "seek": 361500, "start": 3625.0, "end": 3631.0, "text": " As you can see that once again, they are fairly similar values, not identical, but pretty close.", "tokens": [50364, 467, 311, 406, 14800, 11, 457, 309, 311, 1238, 1998, 13, 50514, 50514, 400, 264, 912, 636, 363, 45, 6840, 35, 307, 341, 293, 363, 45, 6840, 35, 2614, 307, 341, 13, 50864, 50864, 1018, 291, 393, 536, 300, 1564, 797, 11, 436, 366, 6457, 2531, 4190, 11, 406, 14800, 11, 457, 1238, 1998, 13, 51164, 51164, 400, 370, 550, 510, 2602, 295, 363, 45, 914, 11, 321, 393, 764, 264, 363, 45, 914, 2614, 2602, 295, 363, 45, 6840, 35, 13, 51414, 51414, 492, 393, 764, 363, 45, 6840, 35, 2614, 13, 51564, 51564, 400, 4696, 264, 24071, 4470, 486, 406, 312, 15653, 886, 709, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10808227298496005, "compression_ratio": 1.7395348837209301, "no_speech_prob": 1.0451125490362756e-05}, {"id": 687, "seek": 361500, "start": 3631.0, "end": 3636.0, "text": " And so then here instead of BN mean, we can use the BN mean running instead of BNSTD.", "tokens": [50364, 467, 311, 406, 14800, 11, 457, 309, 311, 1238, 1998, 13, 50514, 50514, 400, 264, 912, 636, 363, 45, 6840, 35, 307, 341, 293, 363, 45, 6840, 35, 2614, 307, 341, 13, 50864, 50864, 1018, 291, 393, 536, 300, 1564, 797, 11, 436, 366, 6457, 2531, 4190, 11, 406, 14800, 11, 457, 1238, 1998, 13, 51164, 51164, 400, 370, 550, 510, 2602, 295, 363, 45, 914, 11, 321, 393, 764, 264, 363, 45, 914, 2614, 2602, 295, 363, 45, 6840, 35, 13, 51414, 51414, 492, 393, 764, 363, 45, 6840, 35, 2614, 13, 51564, 51564, 400, 4696, 264, 24071, 4470, 486, 406, 312, 15653, 886, 709, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10808227298496005, "compression_ratio": 1.7395348837209301, "no_speech_prob": 1.0451125490362756e-05}, {"id": 688, "seek": 361500, "start": 3636.0, "end": 3639.0, "text": " We can use BNSTD running.", "tokens": [50364, 467, 311, 406, 14800, 11, 457, 309, 311, 1238, 1998, 13, 50514, 50514, 400, 264, 912, 636, 363, 45, 6840, 35, 307, 341, 293, 363, 45, 6840, 35, 2614, 307, 341, 13, 50864, 50864, 1018, 291, 393, 536, 300, 1564, 797, 11, 436, 366, 6457, 2531, 4190, 11, 406, 14800, 11, 457, 1238, 1998, 13, 51164, 51164, 400, 370, 550, 510, 2602, 295, 363, 45, 914, 11, 321, 393, 764, 264, 363, 45, 914, 2614, 2602, 295, 363, 45, 6840, 35, 13, 51414, 51414, 492, 393, 764, 363, 45, 6840, 35, 2614, 13, 51564, 51564, 400, 4696, 264, 24071, 4470, 486, 406, 312, 15653, 886, 709, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10808227298496005, "compression_ratio": 1.7395348837209301, "no_speech_prob": 1.0451125490362756e-05}, {"id": 689, "seek": 361500, "start": 3639.0, "end": 3643.0, "text": " And hopefully the validation loss will not be impacted too much.", "tokens": [50364, 467, 311, 406, 14800, 11, 457, 309, 311, 1238, 1998, 13, 50514, 50514, 400, 264, 912, 636, 363, 45, 6840, 35, 307, 341, 293, 363, 45, 6840, 35, 2614, 307, 341, 13, 50864, 50864, 1018, 291, 393, 536, 300, 1564, 797, 11, 436, 366, 6457, 2531, 4190, 11, 406, 14800, 11, 457, 1238, 1998, 13, 51164, 51164, 400, 370, 550, 510, 2602, 295, 363, 45, 914, 11, 321, 393, 764, 264, 363, 45, 914, 2614, 2602, 295, 363, 45, 6840, 35, 13, 51414, 51414, 492, 393, 764, 363, 45, 6840, 35, 2614, 13, 51564, 51564, 400, 4696, 264, 24071, 4470, 486, 406, 312, 15653, 886, 709, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10808227298496005, "compression_ratio": 1.7395348837209301, "no_speech_prob": 1.0451125490362756e-05}, {"id": 690, "seek": 364300, "start": 3643.0, "end": 3646.0, "text": " So basically identical.", "tokens": [50364, 407, 1936, 14800, 13, 50514, 50514, 400, 341, 636, 321, 600, 20308, 264, 643, 337, 341, 13691, 3233, 295, 38732, 570, 321, 366, 884, 309, 294, 1622, 670, 510, 13, 50914, 50914, 2264, 11, 370, 321, 434, 1920, 1096, 365, 15245, 2710, 2144, 13, 51014, 51014, 821, 366, 787, 732, 544, 5570, 300, 286, 1116, 411, 281, 652, 13, 51114, 51114, 5118, 472, 11, 286, 600, 30193, 257, 5017, 670, 437, 307, 341, 1804, 17889, 884, 510, 13, 51314, 51314, 639, 17889, 307, 2673, 411, 512, 1359, 6806, 1230, 11, 337, 1365, 11, 502, 68, 3671, 1025, 538, 7576, 13, 51564, 51564, 400, 437, 309, 311, 884, 307, 300, 309, 311, 1936, 19965, 257, 10044, 538, 4018, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0828112539697866, "compression_ratio": 1.6533333333333333, "no_speech_prob": 6.048644536349457e-06}, {"id": 691, "seek": 364300, "start": 3646.0, "end": 3654.0, "text": " And this way we've eliminated the need for this explicit stage of calibration because we are doing it in line over here.", "tokens": [50364, 407, 1936, 14800, 13, 50514, 50514, 400, 341, 636, 321, 600, 20308, 264, 643, 337, 341, 13691, 3233, 295, 38732, 570, 321, 366, 884, 309, 294, 1622, 670, 510, 13, 50914, 50914, 2264, 11, 370, 321, 434, 1920, 1096, 365, 15245, 2710, 2144, 13, 51014, 51014, 821, 366, 787, 732, 544, 5570, 300, 286, 1116, 411, 281, 652, 13, 51114, 51114, 5118, 472, 11, 286, 600, 30193, 257, 5017, 670, 437, 307, 341, 1804, 17889, 884, 510, 13, 51314, 51314, 639, 17889, 307, 2673, 411, 512, 1359, 6806, 1230, 11, 337, 1365, 11, 502, 68, 3671, 1025, 538, 7576, 13, 51564, 51564, 400, 437, 309, 311, 884, 307, 300, 309, 311, 1936, 19965, 257, 10044, 538, 4018, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0828112539697866, "compression_ratio": 1.6533333333333333, "no_speech_prob": 6.048644536349457e-06}, {"id": 692, "seek": 364300, "start": 3654.0, "end": 3656.0, "text": " OK, so we're almost done with batch normalization.", "tokens": [50364, 407, 1936, 14800, 13, 50514, 50514, 400, 341, 636, 321, 600, 20308, 264, 643, 337, 341, 13691, 3233, 295, 38732, 570, 321, 366, 884, 309, 294, 1622, 670, 510, 13, 50914, 50914, 2264, 11, 370, 321, 434, 1920, 1096, 365, 15245, 2710, 2144, 13, 51014, 51014, 821, 366, 787, 732, 544, 5570, 300, 286, 1116, 411, 281, 652, 13, 51114, 51114, 5118, 472, 11, 286, 600, 30193, 257, 5017, 670, 437, 307, 341, 1804, 17889, 884, 510, 13, 51314, 51314, 639, 17889, 307, 2673, 411, 512, 1359, 6806, 1230, 11, 337, 1365, 11, 502, 68, 3671, 1025, 538, 7576, 13, 51564, 51564, 400, 437, 309, 311, 884, 307, 300, 309, 311, 1936, 19965, 257, 10044, 538, 4018, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0828112539697866, "compression_ratio": 1.6533333333333333, "no_speech_prob": 6.048644536349457e-06}, {"id": 693, "seek": 364300, "start": 3656.0, "end": 3658.0, "text": " There are only two more notes that I'd like to make.", "tokens": [50364, 407, 1936, 14800, 13, 50514, 50514, 400, 341, 636, 321, 600, 20308, 264, 643, 337, 341, 13691, 3233, 295, 38732, 570, 321, 366, 884, 309, 294, 1622, 670, 510, 13, 50914, 50914, 2264, 11, 370, 321, 434, 1920, 1096, 365, 15245, 2710, 2144, 13, 51014, 51014, 821, 366, 787, 732, 544, 5570, 300, 286, 1116, 411, 281, 652, 13, 51114, 51114, 5118, 472, 11, 286, 600, 30193, 257, 5017, 670, 437, 307, 341, 1804, 17889, 884, 510, 13, 51314, 51314, 639, 17889, 307, 2673, 411, 512, 1359, 6806, 1230, 11, 337, 1365, 11, 502, 68, 3671, 1025, 538, 7576, 13, 51564, 51564, 400, 437, 309, 311, 884, 307, 300, 309, 311, 1936, 19965, 257, 10044, 538, 4018, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0828112539697866, "compression_ratio": 1.6533333333333333, "no_speech_prob": 6.048644536349457e-06}, {"id": 694, "seek": 364300, "start": 3658.0, "end": 3662.0, "text": " Number one, I've skipped a discussion over what is this plus epsilon doing here.", "tokens": [50364, 407, 1936, 14800, 13, 50514, 50514, 400, 341, 636, 321, 600, 20308, 264, 643, 337, 341, 13691, 3233, 295, 38732, 570, 321, 366, 884, 309, 294, 1622, 670, 510, 13, 50914, 50914, 2264, 11, 370, 321, 434, 1920, 1096, 365, 15245, 2710, 2144, 13, 51014, 51014, 821, 366, 787, 732, 544, 5570, 300, 286, 1116, 411, 281, 652, 13, 51114, 51114, 5118, 472, 11, 286, 600, 30193, 257, 5017, 670, 437, 307, 341, 1804, 17889, 884, 510, 13, 51314, 51314, 639, 17889, 307, 2673, 411, 512, 1359, 6806, 1230, 11, 337, 1365, 11, 502, 68, 3671, 1025, 538, 7576, 13, 51564, 51564, 400, 437, 309, 311, 884, 307, 300, 309, 311, 1936, 19965, 257, 10044, 538, 4018, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0828112539697866, "compression_ratio": 1.6533333333333333, "no_speech_prob": 6.048644536349457e-06}, {"id": 695, "seek": 364300, "start": 3662.0, "end": 3667.0, "text": " This epsilon is usually like some small fixed number, for example, 1e negative 5 by default.", "tokens": [50364, 407, 1936, 14800, 13, 50514, 50514, 400, 341, 636, 321, 600, 20308, 264, 643, 337, 341, 13691, 3233, 295, 38732, 570, 321, 366, 884, 309, 294, 1622, 670, 510, 13, 50914, 50914, 2264, 11, 370, 321, 434, 1920, 1096, 365, 15245, 2710, 2144, 13, 51014, 51014, 821, 366, 787, 732, 544, 5570, 300, 286, 1116, 411, 281, 652, 13, 51114, 51114, 5118, 472, 11, 286, 600, 30193, 257, 5017, 670, 437, 307, 341, 1804, 17889, 884, 510, 13, 51314, 51314, 639, 17889, 307, 2673, 411, 512, 1359, 6806, 1230, 11, 337, 1365, 11, 502, 68, 3671, 1025, 538, 7576, 13, 51564, 51564, 400, 437, 309, 311, 884, 307, 300, 309, 311, 1936, 19965, 257, 10044, 538, 4018, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0828112539697866, "compression_ratio": 1.6533333333333333, "no_speech_prob": 6.048644536349457e-06}, {"id": 696, "seek": 364300, "start": 3667.0, "end": 3670.0, "text": " And what it's doing is that it's basically preventing a division by zero.", "tokens": [50364, 407, 1936, 14800, 13, 50514, 50514, 400, 341, 636, 321, 600, 20308, 264, 643, 337, 341, 13691, 3233, 295, 38732, 570, 321, 366, 884, 309, 294, 1622, 670, 510, 13, 50914, 50914, 2264, 11, 370, 321, 434, 1920, 1096, 365, 15245, 2710, 2144, 13, 51014, 51014, 821, 366, 787, 732, 544, 5570, 300, 286, 1116, 411, 281, 652, 13, 51114, 51114, 5118, 472, 11, 286, 600, 30193, 257, 5017, 670, 437, 307, 341, 1804, 17889, 884, 510, 13, 51314, 51314, 639, 17889, 307, 2673, 411, 512, 1359, 6806, 1230, 11, 337, 1365, 11, 502, 68, 3671, 1025, 538, 7576, 13, 51564, 51564, 400, 437, 309, 311, 884, 307, 300, 309, 311, 1936, 19965, 257, 10044, 538, 4018, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0828112539697866, "compression_ratio": 1.6533333333333333, "no_speech_prob": 6.048644536349457e-06}, {"id": 697, "seek": 367000, "start": 3670.0, "end": 3675.0, "text": " In the case that the variance over your batch is exactly zero.", "tokens": [50364, 682, 264, 1389, 300, 264, 21977, 670, 428, 15245, 307, 2293, 4018, 13, 50614, 50614, 682, 300, 1389, 11, 510, 321, 5646, 362, 257, 10044, 538, 4018, 13, 50814, 50814, 583, 570, 295, 264, 1804, 17889, 11, 341, 307, 516, 281, 1813, 257, 1359, 1230, 294, 264, 20687, 2602, 13, 51014, 51014, 400, 721, 486, 312, 544, 731, 48249, 13, 51114, 51114, 407, 841, 1737, 281, 611, 909, 257, 1804, 17889, 510, 295, 257, 588, 1359, 1230, 13, 51314, 51314, 467, 1177, 380, 767, 30797, 1319, 264, 1874, 13, 51414, 51414, 286, 478, 516, 281, 10023, 309, 294, 527, 1389, 445, 570, 341, 307, 17518, 281, 1051, 294, 527, 588, 2199, 1365, 510, 13, 51664, 51664, 400, 264, 1150, 551, 286, 528, 291, 281, 3449, 307, 300, 321, 434, 885, 5964, 906, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.047574402629465294, "compression_ratio": 1.7980769230769231, "no_speech_prob": 2.9769820685032755e-05}, {"id": 698, "seek": 367000, "start": 3675.0, "end": 3679.0, "text": " In that case, here we normally have a division by zero.", "tokens": [50364, 682, 264, 1389, 300, 264, 21977, 670, 428, 15245, 307, 2293, 4018, 13, 50614, 50614, 682, 300, 1389, 11, 510, 321, 5646, 362, 257, 10044, 538, 4018, 13, 50814, 50814, 583, 570, 295, 264, 1804, 17889, 11, 341, 307, 516, 281, 1813, 257, 1359, 1230, 294, 264, 20687, 2602, 13, 51014, 51014, 400, 721, 486, 312, 544, 731, 48249, 13, 51114, 51114, 407, 841, 1737, 281, 611, 909, 257, 1804, 17889, 510, 295, 257, 588, 1359, 1230, 13, 51314, 51314, 467, 1177, 380, 767, 30797, 1319, 264, 1874, 13, 51414, 51414, 286, 478, 516, 281, 10023, 309, 294, 527, 1389, 445, 570, 341, 307, 17518, 281, 1051, 294, 527, 588, 2199, 1365, 510, 13, 51664, 51664, 400, 264, 1150, 551, 286, 528, 291, 281, 3449, 307, 300, 321, 434, 885, 5964, 906, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.047574402629465294, "compression_ratio": 1.7980769230769231, "no_speech_prob": 2.9769820685032755e-05}, {"id": 699, "seek": 367000, "start": 3679.0, "end": 3683.0, "text": " But because of the plus epsilon, this is going to become a small number in the denominator instead.", "tokens": [50364, 682, 264, 1389, 300, 264, 21977, 670, 428, 15245, 307, 2293, 4018, 13, 50614, 50614, 682, 300, 1389, 11, 510, 321, 5646, 362, 257, 10044, 538, 4018, 13, 50814, 50814, 583, 570, 295, 264, 1804, 17889, 11, 341, 307, 516, 281, 1813, 257, 1359, 1230, 294, 264, 20687, 2602, 13, 51014, 51014, 400, 721, 486, 312, 544, 731, 48249, 13, 51114, 51114, 407, 841, 1737, 281, 611, 909, 257, 1804, 17889, 510, 295, 257, 588, 1359, 1230, 13, 51314, 51314, 467, 1177, 380, 767, 30797, 1319, 264, 1874, 13, 51414, 51414, 286, 478, 516, 281, 10023, 309, 294, 527, 1389, 445, 570, 341, 307, 17518, 281, 1051, 294, 527, 588, 2199, 1365, 510, 13, 51664, 51664, 400, 264, 1150, 551, 286, 528, 291, 281, 3449, 307, 300, 321, 434, 885, 5964, 906, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.047574402629465294, "compression_ratio": 1.7980769230769231, "no_speech_prob": 2.9769820685032755e-05}, {"id": 700, "seek": 367000, "start": 3683.0, "end": 3685.0, "text": " And things will be more well behaved.", "tokens": [50364, 682, 264, 1389, 300, 264, 21977, 670, 428, 15245, 307, 2293, 4018, 13, 50614, 50614, 682, 300, 1389, 11, 510, 321, 5646, 362, 257, 10044, 538, 4018, 13, 50814, 50814, 583, 570, 295, 264, 1804, 17889, 11, 341, 307, 516, 281, 1813, 257, 1359, 1230, 294, 264, 20687, 2602, 13, 51014, 51014, 400, 721, 486, 312, 544, 731, 48249, 13, 51114, 51114, 407, 841, 1737, 281, 611, 909, 257, 1804, 17889, 510, 295, 257, 588, 1359, 1230, 13, 51314, 51314, 467, 1177, 380, 767, 30797, 1319, 264, 1874, 13, 51414, 51414, 286, 478, 516, 281, 10023, 309, 294, 527, 1389, 445, 570, 341, 307, 17518, 281, 1051, 294, 527, 588, 2199, 1365, 510, 13, 51664, 51664, 400, 264, 1150, 551, 286, 528, 291, 281, 3449, 307, 300, 321, 434, 885, 5964, 906, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.047574402629465294, "compression_ratio": 1.7980769230769231, "no_speech_prob": 2.9769820685032755e-05}, {"id": 701, "seek": 367000, "start": 3685.0, "end": 3689.0, "text": " So feel free to also add a plus epsilon here of a very small number.", "tokens": [50364, 682, 264, 1389, 300, 264, 21977, 670, 428, 15245, 307, 2293, 4018, 13, 50614, 50614, 682, 300, 1389, 11, 510, 321, 5646, 362, 257, 10044, 538, 4018, 13, 50814, 50814, 583, 570, 295, 264, 1804, 17889, 11, 341, 307, 516, 281, 1813, 257, 1359, 1230, 294, 264, 20687, 2602, 13, 51014, 51014, 400, 721, 486, 312, 544, 731, 48249, 13, 51114, 51114, 407, 841, 1737, 281, 611, 909, 257, 1804, 17889, 510, 295, 257, 588, 1359, 1230, 13, 51314, 51314, 467, 1177, 380, 767, 30797, 1319, 264, 1874, 13, 51414, 51414, 286, 478, 516, 281, 10023, 309, 294, 527, 1389, 445, 570, 341, 307, 17518, 281, 1051, 294, 527, 588, 2199, 1365, 510, 13, 51664, 51664, 400, 264, 1150, 551, 286, 528, 291, 281, 3449, 307, 300, 321, 434, 885, 5964, 906, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.047574402629465294, "compression_ratio": 1.7980769230769231, "no_speech_prob": 2.9769820685032755e-05}, {"id": 702, "seek": 367000, "start": 3689.0, "end": 3691.0, "text": " It doesn't actually substantially change the result.", "tokens": [50364, 682, 264, 1389, 300, 264, 21977, 670, 428, 15245, 307, 2293, 4018, 13, 50614, 50614, 682, 300, 1389, 11, 510, 321, 5646, 362, 257, 10044, 538, 4018, 13, 50814, 50814, 583, 570, 295, 264, 1804, 17889, 11, 341, 307, 516, 281, 1813, 257, 1359, 1230, 294, 264, 20687, 2602, 13, 51014, 51014, 400, 721, 486, 312, 544, 731, 48249, 13, 51114, 51114, 407, 841, 1737, 281, 611, 909, 257, 1804, 17889, 510, 295, 257, 588, 1359, 1230, 13, 51314, 51314, 467, 1177, 380, 767, 30797, 1319, 264, 1874, 13, 51414, 51414, 286, 478, 516, 281, 10023, 309, 294, 527, 1389, 445, 570, 341, 307, 17518, 281, 1051, 294, 527, 588, 2199, 1365, 510, 13, 51664, 51664, 400, 264, 1150, 551, 286, 528, 291, 281, 3449, 307, 300, 321, 434, 885, 5964, 906, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.047574402629465294, "compression_ratio": 1.7980769230769231, "no_speech_prob": 2.9769820685032755e-05}, {"id": 703, "seek": 367000, "start": 3691.0, "end": 3696.0, "text": " I'm going to skip it in our case just because this is unlikely to happen in our very simple example here.", "tokens": [50364, 682, 264, 1389, 300, 264, 21977, 670, 428, 15245, 307, 2293, 4018, 13, 50614, 50614, 682, 300, 1389, 11, 510, 321, 5646, 362, 257, 10044, 538, 4018, 13, 50814, 50814, 583, 570, 295, 264, 1804, 17889, 11, 341, 307, 516, 281, 1813, 257, 1359, 1230, 294, 264, 20687, 2602, 13, 51014, 51014, 400, 721, 486, 312, 544, 731, 48249, 13, 51114, 51114, 407, 841, 1737, 281, 611, 909, 257, 1804, 17889, 510, 295, 257, 588, 1359, 1230, 13, 51314, 51314, 467, 1177, 380, 767, 30797, 1319, 264, 1874, 13, 51414, 51414, 286, 478, 516, 281, 10023, 309, 294, 527, 1389, 445, 570, 341, 307, 17518, 281, 1051, 294, 527, 588, 2199, 1365, 510, 13, 51664, 51664, 400, 264, 1150, 551, 286, 528, 291, 281, 3449, 307, 300, 321, 434, 885, 5964, 906, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.047574402629465294, "compression_ratio": 1.7980769230769231, "no_speech_prob": 2.9769820685032755e-05}, {"id": 704, "seek": 367000, "start": 3696.0, "end": 3699.0, "text": " And the second thing I want you to notice is that we're being wasteful here.", "tokens": [50364, 682, 264, 1389, 300, 264, 21977, 670, 428, 15245, 307, 2293, 4018, 13, 50614, 50614, 682, 300, 1389, 11, 510, 321, 5646, 362, 257, 10044, 538, 4018, 13, 50814, 50814, 583, 570, 295, 264, 1804, 17889, 11, 341, 307, 516, 281, 1813, 257, 1359, 1230, 294, 264, 20687, 2602, 13, 51014, 51014, 400, 721, 486, 312, 544, 731, 48249, 13, 51114, 51114, 407, 841, 1737, 281, 611, 909, 257, 1804, 17889, 510, 295, 257, 588, 1359, 1230, 13, 51314, 51314, 467, 1177, 380, 767, 30797, 1319, 264, 1874, 13, 51414, 51414, 286, 478, 516, 281, 10023, 309, 294, 527, 1389, 445, 570, 341, 307, 17518, 281, 1051, 294, 527, 588, 2199, 1365, 510, 13, 51664, 51664, 400, 264, 1150, 551, 286, 528, 291, 281, 3449, 307, 300, 321, 434, 885, 5964, 906, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.047574402629465294, "compression_ratio": 1.7980769230769231, "no_speech_prob": 2.9769820685032755e-05}, {"id": 705, "seek": 369900, "start": 3699.0, "end": 3701.0, "text": " And it's very subtle.", "tokens": [50364, 400, 309, 311, 588, 13743, 13, 50464, 50464, 583, 558, 510, 689, 321, 366, 5127, 264, 12577, 666, 389, 3712, 578, 11, 613, 32152, 586, 366, 767, 14115, 50814, 50814, 570, 321, 434, 5127, 552, 281, 264, 389, 3712, 578, 13, 50914, 50914, 583, 550, 321, 366, 28258, 264, 914, 337, 633, 472, 295, 613, 22027, 293, 16390, 278, 309, 13, 51214, 51214, 407, 2035, 12577, 291, 909, 510, 307, 516, 281, 483, 16390, 292, 558, 510, 13, 51414, 51414, 400, 370, 613, 32152, 366, 406, 884, 1340, 13, 51564, 51564, 682, 1186, 11, 436, 434, 885, 16390, 292, 484, 293, 436, 500, 380, 2712, 264, 1472, 295, 264, 17108, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06269245147705078, "compression_ratio": 1.8786610878661087, "no_speech_prob": 1.568887455505319e-05}, {"id": 706, "seek": 369900, "start": 3701.0, "end": 3708.0, "text": " But right here where we are adding the bias into Hpreact, these biases now are actually useless", "tokens": [50364, 400, 309, 311, 588, 13743, 13, 50464, 50464, 583, 558, 510, 689, 321, 366, 5127, 264, 12577, 666, 389, 3712, 578, 11, 613, 32152, 586, 366, 767, 14115, 50814, 50814, 570, 321, 434, 5127, 552, 281, 264, 389, 3712, 578, 13, 50914, 50914, 583, 550, 321, 366, 28258, 264, 914, 337, 633, 472, 295, 613, 22027, 293, 16390, 278, 309, 13, 51214, 51214, 407, 2035, 12577, 291, 909, 510, 307, 516, 281, 483, 16390, 292, 558, 510, 13, 51414, 51414, 400, 370, 613, 32152, 366, 406, 884, 1340, 13, 51564, 51564, 682, 1186, 11, 436, 434, 885, 16390, 292, 484, 293, 436, 500, 380, 2712, 264, 1472, 295, 264, 17108, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06269245147705078, "compression_ratio": 1.8786610878661087, "no_speech_prob": 1.568887455505319e-05}, {"id": 707, "seek": 369900, "start": 3708.0, "end": 3710.0, "text": " because we're adding them to the Hpreact.", "tokens": [50364, 400, 309, 311, 588, 13743, 13, 50464, 50464, 583, 558, 510, 689, 321, 366, 5127, 264, 12577, 666, 389, 3712, 578, 11, 613, 32152, 586, 366, 767, 14115, 50814, 50814, 570, 321, 434, 5127, 552, 281, 264, 389, 3712, 578, 13, 50914, 50914, 583, 550, 321, 366, 28258, 264, 914, 337, 633, 472, 295, 613, 22027, 293, 16390, 278, 309, 13, 51214, 51214, 407, 2035, 12577, 291, 909, 510, 307, 516, 281, 483, 16390, 292, 558, 510, 13, 51414, 51414, 400, 370, 613, 32152, 366, 406, 884, 1340, 13, 51564, 51564, 682, 1186, 11, 436, 434, 885, 16390, 292, 484, 293, 436, 500, 380, 2712, 264, 1472, 295, 264, 17108, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06269245147705078, "compression_ratio": 1.8786610878661087, "no_speech_prob": 1.568887455505319e-05}, {"id": 708, "seek": 369900, "start": 3710.0, "end": 3716.0, "text": " But then we are calculating the mean for every one of these neurons and subtracting it.", "tokens": [50364, 400, 309, 311, 588, 13743, 13, 50464, 50464, 583, 558, 510, 689, 321, 366, 5127, 264, 12577, 666, 389, 3712, 578, 11, 613, 32152, 586, 366, 767, 14115, 50814, 50814, 570, 321, 434, 5127, 552, 281, 264, 389, 3712, 578, 13, 50914, 50914, 583, 550, 321, 366, 28258, 264, 914, 337, 633, 472, 295, 613, 22027, 293, 16390, 278, 309, 13, 51214, 51214, 407, 2035, 12577, 291, 909, 510, 307, 516, 281, 483, 16390, 292, 558, 510, 13, 51414, 51414, 400, 370, 613, 32152, 366, 406, 884, 1340, 13, 51564, 51564, 682, 1186, 11, 436, 434, 885, 16390, 292, 484, 293, 436, 500, 380, 2712, 264, 1472, 295, 264, 17108, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06269245147705078, "compression_ratio": 1.8786610878661087, "no_speech_prob": 1.568887455505319e-05}, {"id": 709, "seek": 369900, "start": 3716.0, "end": 3720.0, "text": " So whatever bias you add here is going to get subtracted right here.", "tokens": [50364, 400, 309, 311, 588, 13743, 13, 50464, 50464, 583, 558, 510, 689, 321, 366, 5127, 264, 12577, 666, 389, 3712, 578, 11, 613, 32152, 586, 366, 767, 14115, 50814, 50814, 570, 321, 434, 5127, 552, 281, 264, 389, 3712, 578, 13, 50914, 50914, 583, 550, 321, 366, 28258, 264, 914, 337, 633, 472, 295, 613, 22027, 293, 16390, 278, 309, 13, 51214, 51214, 407, 2035, 12577, 291, 909, 510, 307, 516, 281, 483, 16390, 292, 558, 510, 13, 51414, 51414, 400, 370, 613, 32152, 366, 406, 884, 1340, 13, 51564, 51564, 682, 1186, 11, 436, 434, 885, 16390, 292, 484, 293, 436, 500, 380, 2712, 264, 1472, 295, 264, 17108, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06269245147705078, "compression_ratio": 1.8786610878661087, "no_speech_prob": 1.568887455505319e-05}, {"id": 710, "seek": 369900, "start": 3720.0, "end": 3723.0, "text": " And so these biases are not doing anything.", "tokens": [50364, 400, 309, 311, 588, 13743, 13, 50464, 50464, 583, 558, 510, 689, 321, 366, 5127, 264, 12577, 666, 389, 3712, 578, 11, 613, 32152, 586, 366, 767, 14115, 50814, 50814, 570, 321, 434, 5127, 552, 281, 264, 389, 3712, 578, 13, 50914, 50914, 583, 550, 321, 366, 28258, 264, 914, 337, 633, 472, 295, 613, 22027, 293, 16390, 278, 309, 13, 51214, 51214, 407, 2035, 12577, 291, 909, 510, 307, 516, 281, 483, 16390, 292, 558, 510, 13, 51414, 51414, 400, 370, 613, 32152, 366, 406, 884, 1340, 13, 51564, 51564, 682, 1186, 11, 436, 434, 885, 16390, 292, 484, 293, 436, 500, 380, 2712, 264, 1472, 295, 264, 17108, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06269245147705078, "compression_ratio": 1.8786610878661087, "no_speech_prob": 1.568887455505319e-05}, {"id": 711, "seek": 369900, "start": 3723.0, "end": 3727.0, "text": " In fact, they're being subtracted out and they don't impact the rest of the calculation.", "tokens": [50364, 400, 309, 311, 588, 13743, 13, 50464, 50464, 583, 558, 510, 689, 321, 366, 5127, 264, 12577, 666, 389, 3712, 578, 11, 613, 32152, 586, 366, 767, 14115, 50814, 50814, 570, 321, 434, 5127, 552, 281, 264, 389, 3712, 578, 13, 50914, 50914, 583, 550, 321, 366, 28258, 264, 914, 337, 633, 472, 295, 613, 22027, 293, 16390, 278, 309, 13, 51214, 51214, 407, 2035, 12577, 291, 909, 510, 307, 516, 281, 483, 16390, 292, 558, 510, 13, 51414, 51414, 400, 370, 613, 32152, 366, 406, 884, 1340, 13, 51564, 51564, 682, 1186, 11, 436, 434, 885, 16390, 292, 484, 293, 436, 500, 380, 2712, 264, 1472, 295, 264, 17108, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06269245147705078, "compression_ratio": 1.8786610878661087, "no_speech_prob": 1.568887455505319e-05}, {"id": 712, "seek": 372700, "start": 3727.0, "end": 3730.0, "text": " So if you look at b1.grad, it's actually going to be zero", "tokens": [50364, 407, 498, 291, 574, 412, 272, 16, 13, 7165, 11, 309, 311, 767, 516, 281, 312, 4018, 50514, 50514, 570, 309, 311, 885, 16390, 292, 484, 293, 1177, 380, 767, 362, 604, 1802, 13, 50664, 50664, 400, 370, 5699, 291, 434, 1228, 15245, 2710, 2144, 7914, 11, 50814, 50814, 550, 498, 291, 362, 604, 3364, 7914, 949, 11, 411, 257, 8213, 420, 257, 3754, 420, 746, 411, 300, 11, 51014, 51014, 291, 434, 1101, 766, 1348, 510, 293, 445, 411, 406, 1228, 12577, 13, 51214, 51214, 407, 291, 500, 380, 528, 281, 764, 12577, 13, 51314, 51314, 400, 550, 510, 291, 500, 380, 528, 281, 909, 309, 570, 300, 311, 637, 24274, 13, 51514, 51514, 7156, 11, 321, 362, 341, 15245, 2710, 2144, 12577, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06880160111647386, "compression_ratio": 1.8029739776951672, "no_speech_prob": 6.1438558986992575e-06}, {"id": 713, "seek": 372700, "start": 3730.0, "end": 3733.0, "text": " because it's being subtracted out and doesn't actually have any effect.", "tokens": [50364, 407, 498, 291, 574, 412, 272, 16, 13, 7165, 11, 309, 311, 767, 516, 281, 312, 4018, 50514, 50514, 570, 309, 311, 885, 16390, 292, 484, 293, 1177, 380, 767, 362, 604, 1802, 13, 50664, 50664, 400, 370, 5699, 291, 434, 1228, 15245, 2710, 2144, 7914, 11, 50814, 50814, 550, 498, 291, 362, 604, 3364, 7914, 949, 11, 411, 257, 8213, 420, 257, 3754, 420, 746, 411, 300, 11, 51014, 51014, 291, 434, 1101, 766, 1348, 510, 293, 445, 411, 406, 1228, 12577, 13, 51214, 51214, 407, 291, 500, 380, 528, 281, 764, 12577, 13, 51314, 51314, 400, 550, 510, 291, 500, 380, 528, 281, 909, 309, 570, 300, 311, 637, 24274, 13, 51514, 51514, 7156, 11, 321, 362, 341, 15245, 2710, 2144, 12577, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06880160111647386, "compression_ratio": 1.8029739776951672, "no_speech_prob": 6.1438558986992575e-06}, {"id": 714, "seek": 372700, "start": 3733.0, "end": 3736.0, "text": " And so whenever you're using batch normalization layers,", "tokens": [50364, 407, 498, 291, 574, 412, 272, 16, 13, 7165, 11, 309, 311, 767, 516, 281, 312, 4018, 50514, 50514, 570, 309, 311, 885, 16390, 292, 484, 293, 1177, 380, 767, 362, 604, 1802, 13, 50664, 50664, 400, 370, 5699, 291, 434, 1228, 15245, 2710, 2144, 7914, 11, 50814, 50814, 550, 498, 291, 362, 604, 3364, 7914, 949, 11, 411, 257, 8213, 420, 257, 3754, 420, 746, 411, 300, 11, 51014, 51014, 291, 434, 1101, 766, 1348, 510, 293, 445, 411, 406, 1228, 12577, 13, 51214, 51214, 407, 291, 500, 380, 528, 281, 764, 12577, 13, 51314, 51314, 400, 550, 510, 291, 500, 380, 528, 281, 909, 309, 570, 300, 311, 637, 24274, 13, 51514, 51514, 7156, 11, 321, 362, 341, 15245, 2710, 2144, 12577, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06880160111647386, "compression_ratio": 1.8029739776951672, "no_speech_prob": 6.1438558986992575e-06}, {"id": 715, "seek": 372700, "start": 3736.0, "end": 3740.0, "text": " then if you have any weight layers before, like a linear or a conv or something like that,", "tokens": [50364, 407, 498, 291, 574, 412, 272, 16, 13, 7165, 11, 309, 311, 767, 516, 281, 312, 4018, 50514, 50514, 570, 309, 311, 885, 16390, 292, 484, 293, 1177, 380, 767, 362, 604, 1802, 13, 50664, 50664, 400, 370, 5699, 291, 434, 1228, 15245, 2710, 2144, 7914, 11, 50814, 50814, 550, 498, 291, 362, 604, 3364, 7914, 949, 11, 411, 257, 8213, 420, 257, 3754, 420, 746, 411, 300, 11, 51014, 51014, 291, 434, 1101, 766, 1348, 510, 293, 445, 411, 406, 1228, 12577, 13, 51214, 51214, 407, 291, 500, 380, 528, 281, 764, 12577, 13, 51314, 51314, 400, 550, 510, 291, 500, 380, 528, 281, 909, 309, 570, 300, 311, 637, 24274, 13, 51514, 51514, 7156, 11, 321, 362, 341, 15245, 2710, 2144, 12577, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06880160111647386, "compression_ratio": 1.8029739776951672, "no_speech_prob": 6.1438558986992575e-06}, {"id": 716, "seek": 372700, "start": 3740.0, "end": 3744.0, "text": " you're better off coming here and just like not using bias.", "tokens": [50364, 407, 498, 291, 574, 412, 272, 16, 13, 7165, 11, 309, 311, 767, 516, 281, 312, 4018, 50514, 50514, 570, 309, 311, 885, 16390, 292, 484, 293, 1177, 380, 767, 362, 604, 1802, 13, 50664, 50664, 400, 370, 5699, 291, 434, 1228, 15245, 2710, 2144, 7914, 11, 50814, 50814, 550, 498, 291, 362, 604, 3364, 7914, 949, 11, 411, 257, 8213, 420, 257, 3754, 420, 746, 411, 300, 11, 51014, 51014, 291, 434, 1101, 766, 1348, 510, 293, 445, 411, 406, 1228, 12577, 13, 51214, 51214, 407, 291, 500, 380, 528, 281, 764, 12577, 13, 51314, 51314, 400, 550, 510, 291, 500, 380, 528, 281, 909, 309, 570, 300, 311, 637, 24274, 13, 51514, 51514, 7156, 11, 321, 362, 341, 15245, 2710, 2144, 12577, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06880160111647386, "compression_ratio": 1.8029739776951672, "no_speech_prob": 6.1438558986992575e-06}, {"id": 717, "seek": 372700, "start": 3744.0, "end": 3746.0, "text": " So you don't want to use bias.", "tokens": [50364, 407, 498, 291, 574, 412, 272, 16, 13, 7165, 11, 309, 311, 767, 516, 281, 312, 4018, 50514, 50514, 570, 309, 311, 885, 16390, 292, 484, 293, 1177, 380, 767, 362, 604, 1802, 13, 50664, 50664, 400, 370, 5699, 291, 434, 1228, 15245, 2710, 2144, 7914, 11, 50814, 50814, 550, 498, 291, 362, 604, 3364, 7914, 949, 11, 411, 257, 8213, 420, 257, 3754, 420, 746, 411, 300, 11, 51014, 51014, 291, 434, 1101, 766, 1348, 510, 293, 445, 411, 406, 1228, 12577, 13, 51214, 51214, 407, 291, 500, 380, 528, 281, 764, 12577, 13, 51314, 51314, 400, 550, 510, 291, 500, 380, 528, 281, 909, 309, 570, 300, 311, 637, 24274, 13, 51514, 51514, 7156, 11, 321, 362, 341, 15245, 2710, 2144, 12577, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06880160111647386, "compression_ratio": 1.8029739776951672, "no_speech_prob": 6.1438558986992575e-06}, {"id": 718, "seek": 372700, "start": 3746.0, "end": 3750.0, "text": " And then here you don't want to add it because that's spurious.", "tokens": [50364, 407, 498, 291, 574, 412, 272, 16, 13, 7165, 11, 309, 311, 767, 516, 281, 312, 4018, 50514, 50514, 570, 309, 311, 885, 16390, 292, 484, 293, 1177, 380, 767, 362, 604, 1802, 13, 50664, 50664, 400, 370, 5699, 291, 434, 1228, 15245, 2710, 2144, 7914, 11, 50814, 50814, 550, 498, 291, 362, 604, 3364, 7914, 949, 11, 411, 257, 8213, 420, 257, 3754, 420, 746, 411, 300, 11, 51014, 51014, 291, 434, 1101, 766, 1348, 510, 293, 445, 411, 406, 1228, 12577, 13, 51214, 51214, 407, 291, 500, 380, 528, 281, 764, 12577, 13, 51314, 51314, 400, 550, 510, 291, 500, 380, 528, 281, 909, 309, 570, 300, 311, 637, 24274, 13, 51514, 51514, 7156, 11, 321, 362, 341, 15245, 2710, 2144, 12577, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06880160111647386, "compression_ratio": 1.8029739776951672, "no_speech_prob": 6.1438558986992575e-06}, {"id": 719, "seek": 372700, "start": 3750.0, "end": 3753.0, "text": " Instead, we have this batch normalization bias here.", "tokens": [50364, 407, 498, 291, 574, 412, 272, 16, 13, 7165, 11, 309, 311, 767, 516, 281, 312, 4018, 50514, 50514, 570, 309, 311, 885, 16390, 292, 484, 293, 1177, 380, 767, 362, 604, 1802, 13, 50664, 50664, 400, 370, 5699, 291, 434, 1228, 15245, 2710, 2144, 7914, 11, 50814, 50814, 550, 498, 291, 362, 604, 3364, 7914, 949, 11, 411, 257, 8213, 420, 257, 3754, 420, 746, 411, 300, 11, 51014, 51014, 291, 434, 1101, 766, 1348, 510, 293, 445, 411, 406, 1228, 12577, 13, 51214, 51214, 407, 291, 500, 380, 528, 281, 764, 12577, 13, 51314, 51314, 400, 550, 510, 291, 500, 380, 528, 281, 909, 309, 570, 300, 311, 637, 24274, 13, 51514, 51514, 7156, 11, 321, 362, 341, 15245, 2710, 2144, 12577, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06880160111647386, "compression_ratio": 1.8029739776951672, "no_speech_prob": 6.1438558986992575e-06}, {"id": 720, "seek": 375300, "start": 3753.0, "end": 3758.0, "text": " And that batch normalization bias is now in charge of the biasing of this distribution", "tokens": [50364, 400, 300, 15245, 2710, 2144, 12577, 307, 586, 294, 4602, 295, 264, 3228, 3349, 295, 341, 7316, 50614, 50614, 2602, 295, 341, 272, 16, 300, 321, 632, 510, 7993, 13, 50814, 50814, 400, 370, 1936, 11, 264, 15245, 2710, 2144, 4583, 575, 1080, 1065, 12577, 13, 50964, 50964, 400, 456, 311, 572, 643, 281, 362, 257, 12577, 294, 264, 4583, 949, 309, 51164, 51164, 570, 300, 12577, 307, 516, 281, 312, 16390, 292, 484, 4033, 13, 51314, 51314, 407, 300, 311, 264, 661, 1359, 2607, 281, 312, 5026, 365, 2171, 13, 51414, 51414, 467, 311, 406, 516, 281, 360, 1340, 34915, 13, 51514, 51514, 639, 272, 16, 486, 445, 312, 14115, 13, 51614, 51614, 467, 486, 1128, 483, 604, 16235, 13, 51714, 51714, 467, 486, 406, 1466, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.053544102293072326, "compression_ratio": 1.812274368231047, "no_speech_prob": 3.6119231481279712e-06}, {"id": 721, "seek": 375300, "start": 3758.0, "end": 3762.0, "text": " instead of this b1 that we had here originally.", "tokens": [50364, 400, 300, 15245, 2710, 2144, 12577, 307, 586, 294, 4602, 295, 264, 3228, 3349, 295, 341, 7316, 50614, 50614, 2602, 295, 341, 272, 16, 300, 321, 632, 510, 7993, 13, 50814, 50814, 400, 370, 1936, 11, 264, 15245, 2710, 2144, 4583, 575, 1080, 1065, 12577, 13, 50964, 50964, 400, 456, 311, 572, 643, 281, 362, 257, 12577, 294, 264, 4583, 949, 309, 51164, 51164, 570, 300, 12577, 307, 516, 281, 312, 16390, 292, 484, 4033, 13, 51314, 51314, 407, 300, 311, 264, 661, 1359, 2607, 281, 312, 5026, 365, 2171, 13, 51414, 51414, 467, 311, 406, 516, 281, 360, 1340, 34915, 13, 51514, 51514, 639, 272, 16, 486, 445, 312, 14115, 13, 51614, 51614, 467, 486, 1128, 483, 604, 16235, 13, 51714, 51714, 467, 486, 406, 1466, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.053544102293072326, "compression_ratio": 1.812274368231047, "no_speech_prob": 3.6119231481279712e-06}, {"id": 722, "seek": 375300, "start": 3762.0, "end": 3765.0, "text": " And so basically, the batch normalization layer has its own bias.", "tokens": [50364, 400, 300, 15245, 2710, 2144, 12577, 307, 586, 294, 4602, 295, 264, 3228, 3349, 295, 341, 7316, 50614, 50614, 2602, 295, 341, 272, 16, 300, 321, 632, 510, 7993, 13, 50814, 50814, 400, 370, 1936, 11, 264, 15245, 2710, 2144, 4583, 575, 1080, 1065, 12577, 13, 50964, 50964, 400, 456, 311, 572, 643, 281, 362, 257, 12577, 294, 264, 4583, 949, 309, 51164, 51164, 570, 300, 12577, 307, 516, 281, 312, 16390, 292, 484, 4033, 13, 51314, 51314, 407, 300, 311, 264, 661, 1359, 2607, 281, 312, 5026, 365, 2171, 13, 51414, 51414, 467, 311, 406, 516, 281, 360, 1340, 34915, 13, 51514, 51514, 639, 272, 16, 486, 445, 312, 14115, 13, 51614, 51614, 467, 486, 1128, 483, 604, 16235, 13, 51714, 51714, 467, 486, 406, 1466, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.053544102293072326, "compression_ratio": 1.812274368231047, "no_speech_prob": 3.6119231481279712e-06}, {"id": 723, "seek": 375300, "start": 3765.0, "end": 3769.0, "text": " And there's no need to have a bias in the layer before it", "tokens": [50364, 400, 300, 15245, 2710, 2144, 12577, 307, 586, 294, 4602, 295, 264, 3228, 3349, 295, 341, 7316, 50614, 50614, 2602, 295, 341, 272, 16, 300, 321, 632, 510, 7993, 13, 50814, 50814, 400, 370, 1936, 11, 264, 15245, 2710, 2144, 4583, 575, 1080, 1065, 12577, 13, 50964, 50964, 400, 456, 311, 572, 643, 281, 362, 257, 12577, 294, 264, 4583, 949, 309, 51164, 51164, 570, 300, 12577, 307, 516, 281, 312, 16390, 292, 484, 4033, 13, 51314, 51314, 407, 300, 311, 264, 661, 1359, 2607, 281, 312, 5026, 365, 2171, 13, 51414, 51414, 467, 311, 406, 516, 281, 360, 1340, 34915, 13, 51514, 51514, 639, 272, 16, 486, 445, 312, 14115, 13, 51614, 51614, 467, 486, 1128, 483, 604, 16235, 13, 51714, 51714, 467, 486, 406, 1466, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.053544102293072326, "compression_ratio": 1.812274368231047, "no_speech_prob": 3.6119231481279712e-06}, {"id": 724, "seek": 375300, "start": 3769.0, "end": 3772.0, "text": " because that bias is going to be subtracted out anyway.", "tokens": [50364, 400, 300, 15245, 2710, 2144, 12577, 307, 586, 294, 4602, 295, 264, 3228, 3349, 295, 341, 7316, 50614, 50614, 2602, 295, 341, 272, 16, 300, 321, 632, 510, 7993, 13, 50814, 50814, 400, 370, 1936, 11, 264, 15245, 2710, 2144, 4583, 575, 1080, 1065, 12577, 13, 50964, 50964, 400, 456, 311, 572, 643, 281, 362, 257, 12577, 294, 264, 4583, 949, 309, 51164, 51164, 570, 300, 12577, 307, 516, 281, 312, 16390, 292, 484, 4033, 13, 51314, 51314, 407, 300, 311, 264, 661, 1359, 2607, 281, 312, 5026, 365, 2171, 13, 51414, 51414, 467, 311, 406, 516, 281, 360, 1340, 34915, 13, 51514, 51514, 639, 272, 16, 486, 445, 312, 14115, 13, 51614, 51614, 467, 486, 1128, 483, 604, 16235, 13, 51714, 51714, 467, 486, 406, 1466, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.053544102293072326, "compression_ratio": 1.812274368231047, "no_speech_prob": 3.6119231481279712e-06}, {"id": 725, "seek": 375300, "start": 3772.0, "end": 3774.0, "text": " So that's the other small detail to be careful with sometimes.", "tokens": [50364, 400, 300, 15245, 2710, 2144, 12577, 307, 586, 294, 4602, 295, 264, 3228, 3349, 295, 341, 7316, 50614, 50614, 2602, 295, 341, 272, 16, 300, 321, 632, 510, 7993, 13, 50814, 50814, 400, 370, 1936, 11, 264, 15245, 2710, 2144, 4583, 575, 1080, 1065, 12577, 13, 50964, 50964, 400, 456, 311, 572, 643, 281, 362, 257, 12577, 294, 264, 4583, 949, 309, 51164, 51164, 570, 300, 12577, 307, 516, 281, 312, 16390, 292, 484, 4033, 13, 51314, 51314, 407, 300, 311, 264, 661, 1359, 2607, 281, 312, 5026, 365, 2171, 13, 51414, 51414, 467, 311, 406, 516, 281, 360, 1340, 34915, 13, 51514, 51514, 639, 272, 16, 486, 445, 312, 14115, 13, 51614, 51614, 467, 486, 1128, 483, 604, 16235, 13, 51714, 51714, 467, 486, 406, 1466, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.053544102293072326, "compression_ratio": 1.812274368231047, "no_speech_prob": 3.6119231481279712e-06}, {"id": 726, "seek": 375300, "start": 3774.0, "end": 3776.0, "text": " It's not going to do anything catastrophic.", "tokens": [50364, 400, 300, 15245, 2710, 2144, 12577, 307, 586, 294, 4602, 295, 264, 3228, 3349, 295, 341, 7316, 50614, 50614, 2602, 295, 341, 272, 16, 300, 321, 632, 510, 7993, 13, 50814, 50814, 400, 370, 1936, 11, 264, 15245, 2710, 2144, 4583, 575, 1080, 1065, 12577, 13, 50964, 50964, 400, 456, 311, 572, 643, 281, 362, 257, 12577, 294, 264, 4583, 949, 309, 51164, 51164, 570, 300, 12577, 307, 516, 281, 312, 16390, 292, 484, 4033, 13, 51314, 51314, 407, 300, 311, 264, 661, 1359, 2607, 281, 312, 5026, 365, 2171, 13, 51414, 51414, 467, 311, 406, 516, 281, 360, 1340, 34915, 13, 51514, 51514, 639, 272, 16, 486, 445, 312, 14115, 13, 51614, 51614, 467, 486, 1128, 483, 604, 16235, 13, 51714, 51714, 467, 486, 406, 1466, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.053544102293072326, "compression_ratio": 1.812274368231047, "no_speech_prob": 3.6119231481279712e-06}, {"id": 727, "seek": 375300, "start": 3776.0, "end": 3778.0, "text": " This b1 will just be useless.", "tokens": [50364, 400, 300, 15245, 2710, 2144, 12577, 307, 586, 294, 4602, 295, 264, 3228, 3349, 295, 341, 7316, 50614, 50614, 2602, 295, 341, 272, 16, 300, 321, 632, 510, 7993, 13, 50814, 50814, 400, 370, 1936, 11, 264, 15245, 2710, 2144, 4583, 575, 1080, 1065, 12577, 13, 50964, 50964, 400, 456, 311, 572, 643, 281, 362, 257, 12577, 294, 264, 4583, 949, 309, 51164, 51164, 570, 300, 12577, 307, 516, 281, 312, 16390, 292, 484, 4033, 13, 51314, 51314, 407, 300, 311, 264, 661, 1359, 2607, 281, 312, 5026, 365, 2171, 13, 51414, 51414, 467, 311, 406, 516, 281, 360, 1340, 34915, 13, 51514, 51514, 639, 272, 16, 486, 445, 312, 14115, 13, 51614, 51614, 467, 486, 1128, 483, 604, 16235, 13, 51714, 51714, 467, 486, 406, 1466, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.053544102293072326, "compression_ratio": 1.812274368231047, "no_speech_prob": 3.6119231481279712e-06}, {"id": 728, "seek": 375300, "start": 3778.0, "end": 3780.0, "text": " It will never get any gradient.", "tokens": [50364, 400, 300, 15245, 2710, 2144, 12577, 307, 586, 294, 4602, 295, 264, 3228, 3349, 295, 341, 7316, 50614, 50614, 2602, 295, 341, 272, 16, 300, 321, 632, 510, 7993, 13, 50814, 50814, 400, 370, 1936, 11, 264, 15245, 2710, 2144, 4583, 575, 1080, 1065, 12577, 13, 50964, 50964, 400, 456, 311, 572, 643, 281, 362, 257, 12577, 294, 264, 4583, 949, 309, 51164, 51164, 570, 300, 12577, 307, 516, 281, 312, 16390, 292, 484, 4033, 13, 51314, 51314, 407, 300, 311, 264, 661, 1359, 2607, 281, 312, 5026, 365, 2171, 13, 51414, 51414, 467, 311, 406, 516, 281, 360, 1340, 34915, 13, 51514, 51514, 639, 272, 16, 486, 445, 312, 14115, 13, 51614, 51614, 467, 486, 1128, 483, 604, 16235, 13, 51714, 51714, 467, 486, 406, 1466, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.053544102293072326, "compression_ratio": 1.812274368231047, "no_speech_prob": 3.6119231481279712e-06}, {"id": 729, "seek": 375300, "start": 3780.0, "end": 3781.0, "text": " It will not learn.", "tokens": [50364, 400, 300, 15245, 2710, 2144, 12577, 307, 586, 294, 4602, 295, 264, 3228, 3349, 295, 341, 7316, 50614, 50614, 2602, 295, 341, 272, 16, 300, 321, 632, 510, 7993, 13, 50814, 50814, 400, 370, 1936, 11, 264, 15245, 2710, 2144, 4583, 575, 1080, 1065, 12577, 13, 50964, 50964, 400, 456, 311, 572, 643, 281, 362, 257, 12577, 294, 264, 4583, 949, 309, 51164, 51164, 570, 300, 12577, 307, 516, 281, 312, 16390, 292, 484, 4033, 13, 51314, 51314, 407, 300, 311, 264, 661, 1359, 2607, 281, 312, 5026, 365, 2171, 13, 51414, 51414, 467, 311, 406, 516, 281, 360, 1340, 34915, 13, 51514, 51514, 639, 272, 16, 486, 445, 312, 14115, 13, 51614, 51614, 467, 486, 1128, 483, 604, 16235, 13, 51714, 51714, 467, 486, 406, 1466, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.053544102293072326, "compression_ratio": 1.812274368231047, "no_speech_prob": 3.6119231481279712e-06}, {"id": 730, "seek": 378100, "start": 3781.0, "end": 3783.0, "text": " It will stay constant and it's just wasteful,", "tokens": [50364, 467, 486, 1754, 5754, 293, 309, 311, 445, 5964, 906, 11, 50464, 50464, 457, 309, 1177, 380, 767, 534, 2712, 1340, 5911, 13, 50664, 50664, 2264, 11, 370, 286, 29875, 10296, 264, 3089, 257, 707, 857, 365, 3053, 13, 50814, 50814, 400, 286, 445, 1415, 281, 976, 257, 588, 1702, 12691, 295, 264, 15245, 2710, 2144, 4583, 13, 51014, 51014, 492, 366, 1228, 15245, 2710, 2144, 281, 1969, 264, 12523, 295, 2430, 763, 294, 264, 18161, 2533, 13, 51314, 51314, 467, 307, 2689, 281, 24745, 15245, 2710, 2144, 4583, 2108, 264, 18161, 2533, 13, 51464, 51464, 400, 2673, 321, 486, 1081, 309, 934, 7914, 300, 362, 17596, 763, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07013570734884887, "compression_ratio": 1.7732342007434945, "no_speech_prob": 1.4063395610719454e-05}, {"id": 731, "seek": 378100, "start": 3783.0, "end": 3787.0, "text": " but it doesn't actually really impact anything otherwise.", "tokens": [50364, 467, 486, 1754, 5754, 293, 309, 311, 445, 5964, 906, 11, 50464, 50464, 457, 309, 1177, 380, 767, 534, 2712, 1340, 5911, 13, 50664, 50664, 2264, 11, 370, 286, 29875, 10296, 264, 3089, 257, 707, 857, 365, 3053, 13, 50814, 50814, 400, 286, 445, 1415, 281, 976, 257, 588, 1702, 12691, 295, 264, 15245, 2710, 2144, 4583, 13, 51014, 51014, 492, 366, 1228, 15245, 2710, 2144, 281, 1969, 264, 12523, 295, 2430, 763, 294, 264, 18161, 2533, 13, 51314, 51314, 467, 307, 2689, 281, 24745, 15245, 2710, 2144, 4583, 2108, 264, 18161, 2533, 13, 51464, 51464, 400, 2673, 321, 486, 1081, 309, 934, 7914, 300, 362, 17596, 763, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07013570734884887, "compression_ratio": 1.7732342007434945, "no_speech_prob": 1.4063395610719454e-05}, {"id": 732, "seek": 378100, "start": 3787.0, "end": 3790.0, "text": " OK, so I rearranged the code a little bit with comments.", "tokens": [50364, 467, 486, 1754, 5754, 293, 309, 311, 445, 5964, 906, 11, 50464, 50464, 457, 309, 1177, 380, 767, 534, 2712, 1340, 5911, 13, 50664, 50664, 2264, 11, 370, 286, 29875, 10296, 264, 3089, 257, 707, 857, 365, 3053, 13, 50814, 50814, 400, 286, 445, 1415, 281, 976, 257, 588, 1702, 12691, 295, 264, 15245, 2710, 2144, 4583, 13, 51014, 51014, 492, 366, 1228, 15245, 2710, 2144, 281, 1969, 264, 12523, 295, 2430, 763, 294, 264, 18161, 2533, 13, 51314, 51314, 467, 307, 2689, 281, 24745, 15245, 2710, 2144, 4583, 2108, 264, 18161, 2533, 13, 51464, 51464, 400, 2673, 321, 486, 1081, 309, 934, 7914, 300, 362, 17596, 763, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07013570734884887, "compression_ratio": 1.7732342007434945, "no_speech_prob": 1.4063395610719454e-05}, {"id": 733, "seek": 378100, "start": 3790.0, "end": 3794.0, "text": " And I just wanted to give a very quick summary of the batch normalization layer.", "tokens": [50364, 467, 486, 1754, 5754, 293, 309, 311, 445, 5964, 906, 11, 50464, 50464, 457, 309, 1177, 380, 767, 534, 2712, 1340, 5911, 13, 50664, 50664, 2264, 11, 370, 286, 29875, 10296, 264, 3089, 257, 707, 857, 365, 3053, 13, 50814, 50814, 400, 286, 445, 1415, 281, 976, 257, 588, 1702, 12691, 295, 264, 15245, 2710, 2144, 4583, 13, 51014, 51014, 492, 366, 1228, 15245, 2710, 2144, 281, 1969, 264, 12523, 295, 2430, 763, 294, 264, 18161, 2533, 13, 51314, 51314, 467, 307, 2689, 281, 24745, 15245, 2710, 2144, 4583, 2108, 264, 18161, 2533, 13, 51464, 51464, 400, 2673, 321, 486, 1081, 309, 934, 7914, 300, 362, 17596, 763, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07013570734884887, "compression_ratio": 1.7732342007434945, "no_speech_prob": 1.4063395610719454e-05}, {"id": 734, "seek": 378100, "start": 3794.0, "end": 3800.0, "text": " We are using batch normalization to control the statistics of activations in the neural net.", "tokens": [50364, 467, 486, 1754, 5754, 293, 309, 311, 445, 5964, 906, 11, 50464, 50464, 457, 309, 1177, 380, 767, 534, 2712, 1340, 5911, 13, 50664, 50664, 2264, 11, 370, 286, 29875, 10296, 264, 3089, 257, 707, 857, 365, 3053, 13, 50814, 50814, 400, 286, 445, 1415, 281, 976, 257, 588, 1702, 12691, 295, 264, 15245, 2710, 2144, 4583, 13, 51014, 51014, 492, 366, 1228, 15245, 2710, 2144, 281, 1969, 264, 12523, 295, 2430, 763, 294, 264, 18161, 2533, 13, 51314, 51314, 467, 307, 2689, 281, 24745, 15245, 2710, 2144, 4583, 2108, 264, 18161, 2533, 13, 51464, 51464, 400, 2673, 321, 486, 1081, 309, 934, 7914, 300, 362, 17596, 763, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07013570734884887, "compression_ratio": 1.7732342007434945, "no_speech_prob": 1.4063395610719454e-05}, {"id": 735, "seek": 378100, "start": 3800.0, "end": 3803.0, "text": " It is common to sprinkle batch normalization layer across the neural net.", "tokens": [50364, 467, 486, 1754, 5754, 293, 309, 311, 445, 5964, 906, 11, 50464, 50464, 457, 309, 1177, 380, 767, 534, 2712, 1340, 5911, 13, 50664, 50664, 2264, 11, 370, 286, 29875, 10296, 264, 3089, 257, 707, 857, 365, 3053, 13, 50814, 50814, 400, 286, 445, 1415, 281, 976, 257, 588, 1702, 12691, 295, 264, 15245, 2710, 2144, 4583, 13, 51014, 51014, 492, 366, 1228, 15245, 2710, 2144, 281, 1969, 264, 12523, 295, 2430, 763, 294, 264, 18161, 2533, 13, 51314, 51314, 467, 307, 2689, 281, 24745, 15245, 2710, 2144, 4583, 2108, 264, 18161, 2533, 13, 51464, 51464, 400, 2673, 321, 486, 1081, 309, 934, 7914, 300, 362, 17596, 763, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07013570734884887, "compression_ratio": 1.7732342007434945, "no_speech_prob": 1.4063395610719454e-05}, {"id": 736, "seek": 378100, "start": 3803.0, "end": 3808.0, "text": " And usually we will place it after layers that have multiplications,", "tokens": [50364, 467, 486, 1754, 5754, 293, 309, 311, 445, 5964, 906, 11, 50464, 50464, 457, 309, 1177, 380, 767, 534, 2712, 1340, 5911, 13, 50664, 50664, 2264, 11, 370, 286, 29875, 10296, 264, 3089, 257, 707, 857, 365, 3053, 13, 50814, 50814, 400, 286, 445, 1415, 281, 976, 257, 588, 1702, 12691, 295, 264, 15245, 2710, 2144, 4583, 13, 51014, 51014, 492, 366, 1228, 15245, 2710, 2144, 281, 1969, 264, 12523, 295, 2430, 763, 294, 264, 18161, 2533, 13, 51314, 51314, 467, 307, 2689, 281, 24745, 15245, 2710, 2144, 4583, 2108, 264, 18161, 2533, 13, 51464, 51464, 400, 2673, 321, 486, 1081, 309, 934, 7914, 300, 362, 17596, 763, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07013570734884887, "compression_ratio": 1.7732342007434945, "no_speech_prob": 1.4063395610719454e-05}, {"id": 737, "seek": 380800, "start": 3808.0, "end": 3813.0, "text": " for example, a linear layer or a convolutional layer, which we may cover in the future.", "tokens": [50364, 337, 1365, 11, 257, 8213, 4583, 420, 257, 45216, 304, 4583, 11, 597, 321, 815, 2060, 294, 264, 2027, 13, 50614, 50614, 823, 11, 264, 15245, 2710, 2144, 19501, 575, 9834, 337, 264, 6052, 293, 264, 12577, 13, 50914, 50914, 400, 613, 366, 8895, 1228, 646, 79, 1513, 559, 399, 13, 51014, 51014, 467, 611, 575, 732, 9204, 433, 13, 51164, 51164, 440, 9204, 433, 366, 264, 914, 293, 264, 3832, 25163, 11, 51314, 51314, 264, 2614, 914, 293, 264, 2614, 914, 295, 264, 3832, 25163, 13, 51464, 51464, 400, 613, 366, 406, 8895, 1228, 646, 79, 1513, 559, 399, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.058887481689453125, "compression_ratio": 1.9150943396226414, "no_speech_prob": 8.663821063237265e-06}, {"id": 738, "seek": 380800, "start": 3813.0, "end": 3819.0, "text": " Now, the batch normalization internally has parameters for the gain and the bias.", "tokens": [50364, 337, 1365, 11, 257, 8213, 4583, 420, 257, 45216, 304, 4583, 11, 597, 321, 815, 2060, 294, 264, 2027, 13, 50614, 50614, 823, 11, 264, 15245, 2710, 2144, 19501, 575, 9834, 337, 264, 6052, 293, 264, 12577, 13, 50914, 50914, 400, 613, 366, 8895, 1228, 646, 79, 1513, 559, 399, 13, 51014, 51014, 467, 611, 575, 732, 9204, 433, 13, 51164, 51164, 440, 9204, 433, 366, 264, 914, 293, 264, 3832, 25163, 11, 51314, 51314, 264, 2614, 914, 293, 264, 2614, 914, 295, 264, 3832, 25163, 13, 51464, 51464, 400, 613, 366, 406, 8895, 1228, 646, 79, 1513, 559, 399, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.058887481689453125, "compression_ratio": 1.9150943396226414, "no_speech_prob": 8.663821063237265e-06}, {"id": 739, "seek": 380800, "start": 3819.0, "end": 3821.0, "text": " And these are trained using backpropagation.", "tokens": [50364, 337, 1365, 11, 257, 8213, 4583, 420, 257, 45216, 304, 4583, 11, 597, 321, 815, 2060, 294, 264, 2027, 13, 50614, 50614, 823, 11, 264, 15245, 2710, 2144, 19501, 575, 9834, 337, 264, 6052, 293, 264, 12577, 13, 50914, 50914, 400, 613, 366, 8895, 1228, 646, 79, 1513, 559, 399, 13, 51014, 51014, 467, 611, 575, 732, 9204, 433, 13, 51164, 51164, 440, 9204, 433, 366, 264, 914, 293, 264, 3832, 25163, 11, 51314, 51314, 264, 2614, 914, 293, 264, 2614, 914, 295, 264, 3832, 25163, 13, 51464, 51464, 400, 613, 366, 406, 8895, 1228, 646, 79, 1513, 559, 399, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.058887481689453125, "compression_ratio": 1.9150943396226414, "no_speech_prob": 8.663821063237265e-06}, {"id": 740, "seek": 380800, "start": 3821.0, "end": 3824.0, "text": " It also has two buffers.", "tokens": [50364, 337, 1365, 11, 257, 8213, 4583, 420, 257, 45216, 304, 4583, 11, 597, 321, 815, 2060, 294, 264, 2027, 13, 50614, 50614, 823, 11, 264, 15245, 2710, 2144, 19501, 575, 9834, 337, 264, 6052, 293, 264, 12577, 13, 50914, 50914, 400, 613, 366, 8895, 1228, 646, 79, 1513, 559, 399, 13, 51014, 51014, 467, 611, 575, 732, 9204, 433, 13, 51164, 51164, 440, 9204, 433, 366, 264, 914, 293, 264, 3832, 25163, 11, 51314, 51314, 264, 2614, 914, 293, 264, 2614, 914, 295, 264, 3832, 25163, 13, 51464, 51464, 400, 613, 366, 406, 8895, 1228, 646, 79, 1513, 559, 399, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.058887481689453125, "compression_ratio": 1.9150943396226414, "no_speech_prob": 8.663821063237265e-06}, {"id": 741, "seek": 380800, "start": 3824.0, "end": 3827.0, "text": " The buffers are the mean and the standard deviation,", "tokens": [50364, 337, 1365, 11, 257, 8213, 4583, 420, 257, 45216, 304, 4583, 11, 597, 321, 815, 2060, 294, 264, 2027, 13, 50614, 50614, 823, 11, 264, 15245, 2710, 2144, 19501, 575, 9834, 337, 264, 6052, 293, 264, 12577, 13, 50914, 50914, 400, 613, 366, 8895, 1228, 646, 79, 1513, 559, 399, 13, 51014, 51014, 467, 611, 575, 732, 9204, 433, 13, 51164, 51164, 440, 9204, 433, 366, 264, 914, 293, 264, 3832, 25163, 11, 51314, 51314, 264, 2614, 914, 293, 264, 2614, 914, 295, 264, 3832, 25163, 13, 51464, 51464, 400, 613, 366, 406, 8895, 1228, 646, 79, 1513, 559, 399, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.058887481689453125, "compression_ratio": 1.9150943396226414, "no_speech_prob": 8.663821063237265e-06}, {"id": 742, "seek": 380800, "start": 3827.0, "end": 3830.0, "text": " the running mean and the running mean of the standard deviation.", "tokens": [50364, 337, 1365, 11, 257, 8213, 4583, 420, 257, 45216, 304, 4583, 11, 597, 321, 815, 2060, 294, 264, 2027, 13, 50614, 50614, 823, 11, 264, 15245, 2710, 2144, 19501, 575, 9834, 337, 264, 6052, 293, 264, 12577, 13, 50914, 50914, 400, 613, 366, 8895, 1228, 646, 79, 1513, 559, 399, 13, 51014, 51014, 467, 611, 575, 732, 9204, 433, 13, 51164, 51164, 440, 9204, 433, 366, 264, 914, 293, 264, 3832, 25163, 11, 51314, 51314, 264, 2614, 914, 293, 264, 2614, 914, 295, 264, 3832, 25163, 13, 51464, 51464, 400, 613, 366, 406, 8895, 1228, 646, 79, 1513, 559, 399, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.058887481689453125, "compression_ratio": 1.9150943396226414, "no_speech_prob": 8.663821063237265e-06}, {"id": 743, "seek": 380800, "start": 3830.0, "end": 3833.0, "text": " And these are not trained using backpropagation.", "tokens": [50364, 337, 1365, 11, 257, 8213, 4583, 420, 257, 45216, 304, 4583, 11, 597, 321, 815, 2060, 294, 264, 2027, 13, 50614, 50614, 823, 11, 264, 15245, 2710, 2144, 19501, 575, 9834, 337, 264, 6052, 293, 264, 12577, 13, 50914, 50914, 400, 613, 366, 8895, 1228, 646, 79, 1513, 559, 399, 13, 51014, 51014, 467, 611, 575, 732, 9204, 433, 13, 51164, 51164, 440, 9204, 433, 366, 264, 914, 293, 264, 3832, 25163, 11, 51314, 51314, 264, 2614, 914, 293, 264, 2614, 914, 295, 264, 3832, 25163, 13, 51464, 51464, 400, 613, 366, 406, 8895, 1228, 646, 79, 1513, 559, 399, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.058887481689453125, "compression_ratio": 1.9150943396226414, "no_speech_prob": 8.663821063237265e-06}, {"id": 744, "seek": 383300, "start": 3833.0, "end": 3839.0, "text": " These are trained using this janky update of kind of like a running mean update.", "tokens": [50364, 1981, 366, 8895, 1228, 341, 361, 657, 88, 5623, 295, 733, 295, 411, 257, 2614, 914, 5623, 13, 50664, 50664, 407, 613, 366, 1333, 295, 264, 9834, 293, 264, 9204, 433, 295, 264, 15245, 2710, 2144, 4583, 13, 50964, 50964, 400, 550, 534, 437, 309, 311, 884, 307, 309, 311, 28258, 264, 914, 293, 264, 3832, 25163, 51164, 51164, 295, 264, 2430, 763, 300, 366, 12919, 666, 264, 15245, 2710, 2144, 4583, 670, 300, 15245, 13, 51464, 51464, 1396, 309, 311, 1489, 1794, 300, 15245, 281, 312, 4985, 39148, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06595369095497942, "compression_ratio": 1.8073394495412844, "no_speech_prob": 2.726319507928565e-06}, {"id": 745, "seek": 383300, "start": 3839.0, "end": 3845.0, "text": " So these are sort of the parameters and the buffers of the batch normalization layer.", "tokens": [50364, 1981, 366, 8895, 1228, 341, 361, 657, 88, 5623, 295, 733, 295, 411, 257, 2614, 914, 5623, 13, 50664, 50664, 407, 613, 366, 1333, 295, 264, 9834, 293, 264, 9204, 433, 295, 264, 15245, 2710, 2144, 4583, 13, 50964, 50964, 400, 550, 534, 437, 309, 311, 884, 307, 309, 311, 28258, 264, 914, 293, 264, 3832, 25163, 51164, 51164, 295, 264, 2430, 763, 300, 366, 12919, 666, 264, 15245, 2710, 2144, 4583, 670, 300, 15245, 13, 51464, 51464, 1396, 309, 311, 1489, 1794, 300, 15245, 281, 312, 4985, 39148, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06595369095497942, "compression_ratio": 1.8073394495412844, "no_speech_prob": 2.726319507928565e-06}, {"id": 746, "seek": 383300, "start": 3845.0, "end": 3849.0, "text": " And then really what it's doing is it's calculating the mean and the standard deviation", "tokens": [50364, 1981, 366, 8895, 1228, 341, 361, 657, 88, 5623, 295, 733, 295, 411, 257, 2614, 914, 5623, 13, 50664, 50664, 407, 613, 366, 1333, 295, 264, 9834, 293, 264, 9204, 433, 295, 264, 15245, 2710, 2144, 4583, 13, 50964, 50964, 400, 550, 534, 437, 309, 311, 884, 307, 309, 311, 28258, 264, 914, 293, 264, 3832, 25163, 51164, 51164, 295, 264, 2430, 763, 300, 366, 12919, 666, 264, 15245, 2710, 2144, 4583, 670, 300, 15245, 13, 51464, 51464, 1396, 309, 311, 1489, 1794, 300, 15245, 281, 312, 4985, 39148, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06595369095497942, "compression_ratio": 1.8073394495412844, "no_speech_prob": 2.726319507928565e-06}, {"id": 747, "seek": 383300, "start": 3849.0, "end": 3855.0, "text": " of the activations that are feeding into the batch normalization layer over that batch.", "tokens": [50364, 1981, 366, 8895, 1228, 341, 361, 657, 88, 5623, 295, 733, 295, 411, 257, 2614, 914, 5623, 13, 50664, 50664, 407, 613, 366, 1333, 295, 264, 9834, 293, 264, 9204, 433, 295, 264, 15245, 2710, 2144, 4583, 13, 50964, 50964, 400, 550, 534, 437, 309, 311, 884, 307, 309, 311, 28258, 264, 914, 293, 264, 3832, 25163, 51164, 51164, 295, 264, 2430, 763, 300, 366, 12919, 666, 264, 15245, 2710, 2144, 4583, 670, 300, 15245, 13, 51464, 51464, 1396, 309, 311, 1489, 1794, 300, 15245, 281, 312, 4985, 39148, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06595369095497942, "compression_ratio": 1.8073394495412844, "no_speech_prob": 2.726319507928565e-06}, {"id": 748, "seek": 383300, "start": 3855.0, "end": 3858.0, "text": " Then it's centering that batch to be unit Gaussian.", "tokens": [50364, 1981, 366, 8895, 1228, 341, 361, 657, 88, 5623, 295, 733, 295, 411, 257, 2614, 914, 5623, 13, 50664, 50664, 407, 613, 366, 1333, 295, 264, 9834, 293, 264, 9204, 433, 295, 264, 15245, 2710, 2144, 4583, 13, 50964, 50964, 400, 550, 534, 437, 309, 311, 884, 307, 309, 311, 28258, 264, 914, 293, 264, 3832, 25163, 51164, 51164, 295, 264, 2430, 763, 300, 366, 12919, 666, 264, 15245, 2710, 2144, 4583, 670, 300, 15245, 13, 51464, 51464, 1396, 309, 311, 1489, 1794, 300, 15245, 281, 312, 4985, 39148, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06595369095497942, "compression_ratio": 1.8073394495412844, "no_speech_prob": 2.726319507928565e-06}, {"id": 749, "seek": 385800, "start": 3858.0, "end": 3864.0, "text": " And then it's offsetting and scaling it by the learned bias and gain.", "tokens": [50364, 400, 550, 309, 311, 18687, 783, 293, 21589, 309, 538, 264, 3264, 12577, 293, 6052, 13, 50664, 50664, 400, 550, 322, 1192, 295, 300, 11, 309, 311, 5145, 2837, 295, 264, 914, 293, 3832, 25163, 295, 264, 15743, 13, 50914, 50914, 400, 309, 311, 14916, 341, 2614, 914, 293, 3832, 25163, 13, 51114, 51114, 400, 341, 486, 1780, 312, 1143, 412, 38253, 370, 300, 321, 500, 380, 362, 281, 319, 12, 377, 2905, 264, 914, 293, 3832, 25163, 439, 264, 565, 13, 51414, 51414, 400, 294, 4500, 11, 300, 4045, 505, 281, 1936, 2128, 2609, 5110, 412, 1500, 565, 13, 51664, 51664, 407, 300, 311, 264, 15245, 2710, 2144, 4583, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.051188477225925615, "compression_ratio": 1.8893280632411067, "no_speech_prob": 1.7330329455944593e-06}, {"id": 750, "seek": 385800, "start": 3864.0, "end": 3869.0, "text": " And then on top of that, it's keeping track of the mean and standard deviation of the inputs.", "tokens": [50364, 400, 550, 309, 311, 18687, 783, 293, 21589, 309, 538, 264, 3264, 12577, 293, 6052, 13, 50664, 50664, 400, 550, 322, 1192, 295, 300, 11, 309, 311, 5145, 2837, 295, 264, 914, 293, 3832, 25163, 295, 264, 15743, 13, 50914, 50914, 400, 309, 311, 14916, 341, 2614, 914, 293, 3832, 25163, 13, 51114, 51114, 400, 341, 486, 1780, 312, 1143, 412, 38253, 370, 300, 321, 500, 380, 362, 281, 319, 12, 377, 2905, 264, 914, 293, 3832, 25163, 439, 264, 565, 13, 51414, 51414, 400, 294, 4500, 11, 300, 4045, 505, 281, 1936, 2128, 2609, 5110, 412, 1500, 565, 13, 51664, 51664, 407, 300, 311, 264, 15245, 2710, 2144, 4583, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.051188477225925615, "compression_ratio": 1.8893280632411067, "no_speech_prob": 1.7330329455944593e-06}, {"id": 751, "seek": 385800, "start": 3869.0, "end": 3873.0, "text": " And it's maintaining this running mean and standard deviation.", "tokens": [50364, 400, 550, 309, 311, 18687, 783, 293, 21589, 309, 538, 264, 3264, 12577, 293, 6052, 13, 50664, 50664, 400, 550, 322, 1192, 295, 300, 11, 309, 311, 5145, 2837, 295, 264, 914, 293, 3832, 25163, 295, 264, 15743, 13, 50914, 50914, 400, 309, 311, 14916, 341, 2614, 914, 293, 3832, 25163, 13, 51114, 51114, 400, 341, 486, 1780, 312, 1143, 412, 38253, 370, 300, 321, 500, 380, 362, 281, 319, 12, 377, 2905, 264, 914, 293, 3832, 25163, 439, 264, 565, 13, 51414, 51414, 400, 294, 4500, 11, 300, 4045, 505, 281, 1936, 2128, 2609, 5110, 412, 1500, 565, 13, 51664, 51664, 407, 300, 311, 264, 15245, 2710, 2144, 4583, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.051188477225925615, "compression_ratio": 1.8893280632411067, "no_speech_prob": 1.7330329455944593e-06}, {"id": 752, "seek": 385800, "start": 3873.0, "end": 3879.0, "text": " And this will later be used at inference so that we don't have to re-estimate the mean and standard deviation all the time.", "tokens": [50364, 400, 550, 309, 311, 18687, 783, 293, 21589, 309, 538, 264, 3264, 12577, 293, 6052, 13, 50664, 50664, 400, 550, 322, 1192, 295, 300, 11, 309, 311, 5145, 2837, 295, 264, 914, 293, 3832, 25163, 295, 264, 15743, 13, 50914, 50914, 400, 309, 311, 14916, 341, 2614, 914, 293, 3832, 25163, 13, 51114, 51114, 400, 341, 486, 1780, 312, 1143, 412, 38253, 370, 300, 321, 500, 380, 362, 281, 319, 12, 377, 2905, 264, 914, 293, 3832, 25163, 439, 264, 565, 13, 51414, 51414, 400, 294, 4500, 11, 300, 4045, 505, 281, 1936, 2128, 2609, 5110, 412, 1500, 565, 13, 51664, 51664, 407, 300, 311, 264, 15245, 2710, 2144, 4583, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.051188477225925615, "compression_ratio": 1.8893280632411067, "no_speech_prob": 1.7330329455944593e-06}, {"id": 753, "seek": 385800, "start": 3879.0, "end": 3884.0, "text": " And in addition, that allows us to basically forward individual examples at test time.", "tokens": [50364, 400, 550, 309, 311, 18687, 783, 293, 21589, 309, 538, 264, 3264, 12577, 293, 6052, 13, 50664, 50664, 400, 550, 322, 1192, 295, 300, 11, 309, 311, 5145, 2837, 295, 264, 914, 293, 3832, 25163, 295, 264, 15743, 13, 50914, 50914, 400, 309, 311, 14916, 341, 2614, 914, 293, 3832, 25163, 13, 51114, 51114, 400, 341, 486, 1780, 312, 1143, 412, 38253, 370, 300, 321, 500, 380, 362, 281, 319, 12, 377, 2905, 264, 914, 293, 3832, 25163, 439, 264, 565, 13, 51414, 51414, 400, 294, 4500, 11, 300, 4045, 505, 281, 1936, 2128, 2609, 5110, 412, 1500, 565, 13, 51664, 51664, 407, 300, 311, 264, 15245, 2710, 2144, 4583, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.051188477225925615, "compression_ratio": 1.8893280632411067, "no_speech_prob": 1.7330329455944593e-06}, {"id": 754, "seek": 385800, "start": 3884.0, "end": 3886.0, "text": " So that's the batch normalization layer.", "tokens": [50364, 400, 550, 309, 311, 18687, 783, 293, 21589, 309, 538, 264, 3264, 12577, 293, 6052, 13, 50664, 50664, 400, 550, 322, 1192, 295, 300, 11, 309, 311, 5145, 2837, 295, 264, 914, 293, 3832, 25163, 295, 264, 15743, 13, 50914, 50914, 400, 309, 311, 14916, 341, 2614, 914, 293, 3832, 25163, 13, 51114, 51114, 400, 341, 486, 1780, 312, 1143, 412, 38253, 370, 300, 321, 500, 380, 362, 281, 319, 12, 377, 2905, 264, 914, 293, 3832, 25163, 439, 264, 565, 13, 51414, 51414, 400, 294, 4500, 11, 300, 4045, 505, 281, 1936, 2128, 2609, 5110, 412, 1500, 565, 13, 51664, 51664, 407, 300, 311, 264, 15245, 2710, 2144, 4583, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.051188477225925615, "compression_ratio": 1.8893280632411067, "no_speech_prob": 1.7330329455944593e-06}, {"id": 755, "seek": 388600, "start": 3886.0, "end": 3890.0, "text": " It's a fairly complicated layer, but this is what it's doing internally.", "tokens": [50364, 467, 311, 257, 6457, 6179, 4583, 11, 457, 341, 307, 437, 309, 311, 884, 19501, 13, 50564, 50564, 823, 11, 286, 1415, 281, 855, 291, 257, 707, 857, 295, 257, 957, 1365, 13, 50714, 50714, 407, 291, 393, 3164, 5015, 31890, 11, 597, 307, 257, 27980, 18161, 3209, 13, 50964, 50964, 400, 613, 366, 15836, 295, 18161, 9590, 1143, 337, 3256, 21538, 13, 51164, 51164, 400, 295, 1164, 11, 321, 2378, 380, 808, 281, 5015, 31890, 294, 2607, 11, 370, 286, 478, 406, 516, 281, 2903, 439, 264, 3755, 295, 309, 13, 51464, 51464, 583, 337, 586, 11, 445, 3637, 300, 264, 3256, 23712, 666, 257, 5015, 31890, 322, 264, 1192, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06638246112399632, "compression_ratio": 1.637037037037037, "no_speech_prob": 2.156790060325875e-06}, {"id": 756, "seek": 388600, "start": 3890.0, "end": 3893.0, "text": " Now, I wanted to show you a little bit of a real example.", "tokens": [50364, 467, 311, 257, 6457, 6179, 4583, 11, 457, 341, 307, 437, 309, 311, 884, 19501, 13, 50564, 50564, 823, 11, 286, 1415, 281, 855, 291, 257, 707, 857, 295, 257, 957, 1365, 13, 50714, 50714, 407, 291, 393, 3164, 5015, 31890, 11, 597, 307, 257, 27980, 18161, 3209, 13, 50964, 50964, 400, 613, 366, 15836, 295, 18161, 9590, 1143, 337, 3256, 21538, 13, 51164, 51164, 400, 295, 1164, 11, 321, 2378, 380, 808, 281, 5015, 31890, 294, 2607, 11, 370, 286, 478, 406, 516, 281, 2903, 439, 264, 3755, 295, 309, 13, 51464, 51464, 583, 337, 586, 11, 445, 3637, 300, 264, 3256, 23712, 666, 257, 5015, 31890, 322, 264, 1192, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06638246112399632, "compression_ratio": 1.637037037037037, "no_speech_prob": 2.156790060325875e-06}, {"id": 757, "seek": 388600, "start": 3893.0, "end": 3898.0, "text": " So you can search ResNet, which is a residual neural network.", "tokens": [50364, 467, 311, 257, 6457, 6179, 4583, 11, 457, 341, 307, 437, 309, 311, 884, 19501, 13, 50564, 50564, 823, 11, 286, 1415, 281, 855, 291, 257, 707, 857, 295, 257, 957, 1365, 13, 50714, 50714, 407, 291, 393, 3164, 5015, 31890, 11, 597, 307, 257, 27980, 18161, 3209, 13, 50964, 50964, 400, 613, 366, 15836, 295, 18161, 9590, 1143, 337, 3256, 21538, 13, 51164, 51164, 400, 295, 1164, 11, 321, 2378, 380, 808, 281, 5015, 31890, 294, 2607, 11, 370, 286, 478, 406, 516, 281, 2903, 439, 264, 3755, 295, 309, 13, 51464, 51464, 583, 337, 586, 11, 445, 3637, 300, 264, 3256, 23712, 666, 257, 5015, 31890, 322, 264, 1192, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06638246112399632, "compression_ratio": 1.637037037037037, "no_speech_prob": 2.156790060325875e-06}, {"id": 758, "seek": 388600, "start": 3898.0, "end": 3902.0, "text": " And these are contacts of neural networks used for image classification.", "tokens": [50364, 467, 311, 257, 6457, 6179, 4583, 11, 457, 341, 307, 437, 309, 311, 884, 19501, 13, 50564, 50564, 823, 11, 286, 1415, 281, 855, 291, 257, 707, 857, 295, 257, 957, 1365, 13, 50714, 50714, 407, 291, 393, 3164, 5015, 31890, 11, 597, 307, 257, 27980, 18161, 3209, 13, 50964, 50964, 400, 613, 366, 15836, 295, 18161, 9590, 1143, 337, 3256, 21538, 13, 51164, 51164, 400, 295, 1164, 11, 321, 2378, 380, 808, 281, 5015, 31890, 294, 2607, 11, 370, 286, 478, 406, 516, 281, 2903, 439, 264, 3755, 295, 309, 13, 51464, 51464, 583, 337, 586, 11, 445, 3637, 300, 264, 3256, 23712, 666, 257, 5015, 31890, 322, 264, 1192, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06638246112399632, "compression_ratio": 1.637037037037037, "no_speech_prob": 2.156790060325875e-06}, {"id": 759, "seek": 388600, "start": 3902.0, "end": 3908.0, "text": " And of course, we haven't come to ResNet in detail, so I'm not going to explain all the pieces of it.", "tokens": [50364, 467, 311, 257, 6457, 6179, 4583, 11, 457, 341, 307, 437, 309, 311, 884, 19501, 13, 50564, 50564, 823, 11, 286, 1415, 281, 855, 291, 257, 707, 857, 295, 257, 957, 1365, 13, 50714, 50714, 407, 291, 393, 3164, 5015, 31890, 11, 597, 307, 257, 27980, 18161, 3209, 13, 50964, 50964, 400, 613, 366, 15836, 295, 18161, 9590, 1143, 337, 3256, 21538, 13, 51164, 51164, 400, 295, 1164, 11, 321, 2378, 380, 808, 281, 5015, 31890, 294, 2607, 11, 370, 286, 478, 406, 516, 281, 2903, 439, 264, 3755, 295, 309, 13, 51464, 51464, 583, 337, 586, 11, 445, 3637, 300, 264, 3256, 23712, 666, 257, 5015, 31890, 322, 264, 1192, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06638246112399632, "compression_ratio": 1.637037037037037, "no_speech_prob": 2.156790060325875e-06}, {"id": 760, "seek": 388600, "start": 3908.0, "end": 3912.0, "text": " But for now, just note that the image feeds into a ResNet on the top here.", "tokens": [50364, 467, 311, 257, 6457, 6179, 4583, 11, 457, 341, 307, 437, 309, 311, 884, 19501, 13, 50564, 50564, 823, 11, 286, 1415, 281, 855, 291, 257, 707, 857, 295, 257, 957, 1365, 13, 50714, 50714, 407, 291, 393, 3164, 5015, 31890, 11, 597, 307, 257, 27980, 18161, 3209, 13, 50964, 50964, 400, 613, 366, 15836, 295, 18161, 9590, 1143, 337, 3256, 21538, 13, 51164, 51164, 400, 295, 1164, 11, 321, 2378, 380, 808, 281, 5015, 31890, 294, 2607, 11, 370, 286, 478, 406, 516, 281, 2903, 439, 264, 3755, 295, 309, 13, 51464, 51464, 583, 337, 586, 11, 445, 3637, 300, 264, 3256, 23712, 666, 257, 5015, 31890, 322, 264, 1192, 510, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06638246112399632, "compression_ratio": 1.637037037037037, "no_speech_prob": 2.156790060325875e-06}, {"id": 761, "seek": 391200, "start": 3912.0, "end": 3918.0, "text": " And there's many, many layers with repeating structure all the way to predictions of what's inside that image.", "tokens": [50364, 400, 456, 311, 867, 11, 867, 7914, 365, 18617, 3877, 439, 264, 636, 281, 21264, 295, 437, 311, 1854, 300, 3256, 13, 50664, 50664, 639, 18617, 3877, 307, 1027, 493, 295, 613, 8474, 13, 50814, 50814, 400, 613, 8474, 366, 445, 5123, 3137, 28867, 493, 294, 341, 2452, 18161, 3209, 13, 51014, 51014, 823, 11, 264, 3089, 337, 341, 11, 264, 3461, 1936, 300, 311, 1143, 293, 10477, 5123, 3137, 294, 2638, 11, 307, 1219, 341, 44641, 547, 3461, 13, 51564, 51564, 400, 456, 311, 257, 688, 510, 13, 51614, 51614, 639, 307, 439, 9953, 51, 284, 339, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07260260535675345, "compression_ratio": 1.7208333333333334, "no_speech_prob": 1.5445888493559323e-05}, {"id": 762, "seek": 391200, "start": 3918.0, "end": 3921.0, "text": " This repeating structure is made up of these blocks.", "tokens": [50364, 400, 456, 311, 867, 11, 867, 7914, 365, 18617, 3877, 439, 264, 636, 281, 21264, 295, 437, 311, 1854, 300, 3256, 13, 50664, 50664, 639, 18617, 3877, 307, 1027, 493, 295, 613, 8474, 13, 50814, 50814, 400, 613, 8474, 366, 445, 5123, 3137, 28867, 493, 294, 341, 2452, 18161, 3209, 13, 51014, 51014, 823, 11, 264, 3089, 337, 341, 11, 264, 3461, 1936, 300, 311, 1143, 293, 10477, 5123, 3137, 294, 2638, 11, 307, 1219, 341, 44641, 547, 3461, 13, 51564, 51564, 400, 456, 311, 257, 688, 510, 13, 51614, 51614, 639, 307, 439, 9953, 51, 284, 339, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07260260535675345, "compression_ratio": 1.7208333333333334, "no_speech_prob": 1.5445888493559323e-05}, {"id": 763, "seek": 391200, "start": 3921.0, "end": 3925.0, "text": " And these blocks are just sequentially stacked up in this deep neural network.", "tokens": [50364, 400, 456, 311, 867, 11, 867, 7914, 365, 18617, 3877, 439, 264, 636, 281, 21264, 295, 437, 311, 1854, 300, 3256, 13, 50664, 50664, 639, 18617, 3877, 307, 1027, 493, 295, 613, 8474, 13, 50814, 50814, 400, 613, 8474, 366, 445, 5123, 3137, 28867, 493, 294, 341, 2452, 18161, 3209, 13, 51014, 51014, 823, 11, 264, 3089, 337, 341, 11, 264, 3461, 1936, 300, 311, 1143, 293, 10477, 5123, 3137, 294, 2638, 11, 307, 1219, 341, 44641, 547, 3461, 13, 51564, 51564, 400, 456, 311, 257, 688, 510, 13, 51614, 51614, 639, 307, 439, 9953, 51, 284, 339, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07260260535675345, "compression_ratio": 1.7208333333333334, "no_speech_prob": 1.5445888493559323e-05}, {"id": 764, "seek": 391200, "start": 3925.0, "end": 3936.0, "text": " Now, the code for this, the block basically that's used and repeated sequentially in series, is called this bottleneck block.", "tokens": [50364, 400, 456, 311, 867, 11, 867, 7914, 365, 18617, 3877, 439, 264, 636, 281, 21264, 295, 437, 311, 1854, 300, 3256, 13, 50664, 50664, 639, 18617, 3877, 307, 1027, 493, 295, 613, 8474, 13, 50814, 50814, 400, 613, 8474, 366, 445, 5123, 3137, 28867, 493, 294, 341, 2452, 18161, 3209, 13, 51014, 51014, 823, 11, 264, 3089, 337, 341, 11, 264, 3461, 1936, 300, 311, 1143, 293, 10477, 5123, 3137, 294, 2638, 11, 307, 1219, 341, 44641, 547, 3461, 13, 51564, 51564, 400, 456, 311, 257, 688, 510, 13, 51614, 51614, 639, 307, 439, 9953, 51, 284, 339, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07260260535675345, "compression_ratio": 1.7208333333333334, "no_speech_prob": 1.5445888493559323e-05}, {"id": 765, "seek": 391200, "start": 3936.0, "end": 3937.0, "text": " And there's a lot here.", "tokens": [50364, 400, 456, 311, 867, 11, 867, 7914, 365, 18617, 3877, 439, 264, 636, 281, 21264, 295, 437, 311, 1854, 300, 3256, 13, 50664, 50664, 639, 18617, 3877, 307, 1027, 493, 295, 613, 8474, 13, 50814, 50814, 400, 613, 8474, 366, 445, 5123, 3137, 28867, 493, 294, 341, 2452, 18161, 3209, 13, 51014, 51014, 823, 11, 264, 3089, 337, 341, 11, 264, 3461, 1936, 300, 311, 1143, 293, 10477, 5123, 3137, 294, 2638, 11, 307, 1219, 341, 44641, 547, 3461, 13, 51564, 51564, 400, 456, 311, 257, 688, 510, 13, 51614, 51614, 639, 307, 439, 9953, 51, 284, 339, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07260260535675345, "compression_ratio": 1.7208333333333334, "no_speech_prob": 1.5445888493559323e-05}, {"id": 766, "seek": 391200, "start": 3937.0, "end": 3938.0, "text": " This is all PyTorch.", "tokens": [50364, 400, 456, 311, 867, 11, 867, 7914, 365, 18617, 3877, 439, 264, 636, 281, 21264, 295, 437, 311, 1854, 300, 3256, 13, 50664, 50664, 639, 18617, 3877, 307, 1027, 493, 295, 613, 8474, 13, 50814, 50814, 400, 613, 8474, 366, 445, 5123, 3137, 28867, 493, 294, 341, 2452, 18161, 3209, 13, 51014, 51014, 823, 11, 264, 3089, 337, 341, 11, 264, 3461, 1936, 300, 311, 1143, 293, 10477, 5123, 3137, 294, 2638, 11, 307, 1219, 341, 44641, 547, 3461, 13, 51564, 51564, 400, 456, 311, 257, 688, 510, 13, 51614, 51614, 639, 307, 439, 9953, 51, 284, 339, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07260260535675345, "compression_ratio": 1.7208333333333334, "no_speech_prob": 1.5445888493559323e-05}, {"id": 767, "seek": 393800, "start": 3938.0, "end": 3943.0, "text": " And of course, we haven't covered all of it, but I want to point out some small pieces of it.", "tokens": [50364, 400, 295, 1164, 11, 321, 2378, 380, 5343, 439, 295, 309, 11, 457, 286, 528, 281, 935, 484, 512, 1359, 3755, 295, 309, 13, 50614, 50614, 1692, 294, 264, 3157, 307, 689, 321, 5883, 1125, 264, 18161, 2533, 13, 50714, 50714, 407, 341, 3089, 3461, 510, 307, 1936, 264, 733, 295, 1507, 321, 434, 884, 510, 13, 50914, 50914, 492, 434, 5883, 3319, 439, 264, 7914, 13, 51014, 51014, 400, 294, 264, 2128, 11, 321, 366, 1608, 5489, 577, 264, 18161, 2533, 10672, 1564, 291, 767, 362, 264, 4846, 13, 51214, 51214, 407, 341, 3089, 510, 307, 2051, 264, 3876, 295, 437, 321, 434, 884, 510, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.057246427278260924, "compression_ratio": 1.808695652173913, "no_speech_prob": 4.784981683769729e-06}, {"id": 768, "seek": 393800, "start": 3943.0, "end": 3945.0, "text": " Here in the init is where we initialize the neural net.", "tokens": [50364, 400, 295, 1164, 11, 321, 2378, 380, 5343, 439, 295, 309, 11, 457, 286, 528, 281, 935, 484, 512, 1359, 3755, 295, 309, 13, 50614, 50614, 1692, 294, 264, 3157, 307, 689, 321, 5883, 1125, 264, 18161, 2533, 13, 50714, 50714, 407, 341, 3089, 3461, 510, 307, 1936, 264, 733, 295, 1507, 321, 434, 884, 510, 13, 50914, 50914, 492, 434, 5883, 3319, 439, 264, 7914, 13, 51014, 51014, 400, 294, 264, 2128, 11, 321, 366, 1608, 5489, 577, 264, 18161, 2533, 10672, 1564, 291, 767, 362, 264, 4846, 13, 51214, 51214, 407, 341, 3089, 510, 307, 2051, 264, 3876, 295, 437, 321, 434, 884, 510, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.057246427278260924, "compression_ratio": 1.808695652173913, "no_speech_prob": 4.784981683769729e-06}, {"id": 769, "seek": 393800, "start": 3945.0, "end": 3949.0, "text": " So this code block here is basically the kind of stuff we're doing here.", "tokens": [50364, 400, 295, 1164, 11, 321, 2378, 380, 5343, 439, 295, 309, 11, 457, 286, 528, 281, 935, 484, 512, 1359, 3755, 295, 309, 13, 50614, 50614, 1692, 294, 264, 3157, 307, 689, 321, 5883, 1125, 264, 18161, 2533, 13, 50714, 50714, 407, 341, 3089, 3461, 510, 307, 1936, 264, 733, 295, 1507, 321, 434, 884, 510, 13, 50914, 50914, 492, 434, 5883, 3319, 439, 264, 7914, 13, 51014, 51014, 400, 294, 264, 2128, 11, 321, 366, 1608, 5489, 577, 264, 18161, 2533, 10672, 1564, 291, 767, 362, 264, 4846, 13, 51214, 51214, 407, 341, 3089, 510, 307, 2051, 264, 3876, 295, 437, 321, 434, 884, 510, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.057246427278260924, "compression_ratio": 1.808695652173913, "no_speech_prob": 4.784981683769729e-06}, {"id": 770, "seek": 393800, "start": 3949.0, "end": 3951.0, "text": " We're initializing all the layers.", "tokens": [50364, 400, 295, 1164, 11, 321, 2378, 380, 5343, 439, 295, 309, 11, 457, 286, 528, 281, 935, 484, 512, 1359, 3755, 295, 309, 13, 50614, 50614, 1692, 294, 264, 3157, 307, 689, 321, 5883, 1125, 264, 18161, 2533, 13, 50714, 50714, 407, 341, 3089, 3461, 510, 307, 1936, 264, 733, 295, 1507, 321, 434, 884, 510, 13, 50914, 50914, 492, 434, 5883, 3319, 439, 264, 7914, 13, 51014, 51014, 400, 294, 264, 2128, 11, 321, 366, 1608, 5489, 577, 264, 18161, 2533, 10672, 1564, 291, 767, 362, 264, 4846, 13, 51214, 51214, 407, 341, 3089, 510, 307, 2051, 264, 3876, 295, 437, 321, 434, 884, 510, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.057246427278260924, "compression_ratio": 1.808695652173913, "no_speech_prob": 4.784981683769729e-06}, {"id": 771, "seek": 393800, "start": 3951.0, "end": 3955.0, "text": " And in the forward, we are specifying how the neural net acts once you actually have the input.", "tokens": [50364, 400, 295, 1164, 11, 321, 2378, 380, 5343, 439, 295, 309, 11, 457, 286, 528, 281, 935, 484, 512, 1359, 3755, 295, 309, 13, 50614, 50614, 1692, 294, 264, 3157, 307, 689, 321, 5883, 1125, 264, 18161, 2533, 13, 50714, 50714, 407, 341, 3089, 3461, 510, 307, 1936, 264, 733, 295, 1507, 321, 434, 884, 510, 13, 50914, 50914, 492, 434, 5883, 3319, 439, 264, 7914, 13, 51014, 51014, 400, 294, 264, 2128, 11, 321, 366, 1608, 5489, 577, 264, 18161, 2533, 10672, 1564, 291, 767, 362, 264, 4846, 13, 51214, 51214, 407, 341, 3089, 510, 307, 2051, 264, 3876, 295, 437, 321, 434, 884, 510, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.057246427278260924, "compression_ratio": 1.808695652173913, "no_speech_prob": 4.784981683769729e-06}, {"id": 772, "seek": 393800, "start": 3955.0, "end": 3961.0, "text": " So this code here is along the lines of what we're doing here.", "tokens": [50364, 400, 295, 1164, 11, 321, 2378, 380, 5343, 439, 295, 309, 11, 457, 286, 528, 281, 935, 484, 512, 1359, 3755, 295, 309, 13, 50614, 50614, 1692, 294, 264, 3157, 307, 689, 321, 5883, 1125, 264, 18161, 2533, 13, 50714, 50714, 407, 341, 3089, 3461, 510, 307, 1936, 264, 733, 295, 1507, 321, 434, 884, 510, 13, 50914, 50914, 492, 434, 5883, 3319, 439, 264, 7914, 13, 51014, 51014, 400, 294, 264, 2128, 11, 321, 366, 1608, 5489, 577, 264, 18161, 2533, 10672, 1564, 291, 767, 362, 264, 4846, 13, 51214, 51214, 407, 341, 3089, 510, 307, 2051, 264, 3876, 295, 437, 321, 434, 884, 510, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.057246427278260924, "compression_ratio": 1.808695652173913, "no_speech_prob": 4.784981683769729e-06}, {"id": 773, "seek": 396100, "start": 3961.0, "end": 3969.0, "text": " And now these blocks are replicated and stacked up serially, and that's what a residual network would be.", "tokens": [50364, 400, 586, 613, 8474, 366, 46365, 293, 28867, 493, 816, 2270, 11, 293, 300, 311, 437, 257, 27980, 3209, 576, 312, 13, 50764, 50764, 400, 370, 3449, 437, 311, 2737, 510, 13, 50864, 50864, 2656, 85, 16, 11, 613, 366, 45216, 7914, 13, 51064, 51064, 400, 613, 45216, 7914, 11, 1936, 11, 436, 434, 264, 912, 551, 382, 257, 8213, 4583, 11, 3993, 45216, 7914, 500, 380, 3079, 13, 51464, 51464, 2656, 85, 3386, 7914, 366, 1143, 337, 5267, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11706281843639556, "compression_ratio": 1.688118811881188, "no_speech_prob": 1.0129843758477364e-05}, {"id": 774, "seek": 396100, "start": 3969.0, "end": 3971.0, "text": " And so notice what's happening here.", "tokens": [50364, 400, 586, 613, 8474, 366, 46365, 293, 28867, 493, 816, 2270, 11, 293, 300, 311, 437, 257, 27980, 3209, 576, 312, 13, 50764, 50764, 400, 370, 3449, 437, 311, 2737, 510, 13, 50864, 50864, 2656, 85, 16, 11, 613, 366, 45216, 7914, 13, 51064, 51064, 400, 613, 45216, 7914, 11, 1936, 11, 436, 434, 264, 912, 551, 382, 257, 8213, 4583, 11, 3993, 45216, 7914, 500, 380, 3079, 13, 51464, 51464, 2656, 85, 3386, 7914, 366, 1143, 337, 5267, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11706281843639556, "compression_ratio": 1.688118811881188, "no_speech_prob": 1.0129843758477364e-05}, {"id": 775, "seek": 396100, "start": 3971.0, "end": 3975.0, "text": " Conv1, these are convolution layers.", "tokens": [50364, 400, 586, 613, 8474, 366, 46365, 293, 28867, 493, 816, 2270, 11, 293, 300, 311, 437, 257, 27980, 3209, 576, 312, 13, 50764, 50764, 400, 370, 3449, 437, 311, 2737, 510, 13, 50864, 50864, 2656, 85, 16, 11, 613, 366, 45216, 7914, 13, 51064, 51064, 400, 613, 45216, 7914, 11, 1936, 11, 436, 434, 264, 912, 551, 382, 257, 8213, 4583, 11, 3993, 45216, 7914, 500, 380, 3079, 13, 51464, 51464, 2656, 85, 3386, 7914, 366, 1143, 337, 5267, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11706281843639556, "compression_ratio": 1.688118811881188, "no_speech_prob": 1.0129843758477364e-05}, {"id": 776, "seek": 396100, "start": 3975.0, "end": 3983.0, "text": " And these convolution layers, basically, they're the same thing as a linear layer, except convolution layers don't apply.", "tokens": [50364, 400, 586, 613, 8474, 366, 46365, 293, 28867, 493, 816, 2270, 11, 293, 300, 311, 437, 257, 27980, 3209, 576, 312, 13, 50764, 50764, 400, 370, 3449, 437, 311, 2737, 510, 13, 50864, 50864, 2656, 85, 16, 11, 613, 366, 45216, 7914, 13, 51064, 51064, 400, 613, 45216, 7914, 11, 1936, 11, 436, 434, 264, 912, 551, 382, 257, 8213, 4583, 11, 3993, 45216, 7914, 500, 380, 3079, 13, 51464, 51464, 2656, 85, 3386, 7914, 366, 1143, 337, 5267, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11706281843639556, "compression_ratio": 1.688118811881188, "no_speech_prob": 1.0129843758477364e-05}, {"id": 777, "seek": 396100, "start": 3983.0, "end": 3985.0, "text": " Convolution layers are used for images.", "tokens": [50364, 400, 586, 613, 8474, 366, 46365, 293, 28867, 493, 816, 2270, 11, 293, 300, 311, 437, 257, 27980, 3209, 576, 312, 13, 50764, 50764, 400, 370, 3449, 437, 311, 2737, 510, 13, 50864, 50864, 2656, 85, 16, 11, 613, 366, 45216, 7914, 13, 51064, 51064, 400, 613, 45216, 7914, 11, 1936, 11, 436, 434, 264, 912, 551, 382, 257, 8213, 4583, 11, 3993, 45216, 7914, 500, 380, 3079, 13, 51464, 51464, 2656, 85, 3386, 7914, 366, 1143, 337, 5267, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.11706281843639556, "compression_ratio": 1.688118811881188, "no_speech_prob": 1.0129843758477364e-05}, {"id": 778, "seek": 398500, "start": 3985.0, "end": 3995.0, "text": " And so they have spatial structure, and basically this linear multiplication and bias offset are done on patches instead of the full input.", "tokens": [50364, 400, 370, 436, 362, 23598, 3877, 11, 293, 1936, 341, 8213, 27290, 293, 12577, 18687, 366, 1096, 322, 26531, 2602, 295, 264, 1577, 4846, 13, 50864, 50864, 407, 570, 613, 5267, 362, 3877, 11, 23598, 3877, 11, 3754, 15892, 445, 1936, 360, 261, 87, 1804, 272, 11, 457, 436, 360, 309, 322, 33535, 26531, 295, 264, 4846, 13, 51314, 51314, 583, 5911, 11, 309, 311, 261, 87, 1804, 272, 13, 51464, 51464, 1396, 321, 362, 264, 2710, 4583, 11, 597, 538, 7576, 510, 307, 5883, 1602, 281, 312, 257, 15245, 2026, 294, 568, 35, 11, 370, 257, 732, 18795, 15245, 2710, 2144, 4583, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08582379199840405, "compression_ratio": 1.7350746268656716, "no_speech_prob": 3.2377104162151227e-06}, {"id": 779, "seek": 398500, "start": 3995.0, "end": 4004.0, "text": " So because these images have structure, spatial structure, convolutions just basically do wx plus b, but they do it on overlapping patches of the input.", "tokens": [50364, 400, 370, 436, 362, 23598, 3877, 11, 293, 1936, 341, 8213, 27290, 293, 12577, 18687, 366, 1096, 322, 26531, 2602, 295, 264, 1577, 4846, 13, 50864, 50864, 407, 570, 613, 5267, 362, 3877, 11, 23598, 3877, 11, 3754, 15892, 445, 1936, 360, 261, 87, 1804, 272, 11, 457, 436, 360, 309, 322, 33535, 26531, 295, 264, 4846, 13, 51314, 51314, 583, 5911, 11, 309, 311, 261, 87, 1804, 272, 13, 51464, 51464, 1396, 321, 362, 264, 2710, 4583, 11, 597, 538, 7576, 510, 307, 5883, 1602, 281, 312, 257, 15245, 2026, 294, 568, 35, 11, 370, 257, 732, 18795, 15245, 2710, 2144, 4583, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08582379199840405, "compression_ratio": 1.7350746268656716, "no_speech_prob": 3.2377104162151227e-06}, {"id": 780, "seek": 398500, "start": 4004.0, "end": 4007.0, "text": " But otherwise, it's wx plus b.", "tokens": [50364, 400, 370, 436, 362, 23598, 3877, 11, 293, 1936, 341, 8213, 27290, 293, 12577, 18687, 366, 1096, 322, 26531, 2602, 295, 264, 1577, 4846, 13, 50864, 50864, 407, 570, 613, 5267, 362, 3877, 11, 23598, 3877, 11, 3754, 15892, 445, 1936, 360, 261, 87, 1804, 272, 11, 457, 436, 360, 309, 322, 33535, 26531, 295, 264, 4846, 13, 51314, 51314, 583, 5911, 11, 309, 311, 261, 87, 1804, 272, 13, 51464, 51464, 1396, 321, 362, 264, 2710, 4583, 11, 597, 538, 7576, 510, 307, 5883, 1602, 281, 312, 257, 15245, 2026, 294, 568, 35, 11, 370, 257, 732, 18795, 15245, 2710, 2144, 4583, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08582379199840405, "compression_ratio": 1.7350746268656716, "no_speech_prob": 3.2377104162151227e-06}, {"id": 781, "seek": 398500, "start": 4007.0, "end": 4014.0, "text": " Then we have the normal layer, which by default here is initialized to be a batch norm in 2D, so a two dimensional batch normalization layer.", "tokens": [50364, 400, 370, 436, 362, 23598, 3877, 11, 293, 1936, 341, 8213, 27290, 293, 12577, 18687, 366, 1096, 322, 26531, 2602, 295, 264, 1577, 4846, 13, 50864, 50864, 407, 570, 613, 5267, 362, 3877, 11, 23598, 3877, 11, 3754, 15892, 445, 1936, 360, 261, 87, 1804, 272, 11, 457, 436, 360, 309, 322, 33535, 26531, 295, 264, 4846, 13, 51314, 51314, 583, 5911, 11, 309, 311, 261, 87, 1804, 272, 13, 51464, 51464, 1396, 321, 362, 264, 2710, 4583, 11, 597, 538, 7576, 510, 307, 5883, 1602, 281, 312, 257, 15245, 2026, 294, 568, 35, 11, 370, 257, 732, 18795, 15245, 2710, 2144, 4583, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08582379199840405, "compression_ratio": 1.7350746268656716, "no_speech_prob": 3.2377104162151227e-06}, {"id": 782, "seek": 401400, "start": 4014.0, "end": 4016.0, "text": " And then we have a non-linearity like ReLU.", "tokens": [50364, 400, 550, 321, 362, 257, 2107, 12, 1889, 17409, 411, 1300, 43, 52, 13, 50464, 50464, 407, 2602, 295, 510, 436, 764, 1300, 43, 52, 11, 321, 366, 1228, 1266, 71, 294, 341, 1389, 13, 50764, 50764, 583, 1293, 366, 445, 2107, 12, 28263, 1088, 11, 293, 291, 393, 445, 764, 552, 7226, 30358, 1188, 13, 51014, 51014, 1171, 588, 2452, 9590, 11, 1300, 43, 52, 5850, 25790, 984, 589, 257, 857, 1101, 13, 51214, 51214, 407, 536, 264, 39478, 300, 311, 885, 10477, 510, 13, 51364, 51364, 492, 362, 45216, 11, 15245, 2710, 2144, 11, 1300, 43, 52, 11, 45216, 11, 15245, 2710, 2144, 11, 1300, 43, 52, 11, 1030, 11458, 13, 51614, 51614, 400, 550, 510, 341, 307, 257, 27980, 4984, 300, 321, 2378, 380, 5343, 1939, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10524051580856096, "compression_ratio": 1.7410071942446044, "no_speech_prob": 3.5007742553716525e-06}, {"id": 783, "seek": 401400, "start": 4016.0, "end": 4022.0, "text": " So instead of here they use ReLU, we are using 10h in this case.", "tokens": [50364, 400, 550, 321, 362, 257, 2107, 12, 1889, 17409, 411, 1300, 43, 52, 13, 50464, 50464, 407, 2602, 295, 510, 436, 764, 1300, 43, 52, 11, 321, 366, 1228, 1266, 71, 294, 341, 1389, 13, 50764, 50764, 583, 1293, 366, 445, 2107, 12, 28263, 1088, 11, 293, 291, 393, 445, 764, 552, 7226, 30358, 1188, 13, 51014, 51014, 1171, 588, 2452, 9590, 11, 1300, 43, 52, 5850, 25790, 984, 589, 257, 857, 1101, 13, 51214, 51214, 407, 536, 264, 39478, 300, 311, 885, 10477, 510, 13, 51364, 51364, 492, 362, 45216, 11, 15245, 2710, 2144, 11, 1300, 43, 52, 11, 45216, 11, 15245, 2710, 2144, 11, 1300, 43, 52, 11, 1030, 11458, 13, 51614, 51614, 400, 550, 510, 341, 307, 257, 27980, 4984, 300, 321, 2378, 380, 5343, 1939, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10524051580856096, "compression_ratio": 1.7410071942446044, "no_speech_prob": 3.5007742553716525e-06}, {"id": 784, "seek": 401400, "start": 4022.0, "end": 4027.0, "text": " But both are just non-linearities, and you can just use them relatively interchangeably.", "tokens": [50364, 400, 550, 321, 362, 257, 2107, 12, 1889, 17409, 411, 1300, 43, 52, 13, 50464, 50464, 407, 2602, 295, 510, 436, 764, 1300, 43, 52, 11, 321, 366, 1228, 1266, 71, 294, 341, 1389, 13, 50764, 50764, 583, 1293, 366, 445, 2107, 12, 28263, 1088, 11, 293, 291, 393, 445, 764, 552, 7226, 30358, 1188, 13, 51014, 51014, 1171, 588, 2452, 9590, 11, 1300, 43, 52, 5850, 25790, 984, 589, 257, 857, 1101, 13, 51214, 51214, 407, 536, 264, 39478, 300, 311, 885, 10477, 510, 13, 51364, 51364, 492, 362, 45216, 11, 15245, 2710, 2144, 11, 1300, 43, 52, 11, 45216, 11, 15245, 2710, 2144, 11, 1300, 43, 52, 11, 1030, 11458, 13, 51614, 51614, 400, 550, 510, 341, 307, 257, 27980, 4984, 300, 321, 2378, 380, 5343, 1939, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10524051580856096, "compression_ratio": 1.7410071942446044, "no_speech_prob": 3.5007742553716525e-06}, {"id": 785, "seek": 401400, "start": 4027.0, "end": 4031.0, "text": " For very deep networks, ReLU typically empirically work a bit better.", "tokens": [50364, 400, 550, 321, 362, 257, 2107, 12, 1889, 17409, 411, 1300, 43, 52, 13, 50464, 50464, 407, 2602, 295, 510, 436, 764, 1300, 43, 52, 11, 321, 366, 1228, 1266, 71, 294, 341, 1389, 13, 50764, 50764, 583, 1293, 366, 445, 2107, 12, 28263, 1088, 11, 293, 291, 393, 445, 764, 552, 7226, 30358, 1188, 13, 51014, 51014, 1171, 588, 2452, 9590, 11, 1300, 43, 52, 5850, 25790, 984, 589, 257, 857, 1101, 13, 51214, 51214, 407, 536, 264, 39478, 300, 311, 885, 10477, 510, 13, 51364, 51364, 492, 362, 45216, 11, 15245, 2710, 2144, 11, 1300, 43, 52, 11, 45216, 11, 15245, 2710, 2144, 11, 1300, 43, 52, 11, 1030, 11458, 13, 51614, 51614, 400, 550, 510, 341, 307, 257, 27980, 4984, 300, 321, 2378, 380, 5343, 1939, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10524051580856096, "compression_ratio": 1.7410071942446044, "no_speech_prob": 3.5007742553716525e-06}, {"id": 786, "seek": 401400, "start": 4031.0, "end": 4034.0, "text": " So see the motif that's being repeated here.", "tokens": [50364, 400, 550, 321, 362, 257, 2107, 12, 1889, 17409, 411, 1300, 43, 52, 13, 50464, 50464, 407, 2602, 295, 510, 436, 764, 1300, 43, 52, 11, 321, 366, 1228, 1266, 71, 294, 341, 1389, 13, 50764, 50764, 583, 1293, 366, 445, 2107, 12, 28263, 1088, 11, 293, 291, 393, 445, 764, 552, 7226, 30358, 1188, 13, 51014, 51014, 1171, 588, 2452, 9590, 11, 1300, 43, 52, 5850, 25790, 984, 589, 257, 857, 1101, 13, 51214, 51214, 407, 536, 264, 39478, 300, 311, 885, 10477, 510, 13, 51364, 51364, 492, 362, 45216, 11, 15245, 2710, 2144, 11, 1300, 43, 52, 11, 45216, 11, 15245, 2710, 2144, 11, 1300, 43, 52, 11, 1030, 11458, 13, 51614, 51614, 400, 550, 510, 341, 307, 257, 27980, 4984, 300, 321, 2378, 380, 5343, 1939, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10524051580856096, "compression_ratio": 1.7410071942446044, "no_speech_prob": 3.5007742553716525e-06}, {"id": 787, "seek": 401400, "start": 4034.0, "end": 4039.0, "text": " We have convolution, batch normalization, ReLU, convolution, batch normalization, ReLU, et cetera.", "tokens": [50364, 400, 550, 321, 362, 257, 2107, 12, 1889, 17409, 411, 1300, 43, 52, 13, 50464, 50464, 407, 2602, 295, 510, 436, 764, 1300, 43, 52, 11, 321, 366, 1228, 1266, 71, 294, 341, 1389, 13, 50764, 50764, 583, 1293, 366, 445, 2107, 12, 28263, 1088, 11, 293, 291, 393, 445, 764, 552, 7226, 30358, 1188, 13, 51014, 51014, 1171, 588, 2452, 9590, 11, 1300, 43, 52, 5850, 25790, 984, 589, 257, 857, 1101, 13, 51214, 51214, 407, 536, 264, 39478, 300, 311, 885, 10477, 510, 13, 51364, 51364, 492, 362, 45216, 11, 15245, 2710, 2144, 11, 1300, 43, 52, 11, 45216, 11, 15245, 2710, 2144, 11, 1300, 43, 52, 11, 1030, 11458, 13, 51614, 51614, 400, 550, 510, 341, 307, 257, 27980, 4984, 300, 321, 2378, 380, 5343, 1939, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10524051580856096, "compression_ratio": 1.7410071942446044, "no_speech_prob": 3.5007742553716525e-06}, {"id": 788, "seek": 401400, "start": 4039.0, "end": 4043.0, "text": " And then here this is a residual connection that we haven't covered yet.", "tokens": [50364, 400, 550, 321, 362, 257, 2107, 12, 1889, 17409, 411, 1300, 43, 52, 13, 50464, 50464, 407, 2602, 295, 510, 436, 764, 1300, 43, 52, 11, 321, 366, 1228, 1266, 71, 294, 341, 1389, 13, 50764, 50764, 583, 1293, 366, 445, 2107, 12, 28263, 1088, 11, 293, 291, 393, 445, 764, 552, 7226, 30358, 1188, 13, 51014, 51014, 1171, 588, 2452, 9590, 11, 1300, 43, 52, 5850, 25790, 984, 589, 257, 857, 1101, 13, 51214, 51214, 407, 536, 264, 39478, 300, 311, 885, 10477, 510, 13, 51364, 51364, 492, 362, 45216, 11, 15245, 2710, 2144, 11, 1300, 43, 52, 11, 45216, 11, 15245, 2710, 2144, 11, 1300, 43, 52, 11, 1030, 11458, 13, 51614, 51614, 400, 550, 510, 341, 307, 257, 27980, 4984, 300, 321, 2378, 380, 5343, 1939, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10524051580856096, "compression_ratio": 1.7410071942446044, "no_speech_prob": 3.5007742553716525e-06}, {"id": 789, "seek": 404300, "start": 4043.0, "end": 4045.0, "text": " But basically that's the exact same pattern we have here.", "tokens": [50364, 583, 1936, 300, 311, 264, 1900, 912, 5102, 321, 362, 510, 13, 50464, 50464, 492, 362, 257, 3364, 4583, 11, 411, 257, 45216, 420, 411, 257, 8213, 4583, 11, 15245, 2710, 2144, 11, 293, 550, 1266, 71, 11, 597, 307, 257, 2107, 12, 1889, 17409, 13, 50964, 50964, 583, 1936, 257, 3364, 4583, 11, 257, 2710, 2144, 4583, 11, 293, 2107, 12, 1889, 17409, 13, 51164, 51164, 400, 300, 311, 264, 39478, 300, 291, 576, 312, 41376, 493, 562, 291, 1884, 613, 2452, 18161, 9590, 11, 2293, 382, 309, 311, 1096, 510, 13, 51464, 51464, 400, 472, 544, 551, 286, 1116, 411, 291, 281, 3449, 307, 300, 510, 562, 436, 366, 5883, 3319, 264, 715, 7914, 11, 411, 715, 472, 538, 472, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07847897086556502, "compression_ratio": 1.8726591760299625, "no_speech_prob": 2.2473064746009186e-05}, {"id": 790, "seek": 404300, "start": 4045.0, "end": 4055.0, "text": " We have a weight layer, like a convolution or like a linear layer, batch normalization, and then 10h, which is a non-linearity.", "tokens": [50364, 583, 1936, 300, 311, 264, 1900, 912, 5102, 321, 362, 510, 13, 50464, 50464, 492, 362, 257, 3364, 4583, 11, 411, 257, 45216, 420, 411, 257, 8213, 4583, 11, 15245, 2710, 2144, 11, 293, 550, 1266, 71, 11, 597, 307, 257, 2107, 12, 1889, 17409, 13, 50964, 50964, 583, 1936, 257, 3364, 4583, 11, 257, 2710, 2144, 4583, 11, 293, 2107, 12, 1889, 17409, 13, 51164, 51164, 400, 300, 311, 264, 39478, 300, 291, 576, 312, 41376, 493, 562, 291, 1884, 613, 2452, 18161, 9590, 11, 2293, 382, 309, 311, 1096, 510, 13, 51464, 51464, 400, 472, 544, 551, 286, 1116, 411, 291, 281, 3449, 307, 300, 510, 562, 436, 366, 5883, 3319, 264, 715, 7914, 11, 411, 715, 472, 538, 472, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07847897086556502, "compression_ratio": 1.8726591760299625, "no_speech_prob": 2.2473064746009186e-05}, {"id": 791, "seek": 404300, "start": 4055.0, "end": 4059.0, "text": " But basically a weight layer, a normalization layer, and non-linearity.", "tokens": [50364, 583, 1936, 300, 311, 264, 1900, 912, 5102, 321, 362, 510, 13, 50464, 50464, 492, 362, 257, 3364, 4583, 11, 411, 257, 45216, 420, 411, 257, 8213, 4583, 11, 15245, 2710, 2144, 11, 293, 550, 1266, 71, 11, 597, 307, 257, 2107, 12, 1889, 17409, 13, 50964, 50964, 583, 1936, 257, 3364, 4583, 11, 257, 2710, 2144, 4583, 11, 293, 2107, 12, 1889, 17409, 13, 51164, 51164, 400, 300, 311, 264, 39478, 300, 291, 576, 312, 41376, 493, 562, 291, 1884, 613, 2452, 18161, 9590, 11, 2293, 382, 309, 311, 1096, 510, 13, 51464, 51464, 400, 472, 544, 551, 286, 1116, 411, 291, 281, 3449, 307, 300, 510, 562, 436, 366, 5883, 3319, 264, 715, 7914, 11, 411, 715, 472, 538, 472, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07847897086556502, "compression_ratio": 1.8726591760299625, "no_speech_prob": 2.2473064746009186e-05}, {"id": 792, "seek": 404300, "start": 4059.0, "end": 4065.0, "text": " And that's the motif that you would be stacking up when you create these deep neural networks, exactly as it's done here.", "tokens": [50364, 583, 1936, 300, 311, 264, 1900, 912, 5102, 321, 362, 510, 13, 50464, 50464, 492, 362, 257, 3364, 4583, 11, 411, 257, 45216, 420, 411, 257, 8213, 4583, 11, 15245, 2710, 2144, 11, 293, 550, 1266, 71, 11, 597, 307, 257, 2107, 12, 1889, 17409, 13, 50964, 50964, 583, 1936, 257, 3364, 4583, 11, 257, 2710, 2144, 4583, 11, 293, 2107, 12, 1889, 17409, 13, 51164, 51164, 400, 300, 311, 264, 39478, 300, 291, 576, 312, 41376, 493, 562, 291, 1884, 613, 2452, 18161, 9590, 11, 2293, 382, 309, 311, 1096, 510, 13, 51464, 51464, 400, 472, 544, 551, 286, 1116, 411, 291, 281, 3449, 307, 300, 510, 562, 436, 366, 5883, 3319, 264, 715, 7914, 11, 411, 715, 472, 538, 472, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07847897086556502, "compression_ratio": 1.8726591760299625, "no_speech_prob": 2.2473064746009186e-05}, {"id": 793, "seek": 404300, "start": 4065.0, "end": 4071.0, "text": " And one more thing I'd like you to notice is that here when they are initializing the comp layers, like comp one by one,", "tokens": [50364, 583, 1936, 300, 311, 264, 1900, 912, 5102, 321, 362, 510, 13, 50464, 50464, 492, 362, 257, 3364, 4583, 11, 411, 257, 45216, 420, 411, 257, 8213, 4583, 11, 15245, 2710, 2144, 11, 293, 550, 1266, 71, 11, 597, 307, 257, 2107, 12, 1889, 17409, 13, 50964, 50964, 583, 1936, 257, 3364, 4583, 11, 257, 2710, 2144, 4583, 11, 293, 2107, 12, 1889, 17409, 13, 51164, 51164, 400, 300, 311, 264, 39478, 300, 291, 576, 312, 41376, 493, 562, 291, 1884, 613, 2452, 18161, 9590, 11, 2293, 382, 309, 311, 1096, 510, 13, 51464, 51464, 400, 472, 544, 551, 286, 1116, 411, 291, 281, 3449, 307, 300, 510, 562, 436, 366, 5883, 3319, 264, 715, 7914, 11, 411, 715, 472, 538, 472, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07847897086556502, "compression_ratio": 1.8726591760299625, "no_speech_prob": 2.2473064746009186e-05}, {"id": 794, "seek": 407100, "start": 4071.0, "end": 4074.0, "text": " the depth for that is right here.", "tokens": [50364, 264, 7161, 337, 300, 307, 558, 510, 13, 50514, 50514, 400, 370, 309, 311, 5883, 3319, 364, 297, 77, 13, 24697, 17, 67, 11, 597, 307, 257, 45216, 4583, 294, 9953, 51, 284, 339, 13, 50764, 50764, 400, 456, 311, 257, 3840, 295, 20428, 12869, 510, 300, 286, 478, 406, 516, 281, 2903, 1939, 13, 50914, 50914, 583, 291, 536, 577, 456, 311, 12577, 6915, 7908, 13, 51014, 51014, 440, 12577, 6915, 7908, 307, 2293, 337, 264, 912, 1778, 382, 12577, 307, 406, 1143, 294, 527, 1389, 13, 51314, 51314, 509, 536, 577, 286, 23525, 264, 764, 295, 12577, 13, 51414, 51414, 400, 264, 764, 295, 12577, 307, 637, 24274, 570, 934, 341, 3364, 4583, 11, 456, 311, 257, 15245, 2710, 2144, 13, 51614, 51614, 400, 264, 15245, 2710, 2144, 16390, 82, 300, 12577, 293, 550, 575, 1080, 1065, 12577, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08903235073747306, "compression_ratio": 1.8373702422145328, "no_speech_prob": 1.922244882734958e-05}, {"id": 795, "seek": 407100, "start": 4074.0, "end": 4079.0, "text": " And so it's initializing an nn.conf2d, which is a convolution layer in PyTorch.", "tokens": [50364, 264, 7161, 337, 300, 307, 558, 510, 13, 50514, 50514, 400, 370, 309, 311, 5883, 3319, 364, 297, 77, 13, 24697, 17, 67, 11, 597, 307, 257, 45216, 4583, 294, 9953, 51, 284, 339, 13, 50764, 50764, 400, 456, 311, 257, 3840, 295, 20428, 12869, 510, 300, 286, 478, 406, 516, 281, 2903, 1939, 13, 50914, 50914, 583, 291, 536, 577, 456, 311, 12577, 6915, 7908, 13, 51014, 51014, 440, 12577, 6915, 7908, 307, 2293, 337, 264, 912, 1778, 382, 12577, 307, 406, 1143, 294, 527, 1389, 13, 51314, 51314, 509, 536, 577, 286, 23525, 264, 764, 295, 12577, 13, 51414, 51414, 400, 264, 764, 295, 12577, 307, 637, 24274, 570, 934, 341, 3364, 4583, 11, 456, 311, 257, 15245, 2710, 2144, 13, 51614, 51614, 400, 264, 15245, 2710, 2144, 16390, 82, 300, 12577, 293, 550, 575, 1080, 1065, 12577, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08903235073747306, "compression_ratio": 1.8373702422145328, "no_speech_prob": 1.922244882734958e-05}, {"id": 796, "seek": 407100, "start": 4079.0, "end": 4082.0, "text": " And there's a bunch of keyword arguments here that I'm not going to explain yet.", "tokens": [50364, 264, 7161, 337, 300, 307, 558, 510, 13, 50514, 50514, 400, 370, 309, 311, 5883, 3319, 364, 297, 77, 13, 24697, 17, 67, 11, 597, 307, 257, 45216, 4583, 294, 9953, 51, 284, 339, 13, 50764, 50764, 400, 456, 311, 257, 3840, 295, 20428, 12869, 510, 300, 286, 478, 406, 516, 281, 2903, 1939, 13, 50914, 50914, 583, 291, 536, 577, 456, 311, 12577, 6915, 7908, 13, 51014, 51014, 440, 12577, 6915, 7908, 307, 2293, 337, 264, 912, 1778, 382, 12577, 307, 406, 1143, 294, 527, 1389, 13, 51314, 51314, 509, 536, 577, 286, 23525, 264, 764, 295, 12577, 13, 51414, 51414, 400, 264, 764, 295, 12577, 307, 637, 24274, 570, 934, 341, 3364, 4583, 11, 456, 311, 257, 15245, 2710, 2144, 13, 51614, 51614, 400, 264, 15245, 2710, 2144, 16390, 82, 300, 12577, 293, 550, 575, 1080, 1065, 12577, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08903235073747306, "compression_ratio": 1.8373702422145328, "no_speech_prob": 1.922244882734958e-05}, {"id": 797, "seek": 407100, "start": 4082.0, "end": 4084.0, "text": " But you see how there's bias equals false.", "tokens": [50364, 264, 7161, 337, 300, 307, 558, 510, 13, 50514, 50514, 400, 370, 309, 311, 5883, 3319, 364, 297, 77, 13, 24697, 17, 67, 11, 597, 307, 257, 45216, 4583, 294, 9953, 51, 284, 339, 13, 50764, 50764, 400, 456, 311, 257, 3840, 295, 20428, 12869, 510, 300, 286, 478, 406, 516, 281, 2903, 1939, 13, 50914, 50914, 583, 291, 536, 577, 456, 311, 12577, 6915, 7908, 13, 51014, 51014, 440, 12577, 6915, 7908, 307, 2293, 337, 264, 912, 1778, 382, 12577, 307, 406, 1143, 294, 527, 1389, 13, 51314, 51314, 509, 536, 577, 286, 23525, 264, 764, 295, 12577, 13, 51414, 51414, 400, 264, 764, 295, 12577, 307, 637, 24274, 570, 934, 341, 3364, 4583, 11, 456, 311, 257, 15245, 2710, 2144, 13, 51614, 51614, 400, 264, 15245, 2710, 2144, 16390, 82, 300, 12577, 293, 550, 575, 1080, 1065, 12577, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08903235073747306, "compression_ratio": 1.8373702422145328, "no_speech_prob": 1.922244882734958e-05}, {"id": 798, "seek": 407100, "start": 4084.0, "end": 4090.0, "text": " The bias equals false is exactly for the same reason as bias is not used in our case.", "tokens": [50364, 264, 7161, 337, 300, 307, 558, 510, 13, 50514, 50514, 400, 370, 309, 311, 5883, 3319, 364, 297, 77, 13, 24697, 17, 67, 11, 597, 307, 257, 45216, 4583, 294, 9953, 51, 284, 339, 13, 50764, 50764, 400, 456, 311, 257, 3840, 295, 20428, 12869, 510, 300, 286, 478, 406, 516, 281, 2903, 1939, 13, 50914, 50914, 583, 291, 536, 577, 456, 311, 12577, 6915, 7908, 13, 51014, 51014, 440, 12577, 6915, 7908, 307, 2293, 337, 264, 912, 1778, 382, 12577, 307, 406, 1143, 294, 527, 1389, 13, 51314, 51314, 509, 536, 577, 286, 23525, 264, 764, 295, 12577, 13, 51414, 51414, 400, 264, 764, 295, 12577, 307, 637, 24274, 570, 934, 341, 3364, 4583, 11, 456, 311, 257, 15245, 2710, 2144, 13, 51614, 51614, 400, 264, 15245, 2710, 2144, 16390, 82, 300, 12577, 293, 550, 575, 1080, 1065, 12577, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08903235073747306, "compression_ratio": 1.8373702422145328, "no_speech_prob": 1.922244882734958e-05}, {"id": 799, "seek": 407100, "start": 4090.0, "end": 4092.0, "text": " You see how I erase the use of bias.", "tokens": [50364, 264, 7161, 337, 300, 307, 558, 510, 13, 50514, 50514, 400, 370, 309, 311, 5883, 3319, 364, 297, 77, 13, 24697, 17, 67, 11, 597, 307, 257, 45216, 4583, 294, 9953, 51, 284, 339, 13, 50764, 50764, 400, 456, 311, 257, 3840, 295, 20428, 12869, 510, 300, 286, 478, 406, 516, 281, 2903, 1939, 13, 50914, 50914, 583, 291, 536, 577, 456, 311, 12577, 6915, 7908, 13, 51014, 51014, 440, 12577, 6915, 7908, 307, 2293, 337, 264, 912, 1778, 382, 12577, 307, 406, 1143, 294, 527, 1389, 13, 51314, 51314, 509, 536, 577, 286, 23525, 264, 764, 295, 12577, 13, 51414, 51414, 400, 264, 764, 295, 12577, 307, 637, 24274, 570, 934, 341, 3364, 4583, 11, 456, 311, 257, 15245, 2710, 2144, 13, 51614, 51614, 400, 264, 15245, 2710, 2144, 16390, 82, 300, 12577, 293, 550, 575, 1080, 1065, 12577, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08903235073747306, "compression_ratio": 1.8373702422145328, "no_speech_prob": 1.922244882734958e-05}, {"id": 800, "seek": 407100, "start": 4092.0, "end": 4096.0, "text": " And the use of bias is spurious because after this weight layer, there's a batch normalization.", "tokens": [50364, 264, 7161, 337, 300, 307, 558, 510, 13, 50514, 50514, 400, 370, 309, 311, 5883, 3319, 364, 297, 77, 13, 24697, 17, 67, 11, 597, 307, 257, 45216, 4583, 294, 9953, 51, 284, 339, 13, 50764, 50764, 400, 456, 311, 257, 3840, 295, 20428, 12869, 510, 300, 286, 478, 406, 516, 281, 2903, 1939, 13, 50914, 50914, 583, 291, 536, 577, 456, 311, 12577, 6915, 7908, 13, 51014, 51014, 440, 12577, 6915, 7908, 307, 2293, 337, 264, 912, 1778, 382, 12577, 307, 406, 1143, 294, 527, 1389, 13, 51314, 51314, 509, 536, 577, 286, 23525, 264, 764, 295, 12577, 13, 51414, 51414, 400, 264, 764, 295, 12577, 307, 637, 24274, 570, 934, 341, 3364, 4583, 11, 456, 311, 257, 15245, 2710, 2144, 13, 51614, 51614, 400, 264, 15245, 2710, 2144, 16390, 82, 300, 12577, 293, 550, 575, 1080, 1065, 12577, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08903235073747306, "compression_ratio": 1.8373702422145328, "no_speech_prob": 1.922244882734958e-05}, {"id": 801, "seek": 407100, "start": 4096.0, "end": 4100.0, "text": " And the batch normalization subtracts that bias and then has its own bias.", "tokens": [50364, 264, 7161, 337, 300, 307, 558, 510, 13, 50514, 50514, 400, 370, 309, 311, 5883, 3319, 364, 297, 77, 13, 24697, 17, 67, 11, 597, 307, 257, 45216, 4583, 294, 9953, 51, 284, 339, 13, 50764, 50764, 400, 456, 311, 257, 3840, 295, 20428, 12869, 510, 300, 286, 478, 406, 516, 281, 2903, 1939, 13, 50914, 50914, 583, 291, 536, 577, 456, 311, 12577, 6915, 7908, 13, 51014, 51014, 440, 12577, 6915, 7908, 307, 2293, 337, 264, 912, 1778, 382, 12577, 307, 406, 1143, 294, 527, 1389, 13, 51314, 51314, 509, 536, 577, 286, 23525, 264, 764, 295, 12577, 13, 51414, 51414, 400, 264, 764, 295, 12577, 307, 637, 24274, 570, 934, 341, 3364, 4583, 11, 456, 311, 257, 15245, 2710, 2144, 13, 51614, 51614, 400, 264, 15245, 2710, 2144, 16390, 82, 300, 12577, 293, 550, 575, 1080, 1065, 12577, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08903235073747306, "compression_ratio": 1.8373702422145328, "no_speech_prob": 1.922244882734958e-05}, {"id": 802, "seek": 410000, "start": 4100.0, "end": 4103.0, "text": " So there's no need to introduce these spurious parameters.", "tokens": [50364, 407, 456, 311, 572, 643, 281, 5366, 613, 637, 24274, 9834, 13, 50514, 50514, 467, 2759, 380, 4607, 3389, 13, 467, 311, 445, 14115, 13, 50614, 50614, 400, 370, 570, 436, 362, 341, 39478, 295, 3754, 11, 15245, 11, 293, 1039, 84, 11, 436, 500, 380, 643, 257, 12577, 510, 11, 570, 456, 311, 257, 12577, 1854, 510, 13, 51014, 51014, 407, 11, 538, 264, 636, 11, 341, 1365, 510, 307, 588, 1858, 281, 915, 13, 51214, 51214, 1449, 360, 5015, 31890, 9953, 51, 284, 339, 13, 400, 309, 311, 341, 1365, 510, 13, 51414, 51414, 407, 341, 307, 733, 295, 411, 264, 4127, 11420, 295, 257, 27980, 18161, 3209, 294, 9953, 51, 284, 339, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12348535855611166, "compression_ratio": 1.667953667953668, "no_speech_prob": 4.710837856691796e-06}, {"id": 803, "seek": 410000, "start": 4103.0, "end": 4105.0, "text": " It wouldn't hurt performance. It's just useless.", "tokens": [50364, 407, 456, 311, 572, 643, 281, 5366, 613, 637, 24274, 9834, 13, 50514, 50514, 467, 2759, 380, 4607, 3389, 13, 467, 311, 445, 14115, 13, 50614, 50614, 400, 370, 570, 436, 362, 341, 39478, 295, 3754, 11, 15245, 11, 293, 1039, 84, 11, 436, 500, 380, 643, 257, 12577, 510, 11, 570, 456, 311, 257, 12577, 1854, 510, 13, 51014, 51014, 407, 11, 538, 264, 636, 11, 341, 1365, 510, 307, 588, 1858, 281, 915, 13, 51214, 51214, 1449, 360, 5015, 31890, 9953, 51, 284, 339, 13, 400, 309, 311, 341, 1365, 510, 13, 51414, 51414, 407, 341, 307, 733, 295, 411, 264, 4127, 11420, 295, 257, 27980, 18161, 3209, 294, 9953, 51, 284, 339, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12348535855611166, "compression_ratio": 1.667953667953668, "no_speech_prob": 4.710837856691796e-06}, {"id": 804, "seek": 410000, "start": 4105.0, "end": 4113.0, "text": " And so because they have this motif of conv, batch, and relu, they don't need a bias here, because there's a bias inside here.", "tokens": [50364, 407, 456, 311, 572, 643, 281, 5366, 613, 637, 24274, 9834, 13, 50514, 50514, 467, 2759, 380, 4607, 3389, 13, 467, 311, 445, 14115, 13, 50614, 50614, 400, 370, 570, 436, 362, 341, 39478, 295, 3754, 11, 15245, 11, 293, 1039, 84, 11, 436, 500, 380, 643, 257, 12577, 510, 11, 570, 456, 311, 257, 12577, 1854, 510, 13, 51014, 51014, 407, 11, 538, 264, 636, 11, 341, 1365, 510, 307, 588, 1858, 281, 915, 13, 51214, 51214, 1449, 360, 5015, 31890, 9953, 51, 284, 339, 13, 400, 309, 311, 341, 1365, 510, 13, 51414, 51414, 407, 341, 307, 733, 295, 411, 264, 4127, 11420, 295, 257, 27980, 18161, 3209, 294, 9953, 51, 284, 339, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12348535855611166, "compression_ratio": 1.667953667953668, "no_speech_prob": 4.710837856691796e-06}, {"id": 805, "seek": 410000, "start": 4113.0, "end": 4117.0, "text": " So, by the way, this example here is very easy to find.", "tokens": [50364, 407, 456, 311, 572, 643, 281, 5366, 613, 637, 24274, 9834, 13, 50514, 50514, 467, 2759, 380, 4607, 3389, 13, 467, 311, 445, 14115, 13, 50614, 50614, 400, 370, 570, 436, 362, 341, 39478, 295, 3754, 11, 15245, 11, 293, 1039, 84, 11, 436, 500, 380, 643, 257, 12577, 510, 11, 570, 456, 311, 257, 12577, 1854, 510, 13, 51014, 51014, 407, 11, 538, 264, 636, 11, 341, 1365, 510, 307, 588, 1858, 281, 915, 13, 51214, 51214, 1449, 360, 5015, 31890, 9953, 51, 284, 339, 13, 400, 309, 311, 341, 1365, 510, 13, 51414, 51414, 407, 341, 307, 733, 295, 411, 264, 4127, 11420, 295, 257, 27980, 18161, 3209, 294, 9953, 51, 284, 339, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12348535855611166, "compression_ratio": 1.667953667953668, "no_speech_prob": 4.710837856691796e-06}, {"id": 806, "seek": 410000, "start": 4117.0, "end": 4121.0, "text": " Just do ResNet PyTorch. And it's this example here.", "tokens": [50364, 407, 456, 311, 572, 643, 281, 5366, 613, 637, 24274, 9834, 13, 50514, 50514, 467, 2759, 380, 4607, 3389, 13, 467, 311, 445, 14115, 13, 50614, 50614, 400, 370, 570, 436, 362, 341, 39478, 295, 3754, 11, 15245, 11, 293, 1039, 84, 11, 436, 500, 380, 643, 257, 12577, 510, 11, 570, 456, 311, 257, 12577, 1854, 510, 13, 51014, 51014, 407, 11, 538, 264, 636, 11, 341, 1365, 510, 307, 588, 1858, 281, 915, 13, 51214, 51214, 1449, 360, 5015, 31890, 9953, 51, 284, 339, 13, 400, 309, 311, 341, 1365, 510, 13, 51414, 51414, 407, 341, 307, 733, 295, 411, 264, 4127, 11420, 295, 257, 27980, 18161, 3209, 294, 9953, 51, 284, 339, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12348535855611166, "compression_ratio": 1.667953667953668, "no_speech_prob": 4.710837856691796e-06}, {"id": 807, "seek": 410000, "start": 4121.0, "end": 4126.0, "text": " So this is kind of like the stock implementation of a residual neural network in PyTorch.", "tokens": [50364, 407, 456, 311, 572, 643, 281, 5366, 613, 637, 24274, 9834, 13, 50514, 50514, 467, 2759, 380, 4607, 3389, 13, 467, 311, 445, 14115, 13, 50614, 50614, 400, 370, 570, 436, 362, 341, 39478, 295, 3754, 11, 15245, 11, 293, 1039, 84, 11, 436, 500, 380, 643, 257, 12577, 510, 11, 570, 456, 311, 257, 12577, 1854, 510, 13, 51014, 51014, 407, 11, 538, 264, 636, 11, 341, 1365, 510, 307, 588, 1858, 281, 915, 13, 51214, 51214, 1449, 360, 5015, 31890, 9953, 51, 284, 339, 13, 400, 309, 311, 341, 1365, 510, 13, 51414, 51414, 407, 341, 307, 733, 295, 411, 264, 4127, 11420, 295, 257, 27980, 18161, 3209, 294, 9953, 51, 284, 339, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12348535855611166, "compression_ratio": 1.667953667953668, "no_speech_prob": 4.710837856691796e-06}, {"id": 808, "seek": 412600, "start": 4126.0, "end": 4130.0, "text": " And you can find that here. But of course, I haven't covered many of these parts yet.", "tokens": [50364, 400, 291, 393, 915, 300, 510, 13, 583, 295, 1164, 11, 286, 2378, 380, 5343, 867, 295, 613, 3166, 1939, 13, 50564, 50564, 400, 286, 576, 611, 411, 281, 10515, 16333, 666, 264, 21988, 295, 613, 9953, 51, 284, 339, 7914, 293, 264, 9834, 300, 436, 747, 13, 50914, 50914, 823, 11, 2602, 295, 257, 45216, 304, 4583, 11, 321, 434, 516, 281, 574, 412, 257, 8213, 4583, 570, 300, 311, 264, 472, 300, 321, 434, 1228, 510, 13, 51214, 51214, 639, 307, 257, 8213, 4583, 13, 400, 286, 2378, 380, 5343, 3754, 15892, 1939, 13, 51364, 51364, 583, 382, 286, 2835, 11, 3754, 15892, 366, 1936, 8213, 7914, 3993, 322, 26531, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06124993674775474, "compression_ratio": 1.7849056603773585, "no_speech_prob": 1.4738723621121608e-05}, {"id": 809, "seek": 412600, "start": 4130.0, "end": 4137.0, "text": " And I would also like to briefly descend into the definitions of these PyTorch layers and the parameters that they take.", "tokens": [50364, 400, 291, 393, 915, 300, 510, 13, 583, 295, 1164, 11, 286, 2378, 380, 5343, 867, 295, 613, 3166, 1939, 13, 50564, 50564, 400, 286, 576, 611, 411, 281, 10515, 16333, 666, 264, 21988, 295, 613, 9953, 51, 284, 339, 7914, 293, 264, 9834, 300, 436, 747, 13, 50914, 50914, 823, 11, 2602, 295, 257, 45216, 304, 4583, 11, 321, 434, 516, 281, 574, 412, 257, 8213, 4583, 570, 300, 311, 264, 472, 300, 321, 434, 1228, 510, 13, 51214, 51214, 639, 307, 257, 8213, 4583, 13, 400, 286, 2378, 380, 5343, 3754, 15892, 1939, 13, 51364, 51364, 583, 382, 286, 2835, 11, 3754, 15892, 366, 1936, 8213, 7914, 3993, 322, 26531, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06124993674775474, "compression_ratio": 1.7849056603773585, "no_speech_prob": 1.4738723621121608e-05}, {"id": 810, "seek": 412600, "start": 4137.0, "end": 4143.0, "text": " Now, instead of a convolutional layer, we're going to look at a linear layer because that's the one that we're using here.", "tokens": [50364, 400, 291, 393, 915, 300, 510, 13, 583, 295, 1164, 11, 286, 2378, 380, 5343, 867, 295, 613, 3166, 1939, 13, 50564, 50564, 400, 286, 576, 611, 411, 281, 10515, 16333, 666, 264, 21988, 295, 613, 9953, 51, 284, 339, 7914, 293, 264, 9834, 300, 436, 747, 13, 50914, 50914, 823, 11, 2602, 295, 257, 45216, 304, 4583, 11, 321, 434, 516, 281, 574, 412, 257, 8213, 4583, 570, 300, 311, 264, 472, 300, 321, 434, 1228, 510, 13, 51214, 51214, 639, 307, 257, 8213, 4583, 13, 400, 286, 2378, 380, 5343, 3754, 15892, 1939, 13, 51364, 51364, 583, 382, 286, 2835, 11, 3754, 15892, 366, 1936, 8213, 7914, 3993, 322, 26531, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06124993674775474, "compression_ratio": 1.7849056603773585, "no_speech_prob": 1.4738723621121608e-05}, {"id": 811, "seek": 412600, "start": 4143.0, "end": 4146.0, "text": " This is a linear layer. And I haven't covered convolutions yet.", "tokens": [50364, 400, 291, 393, 915, 300, 510, 13, 583, 295, 1164, 11, 286, 2378, 380, 5343, 867, 295, 613, 3166, 1939, 13, 50564, 50564, 400, 286, 576, 611, 411, 281, 10515, 16333, 666, 264, 21988, 295, 613, 9953, 51, 284, 339, 7914, 293, 264, 9834, 300, 436, 747, 13, 50914, 50914, 823, 11, 2602, 295, 257, 45216, 304, 4583, 11, 321, 434, 516, 281, 574, 412, 257, 8213, 4583, 570, 300, 311, 264, 472, 300, 321, 434, 1228, 510, 13, 51214, 51214, 639, 307, 257, 8213, 4583, 13, 400, 286, 2378, 380, 5343, 3754, 15892, 1939, 13, 51364, 51364, 583, 382, 286, 2835, 11, 3754, 15892, 366, 1936, 8213, 7914, 3993, 322, 26531, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06124993674775474, "compression_ratio": 1.7849056603773585, "no_speech_prob": 1.4738723621121608e-05}, {"id": 812, "seek": 412600, "start": 4146.0, "end": 4151.0, "text": " But as I mentioned, convolutions are basically linear layers except on patches.", "tokens": [50364, 400, 291, 393, 915, 300, 510, 13, 583, 295, 1164, 11, 286, 2378, 380, 5343, 867, 295, 613, 3166, 1939, 13, 50564, 50564, 400, 286, 576, 611, 411, 281, 10515, 16333, 666, 264, 21988, 295, 613, 9953, 51, 284, 339, 7914, 293, 264, 9834, 300, 436, 747, 13, 50914, 50914, 823, 11, 2602, 295, 257, 45216, 304, 4583, 11, 321, 434, 516, 281, 574, 412, 257, 8213, 4583, 570, 300, 311, 264, 472, 300, 321, 434, 1228, 510, 13, 51214, 51214, 639, 307, 257, 8213, 4583, 13, 400, 286, 2378, 380, 5343, 3754, 15892, 1939, 13, 51364, 51364, 583, 382, 286, 2835, 11, 3754, 15892, 366, 1936, 8213, 7914, 3993, 322, 26531, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06124993674775474, "compression_ratio": 1.7849056603773585, "no_speech_prob": 1.4738723621121608e-05}, {"id": 813, "seek": 415100, "start": 4151.0, "end": 4158.0, "text": " So a linear layer performs a wx plus b, except here they're calling the w a transpose.", "tokens": [50364, 407, 257, 8213, 4583, 26213, 257, 261, 87, 1804, 272, 11, 3993, 510, 436, 434, 5141, 264, 261, 257, 25167, 13, 50714, 50714, 407, 264, 3754, 307, 261, 87, 1804, 272, 11, 588, 709, 411, 321, 630, 510, 13, 50864, 50864, 1407, 5883, 1125, 341, 4583, 11, 291, 643, 281, 458, 264, 3429, 294, 11, 264, 3429, 484, 13, 51064, 51064, 400, 300, 311, 370, 300, 436, 393, 5883, 1125, 341, 261, 13, 51264, 51264, 639, 307, 264, 3429, 294, 293, 264, 3429, 484, 13, 407, 436, 458, 577, 955, 264, 3364, 8141, 820, 312, 13, 51564, 51564, 509, 643, 281, 611, 1320, 294, 1968, 420, 406, 291, 528, 257, 12577, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10440218859705432, "compression_ratio": 1.7304347826086957, "no_speech_prob": 1.028916085488163e-05}, {"id": 814, "seek": 415100, "start": 4158.0, "end": 4161.0, "text": " So the conv is wx plus b, very much like we did here.", "tokens": [50364, 407, 257, 8213, 4583, 26213, 257, 261, 87, 1804, 272, 11, 3993, 510, 436, 434, 5141, 264, 261, 257, 25167, 13, 50714, 50714, 407, 264, 3754, 307, 261, 87, 1804, 272, 11, 588, 709, 411, 321, 630, 510, 13, 50864, 50864, 1407, 5883, 1125, 341, 4583, 11, 291, 643, 281, 458, 264, 3429, 294, 11, 264, 3429, 484, 13, 51064, 51064, 400, 300, 311, 370, 300, 436, 393, 5883, 1125, 341, 261, 13, 51264, 51264, 639, 307, 264, 3429, 294, 293, 264, 3429, 484, 13, 407, 436, 458, 577, 955, 264, 3364, 8141, 820, 312, 13, 51564, 51564, 509, 643, 281, 611, 1320, 294, 1968, 420, 406, 291, 528, 257, 12577, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10440218859705432, "compression_ratio": 1.7304347826086957, "no_speech_prob": 1.028916085488163e-05}, {"id": 815, "seek": 415100, "start": 4161.0, "end": 4165.0, "text": " To initialize this layer, you need to know the fan in, the fan out.", "tokens": [50364, 407, 257, 8213, 4583, 26213, 257, 261, 87, 1804, 272, 11, 3993, 510, 436, 434, 5141, 264, 261, 257, 25167, 13, 50714, 50714, 407, 264, 3754, 307, 261, 87, 1804, 272, 11, 588, 709, 411, 321, 630, 510, 13, 50864, 50864, 1407, 5883, 1125, 341, 4583, 11, 291, 643, 281, 458, 264, 3429, 294, 11, 264, 3429, 484, 13, 51064, 51064, 400, 300, 311, 370, 300, 436, 393, 5883, 1125, 341, 261, 13, 51264, 51264, 639, 307, 264, 3429, 294, 293, 264, 3429, 484, 13, 407, 436, 458, 577, 955, 264, 3364, 8141, 820, 312, 13, 51564, 51564, 509, 643, 281, 611, 1320, 294, 1968, 420, 406, 291, 528, 257, 12577, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10440218859705432, "compression_ratio": 1.7304347826086957, "no_speech_prob": 1.028916085488163e-05}, {"id": 816, "seek": 415100, "start": 4165.0, "end": 4169.0, "text": " And that's so that they can initialize this w.", "tokens": [50364, 407, 257, 8213, 4583, 26213, 257, 261, 87, 1804, 272, 11, 3993, 510, 436, 434, 5141, 264, 261, 257, 25167, 13, 50714, 50714, 407, 264, 3754, 307, 261, 87, 1804, 272, 11, 588, 709, 411, 321, 630, 510, 13, 50864, 50864, 1407, 5883, 1125, 341, 4583, 11, 291, 643, 281, 458, 264, 3429, 294, 11, 264, 3429, 484, 13, 51064, 51064, 400, 300, 311, 370, 300, 436, 393, 5883, 1125, 341, 261, 13, 51264, 51264, 639, 307, 264, 3429, 294, 293, 264, 3429, 484, 13, 407, 436, 458, 577, 955, 264, 3364, 8141, 820, 312, 13, 51564, 51564, 509, 643, 281, 611, 1320, 294, 1968, 420, 406, 291, 528, 257, 12577, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10440218859705432, "compression_ratio": 1.7304347826086957, "no_speech_prob": 1.028916085488163e-05}, {"id": 817, "seek": 415100, "start": 4169.0, "end": 4175.0, "text": " This is the fan in and the fan out. So they know how big the weight matrix should be.", "tokens": [50364, 407, 257, 8213, 4583, 26213, 257, 261, 87, 1804, 272, 11, 3993, 510, 436, 434, 5141, 264, 261, 257, 25167, 13, 50714, 50714, 407, 264, 3754, 307, 261, 87, 1804, 272, 11, 588, 709, 411, 321, 630, 510, 13, 50864, 50864, 1407, 5883, 1125, 341, 4583, 11, 291, 643, 281, 458, 264, 3429, 294, 11, 264, 3429, 484, 13, 51064, 51064, 400, 300, 311, 370, 300, 436, 393, 5883, 1125, 341, 261, 13, 51264, 51264, 639, 307, 264, 3429, 294, 293, 264, 3429, 484, 13, 407, 436, 458, 577, 955, 264, 3364, 8141, 820, 312, 13, 51564, 51564, 509, 643, 281, 611, 1320, 294, 1968, 420, 406, 291, 528, 257, 12577, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10440218859705432, "compression_ratio": 1.7304347826086957, "no_speech_prob": 1.028916085488163e-05}, {"id": 818, "seek": 415100, "start": 4175.0, "end": 4179.0, "text": " You need to also pass in whether or not you want a bias.", "tokens": [50364, 407, 257, 8213, 4583, 26213, 257, 261, 87, 1804, 272, 11, 3993, 510, 436, 434, 5141, 264, 261, 257, 25167, 13, 50714, 50714, 407, 264, 3754, 307, 261, 87, 1804, 272, 11, 588, 709, 411, 321, 630, 510, 13, 50864, 50864, 1407, 5883, 1125, 341, 4583, 11, 291, 643, 281, 458, 264, 3429, 294, 11, 264, 3429, 484, 13, 51064, 51064, 400, 300, 311, 370, 300, 436, 393, 5883, 1125, 341, 261, 13, 51264, 51264, 639, 307, 264, 3429, 294, 293, 264, 3429, 484, 13, 407, 436, 458, 577, 955, 264, 3364, 8141, 820, 312, 13, 51564, 51564, 509, 643, 281, 611, 1320, 294, 1968, 420, 406, 291, 528, 257, 12577, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10440218859705432, "compression_ratio": 1.7304347826086957, "no_speech_prob": 1.028916085488163e-05}, {"id": 819, "seek": 417900, "start": 4179.0, "end": 4184.0, "text": " And if you set it to false, then no bias will be inside this layer.", "tokens": [50364, 400, 498, 291, 992, 309, 281, 7908, 11, 550, 572, 12577, 486, 312, 1854, 341, 4583, 13, 50614, 50614, 400, 291, 815, 528, 281, 360, 300, 2293, 411, 294, 527, 1389, 498, 428, 4583, 307, 6263, 538, 257, 2710, 2144, 4583, 1270, 382, 15245, 2026, 13, 50964, 50964, 407, 341, 4045, 291, 281, 1936, 28362, 12577, 13, 51114, 51114, 682, 2115, 295, 264, 5883, 2144, 11, 498, 321, 11173, 760, 510, 11, 341, 307, 10031, 264, 9102, 1143, 1854, 341, 8213, 4583, 13, 51464, 51464, 400, 527, 8213, 4583, 510, 575, 732, 9834, 11, 264, 3364, 293, 264, 12577, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0687257509965163, "compression_ratio": 1.7246963562753037, "no_speech_prob": 7.888946129241958e-06}, {"id": 820, "seek": 417900, "start": 4184.0, "end": 4191.0, "text": " And you may want to do that exactly like in our case if your layer is followed by a normalization layer such as batch norm.", "tokens": [50364, 400, 498, 291, 992, 309, 281, 7908, 11, 550, 572, 12577, 486, 312, 1854, 341, 4583, 13, 50614, 50614, 400, 291, 815, 528, 281, 360, 300, 2293, 411, 294, 527, 1389, 498, 428, 4583, 307, 6263, 538, 257, 2710, 2144, 4583, 1270, 382, 15245, 2026, 13, 50964, 50964, 407, 341, 4045, 291, 281, 1936, 28362, 12577, 13, 51114, 51114, 682, 2115, 295, 264, 5883, 2144, 11, 498, 321, 11173, 760, 510, 11, 341, 307, 10031, 264, 9102, 1143, 1854, 341, 8213, 4583, 13, 51464, 51464, 400, 527, 8213, 4583, 510, 575, 732, 9834, 11, 264, 3364, 293, 264, 12577, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0687257509965163, "compression_ratio": 1.7246963562753037, "no_speech_prob": 7.888946129241958e-06}, {"id": 821, "seek": 417900, "start": 4191.0, "end": 4194.0, "text": " So this allows you to basically disable bias.", "tokens": [50364, 400, 498, 291, 992, 309, 281, 7908, 11, 550, 572, 12577, 486, 312, 1854, 341, 4583, 13, 50614, 50614, 400, 291, 815, 528, 281, 360, 300, 2293, 411, 294, 527, 1389, 498, 428, 4583, 307, 6263, 538, 257, 2710, 2144, 4583, 1270, 382, 15245, 2026, 13, 50964, 50964, 407, 341, 4045, 291, 281, 1936, 28362, 12577, 13, 51114, 51114, 682, 2115, 295, 264, 5883, 2144, 11, 498, 321, 11173, 760, 510, 11, 341, 307, 10031, 264, 9102, 1143, 1854, 341, 8213, 4583, 13, 51464, 51464, 400, 527, 8213, 4583, 510, 575, 732, 9834, 11, 264, 3364, 293, 264, 12577, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0687257509965163, "compression_ratio": 1.7246963562753037, "no_speech_prob": 7.888946129241958e-06}, {"id": 822, "seek": 417900, "start": 4194.0, "end": 4201.0, "text": " In terms of the initialization, if we swing down here, this is reporting the variables used inside this linear layer.", "tokens": [50364, 400, 498, 291, 992, 309, 281, 7908, 11, 550, 572, 12577, 486, 312, 1854, 341, 4583, 13, 50614, 50614, 400, 291, 815, 528, 281, 360, 300, 2293, 411, 294, 527, 1389, 498, 428, 4583, 307, 6263, 538, 257, 2710, 2144, 4583, 1270, 382, 15245, 2026, 13, 50964, 50964, 407, 341, 4045, 291, 281, 1936, 28362, 12577, 13, 51114, 51114, 682, 2115, 295, 264, 5883, 2144, 11, 498, 321, 11173, 760, 510, 11, 341, 307, 10031, 264, 9102, 1143, 1854, 341, 8213, 4583, 13, 51464, 51464, 400, 527, 8213, 4583, 510, 575, 732, 9834, 11, 264, 3364, 293, 264, 12577, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0687257509965163, "compression_ratio": 1.7246963562753037, "no_speech_prob": 7.888946129241958e-06}, {"id": 823, "seek": 417900, "start": 4201.0, "end": 4206.0, "text": " And our linear layer here has two parameters, the weight and the bias.", "tokens": [50364, 400, 498, 291, 992, 309, 281, 7908, 11, 550, 572, 12577, 486, 312, 1854, 341, 4583, 13, 50614, 50614, 400, 291, 815, 528, 281, 360, 300, 2293, 411, 294, 527, 1389, 498, 428, 4583, 307, 6263, 538, 257, 2710, 2144, 4583, 1270, 382, 15245, 2026, 13, 50964, 50964, 407, 341, 4045, 291, 281, 1936, 28362, 12577, 13, 51114, 51114, 682, 2115, 295, 264, 5883, 2144, 11, 498, 321, 11173, 760, 510, 11, 341, 307, 10031, 264, 9102, 1143, 1854, 341, 8213, 4583, 13, 51464, 51464, 400, 527, 8213, 4583, 510, 575, 732, 9834, 11, 264, 3364, 293, 264, 12577, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0687257509965163, "compression_ratio": 1.7246963562753037, "no_speech_prob": 7.888946129241958e-06}, {"id": 824, "seek": 420600, "start": 4206.0, "end": 4209.0, "text": " In the same way, they have a weight and a bias.", "tokens": [50364, 682, 264, 912, 636, 11, 436, 362, 257, 3364, 293, 257, 12577, 13, 50514, 50514, 400, 436, 434, 1417, 466, 577, 436, 5883, 1125, 309, 538, 7576, 13, 50664, 50664, 407, 538, 7576, 11, 9953, 51, 284, 339, 486, 5883, 1125, 428, 17443, 538, 1940, 264, 3429, 294, 293, 550, 884, 472, 670, 3429, 294, 3732, 5593, 13, 51114, 51114, 400, 550, 2602, 295, 257, 2710, 7316, 11, 436, 366, 1228, 257, 9452, 7316, 13, 51364, 51364, 407, 309, 311, 588, 709, 264, 912, 551, 11, 457, 436, 366, 1228, 257, 472, 2602, 295, 1732, 670, 1045, 13, 51614, 51614, 407, 456, 311, 572, 6052, 885, 15598, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06389155430076397, "compression_ratio": 1.8166666666666667, "no_speech_prob": 9.22326034924481e-06}, {"id": 825, "seek": 420600, "start": 4209.0, "end": 4212.0, "text": " And they're talking about how they initialize it by default.", "tokens": [50364, 682, 264, 912, 636, 11, 436, 362, 257, 3364, 293, 257, 12577, 13, 50514, 50514, 400, 436, 434, 1417, 466, 577, 436, 5883, 1125, 309, 538, 7576, 13, 50664, 50664, 407, 538, 7576, 11, 9953, 51, 284, 339, 486, 5883, 1125, 428, 17443, 538, 1940, 264, 3429, 294, 293, 550, 884, 472, 670, 3429, 294, 3732, 5593, 13, 51114, 51114, 400, 550, 2602, 295, 257, 2710, 7316, 11, 436, 366, 1228, 257, 9452, 7316, 13, 51364, 51364, 407, 309, 311, 588, 709, 264, 912, 551, 11, 457, 436, 366, 1228, 257, 472, 2602, 295, 1732, 670, 1045, 13, 51614, 51614, 407, 456, 311, 572, 6052, 885, 15598, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06389155430076397, "compression_ratio": 1.8166666666666667, "no_speech_prob": 9.22326034924481e-06}, {"id": 826, "seek": 420600, "start": 4212.0, "end": 4221.0, "text": " So by default, PyTorch will initialize your weights by taking the fan in and then doing one over fan in square root.", "tokens": [50364, 682, 264, 912, 636, 11, 436, 362, 257, 3364, 293, 257, 12577, 13, 50514, 50514, 400, 436, 434, 1417, 466, 577, 436, 5883, 1125, 309, 538, 7576, 13, 50664, 50664, 407, 538, 7576, 11, 9953, 51, 284, 339, 486, 5883, 1125, 428, 17443, 538, 1940, 264, 3429, 294, 293, 550, 884, 472, 670, 3429, 294, 3732, 5593, 13, 51114, 51114, 400, 550, 2602, 295, 257, 2710, 7316, 11, 436, 366, 1228, 257, 9452, 7316, 13, 51364, 51364, 407, 309, 311, 588, 709, 264, 912, 551, 11, 457, 436, 366, 1228, 257, 472, 2602, 295, 1732, 670, 1045, 13, 51614, 51614, 407, 456, 311, 572, 6052, 885, 15598, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06389155430076397, "compression_ratio": 1.8166666666666667, "no_speech_prob": 9.22326034924481e-06}, {"id": 827, "seek": 420600, "start": 4221.0, "end": 4226.0, "text": " And then instead of a normal distribution, they are using a uniform distribution.", "tokens": [50364, 682, 264, 912, 636, 11, 436, 362, 257, 3364, 293, 257, 12577, 13, 50514, 50514, 400, 436, 434, 1417, 466, 577, 436, 5883, 1125, 309, 538, 7576, 13, 50664, 50664, 407, 538, 7576, 11, 9953, 51, 284, 339, 486, 5883, 1125, 428, 17443, 538, 1940, 264, 3429, 294, 293, 550, 884, 472, 670, 3429, 294, 3732, 5593, 13, 51114, 51114, 400, 550, 2602, 295, 257, 2710, 7316, 11, 436, 366, 1228, 257, 9452, 7316, 13, 51364, 51364, 407, 309, 311, 588, 709, 264, 912, 551, 11, 457, 436, 366, 1228, 257, 472, 2602, 295, 1732, 670, 1045, 13, 51614, 51614, 407, 456, 311, 572, 6052, 885, 15598, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06389155430076397, "compression_ratio": 1.8166666666666667, "no_speech_prob": 9.22326034924481e-06}, {"id": 828, "seek": 420600, "start": 4226.0, "end": 4231.0, "text": " So it's very much the same thing, but they are using a one instead of five over three.", "tokens": [50364, 682, 264, 912, 636, 11, 436, 362, 257, 3364, 293, 257, 12577, 13, 50514, 50514, 400, 436, 434, 1417, 466, 577, 436, 5883, 1125, 309, 538, 7576, 13, 50664, 50664, 407, 538, 7576, 11, 9953, 51, 284, 339, 486, 5883, 1125, 428, 17443, 538, 1940, 264, 3429, 294, 293, 550, 884, 472, 670, 3429, 294, 3732, 5593, 13, 51114, 51114, 400, 550, 2602, 295, 257, 2710, 7316, 11, 436, 366, 1228, 257, 9452, 7316, 13, 51364, 51364, 407, 309, 311, 588, 709, 264, 912, 551, 11, 457, 436, 366, 1228, 257, 472, 2602, 295, 1732, 670, 1045, 13, 51614, 51614, 407, 456, 311, 572, 6052, 885, 15598, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06389155430076397, "compression_ratio": 1.8166666666666667, "no_speech_prob": 9.22326034924481e-06}, {"id": 829, "seek": 420600, "start": 4231.0, "end": 4233.0, "text": " So there's no gain being calculated here.", "tokens": [50364, 682, 264, 912, 636, 11, 436, 362, 257, 3364, 293, 257, 12577, 13, 50514, 50514, 400, 436, 434, 1417, 466, 577, 436, 5883, 1125, 309, 538, 7576, 13, 50664, 50664, 407, 538, 7576, 11, 9953, 51, 284, 339, 486, 5883, 1125, 428, 17443, 538, 1940, 264, 3429, 294, 293, 550, 884, 472, 670, 3429, 294, 3732, 5593, 13, 51114, 51114, 400, 550, 2602, 295, 257, 2710, 7316, 11, 436, 366, 1228, 257, 9452, 7316, 13, 51364, 51364, 407, 309, 311, 588, 709, 264, 912, 551, 11, 457, 436, 366, 1228, 257, 472, 2602, 295, 1732, 670, 1045, 13, 51614, 51614, 407, 456, 311, 572, 6052, 885, 15598, 510, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06389155430076397, "compression_ratio": 1.8166666666666667, "no_speech_prob": 9.22326034924481e-06}, {"id": 830, "seek": 423300, "start": 4233.0, "end": 4240.0, "text": " It's just one, but otherwise it's exactly one over the square root of fan in exactly as we have here.", "tokens": [50364, 467, 311, 445, 472, 11, 457, 5911, 309, 311, 2293, 472, 670, 264, 3732, 5593, 295, 3429, 294, 2293, 382, 321, 362, 510, 13, 50714, 50714, 407, 472, 670, 264, 3732, 5593, 295, 350, 307, 264, 4373, 295, 264, 17443, 13, 50964, 50964, 583, 562, 436, 366, 6316, 264, 3547, 11, 436, 434, 406, 1228, 257, 39148, 538, 7576, 13, 51164, 51164, 814, 434, 1228, 257, 9452, 7316, 538, 7576, 13, 51264, 51264, 400, 370, 436, 2642, 48806, 490, 3671, 3732, 5593, 295, 350, 281, 3732, 5593, 295, 350, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08307031367687469, "compression_ratio": 1.8097560975609757, "no_speech_prob": 2.0462313841562718e-05}, {"id": 831, "seek": 423300, "start": 4240.0, "end": 4245.0, "text": " So one over the square root of k is the scale of the weights.", "tokens": [50364, 467, 311, 445, 472, 11, 457, 5911, 309, 311, 2293, 472, 670, 264, 3732, 5593, 295, 3429, 294, 2293, 382, 321, 362, 510, 13, 50714, 50714, 407, 472, 670, 264, 3732, 5593, 295, 350, 307, 264, 4373, 295, 264, 17443, 13, 50964, 50964, 583, 562, 436, 366, 6316, 264, 3547, 11, 436, 434, 406, 1228, 257, 39148, 538, 7576, 13, 51164, 51164, 814, 434, 1228, 257, 9452, 7316, 538, 7576, 13, 51264, 51264, 400, 370, 436, 2642, 48806, 490, 3671, 3732, 5593, 295, 350, 281, 3732, 5593, 295, 350, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08307031367687469, "compression_ratio": 1.8097560975609757, "no_speech_prob": 2.0462313841562718e-05}, {"id": 832, "seek": 423300, "start": 4245.0, "end": 4249.0, "text": " But when they are drawing the numbers, they're not using a Gaussian by default.", "tokens": [50364, 467, 311, 445, 472, 11, 457, 5911, 309, 311, 2293, 472, 670, 264, 3732, 5593, 295, 3429, 294, 2293, 382, 321, 362, 510, 13, 50714, 50714, 407, 472, 670, 264, 3732, 5593, 295, 350, 307, 264, 4373, 295, 264, 17443, 13, 50964, 50964, 583, 562, 436, 366, 6316, 264, 3547, 11, 436, 434, 406, 1228, 257, 39148, 538, 7576, 13, 51164, 51164, 814, 434, 1228, 257, 9452, 7316, 538, 7576, 13, 51264, 51264, 400, 370, 436, 2642, 48806, 490, 3671, 3732, 5593, 295, 350, 281, 3732, 5593, 295, 350, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08307031367687469, "compression_ratio": 1.8097560975609757, "no_speech_prob": 2.0462313841562718e-05}, {"id": 833, "seek": 423300, "start": 4249.0, "end": 4251.0, "text": " They're using a uniform distribution by default.", "tokens": [50364, 467, 311, 445, 472, 11, 457, 5911, 309, 311, 2293, 472, 670, 264, 3732, 5593, 295, 3429, 294, 2293, 382, 321, 362, 510, 13, 50714, 50714, 407, 472, 670, 264, 3732, 5593, 295, 350, 307, 264, 4373, 295, 264, 17443, 13, 50964, 50964, 583, 562, 436, 366, 6316, 264, 3547, 11, 436, 434, 406, 1228, 257, 39148, 538, 7576, 13, 51164, 51164, 814, 434, 1228, 257, 9452, 7316, 538, 7576, 13, 51264, 51264, 400, 370, 436, 2642, 48806, 490, 3671, 3732, 5593, 295, 350, 281, 3732, 5593, 295, 350, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08307031367687469, "compression_ratio": 1.8097560975609757, "no_speech_prob": 2.0462313841562718e-05}, {"id": 834, "seek": 423300, "start": 4251.0, "end": 4256.0, "text": " And so they draw uniformly from negative square root of k to square root of k.", "tokens": [50364, 467, 311, 445, 472, 11, 457, 5911, 309, 311, 2293, 472, 670, 264, 3732, 5593, 295, 3429, 294, 2293, 382, 321, 362, 510, 13, 50714, 50714, 407, 472, 670, 264, 3732, 5593, 295, 350, 307, 264, 4373, 295, 264, 17443, 13, 50964, 50964, 583, 562, 436, 366, 6316, 264, 3547, 11, 436, 434, 406, 1228, 257, 39148, 538, 7576, 13, 51164, 51164, 814, 434, 1228, 257, 9452, 7316, 538, 7576, 13, 51264, 51264, 400, 370, 436, 2642, 48806, 490, 3671, 3732, 5593, 295, 350, 281, 3732, 5593, 295, 350, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08307031367687469, "compression_ratio": 1.8097560975609757, "no_speech_prob": 2.0462313841562718e-05}, {"id": 835, "seek": 425600, "start": 4256.0, "end": 4263.0, "text": " But it's the exact same thing and the same motivation with respect to what we've seen in this lecture.", "tokens": [50364, 583, 309, 311, 264, 1900, 912, 551, 293, 264, 912, 12335, 365, 3104, 281, 437, 321, 600, 1612, 294, 341, 7991, 13, 50714, 50714, 400, 264, 1778, 436, 434, 884, 341, 307, 498, 291, 362, 257, 9810, 39148, 4846, 11, 50914, 50914, 341, 486, 5586, 300, 484, 295, 341, 4583, 11, 291, 486, 362, 257, 9810, 39148, 5598, 13, 51164, 51164, 400, 291, 1936, 4584, 300, 538, 21589, 264, 17443, 538, 472, 670, 264, 3732, 5593, 295, 3429, 294, 13, 51464, 51464, 407, 300, 311, 437, 341, 307, 884, 13, 51564, 51564, 400, 550, 264, 1150, 551, 307, 264, 15245, 2710, 2144, 4583, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.045177464131955745, "compression_ratio": 1.7967479674796747, "no_speech_prob": 4.425341103342362e-06}, {"id": 836, "seek": 425600, "start": 4263.0, "end": 4267.0, "text": " And the reason they're doing this is if you have a roughly Gaussian input,", "tokens": [50364, 583, 309, 311, 264, 1900, 912, 551, 293, 264, 912, 12335, 365, 3104, 281, 437, 321, 600, 1612, 294, 341, 7991, 13, 50714, 50714, 400, 264, 1778, 436, 434, 884, 341, 307, 498, 291, 362, 257, 9810, 39148, 4846, 11, 50914, 50914, 341, 486, 5586, 300, 484, 295, 341, 4583, 11, 291, 486, 362, 257, 9810, 39148, 5598, 13, 51164, 51164, 400, 291, 1936, 4584, 300, 538, 21589, 264, 17443, 538, 472, 670, 264, 3732, 5593, 295, 3429, 294, 13, 51464, 51464, 407, 300, 311, 437, 341, 307, 884, 13, 51564, 51564, 400, 550, 264, 1150, 551, 307, 264, 15245, 2710, 2144, 4583, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.045177464131955745, "compression_ratio": 1.7967479674796747, "no_speech_prob": 4.425341103342362e-06}, {"id": 837, "seek": 425600, "start": 4267.0, "end": 4272.0, "text": " this will ensure that out of this layer, you will have a roughly Gaussian output.", "tokens": [50364, 583, 309, 311, 264, 1900, 912, 551, 293, 264, 912, 12335, 365, 3104, 281, 437, 321, 600, 1612, 294, 341, 7991, 13, 50714, 50714, 400, 264, 1778, 436, 434, 884, 341, 307, 498, 291, 362, 257, 9810, 39148, 4846, 11, 50914, 50914, 341, 486, 5586, 300, 484, 295, 341, 4583, 11, 291, 486, 362, 257, 9810, 39148, 5598, 13, 51164, 51164, 400, 291, 1936, 4584, 300, 538, 21589, 264, 17443, 538, 472, 670, 264, 3732, 5593, 295, 3429, 294, 13, 51464, 51464, 407, 300, 311, 437, 341, 307, 884, 13, 51564, 51564, 400, 550, 264, 1150, 551, 307, 264, 15245, 2710, 2144, 4583, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.045177464131955745, "compression_ratio": 1.7967479674796747, "no_speech_prob": 4.425341103342362e-06}, {"id": 838, "seek": 425600, "start": 4272.0, "end": 4278.0, "text": " And you basically achieve that by scaling the weights by one over the square root of fan in.", "tokens": [50364, 583, 309, 311, 264, 1900, 912, 551, 293, 264, 912, 12335, 365, 3104, 281, 437, 321, 600, 1612, 294, 341, 7991, 13, 50714, 50714, 400, 264, 1778, 436, 434, 884, 341, 307, 498, 291, 362, 257, 9810, 39148, 4846, 11, 50914, 50914, 341, 486, 5586, 300, 484, 295, 341, 4583, 11, 291, 486, 362, 257, 9810, 39148, 5598, 13, 51164, 51164, 400, 291, 1936, 4584, 300, 538, 21589, 264, 17443, 538, 472, 670, 264, 3732, 5593, 295, 3429, 294, 13, 51464, 51464, 407, 300, 311, 437, 341, 307, 884, 13, 51564, 51564, 400, 550, 264, 1150, 551, 307, 264, 15245, 2710, 2144, 4583, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.045177464131955745, "compression_ratio": 1.7967479674796747, "no_speech_prob": 4.425341103342362e-06}, {"id": 839, "seek": 425600, "start": 4278.0, "end": 4280.0, "text": " So that's what this is doing.", "tokens": [50364, 583, 309, 311, 264, 1900, 912, 551, 293, 264, 912, 12335, 365, 3104, 281, 437, 321, 600, 1612, 294, 341, 7991, 13, 50714, 50714, 400, 264, 1778, 436, 434, 884, 341, 307, 498, 291, 362, 257, 9810, 39148, 4846, 11, 50914, 50914, 341, 486, 5586, 300, 484, 295, 341, 4583, 11, 291, 486, 362, 257, 9810, 39148, 5598, 13, 51164, 51164, 400, 291, 1936, 4584, 300, 538, 21589, 264, 17443, 538, 472, 670, 264, 3732, 5593, 295, 3429, 294, 13, 51464, 51464, 407, 300, 311, 437, 341, 307, 884, 13, 51564, 51564, 400, 550, 264, 1150, 551, 307, 264, 15245, 2710, 2144, 4583, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.045177464131955745, "compression_ratio": 1.7967479674796747, "no_speech_prob": 4.425341103342362e-06}, {"id": 840, "seek": 425600, "start": 4280.0, "end": 4283.0, "text": " And then the second thing is the batch normalization layer.", "tokens": [50364, 583, 309, 311, 264, 1900, 912, 551, 293, 264, 912, 12335, 365, 3104, 281, 437, 321, 600, 1612, 294, 341, 7991, 13, 50714, 50714, 400, 264, 1778, 436, 434, 884, 341, 307, 498, 291, 362, 257, 9810, 39148, 4846, 11, 50914, 50914, 341, 486, 5586, 300, 484, 295, 341, 4583, 11, 291, 486, 362, 257, 9810, 39148, 5598, 13, 51164, 51164, 400, 291, 1936, 4584, 300, 538, 21589, 264, 17443, 538, 472, 670, 264, 3732, 5593, 295, 3429, 294, 13, 51464, 51464, 407, 300, 311, 437, 341, 307, 884, 13, 51564, 51564, 400, 550, 264, 1150, 551, 307, 264, 15245, 2710, 2144, 4583, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.045177464131955745, "compression_ratio": 1.7967479674796747, "no_speech_prob": 4.425341103342362e-06}, {"id": 841, "seek": 428300, "start": 4283.0, "end": 4286.0, "text": " So let's look at what that looks like in PyTorch.", "tokens": [50364, 407, 718, 311, 574, 412, 437, 300, 1542, 411, 294, 9953, 51, 284, 339, 13, 50514, 50514, 407, 510, 321, 362, 257, 472, 18795, 15245, 2710, 2144, 4583, 2293, 382, 321, 366, 1228, 510, 13, 50764, 50764, 400, 456, 366, 257, 1230, 295, 20428, 12869, 516, 666, 309, 382, 731, 13, 50864, 50864, 407, 321, 643, 281, 458, 264, 1230, 295, 4122, 13, 50964, 50964, 1171, 505, 11, 300, 307, 2331, 13, 51064, 51064, 400, 300, 307, 2978, 370, 300, 321, 393, 5883, 1125, 613, 9834, 510, 13, 51264, 51264, 440, 6052, 11, 264, 12577, 11, 293, 264, 9204, 433, 337, 264, 2614, 914, 293, 3832, 25163, 13, 51564, 51564, 1396, 436, 643, 281, 458, 264, 2158, 295, 17889, 510, 13, 51714, 51714, 400, 538, 7576, 11, 341, 307, 472, 3671, 1732, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06446526520443659, "compression_ratio": 1.7239057239057238, "no_speech_prob": 7.646329322597012e-06}, {"id": 842, "seek": 428300, "start": 4286.0, "end": 4291.0, "text": " So here we have a one dimensional batch normalization layer exactly as we are using here.", "tokens": [50364, 407, 718, 311, 574, 412, 437, 300, 1542, 411, 294, 9953, 51, 284, 339, 13, 50514, 50514, 407, 510, 321, 362, 257, 472, 18795, 15245, 2710, 2144, 4583, 2293, 382, 321, 366, 1228, 510, 13, 50764, 50764, 400, 456, 366, 257, 1230, 295, 20428, 12869, 516, 666, 309, 382, 731, 13, 50864, 50864, 407, 321, 643, 281, 458, 264, 1230, 295, 4122, 13, 50964, 50964, 1171, 505, 11, 300, 307, 2331, 13, 51064, 51064, 400, 300, 307, 2978, 370, 300, 321, 393, 5883, 1125, 613, 9834, 510, 13, 51264, 51264, 440, 6052, 11, 264, 12577, 11, 293, 264, 9204, 433, 337, 264, 2614, 914, 293, 3832, 25163, 13, 51564, 51564, 1396, 436, 643, 281, 458, 264, 2158, 295, 17889, 510, 13, 51714, 51714, 400, 538, 7576, 11, 341, 307, 472, 3671, 1732, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06446526520443659, "compression_ratio": 1.7239057239057238, "no_speech_prob": 7.646329322597012e-06}, {"id": 843, "seek": 428300, "start": 4291.0, "end": 4293.0, "text": " And there are a number of keyword arguments going into it as well.", "tokens": [50364, 407, 718, 311, 574, 412, 437, 300, 1542, 411, 294, 9953, 51, 284, 339, 13, 50514, 50514, 407, 510, 321, 362, 257, 472, 18795, 15245, 2710, 2144, 4583, 2293, 382, 321, 366, 1228, 510, 13, 50764, 50764, 400, 456, 366, 257, 1230, 295, 20428, 12869, 516, 666, 309, 382, 731, 13, 50864, 50864, 407, 321, 643, 281, 458, 264, 1230, 295, 4122, 13, 50964, 50964, 1171, 505, 11, 300, 307, 2331, 13, 51064, 51064, 400, 300, 307, 2978, 370, 300, 321, 393, 5883, 1125, 613, 9834, 510, 13, 51264, 51264, 440, 6052, 11, 264, 12577, 11, 293, 264, 9204, 433, 337, 264, 2614, 914, 293, 3832, 25163, 13, 51564, 51564, 1396, 436, 643, 281, 458, 264, 2158, 295, 17889, 510, 13, 51714, 51714, 400, 538, 7576, 11, 341, 307, 472, 3671, 1732, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06446526520443659, "compression_ratio": 1.7239057239057238, "no_speech_prob": 7.646329322597012e-06}, {"id": 844, "seek": 428300, "start": 4293.0, "end": 4295.0, "text": " So we need to know the number of features.", "tokens": [50364, 407, 718, 311, 574, 412, 437, 300, 1542, 411, 294, 9953, 51, 284, 339, 13, 50514, 50514, 407, 510, 321, 362, 257, 472, 18795, 15245, 2710, 2144, 4583, 2293, 382, 321, 366, 1228, 510, 13, 50764, 50764, 400, 456, 366, 257, 1230, 295, 20428, 12869, 516, 666, 309, 382, 731, 13, 50864, 50864, 407, 321, 643, 281, 458, 264, 1230, 295, 4122, 13, 50964, 50964, 1171, 505, 11, 300, 307, 2331, 13, 51064, 51064, 400, 300, 307, 2978, 370, 300, 321, 393, 5883, 1125, 613, 9834, 510, 13, 51264, 51264, 440, 6052, 11, 264, 12577, 11, 293, 264, 9204, 433, 337, 264, 2614, 914, 293, 3832, 25163, 13, 51564, 51564, 1396, 436, 643, 281, 458, 264, 2158, 295, 17889, 510, 13, 51714, 51714, 400, 538, 7576, 11, 341, 307, 472, 3671, 1732, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06446526520443659, "compression_ratio": 1.7239057239057238, "no_speech_prob": 7.646329322597012e-06}, {"id": 845, "seek": 428300, "start": 4295.0, "end": 4297.0, "text": " For us, that is 200.", "tokens": [50364, 407, 718, 311, 574, 412, 437, 300, 1542, 411, 294, 9953, 51, 284, 339, 13, 50514, 50514, 407, 510, 321, 362, 257, 472, 18795, 15245, 2710, 2144, 4583, 2293, 382, 321, 366, 1228, 510, 13, 50764, 50764, 400, 456, 366, 257, 1230, 295, 20428, 12869, 516, 666, 309, 382, 731, 13, 50864, 50864, 407, 321, 643, 281, 458, 264, 1230, 295, 4122, 13, 50964, 50964, 1171, 505, 11, 300, 307, 2331, 13, 51064, 51064, 400, 300, 307, 2978, 370, 300, 321, 393, 5883, 1125, 613, 9834, 510, 13, 51264, 51264, 440, 6052, 11, 264, 12577, 11, 293, 264, 9204, 433, 337, 264, 2614, 914, 293, 3832, 25163, 13, 51564, 51564, 1396, 436, 643, 281, 458, 264, 2158, 295, 17889, 510, 13, 51714, 51714, 400, 538, 7576, 11, 341, 307, 472, 3671, 1732, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06446526520443659, "compression_ratio": 1.7239057239057238, "no_speech_prob": 7.646329322597012e-06}, {"id": 846, "seek": 428300, "start": 4297.0, "end": 4301.0, "text": " And that is needed so that we can initialize these parameters here.", "tokens": [50364, 407, 718, 311, 574, 412, 437, 300, 1542, 411, 294, 9953, 51, 284, 339, 13, 50514, 50514, 407, 510, 321, 362, 257, 472, 18795, 15245, 2710, 2144, 4583, 2293, 382, 321, 366, 1228, 510, 13, 50764, 50764, 400, 456, 366, 257, 1230, 295, 20428, 12869, 516, 666, 309, 382, 731, 13, 50864, 50864, 407, 321, 643, 281, 458, 264, 1230, 295, 4122, 13, 50964, 50964, 1171, 505, 11, 300, 307, 2331, 13, 51064, 51064, 400, 300, 307, 2978, 370, 300, 321, 393, 5883, 1125, 613, 9834, 510, 13, 51264, 51264, 440, 6052, 11, 264, 12577, 11, 293, 264, 9204, 433, 337, 264, 2614, 914, 293, 3832, 25163, 13, 51564, 51564, 1396, 436, 643, 281, 458, 264, 2158, 295, 17889, 510, 13, 51714, 51714, 400, 538, 7576, 11, 341, 307, 472, 3671, 1732, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06446526520443659, "compression_ratio": 1.7239057239057238, "no_speech_prob": 7.646329322597012e-06}, {"id": 847, "seek": 428300, "start": 4301.0, "end": 4307.0, "text": " The gain, the bias, and the buffers for the running mean and standard deviation.", "tokens": [50364, 407, 718, 311, 574, 412, 437, 300, 1542, 411, 294, 9953, 51, 284, 339, 13, 50514, 50514, 407, 510, 321, 362, 257, 472, 18795, 15245, 2710, 2144, 4583, 2293, 382, 321, 366, 1228, 510, 13, 50764, 50764, 400, 456, 366, 257, 1230, 295, 20428, 12869, 516, 666, 309, 382, 731, 13, 50864, 50864, 407, 321, 643, 281, 458, 264, 1230, 295, 4122, 13, 50964, 50964, 1171, 505, 11, 300, 307, 2331, 13, 51064, 51064, 400, 300, 307, 2978, 370, 300, 321, 393, 5883, 1125, 613, 9834, 510, 13, 51264, 51264, 440, 6052, 11, 264, 12577, 11, 293, 264, 9204, 433, 337, 264, 2614, 914, 293, 3832, 25163, 13, 51564, 51564, 1396, 436, 643, 281, 458, 264, 2158, 295, 17889, 510, 13, 51714, 51714, 400, 538, 7576, 11, 341, 307, 472, 3671, 1732, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06446526520443659, "compression_ratio": 1.7239057239057238, "no_speech_prob": 7.646329322597012e-06}, {"id": 848, "seek": 428300, "start": 4307.0, "end": 4310.0, "text": " Then they need to know the value of epsilon here.", "tokens": [50364, 407, 718, 311, 574, 412, 437, 300, 1542, 411, 294, 9953, 51, 284, 339, 13, 50514, 50514, 407, 510, 321, 362, 257, 472, 18795, 15245, 2710, 2144, 4583, 2293, 382, 321, 366, 1228, 510, 13, 50764, 50764, 400, 456, 366, 257, 1230, 295, 20428, 12869, 516, 666, 309, 382, 731, 13, 50864, 50864, 407, 321, 643, 281, 458, 264, 1230, 295, 4122, 13, 50964, 50964, 1171, 505, 11, 300, 307, 2331, 13, 51064, 51064, 400, 300, 307, 2978, 370, 300, 321, 393, 5883, 1125, 613, 9834, 510, 13, 51264, 51264, 440, 6052, 11, 264, 12577, 11, 293, 264, 9204, 433, 337, 264, 2614, 914, 293, 3832, 25163, 13, 51564, 51564, 1396, 436, 643, 281, 458, 264, 2158, 295, 17889, 510, 13, 51714, 51714, 400, 538, 7576, 11, 341, 307, 472, 3671, 1732, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06446526520443659, "compression_ratio": 1.7239057239057238, "no_speech_prob": 7.646329322597012e-06}, {"id": 849, "seek": 428300, "start": 4310.0, "end": 4312.0, "text": " And by default, this is one negative five.", "tokens": [50364, 407, 718, 311, 574, 412, 437, 300, 1542, 411, 294, 9953, 51, 284, 339, 13, 50514, 50514, 407, 510, 321, 362, 257, 472, 18795, 15245, 2710, 2144, 4583, 2293, 382, 321, 366, 1228, 510, 13, 50764, 50764, 400, 456, 366, 257, 1230, 295, 20428, 12869, 516, 666, 309, 382, 731, 13, 50864, 50864, 407, 321, 643, 281, 458, 264, 1230, 295, 4122, 13, 50964, 50964, 1171, 505, 11, 300, 307, 2331, 13, 51064, 51064, 400, 300, 307, 2978, 370, 300, 321, 393, 5883, 1125, 613, 9834, 510, 13, 51264, 51264, 440, 6052, 11, 264, 12577, 11, 293, 264, 9204, 433, 337, 264, 2614, 914, 293, 3832, 25163, 13, 51564, 51564, 1396, 436, 643, 281, 458, 264, 2158, 295, 17889, 510, 13, 51714, 51714, 400, 538, 7576, 11, 341, 307, 472, 3671, 1732, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06446526520443659, "compression_ratio": 1.7239057239057238, "no_speech_prob": 7.646329322597012e-06}, {"id": 850, "seek": 431200, "start": 4312.0, "end": 4314.0, "text": " So we don't typically change this too much.", "tokens": [50364, 407, 321, 500, 380, 5850, 1319, 341, 886, 709, 13, 50464, 50464, 1396, 436, 643, 281, 458, 264, 11244, 13, 50564, 50564, 400, 264, 11244, 510, 11, 382, 436, 2903, 11, 307, 1936, 1143, 337, 613, 2614, 914, 293, 2614, 3832, 25163, 13, 50914, 50914, 407, 538, 7576, 11, 264, 11244, 510, 307, 1958, 13, 16, 13, 51014, 51014, 440, 11244, 321, 366, 1228, 510, 294, 341, 1365, 307, 1958, 13, 628, 16, 13, 51264, 51264, 400, 1936, 11, 291, 815, 528, 281, 1319, 341, 2171, 13, 51464, 51464, 400, 9810, 4124, 11, 498, 291, 362, 257, 588, 2416, 15245, 2744, 11, 51614, 51614, 550, 5850, 437, 291, 603, 536, 307, 300, 562, 291, 12539, 264, 914, 293, 264, 3832, 25163, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06318062449258471, "compression_ratio": 1.8651685393258426, "no_speech_prob": 5.594239155470859e-06}, {"id": 851, "seek": 431200, "start": 4314.0, "end": 4316.0, "text": " Then they need to know the momentum.", "tokens": [50364, 407, 321, 500, 380, 5850, 1319, 341, 886, 709, 13, 50464, 50464, 1396, 436, 643, 281, 458, 264, 11244, 13, 50564, 50564, 400, 264, 11244, 510, 11, 382, 436, 2903, 11, 307, 1936, 1143, 337, 613, 2614, 914, 293, 2614, 3832, 25163, 13, 50914, 50914, 407, 538, 7576, 11, 264, 11244, 510, 307, 1958, 13, 16, 13, 51014, 51014, 440, 11244, 321, 366, 1228, 510, 294, 341, 1365, 307, 1958, 13, 628, 16, 13, 51264, 51264, 400, 1936, 11, 291, 815, 528, 281, 1319, 341, 2171, 13, 51464, 51464, 400, 9810, 4124, 11, 498, 291, 362, 257, 588, 2416, 15245, 2744, 11, 51614, 51614, 550, 5850, 437, 291, 603, 536, 307, 300, 562, 291, 12539, 264, 914, 293, 264, 3832, 25163, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06318062449258471, "compression_ratio": 1.8651685393258426, "no_speech_prob": 5.594239155470859e-06}, {"id": 852, "seek": 431200, "start": 4316.0, "end": 4323.0, "text": " And the momentum here, as they explain, is basically used for these running mean and running standard deviation.", "tokens": [50364, 407, 321, 500, 380, 5850, 1319, 341, 886, 709, 13, 50464, 50464, 1396, 436, 643, 281, 458, 264, 11244, 13, 50564, 50564, 400, 264, 11244, 510, 11, 382, 436, 2903, 11, 307, 1936, 1143, 337, 613, 2614, 914, 293, 2614, 3832, 25163, 13, 50914, 50914, 407, 538, 7576, 11, 264, 11244, 510, 307, 1958, 13, 16, 13, 51014, 51014, 440, 11244, 321, 366, 1228, 510, 294, 341, 1365, 307, 1958, 13, 628, 16, 13, 51264, 51264, 400, 1936, 11, 291, 815, 528, 281, 1319, 341, 2171, 13, 51464, 51464, 400, 9810, 4124, 11, 498, 291, 362, 257, 588, 2416, 15245, 2744, 11, 51614, 51614, 550, 5850, 437, 291, 603, 536, 307, 300, 562, 291, 12539, 264, 914, 293, 264, 3832, 25163, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06318062449258471, "compression_ratio": 1.8651685393258426, "no_speech_prob": 5.594239155470859e-06}, {"id": 853, "seek": 431200, "start": 4323.0, "end": 4325.0, "text": " So by default, the momentum here is 0.1.", "tokens": [50364, 407, 321, 500, 380, 5850, 1319, 341, 886, 709, 13, 50464, 50464, 1396, 436, 643, 281, 458, 264, 11244, 13, 50564, 50564, 400, 264, 11244, 510, 11, 382, 436, 2903, 11, 307, 1936, 1143, 337, 613, 2614, 914, 293, 2614, 3832, 25163, 13, 50914, 50914, 407, 538, 7576, 11, 264, 11244, 510, 307, 1958, 13, 16, 13, 51014, 51014, 440, 11244, 321, 366, 1228, 510, 294, 341, 1365, 307, 1958, 13, 628, 16, 13, 51264, 51264, 400, 1936, 11, 291, 815, 528, 281, 1319, 341, 2171, 13, 51464, 51464, 400, 9810, 4124, 11, 498, 291, 362, 257, 588, 2416, 15245, 2744, 11, 51614, 51614, 550, 5850, 437, 291, 603, 536, 307, 300, 562, 291, 12539, 264, 914, 293, 264, 3832, 25163, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06318062449258471, "compression_ratio": 1.8651685393258426, "no_speech_prob": 5.594239155470859e-06}, {"id": 854, "seek": 431200, "start": 4325.0, "end": 4330.0, "text": " The momentum we are using here in this example is 0.001.", "tokens": [50364, 407, 321, 500, 380, 5850, 1319, 341, 886, 709, 13, 50464, 50464, 1396, 436, 643, 281, 458, 264, 11244, 13, 50564, 50564, 400, 264, 11244, 510, 11, 382, 436, 2903, 11, 307, 1936, 1143, 337, 613, 2614, 914, 293, 2614, 3832, 25163, 13, 50914, 50914, 407, 538, 7576, 11, 264, 11244, 510, 307, 1958, 13, 16, 13, 51014, 51014, 440, 11244, 321, 366, 1228, 510, 294, 341, 1365, 307, 1958, 13, 628, 16, 13, 51264, 51264, 400, 1936, 11, 291, 815, 528, 281, 1319, 341, 2171, 13, 51464, 51464, 400, 9810, 4124, 11, 498, 291, 362, 257, 588, 2416, 15245, 2744, 11, 51614, 51614, 550, 5850, 437, 291, 603, 536, 307, 300, 562, 291, 12539, 264, 914, 293, 264, 3832, 25163, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06318062449258471, "compression_ratio": 1.8651685393258426, "no_speech_prob": 5.594239155470859e-06}, {"id": 855, "seek": 431200, "start": 4330.0, "end": 4334.0, "text": " And basically, you may want to change this sometimes.", "tokens": [50364, 407, 321, 500, 380, 5850, 1319, 341, 886, 709, 13, 50464, 50464, 1396, 436, 643, 281, 458, 264, 11244, 13, 50564, 50564, 400, 264, 11244, 510, 11, 382, 436, 2903, 11, 307, 1936, 1143, 337, 613, 2614, 914, 293, 2614, 3832, 25163, 13, 50914, 50914, 407, 538, 7576, 11, 264, 11244, 510, 307, 1958, 13, 16, 13, 51014, 51014, 440, 11244, 321, 366, 1228, 510, 294, 341, 1365, 307, 1958, 13, 628, 16, 13, 51264, 51264, 400, 1936, 11, 291, 815, 528, 281, 1319, 341, 2171, 13, 51464, 51464, 400, 9810, 4124, 11, 498, 291, 362, 257, 588, 2416, 15245, 2744, 11, 51614, 51614, 550, 5850, 437, 291, 603, 536, 307, 300, 562, 291, 12539, 264, 914, 293, 264, 3832, 25163, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06318062449258471, "compression_ratio": 1.8651685393258426, "no_speech_prob": 5.594239155470859e-06}, {"id": 856, "seek": 431200, "start": 4334.0, "end": 4337.0, "text": " And roughly speaking, if you have a very large batch size,", "tokens": [50364, 407, 321, 500, 380, 5850, 1319, 341, 886, 709, 13, 50464, 50464, 1396, 436, 643, 281, 458, 264, 11244, 13, 50564, 50564, 400, 264, 11244, 510, 11, 382, 436, 2903, 11, 307, 1936, 1143, 337, 613, 2614, 914, 293, 2614, 3832, 25163, 13, 50914, 50914, 407, 538, 7576, 11, 264, 11244, 510, 307, 1958, 13, 16, 13, 51014, 51014, 440, 11244, 321, 366, 1228, 510, 294, 341, 1365, 307, 1958, 13, 628, 16, 13, 51264, 51264, 400, 1936, 11, 291, 815, 528, 281, 1319, 341, 2171, 13, 51464, 51464, 400, 9810, 4124, 11, 498, 291, 362, 257, 588, 2416, 15245, 2744, 11, 51614, 51614, 550, 5850, 437, 291, 603, 536, 307, 300, 562, 291, 12539, 264, 914, 293, 264, 3832, 25163, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06318062449258471, "compression_ratio": 1.8651685393258426, "no_speech_prob": 5.594239155470859e-06}, {"id": 857, "seek": 431200, "start": 4337.0, "end": 4341.0, "text": " then typically what you'll see is that when you estimate the mean and the standard deviation,", "tokens": [50364, 407, 321, 500, 380, 5850, 1319, 341, 886, 709, 13, 50464, 50464, 1396, 436, 643, 281, 458, 264, 11244, 13, 50564, 50564, 400, 264, 11244, 510, 11, 382, 436, 2903, 11, 307, 1936, 1143, 337, 613, 2614, 914, 293, 2614, 3832, 25163, 13, 50914, 50914, 407, 538, 7576, 11, 264, 11244, 510, 307, 1958, 13, 16, 13, 51014, 51014, 440, 11244, 321, 366, 1228, 510, 294, 341, 1365, 307, 1958, 13, 628, 16, 13, 51264, 51264, 400, 1936, 11, 291, 815, 528, 281, 1319, 341, 2171, 13, 51464, 51464, 400, 9810, 4124, 11, 498, 291, 362, 257, 588, 2416, 15245, 2744, 11, 51614, 51614, 550, 5850, 437, 291, 603, 536, 307, 300, 562, 291, 12539, 264, 914, 293, 264, 3832, 25163, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06318062449258471, "compression_ratio": 1.8651685393258426, "no_speech_prob": 5.594239155470859e-06}, {"id": 858, "seek": 434100, "start": 4341.0, "end": 4346.0, "text": " for every single batch size, if it's large enough, you're going to get roughly the same result.", "tokens": [50364, 337, 633, 2167, 15245, 2744, 11, 498, 309, 311, 2416, 1547, 11, 291, 434, 516, 281, 483, 9810, 264, 912, 1874, 13, 50614, 50614, 400, 370, 4412, 11, 291, 393, 764, 4748, 2946, 11244, 11, 411, 1958, 13, 16, 13, 50864, 50864, 583, 337, 257, 15245, 2744, 382, 1359, 382, 8858, 11, 264, 914, 293, 3832, 25163, 510, 1062, 747, 322, 4748, 819, 3547, 11, 51214, 51214, 570, 456, 311, 787, 8858, 5110, 321, 366, 1228, 281, 12539, 264, 914, 293, 3832, 25163, 13, 51414, 51414, 407, 264, 2158, 307, 4473, 926, 257, 688, 13, 51514, 51514, 400, 498, 428, 11244, 307, 1958, 13, 16, 11, 300, 1062, 406, 312, 665, 1547, 337, 341, 2158, 281, 11852, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05556543146977659, "compression_ratio": 1.7508896797153024, "no_speech_prob": 1.4970552001614124e-05}, {"id": 859, "seek": 434100, "start": 4346.0, "end": 4351.0, "text": " And so therefore, you can use slightly higher momentum, like 0.1.", "tokens": [50364, 337, 633, 2167, 15245, 2744, 11, 498, 309, 311, 2416, 1547, 11, 291, 434, 516, 281, 483, 9810, 264, 912, 1874, 13, 50614, 50614, 400, 370, 4412, 11, 291, 393, 764, 4748, 2946, 11244, 11, 411, 1958, 13, 16, 13, 50864, 50864, 583, 337, 257, 15245, 2744, 382, 1359, 382, 8858, 11, 264, 914, 293, 3832, 25163, 510, 1062, 747, 322, 4748, 819, 3547, 11, 51214, 51214, 570, 456, 311, 787, 8858, 5110, 321, 366, 1228, 281, 12539, 264, 914, 293, 3832, 25163, 13, 51414, 51414, 407, 264, 2158, 307, 4473, 926, 257, 688, 13, 51514, 51514, 400, 498, 428, 11244, 307, 1958, 13, 16, 11, 300, 1062, 406, 312, 665, 1547, 337, 341, 2158, 281, 11852, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05556543146977659, "compression_ratio": 1.7508896797153024, "no_speech_prob": 1.4970552001614124e-05}, {"id": 860, "seek": 434100, "start": 4351.0, "end": 4358.0, "text": " But for a batch size as small as 32, the mean and standard deviation here might take on slightly different numbers,", "tokens": [50364, 337, 633, 2167, 15245, 2744, 11, 498, 309, 311, 2416, 1547, 11, 291, 434, 516, 281, 483, 9810, 264, 912, 1874, 13, 50614, 50614, 400, 370, 4412, 11, 291, 393, 764, 4748, 2946, 11244, 11, 411, 1958, 13, 16, 13, 50864, 50864, 583, 337, 257, 15245, 2744, 382, 1359, 382, 8858, 11, 264, 914, 293, 3832, 25163, 510, 1062, 747, 322, 4748, 819, 3547, 11, 51214, 51214, 570, 456, 311, 787, 8858, 5110, 321, 366, 1228, 281, 12539, 264, 914, 293, 3832, 25163, 13, 51414, 51414, 407, 264, 2158, 307, 4473, 926, 257, 688, 13, 51514, 51514, 400, 498, 428, 11244, 307, 1958, 13, 16, 11, 300, 1062, 406, 312, 665, 1547, 337, 341, 2158, 281, 11852, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05556543146977659, "compression_ratio": 1.7508896797153024, "no_speech_prob": 1.4970552001614124e-05}, {"id": 861, "seek": 434100, "start": 4358.0, "end": 4362.0, "text": " because there's only 32 examples we are using to estimate the mean and standard deviation.", "tokens": [50364, 337, 633, 2167, 15245, 2744, 11, 498, 309, 311, 2416, 1547, 11, 291, 434, 516, 281, 483, 9810, 264, 912, 1874, 13, 50614, 50614, 400, 370, 4412, 11, 291, 393, 764, 4748, 2946, 11244, 11, 411, 1958, 13, 16, 13, 50864, 50864, 583, 337, 257, 15245, 2744, 382, 1359, 382, 8858, 11, 264, 914, 293, 3832, 25163, 510, 1062, 747, 322, 4748, 819, 3547, 11, 51214, 51214, 570, 456, 311, 787, 8858, 5110, 321, 366, 1228, 281, 12539, 264, 914, 293, 3832, 25163, 13, 51414, 51414, 407, 264, 2158, 307, 4473, 926, 257, 688, 13, 51514, 51514, 400, 498, 428, 11244, 307, 1958, 13, 16, 11, 300, 1062, 406, 312, 665, 1547, 337, 341, 2158, 281, 11852, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05556543146977659, "compression_ratio": 1.7508896797153024, "no_speech_prob": 1.4970552001614124e-05}, {"id": 862, "seek": 434100, "start": 4362.0, "end": 4364.0, "text": " So the value is changing around a lot.", "tokens": [50364, 337, 633, 2167, 15245, 2744, 11, 498, 309, 311, 2416, 1547, 11, 291, 434, 516, 281, 483, 9810, 264, 912, 1874, 13, 50614, 50614, 400, 370, 4412, 11, 291, 393, 764, 4748, 2946, 11244, 11, 411, 1958, 13, 16, 13, 50864, 50864, 583, 337, 257, 15245, 2744, 382, 1359, 382, 8858, 11, 264, 914, 293, 3832, 25163, 510, 1062, 747, 322, 4748, 819, 3547, 11, 51214, 51214, 570, 456, 311, 787, 8858, 5110, 321, 366, 1228, 281, 12539, 264, 914, 293, 3832, 25163, 13, 51414, 51414, 407, 264, 2158, 307, 4473, 926, 257, 688, 13, 51514, 51514, 400, 498, 428, 11244, 307, 1958, 13, 16, 11, 300, 1062, 406, 312, 665, 1547, 337, 341, 2158, 281, 11852, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05556543146977659, "compression_ratio": 1.7508896797153024, "no_speech_prob": 1.4970552001614124e-05}, {"id": 863, "seek": 434100, "start": 4364.0, "end": 4370.0, "text": " And if your momentum is 0.1, that might not be good enough for this value to settle.", "tokens": [50364, 337, 633, 2167, 15245, 2744, 11, 498, 309, 311, 2416, 1547, 11, 291, 434, 516, 281, 483, 9810, 264, 912, 1874, 13, 50614, 50614, 400, 370, 4412, 11, 291, 393, 764, 4748, 2946, 11244, 11, 411, 1958, 13, 16, 13, 50864, 50864, 583, 337, 257, 15245, 2744, 382, 1359, 382, 8858, 11, 264, 914, 293, 3832, 25163, 510, 1062, 747, 322, 4748, 819, 3547, 11, 51214, 51214, 570, 456, 311, 787, 8858, 5110, 321, 366, 1228, 281, 12539, 264, 914, 293, 3832, 25163, 13, 51414, 51414, 407, 264, 2158, 307, 4473, 926, 257, 688, 13, 51514, 51514, 400, 498, 428, 11244, 307, 1958, 13, 16, 11, 300, 1062, 406, 312, 665, 1547, 337, 341, 2158, 281, 11852, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.05556543146977659, "compression_ratio": 1.7508896797153024, "no_speech_prob": 1.4970552001614124e-05}, {"id": 864, "seek": 437000, "start": 4370.0, "end": 4375.0, "text": " And converge to the actual mean and standard deviation over the entire training set.", "tokens": [50364, 400, 41881, 281, 264, 3539, 914, 293, 3832, 25163, 670, 264, 2302, 3097, 992, 13, 50614, 50614, 400, 370, 1936, 11, 498, 428, 15245, 2744, 307, 588, 1359, 11, 11244, 295, 1958, 13, 16, 307, 7263, 5795, 13, 50864, 50864, 400, 309, 1062, 652, 309, 370, 300, 264, 2614, 914, 293, 3832, 25163, 307, 739, 11077, 886, 709, 1830, 3097, 11, 51114, 51114, 293, 309, 311, 406, 767, 9652, 3249, 6108, 13, 51314, 51314, 2096, 533, 6915, 2074, 24799, 1968, 341, 15245, 2710, 2144, 4583, 575, 613, 1466, 712, 2096, 533, 9834, 11, 51664, 51664, 264, 6052, 293, 264, 12577, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08818302154541016, "compression_ratio": 1.7490494296577948, "no_speech_prob": 2.2826861822977662e-05}, {"id": 865, "seek": 437000, "start": 4375.0, "end": 4380.0, "text": " And so basically, if your batch size is very small, momentum of 0.1 is potentially dangerous.", "tokens": [50364, 400, 41881, 281, 264, 3539, 914, 293, 3832, 25163, 670, 264, 2302, 3097, 992, 13, 50614, 50614, 400, 370, 1936, 11, 498, 428, 15245, 2744, 307, 588, 1359, 11, 11244, 295, 1958, 13, 16, 307, 7263, 5795, 13, 50864, 50864, 400, 309, 1062, 652, 309, 370, 300, 264, 2614, 914, 293, 3832, 25163, 307, 739, 11077, 886, 709, 1830, 3097, 11, 51114, 51114, 293, 309, 311, 406, 767, 9652, 3249, 6108, 13, 51314, 51314, 2096, 533, 6915, 2074, 24799, 1968, 341, 15245, 2710, 2144, 4583, 575, 613, 1466, 712, 2096, 533, 9834, 11, 51664, 51664, 264, 6052, 293, 264, 12577, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08818302154541016, "compression_ratio": 1.7490494296577948, "no_speech_prob": 2.2826861822977662e-05}, {"id": 866, "seek": 437000, "start": 4380.0, "end": 4385.0, "text": " And it might make it so that the running mean and standard deviation is thrashing too much during training,", "tokens": [50364, 400, 41881, 281, 264, 3539, 914, 293, 3832, 25163, 670, 264, 2302, 3097, 992, 13, 50614, 50614, 400, 370, 1936, 11, 498, 428, 15245, 2744, 307, 588, 1359, 11, 11244, 295, 1958, 13, 16, 307, 7263, 5795, 13, 50864, 50864, 400, 309, 1062, 652, 309, 370, 300, 264, 2614, 914, 293, 3832, 25163, 307, 739, 11077, 886, 709, 1830, 3097, 11, 51114, 51114, 293, 309, 311, 406, 767, 9652, 3249, 6108, 13, 51314, 51314, 2096, 533, 6915, 2074, 24799, 1968, 341, 15245, 2710, 2144, 4583, 575, 613, 1466, 712, 2096, 533, 9834, 11, 51664, 51664, 264, 6052, 293, 264, 12577, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08818302154541016, "compression_ratio": 1.7490494296577948, "no_speech_prob": 2.2826861822977662e-05}, {"id": 867, "seek": 437000, "start": 4385.0, "end": 4389.0, "text": " and it's not actually converging properly.", "tokens": [50364, 400, 41881, 281, 264, 3539, 914, 293, 3832, 25163, 670, 264, 2302, 3097, 992, 13, 50614, 50614, 400, 370, 1936, 11, 498, 428, 15245, 2744, 307, 588, 1359, 11, 11244, 295, 1958, 13, 16, 307, 7263, 5795, 13, 50864, 50864, 400, 309, 1062, 652, 309, 370, 300, 264, 2614, 914, 293, 3832, 25163, 307, 739, 11077, 886, 709, 1830, 3097, 11, 51114, 51114, 293, 309, 311, 406, 767, 9652, 3249, 6108, 13, 51314, 51314, 2096, 533, 6915, 2074, 24799, 1968, 341, 15245, 2710, 2144, 4583, 575, 613, 1466, 712, 2096, 533, 9834, 11, 51664, 51664, 264, 6052, 293, 264, 12577, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08818302154541016, "compression_ratio": 1.7490494296577948, "no_speech_prob": 2.2826861822977662e-05}, {"id": 868, "seek": 437000, "start": 4389.0, "end": 4396.0, "text": " affine equals true determines whether this batch normalization layer has these learnable affine parameters,", "tokens": [50364, 400, 41881, 281, 264, 3539, 914, 293, 3832, 25163, 670, 264, 2302, 3097, 992, 13, 50614, 50614, 400, 370, 1936, 11, 498, 428, 15245, 2744, 307, 588, 1359, 11, 11244, 295, 1958, 13, 16, 307, 7263, 5795, 13, 50864, 50864, 400, 309, 1062, 652, 309, 370, 300, 264, 2614, 914, 293, 3832, 25163, 307, 739, 11077, 886, 709, 1830, 3097, 11, 51114, 51114, 293, 309, 311, 406, 767, 9652, 3249, 6108, 13, 51314, 51314, 2096, 533, 6915, 2074, 24799, 1968, 341, 15245, 2710, 2144, 4583, 575, 613, 1466, 712, 2096, 533, 9834, 11, 51664, 51664, 264, 6052, 293, 264, 12577, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08818302154541016, "compression_ratio": 1.7490494296577948, "no_speech_prob": 2.2826861822977662e-05}, {"id": 869, "seek": 437000, "start": 4396.0, "end": 4399.0, "text": " the gain and the bias.", "tokens": [50364, 400, 41881, 281, 264, 3539, 914, 293, 3832, 25163, 670, 264, 2302, 3097, 992, 13, 50614, 50614, 400, 370, 1936, 11, 498, 428, 15245, 2744, 307, 588, 1359, 11, 11244, 295, 1958, 13, 16, 307, 7263, 5795, 13, 50864, 50864, 400, 309, 1062, 652, 309, 370, 300, 264, 2614, 914, 293, 3832, 25163, 307, 739, 11077, 886, 709, 1830, 3097, 11, 51114, 51114, 293, 309, 311, 406, 767, 9652, 3249, 6108, 13, 51314, 51314, 2096, 533, 6915, 2074, 24799, 1968, 341, 15245, 2710, 2144, 4583, 575, 613, 1466, 712, 2096, 533, 9834, 11, 51664, 51664, 264, 6052, 293, 264, 12577, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08818302154541016, "compression_ratio": 1.7490494296577948, "no_speech_prob": 2.2826861822977662e-05}, {"id": 870, "seek": 439900, "start": 4399.0, "end": 4406.0, "text": " This is almost always kept true. I'm not actually sure why you would want to change this to false.", "tokens": [50364, 639, 307, 1920, 1009, 4305, 2074, 13, 286, 478, 406, 767, 988, 983, 291, 576, 528, 281, 1319, 341, 281, 7908, 13, 50714, 50714, 1396, 2837, 2614, 18152, 307, 23751, 1968, 420, 406, 15245, 2710, 2144, 4583, 295, 9953, 51, 284, 339, 486, 312, 884, 341, 13, 51064, 51064, 400, 472, 1778, 291, 815, 528, 281, 10023, 264, 2614, 18152, 307, 570, 291, 815, 528, 281, 11, 337, 1365, 11, 12539, 552, 412, 264, 917, 382, 257, 3233, 732, 11, 411, 341, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10503743708818808, "compression_ratio": 1.6153846153846154, "no_speech_prob": 8.267556950158905e-06}, {"id": 871, "seek": 439900, "start": 4406.0, "end": 4413.0, "text": " Then track running stats is determining whether or not batch normalization layer of PyTorch will be doing this.", "tokens": [50364, 639, 307, 1920, 1009, 4305, 2074, 13, 286, 478, 406, 767, 988, 983, 291, 576, 528, 281, 1319, 341, 281, 7908, 13, 50714, 50714, 1396, 2837, 2614, 18152, 307, 23751, 1968, 420, 406, 15245, 2710, 2144, 4583, 295, 9953, 51, 284, 339, 486, 312, 884, 341, 13, 51064, 51064, 400, 472, 1778, 291, 815, 528, 281, 10023, 264, 2614, 18152, 307, 570, 291, 815, 528, 281, 11, 337, 1365, 11, 12539, 552, 412, 264, 917, 382, 257, 3233, 732, 11, 411, 341, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10503743708818808, "compression_ratio": 1.6153846153846154, "no_speech_prob": 8.267556950158905e-06}, {"id": 872, "seek": 439900, "start": 4413.0, "end": 4423.0, "text": " And one reason you may want to skip the running stats is because you may want to, for example, estimate them at the end as a stage two, like this.", "tokens": [50364, 639, 307, 1920, 1009, 4305, 2074, 13, 286, 478, 406, 767, 988, 983, 291, 576, 528, 281, 1319, 341, 281, 7908, 13, 50714, 50714, 1396, 2837, 2614, 18152, 307, 23751, 1968, 420, 406, 15245, 2710, 2144, 4583, 295, 9953, 51, 284, 339, 486, 312, 884, 341, 13, 51064, 51064, 400, 472, 1778, 291, 815, 528, 281, 10023, 264, 2614, 18152, 307, 570, 291, 815, 528, 281, 11, 337, 1365, 11, 12539, 552, 412, 264, 917, 382, 257, 3233, 732, 11, 411, 341, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10503743708818808, "compression_ratio": 1.6153846153846154, "no_speech_prob": 8.267556950158905e-06}, {"id": 873, "seek": 442300, "start": 4423.0, "end": 4429.0, "text": " And in that case, you don't want the batch normalization layer to be doing all this extra compute that you're not going to use.", "tokens": [50364, 400, 294, 300, 1389, 11, 291, 500, 380, 528, 264, 15245, 2710, 2144, 4583, 281, 312, 884, 439, 341, 2857, 14722, 300, 291, 434, 406, 516, 281, 764, 13, 50664, 50664, 400, 2721, 11, 321, 643, 281, 458, 597, 4302, 321, 434, 516, 281, 1190, 341, 15245, 2710, 2144, 322, 11, 257, 13199, 420, 257, 18407, 11, 50964, 50964, 293, 437, 264, 1412, 2010, 820, 312, 11, 1922, 18356, 11, 2167, 18356, 11, 3834, 18356, 11, 293, 370, 322, 13, 51264, 51264, 407, 300, 311, 264, 15245, 2710, 2144, 4583, 13, 10328, 11, 436, 2113, 281, 264, 3035, 13, 51414, 51414, 467, 311, 264, 912, 8513, 321, 600, 12270, 11, 293, 1203, 307, 264, 912, 2293, 382, 321, 600, 1096, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06760668376135448, "compression_ratio": 1.7864768683274022, "no_speech_prob": 5.421923560788855e-06}, {"id": 874, "seek": 442300, "start": 4429.0, "end": 4435.0, "text": " And finally, we need to know which device we're going to run this batch normalization on, a CPU or a GPU,", "tokens": [50364, 400, 294, 300, 1389, 11, 291, 500, 380, 528, 264, 15245, 2710, 2144, 4583, 281, 312, 884, 439, 341, 2857, 14722, 300, 291, 434, 406, 516, 281, 764, 13, 50664, 50664, 400, 2721, 11, 321, 643, 281, 458, 597, 4302, 321, 434, 516, 281, 1190, 341, 15245, 2710, 2144, 322, 11, 257, 13199, 420, 257, 18407, 11, 50964, 50964, 293, 437, 264, 1412, 2010, 820, 312, 11, 1922, 18356, 11, 2167, 18356, 11, 3834, 18356, 11, 293, 370, 322, 13, 51264, 51264, 407, 300, 311, 264, 15245, 2710, 2144, 4583, 13, 10328, 11, 436, 2113, 281, 264, 3035, 13, 51414, 51414, 467, 311, 264, 912, 8513, 321, 600, 12270, 11, 293, 1203, 307, 264, 912, 2293, 382, 321, 600, 1096, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06760668376135448, "compression_ratio": 1.7864768683274022, "no_speech_prob": 5.421923560788855e-06}, {"id": 875, "seek": 442300, "start": 4435.0, "end": 4441.0, "text": " and what the data type should be, half precision, single precision, double precision, and so on.", "tokens": [50364, 400, 294, 300, 1389, 11, 291, 500, 380, 528, 264, 15245, 2710, 2144, 4583, 281, 312, 884, 439, 341, 2857, 14722, 300, 291, 434, 406, 516, 281, 764, 13, 50664, 50664, 400, 2721, 11, 321, 643, 281, 458, 597, 4302, 321, 434, 516, 281, 1190, 341, 15245, 2710, 2144, 322, 11, 257, 13199, 420, 257, 18407, 11, 50964, 50964, 293, 437, 264, 1412, 2010, 820, 312, 11, 1922, 18356, 11, 2167, 18356, 11, 3834, 18356, 11, 293, 370, 322, 13, 51264, 51264, 407, 300, 311, 264, 15245, 2710, 2144, 4583, 13, 10328, 11, 436, 2113, 281, 264, 3035, 13, 51414, 51414, 467, 311, 264, 912, 8513, 321, 600, 12270, 11, 293, 1203, 307, 264, 912, 2293, 382, 321, 600, 1096, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06760668376135448, "compression_ratio": 1.7864768683274022, "no_speech_prob": 5.421923560788855e-06}, {"id": 876, "seek": 442300, "start": 4441.0, "end": 4444.0, "text": " So that's the batch normalization layer. Otherwise, they link to the paper.", "tokens": [50364, 400, 294, 300, 1389, 11, 291, 500, 380, 528, 264, 15245, 2710, 2144, 4583, 281, 312, 884, 439, 341, 2857, 14722, 300, 291, 434, 406, 516, 281, 764, 13, 50664, 50664, 400, 2721, 11, 321, 643, 281, 458, 597, 4302, 321, 434, 516, 281, 1190, 341, 15245, 2710, 2144, 322, 11, 257, 13199, 420, 257, 18407, 11, 50964, 50964, 293, 437, 264, 1412, 2010, 820, 312, 11, 1922, 18356, 11, 2167, 18356, 11, 3834, 18356, 11, 293, 370, 322, 13, 51264, 51264, 407, 300, 311, 264, 15245, 2710, 2144, 4583, 13, 10328, 11, 436, 2113, 281, 264, 3035, 13, 51414, 51414, 467, 311, 264, 912, 8513, 321, 600, 12270, 11, 293, 1203, 307, 264, 912, 2293, 382, 321, 600, 1096, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06760668376135448, "compression_ratio": 1.7864768683274022, "no_speech_prob": 5.421923560788855e-06}, {"id": 877, "seek": 442300, "start": 4444.0, "end": 4451.0, "text": " It's the same formula we've implemented, and everything is the same exactly as we've done here.", "tokens": [50364, 400, 294, 300, 1389, 11, 291, 500, 380, 528, 264, 15245, 2710, 2144, 4583, 281, 312, 884, 439, 341, 2857, 14722, 300, 291, 434, 406, 516, 281, 764, 13, 50664, 50664, 400, 2721, 11, 321, 643, 281, 458, 597, 4302, 321, 434, 516, 281, 1190, 341, 15245, 2710, 2144, 322, 11, 257, 13199, 420, 257, 18407, 11, 50964, 50964, 293, 437, 264, 1412, 2010, 820, 312, 11, 1922, 18356, 11, 2167, 18356, 11, 3834, 18356, 11, 293, 370, 322, 13, 51264, 51264, 407, 300, 311, 264, 15245, 2710, 2144, 4583, 13, 10328, 11, 436, 2113, 281, 264, 3035, 13, 51414, 51414, 467, 311, 264, 912, 8513, 321, 600, 12270, 11, 293, 1203, 307, 264, 912, 2293, 382, 321, 600, 1096, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06760668376135448, "compression_ratio": 1.7864768683274022, "no_speech_prob": 5.421923560788855e-06}, {"id": 878, "seek": 445100, "start": 4451.0, "end": 4454.0, "text": " OK, so that's everything that I wanted to cover for this lecture.", "tokens": [50364, 2264, 11, 370, 300, 311, 1203, 300, 286, 1415, 281, 2060, 337, 341, 7991, 13, 50514, 50514, 4083, 11, 437, 286, 1415, 281, 751, 466, 307, 264, 7379, 295, 3701, 264, 2430, 763, 293, 264, 2771, 2448, 293, 641, 12523, 294, 18161, 9590, 13, 50814, 50814, 400, 341, 3643, 12980, 1021, 11, 2318, 382, 291, 652, 428, 18161, 9590, 3801, 11, 4833, 11, 293, 7731, 13, 51114, 51114, 492, 2956, 412, 264, 37870, 1936, 412, 264, 5598, 4583, 11, 293, 321, 1866, 300, 498, 291, 362, 732, 6679, 3346, 79, 986, 15607, 51414, 51414, 570, 264, 2430, 763, 366, 886, 16507, 493, 412, 264, 1036, 4583, 11, 291, 393, 917, 493, 365, 613, 22449, 2897, 15352, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.057224596827483376, "compression_ratio": 1.8125, "no_speech_prob": 5.093475465400843e-06}, {"id": 879, "seek": 445100, "start": 4454.0, "end": 4460.0, "text": " Really, what I wanted to talk about is the importance of understanding the activations and the gradients and their statistics in neural networks.", "tokens": [50364, 2264, 11, 370, 300, 311, 1203, 300, 286, 1415, 281, 2060, 337, 341, 7991, 13, 50514, 50514, 4083, 11, 437, 286, 1415, 281, 751, 466, 307, 264, 7379, 295, 3701, 264, 2430, 763, 293, 264, 2771, 2448, 293, 641, 12523, 294, 18161, 9590, 13, 50814, 50814, 400, 341, 3643, 12980, 1021, 11, 2318, 382, 291, 652, 428, 18161, 9590, 3801, 11, 4833, 11, 293, 7731, 13, 51114, 51114, 492, 2956, 412, 264, 37870, 1936, 412, 264, 5598, 4583, 11, 293, 321, 1866, 300, 498, 291, 362, 732, 6679, 3346, 79, 986, 15607, 51414, 51414, 570, 264, 2430, 763, 366, 886, 16507, 493, 412, 264, 1036, 4583, 11, 291, 393, 917, 493, 365, 613, 22449, 2897, 15352, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.057224596827483376, "compression_ratio": 1.8125, "no_speech_prob": 5.093475465400843e-06}, {"id": 880, "seek": 445100, "start": 4460.0, "end": 4466.0, "text": " And this becomes increasingly important, especially as you make your neural networks bigger, larger, and deeper.", "tokens": [50364, 2264, 11, 370, 300, 311, 1203, 300, 286, 1415, 281, 2060, 337, 341, 7991, 13, 50514, 50514, 4083, 11, 437, 286, 1415, 281, 751, 466, 307, 264, 7379, 295, 3701, 264, 2430, 763, 293, 264, 2771, 2448, 293, 641, 12523, 294, 18161, 9590, 13, 50814, 50814, 400, 341, 3643, 12980, 1021, 11, 2318, 382, 291, 652, 428, 18161, 9590, 3801, 11, 4833, 11, 293, 7731, 13, 51114, 51114, 492, 2956, 412, 264, 37870, 1936, 412, 264, 5598, 4583, 11, 293, 321, 1866, 300, 498, 291, 362, 732, 6679, 3346, 79, 986, 15607, 51414, 51414, 570, 264, 2430, 763, 366, 886, 16507, 493, 412, 264, 1036, 4583, 11, 291, 393, 917, 493, 365, 613, 22449, 2897, 15352, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.057224596827483376, "compression_ratio": 1.8125, "no_speech_prob": 5.093475465400843e-06}, {"id": 881, "seek": 445100, "start": 4466.0, "end": 4472.0, "text": " We looked at the distributions basically at the output layer, and we saw that if you have two confident mispredictions", "tokens": [50364, 2264, 11, 370, 300, 311, 1203, 300, 286, 1415, 281, 2060, 337, 341, 7991, 13, 50514, 50514, 4083, 11, 437, 286, 1415, 281, 751, 466, 307, 264, 7379, 295, 3701, 264, 2430, 763, 293, 264, 2771, 2448, 293, 641, 12523, 294, 18161, 9590, 13, 50814, 50814, 400, 341, 3643, 12980, 1021, 11, 2318, 382, 291, 652, 428, 18161, 9590, 3801, 11, 4833, 11, 293, 7731, 13, 51114, 51114, 492, 2956, 412, 264, 37870, 1936, 412, 264, 5598, 4583, 11, 293, 321, 1866, 300, 498, 291, 362, 732, 6679, 3346, 79, 986, 15607, 51414, 51414, 570, 264, 2430, 763, 366, 886, 16507, 493, 412, 264, 1036, 4583, 11, 291, 393, 917, 493, 365, 613, 22449, 2897, 15352, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.057224596827483376, "compression_ratio": 1.8125, "no_speech_prob": 5.093475465400843e-06}, {"id": 882, "seek": 445100, "start": 4472.0, "end": 4477.0, "text": " because the activations are too messed up at the last layer, you can end up with these hockey stick losses.", "tokens": [50364, 2264, 11, 370, 300, 311, 1203, 300, 286, 1415, 281, 2060, 337, 341, 7991, 13, 50514, 50514, 4083, 11, 437, 286, 1415, 281, 751, 466, 307, 264, 7379, 295, 3701, 264, 2430, 763, 293, 264, 2771, 2448, 293, 641, 12523, 294, 18161, 9590, 13, 50814, 50814, 400, 341, 3643, 12980, 1021, 11, 2318, 382, 291, 652, 428, 18161, 9590, 3801, 11, 4833, 11, 293, 7731, 13, 51114, 51114, 492, 2956, 412, 264, 37870, 1936, 412, 264, 5598, 4583, 11, 293, 321, 1866, 300, 498, 291, 362, 732, 6679, 3346, 79, 986, 15607, 51414, 51414, 570, 264, 2430, 763, 366, 886, 16507, 493, 412, 264, 1036, 4583, 11, 291, 393, 917, 493, 365, 613, 22449, 2897, 15352, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.057224596827483376, "compression_ratio": 1.8125, "no_speech_prob": 5.093475465400843e-06}, {"id": 883, "seek": 447700, "start": 4477.0, "end": 4483.0, "text": " And if you fix this, you get a better loss at the end of training because your training is not doing wasteful work.", "tokens": [50364, 400, 498, 291, 3191, 341, 11, 291, 483, 257, 1101, 4470, 412, 264, 917, 295, 3097, 570, 428, 3097, 307, 406, 884, 5964, 906, 589, 13, 50664, 50664, 1396, 321, 611, 1866, 300, 321, 643, 281, 1969, 264, 2430, 763, 13, 492, 500, 380, 528, 552, 281, 30725, 281, 4018, 420, 21411, 281, 13202, 11, 51014, 51014, 570, 300, 291, 393, 1190, 666, 257, 688, 295, 5253, 365, 439, 295, 613, 2107, 28263, 1088, 294, 613, 18161, 36170, 13, 51264, 51264, 400, 1936, 11, 291, 528, 1203, 281, 312, 6457, 42632, 3710, 264, 18161, 2533, 13, 51464, 51464, 509, 528, 9810, 39148, 2430, 763, 3710, 264, 18161, 2533, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06960532939539546, "compression_ratio": 1.7745454545454546, "no_speech_prob": 9.817577847570647e-06}, {"id": 884, "seek": 447700, "start": 4483.0, "end": 4490.0, "text": " Then we also saw that we need to control the activations. We don't want them to squash to zero or explode to infinity,", "tokens": [50364, 400, 498, 291, 3191, 341, 11, 291, 483, 257, 1101, 4470, 412, 264, 917, 295, 3097, 570, 428, 3097, 307, 406, 884, 5964, 906, 589, 13, 50664, 50664, 1396, 321, 611, 1866, 300, 321, 643, 281, 1969, 264, 2430, 763, 13, 492, 500, 380, 528, 552, 281, 30725, 281, 4018, 420, 21411, 281, 13202, 11, 51014, 51014, 570, 300, 291, 393, 1190, 666, 257, 688, 295, 5253, 365, 439, 295, 613, 2107, 28263, 1088, 294, 613, 18161, 36170, 13, 51264, 51264, 400, 1936, 11, 291, 528, 1203, 281, 312, 6457, 42632, 3710, 264, 18161, 2533, 13, 51464, 51464, 509, 528, 9810, 39148, 2430, 763, 3710, 264, 18161, 2533, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06960532939539546, "compression_ratio": 1.7745454545454546, "no_speech_prob": 9.817577847570647e-06}, {"id": 885, "seek": 447700, "start": 4490.0, "end": 4495.0, "text": " because that you can run into a lot of trouble with all of these nonlinearities in these neural nets.", "tokens": [50364, 400, 498, 291, 3191, 341, 11, 291, 483, 257, 1101, 4470, 412, 264, 917, 295, 3097, 570, 428, 3097, 307, 406, 884, 5964, 906, 589, 13, 50664, 50664, 1396, 321, 611, 1866, 300, 321, 643, 281, 1969, 264, 2430, 763, 13, 492, 500, 380, 528, 552, 281, 30725, 281, 4018, 420, 21411, 281, 13202, 11, 51014, 51014, 570, 300, 291, 393, 1190, 666, 257, 688, 295, 5253, 365, 439, 295, 613, 2107, 28263, 1088, 294, 613, 18161, 36170, 13, 51264, 51264, 400, 1936, 11, 291, 528, 1203, 281, 312, 6457, 42632, 3710, 264, 18161, 2533, 13, 51464, 51464, 509, 528, 9810, 39148, 2430, 763, 3710, 264, 18161, 2533, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06960532939539546, "compression_ratio": 1.7745454545454546, "no_speech_prob": 9.817577847570647e-06}, {"id": 886, "seek": 447700, "start": 4495.0, "end": 4499.0, "text": " And basically, you want everything to be fairly homogeneous throughout the neural net.", "tokens": [50364, 400, 498, 291, 3191, 341, 11, 291, 483, 257, 1101, 4470, 412, 264, 917, 295, 3097, 570, 428, 3097, 307, 406, 884, 5964, 906, 589, 13, 50664, 50664, 1396, 321, 611, 1866, 300, 321, 643, 281, 1969, 264, 2430, 763, 13, 492, 500, 380, 528, 552, 281, 30725, 281, 4018, 420, 21411, 281, 13202, 11, 51014, 51014, 570, 300, 291, 393, 1190, 666, 257, 688, 295, 5253, 365, 439, 295, 613, 2107, 28263, 1088, 294, 613, 18161, 36170, 13, 51264, 51264, 400, 1936, 11, 291, 528, 1203, 281, 312, 6457, 42632, 3710, 264, 18161, 2533, 13, 51464, 51464, 509, 528, 9810, 39148, 2430, 763, 3710, 264, 18161, 2533, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06960532939539546, "compression_ratio": 1.7745454545454546, "no_speech_prob": 9.817577847570647e-06}, {"id": 887, "seek": 447700, "start": 4499.0, "end": 4502.0, "text": " You want roughly Gaussian activations throughout the neural net.", "tokens": [50364, 400, 498, 291, 3191, 341, 11, 291, 483, 257, 1101, 4470, 412, 264, 917, 295, 3097, 570, 428, 3097, 307, 406, 884, 5964, 906, 589, 13, 50664, 50664, 1396, 321, 611, 1866, 300, 321, 643, 281, 1969, 264, 2430, 763, 13, 492, 500, 380, 528, 552, 281, 30725, 281, 4018, 420, 21411, 281, 13202, 11, 51014, 51014, 570, 300, 291, 393, 1190, 666, 257, 688, 295, 5253, 365, 439, 295, 613, 2107, 28263, 1088, 294, 613, 18161, 36170, 13, 51264, 51264, 400, 1936, 11, 291, 528, 1203, 281, 312, 6457, 42632, 3710, 264, 18161, 2533, 13, 51464, 51464, 509, 528, 9810, 39148, 2430, 763, 3710, 264, 18161, 2533, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06960532939539546, "compression_ratio": 1.7745454545454546, "no_speech_prob": 9.817577847570647e-06}, {"id": 888, "seek": 450200, "start": 4502.0, "end": 4509.0, "text": " Then we talked about, OK, if we want roughly Gaussian activations, how do we scale these weight matrices and biases", "tokens": [50364, 1396, 321, 2825, 466, 11, 2264, 11, 498, 321, 528, 9810, 39148, 2430, 763, 11, 577, 360, 321, 4373, 613, 3364, 32284, 293, 32152, 50714, 50714, 1830, 5883, 2144, 295, 264, 18161, 2533, 370, 300, 321, 500, 380, 483, 11, 291, 458, 11, 370, 1203, 307, 382, 10164, 382, 1944, 13, 51114, 51114, 407, 300, 2729, 505, 257, 2416, 9194, 294, 10444, 13, 51264, 51264, 400, 550, 286, 2825, 466, 577, 300, 5206, 307, 406, 767, 1944, 337, 709, 11, 709, 7731, 18161, 36170, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08698934383606644, "compression_ratio": 1.559670781893004, "no_speech_prob": 4.936744062433718e-06}, {"id": 889, "seek": 450200, "start": 4509.0, "end": 4517.0, "text": " during initialization of the neural net so that we don't get, you know, so everything is as controlled as possible.", "tokens": [50364, 1396, 321, 2825, 466, 11, 2264, 11, 498, 321, 528, 9810, 39148, 2430, 763, 11, 577, 360, 321, 4373, 613, 3364, 32284, 293, 32152, 50714, 50714, 1830, 5883, 2144, 295, 264, 18161, 2533, 370, 300, 321, 500, 380, 483, 11, 291, 458, 11, 370, 1203, 307, 382, 10164, 382, 1944, 13, 51114, 51114, 407, 300, 2729, 505, 257, 2416, 9194, 294, 10444, 13, 51264, 51264, 400, 550, 286, 2825, 466, 577, 300, 5206, 307, 406, 767, 1944, 337, 709, 11, 709, 7731, 18161, 36170, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08698934383606644, "compression_ratio": 1.559670781893004, "no_speech_prob": 4.936744062433718e-06}, {"id": 890, "seek": 450200, "start": 4517.0, "end": 4520.0, "text": " So that gave us a large boost in improvement.", "tokens": [50364, 1396, 321, 2825, 466, 11, 2264, 11, 498, 321, 528, 9810, 39148, 2430, 763, 11, 577, 360, 321, 4373, 613, 3364, 32284, 293, 32152, 50714, 50714, 1830, 5883, 2144, 295, 264, 18161, 2533, 370, 300, 321, 500, 380, 483, 11, 291, 458, 11, 370, 1203, 307, 382, 10164, 382, 1944, 13, 51114, 51114, 407, 300, 2729, 505, 257, 2416, 9194, 294, 10444, 13, 51264, 51264, 400, 550, 286, 2825, 466, 577, 300, 5206, 307, 406, 767, 1944, 337, 709, 11, 709, 7731, 18161, 36170, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08698934383606644, "compression_ratio": 1.559670781893004, "no_speech_prob": 4.936744062433718e-06}, {"id": 891, "seek": 450200, "start": 4520.0, "end": 4527.0, "text": " And then I talked about how that strategy is not actually possible for much, much deeper neural nets,", "tokens": [50364, 1396, 321, 2825, 466, 11, 2264, 11, 498, 321, 528, 9810, 39148, 2430, 763, 11, 577, 360, 321, 4373, 613, 3364, 32284, 293, 32152, 50714, 50714, 1830, 5883, 2144, 295, 264, 18161, 2533, 370, 300, 321, 500, 380, 483, 11, 291, 458, 11, 370, 1203, 307, 382, 10164, 382, 1944, 13, 51114, 51114, 407, 300, 2729, 505, 257, 2416, 9194, 294, 10444, 13, 51264, 51264, 400, 550, 286, 2825, 466, 577, 300, 5206, 307, 406, 767, 1944, 337, 709, 11, 709, 7731, 18161, 36170, 11, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08698934383606644, "compression_ratio": 1.559670781893004, "no_speech_prob": 4.936744062433718e-06}, {"id": 892, "seek": 452700, "start": 4527.0, "end": 4532.0, "text": " because when you have much deeper neural nets with lots of different types of layers,", "tokens": [50364, 570, 562, 291, 362, 709, 7731, 18161, 36170, 365, 3195, 295, 819, 3467, 295, 7914, 11, 50614, 50614, 309, 3643, 534, 11, 534, 1152, 281, 13402, 992, 264, 17443, 293, 264, 32152, 294, 1270, 257, 636, 50864, 50864, 300, 264, 2430, 763, 366, 9810, 9452, 3710, 264, 18161, 2533, 13, 51064, 51064, 407, 550, 286, 7268, 264, 10710, 295, 264, 2710, 2144, 4583, 13, 51214, 51214, 823, 11, 456, 366, 867, 2710, 2144, 7914, 300, 561, 764, 294, 3124, 13, 51364, 51364, 43068, 2710, 2144, 11, 4583, 2710, 2144, 11, 5197, 2710, 2144, 11, 1594, 2710, 2144, 13, 51614, 51614, 492, 2378, 380, 5343, 881, 295, 552, 11, 457, 286, 600, 7268, 264, 700, 472, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.04367156823476156, "compression_ratio": 1.8920863309352518, "no_speech_prob": 8.396968041779473e-06}, {"id": 893, "seek": 452700, "start": 4532.0, "end": 4537.0, "text": " it becomes really, really hard to precisely set the weights and the biases in such a way", "tokens": [50364, 570, 562, 291, 362, 709, 7731, 18161, 36170, 365, 3195, 295, 819, 3467, 295, 7914, 11, 50614, 50614, 309, 3643, 534, 11, 534, 1152, 281, 13402, 992, 264, 17443, 293, 264, 32152, 294, 1270, 257, 636, 50864, 50864, 300, 264, 2430, 763, 366, 9810, 9452, 3710, 264, 18161, 2533, 13, 51064, 51064, 407, 550, 286, 7268, 264, 10710, 295, 264, 2710, 2144, 4583, 13, 51214, 51214, 823, 11, 456, 366, 867, 2710, 2144, 7914, 300, 561, 764, 294, 3124, 13, 51364, 51364, 43068, 2710, 2144, 11, 4583, 2710, 2144, 11, 5197, 2710, 2144, 11, 1594, 2710, 2144, 13, 51614, 51614, 492, 2378, 380, 5343, 881, 295, 552, 11, 457, 286, 600, 7268, 264, 700, 472, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.04367156823476156, "compression_ratio": 1.8920863309352518, "no_speech_prob": 8.396968041779473e-06}, {"id": 894, "seek": 452700, "start": 4537.0, "end": 4541.0, "text": " that the activations are roughly uniform throughout the neural net.", "tokens": [50364, 570, 562, 291, 362, 709, 7731, 18161, 36170, 365, 3195, 295, 819, 3467, 295, 7914, 11, 50614, 50614, 309, 3643, 534, 11, 534, 1152, 281, 13402, 992, 264, 17443, 293, 264, 32152, 294, 1270, 257, 636, 50864, 50864, 300, 264, 2430, 763, 366, 9810, 9452, 3710, 264, 18161, 2533, 13, 51064, 51064, 407, 550, 286, 7268, 264, 10710, 295, 264, 2710, 2144, 4583, 13, 51214, 51214, 823, 11, 456, 366, 867, 2710, 2144, 7914, 300, 561, 764, 294, 3124, 13, 51364, 51364, 43068, 2710, 2144, 11, 4583, 2710, 2144, 11, 5197, 2710, 2144, 11, 1594, 2710, 2144, 13, 51614, 51614, 492, 2378, 380, 5343, 881, 295, 552, 11, 457, 286, 600, 7268, 264, 700, 472, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.04367156823476156, "compression_ratio": 1.8920863309352518, "no_speech_prob": 8.396968041779473e-06}, {"id": 895, "seek": 452700, "start": 4541.0, "end": 4544.0, "text": " So then I introduced the notion of the normalization layer.", "tokens": [50364, 570, 562, 291, 362, 709, 7731, 18161, 36170, 365, 3195, 295, 819, 3467, 295, 7914, 11, 50614, 50614, 309, 3643, 534, 11, 534, 1152, 281, 13402, 992, 264, 17443, 293, 264, 32152, 294, 1270, 257, 636, 50864, 50864, 300, 264, 2430, 763, 366, 9810, 9452, 3710, 264, 18161, 2533, 13, 51064, 51064, 407, 550, 286, 7268, 264, 10710, 295, 264, 2710, 2144, 4583, 13, 51214, 51214, 823, 11, 456, 366, 867, 2710, 2144, 7914, 300, 561, 764, 294, 3124, 13, 51364, 51364, 43068, 2710, 2144, 11, 4583, 2710, 2144, 11, 5197, 2710, 2144, 11, 1594, 2710, 2144, 13, 51614, 51614, 492, 2378, 380, 5343, 881, 295, 552, 11, 457, 286, 600, 7268, 264, 700, 472, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.04367156823476156, "compression_ratio": 1.8920863309352518, "no_speech_prob": 8.396968041779473e-06}, {"id": 896, "seek": 452700, "start": 4544.0, "end": 4547.0, "text": " Now, there are many normalization layers that people use in practice.", "tokens": [50364, 570, 562, 291, 362, 709, 7731, 18161, 36170, 365, 3195, 295, 819, 3467, 295, 7914, 11, 50614, 50614, 309, 3643, 534, 11, 534, 1152, 281, 13402, 992, 264, 17443, 293, 264, 32152, 294, 1270, 257, 636, 50864, 50864, 300, 264, 2430, 763, 366, 9810, 9452, 3710, 264, 18161, 2533, 13, 51064, 51064, 407, 550, 286, 7268, 264, 10710, 295, 264, 2710, 2144, 4583, 13, 51214, 51214, 823, 11, 456, 366, 867, 2710, 2144, 7914, 300, 561, 764, 294, 3124, 13, 51364, 51364, 43068, 2710, 2144, 11, 4583, 2710, 2144, 11, 5197, 2710, 2144, 11, 1594, 2710, 2144, 13, 51614, 51614, 492, 2378, 380, 5343, 881, 295, 552, 11, 457, 286, 600, 7268, 264, 700, 472, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.04367156823476156, "compression_ratio": 1.8920863309352518, "no_speech_prob": 8.396968041779473e-06}, {"id": 897, "seek": 452700, "start": 4547.0, "end": 4552.0, "text": " Bash normalization, layer normalization, instance normalization, group normalization.", "tokens": [50364, 570, 562, 291, 362, 709, 7731, 18161, 36170, 365, 3195, 295, 819, 3467, 295, 7914, 11, 50614, 50614, 309, 3643, 534, 11, 534, 1152, 281, 13402, 992, 264, 17443, 293, 264, 32152, 294, 1270, 257, 636, 50864, 50864, 300, 264, 2430, 763, 366, 9810, 9452, 3710, 264, 18161, 2533, 13, 51064, 51064, 407, 550, 286, 7268, 264, 10710, 295, 264, 2710, 2144, 4583, 13, 51214, 51214, 823, 11, 456, 366, 867, 2710, 2144, 7914, 300, 561, 764, 294, 3124, 13, 51364, 51364, 43068, 2710, 2144, 11, 4583, 2710, 2144, 11, 5197, 2710, 2144, 11, 1594, 2710, 2144, 13, 51614, 51614, 492, 2378, 380, 5343, 881, 295, 552, 11, 457, 286, 600, 7268, 264, 700, 472, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.04367156823476156, "compression_ratio": 1.8920863309352518, "no_speech_prob": 8.396968041779473e-06}, {"id": 898, "seek": 452700, "start": 4552.0, "end": 4555.0, "text": " We haven't covered most of them, but I've introduced the first one.", "tokens": [50364, 570, 562, 291, 362, 709, 7731, 18161, 36170, 365, 3195, 295, 819, 3467, 295, 7914, 11, 50614, 50614, 309, 3643, 534, 11, 534, 1152, 281, 13402, 992, 264, 17443, 293, 264, 32152, 294, 1270, 257, 636, 50864, 50864, 300, 264, 2430, 763, 366, 9810, 9452, 3710, 264, 18161, 2533, 13, 51064, 51064, 407, 550, 286, 7268, 264, 10710, 295, 264, 2710, 2144, 4583, 13, 51214, 51214, 823, 11, 456, 366, 867, 2710, 2144, 7914, 300, 561, 764, 294, 3124, 13, 51364, 51364, 43068, 2710, 2144, 11, 4583, 2710, 2144, 11, 5197, 2710, 2144, 11, 1594, 2710, 2144, 13, 51614, 51614, 492, 2378, 380, 5343, 881, 295, 552, 11, 457, 286, 600, 7268, 264, 700, 472, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.04367156823476156, "compression_ratio": 1.8920863309352518, "no_speech_prob": 8.396968041779473e-06}, {"id": 899, "seek": 455500, "start": 4555.0, "end": 4560.0, "text": " And also the one that I believe came out first, and that's called bash normalization.", "tokens": [50364, 400, 611, 264, 472, 300, 286, 1697, 1361, 484, 700, 11, 293, 300, 311, 1219, 46183, 2710, 2144, 13, 50614, 50614, 400, 321, 1866, 577, 46183, 2710, 2144, 1985, 13, 50714, 50714, 639, 307, 257, 4583, 300, 291, 393, 24745, 3710, 428, 2452, 18161, 2533, 13, 50914, 50914, 400, 264, 3875, 1558, 307, 498, 291, 528, 9810, 39148, 2430, 763, 11, 731, 11, 550, 747, 428, 2430, 763, 51214, 51214, 293, 747, 264, 914, 293, 3832, 25163, 293, 3056, 428, 1412, 13, 51414, 51414, 400, 291, 393, 360, 300, 570, 264, 1489, 1794, 6916, 307, 819, 9364, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06324310863719267, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.665669495007023e-06}, {"id": 900, "seek": 455500, "start": 4560.0, "end": 4562.0, "text": " And we saw how bash normalization works.", "tokens": [50364, 400, 611, 264, 472, 300, 286, 1697, 1361, 484, 700, 11, 293, 300, 311, 1219, 46183, 2710, 2144, 13, 50614, 50614, 400, 321, 1866, 577, 46183, 2710, 2144, 1985, 13, 50714, 50714, 639, 307, 257, 4583, 300, 291, 393, 24745, 3710, 428, 2452, 18161, 2533, 13, 50914, 50914, 400, 264, 3875, 1558, 307, 498, 291, 528, 9810, 39148, 2430, 763, 11, 731, 11, 550, 747, 428, 2430, 763, 51214, 51214, 293, 747, 264, 914, 293, 3832, 25163, 293, 3056, 428, 1412, 13, 51414, 51414, 400, 291, 393, 360, 300, 570, 264, 1489, 1794, 6916, 307, 819, 9364, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06324310863719267, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.665669495007023e-06}, {"id": 901, "seek": 455500, "start": 4562.0, "end": 4566.0, "text": " This is a layer that you can sprinkle throughout your deep neural net.", "tokens": [50364, 400, 611, 264, 472, 300, 286, 1697, 1361, 484, 700, 11, 293, 300, 311, 1219, 46183, 2710, 2144, 13, 50614, 50614, 400, 321, 1866, 577, 46183, 2710, 2144, 1985, 13, 50714, 50714, 639, 307, 257, 4583, 300, 291, 393, 24745, 3710, 428, 2452, 18161, 2533, 13, 50914, 50914, 400, 264, 3875, 1558, 307, 498, 291, 528, 9810, 39148, 2430, 763, 11, 731, 11, 550, 747, 428, 2430, 763, 51214, 51214, 293, 747, 264, 914, 293, 3832, 25163, 293, 3056, 428, 1412, 13, 51414, 51414, 400, 291, 393, 360, 300, 570, 264, 1489, 1794, 6916, 307, 819, 9364, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06324310863719267, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.665669495007023e-06}, {"id": 902, "seek": 455500, "start": 4566.0, "end": 4572.0, "text": " And the basic idea is if you want roughly Gaussian activations, well, then take your activations", "tokens": [50364, 400, 611, 264, 472, 300, 286, 1697, 1361, 484, 700, 11, 293, 300, 311, 1219, 46183, 2710, 2144, 13, 50614, 50614, 400, 321, 1866, 577, 46183, 2710, 2144, 1985, 13, 50714, 50714, 639, 307, 257, 4583, 300, 291, 393, 24745, 3710, 428, 2452, 18161, 2533, 13, 50914, 50914, 400, 264, 3875, 1558, 307, 498, 291, 528, 9810, 39148, 2430, 763, 11, 731, 11, 550, 747, 428, 2430, 763, 51214, 51214, 293, 747, 264, 914, 293, 3832, 25163, 293, 3056, 428, 1412, 13, 51414, 51414, 400, 291, 393, 360, 300, 570, 264, 1489, 1794, 6916, 307, 819, 9364, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06324310863719267, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.665669495007023e-06}, {"id": 903, "seek": 455500, "start": 4572.0, "end": 4576.0, "text": " and take the mean and standard deviation and center your data.", "tokens": [50364, 400, 611, 264, 472, 300, 286, 1697, 1361, 484, 700, 11, 293, 300, 311, 1219, 46183, 2710, 2144, 13, 50614, 50614, 400, 321, 1866, 577, 46183, 2710, 2144, 1985, 13, 50714, 50714, 639, 307, 257, 4583, 300, 291, 393, 24745, 3710, 428, 2452, 18161, 2533, 13, 50914, 50914, 400, 264, 3875, 1558, 307, 498, 291, 528, 9810, 39148, 2430, 763, 11, 731, 11, 550, 747, 428, 2430, 763, 51214, 51214, 293, 747, 264, 914, 293, 3832, 25163, 293, 3056, 428, 1412, 13, 51414, 51414, 400, 291, 393, 360, 300, 570, 264, 1489, 1794, 6916, 307, 819, 9364, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06324310863719267, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.665669495007023e-06}, {"id": 904, "seek": 455500, "start": 4576.0, "end": 4581.0, "text": " And you can do that because the centering operation is differentiable.", "tokens": [50364, 400, 611, 264, 472, 300, 286, 1697, 1361, 484, 700, 11, 293, 300, 311, 1219, 46183, 2710, 2144, 13, 50614, 50614, 400, 321, 1866, 577, 46183, 2710, 2144, 1985, 13, 50714, 50714, 639, 307, 257, 4583, 300, 291, 393, 24745, 3710, 428, 2452, 18161, 2533, 13, 50914, 50914, 400, 264, 3875, 1558, 307, 498, 291, 528, 9810, 39148, 2430, 763, 11, 731, 11, 550, 747, 428, 2430, 763, 51214, 51214, 293, 747, 264, 914, 293, 3832, 25163, 293, 3056, 428, 1412, 13, 51414, 51414, 400, 291, 393, 360, 300, 570, 264, 1489, 1794, 6916, 307, 819, 9364, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06324310863719267, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.665669495007023e-06}, {"id": 905, "seek": 458100, "start": 4581.0, "end": 4585.0, "text": " But on top of that, we actually had to add a lot of bells and whistles,", "tokens": [50364, 583, 322, 1192, 295, 300, 11, 321, 767, 632, 281, 909, 257, 688, 295, 25474, 293, 49282, 11, 50564, 50564, 293, 300, 2729, 291, 257, 2020, 295, 264, 48705, 295, 264, 46183, 2710, 2144, 4583, 11, 50714, 50714, 570, 586, 321, 434, 1489, 1794, 264, 1412, 11, 300, 311, 869, 11, 457, 5800, 321, 643, 264, 6052, 293, 264, 12577, 13, 50964, 50964, 400, 586, 729, 366, 3847, 712, 13, 51064, 51064, 400, 550, 570, 321, 366, 37447, 439, 264, 3097, 5110, 11, 51214, 51214, 586, 5800, 264, 1168, 307, 577, 360, 291, 360, 264, 38253, 30, 51364, 51364, 2305, 281, 360, 264, 38253, 11, 321, 643, 281, 586, 12539, 613, 914, 293, 3832, 25163, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08246155746844637, "compression_ratio": 1.7769516728624535, "no_speech_prob": 1.3419550668913871e-05}, {"id": 906, "seek": 458100, "start": 4585.0, "end": 4588.0, "text": " and that gave you a sense of the complexities of the bash normalization layer,", "tokens": [50364, 583, 322, 1192, 295, 300, 11, 321, 767, 632, 281, 909, 257, 688, 295, 25474, 293, 49282, 11, 50564, 50564, 293, 300, 2729, 291, 257, 2020, 295, 264, 48705, 295, 264, 46183, 2710, 2144, 4583, 11, 50714, 50714, 570, 586, 321, 434, 1489, 1794, 264, 1412, 11, 300, 311, 869, 11, 457, 5800, 321, 643, 264, 6052, 293, 264, 12577, 13, 50964, 50964, 400, 586, 729, 366, 3847, 712, 13, 51064, 51064, 400, 550, 570, 321, 366, 37447, 439, 264, 3097, 5110, 11, 51214, 51214, 586, 5800, 264, 1168, 307, 577, 360, 291, 360, 264, 38253, 30, 51364, 51364, 2305, 281, 360, 264, 38253, 11, 321, 643, 281, 586, 12539, 613, 914, 293, 3832, 25163, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08246155746844637, "compression_ratio": 1.7769516728624535, "no_speech_prob": 1.3419550668913871e-05}, {"id": 907, "seek": 458100, "start": 4588.0, "end": 4593.0, "text": " because now we're centering the data, that's great, but suddenly we need the gain and the bias.", "tokens": [50364, 583, 322, 1192, 295, 300, 11, 321, 767, 632, 281, 909, 257, 688, 295, 25474, 293, 49282, 11, 50564, 50564, 293, 300, 2729, 291, 257, 2020, 295, 264, 48705, 295, 264, 46183, 2710, 2144, 4583, 11, 50714, 50714, 570, 586, 321, 434, 1489, 1794, 264, 1412, 11, 300, 311, 869, 11, 457, 5800, 321, 643, 264, 6052, 293, 264, 12577, 13, 50964, 50964, 400, 586, 729, 366, 3847, 712, 13, 51064, 51064, 400, 550, 570, 321, 366, 37447, 439, 264, 3097, 5110, 11, 51214, 51214, 586, 5800, 264, 1168, 307, 577, 360, 291, 360, 264, 38253, 30, 51364, 51364, 2305, 281, 360, 264, 38253, 11, 321, 643, 281, 586, 12539, 613, 914, 293, 3832, 25163, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08246155746844637, "compression_ratio": 1.7769516728624535, "no_speech_prob": 1.3419550668913871e-05}, {"id": 908, "seek": 458100, "start": 4593.0, "end": 4595.0, "text": " And now those are trainable.", "tokens": [50364, 583, 322, 1192, 295, 300, 11, 321, 767, 632, 281, 909, 257, 688, 295, 25474, 293, 49282, 11, 50564, 50564, 293, 300, 2729, 291, 257, 2020, 295, 264, 48705, 295, 264, 46183, 2710, 2144, 4583, 11, 50714, 50714, 570, 586, 321, 434, 1489, 1794, 264, 1412, 11, 300, 311, 869, 11, 457, 5800, 321, 643, 264, 6052, 293, 264, 12577, 13, 50964, 50964, 400, 586, 729, 366, 3847, 712, 13, 51064, 51064, 400, 550, 570, 321, 366, 37447, 439, 264, 3097, 5110, 11, 51214, 51214, 586, 5800, 264, 1168, 307, 577, 360, 291, 360, 264, 38253, 30, 51364, 51364, 2305, 281, 360, 264, 38253, 11, 321, 643, 281, 586, 12539, 613, 914, 293, 3832, 25163, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08246155746844637, "compression_ratio": 1.7769516728624535, "no_speech_prob": 1.3419550668913871e-05}, {"id": 909, "seek": 458100, "start": 4595.0, "end": 4598.0, "text": " And then because we are coupling all the training examples,", "tokens": [50364, 583, 322, 1192, 295, 300, 11, 321, 767, 632, 281, 909, 257, 688, 295, 25474, 293, 49282, 11, 50564, 50564, 293, 300, 2729, 291, 257, 2020, 295, 264, 48705, 295, 264, 46183, 2710, 2144, 4583, 11, 50714, 50714, 570, 586, 321, 434, 1489, 1794, 264, 1412, 11, 300, 311, 869, 11, 457, 5800, 321, 643, 264, 6052, 293, 264, 12577, 13, 50964, 50964, 400, 586, 729, 366, 3847, 712, 13, 51064, 51064, 400, 550, 570, 321, 366, 37447, 439, 264, 3097, 5110, 11, 51214, 51214, 586, 5800, 264, 1168, 307, 577, 360, 291, 360, 264, 38253, 30, 51364, 51364, 2305, 281, 360, 264, 38253, 11, 321, 643, 281, 586, 12539, 613, 914, 293, 3832, 25163, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08246155746844637, "compression_ratio": 1.7769516728624535, "no_speech_prob": 1.3419550668913871e-05}, {"id": 910, "seek": 458100, "start": 4598.0, "end": 4601.0, "text": " now suddenly the question is how do you do the inference?", "tokens": [50364, 583, 322, 1192, 295, 300, 11, 321, 767, 632, 281, 909, 257, 688, 295, 25474, 293, 49282, 11, 50564, 50564, 293, 300, 2729, 291, 257, 2020, 295, 264, 48705, 295, 264, 46183, 2710, 2144, 4583, 11, 50714, 50714, 570, 586, 321, 434, 1489, 1794, 264, 1412, 11, 300, 311, 869, 11, 457, 5800, 321, 643, 264, 6052, 293, 264, 12577, 13, 50964, 50964, 400, 586, 729, 366, 3847, 712, 13, 51064, 51064, 400, 550, 570, 321, 366, 37447, 439, 264, 3097, 5110, 11, 51214, 51214, 586, 5800, 264, 1168, 307, 577, 360, 291, 360, 264, 38253, 30, 51364, 51364, 2305, 281, 360, 264, 38253, 11, 321, 643, 281, 586, 12539, 613, 914, 293, 3832, 25163, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08246155746844637, "compression_ratio": 1.7769516728624535, "no_speech_prob": 1.3419550668913871e-05}, {"id": 911, "seek": 458100, "start": 4601.0, "end": 4607.0, "text": " Where to do the inference, we need to now estimate these mean and standard deviation", "tokens": [50364, 583, 322, 1192, 295, 300, 11, 321, 767, 632, 281, 909, 257, 688, 295, 25474, 293, 49282, 11, 50564, 50564, 293, 300, 2729, 291, 257, 2020, 295, 264, 48705, 295, 264, 46183, 2710, 2144, 4583, 11, 50714, 50714, 570, 586, 321, 434, 1489, 1794, 264, 1412, 11, 300, 311, 869, 11, 457, 5800, 321, 643, 264, 6052, 293, 264, 12577, 13, 50964, 50964, 400, 586, 729, 366, 3847, 712, 13, 51064, 51064, 400, 550, 570, 321, 366, 37447, 439, 264, 3097, 5110, 11, 51214, 51214, 586, 5800, 264, 1168, 307, 577, 360, 291, 360, 264, 38253, 30, 51364, 51364, 2305, 281, 360, 264, 38253, 11, 321, 643, 281, 586, 12539, 613, 914, 293, 3832, 25163, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08246155746844637, "compression_ratio": 1.7769516728624535, "no_speech_prob": 1.3419550668913871e-05}, {"id": 912, "seek": 460700, "start": 4607.0, "end": 4611.0, "text": " once over the entire training set and then use those at inference.", "tokens": [50364, 1564, 670, 264, 2302, 3097, 992, 293, 550, 764, 729, 412, 38253, 13, 50564, 50564, 583, 550, 572, 472, 5902, 281, 360, 3233, 732, 13, 50664, 50664, 407, 2602, 321, 4860, 1203, 666, 264, 46183, 2710, 2144, 4583, 1830, 3097, 50864, 50864, 293, 853, 281, 12539, 613, 294, 264, 2614, 9060, 370, 300, 1203, 307, 257, 857, 18587, 13, 51114, 51114, 400, 300, 2709, 505, 264, 46183, 2710, 2144, 4583, 13, 51314, 51314, 400, 382, 286, 2835, 11, 572, 472, 5902, 341, 4583, 13, 51464, 51464, 467, 7700, 257, 2603, 2372, 295, 15120, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08127493761023696, "compression_ratio": 1.752212389380531, "no_speech_prob": 1.22184501378797e-05}, {"id": 913, "seek": 460700, "start": 4611.0, "end": 4613.0, "text": " But then no one likes to do stage two.", "tokens": [50364, 1564, 670, 264, 2302, 3097, 992, 293, 550, 764, 729, 412, 38253, 13, 50564, 50564, 583, 550, 572, 472, 5902, 281, 360, 3233, 732, 13, 50664, 50664, 407, 2602, 321, 4860, 1203, 666, 264, 46183, 2710, 2144, 4583, 1830, 3097, 50864, 50864, 293, 853, 281, 12539, 613, 294, 264, 2614, 9060, 370, 300, 1203, 307, 257, 857, 18587, 13, 51114, 51114, 400, 300, 2709, 505, 264, 46183, 2710, 2144, 4583, 13, 51314, 51314, 400, 382, 286, 2835, 11, 572, 472, 5902, 341, 4583, 13, 51464, 51464, 467, 7700, 257, 2603, 2372, 295, 15120, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08127493761023696, "compression_ratio": 1.752212389380531, "no_speech_prob": 1.22184501378797e-05}, {"id": 914, "seek": 460700, "start": 4613.0, "end": 4617.0, "text": " So instead we fold everything into the bash normalization layer during training", "tokens": [50364, 1564, 670, 264, 2302, 3097, 992, 293, 550, 764, 729, 412, 38253, 13, 50564, 50564, 583, 550, 572, 472, 5902, 281, 360, 3233, 732, 13, 50664, 50664, 407, 2602, 321, 4860, 1203, 666, 264, 46183, 2710, 2144, 4583, 1830, 3097, 50864, 50864, 293, 853, 281, 12539, 613, 294, 264, 2614, 9060, 370, 300, 1203, 307, 257, 857, 18587, 13, 51114, 51114, 400, 300, 2709, 505, 264, 46183, 2710, 2144, 4583, 13, 51314, 51314, 400, 382, 286, 2835, 11, 572, 472, 5902, 341, 4583, 13, 51464, 51464, 467, 7700, 257, 2603, 2372, 295, 15120, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08127493761023696, "compression_ratio": 1.752212389380531, "no_speech_prob": 1.22184501378797e-05}, {"id": 915, "seek": 460700, "start": 4617.0, "end": 4622.0, "text": " and try to estimate these in the running manner so that everything is a bit simpler.", "tokens": [50364, 1564, 670, 264, 2302, 3097, 992, 293, 550, 764, 729, 412, 38253, 13, 50564, 50564, 583, 550, 572, 472, 5902, 281, 360, 3233, 732, 13, 50664, 50664, 407, 2602, 321, 4860, 1203, 666, 264, 46183, 2710, 2144, 4583, 1830, 3097, 50864, 50864, 293, 853, 281, 12539, 613, 294, 264, 2614, 9060, 370, 300, 1203, 307, 257, 857, 18587, 13, 51114, 51114, 400, 300, 2709, 505, 264, 46183, 2710, 2144, 4583, 13, 51314, 51314, 400, 382, 286, 2835, 11, 572, 472, 5902, 341, 4583, 13, 51464, 51464, 467, 7700, 257, 2603, 2372, 295, 15120, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08127493761023696, "compression_ratio": 1.752212389380531, "no_speech_prob": 1.22184501378797e-05}, {"id": 916, "seek": 460700, "start": 4622.0, "end": 4626.0, "text": " And that gives us the bash normalization layer.", "tokens": [50364, 1564, 670, 264, 2302, 3097, 992, 293, 550, 764, 729, 412, 38253, 13, 50564, 50564, 583, 550, 572, 472, 5902, 281, 360, 3233, 732, 13, 50664, 50664, 407, 2602, 321, 4860, 1203, 666, 264, 46183, 2710, 2144, 4583, 1830, 3097, 50864, 50864, 293, 853, 281, 12539, 613, 294, 264, 2614, 9060, 370, 300, 1203, 307, 257, 857, 18587, 13, 51114, 51114, 400, 300, 2709, 505, 264, 46183, 2710, 2144, 4583, 13, 51314, 51314, 400, 382, 286, 2835, 11, 572, 472, 5902, 341, 4583, 13, 51464, 51464, 467, 7700, 257, 2603, 2372, 295, 15120, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08127493761023696, "compression_ratio": 1.752212389380531, "no_speech_prob": 1.22184501378797e-05}, {"id": 917, "seek": 460700, "start": 4626.0, "end": 4629.0, "text": " And as I mentioned, no one likes this layer.", "tokens": [50364, 1564, 670, 264, 2302, 3097, 992, 293, 550, 764, 729, 412, 38253, 13, 50564, 50564, 583, 550, 572, 472, 5902, 281, 360, 3233, 732, 13, 50664, 50664, 407, 2602, 321, 4860, 1203, 666, 264, 46183, 2710, 2144, 4583, 1830, 3097, 50864, 50864, 293, 853, 281, 12539, 613, 294, 264, 2614, 9060, 370, 300, 1203, 307, 257, 857, 18587, 13, 51114, 51114, 400, 300, 2709, 505, 264, 46183, 2710, 2144, 4583, 13, 51314, 51314, 400, 382, 286, 2835, 11, 572, 472, 5902, 341, 4583, 13, 51464, 51464, 467, 7700, 257, 2603, 2372, 295, 15120, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08127493761023696, "compression_ratio": 1.752212389380531, "no_speech_prob": 1.22184501378797e-05}, {"id": 918, "seek": 460700, "start": 4629.0, "end": 4632.0, "text": " It causes a huge amount of bugs.", "tokens": [50364, 1564, 670, 264, 2302, 3097, 992, 293, 550, 764, 729, 412, 38253, 13, 50564, 50564, 583, 550, 572, 472, 5902, 281, 360, 3233, 732, 13, 50664, 50664, 407, 2602, 321, 4860, 1203, 666, 264, 46183, 2710, 2144, 4583, 1830, 3097, 50864, 50864, 293, 853, 281, 12539, 613, 294, 264, 2614, 9060, 370, 300, 1203, 307, 257, 857, 18587, 13, 51114, 51114, 400, 300, 2709, 505, 264, 46183, 2710, 2144, 4583, 13, 51314, 51314, 400, 382, 286, 2835, 11, 572, 472, 5902, 341, 4583, 13, 51464, 51464, 467, 7700, 257, 2603, 2372, 295, 15120, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08127493761023696, "compression_ratio": 1.752212389380531, "no_speech_prob": 1.22184501378797e-05}, {"id": 919, "seek": 463200, "start": 4632.0, "end": 4639.0, "text": " And intuitively it's because it is coupling examples in the forward pass of the neural net.", "tokens": [50364, 400, 46506, 309, 311, 570, 309, 307, 37447, 5110, 294, 264, 2128, 1320, 295, 264, 18161, 2533, 13, 50714, 50714, 400, 286, 600, 3347, 2059, 294, 264, 2671, 365, 341, 4583, 670, 293, 670, 797, 294, 452, 993, 11, 51014, 51014, 293, 286, 500, 380, 528, 291, 281, 9753, 264, 912, 13, 51164, 51164, 407, 1936, 853, 281, 5042, 309, 382, 709, 382, 1944, 13, 51364, 51364, 2188, 295, 264, 661, 20478, 281, 613, 7914, 366, 11, 337, 1365, 11, 1594, 2710, 2144, 420, 4583, 2710, 2144, 11, 51614, 51614, 293, 729, 362, 1813, 544, 2689, 294, 544, 5162, 2452, 2539, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0697882760245845, "compression_ratio": 1.6577946768060836, "no_speech_prob": 3.943734191125259e-05}, {"id": 920, "seek": 463200, "start": 4639.0, "end": 4645.0, "text": " And I've shot myself in the foot with this layer over and over again in my life,", "tokens": [50364, 400, 46506, 309, 311, 570, 309, 307, 37447, 5110, 294, 264, 2128, 1320, 295, 264, 18161, 2533, 13, 50714, 50714, 400, 286, 600, 3347, 2059, 294, 264, 2671, 365, 341, 4583, 670, 293, 670, 797, 294, 452, 993, 11, 51014, 51014, 293, 286, 500, 380, 528, 291, 281, 9753, 264, 912, 13, 51164, 51164, 407, 1936, 853, 281, 5042, 309, 382, 709, 382, 1944, 13, 51364, 51364, 2188, 295, 264, 661, 20478, 281, 613, 7914, 366, 11, 337, 1365, 11, 1594, 2710, 2144, 420, 4583, 2710, 2144, 11, 51614, 51614, 293, 729, 362, 1813, 544, 2689, 294, 544, 5162, 2452, 2539, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0697882760245845, "compression_ratio": 1.6577946768060836, "no_speech_prob": 3.943734191125259e-05}, {"id": 921, "seek": 463200, "start": 4645.0, "end": 4648.0, "text": " and I don't want you to suffer the same.", "tokens": [50364, 400, 46506, 309, 311, 570, 309, 307, 37447, 5110, 294, 264, 2128, 1320, 295, 264, 18161, 2533, 13, 50714, 50714, 400, 286, 600, 3347, 2059, 294, 264, 2671, 365, 341, 4583, 670, 293, 670, 797, 294, 452, 993, 11, 51014, 51014, 293, 286, 500, 380, 528, 291, 281, 9753, 264, 912, 13, 51164, 51164, 407, 1936, 853, 281, 5042, 309, 382, 709, 382, 1944, 13, 51364, 51364, 2188, 295, 264, 661, 20478, 281, 613, 7914, 366, 11, 337, 1365, 11, 1594, 2710, 2144, 420, 4583, 2710, 2144, 11, 51614, 51614, 293, 729, 362, 1813, 544, 2689, 294, 544, 5162, 2452, 2539, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0697882760245845, "compression_ratio": 1.6577946768060836, "no_speech_prob": 3.943734191125259e-05}, {"id": 922, "seek": 463200, "start": 4648.0, "end": 4652.0, "text": " So basically try to avoid it as much as possible.", "tokens": [50364, 400, 46506, 309, 311, 570, 309, 307, 37447, 5110, 294, 264, 2128, 1320, 295, 264, 18161, 2533, 13, 50714, 50714, 400, 286, 600, 3347, 2059, 294, 264, 2671, 365, 341, 4583, 670, 293, 670, 797, 294, 452, 993, 11, 51014, 51014, 293, 286, 500, 380, 528, 291, 281, 9753, 264, 912, 13, 51164, 51164, 407, 1936, 853, 281, 5042, 309, 382, 709, 382, 1944, 13, 51364, 51364, 2188, 295, 264, 661, 20478, 281, 613, 7914, 366, 11, 337, 1365, 11, 1594, 2710, 2144, 420, 4583, 2710, 2144, 11, 51614, 51614, 293, 729, 362, 1813, 544, 2689, 294, 544, 5162, 2452, 2539, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0697882760245845, "compression_ratio": 1.6577946768060836, "no_speech_prob": 3.943734191125259e-05}, {"id": 923, "seek": 463200, "start": 4652.0, "end": 4657.0, "text": " Some of the other alternatives to these layers are, for example, group normalization or layer normalization,", "tokens": [50364, 400, 46506, 309, 311, 570, 309, 307, 37447, 5110, 294, 264, 2128, 1320, 295, 264, 18161, 2533, 13, 50714, 50714, 400, 286, 600, 3347, 2059, 294, 264, 2671, 365, 341, 4583, 670, 293, 670, 797, 294, 452, 993, 11, 51014, 51014, 293, 286, 500, 380, 528, 291, 281, 9753, 264, 912, 13, 51164, 51164, 407, 1936, 853, 281, 5042, 309, 382, 709, 382, 1944, 13, 51364, 51364, 2188, 295, 264, 661, 20478, 281, 613, 7914, 366, 11, 337, 1365, 11, 1594, 2710, 2144, 420, 4583, 2710, 2144, 11, 51614, 51614, 293, 729, 362, 1813, 544, 2689, 294, 544, 5162, 2452, 2539, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0697882760245845, "compression_ratio": 1.6577946768060836, "no_speech_prob": 3.943734191125259e-05}, {"id": 924, "seek": 463200, "start": 4657.0, "end": 4661.0, "text": " and those have become more common in more recent deep learning,", "tokens": [50364, 400, 46506, 309, 311, 570, 309, 307, 37447, 5110, 294, 264, 2128, 1320, 295, 264, 18161, 2533, 13, 50714, 50714, 400, 286, 600, 3347, 2059, 294, 264, 2671, 365, 341, 4583, 670, 293, 670, 797, 294, 452, 993, 11, 51014, 51014, 293, 286, 500, 380, 528, 291, 281, 9753, 264, 912, 13, 51164, 51164, 407, 1936, 853, 281, 5042, 309, 382, 709, 382, 1944, 13, 51364, 51364, 2188, 295, 264, 661, 20478, 281, 613, 7914, 366, 11, 337, 1365, 11, 1594, 2710, 2144, 420, 4583, 2710, 2144, 11, 51614, 51614, 293, 729, 362, 1813, 544, 2689, 294, 544, 5162, 2452, 2539, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0697882760245845, "compression_ratio": 1.6577946768060836, "no_speech_prob": 3.943734191125259e-05}, {"id": 925, "seek": 466100, "start": 4661.0, "end": 4663.0, "text": " but we haven't covered those yet.", "tokens": [50364, 457, 321, 2378, 380, 5343, 729, 1939, 13, 50464, 50464, 583, 2138, 46183, 2710, 2144, 390, 588, 22215, 412, 264, 565, 562, 309, 1361, 484, 294, 9810, 7546, 50764, 50764, 570, 309, 390, 733, 295, 264, 700, 565, 300, 291, 727, 3847, 49927, 709, 7731, 18161, 36170, 13, 51064, 51064, 400, 17879, 264, 1778, 337, 300, 307, 570, 341, 4583, 390, 588, 4942, 51264, 51264, 412, 14905, 264, 12523, 295, 264, 2430, 763, 294, 264, 18161, 2533, 13, 51464, 51464, 407, 300, 311, 264, 1657, 370, 1400, 11, 293, 300, 311, 439, 286, 1415, 281, 2060, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.051288921054046935, "compression_ratio": 1.6539923954372624, "no_speech_prob": 2.4680563001311384e-05}, {"id": 926, "seek": 466100, "start": 4663.0, "end": 4669.0, "text": " But definitely bash normalization was very influential at the time when it came out in roughly 2015", "tokens": [50364, 457, 321, 2378, 380, 5343, 729, 1939, 13, 50464, 50464, 583, 2138, 46183, 2710, 2144, 390, 588, 22215, 412, 264, 565, 562, 309, 1361, 484, 294, 9810, 7546, 50764, 50764, 570, 309, 390, 733, 295, 264, 700, 565, 300, 291, 727, 3847, 49927, 709, 7731, 18161, 36170, 13, 51064, 51064, 400, 17879, 264, 1778, 337, 300, 307, 570, 341, 4583, 390, 588, 4942, 51264, 51264, 412, 14905, 264, 12523, 295, 264, 2430, 763, 294, 264, 18161, 2533, 13, 51464, 51464, 407, 300, 311, 264, 1657, 370, 1400, 11, 293, 300, 311, 439, 286, 1415, 281, 2060, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.051288921054046935, "compression_ratio": 1.6539923954372624, "no_speech_prob": 2.4680563001311384e-05}, {"id": 927, "seek": 466100, "start": 4669.0, "end": 4675.0, "text": " because it was kind of the first time that you could train reliably much deeper neural nets.", "tokens": [50364, 457, 321, 2378, 380, 5343, 729, 1939, 13, 50464, 50464, 583, 2138, 46183, 2710, 2144, 390, 588, 22215, 412, 264, 565, 562, 309, 1361, 484, 294, 9810, 7546, 50764, 50764, 570, 309, 390, 733, 295, 264, 700, 565, 300, 291, 727, 3847, 49927, 709, 7731, 18161, 36170, 13, 51064, 51064, 400, 17879, 264, 1778, 337, 300, 307, 570, 341, 4583, 390, 588, 4942, 51264, 51264, 412, 14905, 264, 12523, 295, 264, 2430, 763, 294, 264, 18161, 2533, 13, 51464, 51464, 407, 300, 311, 264, 1657, 370, 1400, 11, 293, 300, 311, 439, 286, 1415, 281, 2060, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.051288921054046935, "compression_ratio": 1.6539923954372624, "no_speech_prob": 2.4680563001311384e-05}, {"id": 928, "seek": 466100, "start": 4675.0, "end": 4679.0, "text": " And fundamentally the reason for that is because this layer was very effective", "tokens": [50364, 457, 321, 2378, 380, 5343, 729, 1939, 13, 50464, 50464, 583, 2138, 46183, 2710, 2144, 390, 588, 22215, 412, 264, 565, 562, 309, 1361, 484, 294, 9810, 7546, 50764, 50764, 570, 309, 390, 733, 295, 264, 700, 565, 300, 291, 727, 3847, 49927, 709, 7731, 18161, 36170, 13, 51064, 51064, 400, 17879, 264, 1778, 337, 300, 307, 570, 341, 4583, 390, 588, 4942, 51264, 51264, 412, 14905, 264, 12523, 295, 264, 2430, 763, 294, 264, 18161, 2533, 13, 51464, 51464, 407, 300, 311, 264, 1657, 370, 1400, 11, 293, 300, 311, 439, 286, 1415, 281, 2060, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.051288921054046935, "compression_ratio": 1.6539923954372624, "no_speech_prob": 2.4680563001311384e-05}, {"id": 929, "seek": 466100, "start": 4679.0, "end": 4683.0, "text": " at controlling the statistics of the activations in the neural net.", "tokens": [50364, 457, 321, 2378, 380, 5343, 729, 1939, 13, 50464, 50464, 583, 2138, 46183, 2710, 2144, 390, 588, 22215, 412, 264, 565, 562, 309, 1361, 484, 294, 9810, 7546, 50764, 50764, 570, 309, 390, 733, 295, 264, 700, 565, 300, 291, 727, 3847, 49927, 709, 7731, 18161, 36170, 13, 51064, 51064, 400, 17879, 264, 1778, 337, 300, 307, 570, 341, 4583, 390, 588, 4942, 51264, 51264, 412, 14905, 264, 12523, 295, 264, 2430, 763, 294, 264, 18161, 2533, 13, 51464, 51464, 407, 300, 311, 264, 1657, 370, 1400, 11, 293, 300, 311, 439, 286, 1415, 281, 2060, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.051288921054046935, "compression_ratio": 1.6539923954372624, "no_speech_prob": 2.4680563001311384e-05}, {"id": 930, "seek": 466100, "start": 4683.0, "end": 4688.0, "text": " So that's the story so far, and that's all I wanted to cover.", "tokens": [50364, 457, 321, 2378, 380, 5343, 729, 1939, 13, 50464, 50464, 583, 2138, 46183, 2710, 2144, 390, 588, 22215, 412, 264, 565, 562, 309, 1361, 484, 294, 9810, 7546, 50764, 50764, 570, 309, 390, 733, 295, 264, 700, 565, 300, 291, 727, 3847, 49927, 709, 7731, 18161, 36170, 13, 51064, 51064, 400, 17879, 264, 1778, 337, 300, 307, 570, 341, 4583, 390, 588, 4942, 51264, 51264, 412, 14905, 264, 12523, 295, 264, 2430, 763, 294, 264, 18161, 2533, 13, 51464, 51464, 407, 300, 311, 264, 1657, 370, 1400, 11, 293, 300, 311, 439, 286, 1415, 281, 2060, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.051288921054046935, "compression_ratio": 1.6539923954372624, "no_speech_prob": 2.4680563001311384e-05}, {"id": 931, "seek": 468800, "start": 4688.0, "end": 4692.0, "text": " And in the future lectures, hopefully we can start going into recurrent neural nets.", "tokens": [50364, 400, 294, 264, 2027, 16564, 11, 4696, 321, 393, 722, 516, 666, 18680, 1753, 18161, 36170, 13, 50564, 50564, 400, 18680, 1753, 18161, 36170, 11, 382, 321, 603, 536, 11, 366, 445, 588, 11, 588, 2452, 9590, 50814, 50814, 570, 291, 517, 3970, 264, 6367, 562, 291, 767, 19719, 613, 18161, 36170, 13, 51064, 51064, 400, 300, 311, 689, 257, 688, 295, 341, 5215, 926, 264, 24433, 12523, 51364, 51364, 293, 439, 613, 2710, 2144, 7914, 486, 1813, 588, 11, 588, 1021, 337, 665, 3389, 13, 51614, 51614, 407, 321, 603, 536, 300, 958, 565, 13, 4621, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07328123672335755, "compression_ratio": 1.7338709677419355, "no_speech_prob": 2.7964535547653213e-05}, {"id": 932, "seek": 468800, "start": 4692.0, "end": 4697.0, "text": " And recurrent neural nets, as we'll see, are just very, very deep networks", "tokens": [50364, 400, 294, 264, 2027, 16564, 11, 4696, 321, 393, 722, 516, 666, 18680, 1753, 18161, 36170, 13, 50564, 50564, 400, 18680, 1753, 18161, 36170, 11, 382, 321, 603, 536, 11, 366, 445, 588, 11, 588, 2452, 9590, 50814, 50814, 570, 291, 517, 3970, 264, 6367, 562, 291, 767, 19719, 613, 18161, 36170, 13, 51064, 51064, 400, 300, 311, 689, 257, 688, 295, 341, 5215, 926, 264, 24433, 12523, 51364, 51364, 293, 439, 613, 2710, 2144, 7914, 486, 1813, 588, 11, 588, 1021, 337, 665, 3389, 13, 51614, 51614, 407, 321, 603, 536, 300, 958, 565, 13, 4621, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07328123672335755, "compression_ratio": 1.7338709677419355, "no_speech_prob": 2.7964535547653213e-05}, {"id": 933, "seek": 468800, "start": 4697.0, "end": 4702.0, "text": " because you unroll the loop when you actually optimize these neural nets.", "tokens": [50364, 400, 294, 264, 2027, 16564, 11, 4696, 321, 393, 722, 516, 666, 18680, 1753, 18161, 36170, 13, 50564, 50564, 400, 18680, 1753, 18161, 36170, 11, 382, 321, 603, 536, 11, 366, 445, 588, 11, 588, 2452, 9590, 50814, 50814, 570, 291, 517, 3970, 264, 6367, 562, 291, 767, 19719, 613, 18161, 36170, 13, 51064, 51064, 400, 300, 311, 689, 257, 688, 295, 341, 5215, 926, 264, 24433, 12523, 51364, 51364, 293, 439, 613, 2710, 2144, 7914, 486, 1813, 588, 11, 588, 1021, 337, 665, 3389, 13, 51614, 51614, 407, 321, 603, 536, 300, 958, 565, 13, 4621, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07328123672335755, "compression_ratio": 1.7338709677419355, "no_speech_prob": 2.7964535547653213e-05}, {"id": 934, "seek": 468800, "start": 4702.0, "end": 4708.0, "text": " And that's where a lot of this analysis around the activation statistics", "tokens": [50364, 400, 294, 264, 2027, 16564, 11, 4696, 321, 393, 722, 516, 666, 18680, 1753, 18161, 36170, 13, 50564, 50564, 400, 18680, 1753, 18161, 36170, 11, 382, 321, 603, 536, 11, 366, 445, 588, 11, 588, 2452, 9590, 50814, 50814, 570, 291, 517, 3970, 264, 6367, 562, 291, 767, 19719, 613, 18161, 36170, 13, 51064, 51064, 400, 300, 311, 689, 257, 688, 295, 341, 5215, 926, 264, 24433, 12523, 51364, 51364, 293, 439, 613, 2710, 2144, 7914, 486, 1813, 588, 11, 588, 1021, 337, 665, 3389, 13, 51614, 51614, 407, 321, 603, 536, 300, 958, 565, 13, 4621, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07328123672335755, "compression_ratio": 1.7338709677419355, "no_speech_prob": 2.7964535547653213e-05}, {"id": 935, "seek": 468800, "start": 4708.0, "end": 4713.0, "text": " and all these normalization layers will become very, very important for good performance.", "tokens": [50364, 400, 294, 264, 2027, 16564, 11, 4696, 321, 393, 722, 516, 666, 18680, 1753, 18161, 36170, 13, 50564, 50564, 400, 18680, 1753, 18161, 36170, 11, 382, 321, 603, 536, 11, 366, 445, 588, 11, 588, 2452, 9590, 50814, 50814, 570, 291, 517, 3970, 264, 6367, 562, 291, 767, 19719, 613, 18161, 36170, 13, 51064, 51064, 400, 300, 311, 689, 257, 688, 295, 341, 5215, 926, 264, 24433, 12523, 51364, 51364, 293, 439, 613, 2710, 2144, 7914, 486, 1813, 588, 11, 588, 1021, 337, 665, 3389, 13, 51614, 51614, 407, 321, 603, 536, 300, 958, 565, 13, 4621, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07328123672335755, "compression_ratio": 1.7338709677419355, "no_speech_prob": 2.7964535547653213e-05}, {"id": 936, "seek": 468800, "start": 4713.0, "end": 4716.0, "text": " So we'll see that next time. Bye.", "tokens": [50364, 400, 294, 264, 2027, 16564, 11, 4696, 321, 393, 722, 516, 666, 18680, 1753, 18161, 36170, 13, 50564, 50564, 400, 18680, 1753, 18161, 36170, 11, 382, 321, 603, 536, 11, 366, 445, 588, 11, 588, 2452, 9590, 50814, 50814, 570, 291, 517, 3970, 264, 6367, 562, 291, 767, 19719, 613, 18161, 36170, 13, 51064, 51064, 400, 300, 311, 689, 257, 688, 295, 341, 5215, 926, 264, 24433, 12523, 51364, 51364, 293, 439, 613, 2710, 2144, 7914, 486, 1813, 588, 11, 588, 1021, 337, 665, 3389, 13, 51614, 51614, 407, 321, 603, 536, 300, 958, 565, 13, 4621, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07328123672335755, "compression_ratio": 1.7338709677419355, "no_speech_prob": 2.7964535547653213e-05}, {"id": 937, "seek": 471600, "start": 4716.0, "end": 4719.0, "text": " And I would like us to do one more summary here as a bonus.", "tokens": [50364, 400, 286, 576, 411, 505, 281, 360, 472, 544, 12691, 510, 382, 257, 10882, 13, 50514, 50514, 400, 286, 519, 309, 311, 4420, 382, 281, 362, 472, 544, 12691, 295, 1203, 286, 600, 8212, 294, 341, 7991, 13, 50764, 50764, 583, 611, 286, 576, 411, 505, 281, 722, 538, 27822, 5489, 527, 3089, 257, 707, 857, 13, 50914, 50914, 407, 309, 1542, 709, 544, 411, 437, 291, 576, 8593, 294, 9953, 51, 284, 339, 13, 51064, 51064, 407, 291, 603, 536, 300, 286, 486, 3877, 527, 3089, 666, 613, 16679, 11, 51264, 51264, 411, 257, 8213, 10088, 293, 257, 15245, 1254, 10088, 13, 51514, 51514, 400, 286, 478, 3372, 264, 3089, 1854, 613, 16679, 370, 300, 321, 393, 7690, 18161, 9590, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08569412994384766, "compression_ratio": 1.7919708029197081, "no_speech_prob": 0.00425707595422864}, {"id": 938, "seek": 471600, "start": 4719.0, "end": 4724.0, "text": " And I think it's useful as to have one more summary of everything I've presented in this lecture.", "tokens": [50364, 400, 286, 576, 411, 505, 281, 360, 472, 544, 12691, 510, 382, 257, 10882, 13, 50514, 50514, 400, 286, 519, 309, 311, 4420, 382, 281, 362, 472, 544, 12691, 295, 1203, 286, 600, 8212, 294, 341, 7991, 13, 50764, 50764, 583, 611, 286, 576, 411, 505, 281, 722, 538, 27822, 5489, 527, 3089, 257, 707, 857, 13, 50914, 50914, 407, 309, 1542, 709, 544, 411, 437, 291, 576, 8593, 294, 9953, 51, 284, 339, 13, 51064, 51064, 407, 291, 603, 536, 300, 286, 486, 3877, 527, 3089, 666, 613, 16679, 11, 51264, 51264, 411, 257, 8213, 10088, 293, 257, 15245, 1254, 10088, 13, 51514, 51514, 400, 286, 478, 3372, 264, 3089, 1854, 613, 16679, 370, 300, 321, 393, 7690, 18161, 9590, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08569412994384766, "compression_ratio": 1.7919708029197081, "no_speech_prob": 0.00425707595422864}, {"id": 939, "seek": 471600, "start": 4724.0, "end": 4727.0, "text": " But also I would like us to start by torchifying our code a little bit.", "tokens": [50364, 400, 286, 576, 411, 505, 281, 360, 472, 544, 12691, 510, 382, 257, 10882, 13, 50514, 50514, 400, 286, 519, 309, 311, 4420, 382, 281, 362, 472, 544, 12691, 295, 1203, 286, 600, 8212, 294, 341, 7991, 13, 50764, 50764, 583, 611, 286, 576, 411, 505, 281, 722, 538, 27822, 5489, 527, 3089, 257, 707, 857, 13, 50914, 50914, 407, 309, 1542, 709, 544, 411, 437, 291, 576, 8593, 294, 9953, 51, 284, 339, 13, 51064, 51064, 407, 291, 603, 536, 300, 286, 486, 3877, 527, 3089, 666, 613, 16679, 11, 51264, 51264, 411, 257, 8213, 10088, 293, 257, 15245, 1254, 10088, 13, 51514, 51514, 400, 286, 478, 3372, 264, 3089, 1854, 613, 16679, 370, 300, 321, 393, 7690, 18161, 9590, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08569412994384766, "compression_ratio": 1.7919708029197081, "no_speech_prob": 0.00425707595422864}, {"id": 940, "seek": 471600, "start": 4727.0, "end": 4730.0, "text": " So it looks much more like what you would encounter in PyTorch.", "tokens": [50364, 400, 286, 576, 411, 505, 281, 360, 472, 544, 12691, 510, 382, 257, 10882, 13, 50514, 50514, 400, 286, 519, 309, 311, 4420, 382, 281, 362, 472, 544, 12691, 295, 1203, 286, 600, 8212, 294, 341, 7991, 13, 50764, 50764, 583, 611, 286, 576, 411, 505, 281, 722, 538, 27822, 5489, 527, 3089, 257, 707, 857, 13, 50914, 50914, 407, 309, 1542, 709, 544, 411, 437, 291, 576, 8593, 294, 9953, 51, 284, 339, 13, 51064, 51064, 407, 291, 603, 536, 300, 286, 486, 3877, 527, 3089, 666, 613, 16679, 11, 51264, 51264, 411, 257, 8213, 10088, 293, 257, 15245, 1254, 10088, 13, 51514, 51514, 400, 286, 478, 3372, 264, 3089, 1854, 613, 16679, 370, 300, 321, 393, 7690, 18161, 9590, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08569412994384766, "compression_ratio": 1.7919708029197081, "no_speech_prob": 0.00425707595422864}, {"id": 941, "seek": 471600, "start": 4730.0, "end": 4734.0, "text": " So you'll see that I will structure our code into these modules,", "tokens": [50364, 400, 286, 576, 411, 505, 281, 360, 472, 544, 12691, 510, 382, 257, 10882, 13, 50514, 50514, 400, 286, 519, 309, 311, 4420, 382, 281, 362, 472, 544, 12691, 295, 1203, 286, 600, 8212, 294, 341, 7991, 13, 50764, 50764, 583, 611, 286, 576, 411, 505, 281, 722, 538, 27822, 5489, 527, 3089, 257, 707, 857, 13, 50914, 50914, 407, 309, 1542, 709, 544, 411, 437, 291, 576, 8593, 294, 9953, 51, 284, 339, 13, 51064, 51064, 407, 291, 603, 536, 300, 286, 486, 3877, 527, 3089, 666, 613, 16679, 11, 51264, 51264, 411, 257, 8213, 10088, 293, 257, 15245, 1254, 10088, 13, 51514, 51514, 400, 286, 478, 3372, 264, 3089, 1854, 613, 16679, 370, 300, 321, 393, 7690, 18161, 9590, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08569412994384766, "compression_ratio": 1.7919708029197081, "no_speech_prob": 0.00425707595422864}, {"id": 942, "seek": 471600, "start": 4734.0, "end": 4739.0, "text": " like a linear module and a batch form module.", "tokens": [50364, 400, 286, 576, 411, 505, 281, 360, 472, 544, 12691, 510, 382, 257, 10882, 13, 50514, 50514, 400, 286, 519, 309, 311, 4420, 382, 281, 362, 472, 544, 12691, 295, 1203, 286, 600, 8212, 294, 341, 7991, 13, 50764, 50764, 583, 611, 286, 576, 411, 505, 281, 722, 538, 27822, 5489, 527, 3089, 257, 707, 857, 13, 50914, 50914, 407, 309, 1542, 709, 544, 411, 437, 291, 576, 8593, 294, 9953, 51, 284, 339, 13, 51064, 51064, 407, 291, 603, 536, 300, 286, 486, 3877, 527, 3089, 666, 613, 16679, 11, 51264, 51264, 411, 257, 8213, 10088, 293, 257, 15245, 1254, 10088, 13, 51514, 51514, 400, 286, 478, 3372, 264, 3089, 1854, 613, 16679, 370, 300, 321, 393, 7690, 18161, 9590, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08569412994384766, "compression_ratio": 1.7919708029197081, "no_speech_prob": 0.00425707595422864}, {"id": 943, "seek": 471600, "start": 4739.0, "end": 4743.0, "text": " And I'm putting the code inside these modules so that we can construct neural networks", "tokens": [50364, 400, 286, 576, 411, 505, 281, 360, 472, 544, 12691, 510, 382, 257, 10882, 13, 50514, 50514, 400, 286, 519, 309, 311, 4420, 382, 281, 362, 472, 544, 12691, 295, 1203, 286, 600, 8212, 294, 341, 7991, 13, 50764, 50764, 583, 611, 286, 576, 411, 505, 281, 722, 538, 27822, 5489, 527, 3089, 257, 707, 857, 13, 50914, 50914, 407, 309, 1542, 709, 544, 411, 437, 291, 576, 8593, 294, 9953, 51, 284, 339, 13, 51064, 51064, 407, 291, 603, 536, 300, 286, 486, 3877, 527, 3089, 666, 613, 16679, 11, 51264, 51264, 411, 257, 8213, 10088, 293, 257, 15245, 1254, 10088, 13, 51514, 51514, 400, 286, 478, 3372, 264, 3089, 1854, 613, 16679, 370, 300, 321, 393, 7690, 18161, 9590, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08569412994384766, "compression_ratio": 1.7919708029197081, "no_speech_prob": 0.00425707595422864}, {"id": 944, "seek": 474300, "start": 4743.0, "end": 4747.0, "text": " very much like we would construct them in PyTorch. And I will go through this in detail.", "tokens": [50364, 588, 709, 411, 321, 576, 7690, 552, 294, 9953, 51, 284, 339, 13, 400, 286, 486, 352, 807, 341, 294, 2607, 13, 50564, 50564, 407, 321, 603, 1884, 527, 18161, 2533, 13, 50664, 50664, 1396, 321, 486, 360, 264, 19618, 6367, 382, 321, 630, 949, 13, 50864, 50864, 400, 550, 264, 472, 544, 551, 300, 286, 528, 281, 360, 510, 307, 286, 528, 281, 574, 412, 264, 24433, 12523, 11, 51014, 51014, 1293, 294, 264, 2128, 1320, 293, 294, 264, 23897, 1320, 13, 51164, 51164, 400, 550, 510, 321, 362, 264, 13344, 293, 21179, 445, 411, 949, 13, 51364, 51364, 407, 718, 385, 41458, 439, 264, 636, 493, 510, 293, 352, 257, 707, 857, 14009, 13, 51564, 51564, 407, 510, 286, 669, 4084, 257, 8213, 4583, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04625179203412005, "compression_ratio": 1.7377622377622377, "no_speech_prob": 7.721155998297036e-05}, {"id": 945, "seek": 474300, "start": 4747.0, "end": 4749.0, "text": " So we'll create our neural net.", "tokens": [50364, 588, 709, 411, 321, 576, 7690, 552, 294, 9953, 51, 284, 339, 13, 400, 286, 486, 352, 807, 341, 294, 2607, 13, 50564, 50564, 407, 321, 603, 1884, 527, 18161, 2533, 13, 50664, 50664, 1396, 321, 486, 360, 264, 19618, 6367, 382, 321, 630, 949, 13, 50864, 50864, 400, 550, 264, 472, 544, 551, 300, 286, 528, 281, 360, 510, 307, 286, 528, 281, 574, 412, 264, 24433, 12523, 11, 51014, 51014, 1293, 294, 264, 2128, 1320, 293, 294, 264, 23897, 1320, 13, 51164, 51164, 400, 550, 510, 321, 362, 264, 13344, 293, 21179, 445, 411, 949, 13, 51364, 51364, 407, 718, 385, 41458, 439, 264, 636, 493, 510, 293, 352, 257, 707, 857, 14009, 13, 51564, 51564, 407, 510, 286, 669, 4084, 257, 8213, 4583, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04625179203412005, "compression_ratio": 1.7377622377622377, "no_speech_prob": 7.721155998297036e-05}, {"id": 946, "seek": 474300, "start": 4749.0, "end": 4753.0, "text": " Then we will do the optimization loop as we did before.", "tokens": [50364, 588, 709, 411, 321, 576, 7690, 552, 294, 9953, 51, 284, 339, 13, 400, 286, 486, 352, 807, 341, 294, 2607, 13, 50564, 50564, 407, 321, 603, 1884, 527, 18161, 2533, 13, 50664, 50664, 1396, 321, 486, 360, 264, 19618, 6367, 382, 321, 630, 949, 13, 50864, 50864, 400, 550, 264, 472, 544, 551, 300, 286, 528, 281, 360, 510, 307, 286, 528, 281, 574, 412, 264, 24433, 12523, 11, 51014, 51014, 1293, 294, 264, 2128, 1320, 293, 294, 264, 23897, 1320, 13, 51164, 51164, 400, 550, 510, 321, 362, 264, 13344, 293, 21179, 445, 411, 949, 13, 51364, 51364, 407, 718, 385, 41458, 439, 264, 636, 493, 510, 293, 352, 257, 707, 857, 14009, 13, 51564, 51564, 407, 510, 286, 669, 4084, 257, 8213, 4583, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04625179203412005, "compression_ratio": 1.7377622377622377, "no_speech_prob": 7.721155998297036e-05}, {"id": 947, "seek": 474300, "start": 4753.0, "end": 4756.0, "text": " And then the one more thing that I want to do here is I want to look at the activation statistics,", "tokens": [50364, 588, 709, 411, 321, 576, 7690, 552, 294, 9953, 51, 284, 339, 13, 400, 286, 486, 352, 807, 341, 294, 2607, 13, 50564, 50564, 407, 321, 603, 1884, 527, 18161, 2533, 13, 50664, 50664, 1396, 321, 486, 360, 264, 19618, 6367, 382, 321, 630, 949, 13, 50864, 50864, 400, 550, 264, 472, 544, 551, 300, 286, 528, 281, 360, 510, 307, 286, 528, 281, 574, 412, 264, 24433, 12523, 11, 51014, 51014, 1293, 294, 264, 2128, 1320, 293, 294, 264, 23897, 1320, 13, 51164, 51164, 400, 550, 510, 321, 362, 264, 13344, 293, 21179, 445, 411, 949, 13, 51364, 51364, 407, 718, 385, 41458, 439, 264, 636, 493, 510, 293, 352, 257, 707, 857, 14009, 13, 51564, 51564, 407, 510, 286, 669, 4084, 257, 8213, 4583, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04625179203412005, "compression_ratio": 1.7377622377622377, "no_speech_prob": 7.721155998297036e-05}, {"id": 948, "seek": 474300, "start": 4756.0, "end": 4759.0, "text": " both in the forward pass and in the backward pass.", "tokens": [50364, 588, 709, 411, 321, 576, 7690, 552, 294, 9953, 51, 284, 339, 13, 400, 286, 486, 352, 807, 341, 294, 2607, 13, 50564, 50564, 407, 321, 603, 1884, 527, 18161, 2533, 13, 50664, 50664, 1396, 321, 486, 360, 264, 19618, 6367, 382, 321, 630, 949, 13, 50864, 50864, 400, 550, 264, 472, 544, 551, 300, 286, 528, 281, 360, 510, 307, 286, 528, 281, 574, 412, 264, 24433, 12523, 11, 51014, 51014, 1293, 294, 264, 2128, 1320, 293, 294, 264, 23897, 1320, 13, 51164, 51164, 400, 550, 510, 321, 362, 264, 13344, 293, 21179, 445, 411, 949, 13, 51364, 51364, 407, 718, 385, 41458, 439, 264, 636, 493, 510, 293, 352, 257, 707, 857, 14009, 13, 51564, 51564, 407, 510, 286, 669, 4084, 257, 8213, 4583, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04625179203412005, "compression_ratio": 1.7377622377622377, "no_speech_prob": 7.721155998297036e-05}, {"id": 949, "seek": 474300, "start": 4759.0, "end": 4763.0, "text": " And then here we have the evaluation and sampling just like before.", "tokens": [50364, 588, 709, 411, 321, 576, 7690, 552, 294, 9953, 51, 284, 339, 13, 400, 286, 486, 352, 807, 341, 294, 2607, 13, 50564, 50564, 407, 321, 603, 1884, 527, 18161, 2533, 13, 50664, 50664, 1396, 321, 486, 360, 264, 19618, 6367, 382, 321, 630, 949, 13, 50864, 50864, 400, 550, 264, 472, 544, 551, 300, 286, 528, 281, 360, 510, 307, 286, 528, 281, 574, 412, 264, 24433, 12523, 11, 51014, 51014, 1293, 294, 264, 2128, 1320, 293, 294, 264, 23897, 1320, 13, 51164, 51164, 400, 550, 510, 321, 362, 264, 13344, 293, 21179, 445, 411, 949, 13, 51364, 51364, 407, 718, 385, 41458, 439, 264, 636, 493, 510, 293, 352, 257, 707, 857, 14009, 13, 51564, 51564, 407, 510, 286, 669, 4084, 257, 8213, 4583, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04625179203412005, "compression_ratio": 1.7377622377622377, "no_speech_prob": 7.721155998297036e-05}, {"id": 950, "seek": 474300, "start": 4763.0, "end": 4767.0, "text": " So let me rewind all the way up here and go a little bit slower.", "tokens": [50364, 588, 709, 411, 321, 576, 7690, 552, 294, 9953, 51, 284, 339, 13, 400, 286, 486, 352, 807, 341, 294, 2607, 13, 50564, 50564, 407, 321, 603, 1884, 527, 18161, 2533, 13, 50664, 50664, 1396, 321, 486, 360, 264, 19618, 6367, 382, 321, 630, 949, 13, 50864, 50864, 400, 550, 264, 472, 544, 551, 300, 286, 528, 281, 360, 510, 307, 286, 528, 281, 574, 412, 264, 24433, 12523, 11, 51014, 51014, 1293, 294, 264, 2128, 1320, 293, 294, 264, 23897, 1320, 13, 51164, 51164, 400, 550, 510, 321, 362, 264, 13344, 293, 21179, 445, 411, 949, 13, 51364, 51364, 407, 718, 385, 41458, 439, 264, 636, 493, 510, 293, 352, 257, 707, 857, 14009, 13, 51564, 51564, 407, 510, 286, 669, 4084, 257, 8213, 4583, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04625179203412005, "compression_ratio": 1.7377622377622377, "no_speech_prob": 7.721155998297036e-05}, {"id": 951, "seek": 474300, "start": 4767.0, "end": 4769.0, "text": " So here I am creating a linear layer.", "tokens": [50364, 588, 709, 411, 321, 576, 7690, 552, 294, 9953, 51, 284, 339, 13, 400, 286, 486, 352, 807, 341, 294, 2607, 13, 50564, 50564, 407, 321, 603, 1884, 527, 18161, 2533, 13, 50664, 50664, 1396, 321, 486, 360, 264, 19618, 6367, 382, 321, 630, 949, 13, 50864, 50864, 400, 550, 264, 472, 544, 551, 300, 286, 528, 281, 360, 510, 307, 286, 528, 281, 574, 412, 264, 24433, 12523, 11, 51014, 51014, 1293, 294, 264, 2128, 1320, 293, 294, 264, 23897, 1320, 13, 51164, 51164, 400, 550, 510, 321, 362, 264, 13344, 293, 21179, 445, 411, 949, 13, 51364, 51364, 407, 718, 385, 41458, 439, 264, 636, 493, 510, 293, 352, 257, 707, 857, 14009, 13, 51564, 51564, 407, 510, 286, 669, 4084, 257, 8213, 4583, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.04625179203412005, "compression_ratio": 1.7377622377622377, "no_speech_prob": 7.721155998297036e-05}, {"id": 952, "seek": 476900, "start": 4769.0, "end": 4773.0, "text": " You'll notice that torch.nn has lots of different types of layers.", "tokens": [50364, 509, 603, 3449, 300, 27822, 13, 26384, 575, 3195, 295, 819, 3467, 295, 7914, 13, 50564, 50564, 400, 472, 295, 729, 7914, 307, 264, 8213, 4583, 13, 50664, 50664, 7160, 339, 13, 26384, 13, 28263, 2516, 257, 1230, 295, 4846, 4122, 11, 5598, 4122, 11, 50814, 50814, 1968, 420, 406, 321, 820, 362, 12577, 11, 50914, 50914, 293, 550, 264, 4302, 300, 321, 528, 281, 1081, 341, 4583, 322, 11, 293, 264, 1412, 2010, 13, 51114, 51114, 407, 286, 486, 3406, 270, 613, 732, 11, 457, 5911, 321, 362, 264, 1900, 912, 551, 13, 51314, 51314, 492, 362, 264, 3429, 294, 11, 597, 307, 264, 1230, 295, 15743, 11, 3429, 484, 11, 264, 1230, 295, 23930, 11, 51564, 51564, 293, 1968, 420, 406, 321, 528, 281, 764, 257, 12577, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08134345865961332, "compression_ratio": 1.9019607843137254, "no_speech_prob": 1.428446194040589e-05}, {"id": 953, "seek": 476900, "start": 4773.0, "end": 4775.0, "text": " And one of those layers is the linear layer.", "tokens": [50364, 509, 603, 3449, 300, 27822, 13, 26384, 575, 3195, 295, 819, 3467, 295, 7914, 13, 50564, 50564, 400, 472, 295, 729, 7914, 307, 264, 8213, 4583, 13, 50664, 50664, 7160, 339, 13, 26384, 13, 28263, 2516, 257, 1230, 295, 4846, 4122, 11, 5598, 4122, 11, 50814, 50814, 1968, 420, 406, 321, 820, 362, 12577, 11, 50914, 50914, 293, 550, 264, 4302, 300, 321, 528, 281, 1081, 341, 4583, 322, 11, 293, 264, 1412, 2010, 13, 51114, 51114, 407, 286, 486, 3406, 270, 613, 732, 11, 457, 5911, 321, 362, 264, 1900, 912, 551, 13, 51314, 51314, 492, 362, 264, 3429, 294, 11, 597, 307, 264, 1230, 295, 15743, 11, 3429, 484, 11, 264, 1230, 295, 23930, 11, 51564, 51564, 293, 1968, 420, 406, 321, 528, 281, 764, 257, 12577, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08134345865961332, "compression_ratio": 1.9019607843137254, "no_speech_prob": 1.428446194040589e-05}, {"id": 954, "seek": 476900, "start": 4775.0, "end": 4778.0, "text": " Torch.nn.linear takes a number of input features, output features,", "tokens": [50364, 509, 603, 3449, 300, 27822, 13, 26384, 575, 3195, 295, 819, 3467, 295, 7914, 13, 50564, 50564, 400, 472, 295, 729, 7914, 307, 264, 8213, 4583, 13, 50664, 50664, 7160, 339, 13, 26384, 13, 28263, 2516, 257, 1230, 295, 4846, 4122, 11, 5598, 4122, 11, 50814, 50814, 1968, 420, 406, 321, 820, 362, 12577, 11, 50914, 50914, 293, 550, 264, 4302, 300, 321, 528, 281, 1081, 341, 4583, 322, 11, 293, 264, 1412, 2010, 13, 51114, 51114, 407, 286, 486, 3406, 270, 613, 732, 11, 457, 5911, 321, 362, 264, 1900, 912, 551, 13, 51314, 51314, 492, 362, 264, 3429, 294, 11, 597, 307, 264, 1230, 295, 15743, 11, 3429, 484, 11, 264, 1230, 295, 23930, 11, 51564, 51564, 293, 1968, 420, 406, 321, 528, 281, 764, 257, 12577, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08134345865961332, "compression_ratio": 1.9019607843137254, "no_speech_prob": 1.428446194040589e-05}, {"id": 955, "seek": 476900, "start": 4778.0, "end": 4780.0, "text": " whether or not we should have bias,", "tokens": [50364, 509, 603, 3449, 300, 27822, 13, 26384, 575, 3195, 295, 819, 3467, 295, 7914, 13, 50564, 50564, 400, 472, 295, 729, 7914, 307, 264, 8213, 4583, 13, 50664, 50664, 7160, 339, 13, 26384, 13, 28263, 2516, 257, 1230, 295, 4846, 4122, 11, 5598, 4122, 11, 50814, 50814, 1968, 420, 406, 321, 820, 362, 12577, 11, 50914, 50914, 293, 550, 264, 4302, 300, 321, 528, 281, 1081, 341, 4583, 322, 11, 293, 264, 1412, 2010, 13, 51114, 51114, 407, 286, 486, 3406, 270, 613, 732, 11, 457, 5911, 321, 362, 264, 1900, 912, 551, 13, 51314, 51314, 492, 362, 264, 3429, 294, 11, 597, 307, 264, 1230, 295, 15743, 11, 3429, 484, 11, 264, 1230, 295, 23930, 11, 51564, 51564, 293, 1968, 420, 406, 321, 528, 281, 764, 257, 12577, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08134345865961332, "compression_ratio": 1.9019607843137254, "no_speech_prob": 1.428446194040589e-05}, {"id": 956, "seek": 476900, "start": 4780.0, "end": 4784.0, "text": " and then the device that we want to place this layer on, and the data type.", "tokens": [50364, 509, 603, 3449, 300, 27822, 13, 26384, 575, 3195, 295, 819, 3467, 295, 7914, 13, 50564, 50564, 400, 472, 295, 729, 7914, 307, 264, 8213, 4583, 13, 50664, 50664, 7160, 339, 13, 26384, 13, 28263, 2516, 257, 1230, 295, 4846, 4122, 11, 5598, 4122, 11, 50814, 50814, 1968, 420, 406, 321, 820, 362, 12577, 11, 50914, 50914, 293, 550, 264, 4302, 300, 321, 528, 281, 1081, 341, 4583, 322, 11, 293, 264, 1412, 2010, 13, 51114, 51114, 407, 286, 486, 3406, 270, 613, 732, 11, 457, 5911, 321, 362, 264, 1900, 912, 551, 13, 51314, 51314, 492, 362, 264, 3429, 294, 11, 597, 307, 264, 1230, 295, 15743, 11, 3429, 484, 11, 264, 1230, 295, 23930, 11, 51564, 51564, 293, 1968, 420, 406, 321, 528, 281, 764, 257, 12577, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08134345865961332, "compression_ratio": 1.9019607843137254, "no_speech_prob": 1.428446194040589e-05}, {"id": 957, "seek": 476900, "start": 4784.0, "end": 4788.0, "text": " So I will omit these two, but otherwise we have the exact same thing.", "tokens": [50364, 509, 603, 3449, 300, 27822, 13, 26384, 575, 3195, 295, 819, 3467, 295, 7914, 13, 50564, 50564, 400, 472, 295, 729, 7914, 307, 264, 8213, 4583, 13, 50664, 50664, 7160, 339, 13, 26384, 13, 28263, 2516, 257, 1230, 295, 4846, 4122, 11, 5598, 4122, 11, 50814, 50814, 1968, 420, 406, 321, 820, 362, 12577, 11, 50914, 50914, 293, 550, 264, 4302, 300, 321, 528, 281, 1081, 341, 4583, 322, 11, 293, 264, 1412, 2010, 13, 51114, 51114, 407, 286, 486, 3406, 270, 613, 732, 11, 457, 5911, 321, 362, 264, 1900, 912, 551, 13, 51314, 51314, 492, 362, 264, 3429, 294, 11, 597, 307, 264, 1230, 295, 15743, 11, 3429, 484, 11, 264, 1230, 295, 23930, 11, 51564, 51564, 293, 1968, 420, 406, 321, 528, 281, 764, 257, 12577, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08134345865961332, "compression_ratio": 1.9019607843137254, "no_speech_prob": 1.428446194040589e-05}, {"id": 958, "seek": 476900, "start": 4788.0, "end": 4793.0, "text": " We have the fan in, which is the number of inputs, fan out, the number of outputs,", "tokens": [50364, 509, 603, 3449, 300, 27822, 13, 26384, 575, 3195, 295, 819, 3467, 295, 7914, 13, 50564, 50564, 400, 472, 295, 729, 7914, 307, 264, 8213, 4583, 13, 50664, 50664, 7160, 339, 13, 26384, 13, 28263, 2516, 257, 1230, 295, 4846, 4122, 11, 5598, 4122, 11, 50814, 50814, 1968, 420, 406, 321, 820, 362, 12577, 11, 50914, 50914, 293, 550, 264, 4302, 300, 321, 528, 281, 1081, 341, 4583, 322, 11, 293, 264, 1412, 2010, 13, 51114, 51114, 407, 286, 486, 3406, 270, 613, 732, 11, 457, 5911, 321, 362, 264, 1900, 912, 551, 13, 51314, 51314, 492, 362, 264, 3429, 294, 11, 597, 307, 264, 1230, 295, 15743, 11, 3429, 484, 11, 264, 1230, 295, 23930, 11, 51564, 51564, 293, 1968, 420, 406, 321, 528, 281, 764, 257, 12577, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08134345865961332, "compression_ratio": 1.9019607843137254, "no_speech_prob": 1.428446194040589e-05}, {"id": 959, "seek": 476900, "start": 4793.0, "end": 4795.0, "text": " and whether or not we want to use a bias.", "tokens": [50364, 509, 603, 3449, 300, 27822, 13, 26384, 575, 3195, 295, 819, 3467, 295, 7914, 13, 50564, 50564, 400, 472, 295, 729, 7914, 307, 264, 8213, 4583, 13, 50664, 50664, 7160, 339, 13, 26384, 13, 28263, 2516, 257, 1230, 295, 4846, 4122, 11, 5598, 4122, 11, 50814, 50814, 1968, 420, 406, 321, 820, 362, 12577, 11, 50914, 50914, 293, 550, 264, 4302, 300, 321, 528, 281, 1081, 341, 4583, 322, 11, 293, 264, 1412, 2010, 13, 51114, 51114, 407, 286, 486, 3406, 270, 613, 732, 11, 457, 5911, 321, 362, 264, 1900, 912, 551, 13, 51314, 51314, 492, 362, 264, 3429, 294, 11, 597, 307, 264, 1230, 295, 15743, 11, 3429, 484, 11, 264, 1230, 295, 23930, 11, 51564, 51564, 293, 1968, 420, 406, 321, 528, 281, 764, 257, 12577, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08134345865961332, "compression_ratio": 1.9019607843137254, "no_speech_prob": 1.428446194040589e-05}, {"id": 960, "seek": 479500, "start": 4795.0, "end": 4800.0, "text": " And internally inside this layer, there's a weight and a bias, if you'd like it.", "tokens": [50364, 400, 19501, 1854, 341, 4583, 11, 456, 311, 257, 3364, 293, 257, 12577, 11, 498, 291, 1116, 411, 309, 13, 50614, 50614, 467, 307, 7476, 281, 5883, 1125, 264, 3364, 1228, 11, 584, 11, 4974, 3547, 10117, 490, 257, 39148, 13, 50914, 50914, 400, 550, 510, 311, 264, 10822, 5883, 2144, 300, 321, 7152, 1217, 294, 341, 7991, 13, 51114, 51114, 400, 300, 311, 257, 665, 7576, 293, 611, 264, 7576, 300, 286, 1697, 9953, 51, 284, 339, 4960, 13, 51364, 51364, 400, 538, 7576, 11, 264, 12577, 307, 2673, 5883, 1602, 281, 35193, 13, 51514, 51514, 823, 11, 562, 291, 818, 341, 10088, 11, 341, 486, 1936, 8873, 261, 1413, 2031, 1804, 272, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09463734987403165, "compression_ratio": 1.6956521739130435, "no_speech_prob": 9.818043508857954e-06}, {"id": 961, "seek": 479500, "start": 4800.0, "end": 4806.0, "text": " It is typical to initialize the weight using, say, random numbers drawn from a Gaussian.", "tokens": [50364, 400, 19501, 1854, 341, 4583, 11, 456, 311, 257, 3364, 293, 257, 12577, 11, 498, 291, 1116, 411, 309, 13, 50614, 50614, 467, 307, 7476, 281, 5883, 1125, 264, 3364, 1228, 11, 584, 11, 4974, 3547, 10117, 490, 257, 39148, 13, 50914, 50914, 400, 550, 510, 311, 264, 10822, 5883, 2144, 300, 321, 7152, 1217, 294, 341, 7991, 13, 51114, 51114, 400, 300, 311, 257, 665, 7576, 293, 611, 264, 7576, 300, 286, 1697, 9953, 51, 284, 339, 4960, 13, 51364, 51364, 400, 538, 7576, 11, 264, 12577, 307, 2673, 5883, 1602, 281, 35193, 13, 51514, 51514, 823, 11, 562, 291, 818, 341, 10088, 11, 341, 486, 1936, 8873, 261, 1413, 2031, 1804, 272, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09463734987403165, "compression_ratio": 1.6956521739130435, "no_speech_prob": 9.818043508857954e-06}, {"id": 962, "seek": 479500, "start": 4806.0, "end": 4810.0, "text": " And then here's the timing initialization that we discussed already in this lecture.", "tokens": [50364, 400, 19501, 1854, 341, 4583, 11, 456, 311, 257, 3364, 293, 257, 12577, 11, 498, 291, 1116, 411, 309, 13, 50614, 50614, 467, 307, 7476, 281, 5883, 1125, 264, 3364, 1228, 11, 584, 11, 4974, 3547, 10117, 490, 257, 39148, 13, 50914, 50914, 400, 550, 510, 311, 264, 10822, 5883, 2144, 300, 321, 7152, 1217, 294, 341, 7991, 13, 51114, 51114, 400, 300, 311, 257, 665, 7576, 293, 611, 264, 7576, 300, 286, 1697, 9953, 51, 284, 339, 4960, 13, 51364, 51364, 400, 538, 7576, 11, 264, 12577, 307, 2673, 5883, 1602, 281, 35193, 13, 51514, 51514, 823, 11, 562, 291, 818, 341, 10088, 11, 341, 486, 1936, 8873, 261, 1413, 2031, 1804, 272, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09463734987403165, "compression_ratio": 1.6956521739130435, "no_speech_prob": 9.818043508857954e-06}, {"id": 963, "seek": 479500, "start": 4810.0, "end": 4815.0, "text": " And that's a good default and also the default that I believe PyTorch uses.", "tokens": [50364, 400, 19501, 1854, 341, 4583, 11, 456, 311, 257, 3364, 293, 257, 12577, 11, 498, 291, 1116, 411, 309, 13, 50614, 50614, 467, 307, 7476, 281, 5883, 1125, 264, 3364, 1228, 11, 584, 11, 4974, 3547, 10117, 490, 257, 39148, 13, 50914, 50914, 400, 550, 510, 311, 264, 10822, 5883, 2144, 300, 321, 7152, 1217, 294, 341, 7991, 13, 51114, 51114, 400, 300, 311, 257, 665, 7576, 293, 611, 264, 7576, 300, 286, 1697, 9953, 51, 284, 339, 4960, 13, 51364, 51364, 400, 538, 7576, 11, 264, 12577, 307, 2673, 5883, 1602, 281, 35193, 13, 51514, 51514, 823, 11, 562, 291, 818, 341, 10088, 11, 341, 486, 1936, 8873, 261, 1413, 2031, 1804, 272, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09463734987403165, "compression_ratio": 1.6956521739130435, "no_speech_prob": 9.818043508857954e-06}, {"id": 964, "seek": 479500, "start": 4815.0, "end": 4818.0, "text": " And by default, the bias is usually initialized to zeros.", "tokens": [50364, 400, 19501, 1854, 341, 4583, 11, 456, 311, 257, 3364, 293, 257, 12577, 11, 498, 291, 1116, 411, 309, 13, 50614, 50614, 467, 307, 7476, 281, 5883, 1125, 264, 3364, 1228, 11, 584, 11, 4974, 3547, 10117, 490, 257, 39148, 13, 50914, 50914, 400, 550, 510, 311, 264, 10822, 5883, 2144, 300, 321, 7152, 1217, 294, 341, 7991, 13, 51114, 51114, 400, 300, 311, 257, 665, 7576, 293, 611, 264, 7576, 300, 286, 1697, 9953, 51, 284, 339, 4960, 13, 51364, 51364, 400, 538, 7576, 11, 264, 12577, 307, 2673, 5883, 1602, 281, 35193, 13, 51514, 51514, 823, 11, 562, 291, 818, 341, 10088, 11, 341, 486, 1936, 8873, 261, 1413, 2031, 1804, 272, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09463734987403165, "compression_ratio": 1.6956521739130435, "no_speech_prob": 9.818043508857954e-06}, {"id": 965, "seek": 479500, "start": 4818.0, "end": 4823.0, "text": " Now, when you call this module, this will basically calculate w times x plus b,", "tokens": [50364, 400, 19501, 1854, 341, 4583, 11, 456, 311, 257, 3364, 293, 257, 12577, 11, 498, 291, 1116, 411, 309, 13, 50614, 50614, 467, 307, 7476, 281, 5883, 1125, 264, 3364, 1228, 11, 584, 11, 4974, 3547, 10117, 490, 257, 39148, 13, 50914, 50914, 400, 550, 510, 311, 264, 10822, 5883, 2144, 300, 321, 7152, 1217, 294, 341, 7991, 13, 51114, 51114, 400, 300, 311, 257, 665, 7576, 293, 611, 264, 7576, 300, 286, 1697, 9953, 51, 284, 339, 4960, 13, 51364, 51364, 400, 538, 7576, 11, 264, 12577, 307, 2673, 5883, 1602, 281, 35193, 13, 51514, 51514, 823, 11, 562, 291, 818, 341, 10088, 11, 341, 486, 1936, 8873, 261, 1413, 2031, 1804, 272, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09463734987403165, "compression_ratio": 1.6956521739130435, "no_speech_prob": 9.818043508857954e-06}, {"id": 966, "seek": 482300, "start": 4823.0, "end": 4825.0, "text": " if you have nb.", "tokens": [50364, 498, 291, 362, 297, 65, 13, 50464, 50464, 400, 550, 562, 291, 611, 818, 264, 9834, 322, 341, 10088, 11, 50564, 50564, 309, 486, 2736, 264, 10688, 830, 300, 366, 264, 9834, 295, 341, 4583, 13, 50814, 50814, 823, 11, 958, 321, 362, 264, 363, 852, 45, 24440, 2144, 4583, 13, 50914, 50914, 407, 286, 600, 3720, 300, 510, 13, 51064, 51064, 400, 341, 307, 588, 2531, 281, 9953, 51, 284, 339, 13, 26384, 13, 65, 852, 45, 24440, 16, 67, 4583, 11, 382, 4898, 510, 13, 51414, 51414, 407, 286, 478, 733, 295, 1940, 613, 1045, 9834, 510, 11, 264, 10139, 1860, 11, 51664, 51664, 264, 17889, 300, 321, 486, 764, 294, 264, 10044, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11372300783793131, "compression_ratio": 1.6958333333333333, "no_speech_prob": 2.1781565010314807e-05}, {"id": 967, "seek": 482300, "start": 4825.0, "end": 4827.0, "text": " And then when you also call the parameters on this module,", "tokens": [50364, 498, 291, 362, 297, 65, 13, 50464, 50464, 400, 550, 562, 291, 611, 818, 264, 9834, 322, 341, 10088, 11, 50564, 50564, 309, 486, 2736, 264, 10688, 830, 300, 366, 264, 9834, 295, 341, 4583, 13, 50814, 50814, 823, 11, 958, 321, 362, 264, 363, 852, 45, 24440, 2144, 4583, 13, 50914, 50914, 407, 286, 600, 3720, 300, 510, 13, 51064, 51064, 400, 341, 307, 588, 2531, 281, 9953, 51, 284, 339, 13, 26384, 13, 65, 852, 45, 24440, 16, 67, 4583, 11, 382, 4898, 510, 13, 51414, 51414, 407, 286, 478, 733, 295, 1940, 613, 1045, 9834, 510, 11, 264, 10139, 1860, 11, 51664, 51664, 264, 17889, 300, 321, 486, 764, 294, 264, 10044, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11372300783793131, "compression_ratio": 1.6958333333333333, "no_speech_prob": 2.1781565010314807e-05}, {"id": 968, "seek": 482300, "start": 4827.0, "end": 4832.0, "text": " it will return the tensors that are the parameters of this layer.", "tokens": [50364, 498, 291, 362, 297, 65, 13, 50464, 50464, 400, 550, 562, 291, 611, 818, 264, 9834, 322, 341, 10088, 11, 50564, 50564, 309, 486, 2736, 264, 10688, 830, 300, 366, 264, 9834, 295, 341, 4583, 13, 50814, 50814, 823, 11, 958, 321, 362, 264, 363, 852, 45, 24440, 2144, 4583, 13, 50914, 50914, 407, 286, 600, 3720, 300, 510, 13, 51064, 51064, 400, 341, 307, 588, 2531, 281, 9953, 51, 284, 339, 13, 26384, 13, 65, 852, 45, 24440, 16, 67, 4583, 11, 382, 4898, 510, 13, 51414, 51414, 407, 286, 478, 733, 295, 1940, 613, 1045, 9834, 510, 11, 264, 10139, 1860, 11, 51664, 51664, 264, 17889, 300, 321, 486, 764, 294, 264, 10044, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11372300783793131, "compression_ratio": 1.6958333333333333, "no_speech_prob": 2.1781565010314807e-05}, {"id": 969, "seek": 482300, "start": 4832.0, "end": 4834.0, "text": " Now, next we have the BatchNormalization layer.", "tokens": [50364, 498, 291, 362, 297, 65, 13, 50464, 50464, 400, 550, 562, 291, 611, 818, 264, 9834, 322, 341, 10088, 11, 50564, 50564, 309, 486, 2736, 264, 10688, 830, 300, 366, 264, 9834, 295, 341, 4583, 13, 50814, 50814, 823, 11, 958, 321, 362, 264, 363, 852, 45, 24440, 2144, 4583, 13, 50914, 50914, 407, 286, 600, 3720, 300, 510, 13, 51064, 51064, 400, 341, 307, 588, 2531, 281, 9953, 51, 284, 339, 13, 26384, 13, 65, 852, 45, 24440, 16, 67, 4583, 11, 382, 4898, 510, 13, 51414, 51414, 407, 286, 478, 733, 295, 1940, 613, 1045, 9834, 510, 11, 264, 10139, 1860, 11, 51664, 51664, 264, 17889, 300, 321, 486, 764, 294, 264, 10044, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11372300783793131, "compression_ratio": 1.6958333333333333, "no_speech_prob": 2.1781565010314807e-05}, {"id": 970, "seek": 482300, "start": 4834.0, "end": 4837.0, "text": " So I've written that here.", "tokens": [50364, 498, 291, 362, 297, 65, 13, 50464, 50464, 400, 550, 562, 291, 611, 818, 264, 9834, 322, 341, 10088, 11, 50564, 50564, 309, 486, 2736, 264, 10688, 830, 300, 366, 264, 9834, 295, 341, 4583, 13, 50814, 50814, 823, 11, 958, 321, 362, 264, 363, 852, 45, 24440, 2144, 4583, 13, 50914, 50914, 407, 286, 600, 3720, 300, 510, 13, 51064, 51064, 400, 341, 307, 588, 2531, 281, 9953, 51, 284, 339, 13, 26384, 13, 65, 852, 45, 24440, 16, 67, 4583, 11, 382, 4898, 510, 13, 51414, 51414, 407, 286, 478, 733, 295, 1940, 613, 1045, 9834, 510, 11, 264, 10139, 1860, 11, 51664, 51664, 264, 17889, 300, 321, 486, 764, 294, 264, 10044, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11372300783793131, "compression_ratio": 1.6958333333333333, "no_speech_prob": 2.1781565010314807e-05}, {"id": 971, "seek": 482300, "start": 4837.0, "end": 4844.0, "text": " And this is very similar to PyTorch.nn.batchNormal1d layer, as shown here.", "tokens": [50364, 498, 291, 362, 297, 65, 13, 50464, 50464, 400, 550, 562, 291, 611, 818, 264, 9834, 322, 341, 10088, 11, 50564, 50564, 309, 486, 2736, 264, 10688, 830, 300, 366, 264, 9834, 295, 341, 4583, 13, 50814, 50814, 823, 11, 958, 321, 362, 264, 363, 852, 45, 24440, 2144, 4583, 13, 50914, 50914, 407, 286, 600, 3720, 300, 510, 13, 51064, 51064, 400, 341, 307, 588, 2531, 281, 9953, 51, 284, 339, 13, 26384, 13, 65, 852, 45, 24440, 16, 67, 4583, 11, 382, 4898, 510, 13, 51414, 51414, 407, 286, 478, 733, 295, 1940, 613, 1045, 9834, 510, 11, 264, 10139, 1860, 11, 51664, 51664, 264, 17889, 300, 321, 486, 764, 294, 264, 10044, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11372300783793131, "compression_ratio": 1.6958333333333333, "no_speech_prob": 2.1781565010314807e-05}, {"id": 972, "seek": 482300, "start": 4844.0, "end": 4849.0, "text": " So I'm kind of taking these three parameters here, the dimensionality,", "tokens": [50364, 498, 291, 362, 297, 65, 13, 50464, 50464, 400, 550, 562, 291, 611, 818, 264, 9834, 322, 341, 10088, 11, 50564, 50564, 309, 486, 2736, 264, 10688, 830, 300, 366, 264, 9834, 295, 341, 4583, 13, 50814, 50814, 823, 11, 958, 321, 362, 264, 363, 852, 45, 24440, 2144, 4583, 13, 50914, 50914, 407, 286, 600, 3720, 300, 510, 13, 51064, 51064, 400, 341, 307, 588, 2531, 281, 9953, 51, 284, 339, 13, 26384, 13, 65, 852, 45, 24440, 16, 67, 4583, 11, 382, 4898, 510, 13, 51414, 51414, 407, 286, 478, 733, 295, 1940, 613, 1045, 9834, 510, 11, 264, 10139, 1860, 11, 51664, 51664, 264, 17889, 300, 321, 486, 764, 294, 264, 10044, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11372300783793131, "compression_ratio": 1.6958333333333333, "no_speech_prob": 2.1781565010314807e-05}, {"id": 973, "seek": 482300, "start": 4849.0, "end": 4851.0, "text": " the epsilon that we will use in the division,", "tokens": [50364, 498, 291, 362, 297, 65, 13, 50464, 50464, 400, 550, 562, 291, 611, 818, 264, 9834, 322, 341, 10088, 11, 50564, 50564, 309, 486, 2736, 264, 10688, 830, 300, 366, 264, 9834, 295, 341, 4583, 13, 50814, 50814, 823, 11, 958, 321, 362, 264, 363, 852, 45, 24440, 2144, 4583, 13, 50914, 50914, 407, 286, 600, 3720, 300, 510, 13, 51064, 51064, 400, 341, 307, 588, 2531, 281, 9953, 51, 284, 339, 13, 26384, 13, 65, 852, 45, 24440, 16, 67, 4583, 11, 382, 4898, 510, 13, 51414, 51414, 407, 286, 478, 733, 295, 1940, 613, 1045, 9834, 510, 11, 264, 10139, 1860, 11, 51664, 51664, 264, 17889, 300, 321, 486, 764, 294, 264, 10044, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11372300783793131, "compression_ratio": 1.6958333333333333, "no_speech_prob": 2.1781565010314807e-05}, {"id": 974, "seek": 485100, "start": 4851.0, "end": 4855.0, "text": " and the momentum that we will use in keeping track of these running stats,", "tokens": [50364, 293, 264, 11244, 300, 321, 486, 764, 294, 5145, 2837, 295, 613, 2614, 18152, 11, 50564, 50564, 264, 2614, 914, 293, 264, 2614, 21977, 13, 50714, 50714, 823, 11, 9953, 51, 284, 339, 767, 2516, 1596, 257, 1326, 544, 721, 11, 50814, 50814, 457, 286, 478, 11926, 512, 295, 641, 6257, 13, 50914, 50914, 407, 337, 505, 11, 286, 603, 915, 486, 312, 2074, 13, 51014, 51014, 663, 1355, 300, 321, 486, 312, 1228, 257, 15546, 293, 9861, 934, 264, 2710, 2144, 13, 51214, 51214, 440, 2837, 2614, 18152, 486, 312, 2074, 13, 51264, 51264, 407, 321, 486, 312, 5145, 2837, 295, 264, 2614, 914, 293, 264, 2614, 21977, 294, 264, 363, 852, 45, 24440, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0743391195933024, "compression_ratio": 1.9071729957805907, "no_speech_prob": 1.384437746310141e-05}, {"id": 975, "seek": 485100, "start": 4855.0, "end": 4858.0, "text": " the running mean and the running variance.", "tokens": [50364, 293, 264, 11244, 300, 321, 486, 764, 294, 5145, 2837, 295, 613, 2614, 18152, 11, 50564, 50564, 264, 2614, 914, 293, 264, 2614, 21977, 13, 50714, 50714, 823, 11, 9953, 51, 284, 339, 767, 2516, 1596, 257, 1326, 544, 721, 11, 50814, 50814, 457, 286, 478, 11926, 512, 295, 641, 6257, 13, 50914, 50914, 407, 337, 505, 11, 286, 603, 915, 486, 312, 2074, 13, 51014, 51014, 663, 1355, 300, 321, 486, 312, 1228, 257, 15546, 293, 9861, 934, 264, 2710, 2144, 13, 51214, 51214, 440, 2837, 2614, 18152, 486, 312, 2074, 13, 51264, 51264, 407, 321, 486, 312, 5145, 2837, 295, 264, 2614, 914, 293, 264, 2614, 21977, 294, 264, 363, 852, 45, 24440, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0743391195933024, "compression_ratio": 1.9071729957805907, "no_speech_prob": 1.384437746310141e-05}, {"id": 976, "seek": 485100, "start": 4858.0, "end": 4860.0, "text": " Now, PyTorch actually takes quite a few more things,", "tokens": [50364, 293, 264, 11244, 300, 321, 486, 764, 294, 5145, 2837, 295, 613, 2614, 18152, 11, 50564, 50564, 264, 2614, 914, 293, 264, 2614, 21977, 13, 50714, 50714, 823, 11, 9953, 51, 284, 339, 767, 2516, 1596, 257, 1326, 544, 721, 11, 50814, 50814, 457, 286, 478, 11926, 512, 295, 641, 6257, 13, 50914, 50914, 407, 337, 505, 11, 286, 603, 915, 486, 312, 2074, 13, 51014, 51014, 663, 1355, 300, 321, 486, 312, 1228, 257, 15546, 293, 9861, 934, 264, 2710, 2144, 13, 51214, 51214, 440, 2837, 2614, 18152, 486, 312, 2074, 13, 51264, 51264, 407, 321, 486, 312, 5145, 2837, 295, 264, 2614, 914, 293, 264, 2614, 21977, 294, 264, 363, 852, 45, 24440, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0743391195933024, "compression_ratio": 1.9071729957805907, "no_speech_prob": 1.384437746310141e-05}, {"id": 977, "seek": 485100, "start": 4860.0, "end": 4862.0, "text": " but I'm assuming some of their settings.", "tokens": [50364, 293, 264, 11244, 300, 321, 486, 764, 294, 5145, 2837, 295, 613, 2614, 18152, 11, 50564, 50564, 264, 2614, 914, 293, 264, 2614, 21977, 13, 50714, 50714, 823, 11, 9953, 51, 284, 339, 767, 2516, 1596, 257, 1326, 544, 721, 11, 50814, 50814, 457, 286, 478, 11926, 512, 295, 641, 6257, 13, 50914, 50914, 407, 337, 505, 11, 286, 603, 915, 486, 312, 2074, 13, 51014, 51014, 663, 1355, 300, 321, 486, 312, 1228, 257, 15546, 293, 9861, 934, 264, 2710, 2144, 13, 51214, 51214, 440, 2837, 2614, 18152, 486, 312, 2074, 13, 51264, 51264, 407, 321, 486, 312, 5145, 2837, 295, 264, 2614, 914, 293, 264, 2614, 21977, 294, 264, 363, 852, 45, 24440, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0743391195933024, "compression_ratio": 1.9071729957805907, "no_speech_prob": 1.384437746310141e-05}, {"id": 978, "seek": 485100, "start": 4862.0, "end": 4864.0, "text": " So for us, I'll find will be true.", "tokens": [50364, 293, 264, 11244, 300, 321, 486, 764, 294, 5145, 2837, 295, 613, 2614, 18152, 11, 50564, 50564, 264, 2614, 914, 293, 264, 2614, 21977, 13, 50714, 50714, 823, 11, 9953, 51, 284, 339, 767, 2516, 1596, 257, 1326, 544, 721, 11, 50814, 50814, 457, 286, 478, 11926, 512, 295, 641, 6257, 13, 50914, 50914, 407, 337, 505, 11, 286, 603, 915, 486, 312, 2074, 13, 51014, 51014, 663, 1355, 300, 321, 486, 312, 1228, 257, 15546, 293, 9861, 934, 264, 2710, 2144, 13, 51214, 51214, 440, 2837, 2614, 18152, 486, 312, 2074, 13, 51264, 51264, 407, 321, 486, 312, 5145, 2837, 295, 264, 2614, 914, 293, 264, 2614, 21977, 294, 264, 363, 852, 45, 24440, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0743391195933024, "compression_ratio": 1.9071729957805907, "no_speech_prob": 1.384437746310141e-05}, {"id": 979, "seek": 485100, "start": 4864.0, "end": 4868.0, "text": " That means that we will be using a gamma and beta after the normalization.", "tokens": [50364, 293, 264, 11244, 300, 321, 486, 764, 294, 5145, 2837, 295, 613, 2614, 18152, 11, 50564, 50564, 264, 2614, 914, 293, 264, 2614, 21977, 13, 50714, 50714, 823, 11, 9953, 51, 284, 339, 767, 2516, 1596, 257, 1326, 544, 721, 11, 50814, 50814, 457, 286, 478, 11926, 512, 295, 641, 6257, 13, 50914, 50914, 407, 337, 505, 11, 286, 603, 915, 486, 312, 2074, 13, 51014, 51014, 663, 1355, 300, 321, 486, 312, 1228, 257, 15546, 293, 9861, 934, 264, 2710, 2144, 13, 51214, 51214, 440, 2837, 2614, 18152, 486, 312, 2074, 13, 51264, 51264, 407, 321, 486, 312, 5145, 2837, 295, 264, 2614, 914, 293, 264, 2614, 21977, 294, 264, 363, 852, 45, 24440, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0743391195933024, "compression_ratio": 1.9071729957805907, "no_speech_prob": 1.384437746310141e-05}, {"id": 980, "seek": 485100, "start": 4868.0, "end": 4869.0, "text": " The track running stats will be true.", "tokens": [50364, 293, 264, 11244, 300, 321, 486, 764, 294, 5145, 2837, 295, 613, 2614, 18152, 11, 50564, 50564, 264, 2614, 914, 293, 264, 2614, 21977, 13, 50714, 50714, 823, 11, 9953, 51, 284, 339, 767, 2516, 1596, 257, 1326, 544, 721, 11, 50814, 50814, 457, 286, 478, 11926, 512, 295, 641, 6257, 13, 50914, 50914, 407, 337, 505, 11, 286, 603, 915, 486, 312, 2074, 13, 51014, 51014, 663, 1355, 300, 321, 486, 312, 1228, 257, 15546, 293, 9861, 934, 264, 2710, 2144, 13, 51214, 51214, 440, 2837, 2614, 18152, 486, 312, 2074, 13, 51264, 51264, 407, 321, 486, 312, 5145, 2837, 295, 264, 2614, 914, 293, 264, 2614, 21977, 294, 264, 363, 852, 45, 24440, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0743391195933024, "compression_ratio": 1.9071729957805907, "no_speech_prob": 1.384437746310141e-05}, {"id": 981, "seek": 485100, "start": 4869.0, "end": 4874.0, "text": " So we will be keeping track of the running mean and the running variance in the BatchNormal.", "tokens": [50364, 293, 264, 11244, 300, 321, 486, 764, 294, 5145, 2837, 295, 613, 2614, 18152, 11, 50564, 50564, 264, 2614, 914, 293, 264, 2614, 21977, 13, 50714, 50714, 823, 11, 9953, 51, 284, 339, 767, 2516, 1596, 257, 1326, 544, 721, 11, 50814, 50814, 457, 286, 478, 11926, 512, 295, 641, 6257, 13, 50914, 50914, 407, 337, 505, 11, 286, 603, 915, 486, 312, 2074, 13, 51014, 51014, 663, 1355, 300, 321, 486, 312, 1228, 257, 15546, 293, 9861, 934, 264, 2710, 2144, 13, 51214, 51214, 440, 2837, 2614, 18152, 486, 312, 2074, 13, 51264, 51264, 407, 321, 486, 312, 5145, 2837, 295, 264, 2614, 914, 293, 264, 2614, 21977, 294, 264, 363, 852, 45, 24440, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.0743391195933024, "compression_ratio": 1.9071729957805907, "no_speech_prob": 1.384437746310141e-05}, {"id": 982, "seek": 487400, "start": 4874.0, "end": 4882.0, "text": " Our device by default is the CPU, and the data type by default is float32.", "tokens": [50364, 2621, 4302, 538, 7576, 307, 264, 13199, 11, 293, 264, 1412, 2010, 538, 7576, 307, 15706, 11440, 13, 50764, 50764, 407, 729, 366, 264, 7576, 82, 13, 50814, 50814, 10328, 11, 321, 366, 1940, 439, 264, 912, 9834, 294, 341, 363, 852, 45, 24440, 4583, 13, 51014, 51014, 407, 700, 11, 286, 478, 445, 6816, 552, 13, 51164, 51164, 823, 11, 510, 311, 746, 777, 13, 51214, 51214, 821, 311, 257, 2411, 17227, 1760, 11, 597, 538, 7576, 307, 2074, 13, 51314, 51314, 400, 9953, 51, 284, 339, 13, 26384, 16679, 611, 362, 341, 19667, 2411, 17227, 1760, 13, 51514, 51514, 400, 300, 311, 570, 867, 16679, 11, 293, 363, 852, 45, 24440, 307, 5556, 294, 300, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08242944029510998, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.723053901514504e-05}, {"id": 983, "seek": 487400, "start": 4882.0, "end": 4883.0, "text": " So those are the defaults.", "tokens": [50364, 2621, 4302, 538, 7576, 307, 264, 13199, 11, 293, 264, 1412, 2010, 538, 7576, 307, 15706, 11440, 13, 50764, 50764, 407, 729, 366, 264, 7576, 82, 13, 50814, 50814, 10328, 11, 321, 366, 1940, 439, 264, 912, 9834, 294, 341, 363, 852, 45, 24440, 4583, 13, 51014, 51014, 407, 700, 11, 286, 478, 445, 6816, 552, 13, 51164, 51164, 823, 11, 510, 311, 746, 777, 13, 51214, 51214, 821, 311, 257, 2411, 17227, 1760, 11, 597, 538, 7576, 307, 2074, 13, 51314, 51314, 400, 9953, 51, 284, 339, 13, 26384, 16679, 611, 362, 341, 19667, 2411, 17227, 1760, 13, 51514, 51514, 400, 300, 311, 570, 867, 16679, 11, 293, 363, 852, 45, 24440, 307, 5556, 294, 300, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08242944029510998, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.723053901514504e-05}, {"id": 984, "seek": 487400, "start": 4883.0, "end": 4887.0, "text": " Otherwise, we are taking all the same parameters in this BatchNormal layer.", "tokens": [50364, 2621, 4302, 538, 7576, 307, 264, 13199, 11, 293, 264, 1412, 2010, 538, 7576, 307, 15706, 11440, 13, 50764, 50764, 407, 729, 366, 264, 7576, 82, 13, 50814, 50814, 10328, 11, 321, 366, 1940, 439, 264, 912, 9834, 294, 341, 363, 852, 45, 24440, 4583, 13, 51014, 51014, 407, 700, 11, 286, 478, 445, 6816, 552, 13, 51164, 51164, 823, 11, 510, 311, 746, 777, 13, 51214, 51214, 821, 311, 257, 2411, 17227, 1760, 11, 597, 538, 7576, 307, 2074, 13, 51314, 51314, 400, 9953, 51, 284, 339, 13, 26384, 16679, 611, 362, 341, 19667, 2411, 17227, 1760, 13, 51514, 51514, 400, 300, 311, 570, 867, 16679, 11, 293, 363, 852, 45, 24440, 307, 5556, 294, 300, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08242944029510998, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.723053901514504e-05}, {"id": 985, "seek": 487400, "start": 4887.0, "end": 4890.0, "text": " So first, I'm just saving them.", "tokens": [50364, 2621, 4302, 538, 7576, 307, 264, 13199, 11, 293, 264, 1412, 2010, 538, 7576, 307, 15706, 11440, 13, 50764, 50764, 407, 729, 366, 264, 7576, 82, 13, 50814, 50814, 10328, 11, 321, 366, 1940, 439, 264, 912, 9834, 294, 341, 363, 852, 45, 24440, 4583, 13, 51014, 51014, 407, 700, 11, 286, 478, 445, 6816, 552, 13, 51164, 51164, 823, 11, 510, 311, 746, 777, 13, 51214, 51214, 821, 311, 257, 2411, 17227, 1760, 11, 597, 538, 7576, 307, 2074, 13, 51314, 51314, 400, 9953, 51, 284, 339, 13, 26384, 16679, 611, 362, 341, 19667, 2411, 17227, 1760, 13, 51514, 51514, 400, 300, 311, 570, 867, 16679, 11, 293, 363, 852, 45, 24440, 307, 5556, 294, 300, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08242944029510998, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.723053901514504e-05}, {"id": 986, "seek": 487400, "start": 4890.0, "end": 4891.0, "text": " Now, here's something new.", "tokens": [50364, 2621, 4302, 538, 7576, 307, 264, 13199, 11, 293, 264, 1412, 2010, 538, 7576, 307, 15706, 11440, 13, 50764, 50764, 407, 729, 366, 264, 7576, 82, 13, 50814, 50814, 10328, 11, 321, 366, 1940, 439, 264, 912, 9834, 294, 341, 363, 852, 45, 24440, 4583, 13, 51014, 51014, 407, 700, 11, 286, 478, 445, 6816, 552, 13, 51164, 51164, 823, 11, 510, 311, 746, 777, 13, 51214, 51214, 821, 311, 257, 2411, 17227, 1760, 11, 597, 538, 7576, 307, 2074, 13, 51314, 51314, 400, 9953, 51, 284, 339, 13, 26384, 16679, 611, 362, 341, 19667, 2411, 17227, 1760, 13, 51514, 51514, 400, 300, 311, 570, 867, 16679, 11, 293, 363, 852, 45, 24440, 307, 5556, 294, 300, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08242944029510998, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.723053901514504e-05}, {"id": 987, "seek": 487400, "start": 4891.0, "end": 4893.0, "text": " There's a.training, which by default is true.", "tokens": [50364, 2621, 4302, 538, 7576, 307, 264, 13199, 11, 293, 264, 1412, 2010, 538, 7576, 307, 15706, 11440, 13, 50764, 50764, 407, 729, 366, 264, 7576, 82, 13, 50814, 50814, 10328, 11, 321, 366, 1940, 439, 264, 912, 9834, 294, 341, 363, 852, 45, 24440, 4583, 13, 51014, 51014, 407, 700, 11, 286, 478, 445, 6816, 552, 13, 51164, 51164, 823, 11, 510, 311, 746, 777, 13, 51214, 51214, 821, 311, 257, 2411, 17227, 1760, 11, 597, 538, 7576, 307, 2074, 13, 51314, 51314, 400, 9953, 51, 284, 339, 13, 26384, 16679, 611, 362, 341, 19667, 2411, 17227, 1760, 13, 51514, 51514, 400, 300, 311, 570, 867, 16679, 11, 293, 363, 852, 45, 24440, 307, 5556, 294, 300, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08242944029510998, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.723053901514504e-05}, {"id": 988, "seek": 487400, "start": 4893.0, "end": 4897.0, "text": " And PyTorch.nn modules also have this attribute.training.", "tokens": [50364, 2621, 4302, 538, 7576, 307, 264, 13199, 11, 293, 264, 1412, 2010, 538, 7576, 307, 15706, 11440, 13, 50764, 50764, 407, 729, 366, 264, 7576, 82, 13, 50814, 50814, 10328, 11, 321, 366, 1940, 439, 264, 912, 9834, 294, 341, 363, 852, 45, 24440, 4583, 13, 51014, 51014, 407, 700, 11, 286, 478, 445, 6816, 552, 13, 51164, 51164, 823, 11, 510, 311, 746, 777, 13, 51214, 51214, 821, 311, 257, 2411, 17227, 1760, 11, 597, 538, 7576, 307, 2074, 13, 51314, 51314, 400, 9953, 51, 284, 339, 13, 26384, 16679, 611, 362, 341, 19667, 2411, 17227, 1760, 13, 51514, 51514, 400, 300, 311, 570, 867, 16679, 11, 293, 363, 852, 45, 24440, 307, 5556, 294, 300, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08242944029510998, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.723053901514504e-05}, {"id": 989, "seek": 487400, "start": 4897.0, "end": 4901.0, "text": " And that's because many modules, and BatchNormal is included in that,", "tokens": [50364, 2621, 4302, 538, 7576, 307, 264, 13199, 11, 293, 264, 1412, 2010, 538, 7576, 307, 15706, 11440, 13, 50764, 50764, 407, 729, 366, 264, 7576, 82, 13, 50814, 50814, 10328, 11, 321, 366, 1940, 439, 264, 912, 9834, 294, 341, 363, 852, 45, 24440, 4583, 13, 51014, 51014, 407, 700, 11, 286, 478, 445, 6816, 552, 13, 51164, 51164, 823, 11, 510, 311, 746, 777, 13, 51214, 51214, 821, 311, 257, 2411, 17227, 1760, 11, 597, 538, 7576, 307, 2074, 13, 51314, 51314, 400, 9953, 51, 284, 339, 13, 26384, 16679, 611, 362, 341, 19667, 2411, 17227, 1760, 13, 51514, 51514, 400, 300, 311, 570, 867, 16679, 11, 293, 363, 852, 45, 24440, 307, 5556, 294, 300, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08242944029510998, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.723053901514504e-05}, {"id": 990, "seek": 490100, "start": 4901.0, "end": 4905.0, "text": " have a different behavior, whether you are training your enrollment", "tokens": [50364, 362, 257, 819, 5223, 11, 1968, 291, 366, 3097, 428, 22420, 50564, 50564, 420, 1968, 291, 366, 2614, 309, 294, 364, 13344, 4391, 50664, 50664, 293, 28258, 428, 13344, 6064, 420, 1228, 309, 337, 38253, 322, 512, 1500, 5110, 13, 50914, 50914, 400, 363, 852, 45, 24440, 307, 364, 1365, 295, 341, 11, 570, 562, 321, 366, 3097, 11, 51114, 51114, 321, 366, 516, 281, 312, 1228, 264, 914, 293, 264, 21977, 14109, 490, 264, 2190, 15245, 13, 51264, 51264, 583, 1830, 38253, 11, 321, 366, 1228, 264, 2614, 914, 293, 2614, 21977, 13, 51464, 51464, 400, 370, 611, 11, 498, 321, 366, 3097, 11, 321, 366, 25113, 914, 293, 21977, 13, 51664, 51664, 583, 498, 321, 366, 4997, 11, 550, 613, 366, 406, 885, 10588, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07856512433699979, "compression_ratio": 2.066666666666667, "no_speech_prob": 2.046095505647827e-05}, {"id": 991, "seek": 490100, "start": 4905.0, "end": 4907.0, "text": " or whether you are running it in an evaluation mode", "tokens": [50364, 362, 257, 819, 5223, 11, 1968, 291, 366, 3097, 428, 22420, 50564, 50564, 420, 1968, 291, 366, 2614, 309, 294, 364, 13344, 4391, 50664, 50664, 293, 28258, 428, 13344, 6064, 420, 1228, 309, 337, 38253, 322, 512, 1500, 5110, 13, 50914, 50914, 400, 363, 852, 45, 24440, 307, 364, 1365, 295, 341, 11, 570, 562, 321, 366, 3097, 11, 51114, 51114, 321, 366, 516, 281, 312, 1228, 264, 914, 293, 264, 21977, 14109, 490, 264, 2190, 15245, 13, 51264, 51264, 583, 1830, 38253, 11, 321, 366, 1228, 264, 2614, 914, 293, 2614, 21977, 13, 51464, 51464, 400, 370, 611, 11, 498, 321, 366, 3097, 11, 321, 366, 25113, 914, 293, 21977, 13, 51664, 51664, 583, 498, 321, 366, 4997, 11, 550, 613, 366, 406, 885, 10588, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07856512433699979, "compression_ratio": 2.066666666666667, "no_speech_prob": 2.046095505647827e-05}, {"id": 992, "seek": 490100, "start": 4907.0, "end": 4912.0, "text": " and calculating your evaluation laws or using it for inference on some test examples.", "tokens": [50364, 362, 257, 819, 5223, 11, 1968, 291, 366, 3097, 428, 22420, 50564, 50564, 420, 1968, 291, 366, 2614, 309, 294, 364, 13344, 4391, 50664, 50664, 293, 28258, 428, 13344, 6064, 420, 1228, 309, 337, 38253, 322, 512, 1500, 5110, 13, 50914, 50914, 400, 363, 852, 45, 24440, 307, 364, 1365, 295, 341, 11, 570, 562, 321, 366, 3097, 11, 51114, 51114, 321, 366, 516, 281, 312, 1228, 264, 914, 293, 264, 21977, 14109, 490, 264, 2190, 15245, 13, 51264, 51264, 583, 1830, 38253, 11, 321, 366, 1228, 264, 2614, 914, 293, 2614, 21977, 13, 51464, 51464, 400, 370, 611, 11, 498, 321, 366, 3097, 11, 321, 366, 25113, 914, 293, 21977, 13, 51664, 51664, 583, 498, 321, 366, 4997, 11, 550, 613, 366, 406, 885, 10588, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07856512433699979, "compression_ratio": 2.066666666666667, "no_speech_prob": 2.046095505647827e-05}, {"id": 993, "seek": 490100, "start": 4912.0, "end": 4916.0, "text": " And BatchNormal is an example of this, because when we are training,", "tokens": [50364, 362, 257, 819, 5223, 11, 1968, 291, 366, 3097, 428, 22420, 50564, 50564, 420, 1968, 291, 366, 2614, 309, 294, 364, 13344, 4391, 50664, 50664, 293, 28258, 428, 13344, 6064, 420, 1228, 309, 337, 38253, 322, 512, 1500, 5110, 13, 50914, 50914, 400, 363, 852, 45, 24440, 307, 364, 1365, 295, 341, 11, 570, 562, 321, 366, 3097, 11, 51114, 51114, 321, 366, 516, 281, 312, 1228, 264, 914, 293, 264, 21977, 14109, 490, 264, 2190, 15245, 13, 51264, 51264, 583, 1830, 38253, 11, 321, 366, 1228, 264, 2614, 914, 293, 2614, 21977, 13, 51464, 51464, 400, 370, 611, 11, 498, 321, 366, 3097, 11, 321, 366, 25113, 914, 293, 21977, 13, 51664, 51664, 583, 498, 321, 366, 4997, 11, 550, 613, 366, 406, 885, 10588, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07856512433699979, "compression_ratio": 2.066666666666667, "no_speech_prob": 2.046095505647827e-05}, {"id": 994, "seek": 490100, "start": 4916.0, "end": 4919.0, "text": " we are going to be using the mean and the variance estimated from the current batch.", "tokens": [50364, 362, 257, 819, 5223, 11, 1968, 291, 366, 3097, 428, 22420, 50564, 50564, 420, 1968, 291, 366, 2614, 309, 294, 364, 13344, 4391, 50664, 50664, 293, 28258, 428, 13344, 6064, 420, 1228, 309, 337, 38253, 322, 512, 1500, 5110, 13, 50914, 50914, 400, 363, 852, 45, 24440, 307, 364, 1365, 295, 341, 11, 570, 562, 321, 366, 3097, 11, 51114, 51114, 321, 366, 516, 281, 312, 1228, 264, 914, 293, 264, 21977, 14109, 490, 264, 2190, 15245, 13, 51264, 51264, 583, 1830, 38253, 11, 321, 366, 1228, 264, 2614, 914, 293, 2614, 21977, 13, 51464, 51464, 400, 370, 611, 11, 498, 321, 366, 3097, 11, 321, 366, 25113, 914, 293, 21977, 13, 51664, 51664, 583, 498, 321, 366, 4997, 11, 550, 613, 366, 406, 885, 10588, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07856512433699979, "compression_ratio": 2.066666666666667, "no_speech_prob": 2.046095505647827e-05}, {"id": 995, "seek": 490100, "start": 4919.0, "end": 4923.0, "text": " But during inference, we are using the running mean and running variance.", "tokens": [50364, 362, 257, 819, 5223, 11, 1968, 291, 366, 3097, 428, 22420, 50564, 50564, 420, 1968, 291, 366, 2614, 309, 294, 364, 13344, 4391, 50664, 50664, 293, 28258, 428, 13344, 6064, 420, 1228, 309, 337, 38253, 322, 512, 1500, 5110, 13, 50914, 50914, 400, 363, 852, 45, 24440, 307, 364, 1365, 295, 341, 11, 570, 562, 321, 366, 3097, 11, 51114, 51114, 321, 366, 516, 281, 312, 1228, 264, 914, 293, 264, 21977, 14109, 490, 264, 2190, 15245, 13, 51264, 51264, 583, 1830, 38253, 11, 321, 366, 1228, 264, 2614, 914, 293, 2614, 21977, 13, 51464, 51464, 400, 370, 611, 11, 498, 321, 366, 3097, 11, 321, 366, 25113, 914, 293, 21977, 13, 51664, 51664, 583, 498, 321, 366, 4997, 11, 550, 613, 366, 406, 885, 10588, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07856512433699979, "compression_ratio": 2.066666666666667, "no_speech_prob": 2.046095505647827e-05}, {"id": 996, "seek": 490100, "start": 4923.0, "end": 4927.0, "text": " And so also, if we are training, we are updating mean and variance.", "tokens": [50364, 362, 257, 819, 5223, 11, 1968, 291, 366, 3097, 428, 22420, 50564, 50564, 420, 1968, 291, 366, 2614, 309, 294, 364, 13344, 4391, 50664, 50664, 293, 28258, 428, 13344, 6064, 420, 1228, 309, 337, 38253, 322, 512, 1500, 5110, 13, 50914, 50914, 400, 363, 852, 45, 24440, 307, 364, 1365, 295, 341, 11, 570, 562, 321, 366, 3097, 11, 51114, 51114, 321, 366, 516, 281, 312, 1228, 264, 914, 293, 264, 21977, 14109, 490, 264, 2190, 15245, 13, 51264, 51264, 583, 1830, 38253, 11, 321, 366, 1228, 264, 2614, 914, 293, 2614, 21977, 13, 51464, 51464, 400, 370, 611, 11, 498, 321, 366, 3097, 11, 321, 366, 25113, 914, 293, 21977, 13, 51664, 51664, 583, 498, 321, 366, 4997, 11, 550, 613, 366, 406, 885, 10588, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07856512433699979, "compression_ratio": 2.066666666666667, "no_speech_prob": 2.046095505647827e-05}, {"id": 997, "seek": 490100, "start": 4927.0, "end": 4930.0, "text": " But if we are testing, then these are not being updated.", "tokens": [50364, 362, 257, 819, 5223, 11, 1968, 291, 366, 3097, 428, 22420, 50564, 50564, 420, 1968, 291, 366, 2614, 309, 294, 364, 13344, 4391, 50664, 50664, 293, 28258, 428, 13344, 6064, 420, 1228, 309, 337, 38253, 322, 512, 1500, 5110, 13, 50914, 50914, 400, 363, 852, 45, 24440, 307, 364, 1365, 295, 341, 11, 570, 562, 321, 366, 3097, 11, 51114, 51114, 321, 366, 516, 281, 312, 1228, 264, 914, 293, 264, 21977, 14109, 490, 264, 2190, 15245, 13, 51264, 51264, 583, 1830, 38253, 11, 321, 366, 1228, 264, 2614, 914, 293, 2614, 21977, 13, 51464, 51464, 400, 370, 611, 11, 498, 321, 366, 3097, 11, 321, 366, 25113, 914, 293, 21977, 13, 51664, 51664, 583, 498, 321, 366, 4997, 11, 550, 613, 366, 406, 885, 10588, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07856512433699979, "compression_ratio": 2.066666666666667, "no_speech_prob": 2.046095505647827e-05}, {"id": 998, "seek": 493000, "start": 4930.0, "end": 4932.0, "text": " They are kept fixed.", "tokens": [50364, 814, 366, 4305, 6806, 13, 50464, 50464, 400, 370, 341, 7166, 307, 4818, 293, 538, 7576, 2074, 11, 445, 411, 294, 9953, 51, 284, 339, 13, 50664, 50664, 823, 11, 264, 9834, 295, 363, 852, 45, 24440, 502, 35, 366, 264, 15546, 293, 264, 9861, 510, 13, 50964, 50964, 400, 550, 264, 2614, 914, 293, 2614, 21977, 366, 1219, 9204, 433, 294, 9953, 51, 284, 339, 297, 4726, 3474, 1503, 13, 51214, 51214, 400, 613, 9204, 433, 366, 8895, 1228, 21510, 2684, 4274, 510, 20803, 13, 51514, 51514, 400, 436, 366, 406, 644, 295, 264, 646, 38377, 293, 342, 8997, 2750, 16235, 23475, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09021449972082067, "compression_ratio": 1.6428571428571428, "no_speech_prob": 8.800661817076616e-06}, {"id": 999, "seek": 493000, "start": 4932.0, "end": 4936.0, "text": " And so this flag is necessary and by default true, just like in PyTorch.", "tokens": [50364, 814, 366, 4305, 6806, 13, 50464, 50464, 400, 370, 341, 7166, 307, 4818, 293, 538, 7576, 2074, 11, 445, 411, 294, 9953, 51, 284, 339, 13, 50664, 50664, 823, 11, 264, 9834, 295, 363, 852, 45, 24440, 502, 35, 366, 264, 15546, 293, 264, 9861, 510, 13, 50964, 50964, 400, 550, 264, 2614, 914, 293, 2614, 21977, 366, 1219, 9204, 433, 294, 9953, 51, 284, 339, 297, 4726, 3474, 1503, 13, 51214, 51214, 400, 613, 9204, 433, 366, 8895, 1228, 21510, 2684, 4274, 510, 20803, 13, 51514, 51514, 400, 436, 366, 406, 644, 295, 264, 646, 38377, 293, 342, 8997, 2750, 16235, 23475, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09021449972082067, "compression_ratio": 1.6428571428571428, "no_speech_prob": 8.800661817076616e-06}, {"id": 1000, "seek": 493000, "start": 4936.0, "end": 4942.0, "text": " Now, the parameters of BatchNormal 1D are the gamma and the beta here.", "tokens": [50364, 814, 366, 4305, 6806, 13, 50464, 50464, 400, 370, 341, 7166, 307, 4818, 293, 538, 7576, 2074, 11, 445, 411, 294, 9953, 51, 284, 339, 13, 50664, 50664, 823, 11, 264, 9834, 295, 363, 852, 45, 24440, 502, 35, 366, 264, 15546, 293, 264, 9861, 510, 13, 50964, 50964, 400, 550, 264, 2614, 914, 293, 2614, 21977, 366, 1219, 9204, 433, 294, 9953, 51, 284, 339, 297, 4726, 3474, 1503, 13, 51214, 51214, 400, 613, 9204, 433, 366, 8895, 1228, 21510, 2684, 4274, 510, 20803, 13, 51514, 51514, 400, 436, 366, 406, 644, 295, 264, 646, 38377, 293, 342, 8997, 2750, 16235, 23475, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09021449972082067, "compression_ratio": 1.6428571428571428, "no_speech_prob": 8.800661817076616e-06}, {"id": 1001, "seek": 493000, "start": 4942.0, "end": 4947.0, "text": " And then the running mean and running variance are called buffers in PyTorch nomenclature.", "tokens": [50364, 814, 366, 4305, 6806, 13, 50464, 50464, 400, 370, 341, 7166, 307, 4818, 293, 538, 7576, 2074, 11, 445, 411, 294, 9953, 51, 284, 339, 13, 50664, 50664, 823, 11, 264, 9834, 295, 363, 852, 45, 24440, 502, 35, 366, 264, 15546, 293, 264, 9861, 510, 13, 50964, 50964, 400, 550, 264, 2614, 914, 293, 2614, 21977, 366, 1219, 9204, 433, 294, 9953, 51, 284, 339, 297, 4726, 3474, 1503, 13, 51214, 51214, 400, 613, 9204, 433, 366, 8895, 1228, 21510, 2684, 4274, 510, 20803, 13, 51514, 51514, 400, 436, 366, 406, 644, 295, 264, 646, 38377, 293, 342, 8997, 2750, 16235, 23475, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09021449972082067, "compression_ratio": 1.6428571428571428, "no_speech_prob": 8.800661817076616e-06}, {"id": 1002, "seek": 493000, "start": 4947.0, "end": 4953.0, "text": " And these buffers are trained using exponential moving average here explicitly.", "tokens": [50364, 814, 366, 4305, 6806, 13, 50464, 50464, 400, 370, 341, 7166, 307, 4818, 293, 538, 7576, 2074, 11, 445, 411, 294, 9953, 51, 284, 339, 13, 50664, 50664, 823, 11, 264, 9834, 295, 363, 852, 45, 24440, 502, 35, 366, 264, 15546, 293, 264, 9861, 510, 13, 50964, 50964, 400, 550, 264, 2614, 914, 293, 2614, 21977, 366, 1219, 9204, 433, 294, 9953, 51, 284, 339, 297, 4726, 3474, 1503, 13, 51214, 51214, 400, 613, 9204, 433, 366, 8895, 1228, 21510, 2684, 4274, 510, 20803, 13, 51514, 51514, 400, 436, 366, 406, 644, 295, 264, 646, 38377, 293, 342, 8997, 2750, 16235, 23475, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09021449972082067, "compression_ratio": 1.6428571428571428, "no_speech_prob": 8.800661817076616e-06}, {"id": 1003, "seek": 493000, "start": 4953.0, "end": 4957.0, "text": " And they are not part of the back propagation and stochastic gradient descent.", "tokens": [50364, 814, 366, 4305, 6806, 13, 50464, 50464, 400, 370, 341, 7166, 307, 4818, 293, 538, 7576, 2074, 11, 445, 411, 294, 9953, 51, 284, 339, 13, 50664, 50664, 823, 11, 264, 9834, 295, 363, 852, 45, 24440, 502, 35, 366, 264, 15546, 293, 264, 9861, 510, 13, 50964, 50964, 400, 550, 264, 2614, 914, 293, 2614, 21977, 366, 1219, 9204, 433, 294, 9953, 51, 284, 339, 297, 4726, 3474, 1503, 13, 51214, 51214, 400, 613, 9204, 433, 366, 8895, 1228, 21510, 2684, 4274, 510, 20803, 13, 51514, 51514, 400, 436, 366, 406, 644, 295, 264, 646, 38377, 293, 342, 8997, 2750, 16235, 23475, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09021449972082067, "compression_ratio": 1.6428571428571428, "no_speech_prob": 8.800661817076616e-06}, {"id": 1004, "seek": 495700, "start": 4957.0, "end": 4960.0, "text": " So they are not sort of like parameters of this layer.", "tokens": [50364, 407, 436, 366, 406, 1333, 295, 411, 9834, 295, 341, 4583, 13, 50514, 50514, 400, 300, 311, 983, 562, 321, 362, 9834, 510, 11, 321, 787, 2736, 15546, 293, 9861, 13, 50714, 50714, 492, 360, 406, 2736, 264, 914, 293, 264, 21977, 13, 50864, 50864, 639, 307, 8895, 1333, 295, 411, 19501, 510, 633, 2128, 1320, 1228, 21510, 2684, 4274, 13, 51214, 51214, 407, 300, 311, 264, 5883, 2144, 13, 51364, 51364, 823, 11, 294, 257, 2128, 1320, 11, 498, 321, 366, 3097, 11, 550, 321, 764, 264, 914, 293, 264, 21977, 14109, 538, 264, 15245, 13, 51664, 51664, 961, 385, 2235, 493, 264, 3035, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07830103453215179, "compression_ratio": 1.756, "no_speech_prob": 8.939438885136042e-06}, {"id": 1005, "seek": 495700, "start": 4960.0, "end": 4964.0, "text": " And that's why when we have parameters here, we only return gamma and beta.", "tokens": [50364, 407, 436, 366, 406, 1333, 295, 411, 9834, 295, 341, 4583, 13, 50514, 50514, 400, 300, 311, 983, 562, 321, 362, 9834, 510, 11, 321, 787, 2736, 15546, 293, 9861, 13, 50714, 50714, 492, 360, 406, 2736, 264, 914, 293, 264, 21977, 13, 50864, 50864, 639, 307, 8895, 1333, 295, 411, 19501, 510, 633, 2128, 1320, 1228, 21510, 2684, 4274, 13, 51214, 51214, 407, 300, 311, 264, 5883, 2144, 13, 51364, 51364, 823, 11, 294, 257, 2128, 1320, 11, 498, 321, 366, 3097, 11, 550, 321, 764, 264, 914, 293, 264, 21977, 14109, 538, 264, 15245, 13, 51664, 51664, 961, 385, 2235, 493, 264, 3035, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07830103453215179, "compression_ratio": 1.756, "no_speech_prob": 8.939438885136042e-06}, {"id": 1006, "seek": 495700, "start": 4964.0, "end": 4967.0, "text": " We do not return the mean and the variance.", "tokens": [50364, 407, 436, 366, 406, 1333, 295, 411, 9834, 295, 341, 4583, 13, 50514, 50514, 400, 300, 311, 983, 562, 321, 362, 9834, 510, 11, 321, 787, 2736, 15546, 293, 9861, 13, 50714, 50714, 492, 360, 406, 2736, 264, 914, 293, 264, 21977, 13, 50864, 50864, 639, 307, 8895, 1333, 295, 411, 19501, 510, 633, 2128, 1320, 1228, 21510, 2684, 4274, 13, 51214, 51214, 407, 300, 311, 264, 5883, 2144, 13, 51364, 51364, 823, 11, 294, 257, 2128, 1320, 11, 498, 321, 366, 3097, 11, 550, 321, 764, 264, 914, 293, 264, 21977, 14109, 538, 264, 15245, 13, 51664, 51664, 961, 385, 2235, 493, 264, 3035, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07830103453215179, "compression_ratio": 1.756, "no_speech_prob": 8.939438885136042e-06}, {"id": 1007, "seek": 495700, "start": 4967.0, "end": 4974.0, "text": " This is trained sort of like internally here every forward pass using exponential moving average.", "tokens": [50364, 407, 436, 366, 406, 1333, 295, 411, 9834, 295, 341, 4583, 13, 50514, 50514, 400, 300, 311, 983, 562, 321, 362, 9834, 510, 11, 321, 787, 2736, 15546, 293, 9861, 13, 50714, 50714, 492, 360, 406, 2736, 264, 914, 293, 264, 21977, 13, 50864, 50864, 639, 307, 8895, 1333, 295, 411, 19501, 510, 633, 2128, 1320, 1228, 21510, 2684, 4274, 13, 51214, 51214, 407, 300, 311, 264, 5883, 2144, 13, 51364, 51364, 823, 11, 294, 257, 2128, 1320, 11, 498, 321, 366, 3097, 11, 550, 321, 764, 264, 914, 293, 264, 21977, 14109, 538, 264, 15245, 13, 51664, 51664, 961, 385, 2235, 493, 264, 3035, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07830103453215179, "compression_ratio": 1.756, "no_speech_prob": 8.939438885136042e-06}, {"id": 1008, "seek": 495700, "start": 4974.0, "end": 4977.0, "text": " So that's the initialization.", "tokens": [50364, 407, 436, 366, 406, 1333, 295, 411, 9834, 295, 341, 4583, 13, 50514, 50514, 400, 300, 311, 983, 562, 321, 362, 9834, 510, 11, 321, 787, 2736, 15546, 293, 9861, 13, 50714, 50714, 492, 360, 406, 2736, 264, 914, 293, 264, 21977, 13, 50864, 50864, 639, 307, 8895, 1333, 295, 411, 19501, 510, 633, 2128, 1320, 1228, 21510, 2684, 4274, 13, 51214, 51214, 407, 300, 311, 264, 5883, 2144, 13, 51364, 51364, 823, 11, 294, 257, 2128, 1320, 11, 498, 321, 366, 3097, 11, 550, 321, 764, 264, 914, 293, 264, 21977, 14109, 538, 264, 15245, 13, 51664, 51664, 961, 385, 2235, 493, 264, 3035, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07830103453215179, "compression_ratio": 1.756, "no_speech_prob": 8.939438885136042e-06}, {"id": 1009, "seek": 495700, "start": 4977.0, "end": 4983.0, "text": " Now, in a forward pass, if we are training, then we use the mean and the variance estimated by the batch.", "tokens": [50364, 407, 436, 366, 406, 1333, 295, 411, 9834, 295, 341, 4583, 13, 50514, 50514, 400, 300, 311, 983, 562, 321, 362, 9834, 510, 11, 321, 787, 2736, 15546, 293, 9861, 13, 50714, 50714, 492, 360, 406, 2736, 264, 914, 293, 264, 21977, 13, 50864, 50864, 639, 307, 8895, 1333, 295, 411, 19501, 510, 633, 2128, 1320, 1228, 21510, 2684, 4274, 13, 51214, 51214, 407, 300, 311, 264, 5883, 2144, 13, 51364, 51364, 823, 11, 294, 257, 2128, 1320, 11, 498, 321, 366, 3097, 11, 550, 321, 764, 264, 914, 293, 264, 21977, 14109, 538, 264, 15245, 13, 51664, 51664, 961, 385, 2235, 493, 264, 3035, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07830103453215179, "compression_ratio": 1.756, "no_speech_prob": 8.939438885136042e-06}, {"id": 1010, "seek": 495700, "start": 4983.0, "end": 4986.0, "text": " Let me pull up the paper here.", "tokens": [50364, 407, 436, 366, 406, 1333, 295, 411, 9834, 295, 341, 4583, 13, 50514, 50514, 400, 300, 311, 983, 562, 321, 362, 9834, 510, 11, 321, 787, 2736, 15546, 293, 9861, 13, 50714, 50714, 492, 360, 406, 2736, 264, 914, 293, 264, 21977, 13, 50864, 50864, 639, 307, 8895, 1333, 295, 411, 19501, 510, 633, 2128, 1320, 1228, 21510, 2684, 4274, 13, 51214, 51214, 407, 300, 311, 264, 5883, 2144, 13, 51364, 51364, 823, 11, 294, 257, 2128, 1320, 11, 498, 321, 366, 3097, 11, 550, 321, 764, 264, 914, 293, 264, 21977, 14109, 538, 264, 15245, 13, 51664, 51664, 961, 385, 2235, 493, 264, 3035, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07830103453215179, "compression_ratio": 1.756, "no_speech_prob": 8.939438885136042e-06}, {"id": 1011, "seek": 498600, "start": 4986.0, "end": 4989.0, "text": " We calculate the mean and the variance.", "tokens": [50364, 492, 8873, 264, 914, 293, 264, 21977, 13, 50514, 50514, 823, 11, 493, 3673, 11, 286, 390, 8017, 990, 264, 3832, 25163, 293, 5145, 2837, 295, 264, 3832, 25163, 510, 294, 264, 2614, 3832, 25163, 2602, 295, 2614, 21977, 13, 50964, 50964, 583, 718, 311, 1524, 264, 3035, 2293, 13, 51064, 51064, 1692, 436, 8873, 264, 21977, 11, 597, 307, 264, 3832, 25163, 8889, 13, 51264, 51264, 400, 300, 311, 437, 311, 4305, 2837, 295, 294, 264, 2614, 21977, 2602, 295, 257, 2614, 3832, 25163, 13, 51564, 51564, 583, 729, 732, 576, 312, 588, 11, 588, 2531, 11, 286, 1697, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06211369378226144, "compression_ratio": 2.0526315789473686, "no_speech_prob": 9.515716556052212e-06}, {"id": 1012, "seek": 498600, "start": 4989.0, "end": 4998.0, "text": " Now, up above, I was estimating the standard deviation and keeping track of the standard deviation here in the running standard deviation instead of running variance.", "tokens": [50364, 492, 8873, 264, 914, 293, 264, 21977, 13, 50514, 50514, 823, 11, 493, 3673, 11, 286, 390, 8017, 990, 264, 3832, 25163, 293, 5145, 2837, 295, 264, 3832, 25163, 510, 294, 264, 2614, 3832, 25163, 2602, 295, 2614, 21977, 13, 50964, 50964, 583, 718, 311, 1524, 264, 3035, 2293, 13, 51064, 51064, 1692, 436, 8873, 264, 21977, 11, 597, 307, 264, 3832, 25163, 8889, 13, 51264, 51264, 400, 300, 311, 437, 311, 4305, 2837, 295, 294, 264, 2614, 21977, 2602, 295, 257, 2614, 3832, 25163, 13, 51564, 51564, 583, 729, 732, 576, 312, 588, 11, 588, 2531, 11, 286, 1697, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06211369378226144, "compression_ratio": 2.0526315789473686, "no_speech_prob": 9.515716556052212e-06}, {"id": 1013, "seek": 498600, "start": 4998.0, "end": 5000.0, "text": " But let's follow the paper exactly.", "tokens": [50364, 492, 8873, 264, 914, 293, 264, 21977, 13, 50514, 50514, 823, 11, 493, 3673, 11, 286, 390, 8017, 990, 264, 3832, 25163, 293, 5145, 2837, 295, 264, 3832, 25163, 510, 294, 264, 2614, 3832, 25163, 2602, 295, 2614, 21977, 13, 50964, 50964, 583, 718, 311, 1524, 264, 3035, 2293, 13, 51064, 51064, 1692, 436, 8873, 264, 21977, 11, 597, 307, 264, 3832, 25163, 8889, 13, 51264, 51264, 400, 300, 311, 437, 311, 4305, 2837, 295, 294, 264, 2614, 21977, 2602, 295, 257, 2614, 3832, 25163, 13, 51564, 51564, 583, 729, 732, 576, 312, 588, 11, 588, 2531, 11, 286, 1697, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06211369378226144, "compression_ratio": 2.0526315789473686, "no_speech_prob": 9.515716556052212e-06}, {"id": 1014, "seek": 498600, "start": 5000.0, "end": 5004.0, "text": " Here they calculate the variance, which is the standard deviation squared.", "tokens": [50364, 492, 8873, 264, 914, 293, 264, 21977, 13, 50514, 50514, 823, 11, 493, 3673, 11, 286, 390, 8017, 990, 264, 3832, 25163, 293, 5145, 2837, 295, 264, 3832, 25163, 510, 294, 264, 2614, 3832, 25163, 2602, 295, 2614, 21977, 13, 50964, 50964, 583, 718, 311, 1524, 264, 3035, 2293, 13, 51064, 51064, 1692, 436, 8873, 264, 21977, 11, 597, 307, 264, 3832, 25163, 8889, 13, 51264, 51264, 400, 300, 311, 437, 311, 4305, 2837, 295, 294, 264, 2614, 21977, 2602, 295, 257, 2614, 3832, 25163, 13, 51564, 51564, 583, 729, 732, 576, 312, 588, 11, 588, 2531, 11, 286, 1697, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06211369378226144, "compression_ratio": 2.0526315789473686, "no_speech_prob": 9.515716556052212e-06}, {"id": 1015, "seek": 498600, "start": 5004.0, "end": 5010.0, "text": " And that's what's kept track of in the running variance instead of a running standard deviation.", "tokens": [50364, 492, 8873, 264, 914, 293, 264, 21977, 13, 50514, 50514, 823, 11, 493, 3673, 11, 286, 390, 8017, 990, 264, 3832, 25163, 293, 5145, 2837, 295, 264, 3832, 25163, 510, 294, 264, 2614, 3832, 25163, 2602, 295, 2614, 21977, 13, 50964, 50964, 583, 718, 311, 1524, 264, 3035, 2293, 13, 51064, 51064, 1692, 436, 8873, 264, 21977, 11, 597, 307, 264, 3832, 25163, 8889, 13, 51264, 51264, 400, 300, 311, 437, 311, 4305, 2837, 295, 294, 264, 2614, 21977, 2602, 295, 257, 2614, 3832, 25163, 13, 51564, 51564, 583, 729, 732, 576, 312, 588, 11, 588, 2531, 11, 286, 1697, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06211369378226144, "compression_ratio": 2.0526315789473686, "no_speech_prob": 9.515716556052212e-06}, {"id": 1016, "seek": 498600, "start": 5010.0, "end": 5014.0, "text": " But those two would be very, very similar, I believe.", "tokens": [50364, 492, 8873, 264, 914, 293, 264, 21977, 13, 50514, 50514, 823, 11, 493, 3673, 11, 286, 390, 8017, 990, 264, 3832, 25163, 293, 5145, 2837, 295, 264, 3832, 25163, 510, 294, 264, 2614, 3832, 25163, 2602, 295, 2614, 21977, 13, 50964, 50964, 583, 718, 311, 1524, 264, 3035, 2293, 13, 51064, 51064, 1692, 436, 8873, 264, 21977, 11, 597, 307, 264, 3832, 25163, 8889, 13, 51264, 51264, 400, 300, 311, 437, 311, 4305, 2837, 295, 294, 264, 2614, 21977, 2602, 295, 257, 2614, 3832, 25163, 13, 51564, 51564, 583, 729, 732, 576, 312, 588, 11, 588, 2531, 11, 286, 1697, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06211369378226144, "compression_ratio": 2.0526315789473686, "no_speech_prob": 9.515716556052212e-06}, {"id": 1017, "seek": 501400, "start": 5014.0, "end": 5017.0, "text": " If we are not training, then we use the running mean and variance.", "tokens": [50364, 759, 321, 366, 406, 3097, 11, 550, 321, 764, 264, 2614, 914, 293, 21977, 13, 50514, 50514, 492, 2710, 1125, 13, 50614, 50614, 400, 550, 510, 286, 669, 28258, 264, 5598, 295, 341, 4583, 13, 50764, 50764, 400, 286, 478, 611, 49602, 309, 281, 364, 19667, 1219, 5893, 484, 13, 50914, 50914, 823, 11, 5893, 484, 307, 746, 300, 286, 478, 1228, 294, 527, 16679, 510, 13, 51114, 51114, 639, 307, 406, 437, 291, 576, 915, 294, 9953, 51, 284, 339, 13, 51214, 51214, 492, 366, 4748, 31219, 990, 490, 309, 13, 51314, 51314, 286, 478, 4084, 257, 5893, 484, 570, 286, 576, 411, 281, 588, 3612, 6909, 439, 729, 9102, 370, 300, 321, 393, 1884, 12523, 295, 552, 293, 7542, 552, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05884108580942229, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.4284620192483999e-05}, {"id": 1018, "seek": 501400, "start": 5017.0, "end": 5019.0, "text": " We normalize.", "tokens": [50364, 759, 321, 366, 406, 3097, 11, 550, 321, 764, 264, 2614, 914, 293, 21977, 13, 50514, 50514, 492, 2710, 1125, 13, 50614, 50614, 400, 550, 510, 286, 669, 28258, 264, 5598, 295, 341, 4583, 13, 50764, 50764, 400, 286, 478, 611, 49602, 309, 281, 364, 19667, 1219, 5893, 484, 13, 50914, 50914, 823, 11, 5893, 484, 307, 746, 300, 286, 478, 1228, 294, 527, 16679, 510, 13, 51114, 51114, 639, 307, 406, 437, 291, 576, 915, 294, 9953, 51, 284, 339, 13, 51214, 51214, 492, 366, 4748, 31219, 990, 490, 309, 13, 51314, 51314, 286, 478, 4084, 257, 5893, 484, 570, 286, 576, 411, 281, 588, 3612, 6909, 439, 729, 9102, 370, 300, 321, 393, 1884, 12523, 295, 552, 293, 7542, 552, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05884108580942229, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.4284620192483999e-05}, {"id": 1019, "seek": 501400, "start": 5019.0, "end": 5022.0, "text": " And then here I am calculating the output of this layer.", "tokens": [50364, 759, 321, 366, 406, 3097, 11, 550, 321, 764, 264, 2614, 914, 293, 21977, 13, 50514, 50514, 492, 2710, 1125, 13, 50614, 50614, 400, 550, 510, 286, 669, 28258, 264, 5598, 295, 341, 4583, 13, 50764, 50764, 400, 286, 478, 611, 49602, 309, 281, 364, 19667, 1219, 5893, 484, 13, 50914, 50914, 823, 11, 5893, 484, 307, 746, 300, 286, 478, 1228, 294, 527, 16679, 510, 13, 51114, 51114, 639, 307, 406, 437, 291, 576, 915, 294, 9953, 51, 284, 339, 13, 51214, 51214, 492, 366, 4748, 31219, 990, 490, 309, 13, 51314, 51314, 286, 478, 4084, 257, 5893, 484, 570, 286, 576, 411, 281, 588, 3612, 6909, 439, 729, 9102, 370, 300, 321, 393, 1884, 12523, 295, 552, 293, 7542, 552, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05884108580942229, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.4284620192483999e-05}, {"id": 1020, "seek": 501400, "start": 5022.0, "end": 5025.0, "text": " And I'm also assigning it to an attribute called dot out.", "tokens": [50364, 759, 321, 366, 406, 3097, 11, 550, 321, 764, 264, 2614, 914, 293, 21977, 13, 50514, 50514, 492, 2710, 1125, 13, 50614, 50614, 400, 550, 510, 286, 669, 28258, 264, 5598, 295, 341, 4583, 13, 50764, 50764, 400, 286, 478, 611, 49602, 309, 281, 364, 19667, 1219, 5893, 484, 13, 50914, 50914, 823, 11, 5893, 484, 307, 746, 300, 286, 478, 1228, 294, 527, 16679, 510, 13, 51114, 51114, 639, 307, 406, 437, 291, 576, 915, 294, 9953, 51, 284, 339, 13, 51214, 51214, 492, 366, 4748, 31219, 990, 490, 309, 13, 51314, 51314, 286, 478, 4084, 257, 5893, 484, 570, 286, 576, 411, 281, 588, 3612, 6909, 439, 729, 9102, 370, 300, 321, 393, 1884, 12523, 295, 552, 293, 7542, 552, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05884108580942229, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.4284620192483999e-05}, {"id": 1021, "seek": 501400, "start": 5025.0, "end": 5029.0, "text": " Now, dot out is something that I'm using in our modules here.", "tokens": [50364, 759, 321, 366, 406, 3097, 11, 550, 321, 764, 264, 2614, 914, 293, 21977, 13, 50514, 50514, 492, 2710, 1125, 13, 50614, 50614, 400, 550, 510, 286, 669, 28258, 264, 5598, 295, 341, 4583, 13, 50764, 50764, 400, 286, 478, 611, 49602, 309, 281, 364, 19667, 1219, 5893, 484, 13, 50914, 50914, 823, 11, 5893, 484, 307, 746, 300, 286, 478, 1228, 294, 527, 16679, 510, 13, 51114, 51114, 639, 307, 406, 437, 291, 576, 915, 294, 9953, 51, 284, 339, 13, 51214, 51214, 492, 366, 4748, 31219, 990, 490, 309, 13, 51314, 51314, 286, 478, 4084, 257, 5893, 484, 570, 286, 576, 411, 281, 588, 3612, 6909, 439, 729, 9102, 370, 300, 321, 393, 1884, 12523, 295, 552, 293, 7542, 552, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05884108580942229, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.4284620192483999e-05}, {"id": 1022, "seek": 501400, "start": 5029.0, "end": 5031.0, "text": " This is not what you would find in PyTorch.", "tokens": [50364, 759, 321, 366, 406, 3097, 11, 550, 321, 764, 264, 2614, 914, 293, 21977, 13, 50514, 50514, 492, 2710, 1125, 13, 50614, 50614, 400, 550, 510, 286, 669, 28258, 264, 5598, 295, 341, 4583, 13, 50764, 50764, 400, 286, 478, 611, 49602, 309, 281, 364, 19667, 1219, 5893, 484, 13, 50914, 50914, 823, 11, 5893, 484, 307, 746, 300, 286, 478, 1228, 294, 527, 16679, 510, 13, 51114, 51114, 639, 307, 406, 437, 291, 576, 915, 294, 9953, 51, 284, 339, 13, 51214, 51214, 492, 366, 4748, 31219, 990, 490, 309, 13, 51314, 51314, 286, 478, 4084, 257, 5893, 484, 570, 286, 576, 411, 281, 588, 3612, 6909, 439, 729, 9102, 370, 300, 321, 393, 1884, 12523, 295, 552, 293, 7542, 552, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05884108580942229, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.4284620192483999e-05}, {"id": 1023, "seek": 501400, "start": 5031.0, "end": 5033.0, "text": " We are slightly deviating from it.", "tokens": [50364, 759, 321, 366, 406, 3097, 11, 550, 321, 764, 264, 2614, 914, 293, 21977, 13, 50514, 50514, 492, 2710, 1125, 13, 50614, 50614, 400, 550, 510, 286, 669, 28258, 264, 5598, 295, 341, 4583, 13, 50764, 50764, 400, 286, 478, 611, 49602, 309, 281, 364, 19667, 1219, 5893, 484, 13, 50914, 50914, 823, 11, 5893, 484, 307, 746, 300, 286, 478, 1228, 294, 527, 16679, 510, 13, 51114, 51114, 639, 307, 406, 437, 291, 576, 915, 294, 9953, 51, 284, 339, 13, 51214, 51214, 492, 366, 4748, 31219, 990, 490, 309, 13, 51314, 51314, 286, 478, 4084, 257, 5893, 484, 570, 286, 576, 411, 281, 588, 3612, 6909, 439, 729, 9102, 370, 300, 321, 393, 1884, 12523, 295, 552, 293, 7542, 552, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05884108580942229, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.4284620192483999e-05}, {"id": 1024, "seek": 501400, "start": 5033.0, "end": 5041.0, "text": " I'm creating a dot out because I would like to very easily maintain all those variables so that we can create statistics of them and plot them.", "tokens": [50364, 759, 321, 366, 406, 3097, 11, 550, 321, 764, 264, 2614, 914, 293, 21977, 13, 50514, 50514, 492, 2710, 1125, 13, 50614, 50614, 400, 550, 510, 286, 669, 28258, 264, 5598, 295, 341, 4583, 13, 50764, 50764, 400, 286, 478, 611, 49602, 309, 281, 364, 19667, 1219, 5893, 484, 13, 50914, 50914, 823, 11, 5893, 484, 307, 746, 300, 286, 478, 1228, 294, 527, 16679, 510, 13, 51114, 51114, 639, 307, 406, 437, 291, 576, 915, 294, 9953, 51, 284, 339, 13, 51214, 51214, 492, 366, 4748, 31219, 990, 490, 309, 13, 51314, 51314, 286, 478, 4084, 257, 5893, 484, 570, 286, 576, 411, 281, 588, 3612, 6909, 439, 729, 9102, 370, 300, 321, 393, 1884, 12523, 295, 552, 293, 7542, 552, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05884108580942229, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.4284620192483999e-05}, {"id": 1025, "seek": 504100, "start": 5041.0, "end": 5045.0, "text": " But PyTorch and modules will not have a dot out attribute.", "tokens": [50364, 583, 9953, 51, 284, 339, 293, 16679, 486, 406, 362, 257, 5893, 484, 19667, 13, 50564, 50564, 400, 2721, 11, 510, 321, 366, 25113, 264, 9204, 433, 1228, 11, 797, 11, 382, 286, 2835, 11, 21510, 2684, 4274, 11, 2212, 264, 5649, 11244, 13, 50964, 50964, 400, 8906, 11, 291, 603, 3449, 300, 286, 478, 1228, 264, 27822, 13, 77, 664, 6206, 4319, 6598, 13, 51164, 51164, 400, 286, 478, 884, 341, 570, 498, 321, 500, 380, 764, 341, 11, 550, 9953, 51, 284, 339, 486, 722, 2390, 484, 364, 2302, 28270, 4295, 484, 295, 613, 10688, 830, 570, 309, 307, 9650, 300, 321, 486, 4728, 818, 309, 5893, 23897, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07718538201373557, "compression_ratio": 1.6713286713286712, "no_speech_prob": 9.515824785921723e-06}, {"id": 1026, "seek": 504100, "start": 5045.0, "end": 5053.0, "text": " And finally, here we are updating the buffers using, again, as I mentioned, exponential moving average, given the provided momentum.", "tokens": [50364, 583, 9953, 51, 284, 339, 293, 16679, 486, 406, 362, 257, 5893, 484, 19667, 13, 50564, 50564, 400, 2721, 11, 510, 321, 366, 25113, 264, 9204, 433, 1228, 11, 797, 11, 382, 286, 2835, 11, 21510, 2684, 4274, 11, 2212, 264, 5649, 11244, 13, 50964, 50964, 400, 8906, 11, 291, 603, 3449, 300, 286, 478, 1228, 264, 27822, 13, 77, 664, 6206, 4319, 6598, 13, 51164, 51164, 400, 286, 478, 884, 341, 570, 498, 321, 500, 380, 764, 341, 11, 550, 9953, 51, 284, 339, 486, 722, 2390, 484, 364, 2302, 28270, 4295, 484, 295, 613, 10688, 830, 570, 309, 307, 9650, 300, 321, 486, 4728, 818, 309, 5893, 23897, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07718538201373557, "compression_ratio": 1.6713286713286712, "no_speech_prob": 9.515824785921723e-06}, {"id": 1027, "seek": 504100, "start": 5053.0, "end": 5057.0, "text": " And importantly, you'll notice that I'm using the torch.nograd context manager.", "tokens": [50364, 583, 9953, 51, 284, 339, 293, 16679, 486, 406, 362, 257, 5893, 484, 19667, 13, 50564, 50564, 400, 2721, 11, 510, 321, 366, 25113, 264, 9204, 433, 1228, 11, 797, 11, 382, 286, 2835, 11, 21510, 2684, 4274, 11, 2212, 264, 5649, 11244, 13, 50964, 50964, 400, 8906, 11, 291, 603, 3449, 300, 286, 478, 1228, 264, 27822, 13, 77, 664, 6206, 4319, 6598, 13, 51164, 51164, 400, 286, 478, 884, 341, 570, 498, 321, 500, 380, 764, 341, 11, 550, 9953, 51, 284, 339, 486, 722, 2390, 484, 364, 2302, 28270, 4295, 484, 295, 613, 10688, 830, 570, 309, 307, 9650, 300, 321, 486, 4728, 818, 309, 5893, 23897, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07718538201373557, "compression_ratio": 1.6713286713286712, "no_speech_prob": 9.515824785921723e-06}, {"id": 1028, "seek": 504100, "start": 5057.0, "end": 5068.0, "text": " And I'm doing this because if we don't use this, then PyTorch will start building out an entire computational graph out of these tensors because it is expecting that we will eventually call it dot backward.", "tokens": [50364, 583, 9953, 51, 284, 339, 293, 16679, 486, 406, 362, 257, 5893, 484, 19667, 13, 50564, 50564, 400, 2721, 11, 510, 321, 366, 25113, 264, 9204, 433, 1228, 11, 797, 11, 382, 286, 2835, 11, 21510, 2684, 4274, 11, 2212, 264, 5649, 11244, 13, 50964, 50964, 400, 8906, 11, 291, 603, 3449, 300, 286, 478, 1228, 264, 27822, 13, 77, 664, 6206, 4319, 6598, 13, 51164, 51164, 400, 286, 478, 884, 341, 570, 498, 321, 500, 380, 764, 341, 11, 550, 9953, 51, 284, 339, 486, 722, 2390, 484, 364, 2302, 28270, 4295, 484, 295, 613, 10688, 830, 570, 309, 307, 9650, 300, 321, 486, 4728, 818, 309, 5893, 23897, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07718538201373557, "compression_ratio": 1.6713286713286712, "no_speech_prob": 9.515824785921723e-06}, {"id": 1029, "seek": 506800, "start": 5068.0, "end": 5072.0, "text": " But we are never going to be calling dot backward on anything that includes running mean and running variance.", "tokens": [50364, 583, 321, 366, 1128, 516, 281, 312, 5141, 5893, 23897, 322, 1340, 300, 5974, 2614, 914, 293, 2614, 21977, 13, 50564, 50564, 407, 300, 311, 983, 321, 643, 281, 764, 341, 4319, 6598, 370, 300, 321, 366, 406, 1333, 295, 14916, 552, 1228, 439, 341, 4497, 4675, 13, 50964, 50964, 407, 341, 486, 652, 309, 544, 7148, 13, 51064, 51064, 400, 309, 311, 445, 3585, 9953, 51, 284, 339, 300, 456, 486, 312, 572, 23897, 13, 51164, 51164, 492, 445, 362, 257, 3840, 295, 10688, 830, 13, 492, 528, 281, 5623, 552, 13, 51264, 51264, 663, 311, 309, 13, 51364, 51364, 400, 550, 321, 2736, 13, 51464, 51464, 2264, 11, 586, 29053, 760, 11, 321, 362, 264, 1266, 39, 4583, 13, 51614, 51614, 639, 307, 588, 11, 588, 2531, 281, 27822, 13, 3279, 39, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.083341077077303, "compression_ratio": 1.693069306930693, "no_speech_prob": 5.014543148718076e-06}, {"id": 1030, "seek": 506800, "start": 5072.0, "end": 5080.0, "text": " So that's why we need to use this context manager so that we are not sort of maintaining them using all this additional memory.", "tokens": [50364, 583, 321, 366, 1128, 516, 281, 312, 5141, 5893, 23897, 322, 1340, 300, 5974, 2614, 914, 293, 2614, 21977, 13, 50564, 50564, 407, 300, 311, 983, 321, 643, 281, 764, 341, 4319, 6598, 370, 300, 321, 366, 406, 1333, 295, 14916, 552, 1228, 439, 341, 4497, 4675, 13, 50964, 50964, 407, 341, 486, 652, 309, 544, 7148, 13, 51064, 51064, 400, 309, 311, 445, 3585, 9953, 51, 284, 339, 300, 456, 486, 312, 572, 23897, 13, 51164, 51164, 492, 445, 362, 257, 3840, 295, 10688, 830, 13, 492, 528, 281, 5623, 552, 13, 51264, 51264, 663, 311, 309, 13, 51364, 51364, 400, 550, 321, 2736, 13, 51464, 51464, 2264, 11, 586, 29053, 760, 11, 321, 362, 264, 1266, 39, 4583, 13, 51614, 51614, 639, 307, 588, 11, 588, 2531, 281, 27822, 13, 3279, 39, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.083341077077303, "compression_ratio": 1.693069306930693, "no_speech_prob": 5.014543148718076e-06}, {"id": 1031, "seek": 506800, "start": 5080.0, "end": 5082.0, "text": " So this will make it more efficient.", "tokens": [50364, 583, 321, 366, 1128, 516, 281, 312, 5141, 5893, 23897, 322, 1340, 300, 5974, 2614, 914, 293, 2614, 21977, 13, 50564, 50564, 407, 300, 311, 983, 321, 643, 281, 764, 341, 4319, 6598, 370, 300, 321, 366, 406, 1333, 295, 14916, 552, 1228, 439, 341, 4497, 4675, 13, 50964, 50964, 407, 341, 486, 652, 309, 544, 7148, 13, 51064, 51064, 400, 309, 311, 445, 3585, 9953, 51, 284, 339, 300, 456, 486, 312, 572, 23897, 13, 51164, 51164, 492, 445, 362, 257, 3840, 295, 10688, 830, 13, 492, 528, 281, 5623, 552, 13, 51264, 51264, 663, 311, 309, 13, 51364, 51364, 400, 550, 321, 2736, 13, 51464, 51464, 2264, 11, 586, 29053, 760, 11, 321, 362, 264, 1266, 39, 4583, 13, 51614, 51614, 639, 307, 588, 11, 588, 2531, 281, 27822, 13, 3279, 39, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.083341077077303, "compression_ratio": 1.693069306930693, "no_speech_prob": 5.014543148718076e-06}, {"id": 1032, "seek": 506800, "start": 5082.0, "end": 5084.0, "text": " And it's just telling PyTorch that there will be no backward.", "tokens": [50364, 583, 321, 366, 1128, 516, 281, 312, 5141, 5893, 23897, 322, 1340, 300, 5974, 2614, 914, 293, 2614, 21977, 13, 50564, 50564, 407, 300, 311, 983, 321, 643, 281, 764, 341, 4319, 6598, 370, 300, 321, 366, 406, 1333, 295, 14916, 552, 1228, 439, 341, 4497, 4675, 13, 50964, 50964, 407, 341, 486, 652, 309, 544, 7148, 13, 51064, 51064, 400, 309, 311, 445, 3585, 9953, 51, 284, 339, 300, 456, 486, 312, 572, 23897, 13, 51164, 51164, 492, 445, 362, 257, 3840, 295, 10688, 830, 13, 492, 528, 281, 5623, 552, 13, 51264, 51264, 663, 311, 309, 13, 51364, 51364, 400, 550, 321, 2736, 13, 51464, 51464, 2264, 11, 586, 29053, 760, 11, 321, 362, 264, 1266, 39, 4583, 13, 51614, 51614, 639, 307, 588, 11, 588, 2531, 281, 27822, 13, 3279, 39, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.083341077077303, "compression_ratio": 1.693069306930693, "no_speech_prob": 5.014543148718076e-06}, {"id": 1033, "seek": 506800, "start": 5084.0, "end": 5086.0, "text": " We just have a bunch of tensors. We want to update them.", "tokens": [50364, 583, 321, 366, 1128, 516, 281, 312, 5141, 5893, 23897, 322, 1340, 300, 5974, 2614, 914, 293, 2614, 21977, 13, 50564, 50564, 407, 300, 311, 983, 321, 643, 281, 764, 341, 4319, 6598, 370, 300, 321, 366, 406, 1333, 295, 14916, 552, 1228, 439, 341, 4497, 4675, 13, 50964, 50964, 407, 341, 486, 652, 309, 544, 7148, 13, 51064, 51064, 400, 309, 311, 445, 3585, 9953, 51, 284, 339, 300, 456, 486, 312, 572, 23897, 13, 51164, 51164, 492, 445, 362, 257, 3840, 295, 10688, 830, 13, 492, 528, 281, 5623, 552, 13, 51264, 51264, 663, 311, 309, 13, 51364, 51364, 400, 550, 321, 2736, 13, 51464, 51464, 2264, 11, 586, 29053, 760, 11, 321, 362, 264, 1266, 39, 4583, 13, 51614, 51614, 639, 307, 588, 11, 588, 2531, 281, 27822, 13, 3279, 39, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.083341077077303, "compression_ratio": 1.693069306930693, "no_speech_prob": 5.014543148718076e-06}, {"id": 1034, "seek": 506800, "start": 5086.0, "end": 5088.0, "text": " That's it.", "tokens": [50364, 583, 321, 366, 1128, 516, 281, 312, 5141, 5893, 23897, 322, 1340, 300, 5974, 2614, 914, 293, 2614, 21977, 13, 50564, 50564, 407, 300, 311, 983, 321, 643, 281, 764, 341, 4319, 6598, 370, 300, 321, 366, 406, 1333, 295, 14916, 552, 1228, 439, 341, 4497, 4675, 13, 50964, 50964, 407, 341, 486, 652, 309, 544, 7148, 13, 51064, 51064, 400, 309, 311, 445, 3585, 9953, 51, 284, 339, 300, 456, 486, 312, 572, 23897, 13, 51164, 51164, 492, 445, 362, 257, 3840, 295, 10688, 830, 13, 492, 528, 281, 5623, 552, 13, 51264, 51264, 663, 311, 309, 13, 51364, 51364, 400, 550, 321, 2736, 13, 51464, 51464, 2264, 11, 586, 29053, 760, 11, 321, 362, 264, 1266, 39, 4583, 13, 51614, 51614, 639, 307, 588, 11, 588, 2531, 281, 27822, 13, 3279, 39, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.083341077077303, "compression_ratio": 1.693069306930693, "no_speech_prob": 5.014543148718076e-06}, {"id": 1035, "seek": 506800, "start": 5088.0, "end": 5090.0, "text": " And then we return.", "tokens": [50364, 583, 321, 366, 1128, 516, 281, 312, 5141, 5893, 23897, 322, 1340, 300, 5974, 2614, 914, 293, 2614, 21977, 13, 50564, 50564, 407, 300, 311, 983, 321, 643, 281, 764, 341, 4319, 6598, 370, 300, 321, 366, 406, 1333, 295, 14916, 552, 1228, 439, 341, 4497, 4675, 13, 50964, 50964, 407, 341, 486, 652, 309, 544, 7148, 13, 51064, 51064, 400, 309, 311, 445, 3585, 9953, 51, 284, 339, 300, 456, 486, 312, 572, 23897, 13, 51164, 51164, 492, 445, 362, 257, 3840, 295, 10688, 830, 13, 492, 528, 281, 5623, 552, 13, 51264, 51264, 663, 311, 309, 13, 51364, 51364, 400, 550, 321, 2736, 13, 51464, 51464, 2264, 11, 586, 29053, 760, 11, 321, 362, 264, 1266, 39, 4583, 13, 51614, 51614, 639, 307, 588, 11, 588, 2531, 281, 27822, 13, 3279, 39, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.083341077077303, "compression_ratio": 1.693069306930693, "no_speech_prob": 5.014543148718076e-06}, {"id": 1036, "seek": 506800, "start": 5090.0, "end": 5093.0, "text": " OK, now scrolling down, we have the 10H layer.", "tokens": [50364, 583, 321, 366, 1128, 516, 281, 312, 5141, 5893, 23897, 322, 1340, 300, 5974, 2614, 914, 293, 2614, 21977, 13, 50564, 50564, 407, 300, 311, 983, 321, 643, 281, 764, 341, 4319, 6598, 370, 300, 321, 366, 406, 1333, 295, 14916, 552, 1228, 439, 341, 4497, 4675, 13, 50964, 50964, 407, 341, 486, 652, 309, 544, 7148, 13, 51064, 51064, 400, 309, 311, 445, 3585, 9953, 51, 284, 339, 300, 456, 486, 312, 572, 23897, 13, 51164, 51164, 492, 445, 362, 257, 3840, 295, 10688, 830, 13, 492, 528, 281, 5623, 552, 13, 51264, 51264, 663, 311, 309, 13, 51364, 51364, 400, 550, 321, 2736, 13, 51464, 51464, 2264, 11, 586, 29053, 760, 11, 321, 362, 264, 1266, 39, 4583, 13, 51614, 51614, 639, 307, 588, 11, 588, 2531, 281, 27822, 13, 3279, 39, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.083341077077303, "compression_ratio": 1.693069306930693, "no_speech_prob": 5.014543148718076e-06}, {"id": 1037, "seek": 506800, "start": 5093.0, "end": 5096.0, "text": " This is very, very similar to torch.10H.", "tokens": [50364, 583, 321, 366, 1128, 516, 281, 312, 5141, 5893, 23897, 322, 1340, 300, 5974, 2614, 914, 293, 2614, 21977, 13, 50564, 50564, 407, 300, 311, 983, 321, 643, 281, 764, 341, 4319, 6598, 370, 300, 321, 366, 406, 1333, 295, 14916, 552, 1228, 439, 341, 4497, 4675, 13, 50964, 50964, 407, 341, 486, 652, 309, 544, 7148, 13, 51064, 51064, 400, 309, 311, 445, 3585, 9953, 51, 284, 339, 300, 456, 486, 312, 572, 23897, 13, 51164, 51164, 492, 445, 362, 257, 3840, 295, 10688, 830, 13, 492, 528, 281, 5623, 552, 13, 51264, 51264, 663, 311, 309, 13, 51364, 51364, 400, 550, 321, 2736, 13, 51464, 51464, 2264, 11, 586, 29053, 760, 11, 321, 362, 264, 1266, 39, 4583, 13, 51614, 51614, 639, 307, 588, 11, 588, 2531, 281, 27822, 13, 3279, 39, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.083341077077303, "compression_ratio": 1.693069306930693, "no_speech_prob": 5.014543148718076e-06}, {"id": 1038, "seek": 509600, "start": 5096.0, "end": 5100.0, "text": " And it doesn't do too much. It just calculates 10H, as you might expect.", "tokens": [50364, 400, 309, 1177, 380, 360, 886, 709, 13, 467, 445, 4322, 1024, 1266, 39, 11, 382, 291, 1062, 2066, 13, 50564, 50564, 407, 300, 311, 27822, 13, 3279, 39, 13, 50664, 50664, 400, 456, 311, 572, 9834, 294, 341, 4583, 13, 50814, 50814, 583, 570, 613, 366, 7914, 11, 309, 586, 3643, 588, 1858, 281, 1333, 295, 411, 8630, 552, 493, 666, 1936, 445, 257, 1329, 13, 51214, 51214, 400, 321, 393, 360, 439, 264, 5883, 14455, 300, 321, 434, 1143, 281, 13, 51364, 51364, 407, 321, 362, 264, 5883, 1333, 295, 12240, 3584, 8141, 13, 51514, 51514, 492, 362, 527, 7914, 293, 321, 393, 818, 552, 5123, 3137, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.05556974494666384, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.8161726984544657e-06}, {"id": 1039, "seek": 509600, "start": 5100.0, "end": 5102.0, "text": " So that's torch.10H.", "tokens": [50364, 400, 309, 1177, 380, 360, 886, 709, 13, 467, 445, 4322, 1024, 1266, 39, 11, 382, 291, 1062, 2066, 13, 50564, 50564, 407, 300, 311, 27822, 13, 3279, 39, 13, 50664, 50664, 400, 456, 311, 572, 9834, 294, 341, 4583, 13, 50814, 50814, 583, 570, 613, 366, 7914, 11, 309, 586, 3643, 588, 1858, 281, 1333, 295, 411, 8630, 552, 493, 666, 1936, 445, 257, 1329, 13, 51214, 51214, 400, 321, 393, 360, 439, 264, 5883, 14455, 300, 321, 434, 1143, 281, 13, 51364, 51364, 407, 321, 362, 264, 5883, 1333, 295, 12240, 3584, 8141, 13, 51514, 51514, 492, 362, 527, 7914, 293, 321, 393, 818, 552, 5123, 3137, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.05556974494666384, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.8161726984544657e-06}, {"id": 1040, "seek": 509600, "start": 5102.0, "end": 5105.0, "text": " And there's no parameters in this layer.", "tokens": [50364, 400, 309, 1177, 380, 360, 886, 709, 13, 467, 445, 4322, 1024, 1266, 39, 11, 382, 291, 1062, 2066, 13, 50564, 50564, 407, 300, 311, 27822, 13, 3279, 39, 13, 50664, 50664, 400, 456, 311, 572, 9834, 294, 341, 4583, 13, 50814, 50814, 583, 570, 613, 366, 7914, 11, 309, 586, 3643, 588, 1858, 281, 1333, 295, 411, 8630, 552, 493, 666, 1936, 445, 257, 1329, 13, 51214, 51214, 400, 321, 393, 360, 439, 264, 5883, 14455, 300, 321, 434, 1143, 281, 13, 51364, 51364, 407, 321, 362, 264, 5883, 1333, 295, 12240, 3584, 8141, 13, 51514, 51514, 492, 362, 527, 7914, 293, 321, 393, 818, 552, 5123, 3137, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.05556974494666384, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.8161726984544657e-06}, {"id": 1041, "seek": 509600, "start": 5105.0, "end": 5113.0, "text": " But because these are layers, it now becomes very easy to sort of like stack them up into basically just a list.", "tokens": [50364, 400, 309, 1177, 380, 360, 886, 709, 13, 467, 445, 4322, 1024, 1266, 39, 11, 382, 291, 1062, 2066, 13, 50564, 50564, 407, 300, 311, 27822, 13, 3279, 39, 13, 50664, 50664, 400, 456, 311, 572, 9834, 294, 341, 4583, 13, 50814, 50814, 583, 570, 613, 366, 7914, 11, 309, 586, 3643, 588, 1858, 281, 1333, 295, 411, 8630, 552, 493, 666, 1936, 445, 257, 1329, 13, 51214, 51214, 400, 321, 393, 360, 439, 264, 5883, 14455, 300, 321, 434, 1143, 281, 13, 51364, 51364, 407, 321, 362, 264, 5883, 1333, 295, 12240, 3584, 8141, 13, 51514, 51514, 492, 362, 527, 7914, 293, 321, 393, 818, 552, 5123, 3137, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.05556974494666384, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.8161726984544657e-06}, {"id": 1042, "seek": 509600, "start": 5113.0, "end": 5116.0, "text": " And we can do all the initializations that we're used to.", "tokens": [50364, 400, 309, 1177, 380, 360, 886, 709, 13, 467, 445, 4322, 1024, 1266, 39, 11, 382, 291, 1062, 2066, 13, 50564, 50564, 407, 300, 311, 27822, 13, 3279, 39, 13, 50664, 50664, 400, 456, 311, 572, 9834, 294, 341, 4583, 13, 50814, 50814, 583, 570, 613, 366, 7914, 11, 309, 586, 3643, 588, 1858, 281, 1333, 295, 411, 8630, 552, 493, 666, 1936, 445, 257, 1329, 13, 51214, 51214, 400, 321, 393, 360, 439, 264, 5883, 14455, 300, 321, 434, 1143, 281, 13, 51364, 51364, 407, 321, 362, 264, 5883, 1333, 295, 12240, 3584, 8141, 13, 51514, 51514, 492, 362, 527, 7914, 293, 321, 393, 818, 552, 5123, 3137, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.05556974494666384, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.8161726984544657e-06}, {"id": 1043, "seek": 509600, "start": 5116.0, "end": 5119.0, "text": " So we have the initial sort of embedding matrix.", "tokens": [50364, 400, 309, 1177, 380, 360, 886, 709, 13, 467, 445, 4322, 1024, 1266, 39, 11, 382, 291, 1062, 2066, 13, 50564, 50564, 407, 300, 311, 27822, 13, 3279, 39, 13, 50664, 50664, 400, 456, 311, 572, 9834, 294, 341, 4583, 13, 50814, 50814, 583, 570, 613, 366, 7914, 11, 309, 586, 3643, 588, 1858, 281, 1333, 295, 411, 8630, 552, 493, 666, 1936, 445, 257, 1329, 13, 51214, 51214, 400, 321, 393, 360, 439, 264, 5883, 14455, 300, 321, 434, 1143, 281, 13, 51364, 51364, 407, 321, 362, 264, 5883, 1333, 295, 12240, 3584, 8141, 13, 51514, 51514, 492, 362, 527, 7914, 293, 321, 393, 818, 552, 5123, 3137, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.05556974494666384, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.8161726984544657e-06}, {"id": 1044, "seek": 509600, "start": 5119.0, "end": 5122.0, "text": " We have our layers and we can call them sequentially.", "tokens": [50364, 400, 309, 1177, 380, 360, 886, 709, 13, 467, 445, 4322, 1024, 1266, 39, 11, 382, 291, 1062, 2066, 13, 50564, 50564, 407, 300, 311, 27822, 13, 3279, 39, 13, 50664, 50664, 400, 456, 311, 572, 9834, 294, 341, 4583, 13, 50814, 50814, 583, 570, 613, 366, 7914, 11, 309, 586, 3643, 588, 1858, 281, 1333, 295, 411, 8630, 552, 493, 666, 1936, 445, 257, 1329, 13, 51214, 51214, 400, 321, 393, 360, 439, 264, 5883, 14455, 300, 321, 434, 1143, 281, 13, 51364, 51364, 407, 321, 362, 264, 5883, 1333, 295, 12240, 3584, 8141, 13, 51514, 51514, 492, 362, 527, 7914, 293, 321, 393, 818, 552, 5123, 3137, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.05556974494666384, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.8161726984544657e-06}, {"id": 1045, "seek": 512200, "start": 5122.0, "end": 5126.0, "text": " And then again, with torch.nograd, there's some initializations here.", "tokens": [50364, 400, 550, 797, 11, 365, 27822, 13, 1771, 7165, 11, 456, 311, 512, 5883, 14455, 510, 13, 50564, 50564, 407, 321, 528, 281, 652, 264, 5598, 2787, 41167, 257, 857, 1570, 6679, 11, 411, 321, 1866, 13, 50764, 50764, 400, 294, 4500, 281, 300, 11, 570, 321, 366, 1228, 257, 2309, 12, 8376, 260, 2120, 388, 11167, 43276, 2044, 510, 11, 51014, 51014, 370, 291, 536, 577, 286, 478, 41376, 8213, 11, 1266, 39, 11, 8213, 11, 1266, 39, 11, 1030, 11458, 11, 51214, 51214, 286, 478, 516, 281, 312, 1228, 264, 6052, 510, 13, 51314, 51314, 400, 286, 478, 516, 281, 862, 365, 341, 294, 257, 1150, 13, 51414, 51414, 407, 291, 603, 536, 577, 562, 321, 1319, 341, 11, 437, 2314, 281, 264, 12523, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08641286660696713, "compression_ratio": 1.657992565055762, "no_speech_prob": 9.817992577154655e-06}, {"id": 1046, "seek": 512200, "start": 5126.0, "end": 5130.0, "text": " So we want to make the output softmax a bit less confident, like we saw.", "tokens": [50364, 400, 550, 797, 11, 365, 27822, 13, 1771, 7165, 11, 456, 311, 512, 5883, 14455, 510, 13, 50564, 50564, 407, 321, 528, 281, 652, 264, 5598, 2787, 41167, 257, 857, 1570, 6679, 11, 411, 321, 1866, 13, 50764, 50764, 400, 294, 4500, 281, 300, 11, 570, 321, 366, 1228, 257, 2309, 12, 8376, 260, 2120, 388, 11167, 43276, 2044, 510, 11, 51014, 51014, 370, 291, 536, 577, 286, 478, 41376, 8213, 11, 1266, 39, 11, 8213, 11, 1266, 39, 11, 1030, 11458, 11, 51214, 51214, 286, 478, 516, 281, 312, 1228, 264, 6052, 510, 13, 51314, 51314, 400, 286, 478, 516, 281, 862, 365, 341, 294, 257, 1150, 13, 51414, 51414, 407, 291, 603, 536, 577, 562, 321, 1319, 341, 11, 437, 2314, 281, 264, 12523, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08641286660696713, "compression_ratio": 1.657992565055762, "no_speech_prob": 9.817992577154655e-06}, {"id": 1047, "seek": 512200, "start": 5130.0, "end": 5135.0, "text": " And in addition to that, because we are using a six-layer multilayer perceptron here,", "tokens": [50364, 400, 550, 797, 11, 365, 27822, 13, 1771, 7165, 11, 456, 311, 512, 5883, 14455, 510, 13, 50564, 50564, 407, 321, 528, 281, 652, 264, 5598, 2787, 41167, 257, 857, 1570, 6679, 11, 411, 321, 1866, 13, 50764, 50764, 400, 294, 4500, 281, 300, 11, 570, 321, 366, 1228, 257, 2309, 12, 8376, 260, 2120, 388, 11167, 43276, 2044, 510, 11, 51014, 51014, 370, 291, 536, 577, 286, 478, 41376, 8213, 11, 1266, 39, 11, 8213, 11, 1266, 39, 11, 1030, 11458, 11, 51214, 51214, 286, 478, 516, 281, 312, 1228, 264, 6052, 510, 13, 51314, 51314, 400, 286, 478, 516, 281, 862, 365, 341, 294, 257, 1150, 13, 51414, 51414, 407, 291, 603, 536, 577, 562, 321, 1319, 341, 11, 437, 2314, 281, 264, 12523, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08641286660696713, "compression_ratio": 1.657992565055762, "no_speech_prob": 9.817992577154655e-06}, {"id": 1048, "seek": 512200, "start": 5135.0, "end": 5139.0, "text": " so you see how I'm stacking linear, 10H, linear, 10H, et cetera,", "tokens": [50364, 400, 550, 797, 11, 365, 27822, 13, 1771, 7165, 11, 456, 311, 512, 5883, 14455, 510, 13, 50564, 50564, 407, 321, 528, 281, 652, 264, 5598, 2787, 41167, 257, 857, 1570, 6679, 11, 411, 321, 1866, 13, 50764, 50764, 400, 294, 4500, 281, 300, 11, 570, 321, 366, 1228, 257, 2309, 12, 8376, 260, 2120, 388, 11167, 43276, 2044, 510, 11, 51014, 51014, 370, 291, 536, 577, 286, 478, 41376, 8213, 11, 1266, 39, 11, 8213, 11, 1266, 39, 11, 1030, 11458, 11, 51214, 51214, 286, 478, 516, 281, 312, 1228, 264, 6052, 510, 13, 51314, 51314, 400, 286, 478, 516, 281, 862, 365, 341, 294, 257, 1150, 13, 51414, 51414, 407, 291, 603, 536, 577, 562, 321, 1319, 341, 11, 437, 2314, 281, 264, 12523, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08641286660696713, "compression_ratio": 1.657992565055762, "no_speech_prob": 9.817992577154655e-06}, {"id": 1049, "seek": 512200, "start": 5139.0, "end": 5141.0, "text": " I'm going to be using the gain here.", "tokens": [50364, 400, 550, 797, 11, 365, 27822, 13, 1771, 7165, 11, 456, 311, 512, 5883, 14455, 510, 13, 50564, 50564, 407, 321, 528, 281, 652, 264, 5598, 2787, 41167, 257, 857, 1570, 6679, 11, 411, 321, 1866, 13, 50764, 50764, 400, 294, 4500, 281, 300, 11, 570, 321, 366, 1228, 257, 2309, 12, 8376, 260, 2120, 388, 11167, 43276, 2044, 510, 11, 51014, 51014, 370, 291, 536, 577, 286, 478, 41376, 8213, 11, 1266, 39, 11, 8213, 11, 1266, 39, 11, 1030, 11458, 11, 51214, 51214, 286, 478, 516, 281, 312, 1228, 264, 6052, 510, 13, 51314, 51314, 400, 286, 478, 516, 281, 862, 365, 341, 294, 257, 1150, 13, 51414, 51414, 407, 291, 603, 536, 577, 562, 321, 1319, 341, 11, 437, 2314, 281, 264, 12523, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08641286660696713, "compression_ratio": 1.657992565055762, "no_speech_prob": 9.817992577154655e-06}, {"id": 1050, "seek": 512200, "start": 5141.0, "end": 5143.0, "text": " And I'm going to play with this in a second.", "tokens": [50364, 400, 550, 797, 11, 365, 27822, 13, 1771, 7165, 11, 456, 311, 512, 5883, 14455, 510, 13, 50564, 50564, 407, 321, 528, 281, 652, 264, 5598, 2787, 41167, 257, 857, 1570, 6679, 11, 411, 321, 1866, 13, 50764, 50764, 400, 294, 4500, 281, 300, 11, 570, 321, 366, 1228, 257, 2309, 12, 8376, 260, 2120, 388, 11167, 43276, 2044, 510, 11, 51014, 51014, 370, 291, 536, 577, 286, 478, 41376, 8213, 11, 1266, 39, 11, 8213, 11, 1266, 39, 11, 1030, 11458, 11, 51214, 51214, 286, 478, 516, 281, 312, 1228, 264, 6052, 510, 13, 51314, 51314, 400, 286, 478, 516, 281, 862, 365, 341, 294, 257, 1150, 13, 51414, 51414, 407, 291, 603, 536, 577, 562, 321, 1319, 341, 11, 437, 2314, 281, 264, 12523, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08641286660696713, "compression_ratio": 1.657992565055762, "no_speech_prob": 9.817992577154655e-06}, {"id": 1051, "seek": 512200, "start": 5143.0, "end": 5147.0, "text": " So you'll see how when we change this, what happens to the statistics.", "tokens": [50364, 400, 550, 797, 11, 365, 27822, 13, 1771, 7165, 11, 456, 311, 512, 5883, 14455, 510, 13, 50564, 50564, 407, 321, 528, 281, 652, 264, 5598, 2787, 41167, 257, 857, 1570, 6679, 11, 411, 321, 1866, 13, 50764, 50764, 400, 294, 4500, 281, 300, 11, 570, 321, 366, 1228, 257, 2309, 12, 8376, 260, 2120, 388, 11167, 43276, 2044, 510, 11, 51014, 51014, 370, 291, 536, 577, 286, 478, 41376, 8213, 11, 1266, 39, 11, 8213, 11, 1266, 39, 11, 1030, 11458, 11, 51214, 51214, 286, 478, 516, 281, 312, 1228, 264, 6052, 510, 13, 51314, 51314, 400, 286, 478, 516, 281, 862, 365, 341, 294, 257, 1150, 13, 51414, 51414, 407, 291, 603, 536, 577, 562, 321, 1319, 341, 11, 437, 2314, 281, 264, 12523, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08641286660696713, "compression_ratio": 1.657992565055762, "no_speech_prob": 9.817992577154655e-06}, {"id": 1052, "seek": 514700, "start": 5147.0, "end": 5152.0, "text": " Finally, the parameters are basically the embedding matrix and all the parameters in all the layers.", "tokens": [50364, 6288, 11, 264, 9834, 366, 1936, 264, 12240, 3584, 8141, 293, 439, 264, 9834, 294, 439, 264, 7914, 13, 50614, 50614, 400, 3449, 510, 11, 286, 478, 1228, 257, 3834, 1329, 44991, 11, 498, 291, 528, 281, 818, 309, 300, 13, 50814, 50814, 583, 337, 633, 4583, 294, 7914, 293, 337, 633, 13075, 294, 1184, 295, 729, 7914, 11, 51064, 51064, 321, 366, 445, 41376, 493, 439, 729, 2522, 11, 439, 729, 9834, 13, 51264, 51264, 823, 11, 294, 3217, 11, 321, 362, 17835, 11, 1360, 9834, 13, 51464, 51464, 400, 286, 478, 3585, 9953, 51, 284, 339, 300, 439, 295, 552, 3651, 16235, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07083812766118881, "compression_ratio": 1.7188755020080322, "no_speech_prob": 7.4110744208155666e-06}, {"id": 1053, "seek": 514700, "start": 5152.0, "end": 5156.0, "text": " And notice here, I'm using a double list comprehension, if you want to call it that.", "tokens": [50364, 6288, 11, 264, 9834, 366, 1936, 264, 12240, 3584, 8141, 293, 439, 264, 9834, 294, 439, 264, 7914, 13, 50614, 50614, 400, 3449, 510, 11, 286, 478, 1228, 257, 3834, 1329, 44991, 11, 498, 291, 528, 281, 818, 309, 300, 13, 50814, 50814, 583, 337, 633, 4583, 294, 7914, 293, 337, 633, 13075, 294, 1184, 295, 729, 7914, 11, 51064, 51064, 321, 366, 445, 41376, 493, 439, 729, 2522, 11, 439, 729, 9834, 13, 51264, 51264, 823, 11, 294, 3217, 11, 321, 362, 17835, 11, 1360, 9834, 13, 51464, 51464, 400, 286, 478, 3585, 9953, 51, 284, 339, 300, 439, 295, 552, 3651, 16235, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07083812766118881, "compression_ratio": 1.7188755020080322, "no_speech_prob": 7.4110744208155666e-06}, {"id": 1054, "seek": 514700, "start": 5156.0, "end": 5161.0, "text": " But for every layer in layers and for every parameter in each of those layers,", "tokens": [50364, 6288, 11, 264, 9834, 366, 1936, 264, 12240, 3584, 8141, 293, 439, 264, 9834, 294, 439, 264, 7914, 13, 50614, 50614, 400, 3449, 510, 11, 286, 478, 1228, 257, 3834, 1329, 44991, 11, 498, 291, 528, 281, 818, 309, 300, 13, 50814, 50814, 583, 337, 633, 4583, 294, 7914, 293, 337, 633, 13075, 294, 1184, 295, 729, 7914, 11, 51064, 51064, 321, 366, 445, 41376, 493, 439, 729, 2522, 11, 439, 729, 9834, 13, 51264, 51264, 823, 11, 294, 3217, 11, 321, 362, 17835, 11, 1360, 9834, 13, 51464, 51464, 400, 286, 478, 3585, 9953, 51, 284, 339, 300, 439, 295, 552, 3651, 16235, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07083812766118881, "compression_ratio": 1.7188755020080322, "no_speech_prob": 7.4110744208155666e-06}, {"id": 1055, "seek": 514700, "start": 5161.0, "end": 5165.0, "text": " we are just stacking up all those piece, all those parameters.", "tokens": [50364, 6288, 11, 264, 9834, 366, 1936, 264, 12240, 3584, 8141, 293, 439, 264, 9834, 294, 439, 264, 7914, 13, 50614, 50614, 400, 3449, 510, 11, 286, 478, 1228, 257, 3834, 1329, 44991, 11, 498, 291, 528, 281, 818, 309, 300, 13, 50814, 50814, 583, 337, 633, 4583, 294, 7914, 293, 337, 633, 13075, 294, 1184, 295, 729, 7914, 11, 51064, 51064, 321, 366, 445, 41376, 493, 439, 729, 2522, 11, 439, 729, 9834, 13, 51264, 51264, 823, 11, 294, 3217, 11, 321, 362, 17835, 11, 1360, 9834, 13, 51464, 51464, 400, 286, 478, 3585, 9953, 51, 284, 339, 300, 439, 295, 552, 3651, 16235, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07083812766118881, "compression_ratio": 1.7188755020080322, "no_speech_prob": 7.4110744208155666e-06}, {"id": 1056, "seek": 514700, "start": 5165.0, "end": 5169.0, "text": " Now, in total, we have 46,000 parameters.", "tokens": [50364, 6288, 11, 264, 9834, 366, 1936, 264, 12240, 3584, 8141, 293, 439, 264, 9834, 294, 439, 264, 7914, 13, 50614, 50614, 400, 3449, 510, 11, 286, 478, 1228, 257, 3834, 1329, 44991, 11, 498, 291, 528, 281, 818, 309, 300, 13, 50814, 50814, 583, 337, 633, 4583, 294, 7914, 293, 337, 633, 13075, 294, 1184, 295, 729, 7914, 11, 51064, 51064, 321, 366, 445, 41376, 493, 439, 729, 2522, 11, 439, 729, 9834, 13, 51264, 51264, 823, 11, 294, 3217, 11, 321, 362, 17835, 11, 1360, 9834, 13, 51464, 51464, 400, 286, 478, 3585, 9953, 51, 284, 339, 300, 439, 295, 552, 3651, 16235, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07083812766118881, "compression_ratio": 1.7188755020080322, "no_speech_prob": 7.4110744208155666e-06}, {"id": 1057, "seek": 514700, "start": 5169.0, "end": 5176.0, "text": " And I'm telling PyTorch that all of them require gradient.", "tokens": [50364, 6288, 11, 264, 9834, 366, 1936, 264, 12240, 3584, 8141, 293, 439, 264, 9834, 294, 439, 264, 7914, 13, 50614, 50614, 400, 3449, 510, 11, 286, 478, 1228, 257, 3834, 1329, 44991, 11, 498, 291, 528, 281, 818, 309, 300, 13, 50814, 50814, 583, 337, 633, 4583, 294, 7914, 293, 337, 633, 13075, 294, 1184, 295, 729, 7914, 11, 51064, 51064, 321, 366, 445, 41376, 493, 439, 729, 2522, 11, 439, 729, 9834, 13, 51264, 51264, 823, 11, 294, 3217, 11, 321, 362, 17835, 11, 1360, 9834, 13, 51464, 51464, 400, 286, 478, 3585, 9953, 51, 284, 339, 300, 439, 295, 552, 3651, 16235, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07083812766118881, "compression_ratio": 1.7188755020080322, "no_speech_prob": 7.4110744208155666e-06}, {"id": 1058, "seek": 517600, "start": 5176.0, "end": 5180.0, "text": " Then here, we have everything here we are actually mostly used to.", "tokens": [50364, 1396, 510, 11, 321, 362, 1203, 510, 321, 366, 767, 5240, 1143, 281, 13, 50564, 50564, 492, 366, 21179, 15245, 13, 492, 366, 884, 257, 2128, 1320, 13, 50714, 50714, 440, 2128, 1320, 586, 307, 445, 257, 8213, 3861, 295, 439, 264, 7914, 294, 1668, 11, 6263, 538, 264, 3278, 30867, 13, 51014, 51014, 400, 550, 294, 264, 23897, 1320, 11, 291, 603, 3449, 300, 337, 633, 2167, 4583, 11, 51164, 51164, 286, 586, 44497, 670, 439, 264, 23930, 11, 293, 286, 478, 3585, 9953, 51, 284, 339, 281, 18340, 264, 16235, 295, 552, 13, 51414, 51414, 400, 550, 510, 11, 321, 366, 1217, 1143, 281, 439, 264, 2771, 2448, 992, 281, 6022, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08163720470363811, "compression_ratio": 1.7769230769230768, "no_speech_prob": 8.529722435923759e-06}, {"id": 1059, "seek": 517600, "start": 5180.0, "end": 5183.0, "text": " We are sampling batch. We are doing a forward pass.", "tokens": [50364, 1396, 510, 11, 321, 362, 1203, 510, 321, 366, 767, 5240, 1143, 281, 13, 50564, 50564, 492, 366, 21179, 15245, 13, 492, 366, 884, 257, 2128, 1320, 13, 50714, 50714, 440, 2128, 1320, 586, 307, 445, 257, 8213, 3861, 295, 439, 264, 7914, 294, 1668, 11, 6263, 538, 264, 3278, 30867, 13, 51014, 51014, 400, 550, 294, 264, 23897, 1320, 11, 291, 603, 3449, 300, 337, 633, 2167, 4583, 11, 51164, 51164, 286, 586, 44497, 670, 439, 264, 23930, 11, 293, 286, 478, 3585, 9953, 51, 284, 339, 281, 18340, 264, 16235, 295, 552, 13, 51414, 51414, 400, 550, 510, 11, 321, 366, 1217, 1143, 281, 439, 264, 2771, 2448, 992, 281, 6022, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08163720470363811, "compression_ratio": 1.7769230769230768, "no_speech_prob": 8.529722435923759e-06}, {"id": 1060, "seek": 517600, "start": 5183.0, "end": 5189.0, "text": " The forward pass now is just a linear application of all the layers in order, followed by the cross entropy.", "tokens": [50364, 1396, 510, 11, 321, 362, 1203, 510, 321, 366, 767, 5240, 1143, 281, 13, 50564, 50564, 492, 366, 21179, 15245, 13, 492, 366, 884, 257, 2128, 1320, 13, 50714, 50714, 440, 2128, 1320, 586, 307, 445, 257, 8213, 3861, 295, 439, 264, 7914, 294, 1668, 11, 6263, 538, 264, 3278, 30867, 13, 51014, 51014, 400, 550, 294, 264, 23897, 1320, 11, 291, 603, 3449, 300, 337, 633, 2167, 4583, 11, 51164, 51164, 286, 586, 44497, 670, 439, 264, 23930, 11, 293, 286, 478, 3585, 9953, 51, 284, 339, 281, 18340, 264, 16235, 295, 552, 13, 51414, 51414, 400, 550, 510, 11, 321, 366, 1217, 1143, 281, 439, 264, 2771, 2448, 992, 281, 6022, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08163720470363811, "compression_ratio": 1.7769230769230768, "no_speech_prob": 8.529722435923759e-06}, {"id": 1061, "seek": 517600, "start": 5189.0, "end": 5192.0, "text": " And then in the backward pass, you'll notice that for every single layer,", "tokens": [50364, 1396, 510, 11, 321, 362, 1203, 510, 321, 366, 767, 5240, 1143, 281, 13, 50564, 50564, 492, 366, 21179, 15245, 13, 492, 366, 884, 257, 2128, 1320, 13, 50714, 50714, 440, 2128, 1320, 586, 307, 445, 257, 8213, 3861, 295, 439, 264, 7914, 294, 1668, 11, 6263, 538, 264, 3278, 30867, 13, 51014, 51014, 400, 550, 294, 264, 23897, 1320, 11, 291, 603, 3449, 300, 337, 633, 2167, 4583, 11, 51164, 51164, 286, 586, 44497, 670, 439, 264, 23930, 11, 293, 286, 478, 3585, 9953, 51, 284, 339, 281, 18340, 264, 16235, 295, 552, 13, 51414, 51414, 400, 550, 510, 11, 321, 366, 1217, 1143, 281, 439, 264, 2771, 2448, 992, 281, 6022, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08163720470363811, "compression_ratio": 1.7769230769230768, "no_speech_prob": 8.529722435923759e-06}, {"id": 1062, "seek": 517600, "start": 5192.0, "end": 5197.0, "text": " I now iterate over all the outputs, and I'm telling PyTorch to retain the gradient of them.", "tokens": [50364, 1396, 510, 11, 321, 362, 1203, 510, 321, 366, 767, 5240, 1143, 281, 13, 50564, 50564, 492, 366, 21179, 15245, 13, 492, 366, 884, 257, 2128, 1320, 13, 50714, 50714, 440, 2128, 1320, 586, 307, 445, 257, 8213, 3861, 295, 439, 264, 7914, 294, 1668, 11, 6263, 538, 264, 3278, 30867, 13, 51014, 51014, 400, 550, 294, 264, 23897, 1320, 11, 291, 603, 3449, 300, 337, 633, 2167, 4583, 11, 51164, 51164, 286, 586, 44497, 670, 439, 264, 23930, 11, 293, 286, 478, 3585, 9953, 51, 284, 339, 281, 18340, 264, 16235, 295, 552, 13, 51414, 51414, 400, 550, 510, 11, 321, 366, 1217, 1143, 281, 439, 264, 2771, 2448, 992, 281, 6022, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08163720470363811, "compression_ratio": 1.7769230769230768, "no_speech_prob": 8.529722435923759e-06}, {"id": 1063, "seek": 517600, "start": 5197.0, "end": 5201.0, "text": " And then here, we are already used to all the gradients set to none.", "tokens": [50364, 1396, 510, 11, 321, 362, 1203, 510, 321, 366, 767, 5240, 1143, 281, 13, 50564, 50564, 492, 366, 21179, 15245, 13, 492, 366, 884, 257, 2128, 1320, 13, 50714, 50714, 440, 2128, 1320, 586, 307, 445, 257, 8213, 3861, 295, 439, 264, 7914, 294, 1668, 11, 6263, 538, 264, 3278, 30867, 13, 51014, 51014, 400, 550, 294, 264, 23897, 1320, 11, 291, 603, 3449, 300, 337, 633, 2167, 4583, 11, 51164, 51164, 286, 586, 44497, 670, 439, 264, 23930, 11, 293, 286, 478, 3585, 9953, 51, 284, 339, 281, 18340, 264, 16235, 295, 552, 13, 51414, 51414, 400, 550, 510, 11, 321, 366, 1217, 1143, 281, 439, 264, 2771, 2448, 992, 281, 6022, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.08163720470363811, "compression_ratio": 1.7769230769230768, "no_speech_prob": 8.529722435923759e-06}, {"id": 1064, "seek": 520100, "start": 5201.0, "end": 5208.0, "text": " Do the backward to fill in the gradients, do an update using stochastic gradient send, and then track some statistics.", "tokens": [50364, 1144, 264, 23897, 281, 2836, 294, 264, 2771, 2448, 11, 360, 364, 5623, 1228, 342, 8997, 2750, 16235, 2845, 11, 293, 550, 2837, 512, 12523, 13, 50714, 50714, 400, 550, 286, 669, 516, 281, 1821, 934, 257, 2167, 24784, 13, 50914, 50914, 823, 11, 510, 294, 341, 2815, 11, 294, 341, 10686, 11, 286, 478, 5056, 3319, 264, 49816, 82, 295, 264, 2128, 1320, 2430, 763, 11, 51214, 51214, 293, 286, 478, 4682, 884, 309, 412, 264, 1266, 1184, 7914, 13, 51364, 51364, 407, 17138, 990, 670, 439, 264, 7914, 11, 3993, 337, 264, 588, 1036, 472, 11, 597, 307, 1936, 445, 264, 2787, 41167, 4583, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10347698830269478, "compression_ratio": 1.6653992395437263, "no_speech_prob": 6.854028924863087e-06}, {"id": 1065, "seek": 520100, "start": 5208.0, "end": 5212.0, "text": " And then I am going to break after a single iteration.", "tokens": [50364, 1144, 264, 23897, 281, 2836, 294, 264, 2771, 2448, 11, 360, 364, 5623, 1228, 342, 8997, 2750, 16235, 2845, 11, 293, 550, 2837, 512, 12523, 13, 50714, 50714, 400, 550, 286, 669, 516, 281, 1821, 934, 257, 2167, 24784, 13, 50914, 50914, 823, 11, 510, 294, 341, 2815, 11, 294, 341, 10686, 11, 286, 478, 5056, 3319, 264, 49816, 82, 295, 264, 2128, 1320, 2430, 763, 11, 51214, 51214, 293, 286, 478, 4682, 884, 309, 412, 264, 1266, 1184, 7914, 13, 51364, 51364, 407, 17138, 990, 670, 439, 264, 7914, 11, 3993, 337, 264, 588, 1036, 472, 11, 597, 307, 1936, 445, 264, 2787, 41167, 4583, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10347698830269478, "compression_ratio": 1.6653992395437263, "no_speech_prob": 6.854028924863087e-06}, {"id": 1066, "seek": 520100, "start": 5212.0, "end": 5218.0, "text": " Now, here in this cell, in this diagram, I'm visualizing the histograms of the forward pass activations,", "tokens": [50364, 1144, 264, 23897, 281, 2836, 294, 264, 2771, 2448, 11, 360, 364, 5623, 1228, 342, 8997, 2750, 16235, 2845, 11, 293, 550, 2837, 512, 12523, 13, 50714, 50714, 400, 550, 286, 669, 516, 281, 1821, 934, 257, 2167, 24784, 13, 50914, 50914, 823, 11, 510, 294, 341, 2815, 11, 294, 341, 10686, 11, 286, 478, 5056, 3319, 264, 49816, 82, 295, 264, 2128, 1320, 2430, 763, 11, 51214, 51214, 293, 286, 478, 4682, 884, 309, 412, 264, 1266, 1184, 7914, 13, 51364, 51364, 407, 17138, 990, 670, 439, 264, 7914, 11, 3993, 337, 264, 588, 1036, 472, 11, 597, 307, 1936, 445, 264, 2787, 41167, 4583, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10347698830269478, "compression_ratio": 1.6653992395437263, "no_speech_prob": 6.854028924863087e-06}, {"id": 1067, "seek": 520100, "start": 5218.0, "end": 5221.0, "text": " and I'm specifically doing it at the 10 each layers.", "tokens": [50364, 1144, 264, 23897, 281, 2836, 294, 264, 2771, 2448, 11, 360, 364, 5623, 1228, 342, 8997, 2750, 16235, 2845, 11, 293, 550, 2837, 512, 12523, 13, 50714, 50714, 400, 550, 286, 669, 516, 281, 1821, 934, 257, 2167, 24784, 13, 50914, 50914, 823, 11, 510, 294, 341, 2815, 11, 294, 341, 10686, 11, 286, 478, 5056, 3319, 264, 49816, 82, 295, 264, 2128, 1320, 2430, 763, 11, 51214, 51214, 293, 286, 478, 4682, 884, 309, 412, 264, 1266, 1184, 7914, 13, 51364, 51364, 407, 17138, 990, 670, 439, 264, 7914, 11, 3993, 337, 264, 588, 1036, 472, 11, 597, 307, 1936, 445, 264, 2787, 41167, 4583, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10347698830269478, "compression_ratio": 1.6653992395437263, "no_speech_prob": 6.854028924863087e-06}, {"id": 1068, "seek": 520100, "start": 5221.0, "end": 5230.0, "text": " So iterating over all the layers, except for the very last one, which is basically just the softmax layer.", "tokens": [50364, 1144, 264, 23897, 281, 2836, 294, 264, 2771, 2448, 11, 360, 364, 5623, 1228, 342, 8997, 2750, 16235, 2845, 11, 293, 550, 2837, 512, 12523, 13, 50714, 50714, 400, 550, 286, 669, 516, 281, 1821, 934, 257, 2167, 24784, 13, 50914, 50914, 823, 11, 510, 294, 341, 2815, 11, 294, 341, 10686, 11, 286, 478, 5056, 3319, 264, 49816, 82, 295, 264, 2128, 1320, 2430, 763, 11, 51214, 51214, 293, 286, 478, 4682, 884, 309, 412, 264, 1266, 1184, 7914, 13, 51364, 51364, 407, 17138, 990, 670, 439, 264, 7914, 11, 3993, 337, 264, 588, 1036, 472, 11, 597, 307, 1936, 445, 264, 2787, 41167, 4583, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.10347698830269478, "compression_ratio": 1.6653992395437263, "no_speech_prob": 6.854028924863087e-06}, {"id": 1069, "seek": 523000, "start": 5230.0, "end": 5235.0, "text": " If it is a 10 H layer, and I'm using a 10 H layer just because they have a finite output, negative one to one.", "tokens": [50364, 759, 309, 307, 257, 1266, 389, 4583, 11, 293, 286, 478, 1228, 257, 1266, 389, 4583, 445, 570, 436, 362, 257, 19362, 5598, 11, 3671, 472, 281, 472, 13, 50614, 50614, 400, 370, 309, 311, 588, 1858, 281, 23273, 510, 13, 50714, 50714, 407, 291, 536, 3671, 472, 281, 472, 13, 467, 311, 257, 19362, 3613, 293, 1858, 281, 589, 365, 13, 50914, 50914, 286, 747, 264, 484, 40863, 490, 300, 4583, 666, 314, 11, 293, 550, 286, 478, 28258, 264, 914, 11, 264, 3832, 25163, 293, 264, 3043, 27090, 295, 314, 13, 51364, 51364, 400, 264, 636, 286, 6964, 264, 3043, 27090, 307, 300, 314, 5893, 8236, 2158, 307, 5044, 813, 935, 4949, 3407, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13620007832845052, "compression_ratio": 1.7669172932330828, "no_speech_prob": 3.647566700237803e-05}, {"id": 1070, "seek": 523000, "start": 5235.0, "end": 5237.0, "text": " And so it's very easy to visualize here.", "tokens": [50364, 759, 309, 307, 257, 1266, 389, 4583, 11, 293, 286, 478, 1228, 257, 1266, 389, 4583, 445, 570, 436, 362, 257, 19362, 5598, 11, 3671, 472, 281, 472, 13, 50614, 50614, 400, 370, 309, 311, 588, 1858, 281, 23273, 510, 13, 50714, 50714, 407, 291, 536, 3671, 472, 281, 472, 13, 467, 311, 257, 19362, 3613, 293, 1858, 281, 589, 365, 13, 50914, 50914, 286, 747, 264, 484, 40863, 490, 300, 4583, 666, 314, 11, 293, 550, 286, 478, 28258, 264, 914, 11, 264, 3832, 25163, 293, 264, 3043, 27090, 295, 314, 13, 51364, 51364, 400, 264, 636, 286, 6964, 264, 3043, 27090, 307, 300, 314, 5893, 8236, 2158, 307, 5044, 813, 935, 4949, 3407, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13620007832845052, "compression_ratio": 1.7669172932330828, "no_speech_prob": 3.647566700237803e-05}, {"id": 1071, "seek": 523000, "start": 5237.0, "end": 5241.0, "text": " So you see negative one to one. It's a finite range and easy to work with.", "tokens": [50364, 759, 309, 307, 257, 1266, 389, 4583, 11, 293, 286, 478, 1228, 257, 1266, 389, 4583, 445, 570, 436, 362, 257, 19362, 5598, 11, 3671, 472, 281, 472, 13, 50614, 50614, 400, 370, 309, 311, 588, 1858, 281, 23273, 510, 13, 50714, 50714, 407, 291, 536, 3671, 472, 281, 472, 13, 467, 311, 257, 19362, 3613, 293, 1858, 281, 589, 365, 13, 50914, 50914, 286, 747, 264, 484, 40863, 490, 300, 4583, 666, 314, 11, 293, 550, 286, 478, 28258, 264, 914, 11, 264, 3832, 25163, 293, 264, 3043, 27090, 295, 314, 13, 51364, 51364, 400, 264, 636, 286, 6964, 264, 3043, 27090, 307, 300, 314, 5893, 8236, 2158, 307, 5044, 813, 935, 4949, 3407, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13620007832845052, "compression_ratio": 1.7669172932330828, "no_speech_prob": 3.647566700237803e-05}, {"id": 1072, "seek": 523000, "start": 5241.0, "end": 5250.0, "text": " I take the out tensor from that layer into T, and then I'm calculating the mean, the standard deviation and the percent saturation of T.", "tokens": [50364, 759, 309, 307, 257, 1266, 389, 4583, 11, 293, 286, 478, 1228, 257, 1266, 389, 4583, 445, 570, 436, 362, 257, 19362, 5598, 11, 3671, 472, 281, 472, 13, 50614, 50614, 400, 370, 309, 311, 588, 1858, 281, 23273, 510, 13, 50714, 50714, 407, 291, 536, 3671, 472, 281, 472, 13, 467, 311, 257, 19362, 3613, 293, 1858, 281, 589, 365, 13, 50914, 50914, 286, 747, 264, 484, 40863, 490, 300, 4583, 666, 314, 11, 293, 550, 286, 478, 28258, 264, 914, 11, 264, 3832, 25163, 293, 264, 3043, 27090, 295, 314, 13, 51364, 51364, 400, 264, 636, 286, 6964, 264, 3043, 27090, 307, 300, 314, 5893, 8236, 2158, 307, 5044, 813, 935, 4949, 3407, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13620007832845052, "compression_ratio": 1.7669172932330828, "no_speech_prob": 3.647566700237803e-05}, {"id": 1073, "seek": 523000, "start": 5250.0, "end": 5255.0, "text": " And the way I define the percent saturation is that T dot absolute value is greater than point nine seven.", "tokens": [50364, 759, 309, 307, 257, 1266, 389, 4583, 11, 293, 286, 478, 1228, 257, 1266, 389, 4583, 445, 570, 436, 362, 257, 19362, 5598, 11, 3671, 472, 281, 472, 13, 50614, 50614, 400, 370, 309, 311, 588, 1858, 281, 23273, 510, 13, 50714, 50714, 407, 291, 536, 3671, 472, 281, 472, 13, 467, 311, 257, 19362, 3613, 293, 1858, 281, 589, 365, 13, 50914, 50914, 286, 747, 264, 484, 40863, 490, 300, 4583, 666, 314, 11, 293, 550, 286, 478, 28258, 264, 914, 11, 264, 3832, 25163, 293, 264, 3043, 27090, 295, 314, 13, 51364, 51364, 400, 264, 636, 286, 6964, 264, 3043, 27090, 307, 300, 314, 5893, 8236, 2158, 307, 5044, 813, 935, 4949, 3407, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13620007832845052, "compression_ratio": 1.7669172932330828, "no_speech_prob": 3.647566700237803e-05}, {"id": 1074, "seek": 525500, "start": 5255.0, "end": 5262.0, "text": " So that means we are here at the tails of the 10 H. And remember that when we are in the tails of the 10 H, that will actually stop gradients.", "tokens": [50364, 407, 300, 1355, 321, 366, 510, 412, 264, 28537, 295, 264, 1266, 389, 13, 400, 1604, 300, 562, 321, 366, 294, 264, 28537, 295, 264, 1266, 389, 11, 300, 486, 767, 1590, 2771, 2448, 13, 50714, 50714, 407, 321, 500, 380, 528, 341, 281, 312, 886, 1090, 13, 50864, 50864, 823, 11, 510, 286, 478, 5141, 27822, 5893, 49816, 11, 293, 550, 286, 669, 41178, 341, 49816, 13, 51164, 51164, 407, 1936, 437, 341, 307, 884, 307, 300, 633, 819, 2010, 295, 4583, 293, 436, 439, 362, 257, 819, 2017, 13, 51364, 51364, 492, 366, 1237, 412, 577, 867, 4190, 294, 613, 1500, 433, 747, 322, 604, 295, 264, 4190, 2507, 322, 341, 10298, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09143782456715902, "compression_ratio": 1.7509293680297398, "no_speech_prob": 1.406332194164861e-05}, {"id": 1075, "seek": 525500, "start": 5262.0, "end": 5265.0, "text": " So we don't want this to be too high.", "tokens": [50364, 407, 300, 1355, 321, 366, 510, 412, 264, 28537, 295, 264, 1266, 389, 13, 400, 1604, 300, 562, 321, 366, 294, 264, 28537, 295, 264, 1266, 389, 11, 300, 486, 767, 1590, 2771, 2448, 13, 50714, 50714, 407, 321, 500, 380, 528, 341, 281, 312, 886, 1090, 13, 50864, 50864, 823, 11, 510, 286, 478, 5141, 27822, 5893, 49816, 11, 293, 550, 286, 669, 41178, 341, 49816, 13, 51164, 51164, 407, 1936, 437, 341, 307, 884, 307, 300, 633, 819, 2010, 295, 4583, 293, 436, 439, 362, 257, 819, 2017, 13, 51364, 51364, 492, 366, 1237, 412, 577, 867, 4190, 294, 613, 1500, 433, 747, 322, 604, 295, 264, 4190, 2507, 322, 341, 10298, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09143782456715902, "compression_ratio": 1.7509293680297398, "no_speech_prob": 1.406332194164861e-05}, {"id": 1076, "seek": 525500, "start": 5265.0, "end": 5271.0, "text": " Now, here I'm calling torch dot histogram, and then I am plotting this histogram.", "tokens": [50364, 407, 300, 1355, 321, 366, 510, 412, 264, 28537, 295, 264, 1266, 389, 13, 400, 1604, 300, 562, 321, 366, 294, 264, 28537, 295, 264, 1266, 389, 11, 300, 486, 767, 1590, 2771, 2448, 13, 50714, 50714, 407, 321, 500, 380, 528, 341, 281, 312, 886, 1090, 13, 50864, 50864, 823, 11, 510, 286, 478, 5141, 27822, 5893, 49816, 11, 293, 550, 286, 669, 41178, 341, 49816, 13, 51164, 51164, 407, 1936, 437, 341, 307, 884, 307, 300, 633, 819, 2010, 295, 4583, 293, 436, 439, 362, 257, 819, 2017, 13, 51364, 51364, 492, 366, 1237, 412, 577, 867, 4190, 294, 613, 1500, 433, 747, 322, 604, 295, 264, 4190, 2507, 322, 341, 10298, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09143782456715902, "compression_ratio": 1.7509293680297398, "no_speech_prob": 1.406332194164861e-05}, {"id": 1077, "seek": 525500, "start": 5271.0, "end": 5275.0, "text": " So basically what this is doing is that every different type of layer and they all have a different color.", "tokens": [50364, 407, 300, 1355, 321, 366, 510, 412, 264, 28537, 295, 264, 1266, 389, 13, 400, 1604, 300, 562, 321, 366, 294, 264, 28537, 295, 264, 1266, 389, 11, 300, 486, 767, 1590, 2771, 2448, 13, 50714, 50714, 407, 321, 500, 380, 528, 341, 281, 312, 886, 1090, 13, 50864, 50864, 823, 11, 510, 286, 478, 5141, 27822, 5893, 49816, 11, 293, 550, 286, 669, 41178, 341, 49816, 13, 51164, 51164, 407, 1936, 437, 341, 307, 884, 307, 300, 633, 819, 2010, 295, 4583, 293, 436, 439, 362, 257, 819, 2017, 13, 51364, 51364, 492, 366, 1237, 412, 577, 867, 4190, 294, 613, 1500, 433, 747, 322, 604, 295, 264, 4190, 2507, 322, 341, 10298, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09143782456715902, "compression_ratio": 1.7509293680297398, "no_speech_prob": 1.406332194164861e-05}, {"id": 1078, "seek": 525500, "start": 5275.0, "end": 5284.0, "text": " We are looking at how many values in these testers take on any of the values below on this axis here.", "tokens": [50364, 407, 300, 1355, 321, 366, 510, 412, 264, 28537, 295, 264, 1266, 389, 13, 400, 1604, 300, 562, 321, 366, 294, 264, 28537, 295, 264, 1266, 389, 11, 300, 486, 767, 1590, 2771, 2448, 13, 50714, 50714, 407, 321, 500, 380, 528, 341, 281, 312, 886, 1090, 13, 50864, 50864, 823, 11, 510, 286, 478, 5141, 27822, 5893, 49816, 11, 293, 550, 286, 669, 41178, 341, 49816, 13, 51164, 51164, 407, 1936, 437, 341, 307, 884, 307, 300, 633, 819, 2010, 295, 4583, 293, 436, 439, 362, 257, 819, 2017, 13, 51364, 51364, 492, 366, 1237, 412, 577, 867, 4190, 294, 613, 1500, 433, 747, 322, 604, 295, 264, 4190, 2507, 322, 341, 10298, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09143782456715902, "compression_ratio": 1.7509293680297398, "no_speech_prob": 1.406332194164861e-05}, {"id": 1079, "seek": 528400, "start": 5284.0, "end": 5288.0, "text": " So the first layer is fairly saturated here at 20 percent.", "tokens": [50364, 407, 264, 700, 4583, 307, 6457, 25408, 510, 412, 945, 3043, 13, 50564, 50564, 407, 291, 393, 536, 300, 309, 311, 658, 28537, 510, 11, 457, 550, 1203, 1333, 295, 11652, 5660, 13, 50764, 50764, 400, 498, 321, 632, 544, 7914, 510, 11, 309, 576, 767, 445, 31870, 309, 926, 264, 3832, 25163, 295, 466, 935, 2309, 1732, 293, 264, 27090, 576, 312, 9810, 1732, 3043, 13, 51164, 51164, 400, 264, 1778, 300, 341, 11652, 5660, 293, 2709, 505, 257, 1481, 7316, 510, 307, 570, 6052, 307, 992, 281, 1732, 670, 1045, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06846662894966676, "compression_ratio": 1.6984126984126984, "no_speech_prob": 7.296069270523731e-06}, {"id": 1080, "seek": 528400, "start": 5288.0, "end": 5292.0, "text": " So you can see that it's got tails here, but then everything sort of stabilizes.", "tokens": [50364, 407, 264, 700, 4583, 307, 6457, 25408, 510, 412, 945, 3043, 13, 50564, 50564, 407, 291, 393, 536, 300, 309, 311, 658, 28537, 510, 11, 457, 550, 1203, 1333, 295, 11652, 5660, 13, 50764, 50764, 400, 498, 321, 632, 544, 7914, 510, 11, 309, 576, 767, 445, 31870, 309, 926, 264, 3832, 25163, 295, 466, 935, 2309, 1732, 293, 264, 27090, 576, 312, 9810, 1732, 3043, 13, 51164, 51164, 400, 264, 1778, 300, 341, 11652, 5660, 293, 2709, 505, 257, 1481, 7316, 510, 307, 570, 6052, 307, 992, 281, 1732, 670, 1045, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06846662894966676, "compression_ratio": 1.6984126984126984, "no_speech_prob": 7.296069270523731e-06}, {"id": 1081, "seek": 528400, "start": 5292.0, "end": 5300.0, "text": " And if we had more layers here, it would actually just stabilize it around the standard deviation of about point six five and the saturation would be roughly five percent.", "tokens": [50364, 407, 264, 700, 4583, 307, 6457, 25408, 510, 412, 945, 3043, 13, 50564, 50564, 407, 291, 393, 536, 300, 309, 311, 658, 28537, 510, 11, 457, 550, 1203, 1333, 295, 11652, 5660, 13, 50764, 50764, 400, 498, 321, 632, 544, 7914, 510, 11, 309, 576, 767, 445, 31870, 309, 926, 264, 3832, 25163, 295, 466, 935, 2309, 1732, 293, 264, 27090, 576, 312, 9810, 1732, 3043, 13, 51164, 51164, 400, 264, 1778, 300, 341, 11652, 5660, 293, 2709, 505, 257, 1481, 7316, 510, 307, 570, 6052, 307, 992, 281, 1732, 670, 1045, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06846662894966676, "compression_ratio": 1.6984126984126984, "no_speech_prob": 7.296069270523731e-06}, {"id": 1082, "seek": 528400, "start": 5300.0, "end": 5307.0, "text": " And the reason that this stabilizes and gives us a nice distribution here is because gain is set to five over three.", "tokens": [50364, 407, 264, 700, 4583, 307, 6457, 25408, 510, 412, 945, 3043, 13, 50564, 50564, 407, 291, 393, 536, 300, 309, 311, 658, 28537, 510, 11, 457, 550, 1203, 1333, 295, 11652, 5660, 13, 50764, 50764, 400, 498, 321, 632, 544, 7914, 510, 11, 309, 576, 767, 445, 31870, 309, 926, 264, 3832, 25163, 295, 466, 935, 2309, 1732, 293, 264, 27090, 576, 312, 9810, 1732, 3043, 13, 51164, 51164, 400, 264, 1778, 300, 341, 11652, 5660, 293, 2709, 505, 257, 1481, 7316, 510, 307, 570, 6052, 307, 992, 281, 1732, 670, 1045, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06846662894966676, "compression_ratio": 1.6984126984126984, "no_speech_prob": 7.296069270523731e-06}, {"id": 1083, "seek": 530700, "start": 5307.0, "end": 5315.0, "text": " Now, here, this game, you see that by default, we initialize with one over square root of fan in.", "tokens": [50364, 823, 11, 510, 11, 341, 1216, 11, 291, 536, 300, 538, 7576, 11, 321, 5883, 1125, 365, 472, 670, 3732, 5593, 295, 3429, 294, 13, 50764, 50764, 583, 550, 510, 1830, 5883, 2144, 11, 286, 808, 294, 293, 286, 44497, 670, 439, 264, 7914, 13, 50964, 50964, 400, 498, 309, 311, 257, 8213, 4583, 11, 286, 9194, 300, 538, 264, 1216, 13, 51114, 51114, 823, 11, 321, 1866, 300, 472, 13, 407, 1936, 11, 498, 321, 445, 360, 406, 764, 257, 1216, 11, 550, 437, 2314, 498, 286, 2182, 5131, 341, 11, 291, 486, 536, 300, 264, 3832, 25163, 307, 41684, 293, 264, 27090, 307, 1348, 281, 4018, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1239794840854881, "compression_ratio": 1.6745098039215687, "no_speech_prob": 5.8625946621759795e-06}, {"id": 1084, "seek": 530700, "start": 5315.0, "end": 5319.0, "text": " But then here during initialization, I come in and I iterate over all the layers.", "tokens": [50364, 823, 11, 510, 11, 341, 1216, 11, 291, 536, 300, 538, 7576, 11, 321, 5883, 1125, 365, 472, 670, 3732, 5593, 295, 3429, 294, 13, 50764, 50764, 583, 550, 510, 1830, 5883, 2144, 11, 286, 808, 294, 293, 286, 44497, 670, 439, 264, 7914, 13, 50964, 50964, 400, 498, 309, 311, 257, 8213, 4583, 11, 286, 9194, 300, 538, 264, 1216, 13, 51114, 51114, 823, 11, 321, 1866, 300, 472, 13, 407, 1936, 11, 498, 321, 445, 360, 406, 764, 257, 1216, 11, 550, 437, 2314, 498, 286, 2182, 5131, 341, 11, 291, 486, 536, 300, 264, 3832, 25163, 307, 41684, 293, 264, 27090, 307, 1348, 281, 4018, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1239794840854881, "compression_ratio": 1.6745098039215687, "no_speech_prob": 5.8625946621759795e-06}, {"id": 1085, "seek": 530700, "start": 5319.0, "end": 5322.0, "text": " And if it's a linear layer, I boost that by the game.", "tokens": [50364, 823, 11, 510, 11, 341, 1216, 11, 291, 536, 300, 538, 7576, 11, 321, 5883, 1125, 365, 472, 670, 3732, 5593, 295, 3429, 294, 13, 50764, 50764, 583, 550, 510, 1830, 5883, 2144, 11, 286, 808, 294, 293, 286, 44497, 670, 439, 264, 7914, 13, 50964, 50964, 400, 498, 309, 311, 257, 8213, 4583, 11, 286, 9194, 300, 538, 264, 1216, 13, 51114, 51114, 823, 11, 321, 1866, 300, 472, 13, 407, 1936, 11, 498, 321, 445, 360, 406, 764, 257, 1216, 11, 550, 437, 2314, 498, 286, 2182, 5131, 341, 11, 291, 486, 536, 300, 264, 3832, 25163, 307, 41684, 293, 264, 27090, 307, 1348, 281, 4018, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1239794840854881, "compression_ratio": 1.6745098039215687, "no_speech_prob": 5.8625946621759795e-06}, {"id": 1086, "seek": 530700, "start": 5322.0, "end": 5336.0, "text": " Now, we saw that one. So basically, if we just do not use a game, then what happens if I redraw this, you will see that the standard deviation is shrinking and the saturation is coming to zero.", "tokens": [50364, 823, 11, 510, 11, 341, 1216, 11, 291, 536, 300, 538, 7576, 11, 321, 5883, 1125, 365, 472, 670, 3732, 5593, 295, 3429, 294, 13, 50764, 50764, 583, 550, 510, 1830, 5883, 2144, 11, 286, 808, 294, 293, 286, 44497, 670, 439, 264, 7914, 13, 50964, 50964, 400, 498, 309, 311, 257, 8213, 4583, 11, 286, 9194, 300, 538, 264, 1216, 13, 51114, 51114, 823, 11, 321, 1866, 300, 472, 13, 407, 1936, 11, 498, 321, 445, 360, 406, 764, 257, 1216, 11, 550, 437, 2314, 498, 286, 2182, 5131, 341, 11, 291, 486, 536, 300, 264, 3832, 25163, 307, 41684, 293, 264, 27090, 307, 1348, 281, 4018, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1239794840854881, "compression_ratio": 1.6745098039215687, "no_speech_prob": 5.8625946621759795e-06}, {"id": 1087, "seek": 533600, "start": 5336.0, "end": 5340.0, "text": " And basically what's happening is the first layer is, you know, pretty decent.", "tokens": [50364, 400, 1936, 437, 311, 2737, 307, 264, 700, 4583, 307, 11, 291, 458, 11, 1238, 8681, 13, 50564, 50564, 583, 550, 3052, 7914, 366, 445, 733, 295, 411, 41684, 760, 281, 4018, 13, 50764, 50764, 400, 309, 311, 2737, 5692, 11, 457, 309, 311, 41684, 281, 4018, 13, 50914, 50914, 400, 264, 1778, 337, 300, 307, 562, 291, 445, 362, 257, 11141, 295, 8213, 7914, 3312, 11, 550, 257, 550, 5883, 3319, 527, 17443, 294, 341, 9060, 321, 1866, 8046, 576, 362, 1014, 6913, 264, 3832, 25163, 295, 472, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06295189958937625, "compression_ratio": 1.7130801687763713, "no_speech_prob": 2.561217570473673e-06}, {"id": 1088, "seek": 533600, "start": 5340.0, "end": 5344.0, "text": " But then further layers are just kind of like shrinking down to zero.", "tokens": [50364, 400, 1936, 437, 311, 2737, 307, 264, 700, 4583, 307, 11, 291, 458, 11, 1238, 8681, 13, 50564, 50564, 583, 550, 3052, 7914, 366, 445, 733, 295, 411, 41684, 760, 281, 4018, 13, 50764, 50764, 400, 309, 311, 2737, 5692, 11, 457, 309, 311, 41684, 281, 4018, 13, 50914, 50914, 400, 264, 1778, 337, 300, 307, 562, 291, 445, 362, 257, 11141, 295, 8213, 7914, 3312, 11, 550, 257, 550, 5883, 3319, 527, 17443, 294, 341, 9060, 321, 1866, 8046, 576, 362, 1014, 6913, 264, 3832, 25163, 295, 472, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06295189958937625, "compression_ratio": 1.7130801687763713, "no_speech_prob": 2.561217570473673e-06}, {"id": 1089, "seek": 533600, "start": 5344.0, "end": 5347.0, "text": " And it's happening slowly, but it's shrinking to zero.", "tokens": [50364, 400, 1936, 437, 311, 2737, 307, 264, 700, 4583, 307, 11, 291, 458, 11, 1238, 8681, 13, 50564, 50564, 583, 550, 3052, 7914, 366, 445, 733, 295, 411, 41684, 760, 281, 4018, 13, 50764, 50764, 400, 309, 311, 2737, 5692, 11, 457, 309, 311, 41684, 281, 4018, 13, 50914, 50914, 400, 264, 1778, 337, 300, 307, 562, 291, 445, 362, 257, 11141, 295, 8213, 7914, 3312, 11, 550, 257, 550, 5883, 3319, 527, 17443, 294, 341, 9060, 321, 1866, 8046, 576, 362, 1014, 6913, 264, 3832, 25163, 295, 472, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06295189958937625, "compression_ratio": 1.7130801687763713, "no_speech_prob": 2.561217570473673e-06}, {"id": 1090, "seek": 533600, "start": 5347.0, "end": 5361.0, "text": " And the reason for that is when you just have a sandwich of linear layers alone, then a then initializing our weights in this manner we saw previously would have conserved the standard deviation of one.", "tokens": [50364, 400, 1936, 437, 311, 2737, 307, 264, 700, 4583, 307, 11, 291, 458, 11, 1238, 8681, 13, 50564, 50564, 583, 550, 3052, 7914, 366, 445, 733, 295, 411, 41684, 760, 281, 4018, 13, 50764, 50764, 400, 309, 311, 2737, 5692, 11, 457, 309, 311, 41684, 281, 4018, 13, 50914, 50914, 400, 264, 1778, 337, 300, 307, 562, 291, 445, 362, 257, 11141, 295, 8213, 7914, 3312, 11, 550, 257, 550, 5883, 3319, 527, 17443, 294, 341, 9060, 321, 1866, 8046, 576, 362, 1014, 6913, 264, 3832, 25163, 295, 472, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06295189958937625, "compression_ratio": 1.7130801687763713, "no_speech_prob": 2.561217570473673e-06}, {"id": 1091, "seek": 536100, "start": 5361.0, "end": 5369.0, "text": " But because we have this interspersed ten each layers in there, the standard layers are squashing functions.", "tokens": [50364, 583, 570, 321, 362, 341, 728, 4952, 433, 292, 2064, 1184, 7914, 294, 456, 11, 264, 3832, 7914, 366, 2339, 11077, 6828, 13, 50764, 50764, 400, 370, 436, 747, 428, 7316, 293, 436, 4748, 30725, 309, 13, 50914, 50914, 400, 370, 512, 1216, 307, 4818, 281, 1066, 14702, 309, 281, 2092, 264, 2339, 11077, 13, 51264, 51264, 407, 309, 445, 4523, 484, 300, 1732, 670, 1045, 307, 257, 665, 2158, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10872603098551432, "compression_ratio": 1.5854922279792747, "no_speech_prob": 8.267621524282731e-06}, {"id": 1092, "seek": 536100, "start": 5369.0, "end": 5372.0, "text": " And so they take your distribution and they slightly squash it.", "tokens": [50364, 583, 570, 321, 362, 341, 728, 4952, 433, 292, 2064, 1184, 7914, 294, 456, 11, 264, 3832, 7914, 366, 2339, 11077, 6828, 13, 50764, 50764, 400, 370, 436, 747, 428, 7316, 293, 436, 4748, 30725, 309, 13, 50914, 50914, 400, 370, 512, 1216, 307, 4818, 281, 1066, 14702, 309, 281, 2092, 264, 2339, 11077, 13, 51264, 51264, 407, 309, 445, 4523, 484, 300, 1732, 670, 1045, 307, 257, 665, 2158, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10872603098551432, "compression_ratio": 1.5854922279792747, "no_speech_prob": 8.267621524282731e-06}, {"id": 1093, "seek": 536100, "start": 5372.0, "end": 5379.0, "text": " And so some game is necessary to keep expanding it to fight the squashing.", "tokens": [50364, 583, 570, 321, 362, 341, 728, 4952, 433, 292, 2064, 1184, 7914, 294, 456, 11, 264, 3832, 7914, 366, 2339, 11077, 6828, 13, 50764, 50764, 400, 370, 436, 747, 428, 7316, 293, 436, 4748, 30725, 309, 13, 50914, 50914, 400, 370, 512, 1216, 307, 4818, 281, 1066, 14702, 309, 281, 2092, 264, 2339, 11077, 13, 51264, 51264, 407, 309, 445, 4523, 484, 300, 1732, 670, 1045, 307, 257, 665, 2158, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10872603098551432, "compression_ratio": 1.5854922279792747, "no_speech_prob": 8.267621524282731e-06}, {"id": 1094, "seek": 536100, "start": 5379.0, "end": 5383.0, "text": " So it just turns out that five over three is a good value.", "tokens": [50364, 583, 570, 321, 362, 341, 728, 4952, 433, 292, 2064, 1184, 7914, 294, 456, 11, 264, 3832, 7914, 366, 2339, 11077, 6828, 13, 50764, 50764, 400, 370, 436, 747, 428, 7316, 293, 436, 4748, 30725, 309, 13, 50914, 50914, 400, 370, 512, 1216, 307, 4818, 281, 1066, 14702, 309, 281, 2092, 264, 2339, 11077, 13, 51264, 51264, 407, 309, 445, 4523, 484, 300, 1732, 670, 1045, 307, 257, 665, 2158, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10872603098551432, "compression_ratio": 1.5854922279792747, "no_speech_prob": 8.267621524282731e-06}, {"id": 1095, "seek": 538300, "start": 5383.0, "end": 5392.0, "text": " So if we have something too small, like one, we saw that things will come towards zero. But if it's something too high, let's do two.", "tokens": [50364, 407, 498, 321, 362, 746, 886, 1359, 11, 411, 472, 11, 321, 1866, 300, 721, 486, 808, 3030, 4018, 13, 583, 498, 309, 311, 746, 886, 1090, 11, 718, 311, 360, 732, 13, 50814, 50814, 1396, 510, 321, 536, 300, 13, 51014, 51014, 1042, 11, 718, 385, 360, 746, 257, 857, 544, 8084, 570, 370, 309, 311, 257, 857, 544, 8974, 13, 51214, 51214, 961, 311, 853, 1045, 13, 51314, 51314, 2264, 11, 370, 321, 536, 510, 300, 264, 21160, 763, 366, 516, 281, 312, 636, 886, 2416, 13, 51514, 51514, 2264, 11, 370, 1045, 576, 1884, 636, 886, 25408, 2430, 763, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07985947510906469, "compression_ratio": 1.76036866359447, "no_speech_prob": 7.527684829256032e-06}, {"id": 1096, "seek": 538300, "start": 5392.0, "end": 5396.0, "text": " Then here we see that.", "tokens": [50364, 407, 498, 321, 362, 746, 886, 1359, 11, 411, 472, 11, 321, 1866, 300, 721, 486, 808, 3030, 4018, 13, 583, 498, 309, 311, 746, 886, 1090, 11, 718, 311, 360, 732, 13, 50814, 50814, 1396, 510, 321, 536, 300, 13, 51014, 51014, 1042, 11, 718, 385, 360, 746, 257, 857, 544, 8084, 570, 370, 309, 311, 257, 857, 544, 8974, 13, 51214, 51214, 961, 311, 853, 1045, 13, 51314, 51314, 2264, 11, 370, 321, 536, 510, 300, 264, 21160, 763, 366, 516, 281, 312, 636, 886, 2416, 13, 51514, 51514, 2264, 11, 370, 1045, 576, 1884, 636, 886, 25408, 2430, 763, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07985947510906469, "compression_ratio": 1.76036866359447, "no_speech_prob": 7.527684829256032e-06}, {"id": 1097, "seek": 538300, "start": 5396.0, "end": 5400.0, "text": " Well, let me do something a bit more extreme because so it's a bit more visible.", "tokens": [50364, 407, 498, 321, 362, 746, 886, 1359, 11, 411, 472, 11, 321, 1866, 300, 721, 486, 808, 3030, 4018, 13, 583, 498, 309, 311, 746, 886, 1090, 11, 718, 311, 360, 732, 13, 50814, 50814, 1396, 510, 321, 536, 300, 13, 51014, 51014, 1042, 11, 718, 385, 360, 746, 257, 857, 544, 8084, 570, 370, 309, 311, 257, 857, 544, 8974, 13, 51214, 51214, 961, 311, 853, 1045, 13, 51314, 51314, 2264, 11, 370, 321, 536, 510, 300, 264, 21160, 763, 366, 516, 281, 312, 636, 886, 2416, 13, 51514, 51514, 2264, 11, 370, 1045, 576, 1884, 636, 886, 25408, 2430, 763, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07985947510906469, "compression_ratio": 1.76036866359447, "no_speech_prob": 7.527684829256032e-06}, {"id": 1098, "seek": 538300, "start": 5400.0, "end": 5402.0, "text": " Let's try three.", "tokens": [50364, 407, 498, 321, 362, 746, 886, 1359, 11, 411, 472, 11, 321, 1866, 300, 721, 486, 808, 3030, 4018, 13, 583, 498, 309, 311, 746, 886, 1090, 11, 718, 311, 360, 732, 13, 50814, 50814, 1396, 510, 321, 536, 300, 13, 51014, 51014, 1042, 11, 718, 385, 360, 746, 257, 857, 544, 8084, 570, 370, 309, 311, 257, 857, 544, 8974, 13, 51214, 51214, 961, 311, 853, 1045, 13, 51314, 51314, 2264, 11, 370, 321, 536, 510, 300, 264, 21160, 763, 366, 516, 281, 312, 636, 886, 2416, 13, 51514, 51514, 2264, 11, 370, 1045, 576, 1884, 636, 886, 25408, 2430, 763, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07985947510906469, "compression_ratio": 1.76036866359447, "no_speech_prob": 7.527684829256032e-06}, {"id": 1099, "seek": 538300, "start": 5402.0, "end": 5406.0, "text": " OK, so we see here that the saturations are going to be way too large.", "tokens": [50364, 407, 498, 321, 362, 746, 886, 1359, 11, 411, 472, 11, 321, 1866, 300, 721, 486, 808, 3030, 4018, 13, 583, 498, 309, 311, 746, 886, 1090, 11, 718, 311, 360, 732, 13, 50814, 50814, 1396, 510, 321, 536, 300, 13, 51014, 51014, 1042, 11, 718, 385, 360, 746, 257, 857, 544, 8084, 570, 370, 309, 311, 257, 857, 544, 8974, 13, 51214, 51214, 961, 311, 853, 1045, 13, 51314, 51314, 2264, 11, 370, 321, 536, 510, 300, 264, 21160, 763, 366, 516, 281, 312, 636, 886, 2416, 13, 51514, 51514, 2264, 11, 370, 1045, 576, 1884, 636, 886, 25408, 2430, 763, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07985947510906469, "compression_ratio": 1.76036866359447, "no_speech_prob": 7.527684829256032e-06}, {"id": 1100, "seek": 538300, "start": 5406.0, "end": 5410.0, "text": " OK, so three would create way too saturated activations.", "tokens": [50364, 407, 498, 321, 362, 746, 886, 1359, 11, 411, 472, 11, 321, 1866, 300, 721, 486, 808, 3030, 4018, 13, 583, 498, 309, 311, 746, 886, 1090, 11, 718, 311, 360, 732, 13, 50814, 50814, 1396, 510, 321, 536, 300, 13, 51014, 51014, 1042, 11, 718, 385, 360, 746, 257, 857, 544, 8084, 570, 370, 309, 311, 257, 857, 544, 8974, 13, 51214, 51214, 961, 311, 853, 1045, 13, 51314, 51314, 2264, 11, 370, 321, 536, 510, 300, 264, 21160, 763, 366, 516, 281, 312, 636, 886, 2416, 13, 51514, 51514, 2264, 11, 370, 1045, 576, 1884, 636, 886, 25408, 2430, 763, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07985947510906469, "compression_ratio": 1.76036866359447, "no_speech_prob": 7.527684829256032e-06}, {"id": 1101, "seek": 541000, "start": 5410.0, "end": 5417.0, "text": " So five over three is a good setting for a sandwich of linear layers with ten each activations.", "tokens": [50364, 407, 1732, 670, 1045, 307, 257, 665, 3287, 337, 257, 11141, 295, 8213, 7914, 365, 2064, 1184, 2430, 763, 13, 50714, 50714, 400, 309, 9810, 11652, 5660, 264, 3832, 25163, 412, 257, 10585, 935, 13, 50964, 50964, 823, 11, 6095, 11, 286, 362, 572, 1558, 689, 1732, 420, 1045, 1361, 490, 294, 452, 27822, 13, 51214, 51214, 1133, 321, 645, 1237, 412, 264, 1348, 5883, 2144, 11, 286, 536, 25790, 984, 300, 309, 11652, 5660, 264, 11141, 295, 8213, 293, 2064, 1184, 293, 300, 264, 27090, 307, 294, 257, 665, 3613, 13, 51664, 51664, 583, 286, 500, 380, 767, 458, 498, 341, 1361, 484, 295, 512, 5221, 8513, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1222267910442521, "compression_ratio": 1.6832740213523132, "no_speech_prob": 3.446498112680274e-06}, {"id": 1102, "seek": 541000, "start": 5417.0, "end": 5422.0, "text": " And it roughly stabilizes the standard deviation at a reasonable point.", "tokens": [50364, 407, 1732, 670, 1045, 307, 257, 665, 3287, 337, 257, 11141, 295, 8213, 7914, 365, 2064, 1184, 2430, 763, 13, 50714, 50714, 400, 309, 9810, 11652, 5660, 264, 3832, 25163, 412, 257, 10585, 935, 13, 50964, 50964, 823, 11, 6095, 11, 286, 362, 572, 1558, 689, 1732, 420, 1045, 1361, 490, 294, 452, 27822, 13, 51214, 51214, 1133, 321, 645, 1237, 412, 264, 1348, 5883, 2144, 11, 286, 536, 25790, 984, 300, 309, 11652, 5660, 264, 11141, 295, 8213, 293, 2064, 1184, 293, 300, 264, 27090, 307, 294, 257, 665, 3613, 13, 51664, 51664, 583, 286, 500, 380, 767, 458, 498, 341, 1361, 484, 295, 512, 5221, 8513, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1222267910442521, "compression_ratio": 1.6832740213523132, "no_speech_prob": 3.446498112680274e-06}, {"id": 1103, "seek": 541000, "start": 5422.0, "end": 5427.0, "text": " Now, honestly, I have no idea where five or three came from in my torch.", "tokens": [50364, 407, 1732, 670, 1045, 307, 257, 665, 3287, 337, 257, 11141, 295, 8213, 7914, 365, 2064, 1184, 2430, 763, 13, 50714, 50714, 400, 309, 9810, 11652, 5660, 264, 3832, 25163, 412, 257, 10585, 935, 13, 50964, 50964, 823, 11, 6095, 11, 286, 362, 572, 1558, 689, 1732, 420, 1045, 1361, 490, 294, 452, 27822, 13, 51214, 51214, 1133, 321, 645, 1237, 412, 264, 1348, 5883, 2144, 11, 286, 536, 25790, 984, 300, 309, 11652, 5660, 264, 11141, 295, 8213, 293, 2064, 1184, 293, 300, 264, 27090, 307, 294, 257, 665, 3613, 13, 51664, 51664, 583, 286, 500, 380, 767, 458, 498, 341, 1361, 484, 295, 512, 5221, 8513, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1222267910442521, "compression_ratio": 1.6832740213523132, "no_speech_prob": 3.446498112680274e-06}, {"id": 1104, "seek": 541000, "start": 5427.0, "end": 5436.0, "text": " When we were looking at the coming initialization, I see empirically that it stabilizes the sandwich of linear and ten each and that the saturation is in a good range.", "tokens": [50364, 407, 1732, 670, 1045, 307, 257, 665, 3287, 337, 257, 11141, 295, 8213, 7914, 365, 2064, 1184, 2430, 763, 13, 50714, 50714, 400, 309, 9810, 11652, 5660, 264, 3832, 25163, 412, 257, 10585, 935, 13, 50964, 50964, 823, 11, 6095, 11, 286, 362, 572, 1558, 689, 1732, 420, 1045, 1361, 490, 294, 452, 27822, 13, 51214, 51214, 1133, 321, 645, 1237, 412, 264, 1348, 5883, 2144, 11, 286, 536, 25790, 984, 300, 309, 11652, 5660, 264, 11141, 295, 8213, 293, 2064, 1184, 293, 300, 264, 27090, 307, 294, 257, 665, 3613, 13, 51664, 51664, 583, 286, 500, 380, 767, 458, 498, 341, 1361, 484, 295, 512, 5221, 8513, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1222267910442521, "compression_ratio": 1.6832740213523132, "no_speech_prob": 3.446498112680274e-06}, {"id": 1105, "seek": 541000, "start": 5436.0, "end": 5439.0, "text": " But I don't actually know if this came out of some math formula.", "tokens": [50364, 407, 1732, 670, 1045, 307, 257, 665, 3287, 337, 257, 11141, 295, 8213, 7914, 365, 2064, 1184, 2430, 763, 13, 50714, 50714, 400, 309, 9810, 11652, 5660, 264, 3832, 25163, 412, 257, 10585, 935, 13, 50964, 50964, 823, 11, 6095, 11, 286, 362, 572, 1558, 689, 1732, 420, 1045, 1361, 490, 294, 452, 27822, 13, 51214, 51214, 1133, 321, 645, 1237, 412, 264, 1348, 5883, 2144, 11, 286, 536, 25790, 984, 300, 309, 11652, 5660, 264, 11141, 295, 8213, 293, 2064, 1184, 293, 300, 264, 27090, 307, 294, 257, 665, 3613, 13, 51664, 51664, 583, 286, 500, 380, 767, 458, 498, 341, 1361, 484, 295, 512, 5221, 8513, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1222267910442521, "compression_ratio": 1.6832740213523132, "no_speech_prob": 3.446498112680274e-06}, {"id": 1106, "seek": 543900, "start": 5439.0, "end": 5444.0, "text": " I tried searching briefly for where this comes from, but I wasn't able to find anything.", "tokens": [50364, 286, 3031, 10808, 10515, 337, 689, 341, 1487, 490, 11, 457, 286, 2067, 380, 1075, 281, 915, 1340, 13, 50614, 50614, 583, 3297, 321, 536, 300, 25790, 984, 11, 613, 366, 588, 1481, 22526, 13, 50764, 50764, 2621, 27090, 307, 9810, 1732, 3043, 11, 597, 307, 257, 1238, 665, 1230, 13, 50914, 50914, 400, 341, 307, 257, 665, 3287, 295, 264, 6052, 294, 341, 4319, 13, 51164, 51164, 13157, 11, 321, 393, 360, 264, 1900, 912, 551, 365, 264, 2771, 2448, 13, 51314, 51314, 407, 510, 307, 257, 588, 912, 6367, 498, 309, 311, 257, 2064, 1184, 13, 51464, 51464, 583, 2602, 295, 1940, 264, 4583, 300, 484, 11, 286, 478, 1940, 264, 2771, 13, 51614, 51614, 400, 550, 286, 478, 611, 4099, 264, 914, 293, 264, 3832, 25163, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08896571486743528, "compression_ratio": 1.6928104575163399, "no_speech_prob": 5.594133199338103e-06}, {"id": 1107, "seek": 543900, "start": 5444.0, "end": 5447.0, "text": " But certainly we see that empirically, these are very nice ranges.", "tokens": [50364, 286, 3031, 10808, 10515, 337, 689, 341, 1487, 490, 11, 457, 286, 2067, 380, 1075, 281, 915, 1340, 13, 50614, 50614, 583, 3297, 321, 536, 300, 25790, 984, 11, 613, 366, 588, 1481, 22526, 13, 50764, 50764, 2621, 27090, 307, 9810, 1732, 3043, 11, 597, 307, 257, 1238, 665, 1230, 13, 50914, 50914, 400, 341, 307, 257, 665, 3287, 295, 264, 6052, 294, 341, 4319, 13, 51164, 51164, 13157, 11, 321, 393, 360, 264, 1900, 912, 551, 365, 264, 2771, 2448, 13, 51314, 51314, 407, 510, 307, 257, 588, 912, 6367, 498, 309, 311, 257, 2064, 1184, 13, 51464, 51464, 583, 2602, 295, 1940, 264, 4583, 300, 484, 11, 286, 478, 1940, 264, 2771, 13, 51614, 51614, 400, 550, 286, 478, 611, 4099, 264, 914, 293, 264, 3832, 25163, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08896571486743528, "compression_ratio": 1.6928104575163399, "no_speech_prob": 5.594133199338103e-06}, {"id": 1108, "seek": 543900, "start": 5447.0, "end": 5450.0, "text": " Our saturation is roughly five percent, which is a pretty good number.", "tokens": [50364, 286, 3031, 10808, 10515, 337, 689, 341, 1487, 490, 11, 457, 286, 2067, 380, 1075, 281, 915, 1340, 13, 50614, 50614, 583, 3297, 321, 536, 300, 25790, 984, 11, 613, 366, 588, 1481, 22526, 13, 50764, 50764, 2621, 27090, 307, 9810, 1732, 3043, 11, 597, 307, 257, 1238, 665, 1230, 13, 50914, 50914, 400, 341, 307, 257, 665, 3287, 295, 264, 6052, 294, 341, 4319, 13, 51164, 51164, 13157, 11, 321, 393, 360, 264, 1900, 912, 551, 365, 264, 2771, 2448, 13, 51314, 51314, 407, 510, 307, 257, 588, 912, 6367, 498, 309, 311, 257, 2064, 1184, 13, 51464, 51464, 583, 2602, 295, 1940, 264, 4583, 300, 484, 11, 286, 478, 1940, 264, 2771, 13, 51614, 51614, 400, 550, 286, 478, 611, 4099, 264, 914, 293, 264, 3832, 25163, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08896571486743528, "compression_ratio": 1.6928104575163399, "no_speech_prob": 5.594133199338103e-06}, {"id": 1109, "seek": 543900, "start": 5450.0, "end": 5455.0, "text": " And this is a good setting of the gain in this context.", "tokens": [50364, 286, 3031, 10808, 10515, 337, 689, 341, 1487, 490, 11, 457, 286, 2067, 380, 1075, 281, 915, 1340, 13, 50614, 50614, 583, 3297, 321, 536, 300, 25790, 984, 11, 613, 366, 588, 1481, 22526, 13, 50764, 50764, 2621, 27090, 307, 9810, 1732, 3043, 11, 597, 307, 257, 1238, 665, 1230, 13, 50914, 50914, 400, 341, 307, 257, 665, 3287, 295, 264, 6052, 294, 341, 4319, 13, 51164, 51164, 13157, 11, 321, 393, 360, 264, 1900, 912, 551, 365, 264, 2771, 2448, 13, 51314, 51314, 407, 510, 307, 257, 588, 912, 6367, 498, 309, 311, 257, 2064, 1184, 13, 51464, 51464, 583, 2602, 295, 1940, 264, 4583, 300, 484, 11, 286, 478, 1940, 264, 2771, 13, 51614, 51614, 400, 550, 286, 478, 611, 4099, 264, 914, 293, 264, 3832, 25163, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08896571486743528, "compression_ratio": 1.6928104575163399, "no_speech_prob": 5.594133199338103e-06}, {"id": 1110, "seek": 543900, "start": 5455.0, "end": 5458.0, "text": " Similarly, we can do the exact same thing with the gradients.", "tokens": [50364, 286, 3031, 10808, 10515, 337, 689, 341, 1487, 490, 11, 457, 286, 2067, 380, 1075, 281, 915, 1340, 13, 50614, 50614, 583, 3297, 321, 536, 300, 25790, 984, 11, 613, 366, 588, 1481, 22526, 13, 50764, 50764, 2621, 27090, 307, 9810, 1732, 3043, 11, 597, 307, 257, 1238, 665, 1230, 13, 50914, 50914, 400, 341, 307, 257, 665, 3287, 295, 264, 6052, 294, 341, 4319, 13, 51164, 51164, 13157, 11, 321, 393, 360, 264, 1900, 912, 551, 365, 264, 2771, 2448, 13, 51314, 51314, 407, 510, 307, 257, 588, 912, 6367, 498, 309, 311, 257, 2064, 1184, 13, 51464, 51464, 583, 2602, 295, 1940, 264, 4583, 300, 484, 11, 286, 478, 1940, 264, 2771, 13, 51614, 51614, 400, 550, 286, 478, 611, 4099, 264, 914, 293, 264, 3832, 25163, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08896571486743528, "compression_ratio": 1.6928104575163399, "no_speech_prob": 5.594133199338103e-06}, {"id": 1111, "seek": 543900, "start": 5458.0, "end": 5461.0, "text": " So here is a very same loop if it's a ten each.", "tokens": [50364, 286, 3031, 10808, 10515, 337, 689, 341, 1487, 490, 11, 457, 286, 2067, 380, 1075, 281, 915, 1340, 13, 50614, 50614, 583, 3297, 321, 536, 300, 25790, 984, 11, 613, 366, 588, 1481, 22526, 13, 50764, 50764, 2621, 27090, 307, 9810, 1732, 3043, 11, 597, 307, 257, 1238, 665, 1230, 13, 50914, 50914, 400, 341, 307, 257, 665, 3287, 295, 264, 6052, 294, 341, 4319, 13, 51164, 51164, 13157, 11, 321, 393, 360, 264, 1900, 912, 551, 365, 264, 2771, 2448, 13, 51314, 51314, 407, 510, 307, 257, 588, 912, 6367, 498, 309, 311, 257, 2064, 1184, 13, 51464, 51464, 583, 2602, 295, 1940, 264, 4583, 300, 484, 11, 286, 478, 1940, 264, 2771, 13, 51614, 51614, 400, 550, 286, 478, 611, 4099, 264, 914, 293, 264, 3832, 25163, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08896571486743528, "compression_ratio": 1.6928104575163399, "no_speech_prob": 5.594133199338103e-06}, {"id": 1112, "seek": 543900, "start": 5461.0, "end": 5464.0, "text": " But instead of taking the layer that out, I'm taking the grad.", "tokens": [50364, 286, 3031, 10808, 10515, 337, 689, 341, 1487, 490, 11, 457, 286, 2067, 380, 1075, 281, 915, 1340, 13, 50614, 50614, 583, 3297, 321, 536, 300, 25790, 984, 11, 613, 366, 588, 1481, 22526, 13, 50764, 50764, 2621, 27090, 307, 9810, 1732, 3043, 11, 597, 307, 257, 1238, 665, 1230, 13, 50914, 50914, 400, 341, 307, 257, 665, 3287, 295, 264, 6052, 294, 341, 4319, 13, 51164, 51164, 13157, 11, 321, 393, 360, 264, 1900, 912, 551, 365, 264, 2771, 2448, 13, 51314, 51314, 407, 510, 307, 257, 588, 912, 6367, 498, 309, 311, 257, 2064, 1184, 13, 51464, 51464, 583, 2602, 295, 1940, 264, 4583, 300, 484, 11, 286, 478, 1940, 264, 2771, 13, 51614, 51614, 400, 550, 286, 478, 611, 4099, 264, 914, 293, 264, 3832, 25163, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08896571486743528, "compression_ratio": 1.6928104575163399, "no_speech_prob": 5.594133199338103e-06}, {"id": 1113, "seek": 543900, "start": 5464.0, "end": 5467.0, "text": " And then I'm also showing the mean and the standard deviation.", "tokens": [50364, 286, 3031, 10808, 10515, 337, 689, 341, 1487, 490, 11, 457, 286, 2067, 380, 1075, 281, 915, 1340, 13, 50614, 50614, 583, 3297, 321, 536, 300, 25790, 984, 11, 613, 366, 588, 1481, 22526, 13, 50764, 50764, 2621, 27090, 307, 9810, 1732, 3043, 11, 597, 307, 257, 1238, 665, 1230, 13, 50914, 50914, 400, 341, 307, 257, 665, 3287, 295, 264, 6052, 294, 341, 4319, 13, 51164, 51164, 13157, 11, 321, 393, 360, 264, 1900, 912, 551, 365, 264, 2771, 2448, 13, 51314, 51314, 407, 510, 307, 257, 588, 912, 6367, 498, 309, 311, 257, 2064, 1184, 13, 51464, 51464, 583, 2602, 295, 1940, 264, 4583, 300, 484, 11, 286, 478, 1940, 264, 2771, 13, 51614, 51614, 400, 550, 286, 478, 611, 4099, 264, 914, 293, 264, 3832, 25163, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08896571486743528, "compression_ratio": 1.6928104575163399, "no_speech_prob": 5.594133199338103e-06}, {"id": 1114, "seek": 546700, "start": 5467.0, "end": 5470.0, "text": " And I'm plotting the histogram of these values.", "tokens": [50364, 400, 286, 478, 41178, 264, 49816, 295, 613, 4190, 13, 50514, 50514, 400, 370, 291, 603, 536, 300, 264, 16235, 7316, 307, 6457, 10585, 13, 50664, 50664, 400, 294, 1729, 11, 437, 321, 434, 1237, 337, 307, 300, 439, 264, 819, 7914, 294, 341, 11141, 575, 9810, 264, 912, 16235, 13, 50964, 50964, 9514, 366, 406, 41684, 420, 35175, 13, 51114, 51114, 407, 321, 393, 11, 337, 1365, 11, 808, 510, 293, 321, 393, 747, 257, 574, 412, 437, 2314, 498, 341, 1216, 390, 636, 886, 1359, 13, 51364, 51364, 407, 341, 390, 935, 1732, 13, 51514, 51514, 1396, 291, 536, 264, 700, 295, 439, 11, 264, 2430, 763, 366, 41684, 281, 4018, 11, 457, 611, 264, 2771, 2448, 366, 884, 746, 3657, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.047517791390419006, "compression_ratio": 1.7287581699346406, "no_speech_prob": 1.3211203622631729e-05}, {"id": 1115, "seek": 546700, "start": 5470.0, "end": 5473.0, "text": " And so you'll see that the gradient distribution is fairly reasonable.", "tokens": [50364, 400, 286, 478, 41178, 264, 49816, 295, 613, 4190, 13, 50514, 50514, 400, 370, 291, 603, 536, 300, 264, 16235, 7316, 307, 6457, 10585, 13, 50664, 50664, 400, 294, 1729, 11, 437, 321, 434, 1237, 337, 307, 300, 439, 264, 819, 7914, 294, 341, 11141, 575, 9810, 264, 912, 16235, 13, 50964, 50964, 9514, 366, 406, 41684, 420, 35175, 13, 51114, 51114, 407, 321, 393, 11, 337, 1365, 11, 808, 510, 293, 321, 393, 747, 257, 574, 412, 437, 2314, 498, 341, 1216, 390, 636, 886, 1359, 13, 51364, 51364, 407, 341, 390, 935, 1732, 13, 51514, 51514, 1396, 291, 536, 264, 700, 295, 439, 11, 264, 2430, 763, 366, 41684, 281, 4018, 11, 457, 611, 264, 2771, 2448, 366, 884, 746, 3657, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.047517791390419006, "compression_ratio": 1.7287581699346406, "no_speech_prob": 1.3211203622631729e-05}, {"id": 1116, "seek": 546700, "start": 5473.0, "end": 5479.0, "text": " And in particular, what we're looking for is that all the different layers in this sandwich has roughly the same gradient.", "tokens": [50364, 400, 286, 478, 41178, 264, 49816, 295, 613, 4190, 13, 50514, 50514, 400, 370, 291, 603, 536, 300, 264, 16235, 7316, 307, 6457, 10585, 13, 50664, 50664, 400, 294, 1729, 11, 437, 321, 434, 1237, 337, 307, 300, 439, 264, 819, 7914, 294, 341, 11141, 575, 9810, 264, 912, 16235, 13, 50964, 50964, 9514, 366, 406, 41684, 420, 35175, 13, 51114, 51114, 407, 321, 393, 11, 337, 1365, 11, 808, 510, 293, 321, 393, 747, 257, 574, 412, 437, 2314, 498, 341, 1216, 390, 636, 886, 1359, 13, 51364, 51364, 407, 341, 390, 935, 1732, 13, 51514, 51514, 1396, 291, 536, 264, 700, 295, 439, 11, 264, 2430, 763, 366, 41684, 281, 4018, 11, 457, 611, 264, 2771, 2448, 366, 884, 746, 3657, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.047517791390419006, "compression_ratio": 1.7287581699346406, "no_speech_prob": 1.3211203622631729e-05}, {"id": 1117, "seek": 546700, "start": 5479.0, "end": 5482.0, "text": " Things are not shrinking or exploding.", "tokens": [50364, 400, 286, 478, 41178, 264, 49816, 295, 613, 4190, 13, 50514, 50514, 400, 370, 291, 603, 536, 300, 264, 16235, 7316, 307, 6457, 10585, 13, 50664, 50664, 400, 294, 1729, 11, 437, 321, 434, 1237, 337, 307, 300, 439, 264, 819, 7914, 294, 341, 11141, 575, 9810, 264, 912, 16235, 13, 50964, 50964, 9514, 366, 406, 41684, 420, 35175, 13, 51114, 51114, 407, 321, 393, 11, 337, 1365, 11, 808, 510, 293, 321, 393, 747, 257, 574, 412, 437, 2314, 498, 341, 1216, 390, 636, 886, 1359, 13, 51364, 51364, 407, 341, 390, 935, 1732, 13, 51514, 51514, 1396, 291, 536, 264, 700, 295, 439, 11, 264, 2430, 763, 366, 41684, 281, 4018, 11, 457, 611, 264, 2771, 2448, 366, 884, 746, 3657, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.047517791390419006, "compression_ratio": 1.7287581699346406, "no_speech_prob": 1.3211203622631729e-05}, {"id": 1118, "seek": 546700, "start": 5482.0, "end": 5487.0, "text": " So we can, for example, come here and we can take a look at what happens if this game was way too small.", "tokens": [50364, 400, 286, 478, 41178, 264, 49816, 295, 613, 4190, 13, 50514, 50514, 400, 370, 291, 603, 536, 300, 264, 16235, 7316, 307, 6457, 10585, 13, 50664, 50664, 400, 294, 1729, 11, 437, 321, 434, 1237, 337, 307, 300, 439, 264, 819, 7914, 294, 341, 11141, 575, 9810, 264, 912, 16235, 13, 50964, 50964, 9514, 366, 406, 41684, 420, 35175, 13, 51114, 51114, 407, 321, 393, 11, 337, 1365, 11, 808, 510, 293, 321, 393, 747, 257, 574, 412, 437, 2314, 498, 341, 1216, 390, 636, 886, 1359, 13, 51364, 51364, 407, 341, 390, 935, 1732, 13, 51514, 51514, 1396, 291, 536, 264, 700, 295, 439, 11, 264, 2430, 763, 366, 41684, 281, 4018, 11, 457, 611, 264, 2771, 2448, 366, 884, 746, 3657, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.047517791390419006, "compression_ratio": 1.7287581699346406, "no_speech_prob": 1.3211203622631729e-05}, {"id": 1119, "seek": 546700, "start": 5487.0, "end": 5490.0, "text": " So this was point five.", "tokens": [50364, 400, 286, 478, 41178, 264, 49816, 295, 613, 4190, 13, 50514, 50514, 400, 370, 291, 603, 536, 300, 264, 16235, 7316, 307, 6457, 10585, 13, 50664, 50664, 400, 294, 1729, 11, 437, 321, 434, 1237, 337, 307, 300, 439, 264, 819, 7914, 294, 341, 11141, 575, 9810, 264, 912, 16235, 13, 50964, 50964, 9514, 366, 406, 41684, 420, 35175, 13, 51114, 51114, 407, 321, 393, 11, 337, 1365, 11, 808, 510, 293, 321, 393, 747, 257, 574, 412, 437, 2314, 498, 341, 1216, 390, 636, 886, 1359, 13, 51364, 51364, 407, 341, 390, 935, 1732, 13, 51514, 51514, 1396, 291, 536, 264, 700, 295, 439, 11, 264, 2430, 763, 366, 41684, 281, 4018, 11, 457, 611, 264, 2771, 2448, 366, 884, 746, 3657, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.047517791390419006, "compression_ratio": 1.7287581699346406, "no_speech_prob": 1.3211203622631729e-05}, {"id": 1120, "seek": 546700, "start": 5490.0, "end": 5496.0, "text": " Then you see the first of all, the activations are shrinking to zero, but also the gradients are doing something weird.", "tokens": [50364, 400, 286, 478, 41178, 264, 49816, 295, 613, 4190, 13, 50514, 50514, 400, 370, 291, 603, 536, 300, 264, 16235, 7316, 307, 6457, 10585, 13, 50664, 50664, 400, 294, 1729, 11, 437, 321, 434, 1237, 337, 307, 300, 439, 264, 819, 7914, 294, 341, 11141, 575, 9810, 264, 912, 16235, 13, 50964, 50964, 9514, 366, 406, 41684, 420, 35175, 13, 51114, 51114, 407, 321, 393, 11, 337, 1365, 11, 808, 510, 293, 321, 393, 747, 257, 574, 412, 437, 2314, 498, 341, 1216, 390, 636, 886, 1359, 13, 51364, 51364, 407, 341, 390, 935, 1732, 13, 51514, 51514, 1396, 291, 536, 264, 700, 295, 439, 11, 264, 2430, 763, 366, 41684, 281, 4018, 11, 457, 611, 264, 2771, 2448, 366, 884, 746, 3657, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.047517791390419006, "compression_ratio": 1.7287581699346406, "no_speech_prob": 1.3211203622631729e-05}, {"id": 1121, "seek": 549600, "start": 5496.0, "end": 5501.0, "text": " The gradients started out here and then now they're like expanding out.", "tokens": [50364, 440, 2771, 2448, 1409, 484, 510, 293, 550, 586, 436, 434, 411, 14702, 484, 13, 50614, 50614, 400, 14138, 11, 498, 321, 11, 337, 1365, 11, 362, 257, 886, 1090, 295, 257, 6052, 11, 370, 411, 1045, 11, 550, 321, 536, 300, 611, 264, 2771, 2448, 362, 456, 311, 512, 37277, 9889, 516, 322, 689, 382, 291, 352, 666, 7731, 293, 7731, 7914, 11, 264, 2430, 763, 366, 611, 4473, 13, 51264, 51264, 400, 370, 300, 311, 406, 437, 321, 528, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.11508981571641079, "compression_ratio": 1.6183574879227054, "no_speech_prob": 3.340416924402234e-06}, {"id": 1122, "seek": 549600, "start": 5501.0, "end": 5514.0, "text": " And similarly, if we, for example, have a too high of a gain, so like three, then we see that also the gradients have there's some asymmetry going on where as you go into deeper and deeper layers, the activations are also changing.", "tokens": [50364, 440, 2771, 2448, 1409, 484, 510, 293, 550, 586, 436, 434, 411, 14702, 484, 13, 50614, 50614, 400, 14138, 11, 498, 321, 11, 337, 1365, 11, 362, 257, 886, 1090, 295, 257, 6052, 11, 370, 411, 1045, 11, 550, 321, 536, 300, 611, 264, 2771, 2448, 362, 456, 311, 512, 37277, 9889, 516, 322, 689, 382, 291, 352, 666, 7731, 293, 7731, 7914, 11, 264, 2430, 763, 366, 611, 4473, 13, 51264, 51264, 400, 370, 300, 311, 406, 437, 321, 528, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.11508981571641079, "compression_ratio": 1.6183574879227054, "no_speech_prob": 3.340416924402234e-06}, {"id": 1123, "seek": 549600, "start": 5514.0, "end": 5515.0, "text": " And so that's not what we want.", "tokens": [50364, 440, 2771, 2448, 1409, 484, 510, 293, 550, 586, 436, 434, 411, 14702, 484, 13, 50614, 50614, 400, 14138, 11, 498, 321, 11, 337, 1365, 11, 362, 257, 886, 1090, 295, 257, 6052, 11, 370, 411, 1045, 11, 550, 321, 536, 300, 611, 264, 2771, 2448, 362, 456, 311, 512, 37277, 9889, 516, 322, 689, 382, 291, 352, 666, 7731, 293, 7731, 7914, 11, 264, 2430, 763, 366, 611, 4473, 13, 51264, 51264, 400, 370, 300, 311, 406, 437, 321, 528, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.11508981571641079, "compression_ratio": 1.6183574879227054, "no_speech_prob": 3.340416924402234e-06}, {"id": 1124, "seek": 551500, "start": 5515.0, "end": 5527.0, "text": " And in this case, we saw that without the use of bathroom as we are going through right now, we have to very carefully set those gains to get nice activations in both the forward pass and the backward pass.", "tokens": [50364, 400, 294, 341, 1389, 11, 321, 1866, 300, 1553, 264, 764, 295, 8687, 382, 321, 366, 516, 807, 558, 586, 11, 321, 362, 281, 588, 7500, 992, 729, 16823, 281, 483, 1481, 2430, 763, 294, 1293, 264, 2128, 1320, 293, 264, 23897, 1320, 13, 50964, 50964, 823, 11, 949, 321, 1286, 322, 281, 5418, 25138, 11, 286, 576, 611, 411, 281, 747, 257, 574, 412, 437, 2314, 562, 321, 362, 572, 26866, 6815, 510, 13, 51264, 51264, 407, 1189, 3349, 439, 264, 26866, 2107, 28263, 1088, 11, 457, 5145, 264, 1216, 412, 1732, 670, 1045, 11, 321, 586, 362, 445, 257, 7410, 8213, 11141, 13, 51714, 51714, 407, 718, 311, 536, 437, 2314, 281, 264, 2430, 763, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11200473347648246, "compression_ratio": 1.728813559322034, "no_speech_prob": 2.2602323497267207e-06}, {"id": 1125, "seek": 551500, "start": 5527.0, "end": 5533.0, "text": " Now, before we move on to passion realization, I would also like to take a look at what happens when we have no teenage units here.", "tokens": [50364, 400, 294, 341, 1389, 11, 321, 1866, 300, 1553, 264, 764, 295, 8687, 382, 321, 366, 516, 807, 558, 586, 11, 321, 362, 281, 588, 7500, 992, 729, 16823, 281, 483, 1481, 2430, 763, 294, 1293, 264, 2128, 1320, 293, 264, 23897, 1320, 13, 50964, 50964, 823, 11, 949, 321, 1286, 322, 281, 5418, 25138, 11, 286, 576, 611, 411, 281, 747, 257, 574, 412, 437, 2314, 562, 321, 362, 572, 26866, 6815, 510, 13, 51264, 51264, 407, 1189, 3349, 439, 264, 26866, 2107, 28263, 1088, 11, 457, 5145, 264, 1216, 412, 1732, 670, 1045, 11, 321, 586, 362, 445, 257, 7410, 8213, 11141, 13, 51714, 51714, 407, 718, 311, 536, 437, 2314, 281, 264, 2430, 763, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11200473347648246, "compression_ratio": 1.728813559322034, "no_speech_prob": 2.2602323497267207e-06}, {"id": 1126, "seek": 551500, "start": 5533.0, "end": 5542.0, "text": " So erasing all the teenage nonlinearities, but keeping the game at five over three, we now have just a giant linear sandwich.", "tokens": [50364, 400, 294, 341, 1389, 11, 321, 1866, 300, 1553, 264, 764, 295, 8687, 382, 321, 366, 516, 807, 558, 586, 11, 321, 362, 281, 588, 7500, 992, 729, 16823, 281, 483, 1481, 2430, 763, 294, 1293, 264, 2128, 1320, 293, 264, 23897, 1320, 13, 50964, 50964, 823, 11, 949, 321, 1286, 322, 281, 5418, 25138, 11, 286, 576, 611, 411, 281, 747, 257, 574, 412, 437, 2314, 562, 321, 362, 572, 26866, 6815, 510, 13, 51264, 51264, 407, 1189, 3349, 439, 264, 26866, 2107, 28263, 1088, 11, 457, 5145, 264, 1216, 412, 1732, 670, 1045, 11, 321, 586, 362, 445, 257, 7410, 8213, 11141, 13, 51714, 51714, 407, 718, 311, 536, 437, 2314, 281, 264, 2430, 763, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11200473347648246, "compression_ratio": 1.728813559322034, "no_speech_prob": 2.2602323497267207e-06}, {"id": 1127, "seek": 551500, "start": 5542.0, "end": 5544.0, "text": " So let's see what happens to the activations.", "tokens": [50364, 400, 294, 341, 1389, 11, 321, 1866, 300, 1553, 264, 764, 295, 8687, 382, 321, 366, 516, 807, 558, 586, 11, 321, 362, 281, 588, 7500, 992, 729, 16823, 281, 483, 1481, 2430, 763, 294, 1293, 264, 2128, 1320, 293, 264, 23897, 1320, 13, 50964, 50964, 823, 11, 949, 321, 1286, 322, 281, 5418, 25138, 11, 286, 576, 611, 411, 281, 747, 257, 574, 412, 437, 2314, 562, 321, 362, 572, 26866, 6815, 510, 13, 51264, 51264, 407, 1189, 3349, 439, 264, 26866, 2107, 28263, 1088, 11, 457, 5145, 264, 1216, 412, 1732, 670, 1045, 11, 321, 586, 362, 445, 257, 7410, 8213, 11141, 13, 51714, 51714, 407, 718, 311, 536, 437, 2314, 281, 264, 2430, 763, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11200473347648246, "compression_ratio": 1.728813559322034, "no_speech_prob": 2.2602323497267207e-06}, {"id": 1128, "seek": 554400, "start": 5544.0, "end": 5549.0, "text": " As we saw before, the correct game here is one that is the standard deviation preserving game.", "tokens": [50364, 1018, 321, 1866, 949, 11, 264, 3006, 1216, 510, 307, 472, 300, 307, 264, 3832, 25163, 33173, 1216, 13, 50614, 50614, 407, 472, 935, 2309, 11, 2309, 11, 3407, 307, 886, 1090, 13, 50814, 50814, 400, 370, 437, 311, 516, 281, 1051, 586, 307, 264, 3480, 13, 51014, 51014, 286, 362, 281, 1319, 341, 281, 312, 8213, 13, 51114, 51114, 407, 321, 366, 570, 456, 311, 572, 544, 26866, 4150, 13, 51314, 51314, 400, 718, 385, 1319, 341, 281, 8213, 382, 731, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09984042726714036, "compression_ratio": 1.6173469387755102, "no_speech_prob": 1.8057551642414182e-05}, {"id": 1129, "seek": 554400, "start": 5549.0, "end": 5553.0, "text": " So one point six, six, seven is too high.", "tokens": [50364, 1018, 321, 1866, 949, 11, 264, 3006, 1216, 510, 307, 472, 300, 307, 264, 3832, 25163, 33173, 1216, 13, 50614, 50614, 407, 472, 935, 2309, 11, 2309, 11, 3407, 307, 886, 1090, 13, 50814, 50814, 400, 370, 437, 311, 516, 281, 1051, 586, 307, 264, 3480, 13, 51014, 51014, 286, 362, 281, 1319, 341, 281, 312, 8213, 13, 51114, 51114, 407, 321, 366, 570, 456, 311, 572, 544, 26866, 4150, 13, 51314, 51314, 400, 718, 385, 1319, 341, 281, 8213, 382, 731, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09984042726714036, "compression_ratio": 1.6173469387755102, "no_speech_prob": 1.8057551642414182e-05}, {"id": 1130, "seek": 554400, "start": 5553.0, "end": 5557.0, "text": " And so what's going to happen now is the following.", "tokens": [50364, 1018, 321, 1866, 949, 11, 264, 3006, 1216, 510, 307, 472, 300, 307, 264, 3832, 25163, 33173, 1216, 13, 50614, 50614, 407, 472, 935, 2309, 11, 2309, 11, 3407, 307, 886, 1090, 13, 50814, 50814, 400, 370, 437, 311, 516, 281, 1051, 586, 307, 264, 3480, 13, 51014, 51014, 286, 362, 281, 1319, 341, 281, 312, 8213, 13, 51114, 51114, 407, 321, 366, 570, 456, 311, 572, 544, 26866, 4150, 13, 51314, 51314, 400, 718, 385, 1319, 341, 281, 8213, 382, 731, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09984042726714036, "compression_ratio": 1.6173469387755102, "no_speech_prob": 1.8057551642414182e-05}, {"id": 1131, "seek": 554400, "start": 5557.0, "end": 5559.0, "text": " I have to change this to be linear.", "tokens": [50364, 1018, 321, 1866, 949, 11, 264, 3006, 1216, 510, 307, 472, 300, 307, 264, 3832, 25163, 33173, 1216, 13, 50614, 50614, 407, 472, 935, 2309, 11, 2309, 11, 3407, 307, 886, 1090, 13, 50814, 50814, 400, 370, 437, 311, 516, 281, 1051, 586, 307, 264, 3480, 13, 51014, 51014, 286, 362, 281, 1319, 341, 281, 312, 8213, 13, 51114, 51114, 407, 321, 366, 570, 456, 311, 572, 544, 26866, 4150, 13, 51314, 51314, 400, 718, 385, 1319, 341, 281, 8213, 382, 731, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09984042726714036, "compression_ratio": 1.6173469387755102, "no_speech_prob": 1.8057551642414182e-05}, {"id": 1132, "seek": 554400, "start": 5559.0, "end": 5563.0, "text": " So we are because there's no more teenage players.", "tokens": [50364, 1018, 321, 1866, 949, 11, 264, 3006, 1216, 510, 307, 472, 300, 307, 264, 3832, 25163, 33173, 1216, 13, 50614, 50614, 407, 472, 935, 2309, 11, 2309, 11, 3407, 307, 886, 1090, 13, 50814, 50814, 400, 370, 437, 311, 516, 281, 1051, 586, 307, 264, 3480, 13, 51014, 51014, 286, 362, 281, 1319, 341, 281, 312, 8213, 13, 51114, 51114, 407, 321, 366, 570, 456, 311, 572, 544, 26866, 4150, 13, 51314, 51314, 400, 718, 385, 1319, 341, 281, 8213, 382, 731, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09984042726714036, "compression_ratio": 1.6173469387755102, "no_speech_prob": 1.8057551642414182e-05}, {"id": 1133, "seek": 554400, "start": 5563.0, "end": 5566.0, "text": " And let me change this to linear as well.", "tokens": [50364, 1018, 321, 1866, 949, 11, 264, 3006, 1216, 510, 307, 472, 300, 307, 264, 3832, 25163, 33173, 1216, 13, 50614, 50614, 407, 472, 935, 2309, 11, 2309, 11, 3407, 307, 886, 1090, 13, 50814, 50814, 400, 370, 437, 311, 516, 281, 1051, 586, 307, 264, 3480, 13, 51014, 51014, 286, 362, 281, 1319, 341, 281, 312, 8213, 13, 51114, 51114, 407, 321, 366, 570, 456, 311, 572, 544, 26866, 4150, 13, 51314, 51314, 400, 718, 385, 1319, 341, 281, 8213, 382, 731, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.09984042726714036, "compression_ratio": 1.6173469387755102, "no_speech_prob": 1.8057551642414182e-05}, {"id": 1134, "seek": 556600, "start": 5566.0, "end": 5575.0, "text": " So we're seeing is the activations started out on the blue and have by layer four become very diffuse.", "tokens": [50364, 407, 321, 434, 2577, 307, 264, 2430, 763, 1409, 484, 322, 264, 3344, 293, 362, 538, 4583, 1451, 1813, 588, 42165, 13, 50814, 50814, 407, 437, 311, 2737, 281, 264, 2430, 763, 307, 341, 13, 50914, 50914, 400, 365, 264, 2771, 2448, 322, 264, 1192, 4583, 11, 264, 24433, 11, 264, 16235, 12523, 366, 264, 9656, 293, 550, 436, 48696, 382, 291, 352, 760, 7731, 294, 264, 7914, 13, 51414, 51414, 400, 370, 1936, 291, 362, 364, 37277, 9889, 411, 294, 264, 18161, 2533, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09744934553510687, "compression_ratio": 1.7242990654205608, "no_speech_prob": 1.0615244718792383e-05}, {"id": 1135, "seek": 556600, "start": 5575.0, "end": 5577.0, "text": " So what's happening to the activations is this.", "tokens": [50364, 407, 321, 434, 2577, 307, 264, 2430, 763, 1409, 484, 322, 264, 3344, 293, 362, 538, 4583, 1451, 1813, 588, 42165, 13, 50814, 50814, 407, 437, 311, 2737, 281, 264, 2430, 763, 307, 341, 13, 50914, 50914, 400, 365, 264, 2771, 2448, 322, 264, 1192, 4583, 11, 264, 24433, 11, 264, 16235, 12523, 366, 264, 9656, 293, 550, 436, 48696, 382, 291, 352, 760, 7731, 294, 264, 7914, 13, 51414, 51414, 400, 370, 1936, 291, 362, 364, 37277, 9889, 411, 294, 264, 18161, 2533, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09744934553510687, "compression_ratio": 1.7242990654205608, "no_speech_prob": 1.0615244718792383e-05}, {"id": 1136, "seek": 556600, "start": 5577.0, "end": 5587.0, "text": " And with the gradients on the top layer, the activation, the gradient statistics are the purple and then they diminish as you go down deeper in the layers.", "tokens": [50364, 407, 321, 434, 2577, 307, 264, 2430, 763, 1409, 484, 322, 264, 3344, 293, 362, 538, 4583, 1451, 1813, 588, 42165, 13, 50814, 50814, 407, 437, 311, 2737, 281, 264, 2430, 763, 307, 341, 13, 50914, 50914, 400, 365, 264, 2771, 2448, 322, 264, 1192, 4583, 11, 264, 24433, 11, 264, 16235, 12523, 366, 264, 9656, 293, 550, 436, 48696, 382, 291, 352, 760, 7731, 294, 264, 7914, 13, 51414, 51414, 400, 370, 1936, 291, 362, 364, 37277, 9889, 411, 294, 264, 18161, 2533, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09744934553510687, "compression_ratio": 1.7242990654205608, "no_speech_prob": 1.0615244718792383e-05}, {"id": 1137, "seek": 556600, "start": 5587.0, "end": 5590.0, "text": " And so basically you have an asymmetry like in the neural net.", "tokens": [50364, 407, 321, 434, 2577, 307, 264, 2430, 763, 1409, 484, 322, 264, 3344, 293, 362, 538, 4583, 1451, 1813, 588, 42165, 13, 50814, 50814, 407, 437, 311, 2737, 281, 264, 2430, 763, 307, 341, 13, 50914, 50914, 400, 365, 264, 2771, 2448, 322, 264, 1192, 4583, 11, 264, 24433, 11, 264, 16235, 12523, 366, 264, 9656, 293, 550, 436, 48696, 382, 291, 352, 760, 7731, 294, 264, 7914, 13, 51414, 51414, 400, 370, 1936, 291, 362, 364, 37277, 9889, 411, 294, 264, 18161, 2533, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09744934553510687, "compression_ratio": 1.7242990654205608, "no_speech_prob": 1.0615244718792383e-05}, {"id": 1138, "seek": 559000, "start": 5590.0, "end": 5598.0, "text": " And you might imagine that if you have a very deep neural networks, say like 50 layers or something like that, this just this is not a good place to be.", "tokens": [50364, 400, 291, 1062, 3811, 300, 498, 291, 362, 257, 588, 2452, 18161, 9590, 11, 584, 411, 2625, 7914, 420, 746, 411, 300, 11, 341, 445, 341, 307, 406, 257, 665, 1081, 281, 312, 13, 50764, 50764, 407, 300, 311, 983, 949, 300, 2710, 2144, 11, 341, 390, 6252, 12414, 281, 281, 992, 294, 1729, 13, 51114, 51114, 759, 341, 307, 886, 2416, 295, 257, 1216, 11, 341, 2314, 13, 51214, 51214, 400, 498, 309, 311, 886, 707, 295, 257, 1216, 11, 550, 341, 2314, 13, 51414, 51414, 407, 264, 6182, 295, 300, 1936, 2314, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09700744320647885, "compression_ratio": 1.6538461538461537, "no_speech_prob": 4.637694019038463e-06}, {"id": 1139, "seek": 559000, "start": 5598.0, "end": 5605.0, "text": " So that's why before that normalization, this was incredibly tricky to to set in particular.", "tokens": [50364, 400, 291, 1062, 3811, 300, 498, 291, 362, 257, 588, 2452, 18161, 9590, 11, 584, 411, 2625, 7914, 420, 746, 411, 300, 11, 341, 445, 341, 307, 406, 257, 665, 1081, 281, 312, 13, 50764, 50764, 407, 300, 311, 983, 949, 300, 2710, 2144, 11, 341, 390, 6252, 12414, 281, 281, 992, 294, 1729, 13, 51114, 51114, 759, 341, 307, 886, 2416, 295, 257, 1216, 11, 341, 2314, 13, 51214, 51214, 400, 498, 309, 311, 886, 707, 295, 257, 1216, 11, 550, 341, 2314, 13, 51414, 51414, 407, 264, 6182, 295, 300, 1936, 2314, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09700744320647885, "compression_ratio": 1.6538461538461537, "no_speech_prob": 4.637694019038463e-06}, {"id": 1140, "seek": 559000, "start": 5605.0, "end": 5607.0, "text": " If this is too large of a game, this happens.", "tokens": [50364, 400, 291, 1062, 3811, 300, 498, 291, 362, 257, 588, 2452, 18161, 9590, 11, 584, 411, 2625, 7914, 420, 746, 411, 300, 11, 341, 445, 341, 307, 406, 257, 665, 1081, 281, 312, 13, 50764, 50764, 407, 300, 311, 983, 949, 300, 2710, 2144, 11, 341, 390, 6252, 12414, 281, 281, 992, 294, 1729, 13, 51114, 51114, 759, 341, 307, 886, 2416, 295, 257, 1216, 11, 341, 2314, 13, 51214, 51214, 400, 498, 309, 311, 886, 707, 295, 257, 1216, 11, 550, 341, 2314, 13, 51414, 51414, 407, 264, 6182, 295, 300, 1936, 2314, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09700744320647885, "compression_ratio": 1.6538461538461537, "no_speech_prob": 4.637694019038463e-06}, {"id": 1141, "seek": 559000, "start": 5607.0, "end": 5611.0, "text": " And if it's too little of a game, then this happens.", "tokens": [50364, 400, 291, 1062, 3811, 300, 498, 291, 362, 257, 588, 2452, 18161, 9590, 11, 584, 411, 2625, 7914, 420, 746, 411, 300, 11, 341, 445, 341, 307, 406, 257, 665, 1081, 281, 312, 13, 50764, 50764, 407, 300, 311, 983, 949, 300, 2710, 2144, 11, 341, 390, 6252, 12414, 281, 281, 992, 294, 1729, 13, 51114, 51114, 759, 341, 307, 886, 2416, 295, 257, 1216, 11, 341, 2314, 13, 51214, 51214, 400, 498, 309, 311, 886, 707, 295, 257, 1216, 11, 550, 341, 2314, 13, 51414, 51414, 407, 264, 6182, 295, 300, 1936, 2314, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09700744320647885, "compression_ratio": 1.6538461538461537, "no_speech_prob": 4.637694019038463e-06}, {"id": 1142, "seek": 559000, "start": 5611.0, "end": 5613.0, "text": " So the opposite of that basically happens.", "tokens": [50364, 400, 291, 1062, 3811, 300, 498, 291, 362, 257, 588, 2452, 18161, 9590, 11, 584, 411, 2625, 7914, 420, 746, 411, 300, 11, 341, 445, 341, 307, 406, 257, 665, 1081, 281, 312, 13, 50764, 50764, 407, 300, 311, 983, 949, 300, 2710, 2144, 11, 341, 390, 6252, 12414, 281, 281, 992, 294, 1729, 13, 51114, 51114, 759, 341, 307, 886, 2416, 295, 257, 1216, 11, 341, 2314, 13, 51214, 51214, 400, 498, 309, 311, 886, 707, 295, 257, 1216, 11, 550, 341, 2314, 13, 51414, 51414, 407, 264, 6182, 295, 300, 1936, 2314, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.09700744320647885, "compression_ratio": 1.6538461538461537, "no_speech_prob": 4.637694019038463e-06}, {"id": 1143, "seek": 561300, "start": 5613.0, "end": 5622.0, "text": " Here we have a shrinking and a diffusion depending on which direction you look at it from.", "tokens": [50364, 1692, 321, 362, 257, 41684, 293, 257, 25242, 5413, 322, 597, 3513, 291, 574, 412, 309, 490, 13, 50814, 50814, 400, 370, 3297, 341, 307, 406, 437, 291, 528, 13, 50914, 50914, 400, 294, 341, 1389, 11, 264, 3006, 3287, 295, 264, 1216, 307, 2293, 472, 11, 445, 411, 321, 434, 884, 412, 5883, 2144, 13, 51214, 51214, 400, 550, 321, 536, 300, 264, 12523, 337, 264, 2128, 293, 23897, 1320, 366, 731, 48249, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06796017779579645, "compression_ratio": 1.5893719806763285, "no_speech_prob": 2.4060527721303515e-06}, {"id": 1144, "seek": 561300, "start": 5622.0, "end": 5624.0, "text": " And so certainly this is not what you want.", "tokens": [50364, 1692, 321, 362, 257, 41684, 293, 257, 25242, 5413, 322, 597, 3513, 291, 574, 412, 309, 490, 13, 50814, 50814, 400, 370, 3297, 341, 307, 406, 437, 291, 528, 13, 50914, 50914, 400, 294, 341, 1389, 11, 264, 3006, 3287, 295, 264, 1216, 307, 2293, 472, 11, 445, 411, 321, 434, 884, 412, 5883, 2144, 13, 51214, 51214, 400, 550, 321, 536, 300, 264, 12523, 337, 264, 2128, 293, 23897, 1320, 366, 731, 48249, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06796017779579645, "compression_ratio": 1.5893719806763285, "no_speech_prob": 2.4060527721303515e-06}, {"id": 1145, "seek": 561300, "start": 5624.0, "end": 5630.0, "text": " And in this case, the correct setting of the game is exactly one, just like we're doing at initialization.", "tokens": [50364, 1692, 321, 362, 257, 41684, 293, 257, 25242, 5413, 322, 597, 3513, 291, 574, 412, 309, 490, 13, 50814, 50814, 400, 370, 3297, 341, 307, 406, 437, 291, 528, 13, 50914, 50914, 400, 294, 341, 1389, 11, 264, 3006, 3287, 295, 264, 1216, 307, 2293, 472, 11, 445, 411, 321, 434, 884, 412, 5883, 2144, 13, 51214, 51214, 400, 550, 321, 536, 300, 264, 12523, 337, 264, 2128, 293, 23897, 1320, 366, 731, 48249, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06796017779579645, "compression_ratio": 1.5893719806763285, "no_speech_prob": 2.4060527721303515e-06}, {"id": 1146, "seek": 561300, "start": 5630.0, "end": 5636.0, "text": " And then we see that the statistics for the forward and backward pass are well behaved.", "tokens": [50364, 1692, 321, 362, 257, 41684, 293, 257, 25242, 5413, 322, 597, 3513, 291, 574, 412, 309, 490, 13, 50814, 50814, 400, 370, 3297, 341, 307, 406, 437, 291, 528, 13, 50914, 50914, 400, 294, 341, 1389, 11, 264, 3006, 3287, 295, 264, 1216, 307, 2293, 472, 11, 445, 411, 321, 434, 884, 412, 5883, 2144, 13, 51214, 51214, 400, 550, 321, 536, 300, 264, 12523, 337, 264, 2128, 293, 23897, 1320, 366, 731, 48249, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06796017779579645, "compression_ratio": 1.5893719806763285, "no_speech_prob": 2.4060527721303515e-06}, {"id": 1147, "seek": 563600, "start": 5636.0, "end": 5644.0, "text": " And so the reason I want to show you this is that basically like getting your own to train before these normalization layers", "tokens": [50364, 400, 370, 264, 1778, 286, 528, 281, 855, 291, 341, 307, 300, 1936, 411, 1242, 428, 1065, 281, 3847, 949, 613, 2710, 2144, 7914, 50764, 50764, 293, 949, 264, 764, 295, 7339, 5028, 22525, 411, 7938, 11, 597, 321, 920, 362, 281, 2060, 293, 27980, 9271, 293, 370, 322, 13, 51064, 51064, 20620, 18161, 36170, 1936, 574, 411, 341, 13, 51214, 51214, 467, 311, 411, 257, 3217, 22495, 605, 13, 51314, 51314, 509, 362, 281, 652, 988, 300, 1203, 307, 13402, 14161, 5468, 293, 291, 362, 281, 1127, 466, 264, 2430, 763, 293, 264, 2771, 2448, 293, 641, 12523, 13, 51614, 51614, 400, 550, 1310, 291, 393, 3847, 746, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1023550451847545, "compression_ratio": 1.7431506849315068, "no_speech_prob": 7.1827957981440704e-06}, {"id": 1148, "seek": 563600, "start": 5644.0, "end": 5650.0, "text": " and before the use of advanced optimizers like Adam, which we still have to cover and residual connections and so on.", "tokens": [50364, 400, 370, 264, 1778, 286, 528, 281, 855, 291, 341, 307, 300, 1936, 411, 1242, 428, 1065, 281, 3847, 949, 613, 2710, 2144, 7914, 50764, 50764, 293, 949, 264, 764, 295, 7339, 5028, 22525, 411, 7938, 11, 597, 321, 920, 362, 281, 2060, 293, 27980, 9271, 293, 370, 322, 13, 51064, 51064, 20620, 18161, 36170, 1936, 574, 411, 341, 13, 51214, 51214, 467, 311, 411, 257, 3217, 22495, 605, 13, 51314, 51314, 509, 362, 281, 652, 988, 300, 1203, 307, 13402, 14161, 5468, 293, 291, 362, 281, 1127, 466, 264, 2430, 763, 293, 264, 2771, 2448, 293, 641, 12523, 13, 51614, 51614, 400, 550, 1310, 291, 393, 3847, 746, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1023550451847545, "compression_ratio": 1.7431506849315068, "no_speech_prob": 7.1827957981440704e-06}, {"id": 1149, "seek": 563600, "start": 5650.0, "end": 5653.0, "text": " Training neural nets basically look like this.", "tokens": [50364, 400, 370, 264, 1778, 286, 528, 281, 855, 291, 341, 307, 300, 1936, 411, 1242, 428, 1065, 281, 3847, 949, 613, 2710, 2144, 7914, 50764, 50764, 293, 949, 264, 764, 295, 7339, 5028, 22525, 411, 7938, 11, 597, 321, 920, 362, 281, 2060, 293, 27980, 9271, 293, 370, 322, 13, 51064, 51064, 20620, 18161, 36170, 1936, 574, 411, 341, 13, 51214, 51214, 467, 311, 411, 257, 3217, 22495, 605, 13, 51314, 51314, 509, 362, 281, 652, 988, 300, 1203, 307, 13402, 14161, 5468, 293, 291, 362, 281, 1127, 466, 264, 2430, 763, 293, 264, 2771, 2448, 293, 641, 12523, 13, 51614, 51614, 400, 550, 1310, 291, 393, 3847, 746, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1023550451847545, "compression_ratio": 1.7431506849315068, "no_speech_prob": 7.1827957981440704e-06}, {"id": 1150, "seek": 563600, "start": 5653.0, "end": 5655.0, "text": " It's like a total balancing act.", "tokens": [50364, 400, 370, 264, 1778, 286, 528, 281, 855, 291, 341, 307, 300, 1936, 411, 1242, 428, 1065, 281, 3847, 949, 613, 2710, 2144, 7914, 50764, 50764, 293, 949, 264, 764, 295, 7339, 5028, 22525, 411, 7938, 11, 597, 321, 920, 362, 281, 2060, 293, 27980, 9271, 293, 370, 322, 13, 51064, 51064, 20620, 18161, 36170, 1936, 574, 411, 341, 13, 51214, 51214, 467, 311, 411, 257, 3217, 22495, 605, 13, 51314, 51314, 509, 362, 281, 652, 988, 300, 1203, 307, 13402, 14161, 5468, 293, 291, 362, 281, 1127, 466, 264, 2430, 763, 293, 264, 2771, 2448, 293, 641, 12523, 13, 51614, 51614, 400, 550, 1310, 291, 393, 3847, 746, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1023550451847545, "compression_ratio": 1.7431506849315068, "no_speech_prob": 7.1827957981440704e-06}, {"id": 1151, "seek": 563600, "start": 5655.0, "end": 5661.0, "text": " You have to make sure that everything is precisely orchestrated and you have to care about the activations and the gradients and their statistics.", "tokens": [50364, 400, 370, 264, 1778, 286, 528, 281, 855, 291, 341, 307, 300, 1936, 411, 1242, 428, 1065, 281, 3847, 949, 613, 2710, 2144, 7914, 50764, 50764, 293, 949, 264, 764, 295, 7339, 5028, 22525, 411, 7938, 11, 597, 321, 920, 362, 281, 2060, 293, 27980, 9271, 293, 370, 322, 13, 51064, 51064, 20620, 18161, 36170, 1936, 574, 411, 341, 13, 51214, 51214, 467, 311, 411, 257, 3217, 22495, 605, 13, 51314, 51314, 509, 362, 281, 652, 988, 300, 1203, 307, 13402, 14161, 5468, 293, 291, 362, 281, 1127, 466, 264, 2430, 763, 293, 264, 2771, 2448, 293, 641, 12523, 13, 51614, 51614, 400, 550, 1310, 291, 393, 3847, 746, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1023550451847545, "compression_ratio": 1.7431506849315068, "no_speech_prob": 7.1827957981440704e-06}, {"id": 1152, "seek": 563600, "start": 5661.0, "end": 5663.0, "text": " And then maybe you can train something.", "tokens": [50364, 400, 370, 264, 1778, 286, 528, 281, 855, 291, 341, 307, 300, 1936, 411, 1242, 428, 1065, 281, 3847, 949, 613, 2710, 2144, 7914, 50764, 50764, 293, 949, 264, 764, 295, 7339, 5028, 22525, 411, 7938, 11, 597, 321, 920, 362, 281, 2060, 293, 27980, 9271, 293, 370, 322, 13, 51064, 51064, 20620, 18161, 36170, 1936, 574, 411, 341, 13, 51214, 51214, 467, 311, 411, 257, 3217, 22495, 605, 13, 51314, 51314, 509, 362, 281, 652, 988, 300, 1203, 307, 13402, 14161, 5468, 293, 291, 362, 281, 1127, 466, 264, 2430, 763, 293, 264, 2771, 2448, 293, 641, 12523, 13, 51614, 51614, 400, 550, 1310, 291, 393, 3847, 746, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1023550451847545, "compression_ratio": 1.7431506849315068, "no_speech_prob": 7.1827957981440704e-06}, {"id": 1153, "seek": 566300, "start": 5663.0, "end": 5668.0, "text": " But it was basically impossible to train very deep networks. And this is fundamentally the reason for that.", "tokens": [50364, 583, 309, 390, 1936, 6243, 281, 3847, 588, 2452, 9590, 13, 400, 341, 307, 17879, 264, 1778, 337, 300, 13, 50614, 50614, 509, 1116, 362, 281, 312, 588, 11, 588, 5026, 365, 428, 5883, 2144, 13, 50814, 50814, 440, 661, 935, 510, 307, 291, 1062, 312, 3365, 1803, 11, 538, 264, 636, 11, 286, 478, 406, 988, 498, 286, 5343, 341, 13, 51064, 51064, 1545, 360, 321, 643, 613, 1266, 39, 7914, 412, 439, 30, 51214, 51214, 1545, 360, 321, 4090, 552, 293, 550, 362, 281, 3292, 466, 264, 1216, 30, 51364, 51364, 400, 264, 1778, 337, 300, 11, 295, 1164, 11, 307, 300, 498, 291, 445, 362, 257, 8630, 295, 8213, 7914, 11, 550, 3297, 321, 434, 1242, 588, 3612, 1481, 2430, 763, 293, 370, 322, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04266697710210627, "compression_ratio": 1.6719745222929936, "no_speech_prob": 9.368375685880892e-06}, {"id": 1154, "seek": 566300, "start": 5668.0, "end": 5672.0, "text": " You'd have to be very, very careful with your initialization.", "tokens": [50364, 583, 309, 390, 1936, 6243, 281, 3847, 588, 2452, 9590, 13, 400, 341, 307, 17879, 264, 1778, 337, 300, 13, 50614, 50614, 509, 1116, 362, 281, 312, 588, 11, 588, 5026, 365, 428, 5883, 2144, 13, 50814, 50814, 440, 661, 935, 510, 307, 291, 1062, 312, 3365, 1803, 11, 538, 264, 636, 11, 286, 478, 406, 988, 498, 286, 5343, 341, 13, 51064, 51064, 1545, 360, 321, 643, 613, 1266, 39, 7914, 412, 439, 30, 51214, 51214, 1545, 360, 321, 4090, 552, 293, 550, 362, 281, 3292, 466, 264, 1216, 30, 51364, 51364, 400, 264, 1778, 337, 300, 11, 295, 1164, 11, 307, 300, 498, 291, 445, 362, 257, 8630, 295, 8213, 7914, 11, 550, 3297, 321, 434, 1242, 588, 3612, 1481, 2430, 763, 293, 370, 322, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04266697710210627, "compression_ratio": 1.6719745222929936, "no_speech_prob": 9.368375685880892e-06}, {"id": 1155, "seek": 566300, "start": 5672.0, "end": 5677.0, "text": " The other point here is you might be asking yourself, by the way, I'm not sure if I covered this.", "tokens": [50364, 583, 309, 390, 1936, 6243, 281, 3847, 588, 2452, 9590, 13, 400, 341, 307, 17879, 264, 1778, 337, 300, 13, 50614, 50614, 509, 1116, 362, 281, 312, 588, 11, 588, 5026, 365, 428, 5883, 2144, 13, 50814, 50814, 440, 661, 935, 510, 307, 291, 1062, 312, 3365, 1803, 11, 538, 264, 636, 11, 286, 478, 406, 988, 498, 286, 5343, 341, 13, 51064, 51064, 1545, 360, 321, 643, 613, 1266, 39, 7914, 412, 439, 30, 51214, 51214, 1545, 360, 321, 4090, 552, 293, 550, 362, 281, 3292, 466, 264, 1216, 30, 51364, 51364, 400, 264, 1778, 337, 300, 11, 295, 1164, 11, 307, 300, 498, 291, 445, 362, 257, 8630, 295, 8213, 7914, 11, 550, 3297, 321, 434, 1242, 588, 3612, 1481, 2430, 763, 293, 370, 322, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04266697710210627, "compression_ratio": 1.6719745222929936, "no_speech_prob": 9.368375685880892e-06}, {"id": 1156, "seek": 566300, "start": 5677.0, "end": 5680.0, "text": " Why do we need these 10H layers at all?", "tokens": [50364, 583, 309, 390, 1936, 6243, 281, 3847, 588, 2452, 9590, 13, 400, 341, 307, 17879, 264, 1778, 337, 300, 13, 50614, 50614, 509, 1116, 362, 281, 312, 588, 11, 588, 5026, 365, 428, 5883, 2144, 13, 50814, 50814, 440, 661, 935, 510, 307, 291, 1062, 312, 3365, 1803, 11, 538, 264, 636, 11, 286, 478, 406, 988, 498, 286, 5343, 341, 13, 51064, 51064, 1545, 360, 321, 643, 613, 1266, 39, 7914, 412, 439, 30, 51214, 51214, 1545, 360, 321, 4090, 552, 293, 550, 362, 281, 3292, 466, 264, 1216, 30, 51364, 51364, 400, 264, 1778, 337, 300, 11, 295, 1164, 11, 307, 300, 498, 291, 445, 362, 257, 8630, 295, 8213, 7914, 11, 550, 3297, 321, 434, 1242, 588, 3612, 1481, 2430, 763, 293, 370, 322, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04266697710210627, "compression_ratio": 1.6719745222929936, "no_speech_prob": 9.368375685880892e-06}, {"id": 1157, "seek": 566300, "start": 5680.0, "end": 5683.0, "text": " Why do we include them and then have to worry about the game?", "tokens": [50364, 583, 309, 390, 1936, 6243, 281, 3847, 588, 2452, 9590, 13, 400, 341, 307, 17879, 264, 1778, 337, 300, 13, 50614, 50614, 509, 1116, 362, 281, 312, 588, 11, 588, 5026, 365, 428, 5883, 2144, 13, 50814, 50814, 440, 661, 935, 510, 307, 291, 1062, 312, 3365, 1803, 11, 538, 264, 636, 11, 286, 478, 406, 988, 498, 286, 5343, 341, 13, 51064, 51064, 1545, 360, 321, 643, 613, 1266, 39, 7914, 412, 439, 30, 51214, 51214, 1545, 360, 321, 4090, 552, 293, 550, 362, 281, 3292, 466, 264, 1216, 30, 51364, 51364, 400, 264, 1778, 337, 300, 11, 295, 1164, 11, 307, 300, 498, 291, 445, 362, 257, 8630, 295, 8213, 7914, 11, 550, 3297, 321, 434, 1242, 588, 3612, 1481, 2430, 763, 293, 370, 322, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04266697710210627, "compression_ratio": 1.6719745222929936, "no_speech_prob": 9.368375685880892e-06}, {"id": 1158, "seek": 566300, "start": 5683.0, "end": 5692.0, "text": " And the reason for that, of course, is that if you just have a stack of linear layers, then certainly we're getting very easily nice activations and so on.", "tokens": [50364, 583, 309, 390, 1936, 6243, 281, 3847, 588, 2452, 9590, 13, 400, 341, 307, 17879, 264, 1778, 337, 300, 13, 50614, 50614, 509, 1116, 362, 281, 312, 588, 11, 588, 5026, 365, 428, 5883, 2144, 13, 50814, 50814, 440, 661, 935, 510, 307, 291, 1062, 312, 3365, 1803, 11, 538, 264, 636, 11, 286, 478, 406, 988, 498, 286, 5343, 341, 13, 51064, 51064, 1545, 360, 321, 643, 613, 1266, 39, 7914, 412, 439, 30, 51214, 51214, 1545, 360, 321, 4090, 552, 293, 550, 362, 281, 3292, 466, 264, 1216, 30, 51364, 51364, 400, 264, 1778, 337, 300, 11, 295, 1164, 11, 307, 300, 498, 291, 445, 362, 257, 8630, 295, 8213, 7914, 11, 550, 3297, 321, 434, 1242, 588, 3612, 1481, 2430, 763, 293, 370, 322, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.04266697710210627, "compression_ratio": 1.6719745222929936, "no_speech_prob": 9.368375685880892e-06}, {"id": 1159, "seek": 569200, "start": 5692.0, "end": 5694.0, "text": " But this is just a massive linear sandwich.", "tokens": [50364, 583, 341, 307, 445, 257, 5994, 8213, 11141, 13, 50464, 50464, 400, 309, 4523, 484, 300, 309, 48765, 281, 257, 2167, 8213, 4583, 294, 2115, 295, 1080, 10290, 1347, 13, 50714, 50714, 407, 498, 291, 645, 281, 7542, 264, 5598, 382, 257, 2445, 295, 264, 4846, 11, 291, 434, 445, 1242, 257, 8213, 2445, 13, 50964, 50964, 883, 1871, 577, 867, 8213, 7914, 291, 8630, 493, 11, 291, 920, 445, 917, 493, 365, 257, 8213, 9887, 13, 51214, 51214, 1057, 264, 343, 87, 1804, 272, 311, 445, 15584, 666, 257, 2416, 343, 87, 1804, 272, 365, 4748, 819, 343, 82, 293, 4748, 819, 272, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07732729080620163, "compression_ratio": 1.768627450980392, "no_speech_prob": 1.9830928067676723e-05}, {"id": 1160, "seek": 569200, "start": 5694.0, "end": 5699.0, "text": " And it turns out that it collapses to a single linear layer in terms of its representation power.", "tokens": [50364, 583, 341, 307, 445, 257, 5994, 8213, 11141, 13, 50464, 50464, 400, 309, 4523, 484, 300, 309, 48765, 281, 257, 2167, 8213, 4583, 294, 2115, 295, 1080, 10290, 1347, 13, 50714, 50714, 407, 498, 291, 645, 281, 7542, 264, 5598, 382, 257, 2445, 295, 264, 4846, 11, 291, 434, 445, 1242, 257, 8213, 2445, 13, 50964, 50964, 883, 1871, 577, 867, 8213, 7914, 291, 8630, 493, 11, 291, 920, 445, 917, 493, 365, 257, 8213, 9887, 13, 51214, 51214, 1057, 264, 343, 87, 1804, 272, 311, 445, 15584, 666, 257, 2416, 343, 87, 1804, 272, 365, 4748, 819, 343, 82, 293, 4748, 819, 272, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07732729080620163, "compression_ratio": 1.768627450980392, "no_speech_prob": 1.9830928067676723e-05}, {"id": 1161, "seek": 569200, "start": 5699.0, "end": 5704.0, "text": " So if you were to plot the output as a function of the input, you're just getting a linear function.", "tokens": [50364, 583, 341, 307, 445, 257, 5994, 8213, 11141, 13, 50464, 50464, 400, 309, 4523, 484, 300, 309, 48765, 281, 257, 2167, 8213, 4583, 294, 2115, 295, 1080, 10290, 1347, 13, 50714, 50714, 407, 498, 291, 645, 281, 7542, 264, 5598, 382, 257, 2445, 295, 264, 4846, 11, 291, 434, 445, 1242, 257, 8213, 2445, 13, 50964, 50964, 883, 1871, 577, 867, 8213, 7914, 291, 8630, 493, 11, 291, 920, 445, 917, 493, 365, 257, 8213, 9887, 13, 51214, 51214, 1057, 264, 343, 87, 1804, 272, 311, 445, 15584, 666, 257, 2416, 343, 87, 1804, 272, 365, 4748, 819, 343, 82, 293, 4748, 819, 272, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07732729080620163, "compression_ratio": 1.768627450980392, "no_speech_prob": 1.9830928067676723e-05}, {"id": 1162, "seek": 569200, "start": 5704.0, "end": 5709.0, "text": " No matter how many linear layers you stack up, you still just end up with a linear transformation.", "tokens": [50364, 583, 341, 307, 445, 257, 5994, 8213, 11141, 13, 50464, 50464, 400, 309, 4523, 484, 300, 309, 48765, 281, 257, 2167, 8213, 4583, 294, 2115, 295, 1080, 10290, 1347, 13, 50714, 50714, 407, 498, 291, 645, 281, 7542, 264, 5598, 382, 257, 2445, 295, 264, 4846, 11, 291, 434, 445, 1242, 257, 8213, 2445, 13, 50964, 50964, 883, 1871, 577, 867, 8213, 7914, 291, 8630, 493, 11, 291, 920, 445, 917, 493, 365, 257, 8213, 9887, 13, 51214, 51214, 1057, 264, 343, 87, 1804, 272, 311, 445, 15584, 666, 257, 2416, 343, 87, 1804, 272, 365, 4748, 819, 343, 82, 293, 4748, 819, 272, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07732729080620163, "compression_ratio": 1.768627450980392, "no_speech_prob": 1.9830928067676723e-05}, {"id": 1163, "seek": 569200, "start": 5709.0, "end": 5717.0, "text": " All the Wx plus b's just collapse into a large Wx plus b with slightly different Ws and slightly different b.", "tokens": [50364, 583, 341, 307, 445, 257, 5994, 8213, 11141, 13, 50464, 50464, 400, 309, 4523, 484, 300, 309, 48765, 281, 257, 2167, 8213, 4583, 294, 2115, 295, 1080, 10290, 1347, 13, 50714, 50714, 407, 498, 291, 645, 281, 7542, 264, 5598, 382, 257, 2445, 295, 264, 4846, 11, 291, 434, 445, 1242, 257, 8213, 2445, 13, 50964, 50964, 883, 1871, 577, 867, 8213, 7914, 291, 8630, 493, 11, 291, 920, 445, 917, 493, 365, 257, 8213, 9887, 13, 51214, 51214, 1057, 264, 343, 87, 1804, 272, 311, 445, 15584, 666, 257, 2416, 343, 87, 1804, 272, 365, 4748, 819, 343, 82, 293, 4748, 819, 272, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.07732729080620163, "compression_ratio": 1.768627450980392, "no_speech_prob": 1.9830928067676723e-05}, {"id": 1164, "seek": 571700, "start": 5717.0, "end": 5728.0, "text": " But interestingly, even though the forward pass collapses to just a linear layer, because of backpropagation and the dynamics of the backward pass, the optimization is really is not identical.", "tokens": [50364, 583, 25873, 11, 754, 1673, 264, 2128, 1320, 48765, 281, 445, 257, 8213, 4583, 11, 570, 295, 646, 79, 1513, 559, 399, 293, 264, 15679, 295, 264, 23897, 1320, 11, 264, 19618, 307, 534, 307, 406, 14800, 13, 50914, 50914, 509, 767, 917, 493, 365, 439, 3685, 295, 1880, 15679, 294, 264, 23897, 1320, 570, 295, 264, 636, 264, 5021, 4978, 307, 28258, 309, 13, 51414, 51414, 400, 370, 40425, 257, 8213, 4583, 538, 2564, 293, 40425, 257, 11141, 295, 1266, 8213, 7914, 11, 294, 1293, 3331, 11, 729, 366, 445, 257, 8213, 9887, 294, 264, 2128, 1320, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.074717771659777, "compression_ratio": 1.9215686274509804, "no_speech_prob": 4.565788003674243e-06}, {"id": 1165, "seek": 571700, "start": 5728.0, "end": 5738.0, "text": " You actually end up with all kinds of interesting dynamics in the backward pass because of the way the chain rule is calculating it.", "tokens": [50364, 583, 25873, 11, 754, 1673, 264, 2128, 1320, 48765, 281, 445, 257, 8213, 4583, 11, 570, 295, 646, 79, 1513, 559, 399, 293, 264, 15679, 295, 264, 23897, 1320, 11, 264, 19618, 307, 534, 307, 406, 14800, 13, 50914, 50914, 509, 767, 917, 493, 365, 439, 3685, 295, 1880, 15679, 294, 264, 23897, 1320, 570, 295, 264, 636, 264, 5021, 4978, 307, 28258, 309, 13, 51414, 51414, 400, 370, 40425, 257, 8213, 4583, 538, 2564, 293, 40425, 257, 11141, 295, 1266, 8213, 7914, 11, 294, 1293, 3331, 11, 729, 366, 445, 257, 8213, 9887, 294, 264, 2128, 1320, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.074717771659777, "compression_ratio": 1.9215686274509804, "no_speech_prob": 4.565788003674243e-06}, {"id": 1166, "seek": 571700, "start": 5738.0, "end": 5746.0, "text": " And so optimizing a linear layer by itself and optimizing a sandwich of 10 linear layers, in both cases, those are just a linear transformation in the forward pass.", "tokens": [50364, 583, 25873, 11, 754, 1673, 264, 2128, 1320, 48765, 281, 445, 257, 8213, 4583, 11, 570, 295, 646, 79, 1513, 559, 399, 293, 264, 15679, 295, 264, 23897, 1320, 11, 264, 19618, 307, 534, 307, 406, 14800, 13, 50914, 50914, 509, 767, 917, 493, 365, 439, 3685, 295, 1880, 15679, 294, 264, 23897, 1320, 570, 295, 264, 636, 264, 5021, 4978, 307, 28258, 309, 13, 51414, 51414, 400, 370, 40425, 257, 8213, 4583, 538, 2564, 293, 40425, 257, 11141, 295, 1266, 8213, 7914, 11, 294, 1293, 3331, 11, 729, 366, 445, 257, 8213, 9887, 294, 264, 2128, 1320, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.074717771659777, "compression_ratio": 1.9215686274509804, "no_speech_prob": 4.565788003674243e-06}, {"id": 1167, "seek": 574600, "start": 5746.0, "end": 5748.0, "text": " But the training dynamics will be different.", "tokens": [50364, 583, 264, 3097, 15679, 486, 312, 819, 13, 50464, 50464, 400, 456, 311, 2302, 10577, 300, 12477, 11, 294, 1186, 11, 411, 36227, 34666, 8213, 7914, 293, 370, 322, 13, 50764, 50764, 400, 370, 456, 311, 257, 688, 295, 721, 281, 300, 291, 393, 862, 365, 456, 13, 50964, 50964, 583, 1936, 11, 264, 1266, 12, 12415, 2107, 28263, 1088, 2089, 505, 281, 1261, 341, 11141, 490, 445, 257, 8213, 2445, 666, 257, 18161, 3209, 300, 393, 11, 294, 8665, 11, 30874, 604, 23211, 2445, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11349531809488932, "compression_ratio": 1.6276150627615062, "no_speech_prob": 1.2605585652636364e-05}, {"id": 1168, "seek": 574600, "start": 5748.0, "end": 5754.0, "text": " And there's entire papers that analyze, in fact, like infinitely layered linear layers and so on.", "tokens": [50364, 583, 264, 3097, 15679, 486, 312, 819, 13, 50464, 50464, 400, 456, 311, 2302, 10577, 300, 12477, 11, 294, 1186, 11, 411, 36227, 34666, 8213, 7914, 293, 370, 322, 13, 50764, 50764, 400, 370, 456, 311, 257, 688, 295, 721, 281, 300, 291, 393, 862, 365, 456, 13, 50964, 50964, 583, 1936, 11, 264, 1266, 12, 12415, 2107, 28263, 1088, 2089, 505, 281, 1261, 341, 11141, 490, 445, 257, 8213, 2445, 666, 257, 18161, 3209, 300, 393, 11, 294, 8665, 11, 30874, 604, 23211, 2445, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11349531809488932, "compression_ratio": 1.6276150627615062, "no_speech_prob": 1.2605585652636364e-05}, {"id": 1169, "seek": 574600, "start": 5754.0, "end": 5758.0, "text": " And so there's a lot of things to that you can play with there.", "tokens": [50364, 583, 264, 3097, 15679, 486, 312, 819, 13, 50464, 50464, 400, 456, 311, 2302, 10577, 300, 12477, 11, 294, 1186, 11, 411, 36227, 34666, 8213, 7914, 293, 370, 322, 13, 50764, 50764, 400, 370, 456, 311, 257, 688, 295, 721, 281, 300, 291, 393, 862, 365, 456, 13, 50964, 50964, 583, 1936, 11, 264, 1266, 12, 12415, 2107, 28263, 1088, 2089, 505, 281, 1261, 341, 11141, 490, 445, 257, 8213, 2445, 666, 257, 18161, 3209, 300, 393, 11, 294, 8665, 11, 30874, 604, 23211, 2445, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11349531809488932, "compression_ratio": 1.6276150627615062, "no_speech_prob": 1.2605585652636364e-05}, {"id": 1170, "seek": 574600, "start": 5758.0, "end": 5775.0, "text": " But basically, the 10-inch nonlinearities allow us to turn this sandwich from just a linear function into a neural network that can, in principle, approximate any arbitrary function.", "tokens": [50364, 583, 264, 3097, 15679, 486, 312, 819, 13, 50464, 50464, 400, 456, 311, 2302, 10577, 300, 12477, 11, 294, 1186, 11, 411, 36227, 34666, 8213, 7914, 293, 370, 322, 13, 50764, 50764, 400, 370, 456, 311, 257, 688, 295, 721, 281, 300, 291, 393, 862, 365, 456, 13, 50964, 50964, 583, 1936, 11, 264, 1266, 12, 12415, 2107, 28263, 1088, 2089, 505, 281, 1261, 341, 11141, 490, 445, 257, 8213, 2445, 666, 257, 18161, 3209, 300, 393, 11, 294, 8665, 11, 30874, 604, 23211, 2445, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11349531809488932, "compression_ratio": 1.6276150627615062, "no_speech_prob": 1.2605585652636364e-05}, {"id": 1171, "seek": 577500, "start": 5775.0, "end": 5782.0, "text": " OK, so now I've reset the code to use the linear 10-inch sandwich like before, and I've reset everything.", "tokens": [50364, 2264, 11, 370, 586, 286, 600, 14322, 264, 3089, 281, 764, 264, 8213, 1266, 12, 12415, 11141, 411, 949, 11, 293, 286, 600, 14322, 1203, 13, 50714, 50714, 407, 264, 6052, 307, 1732, 670, 1045, 13, 50814, 50814, 492, 393, 1190, 257, 2167, 1823, 295, 19618, 293, 321, 393, 574, 412, 264, 24433, 12523, 295, 264, 2128, 1320, 293, 264, 23897, 1320, 13, 51114, 51114, 583, 286, 600, 3869, 472, 544, 7542, 510, 300, 286, 519, 307, 534, 1021, 281, 574, 412, 562, 291, 434, 3097, 428, 18161, 36170, 293, 281, 1949, 13, 51414, 51414, 400, 6284, 11, 437, 321, 434, 884, 307, 321, 434, 25113, 264, 9834, 295, 264, 18161, 2533, 13, 51614, 51614, 407, 321, 1127, 466, 264, 9834, 293, 641, 4190, 293, 641, 2771, 2448, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06718043994186516, "compression_ratio": 1.7699680511182108, "no_speech_prob": 9.971612598747015e-06}, {"id": 1172, "seek": 577500, "start": 5782.0, "end": 5784.0, "text": " So the gain is five over three.", "tokens": [50364, 2264, 11, 370, 586, 286, 600, 14322, 264, 3089, 281, 764, 264, 8213, 1266, 12, 12415, 11141, 411, 949, 11, 293, 286, 600, 14322, 1203, 13, 50714, 50714, 407, 264, 6052, 307, 1732, 670, 1045, 13, 50814, 50814, 492, 393, 1190, 257, 2167, 1823, 295, 19618, 293, 321, 393, 574, 412, 264, 24433, 12523, 295, 264, 2128, 1320, 293, 264, 23897, 1320, 13, 51114, 51114, 583, 286, 600, 3869, 472, 544, 7542, 510, 300, 286, 519, 307, 534, 1021, 281, 574, 412, 562, 291, 434, 3097, 428, 18161, 36170, 293, 281, 1949, 13, 51414, 51414, 400, 6284, 11, 437, 321, 434, 884, 307, 321, 434, 25113, 264, 9834, 295, 264, 18161, 2533, 13, 51614, 51614, 407, 321, 1127, 466, 264, 9834, 293, 641, 4190, 293, 641, 2771, 2448, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06718043994186516, "compression_ratio": 1.7699680511182108, "no_speech_prob": 9.971612598747015e-06}, {"id": 1173, "seek": 577500, "start": 5784.0, "end": 5790.0, "text": " We can run a single step of optimization and we can look at the activation statistics of the forward pass and the backward pass.", "tokens": [50364, 2264, 11, 370, 586, 286, 600, 14322, 264, 3089, 281, 764, 264, 8213, 1266, 12, 12415, 11141, 411, 949, 11, 293, 286, 600, 14322, 1203, 13, 50714, 50714, 407, 264, 6052, 307, 1732, 670, 1045, 13, 50814, 50814, 492, 393, 1190, 257, 2167, 1823, 295, 19618, 293, 321, 393, 574, 412, 264, 24433, 12523, 295, 264, 2128, 1320, 293, 264, 23897, 1320, 13, 51114, 51114, 583, 286, 600, 3869, 472, 544, 7542, 510, 300, 286, 519, 307, 534, 1021, 281, 574, 412, 562, 291, 434, 3097, 428, 18161, 36170, 293, 281, 1949, 13, 51414, 51414, 400, 6284, 11, 437, 321, 434, 884, 307, 321, 434, 25113, 264, 9834, 295, 264, 18161, 2533, 13, 51614, 51614, 407, 321, 1127, 466, 264, 9834, 293, 641, 4190, 293, 641, 2771, 2448, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06718043994186516, "compression_ratio": 1.7699680511182108, "no_speech_prob": 9.971612598747015e-06}, {"id": 1174, "seek": 577500, "start": 5790.0, "end": 5796.0, "text": " But I've added one more plot here that I think is really important to look at when you're training your neural nets and to consider.", "tokens": [50364, 2264, 11, 370, 586, 286, 600, 14322, 264, 3089, 281, 764, 264, 8213, 1266, 12, 12415, 11141, 411, 949, 11, 293, 286, 600, 14322, 1203, 13, 50714, 50714, 407, 264, 6052, 307, 1732, 670, 1045, 13, 50814, 50814, 492, 393, 1190, 257, 2167, 1823, 295, 19618, 293, 321, 393, 574, 412, 264, 24433, 12523, 295, 264, 2128, 1320, 293, 264, 23897, 1320, 13, 51114, 51114, 583, 286, 600, 3869, 472, 544, 7542, 510, 300, 286, 519, 307, 534, 1021, 281, 574, 412, 562, 291, 434, 3097, 428, 18161, 36170, 293, 281, 1949, 13, 51414, 51414, 400, 6284, 11, 437, 321, 434, 884, 307, 321, 434, 25113, 264, 9834, 295, 264, 18161, 2533, 13, 51614, 51614, 407, 321, 1127, 466, 264, 9834, 293, 641, 4190, 293, 641, 2771, 2448, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06718043994186516, "compression_ratio": 1.7699680511182108, "no_speech_prob": 9.971612598747015e-06}, {"id": 1175, "seek": 577500, "start": 5796.0, "end": 5800.0, "text": " And ultimately, what we're doing is we're updating the parameters of the neural net.", "tokens": [50364, 2264, 11, 370, 586, 286, 600, 14322, 264, 3089, 281, 764, 264, 8213, 1266, 12, 12415, 11141, 411, 949, 11, 293, 286, 600, 14322, 1203, 13, 50714, 50714, 407, 264, 6052, 307, 1732, 670, 1045, 13, 50814, 50814, 492, 393, 1190, 257, 2167, 1823, 295, 19618, 293, 321, 393, 574, 412, 264, 24433, 12523, 295, 264, 2128, 1320, 293, 264, 23897, 1320, 13, 51114, 51114, 583, 286, 600, 3869, 472, 544, 7542, 510, 300, 286, 519, 307, 534, 1021, 281, 574, 412, 562, 291, 434, 3097, 428, 18161, 36170, 293, 281, 1949, 13, 51414, 51414, 400, 6284, 11, 437, 321, 434, 884, 307, 321, 434, 25113, 264, 9834, 295, 264, 18161, 2533, 13, 51614, 51614, 407, 321, 1127, 466, 264, 9834, 293, 641, 4190, 293, 641, 2771, 2448, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06718043994186516, "compression_ratio": 1.7699680511182108, "no_speech_prob": 9.971612598747015e-06}, {"id": 1176, "seek": 577500, "start": 5800.0, "end": 5804.0, "text": " So we care about the parameters and their values and their gradients.", "tokens": [50364, 2264, 11, 370, 586, 286, 600, 14322, 264, 3089, 281, 764, 264, 8213, 1266, 12, 12415, 11141, 411, 949, 11, 293, 286, 600, 14322, 1203, 13, 50714, 50714, 407, 264, 6052, 307, 1732, 670, 1045, 13, 50814, 50814, 492, 393, 1190, 257, 2167, 1823, 295, 19618, 293, 321, 393, 574, 412, 264, 24433, 12523, 295, 264, 2128, 1320, 293, 264, 23897, 1320, 13, 51114, 51114, 583, 286, 600, 3869, 472, 544, 7542, 510, 300, 286, 519, 307, 534, 1021, 281, 574, 412, 562, 291, 434, 3097, 428, 18161, 36170, 293, 281, 1949, 13, 51414, 51414, 400, 6284, 11, 437, 321, 434, 884, 307, 321, 434, 25113, 264, 9834, 295, 264, 18161, 2533, 13, 51614, 51614, 407, 321, 1127, 466, 264, 9834, 293, 641, 4190, 293, 641, 2771, 2448, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06718043994186516, "compression_ratio": 1.7699680511182108, "no_speech_prob": 9.971612598747015e-06}, {"id": 1177, "seek": 580400, "start": 5804.0, "end": 5814.0, "text": " So here what I'm doing is I'm actually iterating over all the parameters available and then I'm only restricting it to the two dimensional parameters, which are basically the weights of these linear layers.", "tokens": [50364, 407, 510, 437, 286, 478, 884, 307, 286, 478, 767, 17138, 990, 670, 439, 264, 9834, 2435, 293, 550, 286, 478, 787, 1472, 37714, 309, 281, 264, 732, 18795, 9834, 11, 597, 366, 1936, 264, 17443, 295, 613, 8213, 7914, 13, 50864, 50864, 400, 286, 478, 31533, 264, 32152, 293, 286, 478, 31533, 264, 8019, 3799, 293, 264, 778, 296, 293, 264, 8414, 340, 445, 337, 25632, 13, 51264, 51264, 583, 291, 393, 611, 747, 257, 574, 412, 729, 382, 731, 13, 51364, 51364, 583, 437, 311, 2737, 365, 264, 17443, 307, 7232, 488, 538, 2564, 13, 51564, 51564, 407, 510, 321, 362, 439, 264, 819, 17443, 11, 641, 10854, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07866016056226648, "compression_ratio": 1.7712177121771218, "no_speech_prob": 9.079875781026203e-06}, {"id": 1178, "seek": 580400, "start": 5814.0, "end": 5822.0, "text": " And I'm skipping the biases and I'm skipping the gammas and the betas and the bastro just for simplicity.", "tokens": [50364, 407, 510, 437, 286, 478, 884, 307, 286, 478, 767, 17138, 990, 670, 439, 264, 9834, 2435, 293, 550, 286, 478, 787, 1472, 37714, 309, 281, 264, 732, 18795, 9834, 11, 597, 366, 1936, 264, 17443, 295, 613, 8213, 7914, 13, 50864, 50864, 400, 286, 478, 31533, 264, 32152, 293, 286, 478, 31533, 264, 8019, 3799, 293, 264, 778, 296, 293, 264, 8414, 340, 445, 337, 25632, 13, 51264, 51264, 583, 291, 393, 611, 747, 257, 574, 412, 729, 382, 731, 13, 51364, 51364, 583, 437, 311, 2737, 365, 264, 17443, 307, 7232, 488, 538, 2564, 13, 51564, 51564, 407, 510, 321, 362, 439, 264, 819, 17443, 11, 641, 10854, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07866016056226648, "compression_ratio": 1.7712177121771218, "no_speech_prob": 9.079875781026203e-06}, {"id": 1179, "seek": 580400, "start": 5822.0, "end": 5824.0, "text": " But you can also take a look at those as well.", "tokens": [50364, 407, 510, 437, 286, 478, 884, 307, 286, 478, 767, 17138, 990, 670, 439, 264, 9834, 2435, 293, 550, 286, 478, 787, 1472, 37714, 309, 281, 264, 732, 18795, 9834, 11, 597, 366, 1936, 264, 17443, 295, 613, 8213, 7914, 13, 50864, 50864, 400, 286, 478, 31533, 264, 32152, 293, 286, 478, 31533, 264, 8019, 3799, 293, 264, 778, 296, 293, 264, 8414, 340, 445, 337, 25632, 13, 51264, 51264, 583, 291, 393, 611, 747, 257, 574, 412, 729, 382, 731, 13, 51364, 51364, 583, 437, 311, 2737, 365, 264, 17443, 307, 7232, 488, 538, 2564, 13, 51564, 51564, 407, 510, 321, 362, 439, 264, 819, 17443, 11, 641, 10854, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07866016056226648, "compression_ratio": 1.7712177121771218, "no_speech_prob": 9.079875781026203e-06}, {"id": 1180, "seek": 580400, "start": 5824.0, "end": 5828.0, "text": " But what's happening with the weights is instructive by itself.", "tokens": [50364, 407, 510, 437, 286, 478, 884, 307, 286, 478, 767, 17138, 990, 670, 439, 264, 9834, 2435, 293, 550, 286, 478, 787, 1472, 37714, 309, 281, 264, 732, 18795, 9834, 11, 597, 366, 1936, 264, 17443, 295, 613, 8213, 7914, 13, 50864, 50864, 400, 286, 478, 31533, 264, 32152, 293, 286, 478, 31533, 264, 8019, 3799, 293, 264, 778, 296, 293, 264, 8414, 340, 445, 337, 25632, 13, 51264, 51264, 583, 291, 393, 611, 747, 257, 574, 412, 729, 382, 731, 13, 51364, 51364, 583, 437, 311, 2737, 365, 264, 17443, 307, 7232, 488, 538, 2564, 13, 51564, 51564, 407, 510, 321, 362, 439, 264, 819, 17443, 11, 641, 10854, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07866016056226648, "compression_ratio": 1.7712177121771218, "no_speech_prob": 9.079875781026203e-06}, {"id": 1181, "seek": 580400, "start": 5828.0, "end": 5832.0, "text": " So here we have all the different weights, their shapes.", "tokens": [50364, 407, 510, 437, 286, 478, 884, 307, 286, 478, 767, 17138, 990, 670, 439, 264, 9834, 2435, 293, 550, 286, 478, 787, 1472, 37714, 309, 281, 264, 732, 18795, 9834, 11, 597, 366, 1936, 264, 17443, 295, 613, 8213, 7914, 13, 50864, 50864, 400, 286, 478, 31533, 264, 32152, 293, 286, 478, 31533, 264, 8019, 3799, 293, 264, 778, 296, 293, 264, 8414, 340, 445, 337, 25632, 13, 51264, 51264, 583, 291, 393, 611, 747, 257, 574, 412, 729, 382, 731, 13, 51364, 51364, 583, 437, 311, 2737, 365, 264, 17443, 307, 7232, 488, 538, 2564, 13, 51564, 51564, 407, 510, 321, 362, 439, 264, 819, 17443, 11, 641, 10854, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07866016056226648, "compression_ratio": 1.7712177121771218, "no_speech_prob": 9.079875781026203e-06}, {"id": 1182, "seek": 583200, "start": 5832.0, "end": 5837.0, "text": " So this is the embedding layer, the first linear layer all the way to the very last linear layer.", "tokens": [50364, 407, 341, 307, 264, 12240, 3584, 4583, 11, 264, 700, 8213, 4583, 439, 264, 636, 281, 264, 588, 1036, 8213, 4583, 13, 50614, 50614, 400, 550, 321, 362, 264, 914, 11, 264, 3832, 25163, 295, 439, 613, 9834, 11, 264, 49816, 13, 50864, 50864, 400, 291, 393, 536, 300, 309, 767, 1177, 380, 574, 300, 2243, 13, 50964, 50964, 407, 456, 311, 512, 5253, 294, 25919, 11, 754, 1673, 613, 2771, 2448, 574, 2264, 13, 51164, 51164, 821, 311, 746, 3657, 516, 322, 510, 13, 51264, 51264, 286, 603, 483, 281, 300, 294, 257, 1150, 13, 51364, 51364, 400, 264, 1036, 551, 510, 307, 264, 16235, 281, 1412, 8509, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06588343988385117, "compression_ratio": 1.7588932806324111, "no_speech_prob": 8.013123988348525e-06}, {"id": 1183, "seek": 583200, "start": 5837.0, "end": 5842.0, "text": " And then we have the mean, the standard deviation of all these parameters, the histogram.", "tokens": [50364, 407, 341, 307, 264, 12240, 3584, 4583, 11, 264, 700, 8213, 4583, 439, 264, 636, 281, 264, 588, 1036, 8213, 4583, 13, 50614, 50614, 400, 550, 321, 362, 264, 914, 11, 264, 3832, 25163, 295, 439, 613, 9834, 11, 264, 49816, 13, 50864, 50864, 400, 291, 393, 536, 300, 309, 767, 1177, 380, 574, 300, 2243, 13, 50964, 50964, 407, 456, 311, 512, 5253, 294, 25919, 11, 754, 1673, 613, 2771, 2448, 574, 2264, 13, 51164, 51164, 821, 311, 746, 3657, 516, 322, 510, 13, 51264, 51264, 286, 603, 483, 281, 300, 294, 257, 1150, 13, 51364, 51364, 400, 264, 1036, 551, 510, 307, 264, 16235, 281, 1412, 8509, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06588343988385117, "compression_ratio": 1.7588932806324111, "no_speech_prob": 8.013123988348525e-06}, {"id": 1184, "seek": 583200, "start": 5842.0, "end": 5844.0, "text": " And you can see that it actually doesn't look that amazing.", "tokens": [50364, 407, 341, 307, 264, 12240, 3584, 4583, 11, 264, 700, 8213, 4583, 439, 264, 636, 281, 264, 588, 1036, 8213, 4583, 13, 50614, 50614, 400, 550, 321, 362, 264, 914, 11, 264, 3832, 25163, 295, 439, 613, 9834, 11, 264, 49816, 13, 50864, 50864, 400, 291, 393, 536, 300, 309, 767, 1177, 380, 574, 300, 2243, 13, 50964, 50964, 407, 456, 311, 512, 5253, 294, 25919, 11, 754, 1673, 613, 2771, 2448, 574, 2264, 13, 51164, 51164, 821, 311, 746, 3657, 516, 322, 510, 13, 51264, 51264, 286, 603, 483, 281, 300, 294, 257, 1150, 13, 51364, 51364, 400, 264, 1036, 551, 510, 307, 264, 16235, 281, 1412, 8509, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06588343988385117, "compression_ratio": 1.7588932806324111, "no_speech_prob": 8.013123988348525e-06}, {"id": 1185, "seek": 583200, "start": 5844.0, "end": 5848.0, "text": " So there's some trouble in paradise, even though these gradients look OK.", "tokens": [50364, 407, 341, 307, 264, 12240, 3584, 4583, 11, 264, 700, 8213, 4583, 439, 264, 636, 281, 264, 588, 1036, 8213, 4583, 13, 50614, 50614, 400, 550, 321, 362, 264, 914, 11, 264, 3832, 25163, 295, 439, 613, 9834, 11, 264, 49816, 13, 50864, 50864, 400, 291, 393, 536, 300, 309, 767, 1177, 380, 574, 300, 2243, 13, 50964, 50964, 407, 456, 311, 512, 5253, 294, 25919, 11, 754, 1673, 613, 2771, 2448, 574, 2264, 13, 51164, 51164, 821, 311, 746, 3657, 516, 322, 510, 13, 51264, 51264, 286, 603, 483, 281, 300, 294, 257, 1150, 13, 51364, 51364, 400, 264, 1036, 551, 510, 307, 264, 16235, 281, 1412, 8509, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06588343988385117, "compression_ratio": 1.7588932806324111, "no_speech_prob": 8.013123988348525e-06}, {"id": 1186, "seek": 583200, "start": 5848.0, "end": 5850.0, "text": " There's something weird going on here.", "tokens": [50364, 407, 341, 307, 264, 12240, 3584, 4583, 11, 264, 700, 8213, 4583, 439, 264, 636, 281, 264, 588, 1036, 8213, 4583, 13, 50614, 50614, 400, 550, 321, 362, 264, 914, 11, 264, 3832, 25163, 295, 439, 613, 9834, 11, 264, 49816, 13, 50864, 50864, 400, 291, 393, 536, 300, 309, 767, 1177, 380, 574, 300, 2243, 13, 50964, 50964, 407, 456, 311, 512, 5253, 294, 25919, 11, 754, 1673, 613, 2771, 2448, 574, 2264, 13, 51164, 51164, 821, 311, 746, 3657, 516, 322, 510, 13, 51264, 51264, 286, 603, 483, 281, 300, 294, 257, 1150, 13, 51364, 51364, 400, 264, 1036, 551, 510, 307, 264, 16235, 281, 1412, 8509, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06588343988385117, "compression_ratio": 1.7588932806324111, "no_speech_prob": 8.013123988348525e-06}, {"id": 1187, "seek": 583200, "start": 5850.0, "end": 5852.0, "text": " I'll get to that in a second.", "tokens": [50364, 407, 341, 307, 264, 12240, 3584, 4583, 11, 264, 700, 8213, 4583, 439, 264, 636, 281, 264, 588, 1036, 8213, 4583, 13, 50614, 50614, 400, 550, 321, 362, 264, 914, 11, 264, 3832, 25163, 295, 439, 613, 9834, 11, 264, 49816, 13, 50864, 50864, 400, 291, 393, 536, 300, 309, 767, 1177, 380, 574, 300, 2243, 13, 50964, 50964, 407, 456, 311, 512, 5253, 294, 25919, 11, 754, 1673, 613, 2771, 2448, 574, 2264, 13, 51164, 51164, 821, 311, 746, 3657, 516, 322, 510, 13, 51264, 51264, 286, 603, 483, 281, 300, 294, 257, 1150, 13, 51364, 51364, 400, 264, 1036, 551, 510, 307, 264, 16235, 281, 1412, 8509, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06588343988385117, "compression_ratio": 1.7588932806324111, "no_speech_prob": 8.013123988348525e-06}, {"id": 1188, "seek": 583200, "start": 5852.0, "end": 5855.0, "text": " And the last thing here is the gradient to data ratio.", "tokens": [50364, 407, 341, 307, 264, 12240, 3584, 4583, 11, 264, 700, 8213, 4583, 439, 264, 636, 281, 264, 588, 1036, 8213, 4583, 13, 50614, 50614, 400, 550, 321, 362, 264, 914, 11, 264, 3832, 25163, 295, 439, 613, 9834, 11, 264, 49816, 13, 50864, 50864, 400, 291, 393, 536, 300, 309, 767, 1177, 380, 574, 300, 2243, 13, 50964, 50964, 407, 456, 311, 512, 5253, 294, 25919, 11, 754, 1673, 613, 2771, 2448, 574, 2264, 13, 51164, 51164, 821, 311, 746, 3657, 516, 322, 510, 13, 51264, 51264, 286, 603, 483, 281, 300, 294, 257, 1150, 13, 51364, 51364, 400, 264, 1036, 551, 510, 307, 264, 16235, 281, 1412, 8509, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.06588343988385117, "compression_ratio": 1.7588932806324111, "no_speech_prob": 8.013123988348525e-06}, {"id": 1189, "seek": 585500, "start": 5855.0, "end": 5865.0, "text": " So sometimes I like to visualize this as well, because what this gives you a sense of is what is the scale of the gradient compared to the scale of the actual values.", "tokens": [50364, 407, 2171, 286, 411, 281, 23273, 341, 382, 731, 11, 570, 437, 341, 2709, 291, 257, 2020, 295, 307, 437, 307, 264, 4373, 295, 264, 16235, 5347, 281, 264, 4373, 295, 264, 3539, 4190, 13, 50864, 50864, 400, 341, 307, 1021, 570, 321, 434, 516, 281, 917, 493, 1940, 257, 1823, 5623, 300, 307, 264, 2539, 3314, 1413, 264, 16235, 3911, 264, 1412, 13, 51264, 51264, 400, 370, 264, 16235, 575, 886, 2416, 295, 15668, 13, 51414, 51414, 759, 264, 3547, 294, 456, 366, 886, 2416, 5347, 281, 264, 3547, 294, 1412, 11, 550, 291, 1116, 312, 294, 5253, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06455431993191059, "compression_ratio": 1.8734177215189873, "no_speech_prob": 3.905298399331514e-06}, {"id": 1190, "seek": 585500, "start": 5865.0, "end": 5873.0, "text": " And this is important because we're going to end up taking a step update that is the learning rate times the gradient onto the data.", "tokens": [50364, 407, 2171, 286, 411, 281, 23273, 341, 382, 731, 11, 570, 437, 341, 2709, 291, 257, 2020, 295, 307, 437, 307, 264, 4373, 295, 264, 16235, 5347, 281, 264, 4373, 295, 264, 3539, 4190, 13, 50864, 50864, 400, 341, 307, 1021, 570, 321, 434, 516, 281, 917, 493, 1940, 257, 1823, 5623, 300, 307, 264, 2539, 3314, 1413, 264, 16235, 3911, 264, 1412, 13, 51264, 51264, 400, 370, 264, 16235, 575, 886, 2416, 295, 15668, 13, 51414, 51414, 759, 264, 3547, 294, 456, 366, 886, 2416, 5347, 281, 264, 3547, 294, 1412, 11, 550, 291, 1116, 312, 294, 5253, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06455431993191059, "compression_ratio": 1.8734177215189873, "no_speech_prob": 3.905298399331514e-06}, {"id": 1191, "seek": 585500, "start": 5873.0, "end": 5876.0, "text": " And so the gradient has too large of magnitude.", "tokens": [50364, 407, 2171, 286, 411, 281, 23273, 341, 382, 731, 11, 570, 437, 341, 2709, 291, 257, 2020, 295, 307, 437, 307, 264, 4373, 295, 264, 16235, 5347, 281, 264, 4373, 295, 264, 3539, 4190, 13, 50864, 50864, 400, 341, 307, 1021, 570, 321, 434, 516, 281, 917, 493, 1940, 257, 1823, 5623, 300, 307, 264, 2539, 3314, 1413, 264, 16235, 3911, 264, 1412, 13, 51264, 51264, 400, 370, 264, 16235, 575, 886, 2416, 295, 15668, 13, 51414, 51414, 759, 264, 3547, 294, 456, 366, 886, 2416, 5347, 281, 264, 3547, 294, 1412, 11, 550, 291, 1116, 312, 294, 5253, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06455431993191059, "compression_ratio": 1.8734177215189873, "no_speech_prob": 3.905298399331514e-06}, {"id": 1192, "seek": 585500, "start": 5876.0, "end": 5881.0, "text": " If the numbers in there are too large compared to the numbers in data, then you'd be in trouble.", "tokens": [50364, 407, 2171, 286, 411, 281, 23273, 341, 382, 731, 11, 570, 437, 341, 2709, 291, 257, 2020, 295, 307, 437, 307, 264, 4373, 295, 264, 16235, 5347, 281, 264, 4373, 295, 264, 3539, 4190, 13, 50864, 50864, 400, 341, 307, 1021, 570, 321, 434, 516, 281, 917, 493, 1940, 257, 1823, 5623, 300, 307, 264, 2539, 3314, 1413, 264, 16235, 3911, 264, 1412, 13, 51264, 51264, 400, 370, 264, 16235, 575, 886, 2416, 295, 15668, 13, 51414, 51414, 759, 264, 3547, 294, 456, 366, 886, 2416, 5347, 281, 264, 3547, 294, 1412, 11, 550, 291, 1116, 312, 294, 5253, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06455431993191059, "compression_ratio": 1.8734177215189873, "no_speech_prob": 3.905298399331514e-06}, {"id": 1193, "seek": 588100, "start": 5881.0, "end": 5885.0, "text": " But in this case, the gradient to data is our low numbers.", "tokens": [50364, 583, 294, 341, 1389, 11, 264, 16235, 281, 1412, 307, 527, 2295, 3547, 13, 50564, 50564, 407, 264, 4190, 1854, 2771, 366, 9714, 1413, 4356, 813, 264, 4190, 1854, 1412, 294, 613, 17443, 11, 881, 295, 552, 13, 50964, 50964, 823, 11, 31357, 11, 300, 307, 406, 2074, 466, 264, 1036, 4583, 13, 51164, 51164, 400, 370, 264, 1036, 4583, 767, 510, 11, 264, 5598, 4583, 11, 307, 257, 857, 295, 257, 3455, 1113, 4003, 294, 264, 636, 300, 341, 307, 4362, 18721, 11, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07000244747508656, "compression_ratio": 1.671497584541063, "no_speech_prob": 9.222769222105853e-06}, {"id": 1194, "seek": 588100, "start": 5885.0, "end": 5893.0, "text": " So the values inside grad are 1000 times smaller than the values inside data in these weights, most of them.", "tokens": [50364, 583, 294, 341, 1389, 11, 264, 16235, 281, 1412, 307, 527, 2295, 3547, 13, 50564, 50564, 407, 264, 4190, 1854, 2771, 366, 9714, 1413, 4356, 813, 264, 4190, 1854, 1412, 294, 613, 17443, 11, 881, 295, 552, 13, 50964, 50964, 823, 11, 31357, 11, 300, 307, 406, 2074, 466, 264, 1036, 4583, 13, 51164, 51164, 400, 370, 264, 1036, 4583, 767, 510, 11, 264, 5598, 4583, 11, 307, 257, 857, 295, 257, 3455, 1113, 4003, 294, 264, 636, 300, 341, 307, 4362, 18721, 11, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07000244747508656, "compression_ratio": 1.671497584541063, "no_speech_prob": 9.222769222105853e-06}, {"id": 1195, "seek": 588100, "start": 5893.0, "end": 5897.0, "text": " Now, notably, that is not true about the last layer.", "tokens": [50364, 583, 294, 341, 1389, 11, 264, 16235, 281, 1412, 307, 527, 2295, 3547, 13, 50564, 50564, 407, 264, 4190, 1854, 2771, 366, 9714, 1413, 4356, 813, 264, 4190, 1854, 1412, 294, 613, 17443, 11, 881, 295, 552, 13, 50964, 50964, 823, 11, 31357, 11, 300, 307, 406, 2074, 466, 264, 1036, 4583, 13, 51164, 51164, 400, 370, 264, 1036, 4583, 767, 510, 11, 264, 5598, 4583, 11, 307, 257, 857, 295, 257, 3455, 1113, 4003, 294, 264, 636, 300, 341, 307, 4362, 18721, 11, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07000244747508656, "compression_ratio": 1.671497584541063, "no_speech_prob": 9.222769222105853e-06}, {"id": 1196, "seek": 588100, "start": 5897.0, "end": 5902.0, "text": " And so the last layer actually here, the output layer, is a bit of a troublemaker in the way that this is currently arranged,", "tokens": [50364, 583, 294, 341, 1389, 11, 264, 16235, 281, 1412, 307, 527, 2295, 3547, 13, 50564, 50564, 407, 264, 4190, 1854, 2771, 366, 9714, 1413, 4356, 813, 264, 4190, 1854, 1412, 294, 613, 17443, 11, 881, 295, 552, 13, 50964, 50964, 823, 11, 31357, 11, 300, 307, 406, 2074, 466, 264, 1036, 4583, 13, 51164, 51164, 400, 370, 264, 1036, 4583, 767, 510, 11, 264, 5598, 4583, 11, 307, 257, 857, 295, 257, 3455, 1113, 4003, 294, 264, 636, 300, 341, 307, 4362, 18721, 11, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.07000244747508656, "compression_ratio": 1.671497584541063, "no_speech_prob": 9.222769222105853e-06}, {"id": 1197, "seek": 590200, "start": 5902.0, "end": 5916.0, "text": " because you can see that the last layer here in pink takes on values that are much larger than some of the values inside the neural net.", "tokens": [50364, 570, 291, 393, 536, 300, 264, 1036, 4583, 510, 294, 7022, 2516, 322, 4190, 300, 366, 709, 4833, 813, 512, 295, 264, 4190, 1854, 264, 18161, 2533, 13, 51064, 51064, 407, 264, 3832, 31219, 763, 366, 9810, 7935, 3671, 1045, 3710, 11, 3993, 337, 264, 1036, 4583, 11, 597, 767, 575, 9810, 7935, 3671, 732, 3832, 25163, 295, 2771, 2448, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10280670752892128, "compression_ratio": 1.776536312849162, "no_speech_prob": 2.8853979529230855e-05}, {"id": 1198, "seek": 590200, "start": 5916.0, "end": 5926.0, "text": " So the standard deviations are roughly wanting negative three throughout, except for the last layer, which actually has roughly wanting negative two standard deviation of gradients.", "tokens": [50364, 570, 291, 393, 536, 300, 264, 1036, 4583, 510, 294, 7022, 2516, 322, 4190, 300, 366, 709, 4833, 813, 512, 295, 264, 4190, 1854, 264, 18161, 2533, 13, 51064, 51064, 407, 264, 3832, 31219, 763, 366, 9810, 7935, 3671, 1045, 3710, 11, 3993, 337, 264, 1036, 4583, 11, 597, 767, 575, 9810, 7935, 3671, 732, 3832, 25163, 295, 2771, 2448, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10280670752892128, "compression_ratio": 1.776536312849162, "no_speech_prob": 2.8853979529230855e-05}, {"id": 1199, "seek": 592600, "start": 5926.0, "end": 5936.0, "text": " And so the gradients on the last layer are currently about 100 times greater, sorry, 10 times greater than all the other weights inside the neural net.", "tokens": [50364, 400, 370, 264, 2771, 2448, 322, 264, 1036, 4583, 366, 4362, 466, 2319, 1413, 5044, 11, 2597, 11, 1266, 1413, 5044, 813, 439, 264, 661, 17443, 1854, 264, 18161, 2533, 13, 50864, 50864, 400, 370, 300, 311, 19011, 570, 294, 264, 2199, 342, 8997, 2750, 16235, 294, 264, 2020, 8657, 11, 291, 576, 312, 3097, 341, 1036, 4583, 466, 1266, 1413, 4663, 813, 291, 576, 312, 3097, 264, 661, 7914, 412, 5883, 2144, 13, 51414, 51414, 823, 11, 341, 767, 411, 733, 295, 32539, 2564, 257, 707, 857, 498, 291, 3847, 337, 257, 857, 2854, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10504252433776856, "compression_ratio": 1.8112449799196788, "no_speech_prob": 1.618665010028053e-05}, {"id": 1200, "seek": 592600, "start": 5936.0, "end": 5947.0, "text": " And so that's problematic because in the simple stochastic gradient in the sense setup, you would be training this last layer about 10 times faster than you would be training the other layers at initialization.", "tokens": [50364, 400, 370, 264, 2771, 2448, 322, 264, 1036, 4583, 366, 4362, 466, 2319, 1413, 5044, 11, 2597, 11, 1266, 1413, 5044, 813, 439, 264, 661, 17443, 1854, 264, 18161, 2533, 13, 50864, 50864, 400, 370, 300, 311, 19011, 570, 294, 264, 2199, 342, 8997, 2750, 16235, 294, 264, 2020, 8657, 11, 291, 576, 312, 3097, 341, 1036, 4583, 466, 1266, 1413, 4663, 813, 291, 576, 312, 3097, 264, 661, 7914, 412, 5883, 2144, 13, 51414, 51414, 823, 11, 341, 767, 411, 733, 295, 32539, 2564, 257, 707, 857, 498, 291, 3847, 337, 257, 857, 2854, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10504252433776856, "compression_ratio": 1.8112449799196788, "no_speech_prob": 1.618665010028053e-05}, {"id": 1201, "seek": 592600, "start": 5947.0, "end": 5951.0, "text": " Now, this actually like kind of fixes itself a little bit if you train for a bit longer.", "tokens": [50364, 400, 370, 264, 2771, 2448, 322, 264, 1036, 4583, 366, 4362, 466, 2319, 1413, 5044, 11, 2597, 11, 1266, 1413, 5044, 813, 439, 264, 661, 17443, 1854, 264, 18161, 2533, 13, 50864, 50864, 400, 370, 300, 311, 19011, 570, 294, 264, 2199, 342, 8997, 2750, 16235, 294, 264, 2020, 8657, 11, 291, 576, 312, 3097, 341, 1036, 4583, 466, 1266, 1413, 4663, 813, 291, 576, 312, 3097, 264, 661, 7914, 412, 5883, 2144, 13, 51414, 51414, 823, 11, 341, 767, 411, 733, 295, 32539, 2564, 257, 707, 857, 498, 291, 3847, 337, 257, 857, 2854, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.10504252433776856, "compression_ratio": 1.8112449799196788, "no_speech_prob": 1.618665010028053e-05}, {"id": 1202, "seek": 595100, "start": 5951.0, "end": 5956.0, "text": " So, for example, if I agree, then 1000 only then do a break.", "tokens": [50364, 407, 11, 337, 1365, 11, 498, 286, 3986, 11, 550, 9714, 787, 550, 360, 257, 1821, 13, 50614, 50614, 961, 385, 5883, 1125, 293, 550, 718, 385, 360, 309, 9714, 4439, 13, 50814, 50814, 400, 934, 9714, 4439, 11, 321, 393, 574, 412, 264, 2128, 1320, 13, 51014, 51014, 2264, 11, 370, 291, 536, 577, 264, 22027, 366, 257, 857, 366, 21160, 990, 257, 857, 13, 51214, 51214, 400, 321, 393, 611, 574, 412, 264, 23897, 1320, 13, 51314, 51314, 583, 5911, 11, 436, 574, 665, 13, 814, 434, 466, 2681, 13, 51414, 51414, 400, 456, 311, 572, 41684, 281, 4018, 420, 35175, 281, 7193, 1088, 13, 51614, 51614, 400, 291, 393, 536, 300, 510, 294, 264, 17443, 11, 721, 366, 611, 11652, 3319, 257, 707, 857, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12221831986398408, "compression_ratio": 1.7602996254681649, "no_speech_prob": 1.5936006093397737e-05}, {"id": 1203, "seek": 595100, "start": 5956.0, "end": 5960.0, "text": " Let me initialize and then let me do it 1000 steps.", "tokens": [50364, 407, 11, 337, 1365, 11, 498, 286, 3986, 11, 550, 9714, 787, 550, 360, 257, 1821, 13, 50614, 50614, 961, 385, 5883, 1125, 293, 550, 718, 385, 360, 309, 9714, 4439, 13, 50814, 50814, 400, 934, 9714, 4439, 11, 321, 393, 574, 412, 264, 2128, 1320, 13, 51014, 51014, 2264, 11, 370, 291, 536, 577, 264, 22027, 366, 257, 857, 366, 21160, 990, 257, 857, 13, 51214, 51214, 400, 321, 393, 611, 574, 412, 264, 23897, 1320, 13, 51314, 51314, 583, 5911, 11, 436, 574, 665, 13, 814, 434, 466, 2681, 13, 51414, 51414, 400, 456, 311, 572, 41684, 281, 4018, 420, 35175, 281, 7193, 1088, 13, 51614, 51614, 400, 291, 393, 536, 300, 510, 294, 264, 17443, 11, 721, 366, 611, 11652, 3319, 257, 707, 857, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12221831986398408, "compression_ratio": 1.7602996254681649, "no_speech_prob": 1.5936006093397737e-05}, {"id": 1204, "seek": 595100, "start": 5960.0, "end": 5964.0, "text": " And after 1000 steps, we can look at the forward pass.", "tokens": [50364, 407, 11, 337, 1365, 11, 498, 286, 3986, 11, 550, 9714, 787, 550, 360, 257, 1821, 13, 50614, 50614, 961, 385, 5883, 1125, 293, 550, 718, 385, 360, 309, 9714, 4439, 13, 50814, 50814, 400, 934, 9714, 4439, 11, 321, 393, 574, 412, 264, 2128, 1320, 13, 51014, 51014, 2264, 11, 370, 291, 536, 577, 264, 22027, 366, 257, 857, 366, 21160, 990, 257, 857, 13, 51214, 51214, 400, 321, 393, 611, 574, 412, 264, 23897, 1320, 13, 51314, 51314, 583, 5911, 11, 436, 574, 665, 13, 814, 434, 466, 2681, 13, 51414, 51414, 400, 456, 311, 572, 41684, 281, 4018, 420, 35175, 281, 7193, 1088, 13, 51614, 51614, 400, 291, 393, 536, 300, 510, 294, 264, 17443, 11, 721, 366, 611, 11652, 3319, 257, 707, 857, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12221831986398408, "compression_ratio": 1.7602996254681649, "no_speech_prob": 1.5936006093397737e-05}, {"id": 1205, "seek": 595100, "start": 5964.0, "end": 5968.0, "text": " OK, so you see how the neurons are a bit are saturating a bit.", "tokens": [50364, 407, 11, 337, 1365, 11, 498, 286, 3986, 11, 550, 9714, 787, 550, 360, 257, 1821, 13, 50614, 50614, 961, 385, 5883, 1125, 293, 550, 718, 385, 360, 309, 9714, 4439, 13, 50814, 50814, 400, 934, 9714, 4439, 11, 321, 393, 574, 412, 264, 2128, 1320, 13, 51014, 51014, 2264, 11, 370, 291, 536, 577, 264, 22027, 366, 257, 857, 366, 21160, 990, 257, 857, 13, 51214, 51214, 400, 321, 393, 611, 574, 412, 264, 23897, 1320, 13, 51314, 51314, 583, 5911, 11, 436, 574, 665, 13, 814, 434, 466, 2681, 13, 51414, 51414, 400, 456, 311, 572, 41684, 281, 4018, 420, 35175, 281, 7193, 1088, 13, 51614, 51614, 400, 291, 393, 536, 300, 510, 294, 264, 17443, 11, 721, 366, 611, 11652, 3319, 257, 707, 857, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12221831986398408, "compression_ratio": 1.7602996254681649, "no_speech_prob": 1.5936006093397737e-05}, {"id": 1206, "seek": 595100, "start": 5968.0, "end": 5970.0, "text": " And we can also look at the backward pass.", "tokens": [50364, 407, 11, 337, 1365, 11, 498, 286, 3986, 11, 550, 9714, 787, 550, 360, 257, 1821, 13, 50614, 50614, 961, 385, 5883, 1125, 293, 550, 718, 385, 360, 309, 9714, 4439, 13, 50814, 50814, 400, 934, 9714, 4439, 11, 321, 393, 574, 412, 264, 2128, 1320, 13, 51014, 51014, 2264, 11, 370, 291, 536, 577, 264, 22027, 366, 257, 857, 366, 21160, 990, 257, 857, 13, 51214, 51214, 400, 321, 393, 611, 574, 412, 264, 23897, 1320, 13, 51314, 51314, 583, 5911, 11, 436, 574, 665, 13, 814, 434, 466, 2681, 13, 51414, 51414, 400, 456, 311, 572, 41684, 281, 4018, 420, 35175, 281, 7193, 1088, 13, 51614, 51614, 400, 291, 393, 536, 300, 510, 294, 264, 17443, 11, 721, 366, 611, 11652, 3319, 257, 707, 857, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12221831986398408, "compression_ratio": 1.7602996254681649, "no_speech_prob": 1.5936006093397737e-05}, {"id": 1207, "seek": 595100, "start": 5970.0, "end": 5972.0, "text": " But otherwise, they look good. They're about equal.", "tokens": [50364, 407, 11, 337, 1365, 11, 498, 286, 3986, 11, 550, 9714, 787, 550, 360, 257, 1821, 13, 50614, 50614, 961, 385, 5883, 1125, 293, 550, 718, 385, 360, 309, 9714, 4439, 13, 50814, 50814, 400, 934, 9714, 4439, 11, 321, 393, 574, 412, 264, 2128, 1320, 13, 51014, 51014, 2264, 11, 370, 291, 536, 577, 264, 22027, 366, 257, 857, 366, 21160, 990, 257, 857, 13, 51214, 51214, 400, 321, 393, 611, 574, 412, 264, 23897, 1320, 13, 51314, 51314, 583, 5911, 11, 436, 574, 665, 13, 814, 434, 466, 2681, 13, 51414, 51414, 400, 456, 311, 572, 41684, 281, 4018, 420, 35175, 281, 7193, 1088, 13, 51614, 51614, 400, 291, 393, 536, 300, 510, 294, 264, 17443, 11, 721, 366, 611, 11652, 3319, 257, 707, 857, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12221831986398408, "compression_ratio": 1.7602996254681649, "no_speech_prob": 1.5936006093397737e-05}, {"id": 1208, "seek": 595100, "start": 5972.0, "end": 5976.0, "text": " And there's no shrinking to zero or exploding to infinities.", "tokens": [50364, 407, 11, 337, 1365, 11, 498, 286, 3986, 11, 550, 9714, 787, 550, 360, 257, 1821, 13, 50614, 50614, 961, 385, 5883, 1125, 293, 550, 718, 385, 360, 309, 9714, 4439, 13, 50814, 50814, 400, 934, 9714, 4439, 11, 321, 393, 574, 412, 264, 2128, 1320, 13, 51014, 51014, 2264, 11, 370, 291, 536, 577, 264, 22027, 366, 257, 857, 366, 21160, 990, 257, 857, 13, 51214, 51214, 400, 321, 393, 611, 574, 412, 264, 23897, 1320, 13, 51314, 51314, 583, 5911, 11, 436, 574, 665, 13, 814, 434, 466, 2681, 13, 51414, 51414, 400, 456, 311, 572, 41684, 281, 4018, 420, 35175, 281, 7193, 1088, 13, 51614, 51614, 400, 291, 393, 536, 300, 510, 294, 264, 17443, 11, 721, 366, 611, 11652, 3319, 257, 707, 857, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12221831986398408, "compression_ratio": 1.7602996254681649, "no_speech_prob": 1.5936006093397737e-05}, {"id": 1209, "seek": 595100, "start": 5976.0, "end": 5980.0, "text": " And you can see that here in the weights, things are also stabilizing a little bit.", "tokens": [50364, 407, 11, 337, 1365, 11, 498, 286, 3986, 11, 550, 9714, 787, 550, 360, 257, 1821, 13, 50614, 50614, 961, 385, 5883, 1125, 293, 550, 718, 385, 360, 309, 9714, 4439, 13, 50814, 50814, 400, 934, 9714, 4439, 11, 321, 393, 574, 412, 264, 2128, 1320, 13, 51014, 51014, 2264, 11, 370, 291, 536, 577, 264, 22027, 366, 257, 857, 366, 21160, 990, 257, 857, 13, 51214, 51214, 400, 321, 393, 611, 574, 412, 264, 23897, 1320, 13, 51314, 51314, 583, 5911, 11, 436, 574, 665, 13, 814, 434, 466, 2681, 13, 51414, 51414, 400, 456, 311, 572, 41684, 281, 4018, 420, 35175, 281, 7193, 1088, 13, 51614, 51614, 400, 291, 393, 536, 300, 510, 294, 264, 17443, 11, 721, 366, 611, 11652, 3319, 257, 707, 857, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.12221831986398408, "compression_ratio": 1.7602996254681649, "no_speech_prob": 1.5936006093397737e-05}, {"id": 1210, "seek": 598000, "start": 5980.0, "end": 5986.0, "text": " So the tails of the last pink layer are actually coming down, coming in during the optimization.", "tokens": [50364, 407, 264, 28537, 295, 264, 1036, 7022, 4583, 366, 767, 1348, 760, 11, 1348, 294, 1830, 264, 19618, 13, 50664, 50664, 583, 3297, 341, 307, 411, 257, 707, 857, 38080, 11, 2318, 498, 291, 366, 1228, 257, 588, 2199, 5623, 4978, 411, 342, 8997, 2750, 16235, 23475, 2602, 295, 257, 4363, 5028, 6545, 411, 7938, 13, 51114, 51114, 823, 286, 1116, 411, 281, 855, 291, 472, 544, 7542, 300, 286, 2673, 574, 412, 562, 286, 3847, 18161, 9590, 13, 51314, 51314, 400, 1936, 11, 264, 16235, 281, 1412, 8509, 307, 406, 767, 300, 27759, 570, 437, 7001, 412, 264, 917, 307, 406, 264, 16235, 281, 1412, 8509, 11, 457, 264, 5623, 281, 264, 1412, 8509, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07558667659759521, "compression_ratio": 1.7745098039215685, "no_speech_prob": 1.6700456399121322e-05}, {"id": 1211, "seek": 598000, "start": 5986.0, "end": 5995.0, "text": " But certainly this is like a little bit troubling, especially if you are using a very simple update rule like stochastic gradient descent instead of a modern optimizer like Adam.", "tokens": [50364, 407, 264, 28537, 295, 264, 1036, 7022, 4583, 366, 767, 1348, 760, 11, 1348, 294, 1830, 264, 19618, 13, 50664, 50664, 583, 3297, 341, 307, 411, 257, 707, 857, 38080, 11, 2318, 498, 291, 366, 1228, 257, 588, 2199, 5623, 4978, 411, 342, 8997, 2750, 16235, 23475, 2602, 295, 257, 4363, 5028, 6545, 411, 7938, 13, 51114, 51114, 823, 286, 1116, 411, 281, 855, 291, 472, 544, 7542, 300, 286, 2673, 574, 412, 562, 286, 3847, 18161, 9590, 13, 51314, 51314, 400, 1936, 11, 264, 16235, 281, 1412, 8509, 307, 406, 767, 300, 27759, 570, 437, 7001, 412, 264, 917, 307, 406, 264, 16235, 281, 1412, 8509, 11, 457, 264, 5623, 281, 264, 1412, 8509, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07558667659759521, "compression_ratio": 1.7745098039215685, "no_speech_prob": 1.6700456399121322e-05}, {"id": 1212, "seek": 598000, "start": 5995.0, "end": 5999.0, "text": " Now I'd like to show you one more plot that I usually look at when I train neural networks.", "tokens": [50364, 407, 264, 28537, 295, 264, 1036, 7022, 4583, 366, 767, 1348, 760, 11, 1348, 294, 1830, 264, 19618, 13, 50664, 50664, 583, 3297, 341, 307, 411, 257, 707, 857, 38080, 11, 2318, 498, 291, 366, 1228, 257, 588, 2199, 5623, 4978, 411, 342, 8997, 2750, 16235, 23475, 2602, 295, 257, 4363, 5028, 6545, 411, 7938, 13, 51114, 51114, 823, 286, 1116, 411, 281, 855, 291, 472, 544, 7542, 300, 286, 2673, 574, 412, 562, 286, 3847, 18161, 9590, 13, 51314, 51314, 400, 1936, 11, 264, 16235, 281, 1412, 8509, 307, 406, 767, 300, 27759, 570, 437, 7001, 412, 264, 917, 307, 406, 264, 16235, 281, 1412, 8509, 11, 457, 264, 5623, 281, 264, 1412, 8509, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07558667659759521, "compression_ratio": 1.7745098039215685, "no_speech_prob": 1.6700456399121322e-05}, {"id": 1213, "seek": 598000, "start": 5999.0, "end": 6008.0, "text": " And basically, the gradient to data ratio is not actually that informative because what matters at the end is not the gradient to data ratio, but the update to the data ratio,", "tokens": [50364, 407, 264, 28537, 295, 264, 1036, 7022, 4583, 366, 767, 1348, 760, 11, 1348, 294, 1830, 264, 19618, 13, 50664, 50664, 583, 3297, 341, 307, 411, 257, 707, 857, 38080, 11, 2318, 498, 291, 366, 1228, 257, 588, 2199, 5623, 4978, 411, 342, 8997, 2750, 16235, 23475, 2602, 295, 257, 4363, 5028, 6545, 411, 7938, 13, 51114, 51114, 823, 286, 1116, 411, 281, 855, 291, 472, 544, 7542, 300, 286, 2673, 574, 412, 562, 286, 3847, 18161, 9590, 13, 51314, 51314, 400, 1936, 11, 264, 16235, 281, 1412, 8509, 307, 406, 767, 300, 27759, 570, 437, 7001, 412, 264, 917, 307, 406, 264, 16235, 281, 1412, 8509, 11, 457, 264, 5623, 281, 264, 1412, 8509, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07558667659759521, "compression_ratio": 1.7745098039215685, "no_speech_prob": 1.6700456399121322e-05}, {"id": 1214, "seek": 600800, "start": 6008.0, "end": 6012.0, "text": " because that is the amount by which we will actually change the data in these tensors.", "tokens": [50364, 570, 300, 307, 264, 2372, 538, 597, 321, 486, 767, 1319, 264, 1412, 294, 613, 10688, 830, 13, 50564, 50564, 407, 1348, 493, 510, 11, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 5366, 257, 777, 5623, 281, 1412, 8509, 13, 50914, 50914, 467, 311, 516, 281, 312, 2731, 13, 400, 321, 434, 516, 281, 1322, 309, 484, 633, 2167, 24784, 13, 51064, 51064, 400, 510, 286, 1116, 411, 281, 1066, 2837, 295, 1936, 264, 8509, 633, 2167, 24784, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.0777765032888829, "compression_ratio": 1.7055837563451777, "no_speech_prob": 7.766659109620377e-06}, {"id": 1215, "seek": 600800, "start": 6012.0, "end": 6019.0, "text": " So coming up here, what I'd like to do is I'd like to introduce a new update to data ratio.", "tokens": [50364, 570, 300, 307, 264, 2372, 538, 597, 321, 486, 767, 1319, 264, 1412, 294, 613, 10688, 830, 13, 50564, 50564, 407, 1348, 493, 510, 11, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 5366, 257, 777, 5623, 281, 1412, 8509, 13, 50914, 50914, 467, 311, 516, 281, 312, 2731, 13, 400, 321, 434, 516, 281, 1322, 309, 484, 633, 2167, 24784, 13, 51064, 51064, 400, 510, 286, 1116, 411, 281, 1066, 2837, 295, 1936, 264, 8509, 633, 2167, 24784, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.0777765032888829, "compression_ratio": 1.7055837563451777, "no_speech_prob": 7.766659109620377e-06}, {"id": 1216, "seek": 600800, "start": 6019.0, "end": 6022.0, "text": " It's going to be lost. And we're going to build it out every single iteration.", "tokens": [50364, 570, 300, 307, 264, 2372, 538, 597, 321, 486, 767, 1319, 264, 1412, 294, 613, 10688, 830, 13, 50564, 50564, 407, 1348, 493, 510, 11, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 5366, 257, 777, 5623, 281, 1412, 8509, 13, 50914, 50914, 467, 311, 516, 281, 312, 2731, 13, 400, 321, 434, 516, 281, 1322, 309, 484, 633, 2167, 24784, 13, 51064, 51064, 400, 510, 286, 1116, 411, 281, 1066, 2837, 295, 1936, 264, 8509, 633, 2167, 24784, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.0777765032888829, "compression_ratio": 1.7055837563451777, "no_speech_prob": 7.766659109620377e-06}, {"id": 1217, "seek": 600800, "start": 6022.0, "end": 6029.0, "text": " And here I'd like to keep track of basically the ratio every single iteration.", "tokens": [50364, 570, 300, 307, 264, 2372, 538, 597, 321, 486, 767, 1319, 264, 1412, 294, 613, 10688, 830, 13, 50564, 50564, 407, 1348, 493, 510, 11, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 5366, 257, 777, 5623, 281, 1412, 8509, 13, 50914, 50914, 467, 311, 516, 281, 312, 2731, 13, 400, 321, 434, 516, 281, 1322, 309, 484, 633, 2167, 24784, 13, 51064, 51064, 400, 510, 286, 1116, 411, 281, 1066, 2837, 295, 1936, 264, 8509, 633, 2167, 24784, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.0777765032888829, "compression_ratio": 1.7055837563451777, "no_speech_prob": 7.766659109620377e-06}, {"id": 1218, "seek": 602900, "start": 6029.0, "end": 6038.0, "text": " So without any gradients, I'm comparing the update, which is learning rate times the gradient.", "tokens": [50364, 407, 1553, 604, 2771, 2448, 11, 286, 478, 15763, 264, 5623, 11, 597, 307, 2539, 3314, 1413, 264, 16235, 13, 50814, 50814, 663, 307, 264, 5623, 300, 321, 434, 516, 281, 3079, 281, 633, 13075, 13, 51014, 51014, 407, 536, 11, 286, 478, 17138, 990, 670, 439, 264, 9834, 293, 550, 286, 478, 1940, 264, 1936, 3832, 25163, 295, 264, 5623, 321, 434, 516, 281, 3079, 293, 6666, 538, 264, 3539, 2701, 11, 264, 1412, 295, 300, 13075, 293, 1080, 3832, 25163, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1398965572488719, "compression_ratio": 1.8413461538461537, "no_speech_prob": 6.240615221031476e-06}, {"id": 1219, "seek": 602900, "start": 6038.0, "end": 6042.0, "text": " That is the update that we're going to apply to every parameter.", "tokens": [50364, 407, 1553, 604, 2771, 2448, 11, 286, 478, 15763, 264, 5623, 11, 597, 307, 2539, 3314, 1413, 264, 16235, 13, 50814, 50814, 663, 307, 264, 5623, 300, 321, 434, 516, 281, 3079, 281, 633, 13075, 13, 51014, 51014, 407, 536, 11, 286, 478, 17138, 990, 670, 439, 264, 9834, 293, 550, 286, 478, 1940, 264, 1936, 3832, 25163, 295, 264, 5623, 321, 434, 516, 281, 3079, 293, 6666, 538, 264, 3539, 2701, 11, 264, 1412, 295, 300, 13075, 293, 1080, 3832, 25163, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1398965572488719, "compression_ratio": 1.8413461538461537, "no_speech_prob": 6.240615221031476e-06}, {"id": 1220, "seek": 602900, "start": 6042.0, "end": 6055.0, "text": " So see, I'm iterating over all the parameters and then I'm taking the basically standard deviation of the update we're going to apply and divided by the actual content, the data of that parameter and its standard deviation.", "tokens": [50364, 407, 1553, 604, 2771, 2448, 11, 286, 478, 15763, 264, 5623, 11, 597, 307, 2539, 3314, 1413, 264, 16235, 13, 50814, 50814, 663, 307, 264, 5623, 300, 321, 434, 516, 281, 3079, 281, 633, 13075, 13, 51014, 51014, 407, 536, 11, 286, 478, 17138, 990, 670, 439, 264, 9834, 293, 550, 286, 478, 1940, 264, 1936, 3832, 25163, 295, 264, 5623, 321, 434, 516, 281, 3079, 293, 6666, 538, 264, 3539, 2701, 11, 264, 1412, 295, 300, 13075, 293, 1080, 3832, 25163, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1398965572488719, "compression_ratio": 1.8413461538461537, "no_speech_prob": 6.240615221031476e-06}, {"id": 1221, "seek": 605500, "start": 6055.0, "end": 6061.0, "text": " So this is the ratio of basically how great are the updates to the values in these tensors.", "tokens": [50364, 407, 341, 307, 264, 8509, 295, 1936, 577, 869, 366, 264, 9205, 281, 264, 4190, 294, 613, 10688, 830, 13, 50664, 50664, 1396, 321, 434, 516, 281, 747, 257, 3565, 295, 309, 13, 400, 767, 11, 286, 1116, 411, 281, 747, 257, 3565, 1266, 445, 370, 309, 311, 257, 22842, 25801, 13, 51064, 51064, 407, 321, 434, 516, 281, 312, 1936, 1237, 412, 264, 12680, 791, 295, 264, 10044, 510, 293, 550, 300, 3174, 281, 1665, 484, 264, 15706, 13, 51514, 51514, 400, 321, 434, 516, 281, 312, 5145, 2837, 295, 341, 337, 439, 264, 9834, 293, 5127, 309, 281, 613, 624, 35, 40863, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0841661943208187, "compression_ratio": 1.7448559670781894, "no_speech_prob": 4.222650659357896e-06}, {"id": 1222, "seek": 605500, "start": 6061.0, "end": 6069.0, "text": " Then we're going to take a log of it. And actually, I'd like to take a log 10 just so it's a nicer visualization.", "tokens": [50364, 407, 341, 307, 264, 8509, 295, 1936, 577, 869, 366, 264, 9205, 281, 264, 4190, 294, 613, 10688, 830, 13, 50664, 50664, 1396, 321, 434, 516, 281, 747, 257, 3565, 295, 309, 13, 400, 767, 11, 286, 1116, 411, 281, 747, 257, 3565, 1266, 445, 370, 309, 311, 257, 22842, 25801, 13, 51064, 51064, 407, 321, 434, 516, 281, 312, 1936, 1237, 412, 264, 12680, 791, 295, 264, 10044, 510, 293, 550, 300, 3174, 281, 1665, 484, 264, 15706, 13, 51514, 51514, 400, 321, 434, 516, 281, 312, 5145, 2837, 295, 341, 337, 439, 264, 9834, 293, 5127, 309, 281, 613, 624, 35, 40863, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0841661943208187, "compression_ratio": 1.7448559670781894, "no_speech_prob": 4.222650659357896e-06}, {"id": 1223, "seek": 605500, "start": 6069.0, "end": 6078.0, "text": " So we're going to be basically looking at the exponents of the division here and then that item to pop out the float.", "tokens": [50364, 407, 341, 307, 264, 8509, 295, 1936, 577, 869, 366, 264, 9205, 281, 264, 4190, 294, 613, 10688, 830, 13, 50664, 50664, 1396, 321, 434, 516, 281, 747, 257, 3565, 295, 309, 13, 400, 767, 11, 286, 1116, 411, 281, 747, 257, 3565, 1266, 445, 370, 309, 311, 257, 22842, 25801, 13, 51064, 51064, 407, 321, 434, 516, 281, 312, 1936, 1237, 412, 264, 12680, 791, 295, 264, 10044, 510, 293, 550, 300, 3174, 281, 1665, 484, 264, 15706, 13, 51514, 51514, 400, 321, 434, 516, 281, 312, 5145, 2837, 295, 341, 337, 439, 264, 9834, 293, 5127, 309, 281, 613, 624, 35, 40863, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0841661943208187, "compression_ratio": 1.7448559670781894, "no_speech_prob": 4.222650659357896e-06}, {"id": 1224, "seek": 605500, "start": 6078.0, "end": 6083.0, "text": " And we're going to be keeping track of this for all the parameters and adding it to these UD tensor.", "tokens": [50364, 407, 341, 307, 264, 8509, 295, 1936, 577, 869, 366, 264, 9205, 281, 264, 4190, 294, 613, 10688, 830, 13, 50664, 50664, 1396, 321, 434, 516, 281, 747, 257, 3565, 295, 309, 13, 400, 767, 11, 286, 1116, 411, 281, 747, 257, 3565, 1266, 445, 370, 309, 311, 257, 22842, 25801, 13, 51064, 51064, 407, 321, 434, 516, 281, 312, 1936, 1237, 412, 264, 12680, 791, 295, 264, 10044, 510, 293, 550, 300, 3174, 281, 1665, 484, 264, 15706, 13, 51514, 51514, 400, 321, 434, 516, 281, 312, 5145, 2837, 295, 341, 337, 439, 264, 9834, 293, 5127, 309, 281, 613, 624, 35, 40863, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0841661943208187, "compression_ratio": 1.7448559670781894, "no_speech_prob": 4.222650659357896e-06}, {"id": 1225, "seek": 608300, "start": 6083.0, "end": 6087.0, "text": " So now let me reinitialize and run a thousand iterations.", "tokens": [50364, 407, 586, 718, 385, 6561, 270, 831, 1125, 293, 1190, 257, 4714, 36540, 13, 50564, 50564, 492, 393, 574, 412, 264, 2430, 763, 11, 264, 2771, 2448, 293, 264, 13075, 2771, 2448, 382, 321, 630, 949, 13, 50914, 50914, 583, 586, 286, 362, 472, 544, 7542, 510, 281, 5366, 13, 51064, 51064, 708, 311, 2737, 510, 307, 689, 633, 15035, 295, 264, 9834, 293, 286, 478, 11525, 1760, 309, 797, 11, 411, 286, 630, 510, 11, 281, 445, 264, 17443, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.13586931789622586, "compression_ratio": 1.6097560975609757, "no_speech_prob": 4.860302396991756e-06}, {"id": 1226, "seek": 608300, "start": 6087.0, "end": 6094.0, "text": " We can look at the activations, the gradients and the parameter gradients as we did before.", "tokens": [50364, 407, 586, 718, 385, 6561, 270, 831, 1125, 293, 1190, 257, 4714, 36540, 13, 50564, 50564, 492, 393, 574, 412, 264, 2430, 763, 11, 264, 2771, 2448, 293, 264, 13075, 2771, 2448, 382, 321, 630, 949, 13, 50914, 50914, 583, 586, 286, 362, 472, 544, 7542, 510, 281, 5366, 13, 51064, 51064, 708, 311, 2737, 510, 307, 689, 633, 15035, 295, 264, 9834, 293, 286, 478, 11525, 1760, 309, 797, 11, 411, 286, 630, 510, 11, 281, 445, 264, 17443, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.13586931789622586, "compression_ratio": 1.6097560975609757, "no_speech_prob": 4.860302396991756e-06}, {"id": 1227, "seek": 608300, "start": 6094.0, "end": 6097.0, "text": " But now I have one more plot here to introduce.", "tokens": [50364, 407, 586, 718, 385, 6561, 270, 831, 1125, 293, 1190, 257, 4714, 36540, 13, 50564, 50564, 492, 393, 574, 412, 264, 2430, 763, 11, 264, 2771, 2448, 293, 264, 13075, 2771, 2448, 382, 321, 630, 949, 13, 50914, 50914, 583, 586, 286, 362, 472, 544, 7542, 510, 281, 5366, 13, 51064, 51064, 708, 311, 2737, 510, 307, 689, 633, 15035, 295, 264, 9834, 293, 286, 478, 11525, 1760, 309, 797, 11, 411, 286, 630, 510, 11, 281, 445, 264, 17443, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.13586931789622586, "compression_ratio": 1.6097560975609757, "no_speech_prob": 4.860302396991756e-06}, {"id": 1228, "seek": 608300, "start": 6097.0, "end": 6104.0, "text": " What's happening here is where every interval of the parameters and I'm constraining it again, like I did here, to just the weights.", "tokens": [50364, 407, 586, 718, 385, 6561, 270, 831, 1125, 293, 1190, 257, 4714, 36540, 13, 50564, 50564, 492, 393, 574, 412, 264, 2430, 763, 11, 264, 2771, 2448, 293, 264, 13075, 2771, 2448, 382, 321, 630, 949, 13, 50914, 50914, 583, 586, 286, 362, 472, 544, 7542, 510, 281, 5366, 13, 51064, 51064, 708, 311, 2737, 510, 307, 689, 633, 15035, 295, 264, 9834, 293, 286, 478, 11525, 1760, 309, 797, 11, 411, 286, 630, 510, 11, 281, 445, 264, 17443, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.13586931789622586, "compression_ratio": 1.6097560975609757, "no_speech_prob": 4.860302396991756e-06}, {"id": 1229, "seek": 610400, "start": 6104.0, "end": 6114.0, "text": " So the number of dimensions in these sensors is two. And then I'm basically plotting all of these update ratios over time.", "tokens": [50364, 407, 264, 1230, 295, 12819, 294, 613, 14840, 307, 732, 13, 400, 550, 286, 478, 1936, 41178, 439, 295, 613, 5623, 32435, 670, 565, 13, 50864, 50864, 407, 562, 286, 7542, 341, 11, 286, 7542, 729, 32435, 293, 291, 393, 536, 300, 436, 16693, 670, 565, 1830, 5883, 2144, 281, 747, 322, 1629, 4190, 13, 51264, 51264, 400, 550, 613, 9205, 366, 411, 722, 11652, 3319, 2673, 1830, 3097, 13, 51464, 51464, 1396, 264, 661, 551, 300, 286, 478, 41178, 510, 307, 286, 478, 41178, 510, 411, 364, 30874, 2158, 300, 307, 257, 5903, 5934, 337, 437, 309, 9810, 820, 312, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06632044630230598, "compression_ratio": 1.83011583011583, "no_speech_prob": 3.785170065384591e-06}, {"id": 1230, "seek": 610400, "start": 6114.0, "end": 6122.0, "text": " So when I plot this, I plot those ratios and you can see that they evolve over time during initialization to take on certain values.", "tokens": [50364, 407, 264, 1230, 295, 12819, 294, 613, 14840, 307, 732, 13, 400, 550, 286, 478, 1936, 41178, 439, 295, 613, 5623, 32435, 670, 565, 13, 50864, 50864, 407, 562, 286, 7542, 341, 11, 286, 7542, 729, 32435, 293, 291, 393, 536, 300, 436, 16693, 670, 565, 1830, 5883, 2144, 281, 747, 322, 1629, 4190, 13, 51264, 51264, 400, 550, 613, 9205, 366, 411, 722, 11652, 3319, 2673, 1830, 3097, 13, 51464, 51464, 1396, 264, 661, 551, 300, 286, 478, 41178, 510, 307, 286, 478, 41178, 510, 411, 364, 30874, 2158, 300, 307, 257, 5903, 5934, 337, 437, 309, 9810, 820, 312, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06632044630230598, "compression_ratio": 1.83011583011583, "no_speech_prob": 3.785170065384591e-06}, {"id": 1231, "seek": 610400, "start": 6122.0, "end": 6126.0, "text": " And then these updates are like start stabilizing usually during training.", "tokens": [50364, 407, 264, 1230, 295, 12819, 294, 613, 14840, 307, 732, 13, 400, 550, 286, 478, 1936, 41178, 439, 295, 613, 5623, 32435, 670, 565, 13, 50864, 50864, 407, 562, 286, 7542, 341, 11, 286, 7542, 729, 32435, 293, 291, 393, 536, 300, 436, 16693, 670, 565, 1830, 5883, 2144, 281, 747, 322, 1629, 4190, 13, 51264, 51264, 400, 550, 613, 9205, 366, 411, 722, 11652, 3319, 2673, 1830, 3097, 13, 51464, 51464, 1396, 264, 661, 551, 300, 286, 478, 41178, 510, 307, 286, 478, 41178, 510, 411, 364, 30874, 2158, 300, 307, 257, 5903, 5934, 337, 437, 309, 9810, 820, 312, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06632044630230598, "compression_ratio": 1.83011583011583, "no_speech_prob": 3.785170065384591e-06}, {"id": 1232, "seek": 610400, "start": 6126.0, "end": 6133.0, "text": " Then the other thing that I'm plotting here is I'm plotting here like an approximate value that is a rough guide for what it roughly should be.", "tokens": [50364, 407, 264, 1230, 295, 12819, 294, 613, 14840, 307, 732, 13, 400, 550, 286, 478, 1936, 41178, 439, 295, 613, 5623, 32435, 670, 565, 13, 50864, 50864, 407, 562, 286, 7542, 341, 11, 286, 7542, 729, 32435, 293, 291, 393, 536, 300, 436, 16693, 670, 565, 1830, 5883, 2144, 281, 747, 322, 1629, 4190, 13, 51264, 51264, 400, 550, 613, 9205, 366, 411, 722, 11652, 3319, 2673, 1830, 3097, 13, 51464, 51464, 1396, 264, 661, 551, 300, 286, 478, 41178, 510, 307, 286, 478, 41178, 510, 411, 364, 30874, 2158, 300, 307, 257, 5903, 5934, 337, 437, 309, 9810, 820, 312, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.06632044630230598, "compression_ratio": 1.83011583011583, "no_speech_prob": 3.785170065384591e-06}, {"id": 1233, "seek": 613300, "start": 6133.0, "end": 6135.0, "text": " And it should be like roughly one in negative three.", "tokens": [50364, 400, 309, 820, 312, 411, 9810, 472, 294, 3671, 1045, 13, 50464, 50464, 400, 370, 300, 1355, 300, 1936, 456, 311, 512, 4190, 294, 341, 40863, 293, 436, 747, 322, 1629, 4190, 293, 264, 9205, 281, 552, 412, 633, 2167, 24784, 366, 572, 544, 813, 9810, 472, 4714, 392, 295, 264, 3539, 15668, 294, 729, 10688, 830, 13, 51264, 51264, 759, 341, 390, 709, 4833, 11, 411, 11, 337, 1365, 11, 498, 341, 390, 498, 264, 3565, 295, 341, 390, 411, 11, 584, 11, 3671, 472, 11, 341, 307, 767, 25113, 729, 4190, 1596, 257, 688, 13, 51714, 51714, 814, 434, 40033, 257, 688, 295, 1319, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09575580906223606, "compression_ratio": 1.7695167286245352, "no_speech_prob": 1.267890752387757e-06}, {"id": 1234, "seek": 613300, "start": 6135.0, "end": 6151.0, "text": " And so that means that basically there's some values in this tensor and they take on certain values and the updates to them at every single iteration are no more than roughly one thousandth of the actual magnitude in those tensors.", "tokens": [50364, 400, 309, 820, 312, 411, 9810, 472, 294, 3671, 1045, 13, 50464, 50464, 400, 370, 300, 1355, 300, 1936, 456, 311, 512, 4190, 294, 341, 40863, 293, 436, 747, 322, 1629, 4190, 293, 264, 9205, 281, 552, 412, 633, 2167, 24784, 366, 572, 544, 813, 9810, 472, 4714, 392, 295, 264, 3539, 15668, 294, 729, 10688, 830, 13, 51264, 51264, 759, 341, 390, 709, 4833, 11, 411, 11, 337, 1365, 11, 498, 341, 390, 498, 264, 3565, 295, 341, 390, 411, 11, 584, 11, 3671, 472, 11, 341, 307, 767, 25113, 729, 4190, 1596, 257, 688, 13, 51714, 51714, 814, 434, 40033, 257, 688, 295, 1319, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09575580906223606, "compression_ratio": 1.7695167286245352, "no_speech_prob": 1.267890752387757e-06}, {"id": 1235, "seek": 613300, "start": 6151.0, "end": 6160.0, "text": " If this was much larger, like, for example, if this was if the log of this was like, say, negative one, this is actually updating those values quite a lot.", "tokens": [50364, 400, 309, 820, 312, 411, 9810, 472, 294, 3671, 1045, 13, 50464, 50464, 400, 370, 300, 1355, 300, 1936, 456, 311, 512, 4190, 294, 341, 40863, 293, 436, 747, 322, 1629, 4190, 293, 264, 9205, 281, 552, 412, 633, 2167, 24784, 366, 572, 544, 813, 9810, 472, 4714, 392, 295, 264, 3539, 15668, 294, 729, 10688, 830, 13, 51264, 51264, 759, 341, 390, 709, 4833, 11, 411, 11, 337, 1365, 11, 498, 341, 390, 498, 264, 3565, 295, 341, 390, 411, 11, 584, 11, 3671, 472, 11, 341, 307, 767, 25113, 729, 4190, 1596, 257, 688, 13, 51714, 51714, 814, 434, 40033, 257, 688, 295, 1319, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09575580906223606, "compression_ratio": 1.7695167286245352, "no_speech_prob": 1.267890752387757e-06}, {"id": 1236, "seek": 613300, "start": 6160.0, "end": 6162.0, "text": " They're undergoing a lot of change.", "tokens": [50364, 400, 309, 820, 312, 411, 9810, 472, 294, 3671, 1045, 13, 50464, 50464, 400, 370, 300, 1355, 300, 1936, 456, 311, 512, 4190, 294, 341, 40863, 293, 436, 747, 322, 1629, 4190, 293, 264, 9205, 281, 552, 412, 633, 2167, 24784, 366, 572, 544, 813, 9810, 472, 4714, 392, 295, 264, 3539, 15668, 294, 729, 10688, 830, 13, 51264, 51264, 759, 341, 390, 709, 4833, 11, 411, 11, 337, 1365, 11, 498, 341, 390, 498, 264, 3565, 295, 341, 390, 411, 11, 584, 11, 3671, 472, 11, 341, 307, 767, 25113, 729, 4190, 1596, 257, 688, 13, 51714, 51714, 814, 434, 40033, 257, 688, 295, 1319, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09575580906223606, "compression_ratio": 1.7695167286245352, "no_speech_prob": 1.267890752387757e-06}, {"id": 1237, "seek": 616200, "start": 6162.0, "end": 6174.0, "text": " But the reason that the final rate, the final layer here is an outlier is because this layer was artificially shrunk down to keep the softmax unconfident.", "tokens": [50364, 583, 264, 1778, 300, 264, 2572, 3314, 11, 264, 2572, 4583, 510, 307, 364, 484, 2753, 307, 570, 341, 4583, 390, 39905, 2270, 9884, 3197, 760, 281, 1066, 264, 2787, 41167, 517, 24697, 1078, 13, 50964, 50964, 407, 510, 291, 536, 577, 321, 12972, 264, 3364, 538, 935, 472, 294, 264, 5883, 2144, 281, 652, 264, 1036, 4583, 17630, 1570, 6679, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.0954097184267911, "compression_ratio": 1.5932203389830508, "no_speech_prob": 6.681391369056655e-07}, {"id": 1238, "seek": 616200, "start": 6174.0, "end": 6184.0, "text": " So here you see how we multiply the weight by point one in the initialization to make the last layer prediction less confident.", "tokens": [50364, 583, 264, 1778, 300, 264, 2572, 3314, 11, 264, 2572, 4583, 510, 307, 364, 484, 2753, 307, 570, 341, 4583, 390, 39905, 2270, 9884, 3197, 760, 281, 1066, 264, 2787, 41167, 517, 24697, 1078, 13, 50964, 50964, 407, 510, 291, 536, 577, 321, 12972, 264, 3364, 538, 935, 472, 294, 264, 5883, 2144, 281, 652, 264, 1036, 4583, 17630, 1570, 6679, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.0954097184267911, "compression_ratio": 1.5932203389830508, "no_speech_prob": 6.681391369056655e-07}, {"id": 1239, "seek": 618400, "start": 6184.0, "end": 6192.0, "text": " That made that artificially made the values inside that tensor way too low. And that's why we're getting temporarily a very high ratio.", "tokens": [50364, 663, 1027, 300, 39905, 2270, 1027, 264, 4190, 1854, 300, 40863, 636, 886, 2295, 13, 400, 300, 311, 983, 321, 434, 1242, 23750, 257, 588, 1090, 8509, 13, 50764, 50764, 583, 291, 536, 300, 300, 11652, 5660, 670, 565, 1564, 300, 3364, 3719, 281, 1466, 3719, 281, 1466, 13, 51064, 51064, 583, 1936, 11, 286, 411, 281, 574, 412, 264, 9303, 295, 341, 5623, 8509, 337, 439, 452, 9834, 2673, 13, 51314, 51314, 400, 286, 411, 281, 652, 988, 300, 309, 311, 406, 886, 709, 3673, 472, 294, 3671, 1045, 9810, 13, 51614, 51614, 407, 926, 3671, 1045, 322, 341, 3565, 7542, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07288612829190548, "compression_ratio": 1.739463601532567, "no_speech_prob": 4.936739060212858e-06}, {"id": 1240, "seek": 618400, "start": 6192.0, "end": 6198.0, "text": " But you see that that stabilizes over time once that weight starts to learn starts to learn.", "tokens": [50364, 663, 1027, 300, 39905, 2270, 1027, 264, 4190, 1854, 300, 40863, 636, 886, 2295, 13, 400, 300, 311, 983, 321, 434, 1242, 23750, 257, 588, 1090, 8509, 13, 50764, 50764, 583, 291, 536, 300, 300, 11652, 5660, 670, 565, 1564, 300, 3364, 3719, 281, 1466, 3719, 281, 1466, 13, 51064, 51064, 583, 1936, 11, 286, 411, 281, 574, 412, 264, 9303, 295, 341, 5623, 8509, 337, 439, 452, 9834, 2673, 13, 51314, 51314, 400, 286, 411, 281, 652, 988, 300, 309, 311, 406, 886, 709, 3673, 472, 294, 3671, 1045, 9810, 13, 51614, 51614, 407, 926, 3671, 1045, 322, 341, 3565, 7542, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07288612829190548, "compression_ratio": 1.739463601532567, "no_speech_prob": 4.936739060212858e-06}, {"id": 1241, "seek": 618400, "start": 6198.0, "end": 6203.0, "text": " But basically, I like to look at the evolution of this update ratio for all my parameters usually.", "tokens": [50364, 663, 1027, 300, 39905, 2270, 1027, 264, 4190, 1854, 300, 40863, 636, 886, 2295, 13, 400, 300, 311, 983, 321, 434, 1242, 23750, 257, 588, 1090, 8509, 13, 50764, 50764, 583, 291, 536, 300, 300, 11652, 5660, 670, 565, 1564, 300, 3364, 3719, 281, 1466, 3719, 281, 1466, 13, 51064, 51064, 583, 1936, 11, 286, 411, 281, 574, 412, 264, 9303, 295, 341, 5623, 8509, 337, 439, 452, 9834, 2673, 13, 51314, 51314, 400, 286, 411, 281, 652, 988, 300, 309, 311, 406, 886, 709, 3673, 472, 294, 3671, 1045, 9810, 13, 51614, 51614, 407, 926, 3671, 1045, 322, 341, 3565, 7542, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07288612829190548, "compression_ratio": 1.739463601532567, "no_speech_prob": 4.936739060212858e-06}, {"id": 1242, "seek": 618400, "start": 6203.0, "end": 6209.0, "text": " And I like to make sure that it's not too much above one in negative three roughly.", "tokens": [50364, 663, 1027, 300, 39905, 2270, 1027, 264, 4190, 1854, 300, 40863, 636, 886, 2295, 13, 400, 300, 311, 983, 321, 434, 1242, 23750, 257, 588, 1090, 8509, 13, 50764, 50764, 583, 291, 536, 300, 300, 11652, 5660, 670, 565, 1564, 300, 3364, 3719, 281, 1466, 3719, 281, 1466, 13, 51064, 51064, 583, 1936, 11, 286, 411, 281, 574, 412, 264, 9303, 295, 341, 5623, 8509, 337, 439, 452, 9834, 2673, 13, 51314, 51314, 400, 286, 411, 281, 652, 988, 300, 309, 311, 406, 886, 709, 3673, 472, 294, 3671, 1045, 9810, 13, 51614, 51614, 407, 926, 3671, 1045, 322, 341, 3565, 7542, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07288612829190548, "compression_ratio": 1.739463601532567, "no_speech_prob": 4.936739060212858e-06}, {"id": 1243, "seek": 618400, "start": 6209.0, "end": 6213.0, "text": " So around negative three on this log plot.", "tokens": [50364, 663, 1027, 300, 39905, 2270, 1027, 264, 4190, 1854, 300, 40863, 636, 886, 2295, 13, 400, 300, 311, 983, 321, 434, 1242, 23750, 257, 588, 1090, 8509, 13, 50764, 50764, 583, 291, 536, 300, 300, 11652, 5660, 670, 565, 1564, 300, 3364, 3719, 281, 1466, 3719, 281, 1466, 13, 51064, 51064, 583, 1936, 11, 286, 411, 281, 574, 412, 264, 9303, 295, 341, 5623, 8509, 337, 439, 452, 9834, 2673, 13, 51314, 51314, 400, 286, 411, 281, 652, 988, 300, 309, 311, 406, 886, 709, 3673, 472, 294, 3671, 1045, 9810, 13, 51614, 51614, 407, 926, 3671, 1045, 322, 341, 3565, 7542, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07288612829190548, "compression_ratio": 1.739463601532567, "no_speech_prob": 4.936739060212858e-06}, {"id": 1244, "seek": 621300, "start": 6213.0, "end": 6217.0, "text": " If it's below negative three, usually that means that the parameters are not training fast enough.", "tokens": [50364, 759, 309, 311, 2507, 3671, 1045, 11, 2673, 300, 1355, 300, 264, 9834, 366, 406, 3097, 2370, 1547, 13, 50564, 50564, 407, 498, 527, 2539, 3314, 390, 588, 2295, 11, 718, 311, 360, 300, 5120, 13, 50764, 50764, 961, 311, 5883, 1125, 293, 550, 718, 311, 767, 360, 257, 2539, 3314, 295, 11, 584, 11, 472, 294, 3671, 1045, 510, 13, 51064, 51064, 407, 4018, 935, 4018, 4018, 472, 13, 51164, 51164, 759, 428, 2539, 3314, 307, 636, 886, 2295, 13, 51364, 51364, 639, 7542, 486, 5850, 10658, 309, 13, 51514, 51514, 407, 291, 536, 577, 439, 295, 613, 9205, 366, 636, 886, 1359, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0651968203553366, "compression_ratio": 1.7375, "no_speech_prob": 3.555925559339812e-06}, {"id": 1245, "seek": 621300, "start": 6217.0, "end": 6221.0, "text": " So if our learning rate was very low, let's do that experiment.", "tokens": [50364, 759, 309, 311, 2507, 3671, 1045, 11, 2673, 300, 1355, 300, 264, 9834, 366, 406, 3097, 2370, 1547, 13, 50564, 50564, 407, 498, 527, 2539, 3314, 390, 588, 2295, 11, 718, 311, 360, 300, 5120, 13, 50764, 50764, 961, 311, 5883, 1125, 293, 550, 718, 311, 767, 360, 257, 2539, 3314, 295, 11, 584, 11, 472, 294, 3671, 1045, 510, 13, 51064, 51064, 407, 4018, 935, 4018, 4018, 472, 13, 51164, 51164, 759, 428, 2539, 3314, 307, 636, 886, 2295, 13, 51364, 51364, 639, 7542, 486, 5850, 10658, 309, 13, 51514, 51514, 407, 291, 536, 577, 439, 295, 613, 9205, 366, 636, 886, 1359, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0651968203553366, "compression_ratio": 1.7375, "no_speech_prob": 3.555925559339812e-06}, {"id": 1246, "seek": 621300, "start": 6221.0, "end": 6227.0, "text": " Let's initialize and then let's actually do a learning rate of, say, one in negative three here.", "tokens": [50364, 759, 309, 311, 2507, 3671, 1045, 11, 2673, 300, 1355, 300, 264, 9834, 366, 406, 3097, 2370, 1547, 13, 50564, 50564, 407, 498, 527, 2539, 3314, 390, 588, 2295, 11, 718, 311, 360, 300, 5120, 13, 50764, 50764, 961, 311, 5883, 1125, 293, 550, 718, 311, 767, 360, 257, 2539, 3314, 295, 11, 584, 11, 472, 294, 3671, 1045, 510, 13, 51064, 51064, 407, 4018, 935, 4018, 4018, 472, 13, 51164, 51164, 759, 428, 2539, 3314, 307, 636, 886, 2295, 13, 51364, 51364, 639, 7542, 486, 5850, 10658, 309, 13, 51514, 51514, 407, 291, 536, 577, 439, 295, 613, 9205, 366, 636, 886, 1359, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0651968203553366, "compression_ratio": 1.7375, "no_speech_prob": 3.555925559339812e-06}, {"id": 1247, "seek": 621300, "start": 6227.0, "end": 6229.0, "text": " So zero point zero zero one.", "tokens": [50364, 759, 309, 311, 2507, 3671, 1045, 11, 2673, 300, 1355, 300, 264, 9834, 366, 406, 3097, 2370, 1547, 13, 50564, 50564, 407, 498, 527, 2539, 3314, 390, 588, 2295, 11, 718, 311, 360, 300, 5120, 13, 50764, 50764, 961, 311, 5883, 1125, 293, 550, 718, 311, 767, 360, 257, 2539, 3314, 295, 11, 584, 11, 472, 294, 3671, 1045, 510, 13, 51064, 51064, 407, 4018, 935, 4018, 4018, 472, 13, 51164, 51164, 759, 428, 2539, 3314, 307, 636, 886, 2295, 13, 51364, 51364, 639, 7542, 486, 5850, 10658, 309, 13, 51514, 51514, 407, 291, 536, 577, 439, 295, 613, 9205, 366, 636, 886, 1359, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0651968203553366, "compression_ratio": 1.7375, "no_speech_prob": 3.555925559339812e-06}, {"id": 1248, "seek": 621300, "start": 6229.0, "end": 6233.0, "text": " If your learning rate is way too low.", "tokens": [50364, 759, 309, 311, 2507, 3671, 1045, 11, 2673, 300, 1355, 300, 264, 9834, 366, 406, 3097, 2370, 1547, 13, 50564, 50564, 407, 498, 527, 2539, 3314, 390, 588, 2295, 11, 718, 311, 360, 300, 5120, 13, 50764, 50764, 961, 311, 5883, 1125, 293, 550, 718, 311, 767, 360, 257, 2539, 3314, 295, 11, 584, 11, 472, 294, 3671, 1045, 510, 13, 51064, 51064, 407, 4018, 935, 4018, 4018, 472, 13, 51164, 51164, 759, 428, 2539, 3314, 307, 636, 886, 2295, 13, 51364, 51364, 639, 7542, 486, 5850, 10658, 309, 13, 51514, 51514, 407, 291, 536, 577, 439, 295, 613, 9205, 366, 636, 886, 1359, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0651968203553366, "compression_ratio": 1.7375, "no_speech_prob": 3.555925559339812e-06}, {"id": 1249, "seek": 621300, "start": 6233.0, "end": 6236.0, "text": " This plot will typically reveal it.", "tokens": [50364, 759, 309, 311, 2507, 3671, 1045, 11, 2673, 300, 1355, 300, 264, 9834, 366, 406, 3097, 2370, 1547, 13, 50564, 50564, 407, 498, 527, 2539, 3314, 390, 588, 2295, 11, 718, 311, 360, 300, 5120, 13, 50764, 50764, 961, 311, 5883, 1125, 293, 550, 718, 311, 767, 360, 257, 2539, 3314, 295, 11, 584, 11, 472, 294, 3671, 1045, 510, 13, 51064, 51064, 407, 4018, 935, 4018, 4018, 472, 13, 51164, 51164, 759, 428, 2539, 3314, 307, 636, 886, 2295, 13, 51364, 51364, 639, 7542, 486, 5850, 10658, 309, 13, 51514, 51514, 407, 291, 536, 577, 439, 295, 613, 9205, 366, 636, 886, 1359, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0651968203553366, "compression_ratio": 1.7375, "no_speech_prob": 3.555925559339812e-06}, {"id": 1250, "seek": 621300, "start": 6236.0, "end": 6240.0, "text": " So you see how all of these updates are way too small.", "tokens": [50364, 759, 309, 311, 2507, 3671, 1045, 11, 2673, 300, 1355, 300, 264, 9834, 366, 406, 3097, 2370, 1547, 13, 50564, 50564, 407, 498, 527, 2539, 3314, 390, 588, 2295, 11, 718, 311, 360, 300, 5120, 13, 50764, 50764, 961, 311, 5883, 1125, 293, 550, 718, 311, 767, 360, 257, 2539, 3314, 295, 11, 584, 11, 472, 294, 3671, 1045, 510, 13, 51064, 51064, 407, 4018, 935, 4018, 4018, 472, 13, 51164, 51164, 759, 428, 2539, 3314, 307, 636, 886, 2295, 13, 51364, 51364, 639, 7542, 486, 5850, 10658, 309, 13, 51514, 51514, 407, 291, 536, 577, 439, 295, 613, 9205, 366, 636, 886, 1359, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.0651968203553366, "compression_ratio": 1.7375, "no_speech_prob": 3.555925559339812e-06}, {"id": 1251, "seek": 624000, "start": 6240.0, "end": 6250.0, "text": " So the size of the update is basically 10,000 times in magnitude to the size of the numbers in that tensor in the first place.", "tokens": [50364, 407, 264, 2744, 295, 264, 5623, 307, 1936, 1266, 11, 1360, 1413, 294, 15668, 281, 264, 2744, 295, 264, 3547, 294, 300, 40863, 294, 264, 700, 1081, 13, 50864, 50864, 407, 341, 307, 257, 29370, 295, 3097, 636, 886, 2964, 13, 51064, 51064, 407, 341, 307, 1071, 636, 281, 2171, 992, 264, 2539, 3314, 293, 281, 483, 257, 2020, 295, 437, 300, 2539, 3314, 820, 312, 13, 51314, 51314, 400, 6284, 11, 341, 307, 746, 300, 291, 576, 1066, 2837, 295, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06751646551974984, "compression_ratio": 1.7339901477832513, "no_speech_prob": 2.482339823473012e-06}, {"id": 1252, "seek": 624000, "start": 6250.0, "end": 6254.0, "text": " So this is a symptom of training way too slow.", "tokens": [50364, 407, 264, 2744, 295, 264, 5623, 307, 1936, 1266, 11, 1360, 1413, 294, 15668, 281, 264, 2744, 295, 264, 3547, 294, 300, 40863, 294, 264, 700, 1081, 13, 50864, 50864, 407, 341, 307, 257, 29370, 295, 3097, 636, 886, 2964, 13, 51064, 51064, 407, 341, 307, 1071, 636, 281, 2171, 992, 264, 2539, 3314, 293, 281, 483, 257, 2020, 295, 437, 300, 2539, 3314, 820, 312, 13, 51314, 51314, 400, 6284, 11, 341, 307, 746, 300, 291, 576, 1066, 2837, 295, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06751646551974984, "compression_ratio": 1.7339901477832513, "no_speech_prob": 2.482339823473012e-06}, {"id": 1253, "seek": 624000, "start": 6254.0, "end": 6259.0, "text": " So this is another way to sometimes set the learning rate and to get a sense of what that learning rate should be.", "tokens": [50364, 407, 264, 2744, 295, 264, 5623, 307, 1936, 1266, 11, 1360, 1413, 294, 15668, 281, 264, 2744, 295, 264, 3547, 294, 300, 40863, 294, 264, 700, 1081, 13, 50864, 50864, 407, 341, 307, 257, 29370, 295, 3097, 636, 886, 2964, 13, 51064, 51064, 407, 341, 307, 1071, 636, 281, 2171, 992, 264, 2539, 3314, 293, 281, 483, 257, 2020, 295, 437, 300, 2539, 3314, 820, 312, 13, 51314, 51314, 400, 6284, 11, 341, 307, 746, 300, 291, 576, 1066, 2837, 295, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06751646551974984, "compression_ratio": 1.7339901477832513, "no_speech_prob": 2.482339823473012e-06}, {"id": 1254, "seek": 624000, "start": 6259.0, "end": 6265.0, "text": " And ultimately, this is something that you would keep track of.", "tokens": [50364, 407, 264, 2744, 295, 264, 5623, 307, 1936, 1266, 11, 1360, 1413, 294, 15668, 281, 264, 2744, 295, 264, 3547, 294, 300, 40863, 294, 264, 700, 1081, 13, 50864, 50864, 407, 341, 307, 257, 29370, 295, 3097, 636, 886, 2964, 13, 51064, 51064, 407, 341, 307, 1071, 636, 281, 2171, 992, 264, 2539, 3314, 293, 281, 483, 257, 2020, 295, 437, 300, 2539, 3314, 820, 312, 13, 51314, 51314, 400, 6284, 11, 341, 307, 746, 300, 291, 576, 1066, 2837, 295, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06751646551974984, "compression_ratio": 1.7339901477832513, "no_speech_prob": 2.482339823473012e-06}, {"id": 1255, "seek": 626500, "start": 6265.0, "end": 6274.0, "text": " If anything, the learning rate here is a little bit on the higher side because you see that we're above the black line of negative three.", "tokens": [50364, 759, 1340, 11, 264, 2539, 3314, 510, 307, 257, 707, 857, 322, 264, 2946, 1252, 570, 291, 536, 300, 321, 434, 3673, 264, 2211, 1622, 295, 3671, 1045, 13, 50814, 50814, 492, 434, 4079, 926, 3671, 732, 935, 1732, 13, 50914, 50914, 467, 311, 411, 11, 2264, 11, 293, 457, 1203, 307, 411, 8344, 11652, 3319, 13, 51114, 51114, 400, 370, 341, 1542, 411, 257, 1238, 8681, 3287, 295, 295, 2539, 6846, 293, 370, 322, 13, 51314, 51314, 583, 341, 307, 746, 281, 574, 412, 13, 400, 562, 721, 366, 3346, 9895, 897, 5468, 11, 291, 486, 291, 486, 536, 588, 2661, 13, 51514, 51514, 407, 11, 337, 1365, 11, 1203, 1542, 1238, 731, 48249, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07999718189239502, "compression_ratio": 1.7366548042704626, "no_speech_prob": 1.1125105629616883e-05}, {"id": 1256, "seek": 626500, "start": 6274.0, "end": 6276.0, "text": " We're somewhere around negative two point five.", "tokens": [50364, 759, 1340, 11, 264, 2539, 3314, 510, 307, 257, 707, 857, 322, 264, 2946, 1252, 570, 291, 536, 300, 321, 434, 3673, 264, 2211, 1622, 295, 3671, 1045, 13, 50814, 50814, 492, 434, 4079, 926, 3671, 732, 935, 1732, 13, 50914, 50914, 467, 311, 411, 11, 2264, 11, 293, 457, 1203, 307, 411, 8344, 11652, 3319, 13, 51114, 51114, 400, 370, 341, 1542, 411, 257, 1238, 8681, 3287, 295, 295, 2539, 6846, 293, 370, 322, 13, 51314, 51314, 583, 341, 307, 746, 281, 574, 412, 13, 400, 562, 721, 366, 3346, 9895, 897, 5468, 11, 291, 486, 291, 486, 536, 588, 2661, 13, 51514, 51514, 407, 11, 337, 1365, 11, 1203, 1542, 1238, 731, 48249, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07999718189239502, "compression_ratio": 1.7366548042704626, "no_speech_prob": 1.1125105629616883e-05}, {"id": 1257, "seek": 626500, "start": 6276.0, "end": 6280.0, "text": " It's like, OK, and but everything is like somewhat stabilizing.", "tokens": [50364, 759, 1340, 11, 264, 2539, 3314, 510, 307, 257, 707, 857, 322, 264, 2946, 1252, 570, 291, 536, 300, 321, 434, 3673, 264, 2211, 1622, 295, 3671, 1045, 13, 50814, 50814, 492, 434, 4079, 926, 3671, 732, 935, 1732, 13, 50914, 50914, 467, 311, 411, 11, 2264, 11, 293, 457, 1203, 307, 411, 8344, 11652, 3319, 13, 51114, 51114, 400, 370, 341, 1542, 411, 257, 1238, 8681, 3287, 295, 295, 2539, 6846, 293, 370, 322, 13, 51314, 51314, 583, 341, 307, 746, 281, 574, 412, 13, 400, 562, 721, 366, 3346, 9895, 897, 5468, 11, 291, 486, 291, 486, 536, 588, 2661, 13, 51514, 51514, 407, 11, 337, 1365, 11, 1203, 1542, 1238, 731, 48249, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07999718189239502, "compression_ratio": 1.7366548042704626, "no_speech_prob": 1.1125105629616883e-05}, {"id": 1258, "seek": 626500, "start": 6280.0, "end": 6284.0, "text": " And so this looks like a pretty decent setting of of learning rates and so on.", "tokens": [50364, 759, 1340, 11, 264, 2539, 3314, 510, 307, 257, 707, 857, 322, 264, 2946, 1252, 570, 291, 536, 300, 321, 434, 3673, 264, 2211, 1622, 295, 3671, 1045, 13, 50814, 50814, 492, 434, 4079, 926, 3671, 732, 935, 1732, 13, 50914, 50914, 467, 311, 411, 11, 2264, 11, 293, 457, 1203, 307, 411, 8344, 11652, 3319, 13, 51114, 51114, 400, 370, 341, 1542, 411, 257, 1238, 8681, 3287, 295, 295, 2539, 6846, 293, 370, 322, 13, 51314, 51314, 583, 341, 307, 746, 281, 574, 412, 13, 400, 562, 721, 366, 3346, 9895, 897, 5468, 11, 291, 486, 291, 486, 536, 588, 2661, 13, 51514, 51514, 407, 11, 337, 1365, 11, 1203, 1542, 1238, 731, 48249, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07999718189239502, "compression_ratio": 1.7366548042704626, "no_speech_prob": 1.1125105629616883e-05}, {"id": 1259, "seek": 626500, "start": 6284.0, "end": 6288.0, "text": " But this is something to look at. And when things are miscalibrated, you will you will see very quickly.", "tokens": [50364, 759, 1340, 11, 264, 2539, 3314, 510, 307, 257, 707, 857, 322, 264, 2946, 1252, 570, 291, 536, 300, 321, 434, 3673, 264, 2211, 1622, 295, 3671, 1045, 13, 50814, 50814, 492, 434, 4079, 926, 3671, 732, 935, 1732, 13, 50914, 50914, 467, 311, 411, 11, 2264, 11, 293, 457, 1203, 307, 411, 8344, 11652, 3319, 13, 51114, 51114, 400, 370, 341, 1542, 411, 257, 1238, 8681, 3287, 295, 295, 2539, 6846, 293, 370, 322, 13, 51314, 51314, 583, 341, 307, 746, 281, 574, 412, 13, 400, 562, 721, 366, 3346, 9895, 897, 5468, 11, 291, 486, 291, 486, 536, 588, 2661, 13, 51514, 51514, 407, 11, 337, 1365, 11, 1203, 1542, 1238, 731, 48249, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07999718189239502, "compression_ratio": 1.7366548042704626, "no_speech_prob": 1.1125105629616883e-05}, {"id": 1260, "seek": 626500, "start": 6288.0, "end": 6292.0, "text": " So, for example, everything looks pretty well behaved.", "tokens": [50364, 759, 1340, 11, 264, 2539, 3314, 510, 307, 257, 707, 857, 322, 264, 2946, 1252, 570, 291, 536, 300, 321, 434, 3673, 264, 2211, 1622, 295, 3671, 1045, 13, 50814, 50814, 492, 434, 4079, 926, 3671, 732, 935, 1732, 13, 50914, 50914, 467, 311, 411, 11, 2264, 11, 293, 457, 1203, 307, 411, 8344, 11652, 3319, 13, 51114, 51114, 400, 370, 341, 1542, 411, 257, 1238, 8681, 3287, 295, 295, 2539, 6846, 293, 370, 322, 13, 51314, 51314, 583, 341, 307, 746, 281, 574, 412, 13, 400, 562, 721, 366, 3346, 9895, 897, 5468, 11, 291, 486, 291, 486, 536, 588, 2661, 13, 51514, 51514, 407, 11, 337, 1365, 11, 1203, 1542, 1238, 731, 48249, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.07999718189239502, "compression_ratio": 1.7366548042704626, "no_speech_prob": 1.1125105629616883e-05}, {"id": 1261, "seek": 629200, "start": 6292.0, "end": 6296.0, "text": " But just as a comparison, when things are not properly calibrated, what does that look like?", "tokens": [50364, 583, 445, 382, 257, 9660, 11, 562, 721, 366, 406, 6108, 21583, 5468, 11, 437, 775, 300, 574, 411, 30, 50564, 50564, 961, 385, 808, 493, 510, 293, 718, 311, 584, 300, 11, 337, 1365, 11, 437, 360, 321, 360, 30, 50814, 50814, 961, 311, 584, 300, 321, 5298, 281, 3079, 341, 3429, 294, 2710, 2144, 13, 51014, 51014, 407, 264, 17443, 1854, 264, 8213, 7914, 366, 445, 257, 6889, 490, 257, 39148, 294, 439, 264, 10232, 13, 51264, 51264, 708, 2314, 281, 527, 577, 360, 321, 3449, 300, 746, 311, 766, 30, 51464, 51464, 1042, 11, 264, 24433, 7542, 486, 980, 291, 11, 13310, 11, 428, 22027, 366, 636, 886, 25408, 13, 51664, 51664, 440, 2771, 2448, 366, 516, 281, 312, 439, 16507, 493, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.057156878251295826, "compression_ratio": 1.6589403973509933, "no_speech_prob": 4.784836619364796e-06}, {"id": 1262, "seek": 629200, "start": 6296.0, "end": 6301.0, "text": " Let me come up here and let's say that, for example, what do we do?", "tokens": [50364, 583, 445, 382, 257, 9660, 11, 562, 721, 366, 406, 6108, 21583, 5468, 11, 437, 775, 300, 574, 411, 30, 50564, 50564, 961, 385, 808, 493, 510, 293, 718, 311, 584, 300, 11, 337, 1365, 11, 437, 360, 321, 360, 30, 50814, 50814, 961, 311, 584, 300, 321, 5298, 281, 3079, 341, 3429, 294, 2710, 2144, 13, 51014, 51014, 407, 264, 17443, 1854, 264, 8213, 7914, 366, 445, 257, 6889, 490, 257, 39148, 294, 439, 264, 10232, 13, 51264, 51264, 708, 2314, 281, 527, 577, 360, 321, 3449, 300, 746, 311, 766, 30, 51464, 51464, 1042, 11, 264, 24433, 7542, 486, 980, 291, 11, 13310, 11, 428, 22027, 366, 636, 886, 25408, 13, 51664, 51664, 440, 2771, 2448, 366, 516, 281, 312, 439, 16507, 493, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.057156878251295826, "compression_ratio": 1.6589403973509933, "no_speech_prob": 4.784836619364796e-06}, {"id": 1263, "seek": 629200, "start": 6301.0, "end": 6305.0, "text": " Let's say that we forgot to apply this fan in normalization.", "tokens": [50364, 583, 445, 382, 257, 9660, 11, 562, 721, 366, 406, 6108, 21583, 5468, 11, 437, 775, 300, 574, 411, 30, 50564, 50564, 961, 385, 808, 493, 510, 293, 718, 311, 584, 300, 11, 337, 1365, 11, 437, 360, 321, 360, 30, 50814, 50814, 961, 311, 584, 300, 321, 5298, 281, 3079, 341, 3429, 294, 2710, 2144, 13, 51014, 51014, 407, 264, 17443, 1854, 264, 8213, 7914, 366, 445, 257, 6889, 490, 257, 39148, 294, 439, 264, 10232, 13, 51264, 51264, 708, 2314, 281, 527, 577, 360, 321, 3449, 300, 746, 311, 766, 30, 51464, 51464, 1042, 11, 264, 24433, 7542, 486, 980, 291, 11, 13310, 11, 428, 22027, 366, 636, 886, 25408, 13, 51664, 51664, 440, 2771, 2448, 366, 516, 281, 312, 439, 16507, 493, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.057156878251295826, "compression_ratio": 1.6589403973509933, "no_speech_prob": 4.784836619364796e-06}, {"id": 1264, "seek": 629200, "start": 6305.0, "end": 6310.0, "text": " So the weights inside the linear layers are just a sample from a Gaussian in all the stages.", "tokens": [50364, 583, 445, 382, 257, 9660, 11, 562, 721, 366, 406, 6108, 21583, 5468, 11, 437, 775, 300, 574, 411, 30, 50564, 50564, 961, 385, 808, 493, 510, 293, 718, 311, 584, 300, 11, 337, 1365, 11, 437, 360, 321, 360, 30, 50814, 50814, 961, 311, 584, 300, 321, 5298, 281, 3079, 341, 3429, 294, 2710, 2144, 13, 51014, 51014, 407, 264, 17443, 1854, 264, 8213, 7914, 366, 445, 257, 6889, 490, 257, 39148, 294, 439, 264, 10232, 13, 51264, 51264, 708, 2314, 281, 527, 577, 360, 321, 3449, 300, 746, 311, 766, 30, 51464, 51464, 1042, 11, 264, 24433, 7542, 486, 980, 291, 11, 13310, 11, 428, 22027, 366, 636, 886, 25408, 13, 51664, 51664, 440, 2771, 2448, 366, 516, 281, 312, 439, 16507, 493, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.057156878251295826, "compression_ratio": 1.6589403973509933, "no_speech_prob": 4.784836619364796e-06}, {"id": 1265, "seek": 629200, "start": 6310.0, "end": 6314.0, "text": " What happens to our how do we notice that something's off?", "tokens": [50364, 583, 445, 382, 257, 9660, 11, 562, 721, 366, 406, 6108, 21583, 5468, 11, 437, 775, 300, 574, 411, 30, 50564, 50564, 961, 385, 808, 493, 510, 293, 718, 311, 584, 300, 11, 337, 1365, 11, 437, 360, 321, 360, 30, 50814, 50814, 961, 311, 584, 300, 321, 5298, 281, 3079, 341, 3429, 294, 2710, 2144, 13, 51014, 51014, 407, 264, 17443, 1854, 264, 8213, 7914, 366, 445, 257, 6889, 490, 257, 39148, 294, 439, 264, 10232, 13, 51264, 51264, 708, 2314, 281, 527, 577, 360, 321, 3449, 300, 746, 311, 766, 30, 51464, 51464, 1042, 11, 264, 24433, 7542, 486, 980, 291, 11, 13310, 11, 428, 22027, 366, 636, 886, 25408, 13, 51664, 51664, 440, 2771, 2448, 366, 516, 281, 312, 439, 16507, 493, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.057156878251295826, "compression_ratio": 1.6589403973509933, "no_speech_prob": 4.784836619364796e-06}, {"id": 1266, "seek": 629200, "start": 6314.0, "end": 6318.0, "text": " Well, the activation plot will tell you, whoa, your neurons are way too saturated.", "tokens": [50364, 583, 445, 382, 257, 9660, 11, 562, 721, 366, 406, 6108, 21583, 5468, 11, 437, 775, 300, 574, 411, 30, 50564, 50564, 961, 385, 808, 493, 510, 293, 718, 311, 584, 300, 11, 337, 1365, 11, 437, 360, 321, 360, 30, 50814, 50814, 961, 311, 584, 300, 321, 5298, 281, 3079, 341, 3429, 294, 2710, 2144, 13, 51014, 51014, 407, 264, 17443, 1854, 264, 8213, 7914, 366, 445, 257, 6889, 490, 257, 39148, 294, 439, 264, 10232, 13, 51264, 51264, 708, 2314, 281, 527, 577, 360, 321, 3449, 300, 746, 311, 766, 30, 51464, 51464, 1042, 11, 264, 24433, 7542, 486, 980, 291, 11, 13310, 11, 428, 22027, 366, 636, 886, 25408, 13, 51664, 51664, 440, 2771, 2448, 366, 516, 281, 312, 439, 16507, 493, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.057156878251295826, "compression_ratio": 1.6589403973509933, "no_speech_prob": 4.784836619364796e-06}, {"id": 1267, "seek": 629200, "start": 6318.0, "end": 6321.0, "text": " The gradients are going to be all messed up.", "tokens": [50364, 583, 445, 382, 257, 9660, 11, 562, 721, 366, 406, 6108, 21583, 5468, 11, 437, 775, 300, 574, 411, 30, 50564, 50564, 961, 385, 808, 493, 510, 293, 718, 311, 584, 300, 11, 337, 1365, 11, 437, 360, 321, 360, 30, 50814, 50814, 961, 311, 584, 300, 321, 5298, 281, 3079, 341, 3429, 294, 2710, 2144, 13, 51014, 51014, 407, 264, 17443, 1854, 264, 8213, 7914, 366, 445, 257, 6889, 490, 257, 39148, 294, 439, 264, 10232, 13, 51264, 51264, 708, 2314, 281, 527, 577, 360, 321, 3449, 300, 746, 311, 766, 30, 51464, 51464, 1042, 11, 264, 24433, 7542, 486, 980, 291, 11, 13310, 11, 428, 22027, 366, 636, 886, 25408, 13, 51664, 51664, 440, 2771, 2448, 366, 516, 281, 312, 439, 16507, 493, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.057156878251295826, "compression_ratio": 1.6589403973509933, "no_speech_prob": 4.784836619364796e-06}, {"id": 1268, "seek": 632100, "start": 6321.0, "end": 6325.0, "text": " The histogram for these weights are going to be all messed up as well.", "tokens": [50364, 440, 49816, 337, 613, 17443, 366, 516, 281, 312, 439, 16507, 493, 382, 731, 13, 50564, 50564, 400, 456, 311, 257, 688, 295, 37277, 9889, 13, 50664, 50664, 400, 550, 498, 321, 574, 510, 11, 286, 9091, 309, 311, 439, 516, 281, 312, 611, 1238, 16507, 493, 13, 50814, 50814, 407, 291, 536, 11, 456, 311, 257, 688, 295, 2983, 265, 6040, 1344, 294, 577, 2370, 613, 7914, 366, 2539, 13, 51114, 51114, 400, 512, 295, 552, 366, 2539, 636, 886, 2370, 13, 51214, 51214, 407, 8470, 472, 8470, 472, 935, 1732, 13, 51364, 51364, 3950, 366, 588, 2416, 3547, 294, 2115, 295, 341, 8509, 13, 51514, 51514, 3764, 11, 291, 820, 312, 4079, 926, 3671, 1045, 293, 406, 709, 544, 466, 300, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04670015722513199, "compression_ratio": 1.794007490636704, "no_speech_prob": 7.071483196341433e-06}, {"id": 1269, "seek": 632100, "start": 6325.0, "end": 6327.0, "text": " And there's a lot of asymmetry.", "tokens": [50364, 440, 49816, 337, 613, 17443, 366, 516, 281, 312, 439, 16507, 493, 382, 731, 13, 50564, 50564, 400, 456, 311, 257, 688, 295, 37277, 9889, 13, 50664, 50664, 400, 550, 498, 321, 574, 510, 11, 286, 9091, 309, 311, 439, 516, 281, 312, 611, 1238, 16507, 493, 13, 50814, 50814, 407, 291, 536, 11, 456, 311, 257, 688, 295, 2983, 265, 6040, 1344, 294, 577, 2370, 613, 7914, 366, 2539, 13, 51114, 51114, 400, 512, 295, 552, 366, 2539, 636, 886, 2370, 13, 51214, 51214, 407, 8470, 472, 8470, 472, 935, 1732, 13, 51364, 51364, 3950, 366, 588, 2416, 3547, 294, 2115, 295, 341, 8509, 13, 51514, 51514, 3764, 11, 291, 820, 312, 4079, 926, 3671, 1045, 293, 406, 709, 544, 466, 300, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04670015722513199, "compression_ratio": 1.794007490636704, "no_speech_prob": 7.071483196341433e-06}, {"id": 1270, "seek": 632100, "start": 6327.0, "end": 6330.0, "text": " And then if we look here, I suspect it's all going to be also pretty messed up.", "tokens": [50364, 440, 49816, 337, 613, 17443, 366, 516, 281, 312, 439, 16507, 493, 382, 731, 13, 50564, 50564, 400, 456, 311, 257, 688, 295, 37277, 9889, 13, 50664, 50664, 400, 550, 498, 321, 574, 510, 11, 286, 9091, 309, 311, 439, 516, 281, 312, 611, 1238, 16507, 493, 13, 50814, 50814, 407, 291, 536, 11, 456, 311, 257, 688, 295, 2983, 265, 6040, 1344, 294, 577, 2370, 613, 7914, 366, 2539, 13, 51114, 51114, 400, 512, 295, 552, 366, 2539, 636, 886, 2370, 13, 51214, 51214, 407, 8470, 472, 8470, 472, 935, 1732, 13, 51364, 51364, 3950, 366, 588, 2416, 3547, 294, 2115, 295, 341, 8509, 13, 51514, 51514, 3764, 11, 291, 820, 312, 4079, 926, 3671, 1045, 293, 406, 709, 544, 466, 300, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04670015722513199, "compression_ratio": 1.794007490636704, "no_speech_prob": 7.071483196341433e-06}, {"id": 1271, "seek": 632100, "start": 6330.0, "end": 6336.0, "text": " So you see, there's a lot of discrepancy in how fast these layers are learning.", "tokens": [50364, 440, 49816, 337, 613, 17443, 366, 516, 281, 312, 439, 16507, 493, 382, 731, 13, 50564, 50564, 400, 456, 311, 257, 688, 295, 37277, 9889, 13, 50664, 50664, 400, 550, 498, 321, 574, 510, 11, 286, 9091, 309, 311, 439, 516, 281, 312, 611, 1238, 16507, 493, 13, 50814, 50814, 407, 291, 536, 11, 456, 311, 257, 688, 295, 2983, 265, 6040, 1344, 294, 577, 2370, 613, 7914, 366, 2539, 13, 51114, 51114, 400, 512, 295, 552, 366, 2539, 636, 886, 2370, 13, 51214, 51214, 407, 8470, 472, 8470, 472, 935, 1732, 13, 51364, 51364, 3950, 366, 588, 2416, 3547, 294, 2115, 295, 341, 8509, 13, 51514, 51514, 3764, 11, 291, 820, 312, 4079, 926, 3671, 1045, 293, 406, 709, 544, 466, 300, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04670015722513199, "compression_ratio": 1.794007490636704, "no_speech_prob": 7.071483196341433e-06}, {"id": 1272, "seek": 632100, "start": 6336.0, "end": 6338.0, "text": " And some of them are learning way too fast.", "tokens": [50364, 440, 49816, 337, 613, 17443, 366, 516, 281, 312, 439, 16507, 493, 382, 731, 13, 50564, 50564, 400, 456, 311, 257, 688, 295, 37277, 9889, 13, 50664, 50664, 400, 550, 498, 321, 574, 510, 11, 286, 9091, 309, 311, 439, 516, 281, 312, 611, 1238, 16507, 493, 13, 50814, 50814, 407, 291, 536, 11, 456, 311, 257, 688, 295, 2983, 265, 6040, 1344, 294, 577, 2370, 613, 7914, 366, 2539, 13, 51114, 51114, 400, 512, 295, 552, 366, 2539, 636, 886, 2370, 13, 51214, 51214, 407, 8470, 472, 8470, 472, 935, 1732, 13, 51364, 51364, 3950, 366, 588, 2416, 3547, 294, 2115, 295, 341, 8509, 13, 51514, 51514, 3764, 11, 291, 820, 312, 4079, 926, 3671, 1045, 293, 406, 709, 544, 466, 300, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04670015722513199, "compression_ratio": 1.794007490636704, "no_speech_prob": 7.071483196341433e-06}, {"id": 1273, "seek": 632100, "start": 6338.0, "end": 6341.0, "text": " So native one native one point five.", "tokens": [50364, 440, 49816, 337, 613, 17443, 366, 516, 281, 312, 439, 16507, 493, 382, 731, 13, 50564, 50564, 400, 456, 311, 257, 688, 295, 37277, 9889, 13, 50664, 50664, 400, 550, 498, 321, 574, 510, 11, 286, 9091, 309, 311, 439, 516, 281, 312, 611, 1238, 16507, 493, 13, 50814, 50814, 407, 291, 536, 11, 456, 311, 257, 688, 295, 2983, 265, 6040, 1344, 294, 577, 2370, 613, 7914, 366, 2539, 13, 51114, 51114, 400, 512, 295, 552, 366, 2539, 636, 886, 2370, 13, 51214, 51214, 407, 8470, 472, 8470, 472, 935, 1732, 13, 51364, 51364, 3950, 366, 588, 2416, 3547, 294, 2115, 295, 341, 8509, 13, 51514, 51514, 3764, 11, 291, 820, 312, 4079, 926, 3671, 1045, 293, 406, 709, 544, 466, 300, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04670015722513199, "compression_ratio": 1.794007490636704, "no_speech_prob": 7.071483196341433e-06}, {"id": 1274, "seek": 632100, "start": 6341.0, "end": 6344.0, "text": " Those are very large numbers in terms of this ratio.", "tokens": [50364, 440, 49816, 337, 613, 17443, 366, 516, 281, 312, 439, 16507, 493, 382, 731, 13, 50564, 50564, 400, 456, 311, 257, 688, 295, 37277, 9889, 13, 50664, 50664, 400, 550, 498, 321, 574, 510, 11, 286, 9091, 309, 311, 439, 516, 281, 312, 611, 1238, 16507, 493, 13, 50814, 50814, 407, 291, 536, 11, 456, 311, 257, 688, 295, 2983, 265, 6040, 1344, 294, 577, 2370, 613, 7914, 366, 2539, 13, 51114, 51114, 400, 512, 295, 552, 366, 2539, 636, 886, 2370, 13, 51214, 51214, 407, 8470, 472, 8470, 472, 935, 1732, 13, 51364, 51364, 3950, 366, 588, 2416, 3547, 294, 2115, 295, 341, 8509, 13, 51514, 51514, 3764, 11, 291, 820, 312, 4079, 926, 3671, 1045, 293, 406, 709, 544, 466, 300, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04670015722513199, "compression_ratio": 1.794007490636704, "no_speech_prob": 7.071483196341433e-06}, {"id": 1275, "seek": 632100, "start": 6344.0, "end": 6348.0, "text": " Again, you should be somewhere around negative three and not much more about that.", "tokens": [50364, 440, 49816, 337, 613, 17443, 366, 516, 281, 312, 439, 16507, 493, 382, 731, 13, 50564, 50564, 400, 456, 311, 257, 688, 295, 37277, 9889, 13, 50664, 50664, 400, 550, 498, 321, 574, 510, 11, 286, 9091, 309, 311, 439, 516, 281, 312, 611, 1238, 16507, 493, 13, 50814, 50814, 407, 291, 536, 11, 456, 311, 257, 688, 295, 2983, 265, 6040, 1344, 294, 577, 2370, 613, 7914, 366, 2539, 13, 51114, 51114, 400, 512, 295, 552, 366, 2539, 636, 886, 2370, 13, 51214, 51214, 407, 8470, 472, 8470, 472, 935, 1732, 13, 51364, 51364, 3950, 366, 588, 2416, 3547, 294, 2115, 295, 341, 8509, 13, 51514, 51514, 3764, 11, 291, 820, 312, 4079, 926, 3671, 1045, 293, 406, 709, 544, 466, 300, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04670015722513199, "compression_ratio": 1.794007490636704, "no_speech_prob": 7.071483196341433e-06}, {"id": 1276, "seek": 634800, "start": 6348.0, "end": 6353.0, "text": " So this is how miscalibrations of your neural nets are going to manifest.", "tokens": [50364, 407, 341, 307, 577, 3346, 9895, 6414, 763, 295, 428, 18161, 36170, 366, 516, 281, 10067, 13, 50614, 50614, 400, 613, 3685, 295, 28609, 510, 366, 257, 665, 636, 295, 1333, 295, 5062, 729, 3346, 9895, 6414, 763, 1333, 295, 281, 428, 3202, 13, 51114, 51114, 400, 370, 291, 393, 2985, 552, 13, 51164, 51164, 2264, 11, 370, 370, 1400, 321, 600, 1612, 300, 562, 321, 362, 341, 8213, 1266, 39, 11141, 11, 51364, 51364, 321, 393, 767, 13402, 21583, 4404, 264, 16823, 293, 652, 264, 2430, 763, 11, 264, 2771, 2448, 293, 264, 9834, 293, 264, 9205, 439, 574, 1238, 8681, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10050663992623303, "compression_ratio": 1.6929133858267718, "no_speech_prob": 2.4824041702231625e-06}, {"id": 1277, "seek": 634800, "start": 6353.0, "end": 6363.0, "text": " And these kinds of plots here are a good way of sort of bringing those miscalibrations sort of to your attention.", "tokens": [50364, 407, 341, 307, 577, 3346, 9895, 6414, 763, 295, 428, 18161, 36170, 366, 516, 281, 10067, 13, 50614, 50614, 400, 613, 3685, 295, 28609, 510, 366, 257, 665, 636, 295, 1333, 295, 5062, 729, 3346, 9895, 6414, 763, 1333, 295, 281, 428, 3202, 13, 51114, 51114, 400, 370, 291, 393, 2985, 552, 13, 51164, 51164, 2264, 11, 370, 370, 1400, 321, 600, 1612, 300, 562, 321, 362, 341, 8213, 1266, 39, 11141, 11, 51364, 51364, 321, 393, 767, 13402, 21583, 4404, 264, 16823, 293, 652, 264, 2430, 763, 11, 264, 2771, 2448, 293, 264, 9834, 293, 264, 9205, 439, 574, 1238, 8681, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10050663992623303, "compression_ratio": 1.6929133858267718, "no_speech_prob": 2.4824041702231625e-06}, {"id": 1278, "seek": 634800, "start": 6363.0, "end": 6364.0, "text": " And so you can address them.", "tokens": [50364, 407, 341, 307, 577, 3346, 9895, 6414, 763, 295, 428, 18161, 36170, 366, 516, 281, 10067, 13, 50614, 50614, 400, 613, 3685, 295, 28609, 510, 366, 257, 665, 636, 295, 1333, 295, 5062, 729, 3346, 9895, 6414, 763, 1333, 295, 281, 428, 3202, 13, 51114, 51114, 400, 370, 291, 393, 2985, 552, 13, 51164, 51164, 2264, 11, 370, 370, 1400, 321, 600, 1612, 300, 562, 321, 362, 341, 8213, 1266, 39, 11141, 11, 51364, 51364, 321, 393, 767, 13402, 21583, 4404, 264, 16823, 293, 652, 264, 2430, 763, 11, 264, 2771, 2448, 293, 264, 9834, 293, 264, 9205, 439, 574, 1238, 8681, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10050663992623303, "compression_ratio": 1.6929133858267718, "no_speech_prob": 2.4824041702231625e-06}, {"id": 1279, "seek": 634800, "start": 6364.0, "end": 6368.0, "text": " OK, so so far we've seen that when we have this linear 10H sandwich,", "tokens": [50364, 407, 341, 307, 577, 3346, 9895, 6414, 763, 295, 428, 18161, 36170, 366, 516, 281, 10067, 13, 50614, 50614, 400, 613, 3685, 295, 28609, 510, 366, 257, 665, 636, 295, 1333, 295, 5062, 729, 3346, 9895, 6414, 763, 1333, 295, 281, 428, 3202, 13, 51114, 51114, 400, 370, 291, 393, 2985, 552, 13, 51164, 51164, 2264, 11, 370, 370, 1400, 321, 600, 1612, 300, 562, 321, 362, 341, 8213, 1266, 39, 11141, 11, 51364, 51364, 321, 393, 767, 13402, 21583, 4404, 264, 16823, 293, 652, 264, 2430, 763, 11, 264, 2771, 2448, 293, 264, 9834, 293, 264, 9205, 439, 574, 1238, 8681, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10050663992623303, "compression_ratio": 1.6929133858267718, "no_speech_prob": 2.4824041702231625e-06}, {"id": 1280, "seek": 634800, "start": 6368.0, "end": 6376.0, "text": " we can actually precisely calibrate the gains and make the activations, the gradients and the parameters and the updates all look pretty decent.", "tokens": [50364, 407, 341, 307, 577, 3346, 9895, 6414, 763, 295, 428, 18161, 36170, 366, 516, 281, 10067, 13, 50614, 50614, 400, 613, 3685, 295, 28609, 510, 366, 257, 665, 636, 295, 1333, 295, 5062, 729, 3346, 9895, 6414, 763, 1333, 295, 281, 428, 3202, 13, 51114, 51114, 400, 370, 291, 393, 2985, 552, 13, 51164, 51164, 2264, 11, 370, 370, 1400, 321, 600, 1612, 300, 562, 321, 362, 341, 8213, 1266, 39, 11141, 11, 51364, 51364, 321, 393, 767, 13402, 21583, 4404, 264, 16823, 293, 652, 264, 2430, 763, 11, 264, 2771, 2448, 293, 264, 9834, 293, 264, 9205, 439, 574, 1238, 8681, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10050663992623303, "compression_ratio": 1.6929133858267718, "no_speech_prob": 2.4824041702231625e-06}, {"id": 1281, "seek": 637600, "start": 6376.0, "end": 6381.0, "text": " But it definitely feels a little bit like balancing of a pencil on your finger.", "tokens": [50364, 583, 309, 2138, 3417, 257, 707, 857, 411, 22495, 295, 257, 10985, 322, 428, 5984, 13, 50614, 50614, 400, 300, 311, 570, 341, 1216, 575, 281, 312, 588, 13402, 21583, 5468, 13, 50864, 50864, 407, 586, 718, 311, 5366, 15245, 9860, 2144, 7914, 666, 264, 3191, 666, 264, 2890, 13, 51064, 51064, 400, 718, 311, 718, 311, 536, 577, 300, 3665, 3191, 264, 1154, 13, 51264, 51264, 407, 510, 286, 478, 516, 281, 747, 264, 8687, 472, 413, 1508, 293, 286, 478, 516, 281, 722, 17221, 309, 1854, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1409753163655599, "compression_ratio": 1.6026200873362446, "no_speech_prob": 3.2886196095205378e-06}, {"id": 1282, "seek": 637600, "start": 6381.0, "end": 6386.0, "text": " And that's because this game has to be very precisely calibrated.", "tokens": [50364, 583, 309, 2138, 3417, 257, 707, 857, 411, 22495, 295, 257, 10985, 322, 428, 5984, 13, 50614, 50614, 400, 300, 311, 570, 341, 1216, 575, 281, 312, 588, 13402, 21583, 5468, 13, 50864, 50864, 407, 586, 718, 311, 5366, 15245, 9860, 2144, 7914, 666, 264, 3191, 666, 264, 2890, 13, 51064, 51064, 400, 718, 311, 718, 311, 536, 577, 300, 3665, 3191, 264, 1154, 13, 51264, 51264, 407, 510, 286, 478, 516, 281, 747, 264, 8687, 472, 413, 1508, 293, 286, 478, 516, 281, 722, 17221, 309, 1854, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1409753163655599, "compression_ratio": 1.6026200873362446, "no_speech_prob": 3.2886196095205378e-06}, {"id": 1283, "seek": 637600, "start": 6386.0, "end": 6390.0, "text": " So now let's introduce batch formalization layers into the fix into the mix.", "tokens": [50364, 583, 309, 2138, 3417, 257, 707, 857, 411, 22495, 295, 257, 10985, 322, 428, 5984, 13, 50614, 50614, 400, 300, 311, 570, 341, 1216, 575, 281, 312, 588, 13402, 21583, 5468, 13, 50864, 50864, 407, 586, 718, 311, 5366, 15245, 9860, 2144, 7914, 666, 264, 3191, 666, 264, 2890, 13, 51064, 51064, 400, 718, 311, 718, 311, 536, 577, 300, 3665, 3191, 264, 1154, 13, 51264, 51264, 407, 510, 286, 478, 516, 281, 747, 264, 8687, 472, 413, 1508, 293, 286, 478, 516, 281, 722, 17221, 309, 1854, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1409753163655599, "compression_ratio": 1.6026200873362446, "no_speech_prob": 3.2886196095205378e-06}, {"id": 1284, "seek": 637600, "start": 6390.0, "end": 6394.0, "text": " And let's let's see how that helps fix the problem.", "tokens": [50364, 583, 309, 2138, 3417, 257, 707, 857, 411, 22495, 295, 257, 10985, 322, 428, 5984, 13, 50614, 50614, 400, 300, 311, 570, 341, 1216, 575, 281, 312, 588, 13402, 21583, 5468, 13, 50864, 50864, 407, 586, 718, 311, 5366, 15245, 9860, 2144, 7914, 666, 264, 3191, 666, 264, 2890, 13, 51064, 51064, 400, 718, 311, 718, 311, 536, 577, 300, 3665, 3191, 264, 1154, 13, 51264, 51264, 407, 510, 286, 478, 516, 281, 747, 264, 8687, 472, 413, 1508, 293, 286, 478, 516, 281, 722, 17221, 309, 1854, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1409753163655599, "compression_ratio": 1.6026200873362446, "no_speech_prob": 3.2886196095205378e-06}, {"id": 1285, "seek": 637600, "start": 6394.0, "end": 6401.0, "text": " So here I'm going to take the bathroom one D class and I'm going to start placing it inside.", "tokens": [50364, 583, 309, 2138, 3417, 257, 707, 857, 411, 22495, 295, 257, 10985, 322, 428, 5984, 13, 50614, 50614, 400, 300, 311, 570, 341, 1216, 575, 281, 312, 588, 13402, 21583, 5468, 13, 50864, 50864, 407, 586, 718, 311, 5366, 15245, 9860, 2144, 7914, 666, 264, 3191, 666, 264, 2890, 13, 51064, 51064, 400, 718, 311, 718, 311, 536, 577, 300, 3665, 3191, 264, 1154, 13, 51264, 51264, 407, 510, 286, 478, 516, 281, 747, 264, 8687, 472, 413, 1508, 293, 286, 478, 516, 281, 722, 17221, 309, 1854, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1409753163655599, "compression_ratio": 1.6026200873362446, "no_speech_prob": 3.2886196095205378e-06}, {"id": 1286, "seek": 640100, "start": 6401.0, "end": 6407.0, "text": " And as I mentioned before, the standard typical place you would place it is between the linear layer.", "tokens": [50364, 400, 382, 286, 2835, 949, 11, 264, 3832, 7476, 1081, 291, 576, 1081, 309, 307, 1296, 264, 8213, 4583, 13, 50664, 50664, 407, 558, 934, 309, 11, 457, 949, 264, 2107, 12, 1889, 17409, 13, 50764, 50764, 583, 561, 362, 2138, 3737, 365, 300, 13, 50864, 50864, 400, 294, 1186, 11, 291, 393, 483, 588, 2531, 3542, 754, 498, 291, 1081, 309, 934, 264, 2107, 12, 1889, 17409, 13, 51164, 51164, 400, 264, 661, 551, 300, 286, 1415, 281, 2152, 307, 309, 311, 3879, 2489, 281, 611, 1081, 309, 412, 264, 917, 934, 264, 1036, 8213, 4583, 293, 949, 264, 4470, 2445, 13, 51514, 51514, 407, 341, 307, 7263, 2489, 382, 731, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05837838262574285, "compression_ratio": 1.8098859315589353, "no_speech_prob": 1.1124750926683191e-05}, {"id": 1287, "seek": 640100, "start": 6407.0, "end": 6409.0, "text": " So right after it, but before the non-linearity.", "tokens": [50364, 400, 382, 286, 2835, 949, 11, 264, 3832, 7476, 1081, 291, 576, 1081, 309, 307, 1296, 264, 8213, 4583, 13, 50664, 50664, 407, 558, 934, 309, 11, 457, 949, 264, 2107, 12, 1889, 17409, 13, 50764, 50764, 583, 561, 362, 2138, 3737, 365, 300, 13, 50864, 50864, 400, 294, 1186, 11, 291, 393, 483, 588, 2531, 3542, 754, 498, 291, 1081, 309, 934, 264, 2107, 12, 1889, 17409, 13, 51164, 51164, 400, 264, 661, 551, 300, 286, 1415, 281, 2152, 307, 309, 311, 3879, 2489, 281, 611, 1081, 309, 412, 264, 917, 934, 264, 1036, 8213, 4583, 293, 949, 264, 4470, 2445, 13, 51514, 51514, 407, 341, 307, 7263, 2489, 382, 731, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05837838262574285, "compression_ratio": 1.8098859315589353, "no_speech_prob": 1.1124750926683191e-05}, {"id": 1288, "seek": 640100, "start": 6409.0, "end": 6411.0, "text": " But people have definitely played with that.", "tokens": [50364, 400, 382, 286, 2835, 949, 11, 264, 3832, 7476, 1081, 291, 576, 1081, 309, 307, 1296, 264, 8213, 4583, 13, 50664, 50664, 407, 558, 934, 309, 11, 457, 949, 264, 2107, 12, 1889, 17409, 13, 50764, 50764, 583, 561, 362, 2138, 3737, 365, 300, 13, 50864, 50864, 400, 294, 1186, 11, 291, 393, 483, 588, 2531, 3542, 754, 498, 291, 1081, 309, 934, 264, 2107, 12, 1889, 17409, 13, 51164, 51164, 400, 264, 661, 551, 300, 286, 1415, 281, 2152, 307, 309, 311, 3879, 2489, 281, 611, 1081, 309, 412, 264, 917, 934, 264, 1036, 8213, 4583, 293, 949, 264, 4470, 2445, 13, 51514, 51514, 407, 341, 307, 7263, 2489, 382, 731, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05837838262574285, "compression_ratio": 1.8098859315589353, "no_speech_prob": 1.1124750926683191e-05}, {"id": 1289, "seek": 640100, "start": 6411.0, "end": 6417.0, "text": " And in fact, you can get very similar results even if you place it after the non-linearity.", "tokens": [50364, 400, 382, 286, 2835, 949, 11, 264, 3832, 7476, 1081, 291, 576, 1081, 309, 307, 1296, 264, 8213, 4583, 13, 50664, 50664, 407, 558, 934, 309, 11, 457, 949, 264, 2107, 12, 1889, 17409, 13, 50764, 50764, 583, 561, 362, 2138, 3737, 365, 300, 13, 50864, 50864, 400, 294, 1186, 11, 291, 393, 483, 588, 2531, 3542, 754, 498, 291, 1081, 309, 934, 264, 2107, 12, 1889, 17409, 13, 51164, 51164, 400, 264, 661, 551, 300, 286, 1415, 281, 2152, 307, 309, 311, 3879, 2489, 281, 611, 1081, 309, 412, 264, 917, 934, 264, 1036, 8213, 4583, 293, 949, 264, 4470, 2445, 13, 51514, 51514, 407, 341, 307, 7263, 2489, 382, 731, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05837838262574285, "compression_ratio": 1.8098859315589353, "no_speech_prob": 1.1124750926683191e-05}, {"id": 1290, "seek": 640100, "start": 6417.0, "end": 6424.0, "text": " And the other thing that I wanted to mention is it's totally fine to also place it at the end after the last linear layer and before the loss function.", "tokens": [50364, 400, 382, 286, 2835, 949, 11, 264, 3832, 7476, 1081, 291, 576, 1081, 309, 307, 1296, 264, 8213, 4583, 13, 50664, 50664, 407, 558, 934, 309, 11, 457, 949, 264, 2107, 12, 1889, 17409, 13, 50764, 50764, 583, 561, 362, 2138, 3737, 365, 300, 13, 50864, 50864, 400, 294, 1186, 11, 291, 393, 483, 588, 2531, 3542, 754, 498, 291, 1081, 309, 934, 264, 2107, 12, 1889, 17409, 13, 51164, 51164, 400, 264, 661, 551, 300, 286, 1415, 281, 2152, 307, 309, 311, 3879, 2489, 281, 611, 1081, 309, 412, 264, 917, 934, 264, 1036, 8213, 4583, 293, 949, 264, 4470, 2445, 13, 51514, 51514, 407, 341, 307, 7263, 2489, 382, 731, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05837838262574285, "compression_ratio": 1.8098859315589353, "no_speech_prob": 1.1124750926683191e-05}, {"id": 1291, "seek": 640100, "start": 6424.0, "end": 6428.0, "text": " So this is potentially fine as well.", "tokens": [50364, 400, 382, 286, 2835, 949, 11, 264, 3832, 7476, 1081, 291, 576, 1081, 309, 307, 1296, 264, 8213, 4583, 13, 50664, 50664, 407, 558, 934, 309, 11, 457, 949, 264, 2107, 12, 1889, 17409, 13, 50764, 50764, 583, 561, 362, 2138, 3737, 365, 300, 13, 50864, 50864, 400, 294, 1186, 11, 291, 393, 483, 588, 2531, 3542, 754, 498, 291, 1081, 309, 934, 264, 2107, 12, 1889, 17409, 13, 51164, 51164, 400, 264, 661, 551, 300, 286, 1415, 281, 2152, 307, 309, 311, 3879, 2489, 281, 611, 1081, 309, 412, 264, 917, 934, 264, 1036, 8213, 4583, 293, 949, 264, 4470, 2445, 13, 51514, 51514, 407, 341, 307, 7263, 2489, 382, 731, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05837838262574285, "compression_ratio": 1.8098859315589353, "no_speech_prob": 1.1124750926683191e-05}, {"id": 1292, "seek": 642800, "start": 6428.0, "end": 6434.0, "text": " And in this case, this would be up, but it would be woke up size.", "tokens": [50364, 400, 294, 341, 1389, 11, 341, 576, 312, 493, 11, 457, 309, 576, 312, 12852, 493, 2744, 13, 50664, 50664, 823, 11, 570, 264, 1036, 4583, 307, 1151, 1808, 11, 321, 576, 406, 312, 4473, 281, 1699, 281, 652, 264, 2787, 41167, 1570, 6679, 13, 50964, 50964, 492, 1116, 312, 4473, 264, 15546, 570, 15546, 11, 1604, 11, 294, 264, 8687, 307, 264, 7006, 300, 17596, 19020, 43582, 365, 264, 5598, 295, 300, 2710, 2144, 13, 51564, 51564, 407, 321, 393, 5883, 1125, 341, 11141, 586, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20467701587048207, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.0143914298387244e-05}, {"id": 1293, "seek": 642800, "start": 6434.0, "end": 6440.0, "text": " Now, because the last layer is best room, we would not be changing to wait to make the softmax less confident.", "tokens": [50364, 400, 294, 341, 1389, 11, 341, 576, 312, 493, 11, 457, 309, 576, 312, 12852, 493, 2744, 13, 50664, 50664, 823, 11, 570, 264, 1036, 4583, 307, 1151, 1808, 11, 321, 576, 406, 312, 4473, 281, 1699, 281, 652, 264, 2787, 41167, 1570, 6679, 13, 50964, 50964, 492, 1116, 312, 4473, 264, 15546, 570, 15546, 11, 1604, 11, 294, 264, 8687, 307, 264, 7006, 300, 17596, 19020, 43582, 365, 264, 5598, 295, 300, 2710, 2144, 13, 51564, 51564, 407, 321, 393, 5883, 1125, 341, 11141, 586, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20467701587048207, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.0143914298387244e-05}, {"id": 1294, "seek": 642800, "start": 6440.0, "end": 6452.0, "text": " We'd be changing the gamma because gamma, remember, in the bathroom is the variable that multiplicatively interacts with the output of that normalization.", "tokens": [50364, 400, 294, 341, 1389, 11, 341, 576, 312, 493, 11, 457, 309, 576, 312, 12852, 493, 2744, 13, 50664, 50664, 823, 11, 570, 264, 1036, 4583, 307, 1151, 1808, 11, 321, 576, 406, 312, 4473, 281, 1699, 281, 652, 264, 2787, 41167, 1570, 6679, 13, 50964, 50964, 492, 1116, 312, 4473, 264, 15546, 570, 15546, 11, 1604, 11, 294, 264, 8687, 307, 264, 7006, 300, 17596, 19020, 43582, 365, 264, 5598, 295, 300, 2710, 2144, 13, 51564, 51564, 407, 321, 393, 5883, 1125, 341, 11141, 586, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20467701587048207, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.0143914298387244e-05}, {"id": 1295, "seek": 642800, "start": 6452.0, "end": 6455.0, "text": " So we can initialize this sandwich now.", "tokens": [50364, 400, 294, 341, 1389, 11, 341, 576, 312, 493, 11, 457, 309, 576, 312, 12852, 493, 2744, 13, 50664, 50664, 823, 11, 570, 264, 1036, 4583, 307, 1151, 1808, 11, 321, 576, 406, 312, 4473, 281, 1699, 281, 652, 264, 2787, 41167, 1570, 6679, 13, 50964, 50964, 492, 1116, 312, 4473, 264, 15546, 570, 15546, 11, 1604, 11, 294, 264, 8687, 307, 264, 7006, 300, 17596, 19020, 43582, 365, 264, 5598, 295, 300, 2710, 2144, 13, 51564, 51564, 407, 321, 393, 5883, 1125, 341, 11141, 586, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20467701587048207, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.0143914298387244e-05}, {"id": 1296, "seek": 645500, "start": 6455.0, "end": 6461.0, "text": " We can train and we can see that the activations are going to, of course, look very good.", "tokens": [50364, 492, 393, 3847, 293, 321, 393, 536, 300, 264, 2430, 763, 366, 516, 281, 11, 295, 1164, 11, 574, 588, 665, 13, 50664, 50664, 400, 436, 366, 516, 281, 4725, 574, 665, 570, 586, 949, 633, 2167, 1266, 389, 4583, 11, 456, 307, 257, 2710, 2144, 294, 264, 25947, 13, 51064, 51064, 407, 341, 307, 2693, 374, 34408, 439, 1542, 1238, 665, 13, 51264, 51264, 467, 311, 516, 281, 312, 3832, 25163, 295, 9810, 935, 2309, 1732, 732, 3043, 293, 9810, 2681, 3832, 25163, 3710, 264, 2302, 7914, 13, 51564, 51564, 407, 1203, 1542, 588, 42632, 13, 51714, 51714, 440, 2771, 2448, 574, 665, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1274578374460203, "compression_ratio": 1.7352941176470589, "no_speech_prob": 6.143840892036678e-06}, {"id": 1297, "seek": 645500, "start": 6461.0, "end": 6469.0, "text": " And they are going to necessarily look good because now before every single 10 H layer, there is a normalization in the bachelor.", "tokens": [50364, 492, 393, 3847, 293, 321, 393, 536, 300, 264, 2430, 763, 366, 516, 281, 11, 295, 1164, 11, 574, 588, 665, 13, 50664, 50664, 400, 436, 366, 516, 281, 4725, 574, 665, 570, 586, 949, 633, 2167, 1266, 389, 4583, 11, 456, 307, 257, 2710, 2144, 294, 264, 25947, 13, 51064, 51064, 407, 341, 307, 2693, 374, 34408, 439, 1542, 1238, 665, 13, 51264, 51264, 467, 311, 516, 281, 312, 3832, 25163, 295, 9810, 935, 2309, 1732, 732, 3043, 293, 9810, 2681, 3832, 25163, 3710, 264, 2302, 7914, 13, 51564, 51564, 407, 1203, 1542, 588, 42632, 13, 51714, 51714, 440, 2771, 2448, 574, 665, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1274578374460203, "compression_ratio": 1.7352941176470589, "no_speech_prob": 6.143840892036678e-06}, {"id": 1298, "seek": 645500, "start": 6469.0, "end": 6473.0, "text": " So this is unsurprisingly all looks pretty good.", "tokens": [50364, 492, 393, 3847, 293, 321, 393, 536, 300, 264, 2430, 763, 366, 516, 281, 11, 295, 1164, 11, 574, 588, 665, 13, 50664, 50664, 400, 436, 366, 516, 281, 4725, 574, 665, 570, 586, 949, 633, 2167, 1266, 389, 4583, 11, 456, 307, 257, 2710, 2144, 294, 264, 25947, 13, 51064, 51064, 407, 341, 307, 2693, 374, 34408, 439, 1542, 1238, 665, 13, 51264, 51264, 467, 311, 516, 281, 312, 3832, 25163, 295, 9810, 935, 2309, 1732, 732, 3043, 293, 9810, 2681, 3832, 25163, 3710, 264, 2302, 7914, 13, 51564, 51564, 407, 1203, 1542, 588, 42632, 13, 51714, 51714, 440, 2771, 2448, 574, 665, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1274578374460203, "compression_ratio": 1.7352941176470589, "no_speech_prob": 6.143840892036678e-06}, {"id": 1299, "seek": 645500, "start": 6473.0, "end": 6479.0, "text": " It's going to be standard deviation of roughly point six five two percent and roughly equal standard deviation throughout the entire layers.", "tokens": [50364, 492, 393, 3847, 293, 321, 393, 536, 300, 264, 2430, 763, 366, 516, 281, 11, 295, 1164, 11, 574, 588, 665, 13, 50664, 50664, 400, 436, 366, 516, 281, 4725, 574, 665, 570, 586, 949, 633, 2167, 1266, 389, 4583, 11, 456, 307, 257, 2710, 2144, 294, 264, 25947, 13, 51064, 51064, 407, 341, 307, 2693, 374, 34408, 439, 1542, 1238, 665, 13, 51264, 51264, 467, 311, 516, 281, 312, 3832, 25163, 295, 9810, 935, 2309, 1732, 732, 3043, 293, 9810, 2681, 3832, 25163, 3710, 264, 2302, 7914, 13, 51564, 51564, 407, 1203, 1542, 588, 42632, 13, 51714, 51714, 440, 2771, 2448, 574, 665, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1274578374460203, "compression_ratio": 1.7352941176470589, "no_speech_prob": 6.143840892036678e-06}, {"id": 1300, "seek": 645500, "start": 6479.0, "end": 6482.0, "text": " So everything looks very homogeneous.", "tokens": [50364, 492, 393, 3847, 293, 321, 393, 536, 300, 264, 2430, 763, 366, 516, 281, 11, 295, 1164, 11, 574, 588, 665, 13, 50664, 50664, 400, 436, 366, 516, 281, 4725, 574, 665, 570, 586, 949, 633, 2167, 1266, 389, 4583, 11, 456, 307, 257, 2710, 2144, 294, 264, 25947, 13, 51064, 51064, 407, 341, 307, 2693, 374, 34408, 439, 1542, 1238, 665, 13, 51264, 51264, 467, 311, 516, 281, 312, 3832, 25163, 295, 9810, 935, 2309, 1732, 732, 3043, 293, 9810, 2681, 3832, 25163, 3710, 264, 2302, 7914, 13, 51564, 51564, 407, 1203, 1542, 588, 42632, 13, 51714, 51714, 440, 2771, 2448, 574, 665, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1274578374460203, "compression_ratio": 1.7352941176470589, "no_speech_prob": 6.143840892036678e-06}, {"id": 1301, "seek": 645500, "start": 6482.0, "end": 6484.0, "text": " The gradients look good.", "tokens": [50364, 492, 393, 3847, 293, 321, 393, 536, 300, 264, 2430, 763, 366, 516, 281, 11, 295, 1164, 11, 574, 588, 665, 13, 50664, 50664, 400, 436, 366, 516, 281, 4725, 574, 665, 570, 586, 949, 633, 2167, 1266, 389, 4583, 11, 456, 307, 257, 2710, 2144, 294, 264, 25947, 13, 51064, 51064, 407, 341, 307, 2693, 374, 34408, 439, 1542, 1238, 665, 13, 51264, 51264, 467, 311, 516, 281, 312, 3832, 25163, 295, 9810, 935, 2309, 1732, 732, 3043, 293, 9810, 2681, 3832, 25163, 3710, 264, 2302, 7914, 13, 51564, 51564, 407, 1203, 1542, 588, 42632, 13, 51714, 51714, 440, 2771, 2448, 574, 665, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1274578374460203, "compression_ratio": 1.7352941176470589, "no_speech_prob": 6.143840892036678e-06}, {"id": 1302, "seek": 648400, "start": 6484.0, "end": 6493.0, "text": " The weights look good and their distributions and then the updates also look pretty reasonable.", "tokens": [50364, 440, 17443, 574, 665, 293, 641, 37870, 293, 550, 264, 9205, 611, 574, 1238, 10585, 13, 50814, 50814, 492, 434, 516, 3673, 3671, 1045, 257, 707, 857, 11, 457, 406, 538, 886, 709, 13, 51014, 51014, 407, 439, 264, 9834, 366, 3097, 294, 9810, 264, 912, 3314, 510, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.10697981996356316, "compression_ratio": 1.440251572327044, "no_speech_prob": 2.642524123075418e-06}, {"id": 1303, "seek": 648400, "start": 6493.0, "end": 6497.0, "text": " We're going above negative three a little bit, but not by too much.", "tokens": [50364, 440, 17443, 574, 665, 293, 641, 37870, 293, 550, 264, 9205, 611, 574, 1238, 10585, 13, 50814, 50814, 492, 434, 516, 3673, 3671, 1045, 257, 707, 857, 11, 457, 406, 538, 886, 709, 13, 51014, 51014, 407, 439, 264, 9834, 366, 3097, 294, 9810, 264, 912, 3314, 510, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.10697981996356316, "compression_ratio": 1.440251572327044, "no_speech_prob": 2.642524123075418e-06}, {"id": 1304, "seek": 648400, "start": 6497.0, "end": 6504.0, "text": " So all the parameters are training in roughly the same rate here.", "tokens": [50364, 440, 17443, 574, 665, 293, 641, 37870, 293, 550, 264, 9205, 611, 574, 1238, 10585, 13, 50814, 50814, 492, 434, 516, 3673, 3671, 1045, 257, 707, 857, 11, 457, 406, 538, 886, 709, 13, 51014, 51014, 407, 439, 264, 9834, 366, 3097, 294, 9810, 264, 912, 3314, 510, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.10697981996356316, "compression_ratio": 1.440251572327044, "no_speech_prob": 2.642524123075418e-06}, {"id": 1305, "seek": 650400, "start": 6504.0, "end": 6514.0, "text": " But now what we've gained is we are going to be slightly less brittle with respect to the gain of these.", "tokens": [50364, 583, 586, 437, 321, 600, 12634, 307, 321, 366, 516, 281, 312, 4748, 1570, 49325, 365, 3104, 281, 264, 6052, 295, 613, 13, 50864, 50864, 407, 11, 337, 1365, 11, 286, 393, 652, 264, 1216, 312, 584, 935, 732, 510, 11, 597, 390, 709, 14009, 813, 437, 321, 632, 365, 264, 1266, 389, 13, 51314, 51314, 583, 382, 321, 603, 536, 11, 264, 2430, 763, 486, 767, 312, 2293, 2002, 11259, 292, 13, 51514, 51514, 400, 300, 311, 570, 295, 11, 797, 11, 341, 13691, 2710, 2144, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12650869203650433, "compression_ratio": 1.519650655021834, "no_speech_prob": 6.240537004487123e-06}, {"id": 1306, "seek": 650400, "start": 6514.0, "end": 6523.0, "text": " So, for example, I can make the game be say point two here, which was much slower than what we had with the 10 H.", "tokens": [50364, 583, 586, 437, 321, 600, 12634, 307, 321, 366, 516, 281, 312, 4748, 1570, 49325, 365, 3104, 281, 264, 6052, 295, 613, 13, 50864, 50864, 407, 11, 337, 1365, 11, 286, 393, 652, 264, 1216, 312, 584, 935, 732, 510, 11, 597, 390, 709, 14009, 813, 437, 321, 632, 365, 264, 1266, 389, 13, 51314, 51314, 583, 382, 321, 603, 536, 11, 264, 2430, 763, 486, 767, 312, 2293, 2002, 11259, 292, 13, 51514, 51514, 400, 300, 311, 570, 295, 11, 797, 11, 341, 13691, 2710, 2144, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12650869203650433, "compression_ratio": 1.519650655021834, "no_speech_prob": 6.240537004487123e-06}, {"id": 1307, "seek": 650400, "start": 6523.0, "end": 6527.0, "text": " But as we'll see, the activations will actually be exactly unaffected.", "tokens": [50364, 583, 586, 437, 321, 600, 12634, 307, 321, 366, 516, 281, 312, 4748, 1570, 49325, 365, 3104, 281, 264, 6052, 295, 613, 13, 50864, 50864, 407, 11, 337, 1365, 11, 286, 393, 652, 264, 1216, 312, 584, 935, 732, 510, 11, 597, 390, 709, 14009, 813, 437, 321, 632, 365, 264, 1266, 389, 13, 51314, 51314, 583, 382, 321, 603, 536, 11, 264, 2430, 763, 486, 767, 312, 2293, 2002, 11259, 292, 13, 51514, 51514, 400, 300, 311, 570, 295, 11, 797, 11, 341, 13691, 2710, 2144, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12650869203650433, "compression_ratio": 1.519650655021834, "no_speech_prob": 6.240537004487123e-06}, {"id": 1308, "seek": 650400, "start": 6527.0, "end": 6530.0, "text": " And that's because of, again, this explicit normalization.", "tokens": [50364, 583, 586, 437, 321, 600, 12634, 307, 321, 366, 516, 281, 312, 4748, 1570, 49325, 365, 3104, 281, 264, 6052, 295, 613, 13, 50864, 50864, 407, 11, 337, 1365, 11, 286, 393, 652, 264, 1216, 312, 584, 935, 732, 510, 11, 597, 390, 709, 14009, 813, 437, 321, 632, 365, 264, 1266, 389, 13, 51314, 51314, 583, 382, 321, 603, 536, 11, 264, 2430, 763, 486, 767, 312, 2293, 2002, 11259, 292, 13, 51514, 51514, 400, 300, 311, 570, 295, 11, 797, 11, 341, 13691, 2710, 2144, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12650869203650433, "compression_ratio": 1.519650655021834, "no_speech_prob": 6.240537004487123e-06}, {"id": 1309, "seek": 653000, "start": 6530.0, "end": 6537.0, "text": " The gradients are going to look OK, the weight gradients are going to look OK, but actually the updates will change.", "tokens": [50364, 440, 2771, 2448, 366, 516, 281, 574, 2264, 11, 264, 3364, 2771, 2448, 366, 516, 281, 574, 2264, 11, 457, 767, 264, 9205, 486, 1319, 13, 50714, 50714, 400, 370, 754, 1673, 264, 2128, 293, 23897, 1320, 281, 257, 588, 2416, 8396, 574, 2264, 570, 295, 264, 23897, 1320, 295, 264, 25947, 293, 577, 264, 4373, 295, 264, 22341, 2430, 763, 43582, 294, 264, 25947, 293, 1080, 23897, 1320, 11, 51364, 51364, 341, 307, 767, 4473, 264, 4373, 295, 264, 9205, 322, 613, 9834, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13919340626577312, "compression_ratio": 1.9615384615384615, "no_speech_prob": 1.1124931916128844e-05}, {"id": 1310, "seek": 653000, "start": 6537.0, "end": 6550.0, "text": " And so even though the forward and backward pass to a very large extent look OK because of the backward pass of the bachelor and how the scale of the incoming activations interacts in the bachelor and its backward pass,", "tokens": [50364, 440, 2771, 2448, 366, 516, 281, 574, 2264, 11, 264, 3364, 2771, 2448, 366, 516, 281, 574, 2264, 11, 457, 767, 264, 9205, 486, 1319, 13, 50714, 50714, 400, 370, 754, 1673, 264, 2128, 293, 23897, 1320, 281, 257, 588, 2416, 8396, 574, 2264, 570, 295, 264, 23897, 1320, 295, 264, 25947, 293, 577, 264, 4373, 295, 264, 22341, 2430, 763, 43582, 294, 264, 25947, 293, 1080, 23897, 1320, 11, 51364, 51364, 341, 307, 767, 4473, 264, 4373, 295, 264, 9205, 322, 613, 9834, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13919340626577312, "compression_ratio": 1.9615384615384615, "no_speech_prob": 1.1124931916128844e-05}, {"id": 1311, "seek": 653000, "start": 6550.0, "end": 6556.0, "text": " this is actually changing the scale of the updates on these parameters.", "tokens": [50364, 440, 2771, 2448, 366, 516, 281, 574, 2264, 11, 264, 3364, 2771, 2448, 366, 516, 281, 574, 2264, 11, 457, 767, 264, 9205, 486, 1319, 13, 50714, 50714, 400, 370, 754, 1673, 264, 2128, 293, 23897, 1320, 281, 257, 588, 2416, 8396, 574, 2264, 570, 295, 264, 23897, 1320, 295, 264, 25947, 293, 577, 264, 4373, 295, 264, 22341, 2430, 763, 43582, 294, 264, 25947, 293, 1080, 23897, 1320, 11, 51364, 51364, 341, 307, 767, 4473, 264, 4373, 295, 264, 9205, 322, 613, 9834, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13919340626577312, "compression_ratio": 1.9615384615384615, "no_speech_prob": 1.1124931916128844e-05}, {"id": 1312, "seek": 655600, "start": 6556.0, "end": 6565.0, "text": " So the gradients of these weights are affected. So we still don't get a completely free pass to pass an arbitrary weights here.", "tokens": [50364, 407, 264, 2771, 2448, 295, 613, 17443, 366, 8028, 13, 407, 321, 920, 500, 380, 483, 257, 2584, 1737, 1320, 281, 1320, 364, 23211, 17443, 510, 13, 50814, 50814, 583, 1203, 1646, 307, 10591, 544, 13956, 294, 2115, 295, 264, 2128, 11, 23897, 293, 264, 3364, 2771, 2448, 13, 51214, 51214, 467, 311, 445, 300, 291, 815, 362, 281, 1533, 2613, 428, 2539, 3314, 498, 291, 366, 4473, 31868, 264, 4373, 295, 264, 2430, 763, 300, 366, 1348, 666, 264, 272, 8188, 830, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0711272196336226, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.356596946308855e-06}, {"id": 1313, "seek": 655600, "start": 6565.0, "end": 6573.0, "text": " But everything else is significantly more robust in terms of the forward, backward and the weight gradients.", "tokens": [50364, 407, 264, 2771, 2448, 295, 613, 17443, 366, 8028, 13, 407, 321, 920, 500, 380, 483, 257, 2584, 1737, 1320, 281, 1320, 364, 23211, 17443, 510, 13, 50814, 50814, 583, 1203, 1646, 307, 10591, 544, 13956, 294, 2115, 295, 264, 2128, 11, 23897, 293, 264, 3364, 2771, 2448, 13, 51214, 51214, 467, 311, 445, 300, 291, 815, 362, 281, 1533, 2613, 428, 2539, 3314, 498, 291, 366, 4473, 31868, 264, 4373, 295, 264, 2430, 763, 300, 366, 1348, 666, 264, 272, 8188, 830, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0711272196336226, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.356596946308855e-06}, {"id": 1314, "seek": 655600, "start": 6573.0, "end": 6581.0, "text": " It's just that you may have to retune your learning rate if you are changing sufficiently the scale of the activations that are coming into the bachelors.", "tokens": [50364, 407, 264, 2771, 2448, 295, 613, 17443, 366, 8028, 13, 407, 321, 920, 500, 380, 483, 257, 2584, 1737, 1320, 281, 1320, 364, 23211, 17443, 510, 13, 50814, 50814, 583, 1203, 1646, 307, 10591, 544, 13956, 294, 2115, 295, 264, 2128, 11, 23897, 293, 264, 3364, 2771, 2448, 13, 51214, 51214, 467, 311, 445, 300, 291, 815, 362, 281, 1533, 2613, 428, 2539, 3314, 498, 291, 366, 4473, 31868, 264, 4373, 295, 264, 2430, 763, 300, 366, 1348, 666, 264, 272, 8188, 830, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.0711272196336226, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.356596946308855e-06}, {"id": 1315, "seek": 658100, "start": 6581.0, "end": 6591.0, "text": " So here, for example, we changed the gains of these linear layers to be greater and we're seeing that the updates are coming out lower as a result.", "tokens": [50364, 407, 510, 11, 337, 1365, 11, 321, 3105, 264, 16823, 295, 613, 8213, 7914, 281, 312, 5044, 293, 321, 434, 2577, 300, 264, 9205, 366, 1348, 484, 3126, 382, 257, 1874, 13, 50864, 50864, 400, 550, 2721, 11, 321, 393, 611, 498, 321, 366, 1228, 272, 8188, 830, 11, 321, 500, 380, 767, 643, 281, 4725, 718, 385, 14322, 341, 281, 472, 13, 51164, 51164, 407, 456, 311, 572, 6052, 13, 492, 500, 380, 4725, 754, 362, 281, 2710, 1125, 538, 3429, 294, 2171, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10555417350168979, "compression_ratio": 1.6192660550458715, "no_speech_prob": 8.529485057806596e-06}, {"id": 1316, "seek": 658100, "start": 6591.0, "end": 6597.0, "text": " And then finally, we can also if we are using bachelors, we don't actually need to necessarily let me reset this to one.", "tokens": [50364, 407, 510, 11, 337, 1365, 11, 321, 3105, 264, 16823, 295, 613, 8213, 7914, 281, 312, 5044, 293, 321, 434, 2577, 300, 264, 9205, 366, 1348, 484, 3126, 382, 257, 1874, 13, 50864, 50864, 400, 550, 2721, 11, 321, 393, 611, 498, 321, 366, 1228, 272, 8188, 830, 11, 321, 500, 380, 767, 643, 281, 4725, 718, 385, 14322, 341, 281, 472, 13, 51164, 51164, 407, 456, 311, 572, 6052, 13, 492, 500, 380, 4725, 754, 362, 281, 2710, 1125, 538, 3429, 294, 2171, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10555417350168979, "compression_ratio": 1.6192660550458715, "no_speech_prob": 8.529485057806596e-06}, {"id": 1317, "seek": 658100, "start": 6597.0, "end": 6603.0, "text": " So there's no gain. We don't necessarily even have to normalize by fan in sometimes.", "tokens": [50364, 407, 510, 11, 337, 1365, 11, 321, 3105, 264, 16823, 295, 613, 8213, 7914, 281, 312, 5044, 293, 321, 434, 2577, 300, 264, 9205, 366, 1348, 484, 3126, 382, 257, 1874, 13, 50864, 50864, 400, 550, 2721, 11, 321, 393, 611, 498, 321, 366, 1228, 272, 8188, 830, 11, 321, 500, 380, 767, 643, 281, 4725, 718, 385, 14322, 341, 281, 472, 13, 51164, 51164, 407, 456, 311, 572, 6052, 13, 492, 500, 380, 4725, 754, 362, 281, 2710, 1125, 538, 3429, 294, 2171, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10555417350168979, "compression_ratio": 1.6192660550458715, "no_speech_prob": 8.529485057806596e-06}, {"id": 1318, "seek": 660300, "start": 6603.0, "end": 6611.0, "text": " So if I take out the fan in, so these are just now random Gaussian, we'll see that because of bachelor, this will actually be relatively well behaved.", "tokens": [50364, 407, 498, 286, 747, 484, 264, 3429, 294, 11, 370, 613, 366, 445, 586, 4974, 39148, 11, 321, 603, 536, 300, 570, 295, 25947, 11, 341, 486, 767, 312, 7226, 731, 48249, 13, 50764, 50764, 407, 341, 307, 574, 11, 295, 1164, 11, 294, 264, 2128, 1320, 574, 665, 13, 51064, 51064, 440, 2771, 2448, 574, 665, 13, 440, 23897, 3364, 9205, 574, 2264, 13, 51364, 51364, 316, 707, 857, 295, 4046, 28537, 322, 512, 295, 264, 7914, 13, 400, 341, 1542, 2264, 382, 731, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.18749092949761284, "compression_ratio": 1.563063063063063, "no_speech_prob": 1.1299827747279778e-05}, {"id": 1319, "seek": 660300, "start": 6611.0, "end": 6617.0, "text": " So this is look, of course, in the forward pass look good.", "tokens": [50364, 407, 498, 286, 747, 484, 264, 3429, 294, 11, 370, 613, 366, 445, 586, 4974, 39148, 11, 321, 603, 536, 300, 570, 295, 25947, 11, 341, 486, 767, 312, 7226, 731, 48249, 13, 50764, 50764, 407, 341, 307, 574, 11, 295, 1164, 11, 294, 264, 2128, 1320, 574, 665, 13, 51064, 51064, 440, 2771, 2448, 574, 665, 13, 440, 23897, 3364, 9205, 574, 2264, 13, 51364, 51364, 316, 707, 857, 295, 4046, 28537, 322, 512, 295, 264, 7914, 13, 400, 341, 1542, 2264, 382, 731, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.18749092949761284, "compression_ratio": 1.563063063063063, "no_speech_prob": 1.1299827747279778e-05}, {"id": 1320, "seek": 660300, "start": 6617.0, "end": 6623.0, "text": " The gradients look good. The backward weight updates look OK.", "tokens": [50364, 407, 498, 286, 747, 484, 264, 3429, 294, 11, 370, 613, 366, 445, 586, 4974, 39148, 11, 321, 603, 536, 300, 570, 295, 25947, 11, 341, 486, 767, 312, 7226, 731, 48249, 13, 50764, 50764, 407, 341, 307, 574, 11, 295, 1164, 11, 294, 264, 2128, 1320, 574, 665, 13, 51064, 51064, 440, 2771, 2448, 574, 665, 13, 440, 23897, 3364, 9205, 574, 2264, 13, 51364, 51364, 316, 707, 857, 295, 4046, 28537, 322, 512, 295, 264, 7914, 13, 400, 341, 1542, 2264, 382, 731, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.18749092949761284, "compression_ratio": 1.563063063063063, "no_speech_prob": 1.1299827747279778e-05}, {"id": 1321, "seek": 660300, "start": 6623.0, "end": 6629.0, "text": " A little bit of fat tails on some of the layers. And this looks OK as well.", "tokens": [50364, 407, 498, 286, 747, 484, 264, 3429, 294, 11, 370, 613, 366, 445, 586, 4974, 39148, 11, 321, 603, 536, 300, 570, 295, 25947, 11, 341, 486, 767, 312, 7226, 731, 48249, 13, 50764, 50764, 407, 341, 307, 574, 11, 295, 1164, 11, 294, 264, 2128, 1320, 574, 665, 13, 51064, 51064, 440, 2771, 2448, 574, 665, 13, 440, 23897, 3364, 9205, 574, 2264, 13, 51364, 51364, 316, 707, 857, 295, 4046, 28537, 322, 512, 295, 264, 7914, 13, 400, 341, 1542, 2264, 382, 731, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.18749092949761284, "compression_ratio": 1.563063063063063, "no_speech_prob": 1.1299827747279778e-05}, {"id": 1322, "seek": 662900, "start": 6629.0, "end": 6633.0, "text": " But as you as you can see, we're significantly below negative three.", "tokens": [50364, 583, 382, 291, 382, 291, 393, 536, 11, 321, 434, 10591, 2507, 3671, 1045, 13, 50564, 50564, 407, 321, 1116, 362, 281, 9961, 493, 264, 2539, 3314, 295, 341, 25947, 370, 300, 321, 366, 3097, 544, 6108, 13, 50864, 50864, 400, 294, 1729, 11, 1237, 412, 341, 9810, 1542, 411, 321, 362, 281, 1266, 87, 264, 2539, 3314, 281, 483, 281, 466, 472, 3671, 1045, 13, 51214, 51214, 407, 321, 808, 510, 293, 321, 576, 1319, 341, 281, 312, 5623, 295, 472, 935, 4018, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10396963826725993, "compression_ratio": 1.6759259259259258, "no_speech_prob": 4.637582151190145e-06}, {"id": 1323, "seek": 662900, "start": 6633.0, "end": 6639.0, "text": " So we'd have to bump up the learning rate of this bachelor so that we are training more properly.", "tokens": [50364, 583, 382, 291, 382, 291, 393, 536, 11, 321, 434, 10591, 2507, 3671, 1045, 13, 50564, 50564, 407, 321, 1116, 362, 281, 9961, 493, 264, 2539, 3314, 295, 341, 25947, 370, 300, 321, 366, 3097, 544, 6108, 13, 50864, 50864, 400, 294, 1729, 11, 1237, 412, 341, 9810, 1542, 411, 321, 362, 281, 1266, 87, 264, 2539, 3314, 281, 483, 281, 466, 472, 3671, 1045, 13, 51214, 51214, 407, 321, 808, 510, 293, 321, 576, 1319, 341, 281, 312, 5623, 295, 472, 935, 4018, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10396963826725993, "compression_ratio": 1.6759259259259258, "no_speech_prob": 4.637582151190145e-06}, {"id": 1324, "seek": 662900, "start": 6639.0, "end": 6646.0, "text": " And in particular, looking at this roughly looks like we have to 10x the learning rate to get to about one negative three.", "tokens": [50364, 583, 382, 291, 382, 291, 393, 536, 11, 321, 434, 10591, 2507, 3671, 1045, 13, 50564, 50564, 407, 321, 1116, 362, 281, 9961, 493, 264, 2539, 3314, 295, 341, 25947, 370, 300, 321, 366, 3097, 544, 6108, 13, 50864, 50864, 400, 294, 1729, 11, 1237, 412, 341, 9810, 1542, 411, 321, 362, 281, 1266, 87, 264, 2539, 3314, 281, 483, 281, 466, 472, 3671, 1045, 13, 51214, 51214, 407, 321, 808, 510, 293, 321, 576, 1319, 341, 281, 312, 5623, 295, 472, 935, 4018, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10396963826725993, "compression_ratio": 1.6759259259259258, "no_speech_prob": 4.637582151190145e-06}, {"id": 1325, "seek": 662900, "start": 6646.0, "end": 6651.0, "text": " So we come here and we would change this to be update of one point zero.", "tokens": [50364, 583, 382, 291, 382, 291, 393, 536, 11, 321, 434, 10591, 2507, 3671, 1045, 13, 50564, 50564, 407, 321, 1116, 362, 281, 9961, 493, 264, 2539, 3314, 295, 341, 25947, 370, 300, 321, 366, 3097, 544, 6108, 13, 50864, 50864, 400, 294, 1729, 11, 1237, 412, 341, 9810, 1542, 411, 321, 362, 281, 1266, 87, 264, 2539, 3314, 281, 483, 281, 466, 472, 3671, 1045, 13, 51214, 51214, 407, 321, 808, 510, 293, 321, 576, 1319, 341, 281, 312, 5623, 295, 472, 935, 4018, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10396963826725993, "compression_ratio": 1.6759259259259258, "no_speech_prob": 4.637582151190145e-06}, {"id": 1326, "seek": 665100, "start": 6651.0, "end": 6659.0, "text": " And if I initialize.", "tokens": [50364, 400, 498, 286, 5883, 1125, 13, 50764, 50764, 1396, 321, 603, 536, 300, 1203, 920, 11, 295, 1164, 11, 1542, 665, 13, 50914, 50914, 400, 586, 321, 366, 9810, 510, 293, 321, 2066, 341, 281, 312, 364, 2264, 3097, 1190, 13, 51164, 51164, 407, 938, 1657, 2099, 11, 321, 366, 10591, 544, 13956, 281, 264, 6052, 295, 613, 8213, 7914, 11, 1968, 420, 406, 321, 362, 281, 3079, 264, 3429, 294, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07957952273519416, "compression_ratio": 1.4545454545454546, "no_speech_prob": 6.240264610823942e-06}, {"id": 1327, "seek": 665100, "start": 6659.0, "end": 6662.0, "text": " Then we'll see that everything still, of course, looks good.", "tokens": [50364, 400, 498, 286, 5883, 1125, 13, 50764, 50764, 1396, 321, 603, 536, 300, 1203, 920, 11, 295, 1164, 11, 1542, 665, 13, 50914, 50914, 400, 586, 321, 366, 9810, 510, 293, 321, 2066, 341, 281, 312, 364, 2264, 3097, 1190, 13, 51164, 51164, 407, 938, 1657, 2099, 11, 321, 366, 10591, 544, 13956, 281, 264, 6052, 295, 613, 8213, 7914, 11, 1968, 420, 406, 321, 362, 281, 3079, 264, 3429, 294, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07957952273519416, "compression_ratio": 1.4545454545454546, "no_speech_prob": 6.240264610823942e-06}, {"id": 1328, "seek": 665100, "start": 6662.0, "end": 6667.0, "text": " And now we are roughly here and we expect this to be an OK training run.", "tokens": [50364, 400, 498, 286, 5883, 1125, 13, 50764, 50764, 1396, 321, 603, 536, 300, 1203, 920, 11, 295, 1164, 11, 1542, 665, 13, 50914, 50914, 400, 586, 321, 366, 9810, 510, 293, 321, 2066, 341, 281, 312, 364, 2264, 3097, 1190, 13, 51164, 51164, 407, 938, 1657, 2099, 11, 321, 366, 10591, 544, 13956, 281, 264, 6052, 295, 613, 8213, 7914, 11, 1968, 420, 406, 321, 362, 281, 3079, 264, 3429, 294, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07957952273519416, "compression_ratio": 1.4545454545454546, "no_speech_prob": 6.240264610823942e-06}, {"id": 1329, "seek": 665100, "start": 6667.0, "end": 6674.0, "text": " So long story short, we are significantly more robust to the gain of these linear layers, whether or not we have to apply the fan in.", "tokens": [50364, 400, 498, 286, 5883, 1125, 13, 50764, 50764, 1396, 321, 603, 536, 300, 1203, 920, 11, 295, 1164, 11, 1542, 665, 13, 50914, 50914, 400, 586, 321, 366, 9810, 510, 293, 321, 2066, 341, 281, 312, 364, 2264, 3097, 1190, 13, 51164, 51164, 407, 938, 1657, 2099, 11, 321, 366, 10591, 544, 13956, 281, 264, 6052, 295, 613, 8213, 7914, 11, 1968, 420, 406, 321, 362, 281, 3079, 264, 3429, 294, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.07957952273519416, "compression_ratio": 1.4545454545454546, "no_speech_prob": 6.240264610823942e-06}, {"id": 1330, "seek": 667400, "start": 6674.0, "end": 6684.0, "text": " And then we can change the game, but we actually do have to worry a little bit about the update scales and making sure that the learning rate is properly calibrated here.", "tokens": [50364, 400, 550, 321, 393, 1319, 264, 1216, 11, 457, 321, 767, 360, 362, 281, 3292, 257, 707, 857, 466, 264, 5623, 17408, 293, 1455, 988, 300, 264, 2539, 3314, 307, 6108, 21583, 5468, 510, 13, 50864, 50864, 583, 264, 2430, 763, 295, 264, 2128, 23897, 1320, 293, 264, 9205, 366, 439, 366, 1237, 10591, 544, 731, 48249, 11, 3993, 337, 264, 4338, 4373, 300, 307, 7263, 885, 19871, 510, 13, 51364, 51364, 2264, 11, 370, 586, 718, 385, 20858, 13, 821, 366, 1045, 721, 286, 390, 7159, 281, 4584, 365, 341, 3541, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06453701392891481, "compression_ratio": 1.674074074074074, "no_speech_prob": 4.565485141938552e-06}, {"id": 1331, "seek": 667400, "start": 6684.0, "end": 6694.0, "text": " But the activations of the forward backward pass and the updates are all are looking significantly more well behaved, except for the global scale that is potentially being adjusted here.", "tokens": [50364, 400, 550, 321, 393, 1319, 264, 1216, 11, 457, 321, 767, 360, 362, 281, 3292, 257, 707, 857, 466, 264, 5623, 17408, 293, 1455, 988, 300, 264, 2539, 3314, 307, 6108, 21583, 5468, 510, 13, 50864, 50864, 583, 264, 2430, 763, 295, 264, 2128, 23897, 1320, 293, 264, 9205, 366, 439, 366, 1237, 10591, 544, 731, 48249, 11, 3993, 337, 264, 4338, 4373, 300, 307, 7263, 885, 19871, 510, 13, 51364, 51364, 2264, 11, 370, 586, 718, 385, 20858, 13, 821, 366, 1045, 721, 286, 390, 7159, 281, 4584, 365, 341, 3541, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06453701392891481, "compression_ratio": 1.674074074074074, "no_speech_prob": 4.565485141938552e-06}, {"id": 1332, "seek": 667400, "start": 6694.0, "end": 6699.0, "text": " OK, so now let me summarize. There are three things I was hoping to achieve with this section.", "tokens": [50364, 400, 550, 321, 393, 1319, 264, 1216, 11, 457, 321, 767, 360, 362, 281, 3292, 257, 707, 857, 466, 264, 5623, 17408, 293, 1455, 988, 300, 264, 2539, 3314, 307, 6108, 21583, 5468, 510, 13, 50864, 50864, 583, 264, 2430, 763, 295, 264, 2128, 23897, 1320, 293, 264, 9205, 366, 439, 366, 1237, 10591, 544, 731, 48249, 11, 3993, 337, 264, 4338, 4373, 300, 307, 7263, 885, 19871, 510, 13, 51364, 51364, 2264, 11, 370, 586, 718, 385, 20858, 13, 821, 366, 1045, 721, 286, 390, 7159, 281, 4584, 365, 341, 3541, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.06453701392891481, "compression_ratio": 1.674074074074074, "no_speech_prob": 4.565485141938552e-06}, {"id": 1333, "seek": 669900, "start": 6699.0, "end": 6709.0, "text": " Number one, I wanted to introduce you to bash normalization, which is one of the first modern innovations that we're looking into that helped stabilize very deep neural networks and their training.", "tokens": [50364, 5118, 472, 11, 286, 1415, 281, 5366, 291, 281, 46183, 2710, 2144, 11, 597, 307, 472, 295, 264, 700, 4363, 24283, 300, 321, 434, 1237, 666, 300, 4254, 31870, 588, 2452, 18161, 9590, 293, 641, 3097, 13, 50864, 50864, 400, 286, 1454, 291, 1223, 577, 264, 46183, 2710, 2144, 1985, 293, 577, 309, 576, 312, 1143, 294, 257, 18161, 3209, 13, 51164, 51164, 5118, 732, 11, 286, 390, 7159, 281, 25878, 284, 339, 2505, 512, 295, 527, 3089, 293, 7019, 309, 493, 666, 613, 16679, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10048991309271918, "compression_ratio": 1.6794871794871795, "no_speech_prob": 1.568350307934452e-05}, {"id": 1334, "seek": 669900, "start": 6709.0, "end": 6715.0, "text": " And I hope you understand how the bash normalization works and how it would be used in a neural network.", "tokens": [50364, 5118, 472, 11, 286, 1415, 281, 5366, 291, 281, 46183, 2710, 2144, 11, 597, 307, 472, 295, 264, 700, 4363, 24283, 300, 321, 434, 1237, 666, 300, 4254, 31870, 588, 2452, 18161, 9590, 293, 641, 3097, 13, 50864, 50864, 400, 286, 1454, 291, 1223, 577, 264, 46183, 2710, 2144, 1985, 293, 577, 309, 576, 312, 1143, 294, 257, 18161, 3209, 13, 51164, 51164, 5118, 732, 11, 286, 390, 7159, 281, 25878, 284, 339, 2505, 512, 295, 527, 3089, 293, 7019, 309, 493, 666, 613, 16679, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10048991309271918, "compression_ratio": 1.6794871794871795, "no_speech_prob": 1.568350307934452e-05}, {"id": 1335, "seek": 669900, "start": 6715.0, "end": 6721.0, "text": " Number two, I was hoping to pytorchify some of our code and wrap it up into these modules.", "tokens": [50364, 5118, 472, 11, 286, 1415, 281, 5366, 291, 281, 46183, 2710, 2144, 11, 597, 307, 472, 295, 264, 700, 4363, 24283, 300, 321, 434, 1237, 666, 300, 4254, 31870, 588, 2452, 18161, 9590, 293, 641, 3097, 13, 50864, 50864, 400, 286, 1454, 291, 1223, 577, 264, 46183, 2710, 2144, 1985, 293, 577, 309, 576, 312, 1143, 294, 257, 18161, 3209, 13, 51164, 51164, 5118, 732, 11, 286, 390, 7159, 281, 25878, 284, 339, 2505, 512, 295, 527, 3089, 293, 7019, 309, 493, 666, 613, 16679, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10048991309271918, "compression_ratio": 1.6794871794871795, "no_speech_prob": 1.568350307934452e-05}, {"id": 1336, "seek": 672100, "start": 6721.0, "end": 6730.0, "text": " So like linear bash arm on the 10 age, etc. These are layers or modules and they can be stacked up into neural nets like Lego building blocks.", "tokens": [50364, 407, 411, 8213, 46183, 3726, 322, 264, 1266, 3205, 11, 5183, 13, 1981, 366, 7914, 420, 16679, 293, 436, 393, 312, 28867, 493, 666, 18161, 36170, 411, 28761, 2390, 8474, 13, 50814, 50814, 400, 613, 7914, 767, 2514, 294, 25878, 284, 339, 13, 51014, 51014, 400, 498, 291, 974, 27822, 293, 550, 550, 291, 393, 767, 264, 636, 286, 600, 17083, 309, 11, 291, 393, 2935, 445, 764, 25878, 284, 339, 538, 2666, 2029, 293, 917, 24347, 281, 439, 613, 819, 7914, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1759255836749899, "compression_ratio": 1.617117117117117, "no_speech_prob": 4.5655642679776065e-06}, {"id": 1337, "seek": 672100, "start": 6730.0, "end": 6734.0, "text": " And these layers actually exist in pytorch.", "tokens": [50364, 407, 411, 8213, 46183, 3726, 322, 264, 1266, 3205, 11, 5183, 13, 1981, 366, 7914, 420, 16679, 293, 436, 393, 312, 28867, 493, 666, 18161, 36170, 411, 28761, 2390, 8474, 13, 50814, 50814, 400, 613, 7914, 767, 2514, 294, 25878, 284, 339, 13, 51014, 51014, 400, 498, 291, 974, 27822, 293, 550, 550, 291, 393, 767, 264, 636, 286, 600, 17083, 309, 11, 291, 393, 2935, 445, 764, 25878, 284, 339, 538, 2666, 2029, 293, 917, 24347, 281, 439, 613, 819, 7914, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1759255836749899, "compression_ratio": 1.617117117117117, "no_speech_prob": 4.5655642679776065e-06}, {"id": 1338, "seek": 672100, "start": 6734.0, "end": 6744.0, "text": " And if you import torch and then then you can actually the way I've constructed it, you can simply just use pytorch by prepending and endowed to all these different layers.", "tokens": [50364, 407, 411, 8213, 46183, 3726, 322, 264, 1266, 3205, 11, 5183, 13, 1981, 366, 7914, 420, 16679, 293, 436, 393, 312, 28867, 493, 666, 18161, 36170, 411, 28761, 2390, 8474, 13, 50814, 50814, 400, 613, 7914, 767, 2514, 294, 25878, 284, 339, 13, 51014, 51014, 400, 498, 291, 974, 27822, 293, 550, 550, 291, 393, 767, 264, 636, 286, 600, 17083, 309, 11, 291, 393, 2935, 445, 764, 25878, 284, 339, 538, 2666, 2029, 293, 917, 24347, 281, 439, 613, 819, 7914, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1759255836749899, "compression_ratio": 1.617117117117117, "no_speech_prob": 4.5655642679776065e-06}, {"id": 1339, "seek": 674400, "start": 6744.0, "end": 6752.0, "text": " And actually everything will just work because the API that I've developed here is identical to the API that pytorch uses.", "tokens": [50364, 400, 767, 1203, 486, 445, 589, 570, 264, 9362, 300, 286, 600, 4743, 510, 307, 14800, 281, 264, 9362, 300, 25878, 284, 339, 4960, 13, 50764, 50764, 400, 264, 11420, 611, 307, 1936, 11, 382, 1400, 382, 286, 478, 3650, 11, 14800, 281, 264, 472, 294, 25878, 284, 339, 13, 51064, 51064, 400, 1230, 1045, 11, 286, 3031, 281, 5366, 291, 281, 264, 27897, 3873, 300, 291, 576, 764, 281, 1223, 1968, 428, 18161, 3209, 307, 294, 257, 665, 1785, 43492, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.062432943388473154, "compression_ratio": 1.6434782608695653, "no_speech_prob": 7.294382157851942e-06}, {"id": 1340, "seek": 674400, "start": 6752.0, "end": 6758.0, "text": " And the implementation also is basically, as far as I'm aware, identical to the one in pytorch.", "tokens": [50364, 400, 767, 1203, 486, 445, 589, 570, 264, 9362, 300, 286, 600, 4743, 510, 307, 14800, 281, 264, 9362, 300, 25878, 284, 339, 4960, 13, 50764, 50764, 400, 264, 11420, 611, 307, 1936, 11, 382, 1400, 382, 286, 478, 3650, 11, 14800, 281, 264, 472, 294, 25878, 284, 339, 13, 51064, 51064, 400, 1230, 1045, 11, 286, 3031, 281, 5366, 291, 281, 264, 27897, 3873, 300, 291, 576, 764, 281, 1223, 1968, 428, 18161, 3209, 307, 294, 257, 665, 1785, 43492, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.062432943388473154, "compression_ratio": 1.6434782608695653, "no_speech_prob": 7.294382157851942e-06}, {"id": 1341, "seek": 674400, "start": 6758.0, "end": 6766.0, "text": " And number three, I tried to introduce you to the diagnostic tools that you would use to understand whether your neural network is in a good state dynamically.", "tokens": [50364, 400, 767, 1203, 486, 445, 589, 570, 264, 9362, 300, 286, 600, 4743, 510, 307, 14800, 281, 264, 9362, 300, 25878, 284, 339, 4960, 13, 50764, 50764, 400, 264, 11420, 611, 307, 1936, 11, 382, 1400, 382, 286, 478, 3650, 11, 14800, 281, 264, 472, 294, 25878, 284, 339, 13, 51064, 51064, 400, 1230, 1045, 11, 286, 3031, 281, 5366, 291, 281, 264, 27897, 3873, 300, 291, 576, 764, 281, 1223, 1968, 428, 18161, 3209, 307, 294, 257, 665, 1785, 43492, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.062432943388473154, "compression_ratio": 1.6434782608695653, "no_speech_prob": 7.294382157851942e-06}, {"id": 1342, "seek": 676600, "start": 6766.0, "end": 6774.0, "text": " So we are looking at the statistics and histograms and activation of the forward pass activation activations, the backward pass gradients.", "tokens": [50364, 407, 321, 366, 1237, 412, 264, 12523, 293, 49816, 82, 293, 24433, 295, 264, 2128, 1320, 24433, 2430, 763, 11, 264, 23897, 1320, 2771, 2448, 13, 50764, 50764, 400, 550, 611, 321, 434, 1237, 412, 264, 2098, 300, 366, 516, 281, 312, 10588, 382, 644, 295, 342, 8997, 2750, 16235, 382, 2207, 13, 51014, 51014, 400, 321, 434, 1237, 412, 641, 1355, 11, 3832, 31219, 763, 293, 611, 264, 8509, 295, 2771, 2448, 281, 1412, 420, 754, 1101, 11, 264, 9205, 281, 1412, 13, 51414, 51414, 400, 321, 1866, 300, 5850, 321, 500, 380, 767, 574, 412, 309, 382, 257, 2167, 30163, 12496, 294, 565, 412, 512, 1729, 24784, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09383882556045264, "compression_ratio": 1.9453125, "no_speech_prob": 3.761603875318542e-05}, {"id": 1343, "seek": 676600, "start": 6774.0, "end": 6779.0, "text": " And then also we're looking at the ways that are going to be updated as part of stochastic gradient ascent.", "tokens": [50364, 407, 321, 366, 1237, 412, 264, 12523, 293, 49816, 82, 293, 24433, 295, 264, 2128, 1320, 24433, 2430, 763, 11, 264, 23897, 1320, 2771, 2448, 13, 50764, 50764, 400, 550, 611, 321, 434, 1237, 412, 264, 2098, 300, 366, 516, 281, 312, 10588, 382, 644, 295, 342, 8997, 2750, 16235, 382, 2207, 13, 51014, 51014, 400, 321, 434, 1237, 412, 641, 1355, 11, 3832, 31219, 763, 293, 611, 264, 8509, 295, 2771, 2448, 281, 1412, 420, 754, 1101, 11, 264, 9205, 281, 1412, 13, 51414, 51414, 400, 321, 1866, 300, 5850, 321, 500, 380, 767, 574, 412, 309, 382, 257, 2167, 30163, 12496, 294, 565, 412, 512, 1729, 24784, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09383882556045264, "compression_ratio": 1.9453125, "no_speech_prob": 3.761603875318542e-05}, {"id": 1344, "seek": 676600, "start": 6779.0, "end": 6787.0, "text": " And we're looking at their means, standard deviations and also the ratio of gradients to data or even better, the updates to data.", "tokens": [50364, 407, 321, 366, 1237, 412, 264, 12523, 293, 49816, 82, 293, 24433, 295, 264, 2128, 1320, 24433, 2430, 763, 11, 264, 23897, 1320, 2771, 2448, 13, 50764, 50764, 400, 550, 611, 321, 434, 1237, 412, 264, 2098, 300, 366, 516, 281, 312, 10588, 382, 644, 295, 342, 8997, 2750, 16235, 382, 2207, 13, 51014, 51014, 400, 321, 434, 1237, 412, 641, 1355, 11, 3832, 31219, 763, 293, 611, 264, 8509, 295, 2771, 2448, 281, 1412, 420, 754, 1101, 11, 264, 9205, 281, 1412, 13, 51414, 51414, 400, 321, 1866, 300, 5850, 321, 500, 380, 767, 574, 412, 309, 382, 257, 2167, 30163, 12496, 294, 565, 412, 512, 1729, 24784, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09383882556045264, "compression_ratio": 1.9453125, "no_speech_prob": 3.761603875318542e-05}, {"id": 1345, "seek": 676600, "start": 6787.0, "end": 6793.0, "text": " And we saw that typically we don't actually look at it as a single snapshot frozen in time at some particular iteration.", "tokens": [50364, 407, 321, 366, 1237, 412, 264, 12523, 293, 49816, 82, 293, 24433, 295, 264, 2128, 1320, 24433, 2430, 763, 11, 264, 23897, 1320, 2771, 2448, 13, 50764, 50764, 400, 550, 611, 321, 434, 1237, 412, 264, 2098, 300, 366, 516, 281, 312, 10588, 382, 644, 295, 342, 8997, 2750, 16235, 382, 2207, 13, 51014, 51014, 400, 321, 434, 1237, 412, 641, 1355, 11, 3832, 31219, 763, 293, 611, 264, 8509, 295, 2771, 2448, 281, 1412, 420, 754, 1101, 11, 264, 9205, 281, 1412, 13, 51414, 51414, 400, 321, 1866, 300, 5850, 321, 500, 380, 767, 574, 412, 309, 382, 257, 2167, 30163, 12496, 294, 565, 412, 512, 1729, 24784, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.09383882556045264, "compression_ratio": 1.9453125, "no_speech_prob": 3.761603875318542e-05}, {"id": 1346, "seek": 679300, "start": 6793.0, "end": 6797.0, "text": " Typically, people look at this as over time, just like I've done here.", "tokens": [50364, 23129, 11, 561, 574, 412, 341, 382, 670, 565, 11, 445, 411, 286, 600, 1096, 510, 13, 50564, 50564, 400, 436, 574, 412, 613, 5623, 281, 1412, 32435, 293, 436, 652, 988, 1203, 1542, 2264, 13, 50764, 50764, 400, 294, 1729, 11, 286, 848, 300, 7935, 3671, 1045, 420, 1936, 3671, 1045, 322, 264, 3565, 4373, 307, 257, 665, 5903, 415, 374, 3142, 337, 437, 291, 528, 341, 8509, 281, 312, 13, 51264, 51264, 400, 498, 309, 311, 636, 886, 1090, 11, 550, 1391, 264, 2539, 3314, 420, 264, 9205, 366, 257, 707, 886, 955, 13, 51514, 51514, 400, 498, 309, 311, 636, 886, 1359, 11, 300, 264, 2539, 3314, 307, 1391, 886, 1359, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07784400266759536, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.0238661565817893e-05}, {"id": 1347, "seek": 679300, "start": 6797.0, "end": 6801.0, "text": " And they look at these update to data ratios and they make sure everything looks OK.", "tokens": [50364, 23129, 11, 561, 574, 412, 341, 382, 670, 565, 11, 445, 411, 286, 600, 1096, 510, 13, 50564, 50564, 400, 436, 574, 412, 613, 5623, 281, 1412, 32435, 293, 436, 652, 988, 1203, 1542, 2264, 13, 50764, 50764, 400, 294, 1729, 11, 286, 848, 300, 7935, 3671, 1045, 420, 1936, 3671, 1045, 322, 264, 3565, 4373, 307, 257, 665, 5903, 415, 374, 3142, 337, 437, 291, 528, 341, 8509, 281, 312, 13, 51264, 51264, 400, 498, 309, 311, 636, 886, 1090, 11, 550, 1391, 264, 2539, 3314, 420, 264, 9205, 366, 257, 707, 886, 955, 13, 51514, 51514, 400, 498, 309, 311, 636, 886, 1359, 11, 300, 264, 2539, 3314, 307, 1391, 886, 1359, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07784400266759536, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.0238661565817893e-05}, {"id": 1348, "seek": 679300, "start": 6801.0, "end": 6811.0, "text": " And in particular, I said that wanting negative three or basically negative three on the log scale is a good rough heuristic for what you want this ratio to be.", "tokens": [50364, 23129, 11, 561, 574, 412, 341, 382, 670, 565, 11, 445, 411, 286, 600, 1096, 510, 13, 50564, 50564, 400, 436, 574, 412, 613, 5623, 281, 1412, 32435, 293, 436, 652, 988, 1203, 1542, 2264, 13, 50764, 50764, 400, 294, 1729, 11, 286, 848, 300, 7935, 3671, 1045, 420, 1936, 3671, 1045, 322, 264, 3565, 4373, 307, 257, 665, 5903, 415, 374, 3142, 337, 437, 291, 528, 341, 8509, 281, 312, 13, 51264, 51264, 400, 498, 309, 311, 636, 886, 1090, 11, 550, 1391, 264, 2539, 3314, 420, 264, 9205, 366, 257, 707, 886, 955, 13, 51514, 51514, 400, 498, 309, 311, 636, 886, 1359, 11, 300, 264, 2539, 3314, 307, 1391, 886, 1359, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07784400266759536, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.0238661565817893e-05}, {"id": 1349, "seek": 679300, "start": 6811.0, "end": 6816.0, "text": " And if it's way too high, then probably the learning rate or the updates are a little too big.", "tokens": [50364, 23129, 11, 561, 574, 412, 341, 382, 670, 565, 11, 445, 411, 286, 600, 1096, 510, 13, 50564, 50564, 400, 436, 574, 412, 613, 5623, 281, 1412, 32435, 293, 436, 652, 988, 1203, 1542, 2264, 13, 50764, 50764, 400, 294, 1729, 11, 286, 848, 300, 7935, 3671, 1045, 420, 1936, 3671, 1045, 322, 264, 3565, 4373, 307, 257, 665, 5903, 415, 374, 3142, 337, 437, 291, 528, 341, 8509, 281, 312, 13, 51264, 51264, 400, 498, 309, 311, 636, 886, 1090, 11, 550, 1391, 264, 2539, 3314, 420, 264, 9205, 366, 257, 707, 886, 955, 13, 51514, 51514, 400, 498, 309, 311, 636, 886, 1359, 11, 300, 264, 2539, 3314, 307, 1391, 886, 1359, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07784400266759536, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.0238661565817893e-05}, {"id": 1350, "seek": 679300, "start": 6816.0, "end": 6819.0, "text": " And if it's way too small, that the learning rate is probably too small.", "tokens": [50364, 23129, 11, 561, 574, 412, 341, 382, 670, 565, 11, 445, 411, 286, 600, 1096, 510, 13, 50564, 50564, 400, 436, 574, 412, 613, 5623, 281, 1412, 32435, 293, 436, 652, 988, 1203, 1542, 2264, 13, 50764, 50764, 400, 294, 1729, 11, 286, 848, 300, 7935, 3671, 1045, 420, 1936, 3671, 1045, 322, 264, 3565, 4373, 307, 257, 665, 5903, 415, 374, 3142, 337, 437, 291, 528, 341, 8509, 281, 312, 13, 51264, 51264, 400, 498, 309, 311, 636, 886, 1090, 11, 550, 1391, 264, 2539, 3314, 420, 264, 9205, 366, 257, 707, 886, 955, 13, 51514, 51514, 400, 498, 309, 311, 636, 886, 1359, 11, 300, 264, 2539, 3314, 307, 1391, 886, 1359, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07784400266759536, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.0238661565817893e-05}, {"id": 1351, "seek": 681900, "start": 6819.0, "end": 6826.0, "text": " So that's just some of the things that you may want to play with when you try to get your neural network to work very well.", "tokens": [50364, 407, 300, 311, 445, 512, 295, 264, 721, 300, 291, 815, 528, 281, 862, 365, 562, 291, 853, 281, 483, 428, 18161, 3209, 281, 589, 588, 731, 13, 50714, 50714, 823, 11, 456, 311, 257, 1230, 295, 721, 286, 630, 406, 853, 281, 4584, 13, 50864, 50864, 286, 630, 406, 853, 281, 4224, 527, 3894, 3389, 11, 382, 364, 1365, 11, 538, 15424, 264, 363, 852, 45, 687, 4583, 13, 51064, 51064, 5135, 11, 286, 630, 853, 293, 286, 1352, 300, 286, 1143, 264, 2539, 3314, 5006, 7513, 300, 286, 600, 7619, 949, 13, 51364, 51364, 286, 3031, 281, 3847, 264, 363, 852, 45, 687, 4583, 11, 363, 852, 45, 687, 18161, 2533, 13, 51564, 51564, 400, 286, 767, 4590, 493, 365, 3542, 300, 366, 588, 11, 588, 2531, 281, 437, 321, 600, 14879, 949, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09191926547459194, "compression_ratio": 1.825503355704698, "no_speech_prob": 6.539535661431728e-06}, {"id": 1352, "seek": 681900, "start": 6826.0, "end": 6829.0, "text": " Now, there's a number of things I did not try to achieve.", "tokens": [50364, 407, 300, 311, 445, 512, 295, 264, 721, 300, 291, 815, 528, 281, 862, 365, 562, 291, 853, 281, 483, 428, 18161, 3209, 281, 589, 588, 731, 13, 50714, 50714, 823, 11, 456, 311, 257, 1230, 295, 721, 286, 630, 406, 853, 281, 4584, 13, 50864, 50864, 286, 630, 406, 853, 281, 4224, 527, 3894, 3389, 11, 382, 364, 1365, 11, 538, 15424, 264, 363, 852, 45, 687, 4583, 13, 51064, 51064, 5135, 11, 286, 630, 853, 293, 286, 1352, 300, 286, 1143, 264, 2539, 3314, 5006, 7513, 300, 286, 600, 7619, 949, 13, 51364, 51364, 286, 3031, 281, 3847, 264, 363, 852, 45, 687, 4583, 11, 363, 852, 45, 687, 18161, 2533, 13, 51564, 51564, 400, 286, 767, 4590, 493, 365, 3542, 300, 366, 588, 11, 588, 2531, 281, 437, 321, 600, 14879, 949, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09191926547459194, "compression_ratio": 1.825503355704698, "no_speech_prob": 6.539535661431728e-06}, {"id": 1353, "seek": 681900, "start": 6829.0, "end": 6833.0, "text": " I did not try to beat our previous performance, as an example, by introducing the BatchNorm layer.", "tokens": [50364, 407, 300, 311, 445, 512, 295, 264, 721, 300, 291, 815, 528, 281, 862, 365, 562, 291, 853, 281, 483, 428, 18161, 3209, 281, 589, 588, 731, 13, 50714, 50714, 823, 11, 456, 311, 257, 1230, 295, 721, 286, 630, 406, 853, 281, 4584, 13, 50864, 50864, 286, 630, 406, 853, 281, 4224, 527, 3894, 3389, 11, 382, 364, 1365, 11, 538, 15424, 264, 363, 852, 45, 687, 4583, 13, 51064, 51064, 5135, 11, 286, 630, 853, 293, 286, 1352, 300, 286, 1143, 264, 2539, 3314, 5006, 7513, 300, 286, 600, 7619, 949, 13, 51364, 51364, 286, 3031, 281, 3847, 264, 363, 852, 45, 687, 4583, 11, 363, 852, 45, 687, 18161, 2533, 13, 51564, 51564, 400, 286, 767, 4590, 493, 365, 3542, 300, 366, 588, 11, 588, 2531, 281, 437, 321, 600, 14879, 949, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09191926547459194, "compression_ratio": 1.825503355704698, "no_speech_prob": 6.539535661431728e-06}, {"id": 1354, "seek": 681900, "start": 6833.0, "end": 6839.0, "text": " Actually, I did try and I found that I used the learning rate finding mechanism that I've described before.", "tokens": [50364, 407, 300, 311, 445, 512, 295, 264, 721, 300, 291, 815, 528, 281, 862, 365, 562, 291, 853, 281, 483, 428, 18161, 3209, 281, 589, 588, 731, 13, 50714, 50714, 823, 11, 456, 311, 257, 1230, 295, 721, 286, 630, 406, 853, 281, 4584, 13, 50864, 50864, 286, 630, 406, 853, 281, 4224, 527, 3894, 3389, 11, 382, 364, 1365, 11, 538, 15424, 264, 363, 852, 45, 687, 4583, 13, 51064, 51064, 5135, 11, 286, 630, 853, 293, 286, 1352, 300, 286, 1143, 264, 2539, 3314, 5006, 7513, 300, 286, 600, 7619, 949, 13, 51364, 51364, 286, 3031, 281, 3847, 264, 363, 852, 45, 687, 4583, 11, 363, 852, 45, 687, 18161, 2533, 13, 51564, 51564, 400, 286, 767, 4590, 493, 365, 3542, 300, 366, 588, 11, 588, 2531, 281, 437, 321, 600, 14879, 949, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09191926547459194, "compression_ratio": 1.825503355704698, "no_speech_prob": 6.539535661431728e-06}, {"id": 1355, "seek": 681900, "start": 6839.0, "end": 6843.0, "text": " I tried to train the BatchNorm layer, BatchNorm neural net.", "tokens": [50364, 407, 300, 311, 445, 512, 295, 264, 721, 300, 291, 815, 528, 281, 862, 365, 562, 291, 853, 281, 483, 428, 18161, 3209, 281, 589, 588, 731, 13, 50714, 50714, 823, 11, 456, 311, 257, 1230, 295, 721, 286, 630, 406, 853, 281, 4584, 13, 50864, 50864, 286, 630, 406, 853, 281, 4224, 527, 3894, 3389, 11, 382, 364, 1365, 11, 538, 15424, 264, 363, 852, 45, 687, 4583, 13, 51064, 51064, 5135, 11, 286, 630, 853, 293, 286, 1352, 300, 286, 1143, 264, 2539, 3314, 5006, 7513, 300, 286, 600, 7619, 949, 13, 51364, 51364, 286, 3031, 281, 3847, 264, 363, 852, 45, 687, 4583, 11, 363, 852, 45, 687, 18161, 2533, 13, 51564, 51564, 400, 286, 767, 4590, 493, 365, 3542, 300, 366, 588, 11, 588, 2531, 281, 437, 321, 600, 14879, 949, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09191926547459194, "compression_ratio": 1.825503355704698, "no_speech_prob": 6.539535661431728e-06}, {"id": 1356, "seek": 681900, "start": 6843.0, "end": 6848.0, "text": " And I actually ended up with results that are very, very similar to what we've obtained before.", "tokens": [50364, 407, 300, 311, 445, 512, 295, 264, 721, 300, 291, 815, 528, 281, 862, 365, 562, 291, 853, 281, 483, 428, 18161, 3209, 281, 589, 588, 731, 13, 50714, 50714, 823, 11, 456, 311, 257, 1230, 295, 721, 286, 630, 406, 853, 281, 4584, 13, 50864, 50864, 286, 630, 406, 853, 281, 4224, 527, 3894, 3389, 11, 382, 364, 1365, 11, 538, 15424, 264, 363, 852, 45, 687, 4583, 13, 51064, 51064, 5135, 11, 286, 630, 853, 293, 286, 1352, 300, 286, 1143, 264, 2539, 3314, 5006, 7513, 300, 286, 600, 7619, 949, 13, 51364, 51364, 286, 3031, 281, 3847, 264, 363, 852, 45, 687, 4583, 11, 363, 852, 45, 687, 18161, 2533, 13, 51564, 51564, 400, 286, 767, 4590, 493, 365, 3542, 300, 366, 588, 11, 588, 2531, 281, 437, 321, 600, 14879, 949, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.09191926547459194, "compression_ratio": 1.825503355704698, "no_speech_prob": 6.539535661431728e-06}, {"id": 1357, "seek": 684800, "start": 6848.0, "end": 6855.0, "text": " And that's because our performance now is not bottlenecked by the optimization, which is what BatchNorm is helping with.", "tokens": [50364, 400, 300, 311, 570, 527, 3389, 586, 307, 406, 44641, 44118, 538, 264, 19618, 11, 597, 307, 437, 363, 852, 45, 687, 307, 4315, 365, 13, 50714, 50714, 440, 3389, 412, 341, 3233, 307, 44641, 44118, 538, 437, 286, 9091, 307, 264, 4319, 4641, 295, 527, 4319, 13, 51064, 51064, 407, 4362, 321, 366, 1940, 1045, 4342, 281, 6069, 264, 6409, 472, 13, 51214, 51214, 400, 286, 519, 321, 643, 281, 352, 4399, 300, 13, 400, 321, 643, 281, 574, 412, 544, 4005, 6331, 1303, 337, 527, 411, 18680, 1753, 18161, 9590, 293, 4088, 433, 294, 1668, 281, 3052, 2944, 264, 3565, 33783, 300, 321, 434, 19626, 322, 341, 1412, 992, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08385752809458766, "compression_ratio": 1.7156862745098038, "no_speech_prob": 4.42447844761773e-06}, {"id": 1358, "seek": 684800, "start": 6855.0, "end": 6862.0, "text": " The performance at this stage is bottlenecked by what I suspect is the context length of our context.", "tokens": [50364, 400, 300, 311, 570, 527, 3389, 586, 307, 406, 44641, 44118, 538, 264, 19618, 11, 597, 307, 437, 363, 852, 45, 687, 307, 4315, 365, 13, 50714, 50714, 440, 3389, 412, 341, 3233, 307, 44641, 44118, 538, 437, 286, 9091, 307, 264, 4319, 4641, 295, 527, 4319, 13, 51064, 51064, 407, 4362, 321, 366, 1940, 1045, 4342, 281, 6069, 264, 6409, 472, 13, 51214, 51214, 400, 286, 519, 321, 643, 281, 352, 4399, 300, 13, 400, 321, 643, 281, 574, 412, 544, 4005, 6331, 1303, 337, 527, 411, 18680, 1753, 18161, 9590, 293, 4088, 433, 294, 1668, 281, 3052, 2944, 264, 3565, 33783, 300, 321, 434, 19626, 322, 341, 1412, 992, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08385752809458766, "compression_ratio": 1.7156862745098038, "no_speech_prob": 4.42447844761773e-06}, {"id": 1359, "seek": 684800, "start": 6862.0, "end": 6865.0, "text": " So currently we are taking three characters to predict the fourth one.", "tokens": [50364, 400, 300, 311, 570, 527, 3389, 586, 307, 406, 44641, 44118, 538, 264, 19618, 11, 597, 307, 437, 363, 852, 45, 687, 307, 4315, 365, 13, 50714, 50714, 440, 3389, 412, 341, 3233, 307, 44641, 44118, 538, 437, 286, 9091, 307, 264, 4319, 4641, 295, 527, 4319, 13, 51064, 51064, 407, 4362, 321, 366, 1940, 1045, 4342, 281, 6069, 264, 6409, 472, 13, 51214, 51214, 400, 286, 519, 321, 643, 281, 352, 4399, 300, 13, 400, 321, 643, 281, 574, 412, 544, 4005, 6331, 1303, 337, 527, 411, 18680, 1753, 18161, 9590, 293, 4088, 433, 294, 1668, 281, 3052, 2944, 264, 3565, 33783, 300, 321, 434, 19626, 322, 341, 1412, 992, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08385752809458766, "compression_ratio": 1.7156862745098038, "no_speech_prob": 4.42447844761773e-06}, {"id": 1360, "seek": 684800, "start": 6865.0, "end": 6876.0, "text": " And I think we need to go beyond that. And we need to look at more powerful architectures for our like recurrent neural networks and transformers in order to further push the log probabilities that we're achieving on this data set.", "tokens": [50364, 400, 300, 311, 570, 527, 3389, 586, 307, 406, 44641, 44118, 538, 264, 19618, 11, 597, 307, 437, 363, 852, 45, 687, 307, 4315, 365, 13, 50714, 50714, 440, 3389, 412, 341, 3233, 307, 44641, 44118, 538, 437, 286, 9091, 307, 264, 4319, 4641, 295, 527, 4319, 13, 51064, 51064, 407, 4362, 321, 366, 1940, 1045, 4342, 281, 6069, 264, 6409, 472, 13, 51214, 51214, 400, 286, 519, 321, 643, 281, 352, 4399, 300, 13, 400, 321, 643, 281, 574, 412, 544, 4005, 6331, 1303, 337, 527, 411, 18680, 1753, 18161, 9590, 293, 4088, 433, 294, 1668, 281, 3052, 2944, 264, 3565, 33783, 300, 321, 434, 19626, 322, 341, 1412, 992, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08385752809458766, "compression_ratio": 1.7156862745098038, "no_speech_prob": 4.42447844761773e-06}, {"id": 1361, "seek": 687600, "start": 6876.0, "end": 6885.0, "text": " And I also did not try to have a full explanation of all of these activations, the gradients and the backward pass and the statistics of all these gradients.", "tokens": [50364, 400, 286, 611, 630, 406, 853, 281, 362, 257, 1577, 10835, 295, 439, 295, 613, 2430, 763, 11, 264, 2771, 2448, 293, 264, 23897, 1320, 293, 264, 12523, 295, 439, 613, 2771, 2448, 13, 50814, 50814, 400, 370, 291, 815, 362, 1352, 512, 295, 264, 3166, 510, 29466, 48314, 293, 1310, 291, 434, 4748, 9019, 466, 11, 2264, 11, 498, 286, 1319, 264, 1216, 510, 11, 577, 808, 300, 321, 643, 257, 819, 2539, 3314, 30, 51264, 51264, 400, 286, 994, 380, 352, 666, 264, 1577, 2607, 570, 291, 1116, 362, 281, 767, 574, 412, 264, 23897, 1320, 295, 439, 613, 819, 7914, 293, 483, 364, 21769, 3701, 295, 577, 300, 1985, 13, 51614, 51614, 400, 286, 630, 406, 352, 666, 300, 294, 341, 7991, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.04414973625769982, "compression_ratio": 1.842622950819672, "no_speech_prob": 4.49439903604798e-06}, {"id": 1362, "seek": 687600, "start": 6885.0, "end": 6894.0, "text": " And so you may have found some of the parts here unintuitive and maybe you're slightly confused about, OK, if I change the game here, how come that we need a different learning rate?", "tokens": [50364, 400, 286, 611, 630, 406, 853, 281, 362, 257, 1577, 10835, 295, 439, 295, 613, 2430, 763, 11, 264, 2771, 2448, 293, 264, 23897, 1320, 293, 264, 12523, 295, 439, 613, 2771, 2448, 13, 50814, 50814, 400, 370, 291, 815, 362, 1352, 512, 295, 264, 3166, 510, 29466, 48314, 293, 1310, 291, 434, 4748, 9019, 466, 11, 2264, 11, 498, 286, 1319, 264, 1216, 510, 11, 577, 808, 300, 321, 643, 257, 819, 2539, 3314, 30, 51264, 51264, 400, 286, 994, 380, 352, 666, 264, 1577, 2607, 570, 291, 1116, 362, 281, 767, 574, 412, 264, 23897, 1320, 295, 439, 613, 819, 7914, 293, 483, 364, 21769, 3701, 295, 577, 300, 1985, 13, 51614, 51614, 400, 286, 630, 406, 352, 666, 300, 294, 341, 7991, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.04414973625769982, "compression_ratio": 1.842622950819672, "no_speech_prob": 4.49439903604798e-06}, {"id": 1363, "seek": 687600, "start": 6894.0, "end": 6901.0, "text": " And I didn't go into the full detail because you'd have to actually look at the backward pass of all these different layers and get an intuitive understanding of how that works.", "tokens": [50364, 400, 286, 611, 630, 406, 853, 281, 362, 257, 1577, 10835, 295, 439, 295, 613, 2430, 763, 11, 264, 2771, 2448, 293, 264, 23897, 1320, 293, 264, 12523, 295, 439, 613, 2771, 2448, 13, 50814, 50814, 400, 370, 291, 815, 362, 1352, 512, 295, 264, 3166, 510, 29466, 48314, 293, 1310, 291, 434, 4748, 9019, 466, 11, 2264, 11, 498, 286, 1319, 264, 1216, 510, 11, 577, 808, 300, 321, 643, 257, 819, 2539, 3314, 30, 51264, 51264, 400, 286, 994, 380, 352, 666, 264, 1577, 2607, 570, 291, 1116, 362, 281, 767, 574, 412, 264, 23897, 1320, 295, 439, 613, 819, 7914, 293, 483, 364, 21769, 3701, 295, 577, 300, 1985, 13, 51614, 51614, 400, 286, 630, 406, 352, 666, 300, 294, 341, 7991, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.04414973625769982, "compression_ratio": 1.842622950819672, "no_speech_prob": 4.49439903604798e-06}, {"id": 1364, "seek": 687600, "start": 6901.0, "end": 6904.0, "text": " And I did not go into that in this lecture.", "tokens": [50364, 400, 286, 611, 630, 406, 853, 281, 362, 257, 1577, 10835, 295, 439, 295, 613, 2430, 763, 11, 264, 2771, 2448, 293, 264, 23897, 1320, 293, 264, 12523, 295, 439, 613, 2771, 2448, 13, 50814, 50814, 400, 370, 291, 815, 362, 1352, 512, 295, 264, 3166, 510, 29466, 48314, 293, 1310, 291, 434, 4748, 9019, 466, 11, 2264, 11, 498, 286, 1319, 264, 1216, 510, 11, 577, 808, 300, 321, 643, 257, 819, 2539, 3314, 30, 51264, 51264, 400, 286, 994, 380, 352, 666, 264, 1577, 2607, 570, 291, 1116, 362, 281, 767, 574, 412, 264, 23897, 1320, 295, 439, 613, 819, 7914, 293, 483, 364, 21769, 3701, 295, 577, 300, 1985, 13, 51614, 51614, 400, 286, 630, 406, 352, 666, 300, 294, 341, 7991, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.04414973625769982, "compression_ratio": 1.842622950819672, "no_speech_prob": 4.49439903604798e-06}, {"id": 1365, "seek": 690400, "start": 6904.0, "end": 6908.0, "text": " The purpose really was just to introduce you to the diagnostic tools and what they look like.", "tokens": [50364, 440, 4334, 534, 390, 445, 281, 5366, 291, 281, 264, 27897, 3873, 293, 437, 436, 574, 411, 13, 50564, 50564, 583, 456, 311, 920, 257, 688, 295, 589, 8877, 322, 264, 21769, 1496, 281, 1223, 264, 5883, 2144, 11, 264, 23897, 1320, 293, 577, 439, 295, 300, 43582, 13, 50914, 50914, 583, 291, 4659, 380, 841, 886, 1578, 570, 6095, 11, 321, 366, 1242, 281, 264, 6492, 4691, 295, 689, 264, 2519, 307, 13, 51314, 51314, 492, 3297, 2378, 380, 11, 286, 576, 584, 11, 13041, 5883, 2144, 293, 321, 2378, 380, 13041, 646, 79, 1513, 559, 399, 13, 51564, 51564, 400, 613, 366, 920, 588, 709, 364, 4967, 1859, 295, 2132, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06082948456462632, "compression_ratio": 1.7310344827586206, "no_speech_prob": 8.528523721906822e-06}, {"id": 1366, "seek": 690400, "start": 6908.0, "end": 6915.0, "text": " But there's still a lot of work remaining on the intuitive level to understand the initialization, the backward pass and how all of that interacts.", "tokens": [50364, 440, 4334, 534, 390, 445, 281, 5366, 291, 281, 264, 27897, 3873, 293, 437, 436, 574, 411, 13, 50564, 50564, 583, 456, 311, 920, 257, 688, 295, 589, 8877, 322, 264, 21769, 1496, 281, 1223, 264, 5883, 2144, 11, 264, 23897, 1320, 293, 577, 439, 295, 300, 43582, 13, 50914, 50914, 583, 291, 4659, 380, 841, 886, 1578, 570, 6095, 11, 321, 366, 1242, 281, 264, 6492, 4691, 295, 689, 264, 2519, 307, 13, 51314, 51314, 492, 3297, 2378, 380, 11, 286, 576, 584, 11, 13041, 5883, 2144, 293, 321, 2378, 380, 13041, 646, 79, 1513, 559, 399, 13, 51564, 51564, 400, 613, 366, 920, 588, 709, 364, 4967, 1859, 295, 2132, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06082948456462632, "compression_ratio": 1.7310344827586206, "no_speech_prob": 8.528523721906822e-06}, {"id": 1367, "seek": 690400, "start": 6915.0, "end": 6923.0, "text": " But you shouldn't feel too bad because honestly, we are getting to the cutting edge of where the field is.", "tokens": [50364, 440, 4334, 534, 390, 445, 281, 5366, 291, 281, 264, 27897, 3873, 293, 437, 436, 574, 411, 13, 50564, 50564, 583, 456, 311, 920, 257, 688, 295, 589, 8877, 322, 264, 21769, 1496, 281, 1223, 264, 5883, 2144, 11, 264, 23897, 1320, 293, 577, 439, 295, 300, 43582, 13, 50914, 50914, 583, 291, 4659, 380, 841, 886, 1578, 570, 6095, 11, 321, 366, 1242, 281, 264, 6492, 4691, 295, 689, 264, 2519, 307, 13, 51314, 51314, 492, 3297, 2378, 380, 11, 286, 576, 584, 11, 13041, 5883, 2144, 293, 321, 2378, 380, 13041, 646, 79, 1513, 559, 399, 13, 51564, 51564, 400, 613, 366, 920, 588, 709, 364, 4967, 1859, 295, 2132, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06082948456462632, "compression_ratio": 1.7310344827586206, "no_speech_prob": 8.528523721906822e-06}, {"id": 1368, "seek": 690400, "start": 6923.0, "end": 6928.0, "text": " We certainly haven't, I would say, solved initialization and we haven't solved backpropagation.", "tokens": [50364, 440, 4334, 534, 390, 445, 281, 5366, 291, 281, 264, 27897, 3873, 293, 437, 436, 574, 411, 13, 50564, 50564, 583, 456, 311, 920, 257, 688, 295, 589, 8877, 322, 264, 21769, 1496, 281, 1223, 264, 5883, 2144, 11, 264, 23897, 1320, 293, 577, 439, 295, 300, 43582, 13, 50914, 50914, 583, 291, 4659, 380, 841, 886, 1578, 570, 6095, 11, 321, 366, 1242, 281, 264, 6492, 4691, 295, 689, 264, 2519, 307, 13, 51314, 51314, 492, 3297, 2378, 380, 11, 286, 576, 584, 11, 13041, 5883, 2144, 293, 321, 2378, 380, 13041, 646, 79, 1513, 559, 399, 13, 51564, 51564, 400, 613, 366, 920, 588, 709, 364, 4967, 1859, 295, 2132, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06082948456462632, "compression_ratio": 1.7310344827586206, "no_speech_prob": 8.528523721906822e-06}, {"id": 1369, "seek": 690400, "start": 6928.0, "end": 6931.0, "text": " And these are still very much an active area of research.", "tokens": [50364, 440, 4334, 534, 390, 445, 281, 5366, 291, 281, 264, 27897, 3873, 293, 437, 436, 574, 411, 13, 50564, 50564, 583, 456, 311, 920, 257, 688, 295, 589, 8877, 322, 264, 21769, 1496, 281, 1223, 264, 5883, 2144, 11, 264, 23897, 1320, 293, 577, 439, 295, 300, 43582, 13, 50914, 50914, 583, 291, 4659, 380, 841, 886, 1578, 570, 6095, 11, 321, 366, 1242, 281, 264, 6492, 4691, 295, 689, 264, 2519, 307, 13, 51314, 51314, 492, 3297, 2378, 380, 11, 286, 576, 584, 11, 13041, 5883, 2144, 293, 321, 2378, 380, 13041, 646, 79, 1513, 559, 399, 13, 51564, 51564, 400, 613, 366, 920, 588, 709, 364, 4967, 1859, 295, 2132, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06082948456462632, "compression_ratio": 1.7310344827586206, "no_speech_prob": 8.528523721906822e-06}, {"id": 1370, "seek": 693100, "start": 6931.0, "end": 6934.0, "text": " People are still trying to figure out what is the best way to initialize these networks?", "tokens": [50364, 3432, 366, 920, 1382, 281, 2573, 484, 437, 307, 264, 1151, 636, 281, 5883, 1125, 613, 9590, 30, 50514, 50514, 708, 307, 264, 1151, 5623, 4978, 281, 764, 30, 50664, 50664, 400, 370, 322, 13, 407, 6022, 295, 341, 307, 534, 13041, 13, 50764, 50764, 400, 321, 500, 380, 534, 362, 439, 264, 6338, 281, 439, 264, 281, 439, 613, 3331, 13, 51014, 51014, 583, 412, 1935, 321, 434, 1455, 4205, 293, 412, 1935, 321, 362, 512, 3873, 281, 980, 505, 1968, 420, 406, 721, 366, 322, 264, 558, 2837, 337, 586, 13, 51414, 51414, 407, 286, 519, 321, 600, 1027, 3353, 4205, 294, 341, 7991, 11, 293, 286, 1454, 291, 4626, 300, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09657753120034428, "compression_ratio": 1.684981684981685, "no_speech_prob": 3.216504046577029e-05}, {"id": 1371, "seek": 693100, "start": 6934.0, "end": 6937.0, "text": " What is the best update rule to use?", "tokens": [50364, 3432, 366, 920, 1382, 281, 2573, 484, 437, 307, 264, 1151, 636, 281, 5883, 1125, 613, 9590, 30, 50514, 50514, 708, 307, 264, 1151, 5623, 4978, 281, 764, 30, 50664, 50664, 400, 370, 322, 13, 407, 6022, 295, 341, 307, 534, 13041, 13, 50764, 50764, 400, 321, 500, 380, 534, 362, 439, 264, 6338, 281, 439, 264, 281, 439, 613, 3331, 13, 51014, 51014, 583, 412, 1935, 321, 434, 1455, 4205, 293, 412, 1935, 321, 362, 512, 3873, 281, 980, 505, 1968, 420, 406, 721, 366, 322, 264, 558, 2837, 337, 586, 13, 51414, 51414, 407, 286, 519, 321, 600, 1027, 3353, 4205, 294, 341, 7991, 11, 293, 286, 1454, 291, 4626, 300, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09657753120034428, "compression_ratio": 1.684981684981685, "no_speech_prob": 3.216504046577029e-05}, {"id": 1372, "seek": 693100, "start": 6937.0, "end": 6939.0, "text": " And so on. So none of this is really solved.", "tokens": [50364, 3432, 366, 920, 1382, 281, 2573, 484, 437, 307, 264, 1151, 636, 281, 5883, 1125, 613, 9590, 30, 50514, 50514, 708, 307, 264, 1151, 5623, 4978, 281, 764, 30, 50664, 50664, 400, 370, 322, 13, 407, 6022, 295, 341, 307, 534, 13041, 13, 50764, 50764, 400, 321, 500, 380, 534, 362, 439, 264, 6338, 281, 439, 264, 281, 439, 613, 3331, 13, 51014, 51014, 583, 412, 1935, 321, 434, 1455, 4205, 293, 412, 1935, 321, 362, 512, 3873, 281, 980, 505, 1968, 420, 406, 721, 366, 322, 264, 558, 2837, 337, 586, 13, 51414, 51414, 407, 286, 519, 321, 600, 1027, 3353, 4205, 294, 341, 7991, 11, 293, 286, 1454, 291, 4626, 300, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09657753120034428, "compression_ratio": 1.684981684981685, "no_speech_prob": 3.216504046577029e-05}, {"id": 1373, "seek": 693100, "start": 6939.0, "end": 6944.0, "text": " And we don't really have all the answers to all the to all these cases.", "tokens": [50364, 3432, 366, 920, 1382, 281, 2573, 484, 437, 307, 264, 1151, 636, 281, 5883, 1125, 613, 9590, 30, 50514, 50514, 708, 307, 264, 1151, 5623, 4978, 281, 764, 30, 50664, 50664, 400, 370, 322, 13, 407, 6022, 295, 341, 307, 534, 13041, 13, 50764, 50764, 400, 321, 500, 380, 534, 362, 439, 264, 6338, 281, 439, 264, 281, 439, 613, 3331, 13, 51014, 51014, 583, 412, 1935, 321, 434, 1455, 4205, 293, 412, 1935, 321, 362, 512, 3873, 281, 980, 505, 1968, 420, 406, 721, 366, 322, 264, 558, 2837, 337, 586, 13, 51414, 51414, 407, 286, 519, 321, 600, 1027, 3353, 4205, 294, 341, 7991, 11, 293, 286, 1454, 291, 4626, 300, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09657753120034428, "compression_ratio": 1.684981684981685, "no_speech_prob": 3.216504046577029e-05}, {"id": 1374, "seek": 693100, "start": 6944.0, "end": 6952.0, "text": " But at least we're making progress and at least we have some tools to tell us whether or not things are on the right track for now.", "tokens": [50364, 3432, 366, 920, 1382, 281, 2573, 484, 437, 307, 264, 1151, 636, 281, 5883, 1125, 613, 9590, 30, 50514, 50514, 708, 307, 264, 1151, 5623, 4978, 281, 764, 30, 50664, 50664, 400, 370, 322, 13, 407, 6022, 295, 341, 307, 534, 13041, 13, 50764, 50764, 400, 321, 500, 380, 534, 362, 439, 264, 6338, 281, 439, 264, 281, 439, 613, 3331, 13, 51014, 51014, 583, 412, 1935, 321, 434, 1455, 4205, 293, 412, 1935, 321, 362, 512, 3873, 281, 980, 505, 1968, 420, 406, 721, 366, 322, 264, 558, 2837, 337, 586, 13, 51414, 51414, 407, 286, 519, 321, 600, 1027, 3353, 4205, 294, 341, 7991, 11, 293, 286, 1454, 291, 4626, 300, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09657753120034428, "compression_ratio": 1.684981684981685, "no_speech_prob": 3.216504046577029e-05}, {"id": 1375, "seek": 693100, "start": 6952.0, "end": 6956.0, "text": " So I think we've made positive progress in this lecture, and I hope you enjoyed that.", "tokens": [50364, 3432, 366, 920, 1382, 281, 2573, 484, 437, 307, 264, 1151, 636, 281, 5883, 1125, 613, 9590, 30, 50514, 50514, 708, 307, 264, 1151, 5623, 4978, 281, 764, 30, 50664, 50664, 400, 370, 322, 13, 407, 6022, 295, 341, 307, 534, 13041, 13, 50764, 50764, 400, 321, 500, 380, 534, 362, 439, 264, 6338, 281, 439, 264, 281, 439, 613, 3331, 13, 51014, 51014, 583, 412, 1935, 321, 434, 1455, 4205, 293, 412, 1935, 321, 362, 512, 3873, 281, 980, 505, 1968, 420, 406, 721, 366, 322, 264, 558, 2837, 337, 586, 13, 51414, 51414, 407, 286, 519, 321, 600, 1027, 3353, 4205, 294, 341, 7991, 11, 293, 286, 1454, 291, 4626, 300, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09657753120034428, "compression_ratio": 1.684981684981685, "no_speech_prob": 3.216504046577029e-05}, {"id": 1376, "seek": 695600, "start": 6956.0, "end": 6961.0, "text": " And I will see you next time.", "tokens": [50364, 400, 286, 486, 536, 291, 958, 565, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14621015028520065, "compression_ratio": 0.7837837837837838, "no_speech_prob": 5.6420227338094264e-05}], "language": "en", "video_id": "P6sfmUTpUmc", "entity": "Andrew Kaparthy"}}