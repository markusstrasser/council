{"video_id": "D59jK8T9dfI", "title": "3.6 Gradient Descent | Gradient Descent Implementation  --[Machine Learning | Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 391, "views": 184, "publish_date": "11/04/2022", "timestamp": 1661299200, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " To fit the parameters of a logistic regression model, we're going to try to find the values of the parameters w and b that minimise the cost function j of w and b. And we're going to apply gradient descent to do this. Let's take a look at how. In this video, we'll focus on how to find a good choice of the parameters w and b. After you've done so, if you give the model a new input x, say a new patient at the hospital with a certain tumour size and age that needs a diagnosis, the model can then make a prediction or it can try to estimate the probability that the label y is 1. The algorithm you can use to minimise the cost function is gradient descent. Here again is the cost function, and so if you want to minimise the cost j as a function of w and b, well, here's the usual gradient descent algorithm where you repeatedly update each parameter as the O value minus alpha, the learning rate, times this derivative term. Let's take a look at the derivative of j with respect to wj, this term up on top here, where as usual j goes from 1 through n, where n is the number of features. If someone were to apply the rules of calculus, you can show that the derivative with respect to wj of the cost function capital J is equal to this expression over here. Is 1 over m times the sum from 1 through m of this error term, that is f minus the label y times xj. Here this xij is the j feature of training example i. Now let's also look at the derivative of j with respect to the parameter b. It turns out to be this expression over here, and it's quite similar to the expression above, except that it is not multiplied by this x super strip i subscript j at the end. So just as a reminder, similar to what you saw for linear regression, the way to carry out these updates is to use simultaneous updates, meaning that you would first compute the right hand side for all of these updates, and then simultaneously overwrite all the values on the left at the same time. So let me take these derivative expressions here and plug them into these terms here. This gives you gradient descent for logistic regression. Now one funny thing you might be wondering is, huh, that's weird. These two equations look exactly like the algorithm we had come up with previously for linear regression. So you might be wondering, is linear regression actually secretly the same as logistic regression? Well, even though these equations look the same, the reason that this is not linear regression is because the definition for the function f of x has changed. In linear regression, f of x is this is w x plus b, but in logistic regression, f of x is defined to be the sigmoid function applied to w x plus b. So although the algorithm written looked the same for both linear regression and logistic regression, actually they're two very different algorithms because the definition for f of x is not the same. When we talked about gradient descent for linear regression previously, you saw how you can monitor gradient descent to make sure it converges. You can just apply the same method for logistic regression to make sure it also converges. I've written out these updates as if you're updating the parameters w j one parameter at a time. Similar to the discussion on vectorized implementations of linear regression, you can also use vectorization to make gradient descent run faster for logistic regression. I won't dive into the details of the vectorized implementation in this video, but you can also learn more about it and see the code in the optional labs. So now you know how to implement gradient descent for logistic regression. You might also remember feature scaling when we were using linear regression, where you saw how feature scaling that is scaling all the features to take on similar ranges of values, say between negative one and plus one, how that can help gradient descent to converge faster. Feature scaling applied the same way to scale the different features to take on similar ranges of values can also speed up gradient descent for logistic regression. In the upcoming optional lab, you also see how the gradient for the regression can be calculated in code. This will be useful to look at because you also implement this in a practice lab at the end of this week. After you run gradient descent in this lab, there'll be a nice set of animated plots that show gradient descent in action. You see the sigmoid function, the control plot of the cost, the 3d surface plot to the cost and the learning curve all evolve as gradient descent runs. There will be another optional lab after that, which is short and sweet, but also very useful because they'll show you how to use the popular scikit-learn library to train the logistic regression model for classification. Many machine learning practitioners in many companies today use scikit-learn regularly as part of their job. And so I hope you check out the scikit-learn function as well and take a look at how that is used. So that's it. You now know how to implement logistic regression. This is a very powerful, very widely used learning algorithm and you now know how to get it to work yourself. Congratulations.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.28, "text": " To fit the parameters of a logistic regression model, we're going to try to find the values", "tokens": [50364, 1407, 3318, 264, 9834, 295, 257, 3565, 3142, 24590, 2316, 11, 321, 434, 516, 281, 853, 281, 915, 264, 4190, 50728, 50728, 295, 264, 9834, 261, 293, 272, 300, 4464, 908, 264, 2063, 2445, 361, 295, 261, 293, 272, 13, 51010, 51010, 400, 321, 434, 516, 281, 3079, 16235, 23475, 281, 360, 341, 13, 51184, 51184, 961, 311, 747, 257, 574, 412, 577, 13, 51302, 51302, 682, 341, 960, 11, 321, 603, 1879, 322, 577, 281, 915, 257, 665, 3922, 295, 264, 9834, 261, 293, 272, 13, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.13857255663190568, "compression_ratio": 1.6979166666666667, "no_speech_prob": 0.004674707539379597}, {"id": 1, "seek": 0, "start": 7.28, "end": 12.92, "text": " of the parameters w and b that minimise the cost function j of w and b.", "tokens": [50364, 1407, 3318, 264, 9834, 295, 257, 3565, 3142, 24590, 2316, 11, 321, 434, 516, 281, 853, 281, 915, 264, 4190, 50728, 50728, 295, 264, 9834, 261, 293, 272, 300, 4464, 908, 264, 2063, 2445, 361, 295, 261, 293, 272, 13, 51010, 51010, 400, 321, 434, 516, 281, 3079, 16235, 23475, 281, 360, 341, 13, 51184, 51184, 961, 311, 747, 257, 574, 412, 577, 13, 51302, 51302, 682, 341, 960, 11, 321, 603, 1879, 322, 577, 281, 915, 257, 665, 3922, 295, 264, 9834, 261, 293, 272, 13, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.13857255663190568, "compression_ratio": 1.6979166666666667, "no_speech_prob": 0.004674707539379597}, {"id": 2, "seek": 0, "start": 12.92, "end": 16.4, "text": " And we're going to apply gradient descent to do this.", "tokens": [50364, 1407, 3318, 264, 9834, 295, 257, 3565, 3142, 24590, 2316, 11, 321, 434, 516, 281, 853, 281, 915, 264, 4190, 50728, 50728, 295, 264, 9834, 261, 293, 272, 300, 4464, 908, 264, 2063, 2445, 361, 295, 261, 293, 272, 13, 51010, 51010, 400, 321, 434, 516, 281, 3079, 16235, 23475, 281, 360, 341, 13, 51184, 51184, 961, 311, 747, 257, 574, 412, 577, 13, 51302, 51302, 682, 341, 960, 11, 321, 603, 1879, 322, 577, 281, 915, 257, 665, 3922, 295, 264, 9834, 261, 293, 272, 13, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.13857255663190568, "compression_ratio": 1.6979166666666667, "no_speech_prob": 0.004674707539379597}, {"id": 3, "seek": 0, "start": 16.4, "end": 18.76, "text": " Let's take a look at how.", "tokens": [50364, 1407, 3318, 264, 9834, 295, 257, 3565, 3142, 24590, 2316, 11, 321, 434, 516, 281, 853, 281, 915, 264, 4190, 50728, 50728, 295, 264, 9834, 261, 293, 272, 300, 4464, 908, 264, 2063, 2445, 361, 295, 261, 293, 272, 13, 51010, 51010, 400, 321, 434, 516, 281, 3079, 16235, 23475, 281, 360, 341, 13, 51184, 51184, 961, 311, 747, 257, 574, 412, 577, 13, 51302, 51302, 682, 341, 960, 11, 321, 603, 1879, 322, 577, 281, 915, 257, 665, 3922, 295, 264, 9834, 261, 293, 272, 13, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.13857255663190568, "compression_ratio": 1.6979166666666667, "no_speech_prob": 0.004674707539379597}, {"id": 4, "seek": 0, "start": 18.76, "end": 24.8, "text": " In this video, we'll focus on how to find a good choice of the parameters w and b.", "tokens": [50364, 1407, 3318, 264, 9834, 295, 257, 3565, 3142, 24590, 2316, 11, 321, 434, 516, 281, 853, 281, 915, 264, 4190, 50728, 50728, 295, 264, 9834, 261, 293, 272, 300, 4464, 908, 264, 2063, 2445, 361, 295, 261, 293, 272, 13, 51010, 51010, 400, 321, 434, 516, 281, 3079, 16235, 23475, 281, 360, 341, 13, 51184, 51184, 961, 311, 747, 257, 574, 412, 577, 13, 51302, 51302, 682, 341, 960, 11, 321, 603, 1879, 322, 577, 281, 915, 257, 665, 3922, 295, 264, 9834, 261, 293, 272, 13, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.13857255663190568, "compression_ratio": 1.6979166666666667, "no_speech_prob": 0.004674707539379597}, {"id": 5, "seek": 2480, "start": 24.8, "end": 31.92, "text": " After you've done so, if you give the model a new input x, say a new patient at the hospital", "tokens": [50364, 2381, 291, 600, 1096, 370, 11, 498, 291, 976, 264, 2316, 257, 777, 4846, 2031, 11, 584, 257, 777, 4537, 412, 264, 4530, 50720, 50720, 365, 257, 1629, 13102, 396, 2744, 293, 3205, 300, 2203, 257, 15217, 11, 264, 2316, 393, 550, 652, 257, 17630, 51078, 51078, 420, 309, 393, 853, 281, 12539, 264, 8482, 300, 264, 7645, 288, 307, 502, 13, 51438, 51438, 440, 9284, 291, 393, 764, 281, 4464, 908, 264, 2063, 2445, 307, 16235, 23475, 13, 51699, 51699], "temperature": 0.0, "avg_logprob": -0.10069804593741176, "compression_ratio": 1.5789473684210527, "no_speech_prob": 9.22268009162508e-06}, {"id": 6, "seek": 2480, "start": 31.92, "end": 39.08, "text": " with a certain tumour size and age that needs a diagnosis, the model can then make a prediction", "tokens": [50364, 2381, 291, 600, 1096, 370, 11, 498, 291, 976, 264, 2316, 257, 777, 4846, 2031, 11, 584, 257, 777, 4537, 412, 264, 4530, 50720, 50720, 365, 257, 1629, 13102, 396, 2744, 293, 3205, 300, 2203, 257, 15217, 11, 264, 2316, 393, 550, 652, 257, 17630, 51078, 51078, 420, 309, 393, 853, 281, 12539, 264, 8482, 300, 264, 7645, 288, 307, 502, 13, 51438, 51438, 440, 9284, 291, 393, 764, 281, 4464, 908, 264, 2063, 2445, 307, 16235, 23475, 13, 51699, 51699], "temperature": 0.0, "avg_logprob": -0.10069804593741176, "compression_ratio": 1.5789473684210527, "no_speech_prob": 9.22268009162508e-06}, {"id": 7, "seek": 2480, "start": 39.08, "end": 46.28, "text": " or it can try to estimate the probability that the label y is 1.", "tokens": [50364, 2381, 291, 600, 1096, 370, 11, 498, 291, 976, 264, 2316, 257, 777, 4846, 2031, 11, 584, 257, 777, 4537, 412, 264, 4530, 50720, 50720, 365, 257, 1629, 13102, 396, 2744, 293, 3205, 300, 2203, 257, 15217, 11, 264, 2316, 393, 550, 652, 257, 17630, 51078, 51078, 420, 309, 393, 853, 281, 12539, 264, 8482, 300, 264, 7645, 288, 307, 502, 13, 51438, 51438, 440, 9284, 291, 393, 764, 281, 4464, 908, 264, 2063, 2445, 307, 16235, 23475, 13, 51699, 51699], "temperature": 0.0, "avg_logprob": -0.10069804593741176, "compression_ratio": 1.5789473684210527, "no_speech_prob": 9.22268009162508e-06}, {"id": 8, "seek": 2480, "start": 46.28, "end": 51.5, "text": " The algorithm you can use to minimise the cost function is gradient descent.", "tokens": [50364, 2381, 291, 600, 1096, 370, 11, 498, 291, 976, 264, 2316, 257, 777, 4846, 2031, 11, 584, 257, 777, 4537, 412, 264, 4530, 50720, 50720, 365, 257, 1629, 13102, 396, 2744, 293, 3205, 300, 2203, 257, 15217, 11, 264, 2316, 393, 550, 652, 257, 17630, 51078, 51078, 420, 309, 393, 853, 281, 12539, 264, 8482, 300, 264, 7645, 288, 307, 502, 13, 51438, 51438, 440, 9284, 291, 393, 764, 281, 4464, 908, 264, 2063, 2445, 307, 16235, 23475, 13, 51699, 51699], "temperature": 0.0, "avg_logprob": -0.10069804593741176, "compression_ratio": 1.5789473684210527, "no_speech_prob": 9.22268009162508e-06}, {"id": 9, "seek": 5150, "start": 51.5, "end": 58.24, "text": " Here again is the cost function, and so if you want to minimise the cost j as a function", "tokens": [50364, 1692, 797, 307, 264, 2063, 2445, 11, 293, 370, 498, 291, 528, 281, 4464, 908, 264, 2063, 361, 382, 257, 2445, 50701, 50701, 295, 261, 293, 272, 11, 731, 11, 510, 311, 264, 7713, 16235, 23475, 9284, 689, 291, 18227, 5623, 51071, 51071, 1184, 13075, 382, 264, 422, 2158, 3175, 8961, 11, 264, 2539, 3314, 11, 1413, 341, 13760, 1433, 13, 51555, 51555], "temperature": 0.0, "avg_logprob": -0.12344866532545823, "compression_ratio": 1.5056179775280898, "no_speech_prob": 3.5558955460146535e-06}, {"id": 10, "seek": 5150, "start": 58.24, "end": 65.64, "text": " of w and b, well, here's the usual gradient descent algorithm where you repeatedly update", "tokens": [50364, 1692, 797, 307, 264, 2063, 2445, 11, 293, 370, 498, 291, 528, 281, 4464, 908, 264, 2063, 361, 382, 257, 2445, 50701, 50701, 295, 261, 293, 272, 11, 731, 11, 510, 311, 264, 7713, 16235, 23475, 9284, 689, 291, 18227, 5623, 51071, 51071, 1184, 13075, 382, 264, 422, 2158, 3175, 8961, 11, 264, 2539, 3314, 11, 1413, 341, 13760, 1433, 13, 51555, 51555], "temperature": 0.0, "avg_logprob": -0.12344866532545823, "compression_ratio": 1.5056179775280898, "no_speech_prob": 3.5558955460146535e-06}, {"id": 11, "seek": 5150, "start": 65.64, "end": 75.32, "text": " each parameter as the O value minus alpha, the learning rate, times this derivative term.", "tokens": [50364, 1692, 797, 307, 264, 2063, 2445, 11, 293, 370, 498, 291, 528, 281, 4464, 908, 264, 2063, 361, 382, 257, 2445, 50701, 50701, 295, 261, 293, 272, 11, 731, 11, 510, 311, 264, 7713, 16235, 23475, 9284, 689, 291, 18227, 5623, 51071, 51071, 1184, 13075, 382, 264, 422, 2158, 3175, 8961, 11, 264, 2539, 3314, 11, 1413, 341, 13760, 1433, 13, 51555, 51555], "temperature": 0.0, "avg_logprob": -0.12344866532545823, "compression_ratio": 1.5056179775280898, "no_speech_prob": 3.5558955460146535e-06}, {"id": 12, "seek": 7532, "start": 75.32, "end": 82.55999999999999, "text": " Let's take a look at the derivative of j with respect to wj, this term up on top here, where", "tokens": [50364, 961, 311, 747, 257, 574, 412, 264, 13760, 295, 361, 365, 3104, 281, 261, 73, 11, 341, 1433, 493, 322, 1192, 510, 11, 689, 50726, 50726, 382, 7713, 361, 1709, 490, 502, 807, 297, 11, 689, 297, 307, 264, 1230, 295, 4122, 13, 51039, 51039, 759, 1580, 645, 281, 3079, 264, 4474, 295, 33400, 11, 291, 393, 855, 300, 264, 13760, 365, 3104, 51324, 51324, 281, 261, 73, 295, 264, 2063, 2445, 4238, 508, 307, 2681, 281, 341, 6114, 670, 510, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.0990105340647143, "compression_ratio": 1.6386138613861385, "no_speech_prob": 1.4823366427663132e-06}, {"id": 13, "seek": 7532, "start": 82.55999999999999, "end": 88.82, "text": " as usual j goes from 1 through n, where n is the number of features.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 264, 13760, 295, 361, 365, 3104, 281, 261, 73, 11, 341, 1433, 493, 322, 1192, 510, 11, 689, 50726, 50726, 382, 7713, 361, 1709, 490, 502, 807, 297, 11, 689, 297, 307, 264, 1230, 295, 4122, 13, 51039, 51039, 759, 1580, 645, 281, 3079, 264, 4474, 295, 33400, 11, 291, 393, 855, 300, 264, 13760, 365, 3104, 51324, 51324, 281, 261, 73, 295, 264, 2063, 2445, 4238, 508, 307, 2681, 281, 341, 6114, 670, 510, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.0990105340647143, "compression_ratio": 1.6386138613861385, "no_speech_prob": 1.4823366427663132e-06}, {"id": 14, "seek": 7532, "start": 88.82, "end": 94.52, "text": " If someone were to apply the rules of calculus, you can show that the derivative with respect", "tokens": [50364, 961, 311, 747, 257, 574, 412, 264, 13760, 295, 361, 365, 3104, 281, 261, 73, 11, 341, 1433, 493, 322, 1192, 510, 11, 689, 50726, 50726, 382, 7713, 361, 1709, 490, 502, 807, 297, 11, 689, 297, 307, 264, 1230, 295, 4122, 13, 51039, 51039, 759, 1580, 645, 281, 3079, 264, 4474, 295, 33400, 11, 291, 393, 855, 300, 264, 13760, 365, 3104, 51324, 51324, 281, 261, 73, 295, 264, 2063, 2445, 4238, 508, 307, 2681, 281, 341, 6114, 670, 510, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.0990105340647143, "compression_ratio": 1.6386138613861385, "no_speech_prob": 1.4823366427663132e-06}, {"id": 15, "seek": 7532, "start": 94.52, "end": 101.35999999999999, "text": " to wj of the cost function capital J is equal to this expression over here.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 264, 13760, 295, 361, 365, 3104, 281, 261, 73, 11, 341, 1433, 493, 322, 1192, 510, 11, 689, 50726, 50726, 382, 7713, 361, 1709, 490, 502, 807, 297, 11, 689, 297, 307, 264, 1230, 295, 4122, 13, 51039, 51039, 759, 1580, 645, 281, 3079, 264, 4474, 295, 33400, 11, 291, 393, 855, 300, 264, 13760, 365, 3104, 51324, 51324, 281, 261, 73, 295, 264, 2063, 2445, 4238, 508, 307, 2681, 281, 341, 6114, 670, 510, 13, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.0990105340647143, "compression_ratio": 1.6386138613861385, "no_speech_prob": 1.4823366427663132e-06}, {"id": 16, "seek": 10136, "start": 101.36, "end": 111.64, "text": " Is 1 over m times the sum from 1 through m of this error term, that is f minus the label", "tokens": [50364, 1119, 502, 670, 275, 1413, 264, 2408, 490, 502, 807, 275, 295, 341, 6713, 1433, 11, 300, 307, 283, 3175, 264, 7645, 50878, 50878, 288, 1413, 2031, 73, 13, 51144, 51144, 1692, 341, 2031, 1718, 307, 264, 361, 4111, 295, 3097, 1365, 741, 13, 51468, 51468, 823, 718, 311, 611, 574, 412, 264, 13760, 295, 361, 365, 3104, 281, 264, 13075, 272, 13, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.16482626502193623, "compression_ratio": 1.4375, "no_speech_prob": 3.844884304271545e-06}, {"id": 17, "seek": 10136, "start": 111.64, "end": 116.96, "text": " y times xj.", "tokens": [50364, 1119, 502, 670, 275, 1413, 264, 2408, 490, 502, 807, 275, 295, 341, 6713, 1433, 11, 300, 307, 283, 3175, 264, 7645, 50878, 50878, 288, 1413, 2031, 73, 13, 51144, 51144, 1692, 341, 2031, 1718, 307, 264, 361, 4111, 295, 3097, 1365, 741, 13, 51468, 51468, 823, 718, 311, 611, 574, 412, 264, 13760, 295, 361, 365, 3104, 281, 264, 13075, 272, 13, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.16482626502193623, "compression_ratio": 1.4375, "no_speech_prob": 3.844884304271545e-06}, {"id": 18, "seek": 10136, "start": 116.96, "end": 123.44, "text": " Here this xij is the j feature of training example i.", "tokens": [50364, 1119, 502, 670, 275, 1413, 264, 2408, 490, 502, 807, 275, 295, 341, 6713, 1433, 11, 300, 307, 283, 3175, 264, 7645, 50878, 50878, 288, 1413, 2031, 73, 13, 51144, 51144, 1692, 341, 2031, 1718, 307, 264, 361, 4111, 295, 3097, 1365, 741, 13, 51468, 51468, 823, 718, 311, 611, 574, 412, 264, 13760, 295, 361, 365, 3104, 281, 264, 13075, 272, 13, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.16482626502193623, "compression_ratio": 1.4375, "no_speech_prob": 3.844884304271545e-06}, {"id": 19, "seek": 10136, "start": 123.44, "end": 129.07999999999998, "text": " Now let's also look at the derivative of j with respect to the parameter b.", "tokens": [50364, 1119, 502, 670, 275, 1413, 264, 2408, 490, 502, 807, 275, 295, 341, 6713, 1433, 11, 300, 307, 283, 3175, 264, 7645, 50878, 50878, 288, 1413, 2031, 73, 13, 51144, 51144, 1692, 341, 2031, 1718, 307, 264, 361, 4111, 295, 3097, 1365, 741, 13, 51468, 51468, 823, 718, 311, 611, 574, 412, 264, 13760, 295, 361, 365, 3104, 281, 264, 13075, 272, 13, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.16482626502193623, "compression_ratio": 1.4375, "no_speech_prob": 3.844884304271545e-06}, {"id": 20, "seek": 12908, "start": 129.08, "end": 135.4, "text": " It turns out to be this expression over here, and it's quite similar to the expression above,", "tokens": [50364, 467, 4523, 484, 281, 312, 341, 6114, 670, 510, 11, 293, 309, 311, 1596, 2531, 281, 264, 6114, 3673, 11, 50680, 50680, 3993, 300, 309, 307, 406, 17207, 538, 341, 2031, 1687, 12828, 741, 2325, 662, 361, 412, 264, 917, 13, 51034, 51034, 407, 445, 382, 257, 13548, 11, 2531, 281, 437, 291, 1866, 337, 8213, 24590, 11, 264, 636, 281, 3985, 51278, 51278, 484, 613, 9205, 307, 281, 764, 46218, 9205, 11, 3620, 300, 291, 576, 700, 14722, 264, 558, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09352756949032054, "compression_ratio": 1.6574074074074074, "no_speech_prob": 4.56582256447291e-06}, {"id": 21, "seek": 12908, "start": 135.4, "end": 142.48000000000002, "text": " except that it is not multiplied by this x super strip i subscript j at the end.", "tokens": [50364, 467, 4523, 484, 281, 312, 341, 6114, 670, 510, 11, 293, 309, 311, 1596, 2531, 281, 264, 6114, 3673, 11, 50680, 50680, 3993, 300, 309, 307, 406, 17207, 538, 341, 2031, 1687, 12828, 741, 2325, 662, 361, 412, 264, 917, 13, 51034, 51034, 407, 445, 382, 257, 13548, 11, 2531, 281, 437, 291, 1866, 337, 8213, 24590, 11, 264, 636, 281, 3985, 51278, 51278, 484, 613, 9205, 307, 281, 764, 46218, 9205, 11, 3620, 300, 291, 576, 700, 14722, 264, 558, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09352756949032054, "compression_ratio": 1.6574074074074074, "no_speech_prob": 4.56582256447291e-06}, {"id": 22, "seek": 12908, "start": 142.48000000000002, "end": 147.36, "text": " So just as a reminder, similar to what you saw for linear regression, the way to carry", "tokens": [50364, 467, 4523, 484, 281, 312, 341, 6114, 670, 510, 11, 293, 309, 311, 1596, 2531, 281, 264, 6114, 3673, 11, 50680, 50680, 3993, 300, 309, 307, 406, 17207, 538, 341, 2031, 1687, 12828, 741, 2325, 662, 361, 412, 264, 917, 13, 51034, 51034, 407, 445, 382, 257, 13548, 11, 2531, 281, 437, 291, 1866, 337, 8213, 24590, 11, 264, 636, 281, 3985, 51278, 51278, 484, 613, 9205, 307, 281, 764, 46218, 9205, 11, 3620, 300, 291, 576, 700, 14722, 264, 558, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09352756949032054, "compression_ratio": 1.6574074074074074, "no_speech_prob": 4.56582256447291e-06}, {"id": 23, "seek": 12908, "start": 147.36, "end": 153.08, "text": " out these updates is to use simultaneous updates, meaning that you would first compute the right", "tokens": [50364, 467, 4523, 484, 281, 312, 341, 6114, 670, 510, 11, 293, 309, 311, 1596, 2531, 281, 264, 6114, 3673, 11, 50680, 50680, 3993, 300, 309, 307, 406, 17207, 538, 341, 2031, 1687, 12828, 741, 2325, 662, 361, 412, 264, 917, 13, 51034, 51034, 407, 445, 382, 257, 13548, 11, 2531, 281, 437, 291, 1866, 337, 8213, 24590, 11, 264, 636, 281, 3985, 51278, 51278, 484, 613, 9205, 307, 281, 764, 46218, 9205, 11, 3620, 300, 291, 576, 700, 14722, 264, 558, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.09352756949032054, "compression_ratio": 1.6574074074074074, "no_speech_prob": 4.56582256447291e-06}, {"id": 24, "seek": 15308, "start": 153.08, "end": 159.60000000000002, "text": " hand side for all of these updates, and then simultaneously overwrite all the values on", "tokens": [50364, 1011, 1252, 337, 439, 295, 613, 9205, 11, 293, 550, 16561, 670, 21561, 439, 264, 4190, 322, 50690, 50690, 264, 1411, 412, 264, 912, 565, 13, 50854, 50854, 407, 718, 385, 747, 613, 13760, 15277, 510, 293, 5452, 552, 666, 613, 2115, 510, 13, 51274, 51274, 639, 2709, 291, 16235, 23475, 337, 3565, 3142, 24590, 13, 51585, 51585, 823, 472, 4074, 551, 291, 1062, 312, 6359, 307, 11, 7020, 11, 300, 311, 3657, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.09288804959028195, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.5446077668457292e-05}, {"id": 25, "seek": 15308, "start": 159.60000000000002, "end": 162.88000000000002, "text": " the left at the same time.", "tokens": [50364, 1011, 1252, 337, 439, 295, 613, 9205, 11, 293, 550, 16561, 670, 21561, 439, 264, 4190, 322, 50690, 50690, 264, 1411, 412, 264, 912, 565, 13, 50854, 50854, 407, 718, 385, 747, 613, 13760, 15277, 510, 293, 5452, 552, 666, 613, 2115, 510, 13, 51274, 51274, 639, 2709, 291, 16235, 23475, 337, 3565, 3142, 24590, 13, 51585, 51585, 823, 472, 4074, 551, 291, 1062, 312, 6359, 307, 11, 7020, 11, 300, 311, 3657, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.09288804959028195, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.5446077668457292e-05}, {"id": 26, "seek": 15308, "start": 162.88000000000002, "end": 171.28, "text": " So let me take these derivative expressions here and plug them into these terms here.", "tokens": [50364, 1011, 1252, 337, 439, 295, 613, 9205, 11, 293, 550, 16561, 670, 21561, 439, 264, 4190, 322, 50690, 50690, 264, 1411, 412, 264, 912, 565, 13, 50854, 50854, 407, 718, 385, 747, 613, 13760, 15277, 510, 293, 5452, 552, 666, 613, 2115, 510, 13, 51274, 51274, 639, 2709, 291, 16235, 23475, 337, 3565, 3142, 24590, 13, 51585, 51585, 823, 472, 4074, 551, 291, 1062, 312, 6359, 307, 11, 7020, 11, 300, 311, 3657, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.09288804959028195, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.5446077668457292e-05}, {"id": 27, "seek": 15308, "start": 171.28, "end": 177.5, "text": " This gives you gradient descent for logistic regression.", "tokens": [50364, 1011, 1252, 337, 439, 295, 613, 9205, 11, 293, 550, 16561, 670, 21561, 439, 264, 4190, 322, 50690, 50690, 264, 1411, 412, 264, 912, 565, 13, 50854, 50854, 407, 718, 385, 747, 613, 13760, 15277, 510, 293, 5452, 552, 666, 613, 2115, 510, 13, 51274, 51274, 639, 2709, 291, 16235, 23475, 337, 3565, 3142, 24590, 13, 51585, 51585, 823, 472, 4074, 551, 291, 1062, 312, 6359, 307, 11, 7020, 11, 300, 311, 3657, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.09288804959028195, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.5446077668457292e-05}, {"id": 28, "seek": 15308, "start": 177.5, "end": 182.52, "text": " Now one funny thing you might be wondering is, huh, that's weird.", "tokens": [50364, 1011, 1252, 337, 439, 295, 613, 9205, 11, 293, 550, 16561, 670, 21561, 439, 264, 4190, 322, 50690, 50690, 264, 1411, 412, 264, 912, 565, 13, 50854, 50854, 407, 718, 385, 747, 613, 13760, 15277, 510, 293, 5452, 552, 666, 613, 2115, 510, 13, 51274, 51274, 639, 2709, 291, 16235, 23475, 337, 3565, 3142, 24590, 13, 51585, 51585, 823, 472, 4074, 551, 291, 1062, 312, 6359, 307, 11, 7020, 11, 300, 311, 3657, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.09288804959028195, "compression_ratio": 1.5603864734299517, "no_speech_prob": 1.5446077668457292e-05}, {"id": 29, "seek": 18252, "start": 182.52, "end": 186.92000000000002, "text": " These two equations look exactly like the algorithm we had come up with previously for", "tokens": [50364, 1981, 732, 11787, 574, 2293, 411, 264, 9284, 321, 632, 808, 493, 365, 8046, 337, 50584, 50584, 8213, 24590, 13, 50676, 50676, 407, 291, 1062, 312, 6359, 11, 307, 8213, 24590, 767, 22611, 264, 912, 382, 3565, 3142, 24590, 30, 50962, 50962, 1042, 11, 754, 1673, 613, 11787, 574, 264, 912, 11, 264, 1778, 300, 341, 307, 406, 8213, 24590, 51314, 51314, 307, 570, 264, 7123, 337, 264, 2445, 283, 295, 2031, 575, 3105, 13, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.08913486818724041, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.89245656656567e-05}, {"id": 30, "seek": 18252, "start": 186.92000000000002, "end": 188.76000000000002, "text": " linear regression.", "tokens": [50364, 1981, 732, 11787, 574, 2293, 411, 264, 9284, 321, 632, 808, 493, 365, 8046, 337, 50584, 50584, 8213, 24590, 13, 50676, 50676, 407, 291, 1062, 312, 6359, 11, 307, 8213, 24590, 767, 22611, 264, 912, 382, 3565, 3142, 24590, 30, 50962, 50962, 1042, 11, 754, 1673, 613, 11787, 574, 264, 912, 11, 264, 1778, 300, 341, 307, 406, 8213, 24590, 51314, 51314, 307, 570, 264, 7123, 337, 264, 2445, 283, 295, 2031, 575, 3105, 13, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.08913486818724041, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.89245656656567e-05}, {"id": 31, "seek": 18252, "start": 188.76000000000002, "end": 194.48000000000002, "text": " So you might be wondering, is linear regression actually secretly the same as logistic regression?", "tokens": [50364, 1981, 732, 11787, 574, 2293, 411, 264, 9284, 321, 632, 808, 493, 365, 8046, 337, 50584, 50584, 8213, 24590, 13, 50676, 50676, 407, 291, 1062, 312, 6359, 11, 307, 8213, 24590, 767, 22611, 264, 912, 382, 3565, 3142, 24590, 30, 50962, 50962, 1042, 11, 754, 1673, 613, 11787, 574, 264, 912, 11, 264, 1778, 300, 341, 307, 406, 8213, 24590, 51314, 51314, 307, 570, 264, 7123, 337, 264, 2445, 283, 295, 2031, 575, 3105, 13, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.08913486818724041, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.89245656656567e-05}, {"id": 32, "seek": 18252, "start": 194.48000000000002, "end": 201.52, "text": " Well, even though these equations look the same, the reason that this is not linear regression", "tokens": [50364, 1981, 732, 11787, 574, 2293, 411, 264, 9284, 321, 632, 808, 493, 365, 8046, 337, 50584, 50584, 8213, 24590, 13, 50676, 50676, 407, 291, 1062, 312, 6359, 11, 307, 8213, 24590, 767, 22611, 264, 912, 382, 3565, 3142, 24590, 30, 50962, 50962, 1042, 11, 754, 1673, 613, 11787, 574, 264, 912, 11, 264, 1778, 300, 341, 307, 406, 8213, 24590, 51314, 51314, 307, 570, 264, 7123, 337, 264, 2445, 283, 295, 2031, 575, 3105, 13, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.08913486818724041, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.89245656656567e-05}, {"id": 33, "seek": 18252, "start": 201.52, "end": 206.92000000000002, "text": " is because the definition for the function f of x has changed.", "tokens": [50364, 1981, 732, 11787, 574, 2293, 411, 264, 9284, 321, 632, 808, 493, 365, 8046, 337, 50584, 50584, 8213, 24590, 13, 50676, 50676, 407, 291, 1062, 312, 6359, 11, 307, 8213, 24590, 767, 22611, 264, 912, 382, 3565, 3142, 24590, 30, 50962, 50962, 1042, 11, 754, 1673, 613, 11787, 574, 264, 912, 11, 264, 1778, 300, 341, 307, 406, 8213, 24590, 51314, 51314, 307, 570, 264, 7123, 337, 264, 2445, 283, 295, 2031, 575, 3105, 13, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.08913486818724041, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.89245656656567e-05}, {"id": 34, "seek": 20692, "start": 206.92, "end": 214.16, "text": " In linear regression, f of x is this is w x plus b, but in logistic regression, f of", "tokens": [50364, 682, 8213, 24590, 11, 283, 295, 2031, 307, 341, 307, 261, 2031, 1804, 272, 11, 457, 294, 3565, 3142, 24590, 11, 283, 295, 50726, 50726, 2031, 307, 7642, 281, 312, 264, 4556, 3280, 327, 2445, 6456, 281, 261, 2031, 1804, 272, 13, 51028, 51028, 407, 4878, 264, 9284, 3720, 2956, 264, 912, 337, 1293, 8213, 24590, 293, 3565, 3142, 51296, 51296, 24590, 11, 767, 436, 434, 732, 588, 819, 14642, 570, 264, 7123, 337, 283, 295, 51568, 51568, 2031, 307, 406, 264, 912, 13, 51718, 51718], "temperature": 0.0, "avg_logprob": -0.1446514888243242, "compression_ratio": 1.8263157894736841, "no_speech_prob": 2.1567927888099803e-06}, {"id": 35, "seek": 20692, "start": 214.16, "end": 220.2, "text": " x is defined to be the sigmoid function applied to w x plus b.", "tokens": [50364, 682, 8213, 24590, 11, 283, 295, 2031, 307, 341, 307, 261, 2031, 1804, 272, 11, 457, 294, 3565, 3142, 24590, 11, 283, 295, 50726, 50726, 2031, 307, 7642, 281, 312, 264, 4556, 3280, 327, 2445, 6456, 281, 261, 2031, 1804, 272, 13, 51028, 51028, 407, 4878, 264, 9284, 3720, 2956, 264, 912, 337, 1293, 8213, 24590, 293, 3565, 3142, 51296, 51296, 24590, 11, 767, 436, 434, 732, 588, 819, 14642, 570, 264, 7123, 337, 283, 295, 51568, 51568, 2031, 307, 406, 264, 912, 13, 51718, 51718], "temperature": 0.0, "avg_logprob": -0.1446514888243242, "compression_ratio": 1.8263157894736841, "no_speech_prob": 2.1567927888099803e-06}, {"id": 36, "seek": 20692, "start": 220.2, "end": 225.56, "text": " So although the algorithm written looked the same for both linear regression and logistic", "tokens": [50364, 682, 8213, 24590, 11, 283, 295, 2031, 307, 341, 307, 261, 2031, 1804, 272, 11, 457, 294, 3565, 3142, 24590, 11, 283, 295, 50726, 50726, 2031, 307, 7642, 281, 312, 264, 4556, 3280, 327, 2445, 6456, 281, 261, 2031, 1804, 272, 13, 51028, 51028, 407, 4878, 264, 9284, 3720, 2956, 264, 912, 337, 1293, 8213, 24590, 293, 3565, 3142, 51296, 51296, 24590, 11, 767, 436, 434, 732, 588, 819, 14642, 570, 264, 7123, 337, 283, 295, 51568, 51568, 2031, 307, 406, 264, 912, 13, 51718, 51718], "temperature": 0.0, "avg_logprob": -0.1446514888243242, "compression_ratio": 1.8263157894736841, "no_speech_prob": 2.1567927888099803e-06}, {"id": 37, "seek": 20692, "start": 225.56, "end": 231.0, "text": " regression, actually they're two very different algorithms because the definition for f of", "tokens": [50364, 682, 8213, 24590, 11, 283, 295, 2031, 307, 341, 307, 261, 2031, 1804, 272, 11, 457, 294, 3565, 3142, 24590, 11, 283, 295, 50726, 50726, 2031, 307, 7642, 281, 312, 264, 4556, 3280, 327, 2445, 6456, 281, 261, 2031, 1804, 272, 13, 51028, 51028, 407, 4878, 264, 9284, 3720, 2956, 264, 912, 337, 1293, 8213, 24590, 293, 3565, 3142, 51296, 51296, 24590, 11, 767, 436, 434, 732, 588, 819, 14642, 570, 264, 7123, 337, 283, 295, 51568, 51568, 2031, 307, 406, 264, 912, 13, 51718, 51718], "temperature": 0.0, "avg_logprob": -0.1446514888243242, "compression_ratio": 1.8263157894736841, "no_speech_prob": 2.1567927888099803e-06}, {"id": 38, "seek": 20692, "start": 231.0, "end": 234.0, "text": " x is not the same.", "tokens": [50364, 682, 8213, 24590, 11, 283, 295, 2031, 307, 341, 307, 261, 2031, 1804, 272, 11, 457, 294, 3565, 3142, 24590, 11, 283, 295, 50726, 50726, 2031, 307, 7642, 281, 312, 264, 4556, 3280, 327, 2445, 6456, 281, 261, 2031, 1804, 272, 13, 51028, 51028, 407, 4878, 264, 9284, 3720, 2956, 264, 912, 337, 1293, 8213, 24590, 293, 3565, 3142, 51296, 51296, 24590, 11, 767, 436, 434, 732, 588, 819, 14642, 570, 264, 7123, 337, 283, 295, 51568, 51568, 2031, 307, 406, 264, 912, 13, 51718, 51718], "temperature": 0.0, "avg_logprob": -0.1446514888243242, "compression_ratio": 1.8263157894736841, "no_speech_prob": 2.1567927888099803e-06}, {"id": 39, "seek": 23400, "start": 234.0, "end": 238.92, "text": " When we talked about gradient descent for linear regression previously, you saw how", "tokens": [50364, 1133, 321, 2825, 466, 16235, 23475, 337, 8213, 24590, 8046, 11, 291, 1866, 577, 50610, 50610, 291, 393, 6002, 16235, 23475, 281, 652, 988, 309, 9652, 2880, 13, 50832, 50832, 509, 393, 445, 3079, 264, 912, 3170, 337, 3565, 3142, 24590, 281, 652, 988, 309, 611, 9652, 2880, 13, 51190, 51190, 286, 600, 3720, 484, 613, 9205, 382, 498, 291, 434, 25113, 264, 9834, 261, 361, 472, 13075, 51524, 51524, 412, 257, 565, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.10873057292057918, "compression_ratio": 1.6974358974358974, "no_speech_prob": 9.276308787775633e-07}, {"id": 40, "seek": 23400, "start": 238.92, "end": 243.36, "text": " you can monitor gradient descent to make sure it converges.", "tokens": [50364, 1133, 321, 2825, 466, 16235, 23475, 337, 8213, 24590, 8046, 11, 291, 1866, 577, 50610, 50610, 291, 393, 6002, 16235, 23475, 281, 652, 988, 309, 9652, 2880, 13, 50832, 50832, 509, 393, 445, 3079, 264, 912, 3170, 337, 3565, 3142, 24590, 281, 652, 988, 309, 611, 9652, 2880, 13, 51190, 51190, 286, 600, 3720, 484, 613, 9205, 382, 498, 291, 434, 25113, 264, 9834, 261, 361, 472, 13075, 51524, 51524, 412, 257, 565, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.10873057292057918, "compression_ratio": 1.6974358974358974, "no_speech_prob": 9.276308787775633e-07}, {"id": 41, "seek": 23400, "start": 243.36, "end": 250.52, "text": " You can just apply the same method for logistic regression to make sure it also converges.", "tokens": [50364, 1133, 321, 2825, 466, 16235, 23475, 337, 8213, 24590, 8046, 11, 291, 1866, 577, 50610, 50610, 291, 393, 6002, 16235, 23475, 281, 652, 988, 309, 9652, 2880, 13, 50832, 50832, 509, 393, 445, 3079, 264, 912, 3170, 337, 3565, 3142, 24590, 281, 652, 988, 309, 611, 9652, 2880, 13, 51190, 51190, 286, 600, 3720, 484, 613, 9205, 382, 498, 291, 434, 25113, 264, 9834, 261, 361, 472, 13075, 51524, 51524, 412, 257, 565, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.10873057292057918, "compression_ratio": 1.6974358974358974, "no_speech_prob": 9.276308787775633e-07}, {"id": 42, "seek": 23400, "start": 250.52, "end": 257.2, "text": " I've written out these updates as if you're updating the parameters w j one parameter", "tokens": [50364, 1133, 321, 2825, 466, 16235, 23475, 337, 8213, 24590, 8046, 11, 291, 1866, 577, 50610, 50610, 291, 393, 6002, 16235, 23475, 281, 652, 988, 309, 9652, 2880, 13, 50832, 50832, 509, 393, 445, 3079, 264, 912, 3170, 337, 3565, 3142, 24590, 281, 652, 988, 309, 611, 9652, 2880, 13, 51190, 51190, 286, 600, 3720, 484, 613, 9205, 382, 498, 291, 434, 25113, 264, 9834, 261, 361, 472, 13075, 51524, 51524, 412, 257, 565, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.10873057292057918, "compression_ratio": 1.6974358974358974, "no_speech_prob": 9.276308787775633e-07}, {"id": 43, "seek": 23400, "start": 257.2, "end": 260.96, "text": " at a time.", "tokens": [50364, 1133, 321, 2825, 466, 16235, 23475, 337, 8213, 24590, 8046, 11, 291, 1866, 577, 50610, 50610, 291, 393, 6002, 16235, 23475, 281, 652, 988, 309, 9652, 2880, 13, 50832, 50832, 509, 393, 445, 3079, 264, 912, 3170, 337, 3565, 3142, 24590, 281, 652, 988, 309, 611, 9652, 2880, 13, 51190, 51190, 286, 600, 3720, 484, 613, 9205, 382, 498, 291, 434, 25113, 264, 9834, 261, 361, 472, 13075, 51524, 51524, 412, 257, 565, 13, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.10873057292057918, "compression_ratio": 1.6974358974358974, "no_speech_prob": 9.276308787775633e-07}, {"id": 44, "seek": 26096, "start": 260.96, "end": 269.15999999999997, "text": " Similar to the discussion on vectorized implementations of linear regression, you can also use vectorization", "tokens": [50364, 10905, 281, 264, 5017, 322, 8062, 1602, 4445, 763, 295, 8213, 24590, 11, 291, 393, 611, 764, 8062, 2144, 50774, 50774, 281, 652, 16235, 23475, 1190, 4663, 337, 3565, 3142, 24590, 13, 51004, 51004, 286, 1582, 380, 9192, 666, 264, 4365, 295, 264, 8062, 1602, 11420, 294, 341, 960, 11, 457, 291, 393, 51254, 51254, 611, 1466, 544, 466, 309, 293, 536, 264, 3089, 294, 264, 17312, 20339, 13, 51474, 51474, 407, 586, 291, 458, 577, 281, 4445, 16235, 23475, 337, 3565, 3142, 24590, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.06587253527694874, "compression_ratio": 1.9134615384615385, "no_speech_prob": 6.3390029936272185e-06}, {"id": 45, "seek": 26096, "start": 269.15999999999997, "end": 273.76, "text": " to make gradient descent run faster for logistic regression.", "tokens": [50364, 10905, 281, 264, 5017, 322, 8062, 1602, 4445, 763, 295, 8213, 24590, 11, 291, 393, 611, 764, 8062, 2144, 50774, 50774, 281, 652, 16235, 23475, 1190, 4663, 337, 3565, 3142, 24590, 13, 51004, 51004, 286, 1582, 380, 9192, 666, 264, 4365, 295, 264, 8062, 1602, 11420, 294, 341, 960, 11, 457, 291, 393, 51254, 51254, 611, 1466, 544, 466, 309, 293, 536, 264, 3089, 294, 264, 17312, 20339, 13, 51474, 51474, 407, 586, 291, 458, 577, 281, 4445, 16235, 23475, 337, 3565, 3142, 24590, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.06587253527694874, "compression_ratio": 1.9134615384615385, "no_speech_prob": 6.3390029936272185e-06}, {"id": 46, "seek": 26096, "start": 273.76, "end": 278.76, "text": " I won't dive into the details of the vectorized implementation in this video, but you can", "tokens": [50364, 10905, 281, 264, 5017, 322, 8062, 1602, 4445, 763, 295, 8213, 24590, 11, 291, 393, 611, 764, 8062, 2144, 50774, 50774, 281, 652, 16235, 23475, 1190, 4663, 337, 3565, 3142, 24590, 13, 51004, 51004, 286, 1582, 380, 9192, 666, 264, 4365, 295, 264, 8062, 1602, 11420, 294, 341, 960, 11, 457, 291, 393, 51254, 51254, 611, 1466, 544, 466, 309, 293, 536, 264, 3089, 294, 264, 17312, 20339, 13, 51474, 51474, 407, 586, 291, 458, 577, 281, 4445, 16235, 23475, 337, 3565, 3142, 24590, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.06587253527694874, "compression_ratio": 1.9134615384615385, "no_speech_prob": 6.3390029936272185e-06}, {"id": 47, "seek": 26096, "start": 278.76, "end": 283.15999999999997, "text": " also learn more about it and see the code in the optional labs.", "tokens": [50364, 10905, 281, 264, 5017, 322, 8062, 1602, 4445, 763, 295, 8213, 24590, 11, 291, 393, 611, 764, 8062, 2144, 50774, 50774, 281, 652, 16235, 23475, 1190, 4663, 337, 3565, 3142, 24590, 13, 51004, 51004, 286, 1582, 380, 9192, 666, 264, 4365, 295, 264, 8062, 1602, 11420, 294, 341, 960, 11, 457, 291, 393, 51254, 51254, 611, 1466, 544, 466, 309, 293, 536, 264, 3089, 294, 264, 17312, 20339, 13, 51474, 51474, 407, 586, 291, 458, 577, 281, 4445, 16235, 23475, 337, 3565, 3142, 24590, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.06587253527694874, "compression_ratio": 1.9134615384615385, "no_speech_prob": 6.3390029936272185e-06}, {"id": 48, "seek": 26096, "start": 283.15999999999997, "end": 289.12, "text": " So now you know how to implement gradient descent for logistic regression.", "tokens": [50364, 10905, 281, 264, 5017, 322, 8062, 1602, 4445, 763, 295, 8213, 24590, 11, 291, 393, 611, 764, 8062, 2144, 50774, 50774, 281, 652, 16235, 23475, 1190, 4663, 337, 3565, 3142, 24590, 13, 51004, 51004, 286, 1582, 380, 9192, 666, 264, 4365, 295, 264, 8062, 1602, 11420, 294, 341, 960, 11, 457, 291, 393, 51254, 51254, 611, 1466, 544, 466, 309, 293, 536, 264, 3089, 294, 264, 17312, 20339, 13, 51474, 51474, 407, 586, 291, 458, 577, 281, 4445, 16235, 23475, 337, 3565, 3142, 24590, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.06587253527694874, "compression_ratio": 1.9134615384615385, "no_speech_prob": 6.3390029936272185e-06}, {"id": 49, "seek": 28912, "start": 289.12, "end": 294.72, "text": " You might also remember feature scaling when we were using linear regression, where you", "tokens": [50364, 509, 1062, 611, 1604, 4111, 21589, 562, 321, 645, 1228, 8213, 24590, 11, 689, 291, 50644, 50644, 1866, 577, 4111, 21589, 300, 307, 21589, 439, 264, 4122, 281, 747, 322, 2531, 22526, 295, 50876, 50876, 4190, 11, 584, 1296, 3671, 472, 293, 1804, 472, 11, 577, 300, 393, 854, 16235, 23475, 281, 51140, 51140, 41881, 4663, 13, 51248, 51248, 3697, 1503, 21589, 6456, 264, 912, 636, 281, 4373, 264, 819, 4122, 281, 747, 322, 2531, 51456, 51456, 22526, 295, 4190, 393, 611, 3073, 493, 16235, 23475, 337, 3565, 3142, 24590, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.08860821974904913, "compression_ratio": 1.9298245614035088, "no_speech_prob": 2.123349759131088e-06}, {"id": 50, "seek": 28912, "start": 294.72, "end": 299.36, "text": " saw how feature scaling that is scaling all the features to take on similar ranges of", "tokens": [50364, 509, 1062, 611, 1604, 4111, 21589, 562, 321, 645, 1228, 8213, 24590, 11, 689, 291, 50644, 50644, 1866, 577, 4111, 21589, 300, 307, 21589, 439, 264, 4122, 281, 747, 322, 2531, 22526, 295, 50876, 50876, 4190, 11, 584, 1296, 3671, 472, 293, 1804, 472, 11, 577, 300, 393, 854, 16235, 23475, 281, 51140, 51140, 41881, 4663, 13, 51248, 51248, 3697, 1503, 21589, 6456, 264, 912, 636, 281, 4373, 264, 819, 4122, 281, 747, 322, 2531, 51456, 51456, 22526, 295, 4190, 393, 611, 3073, 493, 16235, 23475, 337, 3565, 3142, 24590, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.08860821974904913, "compression_ratio": 1.9298245614035088, "no_speech_prob": 2.123349759131088e-06}, {"id": 51, "seek": 28912, "start": 299.36, "end": 304.64, "text": " values, say between negative one and plus one, how that can help gradient descent to", "tokens": [50364, 509, 1062, 611, 1604, 4111, 21589, 562, 321, 645, 1228, 8213, 24590, 11, 689, 291, 50644, 50644, 1866, 577, 4111, 21589, 300, 307, 21589, 439, 264, 4122, 281, 747, 322, 2531, 22526, 295, 50876, 50876, 4190, 11, 584, 1296, 3671, 472, 293, 1804, 472, 11, 577, 300, 393, 854, 16235, 23475, 281, 51140, 51140, 41881, 4663, 13, 51248, 51248, 3697, 1503, 21589, 6456, 264, 912, 636, 281, 4373, 264, 819, 4122, 281, 747, 322, 2531, 51456, 51456, 22526, 295, 4190, 393, 611, 3073, 493, 16235, 23475, 337, 3565, 3142, 24590, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.08860821974904913, "compression_ratio": 1.9298245614035088, "no_speech_prob": 2.123349759131088e-06}, {"id": 52, "seek": 28912, "start": 304.64, "end": 306.8, "text": " converge faster.", "tokens": [50364, 509, 1062, 611, 1604, 4111, 21589, 562, 321, 645, 1228, 8213, 24590, 11, 689, 291, 50644, 50644, 1866, 577, 4111, 21589, 300, 307, 21589, 439, 264, 4122, 281, 747, 322, 2531, 22526, 295, 50876, 50876, 4190, 11, 584, 1296, 3671, 472, 293, 1804, 472, 11, 577, 300, 393, 854, 16235, 23475, 281, 51140, 51140, 41881, 4663, 13, 51248, 51248, 3697, 1503, 21589, 6456, 264, 912, 636, 281, 4373, 264, 819, 4122, 281, 747, 322, 2531, 51456, 51456, 22526, 295, 4190, 393, 611, 3073, 493, 16235, 23475, 337, 3565, 3142, 24590, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.08860821974904913, "compression_ratio": 1.9298245614035088, "no_speech_prob": 2.123349759131088e-06}, {"id": 53, "seek": 28912, "start": 306.8, "end": 310.96, "text": " Feature scaling applied the same way to scale the different features to take on similar", "tokens": [50364, 509, 1062, 611, 1604, 4111, 21589, 562, 321, 645, 1228, 8213, 24590, 11, 689, 291, 50644, 50644, 1866, 577, 4111, 21589, 300, 307, 21589, 439, 264, 4122, 281, 747, 322, 2531, 22526, 295, 50876, 50876, 4190, 11, 584, 1296, 3671, 472, 293, 1804, 472, 11, 577, 300, 393, 854, 16235, 23475, 281, 51140, 51140, 41881, 4663, 13, 51248, 51248, 3697, 1503, 21589, 6456, 264, 912, 636, 281, 4373, 264, 819, 4122, 281, 747, 322, 2531, 51456, 51456, 22526, 295, 4190, 393, 611, 3073, 493, 16235, 23475, 337, 3565, 3142, 24590, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.08860821974904913, "compression_ratio": 1.9298245614035088, "no_speech_prob": 2.123349759131088e-06}, {"id": 54, "seek": 28912, "start": 310.96, "end": 316.68, "text": " ranges of values can also speed up gradient descent for logistic regression.", "tokens": [50364, 509, 1062, 611, 1604, 4111, 21589, 562, 321, 645, 1228, 8213, 24590, 11, 689, 291, 50644, 50644, 1866, 577, 4111, 21589, 300, 307, 21589, 439, 264, 4122, 281, 747, 322, 2531, 22526, 295, 50876, 50876, 4190, 11, 584, 1296, 3671, 472, 293, 1804, 472, 11, 577, 300, 393, 854, 16235, 23475, 281, 51140, 51140, 41881, 4663, 13, 51248, 51248, 3697, 1503, 21589, 6456, 264, 912, 636, 281, 4373, 264, 819, 4122, 281, 747, 322, 2531, 51456, 51456, 22526, 295, 4190, 393, 611, 3073, 493, 16235, 23475, 337, 3565, 3142, 24590, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.08860821974904913, "compression_ratio": 1.9298245614035088, "no_speech_prob": 2.123349759131088e-06}, {"id": 55, "seek": 31668, "start": 316.68, "end": 324.08, "text": " In the upcoming optional lab, you also see how the gradient for the regression can be", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 611, 536, 577, 264, 16235, 337, 264, 24590, 393, 312, 50734, 50734, 15598, 294, 3089, 13, 50838, 50838, 639, 486, 312, 4420, 281, 574, 412, 570, 291, 611, 4445, 341, 294, 257, 3124, 2715, 412, 264, 51076, 51076, 917, 295, 341, 1243, 13, 51190, 51190, 2381, 291, 1190, 16235, 23475, 294, 341, 2715, 11, 456, 603, 312, 257, 1481, 992, 295, 18947, 28609, 300, 51426, 51426, 855, 16235, 23475, 294, 3069, 13, 51556, 51556, 509, 536, 264, 4556, 3280, 327, 2445, 11, 264, 1969, 7542, 295, 264, 2063, 11, 264, 805, 67, 3753, 7542, 281, 264, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.1338400796195057, "compression_ratio": 1.7541666666666667, "no_speech_prob": 6.747861334588379e-06}, {"id": 56, "seek": 31668, "start": 324.08, "end": 326.16, "text": " calculated in code.", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 611, 536, 577, 264, 16235, 337, 264, 24590, 393, 312, 50734, 50734, 15598, 294, 3089, 13, 50838, 50838, 639, 486, 312, 4420, 281, 574, 412, 570, 291, 611, 4445, 341, 294, 257, 3124, 2715, 412, 264, 51076, 51076, 917, 295, 341, 1243, 13, 51190, 51190, 2381, 291, 1190, 16235, 23475, 294, 341, 2715, 11, 456, 603, 312, 257, 1481, 992, 295, 18947, 28609, 300, 51426, 51426, 855, 16235, 23475, 294, 3069, 13, 51556, 51556, 509, 536, 264, 4556, 3280, 327, 2445, 11, 264, 1969, 7542, 295, 264, 2063, 11, 264, 805, 67, 3753, 7542, 281, 264, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.1338400796195057, "compression_ratio": 1.7541666666666667, "no_speech_prob": 6.747861334588379e-06}, {"id": 57, "seek": 31668, "start": 326.16, "end": 330.92, "text": " This will be useful to look at because you also implement this in a practice lab at the", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 611, 536, 577, 264, 16235, 337, 264, 24590, 393, 312, 50734, 50734, 15598, 294, 3089, 13, 50838, 50838, 639, 486, 312, 4420, 281, 574, 412, 570, 291, 611, 4445, 341, 294, 257, 3124, 2715, 412, 264, 51076, 51076, 917, 295, 341, 1243, 13, 51190, 51190, 2381, 291, 1190, 16235, 23475, 294, 341, 2715, 11, 456, 603, 312, 257, 1481, 992, 295, 18947, 28609, 300, 51426, 51426, 855, 16235, 23475, 294, 3069, 13, 51556, 51556, 509, 536, 264, 4556, 3280, 327, 2445, 11, 264, 1969, 7542, 295, 264, 2063, 11, 264, 805, 67, 3753, 7542, 281, 264, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.1338400796195057, "compression_ratio": 1.7541666666666667, "no_speech_prob": 6.747861334588379e-06}, {"id": 58, "seek": 31668, "start": 330.92, "end": 333.2, "text": " end of this week.", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 611, 536, 577, 264, 16235, 337, 264, 24590, 393, 312, 50734, 50734, 15598, 294, 3089, 13, 50838, 50838, 639, 486, 312, 4420, 281, 574, 412, 570, 291, 611, 4445, 341, 294, 257, 3124, 2715, 412, 264, 51076, 51076, 917, 295, 341, 1243, 13, 51190, 51190, 2381, 291, 1190, 16235, 23475, 294, 341, 2715, 11, 456, 603, 312, 257, 1481, 992, 295, 18947, 28609, 300, 51426, 51426, 855, 16235, 23475, 294, 3069, 13, 51556, 51556, 509, 536, 264, 4556, 3280, 327, 2445, 11, 264, 1969, 7542, 295, 264, 2063, 11, 264, 805, 67, 3753, 7542, 281, 264, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.1338400796195057, "compression_ratio": 1.7541666666666667, "no_speech_prob": 6.747861334588379e-06}, {"id": 59, "seek": 31668, "start": 333.2, "end": 337.92, "text": " After you run gradient descent in this lab, there'll be a nice set of animated plots that", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 611, 536, 577, 264, 16235, 337, 264, 24590, 393, 312, 50734, 50734, 15598, 294, 3089, 13, 50838, 50838, 639, 486, 312, 4420, 281, 574, 412, 570, 291, 611, 4445, 341, 294, 257, 3124, 2715, 412, 264, 51076, 51076, 917, 295, 341, 1243, 13, 51190, 51190, 2381, 291, 1190, 16235, 23475, 294, 341, 2715, 11, 456, 603, 312, 257, 1481, 992, 295, 18947, 28609, 300, 51426, 51426, 855, 16235, 23475, 294, 3069, 13, 51556, 51556, 509, 536, 264, 4556, 3280, 327, 2445, 11, 264, 1969, 7542, 295, 264, 2063, 11, 264, 805, 67, 3753, 7542, 281, 264, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.1338400796195057, "compression_ratio": 1.7541666666666667, "no_speech_prob": 6.747861334588379e-06}, {"id": 60, "seek": 31668, "start": 337.92, "end": 340.52, "text": " show gradient descent in action.", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 611, 536, 577, 264, 16235, 337, 264, 24590, 393, 312, 50734, 50734, 15598, 294, 3089, 13, 50838, 50838, 639, 486, 312, 4420, 281, 574, 412, 570, 291, 611, 4445, 341, 294, 257, 3124, 2715, 412, 264, 51076, 51076, 917, 295, 341, 1243, 13, 51190, 51190, 2381, 291, 1190, 16235, 23475, 294, 341, 2715, 11, 456, 603, 312, 257, 1481, 992, 295, 18947, 28609, 300, 51426, 51426, 855, 16235, 23475, 294, 3069, 13, 51556, 51556, 509, 536, 264, 4556, 3280, 327, 2445, 11, 264, 1969, 7542, 295, 264, 2063, 11, 264, 805, 67, 3753, 7542, 281, 264, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.1338400796195057, "compression_ratio": 1.7541666666666667, "no_speech_prob": 6.747861334588379e-06}, {"id": 61, "seek": 31668, "start": 340.52, "end": 345.92, "text": " You see the sigmoid function, the control plot of the cost, the 3d surface plot to the", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 611, 536, 577, 264, 16235, 337, 264, 24590, 393, 312, 50734, 50734, 15598, 294, 3089, 13, 50838, 50838, 639, 486, 312, 4420, 281, 574, 412, 570, 291, 611, 4445, 341, 294, 257, 3124, 2715, 412, 264, 51076, 51076, 917, 295, 341, 1243, 13, 51190, 51190, 2381, 291, 1190, 16235, 23475, 294, 341, 2715, 11, 456, 603, 312, 257, 1481, 992, 295, 18947, 28609, 300, 51426, 51426, 855, 16235, 23475, 294, 3069, 13, 51556, 51556, 509, 536, 264, 4556, 3280, 327, 2445, 11, 264, 1969, 7542, 295, 264, 2063, 11, 264, 805, 67, 3753, 7542, 281, 264, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.1338400796195057, "compression_ratio": 1.7541666666666667, "no_speech_prob": 6.747861334588379e-06}, {"id": 62, "seek": 34592, "start": 345.92, "end": 351.04, "text": " cost and the learning curve all evolve as gradient descent runs.", "tokens": [50364, 2063, 293, 264, 2539, 7605, 439, 16693, 382, 16235, 23475, 6676, 13, 50620, 50620, 821, 486, 312, 1071, 17312, 2715, 934, 300, 11, 597, 307, 2099, 293, 3844, 11, 457, 611, 588, 4420, 50878, 50878, 570, 436, 603, 855, 291, 577, 281, 764, 264, 3743, 2180, 22681, 12, 306, 1083, 6405, 281, 3847, 264, 3565, 3142, 51154, 51154, 24590, 2316, 337, 21538, 13, 51328, 51328, 5126, 3479, 2539, 25742, 294, 867, 3431, 965, 764, 2180, 22681, 12, 306, 1083, 11672, 51576, 51576, 382, 644, 295, 641, 1691, 13, 51658, 51658], "temperature": 0.0, "avg_logprob": -0.09008452166681705, "compression_ratio": 1.6390041493775933, "no_speech_prob": 5.771746600657934e-06}, {"id": 63, "seek": 34592, "start": 351.04, "end": 356.2, "text": " There will be another optional lab after that, which is short and sweet, but also very useful", "tokens": [50364, 2063, 293, 264, 2539, 7605, 439, 16693, 382, 16235, 23475, 6676, 13, 50620, 50620, 821, 486, 312, 1071, 17312, 2715, 934, 300, 11, 597, 307, 2099, 293, 3844, 11, 457, 611, 588, 4420, 50878, 50878, 570, 436, 603, 855, 291, 577, 281, 764, 264, 3743, 2180, 22681, 12, 306, 1083, 6405, 281, 3847, 264, 3565, 3142, 51154, 51154, 24590, 2316, 337, 21538, 13, 51328, 51328, 5126, 3479, 2539, 25742, 294, 867, 3431, 965, 764, 2180, 22681, 12, 306, 1083, 11672, 51576, 51576, 382, 644, 295, 641, 1691, 13, 51658, 51658], "temperature": 0.0, "avg_logprob": -0.09008452166681705, "compression_ratio": 1.6390041493775933, "no_speech_prob": 5.771746600657934e-06}, {"id": 64, "seek": 34592, "start": 356.2, "end": 361.72, "text": " because they'll show you how to use the popular scikit-learn library to train the logistic", "tokens": [50364, 2063, 293, 264, 2539, 7605, 439, 16693, 382, 16235, 23475, 6676, 13, 50620, 50620, 821, 486, 312, 1071, 17312, 2715, 934, 300, 11, 597, 307, 2099, 293, 3844, 11, 457, 611, 588, 4420, 50878, 50878, 570, 436, 603, 855, 291, 577, 281, 764, 264, 3743, 2180, 22681, 12, 306, 1083, 6405, 281, 3847, 264, 3565, 3142, 51154, 51154, 24590, 2316, 337, 21538, 13, 51328, 51328, 5126, 3479, 2539, 25742, 294, 867, 3431, 965, 764, 2180, 22681, 12, 306, 1083, 11672, 51576, 51576, 382, 644, 295, 641, 1691, 13, 51658, 51658], "temperature": 0.0, "avg_logprob": -0.09008452166681705, "compression_ratio": 1.6390041493775933, "no_speech_prob": 5.771746600657934e-06}, {"id": 65, "seek": 34592, "start": 361.72, "end": 365.20000000000005, "text": " regression model for classification.", "tokens": [50364, 2063, 293, 264, 2539, 7605, 439, 16693, 382, 16235, 23475, 6676, 13, 50620, 50620, 821, 486, 312, 1071, 17312, 2715, 934, 300, 11, 597, 307, 2099, 293, 3844, 11, 457, 611, 588, 4420, 50878, 50878, 570, 436, 603, 855, 291, 577, 281, 764, 264, 3743, 2180, 22681, 12, 306, 1083, 6405, 281, 3847, 264, 3565, 3142, 51154, 51154, 24590, 2316, 337, 21538, 13, 51328, 51328, 5126, 3479, 2539, 25742, 294, 867, 3431, 965, 764, 2180, 22681, 12, 306, 1083, 11672, 51576, 51576, 382, 644, 295, 641, 1691, 13, 51658, 51658], "temperature": 0.0, "avg_logprob": -0.09008452166681705, "compression_ratio": 1.6390041493775933, "no_speech_prob": 5.771746600657934e-06}, {"id": 66, "seek": 34592, "start": 365.20000000000005, "end": 370.16, "text": " Many machine learning practitioners in many companies today use scikit-learn regularly", "tokens": [50364, 2063, 293, 264, 2539, 7605, 439, 16693, 382, 16235, 23475, 6676, 13, 50620, 50620, 821, 486, 312, 1071, 17312, 2715, 934, 300, 11, 597, 307, 2099, 293, 3844, 11, 457, 611, 588, 4420, 50878, 50878, 570, 436, 603, 855, 291, 577, 281, 764, 264, 3743, 2180, 22681, 12, 306, 1083, 6405, 281, 3847, 264, 3565, 3142, 51154, 51154, 24590, 2316, 337, 21538, 13, 51328, 51328, 5126, 3479, 2539, 25742, 294, 867, 3431, 965, 764, 2180, 22681, 12, 306, 1083, 11672, 51576, 51576, 382, 644, 295, 641, 1691, 13, 51658, 51658], "temperature": 0.0, "avg_logprob": -0.09008452166681705, "compression_ratio": 1.6390041493775933, "no_speech_prob": 5.771746600657934e-06}, {"id": 67, "seek": 34592, "start": 370.16, "end": 371.8, "text": " as part of their job.", "tokens": [50364, 2063, 293, 264, 2539, 7605, 439, 16693, 382, 16235, 23475, 6676, 13, 50620, 50620, 821, 486, 312, 1071, 17312, 2715, 934, 300, 11, 597, 307, 2099, 293, 3844, 11, 457, 611, 588, 4420, 50878, 50878, 570, 436, 603, 855, 291, 577, 281, 764, 264, 3743, 2180, 22681, 12, 306, 1083, 6405, 281, 3847, 264, 3565, 3142, 51154, 51154, 24590, 2316, 337, 21538, 13, 51328, 51328, 5126, 3479, 2539, 25742, 294, 867, 3431, 965, 764, 2180, 22681, 12, 306, 1083, 11672, 51576, 51576, 382, 644, 295, 641, 1691, 13, 51658, 51658], "temperature": 0.0, "avg_logprob": -0.09008452166681705, "compression_ratio": 1.6390041493775933, "no_speech_prob": 5.771746600657934e-06}, {"id": 68, "seek": 37180, "start": 371.8, "end": 376.40000000000003, "text": " And so I hope you check out the scikit-learn function as well and take a look at how that", "tokens": [50364, 400, 370, 286, 1454, 291, 1520, 484, 264, 2180, 22681, 12, 306, 1083, 2445, 382, 731, 293, 747, 257, 574, 412, 577, 300, 50594, 50594, 307, 1143, 13, 50688, 50688, 407, 300, 311, 309, 13, 50738, 50738, 509, 586, 458, 577, 281, 4445, 3565, 3142, 24590, 13, 50908, 50908, 639, 307, 257, 588, 4005, 11, 588, 13371, 1143, 2539, 9284, 293, 291, 586, 458, 577, 281, 51158, 51158, 483, 309, 281, 589, 1803, 13, 51228, 51228, 9694, 13, 51278], "temperature": 0.0, "avg_logprob": -0.13965308813401212, "compression_ratio": 1.5183246073298429, "no_speech_prob": 1.307063485000981e-06}, {"id": 69, "seek": 37180, "start": 376.40000000000003, "end": 378.28000000000003, "text": " is used.", "tokens": [50364, 400, 370, 286, 1454, 291, 1520, 484, 264, 2180, 22681, 12, 306, 1083, 2445, 382, 731, 293, 747, 257, 574, 412, 577, 300, 50594, 50594, 307, 1143, 13, 50688, 50688, 407, 300, 311, 309, 13, 50738, 50738, 509, 586, 458, 577, 281, 4445, 3565, 3142, 24590, 13, 50908, 50908, 639, 307, 257, 588, 4005, 11, 588, 13371, 1143, 2539, 9284, 293, 291, 586, 458, 577, 281, 51158, 51158, 483, 309, 281, 589, 1803, 13, 51228, 51228, 9694, 13, 51278], "temperature": 0.0, "avg_logprob": -0.13965308813401212, "compression_ratio": 1.5183246073298429, "no_speech_prob": 1.307063485000981e-06}, {"id": 70, "seek": 37180, "start": 378.28000000000003, "end": 379.28000000000003, "text": " So that's it.", "tokens": [50364, 400, 370, 286, 1454, 291, 1520, 484, 264, 2180, 22681, 12, 306, 1083, 2445, 382, 731, 293, 747, 257, 574, 412, 577, 300, 50594, 50594, 307, 1143, 13, 50688, 50688, 407, 300, 311, 309, 13, 50738, 50738, 509, 586, 458, 577, 281, 4445, 3565, 3142, 24590, 13, 50908, 50908, 639, 307, 257, 588, 4005, 11, 588, 13371, 1143, 2539, 9284, 293, 291, 586, 458, 577, 281, 51158, 51158, 483, 309, 281, 589, 1803, 13, 51228, 51228, 9694, 13, 51278], "temperature": 0.0, "avg_logprob": -0.13965308813401212, "compression_ratio": 1.5183246073298429, "no_speech_prob": 1.307063485000981e-06}, {"id": 71, "seek": 37180, "start": 379.28000000000003, "end": 382.68, "text": " You now know how to implement logistic regression.", "tokens": [50364, 400, 370, 286, 1454, 291, 1520, 484, 264, 2180, 22681, 12, 306, 1083, 2445, 382, 731, 293, 747, 257, 574, 412, 577, 300, 50594, 50594, 307, 1143, 13, 50688, 50688, 407, 300, 311, 309, 13, 50738, 50738, 509, 586, 458, 577, 281, 4445, 3565, 3142, 24590, 13, 50908, 50908, 639, 307, 257, 588, 4005, 11, 588, 13371, 1143, 2539, 9284, 293, 291, 586, 458, 577, 281, 51158, 51158, 483, 309, 281, 589, 1803, 13, 51228, 51228, 9694, 13, 51278], "temperature": 0.0, "avg_logprob": -0.13965308813401212, "compression_ratio": 1.5183246073298429, "no_speech_prob": 1.307063485000981e-06}, {"id": 72, "seek": 37180, "start": 382.68, "end": 387.68, "text": " This is a very powerful, very widely used learning algorithm and you now know how to", "tokens": [50364, 400, 370, 286, 1454, 291, 1520, 484, 264, 2180, 22681, 12, 306, 1083, 2445, 382, 731, 293, 747, 257, 574, 412, 577, 300, 50594, 50594, 307, 1143, 13, 50688, 50688, 407, 300, 311, 309, 13, 50738, 50738, 509, 586, 458, 577, 281, 4445, 3565, 3142, 24590, 13, 50908, 50908, 639, 307, 257, 588, 4005, 11, 588, 13371, 1143, 2539, 9284, 293, 291, 586, 458, 577, 281, 51158, 51158, 483, 309, 281, 589, 1803, 13, 51228, 51228, 9694, 13, 51278], "temperature": 0.0, "avg_logprob": -0.13965308813401212, "compression_ratio": 1.5183246073298429, "no_speech_prob": 1.307063485000981e-06}, {"id": 73, "seek": 37180, "start": 387.68, "end": 389.08000000000004, "text": " get it to work yourself.", "tokens": [50364, 400, 370, 286, 1454, 291, 1520, 484, 264, 2180, 22681, 12, 306, 1083, 2445, 382, 731, 293, 747, 257, 574, 412, 577, 300, 50594, 50594, 307, 1143, 13, 50688, 50688, 407, 300, 311, 309, 13, 50738, 50738, 509, 586, 458, 577, 281, 4445, 3565, 3142, 24590, 13, 50908, 50908, 639, 307, 257, 588, 4005, 11, 588, 13371, 1143, 2539, 9284, 293, 291, 586, 458, 577, 281, 51158, 51158, 483, 309, 281, 589, 1803, 13, 51228, 51228, 9694, 13, 51278], "temperature": 0.0, "avg_logprob": -0.13965308813401212, "compression_ratio": 1.5183246073298429, "no_speech_prob": 1.307063485000981e-06}, {"id": 74, "seek": 38908, "start": 389.08, "end": 405.38, "text": " Congratulations.", "tokens": [50364, 9694, 13, 51179, 51179], "temperature": 1.0, "avg_logprob": -1.6656498908996582, "compression_ratio": 0.6666666666666666, "no_speech_prob": 1.3380145901464857e-05}], "language": "en", "video_id": "D59jK8T9dfI", "entity": "ML Specialization, Andrew Ng (2022)"}}