{"video_id": "toUgBQv1BT8", "title": "Lesson 4: Practical Deep Learning for Coders 2022", "description": "00:00:00 - Using Huggingface\n00:03:24 - Finetuning pretrained model\n00:05:14 - ULMFit\n00:09:15 - Transformer\n00:10:52 - Zeiler & Fergus\n00:14:47 - US Patent Phrase to Phase Matching Kaggle competition\n00:16:10 - NLP Classification\n00:20:56 - Kaggle configs, insert python in bash, read competition website\n00:24:51 - Pandas, numpy, matplotlib, & pytorch\n00:29:26 - Tokenization\n00:33:20 - Huggingface model hub\n00:36:40 - Examples of tokenized sentences\n00:38:47 - Numericalization\n00:41:13 - Question: rationale behind how input data was formatted\n00:43:20 - ULMFit fits large documents easily\n00:45:55 - Overfitting & underfitting\n00:50:45 - Splitting the dataset\n00:52:31 - Creating a good validation set\n00:57:13 - Test set\n00:59:00 - Metric vs loss\n01:01:27 - The problem with metrics\n01:04:10 - Pearson correlation\n01:10:27 - Correlation is sensitive to outliers\n01:14:00 - Training a model\n01:19:20 - Question: when is it ok to remove outliers?\n01:22:10 - Predictions\n01:25:30 - Opportunities for research and startups\n01:26:16 - Misusing NLP\n01:33:00 - Question: isn\u2019t the target categorical in this case?\n\nTranscript thanks to wyquek, jmp, bencoman, fmussari, mike.moloch, amr.malik, kurianbenoy, gagan, and Raymond Wu on forums.fast.ai.\n\nTimestamps thanks to RogerS49 and Wyquek on forums.fast.ai.", "author": "Jeremy Howard", "keywords": ["deep learning", "fastai"], "channel_url": "https://www.youtube.com/channel/UCX7Y2qWriXpqocG97SFW2OQ", "length": 5677, "views": 14979, "publish_date": "11/02/2022", "timestamp": 1658361600, "entity": "FastAI", "transcript": {"text": " Hi everybody and welcome to Practical Deep Learning for Coders lesson 4, which I think is the lesson that a lot of the regulars in the community have been most excited about because it's where we're going to get some totally new material, totally new topic we've never covered before. We're going to cover natural language processing in LP and you'll find there there is indeed a chapter about that in the book but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the Fast AI library using recurrent neural networks, RNNs. Today we're going to do something else which is we're going to do transformers and we're not even going to use the Fast AI library at all in fact. So what we're going to be doing today is we're going to be fine-tuning a pre-trained NLP model using a library called hugging face transformers. Now given this is the Fast.AI course you might be wondering why we'd be using a different library other than Fast AI. The reason is that I think that it's really useful for everybody to have experience and practice of using more than one library because you'll get to see the same concepts applied in different ways and I think that's great for your understanding of what these concepts are. Also I really like the hugging face transformers library. It's absolutely the state-of-the-art in NLP and it's well worth knowing. If you're watching this on video by the time you're watching it we will probably have completed our integration of the transformers library into Fast.AI so it's in the process of becoming the main NLP kind of foundation for Fast.AI so you'll be able to combine transformers and Fast.AI together. Yeah so I think there's a lot of benefits to this and in the end you're going to know how to do NLP in a really fantastic library. Now the other thing is hugging face transformers doesn't have the same layered architecture that Fast.AI has which means particularly for beginners the kind of high level, you know, top tier API that you'll be using most of the time is not as kind of ready to go for beginners as you're used to from Fast.AI and so that's actually I think a good thing. You're up to lesson four, you know the basic idea now of how gradient descent works and you know how parameters are learned as part of a flexible function. I think you're ready to try using a somewhat lower level library that does a little bit less for you so it's going to be you know a little bit more work. It's still it's a very well designed library and it's still reasonably high level but you're going to learn to go a little bit deeper and that's kind of how the rest of the course in general is going to be on the whole is we're going to get a bit deeper and a bit deeper and a bit deeper. Now so first of all let's talk about what we're going to be doing with fine-tuning a pre-trained model. We've talked about that in passing before but we haven't really been able to describe it in any detail because you haven't had the foundations. Now you do. You played with these sliders last week that hopefully you've all actually gone into this notebook and dragged them around and tried to get an intuition for like this idea of like moving them up and down makes the loss go up and down and so forth. So I imagine that your job was to move these sliders to get this as nice as possible but when it was given to you the person who gave it to you said oh actually slider A that should be on 2.0 we know for sure and slider B we think it's like around two and a half. Slider C we've got no idea. Now that'd be pretty helpful wouldn't it right because you could like immediately start focusing on the one we have no idea about get that in roughly the right spot and then the one you kind of got a vague idea about you could just tune it a little bit and the one that they said was totally confident you wouldn't move at all you would probably tune these sliders really quickly. That's what a pre-trained model is. A pre-trained model is a bunch of parameters that have already been fit where some of them you're already pretty confident of what they should be and some of them we really have no idea at all and so fine-tuning is the process of taking those ones we have no idea what they should be at all and trying to get them right and then moving the other ones a little bit. The idea of fine-tuning a pre-trained NLP model in this way was pioneered by an algorithm called ULM fit which was first presented actually in a fast AI course I think the very first fast AI course it was later turned into an academic paper by me and in conjunction with a then PhD student named Sebastian Ruda who's now one of the world's top NLP researchers and went on to help inspire a huge change you know huge kind of step improvement in NLP capabilities around the world along with a number of other important innovations at the time. This is the basic process that ULM fit described. Step one was to build something called a language model using basically nearly all of Wikipedia and what the language model did was it tried to predict the next word of a Wikipedia article in fact every next word of every Wikipedia article. Doing that is very difficult you know there are Wikipedia articles which would say things like you know the 17th prime number is dot dot dot or the 40th president of the United States blah said at his residence blah that you know filling in these kinds of things requires understanding a lot about how language is structured and about the world and about math and so forth. So to get good at being a language model a neural network has to get good at a lot of things. It has to understand how language works at a reasonably good level and it needs to understand what it's actually talking about and what is actually true and what is actually not true and the different ways in which things are expressed and so forth. So this was trained using a very similar approach to what we'll be looking at fine-tuning but it started with random weights and at the end of it there was a model that could predict more than 30% of the time correctly what the next word of a Wikipedia article would be. So in this particular case for the ULM fit paper we then took that and we were trying to the first task I did actually for the the fast AI course back when I invented this was to try and figure out whether IMDB movie reviews were positive or negative sentiment did the person like the movie or not. So what I did was I created a second language model so again the language model here is something that predicts the next word of a sentence but rather than using Wikipedia I took this pre-trained model that was trained on Wikipedia and I ran a few more epochs using IMDB movie reviews so it got very good at predicting the next word of an IMDB movie review and then finally I took those weights and I fine-tuned them for the task of predicting whether or not a movie review was positive or negative sentiment. So those were the three steps. This is a particularly interesting approach because this very first model in fact the first two models if you think about it they don't require any labels I didn't have to collect any kind of document categories or do any kind of surveys or collect anything all I needed was the actual text of Wikipedia and movie reviews themselves because the labels was what's the next word of a sentence. Now since we built ULM fit and we used RNNs, recurrent neural networks, to this at about the same time-ish that we released this a new kind of architecture particularly useful for NLP at the time was developed called transformers and transformers were particularly built because they can take really good advantage of modern accelerators like like Google's TPUs. They didn't really kind of allow you to predict the next word of a sentence. It's just not how they're structured for reasons we'll talk about probably in part two of the course. So they threw away the idea of predicting the next word of a sentence and then those instead they did something just as good and pretty clever they took kind of chunks of Wikipedia or whatever text they're looking at and deleted at random a few words and asked the model to predict which what were the words that were deleted essentially so it's a pretty similar idea. Other than that the basic concept was the same as ULM fit they replaced our RNN approach with a transformer model they replaced our language model approach with what's called a masked language model but other than that the basic idea was the same. So today we're going to be looking at models using what's become the you know much more popular approach than ULM fit which is this transformers masked language model approach. Okay John do we have any questions and I should mention we do have a professor from University of Queensland John Williams joining us who will be asking the highest voted questions from the community. What do you got John? Yeah thanks Jeremy look and we might be jumping the gun here I suspect this is where you're going tonight but we've got a good question here on the forum which is how do you go from a model that's trained to predict the next word to a model that can be used for classification? Sure so yeah we will be getting into that in more detail and in fact maybe a good place to start would be the next slide. Kind of give you a sense of this. You might remember in lesson one we looked at this fantastic Zeiler and Fergus paper where we looked at visualizations of the first layer of a image net classification model and layer one had sets of weights that found diagonal edges and here are some examples of bits of photos that successfully matched with and opposite diagonal edges and kind of color gradients and here's some examples of bits of pictures that matched and then layer two combined those and now you know how those were combined right these were rectified linear units that were added together okay and then sets of those rectified linear units the outputs of those they're called activations were then themselves run through a matrix multiplier rectified linear unit added together so that now you don't just have to have edge detectors but layer two had corner detectors and here's some examples of some corners that that corner detector successfully found. Remember these were not engineered in any way they just evolved from the gradient descent training process. Layer two had examples of circle detectors as it turns out and skipping a bit by the time we got to layer five we had bird and lizard eyeball detectors and dog face detectors and flower detectors and so forth. Now you know nowadays you'd have something like a ResNet 50 would be something you'd probably be training pretty regularly in this course so that you know you've got 50 layers not just five layers. Now the later layers do things that are much more specific to the training task which is like actually predicting really what what is it that we're looking at. The early layers pretty unlikely you're going to need to change them much as long as you're looking at like some kind of natural photos right you're going to need edge detectors and gradient detectors. So what we do in in the fine-tuning process is there's actually one extra layer after this which is the layer that actually says what is this you know it's it's a dog or a cat or whatever. We actually delete that we throw it away. So now that last matrix multiply has one output or one output per category you're predicting we throw that away so the model now has that the last matrix that's spitting out you know it depends but generally a few hundred activations. And what we do is as we'll learn more shortly in the coming lesson we we just stick a new random matrix on the end of that and that's what we initially train so it learns to use these kinds of features to predict whatever it is you're trying to predict and then we gradually train all of those layers. So that's basically how it's done and so it's a bit hand wavy but we'll particularly in part two actually build that from scratch ourselves and in fact in this lesson time permitting we're actually going to start going down the process of actually building a real-world deep neural net in Python so we'll be starting to actually make some progress towards that goal. Okay so let's jump into the notebook. So we're going to look at a Kaggle competition that's actually on as I as I speak and I created this notebook called Getting Started with NLP for Absolute Beginners and so the competition is called the US Patent Phrase to Phrase Matching Competition and so I'm going to take you through you know a complete submission to this competition and Kaggle competitions are interesting particularly the ones that are not playground competitions but the real competitions with real money applied they're interesting because this is an actual project that an actual organization is prepared to invest money in getting solved using their actual data. So a lot of people are a bit dismissive of Kaggle competitions as being kind of like not very real and it's certainly true you're not worrying about stuff like productionizing the model but you know in terms of like getting real data about a real problem that real organizations really care about and a very direct way to measure the you know accuracy of your solution you can't really get better than this. So this is a good place a good competition to to experiment with for trying NLP. Now as I mentioned here probably the most widely useful application for NLP is classification and as we've discussed in computer vision classification refers to taking an object and trying to identify a category that object belongs to. So previously we've mainly been looking at images today we're going to be looking at documents. Now in NLP when we say document we don't specifically mean you know a 20 page long you know essay a document could be three or four words or a document could be the entire exaclopedia. So a document is just an input to an NLP model that contains text. Now classifying a document so deciding what category a document belongs to is a surprisingly rich thing to do there's all kinds of stuff you could do with that so for example we've already mentioned sentiment analysis that's a cat that's a classification task we try to decide on the category positive or negative sentiment. Author identification would be taking a document and trying to find the category of author. Legal discovery would be taking documents and putting them into categories according to in or out of scope for a court case. Triage in inbound emails would be putting them into categories of you know throw away send to customer service send to sales etc. So classification is a very very rich area and for people interested in trying out NLP in real life I would suggest classification would be the place I would start for looking for kind of accessible real-world useful problems you can solve right away. Now the Kaggle competition does not immediately look like a classification competition what it contains let me show you some data what it contains is data that looks like this it has a thing they call anchor I think they call target I think they call context and a score. Now these are I can't remember exact details but I think these are from patents and I think on the patents there are various like things they have to fill in in the patent and one of those things is called anchor one of those things is called target and in the competition the goal is to come up with a model that automatically determines which anchor and target pairs are talking about the same thing. So a score of one here wood article and wooden article obviously talking about the same thing the score of zero here abatement and forest region not talking about the same thing. So the basic idea is we're trying to guess the score and it's kind of a classification problem kind of not we're basically trying to classify things into either these two things are the same or these two things aren't the same it's kind of not because we have not just one and zero but also 0.25 0.5 and 0.75. There's also a column called context which is I believe is like the category that this patent was filed in and my understanding is that whether the anchor and the target count as similar or not depends on you know what what the patent was filed under. So how would we take this and turn it into something like a classification problem? So the suggestion I make here is that we could basically say okay let's put the you know some some constants string like text one or field one before the the first column and then something else like text two before the second column or maybe the also the context I should have this well text three in the context and then try to choose a category of meaning similarity different similar or identical so it could basically concatenate those three pieces together call that a document and then try to train a model that can predict these categories. That'd be an example of how we can take this basically similarity problem and turn it into something that looks like a classification problem and we tend to do this a lot in deep learning is we kind of take problems that look a bit novel and different and turn them into a problem that looks like something we recognize. Right so on Kaggle this is a you know larger data set that you're going to need a GPU to run so you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that'll happen for you automatically. Personally you know I like using things like paper space generally better than Kaggle. Like Kaggle is pretty good but you know you only get 30 hours a week of GPU time and the notebook editor for me is not as good as a real JupyterLab environment. So there's some information here I won't go through but it basically describes how you can download stuff to paper space or your own computer as well if you want to. So I basically create this little boolean always in my notebooks called is Kaggle which is going to be true if it's running on Kaggle and false otherwise and any little changes I need to make I'd say if is Kaggle and put those changes. So here you can see here if I'm not on Kaggle and I don't have the data yet then download it and Kaggle has a little API it is quite handy for doing stuff like downloading data and uploading notebooks and stuff like that submitting to competitions. If we are on Kaggle then the data is already going to be there for us which is actually a good reason for beginners to use Kaggle is you don't have to worry about grabbing the data at all it's sitting there for you as soon as you open the notebook. Kaggle has a lot of Python packages installed but not necessarily all the ones you want and at the point I wrote this they didn't have hugging faces datasets package for some reason so you can always just install stuff. So you might remember the exclamation mark means this is not a Python command but a shell command a bash command but it's quite neat you can even put bash commands inside Python conditionals so that's a pretty cool little trick in notebooks. Another cool little trick in notebooks is that if you do use a bash command like LS but you then want to insert the contents of a Python variable just chuck it in parentheses so I've got a Python variable called path and I can go LS path in parentheses and that will LS the contents of the Python variable path so there's another little trick for you. Alright so when we LS that we can see that there's some CSV files so what I'm going to do is kind of take you through roughly the process the kind of process I you know went through as you know when I first look at a competition so the first thing is like already data set indeed what's in it okay so it's got some CSV files you know as well as looking at it here the other thing I would do is I would go to the competition website and if you go to data a lot of people skip over this which is a terrible idea because it actually tells you what the dependent variable means what the different files are what the columns are and so forth so don't just rely on looking at the data itself but look at the information that you're given about the data. So for CSV files, CSV files are comma separated values so they're just text files with a comma between each field and we can read them using pandas which for some reason always always called PD. Pandas is one of I guess like I'm trying to think probably like four key libraries that you have to know to do data science in Python and specifically those four libraries are numpy, matplotlib, pandas and pytorch. So numpy is what we use for basic kind of numerical programming, matplotlib we use for plotting, pandas we use for tables of data and pytorch we use. for deep learning. Those are all covered in a fantastic book by the author is pandas which the new version is actually available for free I believe. Python for data analysis. So if you're not familiar with these libraries just read the whole book it doesn't take too long to get through and it's got lots of cool tips and it's very readable. I do find a lot of people doing this course often I see people kind of trying to jump ahead and and want to be like oh I want to know how to like create a new architecture or build a speech recognition system or whatever and but it then turns out that they don't know how to use these fundamental libraries. So it's always good to be bold and be trying to build things but do also take the time to you know make sure you finish reading the first AI book and read at least Wes McKinney's book. That would be enough to really give you all the basic knowledge you need I think. So with pandas we can read a CSV file and that creates something called a data frame which is just a table of data as you see. So now that we've got a data frame we can see what we're working with and when we ask when in Jupyter we just put the name of a variable containing a data frame we get the first five rows the last five rows and the size so we've got thirty six thousand four hundred seventy three rows. Okay so other things I like to use for understanding a data frame is the describe method. If you pass include equals object that will describe that will describe basically all the kind of the string fields the non-numeric fields. So in this case there's four of those and so you can see here that that anchor field we looked at there's actually only 733 unique values okay so this thing you can see that there's lots of repetition out of 30 36,000 and so there's lots of repetition. This is the most common one it appears 152 times and then context we also see lots of repetition there's 106 of those contexts. So this is a nice little method we can see a lot about the data in in a glance and when I first saw this in this competition I thought well this is actually not that much language data when you think about it that you know each document is very short you know three or four words really and lots of it is repeated. So that's like as I'm looking through it I'm thinking like what are some key features of this data set and that would be something I'd be thinking wow that's you know we've got to do a lot with not very much unique data here. So here's how we can just go ahead and create a single string like I described which contains you know some kind of field separator plus the context the target and the anchor. So we're going to pop that into a field called input. Something slightly weird in pandas is there's two ways of referring to a column you can use square brackets and a string to get the input column or you can just treat it as an attribute. When you're setting it you should always use the forms in here. When reading it you can use either I tend to use this one because it's less typing. So you can see now we've got this these concatenated rows so head is the first few rows. So we've now got some some documents to do NLP with. Now the problem is as you know from the last lesson neural networks work with numbers. We're going to take some numbers and we're going to multiply them by matrices. We're going to replace the negatives with zeros and add them up. We're going to do that a few times. That's our neural network. With some little wrinkles but that's the basic idea. So how on earth do we do that for these strings? So there's basically two steps we're going to take. The first step is to split each of these into tokens. Tokens are basically words. We're going to split it into words. There's a few problems with splitting things into words. The first is that some languages like Chinese don't have words. Or at least certainly not space separated words and in fact in Chinese it's sometimes it's a bit fuzzy to even say where a word begins and ends. And some words are kind of not even the pieces are not next to each other. Another reason is that what we're going to be doing is after we've split it into words or something like words we're going to be getting a list of all of the unique words that appear which is called the vocabulary. And every one of those unique words is going to get a number. As you'll see later on the bigger the vocabulary the more memory is going to get used. The more data we'll need to train. In general we don't want a vocabulary to be too big. So instead nowadays people tend to tokenize into something called sub words which is pieces of words. So I'll show you what it looks like. So the process of turning it into smaller units like words is called tokenization and we call them tokens instead of words. A token is just like the more general concept of like whatever we're splitting it into. So we're going to get hugging face transformers and hugging face datasets doing our work for us. And so what we're going to do is we're going to turn our pandas data frame into a into a hugging face datasets dataset. It's a bit confusing. PyTorch has a class called dataset and hugging face has a class called dataset and they're different things. Okay so this is a hugging face dataset. Hugging face datasets dataset. So we can turn a data frame into a dataset just using the from pandas method. And so we've now got a dataset. So if we take a look it just tells us it's got these features. Okay and remember input is the one we just created with the concatenated strings and here's those 36,000 rows. Okay so now we're going to do these two things. Tokenization which is to split each text up into tokens and then numericalization which is to turn each token into its unique ID based on where it is in the vocabulary. The vocabulary remember being the unique the list of unique tokens. Now particularly in this stage tokenization there's a lot of little decisions that have to be made. The good news is you don't have to make them because whatever pre-trained model you used the people that pre-trained it made some decisions and you're going to have to do exactly the same thing otherwise you'll end up with a different vocabulary to them and that's going to mess everything up. So that means before you start tokenizing you have to decide on what model to use. Hugging face transformers is a lot like Tim. It has a library of I believe hundreds of models. I guess I shouldn't say hugging face transformers it's really the hugging face model hub. 44,000 models so even many more even than Tim's image models and so these models they vary in a couple of ways. There's a variety of different architectures just like in Tim but then something which is different to Tim is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could type patent and see if there's any pre-trained patent there is. Okay so there's a patents there's a whole lot of pre-trained patent models. Isn't that amazing? So quite often thanks to the hugging face model hub you can start your pre-trained model with something that's actually pretty similar to what you actually want to do or at least was trained on the same kind of documents. Having said that there are some just generally pretty good models that work for a lot of things a lot of the time and Deberta v3 is is certainly one of those. This is a very new area NLP has been like practically really effective for you know general users for only a year or two where else for computer vision it's been quite a while. So you'll see you'll find that like a lot of things aren't as quite well bedded down. I don't have a picture to show you of which models are the best or the fastest and the most accurate and whatever right. This a lot of this stuff is like stuff that we're figuring out as a community using competitions like this in fact and this is one of the first NLP competitions actually in the kind of modern NLP era. So you know we've been studying these competitions closely and yeah I can tell you that Deberta is actually a really good starting point for a lot of things so that's why we've picked it. So we pick our model and just like in Tim for image you know our models there's often going to be a small a medium a large and of course we should start with small right because small is going to be faster to train we're going to be doing able to do more iterations and so forth. Okay so at this point remember the only reason we picked our model is because we have to make sure we tokenize in the same way. To tell transformers that we want to tokenize the same way that the people that built a model did we use something called auto tokenizer. It's nothing fancy it's basically just a dictionary which says oh which model uses which tokenizer. So when we say auto tokenizer from pre-trained it will download the vocabulary and the details about how this particular model tokenized data set. So at this point we can now take that tokenizer and pass the string to it. So if I pass the string g'day folks I'm Jeremy from fast.ai you'll see it's kind of putting it into words kind of not. So if you've ever wondered whether g'day is one word or two you know it's actually three tokens according to this tokenizer and I'm is three tokens and fast.ai is three tokens this punctuation is a token and so you kind of get the idea. These underscores here that represents the start of a word right so that's kind of this is concept that like the start of a word is kind of part of the token. So if you see a capital I in the middle of a word versus the start of a word that's kind of means a different thing. So this is what happens when we tokenize this sentence using the tokenizer that the Deberta v3 developers used. So here's a less common unless you're a big platypus fan like me less common sentence a platypus is an ornithorhynchus and athenus and so okay in this particular vocabulary platypus got its own word its own token but ornithorhynchus didn't and so I still remember grade one for some reason our teacher got us all to learn how to spell ornithorhynchus so one of my favorite words. So you can see here it's been split into or, ni, tho, rink, us. So every one of these tokens you see here is going to be in the vocabulary right the list of unique tokens that was created when this when this particular model this pre-trained model was first trained so somewhere in that list we'll find underscore capital A and it'll have a number and so that's how we'll be able to turn these into numbers. So this first process is called tokenization and then the thing where we take these tokens and turn them into numbers is called numericalization. So our data set remember we put our string into the input field so here's a function that takes a document grabs its input and tokenizes it okay so we'll call this our tokenization function. Tokenization can take a minute or two so we may as well get all of our processes used doing it at the same time to save some time so if you use the data set dot map it will parallelize that process and just pass in your function. Make sure you pass batched equals true so it can do a bunch at a time and behind the scenes this is going through something called the tokenizes library which is a pretty optimized Rust library that uses you know SIMD and parallel processing and so forth so with batched equals true it'll be able to do more stuff at once. So look it only took six seconds so pretty first. So now when we look at a row of our tokenized data set it's going to contain exactly the same as our original data set no it's not going to take exactly the same as the original data set it's going to contain exactly the same input as our original data set and it's also going to contain a bunch of numbers. These numbers are the position in the vocabulary of each of the tokenized each of the tokens in the string. So we've now successfully turned a string into a list of numbers. So that is a great first step. So we can see how this works we can see for example that we've got of at this a separate word that's going to be an underscore OF in the vocabulary we can grab the vocabulary look up of find that it's 265 and check here yep here it is 265 okay so it's not rocket science right it's just looking stuff up in a dictionary to get the numbers. Okay so that is the tokenization and numericalization necessary in NLP to turn our documents into numbers to allow us to put it into our model. Any questions so far John? Excuse me yeah thanks Jeremy so there's a there's a couple and this seems like a good time to throw them out and it's related to how you've formatted your input data into these sentences that you've just tokenized. Yeah so one question was really about how you choose those keywords and the the order of the fields that you you know so so I guess just you know interested in an explanation is there any is it is it more art or science how you know it's arbitrary I tried a few things I tried X you know I tried putting them backwards you know doesn't matter we just want some way something that it can learn from right so if I just concatenated it without these headers before each one it wouldn't know where abatement of pollution ended and where abatement started right so I did just something that it can learn from this is a nice thing about neural nets they're so flexible as long as you give it the information somehow it doesn't really matter how you give it to give it the information as long as it's there right I could have used punctuation I could have put like I don't know one semicolon here and two here and three here yeah it's not a big deal like at the level where you're like trying to get an extra half a percent to get up the leaderboard or cackle competition you may find tweaking these things makes tiny differences but in practice you won't generally find it it matters too much right thank you and I guess the second part of that excuse me again somebody's asking if one of their their fields was a particularly long so it was a thousand characters is there any special handling required there do you need to do you need to re-inject those kind of special marker tokens does it does it change if you've got much bigger fields that you're trying to learn and query yeah long documents and you'll am fit require no special consideration so IMDB in fact has multi thousand word movie reviews and it works great to this day you'll am fit is probably the best approach you know for reasonably quickly and easily using large documents otherwise if you use transformer based approaches large documents are challenging specifically transformers has to basically have to do the whole document at once where else you all am fit can split it into multiple pieces and read it gradually and so that means you'll find that people trying to work with large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory so yeah generally speaking I would say if you're trying to do stuff with documents of over mm-hmm 2,000 words you might want to look at you all am fit try transformers see if it works for you but you know I'd certainly try both under 2,000 words you know transformers should be fine unless you've got a you know nothing but like a laptop GPU or something with not much memory so hacking face transformers has these you know as I say it right now that I find them somewhat obscure and not particularly well documented expectations about your data that you kind of have to figure out and one of those is that it expects that your target is a column called labels so once I figured that out I just went got our tokenized data set and renamed our score column to labels and everything started working so probably is you know I don't know if at some point they'll make this a bit more flexible but probably best to just call your target labels and life will be easy you might have seen back when I do an LS path that there was another data set there called test CSV and if you look at it it looks a lot like our training set train our other CSV that we've been working with but it's missing the score the labels this is this is called a test set and so we're going to talk a little bit about that now because my claim here is that perhaps the most important idea and machine learning is the idea of having separate training validation and test datasets yeah so test and validation sets are all about identifying and controlling for something called overfitting and we're going to try and learn about this through example so this is the same information that's in that Kaggle notebook I've just put it on some slides here so I'm going to create a function here called plot poly and I'm actually going to use the same data that I don't know if you're a member we used it earlier for trying to fit this quadratic we created a X and some X and some Y data this is the data we're going to use and we're going to use this to look at overfitting so the details of this function don't matter too much what matters is what we do with it which is that it allows us to basically pass in the degree of a polynomial so for those of you that remember a first degree polynomial is just a line it's y equals a X a second degree polynomial will be y equals a squared X plus B X plus C third degree polynomial will have a cubic fourth degree you know quartic and so forth and what I've done here is I've plotted what happens if we try to fit a line to our data it doesn't fit very well so what happened here is we we did a linear regression and what we're using here is a very cool library called scikit-learn scikit-learn is something that you know I think it'd be fair to say it's mainly designed for kind of classic machine learning methods like kind of linear aggression and stuff like that I'm very advanced versions of these things but it's also great for doing these quick and dirty things so in this case I wanted to do a what's called a polynomial regression which is fitting your polynomial to data and it's just these two lines of code it's a super nice library so in this case a degree one polynomial is just a line so I fit it and then I show it with the data and there it is now that's what we call underfit which is to say there's not enough kind of complexity in this model I fit to to match the data that's there so an underfit model is a problem it's got to be systematically biased you know all the stuff up here we're going to be predicting too low all the stuff down here we're predicting too low all the stuff in the middle would be predicting too high a common misunderstanding is that like simpler models are kind of more reliable in some way but models that are too simple will be systematically incorrect as you see here what happens if we fit a 10 degree polynomial that's not great either in this case it's not really showing us what the actual remember this was originally a quadratic this is meant to match right particularly at the ends here it's predicting things that are way above what we would expect in real life right and it's trying to get really it's trying really hard to get through this point but clearly this point was just some noise right so this is what we call overfit it's done a good job of fitting to our exact data points but if we sample some more data points from this distribution honestly we probably would suspect they're not going to be very close to this particularly if they're a bit beyond the edges so that's what overfitting looks like we don't want under fitting all over fitting now under fitting is actually pretty easy to recognize because we can actually look at our training data and see that it's not very close over fitting is a bit harder to recognize because the training data is actually very close now on the other hand here's what happens if we fit a quadratic and here I've got both the real line and the fit line and you can see they're pretty close and that's of course what we actually want so how do we tell whether we have something more like this or something more like this well what we do is we do something pretty straightforward is we take our original data set these points and we remove a few of them so let's say 20% of them we then fit our model using only those points we haven't removed and then we measure how good it is by looking at only the points we removed so in this case let's say we had removed I'm just trying to think if I had removed this point here right then it might have kind of gone off down over here and so then when we look at how well it fits we would say oh this one's miles away the model the data that we take away and don't let the model see it when it's training is called the validation set so in first AI we've seen splitters before right the splitters are the things that separate out the validation set fast AI won't let you train a model without a validation set fast AI always shows you your metrics so things like accuracy measured only on the validation set this is really unusual most libraries make it really easy to shoot yourself in the foot by not having a validation set or accidentally not using it correctly so fast AI won't even let you do that so you've got to be particularly careful when using other libraries hacking face transformers is good about this so they make sure that they do show you your metrics on a validation set now creating a good validation set is not generally as simple as just randomly pulling some of your data out of your model out of the data that you passed that you train with your model the reason why is imagine that this was the data you were trying to fit something to okay and you randomly remove some so it looks like this that looks very easy doesn't it because you've kind of like still got all the data you would want around the points and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates so if you credit your validation set by randomly removing stuff from the middle it's not really a good indication of how you're going to be using this model instead you should truncate and remove the last couple of weeks so if this was your validation set and this is your training set that's going to be actually testing whether you can use this to predict the future rather than using it to predict the past Kaggle competitions are a fantastic way to test your ability to create a good validation set because Kaggle competitions only allow you to submit generally a couple of times a day the data set that you are scored on in the leaderboard during that time is actually only a small subset in fact is a totally separate subset to the one you'll be scored on on the end of the competition and so most beginners on Kaggle overfit and it's not until you've done it that you will get that visceral feeling of like oh my god I overfit in the real world outside of Kaggle you will often not even know that you overfit you just destroy value for your organization silently so it's a really good idea to do this kind of stuff on Kaggle a few times first in real competitions to really make sure that you are confident you know how to avoid overfitting how to find a good validation set and how to interpret it correctly and you really don't get that until you screw it up a few times good example of this was there was a distracted driver competition on Kaggle there are these kind of pictures from inside a car and the idea was that you had to try and predict whether somebody was driving in a distracted way or not and on Kaggle they did something pretty smart the test set so the thing that they scored you on the leaderboard contained people that didn't exist at all in the competition data that you train the model with so if you wanted to create an effective validation set in this competition you would have to make sure that you separated the photos so that your validation set contained photos of people that aren't in the data you're training your model on there was another one like that the Kaggle fisheries competition which had boats that didn't appear so there were basically pictures of boats you're meant to try to guess predict what fish were in the pictures and it turned out that a lot of people accidentally figured out what the fish were by looking at the boat because certain boats tended to catch certain kinds of fish and so by messing up their validation set they were really overconfident of the accuracy of their model I'll mention in passing if you've been around Kaggle a bit you'll see people talk about cross validation a lot I'm just going to mention be very very careful cross validation is explicitly not about building a good validation set so you've got to be super super careful if you ever do that another thing I'll mention is that scikit-learn conveniently offers something called train test split as does hugging face datasets as does fast AI we have something called random split it can be encouraged it can almost feel like it's encouraging you to use a randomized validation set because there are these methods that do it for you but yeah be very very careful because very very often that's not what you want okay so if you want what a validation set is so that's the bit that you pull out of your data that you don't train with but you do measure your accuracy with so what's a test set it's basically another validation set but you don't even use it for tracking your accuracy while you build your model why not well imagine you tried two new models every day for three months that's how long a Kaggle competition goes for so you would have tried 180 models and then you look at the accuracy on the validation set for each one some of those models you would have got a good accuracy on the validation set potentially because of pure chance just a coincidence and then you get all excited and you submit that to Kaggle and you think you're going to win the competition and you mess it up and that's because you actually overfit using the validation set so you actually want to know whether you've really found a good model or not so in fact on Kaggle they have two two test sets they've got the one that gives you feedback on the leaderboard during the competition and a second test set which you don't get to see until after the competition is finished so in real life you've got to be very careful about this not to try so many models during your model building process that you accidentally find one that's good by coincidence and only if you have a test set that you've held out or you know that now that leads to the obvious question which is very challenging is you spent three months working on a model worked well on your validation set you did a good job of locking that test set away in a safe so you weren't allowed to use it and at the end of the three months you finally checked in on the test set and it's terrible what do you do honestly you have to go back to square one you know there really isn't any choice other than starting again so this is tough but it's better to know right better to know than to not know so that's what a test sets for so you've got a validation set what are you going to do with it what you're going to do with a validation set is you're going to measure some metrics so a metric is something like accuracy it's a number that tells you how good is your model now on Kaggle this is very easy what metrics should we use well they tell us go to overview click on evaluation and find out and it says oh we will evaluate on the Pearson correlation coefficient therefore this is the metric you care about so one obvious question is is this the same as the loss function is this the thing that we will take the derivative of and find the gradient and use that to improve our parameters during training and the answer is maybe sometimes but probably not for example consider accuracy now if we were using accuracy to calculate our derivative and get the gradient you can have a model that's actually slightly better you know it's slightly like it's doing a better job of recognizing dogs and cats but not so much better that it's actually caused any incorrectly classified cat to become a dog so the accuracy doesn't change at all so the gradient zero you don't want stuff like that you don't want bumpy functions because they don't have nice gradients often they don't have gradients at all they're basically zero nearly everywhere you want a function that's nice and smooth something like for instance the average absolute error mean absolute error which we've used before so that's the difference between your metrics and your loss now be careful right because when you're training your model spending all of its time trying to improve the loss and most of the time that's not the same as the thing you actually care about which is your metric so you got to keep those two different things in mind the other thing to keep in mind is that in real life you can't go to a website and be told what metrics use in real life the the the model that you choose there isn't one number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time in real life the model you use is a part of a complex process often involving humans both as users or customers and as people you know involved in in as part of the process there's all kinds of things that are changing over time and there's lots and lots of outcomes of decisions that are made one metric is not enough to capture all of that unfortunately because it's so convenient to pick one metric and use that to say I've got a good model that very often finds its way into into industry into government where people roll out these things that are good on the one metric that happened to be easy to measure and again and again we found people's lives turned upside down because of how badly they get screwed up by models that have been incorrectly measured using a single metric so my partner Rachel Thomas has written this article which I recommend you read about the problem with metrics is a big problem for AI it's not just an AI thing there's actually this thing called good arts law that states when I measure becomes a target it ceases to be a good measure the thing is so when I was a management consultant you know 20 years ago we were always kind of part of these strategic things trying to like find key performance indicators and ways to kind of you know set commission rates for salespeople and we were really doing a lot of this like stuff which is basically about picking metrics and you know we see that happen go wrong in industry all the time AI is dramatically worse because AI is so good at optimizing metrics and so that's why you have to be extra extra extra careful about metrics when you are trying to use a model in real life anyway as I said in Kaggle we don't have to worry about any of that we're just going to use the Pearson correlation coefficient which is all very well as long as you know what the hell the Pearson correlation coefficient is if you don't let's learn about it so Pearson correlation coefficient is usually abbreviated using letter R and it's the most widely used measure of how similar two variables are and so if your predictions are very similar to the real values then the Pearson correlation coefficient will be high and that's what you want R can be between minus one and one minus one means you predicted exactly the wrong answer which in a Kaggle competition be great because then you can just reverse all of your answers and you'll be perfect plus one means you got everything exactly correct generally speaking in courses or textbooks when they teach you about the Pearson correlation coefficient at that point at this point they will show you a mathematical function I'm not going to do that because that tells you nothing about the Pearson correlation coefficient what we actually care about is not the mathematical function but how it behaves and I find most people even who work in data science have not actually looked at a bunch of data sets to understand how R behaves so let's do that right now so that you're not one of those people the best way I find to understand how data behaves in real life is to look at real life data so there's a data set scikit-learn comes with a number of data sets and one of them is called California housing and it's a data set where each row is a district and it's kind of demographic it's sorry it's information some demographic information about different districts and about the value of houses in that district I'm not going to try to plot the whole things it's too big and this is a very common question I have from people is how do I plot data sets with far too many points the answer is very simple get less points so I just randomly grab a thousand points whatever you see with a thousand points it's going to be the same as what you see with a million points there's no point no reason to plot huge amounts of data generally just grab a random sample now numpy has something called core coef so get the correlation coefficient between every variable and every other variable and it returns a matrix so I can look down here and so for example here is the correlation coefficient between variable 1 and variable 1 which of course is exactly perfectly 1.0 right because variable 1 is the same as variable 1 here is the small inverse correlation between variable 1 and variable 2 and medium-sized positive correlation between variable 1 and variable 3 and so forth this is symmetric about the diagonal because the correlation between variable 1 and variable 8 is the same as the correlation between variable 8 and variable 1 so this is a correlation coefficient matrix so that's great when we wanted to get a bunch of values all at once for the Kaggle competition we don't want that we just want a single correlation number if we just pass in a pair of variables we still get a matrix which is kind of weird it's kind of it's not weird it's not what we want so we should grab one of these so when I want to grab a correlation coefficient I'll just return the zeroth row first column so that's what core is that's going to be our single correlation coefficient so let's look at the correlation between two things for example may median income and medium house value 0.67 okay is that high medium low how big is that what does it look like so the main thing we need to understand is what these things look like so what I suggest we do is we're going to take a 10 minute break nine minute break we'll come back at half past and then we're going to look at some examples of correlation coefficients okay welcome back so what I've done here is I've created a little function called show correlations and a passing a data frame and a couple of columns as strings going to grab each of those columns as series do a scatter plot and then show the correlation so we already mentioned medium income and medium house valuation of 0.68 so here it is here's what point six eight looks like so you know I don't know if you had some intuition about what you expected but as you can see it's still plenty of variation even at that reasonably high correlation also you can see here that visualizing your data is very important if you are working with this data set because you can immediately see all these dots along here that's clearly truncation right so this is like when it's not until you look at pictures like this that you're going to pick stuff like this up pictures are great oh little trick on the scatter plot I put alpha is 0.5 that creates some transparency for these kind of scatter plots that really helps because it like kind of creates darker areas in places where there's lots of dots so yeah the alpha and scatter plots is nice okay here's another pair so this one's gone down from point six eight to point four three median income versus the number of rooms per house as you'd expect more rooms it's more income but this is a very weird looking thing now you'll find that a lot of these statistical measures like correlation rely on the square of the difference and when you have big outliers like this the square of the difference goes crazy and so this is another place we'd want to look at the data first you say oh that's that's going to be a bit of an issue there's probably more correlation here but there's a few examples of some houses with lots and lots of room where people that aren't very rich live maybe these are some kind of shared shared accommodation or something so R is very sensitive to outliers so let's get rid of the houses the rooms with 15 rooms the houses with 15 rooms or more and now you can see it's gone up from point four three to point six eight even though we probably only got rid of one two three four five six even got you got rid of seven data points so we're going to be very careful of outliers and that means if you're trying to win a cackle competition where the metric is correlation and you just get a couple of rows really badly wrong then that's going to be a disaster to your score right so you've got to make sure that you do a pretty good job of every room so there's what a correlation of point six eight looks like okay here's a correlation of point three four and this is kind of interesting isn't it because point three four sounds like quite a good relationship but you almost can't see it so this is something I strongly suggest is if you're working with a new metric is draw some pictures of a few different levels of that metric to kind of try to get a feel for like what does it mean you know what does point six look like what does point three look like and so forth and here's an example of a correlation of minus point two this very slight negative slope okay so there's just more of a kind of a general tip of something I like to do when playing with a new metric and I recommend you do as well I think we've now got a sense of what the correlation feels like now you can go look up the equation on Wikipedia if you're into that kind of thing we need to report the correlation after each epoch because we want to know how our training is going hugging face expects you to return a dictionary because it's going to use the keys of the dictionary to like label each metric so here's something that gets the correlation and returns it as a dictionary with the label Pearson okay so we've done metrics we've done our training validation split oh we might have actually skipped over the bit where we actually did the split today I did so to actually do the split because in this Kaggle competition I've got another notebook we'll look at later where we actually split this properly but here we're just going to do a random split just to keep things simple for now of 25% will be of the data will be a validation set so if we go DS train test split it returns a data set dict which has a train and a test so that looks a lot like a data sets object in fast AI very similar idea so this will be the thing that we'll be able to train with so it's going to train with this data set and return the metrics on this data set this is really a validation set but hugging face data sets calls it test okay we're now ready to train our model in fast AI we use something called a learner the equivalent in hugging face transformers is called trainer so we'll bring that in something we'll learn about quite shortly is the idea of many batches and batch sizes in short each time we pass some data to our model for training it's going to return it's going to send through a few rows at a time to the GPU so that it can calculate those in parallel those bunch of rows is called a batch or a mini batch and the number of rows is called the batch size so here we're going to set the batch size to 128 generally speaking the larger your batch size the more it can do in parallel at once and it'll be faster but if you make it too big you're going to out of memory error on your GPU so you know it's a bit of trial and error to find a batch size that works epochs we've seen before then we've got the learning rate we'll talk in the next lesson unless we get to this lesson about a technique to automatically find a or semi automatically find a good learning rate we already know what a learning rate is from the last lesson I played around and found one that seems to train quite quickly without falling apart so I just tried a few generally I kind of you know if I if I don't have a so hacking face transformers doesn't have something to help you find the learning rate this the integration we're doing in fast AI will let you do that but if you're using a framework that doesn't have that you can just start with a really low running rate and then kind of double it and keep doubling it until it falls apart hacking face transformers uses this thing called training arguments which is a class we just provide all of the kind of configuration so you have to tell it what your learning rate is this stuff here is the same as what we call basically fit one cycle in fast AI you always want this to be true because it's going to be faster pretty much and then the root this stuff here you can probably use exactly the same every time there's a lot of boilerplate compared to fast AI as you see this stuff you can probably use the same every time okay so we now need to create our model so the equivalent of the vision learner function that we've used to automatically create a reasonable vision model in hacking face transformers they've got lots of different ones depending on what you're trying to do so we're trying to do classification as we've discussed of sequences so if we call auto model for sequence classification it will create a model that is appropriate for classifying sequences from a train pre-trained model and this is the name of the model that we used dead earlier that the bird of e3 it has to know when it adds that random matrix to the end how many outputs it needs to have so we have one label which is the score so that's going to create our model and then this is the equivalent of creating a learner it contains a model and the data the training data and the test data again there's a lot more boilerplate here than fast AI but you can kind of see the same basic steps so here we just have to do a little bit more manually but it's not you know there's nothing too crazy so it's going to tokenize it for us using that function and then these are the matrix matrix matrix that will print out each time that's that little function we created which returns a dictionary at the moment I find hugging face transformers very verbose it spits out lots and lots and lots of text which you can ignore and we can finally call train which will spit out much more text again which you can ignore and as you can see as it trains it's printing out the loss and here's our Pearson correlation coefficient so it's training and we've got a point eight three four correlations that's pretty cool right I mean it took what does next you say but it just took a here we are five minutes to run maybe that's five minutes per epoch on Kaggle which doesn't have particularly great GPUs but good for free and we've got something that is you know got a very high level of correlation in assessing how similar the two columns are and the only reason it could do that is because it used a pre-trained model right there's no way you could just have that tiny amount of information and figure out whether those two columns are very similar this pre-trained model already knows a lot about language it already has a good sense of whether two phrases are similar or not and we've just fine-tuned it you can see given that after one epoch it was already at point eight you know we this was a model that already did something pretty close to what we needed it didn't really need that much extra tuning for this particular task I've got any questions there John yeah we do it's actually a bit back on the topic before where you were showing us the visual interpretation of the Pearson coefficient and you're talking about outliers yeah and we've got a question here from Kevin asking how do you decide when it's okay to remove outliers like you you you pointed out some in that data set and clearly your model is going to train a lot better if you clean that up but and I think Kevin's point here is you know those kinds of outliers will probably exist in the test set as well so I think he's just looking for some practical advice on how you handle that in a more general sense so outliers should never just be removed like for modeling so if we take the example of the California date housing data set you know if I was really working with that data set in real life I would be saying oh that's interesting it seems like there's a separate group of districts with a different kind of behavior now my guess is that they're going to be kind of like dorms or something like that you know probably low-income housing and so I would be saying like oh clearly from looking at this data set these two different groups can't be treated the same way they have very different behaviors and I would probably split them into two separate analyses you know the word outlier in it kind of exists in a statistical sense right there can be things that are well outside our normal distribution and mess up our kind of metrics and things it doesn't exist in a real sense it doesn't exist in a sense of like oh things that we should like ignore or throw away you know some of the most useful kind of insights I've had in my life in data projects has been by digging into outliers so-called outliers and understanding what are they where did they come from and it's kind of often in those edge cases that you discover really important things about like where processes go wrong or about you know kinds of behaviors you didn't even know existed or indeed about you know kind of labeling problems or process problems which you really want to fix them at the source because otherwise when you go into production you're going to have more of those so called outliers so yeah I'd say never delete outliers without investigating them and having a strategy for like understanding where they came from and like what should you do about them all right so now that we've got a trained model you'll see that it actually behaves you know really a lot like a fast AI learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity I'm like oh yeah this looks like stuff I've seen before you know like a bit more wordy and some slight changes but it really is very very similar to the way we've done it before because now that we've got a trained trainer rather than learner we can call predict and now we're going to pass in our data set from the Kaggle test file and so that's going to give us our predictions which we can cast to float and here they are so here are the predictions we made of similarity now again not just for your inputs but also if your outputs always look at them always right and interestingly I looked at quite a few Kaggle notebooks from other people for this competition and nearly all of them had the problem we have right now which is negative predictions and predictions over one so I'll be showing you how to fix this in a more proper way maybe hopefully in the next lesson but for now you know we could at least just round these off right because we know that none of the scores are going to be bigger than one or smaller than zero so our correlation coefficient will definitely improve if we at least round this up to zero and round this down to one as I say there are better ways to do this but that's certainly better than nothing so in pytorch you might remember from when we looked at value there's a thing called clip and that will clip everything under zero to zero and everything over one to one and so now that looks much better so here's our predictions so Kaggle expects submissions to generally be in a CSV file and hacking face datasets it kind of looks a lot like pandas really we can create our submission file from with our two columns called CSV and there we go that's basically it so yeah you know it's it's it's kind of nice to see how you know in the sense how far deep learning has come since we started this course a few years ago that that nowadays you know there are multiple libraries around to kind of do this the same thing we can you know use them in multiple application areas they all look kind of pretty familiar they're reasonably beginner-friendly and NLP because it's kind of like the most recent area that's really become effective in the last year or two is probably where the biggest opportunities are for you know big wins both in research and commercialization and so if you're looking to build a startup for example one of the key things that VCs look for you know that they'll ask is like well why now you know why why would you build this company now and of course you know with NLP the answer is really simple it's like it can often be like well until last year this wasn't possible you know where it took ten times more time or it took ten times more money or whatever so I think NLP is a huge opportunity area okay so it's worth thinking about both use and misuse of modern NLP and I want to show you a subreddit here is a conversation on a subreddit from a couple of years ago I'll let you have a quick read of it so the question I want you to be thinking about is what subreddit do you think this comes from this this debate about military spending and the answer is it comes from a subreddit that posts automatically generated conversations between GPT-2 models now this is a like a totally previous generation of model they're much much better now so even then you could see these models were generating context appropriate believable pros you know I would strongly believe that like any of our kind of like upper tier of competent fast AI alumni would be fairly easily able to create a bot which could create context appropriate pros on Twitter or Facebook groups or whatever lay you know arguing for a side of an argument and you could scale that up such that 99% of Twitter was these bots and nobody would know you know nobody would know and that's very worrying to me because a lot of you know a lot of kind of the way people see the world is now really coming out of their their social media conversations which at this point they're they're controllable like it would not be that hard to create something that's kind of optimized towards moving a point of view amongst a billion people you know in a very subtle way very gradually over a long period of time by multiple bots each pretending to argue with each other and one of them getting the upper hand and so forth here is the start of a article in the Guardian which I'll let you read this article was you know quite long these are just the first few paragraphs and at the end it explains that this article was written by GPT-3 it was given the instruction please write a short op-ed around 500 words keep the language simple and concise focus on why humans have nothing to fear from AI so GPT-3 produced eight outputs and then they say basically the editors at the Guardian did about the same level of editing that they would do for humans in fact they found it a bit less editing required than a humans so you know again like you can create longer pieces of context appropriate prose designed to argue a particular point of view what kind of things might this be used for you know there we won't know probably for decades if ever but sometimes we get a clue based on older technology here's something from back 2017 and the pre kind of deep learning NLP days there were millions of submissions to the FTC about the net neutrality situation in America very very heavily biased towards the point of view of saying we want to get rid of net neutrality an analysis by Jeff Cowell showed that something like 99% of them and in particular nearly all of the ones which were pro removal of net neutrality were clearly auto-generated by basically if you look at the green there's like selecting from a menu so we've got Americans as opposed to Washington bureaucrats deserve to enjoy the services they desire individuals as opposed to Washington bureaucrats should be just little people like me as opposed to so-called experts and you get the idea now this is an example of a very very you know simple approach to auto generating huge amounts of text we don't know for sure but it looks like this might have been successful because this went through you know despite what seems to be actually overwhelming disagreement from the public that everybody almost everybody likes net neutrality the FTC got rid of it and this was a big part of the basis was like oh we got all these comments from the public and everybody said they don't want net neutrality so imagine a similar thing where you absolutely couldn't do this you couldn't figure it out because everyone was really very compelling and very different that's you know it's kind of worrying about how we deal with that you know I will say when I talk about this stuff often people say oh no worries we'll be able to model to recognize you know bot generated content but you know if I put my black hat on I'm like no that's not going to work right if you told me to build something that beats the bot classifiers I'd say no worries easy you know I will take the code or the surface or service or whatever that does the bot classifying and I will include beating that in my loss function and I will fine-tune my model until it beats the bot classifier you know when I used to run an email company we had a similar problem with spam prevention you know spammers could always take a spam prevention algorithm and change their emails until it didn't get the spam prevention algorithm anymore for example so yeah so I'm really excited about the opportunities for for students in this course to build you know I think very valuable businesses really cool research and so forth using these pretty new NLP techniques that are now pretty accessible and I'm also really worried about the things that might go wrong I do think though that the more people that understand these capabilities the less chance they'll go wrong John was there some questions yeah I mean it's a throwback to the to the workbook that you had before yeah that's the one the question Manikandan is asking shouldn't num labels be five zero zero point two five zero point five zero point seven five one instead of one isn't the target a categorical or are we considering this as a regression problem yeah it's a good question so there's one label because there's one column even if this was being treated as a categorical problem with five categories it's still considered one label in this case though we're actually treating it as a regression problem it's just one of the things it's a bit tricky I was trying to figure this out just the other day it's not documented as far as I can tell but on the hugging face transformers website but if you pass in one label to auto model for sequence classification it turns it into a regression problem which is actually why we ended up with predictions that were less than zero and bigger than one so we'll be learning next time about the use of sigmoid functions to resolve this problem and that should that should fix it up for us okay great well thanks everybody I hope you enjoyed learning about NLP as much as I enjoyed putting this together I'm really excited about it and can't wait for next week's lesson see ya", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.16, "text": " Hi everybody and welcome to Practical Deep Learning for Coders lesson 4, which", "tokens": [50364, 2421, 2201, 293, 2928, 281, 19170, 804, 14895, 15205, 337, 383, 378, 433, 6898, 1017, 11, 597, 50672, 50672, 286, 519, 307, 264, 6898, 300, 257, 688, 295, 264, 9837, 685, 294, 264, 1768, 362, 668, 881, 50912, 50912, 2919, 466, 570, 309, 311, 689, 321, 434, 516, 281, 483, 512, 3879, 777, 2527, 11, 51212, 51212, 3879, 777, 4829, 321, 600, 1128, 5343, 949, 13, 492, 434, 516, 281, 2060, 3303, 51459, 51459, 2856, 9007, 294, 38095, 293, 291, 603, 915, 456, 456, 307, 6451, 257, 7187, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.14680187598518704, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.06543125957250595}, {"id": 1, "seek": 0, "start": 6.16, "end": 10.96, "text": " I think is the lesson that a lot of the regulars in the community have been most", "tokens": [50364, 2421, 2201, 293, 2928, 281, 19170, 804, 14895, 15205, 337, 383, 378, 433, 6898, 1017, 11, 597, 50672, 50672, 286, 519, 307, 264, 6898, 300, 257, 688, 295, 264, 9837, 685, 294, 264, 1768, 362, 668, 881, 50912, 50912, 2919, 466, 570, 309, 311, 689, 321, 434, 516, 281, 483, 512, 3879, 777, 2527, 11, 51212, 51212, 3879, 777, 4829, 321, 600, 1128, 5343, 949, 13, 492, 434, 516, 281, 2060, 3303, 51459, 51459, 2856, 9007, 294, 38095, 293, 291, 603, 915, 456, 456, 307, 6451, 257, 7187, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.14680187598518704, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.06543125957250595}, {"id": 2, "seek": 0, "start": 10.96, "end": 16.96, "text": " excited about because it's where we're going to get some totally new material,", "tokens": [50364, 2421, 2201, 293, 2928, 281, 19170, 804, 14895, 15205, 337, 383, 378, 433, 6898, 1017, 11, 597, 50672, 50672, 286, 519, 307, 264, 6898, 300, 257, 688, 295, 264, 9837, 685, 294, 264, 1768, 362, 668, 881, 50912, 50912, 2919, 466, 570, 309, 311, 689, 321, 434, 516, 281, 483, 512, 3879, 777, 2527, 11, 51212, 51212, 3879, 777, 4829, 321, 600, 1128, 5343, 949, 13, 492, 434, 516, 281, 2060, 3303, 51459, 51459, 2856, 9007, 294, 38095, 293, 291, 603, 915, 456, 456, 307, 6451, 257, 7187, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.14680187598518704, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.06543125957250595}, {"id": 3, "seek": 0, "start": 16.96, "end": 21.900000000000002, "text": " totally new topic we've never covered before. We're going to cover natural", "tokens": [50364, 2421, 2201, 293, 2928, 281, 19170, 804, 14895, 15205, 337, 383, 378, 433, 6898, 1017, 11, 597, 50672, 50672, 286, 519, 307, 264, 6898, 300, 257, 688, 295, 264, 9837, 685, 294, 264, 1768, 362, 668, 881, 50912, 50912, 2919, 466, 570, 309, 311, 689, 321, 434, 516, 281, 483, 512, 3879, 777, 2527, 11, 51212, 51212, 3879, 777, 4829, 321, 600, 1128, 5343, 949, 13, 492, 434, 516, 281, 2060, 3303, 51459, 51459, 2856, 9007, 294, 38095, 293, 291, 603, 915, 456, 456, 307, 6451, 257, 7187, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.14680187598518704, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.06543125957250595}, {"id": 4, "seek": 0, "start": 21.900000000000002, "end": 25.92, "text": " language processing in LP and you'll find there there is indeed a chapter", "tokens": [50364, 2421, 2201, 293, 2928, 281, 19170, 804, 14895, 15205, 337, 383, 378, 433, 6898, 1017, 11, 597, 50672, 50672, 286, 519, 307, 264, 6898, 300, 257, 688, 295, 264, 9837, 685, 294, 264, 1768, 362, 668, 881, 50912, 50912, 2919, 466, 570, 309, 311, 689, 321, 434, 516, 281, 483, 512, 3879, 777, 2527, 11, 51212, 51212, 3879, 777, 4829, 321, 600, 1128, 5343, 949, 13, 492, 434, 516, 281, 2060, 3303, 51459, 51459, 2856, 9007, 294, 38095, 293, 291, 603, 915, 456, 456, 307, 6451, 257, 7187, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.14680187598518704, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.06543125957250595}, {"id": 5, "seek": 2592, "start": 25.92, "end": 30.400000000000002, "text": " about that in the book but we're going to do it in a totally different way to", "tokens": [50364, 466, 300, 294, 264, 1446, 457, 321, 434, 516, 281, 360, 309, 294, 257, 3879, 819, 636, 281, 50588, 50588, 577, 309, 311, 1096, 294, 264, 1446, 13, 682, 264, 1446, 321, 360, 426, 45196, 1228, 264, 15968, 7318, 6405, 1228, 50902, 50902, 18680, 1753, 18161, 9590, 11, 45702, 45, 82, 13, 2692, 321, 434, 516, 281, 360, 746, 1646, 597, 51267, 51267, 307, 321, 434, 516, 281, 360, 4088, 433, 293, 321, 434, 406, 754, 516, 281, 764, 264, 15968, 7318, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.12206441302632176, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.00013542566739488393}, {"id": 6, "seek": 2592, "start": 30.400000000000002, "end": 36.68, "text": " how it's done in the book. In the book we do NLP using the Fast AI library using", "tokens": [50364, 466, 300, 294, 264, 1446, 457, 321, 434, 516, 281, 360, 309, 294, 257, 3879, 819, 636, 281, 50588, 50588, 577, 309, 311, 1096, 294, 264, 1446, 13, 682, 264, 1446, 321, 360, 426, 45196, 1228, 264, 15968, 7318, 6405, 1228, 50902, 50902, 18680, 1753, 18161, 9590, 11, 45702, 45, 82, 13, 2692, 321, 434, 516, 281, 360, 746, 1646, 597, 51267, 51267, 307, 321, 434, 516, 281, 360, 4088, 433, 293, 321, 434, 406, 754, 516, 281, 764, 264, 15968, 7318, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.12206441302632176, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.00013542566739488393}, {"id": 7, "seek": 2592, "start": 36.68, "end": 43.980000000000004, "text": " recurrent neural networks, RNNs. Today we're going to do something else which", "tokens": [50364, 466, 300, 294, 264, 1446, 457, 321, 434, 516, 281, 360, 309, 294, 257, 3879, 819, 636, 281, 50588, 50588, 577, 309, 311, 1096, 294, 264, 1446, 13, 682, 264, 1446, 321, 360, 426, 45196, 1228, 264, 15968, 7318, 6405, 1228, 50902, 50902, 18680, 1753, 18161, 9590, 11, 45702, 45, 82, 13, 2692, 321, 434, 516, 281, 360, 746, 1646, 597, 51267, 51267, 307, 321, 434, 516, 281, 360, 4088, 433, 293, 321, 434, 406, 754, 516, 281, 764, 264, 15968, 7318, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.12206441302632176, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.00013542566739488393}, {"id": 8, "seek": 2592, "start": 43.980000000000004, "end": 50.08, "text": " is we're going to do transformers and we're not even going to use the Fast AI", "tokens": [50364, 466, 300, 294, 264, 1446, 457, 321, 434, 516, 281, 360, 309, 294, 257, 3879, 819, 636, 281, 50588, 50588, 577, 309, 311, 1096, 294, 264, 1446, 13, 682, 264, 1446, 321, 360, 426, 45196, 1228, 264, 15968, 7318, 6405, 1228, 50902, 50902, 18680, 1753, 18161, 9590, 11, 45702, 45, 82, 13, 2692, 321, 434, 516, 281, 360, 746, 1646, 597, 51267, 51267, 307, 321, 434, 516, 281, 360, 4088, 433, 293, 321, 434, 406, 754, 516, 281, 764, 264, 15968, 7318, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.12206441302632176, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.00013542566739488393}, {"id": 9, "seek": 5008, "start": 50.08, "end": 56.839999999999996, "text": " library at all in fact. So what we're going to be doing today is we're going", "tokens": [50364, 6405, 412, 439, 294, 1186, 13, 407, 437, 321, 434, 516, 281, 312, 884, 965, 307, 321, 434, 516, 50702, 50702, 281, 312, 2489, 12, 83, 37726, 257, 659, 12, 17227, 2001, 426, 45196, 2316, 1228, 257, 6405, 1219, 41706, 1851, 51074, 51074, 4088, 433, 13, 823, 2212, 341, 307, 264, 15968, 13, 48698, 1164, 291, 1062, 312, 6359, 983, 51318, 51318, 321, 1116, 312, 1228, 257, 819, 6405, 661, 813, 15968, 7318, 13, 440, 1778, 307, 300, 286, 519, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.11779722713288807, "compression_ratio": 1.541871921182266, "no_speech_prob": 4.682477083406411e-05}, {"id": 10, "seek": 5008, "start": 56.839999999999996, "end": 64.28, "text": " to be fine-tuning a pre-trained NLP model using a library called hugging face", "tokens": [50364, 6405, 412, 439, 294, 1186, 13, 407, 437, 321, 434, 516, 281, 312, 884, 965, 307, 321, 434, 516, 50702, 50702, 281, 312, 2489, 12, 83, 37726, 257, 659, 12, 17227, 2001, 426, 45196, 2316, 1228, 257, 6405, 1219, 41706, 1851, 51074, 51074, 4088, 433, 13, 823, 2212, 341, 307, 264, 15968, 13, 48698, 1164, 291, 1062, 312, 6359, 983, 51318, 51318, 321, 1116, 312, 1228, 257, 819, 6405, 661, 813, 15968, 7318, 13, 440, 1778, 307, 300, 286, 519, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.11779722713288807, "compression_ratio": 1.541871921182266, "no_speech_prob": 4.682477083406411e-05}, {"id": 11, "seek": 5008, "start": 64.28, "end": 69.16, "text": " transformers. Now given this is the Fast.AI course you might be wondering why", "tokens": [50364, 6405, 412, 439, 294, 1186, 13, 407, 437, 321, 434, 516, 281, 312, 884, 965, 307, 321, 434, 516, 50702, 50702, 281, 312, 2489, 12, 83, 37726, 257, 659, 12, 17227, 2001, 426, 45196, 2316, 1228, 257, 6405, 1219, 41706, 1851, 51074, 51074, 4088, 433, 13, 823, 2212, 341, 307, 264, 15968, 13, 48698, 1164, 291, 1062, 312, 6359, 983, 51318, 51318, 321, 1116, 312, 1228, 257, 819, 6405, 661, 813, 15968, 7318, 13, 440, 1778, 307, 300, 286, 519, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.11779722713288807, "compression_ratio": 1.541871921182266, "no_speech_prob": 4.682477083406411e-05}, {"id": 12, "seek": 5008, "start": 69.16, "end": 74.84, "text": " we'd be using a different library other than Fast AI. The reason is that I think", "tokens": [50364, 6405, 412, 439, 294, 1186, 13, 407, 437, 321, 434, 516, 281, 312, 884, 965, 307, 321, 434, 516, 50702, 50702, 281, 312, 2489, 12, 83, 37726, 257, 659, 12, 17227, 2001, 426, 45196, 2316, 1228, 257, 6405, 1219, 41706, 1851, 51074, 51074, 4088, 433, 13, 823, 2212, 341, 307, 264, 15968, 13, 48698, 1164, 291, 1062, 312, 6359, 983, 51318, 51318, 321, 1116, 312, 1228, 257, 819, 6405, 661, 813, 15968, 7318, 13, 440, 1778, 307, 300, 286, 519, 51602, 51602], "temperature": 0.0, "avg_logprob": -0.11779722713288807, "compression_ratio": 1.541871921182266, "no_speech_prob": 4.682477083406411e-05}, {"id": 13, "seek": 7484, "start": 74.84, "end": 82.52000000000001, "text": " that it's really useful for everybody to have experience and practice of using", "tokens": [50364, 300, 309, 311, 534, 4420, 337, 2201, 281, 362, 1752, 293, 3124, 295, 1228, 50748, 50748, 544, 813, 472, 6405, 570, 291, 603, 483, 281, 536, 264, 912, 10392, 6456, 294, 51096, 51096, 819, 2098, 293, 286, 519, 300, 311, 869, 337, 428, 3701, 295, 437, 613, 51324, 51324, 10392, 366, 13, 2743, 286, 534, 411, 264, 41706, 1851, 4088, 433, 6405, 13, 467, 311, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.08671887024589207, "compression_ratio": 1.5527638190954773, "no_speech_prob": 3.704370465129614e-05}, {"id": 14, "seek": 7484, "start": 82.52000000000001, "end": 89.48, "text": " more than one library because you'll get to see the same concepts applied in", "tokens": [50364, 300, 309, 311, 534, 4420, 337, 2201, 281, 362, 1752, 293, 3124, 295, 1228, 50748, 50748, 544, 813, 472, 6405, 570, 291, 603, 483, 281, 536, 264, 912, 10392, 6456, 294, 51096, 51096, 819, 2098, 293, 286, 519, 300, 311, 869, 337, 428, 3701, 295, 437, 613, 51324, 51324, 10392, 366, 13, 2743, 286, 534, 411, 264, 41706, 1851, 4088, 433, 6405, 13, 467, 311, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.08671887024589207, "compression_ratio": 1.5527638190954773, "no_speech_prob": 3.704370465129614e-05}, {"id": 15, "seek": 7484, "start": 89.48, "end": 94.04, "text": " different ways and I think that's great for your understanding of what these", "tokens": [50364, 300, 309, 311, 534, 4420, 337, 2201, 281, 362, 1752, 293, 3124, 295, 1228, 50748, 50748, 544, 813, 472, 6405, 570, 291, 603, 483, 281, 536, 264, 912, 10392, 6456, 294, 51096, 51096, 819, 2098, 293, 286, 519, 300, 311, 869, 337, 428, 3701, 295, 437, 613, 51324, 51324, 10392, 366, 13, 2743, 286, 534, 411, 264, 41706, 1851, 4088, 433, 6405, 13, 467, 311, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.08671887024589207, "compression_ratio": 1.5527638190954773, "no_speech_prob": 3.704370465129614e-05}, {"id": 16, "seek": 7484, "start": 94.04, "end": 100.52000000000001, "text": " concepts are. Also I really like the hugging face transformers library. It's", "tokens": [50364, 300, 309, 311, 534, 4420, 337, 2201, 281, 362, 1752, 293, 3124, 295, 1228, 50748, 50748, 544, 813, 472, 6405, 570, 291, 603, 483, 281, 536, 264, 912, 10392, 6456, 294, 51096, 51096, 819, 2098, 293, 286, 519, 300, 311, 869, 337, 428, 3701, 295, 437, 613, 51324, 51324, 10392, 366, 13, 2743, 286, 534, 411, 264, 41706, 1851, 4088, 433, 6405, 13, 467, 311, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.08671887024589207, "compression_ratio": 1.5527638190954773, "no_speech_prob": 3.704370465129614e-05}, {"id": 17, "seek": 10052, "start": 100.52, "end": 107.28, "text": " absolutely the state-of-the-art in NLP and it's well worth knowing. If you're", "tokens": [50364, 3122, 264, 1785, 12, 2670, 12, 3322, 12, 446, 294, 426, 45196, 293, 309, 311, 731, 3163, 5276, 13, 759, 291, 434, 50702, 50702, 1976, 341, 322, 960, 538, 264, 565, 291, 434, 1976, 309, 321, 486, 1391, 362, 50850, 50850, 7365, 527, 10980, 295, 264, 4088, 433, 6405, 666, 15968, 13, 48698, 370, 309, 311, 51040, 51040, 294, 264, 1399, 295, 5617, 264, 2135, 426, 45196, 733, 295, 7030, 337, 15968, 13, 48698, 370, 291, 603, 51426, 51426, 312, 1075, 281, 10432, 4088, 433, 293, 15968, 13, 48698, 1214, 13, 865, 370, 286, 519, 456, 311, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.13278740231353459, "compression_ratio": 1.668103448275862, "no_speech_prob": 5.389390207710676e-05}, {"id": 18, "seek": 10052, "start": 107.28, "end": 110.24, "text": " watching this on video by the time you're watching it we will probably have", "tokens": [50364, 3122, 264, 1785, 12, 2670, 12, 3322, 12, 446, 294, 426, 45196, 293, 309, 311, 731, 3163, 5276, 13, 759, 291, 434, 50702, 50702, 1976, 341, 322, 960, 538, 264, 565, 291, 434, 1976, 309, 321, 486, 1391, 362, 50850, 50850, 7365, 527, 10980, 295, 264, 4088, 433, 6405, 666, 15968, 13, 48698, 370, 309, 311, 51040, 51040, 294, 264, 1399, 295, 5617, 264, 2135, 426, 45196, 733, 295, 7030, 337, 15968, 13, 48698, 370, 291, 603, 51426, 51426, 312, 1075, 281, 10432, 4088, 433, 293, 15968, 13, 48698, 1214, 13, 865, 370, 286, 519, 456, 311, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.13278740231353459, "compression_ratio": 1.668103448275862, "no_speech_prob": 5.389390207710676e-05}, {"id": 19, "seek": 10052, "start": 110.24, "end": 114.03999999999999, "text": " completed our integration of the transformers library into Fast.AI so it's", "tokens": [50364, 3122, 264, 1785, 12, 2670, 12, 3322, 12, 446, 294, 426, 45196, 293, 309, 311, 731, 3163, 5276, 13, 759, 291, 434, 50702, 50702, 1976, 341, 322, 960, 538, 264, 565, 291, 434, 1976, 309, 321, 486, 1391, 362, 50850, 50850, 7365, 527, 10980, 295, 264, 4088, 433, 6405, 666, 15968, 13, 48698, 370, 309, 311, 51040, 51040, 294, 264, 1399, 295, 5617, 264, 2135, 426, 45196, 733, 295, 7030, 337, 15968, 13, 48698, 370, 291, 603, 51426, 51426, 312, 1075, 281, 10432, 4088, 433, 293, 15968, 13, 48698, 1214, 13, 865, 370, 286, 519, 456, 311, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.13278740231353459, "compression_ratio": 1.668103448275862, "no_speech_prob": 5.389390207710676e-05}, {"id": 20, "seek": 10052, "start": 114.03999999999999, "end": 121.75999999999999, "text": " in the process of becoming the main NLP kind of foundation for Fast.AI so you'll", "tokens": [50364, 3122, 264, 1785, 12, 2670, 12, 3322, 12, 446, 294, 426, 45196, 293, 309, 311, 731, 3163, 5276, 13, 759, 291, 434, 50702, 50702, 1976, 341, 322, 960, 538, 264, 565, 291, 434, 1976, 309, 321, 486, 1391, 362, 50850, 50850, 7365, 527, 10980, 295, 264, 4088, 433, 6405, 666, 15968, 13, 48698, 370, 309, 311, 51040, 51040, 294, 264, 1399, 295, 5617, 264, 2135, 426, 45196, 733, 295, 7030, 337, 15968, 13, 48698, 370, 291, 603, 51426, 51426, 312, 1075, 281, 10432, 4088, 433, 293, 15968, 13, 48698, 1214, 13, 865, 370, 286, 519, 456, 311, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.13278740231353459, "compression_ratio": 1.668103448275862, "no_speech_prob": 5.389390207710676e-05}, {"id": 21, "seek": 10052, "start": 121.75999999999999, "end": 129.24, "text": " be able to combine transformers and Fast.AI together. Yeah so I think there's", "tokens": [50364, 3122, 264, 1785, 12, 2670, 12, 3322, 12, 446, 294, 426, 45196, 293, 309, 311, 731, 3163, 5276, 13, 759, 291, 434, 50702, 50702, 1976, 341, 322, 960, 538, 264, 565, 291, 434, 1976, 309, 321, 486, 1391, 362, 50850, 50850, 7365, 527, 10980, 295, 264, 4088, 433, 6405, 666, 15968, 13, 48698, 370, 309, 311, 51040, 51040, 294, 264, 1399, 295, 5617, 264, 2135, 426, 45196, 733, 295, 7030, 337, 15968, 13, 48698, 370, 291, 603, 51426, 51426, 312, 1075, 281, 10432, 4088, 433, 293, 15968, 13, 48698, 1214, 13, 865, 370, 286, 519, 456, 311, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.13278740231353459, "compression_ratio": 1.668103448275862, "no_speech_prob": 5.389390207710676e-05}, {"id": 22, "seek": 12924, "start": 129.24, "end": 133.8, "text": " a lot of benefits to this and in the end you're going to know how to do NLP in a", "tokens": [50364, 257, 688, 295, 5311, 281, 341, 293, 294, 264, 917, 291, 434, 516, 281, 458, 577, 281, 360, 426, 45196, 294, 257, 50592, 50592, 534, 5456, 6405, 13, 823, 264, 661, 551, 307, 41706, 1851, 4088, 433, 50894, 50894, 1177, 380, 362, 264, 912, 34666, 9482, 300, 15968, 13, 48698, 575, 597, 51160, 51160, 1355, 4098, 337, 26992, 264, 733, 295, 1090, 1496, 11, 291, 458, 11, 1192, 51468, 51468, 12362, 9362, 300, 291, 603, 312, 1228, 881, 295, 264, 565, 307, 406, 382, 733, 295, 1919, 281, 352, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1300124301705309, "compression_ratio": 1.5439330543933054, "no_speech_prob": 3.7038080336060375e-05}, {"id": 23, "seek": 12924, "start": 133.8, "end": 139.84, "text": " really fantastic library. Now the other thing is hugging face transformers", "tokens": [50364, 257, 688, 295, 5311, 281, 341, 293, 294, 264, 917, 291, 434, 516, 281, 458, 577, 281, 360, 426, 45196, 294, 257, 50592, 50592, 534, 5456, 6405, 13, 823, 264, 661, 551, 307, 41706, 1851, 4088, 433, 50894, 50894, 1177, 380, 362, 264, 912, 34666, 9482, 300, 15968, 13, 48698, 575, 597, 51160, 51160, 1355, 4098, 337, 26992, 264, 733, 295, 1090, 1496, 11, 291, 458, 11, 1192, 51468, 51468, 12362, 9362, 300, 291, 603, 312, 1228, 881, 295, 264, 565, 307, 406, 382, 733, 295, 1919, 281, 352, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1300124301705309, "compression_ratio": 1.5439330543933054, "no_speech_prob": 3.7038080336060375e-05}, {"id": 24, "seek": 12924, "start": 139.84, "end": 145.16, "text": " doesn't have the same layered architecture that Fast.AI has which", "tokens": [50364, 257, 688, 295, 5311, 281, 341, 293, 294, 264, 917, 291, 434, 516, 281, 458, 577, 281, 360, 426, 45196, 294, 257, 50592, 50592, 534, 5456, 6405, 13, 823, 264, 661, 551, 307, 41706, 1851, 4088, 433, 50894, 50894, 1177, 380, 362, 264, 912, 34666, 9482, 300, 15968, 13, 48698, 575, 597, 51160, 51160, 1355, 4098, 337, 26992, 264, 733, 295, 1090, 1496, 11, 291, 458, 11, 1192, 51468, 51468, 12362, 9362, 300, 291, 603, 312, 1228, 881, 295, 264, 565, 307, 406, 382, 733, 295, 1919, 281, 352, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1300124301705309, "compression_ratio": 1.5439330543933054, "no_speech_prob": 3.7038080336060375e-05}, {"id": 25, "seek": 12924, "start": 145.16, "end": 151.32000000000002, "text": " means particularly for beginners the kind of high level, you know, top", "tokens": [50364, 257, 688, 295, 5311, 281, 341, 293, 294, 264, 917, 291, 434, 516, 281, 458, 577, 281, 360, 426, 45196, 294, 257, 50592, 50592, 534, 5456, 6405, 13, 823, 264, 661, 551, 307, 41706, 1851, 4088, 433, 50894, 50894, 1177, 380, 362, 264, 912, 34666, 9482, 300, 15968, 13, 48698, 575, 597, 51160, 51160, 1355, 4098, 337, 26992, 264, 733, 295, 1090, 1496, 11, 291, 458, 11, 1192, 51468, 51468, 12362, 9362, 300, 291, 603, 312, 1228, 881, 295, 264, 565, 307, 406, 382, 733, 295, 1919, 281, 352, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1300124301705309, "compression_ratio": 1.5439330543933054, "no_speech_prob": 3.7038080336060375e-05}, {"id": 26, "seek": 12924, "start": 151.32000000000002, "end": 158.0, "text": " tier API that you'll be using most of the time is not as kind of ready to go", "tokens": [50364, 257, 688, 295, 5311, 281, 341, 293, 294, 264, 917, 291, 434, 516, 281, 458, 577, 281, 360, 426, 45196, 294, 257, 50592, 50592, 534, 5456, 6405, 13, 823, 264, 661, 551, 307, 41706, 1851, 4088, 433, 50894, 50894, 1177, 380, 362, 264, 912, 34666, 9482, 300, 15968, 13, 48698, 575, 597, 51160, 51160, 1355, 4098, 337, 26992, 264, 733, 295, 1090, 1496, 11, 291, 458, 11, 1192, 51468, 51468, 12362, 9362, 300, 291, 603, 312, 1228, 881, 295, 264, 565, 307, 406, 382, 733, 295, 1919, 281, 352, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.1300124301705309, "compression_ratio": 1.5439330543933054, "no_speech_prob": 3.7038080336060375e-05}, {"id": 27, "seek": 15800, "start": 158.0, "end": 162.88, "text": " for beginners as you're used to from Fast.AI and so that's actually I think a", "tokens": [50364, 337, 26992, 382, 291, 434, 1143, 281, 490, 15968, 13, 48698, 293, 370, 300, 311, 767, 286, 519, 257, 50608, 50608, 665, 551, 13, 509, 434, 493, 281, 6898, 1451, 11, 291, 458, 264, 3875, 1558, 586, 295, 577, 16235, 50828, 50828, 23475, 1985, 293, 291, 458, 577, 9834, 366, 3264, 382, 644, 295, 257, 51110, 51110, 11358, 2445, 13, 286, 519, 291, 434, 1919, 281, 853, 1228, 257, 8344, 3126, 1496, 51462, 51462, 6405, 300, 775, 257, 707, 857, 1570, 337, 291, 370, 309, 311, 516, 281, 312, 291, 458, 257, 51628, 51628, 707, 857, 544, 589, 13, 467, 311, 920, 309, 311, 257, 588, 731, 4761, 6405, 293, 309, 311, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.13425849223959035, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.000165989447850734}, {"id": 28, "seek": 15800, "start": 162.88, "end": 167.28, "text": " good thing. You're up to lesson four, you know the basic idea now of how gradient", "tokens": [50364, 337, 26992, 382, 291, 434, 1143, 281, 490, 15968, 13, 48698, 293, 370, 300, 311, 767, 286, 519, 257, 50608, 50608, 665, 551, 13, 509, 434, 493, 281, 6898, 1451, 11, 291, 458, 264, 3875, 1558, 586, 295, 577, 16235, 50828, 50828, 23475, 1985, 293, 291, 458, 577, 9834, 366, 3264, 382, 644, 295, 257, 51110, 51110, 11358, 2445, 13, 286, 519, 291, 434, 1919, 281, 853, 1228, 257, 8344, 3126, 1496, 51462, 51462, 6405, 300, 775, 257, 707, 857, 1570, 337, 291, 370, 309, 311, 516, 281, 312, 291, 458, 257, 51628, 51628, 707, 857, 544, 589, 13, 467, 311, 920, 309, 311, 257, 588, 731, 4761, 6405, 293, 309, 311, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.13425849223959035, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.000165989447850734}, {"id": 29, "seek": 15800, "start": 167.28, "end": 172.92, "text": " descent works and you know how parameters are learned as part of a", "tokens": [50364, 337, 26992, 382, 291, 434, 1143, 281, 490, 15968, 13, 48698, 293, 370, 300, 311, 767, 286, 519, 257, 50608, 50608, 665, 551, 13, 509, 434, 493, 281, 6898, 1451, 11, 291, 458, 264, 3875, 1558, 586, 295, 577, 16235, 50828, 50828, 23475, 1985, 293, 291, 458, 577, 9834, 366, 3264, 382, 644, 295, 257, 51110, 51110, 11358, 2445, 13, 286, 519, 291, 434, 1919, 281, 853, 1228, 257, 8344, 3126, 1496, 51462, 51462, 6405, 300, 775, 257, 707, 857, 1570, 337, 291, 370, 309, 311, 516, 281, 312, 291, 458, 257, 51628, 51628, 707, 857, 544, 589, 13, 467, 311, 920, 309, 311, 257, 588, 731, 4761, 6405, 293, 309, 311, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.13425849223959035, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.000165989447850734}, {"id": 30, "seek": 15800, "start": 172.92, "end": 179.96, "text": " flexible function. I think you're ready to try using a somewhat lower level", "tokens": [50364, 337, 26992, 382, 291, 434, 1143, 281, 490, 15968, 13, 48698, 293, 370, 300, 311, 767, 286, 519, 257, 50608, 50608, 665, 551, 13, 509, 434, 493, 281, 6898, 1451, 11, 291, 458, 264, 3875, 1558, 586, 295, 577, 16235, 50828, 50828, 23475, 1985, 293, 291, 458, 577, 9834, 366, 3264, 382, 644, 295, 257, 51110, 51110, 11358, 2445, 13, 286, 519, 291, 434, 1919, 281, 853, 1228, 257, 8344, 3126, 1496, 51462, 51462, 6405, 300, 775, 257, 707, 857, 1570, 337, 291, 370, 309, 311, 516, 281, 312, 291, 458, 257, 51628, 51628, 707, 857, 544, 589, 13, 467, 311, 920, 309, 311, 257, 588, 731, 4761, 6405, 293, 309, 311, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.13425849223959035, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.000165989447850734}, {"id": 31, "seek": 15800, "start": 179.96, "end": 183.28, "text": " library that does a little bit less for you so it's going to be you know a", "tokens": [50364, 337, 26992, 382, 291, 434, 1143, 281, 490, 15968, 13, 48698, 293, 370, 300, 311, 767, 286, 519, 257, 50608, 50608, 665, 551, 13, 509, 434, 493, 281, 6898, 1451, 11, 291, 458, 264, 3875, 1558, 586, 295, 577, 16235, 50828, 50828, 23475, 1985, 293, 291, 458, 577, 9834, 366, 3264, 382, 644, 295, 257, 51110, 51110, 11358, 2445, 13, 286, 519, 291, 434, 1919, 281, 853, 1228, 257, 8344, 3126, 1496, 51462, 51462, 6405, 300, 775, 257, 707, 857, 1570, 337, 291, 370, 309, 311, 516, 281, 312, 291, 458, 257, 51628, 51628, 707, 857, 544, 589, 13, 467, 311, 920, 309, 311, 257, 588, 731, 4761, 6405, 293, 309, 311, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.13425849223959035, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.000165989447850734}, {"id": 32, "seek": 15800, "start": 183.28, "end": 186.4, "text": " little bit more work. It's still it's a very well designed library and it's", "tokens": [50364, 337, 26992, 382, 291, 434, 1143, 281, 490, 15968, 13, 48698, 293, 370, 300, 311, 767, 286, 519, 257, 50608, 50608, 665, 551, 13, 509, 434, 493, 281, 6898, 1451, 11, 291, 458, 264, 3875, 1558, 586, 295, 577, 16235, 50828, 50828, 23475, 1985, 293, 291, 458, 577, 9834, 366, 3264, 382, 644, 295, 257, 51110, 51110, 11358, 2445, 13, 286, 519, 291, 434, 1919, 281, 853, 1228, 257, 8344, 3126, 1496, 51462, 51462, 6405, 300, 775, 257, 707, 857, 1570, 337, 291, 370, 309, 311, 516, 281, 312, 291, 458, 257, 51628, 51628, 707, 857, 544, 589, 13, 467, 311, 920, 309, 311, 257, 588, 731, 4761, 6405, 293, 309, 311, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.13425849223959035, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.000165989447850734}, {"id": 33, "seek": 18640, "start": 186.4, "end": 191.24, "text": " still reasonably high level but you're going to learn to go a little bit deeper", "tokens": [50364, 920, 23551, 1090, 1496, 457, 291, 434, 516, 281, 1466, 281, 352, 257, 707, 857, 7731, 50606, 50606, 293, 300, 311, 733, 295, 577, 264, 1472, 295, 264, 1164, 294, 2674, 307, 516, 281, 312, 322, 264, 50772, 50772, 1379, 307, 321, 434, 516, 281, 483, 257, 857, 7731, 293, 257, 857, 7731, 293, 257, 857, 7731, 13, 823, 370, 51020, 51020, 700, 295, 439, 718, 311, 751, 466, 437, 321, 434, 516, 281, 312, 884, 365, 2489, 12, 83, 37726, 257, 51346, 51346, 659, 12, 17227, 2001, 2316, 13, 492, 600, 2825, 466, 300, 294, 8437, 949, 457, 321, 2378, 380, 51534, 51534, 534, 668, 1075, 281, 6786, 309, 294, 604, 2607, 570, 291, 2378, 380, 632, 264, 51779, 51779], "temperature": 0.0, "avg_logprob": -0.08134393769551099, "compression_ratio": 1.8464566929133859, "no_speech_prob": 7.483411900466308e-05}, {"id": 34, "seek": 18640, "start": 191.24, "end": 194.56, "text": " and that's kind of how the rest of the course in general is going to be on the", "tokens": [50364, 920, 23551, 1090, 1496, 457, 291, 434, 516, 281, 1466, 281, 352, 257, 707, 857, 7731, 50606, 50606, 293, 300, 311, 733, 295, 577, 264, 1472, 295, 264, 1164, 294, 2674, 307, 516, 281, 312, 322, 264, 50772, 50772, 1379, 307, 321, 434, 516, 281, 483, 257, 857, 7731, 293, 257, 857, 7731, 293, 257, 857, 7731, 13, 823, 370, 51020, 51020, 700, 295, 439, 718, 311, 751, 466, 437, 321, 434, 516, 281, 312, 884, 365, 2489, 12, 83, 37726, 257, 51346, 51346, 659, 12, 17227, 2001, 2316, 13, 492, 600, 2825, 466, 300, 294, 8437, 949, 457, 321, 2378, 380, 51534, 51534, 534, 668, 1075, 281, 6786, 309, 294, 604, 2607, 570, 291, 2378, 380, 632, 264, 51779, 51779], "temperature": 0.0, "avg_logprob": -0.08134393769551099, "compression_ratio": 1.8464566929133859, "no_speech_prob": 7.483411900466308e-05}, {"id": 35, "seek": 18640, "start": 194.56, "end": 199.52, "text": " whole is we're going to get a bit deeper and a bit deeper and a bit deeper. Now so", "tokens": [50364, 920, 23551, 1090, 1496, 457, 291, 434, 516, 281, 1466, 281, 352, 257, 707, 857, 7731, 50606, 50606, 293, 300, 311, 733, 295, 577, 264, 1472, 295, 264, 1164, 294, 2674, 307, 516, 281, 312, 322, 264, 50772, 50772, 1379, 307, 321, 434, 516, 281, 483, 257, 857, 7731, 293, 257, 857, 7731, 293, 257, 857, 7731, 13, 823, 370, 51020, 51020, 700, 295, 439, 718, 311, 751, 466, 437, 321, 434, 516, 281, 312, 884, 365, 2489, 12, 83, 37726, 257, 51346, 51346, 659, 12, 17227, 2001, 2316, 13, 492, 600, 2825, 466, 300, 294, 8437, 949, 457, 321, 2378, 380, 51534, 51534, 534, 668, 1075, 281, 6786, 309, 294, 604, 2607, 570, 291, 2378, 380, 632, 264, 51779, 51779], "temperature": 0.0, "avg_logprob": -0.08134393769551099, "compression_ratio": 1.8464566929133859, "no_speech_prob": 7.483411900466308e-05}, {"id": 36, "seek": 18640, "start": 199.52, "end": 206.04000000000002, "text": " first of all let's talk about what we're going to be doing with fine-tuning a", "tokens": [50364, 920, 23551, 1090, 1496, 457, 291, 434, 516, 281, 1466, 281, 352, 257, 707, 857, 7731, 50606, 50606, 293, 300, 311, 733, 295, 577, 264, 1472, 295, 264, 1164, 294, 2674, 307, 516, 281, 312, 322, 264, 50772, 50772, 1379, 307, 321, 434, 516, 281, 483, 257, 857, 7731, 293, 257, 857, 7731, 293, 257, 857, 7731, 13, 823, 370, 51020, 51020, 700, 295, 439, 718, 311, 751, 466, 437, 321, 434, 516, 281, 312, 884, 365, 2489, 12, 83, 37726, 257, 51346, 51346, 659, 12, 17227, 2001, 2316, 13, 492, 600, 2825, 466, 300, 294, 8437, 949, 457, 321, 2378, 380, 51534, 51534, 534, 668, 1075, 281, 6786, 309, 294, 604, 2607, 570, 291, 2378, 380, 632, 264, 51779, 51779], "temperature": 0.0, "avg_logprob": -0.08134393769551099, "compression_ratio": 1.8464566929133859, "no_speech_prob": 7.483411900466308e-05}, {"id": 37, "seek": 18640, "start": 206.04000000000002, "end": 209.8, "text": " pre-trained model. We've talked about that in passing before but we haven't", "tokens": [50364, 920, 23551, 1090, 1496, 457, 291, 434, 516, 281, 1466, 281, 352, 257, 707, 857, 7731, 50606, 50606, 293, 300, 311, 733, 295, 577, 264, 1472, 295, 264, 1164, 294, 2674, 307, 516, 281, 312, 322, 264, 50772, 50772, 1379, 307, 321, 434, 516, 281, 483, 257, 857, 7731, 293, 257, 857, 7731, 293, 257, 857, 7731, 13, 823, 370, 51020, 51020, 700, 295, 439, 718, 311, 751, 466, 437, 321, 434, 516, 281, 312, 884, 365, 2489, 12, 83, 37726, 257, 51346, 51346, 659, 12, 17227, 2001, 2316, 13, 492, 600, 2825, 466, 300, 294, 8437, 949, 457, 321, 2378, 380, 51534, 51534, 534, 668, 1075, 281, 6786, 309, 294, 604, 2607, 570, 291, 2378, 380, 632, 264, 51779, 51779], "temperature": 0.0, "avg_logprob": -0.08134393769551099, "compression_ratio": 1.8464566929133859, "no_speech_prob": 7.483411900466308e-05}, {"id": 38, "seek": 18640, "start": 209.8, "end": 214.70000000000002, "text": " really been able to describe it in any detail because you haven't had the", "tokens": [50364, 920, 23551, 1090, 1496, 457, 291, 434, 516, 281, 1466, 281, 352, 257, 707, 857, 7731, 50606, 50606, 293, 300, 311, 733, 295, 577, 264, 1472, 295, 264, 1164, 294, 2674, 307, 516, 281, 312, 322, 264, 50772, 50772, 1379, 307, 321, 434, 516, 281, 483, 257, 857, 7731, 293, 257, 857, 7731, 293, 257, 857, 7731, 13, 823, 370, 51020, 51020, 700, 295, 439, 718, 311, 751, 466, 437, 321, 434, 516, 281, 312, 884, 365, 2489, 12, 83, 37726, 257, 51346, 51346, 659, 12, 17227, 2001, 2316, 13, 492, 600, 2825, 466, 300, 294, 8437, 949, 457, 321, 2378, 380, 51534, 51534, 534, 668, 1075, 281, 6786, 309, 294, 604, 2607, 570, 291, 2378, 380, 632, 264, 51779, 51779], "temperature": 0.0, "avg_logprob": -0.08134393769551099, "compression_ratio": 1.8464566929133859, "no_speech_prob": 7.483411900466308e-05}, {"id": 39, "seek": 21470, "start": 214.7, "end": 220.39999999999998, "text": " foundations. Now you do. You played with these sliders last week that hopefully", "tokens": [50364, 22467, 13, 823, 291, 360, 13, 509, 3737, 365, 613, 1061, 6936, 1036, 1243, 300, 4696, 50649, 50649, 291, 600, 439, 767, 2780, 666, 341, 21060, 293, 25717, 552, 926, 293, 3031, 50857, 50857, 281, 483, 364, 24002, 337, 411, 341, 1558, 295, 411, 2684, 552, 493, 293, 760, 1669, 51063, 51063, 264, 4470, 352, 493, 293, 760, 293, 370, 5220, 13, 407, 286, 3811, 300, 428, 1691, 390, 281, 1286, 51385, 51385, 613, 1061, 6936, 281, 483, 341, 382, 1481, 382, 1944, 457, 562, 309, 390, 2212, 281, 291, 264, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.11840758072702508, "compression_ratio": 1.6883116883116882, "no_speech_prob": 8.884603448677808e-05}, {"id": 40, "seek": 21470, "start": 220.39999999999998, "end": 224.56, "text": " you've all actually gone into this notebook and dragged them around and tried", "tokens": [50364, 22467, 13, 823, 291, 360, 13, 509, 3737, 365, 613, 1061, 6936, 1036, 1243, 300, 4696, 50649, 50649, 291, 600, 439, 767, 2780, 666, 341, 21060, 293, 25717, 552, 926, 293, 3031, 50857, 50857, 281, 483, 364, 24002, 337, 411, 341, 1558, 295, 411, 2684, 552, 493, 293, 760, 1669, 51063, 51063, 264, 4470, 352, 493, 293, 760, 293, 370, 5220, 13, 407, 286, 3811, 300, 428, 1691, 390, 281, 1286, 51385, 51385, 613, 1061, 6936, 281, 483, 341, 382, 1481, 382, 1944, 457, 562, 309, 390, 2212, 281, 291, 264, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.11840758072702508, "compression_ratio": 1.6883116883116882, "no_speech_prob": 8.884603448677808e-05}, {"id": 41, "seek": 21470, "start": 224.56, "end": 228.67999999999998, "text": " to get an intuition for like this idea of like moving them up and down makes", "tokens": [50364, 22467, 13, 823, 291, 360, 13, 509, 3737, 365, 613, 1061, 6936, 1036, 1243, 300, 4696, 50649, 50649, 291, 600, 439, 767, 2780, 666, 341, 21060, 293, 25717, 552, 926, 293, 3031, 50857, 50857, 281, 483, 364, 24002, 337, 411, 341, 1558, 295, 411, 2684, 552, 493, 293, 760, 1669, 51063, 51063, 264, 4470, 352, 493, 293, 760, 293, 370, 5220, 13, 407, 286, 3811, 300, 428, 1691, 390, 281, 1286, 51385, 51385, 613, 1061, 6936, 281, 483, 341, 382, 1481, 382, 1944, 457, 562, 309, 390, 2212, 281, 291, 264, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.11840758072702508, "compression_ratio": 1.6883116883116882, "no_speech_prob": 8.884603448677808e-05}, {"id": 42, "seek": 21470, "start": 228.67999999999998, "end": 235.12, "text": " the loss go up and down and so forth. So I imagine that your job was to move", "tokens": [50364, 22467, 13, 823, 291, 360, 13, 509, 3737, 365, 613, 1061, 6936, 1036, 1243, 300, 4696, 50649, 50649, 291, 600, 439, 767, 2780, 666, 341, 21060, 293, 25717, 552, 926, 293, 3031, 50857, 50857, 281, 483, 364, 24002, 337, 411, 341, 1558, 295, 411, 2684, 552, 493, 293, 760, 1669, 51063, 51063, 264, 4470, 352, 493, 293, 760, 293, 370, 5220, 13, 407, 286, 3811, 300, 428, 1691, 390, 281, 1286, 51385, 51385, 613, 1061, 6936, 281, 483, 341, 382, 1481, 382, 1944, 457, 562, 309, 390, 2212, 281, 291, 264, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.11840758072702508, "compression_ratio": 1.6883116883116882, "no_speech_prob": 8.884603448677808e-05}, {"id": 43, "seek": 21470, "start": 235.12, "end": 241.28, "text": " these sliders to get this as nice as possible but when it was given to you the", "tokens": [50364, 22467, 13, 823, 291, 360, 13, 509, 3737, 365, 613, 1061, 6936, 1036, 1243, 300, 4696, 50649, 50649, 291, 600, 439, 767, 2780, 666, 341, 21060, 293, 25717, 552, 926, 293, 3031, 50857, 50857, 281, 483, 364, 24002, 337, 411, 341, 1558, 295, 411, 2684, 552, 493, 293, 760, 1669, 51063, 51063, 264, 4470, 352, 493, 293, 760, 293, 370, 5220, 13, 407, 286, 3811, 300, 428, 1691, 390, 281, 1286, 51385, 51385, 613, 1061, 6936, 281, 483, 341, 382, 1481, 382, 1944, 457, 562, 309, 390, 2212, 281, 291, 264, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.11840758072702508, "compression_ratio": 1.6883116883116882, "no_speech_prob": 8.884603448677808e-05}, {"id": 44, "seek": 24128, "start": 241.28, "end": 246.56, "text": " person who gave it to you said oh actually slider A that should be on 2.0", "tokens": [50364, 954, 567, 2729, 309, 281, 291, 848, 1954, 767, 26046, 316, 300, 820, 312, 322, 568, 13, 15, 50628, 50628, 321, 458, 337, 988, 293, 26046, 363, 321, 519, 309, 311, 411, 926, 732, 293, 257, 1922, 13, 6187, 1438, 383, 51004, 51004, 321, 600, 658, 572, 1558, 13, 823, 300, 1116, 312, 1238, 4961, 2759, 380, 309, 558, 570, 291, 51238, 51238, 727, 411, 4258, 722, 8416, 322, 264, 472, 321, 362, 572, 1558, 466, 483, 300, 51456, 51456, 294, 9810, 264, 558, 4008, 293, 550, 264, 472, 291, 733, 295, 658, 257, 24247, 1558, 466, 51622, 51622, 291, 727, 445, 10864, 309, 257, 707, 857, 293, 264, 472, 300, 436, 848, 390, 3879, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.1284497765933766, "compression_ratio": 1.7222222222222223, "no_speech_prob": 8.217339927796274e-05}, {"id": 45, "seek": 24128, "start": 246.56, "end": 254.08, "text": " we know for sure and slider B we think it's like around two and a half. Slider C", "tokens": [50364, 954, 567, 2729, 309, 281, 291, 848, 1954, 767, 26046, 316, 300, 820, 312, 322, 568, 13, 15, 50628, 50628, 321, 458, 337, 988, 293, 26046, 363, 321, 519, 309, 311, 411, 926, 732, 293, 257, 1922, 13, 6187, 1438, 383, 51004, 51004, 321, 600, 658, 572, 1558, 13, 823, 300, 1116, 312, 1238, 4961, 2759, 380, 309, 558, 570, 291, 51238, 51238, 727, 411, 4258, 722, 8416, 322, 264, 472, 321, 362, 572, 1558, 466, 483, 300, 51456, 51456, 294, 9810, 264, 558, 4008, 293, 550, 264, 472, 291, 733, 295, 658, 257, 24247, 1558, 466, 51622, 51622, 291, 727, 445, 10864, 309, 257, 707, 857, 293, 264, 472, 300, 436, 848, 390, 3879, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.1284497765933766, "compression_ratio": 1.7222222222222223, "no_speech_prob": 8.217339927796274e-05}, {"id": 46, "seek": 24128, "start": 254.08, "end": 258.76, "text": " we've got no idea. Now that'd be pretty helpful wouldn't it right because you", "tokens": [50364, 954, 567, 2729, 309, 281, 291, 848, 1954, 767, 26046, 316, 300, 820, 312, 322, 568, 13, 15, 50628, 50628, 321, 458, 337, 988, 293, 26046, 363, 321, 519, 309, 311, 411, 926, 732, 293, 257, 1922, 13, 6187, 1438, 383, 51004, 51004, 321, 600, 658, 572, 1558, 13, 823, 300, 1116, 312, 1238, 4961, 2759, 380, 309, 558, 570, 291, 51238, 51238, 727, 411, 4258, 722, 8416, 322, 264, 472, 321, 362, 572, 1558, 466, 483, 300, 51456, 51456, 294, 9810, 264, 558, 4008, 293, 550, 264, 472, 291, 733, 295, 658, 257, 24247, 1558, 466, 51622, 51622, 291, 727, 445, 10864, 309, 257, 707, 857, 293, 264, 472, 300, 436, 848, 390, 3879, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.1284497765933766, "compression_ratio": 1.7222222222222223, "no_speech_prob": 8.217339927796274e-05}, {"id": 47, "seek": 24128, "start": 258.76, "end": 263.12, "text": " could like immediately start focusing on the one we have no idea about get that", "tokens": [50364, 954, 567, 2729, 309, 281, 291, 848, 1954, 767, 26046, 316, 300, 820, 312, 322, 568, 13, 15, 50628, 50628, 321, 458, 337, 988, 293, 26046, 363, 321, 519, 309, 311, 411, 926, 732, 293, 257, 1922, 13, 6187, 1438, 383, 51004, 51004, 321, 600, 658, 572, 1558, 13, 823, 300, 1116, 312, 1238, 4961, 2759, 380, 309, 558, 570, 291, 51238, 51238, 727, 411, 4258, 722, 8416, 322, 264, 472, 321, 362, 572, 1558, 466, 483, 300, 51456, 51456, 294, 9810, 264, 558, 4008, 293, 550, 264, 472, 291, 733, 295, 658, 257, 24247, 1558, 466, 51622, 51622, 291, 727, 445, 10864, 309, 257, 707, 857, 293, 264, 472, 300, 436, 848, 390, 3879, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.1284497765933766, "compression_ratio": 1.7222222222222223, "no_speech_prob": 8.217339927796274e-05}, {"id": 48, "seek": 24128, "start": 263.12, "end": 266.44, "text": " in roughly the right spot and then the one you kind of got a vague idea about", "tokens": [50364, 954, 567, 2729, 309, 281, 291, 848, 1954, 767, 26046, 316, 300, 820, 312, 322, 568, 13, 15, 50628, 50628, 321, 458, 337, 988, 293, 26046, 363, 321, 519, 309, 311, 411, 926, 732, 293, 257, 1922, 13, 6187, 1438, 383, 51004, 51004, 321, 600, 658, 572, 1558, 13, 823, 300, 1116, 312, 1238, 4961, 2759, 380, 309, 558, 570, 291, 51238, 51238, 727, 411, 4258, 722, 8416, 322, 264, 472, 321, 362, 572, 1558, 466, 483, 300, 51456, 51456, 294, 9810, 264, 558, 4008, 293, 550, 264, 472, 291, 733, 295, 658, 257, 24247, 1558, 466, 51622, 51622, 291, 727, 445, 10864, 309, 257, 707, 857, 293, 264, 472, 300, 436, 848, 390, 3879, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.1284497765933766, "compression_ratio": 1.7222222222222223, "no_speech_prob": 8.217339927796274e-05}, {"id": 49, "seek": 24128, "start": 266.44, "end": 269.24, "text": " you could just tune it a little bit and the one that they said was totally", "tokens": [50364, 954, 567, 2729, 309, 281, 291, 848, 1954, 767, 26046, 316, 300, 820, 312, 322, 568, 13, 15, 50628, 50628, 321, 458, 337, 988, 293, 26046, 363, 321, 519, 309, 311, 411, 926, 732, 293, 257, 1922, 13, 6187, 1438, 383, 51004, 51004, 321, 600, 658, 572, 1558, 13, 823, 300, 1116, 312, 1238, 4961, 2759, 380, 309, 558, 570, 291, 51238, 51238, 727, 411, 4258, 722, 8416, 322, 264, 472, 321, 362, 572, 1558, 466, 483, 300, 51456, 51456, 294, 9810, 264, 558, 4008, 293, 550, 264, 472, 291, 733, 295, 658, 257, 24247, 1558, 466, 51622, 51622, 291, 727, 445, 10864, 309, 257, 707, 857, 293, 264, 472, 300, 436, 848, 390, 3879, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.1284497765933766, "compression_ratio": 1.7222222222222223, "no_speech_prob": 8.217339927796274e-05}, {"id": 50, "seek": 26924, "start": 269.24, "end": 273.32, "text": " confident you wouldn't move at all you would probably tune these sliders really", "tokens": [50364, 6679, 291, 2759, 380, 1286, 412, 439, 291, 576, 1391, 10864, 613, 1061, 6936, 534, 50568, 50568, 2661, 13, 663, 311, 437, 257, 659, 12, 17227, 2001, 2316, 307, 13, 316, 659, 12, 17227, 2001, 2316, 307, 257, 3840, 295, 50930, 50930, 9834, 300, 362, 1217, 668, 3318, 689, 512, 295, 552, 291, 434, 1217, 1238, 51180, 51180, 6679, 295, 437, 436, 820, 312, 293, 512, 295, 552, 321, 534, 362, 572, 1558, 412, 51396, 51396, 439, 293, 370, 2489, 12, 83, 37726, 307, 264, 1399, 295, 1940, 729, 2306, 321, 362, 572, 1558, 437, 51638, 51638, 436, 820, 312, 412, 439, 293, 1382, 281, 483, 552, 558, 293, 550, 2684, 264, 661, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.07511128319634332, "compression_ratio": 1.9183673469387754, "no_speech_prob": 4.469284613151103e-05}, {"id": 51, "seek": 26924, "start": 273.32, "end": 280.56, "text": " quickly. That's what a pre-trained model is. A pre-trained model is a bunch of", "tokens": [50364, 6679, 291, 2759, 380, 1286, 412, 439, 291, 576, 1391, 10864, 613, 1061, 6936, 534, 50568, 50568, 2661, 13, 663, 311, 437, 257, 659, 12, 17227, 2001, 2316, 307, 13, 316, 659, 12, 17227, 2001, 2316, 307, 257, 3840, 295, 50930, 50930, 9834, 300, 362, 1217, 668, 3318, 689, 512, 295, 552, 291, 434, 1217, 1238, 51180, 51180, 6679, 295, 437, 436, 820, 312, 293, 512, 295, 552, 321, 534, 362, 572, 1558, 412, 51396, 51396, 439, 293, 370, 2489, 12, 83, 37726, 307, 264, 1399, 295, 1940, 729, 2306, 321, 362, 572, 1558, 437, 51638, 51638, 436, 820, 312, 412, 439, 293, 1382, 281, 483, 552, 558, 293, 550, 2684, 264, 661, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.07511128319634332, "compression_ratio": 1.9183673469387754, "no_speech_prob": 4.469284613151103e-05}, {"id": 52, "seek": 26924, "start": 280.56, "end": 285.56, "text": " parameters that have already been fit where some of them you're already pretty", "tokens": [50364, 6679, 291, 2759, 380, 1286, 412, 439, 291, 576, 1391, 10864, 613, 1061, 6936, 534, 50568, 50568, 2661, 13, 663, 311, 437, 257, 659, 12, 17227, 2001, 2316, 307, 13, 316, 659, 12, 17227, 2001, 2316, 307, 257, 3840, 295, 50930, 50930, 9834, 300, 362, 1217, 668, 3318, 689, 512, 295, 552, 291, 434, 1217, 1238, 51180, 51180, 6679, 295, 437, 436, 820, 312, 293, 512, 295, 552, 321, 534, 362, 572, 1558, 412, 51396, 51396, 439, 293, 370, 2489, 12, 83, 37726, 307, 264, 1399, 295, 1940, 729, 2306, 321, 362, 572, 1558, 437, 51638, 51638, 436, 820, 312, 412, 439, 293, 1382, 281, 483, 552, 558, 293, 550, 2684, 264, 661, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.07511128319634332, "compression_ratio": 1.9183673469387754, "no_speech_prob": 4.469284613151103e-05}, {"id": 53, "seek": 26924, "start": 285.56, "end": 289.88, "text": " confident of what they should be and some of them we really have no idea at", "tokens": [50364, 6679, 291, 2759, 380, 1286, 412, 439, 291, 576, 1391, 10864, 613, 1061, 6936, 534, 50568, 50568, 2661, 13, 663, 311, 437, 257, 659, 12, 17227, 2001, 2316, 307, 13, 316, 659, 12, 17227, 2001, 2316, 307, 257, 3840, 295, 50930, 50930, 9834, 300, 362, 1217, 668, 3318, 689, 512, 295, 552, 291, 434, 1217, 1238, 51180, 51180, 6679, 295, 437, 436, 820, 312, 293, 512, 295, 552, 321, 534, 362, 572, 1558, 412, 51396, 51396, 439, 293, 370, 2489, 12, 83, 37726, 307, 264, 1399, 295, 1940, 729, 2306, 321, 362, 572, 1558, 437, 51638, 51638, 436, 820, 312, 412, 439, 293, 1382, 281, 483, 552, 558, 293, 550, 2684, 264, 661, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.07511128319634332, "compression_ratio": 1.9183673469387754, "no_speech_prob": 4.469284613151103e-05}, {"id": 54, "seek": 26924, "start": 289.88, "end": 294.72, "text": " all and so fine-tuning is the process of taking those ones we have no idea what", "tokens": [50364, 6679, 291, 2759, 380, 1286, 412, 439, 291, 576, 1391, 10864, 613, 1061, 6936, 534, 50568, 50568, 2661, 13, 663, 311, 437, 257, 659, 12, 17227, 2001, 2316, 307, 13, 316, 659, 12, 17227, 2001, 2316, 307, 257, 3840, 295, 50930, 50930, 9834, 300, 362, 1217, 668, 3318, 689, 512, 295, 552, 291, 434, 1217, 1238, 51180, 51180, 6679, 295, 437, 436, 820, 312, 293, 512, 295, 552, 321, 534, 362, 572, 1558, 412, 51396, 51396, 439, 293, 370, 2489, 12, 83, 37726, 307, 264, 1399, 295, 1940, 729, 2306, 321, 362, 572, 1558, 437, 51638, 51638, 436, 820, 312, 412, 439, 293, 1382, 281, 483, 552, 558, 293, 550, 2684, 264, 661, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.07511128319634332, "compression_ratio": 1.9183673469387754, "no_speech_prob": 4.469284613151103e-05}, {"id": 55, "seek": 26924, "start": 294.72, "end": 297.92, "text": " they should be at all and trying to get them right and then moving the other", "tokens": [50364, 6679, 291, 2759, 380, 1286, 412, 439, 291, 576, 1391, 10864, 613, 1061, 6936, 534, 50568, 50568, 2661, 13, 663, 311, 437, 257, 659, 12, 17227, 2001, 2316, 307, 13, 316, 659, 12, 17227, 2001, 2316, 307, 257, 3840, 295, 50930, 50930, 9834, 300, 362, 1217, 668, 3318, 689, 512, 295, 552, 291, 434, 1217, 1238, 51180, 51180, 6679, 295, 437, 436, 820, 312, 293, 512, 295, 552, 321, 534, 362, 572, 1558, 412, 51396, 51396, 439, 293, 370, 2489, 12, 83, 37726, 307, 264, 1399, 295, 1940, 729, 2306, 321, 362, 572, 1558, 437, 51638, 51638, 436, 820, 312, 412, 439, 293, 1382, 281, 483, 552, 558, 293, 550, 2684, 264, 661, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.07511128319634332, "compression_ratio": 1.9183673469387754, "no_speech_prob": 4.469284613151103e-05}, {"id": 56, "seek": 29792, "start": 297.92, "end": 307.44, "text": " ones a little bit. The idea of fine-tuning a pre-trained NLP model in", "tokens": [50364, 2306, 257, 707, 857, 13, 440, 1558, 295, 2489, 12, 83, 37726, 257, 659, 12, 17227, 2001, 426, 45196, 2316, 294, 50840, 50840, 341, 636, 390, 19761, 4073, 538, 364, 9284, 1219, 624, 43, 44, 3318, 597, 390, 700, 8212, 51140, 51140, 767, 294, 257, 2370, 7318, 1164, 286, 519, 264, 588, 700, 2370, 7318, 1164, 309, 390, 1780, 51430, 51430, 3574, 666, 364, 7778, 3035, 538, 385, 293, 294, 27482, 365, 257, 550, 14476, 3107, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.14530171155929567, "compression_ratio": 1.4879227053140096, "no_speech_prob": 1.47358978210832e-05}, {"id": 57, "seek": 29792, "start": 307.44, "end": 313.44, "text": " this way was pioneered by an algorithm called ULM fit which was first presented", "tokens": [50364, 2306, 257, 707, 857, 13, 440, 1558, 295, 2489, 12, 83, 37726, 257, 659, 12, 17227, 2001, 426, 45196, 2316, 294, 50840, 50840, 341, 636, 390, 19761, 4073, 538, 364, 9284, 1219, 624, 43, 44, 3318, 597, 390, 700, 8212, 51140, 51140, 767, 294, 257, 2370, 7318, 1164, 286, 519, 264, 588, 700, 2370, 7318, 1164, 309, 390, 1780, 51430, 51430, 3574, 666, 364, 7778, 3035, 538, 385, 293, 294, 27482, 365, 257, 550, 14476, 3107, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.14530171155929567, "compression_ratio": 1.4879227053140096, "no_speech_prob": 1.47358978210832e-05}, {"id": 58, "seek": 29792, "start": 313.44, "end": 319.24, "text": " actually in a fast AI course I think the very first fast AI course it was later", "tokens": [50364, 2306, 257, 707, 857, 13, 440, 1558, 295, 2489, 12, 83, 37726, 257, 659, 12, 17227, 2001, 426, 45196, 2316, 294, 50840, 50840, 341, 636, 390, 19761, 4073, 538, 364, 9284, 1219, 624, 43, 44, 3318, 597, 390, 700, 8212, 51140, 51140, 767, 294, 257, 2370, 7318, 1164, 286, 519, 264, 588, 700, 2370, 7318, 1164, 309, 390, 1780, 51430, 51430, 3574, 666, 364, 7778, 3035, 538, 385, 293, 294, 27482, 365, 257, 550, 14476, 3107, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.14530171155929567, "compression_ratio": 1.4879227053140096, "no_speech_prob": 1.47358978210832e-05}, {"id": 59, "seek": 29792, "start": 319.24, "end": 324.08000000000004, "text": " turned into an academic paper by me and in conjunction with a then PhD student", "tokens": [50364, 2306, 257, 707, 857, 13, 440, 1558, 295, 2489, 12, 83, 37726, 257, 659, 12, 17227, 2001, 426, 45196, 2316, 294, 50840, 50840, 341, 636, 390, 19761, 4073, 538, 364, 9284, 1219, 624, 43, 44, 3318, 597, 390, 700, 8212, 51140, 51140, 767, 294, 257, 2370, 7318, 1164, 286, 519, 264, 588, 700, 2370, 7318, 1164, 309, 390, 1780, 51430, 51430, 3574, 666, 364, 7778, 3035, 538, 385, 293, 294, 27482, 365, 257, 550, 14476, 3107, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.14530171155929567, "compression_ratio": 1.4879227053140096, "no_speech_prob": 1.47358978210832e-05}, {"id": 60, "seek": 32408, "start": 324.08, "end": 328.91999999999996, "text": " named Sebastian Ruda who's now one of the world's top NLP researchers and went on", "tokens": [50364, 4926, 31102, 497, 11152, 567, 311, 586, 472, 295, 264, 1002, 311, 1192, 426, 45196, 10309, 293, 1437, 322, 50606, 50606, 281, 854, 15638, 257, 2603, 1319, 291, 458, 2603, 733, 295, 1823, 10444, 294, 426, 45196, 50988, 50988, 10862, 926, 264, 1002, 2051, 365, 257, 1230, 295, 661, 1021, 24283, 51213, 51213, 412, 264, 565, 13, 639, 307, 264, 3875, 1399, 300, 624, 43, 44, 3318, 7619, 13, 5470, 472, 390, 281, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.11095199337253323, "compression_ratio": 1.5023696682464456, "no_speech_prob": 4.261991853127256e-05}, {"id": 61, "seek": 32408, "start": 328.91999999999996, "end": 336.56, "text": " to help inspire a huge change you know huge kind of step improvement in NLP", "tokens": [50364, 4926, 31102, 497, 11152, 567, 311, 586, 472, 295, 264, 1002, 311, 1192, 426, 45196, 10309, 293, 1437, 322, 50606, 50606, 281, 854, 15638, 257, 2603, 1319, 291, 458, 2603, 733, 295, 1823, 10444, 294, 426, 45196, 50988, 50988, 10862, 926, 264, 1002, 2051, 365, 257, 1230, 295, 661, 1021, 24283, 51213, 51213, 412, 264, 565, 13, 639, 307, 264, 3875, 1399, 300, 624, 43, 44, 3318, 7619, 13, 5470, 472, 390, 281, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.11095199337253323, "compression_ratio": 1.5023696682464456, "no_speech_prob": 4.261991853127256e-05}, {"id": 62, "seek": 32408, "start": 336.56, "end": 341.06, "text": " capabilities around the world along with a number of other important innovations", "tokens": [50364, 4926, 31102, 497, 11152, 567, 311, 586, 472, 295, 264, 1002, 311, 1192, 426, 45196, 10309, 293, 1437, 322, 50606, 50606, 281, 854, 15638, 257, 2603, 1319, 291, 458, 2603, 733, 295, 1823, 10444, 294, 426, 45196, 50988, 50988, 10862, 926, 264, 1002, 2051, 365, 257, 1230, 295, 661, 1021, 24283, 51213, 51213, 412, 264, 565, 13, 639, 307, 264, 3875, 1399, 300, 624, 43, 44, 3318, 7619, 13, 5470, 472, 390, 281, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.11095199337253323, "compression_ratio": 1.5023696682464456, "no_speech_prob": 4.261991853127256e-05}, {"id": 63, "seek": 32408, "start": 341.06, "end": 352.79999999999995, "text": " at the time. This is the basic process that ULM fit described. Step one was to", "tokens": [50364, 4926, 31102, 497, 11152, 567, 311, 586, 472, 295, 264, 1002, 311, 1192, 426, 45196, 10309, 293, 1437, 322, 50606, 50606, 281, 854, 15638, 257, 2603, 1319, 291, 458, 2603, 733, 295, 1823, 10444, 294, 426, 45196, 50988, 50988, 10862, 926, 264, 1002, 2051, 365, 257, 1230, 295, 661, 1021, 24283, 51213, 51213, 412, 264, 565, 13, 639, 307, 264, 3875, 1399, 300, 624, 43, 44, 3318, 7619, 13, 5470, 472, 390, 281, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.11095199337253323, "compression_ratio": 1.5023696682464456, "no_speech_prob": 4.261991853127256e-05}, {"id": 64, "seek": 35280, "start": 352.8, "end": 357.68, "text": " build something called a language model using basically nearly all of Wikipedia", "tokens": [50364, 1322, 746, 1219, 257, 2856, 2316, 1228, 1936, 6217, 439, 295, 28999, 50608, 50608, 293, 437, 264, 2856, 2316, 630, 390, 309, 3031, 281, 6069, 264, 958, 1349, 295, 257, 50966, 50966, 28999, 7222, 294, 1186, 633, 958, 1349, 295, 633, 28999, 7222, 13, 18496, 51336, 51336, 300, 307, 588, 2252, 291, 458, 456, 366, 28999, 11290, 597, 576, 584, 51590, 51590], "temperature": 0.0, "avg_logprob": -0.09236527234315872, "compression_ratio": 1.7848837209302326, "no_speech_prob": 0.00017124829173553735}, {"id": 65, "seek": 35280, "start": 357.68, "end": 364.84000000000003, "text": " and what the language model did was it tried to predict the next word of a", "tokens": [50364, 1322, 746, 1219, 257, 2856, 2316, 1228, 1936, 6217, 439, 295, 28999, 50608, 50608, 293, 437, 264, 2856, 2316, 630, 390, 309, 3031, 281, 6069, 264, 958, 1349, 295, 257, 50966, 50966, 28999, 7222, 294, 1186, 633, 958, 1349, 295, 633, 28999, 7222, 13, 18496, 51336, 51336, 300, 307, 588, 2252, 291, 458, 456, 366, 28999, 11290, 597, 576, 584, 51590, 51590], "temperature": 0.0, "avg_logprob": -0.09236527234315872, "compression_ratio": 1.7848837209302326, "no_speech_prob": 0.00017124829173553735}, {"id": 66, "seek": 35280, "start": 364.84000000000003, "end": 372.24, "text": " Wikipedia article in fact every next word of every Wikipedia article. Doing", "tokens": [50364, 1322, 746, 1219, 257, 2856, 2316, 1228, 1936, 6217, 439, 295, 28999, 50608, 50608, 293, 437, 264, 2856, 2316, 630, 390, 309, 3031, 281, 6069, 264, 958, 1349, 295, 257, 50966, 50966, 28999, 7222, 294, 1186, 633, 958, 1349, 295, 633, 28999, 7222, 13, 18496, 51336, 51336, 300, 307, 588, 2252, 291, 458, 456, 366, 28999, 11290, 597, 576, 584, 51590, 51590], "temperature": 0.0, "avg_logprob": -0.09236527234315872, "compression_ratio": 1.7848837209302326, "no_speech_prob": 0.00017124829173553735}, {"id": 67, "seek": 35280, "start": 372.24, "end": 377.32, "text": " that is very difficult you know there are Wikipedia articles which would say", "tokens": [50364, 1322, 746, 1219, 257, 2856, 2316, 1228, 1936, 6217, 439, 295, 28999, 50608, 50608, 293, 437, 264, 2856, 2316, 630, 390, 309, 3031, 281, 6069, 264, 958, 1349, 295, 257, 50966, 50966, 28999, 7222, 294, 1186, 633, 958, 1349, 295, 633, 28999, 7222, 13, 18496, 51336, 51336, 300, 307, 588, 2252, 291, 458, 456, 366, 28999, 11290, 597, 576, 584, 51590, 51590], "temperature": 0.0, "avg_logprob": -0.09236527234315872, "compression_ratio": 1.7848837209302326, "no_speech_prob": 0.00017124829173553735}, {"id": 68, "seek": 37732, "start": 377.32, "end": 387.96, "text": " things like you know the 17th prime number is dot dot dot or the 40th", "tokens": [50364, 721, 411, 291, 458, 264, 3282, 392, 5835, 1230, 307, 5893, 5893, 5893, 420, 264, 3356, 392, 50896, 50896, 3868, 295, 264, 2824, 3040, 12288, 848, 412, 702, 19607, 12288, 300, 291, 458, 51232, 51232, 10623, 294, 613, 3685, 295, 721, 7029, 3701, 257, 688, 466, 577, 51496, 51496], "temperature": 0.0, "avg_logprob": -0.13682909572825713, "compression_ratio": 1.5034482758620689, "no_speech_prob": 4.6104985813144594e-05}, {"id": 69, "seek": 37732, "start": 387.96, "end": 394.68, "text": " president of the United States blah said at his residence blah that you know", "tokens": [50364, 721, 411, 291, 458, 264, 3282, 392, 5835, 1230, 307, 5893, 5893, 5893, 420, 264, 3356, 392, 50896, 50896, 3868, 295, 264, 2824, 3040, 12288, 848, 412, 702, 19607, 12288, 300, 291, 458, 51232, 51232, 10623, 294, 613, 3685, 295, 721, 7029, 3701, 257, 688, 466, 577, 51496, 51496], "temperature": 0.0, "avg_logprob": -0.13682909572825713, "compression_ratio": 1.5034482758620689, "no_speech_prob": 4.6104985813144594e-05}, {"id": 70, "seek": 37732, "start": 394.68, "end": 399.96, "text": " filling in these kinds of things requires understanding a lot about how", "tokens": [50364, 721, 411, 291, 458, 264, 3282, 392, 5835, 1230, 307, 5893, 5893, 5893, 420, 264, 3356, 392, 50896, 50896, 3868, 295, 264, 2824, 3040, 12288, 848, 412, 702, 19607, 12288, 300, 291, 458, 51232, 51232, 10623, 294, 613, 3685, 295, 721, 7029, 3701, 257, 688, 466, 577, 51496, 51496], "temperature": 0.0, "avg_logprob": -0.13682909572825713, "compression_ratio": 1.5034482758620689, "no_speech_prob": 4.6104985813144594e-05}, {"id": 71, "seek": 39996, "start": 399.96, "end": 407.91999999999996, "text": " language is structured and about the world and about math and so forth. So to", "tokens": [50364, 2856, 307, 18519, 293, 466, 264, 1002, 293, 466, 5221, 293, 370, 5220, 13, 407, 281, 50762, 50762, 483, 665, 412, 885, 257, 2856, 2316, 257, 18161, 3209, 575, 281, 483, 665, 412, 257, 688, 51028, 51028, 295, 721, 13, 467, 575, 281, 1223, 577, 2856, 1985, 412, 257, 23551, 665, 51336, 51336, 1496, 293, 309, 2203, 281, 1223, 437, 309, 311, 767, 1417, 466, 293, 437, 307, 51534, 51534, 767, 2074, 293, 437, 307, 767, 406, 2074, 293, 264, 819, 2098, 294, 597, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.07406179471449419, "compression_ratio": 1.9191919191919191, "no_speech_prob": 3.2190375350182876e-05}, {"id": 72, "seek": 39996, "start": 407.91999999999996, "end": 413.24, "text": " get good at being a language model a neural network has to get good at a lot", "tokens": [50364, 2856, 307, 18519, 293, 466, 264, 1002, 293, 466, 5221, 293, 370, 5220, 13, 407, 281, 50762, 50762, 483, 665, 412, 885, 257, 2856, 2316, 257, 18161, 3209, 575, 281, 483, 665, 412, 257, 688, 51028, 51028, 295, 721, 13, 467, 575, 281, 1223, 577, 2856, 1985, 412, 257, 23551, 665, 51336, 51336, 1496, 293, 309, 2203, 281, 1223, 437, 309, 311, 767, 1417, 466, 293, 437, 307, 51534, 51534, 767, 2074, 293, 437, 307, 767, 406, 2074, 293, 264, 819, 2098, 294, 597, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.07406179471449419, "compression_ratio": 1.9191919191919191, "no_speech_prob": 3.2190375350182876e-05}, {"id": 73, "seek": 39996, "start": 413.24, "end": 419.4, "text": " of things. It has to understand how language works at a reasonably good", "tokens": [50364, 2856, 307, 18519, 293, 466, 264, 1002, 293, 466, 5221, 293, 370, 5220, 13, 407, 281, 50762, 50762, 483, 665, 412, 885, 257, 2856, 2316, 257, 18161, 3209, 575, 281, 483, 665, 412, 257, 688, 51028, 51028, 295, 721, 13, 467, 575, 281, 1223, 577, 2856, 1985, 412, 257, 23551, 665, 51336, 51336, 1496, 293, 309, 2203, 281, 1223, 437, 309, 311, 767, 1417, 466, 293, 437, 307, 51534, 51534, 767, 2074, 293, 437, 307, 767, 406, 2074, 293, 264, 819, 2098, 294, 597, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.07406179471449419, "compression_ratio": 1.9191919191919191, "no_speech_prob": 3.2190375350182876e-05}, {"id": 74, "seek": 39996, "start": 419.4, "end": 423.35999999999996, "text": " level and it needs to understand what it's actually talking about and what is", "tokens": [50364, 2856, 307, 18519, 293, 466, 264, 1002, 293, 466, 5221, 293, 370, 5220, 13, 407, 281, 50762, 50762, 483, 665, 412, 885, 257, 2856, 2316, 257, 18161, 3209, 575, 281, 483, 665, 412, 257, 688, 51028, 51028, 295, 721, 13, 467, 575, 281, 1223, 577, 2856, 1985, 412, 257, 23551, 665, 51336, 51336, 1496, 293, 309, 2203, 281, 1223, 437, 309, 311, 767, 1417, 466, 293, 437, 307, 51534, 51534, 767, 2074, 293, 437, 307, 767, 406, 2074, 293, 264, 819, 2098, 294, 597, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.07406179471449419, "compression_ratio": 1.9191919191919191, "no_speech_prob": 3.2190375350182876e-05}, {"id": 75, "seek": 39996, "start": 423.35999999999996, "end": 426.71999999999997, "text": " actually true and what is actually not true and the different ways in which", "tokens": [50364, 2856, 307, 18519, 293, 466, 264, 1002, 293, 466, 5221, 293, 370, 5220, 13, 407, 281, 50762, 50762, 483, 665, 412, 885, 257, 2856, 2316, 257, 18161, 3209, 575, 281, 483, 665, 412, 257, 688, 51028, 51028, 295, 721, 13, 467, 575, 281, 1223, 577, 2856, 1985, 412, 257, 23551, 665, 51336, 51336, 1496, 293, 309, 2203, 281, 1223, 437, 309, 311, 767, 1417, 466, 293, 437, 307, 51534, 51534, 767, 2074, 293, 437, 307, 767, 406, 2074, 293, 264, 819, 2098, 294, 597, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.07406179471449419, "compression_ratio": 1.9191919191919191, "no_speech_prob": 3.2190375350182876e-05}, {"id": 76, "seek": 42672, "start": 426.72, "end": 434.88000000000005, "text": " things are expressed and so forth. So this was trained using a very similar", "tokens": [50364, 721, 366, 12675, 293, 370, 5220, 13, 407, 341, 390, 8895, 1228, 257, 588, 2531, 50772, 50772, 3109, 281, 437, 321, 603, 312, 1237, 412, 2489, 12, 83, 37726, 457, 309, 1409, 365, 4974, 50952, 50952, 17443, 293, 412, 264, 917, 295, 309, 456, 390, 257, 2316, 300, 727, 6069, 544, 813, 2217, 4, 51220, 51220, 295, 264, 565, 8944, 437, 264, 958, 1349, 295, 257, 28999, 7222, 576, 312, 13, 407, 294, 51602, 51602, 341, 1729, 1389, 337, 264, 624, 43, 44, 3318, 3035, 321, 550, 1890, 300, 293, 321, 645, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.08078328768412273, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.00010068724805023521}, {"id": 77, "seek": 42672, "start": 434.88000000000005, "end": 438.48, "text": " approach to what we'll be looking at fine-tuning but it started with random", "tokens": [50364, 721, 366, 12675, 293, 370, 5220, 13, 407, 341, 390, 8895, 1228, 257, 588, 2531, 50772, 50772, 3109, 281, 437, 321, 603, 312, 1237, 412, 2489, 12, 83, 37726, 457, 309, 1409, 365, 4974, 50952, 50952, 17443, 293, 412, 264, 917, 295, 309, 456, 390, 257, 2316, 300, 727, 6069, 544, 813, 2217, 4, 51220, 51220, 295, 264, 565, 8944, 437, 264, 958, 1349, 295, 257, 28999, 7222, 576, 312, 13, 407, 294, 51602, 51602, 341, 1729, 1389, 337, 264, 624, 43, 44, 3318, 3035, 321, 550, 1890, 300, 293, 321, 645, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.08078328768412273, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.00010068724805023521}, {"id": 78, "seek": 42672, "start": 438.48, "end": 443.84000000000003, "text": " weights and at the end of it there was a model that could predict more than 30%", "tokens": [50364, 721, 366, 12675, 293, 370, 5220, 13, 407, 341, 390, 8895, 1228, 257, 588, 2531, 50772, 50772, 3109, 281, 437, 321, 603, 312, 1237, 412, 2489, 12, 83, 37726, 457, 309, 1409, 365, 4974, 50952, 50952, 17443, 293, 412, 264, 917, 295, 309, 456, 390, 257, 2316, 300, 727, 6069, 544, 813, 2217, 4, 51220, 51220, 295, 264, 565, 8944, 437, 264, 958, 1349, 295, 257, 28999, 7222, 576, 312, 13, 407, 294, 51602, 51602, 341, 1729, 1389, 337, 264, 624, 43, 44, 3318, 3035, 321, 550, 1890, 300, 293, 321, 645, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.08078328768412273, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.00010068724805023521}, {"id": 79, "seek": 42672, "start": 443.84000000000003, "end": 451.48, "text": " of the time correctly what the next word of a Wikipedia article would be. So in", "tokens": [50364, 721, 366, 12675, 293, 370, 5220, 13, 407, 341, 390, 8895, 1228, 257, 588, 2531, 50772, 50772, 3109, 281, 437, 321, 603, 312, 1237, 412, 2489, 12, 83, 37726, 457, 309, 1409, 365, 4974, 50952, 50952, 17443, 293, 412, 264, 917, 295, 309, 456, 390, 257, 2316, 300, 727, 6069, 544, 813, 2217, 4, 51220, 51220, 295, 264, 565, 8944, 437, 264, 958, 1349, 295, 257, 28999, 7222, 576, 312, 13, 407, 294, 51602, 51602, 341, 1729, 1389, 337, 264, 624, 43, 44, 3318, 3035, 321, 550, 1890, 300, 293, 321, 645, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.08078328768412273, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.00010068724805023521}, {"id": 80, "seek": 42672, "start": 451.48, "end": 456.0, "text": " this particular case for the ULM fit paper we then took that and we were", "tokens": [50364, 721, 366, 12675, 293, 370, 5220, 13, 407, 341, 390, 8895, 1228, 257, 588, 2531, 50772, 50772, 3109, 281, 437, 321, 603, 312, 1237, 412, 2489, 12, 83, 37726, 457, 309, 1409, 365, 4974, 50952, 50952, 17443, 293, 412, 264, 917, 295, 309, 456, 390, 257, 2316, 300, 727, 6069, 544, 813, 2217, 4, 51220, 51220, 295, 264, 565, 8944, 437, 264, 958, 1349, 295, 257, 28999, 7222, 576, 312, 13, 407, 294, 51602, 51602, 341, 1729, 1389, 337, 264, 624, 43, 44, 3318, 3035, 321, 550, 1890, 300, 293, 321, 645, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.08078328768412273, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.00010068724805023521}, {"id": 81, "seek": 45600, "start": 456.0, "end": 460.92, "text": " trying to the first task I did actually for the the fast AI course back when I", "tokens": [50364, 1382, 281, 264, 700, 5633, 286, 630, 767, 337, 264, 264, 2370, 7318, 1164, 646, 562, 286, 50610, 50610, 14479, 341, 390, 281, 853, 293, 2573, 484, 1968, 21463, 27735, 3169, 10229, 645, 3353, 50862, 50862, 420, 3671, 16149, 630, 264, 954, 411, 264, 3169, 420, 406, 13, 407, 437, 286, 630, 390, 51184, 51184, 286, 2942, 257, 1150, 2856, 2316, 370, 797, 264, 2856, 2316, 510, 307, 51392, 51392, 746, 300, 6069, 82, 264, 958, 1349, 295, 257, 8174, 457, 2831, 813, 1228, 51546, 51546, 28999, 286, 1890, 341, 659, 12, 17227, 2001, 2316, 300, 390, 8895, 322, 28999, 293, 286, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.1156760701593363, "compression_ratio": 1.7245283018867925, "no_speech_prob": 5.828067151014693e-05}, {"id": 82, "seek": 45600, "start": 460.92, "end": 465.96, "text": " invented this was to try and figure out whether IMDB movie reviews were positive", "tokens": [50364, 1382, 281, 264, 700, 5633, 286, 630, 767, 337, 264, 264, 2370, 7318, 1164, 646, 562, 286, 50610, 50610, 14479, 341, 390, 281, 853, 293, 2573, 484, 1968, 21463, 27735, 3169, 10229, 645, 3353, 50862, 50862, 420, 3671, 16149, 630, 264, 954, 411, 264, 3169, 420, 406, 13, 407, 437, 286, 630, 390, 51184, 51184, 286, 2942, 257, 1150, 2856, 2316, 370, 797, 264, 2856, 2316, 510, 307, 51392, 51392, 746, 300, 6069, 82, 264, 958, 1349, 295, 257, 8174, 457, 2831, 813, 1228, 51546, 51546, 28999, 286, 1890, 341, 659, 12, 17227, 2001, 2316, 300, 390, 8895, 322, 28999, 293, 286, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.1156760701593363, "compression_ratio": 1.7245283018867925, "no_speech_prob": 5.828067151014693e-05}, {"id": 83, "seek": 45600, "start": 465.96, "end": 472.4, "text": " or negative sentiment did the person like the movie or not. So what I did was", "tokens": [50364, 1382, 281, 264, 700, 5633, 286, 630, 767, 337, 264, 264, 2370, 7318, 1164, 646, 562, 286, 50610, 50610, 14479, 341, 390, 281, 853, 293, 2573, 484, 1968, 21463, 27735, 3169, 10229, 645, 3353, 50862, 50862, 420, 3671, 16149, 630, 264, 954, 411, 264, 3169, 420, 406, 13, 407, 437, 286, 630, 390, 51184, 51184, 286, 2942, 257, 1150, 2856, 2316, 370, 797, 264, 2856, 2316, 510, 307, 51392, 51392, 746, 300, 6069, 82, 264, 958, 1349, 295, 257, 8174, 457, 2831, 813, 1228, 51546, 51546, 28999, 286, 1890, 341, 659, 12, 17227, 2001, 2316, 300, 390, 8895, 322, 28999, 293, 286, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.1156760701593363, "compression_ratio": 1.7245283018867925, "no_speech_prob": 5.828067151014693e-05}, {"id": 84, "seek": 45600, "start": 472.4, "end": 476.56, "text": " I created a second language model so again the language model here is", "tokens": [50364, 1382, 281, 264, 700, 5633, 286, 630, 767, 337, 264, 264, 2370, 7318, 1164, 646, 562, 286, 50610, 50610, 14479, 341, 390, 281, 853, 293, 2573, 484, 1968, 21463, 27735, 3169, 10229, 645, 3353, 50862, 50862, 420, 3671, 16149, 630, 264, 954, 411, 264, 3169, 420, 406, 13, 407, 437, 286, 630, 390, 51184, 51184, 286, 2942, 257, 1150, 2856, 2316, 370, 797, 264, 2856, 2316, 510, 307, 51392, 51392, 746, 300, 6069, 82, 264, 958, 1349, 295, 257, 8174, 457, 2831, 813, 1228, 51546, 51546, 28999, 286, 1890, 341, 659, 12, 17227, 2001, 2316, 300, 390, 8895, 322, 28999, 293, 286, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.1156760701593363, "compression_ratio": 1.7245283018867925, "no_speech_prob": 5.828067151014693e-05}, {"id": 85, "seek": 45600, "start": 476.56, "end": 479.64, "text": " something that predicts the next word of a sentence but rather than using", "tokens": [50364, 1382, 281, 264, 700, 5633, 286, 630, 767, 337, 264, 264, 2370, 7318, 1164, 646, 562, 286, 50610, 50610, 14479, 341, 390, 281, 853, 293, 2573, 484, 1968, 21463, 27735, 3169, 10229, 645, 3353, 50862, 50862, 420, 3671, 16149, 630, 264, 954, 411, 264, 3169, 420, 406, 13, 407, 437, 286, 630, 390, 51184, 51184, 286, 2942, 257, 1150, 2856, 2316, 370, 797, 264, 2856, 2316, 510, 307, 51392, 51392, 746, 300, 6069, 82, 264, 958, 1349, 295, 257, 8174, 457, 2831, 813, 1228, 51546, 51546, 28999, 286, 1890, 341, 659, 12, 17227, 2001, 2316, 300, 390, 8895, 322, 28999, 293, 286, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.1156760701593363, "compression_ratio": 1.7245283018867925, "no_speech_prob": 5.828067151014693e-05}, {"id": 86, "seek": 45600, "start": 479.64, "end": 485.32, "text": " Wikipedia I took this pre-trained model that was trained on Wikipedia and I", "tokens": [50364, 1382, 281, 264, 700, 5633, 286, 630, 767, 337, 264, 264, 2370, 7318, 1164, 646, 562, 286, 50610, 50610, 14479, 341, 390, 281, 853, 293, 2573, 484, 1968, 21463, 27735, 3169, 10229, 645, 3353, 50862, 50862, 420, 3671, 16149, 630, 264, 954, 411, 264, 3169, 420, 406, 13, 407, 437, 286, 630, 390, 51184, 51184, 286, 2942, 257, 1150, 2856, 2316, 370, 797, 264, 2856, 2316, 510, 307, 51392, 51392, 746, 300, 6069, 82, 264, 958, 1349, 295, 257, 8174, 457, 2831, 813, 1228, 51546, 51546, 28999, 286, 1890, 341, 659, 12, 17227, 2001, 2316, 300, 390, 8895, 322, 28999, 293, 286, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.1156760701593363, "compression_ratio": 1.7245283018867925, "no_speech_prob": 5.828067151014693e-05}, {"id": 87, "seek": 48532, "start": 485.32, "end": 491.32, "text": " ran a few more epochs using IMDB movie reviews so it got very good at", "tokens": [50364, 5872, 257, 1326, 544, 30992, 28346, 1228, 21463, 27735, 3169, 10229, 370, 309, 658, 588, 665, 412, 50664, 50664, 32884, 264, 958, 1349, 295, 364, 21463, 27735, 3169, 3131, 293, 550, 2721, 286, 1890, 50940, 50940, 729, 17443, 293, 286, 2489, 12, 83, 43703, 552, 337, 264, 5633, 295, 32884, 1968, 420, 406, 257, 51339, 51339, 3169, 3131, 390, 3353, 420, 3671, 16149, 13, 407, 729, 645, 264, 1045, 4439, 13, 51678, 51682], "temperature": 0.0, "avg_logprob": -0.11527453104654949, "compression_ratio": 1.6324324324324324, "no_speech_prob": 3.762710184673779e-05}, {"id": 88, "seek": 48532, "start": 491.32, "end": 496.84, "text": " predicting the next word of an IMDB movie review and then finally I took", "tokens": [50364, 5872, 257, 1326, 544, 30992, 28346, 1228, 21463, 27735, 3169, 10229, 370, 309, 658, 588, 665, 412, 50664, 50664, 32884, 264, 958, 1349, 295, 364, 21463, 27735, 3169, 3131, 293, 550, 2721, 286, 1890, 50940, 50940, 729, 17443, 293, 286, 2489, 12, 83, 43703, 552, 337, 264, 5633, 295, 32884, 1968, 420, 406, 257, 51339, 51339, 3169, 3131, 390, 3353, 420, 3671, 16149, 13, 407, 729, 645, 264, 1045, 4439, 13, 51678, 51682], "temperature": 0.0, "avg_logprob": -0.11527453104654949, "compression_ratio": 1.6324324324324324, "no_speech_prob": 3.762710184673779e-05}, {"id": 89, "seek": 48532, "start": 496.84, "end": 504.82, "text": " those weights and I fine-tuned them for the task of predicting whether or not a", "tokens": [50364, 5872, 257, 1326, 544, 30992, 28346, 1228, 21463, 27735, 3169, 10229, 370, 309, 658, 588, 665, 412, 50664, 50664, 32884, 264, 958, 1349, 295, 364, 21463, 27735, 3169, 3131, 293, 550, 2721, 286, 1890, 50940, 50940, 729, 17443, 293, 286, 2489, 12, 83, 43703, 552, 337, 264, 5633, 295, 32884, 1968, 420, 406, 257, 51339, 51339, 3169, 3131, 390, 3353, 420, 3671, 16149, 13, 407, 729, 645, 264, 1045, 4439, 13, 51678, 51682], "temperature": 0.0, "avg_logprob": -0.11527453104654949, "compression_ratio": 1.6324324324324324, "no_speech_prob": 3.762710184673779e-05}, {"id": 90, "seek": 48532, "start": 504.82, "end": 511.6, "text": " movie review was positive or negative sentiment. So those were the three steps.", "tokens": [50364, 5872, 257, 1326, 544, 30992, 28346, 1228, 21463, 27735, 3169, 10229, 370, 309, 658, 588, 665, 412, 50664, 50664, 32884, 264, 958, 1349, 295, 364, 21463, 27735, 3169, 3131, 293, 550, 2721, 286, 1890, 50940, 50940, 729, 17443, 293, 286, 2489, 12, 83, 43703, 552, 337, 264, 5633, 295, 32884, 1968, 420, 406, 257, 51339, 51339, 3169, 3131, 390, 3353, 420, 3671, 16149, 13, 407, 729, 645, 264, 1045, 4439, 13, 51678, 51682], "temperature": 0.0, "avg_logprob": -0.11527453104654949, "compression_ratio": 1.6324324324324324, "no_speech_prob": 3.762710184673779e-05}, {"id": 91, "seek": 51160, "start": 511.6, "end": 517.4, "text": " This is a particularly interesting approach because this very first model", "tokens": [50364, 639, 307, 257, 4098, 1880, 3109, 570, 341, 588, 700, 2316, 50654, 50654, 294, 1186, 264, 700, 732, 5245, 498, 291, 519, 466, 309, 436, 500, 380, 3651, 604, 50812, 50812, 16949, 286, 994, 380, 362, 281, 2500, 604, 733, 295, 4166, 10479, 420, 360, 604, 733, 295, 51082, 51082, 22711, 420, 2500, 1340, 439, 286, 2978, 390, 264, 3539, 2487, 295, 28999, 293, 51324, 51324, 3169, 10229, 2969, 570, 264, 16949, 390, 437, 311, 264, 958, 1349, 295, 257, 51518, 51518], "temperature": 0.0, "avg_logprob": -0.09478936308906191, "compression_ratio": 1.6858407079646018, "no_speech_prob": 6.203883822308853e-05}, {"id": 92, "seek": 51160, "start": 517.4, "end": 520.5600000000001, "text": " in fact the first two models if you think about it they don't require any", "tokens": [50364, 639, 307, 257, 4098, 1880, 3109, 570, 341, 588, 700, 2316, 50654, 50654, 294, 1186, 264, 700, 732, 5245, 498, 291, 519, 466, 309, 436, 500, 380, 3651, 604, 50812, 50812, 16949, 286, 994, 380, 362, 281, 2500, 604, 733, 295, 4166, 10479, 420, 360, 604, 733, 295, 51082, 51082, 22711, 420, 2500, 1340, 439, 286, 2978, 390, 264, 3539, 2487, 295, 28999, 293, 51324, 51324, 3169, 10229, 2969, 570, 264, 16949, 390, 437, 311, 264, 958, 1349, 295, 257, 51518, 51518], "temperature": 0.0, "avg_logprob": -0.09478936308906191, "compression_ratio": 1.6858407079646018, "no_speech_prob": 6.203883822308853e-05}, {"id": 93, "seek": 51160, "start": 520.5600000000001, "end": 525.96, "text": " labels I didn't have to collect any kind of document categories or do any kind of", "tokens": [50364, 639, 307, 257, 4098, 1880, 3109, 570, 341, 588, 700, 2316, 50654, 50654, 294, 1186, 264, 700, 732, 5245, 498, 291, 519, 466, 309, 436, 500, 380, 3651, 604, 50812, 50812, 16949, 286, 994, 380, 362, 281, 2500, 604, 733, 295, 4166, 10479, 420, 360, 604, 733, 295, 51082, 51082, 22711, 420, 2500, 1340, 439, 286, 2978, 390, 264, 3539, 2487, 295, 28999, 293, 51324, 51324, 3169, 10229, 2969, 570, 264, 16949, 390, 437, 311, 264, 958, 1349, 295, 257, 51518, 51518], "temperature": 0.0, "avg_logprob": -0.09478936308906191, "compression_ratio": 1.6858407079646018, "no_speech_prob": 6.203883822308853e-05}, {"id": 94, "seek": 51160, "start": 525.96, "end": 530.8000000000001, "text": " surveys or collect anything all I needed was the actual text of Wikipedia and", "tokens": [50364, 639, 307, 257, 4098, 1880, 3109, 570, 341, 588, 700, 2316, 50654, 50654, 294, 1186, 264, 700, 732, 5245, 498, 291, 519, 466, 309, 436, 500, 380, 3651, 604, 50812, 50812, 16949, 286, 994, 380, 362, 281, 2500, 604, 733, 295, 4166, 10479, 420, 360, 604, 733, 295, 51082, 51082, 22711, 420, 2500, 1340, 439, 286, 2978, 390, 264, 3539, 2487, 295, 28999, 293, 51324, 51324, 3169, 10229, 2969, 570, 264, 16949, 390, 437, 311, 264, 958, 1349, 295, 257, 51518, 51518], "temperature": 0.0, "avg_logprob": -0.09478936308906191, "compression_ratio": 1.6858407079646018, "no_speech_prob": 6.203883822308853e-05}, {"id": 95, "seek": 51160, "start": 530.8000000000001, "end": 534.6800000000001, "text": " movie reviews themselves because the labels was what's the next word of a", "tokens": [50364, 639, 307, 257, 4098, 1880, 3109, 570, 341, 588, 700, 2316, 50654, 50654, 294, 1186, 264, 700, 732, 5245, 498, 291, 519, 466, 309, 436, 500, 380, 3651, 604, 50812, 50812, 16949, 286, 994, 380, 362, 281, 2500, 604, 733, 295, 4166, 10479, 420, 360, 604, 733, 295, 51082, 51082, 22711, 420, 2500, 1340, 439, 286, 2978, 390, 264, 3539, 2487, 295, 28999, 293, 51324, 51324, 3169, 10229, 2969, 570, 264, 16949, 390, 437, 311, 264, 958, 1349, 295, 257, 51518, 51518], "temperature": 0.0, "avg_logprob": -0.09478936308906191, "compression_ratio": 1.6858407079646018, "no_speech_prob": 6.203883822308853e-05}, {"id": 96, "seek": 53468, "start": 534.68, "end": 545.4399999999999, "text": " sentence. Now since we built ULM fit and we used RNNs, recurrent neural networks,", "tokens": [50364, 8174, 13, 823, 1670, 321, 3094, 624, 43, 44, 3318, 293, 321, 1143, 45702, 45, 82, 11, 18680, 1753, 18161, 9590, 11, 50902, 50902, 281, 341, 412, 466, 264, 912, 565, 12, 742, 300, 321, 4736, 341, 257, 777, 733, 295, 51198, 51198, 9482, 4098, 4420, 337, 426, 45196, 412, 264, 565, 390, 4743, 1219, 51426, 51426, 4088, 433, 293, 4088, 433, 645, 4098, 3094, 570, 436, 393, 747, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.1876853002260809, "compression_ratio": 1.5729166666666667, "no_speech_prob": 3.480424857116304e-05}, {"id": 97, "seek": 53468, "start": 545.4399999999999, "end": 551.3599999999999, "text": " to this at about the same time-ish that we released this a new kind of", "tokens": [50364, 8174, 13, 823, 1670, 321, 3094, 624, 43, 44, 3318, 293, 321, 1143, 45702, 45, 82, 11, 18680, 1753, 18161, 9590, 11, 50902, 50902, 281, 341, 412, 466, 264, 912, 565, 12, 742, 300, 321, 4736, 341, 257, 777, 733, 295, 51198, 51198, 9482, 4098, 4420, 337, 426, 45196, 412, 264, 565, 390, 4743, 1219, 51426, 51426, 4088, 433, 293, 4088, 433, 645, 4098, 3094, 570, 436, 393, 747, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.1876853002260809, "compression_ratio": 1.5729166666666667, "no_speech_prob": 3.480424857116304e-05}, {"id": 98, "seek": 53468, "start": 551.3599999999999, "end": 555.92, "text": " architecture particularly useful for NLP at the time was developed called", "tokens": [50364, 8174, 13, 823, 1670, 321, 3094, 624, 43, 44, 3318, 293, 321, 1143, 45702, 45, 82, 11, 18680, 1753, 18161, 9590, 11, 50902, 50902, 281, 341, 412, 466, 264, 912, 565, 12, 742, 300, 321, 4736, 341, 257, 777, 733, 295, 51198, 51198, 9482, 4098, 4420, 337, 426, 45196, 412, 264, 565, 390, 4743, 1219, 51426, 51426, 4088, 433, 293, 4088, 433, 645, 4098, 3094, 570, 436, 393, 747, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.1876853002260809, "compression_ratio": 1.5729166666666667, "no_speech_prob": 3.480424857116304e-05}, {"id": 99, "seek": 53468, "start": 555.92, "end": 560.52, "text": " transformers and transformers were particularly built because they can take", "tokens": [50364, 8174, 13, 823, 1670, 321, 3094, 624, 43, 44, 3318, 293, 321, 1143, 45702, 45, 82, 11, 18680, 1753, 18161, 9590, 11, 50902, 50902, 281, 341, 412, 466, 264, 912, 565, 12, 742, 300, 321, 4736, 341, 257, 777, 733, 295, 51198, 51198, 9482, 4098, 4420, 337, 426, 45196, 412, 264, 565, 390, 4743, 1219, 51426, 51426, 4088, 433, 293, 4088, 433, 645, 4098, 3094, 570, 436, 393, 747, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.1876853002260809, "compression_ratio": 1.5729166666666667, "no_speech_prob": 3.480424857116304e-05}, {"id": 100, "seek": 56052, "start": 560.52, "end": 567.4399999999999, "text": " really good advantage of modern accelerators like like Google's TPUs.", "tokens": [50364, 534, 665, 5002, 295, 4363, 10172, 3391, 411, 411, 3329, 311, 314, 8115, 82, 13, 50710, 50762, 814, 994, 380, 534, 733, 295, 2089, 291, 281, 6069, 264, 958, 1349, 295, 257, 8174, 13, 467, 311, 51176, 51176, 445, 406, 577, 436, 434, 18519, 337, 4112, 321, 603, 751, 466, 1391, 294, 51346, 51346, 644, 732, 295, 264, 1164, 13, 407, 436, 11918, 1314, 264, 1558, 295, 32884, 264, 958, 51548, 51548, 1349, 295, 257, 8174, 293, 550, 729, 2602, 436, 630, 746, 445, 382, 665, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1414128409491645, "compression_ratio": 1.6504424778761062, "no_speech_prob": 6.204412056831643e-05}, {"id": 101, "seek": 56052, "start": 568.48, "end": 576.76, "text": " They didn't really kind of allow you to predict the next word of a sentence. It's", "tokens": [50364, 534, 665, 5002, 295, 4363, 10172, 3391, 411, 411, 3329, 311, 314, 8115, 82, 13, 50710, 50762, 814, 994, 380, 534, 733, 295, 2089, 291, 281, 6069, 264, 958, 1349, 295, 257, 8174, 13, 467, 311, 51176, 51176, 445, 406, 577, 436, 434, 18519, 337, 4112, 321, 603, 751, 466, 1391, 294, 51346, 51346, 644, 732, 295, 264, 1164, 13, 407, 436, 11918, 1314, 264, 1558, 295, 32884, 264, 958, 51548, 51548, 1349, 295, 257, 8174, 293, 550, 729, 2602, 436, 630, 746, 445, 382, 665, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1414128409491645, "compression_ratio": 1.6504424778761062, "no_speech_prob": 6.204412056831643e-05}, {"id": 102, "seek": 56052, "start": 576.76, "end": 580.16, "text": " just not how they're structured for reasons we'll talk about probably in", "tokens": [50364, 534, 665, 5002, 295, 4363, 10172, 3391, 411, 411, 3329, 311, 314, 8115, 82, 13, 50710, 50762, 814, 994, 380, 534, 733, 295, 2089, 291, 281, 6069, 264, 958, 1349, 295, 257, 8174, 13, 467, 311, 51176, 51176, 445, 406, 577, 436, 434, 18519, 337, 4112, 321, 603, 751, 466, 1391, 294, 51346, 51346, 644, 732, 295, 264, 1164, 13, 407, 436, 11918, 1314, 264, 1558, 295, 32884, 264, 958, 51548, 51548, 1349, 295, 257, 8174, 293, 550, 729, 2602, 436, 630, 746, 445, 382, 665, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1414128409491645, "compression_ratio": 1.6504424778761062, "no_speech_prob": 6.204412056831643e-05}, {"id": 103, "seek": 56052, "start": 580.16, "end": 584.1999999999999, "text": " part two of the course. So they threw away the idea of predicting the next", "tokens": [50364, 534, 665, 5002, 295, 4363, 10172, 3391, 411, 411, 3329, 311, 314, 8115, 82, 13, 50710, 50762, 814, 994, 380, 534, 733, 295, 2089, 291, 281, 6069, 264, 958, 1349, 295, 257, 8174, 13, 467, 311, 51176, 51176, 445, 406, 577, 436, 434, 18519, 337, 4112, 321, 603, 751, 466, 1391, 294, 51346, 51346, 644, 732, 295, 264, 1164, 13, 407, 436, 11918, 1314, 264, 1558, 295, 32884, 264, 958, 51548, 51548, 1349, 295, 257, 8174, 293, 550, 729, 2602, 436, 630, 746, 445, 382, 665, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1414128409491645, "compression_ratio": 1.6504424778761062, "no_speech_prob": 6.204412056831643e-05}, {"id": 104, "seek": 56052, "start": 584.1999999999999, "end": 587.52, "text": " word of a sentence and then those instead they did something just as good", "tokens": [50364, 534, 665, 5002, 295, 4363, 10172, 3391, 411, 411, 3329, 311, 314, 8115, 82, 13, 50710, 50762, 814, 994, 380, 534, 733, 295, 2089, 291, 281, 6069, 264, 958, 1349, 295, 257, 8174, 13, 467, 311, 51176, 51176, 445, 406, 577, 436, 434, 18519, 337, 4112, 321, 603, 751, 466, 1391, 294, 51346, 51346, 644, 732, 295, 264, 1164, 13, 407, 436, 11918, 1314, 264, 1558, 295, 32884, 264, 958, 51548, 51548, 1349, 295, 257, 8174, 293, 550, 729, 2602, 436, 630, 746, 445, 382, 665, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1414128409491645, "compression_ratio": 1.6504424778761062, "no_speech_prob": 6.204412056831643e-05}, {"id": 105, "seek": 58752, "start": 587.52, "end": 593.84, "text": " and pretty clever they took kind of chunks of Wikipedia or whatever text", "tokens": [50364, 293, 1238, 13494, 436, 1890, 733, 295, 24004, 295, 28999, 420, 2035, 2487, 50680, 50680, 436, 434, 1237, 412, 293, 22981, 412, 4974, 257, 1326, 2283, 293, 2351, 264, 2316, 281, 50943, 50943, 6069, 597, 437, 645, 264, 2283, 300, 645, 22981, 4476, 370, 309, 311, 257, 51172, 51172, 1238, 2531, 1558, 13, 5358, 813, 300, 264, 3875, 3410, 390, 264, 912, 382, 624, 43, 44, 3318, 51420, 51420, 436, 10772, 527, 45702, 45, 3109, 365, 257, 31782, 2316, 436, 10772, 527, 51648, 51648, 2856, 2316, 3109, 365, 437, 311, 1219, 257, 45249, 2856, 2316, 457, 661, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.10460534426245359, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.00012144495121901855}, {"id": 106, "seek": 58752, "start": 593.84, "end": 599.1, "text": " they're looking at and deleted at random a few words and asked the model to", "tokens": [50364, 293, 1238, 13494, 436, 1890, 733, 295, 24004, 295, 28999, 420, 2035, 2487, 50680, 50680, 436, 434, 1237, 412, 293, 22981, 412, 4974, 257, 1326, 2283, 293, 2351, 264, 2316, 281, 50943, 50943, 6069, 597, 437, 645, 264, 2283, 300, 645, 22981, 4476, 370, 309, 311, 257, 51172, 51172, 1238, 2531, 1558, 13, 5358, 813, 300, 264, 3875, 3410, 390, 264, 912, 382, 624, 43, 44, 3318, 51420, 51420, 436, 10772, 527, 45702, 45, 3109, 365, 257, 31782, 2316, 436, 10772, 527, 51648, 51648, 2856, 2316, 3109, 365, 437, 311, 1219, 257, 45249, 2856, 2316, 457, 661, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.10460534426245359, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.00012144495121901855}, {"id": 107, "seek": 58752, "start": 599.1, "end": 603.68, "text": " predict which what were the words that were deleted essentially so it's a", "tokens": [50364, 293, 1238, 13494, 436, 1890, 733, 295, 24004, 295, 28999, 420, 2035, 2487, 50680, 50680, 436, 434, 1237, 412, 293, 22981, 412, 4974, 257, 1326, 2283, 293, 2351, 264, 2316, 281, 50943, 50943, 6069, 597, 437, 645, 264, 2283, 300, 645, 22981, 4476, 370, 309, 311, 257, 51172, 51172, 1238, 2531, 1558, 13, 5358, 813, 300, 264, 3875, 3410, 390, 264, 912, 382, 624, 43, 44, 3318, 51420, 51420, 436, 10772, 527, 45702, 45, 3109, 365, 257, 31782, 2316, 436, 10772, 527, 51648, 51648, 2856, 2316, 3109, 365, 437, 311, 1219, 257, 45249, 2856, 2316, 457, 661, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.10460534426245359, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.00012144495121901855}, {"id": 108, "seek": 58752, "start": 603.68, "end": 608.64, "text": " pretty similar idea. Other than that the basic concept was the same as ULM fit", "tokens": [50364, 293, 1238, 13494, 436, 1890, 733, 295, 24004, 295, 28999, 420, 2035, 2487, 50680, 50680, 436, 434, 1237, 412, 293, 22981, 412, 4974, 257, 1326, 2283, 293, 2351, 264, 2316, 281, 50943, 50943, 6069, 597, 437, 645, 264, 2283, 300, 645, 22981, 4476, 370, 309, 311, 257, 51172, 51172, 1238, 2531, 1558, 13, 5358, 813, 300, 264, 3875, 3410, 390, 264, 912, 382, 624, 43, 44, 3318, 51420, 51420, 436, 10772, 527, 45702, 45, 3109, 365, 257, 31782, 2316, 436, 10772, 527, 51648, 51648, 2856, 2316, 3109, 365, 437, 311, 1219, 257, 45249, 2856, 2316, 457, 661, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.10460534426245359, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.00012144495121901855}, {"id": 109, "seek": 58752, "start": 608.64, "end": 613.1999999999999, "text": " they replaced our RNN approach with a transformer model they replaced our", "tokens": [50364, 293, 1238, 13494, 436, 1890, 733, 295, 24004, 295, 28999, 420, 2035, 2487, 50680, 50680, 436, 434, 1237, 412, 293, 22981, 412, 4974, 257, 1326, 2283, 293, 2351, 264, 2316, 281, 50943, 50943, 6069, 597, 437, 645, 264, 2283, 300, 645, 22981, 4476, 370, 309, 311, 257, 51172, 51172, 1238, 2531, 1558, 13, 5358, 813, 300, 264, 3875, 3410, 390, 264, 912, 382, 624, 43, 44, 3318, 51420, 51420, 436, 10772, 527, 45702, 45, 3109, 365, 257, 31782, 2316, 436, 10772, 527, 51648, 51648, 2856, 2316, 3109, 365, 437, 311, 1219, 257, 45249, 2856, 2316, 457, 661, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.10460534426245359, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.00012144495121901855}, {"id": 110, "seek": 58752, "start": 613.1999999999999, "end": 616.84, "text": " language model approach with what's called a masked language model but other", "tokens": [50364, 293, 1238, 13494, 436, 1890, 733, 295, 24004, 295, 28999, 420, 2035, 2487, 50680, 50680, 436, 434, 1237, 412, 293, 22981, 412, 4974, 257, 1326, 2283, 293, 2351, 264, 2316, 281, 50943, 50943, 6069, 597, 437, 645, 264, 2283, 300, 645, 22981, 4476, 370, 309, 311, 257, 51172, 51172, 1238, 2531, 1558, 13, 5358, 813, 300, 264, 3875, 3410, 390, 264, 912, 382, 624, 43, 44, 3318, 51420, 51420, 436, 10772, 527, 45702, 45, 3109, 365, 257, 31782, 2316, 436, 10772, 527, 51648, 51648, 2856, 2316, 3109, 365, 437, 311, 1219, 257, 45249, 2856, 2316, 457, 661, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.10460534426245359, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.00012144495121901855}, {"id": 111, "seek": 61684, "start": 616.84, "end": 620.4, "text": " than that the basic idea was the same. So today we're going to be looking at", "tokens": [50364, 813, 300, 264, 3875, 1558, 390, 264, 912, 13, 407, 965, 321, 434, 516, 281, 312, 1237, 412, 50542, 50542, 5245, 1228, 437, 311, 1813, 264, 291, 458, 709, 544, 3743, 3109, 813, 624, 43, 44, 3318, 50948, 50948, 597, 307, 341, 4088, 433, 45249, 2856, 2316, 3109, 13, 1033, 2619, 360, 321, 51200, 51200, 362, 604, 1651, 293, 286, 820, 2152, 321, 360, 362, 257, 8304, 490, 3535, 51502, 51502, 295, 36913, 2619, 12929, 5549, 505, 567, 486, 312, 3365, 264, 6343, 13415, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.09154940735210072, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.00012523445184342563}, {"id": 112, "seek": 61684, "start": 620.4, "end": 628.52, "text": " models using what's become the you know much more popular approach than ULM fit", "tokens": [50364, 813, 300, 264, 3875, 1558, 390, 264, 912, 13, 407, 965, 321, 434, 516, 281, 312, 1237, 412, 50542, 50542, 5245, 1228, 437, 311, 1813, 264, 291, 458, 709, 544, 3743, 3109, 813, 624, 43, 44, 3318, 50948, 50948, 597, 307, 341, 4088, 433, 45249, 2856, 2316, 3109, 13, 1033, 2619, 360, 321, 51200, 51200, 362, 604, 1651, 293, 286, 820, 2152, 321, 360, 362, 257, 8304, 490, 3535, 51502, 51502, 295, 36913, 2619, 12929, 5549, 505, 567, 486, 312, 3365, 264, 6343, 13415, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.09154940735210072, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.00012523445184342563}, {"id": 113, "seek": 61684, "start": 628.52, "end": 633.5600000000001, "text": " which is this transformers masked language model approach. Okay John do we", "tokens": [50364, 813, 300, 264, 3875, 1558, 390, 264, 912, 13, 407, 965, 321, 434, 516, 281, 312, 1237, 412, 50542, 50542, 5245, 1228, 437, 311, 1813, 264, 291, 458, 709, 544, 3743, 3109, 813, 624, 43, 44, 3318, 50948, 50948, 597, 307, 341, 4088, 433, 45249, 2856, 2316, 3109, 13, 1033, 2619, 360, 321, 51200, 51200, 362, 604, 1651, 293, 286, 820, 2152, 321, 360, 362, 257, 8304, 490, 3535, 51502, 51502, 295, 36913, 2619, 12929, 5549, 505, 567, 486, 312, 3365, 264, 6343, 13415, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.09154940735210072, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.00012523445184342563}, {"id": 114, "seek": 61684, "start": 633.5600000000001, "end": 639.6, "text": " have any questions and I should mention we do have a professor from University", "tokens": [50364, 813, 300, 264, 3875, 1558, 390, 264, 912, 13, 407, 965, 321, 434, 516, 281, 312, 1237, 412, 50542, 50542, 5245, 1228, 437, 311, 1813, 264, 291, 458, 709, 544, 3743, 3109, 813, 624, 43, 44, 3318, 50948, 50948, 597, 307, 341, 4088, 433, 45249, 2856, 2316, 3109, 13, 1033, 2619, 360, 321, 51200, 51200, 362, 604, 1651, 293, 286, 820, 2152, 321, 360, 362, 257, 8304, 490, 3535, 51502, 51502, 295, 36913, 2619, 12929, 5549, 505, 567, 486, 312, 3365, 264, 6343, 13415, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.09154940735210072, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.00012523445184342563}, {"id": 115, "seek": 61684, "start": 639.6, "end": 644.9200000000001, "text": " of Queensland John Williams joining us who will be asking the highest voted", "tokens": [50364, 813, 300, 264, 3875, 1558, 390, 264, 912, 13, 407, 965, 321, 434, 516, 281, 312, 1237, 412, 50542, 50542, 5245, 1228, 437, 311, 1813, 264, 291, 458, 709, 544, 3743, 3109, 813, 624, 43, 44, 3318, 50948, 50948, 597, 307, 341, 4088, 433, 45249, 2856, 2316, 3109, 13, 1033, 2619, 360, 321, 51200, 51200, 362, 604, 1651, 293, 286, 820, 2152, 321, 360, 362, 257, 8304, 490, 3535, 51502, 51502, 295, 36913, 2619, 12929, 5549, 505, 567, 486, 312, 3365, 264, 6343, 13415, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.09154940735210072, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.00012523445184342563}, {"id": 116, "seek": 64492, "start": 644.92, "end": 649.5999999999999, "text": " questions from the community. What do you got John? Yeah thanks Jeremy look and we", "tokens": [50364, 1651, 490, 264, 1768, 13, 708, 360, 291, 658, 2619, 30, 865, 3231, 17809, 574, 293, 321, 50598, 50598, 1062, 312, 11233, 264, 3874, 510, 286, 9091, 341, 307, 689, 291, 434, 516, 4440, 457, 50784, 50784, 321, 600, 658, 257, 665, 1168, 510, 322, 264, 17542, 597, 307, 577, 360, 291, 352, 490, 257, 50958, 50958, 2316, 300, 311, 8895, 281, 6069, 264, 958, 1349, 281, 257, 2316, 300, 393, 312, 1143, 337, 51236, 51236, 21538, 30, 4894, 370, 1338, 321, 486, 312, 1242, 666, 300, 294, 544, 2607, 293, 294, 51601, 51601, 1186, 1310, 257, 665, 1081, 281, 722, 576, 312, 264, 958, 4137, 13, 9242, 295, 976, 291, 257, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.08457728089957402, "compression_ratio": 1.6725978647686832, "no_speech_prob": 0.0004026847018394619}, {"id": 117, "seek": 64492, "start": 649.5999999999999, "end": 653.3199999999999, "text": " might be jumping the gun here I suspect this is where you're going tonight but", "tokens": [50364, 1651, 490, 264, 1768, 13, 708, 360, 291, 658, 2619, 30, 865, 3231, 17809, 574, 293, 321, 50598, 50598, 1062, 312, 11233, 264, 3874, 510, 286, 9091, 341, 307, 689, 291, 434, 516, 4440, 457, 50784, 50784, 321, 600, 658, 257, 665, 1168, 510, 322, 264, 17542, 597, 307, 577, 360, 291, 352, 490, 257, 50958, 50958, 2316, 300, 311, 8895, 281, 6069, 264, 958, 1349, 281, 257, 2316, 300, 393, 312, 1143, 337, 51236, 51236, 21538, 30, 4894, 370, 1338, 321, 486, 312, 1242, 666, 300, 294, 544, 2607, 293, 294, 51601, 51601, 1186, 1310, 257, 665, 1081, 281, 722, 576, 312, 264, 958, 4137, 13, 9242, 295, 976, 291, 257, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.08457728089957402, "compression_ratio": 1.6725978647686832, "no_speech_prob": 0.0004026847018394619}, {"id": 118, "seek": 64492, "start": 653.3199999999999, "end": 656.8, "text": " we've got a good question here on the forum which is how do you go from a", "tokens": [50364, 1651, 490, 264, 1768, 13, 708, 360, 291, 658, 2619, 30, 865, 3231, 17809, 574, 293, 321, 50598, 50598, 1062, 312, 11233, 264, 3874, 510, 286, 9091, 341, 307, 689, 291, 434, 516, 4440, 457, 50784, 50784, 321, 600, 658, 257, 665, 1168, 510, 322, 264, 17542, 597, 307, 577, 360, 291, 352, 490, 257, 50958, 50958, 2316, 300, 311, 8895, 281, 6069, 264, 958, 1349, 281, 257, 2316, 300, 393, 312, 1143, 337, 51236, 51236, 21538, 30, 4894, 370, 1338, 321, 486, 312, 1242, 666, 300, 294, 544, 2607, 293, 294, 51601, 51601, 1186, 1310, 257, 665, 1081, 281, 722, 576, 312, 264, 958, 4137, 13, 9242, 295, 976, 291, 257, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.08457728089957402, "compression_ratio": 1.6725978647686832, "no_speech_prob": 0.0004026847018394619}, {"id": 119, "seek": 64492, "start": 656.8, "end": 662.36, "text": " model that's trained to predict the next word to a model that can be used for", "tokens": [50364, 1651, 490, 264, 1768, 13, 708, 360, 291, 658, 2619, 30, 865, 3231, 17809, 574, 293, 321, 50598, 50598, 1062, 312, 11233, 264, 3874, 510, 286, 9091, 341, 307, 689, 291, 434, 516, 4440, 457, 50784, 50784, 321, 600, 658, 257, 665, 1168, 510, 322, 264, 17542, 597, 307, 577, 360, 291, 352, 490, 257, 50958, 50958, 2316, 300, 311, 8895, 281, 6069, 264, 958, 1349, 281, 257, 2316, 300, 393, 312, 1143, 337, 51236, 51236, 21538, 30, 4894, 370, 1338, 321, 486, 312, 1242, 666, 300, 294, 544, 2607, 293, 294, 51601, 51601, 1186, 1310, 257, 665, 1081, 281, 722, 576, 312, 264, 958, 4137, 13, 9242, 295, 976, 291, 257, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.08457728089957402, "compression_ratio": 1.6725978647686832, "no_speech_prob": 0.0004026847018394619}, {"id": 120, "seek": 64492, "start": 662.36, "end": 669.66, "text": " classification? Sure so yeah we will be getting into that in more detail and in", "tokens": [50364, 1651, 490, 264, 1768, 13, 708, 360, 291, 658, 2619, 30, 865, 3231, 17809, 574, 293, 321, 50598, 50598, 1062, 312, 11233, 264, 3874, 510, 286, 9091, 341, 307, 689, 291, 434, 516, 4440, 457, 50784, 50784, 321, 600, 658, 257, 665, 1168, 510, 322, 264, 17542, 597, 307, 577, 360, 291, 352, 490, 257, 50958, 50958, 2316, 300, 311, 8895, 281, 6069, 264, 958, 1349, 281, 257, 2316, 300, 393, 312, 1143, 337, 51236, 51236, 21538, 30, 4894, 370, 1338, 321, 486, 312, 1242, 666, 300, 294, 544, 2607, 293, 294, 51601, 51601, 1186, 1310, 257, 665, 1081, 281, 722, 576, 312, 264, 958, 4137, 13, 9242, 295, 976, 291, 257, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.08457728089957402, "compression_ratio": 1.6725978647686832, "no_speech_prob": 0.0004026847018394619}, {"id": 121, "seek": 64492, "start": 669.66, "end": 674.4399999999999, "text": " fact maybe a good place to start would be the next slide. Kind of give you a", "tokens": [50364, 1651, 490, 264, 1768, 13, 708, 360, 291, 658, 2619, 30, 865, 3231, 17809, 574, 293, 321, 50598, 50598, 1062, 312, 11233, 264, 3874, 510, 286, 9091, 341, 307, 689, 291, 434, 516, 4440, 457, 50784, 50784, 321, 600, 658, 257, 665, 1168, 510, 322, 264, 17542, 597, 307, 577, 360, 291, 352, 490, 257, 50958, 50958, 2316, 300, 311, 8895, 281, 6069, 264, 958, 1349, 281, 257, 2316, 300, 393, 312, 1143, 337, 51236, 51236, 21538, 30, 4894, 370, 1338, 321, 486, 312, 1242, 666, 300, 294, 544, 2607, 293, 294, 51601, 51601, 1186, 1310, 257, 665, 1081, 281, 722, 576, 312, 264, 958, 4137, 13, 9242, 295, 976, 291, 257, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.08457728089957402, "compression_ratio": 1.6725978647686832, "no_speech_prob": 0.0004026847018394619}, {"id": 122, "seek": 67444, "start": 674.44, "end": 679.1600000000001, "text": " sense of this. You might remember in lesson one we looked at this fantastic", "tokens": [50364, 2020, 295, 341, 13, 509, 1062, 1604, 294, 6898, 472, 321, 2956, 412, 341, 5456, 50600, 50600, 4853, 5441, 293, 36790, 3035, 689, 321, 2956, 412, 5056, 14455, 295, 264, 700, 4583, 295, 50840, 50840, 257, 3256, 2533, 21538, 2316, 293, 4583, 472, 632, 6352, 295, 17443, 300, 1352, 51288, 51288, 21539, 8819, 293, 510, 366, 512, 5110, 295, 9239, 295, 5787, 300, 51500, 51500, 10727, 21447, 365, 293, 6182, 21539, 8819, 293, 733, 295, 2017, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.16744829416275026, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.00012712219904642552}, {"id": 123, "seek": 67444, "start": 679.1600000000001, "end": 683.96, "text": " Zeiler and Fergus paper where we looked at visualizations of the first layer of", "tokens": [50364, 2020, 295, 341, 13, 509, 1062, 1604, 294, 6898, 472, 321, 2956, 412, 341, 5456, 50600, 50600, 4853, 5441, 293, 36790, 3035, 689, 321, 2956, 412, 5056, 14455, 295, 264, 700, 4583, 295, 50840, 50840, 257, 3256, 2533, 21538, 2316, 293, 4583, 472, 632, 6352, 295, 17443, 300, 1352, 51288, 51288, 21539, 8819, 293, 510, 366, 512, 5110, 295, 9239, 295, 5787, 300, 51500, 51500, 10727, 21447, 365, 293, 6182, 21539, 8819, 293, 733, 295, 2017, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.16744829416275026, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.00012712219904642552}, {"id": 124, "seek": 67444, "start": 683.96, "end": 692.9200000000001, "text": " a image net classification model and layer one had sets of weights that found", "tokens": [50364, 2020, 295, 341, 13, 509, 1062, 1604, 294, 6898, 472, 321, 2956, 412, 341, 5456, 50600, 50600, 4853, 5441, 293, 36790, 3035, 689, 321, 2956, 412, 5056, 14455, 295, 264, 700, 4583, 295, 50840, 50840, 257, 3256, 2533, 21538, 2316, 293, 4583, 472, 632, 6352, 295, 17443, 300, 1352, 51288, 51288, 21539, 8819, 293, 510, 366, 512, 5110, 295, 9239, 295, 5787, 300, 51500, 51500, 10727, 21447, 365, 293, 6182, 21539, 8819, 293, 733, 295, 2017, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.16744829416275026, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.00012712219904642552}, {"id": 125, "seek": 67444, "start": 692.9200000000001, "end": 697.1600000000001, "text": " diagonal edges and here are some examples of bits of photos that", "tokens": [50364, 2020, 295, 341, 13, 509, 1062, 1604, 294, 6898, 472, 321, 2956, 412, 341, 5456, 50600, 50600, 4853, 5441, 293, 36790, 3035, 689, 321, 2956, 412, 5056, 14455, 295, 264, 700, 4583, 295, 50840, 50840, 257, 3256, 2533, 21538, 2316, 293, 4583, 472, 632, 6352, 295, 17443, 300, 1352, 51288, 51288, 21539, 8819, 293, 510, 366, 512, 5110, 295, 9239, 295, 5787, 300, 51500, 51500, 10727, 21447, 365, 293, 6182, 21539, 8819, 293, 733, 295, 2017, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.16744829416275026, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.00012712219904642552}, {"id": 126, "seek": 67444, "start": 697.1600000000001, "end": 702.24, "text": " successfully matched with and opposite diagonal edges and kind of color", "tokens": [50364, 2020, 295, 341, 13, 509, 1062, 1604, 294, 6898, 472, 321, 2956, 412, 341, 5456, 50600, 50600, 4853, 5441, 293, 36790, 3035, 689, 321, 2956, 412, 5056, 14455, 295, 264, 700, 4583, 295, 50840, 50840, 257, 3256, 2533, 21538, 2316, 293, 4583, 472, 632, 6352, 295, 17443, 300, 1352, 51288, 51288, 21539, 8819, 293, 510, 366, 512, 5110, 295, 9239, 295, 5787, 300, 51500, 51500, 10727, 21447, 365, 293, 6182, 21539, 8819, 293, 733, 295, 2017, 51754, 51754], "temperature": 0.0, "avg_logprob": -0.16744829416275026, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.00012712219904642552}, {"id": 127, "seek": 70224, "start": 702.24, "end": 706.36, "text": " gradients and here's some examples of bits of pictures that matched and then", "tokens": [50364, 2771, 2448, 293, 510, 311, 512, 5110, 295, 9239, 295, 5242, 300, 21447, 293, 550, 50570, 50570, 4583, 732, 9354, 729, 293, 586, 291, 458, 577, 729, 645, 9354, 558, 613, 50840, 50840, 645, 11048, 2587, 8213, 6815, 300, 645, 3869, 1214, 1392, 293, 550, 6352, 295, 51150, 51150, 729, 11048, 2587, 8213, 6815, 264, 23930, 295, 729, 436, 434, 1219, 2430, 763, 51310, 51310, 645, 550, 2969, 1190, 807, 257, 8141, 44106, 11048, 2587, 8213, 4985, 51510, 51510, 3869, 1214, 370, 300, 586, 291, 500, 380, 445, 362, 281, 362, 4691, 46866, 457, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.1078572857136629, "compression_ratio": 1.9868995633187774, "no_speech_prob": 4.683050065068528e-05}, {"id": 128, "seek": 70224, "start": 706.36, "end": 711.76, "text": " layer two combined those and now you know how those were combined right these", "tokens": [50364, 2771, 2448, 293, 510, 311, 512, 5110, 295, 9239, 295, 5242, 300, 21447, 293, 550, 50570, 50570, 4583, 732, 9354, 729, 293, 586, 291, 458, 577, 729, 645, 9354, 558, 613, 50840, 50840, 645, 11048, 2587, 8213, 6815, 300, 645, 3869, 1214, 1392, 293, 550, 6352, 295, 51150, 51150, 729, 11048, 2587, 8213, 6815, 264, 23930, 295, 729, 436, 434, 1219, 2430, 763, 51310, 51310, 645, 550, 2969, 1190, 807, 257, 8141, 44106, 11048, 2587, 8213, 4985, 51510, 51510, 3869, 1214, 370, 300, 586, 291, 500, 380, 445, 362, 281, 362, 4691, 46866, 457, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.1078572857136629, "compression_ratio": 1.9868995633187774, "no_speech_prob": 4.683050065068528e-05}, {"id": 129, "seek": 70224, "start": 711.76, "end": 717.96, "text": " were rectified linear units that were added together okay and then sets of", "tokens": [50364, 2771, 2448, 293, 510, 311, 512, 5110, 295, 9239, 295, 5242, 300, 21447, 293, 550, 50570, 50570, 4583, 732, 9354, 729, 293, 586, 291, 458, 577, 729, 645, 9354, 558, 613, 50840, 50840, 645, 11048, 2587, 8213, 6815, 300, 645, 3869, 1214, 1392, 293, 550, 6352, 295, 51150, 51150, 729, 11048, 2587, 8213, 6815, 264, 23930, 295, 729, 436, 434, 1219, 2430, 763, 51310, 51310, 645, 550, 2969, 1190, 807, 257, 8141, 44106, 11048, 2587, 8213, 4985, 51510, 51510, 3869, 1214, 370, 300, 586, 291, 500, 380, 445, 362, 281, 362, 4691, 46866, 457, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.1078572857136629, "compression_ratio": 1.9868995633187774, "no_speech_prob": 4.683050065068528e-05}, {"id": 130, "seek": 70224, "start": 717.96, "end": 721.16, "text": " those rectified linear units the outputs of those they're called activations", "tokens": [50364, 2771, 2448, 293, 510, 311, 512, 5110, 295, 9239, 295, 5242, 300, 21447, 293, 550, 50570, 50570, 4583, 732, 9354, 729, 293, 586, 291, 458, 577, 729, 645, 9354, 558, 613, 50840, 50840, 645, 11048, 2587, 8213, 6815, 300, 645, 3869, 1214, 1392, 293, 550, 6352, 295, 51150, 51150, 729, 11048, 2587, 8213, 6815, 264, 23930, 295, 729, 436, 434, 1219, 2430, 763, 51310, 51310, 645, 550, 2969, 1190, 807, 257, 8141, 44106, 11048, 2587, 8213, 4985, 51510, 51510, 3869, 1214, 370, 300, 586, 291, 500, 380, 445, 362, 281, 362, 4691, 46866, 457, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.1078572857136629, "compression_ratio": 1.9868995633187774, "no_speech_prob": 4.683050065068528e-05}, {"id": 131, "seek": 70224, "start": 721.16, "end": 725.16, "text": " were then themselves run through a matrix multiplier rectified linear unit", "tokens": [50364, 2771, 2448, 293, 510, 311, 512, 5110, 295, 9239, 295, 5242, 300, 21447, 293, 550, 50570, 50570, 4583, 732, 9354, 729, 293, 586, 291, 458, 577, 729, 645, 9354, 558, 613, 50840, 50840, 645, 11048, 2587, 8213, 6815, 300, 645, 3869, 1214, 1392, 293, 550, 6352, 295, 51150, 51150, 729, 11048, 2587, 8213, 6815, 264, 23930, 295, 729, 436, 434, 1219, 2430, 763, 51310, 51310, 645, 550, 2969, 1190, 807, 257, 8141, 44106, 11048, 2587, 8213, 4985, 51510, 51510, 3869, 1214, 370, 300, 586, 291, 500, 380, 445, 362, 281, 362, 4691, 46866, 457, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.1078572857136629, "compression_ratio": 1.9868995633187774, "no_speech_prob": 4.683050065068528e-05}, {"id": 132, "seek": 70224, "start": 725.16, "end": 729.4, "text": " added together so that now you don't just have to have edge detectors but", "tokens": [50364, 2771, 2448, 293, 510, 311, 512, 5110, 295, 9239, 295, 5242, 300, 21447, 293, 550, 50570, 50570, 4583, 732, 9354, 729, 293, 586, 291, 458, 577, 729, 645, 9354, 558, 613, 50840, 50840, 645, 11048, 2587, 8213, 6815, 300, 645, 3869, 1214, 1392, 293, 550, 6352, 295, 51150, 51150, 729, 11048, 2587, 8213, 6815, 264, 23930, 295, 729, 436, 434, 1219, 2430, 763, 51310, 51310, 645, 550, 2969, 1190, 807, 257, 8141, 44106, 11048, 2587, 8213, 4985, 51510, 51510, 3869, 1214, 370, 300, 586, 291, 500, 380, 445, 362, 281, 362, 4691, 46866, 457, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.1078572857136629, "compression_ratio": 1.9868995633187774, "no_speech_prob": 4.683050065068528e-05}, {"id": 133, "seek": 72940, "start": 729.4, "end": 733.92, "text": " layer two had corner detectors and here's some examples of some corners", "tokens": [50364, 4583, 732, 632, 4538, 46866, 293, 510, 311, 512, 5110, 295, 512, 12413, 50590, 50590, 300, 300, 4538, 25712, 10727, 1352, 13, 5459, 613, 645, 406, 50780, 50780, 38648, 294, 604, 636, 436, 445, 14178, 490, 264, 16235, 23475, 3097, 51048, 51048, 1399, 13, 35166, 732, 632, 5110, 295, 6329, 46866, 382, 309, 4523, 484, 293, 51322, 51322, 31533, 257, 857, 538, 264, 565, 321, 658, 281, 4583, 1732, 321, 632, 5255, 293, 39215, 38868, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.13873428634450405, "compression_ratio": 1.7069767441860466, "no_speech_prob": 0.00010070598364109173}, {"id": 134, "seek": 72940, "start": 733.92, "end": 737.72, "text": " that that corner detector successfully found. Remember these were not", "tokens": [50364, 4583, 732, 632, 4538, 46866, 293, 510, 311, 512, 5110, 295, 512, 12413, 50590, 50590, 300, 300, 4538, 25712, 10727, 1352, 13, 5459, 613, 645, 406, 50780, 50780, 38648, 294, 604, 636, 436, 445, 14178, 490, 264, 16235, 23475, 3097, 51048, 51048, 1399, 13, 35166, 732, 632, 5110, 295, 6329, 46866, 382, 309, 4523, 484, 293, 51322, 51322, 31533, 257, 857, 538, 264, 565, 321, 658, 281, 4583, 1732, 321, 632, 5255, 293, 39215, 38868, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.13873428634450405, "compression_ratio": 1.7069767441860466, "no_speech_prob": 0.00010070598364109173}, {"id": 135, "seek": 72940, "start": 737.72, "end": 743.0799999999999, "text": " engineered in any way they just evolved from the gradient descent training", "tokens": [50364, 4583, 732, 632, 4538, 46866, 293, 510, 311, 512, 5110, 295, 512, 12413, 50590, 50590, 300, 300, 4538, 25712, 10727, 1352, 13, 5459, 613, 645, 406, 50780, 50780, 38648, 294, 604, 636, 436, 445, 14178, 490, 264, 16235, 23475, 3097, 51048, 51048, 1399, 13, 35166, 732, 632, 5110, 295, 6329, 46866, 382, 309, 4523, 484, 293, 51322, 51322, 31533, 257, 857, 538, 264, 565, 321, 658, 281, 4583, 1732, 321, 632, 5255, 293, 39215, 38868, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.13873428634450405, "compression_ratio": 1.7069767441860466, "no_speech_prob": 0.00010070598364109173}, {"id": 136, "seek": 72940, "start": 743.0799999999999, "end": 748.56, "text": " process. Layer two had examples of circle detectors as it turns out and", "tokens": [50364, 4583, 732, 632, 4538, 46866, 293, 510, 311, 512, 5110, 295, 512, 12413, 50590, 50590, 300, 300, 4538, 25712, 10727, 1352, 13, 5459, 613, 645, 406, 50780, 50780, 38648, 294, 604, 636, 436, 445, 14178, 490, 264, 16235, 23475, 3097, 51048, 51048, 1399, 13, 35166, 732, 632, 5110, 295, 6329, 46866, 382, 309, 4523, 484, 293, 51322, 51322, 31533, 257, 857, 538, 264, 565, 321, 658, 281, 4583, 1732, 321, 632, 5255, 293, 39215, 38868, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.13873428634450405, "compression_ratio": 1.7069767441860466, "no_speech_prob": 0.00010070598364109173}, {"id": 137, "seek": 72940, "start": 748.56, "end": 755.24, "text": " skipping a bit by the time we got to layer five we had bird and lizard eyeball", "tokens": [50364, 4583, 732, 632, 4538, 46866, 293, 510, 311, 512, 5110, 295, 512, 12413, 50590, 50590, 300, 300, 4538, 25712, 10727, 1352, 13, 5459, 613, 645, 406, 50780, 50780, 38648, 294, 604, 636, 436, 445, 14178, 490, 264, 16235, 23475, 3097, 51048, 51048, 1399, 13, 35166, 732, 632, 5110, 295, 6329, 46866, 382, 309, 4523, 484, 293, 51322, 51322, 31533, 257, 857, 538, 264, 565, 321, 658, 281, 4583, 1732, 321, 632, 5255, 293, 39215, 38868, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.13873428634450405, "compression_ratio": 1.7069767441860466, "no_speech_prob": 0.00010070598364109173}, {"id": 138, "seek": 75524, "start": 755.24, "end": 767.36, "text": " detectors and dog face detectors and flower detectors and so forth. Now you", "tokens": [50364, 46866, 293, 3000, 1851, 46866, 293, 8617, 46866, 293, 370, 5220, 13, 823, 291, 50970, 50970, 458, 13434, 291, 1116, 362, 746, 411, 257, 5015, 31890, 2625, 576, 312, 746, 291, 1116, 51122, 51122, 1391, 312, 3097, 1238, 11672, 294, 341, 1164, 370, 300, 291, 458, 291, 600, 658, 51264, 51264, 2625, 7914, 406, 445, 1732, 7914, 13, 823, 264, 1780, 7914, 360, 721, 300, 366, 709, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.10683670178265639, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.39974210166838e-05}, {"id": 139, "seek": 75524, "start": 767.36, "end": 770.4, "text": " know nowadays you'd have something like a ResNet 50 would be something you'd", "tokens": [50364, 46866, 293, 3000, 1851, 46866, 293, 8617, 46866, 293, 370, 5220, 13, 823, 291, 50970, 50970, 458, 13434, 291, 1116, 362, 746, 411, 257, 5015, 31890, 2625, 576, 312, 746, 291, 1116, 51122, 51122, 1391, 312, 3097, 1238, 11672, 294, 341, 1164, 370, 300, 291, 458, 291, 600, 658, 51264, 51264, 2625, 7914, 406, 445, 1732, 7914, 13, 823, 264, 1780, 7914, 360, 721, 300, 366, 709, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.10683670178265639, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.39974210166838e-05}, {"id": 140, "seek": 75524, "start": 770.4, "end": 773.24, "text": " probably be training pretty regularly in this course so that you know you've got", "tokens": [50364, 46866, 293, 3000, 1851, 46866, 293, 8617, 46866, 293, 370, 5220, 13, 823, 291, 50970, 50970, 458, 13434, 291, 1116, 362, 746, 411, 257, 5015, 31890, 2625, 576, 312, 746, 291, 1116, 51122, 51122, 1391, 312, 3097, 1238, 11672, 294, 341, 1164, 370, 300, 291, 458, 291, 600, 658, 51264, 51264, 2625, 7914, 406, 445, 1732, 7914, 13, 823, 264, 1780, 7914, 360, 721, 300, 366, 709, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.10683670178265639, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.39974210166838e-05}, {"id": 141, "seek": 75524, "start": 773.24, "end": 780.4, "text": " 50 layers not just five layers. Now the later layers do things that are much", "tokens": [50364, 46866, 293, 3000, 1851, 46866, 293, 8617, 46866, 293, 370, 5220, 13, 823, 291, 50970, 50970, 458, 13434, 291, 1116, 362, 746, 411, 257, 5015, 31890, 2625, 576, 312, 746, 291, 1116, 51122, 51122, 1391, 312, 3097, 1238, 11672, 294, 341, 1164, 370, 300, 291, 458, 291, 600, 658, 51264, 51264, 2625, 7914, 406, 445, 1732, 7914, 13, 823, 264, 1780, 7914, 360, 721, 300, 366, 709, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.10683670178265639, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.39974210166838e-05}, {"id": 142, "seek": 78040, "start": 780.4, "end": 787.3199999999999, "text": " more specific to the training task which is like actually predicting really what", "tokens": [50364, 544, 2685, 281, 264, 3097, 5633, 597, 307, 411, 767, 32884, 534, 437, 50710, 50710, 437, 307, 309, 300, 321, 434, 1237, 412, 13, 440, 2440, 7914, 1238, 17518, 291, 434, 50946, 50946, 516, 281, 643, 281, 1319, 552, 709, 382, 938, 382, 291, 434, 1237, 412, 411, 512, 733, 51116, 51116, 295, 3303, 5787, 558, 291, 434, 516, 281, 643, 4691, 46866, 293, 16235, 51290, 51290, 46866, 13, 407, 437, 321, 360, 294, 294, 264, 2489, 12, 83, 37726, 1399, 307, 456, 311, 767, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.10938546898659696, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.00010070203279610723}, {"id": 143, "seek": 78040, "start": 787.3199999999999, "end": 792.04, "text": " what is it that we're looking at. The early layers pretty unlikely you're", "tokens": [50364, 544, 2685, 281, 264, 3097, 5633, 597, 307, 411, 767, 32884, 534, 437, 50710, 50710, 437, 307, 309, 300, 321, 434, 1237, 412, 13, 440, 2440, 7914, 1238, 17518, 291, 434, 50946, 50946, 516, 281, 643, 281, 1319, 552, 709, 382, 938, 382, 291, 434, 1237, 412, 411, 512, 733, 51116, 51116, 295, 3303, 5787, 558, 291, 434, 516, 281, 643, 4691, 46866, 293, 16235, 51290, 51290, 46866, 13, 407, 437, 321, 360, 294, 294, 264, 2489, 12, 83, 37726, 1399, 307, 456, 311, 767, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.10938546898659696, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.00010070203279610723}, {"id": 144, "seek": 78040, "start": 792.04, "end": 795.4399999999999, "text": " going to need to change them much as long as you're looking at like some kind", "tokens": [50364, 544, 2685, 281, 264, 3097, 5633, 597, 307, 411, 767, 32884, 534, 437, 50710, 50710, 437, 307, 309, 300, 321, 434, 1237, 412, 13, 440, 2440, 7914, 1238, 17518, 291, 434, 50946, 50946, 516, 281, 643, 281, 1319, 552, 709, 382, 938, 382, 291, 434, 1237, 412, 411, 512, 733, 51116, 51116, 295, 3303, 5787, 558, 291, 434, 516, 281, 643, 4691, 46866, 293, 16235, 51290, 51290, 46866, 13, 407, 437, 321, 360, 294, 294, 264, 2489, 12, 83, 37726, 1399, 307, 456, 311, 767, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.10938546898659696, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.00010070203279610723}, {"id": 145, "seek": 78040, "start": 795.4399999999999, "end": 798.92, "text": " of natural photos right you're going to need edge detectors and gradient", "tokens": [50364, 544, 2685, 281, 264, 3097, 5633, 597, 307, 411, 767, 32884, 534, 437, 50710, 50710, 437, 307, 309, 300, 321, 434, 1237, 412, 13, 440, 2440, 7914, 1238, 17518, 291, 434, 50946, 50946, 516, 281, 643, 281, 1319, 552, 709, 382, 938, 382, 291, 434, 1237, 412, 411, 512, 733, 51116, 51116, 295, 3303, 5787, 558, 291, 434, 516, 281, 643, 4691, 46866, 293, 16235, 51290, 51290, 46866, 13, 407, 437, 321, 360, 294, 294, 264, 2489, 12, 83, 37726, 1399, 307, 456, 311, 767, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.10938546898659696, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.00010070203279610723}, {"id": 146, "seek": 78040, "start": 798.92, "end": 807.84, "text": " detectors. So what we do in in the fine-tuning process is there's actually", "tokens": [50364, 544, 2685, 281, 264, 3097, 5633, 597, 307, 411, 767, 32884, 534, 437, 50710, 50710, 437, 307, 309, 300, 321, 434, 1237, 412, 13, 440, 2440, 7914, 1238, 17518, 291, 434, 50946, 50946, 516, 281, 643, 281, 1319, 552, 709, 382, 938, 382, 291, 434, 1237, 412, 411, 512, 733, 51116, 51116, 295, 3303, 5787, 558, 291, 434, 516, 281, 643, 4691, 46866, 293, 16235, 51290, 51290, 46866, 13, 407, 437, 321, 360, 294, 294, 264, 2489, 12, 83, 37726, 1399, 307, 456, 311, 767, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.10938546898659696, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.00010070203279610723}, {"id": 147, "seek": 80784, "start": 807.84, "end": 811.12, "text": " one extra layer after this which is the layer that actually says what is this", "tokens": [50364, 472, 2857, 4583, 934, 341, 597, 307, 264, 4583, 300, 767, 1619, 437, 307, 341, 50528, 50528, 291, 458, 309, 311, 309, 311, 257, 3000, 420, 257, 3857, 420, 2035, 13, 492, 767, 12097, 300, 321, 50736, 50736, 3507, 309, 1314, 13, 407, 586, 300, 1036, 8141, 12972, 575, 472, 5598, 420, 472, 5598, 51088, 51088, 680, 7719, 291, 434, 32884, 321, 3507, 300, 1314, 370, 264, 2316, 586, 575, 300, 264, 51342, 51342, 1036, 8141, 300, 311, 637, 2414, 484, 291, 458, 309, 5946, 457, 5101, 257, 1326, 3262, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.13320788931339345, "compression_ratio": 1.8215962441314555, "no_speech_prob": 3.071516039199196e-05}, {"id": 148, "seek": 80784, "start": 811.12, "end": 815.2800000000001, "text": " you know it's it's a dog or a cat or whatever. We actually delete that we", "tokens": [50364, 472, 2857, 4583, 934, 341, 597, 307, 264, 4583, 300, 767, 1619, 437, 307, 341, 50528, 50528, 291, 458, 309, 311, 309, 311, 257, 3000, 420, 257, 3857, 420, 2035, 13, 492, 767, 12097, 300, 321, 50736, 50736, 3507, 309, 1314, 13, 407, 586, 300, 1036, 8141, 12972, 575, 472, 5598, 420, 472, 5598, 51088, 51088, 680, 7719, 291, 434, 32884, 321, 3507, 300, 1314, 370, 264, 2316, 586, 575, 300, 264, 51342, 51342, 1036, 8141, 300, 311, 637, 2414, 484, 291, 458, 309, 5946, 457, 5101, 257, 1326, 3262, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.13320788931339345, "compression_ratio": 1.8215962441314555, "no_speech_prob": 3.071516039199196e-05}, {"id": 149, "seek": 80784, "start": 815.2800000000001, "end": 822.32, "text": " throw it away. So now that last matrix multiply has one output or one output", "tokens": [50364, 472, 2857, 4583, 934, 341, 597, 307, 264, 4583, 300, 767, 1619, 437, 307, 341, 50528, 50528, 291, 458, 309, 311, 309, 311, 257, 3000, 420, 257, 3857, 420, 2035, 13, 492, 767, 12097, 300, 321, 50736, 50736, 3507, 309, 1314, 13, 407, 586, 300, 1036, 8141, 12972, 575, 472, 5598, 420, 472, 5598, 51088, 51088, 680, 7719, 291, 434, 32884, 321, 3507, 300, 1314, 370, 264, 2316, 586, 575, 300, 264, 51342, 51342, 1036, 8141, 300, 311, 637, 2414, 484, 291, 458, 309, 5946, 457, 5101, 257, 1326, 3262, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.13320788931339345, "compression_ratio": 1.8215962441314555, "no_speech_prob": 3.071516039199196e-05}, {"id": 150, "seek": 80784, "start": 822.32, "end": 827.4, "text": " per category you're predicting we throw that away so the model now has that the", "tokens": [50364, 472, 2857, 4583, 934, 341, 597, 307, 264, 4583, 300, 767, 1619, 437, 307, 341, 50528, 50528, 291, 458, 309, 311, 309, 311, 257, 3000, 420, 257, 3857, 420, 2035, 13, 492, 767, 12097, 300, 321, 50736, 50736, 3507, 309, 1314, 13, 407, 586, 300, 1036, 8141, 12972, 575, 472, 5598, 420, 472, 5598, 51088, 51088, 680, 7719, 291, 434, 32884, 321, 3507, 300, 1314, 370, 264, 2316, 586, 575, 300, 264, 51342, 51342, 1036, 8141, 300, 311, 637, 2414, 484, 291, 458, 309, 5946, 457, 5101, 257, 1326, 3262, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.13320788931339345, "compression_ratio": 1.8215962441314555, "no_speech_prob": 3.071516039199196e-05}, {"id": 151, "seek": 80784, "start": 827.4, "end": 832.76, "text": " last matrix that's spitting out you know it depends but generally a few hundred", "tokens": [50364, 472, 2857, 4583, 934, 341, 597, 307, 264, 4583, 300, 767, 1619, 437, 307, 341, 50528, 50528, 291, 458, 309, 311, 309, 311, 257, 3000, 420, 257, 3857, 420, 2035, 13, 492, 767, 12097, 300, 321, 50736, 50736, 3507, 309, 1314, 13, 407, 586, 300, 1036, 8141, 12972, 575, 472, 5598, 420, 472, 5598, 51088, 51088, 680, 7719, 291, 434, 32884, 321, 3507, 300, 1314, 370, 264, 2316, 586, 575, 300, 264, 51342, 51342, 1036, 8141, 300, 311, 637, 2414, 484, 291, 458, 309, 5946, 457, 5101, 257, 1326, 3262, 51610, 51610], "temperature": 0.0, "avg_logprob": -0.13320788931339345, "compression_ratio": 1.8215962441314555, "no_speech_prob": 3.071516039199196e-05}, {"id": 152, "seek": 83276, "start": 832.76, "end": 838.88, "text": " activations. And what we do is as we'll learn more shortly in the coming lesson", "tokens": [50364, 2430, 763, 13, 400, 437, 321, 360, 307, 382, 321, 603, 1466, 544, 13392, 294, 264, 1348, 6898, 50670, 50670, 321, 321, 445, 2897, 257, 777, 4974, 8141, 322, 264, 917, 295, 300, 293, 300, 311, 437, 321, 50964, 50964, 9105, 3847, 370, 309, 27152, 281, 764, 613, 3685, 295, 4122, 281, 6069, 51348, 51348, 2035, 309, 307, 291, 434, 1382, 281, 6069, 293, 550, 321, 13145, 3847, 439, 295, 729, 51576, 51576, 7914, 13, 407, 300, 311, 1936, 577, 309, 311, 1096, 293, 370, 309, 311, 257, 857, 1011, 261, 15498, 457, 321, 603, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.105332066314389, "compression_ratio": 1.7004405286343611, "no_speech_prob": 4.610777250491083e-05}, {"id": 153, "seek": 83276, "start": 838.88, "end": 844.76, "text": " we we just stick a new random matrix on the end of that and that's what we", "tokens": [50364, 2430, 763, 13, 400, 437, 321, 360, 307, 382, 321, 603, 1466, 544, 13392, 294, 264, 1348, 6898, 50670, 50670, 321, 321, 445, 2897, 257, 777, 4974, 8141, 322, 264, 917, 295, 300, 293, 300, 311, 437, 321, 50964, 50964, 9105, 3847, 370, 309, 27152, 281, 764, 613, 3685, 295, 4122, 281, 6069, 51348, 51348, 2035, 309, 307, 291, 434, 1382, 281, 6069, 293, 550, 321, 13145, 3847, 439, 295, 729, 51576, 51576, 7914, 13, 407, 300, 311, 1936, 577, 309, 311, 1096, 293, 370, 309, 311, 257, 857, 1011, 261, 15498, 457, 321, 603, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.105332066314389, "compression_ratio": 1.7004405286343611, "no_speech_prob": 4.610777250491083e-05}, {"id": 154, "seek": 83276, "start": 844.76, "end": 852.4399999999999, "text": " initially train so it learns to use these kinds of features to predict", "tokens": [50364, 2430, 763, 13, 400, 437, 321, 360, 307, 382, 321, 603, 1466, 544, 13392, 294, 264, 1348, 6898, 50670, 50670, 321, 321, 445, 2897, 257, 777, 4974, 8141, 322, 264, 917, 295, 300, 293, 300, 311, 437, 321, 50964, 50964, 9105, 3847, 370, 309, 27152, 281, 764, 613, 3685, 295, 4122, 281, 6069, 51348, 51348, 2035, 309, 307, 291, 434, 1382, 281, 6069, 293, 550, 321, 13145, 3847, 439, 295, 729, 51576, 51576, 7914, 13, 407, 300, 311, 1936, 577, 309, 311, 1096, 293, 370, 309, 311, 257, 857, 1011, 261, 15498, 457, 321, 603, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.105332066314389, "compression_ratio": 1.7004405286343611, "no_speech_prob": 4.610777250491083e-05}, {"id": 155, "seek": 83276, "start": 852.4399999999999, "end": 857.0, "text": " whatever it is you're trying to predict and then we gradually train all of those", "tokens": [50364, 2430, 763, 13, 400, 437, 321, 360, 307, 382, 321, 603, 1466, 544, 13392, 294, 264, 1348, 6898, 50670, 50670, 321, 321, 445, 2897, 257, 777, 4974, 8141, 322, 264, 917, 295, 300, 293, 300, 311, 437, 321, 50964, 50964, 9105, 3847, 370, 309, 27152, 281, 764, 613, 3685, 295, 4122, 281, 6069, 51348, 51348, 2035, 309, 307, 291, 434, 1382, 281, 6069, 293, 550, 321, 13145, 3847, 439, 295, 729, 51576, 51576, 7914, 13, 407, 300, 311, 1936, 577, 309, 311, 1096, 293, 370, 309, 311, 257, 857, 1011, 261, 15498, 457, 321, 603, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.105332066314389, "compression_ratio": 1.7004405286343611, "no_speech_prob": 4.610777250491083e-05}, {"id": 156, "seek": 83276, "start": 857.0, "end": 861.88, "text": " layers. So that's basically how it's done and so it's a bit hand wavy but we'll", "tokens": [50364, 2430, 763, 13, 400, 437, 321, 360, 307, 382, 321, 603, 1466, 544, 13392, 294, 264, 1348, 6898, 50670, 50670, 321, 321, 445, 2897, 257, 777, 4974, 8141, 322, 264, 917, 295, 300, 293, 300, 311, 437, 321, 50964, 50964, 9105, 3847, 370, 309, 27152, 281, 764, 613, 3685, 295, 4122, 281, 6069, 51348, 51348, 2035, 309, 307, 291, 434, 1382, 281, 6069, 293, 550, 321, 13145, 3847, 439, 295, 729, 51576, 51576, 7914, 13, 407, 300, 311, 1936, 577, 309, 311, 1096, 293, 370, 309, 311, 257, 857, 1011, 261, 15498, 457, 321, 603, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.105332066314389, "compression_ratio": 1.7004405286343611, "no_speech_prob": 4.610777250491083e-05}, {"id": 157, "seek": 86188, "start": 861.88, "end": 868.88, "text": " particularly in part two actually build that from scratch ourselves and in fact", "tokens": [50364, 4098, 294, 644, 732, 767, 1322, 300, 490, 8459, 4175, 293, 294, 1186, 50714, 50714, 294, 341, 6898, 565, 4784, 2414, 321, 434, 767, 516, 281, 722, 516, 760, 264, 50842, 50842, 1399, 295, 767, 2390, 257, 957, 12, 13217, 2452, 18161, 2533, 294, 15329, 370, 51148, 51148, 321, 603, 312, 2891, 281, 767, 652, 512, 4205, 3030, 300, 3387, 13, 1033, 370, 718, 311, 51520, 51520, 3012, 666, 264, 21060, 13, 407, 321, 434, 516, 281, 574, 412, 257, 48751, 22631, 6211, 300, 311, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.11501393693216731, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0001123276015277952}, {"id": 158, "seek": 86188, "start": 868.88, "end": 871.4399999999999, "text": " in this lesson time permitting we're actually going to start going down the", "tokens": [50364, 4098, 294, 644, 732, 767, 1322, 300, 490, 8459, 4175, 293, 294, 1186, 50714, 50714, 294, 341, 6898, 565, 4784, 2414, 321, 434, 767, 516, 281, 722, 516, 760, 264, 50842, 50842, 1399, 295, 767, 2390, 257, 957, 12, 13217, 2452, 18161, 2533, 294, 15329, 370, 51148, 51148, 321, 603, 312, 2891, 281, 767, 652, 512, 4205, 3030, 300, 3387, 13, 1033, 370, 718, 311, 51520, 51520, 3012, 666, 264, 21060, 13, 407, 321, 434, 516, 281, 574, 412, 257, 48751, 22631, 6211, 300, 311, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.11501393693216731, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0001123276015277952}, {"id": 159, "seek": 86188, "start": 871.4399999999999, "end": 877.56, "text": " process of actually building a real-world deep neural net in Python so", "tokens": [50364, 4098, 294, 644, 732, 767, 1322, 300, 490, 8459, 4175, 293, 294, 1186, 50714, 50714, 294, 341, 6898, 565, 4784, 2414, 321, 434, 767, 516, 281, 722, 516, 760, 264, 50842, 50842, 1399, 295, 767, 2390, 257, 957, 12, 13217, 2452, 18161, 2533, 294, 15329, 370, 51148, 51148, 321, 603, 312, 2891, 281, 767, 652, 512, 4205, 3030, 300, 3387, 13, 1033, 370, 718, 311, 51520, 51520, 3012, 666, 264, 21060, 13, 407, 321, 434, 516, 281, 574, 412, 257, 48751, 22631, 6211, 300, 311, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.11501393693216731, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0001123276015277952}, {"id": 160, "seek": 86188, "start": 877.56, "end": 885.0, "text": " we'll be starting to actually make some progress towards that goal. Okay so let's", "tokens": [50364, 4098, 294, 644, 732, 767, 1322, 300, 490, 8459, 4175, 293, 294, 1186, 50714, 50714, 294, 341, 6898, 565, 4784, 2414, 321, 434, 767, 516, 281, 722, 516, 760, 264, 50842, 50842, 1399, 295, 767, 2390, 257, 957, 12, 13217, 2452, 18161, 2533, 294, 15329, 370, 51148, 51148, 321, 603, 312, 2891, 281, 767, 652, 512, 4205, 3030, 300, 3387, 13, 1033, 370, 718, 311, 51520, 51520, 3012, 666, 264, 21060, 13, 407, 321, 434, 516, 281, 574, 412, 257, 48751, 22631, 6211, 300, 311, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.11501393693216731, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0001123276015277952}, {"id": 161, "seek": 86188, "start": 885.0, "end": 889.72, "text": " jump into the notebook. So we're going to look at a Kaggle competition that's", "tokens": [50364, 4098, 294, 644, 732, 767, 1322, 300, 490, 8459, 4175, 293, 294, 1186, 50714, 50714, 294, 341, 6898, 565, 4784, 2414, 321, 434, 767, 516, 281, 722, 516, 760, 264, 50842, 50842, 1399, 295, 767, 2390, 257, 957, 12, 13217, 2452, 18161, 2533, 294, 15329, 370, 51148, 51148, 321, 603, 312, 2891, 281, 767, 652, 512, 4205, 3030, 300, 3387, 13, 1033, 370, 718, 311, 51520, 51520, 3012, 666, 264, 21060, 13, 407, 321, 434, 516, 281, 574, 412, 257, 48751, 22631, 6211, 300, 311, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.11501393693216731, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0001123276015277952}, {"id": 162, "seek": 88972, "start": 889.72, "end": 896.48, "text": " actually on as I as I speak and I created this notebook called Getting", "tokens": [50364, 767, 322, 382, 286, 382, 286, 1710, 293, 286, 2942, 341, 21060, 1219, 13674, 50702, 50702, 39715, 365, 426, 45196, 337, 43965, 1169, 20660, 2999, 293, 370, 264, 6211, 307, 1219, 264, 2546, 50978, 50978, 4379, 317, 430, 1703, 651, 281, 430, 1703, 651, 26178, 278, 43634, 293, 370, 286, 478, 516, 281, 747, 51346, 51346, 291, 807, 291, 458, 257, 3566, 23689, 281, 341, 6211, 293, 48751, 22631, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.13750955503280848, "compression_ratio": 1.5473684210526315, "no_speech_prob": 4.985099076293409e-05}, {"id": 163, "seek": 88972, "start": 896.48, "end": 902.0, "text": " Started with NLP for Absolute Beginners and so the competition is called the US", "tokens": [50364, 767, 322, 382, 286, 382, 286, 1710, 293, 286, 2942, 341, 21060, 1219, 13674, 50702, 50702, 39715, 365, 426, 45196, 337, 43965, 1169, 20660, 2999, 293, 370, 264, 6211, 307, 1219, 264, 2546, 50978, 50978, 4379, 317, 430, 1703, 651, 281, 430, 1703, 651, 26178, 278, 43634, 293, 370, 286, 478, 516, 281, 747, 51346, 51346, 291, 807, 291, 458, 257, 3566, 23689, 281, 341, 6211, 293, 48751, 22631, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.13750955503280848, "compression_ratio": 1.5473684210526315, "no_speech_prob": 4.985099076293409e-05}, {"id": 164, "seek": 88972, "start": 902.0, "end": 909.36, "text": " Patent Phrase to Phrase Matching Competition and so I'm going to take", "tokens": [50364, 767, 322, 382, 286, 382, 286, 1710, 293, 286, 2942, 341, 21060, 1219, 13674, 50702, 50702, 39715, 365, 426, 45196, 337, 43965, 1169, 20660, 2999, 293, 370, 264, 6211, 307, 1219, 264, 2546, 50978, 50978, 4379, 317, 430, 1703, 651, 281, 430, 1703, 651, 26178, 278, 43634, 293, 370, 286, 478, 516, 281, 747, 51346, 51346, 291, 807, 291, 458, 257, 3566, 23689, 281, 341, 6211, 293, 48751, 22631, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.13750955503280848, "compression_ratio": 1.5473684210526315, "no_speech_prob": 4.985099076293409e-05}, {"id": 165, "seek": 88972, "start": 909.36, "end": 916.9200000000001, "text": " you through you know a complete submission to this competition and Kaggle", "tokens": [50364, 767, 322, 382, 286, 382, 286, 1710, 293, 286, 2942, 341, 21060, 1219, 13674, 50702, 50702, 39715, 365, 426, 45196, 337, 43965, 1169, 20660, 2999, 293, 370, 264, 6211, 307, 1219, 264, 2546, 50978, 50978, 4379, 317, 430, 1703, 651, 281, 430, 1703, 651, 26178, 278, 43634, 293, 370, 286, 478, 516, 281, 747, 51346, 51346, 291, 807, 291, 458, 257, 3566, 23689, 281, 341, 6211, 293, 48751, 22631, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.13750955503280848, "compression_ratio": 1.5473684210526315, "no_speech_prob": 4.985099076293409e-05}, {"id": 166, "seek": 91692, "start": 916.92, "end": 919.9599999999999, "text": " competitions are interesting particularly the ones that are not playground", "tokens": [50364, 26185, 366, 1880, 4098, 264, 2306, 300, 366, 406, 24646, 50516, 50516, 26185, 457, 264, 957, 26185, 365, 957, 1460, 6456, 436, 434, 50678, 50678, 1880, 570, 341, 307, 364, 3539, 1716, 300, 364, 3539, 4475, 307, 50952, 50952, 4927, 281, 1963, 1460, 294, 1242, 13041, 1228, 641, 3539, 1412, 13, 407, 257, 688, 51260, 51260, 295, 561, 366, 257, 857, 16974, 488, 295, 48751, 22631, 26185, 382, 885, 733, 295, 411, 406, 51504, 51504, 588, 957, 293, 309, 311, 3297, 2074, 291, 434, 406, 18788, 466, 1507, 411, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.10847842308782762, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.0001633364154258743}, {"id": 167, "seek": 91692, "start": 919.9599999999999, "end": 923.1999999999999, "text": " competitions but the real competitions with real money applied they're", "tokens": [50364, 26185, 366, 1880, 4098, 264, 2306, 300, 366, 406, 24646, 50516, 50516, 26185, 457, 264, 957, 26185, 365, 957, 1460, 6456, 436, 434, 50678, 50678, 1880, 570, 341, 307, 364, 3539, 1716, 300, 364, 3539, 4475, 307, 50952, 50952, 4927, 281, 1963, 1460, 294, 1242, 13041, 1228, 641, 3539, 1412, 13, 407, 257, 688, 51260, 51260, 295, 561, 366, 257, 857, 16974, 488, 295, 48751, 22631, 26185, 382, 885, 733, 295, 411, 406, 51504, 51504, 588, 957, 293, 309, 311, 3297, 2074, 291, 434, 406, 18788, 466, 1507, 411, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.10847842308782762, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.0001633364154258743}, {"id": 168, "seek": 91692, "start": 923.1999999999999, "end": 928.68, "text": " interesting because this is an actual project that an actual organization is", "tokens": [50364, 26185, 366, 1880, 4098, 264, 2306, 300, 366, 406, 24646, 50516, 50516, 26185, 457, 264, 957, 26185, 365, 957, 1460, 6456, 436, 434, 50678, 50678, 1880, 570, 341, 307, 364, 3539, 1716, 300, 364, 3539, 4475, 307, 50952, 50952, 4927, 281, 1963, 1460, 294, 1242, 13041, 1228, 641, 3539, 1412, 13, 407, 257, 688, 51260, 51260, 295, 561, 366, 257, 857, 16974, 488, 295, 48751, 22631, 26185, 382, 885, 733, 295, 411, 406, 51504, 51504, 588, 957, 293, 309, 311, 3297, 2074, 291, 434, 406, 18788, 466, 1507, 411, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.10847842308782762, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.0001633364154258743}, {"id": 169, "seek": 91692, "start": 928.68, "end": 934.8399999999999, "text": " prepared to invest money in getting solved using their actual data. So a lot", "tokens": [50364, 26185, 366, 1880, 4098, 264, 2306, 300, 366, 406, 24646, 50516, 50516, 26185, 457, 264, 957, 26185, 365, 957, 1460, 6456, 436, 434, 50678, 50678, 1880, 570, 341, 307, 364, 3539, 1716, 300, 364, 3539, 4475, 307, 50952, 50952, 4927, 281, 1963, 1460, 294, 1242, 13041, 1228, 641, 3539, 1412, 13, 407, 257, 688, 51260, 51260, 295, 561, 366, 257, 857, 16974, 488, 295, 48751, 22631, 26185, 382, 885, 733, 295, 411, 406, 51504, 51504, 588, 957, 293, 309, 311, 3297, 2074, 291, 434, 406, 18788, 466, 1507, 411, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.10847842308782762, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.0001633364154258743}, {"id": 170, "seek": 91692, "start": 934.8399999999999, "end": 939.7199999999999, "text": " of people are a bit dismissive of Kaggle competitions as being kind of like not", "tokens": [50364, 26185, 366, 1880, 4098, 264, 2306, 300, 366, 406, 24646, 50516, 50516, 26185, 457, 264, 957, 26185, 365, 957, 1460, 6456, 436, 434, 50678, 50678, 1880, 570, 341, 307, 364, 3539, 1716, 300, 364, 3539, 4475, 307, 50952, 50952, 4927, 281, 1963, 1460, 294, 1242, 13041, 1228, 641, 3539, 1412, 13, 407, 257, 688, 51260, 51260, 295, 561, 366, 257, 857, 16974, 488, 295, 48751, 22631, 26185, 382, 885, 733, 295, 411, 406, 51504, 51504, 588, 957, 293, 309, 311, 3297, 2074, 291, 434, 406, 18788, 466, 1507, 411, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.10847842308782762, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.0001633364154258743}, {"id": 171, "seek": 91692, "start": 939.7199999999999, "end": 942.3199999999999, "text": " very real and it's certainly true you're not worrying about stuff like", "tokens": [50364, 26185, 366, 1880, 4098, 264, 2306, 300, 366, 406, 24646, 50516, 50516, 26185, 457, 264, 957, 26185, 365, 957, 1460, 6456, 436, 434, 50678, 50678, 1880, 570, 341, 307, 364, 3539, 1716, 300, 364, 3539, 4475, 307, 50952, 50952, 4927, 281, 1963, 1460, 294, 1242, 13041, 1228, 641, 3539, 1412, 13, 407, 257, 688, 51260, 51260, 295, 561, 366, 257, 857, 16974, 488, 295, 48751, 22631, 26185, 382, 885, 733, 295, 411, 406, 51504, 51504, 588, 957, 293, 309, 311, 3297, 2074, 291, 434, 406, 18788, 466, 1507, 411, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.10847842308782762, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.0001633364154258743}, {"id": 172, "seek": 94232, "start": 942.32, "end": 947.96, "text": " productionizing the model but you know in terms of like getting real data about", "tokens": [50364, 4265, 3319, 264, 2316, 457, 291, 458, 294, 2115, 295, 411, 1242, 957, 1412, 466, 50646, 50646, 257, 957, 1154, 300, 957, 6150, 534, 1127, 466, 293, 257, 588, 2047, 636, 50872, 50872, 281, 3481, 264, 291, 458, 14170, 295, 428, 3827, 291, 393, 380, 534, 483, 1101, 51066, 51066, 813, 341, 13, 407, 341, 307, 257, 665, 1081, 257, 665, 6211, 281, 281, 5120, 365, 51328, 51328, 337, 1382, 426, 45196, 13, 823, 382, 286, 2835, 510, 1391, 264, 881, 13371, 4420, 51586, 51586, 3861, 337, 426, 45196, 307, 21538, 293, 382, 321, 600, 7152, 294, 3820, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1071168675142176, "compression_ratio": 1.673992673992674, "no_speech_prob": 3.0238257750170305e-05}, {"id": 173, "seek": 94232, "start": 947.96, "end": 952.48, "text": " a real problem that real organizations really care about and a very direct way", "tokens": [50364, 4265, 3319, 264, 2316, 457, 291, 458, 294, 2115, 295, 411, 1242, 957, 1412, 466, 50646, 50646, 257, 957, 1154, 300, 957, 6150, 534, 1127, 466, 293, 257, 588, 2047, 636, 50872, 50872, 281, 3481, 264, 291, 458, 14170, 295, 428, 3827, 291, 393, 380, 534, 483, 1101, 51066, 51066, 813, 341, 13, 407, 341, 307, 257, 665, 1081, 257, 665, 6211, 281, 281, 5120, 365, 51328, 51328, 337, 1382, 426, 45196, 13, 823, 382, 286, 2835, 510, 1391, 264, 881, 13371, 4420, 51586, 51586, 3861, 337, 426, 45196, 307, 21538, 293, 382, 321, 600, 7152, 294, 3820, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1071168675142176, "compression_ratio": 1.673992673992674, "no_speech_prob": 3.0238257750170305e-05}, {"id": 174, "seek": 94232, "start": 952.48, "end": 956.36, "text": " to measure the you know accuracy of your solution you can't really get better", "tokens": [50364, 4265, 3319, 264, 2316, 457, 291, 458, 294, 2115, 295, 411, 1242, 957, 1412, 466, 50646, 50646, 257, 957, 1154, 300, 957, 6150, 534, 1127, 466, 293, 257, 588, 2047, 636, 50872, 50872, 281, 3481, 264, 291, 458, 14170, 295, 428, 3827, 291, 393, 380, 534, 483, 1101, 51066, 51066, 813, 341, 13, 407, 341, 307, 257, 665, 1081, 257, 665, 6211, 281, 281, 5120, 365, 51328, 51328, 337, 1382, 426, 45196, 13, 823, 382, 286, 2835, 510, 1391, 264, 881, 13371, 4420, 51586, 51586, 3861, 337, 426, 45196, 307, 21538, 293, 382, 321, 600, 7152, 294, 3820, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1071168675142176, "compression_ratio": 1.673992673992674, "no_speech_prob": 3.0238257750170305e-05}, {"id": 175, "seek": 94232, "start": 956.36, "end": 961.6, "text": " than this. So this is a good place a good competition to to experiment with", "tokens": [50364, 4265, 3319, 264, 2316, 457, 291, 458, 294, 2115, 295, 411, 1242, 957, 1412, 466, 50646, 50646, 257, 957, 1154, 300, 957, 6150, 534, 1127, 466, 293, 257, 588, 2047, 636, 50872, 50872, 281, 3481, 264, 291, 458, 14170, 295, 428, 3827, 291, 393, 380, 534, 483, 1101, 51066, 51066, 813, 341, 13, 407, 341, 307, 257, 665, 1081, 257, 665, 6211, 281, 281, 5120, 365, 51328, 51328, 337, 1382, 426, 45196, 13, 823, 382, 286, 2835, 510, 1391, 264, 881, 13371, 4420, 51586, 51586, 3861, 337, 426, 45196, 307, 21538, 293, 382, 321, 600, 7152, 294, 3820, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1071168675142176, "compression_ratio": 1.673992673992674, "no_speech_prob": 3.0238257750170305e-05}, {"id": 176, "seek": 94232, "start": 961.6, "end": 966.7600000000001, "text": " for trying NLP. Now as I mentioned here probably the most widely useful", "tokens": [50364, 4265, 3319, 264, 2316, 457, 291, 458, 294, 2115, 295, 411, 1242, 957, 1412, 466, 50646, 50646, 257, 957, 1154, 300, 957, 6150, 534, 1127, 466, 293, 257, 588, 2047, 636, 50872, 50872, 281, 3481, 264, 291, 458, 14170, 295, 428, 3827, 291, 393, 380, 534, 483, 1101, 51066, 51066, 813, 341, 13, 407, 341, 307, 257, 665, 1081, 257, 665, 6211, 281, 281, 5120, 365, 51328, 51328, 337, 1382, 426, 45196, 13, 823, 382, 286, 2835, 510, 1391, 264, 881, 13371, 4420, 51586, 51586, 3861, 337, 426, 45196, 307, 21538, 293, 382, 321, 600, 7152, 294, 3820, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1071168675142176, "compression_ratio": 1.673992673992674, "no_speech_prob": 3.0238257750170305e-05}, {"id": 177, "seek": 94232, "start": 966.7600000000001, "end": 972.1600000000001, "text": " application for NLP is classification and as we've discussed in computer", "tokens": [50364, 4265, 3319, 264, 2316, 457, 291, 458, 294, 2115, 295, 411, 1242, 957, 1412, 466, 50646, 50646, 257, 957, 1154, 300, 957, 6150, 534, 1127, 466, 293, 257, 588, 2047, 636, 50872, 50872, 281, 3481, 264, 291, 458, 14170, 295, 428, 3827, 291, 393, 380, 534, 483, 1101, 51066, 51066, 813, 341, 13, 407, 341, 307, 257, 665, 1081, 257, 665, 6211, 281, 281, 5120, 365, 51328, 51328, 337, 1382, 426, 45196, 13, 823, 382, 286, 2835, 510, 1391, 264, 881, 13371, 4420, 51586, 51586, 3861, 337, 426, 45196, 307, 21538, 293, 382, 321, 600, 7152, 294, 3820, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.1071168675142176, "compression_ratio": 1.673992673992674, "no_speech_prob": 3.0238257750170305e-05}, {"id": 178, "seek": 97216, "start": 972.16, "end": 976.56, "text": " vision classification refers to taking an object and trying to identify a", "tokens": [50364, 5201, 21538, 14942, 281, 1940, 364, 2657, 293, 1382, 281, 5876, 257, 50584, 50584, 7719, 300, 2657, 12953, 281, 13, 407, 8046, 321, 600, 8704, 668, 1237, 412, 50860, 50860, 5267, 965, 321, 434, 516, 281, 312, 1237, 412, 8512, 13, 823, 294, 426, 45196, 562, 321, 584, 51162, 51162, 4166, 321, 500, 380, 4682, 914, 291, 458, 257, 945, 3028, 938, 291, 458, 16238, 257, 51560, 51560, 4166, 727, 312, 1045, 420, 1451, 2283, 420, 257, 4166, 727, 312, 264, 2302, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.1038505421128384, "compression_ratio": 1.7077625570776256, "no_speech_prob": 8.346622053068131e-05}, {"id": 179, "seek": 97216, "start": 976.56, "end": 982.0799999999999, "text": " category that object belongs to. So previously we've mainly been looking at", "tokens": [50364, 5201, 21538, 14942, 281, 1940, 364, 2657, 293, 1382, 281, 5876, 257, 50584, 50584, 7719, 300, 2657, 12953, 281, 13, 407, 8046, 321, 600, 8704, 668, 1237, 412, 50860, 50860, 5267, 965, 321, 434, 516, 281, 312, 1237, 412, 8512, 13, 823, 294, 426, 45196, 562, 321, 584, 51162, 51162, 4166, 321, 500, 380, 4682, 914, 291, 458, 257, 945, 3028, 938, 291, 458, 16238, 257, 51560, 51560, 4166, 727, 312, 1045, 420, 1451, 2283, 420, 257, 4166, 727, 312, 264, 2302, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.1038505421128384, "compression_ratio": 1.7077625570776256, "no_speech_prob": 8.346622053068131e-05}, {"id": 180, "seek": 97216, "start": 982.0799999999999, "end": 988.12, "text": " images today we're going to be looking at documents. Now in NLP when we say", "tokens": [50364, 5201, 21538, 14942, 281, 1940, 364, 2657, 293, 1382, 281, 5876, 257, 50584, 50584, 7719, 300, 2657, 12953, 281, 13, 407, 8046, 321, 600, 8704, 668, 1237, 412, 50860, 50860, 5267, 965, 321, 434, 516, 281, 312, 1237, 412, 8512, 13, 823, 294, 426, 45196, 562, 321, 584, 51162, 51162, 4166, 321, 500, 380, 4682, 914, 291, 458, 257, 945, 3028, 938, 291, 458, 16238, 257, 51560, 51560, 4166, 727, 312, 1045, 420, 1451, 2283, 420, 257, 4166, 727, 312, 264, 2302, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.1038505421128384, "compression_ratio": 1.7077625570776256, "no_speech_prob": 8.346622053068131e-05}, {"id": 181, "seek": 97216, "start": 988.12, "end": 996.0799999999999, "text": " document we don't specifically mean you know a 20 page long you know essay a", "tokens": [50364, 5201, 21538, 14942, 281, 1940, 364, 2657, 293, 1382, 281, 5876, 257, 50584, 50584, 7719, 300, 2657, 12953, 281, 13, 407, 8046, 321, 600, 8704, 668, 1237, 412, 50860, 50860, 5267, 965, 321, 434, 516, 281, 312, 1237, 412, 8512, 13, 823, 294, 426, 45196, 562, 321, 584, 51162, 51162, 4166, 321, 500, 380, 4682, 914, 291, 458, 257, 945, 3028, 938, 291, 458, 16238, 257, 51560, 51560, 4166, 727, 312, 1045, 420, 1451, 2283, 420, 257, 4166, 727, 312, 264, 2302, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.1038505421128384, "compression_ratio": 1.7077625570776256, "no_speech_prob": 8.346622053068131e-05}, {"id": 182, "seek": 97216, "start": 996.0799999999999, "end": 1000.3199999999999, "text": " document could be three or four words or a document could be the entire", "tokens": [50364, 5201, 21538, 14942, 281, 1940, 364, 2657, 293, 1382, 281, 5876, 257, 50584, 50584, 7719, 300, 2657, 12953, 281, 13, 407, 8046, 321, 600, 8704, 668, 1237, 412, 50860, 50860, 5267, 965, 321, 434, 516, 281, 312, 1237, 412, 8512, 13, 823, 294, 426, 45196, 562, 321, 584, 51162, 51162, 4166, 321, 500, 380, 4682, 914, 291, 458, 257, 945, 3028, 938, 291, 458, 16238, 257, 51560, 51560, 4166, 727, 312, 1045, 420, 1451, 2283, 420, 257, 4166, 727, 312, 264, 2302, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.1038505421128384, "compression_ratio": 1.7077625570776256, "no_speech_prob": 8.346622053068131e-05}, {"id": 183, "seek": 100032, "start": 1000.32, "end": 1009.44, "text": " exaclopedia. So a document is just an input to an NLP model that contains text.", "tokens": [50364, 454, 326, 75, 47795, 13, 407, 257, 4166, 307, 445, 364, 4846, 281, 364, 426, 45196, 2316, 300, 8306, 2487, 13, 50820, 50820, 823, 1508, 5489, 257, 4166, 370, 17990, 437, 7719, 257, 4166, 12953, 281, 307, 257, 51208, 51208, 17600, 4593, 551, 281, 360, 456, 311, 439, 3685, 295, 1507, 291, 727, 360, 365, 51480, 51480, 300, 370, 337, 1365, 321, 600, 1217, 2835, 16149, 5215, 300, 311, 257, 51642, 51642, 3857, 300, 311, 257, 21538, 5633, 321, 853, 281, 4536, 322, 264, 7719, 3353, 420, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.161086176777934, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.00011059915414080024}, {"id": 184, "seek": 100032, "start": 1009.44, "end": 1017.2, "text": " Now classifying a document so deciding what category a document belongs to is a", "tokens": [50364, 454, 326, 75, 47795, 13, 407, 257, 4166, 307, 445, 364, 4846, 281, 364, 426, 45196, 2316, 300, 8306, 2487, 13, 50820, 50820, 823, 1508, 5489, 257, 4166, 370, 17990, 437, 7719, 257, 4166, 12953, 281, 307, 257, 51208, 51208, 17600, 4593, 551, 281, 360, 456, 311, 439, 3685, 295, 1507, 291, 727, 360, 365, 51480, 51480, 300, 370, 337, 1365, 321, 600, 1217, 2835, 16149, 5215, 300, 311, 257, 51642, 51642, 3857, 300, 311, 257, 21538, 5633, 321, 853, 281, 4536, 322, 264, 7719, 3353, 420, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.161086176777934, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.00011059915414080024}, {"id": 185, "seek": 100032, "start": 1017.2, "end": 1022.6400000000001, "text": " surprisingly rich thing to do there's all kinds of stuff you could do with", "tokens": [50364, 454, 326, 75, 47795, 13, 407, 257, 4166, 307, 445, 364, 4846, 281, 364, 426, 45196, 2316, 300, 8306, 2487, 13, 50820, 50820, 823, 1508, 5489, 257, 4166, 370, 17990, 437, 7719, 257, 4166, 12953, 281, 307, 257, 51208, 51208, 17600, 4593, 551, 281, 360, 456, 311, 439, 3685, 295, 1507, 291, 727, 360, 365, 51480, 51480, 300, 370, 337, 1365, 321, 600, 1217, 2835, 16149, 5215, 300, 311, 257, 51642, 51642, 3857, 300, 311, 257, 21538, 5633, 321, 853, 281, 4536, 322, 264, 7719, 3353, 420, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.161086176777934, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.00011059915414080024}, {"id": 186, "seek": 100032, "start": 1022.6400000000001, "end": 1025.88, "text": " that so for example we've already mentioned sentiment analysis that's a", "tokens": [50364, 454, 326, 75, 47795, 13, 407, 257, 4166, 307, 445, 364, 4846, 281, 364, 426, 45196, 2316, 300, 8306, 2487, 13, 50820, 50820, 823, 1508, 5489, 257, 4166, 370, 17990, 437, 7719, 257, 4166, 12953, 281, 307, 257, 51208, 51208, 17600, 4593, 551, 281, 360, 456, 311, 439, 3685, 295, 1507, 291, 727, 360, 365, 51480, 51480, 300, 370, 337, 1365, 321, 600, 1217, 2835, 16149, 5215, 300, 311, 257, 51642, 51642, 3857, 300, 311, 257, 21538, 5633, 321, 853, 281, 4536, 322, 264, 7719, 3353, 420, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.161086176777934, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.00011059915414080024}, {"id": 187, "seek": 100032, "start": 1025.88, "end": 1029.56, "text": " cat that's a classification task we try to decide on the category positive or", "tokens": [50364, 454, 326, 75, 47795, 13, 407, 257, 4166, 307, 445, 364, 4846, 281, 364, 426, 45196, 2316, 300, 8306, 2487, 13, 50820, 50820, 823, 1508, 5489, 257, 4166, 370, 17990, 437, 7719, 257, 4166, 12953, 281, 307, 257, 51208, 51208, 17600, 4593, 551, 281, 360, 456, 311, 439, 3685, 295, 1507, 291, 727, 360, 365, 51480, 51480, 300, 370, 337, 1365, 321, 600, 1217, 2835, 16149, 5215, 300, 311, 257, 51642, 51642, 3857, 300, 311, 257, 21538, 5633, 321, 853, 281, 4536, 322, 264, 7719, 3353, 420, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.161086176777934, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.00011059915414080024}, {"id": 188, "seek": 102956, "start": 1029.56, "end": 1033.8799999999999, "text": " negative sentiment. Author identification would be taking a document and trying", "tokens": [50364, 3671, 16149, 13, 20216, 22065, 576, 312, 1940, 257, 4166, 293, 1382, 50580, 50580, 281, 915, 264, 7719, 295, 3793, 13, 33577, 12114, 576, 312, 1940, 8512, 293, 50894, 50894, 3372, 552, 666, 10479, 4650, 281, 294, 420, 484, 295, 11923, 337, 257, 4753, 1389, 13, 51142, 51142, 314, 6200, 294, 294, 18767, 12524, 576, 312, 3372, 552, 666, 10479, 295, 291, 458, 51396, 51396, 3507, 1314, 2845, 281, 5474, 2643, 2845, 281, 5763, 5183, 13, 407, 21538, 307, 257, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1562423138391404, "compression_ratio": 1.792626728110599, "no_speech_prob": 5.5603799410164356e-05}, {"id": 189, "seek": 102956, "start": 1033.8799999999999, "end": 1040.1599999999999, "text": " to find the category of author. Legal discovery would be taking documents and", "tokens": [50364, 3671, 16149, 13, 20216, 22065, 576, 312, 1940, 257, 4166, 293, 1382, 50580, 50580, 281, 915, 264, 7719, 295, 3793, 13, 33577, 12114, 576, 312, 1940, 8512, 293, 50894, 50894, 3372, 552, 666, 10479, 4650, 281, 294, 420, 484, 295, 11923, 337, 257, 4753, 1389, 13, 51142, 51142, 314, 6200, 294, 294, 18767, 12524, 576, 312, 3372, 552, 666, 10479, 295, 291, 458, 51396, 51396, 3507, 1314, 2845, 281, 5474, 2643, 2845, 281, 5763, 5183, 13, 407, 21538, 307, 257, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1562423138391404, "compression_ratio": 1.792626728110599, "no_speech_prob": 5.5603799410164356e-05}, {"id": 190, "seek": 102956, "start": 1040.1599999999999, "end": 1045.12, "text": " putting them into categories according to in or out of scope for a court case.", "tokens": [50364, 3671, 16149, 13, 20216, 22065, 576, 312, 1940, 257, 4166, 293, 1382, 50580, 50580, 281, 915, 264, 7719, 295, 3793, 13, 33577, 12114, 576, 312, 1940, 8512, 293, 50894, 50894, 3372, 552, 666, 10479, 4650, 281, 294, 420, 484, 295, 11923, 337, 257, 4753, 1389, 13, 51142, 51142, 314, 6200, 294, 294, 18767, 12524, 576, 312, 3372, 552, 666, 10479, 295, 291, 458, 51396, 51396, 3507, 1314, 2845, 281, 5474, 2643, 2845, 281, 5763, 5183, 13, 407, 21538, 307, 257, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1562423138391404, "compression_ratio": 1.792626728110599, "no_speech_prob": 5.5603799410164356e-05}, {"id": 191, "seek": 102956, "start": 1045.12, "end": 1050.2, "text": " Triage in inbound emails would be putting them into categories of you know", "tokens": [50364, 3671, 16149, 13, 20216, 22065, 576, 312, 1940, 257, 4166, 293, 1382, 50580, 50580, 281, 915, 264, 7719, 295, 3793, 13, 33577, 12114, 576, 312, 1940, 8512, 293, 50894, 50894, 3372, 552, 666, 10479, 4650, 281, 294, 420, 484, 295, 11923, 337, 257, 4753, 1389, 13, 51142, 51142, 314, 6200, 294, 294, 18767, 12524, 576, 312, 3372, 552, 666, 10479, 295, 291, 458, 51396, 51396, 3507, 1314, 2845, 281, 5474, 2643, 2845, 281, 5763, 5183, 13, 407, 21538, 307, 257, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1562423138391404, "compression_ratio": 1.792626728110599, "no_speech_prob": 5.5603799410164356e-05}, {"id": 192, "seek": 102956, "start": 1050.2, "end": 1059.12, "text": " throw away send to customer service send to sales etc. So classification is a", "tokens": [50364, 3671, 16149, 13, 20216, 22065, 576, 312, 1940, 257, 4166, 293, 1382, 50580, 50580, 281, 915, 264, 7719, 295, 3793, 13, 33577, 12114, 576, 312, 1940, 8512, 293, 50894, 50894, 3372, 552, 666, 10479, 4650, 281, 294, 420, 484, 295, 11923, 337, 257, 4753, 1389, 13, 51142, 51142, 314, 6200, 294, 294, 18767, 12524, 576, 312, 3372, 552, 666, 10479, 295, 291, 458, 51396, 51396, 3507, 1314, 2845, 281, 5474, 2643, 2845, 281, 5763, 5183, 13, 407, 21538, 307, 257, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1562423138391404, "compression_ratio": 1.792626728110599, "no_speech_prob": 5.5603799410164356e-05}, {"id": 193, "seek": 105912, "start": 1059.12, "end": 1067.04, "text": " very very rich area and for people interested in trying out NLP in real", "tokens": [50364, 588, 588, 4593, 1859, 293, 337, 561, 3102, 294, 1382, 484, 426, 45196, 294, 957, 50760, 50760, 993, 286, 576, 3402, 21538, 576, 312, 264, 1081, 286, 576, 722, 337, 50934, 50934, 1237, 337, 733, 295, 9515, 957, 12, 13217, 4420, 2740, 291, 393, 5039, 51168, 51168, 558, 1314, 13, 823, 264, 48751, 22631, 6211, 775, 406, 4258, 574, 411, 257, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.0804085217989408, "compression_ratio": 1.5078534031413613, "no_speech_prob": 3.647037738119252e-05}, {"id": 194, "seek": 105912, "start": 1067.04, "end": 1070.52, "text": " life I would suggest classification would be the place I would start for", "tokens": [50364, 588, 588, 4593, 1859, 293, 337, 561, 3102, 294, 1382, 484, 426, 45196, 294, 957, 50760, 50760, 993, 286, 576, 3402, 21538, 576, 312, 264, 1081, 286, 576, 722, 337, 50934, 50934, 1237, 337, 733, 295, 9515, 957, 12, 13217, 4420, 2740, 291, 393, 5039, 51168, 51168, 558, 1314, 13, 823, 264, 48751, 22631, 6211, 775, 406, 4258, 574, 411, 257, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.0804085217989408, "compression_ratio": 1.5078534031413613, "no_speech_prob": 3.647037738119252e-05}, {"id": 195, "seek": 105912, "start": 1070.52, "end": 1075.1999999999998, "text": " looking for kind of accessible real-world useful problems you can solve", "tokens": [50364, 588, 588, 4593, 1859, 293, 337, 561, 3102, 294, 1382, 484, 426, 45196, 294, 957, 50760, 50760, 993, 286, 576, 3402, 21538, 576, 312, 264, 1081, 286, 576, 722, 337, 50934, 50934, 1237, 337, 733, 295, 9515, 957, 12, 13217, 4420, 2740, 291, 393, 5039, 51168, 51168, 558, 1314, 13, 823, 264, 48751, 22631, 6211, 775, 406, 4258, 574, 411, 257, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.0804085217989408, "compression_ratio": 1.5078534031413613, "no_speech_prob": 3.647037738119252e-05}, {"id": 196, "seek": 105912, "start": 1075.1999999999998, "end": 1082.6399999999999, "text": " right away. Now the Kaggle competition does not immediately look like a", "tokens": [50364, 588, 588, 4593, 1859, 293, 337, 561, 3102, 294, 1382, 484, 426, 45196, 294, 957, 50760, 50760, 993, 286, 576, 3402, 21538, 576, 312, 264, 1081, 286, 576, 722, 337, 50934, 50934, 1237, 337, 733, 295, 9515, 957, 12, 13217, 4420, 2740, 291, 393, 5039, 51168, 51168, 558, 1314, 13, 823, 264, 48751, 22631, 6211, 775, 406, 4258, 574, 411, 257, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.0804085217989408, "compression_ratio": 1.5078534031413613, "no_speech_prob": 3.647037738119252e-05}, {"id": 197, "seek": 108264, "start": 1082.64, "end": 1093.1200000000001, "text": " classification competition what it contains let me show you some data what", "tokens": [50364, 21538, 6211, 437, 309, 8306, 718, 385, 855, 291, 512, 1412, 437, 50888, 50888, 309, 8306, 307, 1412, 300, 1542, 411, 341, 309, 575, 257, 551, 436, 818, 18487, 286, 519, 51144, 51144, 436, 818, 3779, 286, 519, 436, 818, 4319, 293, 257, 6175, 13, 823, 613, 366, 286, 393, 380, 51548, 51548, 1604, 1900, 4365, 457, 286, 519, 613, 366, 490, 38142, 293, 286, 519, 322, 264, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.1471745040681627, "compression_ratio": 1.7613636363636365, "no_speech_prob": 6.107328226789832e-05}, {"id": 198, "seek": 108264, "start": 1093.1200000000001, "end": 1098.24, "text": " it contains is data that looks like this it has a thing they call anchor I think", "tokens": [50364, 21538, 6211, 437, 309, 8306, 718, 385, 855, 291, 512, 1412, 437, 50888, 50888, 309, 8306, 307, 1412, 300, 1542, 411, 341, 309, 575, 257, 551, 436, 818, 18487, 286, 519, 51144, 51144, 436, 818, 3779, 286, 519, 436, 818, 4319, 293, 257, 6175, 13, 823, 613, 366, 286, 393, 380, 51548, 51548, 1604, 1900, 4365, 457, 286, 519, 613, 366, 490, 38142, 293, 286, 519, 322, 264, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.1471745040681627, "compression_ratio": 1.7613636363636365, "no_speech_prob": 6.107328226789832e-05}, {"id": 199, "seek": 108264, "start": 1098.24, "end": 1106.3200000000002, "text": " they call target I think they call context and a score. Now these are I can't", "tokens": [50364, 21538, 6211, 437, 309, 8306, 718, 385, 855, 291, 512, 1412, 437, 50888, 50888, 309, 8306, 307, 1412, 300, 1542, 411, 341, 309, 575, 257, 551, 436, 818, 18487, 286, 519, 51144, 51144, 436, 818, 3779, 286, 519, 436, 818, 4319, 293, 257, 6175, 13, 823, 613, 366, 286, 393, 380, 51548, 51548, 1604, 1900, 4365, 457, 286, 519, 613, 366, 490, 38142, 293, 286, 519, 322, 264, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.1471745040681627, "compression_ratio": 1.7613636363636365, "no_speech_prob": 6.107328226789832e-05}, {"id": 200, "seek": 108264, "start": 1106.3200000000002, "end": 1110.4, "text": " remember exact details but I think these are from patents and I think on the", "tokens": [50364, 21538, 6211, 437, 309, 8306, 718, 385, 855, 291, 512, 1412, 437, 50888, 50888, 309, 8306, 307, 1412, 300, 1542, 411, 341, 309, 575, 257, 551, 436, 818, 18487, 286, 519, 51144, 51144, 436, 818, 3779, 286, 519, 436, 818, 4319, 293, 257, 6175, 13, 823, 613, 366, 286, 393, 380, 51548, 51548, 1604, 1900, 4365, 457, 286, 519, 613, 366, 490, 38142, 293, 286, 519, 322, 264, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.1471745040681627, "compression_ratio": 1.7613636363636365, "no_speech_prob": 6.107328226789832e-05}, {"id": 201, "seek": 111040, "start": 1110.4, "end": 1115.92, "text": " patents there are various like things they have to fill in in the patent and", "tokens": [50364, 38142, 456, 366, 3683, 411, 721, 436, 362, 281, 2836, 294, 294, 264, 20495, 293, 50640, 50640, 472, 295, 729, 721, 307, 1219, 18487, 472, 295, 729, 721, 307, 1219, 3779, 293, 294, 50894, 50894, 264, 6211, 264, 3387, 307, 281, 808, 493, 365, 257, 2316, 300, 6772, 51076, 51076, 24799, 597, 18487, 293, 3779, 15494, 366, 1417, 466, 264, 912, 551, 13, 407, 257, 51382, 51382, 6175, 295, 472, 510, 4576, 7222, 293, 14744, 7222, 2745, 1417, 466, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.1093367496168757, "compression_ratio": 1.8817733990147782, "no_speech_prob": 9.458571003051475e-05}, {"id": 202, "seek": 111040, "start": 1115.92, "end": 1121.0, "text": " one of those things is called anchor one of those things is called target and in", "tokens": [50364, 38142, 456, 366, 3683, 411, 721, 436, 362, 281, 2836, 294, 294, 264, 20495, 293, 50640, 50640, 472, 295, 729, 721, 307, 1219, 18487, 472, 295, 729, 721, 307, 1219, 3779, 293, 294, 50894, 50894, 264, 6211, 264, 3387, 307, 281, 808, 493, 365, 257, 2316, 300, 6772, 51076, 51076, 24799, 597, 18487, 293, 3779, 15494, 366, 1417, 466, 264, 912, 551, 13, 407, 257, 51382, 51382, 6175, 295, 472, 510, 4576, 7222, 293, 14744, 7222, 2745, 1417, 466, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.1093367496168757, "compression_ratio": 1.8817733990147782, "no_speech_prob": 9.458571003051475e-05}, {"id": 203, "seek": 111040, "start": 1121.0, "end": 1124.64, "text": " the competition the goal is to come up with a model that automatically", "tokens": [50364, 38142, 456, 366, 3683, 411, 721, 436, 362, 281, 2836, 294, 294, 264, 20495, 293, 50640, 50640, 472, 295, 729, 721, 307, 1219, 18487, 472, 295, 729, 721, 307, 1219, 3779, 293, 294, 50894, 50894, 264, 6211, 264, 3387, 307, 281, 808, 493, 365, 257, 2316, 300, 6772, 51076, 51076, 24799, 597, 18487, 293, 3779, 15494, 366, 1417, 466, 264, 912, 551, 13, 407, 257, 51382, 51382, 6175, 295, 472, 510, 4576, 7222, 293, 14744, 7222, 2745, 1417, 466, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.1093367496168757, "compression_ratio": 1.8817733990147782, "no_speech_prob": 9.458571003051475e-05}, {"id": 204, "seek": 111040, "start": 1124.64, "end": 1130.76, "text": " determines which anchor and target pairs are talking about the same thing. So a", "tokens": [50364, 38142, 456, 366, 3683, 411, 721, 436, 362, 281, 2836, 294, 294, 264, 20495, 293, 50640, 50640, 472, 295, 729, 721, 307, 1219, 18487, 472, 295, 729, 721, 307, 1219, 3779, 293, 294, 50894, 50894, 264, 6211, 264, 3387, 307, 281, 808, 493, 365, 257, 2316, 300, 6772, 51076, 51076, 24799, 597, 18487, 293, 3779, 15494, 366, 1417, 466, 264, 912, 551, 13, 407, 257, 51382, 51382, 6175, 295, 472, 510, 4576, 7222, 293, 14744, 7222, 2745, 1417, 466, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.1093367496168757, "compression_ratio": 1.8817733990147782, "no_speech_prob": 9.458571003051475e-05}, {"id": 205, "seek": 111040, "start": 1130.76, "end": 1136.0800000000002, "text": " score of one here wood article and wooden article obviously talking about", "tokens": [50364, 38142, 456, 366, 3683, 411, 721, 436, 362, 281, 2836, 294, 294, 264, 20495, 293, 50640, 50640, 472, 295, 729, 721, 307, 1219, 18487, 472, 295, 729, 721, 307, 1219, 3779, 293, 294, 50894, 50894, 264, 6211, 264, 3387, 307, 281, 808, 493, 365, 257, 2316, 300, 6772, 51076, 51076, 24799, 597, 18487, 293, 3779, 15494, 366, 1417, 466, 264, 912, 551, 13, 407, 257, 51382, 51382, 6175, 295, 472, 510, 4576, 7222, 293, 14744, 7222, 2745, 1417, 466, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.1093367496168757, "compression_ratio": 1.8817733990147782, "no_speech_prob": 9.458571003051475e-05}, {"id": 206, "seek": 113608, "start": 1136.08, "end": 1141.4399999999998, "text": " the same thing the score of zero here abatement and forest region not talking", "tokens": [50364, 264, 912, 551, 264, 6175, 295, 4018, 510, 410, 267, 1712, 293, 6719, 4458, 406, 1417, 50632, 50632, 466, 264, 912, 551, 13, 407, 264, 3875, 1558, 307, 321, 434, 1382, 281, 2041, 264, 6175, 293, 50940, 50940, 309, 311, 733, 295, 257, 21538, 1154, 733, 295, 406, 321, 434, 1936, 1382, 281, 51202, 51202, 33872, 721, 666, 2139, 613, 732, 721, 366, 264, 912, 420, 613, 732, 721, 51392, 51392, 3212, 380, 264, 912, 309, 311, 733, 295, 406, 570, 321, 362, 406, 445, 472, 293, 4018, 457, 611, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.11488621285621156, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.0001195369113702327}, {"id": 207, "seek": 113608, "start": 1141.4399999999998, "end": 1147.6, "text": " about the same thing. So the basic idea is we're trying to guess the score and", "tokens": [50364, 264, 912, 551, 264, 6175, 295, 4018, 510, 410, 267, 1712, 293, 6719, 4458, 406, 1417, 50632, 50632, 466, 264, 912, 551, 13, 407, 264, 3875, 1558, 307, 321, 434, 1382, 281, 2041, 264, 6175, 293, 50940, 50940, 309, 311, 733, 295, 257, 21538, 1154, 733, 295, 406, 321, 434, 1936, 1382, 281, 51202, 51202, 33872, 721, 666, 2139, 613, 732, 721, 366, 264, 912, 420, 613, 732, 721, 51392, 51392, 3212, 380, 264, 912, 309, 311, 733, 295, 406, 570, 321, 362, 406, 445, 472, 293, 4018, 457, 611, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.11488621285621156, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.0001195369113702327}, {"id": 208, "seek": 113608, "start": 1147.6, "end": 1152.84, "text": " it's kind of a classification problem kind of not we're basically trying to", "tokens": [50364, 264, 912, 551, 264, 6175, 295, 4018, 510, 410, 267, 1712, 293, 6719, 4458, 406, 1417, 50632, 50632, 466, 264, 912, 551, 13, 407, 264, 3875, 1558, 307, 321, 434, 1382, 281, 2041, 264, 6175, 293, 50940, 50940, 309, 311, 733, 295, 257, 21538, 1154, 733, 295, 406, 321, 434, 1936, 1382, 281, 51202, 51202, 33872, 721, 666, 2139, 613, 732, 721, 366, 264, 912, 420, 613, 732, 721, 51392, 51392, 3212, 380, 264, 912, 309, 311, 733, 295, 406, 570, 321, 362, 406, 445, 472, 293, 4018, 457, 611, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.11488621285621156, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.0001195369113702327}, {"id": 209, "seek": 113608, "start": 1152.84, "end": 1156.6399999999999, "text": " classify things into either these two things are the same or these two things", "tokens": [50364, 264, 912, 551, 264, 6175, 295, 4018, 510, 410, 267, 1712, 293, 6719, 4458, 406, 1417, 50632, 50632, 466, 264, 912, 551, 13, 407, 264, 3875, 1558, 307, 321, 434, 1382, 281, 2041, 264, 6175, 293, 50940, 50940, 309, 311, 733, 295, 257, 21538, 1154, 733, 295, 406, 321, 434, 1936, 1382, 281, 51202, 51202, 33872, 721, 666, 2139, 613, 732, 721, 366, 264, 912, 420, 613, 732, 721, 51392, 51392, 3212, 380, 264, 912, 309, 311, 733, 295, 406, 570, 321, 362, 406, 445, 472, 293, 4018, 457, 611, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.11488621285621156, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.0001195369113702327}, {"id": 210, "seek": 113608, "start": 1156.6399999999999, "end": 1160.6399999999999, "text": " aren't the same it's kind of not because we have not just one and zero but also", "tokens": [50364, 264, 912, 551, 264, 6175, 295, 4018, 510, 410, 267, 1712, 293, 6719, 4458, 406, 1417, 50632, 50632, 466, 264, 912, 551, 13, 407, 264, 3875, 1558, 307, 321, 434, 1382, 281, 2041, 264, 6175, 293, 50940, 50940, 309, 311, 733, 295, 257, 21538, 1154, 733, 295, 406, 321, 434, 1936, 1382, 281, 51202, 51202, 33872, 721, 666, 2139, 613, 732, 721, 366, 264, 912, 420, 613, 732, 721, 51392, 51392, 3212, 380, 264, 912, 309, 311, 733, 295, 406, 570, 321, 362, 406, 445, 472, 293, 4018, 457, 611, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.11488621285621156, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.0001195369113702327}, {"id": 211, "seek": 116064, "start": 1160.64, "end": 1167.8000000000002, "text": " 0.25 0.5 and 0.75. There's also a column called context which is I believe is", "tokens": [50364, 1958, 13, 6074, 1958, 13, 20, 293, 1958, 13, 11901, 13, 821, 311, 611, 257, 7738, 1219, 4319, 597, 307, 286, 1697, 307, 50722, 50722, 411, 264, 7719, 300, 341, 20495, 390, 18789, 294, 293, 452, 3701, 307, 300, 50988, 50988, 1968, 264, 18487, 293, 264, 3779, 1207, 382, 2531, 420, 406, 5946, 322, 291, 458, 51282, 51282, 437, 437, 264, 20495, 390, 18789, 833, 13, 407, 577, 576, 321, 747, 341, 293, 1261, 309, 666, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.11354237794876099, "compression_ratio": 1.5728643216080402, "no_speech_prob": 2.1443871446535923e-05}, {"id": 212, "seek": 116064, "start": 1167.8000000000002, "end": 1173.1200000000001, "text": " like the category that this patent was filed in and my understanding is that", "tokens": [50364, 1958, 13, 6074, 1958, 13, 20, 293, 1958, 13, 11901, 13, 821, 311, 611, 257, 7738, 1219, 4319, 597, 307, 286, 1697, 307, 50722, 50722, 411, 264, 7719, 300, 341, 20495, 390, 18789, 294, 293, 452, 3701, 307, 300, 50988, 50988, 1968, 264, 18487, 293, 264, 3779, 1207, 382, 2531, 420, 406, 5946, 322, 291, 458, 51282, 51282, 437, 437, 264, 20495, 390, 18789, 833, 13, 407, 577, 576, 321, 747, 341, 293, 1261, 309, 666, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.11354237794876099, "compression_ratio": 1.5728643216080402, "no_speech_prob": 2.1443871446535923e-05}, {"id": 213, "seek": 116064, "start": 1173.1200000000001, "end": 1179.0, "text": " whether the anchor and the target count as similar or not depends on you know", "tokens": [50364, 1958, 13, 6074, 1958, 13, 20, 293, 1958, 13, 11901, 13, 821, 311, 611, 257, 7738, 1219, 4319, 597, 307, 286, 1697, 307, 50722, 50722, 411, 264, 7719, 300, 341, 20495, 390, 18789, 294, 293, 452, 3701, 307, 300, 50988, 50988, 1968, 264, 18487, 293, 264, 3779, 1207, 382, 2531, 420, 406, 5946, 322, 291, 458, 51282, 51282, 437, 437, 264, 20495, 390, 18789, 833, 13, 407, 577, 576, 321, 747, 341, 293, 1261, 309, 666, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.11354237794876099, "compression_ratio": 1.5728643216080402, "no_speech_prob": 2.1443871446535923e-05}, {"id": 214, "seek": 116064, "start": 1179.0, "end": 1187.1200000000001, "text": " what what the patent was filed under. So how would we take this and turn it into", "tokens": [50364, 1958, 13, 6074, 1958, 13, 20, 293, 1958, 13, 11901, 13, 821, 311, 611, 257, 7738, 1219, 4319, 597, 307, 286, 1697, 307, 50722, 50722, 411, 264, 7719, 300, 341, 20495, 390, 18789, 294, 293, 452, 3701, 307, 300, 50988, 50988, 1968, 264, 18487, 293, 264, 3779, 1207, 382, 2531, 420, 406, 5946, 322, 291, 458, 51282, 51282, 437, 437, 264, 20495, 390, 18789, 833, 13, 407, 577, 576, 321, 747, 341, 293, 1261, 309, 666, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.11354237794876099, "compression_ratio": 1.5728643216080402, "no_speech_prob": 2.1443871446535923e-05}, {"id": 215, "seek": 118712, "start": 1187.12, "end": 1195.6799999999998, "text": " something like a classification problem? So the suggestion I make here is that we", "tokens": [50364, 746, 411, 257, 21538, 1154, 30, 407, 264, 16541, 286, 652, 510, 307, 300, 321, 50792, 50792, 727, 1936, 584, 1392, 718, 311, 829, 264, 291, 458, 512, 512, 35870, 6798, 411, 51118, 51118, 2487, 472, 420, 2519, 472, 949, 264, 264, 700, 7738, 293, 550, 746, 1646, 411, 51390, 51390, 2487, 732, 949, 264, 1150, 7738, 420, 1310, 264, 611, 264, 4319, 286, 820, 362, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.16277739661080495, "compression_ratio": 1.65625, "no_speech_prob": 3.6450182960834354e-05}, {"id": 216, "seek": 118712, "start": 1195.6799999999998, "end": 1202.1999999999998, "text": " could basically say okay let's put the you know some some constants string like", "tokens": [50364, 746, 411, 257, 21538, 1154, 30, 407, 264, 16541, 286, 652, 510, 307, 300, 321, 50792, 50792, 727, 1936, 584, 1392, 718, 311, 829, 264, 291, 458, 512, 512, 35870, 6798, 411, 51118, 51118, 2487, 472, 420, 2519, 472, 949, 264, 264, 700, 7738, 293, 550, 746, 1646, 411, 51390, 51390, 2487, 732, 949, 264, 1150, 7738, 420, 1310, 264, 611, 264, 4319, 286, 820, 362, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.16277739661080495, "compression_ratio": 1.65625, "no_speech_prob": 3.6450182960834354e-05}, {"id": 217, "seek": 118712, "start": 1202.1999999999998, "end": 1207.6399999999999, "text": " text one or field one before the the first column and then something else like", "tokens": [50364, 746, 411, 257, 21538, 1154, 30, 407, 264, 16541, 286, 652, 510, 307, 300, 321, 50792, 50792, 727, 1936, 584, 1392, 718, 311, 829, 264, 291, 458, 512, 512, 35870, 6798, 411, 51118, 51118, 2487, 472, 420, 2519, 472, 949, 264, 264, 700, 7738, 293, 550, 746, 1646, 411, 51390, 51390, 2487, 732, 949, 264, 1150, 7738, 420, 1310, 264, 611, 264, 4319, 286, 820, 362, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.16277739661080495, "compression_ratio": 1.65625, "no_speech_prob": 3.6450182960834354e-05}, {"id": 218, "seek": 118712, "start": 1207.6399999999999, "end": 1213.8, "text": " text two before the second column or maybe the also the context I should have", "tokens": [50364, 746, 411, 257, 21538, 1154, 30, 407, 264, 16541, 286, 652, 510, 307, 300, 321, 50792, 50792, 727, 1936, 584, 1392, 718, 311, 829, 264, 291, 458, 512, 512, 35870, 6798, 411, 51118, 51118, 2487, 472, 420, 2519, 472, 949, 264, 264, 700, 7738, 293, 550, 746, 1646, 411, 51390, 51390, 2487, 732, 949, 264, 1150, 7738, 420, 1310, 264, 611, 264, 4319, 286, 820, 362, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.16277739661080495, "compression_ratio": 1.65625, "no_speech_prob": 3.6450182960834354e-05}, {"id": 219, "seek": 121380, "start": 1213.8, "end": 1218.12, "text": " this well text three in the context and then try to choose a category of meaning", "tokens": [50364, 341, 731, 2487, 1045, 294, 264, 4319, 293, 550, 853, 281, 2826, 257, 7719, 295, 3620, 50580, 50580, 32194, 819, 2531, 420, 14800, 370, 309, 727, 1936, 50712, 50712, 1588, 7186, 473, 729, 1045, 3755, 1214, 818, 300, 257, 4166, 293, 550, 853, 281, 50984, 50984, 3847, 257, 2316, 300, 393, 6069, 613, 10479, 13, 663, 1116, 312, 364, 1365, 295, 577, 51300, 51300, 321, 393, 747, 341, 1936, 32194, 1154, 293, 1261, 309, 666, 746, 300, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13699042944260587, "compression_ratio": 1.7327188940092166, "no_speech_prob": 0.00013339861470740288}, {"id": 220, "seek": 121380, "start": 1218.12, "end": 1220.76, "text": " similarity different similar or identical so it could basically", "tokens": [50364, 341, 731, 2487, 1045, 294, 264, 4319, 293, 550, 853, 281, 2826, 257, 7719, 295, 3620, 50580, 50580, 32194, 819, 2531, 420, 14800, 370, 309, 727, 1936, 50712, 50712, 1588, 7186, 473, 729, 1045, 3755, 1214, 818, 300, 257, 4166, 293, 550, 853, 281, 50984, 50984, 3847, 257, 2316, 300, 393, 6069, 613, 10479, 13, 663, 1116, 312, 364, 1365, 295, 577, 51300, 51300, 321, 393, 747, 341, 1936, 32194, 1154, 293, 1261, 309, 666, 746, 300, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13699042944260587, "compression_ratio": 1.7327188940092166, "no_speech_prob": 0.00013339861470740288}, {"id": 221, "seek": 121380, "start": 1220.76, "end": 1226.2, "text": " concatenate those three pieces together call that a document and then try to", "tokens": [50364, 341, 731, 2487, 1045, 294, 264, 4319, 293, 550, 853, 281, 2826, 257, 7719, 295, 3620, 50580, 50580, 32194, 819, 2531, 420, 14800, 370, 309, 727, 1936, 50712, 50712, 1588, 7186, 473, 729, 1045, 3755, 1214, 818, 300, 257, 4166, 293, 550, 853, 281, 50984, 50984, 3847, 257, 2316, 300, 393, 6069, 613, 10479, 13, 663, 1116, 312, 364, 1365, 295, 577, 51300, 51300, 321, 393, 747, 341, 1936, 32194, 1154, 293, 1261, 309, 666, 746, 300, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13699042944260587, "compression_ratio": 1.7327188940092166, "no_speech_prob": 0.00013339861470740288}, {"id": 222, "seek": 121380, "start": 1226.2, "end": 1232.52, "text": " train a model that can predict these categories. That'd be an example of how", "tokens": [50364, 341, 731, 2487, 1045, 294, 264, 4319, 293, 550, 853, 281, 2826, 257, 7719, 295, 3620, 50580, 50580, 32194, 819, 2531, 420, 14800, 370, 309, 727, 1936, 50712, 50712, 1588, 7186, 473, 729, 1045, 3755, 1214, 818, 300, 257, 4166, 293, 550, 853, 281, 50984, 50984, 3847, 257, 2316, 300, 393, 6069, 613, 10479, 13, 663, 1116, 312, 364, 1365, 295, 577, 51300, 51300, 321, 393, 747, 341, 1936, 32194, 1154, 293, 1261, 309, 666, 746, 300, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13699042944260587, "compression_ratio": 1.7327188940092166, "no_speech_prob": 0.00013339861470740288}, {"id": 223, "seek": 121380, "start": 1232.52, "end": 1239.8, "text": " we can take this basically similarity problem and turn it into something that", "tokens": [50364, 341, 731, 2487, 1045, 294, 264, 4319, 293, 550, 853, 281, 2826, 257, 7719, 295, 3620, 50580, 50580, 32194, 819, 2531, 420, 14800, 370, 309, 727, 1936, 50712, 50712, 1588, 7186, 473, 729, 1045, 3755, 1214, 818, 300, 257, 4166, 293, 550, 853, 281, 50984, 50984, 3847, 257, 2316, 300, 393, 6069, 613, 10479, 13, 663, 1116, 312, 364, 1365, 295, 577, 51300, 51300, 321, 393, 747, 341, 1936, 32194, 1154, 293, 1261, 309, 666, 746, 300, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13699042944260587, "compression_ratio": 1.7327188940092166, "no_speech_prob": 0.00013339861470740288}, {"id": 224, "seek": 123980, "start": 1239.8, "end": 1244.68, "text": " looks like a classification problem and we tend to do this a lot in deep learning", "tokens": [50364, 1542, 411, 257, 21538, 1154, 293, 321, 3928, 281, 360, 341, 257, 688, 294, 2452, 2539, 50608, 50608, 307, 321, 733, 295, 747, 2740, 300, 574, 257, 857, 7613, 293, 819, 293, 1261, 552, 50944, 50944, 666, 257, 1154, 300, 1542, 411, 746, 321, 5521, 13, 1779, 370, 322, 48751, 22631, 341, 307, 51364, 51364, 257, 291, 458, 4833, 1412, 992, 300, 291, 434, 516, 281, 643, 257, 18407, 281, 1190, 370, 291, 393, 51687, 51687], "temperature": 0.0, "avg_logprob": -0.11424092757396209, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.00012146291555836797}, {"id": 225, "seek": 123980, "start": 1244.68, "end": 1251.3999999999999, "text": " is we kind of take problems that look a bit novel and different and turn them", "tokens": [50364, 1542, 411, 257, 21538, 1154, 293, 321, 3928, 281, 360, 341, 257, 688, 294, 2452, 2539, 50608, 50608, 307, 321, 733, 295, 747, 2740, 300, 574, 257, 857, 7613, 293, 819, 293, 1261, 552, 50944, 50944, 666, 257, 1154, 300, 1542, 411, 746, 321, 5521, 13, 1779, 370, 322, 48751, 22631, 341, 307, 51364, 51364, 257, 291, 458, 4833, 1412, 992, 300, 291, 434, 516, 281, 643, 257, 18407, 281, 1190, 370, 291, 393, 51687, 51687], "temperature": 0.0, "avg_logprob": -0.11424092757396209, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.00012146291555836797}, {"id": 226, "seek": 123980, "start": 1251.3999999999999, "end": 1259.8, "text": " into a problem that looks like something we recognize. Right so on Kaggle this is", "tokens": [50364, 1542, 411, 257, 21538, 1154, 293, 321, 3928, 281, 360, 341, 257, 688, 294, 2452, 2539, 50608, 50608, 307, 321, 733, 295, 747, 2740, 300, 574, 257, 857, 7613, 293, 819, 293, 1261, 552, 50944, 50944, 666, 257, 1154, 300, 1542, 411, 746, 321, 5521, 13, 1779, 370, 322, 48751, 22631, 341, 307, 51364, 51364, 257, 291, 458, 4833, 1412, 992, 300, 291, 434, 516, 281, 643, 257, 18407, 281, 1190, 370, 291, 393, 51687, 51687], "temperature": 0.0, "avg_logprob": -0.11424092757396209, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.00012146291555836797}, {"id": 227, "seek": 123980, "start": 1259.8, "end": 1266.26, "text": " a you know larger data set that you're going to need a GPU to run so you can", "tokens": [50364, 1542, 411, 257, 21538, 1154, 293, 321, 3928, 281, 360, 341, 257, 688, 294, 2452, 2539, 50608, 50608, 307, 321, 733, 295, 747, 2740, 300, 574, 257, 857, 7613, 293, 819, 293, 1261, 552, 50944, 50944, 666, 257, 1154, 300, 1542, 411, 746, 321, 5521, 13, 1779, 370, 322, 48751, 22631, 341, 307, 51364, 51364, 257, 291, 458, 4833, 1412, 992, 300, 291, 434, 516, 281, 643, 257, 18407, 281, 1190, 370, 291, 393, 51687, 51687], "temperature": 0.0, "avg_logprob": -0.11424092757396209, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.00012146291555836797}, {"id": 228, "seek": 126626, "start": 1266.26, "end": 1270.4, "text": " click on the accelerator button and choose GPU to make sure that you're using", "tokens": [50364, 2052, 322, 264, 39889, 2960, 293, 2826, 18407, 281, 652, 988, 300, 291, 434, 1228, 50571, 50571, 257, 18407, 13, 759, 291, 2052, 5055, 293, 8129, 322, 452, 4166, 286, 519, 300, 603, 1051, 337, 291, 50809, 50809, 6772, 13, 21079, 291, 458, 286, 411, 1228, 721, 411, 3035, 1901, 51211, 51211, 5101, 1101, 813, 48751, 22631, 13, 1743, 48751, 22631, 307, 1238, 665, 457, 291, 458, 291, 787, 483, 51473, 51473, 2217, 2496, 257, 1243, 295, 18407, 565, 293, 264, 21060, 9839, 337, 385, 307, 406, 382, 665, 382, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.1370954209185661, "compression_ratio": 1.6074380165289257, "no_speech_prob": 8.479832467855886e-05}, {"id": 229, "seek": 126626, "start": 1270.4, "end": 1275.16, "text": " a GPU. If you click copy and edit on my document I think that'll happen for you", "tokens": [50364, 2052, 322, 264, 39889, 2960, 293, 2826, 18407, 281, 652, 988, 300, 291, 434, 1228, 50571, 50571, 257, 18407, 13, 759, 291, 2052, 5055, 293, 8129, 322, 452, 4166, 286, 519, 300, 603, 1051, 337, 291, 50809, 50809, 6772, 13, 21079, 291, 458, 286, 411, 1228, 721, 411, 3035, 1901, 51211, 51211, 5101, 1101, 813, 48751, 22631, 13, 1743, 48751, 22631, 307, 1238, 665, 457, 291, 458, 291, 787, 483, 51473, 51473, 2217, 2496, 257, 1243, 295, 18407, 565, 293, 264, 21060, 9839, 337, 385, 307, 406, 382, 665, 382, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.1370954209185661, "compression_ratio": 1.6074380165289257, "no_speech_prob": 8.479832467855886e-05}, {"id": 230, "seek": 126626, "start": 1275.16, "end": 1283.2, "text": " automatically. Personally you know I like using things like paper space", "tokens": [50364, 2052, 322, 264, 39889, 2960, 293, 2826, 18407, 281, 652, 988, 300, 291, 434, 1228, 50571, 50571, 257, 18407, 13, 759, 291, 2052, 5055, 293, 8129, 322, 452, 4166, 286, 519, 300, 603, 1051, 337, 291, 50809, 50809, 6772, 13, 21079, 291, 458, 286, 411, 1228, 721, 411, 3035, 1901, 51211, 51211, 5101, 1101, 813, 48751, 22631, 13, 1743, 48751, 22631, 307, 1238, 665, 457, 291, 458, 291, 787, 483, 51473, 51473, 2217, 2496, 257, 1243, 295, 18407, 565, 293, 264, 21060, 9839, 337, 385, 307, 406, 382, 665, 382, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.1370954209185661, "compression_ratio": 1.6074380165289257, "no_speech_prob": 8.479832467855886e-05}, {"id": 231, "seek": 126626, "start": 1283.2, "end": 1288.44, "text": " generally better than Kaggle. Like Kaggle is pretty good but you know you only get", "tokens": [50364, 2052, 322, 264, 39889, 2960, 293, 2826, 18407, 281, 652, 988, 300, 291, 434, 1228, 50571, 50571, 257, 18407, 13, 759, 291, 2052, 5055, 293, 8129, 322, 452, 4166, 286, 519, 300, 603, 1051, 337, 291, 50809, 50809, 6772, 13, 21079, 291, 458, 286, 411, 1228, 721, 411, 3035, 1901, 51211, 51211, 5101, 1101, 813, 48751, 22631, 13, 1743, 48751, 22631, 307, 1238, 665, 457, 291, 458, 291, 787, 483, 51473, 51473, 2217, 2496, 257, 1243, 295, 18407, 565, 293, 264, 21060, 9839, 337, 385, 307, 406, 382, 665, 382, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.1370954209185661, "compression_ratio": 1.6074380165289257, "no_speech_prob": 8.479832467855886e-05}, {"id": 232, "seek": 126626, "start": 1288.44, "end": 1294.04, "text": " 30 hours a week of GPU time and the notebook editor for me is not as good as", "tokens": [50364, 2052, 322, 264, 39889, 2960, 293, 2826, 18407, 281, 652, 988, 300, 291, 434, 1228, 50571, 50571, 257, 18407, 13, 759, 291, 2052, 5055, 293, 8129, 322, 452, 4166, 286, 519, 300, 603, 1051, 337, 291, 50809, 50809, 6772, 13, 21079, 291, 458, 286, 411, 1228, 721, 411, 3035, 1901, 51211, 51211, 5101, 1101, 813, 48751, 22631, 13, 1743, 48751, 22631, 307, 1238, 665, 457, 291, 458, 291, 787, 483, 51473, 51473, 2217, 2496, 257, 1243, 295, 18407, 565, 293, 264, 21060, 9839, 337, 385, 307, 406, 382, 665, 382, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.1370954209185661, "compression_ratio": 1.6074380165289257, "no_speech_prob": 8.479832467855886e-05}, {"id": 233, "seek": 129404, "start": 1294.04, "end": 1298.1599999999999, "text": " a real JupyterLab environment. So there's some information here I won't go", "tokens": [50364, 257, 957, 22125, 88, 391, 37880, 2823, 13, 407, 456, 311, 512, 1589, 510, 286, 1582, 380, 352, 50570, 50570, 807, 457, 309, 1936, 15626, 577, 291, 393, 5484, 1507, 281, 3035, 1901, 420, 50898, 50898, 428, 1065, 3820, 382, 731, 498, 291, 528, 281, 13, 407, 286, 1936, 1884, 341, 707, 51208, 51208, 748, 4812, 282, 1009, 294, 452, 43782, 1219, 307, 48751, 22631, 597, 307, 516, 281, 312, 2074, 498, 51386, 51386, 309, 311, 2614, 322, 48751, 22631, 293, 7908, 5911, 293, 604, 707, 2962, 286, 643, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.10637394074470766, "compression_ratio": 1.5833333333333333, "no_speech_prob": 5.7378339988645166e-05}, {"id": 234, "seek": 129404, "start": 1298.1599999999999, "end": 1304.72, "text": " through but it basically describes how you can download stuff to paper space or", "tokens": [50364, 257, 957, 22125, 88, 391, 37880, 2823, 13, 407, 456, 311, 512, 1589, 510, 286, 1582, 380, 352, 50570, 50570, 807, 457, 309, 1936, 15626, 577, 291, 393, 5484, 1507, 281, 3035, 1901, 420, 50898, 50898, 428, 1065, 3820, 382, 731, 498, 291, 528, 281, 13, 407, 286, 1936, 1884, 341, 707, 51208, 51208, 748, 4812, 282, 1009, 294, 452, 43782, 1219, 307, 48751, 22631, 597, 307, 516, 281, 312, 2074, 498, 51386, 51386, 309, 311, 2614, 322, 48751, 22631, 293, 7908, 5911, 293, 604, 707, 2962, 286, 643, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.10637394074470766, "compression_ratio": 1.5833333333333333, "no_speech_prob": 5.7378339988645166e-05}, {"id": 235, "seek": 129404, "start": 1304.72, "end": 1310.92, "text": " your own computer as well if you want to. So I basically create this little", "tokens": [50364, 257, 957, 22125, 88, 391, 37880, 2823, 13, 407, 456, 311, 512, 1589, 510, 286, 1582, 380, 352, 50570, 50570, 807, 457, 309, 1936, 15626, 577, 291, 393, 5484, 1507, 281, 3035, 1901, 420, 50898, 50898, 428, 1065, 3820, 382, 731, 498, 291, 528, 281, 13, 407, 286, 1936, 1884, 341, 707, 51208, 51208, 748, 4812, 282, 1009, 294, 452, 43782, 1219, 307, 48751, 22631, 597, 307, 516, 281, 312, 2074, 498, 51386, 51386, 309, 311, 2614, 322, 48751, 22631, 293, 7908, 5911, 293, 604, 707, 2962, 286, 643, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.10637394074470766, "compression_ratio": 1.5833333333333333, "no_speech_prob": 5.7378339988645166e-05}, {"id": 236, "seek": 129404, "start": 1310.92, "end": 1314.48, "text": " boolean always in my notebooks called is Kaggle which is going to be true if", "tokens": [50364, 257, 957, 22125, 88, 391, 37880, 2823, 13, 407, 456, 311, 512, 1589, 510, 286, 1582, 380, 352, 50570, 50570, 807, 457, 309, 1936, 15626, 577, 291, 393, 5484, 1507, 281, 3035, 1901, 420, 50898, 50898, 428, 1065, 3820, 382, 731, 498, 291, 528, 281, 13, 407, 286, 1936, 1884, 341, 707, 51208, 51208, 748, 4812, 282, 1009, 294, 452, 43782, 1219, 307, 48751, 22631, 597, 307, 516, 281, 312, 2074, 498, 51386, 51386, 309, 311, 2614, 322, 48751, 22631, 293, 7908, 5911, 293, 604, 707, 2962, 286, 643, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.10637394074470766, "compression_ratio": 1.5833333333333333, "no_speech_prob": 5.7378339988645166e-05}, {"id": 237, "seek": 129404, "start": 1314.48, "end": 1317.48, "text": " it's running on Kaggle and false otherwise and any little changes I need", "tokens": [50364, 257, 957, 22125, 88, 391, 37880, 2823, 13, 407, 456, 311, 512, 1589, 510, 286, 1582, 380, 352, 50570, 50570, 807, 457, 309, 1936, 15626, 577, 291, 393, 5484, 1507, 281, 3035, 1901, 420, 50898, 50898, 428, 1065, 3820, 382, 731, 498, 291, 528, 281, 13, 407, 286, 1936, 1884, 341, 707, 51208, 51208, 748, 4812, 282, 1009, 294, 452, 43782, 1219, 307, 48751, 22631, 597, 307, 516, 281, 312, 2074, 498, 51386, 51386, 309, 311, 2614, 322, 48751, 22631, 293, 7908, 5911, 293, 604, 707, 2962, 286, 643, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.10637394074470766, "compression_ratio": 1.5833333333333333, "no_speech_prob": 5.7378339988645166e-05}, {"id": 238, "seek": 131748, "start": 1317.48, "end": 1326.3600000000001, "text": " to make I'd say if is Kaggle and put those changes. So here you can see here", "tokens": [50364, 281, 652, 286, 1116, 584, 498, 307, 48751, 22631, 293, 829, 729, 2962, 13, 407, 510, 291, 393, 536, 510, 50808, 50808, 498, 286, 478, 406, 322, 48751, 22631, 293, 286, 500, 380, 362, 264, 1412, 1939, 550, 5484, 309, 293, 48751, 22631, 51068, 51068, 575, 257, 707, 9362, 309, 307, 1596, 13239, 337, 884, 1507, 411, 32529, 1412, 293, 51288, 51288, 27301, 43782, 293, 1507, 411, 300, 31836, 281, 26185, 13, 759, 321, 366, 322, 51620, 51620, 48751, 22631, 550, 264, 1412, 307, 1217, 516, 281, 312, 456, 337, 505, 597, 307, 767, 257, 665, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.1053991413116455, "compression_ratio": 1.70995670995671, "no_speech_prob": 5.143578891875222e-05}, {"id": 239, "seek": 131748, "start": 1326.3600000000001, "end": 1331.56, "text": " if I'm not on Kaggle and I don't have the data yet then download it and Kaggle", "tokens": [50364, 281, 652, 286, 1116, 584, 498, 307, 48751, 22631, 293, 829, 729, 2962, 13, 407, 510, 291, 393, 536, 510, 50808, 50808, 498, 286, 478, 406, 322, 48751, 22631, 293, 286, 500, 380, 362, 264, 1412, 1939, 550, 5484, 309, 293, 48751, 22631, 51068, 51068, 575, 257, 707, 9362, 309, 307, 1596, 13239, 337, 884, 1507, 411, 32529, 1412, 293, 51288, 51288, 27301, 43782, 293, 1507, 411, 300, 31836, 281, 26185, 13, 759, 321, 366, 322, 51620, 51620, 48751, 22631, 550, 264, 1412, 307, 1217, 516, 281, 312, 456, 337, 505, 597, 307, 767, 257, 665, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.1053991413116455, "compression_ratio": 1.70995670995671, "no_speech_prob": 5.143578891875222e-05}, {"id": 240, "seek": 131748, "start": 1331.56, "end": 1335.96, "text": " has a little API it is quite handy for doing stuff like downloading data and", "tokens": [50364, 281, 652, 286, 1116, 584, 498, 307, 48751, 22631, 293, 829, 729, 2962, 13, 407, 510, 291, 393, 536, 510, 50808, 50808, 498, 286, 478, 406, 322, 48751, 22631, 293, 286, 500, 380, 362, 264, 1412, 1939, 550, 5484, 309, 293, 48751, 22631, 51068, 51068, 575, 257, 707, 9362, 309, 307, 1596, 13239, 337, 884, 1507, 411, 32529, 1412, 293, 51288, 51288, 27301, 43782, 293, 1507, 411, 300, 31836, 281, 26185, 13, 759, 321, 366, 322, 51620, 51620, 48751, 22631, 550, 264, 1412, 307, 1217, 516, 281, 312, 456, 337, 505, 597, 307, 767, 257, 665, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.1053991413116455, "compression_ratio": 1.70995670995671, "no_speech_prob": 5.143578891875222e-05}, {"id": 241, "seek": 131748, "start": 1335.96, "end": 1342.6, "text": " uploading notebooks and stuff like that submitting to competitions. If we are on", "tokens": [50364, 281, 652, 286, 1116, 584, 498, 307, 48751, 22631, 293, 829, 729, 2962, 13, 407, 510, 291, 393, 536, 510, 50808, 50808, 498, 286, 478, 406, 322, 48751, 22631, 293, 286, 500, 380, 362, 264, 1412, 1939, 550, 5484, 309, 293, 48751, 22631, 51068, 51068, 575, 257, 707, 9362, 309, 307, 1596, 13239, 337, 884, 1507, 411, 32529, 1412, 293, 51288, 51288, 27301, 43782, 293, 1507, 411, 300, 31836, 281, 26185, 13, 759, 321, 366, 322, 51620, 51620, 48751, 22631, 550, 264, 1412, 307, 1217, 516, 281, 312, 456, 337, 505, 597, 307, 767, 257, 665, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.1053991413116455, "compression_ratio": 1.70995670995671, "no_speech_prob": 5.143578891875222e-05}, {"id": 242, "seek": 131748, "start": 1342.6, "end": 1346.16, "text": " Kaggle then the data is already going to be there for us which is actually a good", "tokens": [50364, 281, 652, 286, 1116, 584, 498, 307, 48751, 22631, 293, 829, 729, 2962, 13, 407, 510, 291, 393, 536, 510, 50808, 50808, 498, 286, 478, 406, 322, 48751, 22631, 293, 286, 500, 380, 362, 264, 1412, 1939, 550, 5484, 309, 293, 48751, 22631, 51068, 51068, 575, 257, 707, 9362, 309, 307, 1596, 13239, 337, 884, 1507, 411, 32529, 1412, 293, 51288, 51288, 27301, 43782, 293, 1507, 411, 300, 31836, 281, 26185, 13, 759, 321, 366, 322, 51620, 51620, 48751, 22631, 550, 264, 1412, 307, 1217, 516, 281, 312, 456, 337, 505, 597, 307, 767, 257, 665, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.1053991413116455, "compression_ratio": 1.70995670995671, "no_speech_prob": 5.143578891875222e-05}, {"id": 243, "seek": 134616, "start": 1346.16, "end": 1349.16, "text": " reason for beginners to use Kaggle is you don't have to worry about", "tokens": [50364, 1778, 337, 26992, 281, 764, 48751, 22631, 307, 291, 500, 380, 362, 281, 3292, 466, 50514, 50514, 23771, 264, 1412, 412, 439, 309, 311, 3798, 456, 337, 291, 382, 2321, 382, 291, 1269, 264, 50660, 50660, 21060, 13, 48751, 22631, 575, 257, 688, 295, 15329, 17401, 8899, 457, 406, 4725, 51112, 51112, 439, 264, 2306, 291, 528, 293, 412, 264, 935, 286, 4114, 341, 436, 994, 380, 362, 51288, 51288, 41706, 8475, 42856, 7372, 337, 512, 1778, 370, 291, 393, 1009, 445, 3625, 51500, 51500, 1507, 13, 407, 291, 1062, 1604, 264, 1624, 43233, 1491, 1355, 341, 307, 406, 257, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.11133019740764911, "compression_ratio": 1.689922480620155, "no_speech_prob": 4.331217860453762e-05}, {"id": 244, "seek": 134616, "start": 1349.16, "end": 1352.0800000000002, "text": " grabbing the data at all it's sitting there for you as soon as you open the", "tokens": [50364, 1778, 337, 26992, 281, 764, 48751, 22631, 307, 291, 500, 380, 362, 281, 3292, 466, 50514, 50514, 23771, 264, 1412, 412, 439, 309, 311, 3798, 456, 337, 291, 382, 2321, 382, 291, 1269, 264, 50660, 50660, 21060, 13, 48751, 22631, 575, 257, 688, 295, 15329, 17401, 8899, 457, 406, 4725, 51112, 51112, 439, 264, 2306, 291, 528, 293, 412, 264, 935, 286, 4114, 341, 436, 994, 380, 362, 51288, 51288, 41706, 8475, 42856, 7372, 337, 512, 1778, 370, 291, 393, 1009, 445, 3625, 51500, 51500, 1507, 13, 407, 291, 1062, 1604, 264, 1624, 43233, 1491, 1355, 341, 307, 406, 257, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.11133019740764911, "compression_ratio": 1.689922480620155, "no_speech_prob": 4.331217860453762e-05}, {"id": 245, "seek": 134616, "start": 1352.0800000000002, "end": 1361.1200000000001, "text": " notebook. Kaggle has a lot of Python packages installed but not necessarily", "tokens": [50364, 1778, 337, 26992, 281, 764, 48751, 22631, 307, 291, 500, 380, 362, 281, 3292, 466, 50514, 50514, 23771, 264, 1412, 412, 439, 309, 311, 3798, 456, 337, 291, 382, 2321, 382, 291, 1269, 264, 50660, 50660, 21060, 13, 48751, 22631, 575, 257, 688, 295, 15329, 17401, 8899, 457, 406, 4725, 51112, 51112, 439, 264, 2306, 291, 528, 293, 412, 264, 935, 286, 4114, 341, 436, 994, 380, 362, 51288, 51288, 41706, 8475, 42856, 7372, 337, 512, 1778, 370, 291, 393, 1009, 445, 3625, 51500, 51500, 1507, 13, 407, 291, 1062, 1604, 264, 1624, 43233, 1491, 1355, 341, 307, 406, 257, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.11133019740764911, "compression_ratio": 1.689922480620155, "no_speech_prob": 4.331217860453762e-05}, {"id": 246, "seek": 134616, "start": 1361.1200000000001, "end": 1364.64, "text": " all the ones you want and at the point I wrote this they didn't have", "tokens": [50364, 1778, 337, 26992, 281, 764, 48751, 22631, 307, 291, 500, 380, 362, 281, 3292, 466, 50514, 50514, 23771, 264, 1412, 412, 439, 309, 311, 3798, 456, 337, 291, 382, 2321, 382, 291, 1269, 264, 50660, 50660, 21060, 13, 48751, 22631, 575, 257, 688, 295, 15329, 17401, 8899, 457, 406, 4725, 51112, 51112, 439, 264, 2306, 291, 528, 293, 412, 264, 935, 286, 4114, 341, 436, 994, 380, 362, 51288, 51288, 41706, 8475, 42856, 7372, 337, 512, 1778, 370, 291, 393, 1009, 445, 3625, 51500, 51500, 1507, 13, 407, 291, 1062, 1604, 264, 1624, 43233, 1491, 1355, 341, 307, 406, 257, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.11133019740764911, "compression_ratio": 1.689922480620155, "no_speech_prob": 4.331217860453762e-05}, {"id": 247, "seek": 134616, "start": 1364.64, "end": 1368.88, "text": " hugging faces datasets package for some reason so you can always just install", "tokens": [50364, 1778, 337, 26992, 281, 764, 48751, 22631, 307, 291, 500, 380, 362, 281, 3292, 466, 50514, 50514, 23771, 264, 1412, 412, 439, 309, 311, 3798, 456, 337, 291, 382, 2321, 382, 291, 1269, 264, 50660, 50660, 21060, 13, 48751, 22631, 575, 257, 688, 295, 15329, 17401, 8899, 457, 406, 4725, 51112, 51112, 439, 264, 2306, 291, 528, 293, 412, 264, 935, 286, 4114, 341, 436, 994, 380, 362, 51288, 51288, 41706, 8475, 42856, 7372, 337, 512, 1778, 370, 291, 393, 1009, 445, 3625, 51500, 51500, 1507, 13, 407, 291, 1062, 1604, 264, 1624, 43233, 1491, 1355, 341, 307, 406, 257, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.11133019740764911, "compression_ratio": 1.689922480620155, "no_speech_prob": 4.331217860453762e-05}, {"id": 248, "seek": 134616, "start": 1368.88, "end": 1373.44, "text": " stuff. So you might remember the exclamation mark means this is not a", "tokens": [50364, 1778, 337, 26992, 281, 764, 48751, 22631, 307, 291, 500, 380, 362, 281, 3292, 466, 50514, 50514, 23771, 264, 1412, 412, 439, 309, 311, 3798, 456, 337, 291, 382, 2321, 382, 291, 1269, 264, 50660, 50660, 21060, 13, 48751, 22631, 575, 257, 688, 295, 15329, 17401, 8899, 457, 406, 4725, 51112, 51112, 439, 264, 2306, 291, 528, 293, 412, 264, 935, 286, 4114, 341, 436, 994, 380, 362, 51288, 51288, 41706, 8475, 42856, 7372, 337, 512, 1778, 370, 291, 393, 1009, 445, 3625, 51500, 51500, 1507, 13, 407, 291, 1062, 1604, 264, 1624, 43233, 1491, 1355, 341, 307, 406, 257, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.11133019740764911, "compression_ratio": 1.689922480620155, "no_speech_prob": 4.331217860453762e-05}, {"id": 249, "seek": 137344, "start": 1373.44, "end": 1378.68, "text": " Python command but a shell command a bash command but it's quite neat you can", "tokens": [50364, 15329, 5622, 457, 257, 8720, 5622, 257, 46183, 5622, 457, 309, 311, 1596, 10654, 291, 393, 50626, 50626, 754, 829, 46183, 16901, 1854, 15329, 4188, 1124, 370, 300, 311, 257, 1238, 1627, 50896, 50896, 707, 4282, 294, 43782, 13, 3996, 1627, 707, 4282, 294, 43782, 307, 300, 498, 291, 51322, 51322, 360, 764, 257, 46183, 5622, 411, 36657, 457, 291, 550, 528, 281, 8969, 264, 15768, 295, 257, 51566, 51566, 15329, 7006, 445, 20870, 309, 294, 34153, 370, 286, 600, 658, 257, 15329, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09899489084879558, "compression_ratio": 1.8067632850241546, "no_speech_prob": 0.00010552773164818063}, {"id": 250, "seek": 137344, "start": 1378.68, "end": 1384.0800000000002, "text": " even put bash commands inside Python conditionals so that's a pretty cool", "tokens": [50364, 15329, 5622, 457, 257, 8720, 5622, 257, 46183, 5622, 457, 309, 311, 1596, 10654, 291, 393, 50626, 50626, 754, 829, 46183, 16901, 1854, 15329, 4188, 1124, 370, 300, 311, 257, 1238, 1627, 50896, 50896, 707, 4282, 294, 43782, 13, 3996, 1627, 707, 4282, 294, 43782, 307, 300, 498, 291, 51322, 51322, 360, 764, 257, 46183, 5622, 411, 36657, 457, 291, 550, 528, 281, 8969, 264, 15768, 295, 257, 51566, 51566, 15329, 7006, 445, 20870, 309, 294, 34153, 370, 286, 600, 658, 257, 15329, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09899489084879558, "compression_ratio": 1.8067632850241546, "no_speech_prob": 0.00010552773164818063}, {"id": 251, "seek": 137344, "start": 1384.0800000000002, "end": 1392.6000000000001, "text": " little trick in notebooks. Another cool little trick in notebooks is that if you", "tokens": [50364, 15329, 5622, 457, 257, 8720, 5622, 257, 46183, 5622, 457, 309, 311, 1596, 10654, 291, 393, 50626, 50626, 754, 829, 46183, 16901, 1854, 15329, 4188, 1124, 370, 300, 311, 257, 1238, 1627, 50896, 50896, 707, 4282, 294, 43782, 13, 3996, 1627, 707, 4282, 294, 43782, 307, 300, 498, 291, 51322, 51322, 360, 764, 257, 46183, 5622, 411, 36657, 457, 291, 550, 528, 281, 8969, 264, 15768, 295, 257, 51566, 51566, 15329, 7006, 445, 20870, 309, 294, 34153, 370, 286, 600, 658, 257, 15329, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09899489084879558, "compression_ratio": 1.8067632850241546, "no_speech_prob": 0.00010552773164818063}, {"id": 252, "seek": 137344, "start": 1392.6000000000001, "end": 1397.48, "text": " do use a bash command like LS but you then want to insert the contents of a", "tokens": [50364, 15329, 5622, 457, 257, 8720, 5622, 257, 46183, 5622, 457, 309, 311, 1596, 10654, 291, 393, 50626, 50626, 754, 829, 46183, 16901, 1854, 15329, 4188, 1124, 370, 300, 311, 257, 1238, 1627, 50896, 50896, 707, 4282, 294, 43782, 13, 3996, 1627, 707, 4282, 294, 43782, 307, 300, 498, 291, 51322, 51322, 360, 764, 257, 46183, 5622, 411, 36657, 457, 291, 550, 528, 281, 8969, 264, 15768, 295, 257, 51566, 51566, 15329, 7006, 445, 20870, 309, 294, 34153, 370, 286, 600, 658, 257, 15329, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09899489084879558, "compression_ratio": 1.8067632850241546, "no_speech_prob": 0.00010552773164818063}, {"id": 253, "seek": 137344, "start": 1397.48, "end": 1401.76, "text": " Python variable just chuck it in parentheses so I've got a Python", "tokens": [50364, 15329, 5622, 457, 257, 8720, 5622, 257, 46183, 5622, 457, 309, 311, 1596, 10654, 291, 393, 50626, 50626, 754, 829, 46183, 16901, 1854, 15329, 4188, 1124, 370, 300, 311, 257, 1238, 1627, 50896, 50896, 707, 4282, 294, 43782, 13, 3996, 1627, 707, 4282, 294, 43782, 307, 300, 498, 291, 51322, 51322, 360, 764, 257, 46183, 5622, 411, 36657, 457, 291, 550, 528, 281, 8969, 264, 15768, 295, 257, 51566, 51566, 15329, 7006, 445, 20870, 309, 294, 34153, 370, 286, 600, 658, 257, 15329, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09899489084879558, "compression_ratio": 1.8067632850241546, "no_speech_prob": 0.00010552773164818063}, {"id": 254, "seek": 140176, "start": 1401.76, "end": 1409.08, "text": " variable called path and I can go LS path in parentheses and that will LS the", "tokens": [50364, 7006, 1219, 3100, 293, 286, 393, 352, 36657, 3100, 294, 34153, 293, 300, 486, 36657, 264, 50730, 50730, 15768, 295, 264, 15329, 7006, 3100, 370, 456, 311, 1071, 707, 4282, 337, 291, 13, 50954, 50954, 2798, 370, 562, 321, 36657, 300, 321, 393, 536, 300, 456, 311, 512, 48814, 7098, 370, 437, 286, 478, 51170, 51170, 516, 281, 360, 307, 733, 295, 747, 291, 807, 9810, 264, 1399, 264, 733, 295, 1399, 51392, 51392, 286, 291, 458, 1437, 807, 382, 291, 458, 562, 286, 700, 574, 412, 257, 6211, 370, 264, 51618, 51618, 700, 551, 307, 411, 1217, 1412, 992, 6451, 437, 311, 294, 309, 1392, 370, 309, 311, 658, 512, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11538574384606402, "compression_ratio": 1.7407407407407407, "no_speech_prob": 6.012775338604115e-05}, {"id": 255, "seek": 140176, "start": 1409.08, "end": 1413.56, "text": " contents of the Python variable path so there's another little trick for you.", "tokens": [50364, 7006, 1219, 3100, 293, 286, 393, 352, 36657, 3100, 294, 34153, 293, 300, 486, 36657, 264, 50730, 50730, 15768, 295, 264, 15329, 7006, 3100, 370, 456, 311, 1071, 707, 4282, 337, 291, 13, 50954, 50954, 2798, 370, 562, 321, 36657, 300, 321, 393, 536, 300, 456, 311, 512, 48814, 7098, 370, 437, 286, 478, 51170, 51170, 516, 281, 360, 307, 733, 295, 747, 291, 807, 9810, 264, 1399, 264, 733, 295, 1399, 51392, 51392, 286, 291, 458, 1437, 807, 382, 291, 458, 562, 286, 700, 574, 412, 257, 6211, 370, 264, 51618, 51618, 700, 551, 307, 411, 1217, 1412, 992, 6451, 437, 311, 294, 309, 1392, 370, 309, 311, 658, 512, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11538574384606402, "compression_ratio": 1.7407407407407407, "no_speech_prob": 6.012775338604115e-05}, {"id": 256, "seek": 140176, "start": 1413.56, "end": 1417.8799999999999, "text": " Alright so when we LS that we can see that there's some CSV files so what I'm", "tokens": [50364, 7006, 1219, 3100, 293, 286, 393, 352, 36657, 3100, 294, 34153, 293, 300, 486, 36657, 264, 50730, 50730, 15768, 295, 264, 15329, 7006, 3100, 370, 456, 311, 1071, 707, 4282, 337, 291, 13, 50954, 50954, 2798, 370, 562, 321, 36657, 300, 321, 393, 536, 300, 456, 311, 512, 48814, 7098, 370, 437, 286, 478, 51170, 51170, 516, 281, 360, 307, 733, 295, 747, 291, 807, 9810, 264, 1399, 264, 733, 295, 1399, 51392, 51392, 286, 291, 458, 1437, 807, 382, 291, 458, 562, 286, 700, 574, 412, 257, 6211, 370, 264, 51618, 51618, 700, 551, 307, 411, 1217, 1412, 992, 6451, 437, 311, 294, 309, 1392, 370, 309, 311, 658, 512, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11538574384606402, "compression_ratio": 1.7407407407407407, "no_speech_prob": 6.012775338604115e-05}, {"id": 257, "seek": 140176, "start": 1417.8799999999999, "end": 1422.32, "text": " going to do is kind of take you through roughly the process the kind of process", "tokens": [50364, 7006, 1219, 3100, 293, 286, 393, 352, 36657, 3100, 294, 34153, 293, 300, 486, 36657, 264, 50730, 50730, 15768, 295, 264, 15329, 7006, 3100, 370, 456, 311, 1071, 707, 4282, 337, 291, 13, 50954, 50954, 2798, 370, 562, 321, 36657, 300, 321, 393, 536, 300, 456, 311, 512, 48814, 7098, 370, 437, 286, 478, 51170, 51170, 516, 281, 360, 307, 733, 295, 747, 291, 807, 9810, 264, 1399, 264, 733, 295, 1399, 51392, 51392, 286, 291, 458, 1437, 807, 382, 291, 458, 562, 286, 700, 574, 412, 257, 6211, 370, 264, 51618, 51618, 700, 551, 307, 411, 1217, 1412, 992, 6451, 437, 311, 294, 309, 1392, 370, 309, 311, 658, 512, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11538574384606402, "compression_ratio": 1.7407407407407407, "no_speech_prob": 6.012775338604115e-05}, {"id": 258, "seek": 140176, "start": 1422.32, "end": 1426.84, "text": " I you know went through as you know when I first look at a competition so the", "tokens": [50364, 7006, 1219, 3100, 293, 286, 393, 352, 36657, 3100, 294, 34153, 293, 300, 486, 36657, 264, 50730, 50730, 15768, 295, 264, 15329, 7006, 3100, 370, 456, 311, 1071, 707, 4282, 337, 291, 13, 50954, 50954, 2798, 370, 562, 321, 36657, 300, 321, 393, 536, 300, 456, 311, 512, 48814, 7098, 370, 437, 286, 478, 51170, 51170, 516, 281, 360, 307, 733, 295, 747, 291, 807, 9810, 264, 1399, 264, 733, 295, 1399, 51392, 51392, 286, 291, 458, 1437, 807, 382, 291, 458, 562, 286, 700, 574, 412, 257, 6211, 370, 264, 51618, 51618, 700, 551, 307, 411, 1217, 1412, 992, 6451, 437, 311, 294, 309, 1392, 370, 309, 311, 658, 512, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11538574384606402, "compression_ratio": 1.7407407407407407, "no_speech_prob": 6.012775338604115e-05}, {"id": 259, "seek": 140176, "start": 1426.84, "end": 1430.6, "text": " first thing is like already data set indeed what's in it okay so it's got some", "tokens": [50364, 7006, 1219, 3100, 293, 286, 393, 352, 36657, 3100, 294, 34153, 293, 300, 486, 36657, 264, 50730, 50730, 15768, 295, 264, 15329, 7006, 3100, 370, 456, 311, 1071, 707, 4282, 337, 291, 13, 50954, 50954, 2798, 370, 562, 321, 36657, 300, 321, 393, 536, 300, 456, 311, 512, 48814, 7098, 370, 437, 286, 478, 51170, 51170, 516, 281, 360, 307, 733, 295, 747, 291, 807, 9810, 264, 1399, 264, 733, 295, 1399, 51392, 51392, 286, 291, 458, 1437, 807, 382, 291, 458, 562, 286, 700, 574, 412, 257, 6211, 370, 264, 51618, 51618, 700, 551, 307, 411, 1217, 1412, 992, 6451, 437, 311, 294, 309, 1392, 370, 309, 311, 658, 512, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.11538574384606402, "compression_ratio": 1.7407407407407407, "no_speech_prob": 6.012775338604115e-05}, {"id": 260, "seek": 143060, "start": 1430.6, "end": 1438.4399999999998, "text": " CSV files you know as well as looking at it here the other thing I would do is I", "tokens": [50364, 48814, 7098, 291, 458, 382, 731, 382, 1237, 412, 309, 510, 264, 661, 551, 286, 576, 360, 307, 286, 50756, 50756, 576, 352, 281, 264, 6211, 3144, 293, 498, 291, 352, 281, 1412, 257, 688, 295, 561, 10023, 51246, 51246, 670, 341, 597, 307, 257, 6237, 1558, 570, 309, 767, 5112, 291, 437, 264, 51446, 51446, 12334, 7006, 1355, 437, 264, 819, 7098, 366, 437, 264, 13766, 366, 51618, 51618], "temperature": 0.0, "avg_logprob": -0.06530868344836765, "compression_ratio": 1.612565445026178, "no_speech_prob": 3.535318319336511e-05}, {"id": 261, "seek": 143060, "start": 1438.4399999999998, "end": 1448.24, "text": " would go to the competition website and if you go to data a lot of people skip", "tokens": [50364, 48814, 7098, 291, 458, 382, 731, 382, 1237, 412, 309, 510, 264, 661, 551, 286, 576, 360, 307, 286, 50756, 50756, 576, 352, 281, 264, 6211, 3144, 293, 498, 291, 352, 281, 1412, 257, 688, 295, 561, 10023, 51246, 51246, 670, 341, 597, 307, 257, 6237, 1558, 570, 309, 767, 5112, 291, 437, 264, 51446, 51446, 12334, 7006, 1355, 437, 264, 819, 7098, 366, 437, 264, 13766, 366, 51618, 51618], "temperature": 0.0, "avg_logprob": -0.06530868344836765, "compression_ratio": 1.612565445026178, "no_speech_prob": 3.535318319336511e-05}, {"id": 262, "seek": 143060, "start": 1448.24, "end": 1452.24, "text": " over this which is a terrible idea because it actually tells you what the", "tokens": [50364, 48814, 7098, 291, 458, 382, 731, 382, 1237, 412, 309, 510, 264, 661, 551, 286, 576, 360, 307, 286, 50756, 50756, 576, 352, 281, 264, 6211, 3144, 293, 498, 291, 352, 281, 1412, 257, 688, 295, 561, 10023, 51246, 51246, 670, 341, 597, 307, 257, 6237, 1558, 570, 309, 767, 5112, 291, 437, 264, 51446, 51446, 12334, 7006, 1355, 437, 264, 819, 7098, 366, 437, 264, 13766, 366, 51618, 51618], "temperature": 0.0, "avg_logprob": -0.06530868344836765, "compression_ratio": 1.612565445026178, "no_speech_prob": 3.535318319336511e-05}, {"id": 263, "seek": 143060, "start": 1452.24, "end": 1455.6799999999998, "text": " dependent variable means what the different files are what the columns are", "tokens": [50364, 48814, 7098, 291, 458, 382, 731, 382, 1237, 412, 309, 510, 264, 661, 551, 286, 576, 360, 307, 286, 50756, 50756, 576, 352, 281, 264, 6211, 3144, 293, 498, 291, 352, 281, 1412, 257, 688, 295, 561, 10023, 51246, 51246, 670, 341, 597, 307, 257, 6237, 1558, 570, 309, 767, 5112, 291, 437, 264, 51446, 51446, 12334, 7006, 1355, 437, 264, 819, 7098, 366, 437, 264, 13766, 366, 51618, 51618], "temperature": 0.0, "avg_logprob": -0.06530868344836765, "compression_ratio": 1.612565445026178, "no_speech_prob": 3.535318319336511e-05}, {"id": 264, "seek": 145568, "start": 1455.68, "end": 1462.16, "text": " and so forth so don't just rely on looking at the data itself but look at", "tokens": [50364, 293, 370, 5220, 370, 500, 380, 445, 10687, 322, 1237, 412, 264, 1412, 2564, 457, 574, 412, 50688, 50688, 264, 1589, 300, 291, 434, 2212, 466, 264, 1412, 13, 407, 337, 48814, 7098, 11, 48814, 7098, 366, 51308, 51308, 22117, 12005, 4190, 370, 436, 434, 445, 2487, 7098, 365, 257, 22117, 1296, 1184, 51470, 51470, 2519, 293, 321, 393, 1401, 552, 1228, 4565, 296, 597, 337, 512, 1778, 1009, 1009, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.14217058387962547, "compression_ratio": 1.5906735751295338, "no_speech_prob": 2.3921133106341586e-05}, {"id": 265, "seek": 145568, "start": 1462.16, "end": 1474.5600000000002, "text": " the information that you're given about the data. So for CSV files, CSV files are", "tokens": [50364, 293, 370, 5220, 370, 500, 380, 445, 10687, 322, 1237, 412, 264, 1412, 2564, 457, 574, 412, 50688, 50688, 264, 1589, 300, 291, 434, 2212, 466, 264, 1412, 13, 407, 337, 48814, 7098, 11, 48814, 7098, 366, 51308, 51308, 22117, 12005, 4190, 370, 436, 434, 445, 2487, 7098, 365, 257, 22117, 1296, 1184, 51470, 51470, 2519, 293, 321, 393, 1401, 552, 1228, 4565, 296, 597, 337, 512, 1778, 1009, 1009, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.14217058387962547, "compression_ratio": 1.5906735751295338, "no_speech_prob": 2.3921133106341586e-05}, {"id": 266, "seek": 145568, "start": 1474.5600000000002, "end": 1477.8, "text": " comma separated values so they're just text files with a comma between each", "tokens": [50364, 293, 370, 5220, 370, 500, 380, 445, 10687, 322, 1237, 412, 264, 1412, 2564, 457, 574, 412, 50688, 50688, 264, 1589, 300, 291, 434, 2212, 466, 264, 1412, 13, 407, 337, 48814, 7098, 11, 48814, 7098, 366, 51308, 51308, 22117, 12005, 4190, 370, 436, 434, 445, 2487, 7098, 365, 257, 22117, 1296, 1184, 51470, 51470, 2519, 293, 321, 393, 1401, 552, 1228, 4565, 296, 597, 337, 512, 1778, 1009, 1009, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.14217058387962547, "compression_ratio": 1.5906735751295338, "no_speech_prob": 2.3921133106341586e-05}, {"id": 267, "seek": 145568, "start": 1477.8, "end": 1485.24, "text": " field and we can read them using pandas which for some reason always always", "tokens": [50364, 293, 370, 5220, 370, 500, 380, 445, 10687, 322, 1237, 412, 264, 1412, 2564, 457, 574, 412, 50688, 50688, 264, 1589, 300, 291, 434, 2212, 466, 264, 1412, 13, 407, 337, 48814, 7098, 11, 48814, 7098, 366, 51308, 51308, 22117, 12005, 4190, 370, 436, 434, 445, 2487, 7098, 365, 257, 22117, 1296, 1184, 51470, 51470, 2519, 293, 321, 393, 1401, 552, 1228, 4565, 296, 597, 337, 512, 1778, 1009, 1009, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.14217058387962547, "compression_ratio": 1.5906735751295338, "no_speech_prob": 2.3921133106341586e-05}, {"id": 268, "seek": 148524, "start": 1485.24, "end": 1493.4, "text": " called PD. Pandas is one of I guess like I'm trying to think probably like four", "tokens": [50364, 1219, 10464, 13, 16995, 296, 307, 472, 295, 286, 2041, 411, 286, 478, 1382, 281, 519, 1391, 411, 1451, 50772, 50772, 2141, 15148, 300, 291, 362, 281, 458, 281, 360, 1412, 3497, 294, 15329, 293, 51160, 51160], "temperature": 0.0, "avg_logprob": -0.14764560797275642, "compression_ratio": 1.2333333333333334, "no_speech_prob": 3.535439827828668e-05}, {"id": 269, "seek": 148524, "start": 1493.4, "end": 1501.16, "text": " key libraries that you have to know to do data science in Python and", "tokens": [50364, 1219, 10464, 13, 16995, 296, 307, 472, 295, 286, 2041, 411, 286, 478, 1382, 281, 519, 1391, 411, 1451, 50772, 50772, 2141, 15148, 300, 291, 362, 281, 458, 281, 360, 1412, 3497, 294, 15329, 293, 51160, 51160], "temperature": 0.0, "avg_logprob": -0.14764560797275642, "compression_ratio": 1.2333333333333334, "no_speech_prob": 3.535439827828668e-05}, {"id": 270, "seek": 150116, "start": 1501.16, "end": 1518.0, "text": " specifically those four libraries are numpy, matplotlib, pandas and pytorch.", "tokens": [50364, 4682, 729, 1451, 15148, 366, 1031, 8200, 11, 3803, 564, 310, 38270, 11, 4565, 296, 293, 25878, 284, 339, 13, 51206, 51206, 407, 1031, 8200, 307, 437, 321, 764, 337, 3875, 733, 295, 29054, 9410, 11, 3803, 564, 310, 38270, 51572, 51572, 321, 764, 337, 41178, 11, 4565, 296, 321, 764, 337, 8020, 295, 1412, 293, 25878, 284, 339, 321, 764, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.22029259710600882, "compression_ratio": 1.661764705882353, "no_speech_prob": 4.907979382551275e-05}, {"id": 271, "seek": 150116, "start": 1518.0, "end": 1525.3200000000002, "text": " So numpy is what we use for basic kind of numerical programming, matplotlib", "tokens": [50364, 4682, 729, 1451, 15148, 366, 1031, 8200, 11, 3803, 564, 310, 38270, 11, 4565, 296, 293, 25878, 284, 339, 13, 51206, 51206, 407, 1031, 8200, 307, 437, 321, 764, 337, 3875, 733, 295, 29054, 9410, 11, 3803, 564, 310, 38270, 51572, 51572, 321, 764, 337, 41178, 11, 4565, 296, 321, 764, 337, 8020, 295, 1412, 293, 25878, 284, 339, 321, 764, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.22029259710600882, "compression_ratio": 1.661764705882353, "no_speech_prob": 4.907979382551275e-05}, {"id": 272, "seek": 150116, "start": 1525.3200000000002, "end": 1530.4, "text": " we use for plotting, pandas we use for tables of data and pytorch we use.", "tokens": [50364, 4682, 729, 1451, 15148, 366, 1031, 8200, 11, 3803, 564, 310, 38270, 11, 4565, 296, 293, 25878, 284, 339, 13, 51206, 51206, 407, 1031, 8200, 307, 437, 321, 764, 337, 3875, 733, 295, 29054, 9410, 11, 3803, 564, 310, 38270, 51572, 51572, 321, 764, 337, 41178, 11, 4565, 296, 321, 764, 337, 8020, 295, 1412, 293, 25878, 284, 339, 321, 764, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.22029259710600882, "compression_ratio": 1.661764705882353, "no_speech_prob": 4.907979382551275e-05}, {"id": 273, "seek": 153040, "start": 1530.4, "end": 1543.3600000000001, "text": " for deep learning. Those are all covered in a fantastic book by the author is", "tokens": [50364, 337, 2452, 2539, 13, 3950, 366, 439, 5343, 294, 257, 5456, 1446, 538, 264, 3793, 307, 51012, 51012, 4565, 296, 597, 264, 777, 3037, 307, 767, 2435, 337, 1737, 286, 1697, 13, 51364, 51396, 15329, 337, 1412, 5215, 13, 407, 498, 291, 434, 406, 4963, 365, 613, 15148, 445, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.20446950084758256, "compression_ratio": 1.3865030674846626, "no_speech_prob": 7.481790817109868e-05}, {"id": 274, "seek": 153040, "start": 1543.3600000000001, "end": 1550.4, "text": " pandas which the new version is actually available for free I believe.", "tokens": [50364, 337, 2452, 2539, 13, 3950, 366, 439, 5343, 294, 257, 5456, 1446, 538, 264, 3793, 307, 51012, 51012, 4565, 296, 597, 264, 777, 3037, 307, 767, 2435, 337, 1737, 286, 1697, 13, 51364, 51396, 15329, 337, 1412, 5215, 13, 407, 498, 291, 434, 406, 4963, 365, 613, 15148, 445, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.20446950084758256, "compression_ratio": 1.3865030674846626, "no_speech_prob": 7.481790817109868e-05}, {"id": 275, "seek": 153040, "start": 1551.0400000000002, "end": 1558.1200000000001, "text": " Python for data analysis. So if you're not familiar with these libraries just", "tokens": [50364, 337, 2452, 2539, 13, 3950, 366, 439, 5343, 294, 257, 5456, 1446, 538, 264, 3793, 307, 51012, 51012, 4565, 296, 597, 264, 777, 3037, 307, 767, 2435, 337, 1737, 286, 1697, 13, 51364, 51396, 15329, 337, 1412, 5215, 13, 407, 498, 291, 434, 406, 4963, 365, 613, 15148, 445, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.20446950084758256, "compression_ratio": 1.3865030674846626, "no_speech_prob": 7.481790817109868e-05}, {"id": 276, "seek": 155812, "start": 1558.12, "end": 1561.1599999999999, "text": " read the whole book it doesn't take too long to get through and it's got lots of", "tokens": [50364, 1401, 264, 1379, 1446, 309, 1177, 380, 747, 886, 938, 281, 483, 807, 293, 309, 311, 658, 3195, 295, 50516, 50516, 1627, 6082, 293, 309, 311, 588, 49857, 13, 286, 360, 915, 257, 688, 295, 561, 884, 341, 1164, 50870, 50890, 2049, 286, 536, 561, 733, 295, 1382, 281, 3012, 2286, 293, 293, 528, 281, 312, 411, 1954, 286, 51166, 51166, 528, 281, 458, 577, 281, 411, 1884, 257, 777, 9482, 420, 1322, 257, 6218, 51370, 51370, 11150, 1185, 420, 2035, 293, 457, 309, 550, 4523, 484, 300, 436, 500, 380, 458, 51584, 51584, 577, 281, 764, 613, 8088, 15148, 13, 407, 309, 311, 1009, 665, 281, 312, 11928, 293, 312, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.11803194958230724, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.00016597550711594522}, {"id": 277, "seek": 155812, "start": 1561.1599999999999, "end": 1568.2399999999998, "text": " cool tips and it's very readable. I do find a lot of people doing this course", "tokens": [50364, 1401, 264, 1379, 1446, 309, 1177, 380, 747, 886, 938, 281, 483, 807, 293, 309, 311, 658, 3195, 295, 50516, 50516, 1627, 6082, 293, 309, 311, 588, 49857, 13, 286, 360, 915, 257, 688, 295, 561, 884, 341, 1164, 50870, 50890, 2049, 286, 536, 561, 733, 295, 1382, 281, 3012, 2286, 293, 293, 528, 281, 312, 411, 1954, 286, 51166, 51166, 528, 281, 458, 577, 281, 411, 1884, 257, 777, 9482, 420, 1322, 257, 6218, 51370, 51370, 11150, 1185, 420, 2035, 293, 457, 309, 550, 4523, 484, 300, 436, 500, 380, 458, 51584, 51584, 577, 281, 764, 613, 8088, 15148, 13, 407, 309, 311, 1009, 665, 281, 312, 11928, 293, 312, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.11803194958230724, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.00016597550711594522}, {"id": 278, "seek": 155812, "start": 1568.6399999999999, "end": 1574.1599999999999, "text": " often I see people kind of trying to jump ahead and and want to be like oh I", "tokens": [50364, 1401, 264, 1379, 1446, 309, 1177, 380, 747, 886, 938, 281, 483, 807, 293, 309, 311, 658, 3195, 295, 50516, 50516, 1627, 6082, 293, 309, 311, 588, 49857, 13, 286, 360, 915, 257, 688, 295, 561, 884, 341, 1164, 50870, 50890, 2049, 286, 536, 561, 733, 295, 1382, 281, 3012, 2286, 293, 293, 528, 281, 312, 411, 1954, 286, 51166, 51166, 528, 281, 458, 577, 281, 411, 1884, 257, 777, 9482, 420, 1322, 257, 6218, 51370, 51370, 11150, 1185, 420, 2035, 293, 457, 309, 550, 4523, 484, 300, 436, 500, 380, 458, 51584, 51584, 577, 281, 764, 613, 8088, 15148, 13, 407, 309, 311, 1009, 665, 281, 312, 11928, 293, 312, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.11803194958230724, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.00016597550711594522}, {"id": 279, "seek": 155812, "start": 1574.1599999999999, "end": 1578.2399999999998, "text": " want to know how to like create a new architecture or build a speech", "tokens": [50364, 1401, 264, 1379, 1446, 309, 1177, 380, 747, 886, 938, 281, 483, 807, 293, 309, 311, 658, 3195, 295, 50516, 50516, 1627, 6082, 293, 309, 311, 588, 49857, 13, 286, 360, 915, 257, 688, 295, 561, 884, 341, 1164, 50870, 50890, 2049, 286, 536, 561, 733, 295, 1382, 281, 3012, 2286, 293, 293, 528, 281, 312, 411, 1954, 286, 51166, 51166, 528, 281, 458, 577, 281, 411, 1884, 257, 777, 9482, 420, 1322, 257, 6218, 51370, 51370, 11150, 1185, 420, 2035, 293, 457, 309, 550, 4523, 484, 300, 436, 500, 380, 458, 51584, 51584, 577, 281, 764, 613, 8088, 15148, 13, 407, 309, 311, 1009, 665, 281, 312, 11928, 293, 312, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.11803194958230724, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.00016597550711594522}, {"id": 280, "seek": 155812, "start": 1578.2399999999998, "end": 1582.52, "text": " recognition system or whatever and but it then turns out that they don't know", "tokens": [50364, 1401, 264, 1379, 1446, 309, 1177, 380, 747, 886, 938, 281, 483, 807, 293, 309, 311, 658, 3195, 295, 50516, 50516, 1627, 6082, 293, 309, 311, 588, 49857, 13, 286, 360, 915, 257, 688, 295, 561, 884, 341, 1164, 50870, 50890, 2049, 286, 536, 561, 733, 295, 1382, 281, 3012, 2286, 293, 293, 528, 281, 312, 411, 1954, 286, 51166, 51166, 528, 281, 458, 577, 281, 411, 1884, 257, 777, 9482, 420, 1322, 257, 6218, 51370, 51370, 11150, 1185, 420, 2035, 293, 457, 309, 550, 4523, 484, 300, 436, 500, 380, 458, 51584, 51584, 577, 281, 764, 613, 8088, 15148, 13, 407, 309, 311, 1009, 665, 281, 312, 11928, 293, 312, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.11803194958230724, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.00016597550711594522}, {"id": 281, "seek": 155812, "start": 1582.52, "end": 1586.28, "text": " how to use these fundamental libraries. So it's always good to be bold and be", "tokens": [50364, 1401, 264, 1379, 1446, 309, 1177, 380, 747, 886, 938, 281, 483, 807, 293, 309, 311, 658, 3195, 295, 50516, 50516, 1627, 6082, 293, 309, 311, 588, 49857, 13, 286, 360, 915, 257, 688, 295, 561, 884, 341, 1164, 50870, 50890, 2049, 286, 536, 561, 733, 295, 1382, 281, 3012, 2286, 293, 293, 528, 281, 312, 411, 1954, 286, 51166, 51166, 528, 281, 458, 577, 281, 411, 1884, 257, 777, 9482, 420, 1322, 257, 6218, 51370, 51370, 11150, 1185, 420, 2035, 293, 457, 309, 550, 4523, 484, 300, 436, 500, 380, 458, 51584, 51584, 577, 281, 764, 613, 8088, 15148, 13, 407, 309, 311, 1009, 665, 281, 312, 11928, 293, 312, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.11803194958230724, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.00016597550711594522}, {"id": 282, "seek": 158628, "start": 1586.28, "end": 1589.84, "text": " trying to build things but do also take the time to you know make sure you", "tokens": [50364, 1382, 281, 1322, 721, 457, 360, 611, 747, 264, 565, 281, 291, 458, 652, 988, 291, 50542, 50542, 2413, 3760, 264, 700, 7318, 1446, 293, 1401, 412, 1935, 23843, 21765, 259, 2397, 311, 1446, 13, 50850, 50850, 663, 576, 312, 1547, 281, 534, 976, 291, 439, 264, 3875, 3601, 291, 643, 286, 519, 13, 51082, 51082, 407, 365, 4565, 296, 321, 393, 1401, 257, 48814, 3991, 293, 300, 7829, 746, 1219, 257, 51282, 51282, 1412, 3920, 597, 307, 445, 257, 3199, 295, 1412, 382, 291, 536, 13, 407, 586, 300, 321, 600, 658, 257, 1412, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.10844787443527068, "compression_ratio": 1.6, "no_speech_prob": 3.372936771484092e-05}, {"id": 283, "seek": 158628, "start": 1589.84, "end": 1596.0, "text": " finish reading the first AI book and read at least Wes McKinney's book.", "tokens": [50364, 1382, 281, 1322, 721, 457, 360, 611, 747, 264, 565, 281, 291, 458, 652, 988, 291, 50542, 50542, 2413, 3760, 264, 700, 7318, 1446, 293, 1401, 412, 1935, 23843, 21765, 259, 2397, 311, 1446, 13, 50850, 50850, 663, 576, 312, 1547, 281, 534, 976, 291, 439, 264, 3875, 3601, 291, 643, 286, 519, 13, 51082, 51082, 407, 365, 4565, 296, 321, 393, 1401, 257, 48814, 3991, 293, 300, 7829, 746, 1219, 257, 51282, 51282, 1412, 3920, 597, 307, 445, 257, 3199, 295, 1412, 382, 291, 536, 13, 407, 586, 300, 321, 600, 658, 257, 1412, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.10844787443527068, "compression_ratio": 1.6, "no_speech_prob": 3.372936771484092e-05}, {"id": 284, "seek": 158628, "start": 1596.0, "end": 1600.6399999999999, "text": " That would be enough to really give you all the basic knowledge you need I think.", "tokens": [50364, 1382, 281, 1322, 721, 457, 360, 611, 747, 264, 565, 281, 291, 458, 652, 988, 291, 50542, 50542, 2413, 3760, 264, 700, 7318, 1446, 293, 1401, 412, 1935, 23843, 21765, 259, 2397, 311, 1446, 13, 50850, 50850, 663, 576, 312, 1547, 281, 534, 976, 291, 439, 264, 3875, 3601, 291, 643, 286, 519, 13, 51082, 51082, 407, 365, 4565, 296, 321, 393, 1401, 257, 48814, 3991, 293, 300, 7829, 746, 1219, 257, 51282, 51282, 1412, 3920, 597, 307, 445, 257, 3199, 295, 1412, 382, 291, 536, 13, 407, 586, 300, 321, 600, 658, 257, 1412, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.10844787443527068, "compression_ratio": 1.6, "no_speech_prob": 3.372936771484092e-05}, {"id": 285, "seek": 158628, "start": 1600.6399999999999, "end": 1604.6399999999999, "text": " So with pandas we can read a CSV file and that creates something called a", "tokens": [50364, 1382, 281, 1322, 721, 457, 360, 611, 747, 264, 565, 281, 291, 458, 652, 988, 291, 50542, 50542, 2413, 3760, 264, 700, 7318, 1446, 293, 1401, 412, 1935, 23843, 21765, 259, 2397, 311, 1446, 13, 50850, 50850, 663, 576, 312, 1547, 281, 534, 976, 291, 439, 264, 3875, 3601, 291, 643, 286, 519, 13, 51082, 51082, 407, 365, 4565, 296, 321, 393, 1401, 257, 48814, 3991, 293, 300, 7829, 746, 1219, 257, 51282, 51282, 1412, 3920, 597, 307, 445, 257, 3199, 295, 1412, 382, 291, 536, 13, 407, 586, 300, 321, 600, 658, 257, 1412, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.10844787443527068, "compression_ratio": 1.6, "no_speech_prob": 3.372936771484092e-05}, {"id": 286, "seek": 158628, "start": 1604.6399999999999, "end": 1611.96, "text": " data frame which is just a table of data as you see. So now that we've got a data", "tokens": [50364, 1382, 281, 1322, 721, 457, 360, 611, 747, 264, 565, 281, 291, 458, 652, 988, 291, 50542, 50542, 2413, 3760, 264, 700, 7318, 1446, 293, 1401, 412, 1935, 23843, 21765, 259, 2397, 311, 1446, 13, 50850, 50850, 663, 576, 312, 1547, 281, 534, 976, 291, 439, 264, 3875, 3601, 291, 643, 286, 519, 13, 51082, 51082, 407, 365, 4565, 296, 321, 393, 1401, 257, 48814, 3991, 293, 300, 7829, 746, 1219, 257, 51282, 51282, 1412, 3920, 597, 307, 445, 257, 3199, 295, 1412, 382, 291, 536, 13, 407, 586, 300, 321, 600, 658, 257, 1412, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.10844787443527068, "compression_ratio": 1.6, "no_speech_prob": 3.372936771484092e-05}, {"id": 287, "seek": 161196, "start": 1611.96, "end": 1619.4, "text": " frame we can see what we're working with and when we ask when in Jupyter we just", "tokens": [50364, 3920, 321, 393, 536, 437, 321, 434, 1364, 365, 293, 562, 321, 1029, 562, 294, 22125, 88, 391, 321, 445, 50736, 50736, 829, 264, 1315, 295, 257, 7006, 19273, 257, 1412, 3920, 321, 483, 264, 700, 1732, 13241, 50876, 50876, 264, 1036, 1732, 13241, 293, 264, 2744, 370, 321, 600, 658, 11790, 2309, 4714, 1451, 51050, 51050, 3262, 25662, 1045, 13241, 13, 1033, 370, 661, 721, 286, 411, 281, 764, 337, 3701, 257, 51412, 51412, 1412, 3920, 307, 264, 6786, 3170, 13, 759, 291, 1320, 4090, 6915, 2657, 300, 486, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.1215675638077107, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.000277877930784598}, {"id": 288, "seek": 161196, "start": 1619.4, "end": 1622.2, "text": " put the name of a variable containing a data frame we get the first five rows", "tokens": [50364, 3920, 321, 393, 536, 437, 321, 434, 1364, 365, 293, 562, 321, 1029, 562, 294, 22125, 88, 391, 321, 445, 50736, 50736, 829, 264, 1315, 295, 257, 7006, 19273, 257, 1412, 3920, 321, 483, 264, 700, 1732, 13241, 50876, 50876, 264, 1036, 1732, 13241, 293, 264, 2744, 370, 321, 600, 658, 11790, 2309, 4714, 1451, 51050, 51050, 3262, 25662, 1045, 13241, 13, 1033, 370, 661, 721, 286, 411, 281, 764, 337, 3701, 257, 51412, 51412, 1412, 3920, 307, 264, 6786, 3170, 13, 759, 291, 1320, 4090, 6915, 2657, 300, 486, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.1215675638077107, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.000277877930784598}, {"id": 289, "seek": 161196, "start": 1622.2, "end": 1625.68, "text": " the last five rows and the size so we've got thirty six thousand four", "tokens": [50364, 3920, 321, 393, 536, 437, 321, 434, 1364, 365, 293, 562, 321, 1029, 562, 294, 22125, 88, 391, 321, 445, 50736, 50736, 829, 264, 1315, 295, 257, 7006, 19273, 257, 1412, 3920, 321, 483, 264, 700, 1732, 13241, 50876, 50876, 264, 1036, 1732, 13241, 293, 264, 2744, 370, 321, 600, 658, 11790, 2309, 4714, 1451, 51050, 51050, 3262, 25662, 1045, 13241, 13, 1033, 370, 661, 721, 286, 411, 281, 764, 337, 3701, 257, 51412, 51412, 1412, 3920, 307, 264, 6786, 3170, 13, 759, 291, 1320, 4090, 6915, 2657, 300, 486, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.1215675638077107, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.000277877930784598}, {"id": 290, "seek": 161196, "start": 1625.68, "end": 1632.92, "text": " hundred seventy three rows. Okay so other things I like to use for understanding a", "tokens": [50364, 3920, 321, 393, 536, 437, 321, 434, 1364, 365, 293, 562, 321, 1029, 562, 294, 22125, 88, 391, 321, 445, 50736, 50736, 829, 264, 1315, 295, 257, 7006, 19273, 257, 1412, 3920, 321, 483, 264, 700, 1732, 13241, 50876, 50876, 264, 1036, 1732, 13241, 293, 264, 2744, 370, 321, 600, 658, 11790, 2309, 4714, 1451, 51050, 51050, 3262, 25662, 1045, 13241, 13, 1033, 370, 661, 721, 286, 411, 281, 764, 337, 3701, 257, 51412, 51412, 1412, 3920, 307, 264, 6786, 3170, 13, 759, 291, 1320, 4090, 6915, 2657, 300, 486, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.1215675638077107, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.000277877930784598}, {"id": 291, "seek": 161196, "start": 1632.92, "end": 1640.64, "text": " data frame is the describe method. If you pass include equals object that will", "tokens": [50364, 3920, 321, 393, 536, 437, 321, 434, 1364, 365, 293, 562, 321, 1029, 562, 294, 22125, 88, 391, 321, 445, 50736, 50736, 829, 264, 1315, 295, 257, 7006, 19273, 257, 1412, 3920, 321, 483, 264, 700, 1732, 13241, 50876, 50876, 264, 1036, 1732, 13241, 293, 264, 2744, 370, 321, 600, 658, 11790, 2309, 4714, 1451, 51050, 51050, 3262, 25662, 1045, 13241, 13, 1033, 370, 661, 721, 286, 411, 281, 764, 337, 3701, 257, 51412, 51412, 1412, 3920, 307, 264, 6786, 3170, 13, 759, 291, 1320, 4090, 6915, 2657, 300, 486, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.1215675638077107, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.000277877930784598}, {"id": 292, "seek": 164064, "start": 1640.64, "end": 1643.76, "text": " describe that will describe basically all the kind of the string fields the", "tokens": [50364, 6786, 300, 486, 6786, 1936, 439, 264, 733, 295, 264, 6798, 7909, 264, 50520, 50520, 2107, 12, 77, 15583, 299, 7909, 13, 407, 294, 341, 1389, 456, 311, 1451, 295, 729, 293, 370, 291, 393, 536, 50842, 50842, 510, 300, 300, 18487, 2519, 321, 2956, 412, 456, 311, 767, 787, 1614, 10191, 3845, 4190, 51050, 51050, 1392, 370, 341, 551, 291, 393, 536, 300, 456, 311, 3195, 295, 30432, 484, 295, 2217, 51276, 51276, 8652, 11, 1360, 293, 370, 456, 311, 3195, 295, 30432, 13, 639, 307, 264, 881, 2689, 472, 309, 7038, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.1342147551861006, "compression_ratio": 1.751131221719457, "no_speech_prob": 9.023663005791605e-05}, {"id": 293, "seek": 164064, "start": 1643.76, "end": 1650.2, "text": " non-numeric fields. So in this case there's four of those and so you can see", "tokens": [50364, 6786, 300, 486, 6786, 1936, 439, 264, 733, 295, 264, 6798, 7909, 264, 50520, 50520, 2107, 12, 77, 15583, 299, 7909, 13, 407, 294, 341, 1389, 456, 311, 1451, 295, 729, 293, 370, 291, 393, 536, 50842, 50842, 510, 300, 300, 18487, 2519, 321, 2956, 412, 456, 311, 767, 787, 1614, 10191, 3845, 4190, 51050, 51050, 1392, 370, 341, 551, 291, 393, 536, 300, 456, 311, 3195, 295, 30432, 484, 295, 2217, 51276, 51276, 8652, 11, 1360, 293, 370, 456, 311, 3195, 295, 30432, 13, 639, 307, 264, 881, 2689, 472, 309, 7038, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.1342147551861006, "compression_ratio": 1.751131221719457, "no_speech_prob": 9.023663005791605e-05}, {"id": 294, "seek": 164064, "start": 1650.2, "end": 1654.3600000000001, "text": " here that that anchor field we looked at there's actually only 733 unique values", "tokens": [50364, 6786, 300, 486, 6786, 1936, 439, 264, 733, 295, 264, 6798, 7909, 264, 50520, 50520, 2107, 12, 77, 15583, 299, 7909, 13, 407, 294, 341, 1389, 456, 311, 1451, 295, 729, 293, 370, 291, 393, 536, 50842, 50842, 510, 300, 300, 18487, 2519, 321, 2956, 412, 456, 311, 767, 787, 1614, 10191, 3845, 4190, 51050, 51050, 1392, 370, 341, 551, 291, 393, 536, 300, 456, 311, 3195, 295, 30432, 484, 295, 2217, 51276, 51276, 8652, 11, 1360, 293, 370, 456, 311, 3195, 295, 30432, 13, 639, 307, 264, 881, 2689, 472, 309, 7038, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.1342147551861006, "compression_ratio": 1.751131221719457, "no_speech_prob": 9.023663005791605e-05}, {"id": 295, "seek": 164064, "start": 1654.3600000000001, "end": 1658.88, "text": " okay so this thing you can see that there's lots of repetition out of 30", "tokens": [50364, 6786, 300, 486, 6786, 1936, 439, 264, 733, 295, 264, 6798, 7909, 264, 50520, 50520, 2107, 12, 77, 15583, 299, 7909, 13, 407, 294, 341, 1389, 456, 311, 1451, 295, 729, 293, 370, 291, 393, 536, 50842, 50842, 510, 300, 300, 18487, 2519, 321, 2956, 412, 456, 311, 767, 787, 1614, 10191, 3845, 4190, 51050, 51050, 1392, 370, 341, 551, 291, 393, 536, 300, 456, 311, 3195, 295, 30432, 484, 295, 2217, 51276, 51276, 8652, 11, 1360, 293, 370, 456, 311, 3195, 295, 30432, 13, 639, 307, 264, 881, 2689, 472, 309, 7038, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.1342147551861006, "compression_ratio": 1.751131221719457, "no_speech_prob": 9.023663005791605e-05}, {"id": 296, "seek": 164064, "start": 1658.88, "end": 1667.0400000000002, "text": " 36,000 and so there's lots of repetition. This is the most common one it appears", "tokens": [50364, 6786, 300, 486, 6786, 1936, 439, 264, 733, 295, 264, 6798, 7909, 264, 50520, 50520, 2107, 12, 77, 15583, 299, 7909, 13, 407, 294, 341, 1389, 456, 311, 1451, 295, 729, 293, 370, 291, 393, 536, 50842, 50842, 510, 300, 300, 18487, 2519, 321, 2956, 412, 456, 311, 767, 787, 1614, 10191, 3845, 4190, 51050, 51050, 1392, 370, 341, 551, 291, 393, 536, 300, 456, 311, 3195, 295, 30432, 484, 295, 2217, 51276, 51276, 8652, 11, 1360, 293, 370, 456, 311, 3195, 295, 30432, 13, 639, 307, 264, 881, 2689, 472, 309, 7038, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.1342147551861006, "compression_ratio": 1.751131221719457, "no_speech_prob": 9.023663005791605e-05}, {"id": 297, "seek": 166704, "start": 1667.04, "end": 1673.0, "text": " 152 times and then context we also see lots of repetition there's 106 of those", "tokens": [50364, 2119, 17, 1413, 293, 550, 4319, 321, 611, 536, 3195, 295, 30432, 456, 311, 1266, 21, 295, 729, 50662, 50662, 30628, 13, 407, 341, 307, 257, 1481, 707, 3170, 321, 393, 536, 257, 688, 466, 264, 1412, 294, 294, 257, 50882, 50882, 21094, 293, 562, 286, 700, 1866, 341, 294, 341, 6211, 286, 1194, 731, 341, 307, 51098, 51098, 767, 406, 300, 709, 2856, 1412, 562, 291, 519, 466, 309, 300, 291, 458, 51350, 51350, 1184, 4166, 307, 588, 2099, 291, 458, 1045, 420, 1451, 2283, 534, 293, 3195, 295, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.10623672160696476, "compression_ratio": 1.72, "no_speech_prob": 7.842190098017454e-05}, {"id": 298, "seek": 166704, "start": 1673.0, "end": 1677.3999999999999, "text": " contexts. So this is a nice little method we can see a lot about the data in in a", "tokens": [50364, 2119, 17, 1413, 293, 550, 4319, 321, 611, 536, 3195, 295, 30432, 456, 311, 1266, 21, 295, 729, 50662, 50662, 30628, 13, 407, 341, 307, 257, 1481, 707, 3170, 321, 393, 536, 257, 688, 466, 264, 1412, 294, 294, 257, 50882, 50882, 21094, 293, 562, 286, 700, 1866, 341, 294, 341, 6211, 286, 1194, 731, 341, 307, 51098, 51098, 767, 406, 300, 709, 2856, 1412, 562, 291, 519, 466, 309, 300, 291, 458, 51350, 51350, 1184, 4166, 307, 588, 2099, 291, 458, 1045, 420, 1451, 2283, 534, 293, 3195, 295, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.10623672160696476, "compression_ratio": 1.72, "no_speech_prob": 7.842190098017454e-05}, {"id": 299, "seek": 166704, "start": 1677.3999999999999, "end": 1681.72, "text": " glance and when I first saw this in this competition I thought well this is", "tokens": [50364, 2119, 17, 1413, 293, 550, 4319, 321, 611, 536, 3195, 295, 30432, 456, 311, 1266, 21, 295, 729, 50662, 50662, 30628, 13, 407, 341, 307, 257, 1481, 707, 3170, 321, 393, 536, 257, 688, 466, 264, 1412, 294, 294, 257, 50882, 50882, 21094, 293, 562, 286, 700, 1866, 341, 294, 341, 6211, 286, 1194, 731, 341, 307, 51098, 51098, 767, 406, 300, 709, 2856, 1412, 562, 291, 519, 466, 309, 300, 291, 458, 51350, 51350, 1184, 4166, 307, 588, 2099, 291, 458, 1045, 420, 1451, 2283, 534, 293, 3195, 295, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.10623672160696476, "compression_ratio": 1.72, "no_speech_prob": 7.842190098017454e-05}, {"id": 300, "seek": 166704, "start": 1681.72, "end": 1686.76, "text": " actually not that much language data when you think about it that you know", "tokens": [50364, 2119, 17, 1413, 293, 550, 4319, 321, 611, 536, 3195, 295, 30432, 456, 311, 1266, 21, 295, 729, 50662, 50662, 30628, 13, 407, 341, 307, 257, 1481, 707, 3170, 321, 393, 536, 257, 688, 466, 264, 1412, 294, 294, 257, 50882, 50882, 21094, 293, 562, 286, 700, 1866, 341, 294, 341, 6211, 286, 1194, 731, 341, 307, 51098, 51098, 767, 406, 300, 709, 2856, 1412, 562, 291, 519, 466, 309, 300, 291, 458, 51350, 51350, 1184, 4166, 307, 588, 2099, 291, 458, 1045, 420, 1451, 2283, 534, 293, 3195, 295, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.10623672160696476, "compression_ratio": 1.72, "no_speech_prob": 7.842190098017454e-05}, {"id": 301, "seek": 166704, "start": 1686.76, "end": 1693.3999999999999, "text": " each document is very short you know three or four words really and lots of", "tokens": [50364, 2119, 17, 1413, 293, 550, 4319, 321, 611, 536, 3195, 295, 30432, 456, 311, 1266, 21, 295, 729, 50662, 50662, 30628, 13, 407, 341, 307, 257, 1481, 707, 3170, 321, 393, 536, 257, 688, 466, 264, 1412, 294, 294, 257, 50882, 50882, 21094, 293, 562, 286, 700, 1866, 341, 294, 341, 6211, 286, 1194, 731, 341, 307, 51098, 51098, 767, 406, 300, 709, 2856, 1412, 562, 291, 519, 466, 309, 300, 291, 458, 51350, 51350, 1184, 4166, 307, 588, 2099, 291, 458, 1045, 420, 1451, 2283, 534, 293, 3195, 295, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.10623672160696476, "compression_ratio": 1.72, "no_speech_prob": 7.842190098017454e-05}, {"id": 302, "seek": 169340, "start": 1693.4, "end": 1697.92, "text": " it is repeated. So that's like as I'm looking through it I'm thinking like", "tokens": [50364, 309, 307, 10477, 13, 407, 300, 311, 411, 382, 286, 478, 1237, 807, 309, 286, 478, 1953, 411, 50590, 50590, 437, 366, 512, 2141, 4122, 295, 341, 1412, 992, 293, 300, 576, 312, 746, 286, 1116, 312, 50754, 50754, 1953, 6076, 300, 311, 291, 458, 321, 600, 658, 281, 360, 257, 688, 365, 406, 588, 709, 3845, 50928, 50928, 1412, 510, 13, 407, 510, 311, 577, 321, 393, 445, 352, 2286, 293, 1884, 257, 2167, 6798, 411, 286, 51314, 51314, 7619, 597, 8306, 291, 458, 512, 733, 295, 2519, 3128, 1639, 1804, 264, 4319, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.09635234365657884, "compression_ratio": 1.6883116883116882, "no_speech_prob": 3.426567855058238e-05}, {"id": 303, "seek": 169340, "start": 1697.92, "end": 1701.2, "text": " what are some key features of this data set and that would be something I'd be", "tokens": [50364, 309, 307, 10477, 13, 407, 300, 311, 411, 382, 286, 478, 1237, 807, 309, 286, 478, 1953, 411, 50590, 50590, 437, 366, 512, 2141, 4122, 295, 341, 1412, 992, 293, 300, 576, 312, 746, 286, 1116, 312, 50754, 50754, 1953, 6076, 300, 311, 291, 458, 321, 600, 658, 281, 360, 257, 688, 365, 406, 588, 709, 3845, 50928, 50928, 1412, 510, 13, 407, 510, 311, 577, 321, 393, 445, 352, 2286, 293, 1884, 257, 2167, 6798, 411, 286, 51314, 51314, 7619, 597, 8306, 291, 458, 512, 733, 295, 2519, 3128, 1639, 1804, 264, 4319, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.09635234365657884, "compression_ratio": 1.6883116883116882, "no_speech_prob": 3.426567855058238e-05}, {"id": 304, "seek": 169340, "start": 1701.2, "end": 1704.68, "text": " thinking wow that's you know we've got to do a lot with not very much unique", "tokens": [50364, 309, 307, 10477, 13, 407, 300, 311, 411, 382, 286, 478, 1237, 807, 309, 286, 478, 1953, 411, 50590, 50590, 437, 366, 512, 2141, 4122, 295, 341, 1412, 992, 293, 300, 576, 312, 746, 286, 1116, 312, 50754, 50754, 1953, 6076, 300, 311, 291, 458, 321, 600, 658, 281, 360, 257, 688, 365, 406, 588, 709, 3845, 50928, 50928, 1412, 510, 13, 407, 510, 311, 577, 321, 393, 445, 352, 2286, 293, 1884, 257, 2167, 6798, 411, 286, 51314, 51314, 7619, 597, 8306, 291, 458, 512, 733, 295, 2519, 3128, 1639, 1804, 264, 4319, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.09635234365657884, "compression_ratio": 1.6883116883116882, "no_speech_prob": 3.426567855058238e-05}, {"id": 305, "seek": 169340, "start": 1704.68, "end": 1712.4, "text": " data here. So here's how we can just go ahead and create a single string like I", "tokens": [50364, 309, 307, 10477, 13, 407, 300, 311, 411, 382, 286, 478, 1237, 807, 309, 286, 478, 1953, 411, 50590, 50590, 437, 366, 512, 2141, 4122, 295, 341, 1412, 992, 293, 300, 576, 312, 746, 286, 1116, 312, 50754, 50754, 1953, 6076, 300, 311, 291, 458, 321, 600, 658, 281, 360, 257, 688, 365, 406, 588, 709, 3845, 50928, 50928, 1412, 510, 13, 407, 510, 311, 577, 321, 393, 445, 352, 2286, 293, 1884, 257, 2167, 6798, 411, 286, 51314, 51314, 7619, 597, 8306, 291, 458, 512, 733, 295, 2519, 3128, 1639, 1804, 264, 4319, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.09635234365657884, "compression_ratio": 1.6883116883116882, "no_speech_prob": 3.426567855058238e-05}, {"id": 306, "seek": 169340, "start": 1712.4, "end": 1718.8000000000002, "text": " described which contains you know some kind of field separator plus the context", "tokens": [50364, 309, 307, 10477, 13, 407, 300, 311, 411, 382, 286, 478, 1237, 807, 309, 286, 478, 1953, 411, 50590, 50590, 437, 366, 512, 2141, 4122, 295, 341, 1412, 992, 293, 300, 576, 312, 746, 286, 1116, 312, 50754, 50754, 1953, 6076, 300, 311, 291, 458, 321, 600, 658, 281, 360, 257, 688, 365, 406, 588, 709, 3845, 50928, 50928, 1412, 510, 13, 407, 510, 311, 577, 321, 393, 445, 352, 2286, 293, 1884, 257, 2167, 6798, 411, 286, 51314, 51314, 7619, 597, 8306, 291, 458, 512, 733, 295, 2519, 3128, 1639, 1804, 264, 4319, 51634, 51634], "temperature": 0.0, "avg_logprob": -0.09635234365657884, "compression_ratio": 1.6883116883116882, "no_speech_prob": 3.426567855058238e-05}, {"id": 307, "seek": 171880, "start": 1718.8, "end": 1726.52, "text": " the target and the anchor. So we're going to pop that into a field called input.", "tokens": [50364, 264, 3779, 293, 264, 18487, 13, 407, 321, 434, 516, 281, 1665, 300, 666, 257, 2519, 1219, 4846, 13, 50750, 50750, 6595, 4748, 3657, 294, 4565, 296, 307, 456, 311, 732, 2098, 295, 13761, 281, 257, 50964, 50964, 7738, 291, 393, 764, 3732, 26179, 293, 257, 6798, 281, 483, 264, 4846, 7738, 420, 291, 51246, 51246, 393, 445, 2387, 309, 382, 364, 19667, 13, 1133, 291, 434, 3287, 309, 291, 820, 1009, 764, 51470, 51470, 264, 6422, 294, 510, 13, 1133, 3760, 309, 291, 393, 764, 2139, 286, 3928, 281, 764, 341, 472, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.094509350884821, "compression_ratio": 1.7366071428571428, "no_speech_prob": 2.5464179998380132e-05}, {"id": 308, "seek": 171880, "start": 1726.52, "end": 1730.8, "text": " Something slightly weird in pandas is there's two ways of referring to a", "tokens": [50364, 264, 3779, 293, 264, 18487, 13, 407, 321, 434, 516, 281, 1665, 300, 666, 257, 2519, 1219, 4846, 13, 50750, 50750, 6595, 4748, 3657, 294, 4565, 296, 307, 456, 311, 732, 2098, 295, 13761, 281, 257, 50964, 50964, 7738, 291, 393, 764, 3732, 26179, 293, 257, 6798, 281, 483, 264, 4846, 7738, 420, 291, 51246, 51246, 393, 445, 2387, 309, 382, 364, 19667, 13, 1133, 291, 434, 3287, 309, 291, 820, 1009, 764, 51470, 51470, 264, 6422, 294, 510, 13, 1133, 3760, 309, 291, 393, 764, 2139, 286, 3928, 281, 764, 341, 472, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.094509350884821, "compression_ratio": 1.7366071428571428, "no_speech_prob": 2.5464179998380132e-05}, {"id": 309, "seek": 171880, "start": 1730.8, "end": 1736.44, "text": " column you can use square brackets and a string to get the input column or you", "tokens": [50364, 264, 3779, 293, 264, 18487, 13, 407, 321, 434, 516, 281, 1665, 300, 666, 257, 2519, 1219, 4846, 13, 50750, 50750, 6595, 4748, 3657, 294, 4565, 296, 307, 456, 311, 732, 2098, 295, 13761, 281, 257, 50964, 50964, 7738, 291, 393, 764, 3732, 26179, 293, 257, 6798, 281, 483, 264, 4846, 7738, 420, 291, 51246, 51246, 393, 445, 2387, 309, 382, 364, 19667, 13, 1133, 291, 434, 3287, 309, 291, 820, 1009, 764, 51470, 51470, 264, 6422, 294, 510, 13, 1133, 3760, 309, 291, 393, 764, 2139, 286, 3928, 281, 764, 341, 472, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.094509350884821, "compression_ratio": 1.7366071428571428, "no_speech_prob": 2.5464179998380132e-05}, {"id": 310, "seek": 171880, "start": 1736.44, "end": 1740.9199999999998, "text": " can just treat it as an attribute. When you're setting it you should always use", "tokens": [50364, 264, 3779, 293, 264, 18487, 13, 407, 321, 434, 516, 281, 1665, 300, 666, 257, 2519, 1219, 4846, 13, 50750, 50750, 6595, 4748, 3657, 294, 4565, 296, 307, 456, 311, 732, 2098, 295, 13761, 281, 257, 50964, 50964, 7738, 291, 393, 764, 3732, 26179, 293, 257, 6798, 281, 483, 264, 4846, 7738, 420, 291, 51246, 51246, 393, 445, 2387, 309, 382, 364, 19667, 13, 1133, 291, 434, 3287, 309, 291, 820, 1009, 764, 51470, 51470, 264, 6422, 294, 510, 13, 1133, 3760, 309, 291, 393, 764, 2139, 286, 3928, 281, 764, 341, 472, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.094509350884821, "compression_ratio": 1.7366071428571428, "no_speech_prob": 2.5464179998380132e-05}, {"id": 311, "seek": 171880, "start": 1740.9199999999998, "end": 1747.48, "text": " the forms in here. When reading it you can use either I tend to use this one", "tokens": [50364, 264, 3779, 293, 264, 18487, 13, 407, 321, 434, 516, 281, 1665, 300, 666, 257, 2519, 1219, 4846, 13, 50750, 50750, 6595, 4748, 3657, 294, 4565, 296, 307, 456, 311, 732, 2098, 295, 13761, 281, 257, 50964, 50964, 7738, 291, 393, 764, 3732, 26179, 293, 257, 6798, 281, 483, 264, 4846, 7738, 420, 291, 51246, 51246, 393, 445, 2387, 309, 382, 364, 19667, 13, 1133, 291, 434, 3287, 309, 291, 820, 1009, 764, 51470, 51470, 264, 6422, 294, 510, 13, 1133, 3760, 309, 291, 393, 764, 2139, 286, 3928, 281, 764, 341, 472, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.094509350884821, "compression_ratio": 1.7366071428571428, "no_speech_prob": 2.5464179998380132e-05}, {"id": 312, "seek": 174748, "start": 1747.48, "end": 1752.24, "text": " because it's less typing. So you can see now we've got this these concatenated", "tokens": [50364, 570, 309, 311, 1570, 18444, 13, 407, 291, 393, 536, 586, 321, 600, 658, 341, 613, 1588, 7186, 770, 50602, 50602, 13241, 370, 1378, 307, 264, 700, 1326, 13241, 13, 407, 321, 600, 586, 658, 512, 512, 8512, 281, 360, 51062, 51062, 426, 45196, 365, 13, 823, 264, 1154, 307, 382, 291, 458, 490, 264, 1036, 6898, 18161, 9590, 51364, 51364, 589, 365, 3547, 13, 492, 434, 516, 281, 747, 512, 3547, 293, 321, 434, 516, 281, 51596, 51596, 12972, 552, 538, 32284, 13, 492, 434, 516, 281, 7406, 264, 40019, 365, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12426866094271342, "compression_ratio": 1.6816143497757847, "no_speech_prob": 3.071626997552812e-05}, {"id": 313, "seek": 174748, "start": 1752.24, "end": 1761.44, "text": " rows so head is the first few rows. So we've now got some some documents to do", "tokens": [50364, 570, 309, 311, 1570, 18444, 13, 407, 291, 393, 536, 586, 321, 600, 658, 341, 613, 1588, 7186, 770, 50602, 50602, 13241, 370, 1378, 307, 264, 700, 1326, 13241, 13, 407, 321, 600, 586, 658, 512, 512, 8512, 281, 360, 51062, 51062, 426, 45196, 365, 13, 823, 264, 1154, 307, 382, 291, 458, 490, 264, 1036, 6898, 18161, 9590, 51364, 51364, 589, 365, 3547, 13, 492, 434, 516, 281, 747, 512, 3547, 293, 321, 434, 516, 281, 51596, 51596, 12972, 552, 538, 32284, 13, 492, 434, 516, 281, 7406, 264, 40019, 365, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12426866094271342, "compression_ratio": 1.6816143497757847, "no_speech_prob": 3.071626997552812e-05}, {"id": 314, "seek": 174748, "start": 1761.44, "end": 1767.48, "text": " NLP with. Now the problem is as you know from the last lesson neural networks", "tokens": [50364, 570, 309, 311, 1570, 18444, 13, 407, 291, 393, 536, 586, 321, 600, 658, 341, 613, 1588, 7186, 770, 50602, 50602, 13241, 370, 1378, 307, 264, 700, 1326, 13241, 13, 407, 321, 600, 586, 658, 512, 512, 8512, 281, 360, 51062, 51062, 426, 45196, 365, 13, 823, 264, 1154, 307, 382, 291, 458, 490, 264, 1036, 6898, 18161, 9590, 51364, 51364, 589, 365, 3547, 13, 492, 434, 516, 281, 747, 512, 3547, 293, 321, 434, 516, 281, 51596, 51596, 12972, 552, 538, 32284, 13, 492, 434, 516, 281, 7406, 264, 40019, 365, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12426866094271342, "compression_ratio": 1.6816143497757847, "no_speech_prob": 3.071626997552812e-05}, {"id": 315, "seek": 174748, "start": 1767.48, "end": 1772.1200000000001, "text": " work with numbers. We're going to take some numbers and we're going to", "tokens": [50364, 570, 309, 311, 1570, 18444, 13, 407, 291, 393, 536, 586, 321, 600, 658, 341, 613, 1588, 7186, 770, 50602, 50602, 13241, 370, 1378, 307, 264, 700, 1326, 13241, 13, 407, 321, 600, 586, 658, 512, 512, 8512, 281, 360, 51062, 51062, 426, 45196, 365, 13, 823, 264, 1154, 307, 382, 291, 458, 490, 264, 1036, 6898, 18161, 9590, 51364, 51364, 589, 365, 3547, 13, 492, 434, 516, 281, 747, 512, 3547, 293, 321, 434, 516, 281, 51596, 51596, 12972, 552, 538, 32284, 13, 492, 434, 516, 281, 7406, 264, 40019, 365, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12426866094271342, "compression_ratio": 1.6816143497757847, "no_speech_prob": 3.071626997552812e-05}, {"id": 316, "seek": 174748, "start": 1772.1200000000001, "end": 1777.3600000000001, "text": " multiply them by matrices. We're going to replace the negatives with", "tokens": [50364, 570, 309, 311, 1570, 18444, 13, 407, 291, 393, 536, 586, 321, 600, 658, 341, 613, 1588, 7186, 770, 50602, 50602, 13241, 370, 1378, 307, 264, 700, 1326, 13241, 13, 407, 321, 600, 586, 658, 512, 512, 8512, 281, 360, 51062, 51062, 426, 45196, 365, 13, 823, 264, 1154, 307, 382, 291, 458, 490, 264, 1036, 6898, 18161, 9590, 51364, 51364, 589, 365, 3547, 13, 492, 434, 516, 281, 747, 512, 3547, 293, 321, 434, 516, 281, 51596, 51596, 12972, 552, 538, 32284, 13, 492, 434, 516, 281, 7406, 264, 40019, 365, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.12426866094271342, "compression_ratio": 1.6816143497757847, "no_speech_prob": 3.071626997552812e-05}, {"id": 317, "seek": 177736, "start": 1777.36, "end": 1780.84, "text": " zeros and add them up. We're going to do that a few times. That's our neural", "tokens": [50364, 35193, 293, 909, 552, 493, 13, 492, 434, 516, 281, 360, 300, 257, 1326, 1413, 13, 663, 311, 527, 18161, 50538, 50538, 3209, 13, 2022, 512, 707, 34822, 457, 300, 311, 264, 3875, 1558, 13, 407, 577, 322, 4120, 360, 50788, 50788, 321, 360, 300, 337, 613, 13985, 30, 407, 456, 311, 1936, 732, 4439, 321, 434, 516, 281, 747, 13, 51200, 51200, 440, 700, 1823, 307, 281, 7472, 1184, 295, 613, 666, 22667, 13, 11036, 694, 366, 1936, 2283, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.10727247350356157, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.00014423670654650778}, {"id": 318, "seek": 177736, "start": 1780.84, "end": 1785.84, "text": " network. With some little wrinkles but that's the basic idea. So how on earth do", "tokens": [50364, 35193, 293, 909, 552, 493, 13, 492, 434, 516, 281, 360, 300, 257, 1326, 1413, 13, 663, 311, 527, 18161, 50538, 50538, 3209, 13, 2022, 512, 707, 34822, 457, 300, 311, 264, 3875, 1558, 13, 407, 577, 322, 4120, 360, 50788, 50788, 321, 360, 300, 337, 613, 13985, 30, 407, 456, 311, 1936, 732, 4439, 321, 434, 516, 281, 747, 13, 51200, 51200, 440, 700, 1823, 307, 281, 7472, 1184, 295, 613, 666, 22667, 13, 11036, 694, 366, 1936, 2283, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.10727247350356157, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.00014423670654650778}, {"id": 319, "seek": 177736, "start": 1785.84, "end": 1794.08, "text": " we do that for these strings? So there's basically two steps we're going to take.", "tokens": [50364, 35193, 293, 909, 552, 493, 13, 492, 434, 516, 281, 360, 300, 257, 1326, 1413, 13, 663, 311, 527, 18161, 50538, 50538, 3209, 13, 2022, 512, 707, 34822, 457, 300, 311, 264, 3875, 1558, 13, 407, 577, 322, 4120, 360, 50788, 50788, 321, 360, 300, 337, 613, 13985, 30, 407, 456, 311, 1936, 732, 4439, 321, 434, 516, 281, 747, 13, 51200, 51200, 440, 700, 1823, 307, 281, 7472, 1184, 295, 613, 666, 22667, 13, 11036, 694, 366, 1936, 2283, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.10727247350356157, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.00014423670654650778}, {"id": 320, "seek": 177736, "start": 1794.08, "end": 1801.6, "text": " The first step is to split each of these into tokens. Tokens are basically words.", "tokens": [50364, 35193, 293, 909, 552, 493, 13, 492, 434, 516, 281, 360, 300, 257, 1326, 1413, 13, 663, 311, 527, 18161, 50538, 50538, 3209, 13, 2022, 512, 707, 34822, 457, 300, 311, 264, 3875, 1558, 13, 407, 577, 322, 4120, 360, 50788, 50788, 321, 360, 300, 337, 613, 13985, 30, 407, 456, 311, 1936, 732, 4439, 321, 434, 516, 281, 747, 13, 51200, 51200, 440, 700, 1823, 307, 281, 7472, 1184, 295, 613, 666, 22667, 13, 11036, 694, 366, 1936, 2283, 13, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.10727247350356157, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.00014423670654650778}, {"id": 321, "seek": 180160, "start": 1801.6, "end": 1809.1599999999999, "text": " We're going to split it into words. There's a few problems with splitting", "tokens": [50364, 492, 434, 516, 281, 7472, 309, 666, 2283, 13, 821, 311, 257, 1326, 2740, 365, 30348, 50742, 50742, 721, 666, 2283, 13, 440, 700, 307, 300, 512, 8650, 411, 4649, 500, 380, 51002, 51002, 362, 2283, 13, 1610, 412, 1935, 3297, 406, 1901, 12005, 2283, 293, 294, 1186, 294, 51227, 51227, 4649, 309, 311, 2171, 309, 311, 257, 857, 34710, 281, 754, 584, 689, 257, 1349, 7338, 293, 5314, 13, 51466, 51466, 400, 512, 2283, 366, 733, 295, 406, 754, 264, 3755, 366, 406, 958, 281, 1184, 661, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.1623327398812899, "compression_ratio": 1.6491228070175439, "no_speech_prob": 6.302013207459822e-05}, {"id": 322, "seek": 180160, "start": 1809.1599999999999, "end": 1814.36, "text": " things into words. The first is that some languages like Chinese don't", "tokens": [50364, 492, 434, 516, 281, 7472, 309, 666, 2283, 13, 821, 311, 257, 1326, 2740, 365, 30348, 50742, 50742, 721, 666, 2283, 13, 440, 700, 307, 300, 512, 8650, 411, 4649, 500, 380, 51002, 51002, 362, 2283, 13, 1610, 412, 1935, 3297, 406, 1901, 12005, 2283, 293, 294, 1186, 294, 51227, 51227, 4649, 309, 311, 2171, 309, 311, 257, 857, 34710, 281, 754, 584, 689, 257, 1349, 7338, 293, 5314, 13, 51466, 51466, 400, 512, 2283, 366, 733, 295, 406, 754, 264, 3755, 366, 406, 958, 281, 1184, 661, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.1623327398812899, "compression_ratio": 1.6491228070175439, "no_speech_prob": 6.302013207459822e-05}, {"id": 323, "seek": 180160, "start": 1814.36, "end": 1818.86, "text": " have words. Or at least certainly not space separated words and in fact in", "tokens": [50364, 492, 434, 516, 281, 7472, 309, 666, 2283, 13, 821, 311, 257, 1326, 2740, 365, 30348, 50742, 50742, 721, 666, 2283, 13, 440, 700, 307, 300, 512, 8650, 411, 4649, 500, 380, 51002, 51002, 362, 2283, 13, 1610, 412, 1935, 3297, 406, 1901, 12005, 2283, 293, 294, 1186, 294, 51227, 51227, 4649, 309, 311, 2171, 309, 311, 257, 857, 34710, 281, 754, 584, 689, 257, 1349, 7338, 293, 5314, 13, 51466, 51466, 400, 512, 2283, 366, 733, 295, 406, 754, 264, 3755, 366, 406, 958, 281, 1184, 661, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.1623327398812899, "compression_ratio": 1.6491228070175439, "no_speech_prob": 6.302013207459822e-05}, {"id": 324, "seek": 180160, "start": 1818.86, "end": 1823.6399999999999, "text": " Chinese it's sometimes it's a bit fuzzy to even say where a word begins and ends.", "tokens": [50364, 492, 434, 516, 281, 7472, 309, 666, 2283, 13, 821, 311, 257, 1326, 2740, 365, 30348, 50742, 50742, 721, 666, 2283, 13, 440, 700, 307, 300, 512, 8650, 411, 4649, 500, 380, 51002, 51002, 362, 2283, 13, 1610, 412, 1935, 3297, 406, 1901, 12005, 2283, 293, 294, 1186, 294, 51227, 51227, 4649, 309, 311, 2171, 309, 311, 257, 857, 34710, 281, 754, 584, 689, 257, 1349, 7338, 293, 5314, 13, 51466, 51466, 400, 512, 2283, 366, 733, 295, 406, 754, 264, 3755, 366, 406, 958, 281, 1184, 661, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.1623327398812899, "compression_ratio": 1.6491228070175439, "no_speech_prob": 6.302013207459822e-05}, {"id": 325, "seek": 180160, "start": 1823.6399999999999, "end": 1828.52, "text": " And some words are kind of not even the pieces are not next to each other.", "tokens": [50364, 492, 434, 516, 281, 7472, 309, 666, 2283, 13, 821, 311, 257, 1326, 2740, 365, 30348, 50742, 50742, 721, 666, 2283, 13, 440, 700, 307, 300, 512, 8650, 411, 4649, 500, 380, 51002, 51002, 362, 2283, 13, 1610, 412, 1935, 3297, 406, 1901, 12005, 2283, 293, 294, 1186, 294, 51227, 51227, 4649, 309, 311, 2171, 309, 311, 257, 857, 34710, 281, 754, 584, 689, 257, 1349, 7338, 293, 5314, 13, 51466, 51466, 400, 512, 2283, 366, 733, 295, 406, 754, 264, 3755, 366, 406, 958, 281, 1184, 661, 13, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.1623327398812899, "compression_ratio": 1.6491228070175439, "no_speech_prob": 6.302013207459822e-05}, {"id": 326, "seek": 182852, "start": 1828.52, "end": 1833.0, "text": " Another reason is that what we're going to be doing is after we've split it into", "tokens": [50364, 3996, 1778, 307, 300, 437, 321, 434, 516, 281, 312, 884, 307, 934, 321, 600, 7472, 309, 666, 50588, 50588, 2283, 420, 746, 411, 2283, 321, 434, 516, 281, 312, 1242, 257, 1329, 295, 439, 295, 264, 50822, 50822, 3845, 2283, 300, 4204, 597, 307, 1219, 264, 19864, 13, 400, 633, 472, 295, 729, 51047, 51047, 3845, 2283, 307, 516, 281, 483, 257, 1230, 13, 1018, 291, 603, 536, 1780, 322, 264, 3801, 264, 51272, 51272, 19864, 264, 544, 4675, 307, 516, 281, 483, 1143, 13, 440, 544, 1412, 321, 603, 643, 281, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.10596749217239852, "compression_ratio": 1.8443396226415094, "no_speech_prob": 8.612930832896382e-05}, {"id": 327, "seek": 182852, "start": 1833.0, "end": 1837.68, "text": " words or something like words we're going to be getting a list of all of the", "tokens": [50364, 3996, 1778, 307, 300, 437, 321, 434, 516, 281, 312, 884, 307, 934, 321, 600, 7472, 309, 666, 50588, 50588, 2283, 420, 746, 411, 2283, 321, 434, 516, 281, 312, 1242, 257, 1329, 295, 439, 295, 264, 50822, 50822, 3845, 2283, 300, 4204, 597, 307, 1219, 264, 19864, 13, 400, 633, 472, 295, 729, 51047, 51047, 3845, 2283, 307, 516, 281, 483, 257, 1230, 13, 1018, 291, 603, 536, 1780, 322, 264, 3801, 264, 51272, 51272, 19864, 264, 544, 4675, 307, 516, 281, 483, 1143, 13, 440, 544, 1412, 321, 603, 643, 281, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.10596749217239852, "compression_ratio": 1.8443396226415094, "no_speech_prob": 8.612930832896382e-05}, {"id": 328, "seek": 182852, "start": 1837.68, "end": 1842.18, "text": " unique words that appear which is called the vocabulary. And every one of those", "tokens": [50364, 3996, 1778, 307, 300, 437, 321, 434, 516, 281, 312, 884, 307, 934, 321, 600, 7472, 309, 666, 50588, 50588, 2283, 420, 746, 411, 2283, 321, 434, 516, 281, 312, 1242, 257, 1329, 295, 439, 295, 264, 50822, 50822, 3845, 2283, 300, 4204, 597, 307, 1219, 264, 19864, 13, 400, 633, 472, 295, 729, 51047, 51047, 3845, 2283, 307, 516, 281, 483, 257, 1230, 13, 1018, 291, 603, 536, 1780, 322, 264, 3801, 264, 51272, 51272, 19864, 264, 544, 4675, 307, 516, 281, 483, 1143, 13, 440, 544, 1412, 321, 603, 643, 281, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.10596749217239852, "compression_ratio": 1.8443396226415094, "no_speech_prob": 8.612930832896382e-05}, {"id": 329, "seek": 182852, "start": 1842.18, "end": 1846.68, "text": " unique words is going to get a number. As you'll see later on the bigger the", "tokens": [50364, 3996, 1778, 307, 300, 437, 321, 434, 516, 281, 312, 884, 307, 934, 321, 600, 7472, 309, 666, 50588, 50588, 2283, 420, 746, 411, 2283, 321, 434, 516, 281, 312, 1242, 257, 1329, 295, 439, 295, 264, 50822, 50822, 3845, 2283, 300, 4204, 597, 307, 1219, 264, 19864, 13, 400, 633, 472, 295, 729, 51047, 51047, 3845, 2283, 307, 516, 281, 483, 257, 1230, 13, 1018, 291, 603, 536, 1780, 322, 264, 3801, 264, 51272, 51272, 19864, 264, 544, 4675, 307, 516, 281, 483, 1143, 13, 440, 544, 1412, 321, 603, 643, 281, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.10596749217239852, "compression_ratio": 1.8443396226415094, "no_speech_prob": 8.612930832896382e-05}, {"id": 330, "seek": 182852, "start": 1846.68, "end": 1852.68, "text": " vocabulary the more memory is going to get used. The more data we'll need to", "tokens": [50364, 3996, 1778, 307, 300, 437, 321, 434, 516, 281, 312, 884, 307, 934, 321, 600, 7472, 309, 666, 50588, 50588, 2283, 420, 746, 411, 2283, 321, 434, 516, 281, 312, 1242, 257, 1329, 295, 439, 295, 264, 50822, 50822, 3845, 2283, 300, 4204, 597, 307, 1219, 264, 19864, 13, 400, 633, 472, 295, 729, 51047, 51047, 3845, 2283, 307, 516, 281, 483, 257, 1230, 13, 1018, 291, 603, 536, 1780, 322, 264, 3801, 264, 51272, 51272, 19864, 264, 544, 4675, 307, 516, 281, 483, 1143, 13, 440, 544, 1412, 321, 603, 643, 281, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.10596749217239852, "compression_ratio": 1.8443396226415094, "no_speech_prob": 8.612930832896382e-05}, {"id": 331, "seek": 185268, "start": 1852.68, "end": 1864.92, "text": " train. In general we don't want a vocabulary to be too big. So instead", "tokens": [50364, 3847, 13, 682, 2674, 321, 500, 380, 528, 257, 19864, 281, 312, 886, 955, 13, 407, 2602, 50976, 50976, 13434, 561, 3928, 281, 14862, 1125, 666, 746, 1219, 1422, 2283, 597, 307, 51214, 51214, 3755, 295, 2283, 13, 407, 286, 603, 855, 291, 437, 309, 1542, 411, 13, 407, 264, 1399, 295, 6246, 309, 51424, 51424, 666, 4356, 6815, 411, 2283, 307, 1219, 14862, 2144, 293, 321, 818, 552, 22667, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11266056266990868, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00011406389239709824}, {"id": 332, "seek": 185268, "start": 1864.92, "end": 1869.68, "text": " nowadays people tend to tokenize into something called sub words which is", "tokens": [50364, 3847, 13, 682, 2674, 321, 500, 380, 528, 257, 19864, 281, 312, 886, 955, 13, 407, 2602, 50976, 50976, 13434, 561, 3928, 281, 14862, 1125, 666, 746, 1219, 1422, 2283, 597, 307, 51214, 51214, 3755, 295, 2283, 13, 407, 286, 603, 855, 291, 437, 309, 1542, 411, 13, 407, 264, 1399, 295, 6246, 309, 51424, 51424, 666, 4356, 6815, 411, 2283, 307, 1219, 14862, 2144, 293, 321, 818, 552, 22667, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11266056266990868, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00011406389239709824}, {"id": 333, "seek": 185268, "start": 1869.68, "end": 1873.88, "text": " pieces of words. So I'll show you what it looks like. So the process of turning it", "tokens": [50364, 3847, 13, 682, 2674, 321, 500, 380, 528, 257, 19864, 281, 312, 886, 955, 13, 407, 2602, 50976, 50976, 13434, 561, 3928, 281, 14862, 1125, 666, 746, 1219, 1422, 2283, 597, 307, 51214, 51214, 3755, 295, 2283, 13, 407, 286, 603, 855, 291, 437, 309, 1542, 411, 13, 407, 264, 1399, 295, 6246, 309, 51424, 51424, 666, 4356, 6815, 411, 2283, 307, 1219, 14862, 2144, 293, 321, 818, 552, 22667, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11266056266990868, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00011406389239709824}, {"id": 334, "seek": 185268, "start": 1873.88, "end": 1879.68, "text": " into smaller units like words is called tokenization and we call them tokens", "tokens": [50364, 3847, 13, 682, 2674, 321, 500, 380, 528, 257, 19864, 281, 312, 886, 955, 13, 407, 2602, 50976, 50976, 13434, 561, 3928, 281, 14862, 1125, 666, 746, 1219, 1422, 2283, 597, 307, 51214, 51214, 3755, 295, 2283, 13, 407, 286, 603, 855, 291, 437, 309, 1542, 411, 13, 407, 264, 1399, 295, 6246, 309, 51424, 51424, 666, 4356, 6815, 411, 2283, 307, 1219, 14862, 2144, 293, 321, 818, 552, 22667, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11266056266990868, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00011406389239709824}, {"id": 335, "seek": 187968, "start": 1879.68, "end": 1882.88, "text": " instead of words. A token is just like the more general concept of like whatever", "tokens": [50364, 2602, 295, 2283, 13, 316, 14862, 307, 445, 411, 264, 544, 2674, 3410, 295, 411, 2035, 50524, 50524, 321, 434, 30348, 309, 666, 13, 407, 321, 434, 516, 281, 483, 41706, 1851, 4088, 433, 293, 50846, 50846, 41706, 1851, 42856, 884, 527, 589, 337, 505, 13, 400, 370, 437, 321, 434, 516, 281, 360, 307, 321, 434, 51144, 51144, 516, 281, 1261, 527, 4565, 296, 1412, 3920, 666, 257, 666, 257, 41706, 1851, 42856, 28872, 13, 51478, 51478, 467, 311, 257, 857, 13181, 13, 9953, 51, 284, 339, 575, 257, 1508, 1219, 28872, 293, 41706, 1851, 575, 257, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.1311355852613262, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.00017387537809554487}, {"id": 336, "seek": 187968, "start": 1882.88, "end": 1889.3200000000002, "text": " we're splitting it into. So we're going to get hugging face transformers and", "tokens": [50364, 2602, 295, 2283, 13, 316, 14862, 307, 445, 411, 264, 544, 2674, 3410, 295, 411, 2035, 50524, 50524, 321, 434, 30348, 309, 666, 13, 407, 321, 434, 516, 281, 483, 41706, 1851, 4088, 433, 293, 50846, 50846, 41706, 1851, 42856, 884, 527, 589, 337, 505, 13, 400, 370, 437, 321, 434, 516, 281, 360, 307, 321, 434, 51144, 51144, 516, 281, 1261, 527, 4565, 296, 1412, 3920, 666, 257, 666, 257, 41706, 1851, 42856, 28872, 13, 51478, 51478, 467, 311, 257, 857, 13181, 13, 9953, 51, 284, 339, 575, 257, 1508, 1219, 28872, 293, 41706, 1851, 575, 257, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.1311355852613262, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.00017387537809554487}, {"id": 337, "seek": 187968, "start": 1889.3200000000002, "end": 1895.28, "text": " hugging face datasets doing our work for us. And so what we're going to do is we're", "tokens": [50364, 2602, 295, 2283, 13, 316, 14862, 307, 445, 411, 264, 544, 2674, 3410, 295, 411, 2035, 50524, 50524, 321, 434, 30348, 309, 666, 13, 407, 321, 434, 516, 281, 483, 41706, 1851, 4088, 433, 293, 50846, 50846, 41706, 1851, 42856, 884, 527, 589, 337, 505, 13, 400, 370, 437, 321, 434, 516, 281, 360, 307, 321, 434, 51144, 51144, 516, 281, 1261, 527, 4565, 296, 1412, 3920, 666, 257, 666, 257, 41706, 1851, 42856, 28872, 13, 51478, 51478, 467, 311, 257, 857, 13181, 13, 9953, 51, 284, 339, 575, 257, 1508, 1219, 28872, 293, 41706, 1851, 575, 257, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.1311355852613262, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.00017387537809554487}, {"id": 338, "seek": 187968, "start": 1895.28, "end": 1901.96, "text": " going to turn our pandas data frame into a into a hugging face datasets dataset.", "tokens": [50364, 2602, 295, 2283, 13, 316, 14862, 307, 445, 411, 264, 544, 2674, 3410, 295, 411, 2035, 50524, 50524, 321, 434, 30348, 309, 666, 13, 407, 321, 434, 516, 281, 483, 41706, 1851, 4088, 433, 293, 50846, 50846, 41706, 1851, 42856, 884, 527, 589, 337, 505, 13, 400, 370, 437, 321, 434, 516, 281, 360, 307, 321, 434, 51144, 51144, 516, 281, 1261, 527, 4565, 296, 1412, 3920, 666, 257, 666, 257, 41706, 1851, 42856, 28872, 13, 51478, 51478, 467, 311, 257, 857, 13181, 13, 9953, 51, 284, 339, 575, 257, 1508, 1219, 28872, 293, 41706, 1851, 575, 257, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.1311355852613262, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.00017387537809554487}, {"id": 339, "seek": 187968, "start": 1901.96, "end": 1908.92, "text": " It's a bit confusing. PyTorch has a class called dataset and hugging face has a", "tokens": [50364, 2602, 295, 2283, 13, 316, 14862, 307, 445, 411, 264, 544, 2674, 3410, 295, 411, 2035, 50524, 50524, 321, 434, 30348, 309, 666, 13, 407, 321, 434, 516, 281, 483, 41706, 1851, 4088, 433, 293, 50846, 50846, 41706, 1851, 42856, 884, 527, 589, 337, 505, 13, 400, 370, 437, 321, 434, 516, 281, 360, 307, 321, 434, 51144, 51144, 516, 281, 1261, 527, 4565, 296, 1412, 3920, 666, 257, 666, 257, 41706, 1851, 42856, 28872, 13, 51478, 51478, 467, 311, 257, 857, 13181, 13, 9953, 51, 284, 339, 575, 257, 1508, 1219, 28872, 293, 41706, 1851, 575, 257, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.1311355852613262, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.00017387537809554487}, {"id": 340, "seek": 190892, "start": 1908.92, "end": 1911.8400000000001, "text": " class called dataset and they're different things. Okay so this is a", "tokens": [50364, 1508, 1219, 28872, 293, 436, 434, 819, 721, 13, 1033, 370, 341, 307, 257, 50510, 50510, 41706, 1851, 28872, 13, 46892, 3249, 1851, 42856, 28872, 13, 407, 321, 393, 1261, 257, 1412, 50764, 50764, 3920, 666, 257, 28872, 445, 1228, 264, 490, 4565, 296, 3170, 13, 400, 370, 321, 600, 586, 658, 257, 51020, 51020, 28872, 13, 407, 498, 321, 747, 257, 574, 309, 445, 5112, 505, 309, 311, 658, 613, 4122, 13, 51280, 51280, 1033, 293, 1604, 4846, 307, 264, 472, 321, 445, 2942, 365, 264, 1588, 7186, 770, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.1875913904068318, "compression_ratio": 1.6990740740740742, "no_speech_prob": 3.071036553592421e-05}, {"id": 341, "seek": 190892, "start": 1911.8400000000001, "end": 1916.92, "text": " hugging face dataset. Hugging face datasets dataset. So we can turn a data", "tokens": [50364, 1508, 1219, 28872, 293, 436, 434, 819, 721, 13, 1033, 370, 341, 307, 257, 50510, 50510, 41706, 1851, 28872, 13, 46892, 3249, 1851, 42856, 28872, 13, 407, 321, 393, 1261, 257, 1412, 50764, 50764, 3920, 666, 257, 28872, 445, 1228, 264, 490, 4565, 296, 3170, 13, 400, 370, 321, 600, 586, 658, 257, 51020, 51020, 28872, 13, 407, 498, 321, 747, 257, 574, 309, 445, 5112, 505, 309, 311, 658, 613, 4122, 13, 51280, 51280, 1033, 293, 1604, 4846, 307, 264, 472, 321, 445, 2942, 365, 264, 1588, 7186, 770, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.1875913904068318, "compression_ratio": 1.6990740740740742, "no_speech_prob": 3.071036553592421e-05}, {"id": 342, "seek": 190892, "start": 1916.92, "end": 1922.04, "text": " frame into a dataset just using the from pandas method. And so we've now got a", "tokens": [50364, 1508, 1219, 28872, 293, 436, 434, 819, 721, 13, 1033, 370, 341, 307, 257, 50510, 50510, 41706, 1851, 28872, 13, 46892, 3249, 1851, 42856, 28872, 13, 407, 321, 393, 1261, 257, 1412, 50764, 50764, 3920, 666, 257, 28872, 445, 1228, 264, 490, 4565, 296, 3170, 13, 400, 370, 321, 600, 586, 658, 257, 51020, 51020, 28872, 13, 407, 498, 321, 747, 257, 574, 309, 445, 5112, 505, 309, 311, 658, 613, 4122, 13, 51280, 51280, 1033, 293, 1604, 4846, 307, 264, 472, 321, 445, 2942, 365, 264, 1588, 7186, 770, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.1875913904068318, "compression_ratio": 1.6990740740740742, "no_speech_prob": 3.071036553592421e-05}, {"id": 343, "seek": 190892, "start": 1922.04, "end": 1927.24, "text": " dataset. So if we take a look it just tells us it's got these features.", "tokens": [50364, 1508, 1219, 28872, 293, 436, 434, 819, 721, 13, 1033, 370, 341, 307, 257, 50510, 50510, 41706, 1851, 28872, 13, 46892, 3249, 1851, 42856, 28872, 13, 407, 321, 393, 1261, 257, 1412, 50764, 50764, 3920, 666, 257, 28872, 445, 1228, 264, 490, 4565, 296, 3170, 13, 400, 370, 321, 600, 586, 658, 257, 51020, 51020, 28872, 13, 407, 498, 321, 747, 257, 574, 309, 445, 5112, 505, 309, 311, 658, 613, 4122, 13, 51280, 51280, 1033, 293, 1604, 4846, 307, 264, 472, 321, 445, 2942, 365, 264, 1588, 7186, 770, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.1875913904068318, "compression_ratio": 1.6990740740740742, "no_speech_prob": 3.071036553592421e-05}, {"id": 344, "seek": 190892, "start": 1927.24, "end": 1932.68, "text": " Okay and remember input is the one we just created with the concatenated", "tokens": [50364, 1508, 1219, 28872, 293, 436, 434, 819, 721, 13, 1033, 370, 341, 307, 257, 50510, 50510, 41706, 1851, 28872, 13, 46892, 3249, 1851, 42856, 28872, 13, 407, 321, 393, 1261, 257, 1412, 50764, 50764, 3920, 666, 257, 28872, 445, 1228, 264, 490, 4565, 296, 3170, 13, 400, 370, 321, 600, 586, 658, 257, 51020, 51020, 28872, 13, 407, 498, 321, 747, 257, 574, 309, 445, 5112, 505, 309, 311, 658, 613, 4122, 13, 51280, 51280, 1033, 293, 1604, 4846, 307, 264, 472, 321, 445, 2942, 365, 264, 1588, 7186, 770, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.1875913904068318, "compression_ratio": 1.6990740740740742, "no_speech_prob": 3.071036553592421e-05}, {"id": 345, "seek": 193268, "start": 1932.68, "end": 1940.92, "text": " strings and here's those 36,000 rows. Okay so now we're going to do these two", "tokens": [50364, 13985, 293, 510, 311, 729, 8652, 11, 1360, 13241, 13, 1033, 370, 586, 321, 434, 516, 281, 360, 613, 732, 50776, 50776, 721, 13, 314, 8406, 2144, 597, 307, 281, 7472, 1184, 2487, 493, 666, 22667, 293, 550, 51000, 51000, 29054, 2144, 597, 307, 281, 1261, 1184, 14862, 666, 1080, 3845, 7348, 2361, 322, 689, 51236, 51236, 309, 307, 294, 264, 19864, 13, 440, 19864, 1604, 885, 264, 3845, 264, 1329, 295, 51464, 51464, 3845, 22667, 13, 823, 4098, 294, 341, 3233, 14862, 2144, 456, 311, 257, 688, 295, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.12355357344432544, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.00016343207971658558}, {"id": 346, "seek": 193268, "start": 1940.92, "end": 1945.4, "text": " things. Tokenization which is to split each text up into tokens and then", "tokens": [50364, 13985, 293, 510, 311, 729, 8652, 11, 1360, 13241, 13, 1033, 370, 586, 321, 434, 516, 281, 360, 613, 732, 50776, 50776, 721, 13, 314, 8406, 2144, 597, 307, 281, 7472, 1184, 2487, 493, 666, 22667, 293, 550, 51000, 51000, 29054, 2144, 597, 307, 281, 1261, 1184, 14862, 666, 1080, 3845, 7348, 2361, 322, 689, 51236, 51236, 309, 307, 294, 264, 19864, 13, 440, 19864, 1604, 885, 264, 3845, 264, 1329, 295, 51464, 51464, 3845, 22667, 13, 823, 4098, 294, 341, 3233, 14862, 2144, 456, 311, 257, 688, 295, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.12355357344432544, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.00016343207971658558}, {"id": 347, "seek": 193268, "start": 1945.4, "end": 1950.1200000000001, "text": " numericalization which is to turn each token into its unique ID based on where", "tokens": [50364, 13985, 293, 510, 311, 729, 8652, 11, 1360, 13241, 13, 1033, 370, 586, 321, 434, 516, 281, 360, 613, 732, 50776, 50776, 721, 13, 314, 8406, 2144, 597, 307, 281, 7472, 1184, 2487, 493, 666, 22667, 293, 550, 51000, 51000, 29054, 2144, 597, 307, 281, 1261, 1184, 14862, 666, 1080, 3845, 7348, 2361, 322, 689, 51236, 51236, 309, 307, 294, 264, 19864, 13, 440, 19864, 1604, 885, 264, 3845, 264, 1329, 295, 51464, 51464, 3845, 22667, 13, 823, 4098, 294, 341, 3233, 14862, 2144, 456, 311, 257, 688, 295, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.12355357344432544, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.00016343207971658558}, {"id": 348, "seek": 193268, "start": 1950.1200000000001, "end": 1954.68, "text": " it is in the vocabulary. The vocabulary remember being the unique the list of", "tokens": [50364, 13985, 293, 510, 311, 729, 8652, 11, 1360, 13241, 13, 1033, 370, 586, 321, 434, 516, 281, 360, 613, 732, 50776, 50776, 721, 13, 314, 8406, 2144, 597, 307, 281, 7472, 1184, 2487, 493, 666, 22667, 293, 550, 51000, 51000, 29054, 2144, 597, 307, 281, 1261, 1184, 14862, 666, 1080, 3845, 7348, 2361, 322, 689, 51236, 51236, 309, 307, 294, 264, 19864, 13, 440, 19864, 1604, 885, 264, 3845, 264, 1329, 295, 51464, 51464, 3845, 22667, 13, 823, 4098, 294, 341, 3233, 14862, 2144, 456, 311, 257, 688, 295, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.12355357344432544, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.00016343207971658558}, {"id": 349, "seek": 193268, "start": 1954.68, "end": 1962.64, "text": " unique tokens. Now particularly in this stage tokenization there's a lot of", "tokens": [50364, 13985, 293, 510, 311, 729, 8652, 11, 1360, 13241, 13, 1033, 370, 586, 321, 434, 516, 281, 360, 613, 732, 50776, 50776, 721, 13, 314, 8406, 2144, 597, 307, 281, 7472, 1184, 2487, 493, 666, 22667, 293, 550, 51000, 51000, 29054, 2144, 597, 307, 281, 1261, 1184, 14862, 666, 1080, 3845, 7348, 2361, 322, 689, 51236, 51236, 309, 307, 294, 264, 19864, 13, 440, 19864, 1604, 885, 264, 3845, 264, 1329, 295, 51464, 51464, 3845, 22667, 13, 823, 4098, 294, 341, 3233, 14862, 2144, 456, 311, 257, 688, 295, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.12355357344432544, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.00016343207971658558}, {"id": 350, "seek": 196264, "start": 1962.64, "end": 1969.16, "text": " little decisions that have to be made. The good news is you don't have to make", "tokens": [50364, 707, 5327, 300, 362, 281, 312, 1027, 13, 440, 665, 2583, 307, 291, 500, 380, 362, 281, 652, 50690, 50690, 552, 570, 2035, 659, 12, 17227, 2001, 2316, 291, 1143, 264, 561, 300, 659, 12, 17227, 2001, 309, 50972, 50972, 1027, 512, 5327, 293, 291, 434, 516, 281, 362, 281, 360, 2293, 264, 912, 551, 51192, 51192, 5911, 291, 603, 917, 493, 365, 257, 819, 19864, 281, 552, 293, 300, 311, 516, 281, 51392, 51392, 2082, 1203, 493, 13, 407, 300, 1355, 949, 291, 722, 14862, 3319, 291, 362, 281, 4536, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.0839662652266653, "compression_ratio": 1.7544642857142858, "no_speech_prob": 8.092046482488513e-05}, {"id": 351, "seek": 196264, "start": 1969.16, "end": 1974.8000000000002, "text": " them because whatever pre-trained model you used the people that pre-trained it", "tokens": [50364, 707, 5327, 300, 362, 281, 312, 1027, 13, 440, 665, 2583, 307, 291, 500, 380, 362, 281, 652, 50690, 50690, 552, 570, 2035, 659, 12, 17227, 2001, 2316, 291, 1143, 264, 561, 300, 659, 12, 17227, 2001, 309, 50972, 50972, 1027, 512, 5327, 293, 291, 434, 516, 281, 362, 281, 360, 2293, 264, 912, 551, 51192, 51192, 5911, 291, 603, 917, 493, 365, 257, 819, 19864, 281, 552, 293, 300, 311, 516, 281, 51392, 51392, 2082, 1203, 493, 13, 407, 300, 1355, 949, 291, 722, 14862, 3319, 291, 362, 281, 4536, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.0839662652266653, "compression_ratio": 1.7544642857142858, "no_speech_prob": 8.092046482488513e-05}, {"id": 352, "seek": 196264, "start": 1974.8000000000002, "end": 1979.2, "text": " made some decisions and you're going to have to do exactly the same thing", "tokens": [50364, 707, 5327, 300, 362, 281, 312, 1027, 13, 440, 665, 2583, 307, 291, 500, 380, 362, 281, 652, 50690, 50690, 552, 570, 2035, 659, 12, 17227, 2001, 2316, 291, 1143, 264, 561, 300, 659, 12, 17227, 2001, 309, 50972, 50972, 1027, 512, 5327, 293, 291, 434, 516, 281, 362, 281, 360, 2293, 264, 912, 551, 51192, 51192, 5911, 291, 603, 917, 493, 365, 257, 819, 19864, 281, 552, 293, 300, 311, 516, 281, 51392, 51392, 2082, 1203, 493, 13, 407, 300, 1355, 949, 291, 722, 14862, 3319, 291, 362, 281, 4536, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.0839662652266653, "compression_ratio": 1.7544642857142858, "no_speech_prob": 8.092046482488513e-05}, {"id": 353, "seek": 196264, "start": 1979.2, "end": 1983.2, "text": " otherwise you'll end up with a different vocabulary to them and that's going to", "tokens": [50364, 707, 5327, 300, 362, 281, 312, 1027, 13, 440, 665, 2583, 307, 291, 500, 380, 362, 281, 652, 50690, 50690, 552, 570, 2035, 659, 12, 17227, 2001, 2316, 291, 1143, 264, 561, 300, 659, 12, 17227, 2001, 309, 50972, 50972, 1027, 512, 5327, 293, 291, 434, 516, 281, 362, 281, 360, 2293, 264, 912, 551, 51192, 51192, 5911, 291, 603, 917, 493, 365, 257, 819, 19864, 281, 552, 293, 300, 311, 516, 281, 51392, 51392, 2082, 1203, 493, 13, 407, 300, 1355, 949, 291, 722, 14862, 3319, 291, 362, 281, 4536, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.0839662652266653, "compression_ratio": 1.7544642857142858, "no_speech_prob": 8.092046482488513e-05}, {"id": 354, "seek": 196264, "start": 1983.2, "end": 1987.96, "text": " mess everything up. So that means before you start tokenizing you have to decide", "tokens": [50364, 707, 5327, 300, 362, 281, 312, 1027, 13, 440, 665, 2583, 307, 291, 500, 380, 362, 281, 652, 50690, 50690, 552, 570, 2035, 659, 12, 17227, 2001, 2316, 291, 1143, 264, 561, 300, 659, 12, 17227, 2001, 309, 50972, 50972, 1027, 512, 5327, 293, 291, 434, 516, 281, 362, 281, 360, 2293, 264, 912, 551, 51192, 51192, 5911, 291, 603, 917, 493, 365, 257, 819, 19864, 281, 552, 293, 300, 311, 516, 281, 51392, 51392, 2082, 1203, 493, 13, 407, 300, 1355, 949, 291, 722, 14862, 3319, 291, 362, 281, 4536, 51630, 51630], "temperature": 0.0, "avg_logprob": -0.0839662652266653, "compression_ratio": 1.7544642857142858, "no_speech_prob": 8.092046482488513e-05}, {"id": 355, "seek": 198796, "start": 1987.96, "end": 1994.28, "text": " on what model to use. Hugging face transformers is a lot like Tim. It has a", "tokens": [50364, 322, 437, 2316, 281, 764, 13, 46892, 3249, 1851, 4088, 433, 307, 257, 688, 411, 7172, 13, 467, 575, 257, 50680, 50680, 6405, 295, 286, 1697, 6779, 295, 5245, 13, 286, 2041, 286, 4659, 380, 584, 41706, 1851, 51082, 51082, 4088, 433, 309, 311, 534, 264, 41706, 1851, 2316, 11838, 13, 16408, 11, 1360, 5245, 370, 754, 867, 51402, 51402, 544, 754, 813, 7172, 311, 3256, 5245, 293, 370, 613, 5245, 436, 10559, 294, 257, 1916, 295, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.11860610820628979, "compression_ratio": 1.6335078534031413, "no_speech_prob": 4.5395223423838615e-05}, {"id": 356, "seek": 198796, "start": 1994.28, "end": 2002.32, "text": " library of I believe hundreds of models. I guess I shouldn't say hugging face", "tokens": [50364, 322, 437, 2316, 281, 764, 13, 46892, 3249, 1851, 4088, 433, 307, 257, 688, 411, 7172, 13, 467, 575, 257, 50680, 50680, 6405, 295, 286, 1697, 6779, 295, 5245, 13, 286, 2041, 286, 4659, 380, 584, 41706, 1851, 51082, 51082, 4088, 433, 309, 311, 534, 264, 41706, 1851, 2316, 11838, 13, 16408, 11, 1360, 5245, 370, 754, 867, 51402, 51402, 544, 754, 813, 7172, 311, 3256, 5245, 293, 370, 613, 5245, 436, 10559, 294, 257, 1916, 295, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.11860610820628979, "compression_ratio": 1.6335078534031413, "no_speech_prob": 4.5395223423838615e-05}, {"id": 357, "seek": 198796, "start": 2002.32, "end": 2008.72, "text": " transformers it's really the hugging face model hub. 44,000 models so even many", "tokens": [50364, 322, 437, 2316, 281, 764, 13, 46892, 3249, 1851, 4088, 433, 307, 257, 688, 411, 7172, 13, 467, 575, 257, 50680, 50680, 6405, 295, 286, 1697, 6779, 295, 5245, 13, 286, 2041, 286, 4659, 380, 584, 41706, 1851, 51082, 51082, 4088, 433, 309, 311, 534, 264, 41706, 1851, 2316, 11838, 13, 16408, 11, 1360, 5245, 370, 754, 867, 51402, 51402, 544, 754, 813, 7172, 311, 3256, 5245, 293, 370, 613, 5245, 436, 10559, 294, 257, 1916, 295, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.11860610820628979, "compression_ratio": 1.6335078534031413, "no_speech_prob": 4.5395223423838615e-05}, {"id": 358, "seek": 198796, "start": 2008.72, "end": 2015.88, "text": " more even than Tim's image models and so these models they vary in a couple of", "tokens": [50364, 322, 437, 2316, 281, 764, 13, 46892, 3249, 1851, 4088, 433, 307, 257, 688, 411, 7172, 13, 467, 575, 257, 50680, 50680, 6405, 295, 286, 1697, 6779, 295, 5245, 13, 286, 2041, 286, 4659, 380, 584, 41706, 1851, 51082, 51082, 4088, 433, 309, 311, 534, 264, 41706, 1851, 2316, 11838, 13, 16408, 11, 1360, 5245, 370, 754, 867, 51402, 51402, 544, 754, 813, 7172, 311, 3256, 5245, 293, 370, 613, 5245, 436, 10559, 294, 257, 1916, 295, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.11860610820628979, "compression_ratio": 1.6335078534031413, "no_speech_prob": 4.5395223423838615e-05}, {"id": 359, "seek": 201588, "start": 2015.88, "end": 2021.68, "text": " ways. There's a variety of different architectures just like in Tim but then", "tokens": [50364, 2098, 13, 821, 311, 257, 5673, 295, 819, 6331, 1303, 445, 411, 294, 7172, 457, 550, 50654, 50654, 746, 597, 307, 819, 281, 7172, 307, 300, 1184, 295, 729, 6331, 1303, 393, 312, 50824, 50824, 8895, 322, 819, 1181, 79, 8355, 337, 12606, 819, 2740, 13, 407, 337, 1365, 51054, 51054, 286, 727, 2010, 20495, 293, 536, 498, 456, 311, 604, 659, 12, 17227, 2001, 20495, 456, 307, 13, 1033, 370, 51282, 51282, 456, 311, 257, 38142, 456, 311, 257, 1379, 688, 295, 659, 12, 17227, 2001, 20495, 5245, 13, 6998, 380, 300, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.10382344550693158, "compression_ratio": 1.7935779816513762, "no_speech_prob": 4.90796628582757e-05}, {"id": 360, "seek": 201588, "start": 2021.68, "end": 2025.0800000000002, "text": " something which is different to Tim is that each of those architectures can be", "tokens": [50364, 2098, 13, 821, 311, 257, 5673, 295, 819, 6331, 1303, 445, 411, 294, 7172, 457, 550, 50654, 50654, 746, 597, 307, 819, 281, 7172, 307, 300, 1184, 295, 729, 6331, 1303, 393, 312, 50824, 50824, 8895, 322, 819, 1181, 79, 8355, 337, 12606, 819, 2740, 13, 407, 337, 1365, 51054, 51054, 286, 727, 2010, 20495, 293, 536, 498, 456, 311, 604, 659, 12, 17227, 2001, 20495, 456, 307, 13, 1033, 370, 51282, 51282, 456, 311, 257, 38142, 456, 311, 257, 1379, 688, 295, 659, 12, 17227, 2001, 20495, 5245, 13, 6998, 380, 300, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.10382344550693158, "compression_ratio": 1.7935779816513762, "no_speech_prob": 4.90796628582757e-05}, {"id": 361, "seek": 201588, "start": 2025.0800000000002, "end": 2029.68, "text": " trained on different corpuses for solving different problems. So for example", "tokens": [50364, 2098, 13, 821, 311, 257, 5673, 295, 819, 6331, 1303, 445, 411, 294, 7172, 457, 550, 50654, 50654, 746, 597, 307, 819, 281, 7172, 307, 300, 1184, 295, 729, 6331, 1303, 393, 312, 50824, 50824, 8895, 322, 819, 1181, 79, 8355, 337, 12606, 819, 2740, 13, 407, 337, 1365, 51054, 51054, 286, 727, 2010, 20495, 293, 536, 498, 456, 311, 604, 659, 12, 17227, 2001, 20495, 456, 307, 13, 1033, 370, 51282, 51282, 456, 311, 257, 38142, 456, 311, 257, 1379, 688, 295, 659, 12, 17227, 2001, 20495, 5245, 13, 6998, 380, 300, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.10382344550693158, "compression_ratio": 1.7935779816513762, "no_speech_prob": 4.90796628582757e-05}, {"id": 362, "seek": 201588, "start": 2029.68, "end": 2034.24, "text": " I could type patent and see if there's any pre-trained patent there is. Okay so", "tokens": [50364, 2098, 13, 821, 311, 257, 5673, 295, 819, 6331, 1303, 445, 411, 294, 7172, 457, 550, 50654, 50654, 746, 597, 307, 819, 281, 7172, 307, 300, 1184, 295, 729, 6331, 1303, 393, 312, 50824, 50824, 8895, 322, 819, 1181, 79, 8355, 337, 12606, 819, 2740, 13, 407, 337, 1365, 51054, 51054, 286, 727, 2010, 20495, 293, 536, 498, 456, 311, 604, 659, 12, 17227, 2001, 20495, 456, 307, 13, 1033, 370, 51282, 51282, 456, 311, 257, 38142, 456, 311, 257, 1379, 688, 295, 659, 12, 17227, 2001, 20495, 5245, 13, 6998, 380, 300, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.10382344550693158, "compression_ratio": 1.7935779816513762, "no_speech_prob": 4.90796628582757e-05}, {"id": 363, "seek": 201588, "start": 2034.24, "end": 2039.2, "text": " there's a patents there's a whole lot of pre-trained patent models. Isn't that", "tokens": [50364, 2098, 13, 821, 311, 257, 5673, 295, 819, 6331, 1303, 445, 411, 294, 7172, 457, 550, 50654, 50654, 746, 597, 307, 819, 281, 7172, 307, 300, 1184, 295, 729, 6331, 1303, 393, 312, 50824, 50824, 8895, 322, 819, 1181, 79, 8355, 337, 12606, 819, 2740, 13, 407, 337, 1365, 51054, 51054, 286, 727, 2010, 20495, 293, 536, 498, 456, 311, 604, 659, 12, 17227, 2001, 20495, 456, 307, 13, 1033, 370, 51282, 51282, 456, 311, 257, 38142, 456, 311, 257, 1379, 688, 295, 659, 12, 17227, 2001, 20495, 5245, 13, 6998, 380, 300, 51530, 51530], "temperature": 0.0, "avg_logprob": -0.10382344550693158, "compression_ratio": 1.7935779816513762, "no_speech_prob": 4.90796628582757e-05}, {"id": 364, "seek": 203920, "start": 2039.2, "end": 2046.72, "text": " amazing? So quite often thanks to the hugging face model hub you can start", "tokens": [50364, 2243, 30, 407, 1596, 2049, 3231, 281, 264, 41706, 1851, 2316, 11838, 291, 393, 722, 50740, 50740, 428, 659, 12, 17227, 2001, 2316, 365, 746, 300, 311, 767, 1238, 2531, 281, 437, 50968, 50968, 291, 767, 528, 281, 360, 420, 412, 1935, 390, 8895, 322, 264, 912, 733, 295, 8512, 13, 51274, 51274, 10222, 848, 300, 456, 366, 512, 445, 5101, 1238, 665, 5245, 300, 589, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.09690514292035785, "compression_ratio": 1.577319587628866, "no_speech_prob": 1.0952137017739005e-05}, {"id": 365, "seek": 203920, "start": 2046.72, "end": 2051.28, "text": " your pre-trained model with something that's actually pretty similar to what", "tokens": [50364, 2243, 30, 407, 1596, 2049, 3231, 281, 264, 41706, 1851, 2316, 11838, 291, 393, 722, 50740, 50740, 428, 659, 12, 17227, 2001, 2316, 365, 746, 300, 311, 767, 1238, 2531, 281, 437, 50968, 50968, 291, 767, 528, 281, 360, 420, 412, 1935, 390, 8895, 322, 264, 912, 733, 295, 8512, 13, 51274, 51274, 10222, 848, 300, 456, 366, 512, 445, 5101, 1238, 665, 5245, 300, 589, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.09690514292035785, "compression_ratio": 1.577319587628866, "no_speech_prob": 1.0952137017739005e-05}, {"id": 366, "seek": 203920, "start": 2051.28, "end": 2057.4, "text": " you actually want to do or at least was trained on the same kind of documents.", "tokens": [50364, 2243, 30, 407, 1596, 2049, 3231, 281, 264, 41706, 1851, 2316, 11838, 291, 393, 722, 50740, 50740, 428, 659, 12, 17227, 2001, 2316, 365, 746, 300, 311, 767, 1238, 2531, 281, 437, 50968, 50968, 291, 767, 528, 281, 360, 420, 412, 1935, 390, 8895, 322, 264, 912, 733, 295, 8512, 13, 51274, 51274, 10222, 848, 300, 456, 366, 512, 445, 5101, 1238, 665, 5245, 300, 589, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.09690514292035785, "compression_ratio": 1.577319587628866, "no_speech_prob": 1.0952137017739005e-05}, {"id": 367, "seek": 203920, "start": 2057.4, "end": 2065.6, "text": " Having said that there are some just generally pretty good models that work", "tokens": [50364, 2243, 30, 407, 1596, 2049, 3231, 281, 264, 41706, 1851, 2316, 11838, 291, 393, 722, 50740, 50740, 428, 659, 12, 17227, 2001, 2316, 365, 746, 300, 311, 767, 1238, 2531, 281, 437, 50968, 50968, 291, 767, 528, 281, 360, 420, 412, 1935, 390, 8895, 322, 264, 912, 733, 295, 8512, 13, 51274, 51274, 10222, 848, 300, 456, 366, 512, 445, 5101, 1238, 665, 5245, 300, 589, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.09690514292035785, "compression_ratio": 1.577319587628866, "no_speech_prob": 1.0952137017739005e-05}, {"id": 368, "seek": 206560, "start": 2065.6, "end": 2073.72, "text": " for a lot of things a lot of the time and Deberta v3 is is certainly one of", "tokens": [50364, 337, 257, 688, 295, 721, 257, 688, 295, 264, 565, 293, 1346, 607, 1328, 371, 18, 307, 307, 3297, 472, 295, 50770, 50770, 729, 13, 639, 307, 257, 588, 777, 1859, 426, 45196, 575, 668, 411, 15667, 534, 4942, 51390, 51390, 337, 291, 458, 2674, 5022, 337, 787, 257, 1064, 420, 732, 689, 1646, 337, 3820, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.17186282475789388, "compression_ratio": 1.455128205128205, "no_speech_prob": 0.00012532078835647553}, {"id": 369, "seek": 206560, "start": 2073.72, "end": 2086.12, "text": " those. This is a very new area NLP has been like practically really effective", "tokens": [50364, 337, 257, 688, 295, 721, 257, 688, 295, 264, 565, 293, 1346, 607, 1328, 371, 18, 307, 307, 3297, 472, 295, 50770, 50770, 729, 13, 639, 307, 257, 588, 777, 1859, 426, 45196, 575, 668, 411, 15667, 534, 4942, 51390, 51390, 337, 291, 458, 2674, 5022, 337, 787, 257, 1064, 420, 732, 689, 1646, 337, 3820, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.17186282475789388, "compression_ratio": 1.455128205128205, "no_speech_prob": 0.00012532078835647553}, {"id": 370, "seek": 206560, "start": 2086.12, "end": 2094.16, "text": " for you know general users for only a year or two where else for computer", "tokens": [50364, 337, 257, 688, 295, 721, 257, 688, 295, 264, 565, 293, 1346, 607, 1328, 371, 18, 307, 307, 3297, 472, 295, 50770, 50770, 729, 13, 639, 307, 257, 588, 777, 1859, 426, 45196, 575, 668, 411, 15667, 534, 4942, 51390, 51390, 337, 291, 458, 2674, 5022, 337, 787, 257, 1064, 420, 732, 689, 1646, 337, 3820, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.17186282475789388, "compression_ratio": 1.455128205128205, "no_speech_prob": 0.00012532078835647553}, {"id": 371, "seek": 209416, "start": 2094.16, "end": 2098.24, "text": " vision it's been quite a while. So you'll see you'll find that like a lot of", "tokens": [50364, 5201, 309, 311, 668, 1596, 257, 1339, 13, 407, 291, 603, 536, 291, 603, 915, 300, 411, 257, 688, 295, 50568, 50568, 721, 3212, 380, 382, 1596, 731, 2901, 9207, 760, 13, 286, 500, 380, 362, 257, 3036, 281, 855, 291, 295, 50826, 50826, 597, 5245, 366, 264, 1151, 420, 264, 14573, 293, 264, 881, 8559, 293, 2035, 558, 13, 51056, 51056, 639, 257, 688, 295, 341, 1507, 307, 411, 1507, 300, 321, 434, 15213, 484, 382, 257, 1768, 51340, 51340, 1228, 26185, 411, 341, 294, 1186, 293, 341, 307, 472, 295, 264, 700, 426, 45196, 51590, 51590], "temperature": 0.0, "avg_logprob": -0.12910465240478516, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.00012927949137520045}, {"id": 372, "seek": 209416, "start": 2098.24, "end": 2103.3999999999996, "text": " things aren't as quite well bedded down. I don't have a picture to show you of", "tokens": [50364, 5201, 309, 311, 668, 1596, 257, 1339, 13, 407, 291, 603, 536, 291, 603, 915, 300, 411, 257, 688, 295, 50568, 50568, 721, 3212, 380, 382, 1596, 731, 2901, 9207, 760, 13, 286, 500, 380, 362, 257, 3036, 281, 855, 291, 295, 50826, 50826, 597, 5245, 366, 264, 1151, 420, 264, 14573, 293, 264, 881, 8559, 293, 2035, 558, 13, 51056, 51056, 639, 257, 688, 295, 341, 1507, 307, 411, 1507, 300, 321, 434, 15213, 484, 382, 257, 1768, 51340, 51340, 1228, 26185, 411, 341, 294, 1186, 293, 341, 307, 472, 295, 264, 700, 426, 45196, 51590, 51590], "temperature": 0.0, "avg_logprob": -0.12910465240478516, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.00012927949137520045}, {"id": 373, "seek": 209416, "start": 2103.3999999999996, "end": 2108.0, "text": " which models are the best or the fastest and the most accurate and whatever right.", "tokens": [50364, 5201, 309, 311, 668, 1596, 257, 1339, 13, 407, 291, 603, 536, 291, 603, 915, 300, 411, 257, 688, 295, 50568, 50568, 721, 3212, 380, 382, 1596, 731, 2901, 9207, 760, 13, 286, 500, 380, 362, 257, 3036, 281, 855, 291, 295, 50826, 50826, 597, 5245, 366, 264, 1151, 420, 264, 14573, 293, 264, 881, 8559, 293, 2035, 558, 13, 51056, 51056, 639, 257, 688, 295, 341, 1507, 307, 411, 1507, 300, 321, 434, 15213, 484, 382, 257, 1768, 51340, 51340, 1228, 26185, 411, 341, 294, 1186, 293, 341, 307, 472, 295, 264, 700, 426, 45196, 51590, 51590], "temperature": 0.0, "avg_logprob": -0.12910465240478516, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.00012927949137520045}, {"id": 374, "seek": 209416, "start": 2108.0, "end": 2113.68, "text": " This a lot of this stuff is like stuff that we're figuring out as a community", "tokens": [50364, 5201, 309, 311, 668, 1596, 257, 1339, 13, 407, 291, 603, 536, 291, 603, 915, 300, 411, 257, 688, 295, 50568, 50568, 721, 3212, 380, 382, 1596, 731, 2901, 9207, 760, 13, 286, 500, 380, 362, 257, 3036, 281, 855, 291, 295, 50826, 50826, 597, 5245, 366, 264, 1151, 420, 264, 14573, 293, 264, 881, 8559, 293, 2035, 558, 13, 51056, 51056, 639, 257, 688, 295, 341, 1507, 307, 411, 1507, 300, 321, 434, 15213, 484, 382, 257, 1768, 51340, 51340, 1228, 26185, 411, 341, 294, 1186, 293, 341, 307, 472, 295, 264, 700, 426, 45196, 51590, 51590], "temperature": 0.0, "avg_logprob": -0.12910465240478516, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.00012927949137520045}, {"id": 375, "seek": 209416, "start": 2113.68, "end": 2118.68, "text": " using competitions like this in fact and this is one of the first NLP", "tokens": [50364, 5201, 309, 311, 668, 1596, 257, 1339, 13, 407, 291, 603, 536, 291, 603, 915, 300, 411, 257, 688, 295, 50568, 50568, 721, 3212, 380, 382, 1596, 731, 2901, 9207, 760, 13, 286, 500, 380, 362, 257, 3036, 281, 855, 291, 295, 50826, 50826, 597, 5245, 366, 264, 1151, 420, 264, 14573, 293, 264, 881, 8559, 293, 2035, 558, 13, 51056, 51056, 639, 257, 688, 295, 341, 1507, 307, 411, 1507, 300, 321, 434, 15213, 484, 382, 257, 1768, 51340, 51340, 1228, 26185, 411, 341, 294, 1186, 293, 341, 307, 472, 295, 264, 700, 426, 45196, 51590, 51590], "temperature": 0.0, "avg_logprob": -0.12910465240478516, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.00012927949137520045}, {"id": 376, "seek": 211868, "start": 2118.68, "end": 2124.2799999999997, "text": " competitions actually in the kind of modern NLP era. So you know we've been", "tokens": [50364, 26185, 767, 294, 264, 733, 295, 4363, 426, 45196, 4249, 13, 407, 291, 458, 321, 600, 668, 50644, 50644, 7601, 613, 26185, 8185, 293, 1338, 286, 393, 980, 291, 300, 1346, 607, 1328, 307, 50878, 50878, 767, 257, 534, 665, 2891, 935, 337, 257, 688, 295, 721, 370, 300, 311, 983, 321, 600, 51086, 51086, 6183, 309, 13, 407, 321, 1888, 527, 2316, 293, 445, 411, 294, 7172, 337, 3256, 291, 458, 527, 51300, 51300, 5245, 456, 311, 2049, 516, 281, 312, 257, 1359, 257, 6399, 257, 2416, 293, 295, 1164, 321, 820, 51514, 51514, 722, 365, 1359, 558, 570, 1359, 307, 516, 281, 312, 4663, 281, 3847, 321, 434, 516, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.11737423772397249, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.00032485564588569105}, {"id": 377, "seek": 211868, "start": 2124.2799999999997, "end": 2128.96, "text": " studying these competitions closely and yeah I can tell you that Deberta is", "tokens": [50364, 26185, 767, 294, 264, 733, 295, 4363, 426, 45196, 4249, 13, 407, 291, 458, 321, 600, 668, 50644, 50644, 7601, 613, 26185, 8185, 293, 1338, 286, 393, 980, 291, 300, 1346, 607, 1328, 307, 50878, 50878, 767, 257, 534, 665, 2891, 935, 337, 257, 688, 295, 721, 370, 300, 311, 983, 321, 600, 51086, 51086, 6183, 309, 13, 407, 321, 1888, 527, 2316, 293, 445, 411, 294, 7172, 337, 3256, 291, 458, 527, 51300, 51300, 5245, 456, 311, 2049, 516, 281, 312, 257, 1359, 257, 6399, 257, 2416, 293, 295, 1164, 321, 820, 51514, 51514, 722, 365, 1359, 558, 570, 1359, 307, 516, 281, 312, 4663, 281, 3847, 321, 434, 516, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.11737423772397249, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.00032485564588569105}, {"id": 378, "seek": 211868, "start": 2128.96, "end": 2133.12, "text": " actually a really good starting point for a lot of things so that's why we've", "tokens": [50364, 26185, 767, 294, 264, 733, 295, 4363, 426, 45196, 4249, 13, 407, 291, 458, 321, 600, 668, 50644, 50644, 7601, 613, 26185, 8185, 293, 1338, 286, 393, 980, 291, 300, 1346, 607, 1328, 307, 50878, 50878, 767, 257, 534, 665, 2891, 935, 337, 257, 688, 295, 721, 370, 300, 311, 983, 321, 600, 51086, 51086, 6183, 309, 13, 407, 321, 1888, 527, 2316, 293, 445, 411, 294, 7172, 337, 3256, 291, 458, 527, 51300, 51300, 5245, 456, 311, 2049, 516, 281, 312, 257, 1359, 257, 6399, 257, 2416, 293, 295, 1164, 321, 820, 51514, 51514, 722, 365, 1359, 558, 570, 1359, 307, 516, 281, 312, 4663, 281, 3847, 321, 434, 516, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.11737423772397249, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.00032485564588569105}, {"id": 379, "seek": 211868, "start": 2133.12, "end": 2137.3999999999996, "text": " picked it. So we pick our model and just like in Tim for image you know our", "tokens": [50364, 26185, 767, 294, 264, 733, 295, 4363, 426, 45196, 4249, 13, 407, 291, 458, 321, 600, 668, 50644, 50644, 7601, 613, 26185, 8185, 293, 1338, 286, 393, 980, 291, 300, 1346, 607, 1328, 307, 50878, 50878, 767, 257, 534, 665, 2891, 935, 337, 257, 688, 295, 721, 370, 300, 311, 983, 321, 600, 51086, 51086, 6183, 309, 13, 407, 321, 1888, 527, 2316, 293, 445, 411, 294, 7172, 337, 3256, 291, 458, 527, 51300, 51300, 5245, 456, 311, 2049, 516, 281, 312, 257, 1359, 257, 6399, 257, 2416, 293, 295, 1164, 321, 820, 51514, 51514, 722, 365, 1359, 558, 570, 1359, 307, 516, 281, 312, 4663, 281, 3847, 321, 434, 516, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.11737423772397249, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.00032485564588569105}, {"id": 380, "seek": 211868, "start": 2137.3999999999996, "end": 2141.68, "text": " models there's often going to be a small a medium a large and of course we should", "tokens": [50364, 26185, 767, 294, 264, 733, 295, 4363, 426, 45196, 4249, 13, 407, 291, 458, 321, 600, 668, 50644, 50644, 7601, 613, 26185, 8185, 293, 1338, 286, 393, 980, 291, 300, 1346, 607, 1328, 307, 50878, 50878, 767, 257, 534, 665, 2891, 935, 337, 257, 688, 295, 721, 370, 300, 311, 983, 321, 600, 51086, 51086, 6183, 309, 13, 407, 321, 1888, 527, 2316, 293, 445, 411, 294, 7172, 337, 3256, 291, 458, 527, 51300, 51300, 5245, 456, 311, 2049, 516, 281, 312, 257, 1359, 257, 6399, 257, 2416, 293, 295, 1164, 321, 820, 51514, 51514, 722, 365, 1359, 558, 570, 1359, 307, 516, 281, 312, 4663, 281, 3847, 321, 434, 516, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.11737423772397249, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.00032485564588569105}, {"id": 381, "seek": 211868, "start": 2141.68, "end": 2146.04, "text": " start with small right because small is going to be faster to train we're going", "tokens": [50364, 26185, 767, 294, 264, 733, 295, 4363, 426, 45196, 4249, 13, 407, 291, 458, 321, 600, 668, 50644, 50644, 7601, 613, 26185, 8185, 293, 1338, 286, 393, 980, 291, 300, 1346, 607, 1328, 307, 50878, 50878, 767, 257, 534, 665, 2891, 935, 337, 257, 688, 295, 721, 370, 300, 311, 983, 321, 600, 51086, 51086, 6183, 309, 13, 407, 321, 1888, 527, 2316, 293, 445, 411, 294, 7172, 337, 3256, 291, 458, 527, 51300, 51300, 5245, 456, 311, 2049, 516, 281, 312, 257, 1359, 257, 6399, 257, 2416, 293, 295, 1164, 321, 820, 51514, 51514, 722, 365, 1359, 558, 570, 1359, 307, 516, 281, 312, 4663, 281, 3847, 321, 434, 516, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.11737423772397249, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.00032485564588569105}, {"id": 382, "seek": 214604, "start": 2146.04, "end": 2158.48, "text": " to be doing able to do more iterations and so forth. Okay so at this point", "tokens": [50364, 281, 312, 884, 1075, 281, 360, 544, 36540, 293, 370, 5220, 13, 1033, 370, 412, 341, 935, 50986, 50986, 1604, 264, 787, 1778, 321, 6183, 527, 2316, 307, 570, 321, 362, 281, 652, 988, 321, 51140, 51140, 14862, 1125, 294, 264, 912, 636, 13, 1407, 980, 4088, 433, 300, 321, 528, 281, 14862, 1125, 51562, 51562, 264, 912, 636, 300, 264, 561, 300, 3094, 257, 2316, 630, 321, 764, 746, 1219, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.10534174601236979, "compression_ratio": 1.597883597883598, "no_speech_prob": 4.6824490709695965e-05}, {"id": 383, "seek": 214604, "start": 2158.48, "end": 2161.56, "text": " remember the only reason we picked our model is because we have to make sure we", "tokens": [50364, 281, 312, 884, 1075, 281, 360, 544, 36540, 293, 370, 5220, 13, 1033, 370, 412, 341, 935, 50986, 50986, 1604, 264, 787, 1778, 321, 6183, 527, 2316, 307, 570, 321, 362, 281, 652, 988, 321, 51140, 51140, 14862, 1125, 294, 264, 912, 636, 13, 1407, 980, 4088, 433, 300, 321, 528, 281, 14862, 1125, 51562, 51562, 264, 912, 636, 300, 264, 561, 300, 3094, 257, 2316, 630, 321, 764, 746, 1219, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.10534174601236979, "compression_ratio": 1.597883597883598, "no_speech_prob": 4.6824490709695965e-05}, {"id": 384, "seek": 214604, "start": 2161.56, "end": 2170.0, "text": " tokenize in the same way. To tell transformers that we want to tokenize", "tokens": [50364, 281, 312, 884, 1075, 281, 360, 544, 36540, 293, 370, 5220, 13, 1033, 370, 412, 341, 935, 50986, 50986, 1604, 264, 787, 1778, 321, 6183, 527, 2316, 307, 570, 321, 362, 281, 652, 988, 321, 51140, 51140, 14862, 1125, 294, 264, 912, 636, 13, 1407, 980, 4088, 433, 300, 321, 528, 281, 14862, 1125, 51562, 51562, 264, 912, 636, 300, 264, 561, 300, 3094, 257, 2316, 630, 321, 764, 746, 1219, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.10534174601236979, "compression_ratio": 1.597883597883598, "no_speech_prob": 4.6824490709695965e-05}, {"id": 385, "seek": 214604, "start": 2170.0, "end": 2173.92, "text": " the same way that the people that built a model did we use something called", "tokens": [50364, 281, 312, 884, 1075, 281, 360, 544, 36540, 293, 370, 5220, 13, 1033, 370, 412, 341, 935, 50986, 50986, 1604, 264, 787, 1778, 321, 6183, 527, 2316, 307, 570, 321, 362, 281, 652, 988, 321, 51140, 51140, 14862, 1125, 294, 264, 912, 636, 13, 1407, 980, 4088, 433, 300, 321, 528, 281, 14862, 1125, 51562, 51562, 264, 912, 636, 300, 264, 561, 300, 3094, 257, 2316, 630, 321, 764, 746, 1219, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.10534174601236979, "compression_ratio": 1.597883597883598, "no_speech_prob": 4.6824490709695965e-05}, {"id": 386, "seek": 217392, "start": 2173.92, "end": 2178.08, "text": " auto tokenizer. It's nothing fancy it's basically just a dictionary which says", "tokens": [50364, 8399, 14862, 6545, 13, 467, 311, 1825, 10247, 309, 311, 1936, 445, 257, 25890, 597, 1619, 50572, 50572, 1954, 597, 2316, 4960, 597, 14862, 6545, 13, 407, 562, 321, 584, 8399, 14862, 6545, 490, 659, 12, 17227, 2001, 50806, 50806, 309, 486, 5484, 264, 19864, 293, 264, 4365, 466, 577, 341, 1729, 2316, 51100, 51100, 14862, 1602, 1412, 992, 13, 407, 412, 341, 935, 321, 393, 586, 747, 300, 14862, 6545, 293, 1320, 264, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.14906643598507613, "compression_ratio": 1.6822916666666667, "no_speech_prob": 0.00015594078286085278}, {"id": 387, "seek": 217392, "start": 2178.08, "end": 2182.76, "text": " oh which model uses which tokenizer. So when we say auto tokenizer from pre-trained", "tokens": [50364, 8399, 14862, 6545, 13, 467, 311, 1825, 10247, 309, 311, 1936, 445, 257, 25890, 597, 1619, 50572, 50572, 1954, 597, 2316, 4960, 597, 14862, 6545, 13, 407, 562, 321, 584, 8399, 14862, 6545, 490, 659, 12, 17227, 2001, 50806, 50806, 309, 486, 5484, 264, 19864, 293, 264, 4365, 466, 577, 341, 1729, 2316, 51100, 51100, 14862, 1602, 1412, 992, 13, 407, 412, 341, 935, 321, 393, 586, 747, 300, 14862, 6545, 293, 1320, 264, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.14906643598507613, "compression_ratio": 1.6822916666666667, "no_speech_prob": 0.00015594078286085278}, {"id": 388, "seek": 217392, "start": 2182.76, "end": 2188.64, "text": " it will download the vocabulary and the details about how this particular model", "tokens": [50364, 8399, 14862, 6545, 13, 467, 311, 1825, 10247, 309, 311, 1936, 445, 257, 25890, 597, 1619, 50572, 50572, 1954, 597, 2316, 4960, 597, 14862, 6545, 13, 407, 562, 321, 584, 8399, 14862, 6545, 490, 659, 12, 17227, 2001, 50806, 50806, 309, 486, 5484, 264, 19864, 293, 264, 4365, 466, 577, 341, 1729, 2316, 51100, 51100, 14862, 1602, 1412, 992, 13, 407, 412, 341, 935, 321, 393, 586, 747, 300, 14862, 6545, 293, 1320, 264, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.14906643598507613, "compression_ratio": 1.6822916666666667, "no_speech_prob": 0.00015594078286085278}, {"id": 389, "seek": 217392, "start": 2188.64, "end": 2199.4, "text": " tokenized data set. So at this point we can now take that tokenizer and pass the", "tokens": [50364, 8399, 14862, 6545, 13, 467, 311, 1825, 10247, 309, 311, 1936, 445, 257, 25890, 597, 1619, 50572, 50572, 1954, 597, 2316, 4960, 597, 14862, 6545, 13, 407, 562, 321, 584, 8399, 14862, 6545, 490, 659, 12, 17227, 2001, 50806, 50806, 309, 486, 5484, 264, 19864, 293, 264, 4365, 466, 577, 341, 1729, 2316, 51100, 51100, 14862, 1602, 1412, 992, 13, 407, 412, 341, 935, 321, 393, 586, 747, 300, 14862, 6545, 293, 1320, 264, 51638, 51638], "temperature": 0.0, "avg_logprob": -0.14906643598507613, "compression_ratio": 1.6822916666666667, "no_speech_prob": 0.00015594078286085278}, {"id": 390, "seek": 219940, "start": 2199.4, "end": 2205.8, "text": " string to it. So if I pass the string g'day folks I'm Jeremy from fast.ai", "tokens": [50364, 6798, 281, 309, 13, 407, 498, 286, 1320, 264, 6798, 290, 1116, 320, 4024, 286, 478, 17809, 490, 2370, 13, 1301, 50684, 50684, 291, 603, 536, 309, 311, 733, 295, 3372, 309, 666, 2283, 733, 295, 406, 13, 407, 498, 291, 600, 1562, 51074, 51074, 17055, 1968, 290, 1116, 320, 307, 472, 1349, 420, 732, 291, 458, 309, 311, 767, 1045, 22667, 51330, 51330, 4650, 281, 341, 14862, 6545, 293, 286, 478, 307, 1045, 22667, 293, 2370, 13, 1301, 307, 1045, 22667, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.14791161514991938, "compression_ratio": 1.6923076923076923, "no_speech_prob": 8.089164475677535e-05}, {"id": 391, "seek": 219940, "start": 2205.8, "end": 2213.6, "text": " you'll see it's kind of putting it into words kind of not. So if you've ever", "tokens": [50364, 6798, 281, 309, 13, 407, 498, 286, 1320, 264, 6798, 290, 1116, 320, 4024, 286, 478, 17809, 490, 2370, 13, 1301, 50684, 50684, 291, 603, 536, 309, 311, 733, 295, 3372, 309, 666, 2283, 733, 295, 406, 13, 407, 498, 291, 600, 1562, 51074, 51074, 17055, 1968, 290, 1116, 320, 307, 472, 1349, 420, 732, 291, 458, 309, 311, 767, 1045, 22667, 51330, 51330, 4650, 281, 341, 14862, 6545, 293, 286, 478, 307, 1045, 22667, 293, 2370, 13, 1301, 307, 1045, 22667, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.14791161514991938, "compression_ratio": 1.6923076923076923, "no_speech_prob": 8.089164475677535e-05}, {"id": 392, "seek": 219940, "start": 2213.6, "end": 2218.7200000000003, "text": " wondered whether g'day is one word or two you know it's actually three tokens", "tokens": [50364, 6798, 281, 309, 13, 407, 498, 286, 1320, 264, 6798, 290, 1116, 320, 4024, 286, 478, 17809, 490, 2370, 13, 1301, 50684, 50684, 291, 603, 536, 309, 311, 733, 295, 3372, 309, 666, 2283, 733, 295, 406, 13, 407, 498, 291, 600, 1562, 51074, 51074, 17055, 1968, 290, 1116, 320, 307, 472, 1349, 420, 732, 291, 458, 309, 311, 767, 1045, 22667, 51330, 51330, 4650, 281, 341, 14862, 6545, 293, 286, 478, 307, 1045, 22667, 293, 2370, 13, 1301, 307, 1045, 22667, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.14791161514991938, "compression_ratio": 1.6923076923076923, "no_speech_prob": 8.089164475677535e-05}, {"id": 393, "seek": 219940, "start": 2218.7200000000003, "end": 2225.52, "text": " according to this tokenizer and I'm is three tokens and fast.ai is three tokens", "tokens": [50364, 6798, 281, 309, 13, 407, 498, 286, 1320, 264, 6798, 290, 1116, 320, 4024, 286, 478, 17809, 490, 2370, 13, 1301, 50684, 50684, 291, 603, 536, 309, 311, 733, 295, 3372, 309, 666, 2283, 733, 295, 406, 13, 407, 498, 291, 600, 1562, 51074, 51074, 17055, 1968, 290, 1116, 320, 307, 472, 1349, 420, 732, 291, 458, 309, 311, 767, 1045, 22667, 51330, 51330, 4650, 281, 341, 14862, 6545, 293, 286, 478, 307, 1045, 22667, 293, 2370, 13, 1301, 307, 1045, 22667, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.14791161514991938, "compression_ratio": 1.6923076923076923, "no_speech_prob": 8.089164475677535e-05}, {"id": 394, "seek": 222552, "start": 2225.52, "end": 2231.12, "text": " this punctuation is a token and so you kind of get the idea. These underscores", "tokens": [50364, 341, 27006, 16073, 307, 257, 14862, 293, 370, 291, 733, 295, 483, 264, 1558, 13, 1981, 16692, 66, 2706, 50644, 50644, 510, 300, 8855, 264, 722, 295, 257, 1349, 558, 370, 300, 311, 733, 295, 341, 307, 3410, 50896, 50896, 300, 411, 264, 722, 295, 257, 1349, 307, 733, 295, 644, 295, 264, 14862, 13, 407, 498, 291, 536, 257, 51082, 51082, 4238, 286, 294, 264, 2808, 295, 257, 1349, 5717, 264, 722, 295, 257, 1349, 300, 311, 733, 295, 1355, 51226, 51226, 257, 819, 551, 13, 407, 341, 307, 437, 2314, 562, 321, 14862, 1125, 341, 8174, 51494, 51494], "temperature": 0.0, "avg_logprob": -0.10293338813033759, "compression_ratio": 1.8708133971291867, "no_speech_prob": 0.0001195843651657924}, {"id": 395, "seek": 222552, "start": 2231.12, "end": 2236.16, "text": " here that represents the start of a word right so that's kind of this is concept", "tokens": [50364, 341, 27006, 16073, 307, 257, 14862, 293, 370, 291, 733, 295, 483, 264, 1558, 13, 1981, 16692, 66, 2706, 50644, 50644, 510, 300, 8855, 264, 722, 295, 257, 1349, 558, 370, 300, 311, 733, 295, 341, 307, 3410, 50896, 50896, 300, 411, 264, 722, 295, 257, 1349, 307, 733, 295, 644, 295, 264, 14862, 13, 407, 498, 291, 536, 257, 51082, 51082, 4238, 286, 294, 264, 2808, 295, 257, 1349, 5717, 264, 722, 295, 257, 1349, 300, 311, 733, 295, 1355, 51226, 51226, 257, 819, 551, 13, 407, 341, 307, 437, 2314, 562, 321, 14862, 1125, 341, 8174, 51494, 51494], "temperature": 0.0, "avg_logprob": -0.10293338813033759, "compression_ratio": 1.8708133971291867, "no_speech_prob": 0.0001195843651657924}, {"id": 396, "seek": 222552, "start": 2236.16, "end": 2239.88, "text": " that like the start of a word is kind of part of the token. So if you see a", "tokens": [50364, 341, 27006, 16073, 307, 257, 14862, 293, 370, 291, 733, 295, 483, 264, 1558, 13, 1981, 16692, 66, 2706, 50644, 50644, 510, 300, 8855, 264, 722, 295, 257, 1349, 558, 370, 300, 311, 733, 295, 341, 307, 3410, 50896, 50896, 300, 411, 264, 722, 295, 257, 1349, 307, 733, 295, 644, 295, 264, 14862, 13, 407, 498, 291, 536, 257, 51082, 51082, 4238, 286, 294, 264, 2808, 295, 257, 1349, 5717, 264, 722, 295, 257, 1349, 300, 311, 733, 295, 1355, 51226, 51226, 257, 819, 551, 13, 407, 341, 307, 437, 2314, 562, 321, 14862, 1125, 341, 8174, 51494, 51494], "temperature": 0.0, "avg_logprob": -0.10293338813033759, "compression_ratio": 1.8708133971291867, "no_speech_prob": 0.0001195843651657924}, {"id": 397, "seek": 222552, "start": 2239.88, "end": 2242.7599999999998, "text": " capital I in the middle of a word versus the start of a word that's kind of means", "tokens": [50364, 341, 27006, 16073, 307, 257, 14862, 293, 370, 291, 733, 295, 483, 264, 1558, 13, 1981, 16692, 66, 2706, 50644, 50644, 510, 300, 8855, 264, 722, 295, 257, 1349, 558, 370, 300, 311, 733, 295, 341, 307, 3410, 50896, 50896, 300, 411, 264, 722, 295, 257, 1349, 307, 733, 295, 644, 295, 264, 14862, 13, 407, 498, 291, 536, 257, 51082, 51082, 4238, 286, 294, 264, 2808, 295, 257, 1349, 5717, 264, 722, 295, 257, 1349, 300, 311, 733, 295, 1355, 51226, 51226, 257, 819, 551, 13, 407, 341, 307, 437, 2314, 562, 321, 14862, 1125, 341, 8174, 51494, 51494], "temperature": 0.0, "avg_logprob": -0.10293338813033759, "compression_ratio": 1.8708133971291867, "no_speech_prob": 0.0001195843651657924}, {"id": 398, "seek": 222552, "start": 2242.7599999999998, "end": 2248.12, "text": " a different thing. So this is what happens when we tokenize this sentence", "tokens": [50364, 341, 27006, 16073, 307, 257, 14862, 293, 370, 291, 733, 295, 483, 264, 1558, 13, 1981, 16692, 66, 2706, 50644, 50644, 510, 300, 8855, 264, 722, 295, 257, 1349, 558, 370, 300, 311, 733, 295, 341, 307, 3410, 50896, 50896, 300, 411, 264, 722, 295, 257, 1349, 307, 733, 295, 644, 295, 264, 14862, 13, 407, 498, 291, 536, 257, 51082, 51082, 4238, 286, 294, 264, 2808, 295, 257, 1349, 5717, 264, 722, 295, 257, 1349, 300, 311, 733, 295, 1355, 51226, 51226, 257, 819, 551, 13, 407, 341, 307, 437, 2314, 562, 321, 14862, 1125, 341, 8174, 51494, 51494], "temperature": 0.0, "avg_logprob": -0.10293338813033759, "compression_ratio": 1.8708133971291867, "no_speech_prob": 0.0001195843651657924}, {"id": 399, "seek": 224812, "start": 2248.12, "end": 2260.8399999999997, "text": " using the tokenizer that the Deberta v3 developers used. So here's a less common", "tokens": [50364, 1228, 264, 14862, 6545, 300, 264, 1346, 607, 1328, 371, 18, 8849, 1143, 13, 407, 510, 311, 257, 1570, 2689, 51000, 51000, 5969, 291, 434, 257, 955, 3403, 88, 31624, 3429, 411, 385, 1570, 2689, 8174, 257, 3403, 88, 31624, 307, 364, 51326, 51326, 420, 77, 355, 284, 71, 2534, 339, 301, 293, 257, 19096, 301, 293, 370, 1392, 294, 341, 1729, 19864, 3403, 88, 31624, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.21674822398594448, "compression_ratio": 1.515923566878981, "no_speech_prob": 9.913610119838268e-05}, {"id": 400, "seek": 224812, "start": 2260.8399999999997, "end": 2267.3599999999997, "text": " unless you're a big platypus fan like me less common sentence a platypus is an", "tokens": [50364, 1228, 264, 14862, 6545, 300, 264, 1346, 607, 1328, 371, 18, 8849, 1143, 13, 407, 510, 311, 257, 1570, 2689, 51000, 51000, 5969, 291, 434, 257, 955, 3403, 88, 31624, 3429, 411, 385, 1570, 2689, 8174, 257, 3403, 88, 31624, 307, 364, 51326, 51326, 420, 77, 355, 284, 71, 2534, 339, 301, 293, 257, 19096, 301, 293, 370, 1392, 294, 341, 1729, 19864, 3403, 88, 31624, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.21674822398594448, "compression_ratio": 1.515923566878981, "no_speech_prob": 9.913610119838268e-05}, {"id": 401, "seek": 224812, "start": 2267.3599999999997, "end": 2272.2799999999997, "text": " ornithorhynchus and athenus and so okay in this particular vocabulary platypus", "tokens": [50364, 1228, 264, 14862, 6545, 300, 264, 1346, 607, 1328, 371, 18, 8849, 1143, 13, 407, 510, 311, 257, 1570, 2689, 51000, 51000, 5969, 291, 434, 257, 955, 3403, 88, 31624, 3429, 411, 385, 1570, 2689, 8174, 257, 3403, 88, 31624, 307, 364, 51326, 51326, 420, 77, 355, 284, 71, 2534, 339, 301, 293, 257, 19096, 301, 293, 370, 1392, 294, 341, 1729, 19864, 3403, 88, 31624, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.21674822398594448, "compression_ratio": 1.515923566878981, "no_speech_prob": 9.913610119838268e-05}, {"id": 402, "seek": 227228, "start": 2272.28, "end": 2278.5800000000004, "text": " got its own word its own token but ornithorhynchus didn't and so I still", "tokens": [50364, 658, 1080, 1065, 1349, 1080, 1065, 14862, 457, 420, 77, 355, 284, 71, 2534, 339, 301, 994, 380, 293, 370, 286, 920, 50679, 50679, 1604, 7204, 472, 337, 512, 1778, 527, 5027, 658, 505, 439, 281, 1466, 577, 281, 9827, 50944, 50944, 420, 77, 355, 284, 71, 2534, 339, 301, 370, 472, 295, 452, 2954, 2283, 13, 407, 291, 393, 536, 510, 309, 311, 668, 51224, 51224, 7472, 666, 420, 11, 3867, 11, 27899, 11, 367, 475, 11, 505, 13, 407, 633, 472, 295, 613, 22667, 291, 536, 510, 307, 516, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13212228072317023, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.0001940795627888292}, {"id": 403, "seek": 227228, "start": 2278.5800000000004, "end": 2283.88, "text": " remember grade one for some reason our teacher got us all to learn how to spell", "tokens": [50364, 658, 1080, 1065, 1349, 1080, 1065, 14862, 457, 420, 77, 355, 284, 71, 2534, 339, 301, 994, 380, 293, 370, 286, 920, 50679, 50679, 1604, 7204, 472, 337, 512, 1778, 527, 5027, 658, 505, 439, 281, 1466, 577, 281, 9827, 50944, 50944, 420, 77, 355, 284, 71, 2534, 339, 301, 370, 472, 295, 452, 2954, 2283, 13, 407, 291, 393, 536, 510, 309, 311, 668, 51224, 51224, 7472, 666, 420, 11, 3867, 11, 27899, 11, 367, 475, 11, 505, 13, 407, 633, 472, 295, 613, 22667, 291, 536, 510, 307, 516, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13212228072317023, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.0001940795627888292}, {"id": 404, "seek": 227228, "start": 2283.88, "end": 2289.48, "text": " ornithorhynchus so one of my favorite words. So you can see here it's been", "tokens": [50364, 658, 1080, 1065, 1349, 1080, 1065, 14862, 457, 420, 77, 355, 284, 71, 2534, 339, 301, 994, 380, 293, 370, 286, 920, 50679, 50679, 1604, 7204, 472, 337, 512, 1778, 527, 5027, 658, 505, 439, 281, 1466, 577, 281, 9827, 50944, 50944, 420, 77, 355, 284, 71, 2534, 339, 301, 370, 472, 295, 452, 2954, 2283, 13, 407, 291, 393, 536, 510, 309, 311, 668, 51224, 51224, 7472, 666, 420, 11, 3867, 11, 27899, 11, 367, 475, 11, 505, 13, 407, 633, 472, 295, 613, 22667, 291, 536, 510, 307, 516, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13212228072317023, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.0001940795627888292}, {"id": 405, "seek": 227228, "start": 2289.48, "end": 2299.28, "text": " split into or, ni, tho, rink, us. So every one of these tokens you see here is going", "tokens": [50364, 658, 1080, 1065, 1349, 1080, 1065, 14862, 457, 420, 77, 355, 284, 71, 2534, 339, 301, 994, 380, 293, 370, 286, 920, 50679, 50679, 1604, 7204, 472, 337, 512, 1778, 527, 5027, 658, 505, 439, 281, 1466, 577, 281, 9827, 50944, 50944, 420, 77, 355, 284, 71, 2534, 339, 301, 370, 472, 295, 452, 2954, 2283, 13, 407, 291, 393, 536, 510, 309, 311, 668, 51224, 51224, 7472, 666, 420, 11, 3867, 11, 27899, 11, 367, 475, 11, 505, 13, 407, 633, 472, 295, 613, 22667, 291, 536, 510, 307, 516, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13212228072317023, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.0001940795627888292}, {"id": 406, "seek": 229928, "start": 2299.28, "end": 2303.96, "text": " to be in the vocabulary right the list of unique tokens that was created when", "tokens": [50364, 281, 312, 294, 264, 19864, 558, 264, 1329, 295, 3845, 22667, 300, 390, 2942, 562, 50598, 50598, 341, 562, 341, 1729, 2316, 341, 659, 12, 17227, 2001, 2316, 390, 700, 8895, 370, 50934, 50934, 4079, 294, 300, 1329, 321, 603, 915, 37556, 4238, 316, 293, 309, 603, 362, 257, 51198, 51198, 1230, 293, 370, 300, 311, 577, 321, 603, 312, 1075, 281, 1261, 613, 666, 3547, 13, 407, 341, 700, 51506, 51506, 1399, 307, 1219, 14862, 2144, 293, 550, 264, 551, 689, 321, 747, 613, 22667, 293, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.10682573423280821, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.00017671806563157588}, {"id": 407, "seek": 229928, "start": 2303.96, "end": 2310.6800000000003, "text": " this when this particular model this pre-trained model was first trained so", "tokens": [50364, 281, 312, 294, 264, 19864, 558, 264, 1329, 295, 3845, 22667, 300, 390, 2942, 562, 50598, 50598, 341, 562, 341, 1729, 2316, 341, 659, 12, 17227, 2001, 2316, 390, 700, 8895, 370, 50934, 50934, 4079, 294, 300, 1329, 321, 603, 915, 37556, 4238, 316, 293, 309, 603, 362, 257, 51198, 51198, 1230, 293, 370, 300, 311, 577, 321, 603, 312, 1075, 281, 1261, 613, 666, 3547, 13, 407, 341, 700, 51506, 51506, 1399, 307, 1219, 14862, 2144, 293, 550, 264, 551, 689, 321, 747, 613, 22667, 293, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.10682573423280821, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.00017671806563157588}, {"id": 408, "seek": 229928, "start": 2310.6800000000003, "end": 2315.96, "text": " somewhere in that list we'll find underscore capital A and it'll have a", "tokens": [50364, 281, 312, 294, 264, 19864, 558, 264, 1329, 295, 3845, 22667, 300, 390, 2942, 562, 50598, 50598, 341, 562, 341, 1729, 2316, 341, 659, 12, 17227, 2001, 2316, 390, 700, 8895, 370, 50934, 50934, 4079, 294, 300, 1329, 321, 603, 915, 37556, 4238, 316, 293, 309, 603, 362, 257, 51198, 51198, 1230, 293, 370, 300, 311, 577, 321, 603, 312, 1075, 281, 1261, 613, 666, 3547, 13, 407, 341, 700, 51506, 51506, 1399, 307, 1219, 14862, 2144, 293, 550, 264, 551, 689, 321, 747, 613, 22667, 293, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.10682573423280821, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.00017671806563157588}, {"id": 409, "seek": 229928, "start": 2315.96, "end": 2322.1200000000003, "text": " number and so that's how we'll be able to turn these into numbers. So this first", "tokens": [50364, 281, 312, 294, 264, 19864, 558, 264, 1329, 295, 3845, 22667, 300, 390, 2942, 562, 50598, 50598, 341, 562, 341, 1729, 2316, 341, 659, 12, 17227, 2001, 2316, 390, 700, 8895, 370, 50934, 50934, 4079, 294, 300, 1329, 321, 603, 915, 37556, 4238, 316, 293, 309, 603, 362, 257, 51198, 51198, 1230, 293, 370, 300, 311, 577, 321, 603, 312, 1075, 281, 1261, 613, 666, 3547, 13, 407, 341, 700, 51506, 51506, 1399, 307, 1219, 14862, 2144, 293, 550, 264, 551, 689, 321, 747, 613, 22667, 293, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.10682573423280821, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.00017671806563157588}, {"id": 410, "seek": 229928, "start": 2322.1200000000003, "end": 2326.32, "text": " process is called tokenization and then the thing where we take these tokens and", "tokens": [50364, 281, 312, 294, 264, 19864, 558, 264, 1329, 295, 3845, 22667, 300, 390, 2942, 562, 50598, 50598, 341, 562, 341, 1729, 2316, 341, 659, 12, 17227, 2001, 2316, 390, 700, 8895, 370, 50934, 50934, 4079, 294, 300, 1329, 321, 603, 915, 37556, 4238, 316, 293, 309, 603, 362, 257, 51198, 51198, 1230, 293, 370, 300, 311, 577, 321, 603, 312, 1075, 281, 1261, 613, 666, 3547, 13, 407, 341, 700, 51506, 51506, 1399, 307, 1219, 14862, 2144, 293, 550, 264, 551, 689, 321, 747, 613, 22667, 293, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.10682573423280821, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.00017671806563157588}, {"id": 411, "seek": 232632, "start": 2326.32, "end": 2333.04, "text": " turn them into numbers is called numericalization. So our data set", "tokens": [50364, 1261, 552, 666, 3547, 307, 1219, 29054, 2144, 13, 407, 527, 1412, 992, 50700, 50700, 1604, 321, 829, 527, 6798, 666, 264, 4846, 2519, 370, 510, 311, 257, 2445, 300, 50994, 50994, 2516, 257, 4166, 30028, 1080, 4846, 293, 14862, 5660, 309, 1392, 370, 321, 603, 818, 341, 527, 51324, 51324, 14862, 2144, 2445, 13, 314, 8406, 2144, 393, 747, 257, 3456, 420, 732, 370, 321, 815, 382, 731, 51588, 51588, 483, 439, 295, 527, 7555, 1143, 884, 309, 412, 264, 912, 565, 281, 3155, 512, 565, 370, 498, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10196858067666331, "compression_ratio": 1.7136363636363636, "no_speech_prob": 3.269575972808525e-05}, {"id": 412, "seek": 232632, "start": 2333.04, "end": 2338.92, "text": " remember we put our string into the input field so here's a function that", "tokens": [50364, 1261, 552, 666, 3547, 307, 1219, 29054, 2144, 13, 407, 527, 1412, 992, 50700, 50700, 1604, 321, 829, 527, 6798, 666, 264, 4846, 2519, 370, 510, 311, 257, 2445, 300, 50994, 50994, 2516, 257, 4166, 30028, 1080, 4846, 293, 14862, 5660, 309, 1392, 370, 321, 603, 818, 341, 527, 51324, 51324, 14862, 2144, 2445, 13, 314, 8406, 2144, 393, 747, 257, 3456, 420, 732, 370, 321, 815, 382, 731, 51588, 51588, 483, 439, 295, 527, 7555, 1143, 884, 309, 412, 264, 912, 565, 281, 3155, 512, 565, 370, 498, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10196858067666331, "compression_ratio": 1.7136363636363636, "no_speech_prob": 3.269575972808525e-05}, {"id": 413, "seek": 232632, "start": 2338.92, "end": 2345.52, "text": " takes a document grabs its input and tokenizes it okay so we'll call this our", "tokens": [50364, 1261, 552, 666, 3547, 307, 1219, 29054, 2144, 13, 407, 527, 1412, 992, 50700, 50700, 1604, 321, 829, 527, 6798, 666, 264, 4846, 2519, 370, 510, 311, 257, 2445, 300, 50994, 50994, 2516, 257, 4166, 30028, 1080, 4846, 293, 14862, 5660, 309, 1392, 370, 321, 603, 818, 341, 527, 51324, 51324, 14862, 2144, 2445, 13, 314, 8406, 2144, 393, 747, 257, 3456, 420, 732, 370, 321, 815, 382, 731, 51588, 51588, 483, 439, 295, 527, 7555, 1143, 884, 309, 412, 264, 912, 565, 281, 3155, 512, 565, 370, 498, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10196858067666331, "compression_ratio": 1.7136363636363636, "no_speech_prob": 3.269575972808525e-05}, {"id": 414, "seek": 232632, "start": 2345.52, "end": 2350.8, "text": " tokenization function. Tokenization can take a minute or two so we may as well", "tokens": [50364, 1261, 552, 666, 3547, 307, 1219, 29054, 2144, 13, 407, 527, 1412, 992, 50700, 50700, 1604, 321, 829, 527, 6798, 666, 264, 4846, 2519, 370, 510, 311, 257, 2445, 300, 50994, 50994, 2516, 257, 4166, 30028, 1080, 4846, 293, 14862, 5660, 309, 1392, 370, 321, 603, 818, 341, 527, 51324, 51324, 14862, 2144, 2445, 13, 314, 8406, 2144, 393, 747, 257, 3456, 420, 732, 370, 321, 815, 382, 731, 51588, 51588, 483, 439, 295, 527, 7555, 1143, 884, 309, 412, 264, 912, 565, 281, 3155, 512, 565, 370, 498, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10196858067666331, "compression_ratio": 1.7136363636363636, "no_speech_prob": 3.269575972808525e-05}, {"id": 415, "seek": 232632, "start": 2350.8, "end": 2355.28, "text": " get all of our processes used doing it at the same time to save some time so if", "tokens": [50364, 1261, 552, 666, 3547, 307, 1219, 29054, 2144, 13, 407, 527, 1412, 992, 50700, 50700, 1604, 321, 829, 527, 6798, 666, 264, 4846, 2519, 370, 510, 311, 257, 2445, 300, 50994, 50994, 2516, 257, 4166, 30028, 1080, 4846, 293, 14862, 5660, 309, 1392, 370, 321, 603, 818, 341, 527, 51324, 51324, 14862, 2144, 2445, 13, 314, 8406, 2144, 393, 747, 257, 3456, 420, 732, 370, 321, 815, 382, 731, 51588, 51588, 483, 439, 295, 527, 7555, 1143, 884, 309, 412, 264, 912, 565, 281, 3155, 512, 565, 370, 498, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.10196858067666331, "compression_ratio": 1.7136363636363636, "no_speech_prob": 3.269575972808525e-05}, {"id": 416, "seek": 235528, "start": 2355.28, "end": 2360.0800000000004, "text": " you use the data set dot map it will parallelize that process and just pass", "tokens": [50364, 291, 764, 264, 1412, 992, 5893, 4471, 309, 486, 8952, 1125, 300, 1399, 293, 445, 1320, 50604, 50604, 294, 428, 2445, 13, 4387, 988, 291, 1320, 15245, 292, 6915, 2074, 370, 309, 393, 360, 257, 3840, 50800, 50800, 412, 257, 565, 293, 2261, 264, 8026, 341, 307, 516, 807, 746, 1219, 264, 50988, 50988, 14862, 5660, 6405, 597, 307, 257, 1238, 26941, 34952, 6405, 300, 4960, 291, 51293, 51293, 458, 24738, 35, 293, 8952, 9007, 293, 370, 5220, 370, 365, 15245, 292, 6915, 2074, 309, 603, 51520, 51520, 312, 1075, 281, 360, 544, 1507, 412, 1564, 13, 407, 574, 309, 787, 1890, 2309, 3949, 370, 1238, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.11076451214877042, "compression_ratio": 1.6813186813186813, "no_speech_prob": 6.502245378214866e-05}, {"id": 417, "seek": 235528, "start": 2360.0800000000004, "end": 2364.0, "text": " in your function. Make sure you pass batched equals true so it can do a bunch", "tokens": [50364, 291, 764, 264, 1412, 992, 5893, 4471, 309, 486, 8952, 1125, 300, 1399, 293, 445, 1320, 50604, 50604, 294, 428, 2445, 13, 4387, 988, 291, 1320, 15245, 292, 6915, 2074, 370, 309, 393, 360, 257, 3840, 50800, 50800, 412, 257, 565, 293, 2261, 264, 8026, 341, 307, 516, 807, 746, 1219, 264, 50988, 50988, 14862, 5660, 6405, 597, 307, 257, 1238, 26941, 34952, 6405, 300, 4960, 291, 51293, 51293, 458, 24738, 35, 293, 8952, 9007, 293, 370, 5220, 370, 365, 15245, 292, 6915, 2074, 309, 603, 51520, 51520, 312, 1075, 281, 360, 544, 1507, 412, 1564, 13, 407, 574, 309, 787, 1890, 2309, 3949, 370, 1238, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.11076451214877042, "compression_ratio": 1.6813186813186813, "no_speech_prob": 6.502245378214866e-05}, {"id": 418, "seek": 235528, "start": 2364.0, "end": 2367.76, "text": " at a time and behind the scenes this is going through something called the", "tokens": [50364, 291, 764, 264, 1412, 992, 5893, 4471, 309, 486, 8952, 1125, 300, 1399, 293, 445, 1320, 50604, 50604, 294, 428, 2445, 13, 4387, 988, 291, 1320, 15245, 292, 6915, 2074, 370, 309, 393, 360, 257, 3840, 50800, 50800, 412, 257, 565, 293, 2261, 264, 8026, 341, 307, 516, 807, 746, 1219, 264, 50988, 50988, 14862, 5660, 6405, 597, 307, 257, 1238, 26941, 34952, 6405, 300, 4960, 291, 51293, 51293, 458, 24738, 35, 293, 8952, 9007, 293, 370, 5220, 370, 365, 15245, 292, 6915, 2074, 309, 603, 51520, 51520, 312, 1075, 281, 360, 544, 1507, 412, 1564, 13, 407, 574, 309, 787, 1890, 2309, 3949, 370, 1238, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.11076451214877042, "compression_ratio": 1.6813186813186813, "no_speech_prob": 6.502245378214866e-05}, {"id": 419, "seek": 235528, "start": 2367.76, "end": 2373.86, "text": " tokenizes library which is a pretty optimized Rust library that uses you", "tokens": [50364, 291, 764, 264, 1412, 992, 5893, 4471, 309, 486, 8952, 1125, 300, 1399, 293, 445, 1320, 50604, 50604, 294, 428, 2445, 13, 4387, 988, 291, 1320, 15245, 292, 6915, 2074, 370, 309, 393, 360, 257, 3840, 50800, 50800, 412, 257, 565, 293, 2261, 264, 8026, 341, 307, 516, 807, 746, 1219, 264, 50988, 50988, 14862, 5660, 6405, 597, 307, 257, 1238, 26941, 34952, 6405, 300, 4960, 291, 51293, 51293, 458, 24738, 35, 293, 8952, 9007, 293, 370, 5220, 370, 365, 15245, 292, 6915, 2074, 309, 603, 51520, 51520, 312, 1075, 281, 360, 544, 1507, 412, 1564, 13, 407, 574, 309, 787, 1890, 2309, 3949, 370, 1238, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.11076451214877042, "compression_ratio": 1.6813186813186813, "no_speech_prob": 6.502245378214866e-05}, {"id": 420, "seek": 235528, "start": 2373.86, "end": 2378.4, "text": " know SIMD and parallel processing and so forth so with batched equals true it'll", "tokens": [50364, 291, 764, 264, 1412, 992, 5893, 4471, 309, 486, 8952, 1125, 300, 1399, 293, 445, 1320, 50604, 50604, 294, 428, 2445, 13, 4387, 988, 291, 1320, 15245, 292, 6915, 2074, 370, 309, 393, 360, 257, 3840, 50800, 50800, 412, 257, 565, 293, 2261, 264, 8026, 341, 307, 516, 807, 746, 1219, 264, 50988, 50988, 14862, 5660, 6405, 597, 307, 257, 1238, 26941, 34952, 6405, 300, 4960, 291, 51293, 51293, 458, 24738, 35, 293, 8952, 9007, 293, 370, 5220, 370, 365, 15245, 292, 6915, 2074, 309, 603, 51520, 51520, 312, 1075, 281, 360, 544, 1507, 412, 1564, 13, 407, 574, 309, 787, 1890, 2309, 3949, 370, 1238, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.11076451214877042, "compression_ratio": 1.6813186813186813, "no_speech_prob": 6.502245378214866e-05}, {"id": 421, "seek": 235528, "start": 2378.4, "end": 2384.1600000000003, "text": " be able to do more stuff at once. So look it only took six seconds so pretty", "tokens": [50364, 291, 764, 264, 1412, 992, 5893, 4471, 309, 486, 8952, 1125, 300, 1399, 293, 445, 1320, 50604, 50604, 294, 428, 2445, 13, 4387, 988, 291, 1320, 15245, 292, 6915, 2074, 370, 309, 393, 360, 257, 3840, 50800, 50800, 412, 257, 565, 293, 2261, 264, 8026, 341, 307, 516, 807, 746, 1219, 264, 50988, 50988, 14862, 5660, 6405, 597, 307, 257, 1238, 26941, 34952, 6405, 300, 4960, 291, 51293, 51293, 458, 24738, 35, 293, 8952, 9007, 293, 370, 5220, 370, 365, 15245, 292, 6915, 2074, 309, 603, 51520, 51520, 312, 1075, 281, 360, 544, 1507, 412, 1564, 13, 407, 574, 309, 787, 1890, 2309, 3949, 370, 1238, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.11076451214877042, "compression_ratio": 1.6813186813186813, "no_speech_prob": 6.502245378214866e-05}, {"id": 422, "seek": 238416, "start": 2384.16, "end": 2391.96, "text": " first. So now when we look at a row of our tokenized data set it's going to", "tokens": [50364, 700, 13, 407, 586, 562, 321, 574, 412, 257, 5386, 295, 527, 14862, 1602, 1412, 992, 309, 311, 516, 281, 50754, 50754, 5304, 2293, 264, 912, 382, 527, 3380, 1412, 992, 572, 309, 311, 406, 516, 281, 747, 51042, 51042, 2293, 264, 912, 382, 264, 3380, 1412, 992, 309, 311, 516, 281, 5304, 2293, 264, 912, 51176, 51176, 4846, 382, 527, 3380, 1412, 992, 293, 309, 311, 611, 516, 281, 5304, 257, 3840, 295, 3547, 13, 51420, 51420, 1981, 3547, 366, 264, 2535, 294, 264, 19864, 295, 1184, 295, 264, 14862, 1602, 1184, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.14153680113173023, "compression_ratio": 2.183333333333333, "no_speech_prob": 3.535273572197184e-05}, {"id": 423, "seek": 238416, "start": 2391.96, "end": 2397.72, "text": " contain exactly the same as our original data set no it's not going to take", "tokens": [50364, 700, 13, 407, 586, 562, 321, 574, 412, 257, 5386, 295, 527, 14862, 1602, 1412, 992, 309, 311, 516, 281, 50754, 50754, 5304, 2293, 264, 912, 382, 527, 3380, 1412, 992, 572, 309, 311, 406, 516, 281, 747, 51042, 51042, 2293, 264, 912, 382, 264, 3380, 1412, 992, 309, 311, 516, 281, 5304, 2293, 264, 912, 51176, 51176, 4846, 382, 527, 3380, 1412, 992, 293, 309, 311, 611, 516, 281, 5304, 257, 3840, 295, 3547, 13, 51420, 51420, 1981, 3547, 366, 264, 2535, 294, 264, 19864, 295, 1184, 295, 264, 14862, 1602, 1184, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.14153680113173023, "compression_ratio": 2.183333333333333, "no_speech_prob": 3.535273572197184e-05}, {"id": 424, "seek": 238416, "start": 2397.72, "end": 2400.3999999999996, "text": " exactly the same as the original data set it's going to contain exactly the same", "tokens": [50364, 700, 13, 407, 586, 562, 321, 574, 412, 257, 5386, 295, 527, 14862, 1602, 1412, 992, 309, 311, 516, 281, 50754, 50754, 5304, 2293, 264, 912, 382, 527, 3380, 1412, 992, 572, 309, 311, 406, 516, 281, 747, 51042, 51042, 2293, 264, 912, 382, 264, 3380, 1412, 992, 309, 311, 516, 281, 5304, 2293, 264, 912, 51176, 51176, 4846, 382, 527, 3380, 1412, 992, 293, 309, 311, 611, 516, 281, 5304, 257, 3840, 295, 3547, 13, 51420, 51420, 1981, 3547, 366, 264, 2535, 294, 264, 19864, 295, 1184, 295, 264, 14862, 1602, 1184, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.14153680113173023, "compression_ratio": 2.183333333333333, "no_speech_prob": 3.535273572197184e-05}, {"id": 425, "seek": 238416, "start": 2400.3999999999996, "end": 2405.2799999999997, "text": " input as our original data set and it's also going to contain a bunch of numbers.", "tokens": [50364, 700, 13, 407, 586, 562, 321, 574, 412, 257, 5386, 295, 527, 14862, 1602, 1412, 992, 309, 311, 516, 281, 50754, 50754, 5304, 2293, 264, 912, 382, 527, 3380, 1412, 992, 572, 309, 311, 406, 516, 281, 747, 51042, 51042, 2293, 264, 912, 382, 264, 3380, 1412, 992, 309, 311, 516, 281, 5304, 2293, 264, 912, 51176, 51176, 4846, 382, 527, 3380, 1412, 992, 293, 309, 311, 611, 516, 281, 5304, 257, 3840, 295, 3547, 13, 51420, 51420, 1981, 3547, 366, 264, 2535, 294, 264, 19864, 295, 1184, 295, 264, 14862, 1602, 1184, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.14153680113173023, "compression_ratio": 2.183333333333333, "no_speech_prob": 3.535273572197184e-05}, {"id": 426, "seek": 238416, "start": 2405.2799999999997, "end": 2413.3599999999997, "text": " These numbers are the position in the vocabulary of each of the tokenized each", "tokens": [50364, 700, 13, 407, 586, 562, 321, 574, 412, 257, 5386, 295, 527, 14862, 1602, 1412, 992, 309, 311, 516, 281, 50754, 50754, 5304, 2293, 264, 912, 382, 527, 3380, 1412, 992, 572, 309, 311, 406, 516, 281, 747, 51042, 51042, 2293, 264, 912, 382, 264, 3380, 1412, 992, 309, 311, 516, 281, 5304, 2293, 264, 912, 51176, 51176, 4846, 382, 527, 3380, 1412, 992, 293, 309, 311, 611, 516, 281, 5304, 257, 3840, 295, 3547, 13, 51420, 51420, 1981, 3547, 366, 264, 2535, 294, 264, 19864, 295, 1184, 295, 264, 14862, 1602, 1184, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.14153680113173023, "compression_ratio": 2.183333333333333, "no_speech_prob": 3.535273572197184e-05}, {"id": 427, "seek": 241336, "start": 2413.36, "end": 2419.76, "text": " of the tokens in the string. So we've now successfully turned a string into a list", "tokens": [50364, 295, 264, 22667, 294, 264, 6798, 13, 407, 321, 600, 586, 10727, 3574, 257, 6798, 666, 257, 1329, 50684, 50684, 295, 3547, 13, 407, 300, 307, 257, 869, 700, 1823, 13, 407, 321, 393, 536, 577, 341, 1985, 321, 393, 536, 51114, 51114, 337, 1365, 300, 321, 600, 658, 295, 412, 341, 257, 4994, 1349, 300, 311, 516, 281, 312, 364, 51360, 51360, 37556, 11944, 294, 264, 19864, 321, 393, 4444, 264, 19864, 574, 493, 295, 915, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.16095525247079354, "compression_ratio": 1.6544502617801047, "no_speech_prob": 6.108218804001808e-05}, {"id": 428, "seek": 241336, "start": 2419.76, "end": 2428.36, "text": " of numbers. So that is a great first step. So we can see how this works we can see", "tokens": [50364, 295, 264, 22667, 294, 264, 6798, 13, 407, 321, 600, 586, 10727, 3574, 257, 6798, 666, 257, 1329, 50684, 50684, 295, 3547, 13, 407, 300, 307, 257, 869, 700, 1823, 13, 407, 321, 393, 536, 577, 341, 1985, 321, 393, 536, 51114, 51114, 337, 1365, 300, 321, 600, 658, 295, 412, 341, 257, 4994, 1349, 300, 311, 516, 281, 312, 364, 51360, 51360, 37556, 11944, 294, 264, 19864, 321, 393, 4444, 264, 19864, 574, 493, 295, 915, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.16095525247079354, "compression_ratio": 1.6544502617801047, "no_speech_prob": 6.108218804001808e-05}, {"id": 429, "seek": 241336, "start": 2428.36, "end": 2433.28, "text": " for example that we've got of at this a separate word that's going to be an", "tokens": [50364, 295, 264, 22667, 294, 264, 6798, 13, 407, 321, 600, 586, 10727, 3574, 257, 6798, 666, 257, 1329, 50684, 50684, 295, 3547, 13, 407, 300, 307, 257, 869, 700, 1823, 13, 407, 321, 393, 536, 577, 341, 1985, 321, 393, 536, 51114, 51114, 337, 1365, 300, 321, 600, 658, 295, 412, 341, 257, 4994, 1349, 300, 311, 516, 281, 312, 364, 51360, 51360, 37556, 11944, 294, 264, 19864, 321, 393, 4444, 264, 19864, 574, 493, 295, 915, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.16095525247079354, "compression_ratio": 1.6544502617801047, "no_speech_prob": 6.108218804001808e-05}, {"id": 430, "seek": 241336, "start": 2433.28, "end": 2440.88, "text": " underscore OF in the vocabulary we can grab the vocabulary look up of find", "tokens": [50364, 295, 264, 22667, 294, 264, 6798, 13, 407, 321, 600, 586, 10727, 3574, 257, 6798, 666, 257, 1329, 50684, 50684, 295, 3547, 13, 407, 300, 307, 257, 869, 700, 1823, 13, 407, 321, 393, 536, 577, 341, 1985, 321, 393, 536, 51114, 51114, 337, 1365, 300, 321, 600, 658, 295, 412, 341, 257, 4994, 1349, 300, 311, 516, 281, 312, 364, 51360, 51360, 37556, 11944, 294, 264, 19864, 321, 393, 4444, 264, 19864, 574, 493, 295, 915, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.16095525247079354, "compression_ratio": 1.6544502617801047, "no_speech_prob": 6.108218804001808e-05}, {"id": 431, "seek": 244088, "start": 2440.88, "end": 2447.6400000000003, "text": " that it's 265 and check here yep here it is 265 okay so it's not rocket science", "tokens": [50364, 300, 309, 311, 7551, 20, 293, 1520, 510, 18633, 510, 309, 307, 7551, 20, 1392, 370, 309, 311, 406, 13012, 3497, 50702, 50702, 558, 309, 311, 445, 1237, 1507, 493, 294, 257, 25890, 281, 483, 264, 3547, 13, 1033, 370, 51176, 51176, 300, 307, 264, 14862, 2144, 293, 29054, 2144, 4818, 294, 426, 45196, 281, 51456, 51456, 1261, 527, 8512, 666, 3547, 281, 2089, 505, 281, 829, 309, 666, 527, 2316, 13, 2639, 1651, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.09984981096707858, "compression_ratio": 1.5692307692307692, "no_speech_prob": 5.064181823399849e-05}, {"id": 432, "seek": 244088, "start": 2447.6400000000003, "end": 2457.12, "text": " right it's just looking stuff up in a dictionary to get the numbers. Okay so", "tokens": [50364, 300, 309, 311, 7551, 20, 293, 1520, 510, 18633, 510, 309, 307, 7551, 20, 1392, 370, 309, 311, 406, 13012, 3497, 50702, 50702, 558, 309, 311, 445, 1237, 1507, 493, 294, 257, 25890, 281, 483, 264, 3547, 13, 1033, 370, 51176, 51176, 300, 307, 264, 14862, 2144, 293, 29054, 2144, 4818, 294, 426, 45196, 281, 51456, 51456, 1261, 527, 8512, 666, 3547, 281, 2089, 505, 281, 829, 309, 666, 527, 2316, 13, 2639, 1651, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.09984981096707858, "compression_ratio": 1.5692307692307692, "no_speech_prob": 5.064181823399849e-05}, {"id": 433, "seek": 244088, "start": 2457.12, "end": 2462.7200000000003, "text": " that is the tokenization and numericalization necessary in NLP to", "tokens": [50364, 300, 309, 311, 7551, 20, 293, 1520, 510, 18633, 510, 309, 307, 7551, 20, 1392, 370, 309, 311, 406, 13012, 3497, 50702, 50702, 558, 309, 311, 445, 1237, 1507, 493, 294, 257, 25890, 281, 483, 264, 3547, 13, 1033, 370, 51176, 51176, 300, 307, 264, 14862, 2144, 293, 29054, 2144, 4818, 294, 426, 45196, 281, 51456, 51456, 1261, 527, 8512, 666, 3547, 281, 2089, 505, 281, 829, 309, 666, 527, 2316, 13, 2639, 1651, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.09984981096707858, "compression_ratio": 1.5692307692307692, "no_speech_prob": 5.064181823399849e-05}, {"id": 434, "seek": 244088, "start": 2462.7200000000003, "end": 2470.12, "text": " turn our documents into numbers to allow us to put it into our model. Any questions", "tokens": [50364, 300, 309, 311, 7551, 20, 293, 1520, 510, 18633, 510, 309, 307, 7551, 20, 1392, 370, 309, 311, 406, 13012, 3497, 50702, 50702, 558, 309, 311, 445, 1237, 1507, 493, 294, 257, 25890, 281, 483, 264, 3547, 13, 1033, 370, 51176, 51176, 300, 307, 264, 14862, 2144, 293, 29054, 2144, 4818, 294, 426, 45196, 281, 51456, 51456, 1261, 527, 8512, 666, 3547, 281, 2089, 505, 281, 829, 309, 666, 527, 2316, 13, 2639, 1651, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.09984981096707858, "compression_ratio": 1.5692307692307692, "no_speech_prob": 5.064181823399849e-05}, {"id": 435, "seek": 247012, "start": 2470.12, "end": 2475.72, "text": " so far John? Excuse me yeah thanks Jeremy so there's a there's a couple and this", "tokens": [50364, 370, 1400, 2619, 30, 11359, 385, 1338, 3231, 17809, 370, 456, 311, 257, 456, 311, 257, 1916, 293, 341, 50644, 50644, 2544, 411, 257, 665, 565, 281, 3507, 552, 484, 293, 309, 311, 4077, 281, 577, 291, 600, 1254, 32509, 50850, 50850, 428, 4846, 1412, 666, 613, 16579, 300, 291, 600, 445, 14862, 1602, 13, 865, 370, 472, 51157, 51157, 1168, 390, 534, 466, 577, 291, 2826, 729, 21009, 293, 264, 264, 1668, 295, 51474, 51474, 264, 7909, 300, 291, 291, 458, 370, 370, 286, 2041, 445, 291, 458, 3102, 294, 364, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.15196685989697775, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0003778052923735231}, {"id": 436, "seek": 247012, "start": 2475.72, "end": 2479.8399999999997, "text": " seems like a good time to throw them out and it's related to how you've formatted", "tokens": [50364, 370, 1400, 2619, 30, 11359, 385, 1338, 3231, 17809, 370, 456, 311, 257, 456, 311, 257, 1916, 293, 341, 50644, 50644, 2544, 411, 257, 665, 565, 281, 3507, 552, 484, 293, 309, 311, 4077, 281, 577, 291, 600, 1254, 32509, 50850, 50850, 428, 4846, 1412, 666, 613, 16579, 300, 291, 600, 445, 14862, 1602, 13, 865, 370, 472, 51157, 51157, 1168, 390, 534, 466, 577, 291, 2826, 729, 21009, 293, 264, 264, 1668, 295, 51474, 51474, 264, 7909, 300, 291, 291, 458, 370, 370, 286, 2041, 445, 291, 458, 3102, 294, 364, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.15196685989697775, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0003778052923735231}, {"id": 437, "seek": 247012, "start": 2479.8399999999997, "end": 2485.98, "text": " your input data into these sentences that you've just tokenized. Yeah so one", "tokens": [50364, 370, 1400, 2619, 30, 11359, 385, 1338, 3231, 17809, 370, 456, 311, 257, 456, 311, 257, 1916, 293, 341, 50644, 50644, 2544, 411, 257, 665, 565, 281, 3507, 552, 484, 293, 309, 311, 4077, 281, 577, 291, 600, 1254, 32509, 50850, 50850, 428, 4846, 1412, 666, 613, 16579, 300, 291, 600, 445, 14862, 1602, 13, 865, 370, 472, 51157, 51157, 1168, 390, 534, 466, 577, 291, 2826, 729, 21009, 293, 264, 264, 1668, 295, 51474, 51474, 264, 7909, 300, 291, 291, 458, 370, 370, 286, 2041, 445, 291, 458, 3102, 294, 364, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.15196685989697775, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0003778052923735231}, {"id": 438, "seek": 247012, "start": 2485.98, "end": 2492.3199999999997, "text": " question was really about how you choose those keywords and the the order of", "tokens": [50364, 370, 1400, 2619, 30, 11359, 385, 1338, 3231, 17809, 370, 456, 311, 257, 456, 311, 257, 1916, 293, 341, 50644, 50644, 2544, 411, 257, 665, 565, 281, 3507, 552, 484, 293, 309, 311, 4077, 281, 577, 291, 600, 1254, 32509, 50850, 50850, 428, 4846, 1412, 666, 613, 16579, 300, 291, 600, 445, 14862, 1602, 13, 865, 370, 472, 51157, 51157, 1168, 390, 534, 466, 577, 291, 2826, 729, 21009, 293, 264, 264, 1668, 295, 51474, 51474, 264, 7909, 300, 291, 291, 458, 370, 370, 286, 2041, 445, 291, 458, 3102, 294, 364, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.15196685989697775, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0003778052923735231}, {"id": 439, "seek": 247012, "start": 2492.3199999999997, "end": 2497.0, "text": " the fields that you you know so so I guess just you know interested in an", "tokens": [50364, 370, 1400, 2619, 30, 11359, 385, 1338, 3231, 17809, 370, 456, 311, 257, 456, 311, 257, 1916, 293, 341, 50644, 50644, 2544, 411, 257, 665, 565, 281, 3507, 552, 484, 293, 309, 311, 4077, 281, 577, 291, 600, 1254, 32509, 50850, 50850, 428, 4846, 1412, 666, 613, 16579, 300, 291, 600, 445, 14862, 1602, 13, 865, 370, 472, 51157, 51157, 1168, 390, 534, 466, 577, 291, 2826, 729, 21009, 293, 264, 264, 1668, 295, 51474, 51474, 264, 7909, 300, 291, 291, 458, 370, 370, 286, 2041, 445, 291, 458, 3102, 294, 364, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.15196685989697775, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0003778052923735231}, {"id": 440, "seek": 249700, "start": 2497.0, "end": 2501.16, "text": " explanation is there any is it is it more art or science how you know it's", "tokens": [50364, 10835, 307, 456, 604, 307, 309, 307, 309, 544, 1523, 420, 3497, 577, 291, 458, 309, 311, 50572, 50572, 23211, 286, 3031, 257, 1326, 721, 286, 3031, 1783, 291, 458, 286, 3031, 3372, 552, 50794, 50794, 12204, 291, 458, 1177, 380, 1871, 321, 445, 528, 512, 636, 746, 300, 309, 393, 51134, 51134, 1466, 490, 558, 370, 498, 286, 445, 1588, 7186, 770, 309, 1553, 613, 45101, 51376, 51376, 949, 1184, 472, 309, 2759, 380, 458, 689, 410, 267, 1712, 295, 16727, 4590, 293, 689, 51592, 51592, 410, 267, 1712, 1409, 558, 370, 286, 630, 445, 746, 300, 309, 393, 1466, 490, 341, 307, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.11191298343517163, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.00013131654122844338}, {"id": 441, "seek": 249700, "start": 2501.16, "end": 2505.6, "text": " arbitrary I tried a few things I tried X you know I tried putting them", "tokens": [50364, 10835, 307, 456, 604, 307, 309, 307, 309, 544, 1523, 420, 3497, 577, 291, 458, 309, 311, 50572, 50572, 23211, 286, 3031, 257, 1326, 721, 286, 3031, 1783, 291, 458, 286, 3031, 3372, 552, 50794, 50794, 12204, 291, 458, 1177, 380, 1871, 321, 445, 528, 512, 636, 746, 300, 309, 393, 51134, 51134, 1466, 490, 558, 370, 498, 286, 445, 1588, 7186, 770, 309, 1553, 613, 45101, 51376, 51376, 949, 1184, 472, 309, 2759, 380, 458, 689, 410, 267, 1712, 295, 16727, 4590, 293, 689, 51592, 51592, 410, 267, 1712, 1409, 558, 370, 286, 630, 445, 746, 300, 309, 393, 1466, 490, 341, 307, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.11191298343517163, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.00013131654122844338}, {"id": 442, "seek": 249700, "start": 2505.6, "end": 2512.4, "text": " backwards you know doesn't matter we just want some way something that it can", "tokens": [50364, 10835, 307, 456, 604, 307, 309, 307, 309, 544, 1523, 420, 3497, 577, 291, 458, 309, 311, 50572, 50572, 23211, 286, 3031, 257, 1326, 721, 286, 3031, 1783, 291, 458, 286, 3031, 3372, 552, 50794, 50794, 12204, 291, 458, 1177, 380, 1871, 321, 445, 528, 512, 636, 746, 300, 309, 393, 51134, 51134, 1466, 490, 558, 370, 498, 286, 445, 1588, 7186, 770, 309, 1553, 613, 45101, 51376, 51376, 949, 1184, 472, 309, 2759, 380, 458, 689, 410, 267, 1712, 295, 16727, 4590, 293, 689, 51592, 51592, 410, 267, 1712, 1409, 558, 370, 286, 630, 445, 746, 300, 309, 393, 1466, 490, 341, 307, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.11191298343517163, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.00013131654122844338}, {"id": 443, "seek": 249700, "start": 2512.4, "end": 2517.24, "text": " learn from right so if I just concatenated it without these headers", "tokens": [50364, 10835, 307, 456, 604, 307, 309, 307, 309, 544, 1523, 420, 3497, 577, 291, 458, 309, 311, 50572, 50572, 23211, 286, 3031, 257, 1326, 721, 286, 3031, 1783, 291, 458, 286, 3031, 3372, 552, 50794, 50794, 12204, 291, 458, 1177, 380, 1871, 321, 445, 528, 512, 636, 746, 300, 309, 393, 51134, 51134, 1466, 490, 558, 370, 498, 286, 445, 1588, 7186, 770, 309, 1553, 613, 45101, 51376, 51376, 949, 1184, 472, 309, 2759, 380, 458, 689, 410, 267, 1712, 295, 16727, 4590, 293, 689, 51592, 51592, 410, 267, 1712, 1409, 558, 370, 286, 630, 445, 746, 300, 309, 393, 1466, 490, 341, 307, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.11191298343517163, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.00013131654122844338}, {"id": 444, "seek": 249700, "start": 2517.24, "end": 2521.56, "text": " before each one it wouldn't know where abatement of pollution ended and where", "tokens": [50364, 10835, 307, 456, 604, 307, 309, 307, 309, 544, 1523, 420, 3497, 577, 291, 458, 309, 311, 50572, 50572, 23211, 286, 3031, 257, 1326, 721, 286, 3031, 1783, 291, 458, 286, 3031, 3372, 552, 50794, 50794, 12204, 291, 458, 1177, 380, 1871, 321, 445, 528, 512, 636, 746, 300, 309, 393, 51134, 51134, 1466, 490, 558, 370, 498, 286, 445, 1588, 7186, 770, 309, 1553, 613, 45101, 51376, 51376, 949, 1184, 472, 309, 2759, 380, 458, 689, 410, 267, 1712, 295, 16727, 4590, 293, 689, 51592, 51592, 410, 267, 1712, 1409, 558, 370, 286, 630, 445, 746, 300, 309, 393, 1466, 490, 341, 307, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.11191298343517163, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.00013131654122844338}, {"id": 445, "seek": 249700, "start": 2521.56, "end": 2525.28, "text": " abatement started right so I did just something that it can learn from this is", "tokens": [50364, 10835, 307, 456, 604, 307, 309, 307, 309, 544, 1523, 420, 3497, 577, 291, 458, 309, 311, 50572, 50572, 23211, 286, 3031, 257, 1326, 721, 286, 3031, 1783, 291, 458, 286, 3031, 3372, 552, 50794, 50794, 12204, 291, 458, 1177, 380, 1871, 321, 445, 528, 512, 636, 746, 300, 309, 393, 51134, 51134, 1466, 490, 558, 370, 498, 286, 445, 1588, 7186, 770, 309, 1553, 613, 45101, 51376, 51376, 949, 1184, 472, 309, 2759, 380, 458, 689, 410, 267, 1712, 295, 16727, 4590, 293, 689, 51592, 51592, 410, 267, 1712, 1409, 558, 370, 286, 630, 445, 746, 300, 309, 393, 1466, 490, 341, 307, 51778, 51778], "temperature": 0.0, "avg_logprob": -0.11191298343517163, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.00013131654122844338}, {"id": 446, "seek": 252528, "start": 2525.28, "end": 2531.2400000000002, "text": " a nice thing about neural nets they're so flexible as long as you give it the", "tokens": [50364, 257, 1481, 551, 466, 18161, 36170, 436, 434, 370, 11358, 382, 938, 382, 291, 976, 309, 264, 50662, 50662, 1589, 6063, 309, 1177, 380, 534, 1871, 577, 291, 976, 309, 281, 976, 309, 264, 50898, 50898, 1589, 382, 938, 382, 309, 311, 456, 558, 286, 727, 362, 1143, 27006, 16073, 286, 727, 51174, 51174, 362, 829, 411, 286, 500, 380, 458, 472, 27515, 38780, 510, 293, 732, 510, 293, 1045, 510, 1338, 51430, 51430, 309, 311, 406, 257, 955, 2028, 411, 412, 264, 1496, 689, 291, 434, 411, 1382, 281, 483, 364, 2857, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.0808496770170546, "compression_ratio": 1.8701923076923077, "no_speech_prob": 0.00017396666225977242}, {"id": 447, "seek": 252528, "start": 2531.2400000000002, "end": 2535.96, "text": " information somehow it doesn't really matter how you give it to give it the", "tokens": [50364, 257, 1481, 551, 466, 18161, 36170, 436, 434, 370, 11358, 382, 938, 382, 291, 976, 309, 264, 50662, 50662, 1589, 6063, 309, 1177, 380, 534, 1871, 577, 291, 976, 309, 281, 976, 309, 264, 50898, 50898, 1589, 382, 938, 382, 309, 311, 456, 558, 286, 727, 362, 1143, 27006, 16073, 286, 727, 51174, 51174, 362, 829, 411, 286, 500, 380, 458, 472, 27515, 38780, 510, 293, 732, 510, 293, 1045, 510, 1338, 51430, 51430, 309, 311, 406, 257, 955, 2028, 411, 412, 264, 1496, 689, 291, 434, 411, 1382, 281, 483, 364, 2857, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.0808496770170546, "compression_ratio": 1.8701923076923077, "no_speech_prob": 0.00017396666225977242}, {"id": 448, "seek": 252528, "start": 2535.96, "end": 2541.48, "text": " information as long as it's there right I could have used punctuation I could", "tokens": [50364, 257, 1481, 551, 466, 18161, 36170, 436, 434, 370, 11358, 382, 938, 382, 291, 976, 309, 264, 50662, 50662, 1589, 6063, 309, 1177, 380, 534, 1871, 577, 291, 976, 309, 281, 976, 309, 264, 50898, 50898, 1589, 382, 938, 382, 309, 311, 456, 558, 286, 727, 362, 1143, 27006, 16073, 286, 727, 51174, 51174, 362, 829, 411, 286, 500, 380, 458, 472, 27515, 38780, 510, 293, 732, 510, 293, 1045, 510, 1338, 51430, 51430, 309, 311, 406, 257, 955, 2028, 411, 412, 264, 1496, 689, 291, 434, 411, 1382, 281, 483, 364, 2857, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.0808496770170546, "compression_ratio": 1.8701923076923077, "no_speech_prob": 0.00017396666225977242}, {"id": 449, "seek": 252528, "start": 2541.48, "end": 2546.6000000000004, "text": " have put like I don't know one semicolon here and two here and three here yeah", "tokens": [50364, 257, 1481, 551, 466, 18161, 36170, 436, 434, 370, 11358, 382, 938, 382, 291, 976, 309, 264, 50662, 50662, 1589, 6063, 309, 1177, 380, 534, 1871, 577, 291, 976, 309, 281, 976, 309, 264, 50898, 50898, 1589, 382, 938, 382, 309, 311, 456, 558, 286, 727, 362, 1143, 27006, 16073, 286, 727, 51174, 51174, 362, 829, 411, 286, 500, 380, 458, 472, 27515, 38780, 510, 293, 732, 510, 293, 1045, 510, 1338, 51430, 51430, 309, 311, 406, 257, 955, 2028, 411, 412, 264, 1496, 689, 291, 434, 411, 1382, 281, 483, 364, 2857, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.0808496770170546, "compression_ratio": 1.8701923076923077, "no_speech_prob": 0.00017396666225977242}, {"id": 450, "seek": 252528, "start": 2546.6000000000004, "end": 2551.92, "text": " it's not a big deal like at the level where you're like trying to get an extra", "tokens": [50364, 257, 1481, 551, 466, 18161, 36170, 436, 434, 370, 11358, 382, 938, 382, 291, 976, 309, 264, 50662, 50662, 1589, 6063, 309, 1177, 380, 534, 1871, 577, 291, 976, 309, 281, 976, 309, 264, 50898, 50898, 1589, 382, 938, 382, 309, 311, 456, 558, 286, 727, 362, 1143, 27006, 16073, 286, 727, 51174, 51174, 362, 829, 411, 286, 500, 380, 458, 472, 27515, 38780, 510, 293, 732, 510, 293, 1045, 510, 1338, 51430, 51430, 309, 311, 406, 257, 955, 2028, 411, 412, 264, 1496, 689, 291, 434, 411, 1382, 281, 483, 364, 2857, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.0808496770170546, "compression_ratio": 1.8701923076923077, "no_speech_prob": 0.00017396666225977242}, {"id": 451, "seek": 255192, "start": 2551.92, "end": 2555.4, "text": " half a percent to get up the leaderboard or cackle competition you may find", "tokens": [50364, 1922, 257, 3043, 281, 483, 493, 264, 5263, 3787, 420, 269, 501, 306, 6211, 291, 815, 915, 50538, 50538, 6986, 2456, 613, 721, 1669, 5870, 7300, 457, 294, 3124, 291, 1582, 380, 50788, 50788, 5101, 915, 309, 309, 7001, 886, 709, 558, 1309, 291, 293, 286, 2041, 264, 1150, 51086, 51086, 644, 295, 300, 8960, 385, 797, 2618, 311, 3365, 498, 472, 295, 641, 641, 7909, 390, 51378, 51378, 257, 4098, 938, 370, 309, 390, 257, 4714, 4342, 307, 456, 604, 2121, 13175, 51624, 51624, 4739, 456, 360, 291, 643, 281, 360, 291, 643, 281, 319, 12, 259, 1020, 729, 733, 295, 2121, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.12070227114953727, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.00017391156870871782}, {"id": 452, "seek": 255192, "start": 2555.4, "end": 2560.4, "text": " tweaking these things makes tiny differences but in practice you won't", "tokens": [50364, 1922, 257, 3043, 281, 483, 493, 264, 5263, 3787, 420, 269, 501, 306, 6211, 291, 815, 915, 50538, 50538, 6986, 2456, 613, 721, 1669, 5870, 7300, 457, 294, 3124, 291, 1582, 380, 50788, 50788, 5101, 915, 309, 309, 7001, 886, 709, 558, 1309, 291, 293, 286, 2041, 264, 1150, 51086, 51086, 644, 295, 300, 8960, 385, 797, 2618, 311, 3365, 498, 472, 295, 641, 641, 7909, 390, 51378, 51378, 257, 4098, 938, 370, 309, 390, 257, 4714, 4342, 307, 456, 604, 2121, 13175, 51624, 51624, 4739, 456, 360, 291, 643, 281, 360, 291, 643, 281, 319, 12, 259, 1020, 729, 733, 295, 2121, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.12070227114953727, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.00017391156870871782}, {"id": 453, "seek": 255192, "start": 2560.4, "end": 2566.36, "text": " generally find it it matters too much right thank you and I guess the second", "tokens": [50364, 1922, 257, 3043, 281, 483, 493, 264, 5263, 3787, 420, 269, 501, 306, 6211, 291, 815, 915, 50538, 50538, 6986, 2456, 613, 721, 1669, 5870, 7300, 457, 294, 3124, 291, 1582, 380, 50788, 50788, 5101, 915, 309, 309, 7001, 886, 709, 558, 1309, 291, 293, 286, 2041, 264, 1150, 51086, 51086, 644, 295, 300, 8960, 385, 797, 2618, 311, 3365, 498, 472, 295, 641, 641, 7909, 390, 51378, 51378, 257, 4098, 938, 370, 309, 390, 257, 4714, 4342, 307, 456, 604, 2121, 13175, 51624, 51624, 4739, 456, 360, 291, 643, 281, 360, 291, 643, 281, 319, 12, 259, 1020, 729, 733, 295, 2121, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.12070227114953727, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.00017391156870871782}, {"id": 454, "seek": 255192, "start": 2566.36, "end": 2572.2000000000003, "text": " part of that excuse me again somebody's asking if one of their their fields was", "tokens": [50364, 1922, 257, 3043, 281, 483, 493, 264, 5263, 3787, 420, 269, 501, 306, 6211, 291, 815, 915, 50538, 50538, 6986, 2456, 613, 721, 1669, 5870, 7300, 457, 294, 3124, 291, 1582, 380, 50788, 50788, 5101, 915, 309, 309, 7001, 886, 709, 558, 1309, 291, 293, 286, 2041, 264, 1150, 51086, 51086, 644, 295, 300, 8960, 385, 797, 2618, 311, 3365, 498, 472, 295, 641, 641, 7909, 390, 51378, 51378, 257, 4098, 938, 370, 309, 390, 257, 4714, 4342, 307, 456, 604, 2121, 13175, 51624, 51624, 4739, 456, 360, 291, 643, 281, 360, 291, 643, 281, 319, 12, 259, 1020, 729, 733, 295, 2121, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.12070227114953727, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.00017391156870871782}, {"id": 455, "seek": 255192, "start": 2572.2000000000003, "end": 2577.12, "text": " a particularly long so it was a thousand characters is there any special handling", "tokens": [50364, 1922, 257, 3043, 281, 483, 493, 264, 5263, 3787, 420, 269, 501, 306, 6211, 291, 815, 915, 50538, 50538, 6986, 2456, 613, 721, 1669, 5870, 7300, 457, 294, 3124, 291, 1582, 380, 50788, 50788, 5101, 915, 309, 309, 7001, 886, 709, 558, 1309, 291, 293, 286, 2041, 264, 1150, 51086, 51086, 644, 295, 300, 8960, 385, 797, 2618, 311, 3365, 498, 472, 295, 641, 641, 7909, 390, 51378, 51378, 257, 4098, 938, 370, 309, 390, 257, 4714, 4342, 307, 456, 604, 2121, 13175, 51624, 51624, 4739, 456, 360, 291, 643, 281, 360, 291, 643, 281, 319, 12, 259, 1020, 729, 733, 295, 2121, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.12070227114953727, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.00017391156870871782}, {"id": 456, "seek": 255192, "start": 2577.12, "end": 2581.6, "text": " required there do you need to do you need to re-inject those kind of special", "tokens": [50364, 1922, 257, 3043, 281, 483, 493, 264, 5263, 3787, 420, 269, 501, 306, 6211, 291, 815, 915, 50538, 50538, 6986, 2456, 613, 721, 1669, 5870, 7300, 457, 294, 3124, 291, 1582, 380, 50788, 50788, 5101, 915, 309, 309, 7001, 886, 709, 558, 1309, 291, 293, 286, 2041, 264, 1150, 51086, 51086, 644, 295, 300, 8960, 385, 797, 2618, 311, 3365, 498, 472, 295, 641, 641, 7909, 390, 51378, 51378, 257, 4098, 938, 370, 309, 390, 257, 4714, 4342, 307, 456, 604, 2121, 13175, 51624, 51624, 4739, 456, 360, 291, 643, 281, 360, 291, 643, 281, 319, 12, 259, 1020, 729, 733, 295, 2121, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.12070227114953727, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.00017391156870871782}, {"id": 457, "seek": 258160, "start": 2581.6, "end": 2586.16, "text": " marker tokens does it does it change if you've got much bigger fields that", "tokens": [50364, 15247, 22667, 775, 309, 775, 309, 1319, 498, 291, 600, 658, 709, 3801, 7909, 300, 50592, 50592, 291, 434, 1382, 281, 1466, 293, 14581, 1338, 938, 8512, 293, 291, 603, 669, 3318, 3651, 51032, 51032, 572, 2121, 12381, 370, 21463, 27735, 294, 1186, 575, 4825, 4714, 1349, 3169, 10229, 51394, 51394, 293, 309, 1985, 869, 281, 341, 786, 291, 603, 669, 3318, 307, 1391, 264, 1151, 3109, 291, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.16911615265740287, "compression_ratio": 1.5320197044334976, "no_speech_prob": 5.6484372180420905e-05}, {"id": 458, "seek": 258160, "start": 2586.16, "end": 2594.96, "text": " you're trying to learn and query yeah long documents and you'll am fit require", "tokens": [50364, 15247, 22667, 775, 309, 775, 309, 1319, 498, 291, 600, 658, 709, 3801, 7909, 300, 50592, 50592, 291, 434, 1382, 281, 1466, 293, 14581, 1338, 938, 8512, 293, 291, 603, 669, 3318, 3651, 51032, 51032, 572, 2121, 12381, 370, 21463, 27735, 294, 1186, 575, 4825, 4714, 1349, 3169, 10229, 51394, 51394, 293, 309, 1985, 869, 281, 341, 786, 291, 603, 669, 3318, 307, 1391, 264, 1151, 3109, 291, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.16911615265740287, "compression_ratio": 1.5320197044334976, "no_speech_prob": 5.6484372180420905e-05}, {"id": 459, "seek": 258160, "start": 2594.96, "end": 2602.2, "text": " no special consideration so IMDB in fact has multi thousand word movie reviews", "tokens": [50364, 15247, 22667, 775, 309, 775, 309, 1319, 498, 291, 600, 658, 709, 3801, 7909, 300, 50592, 50592, 291, 434, 1382, 281, 1466, 293, 14581, 1338, 938, 8512, 293, 291, 603, 669, 3318, 3651, 51032, 51032, 572, 2121, 12381, 370, 21463, 27735, 294, 1186, 575, 4825, 4714, 1349, 3169, 10229, 51394, 51394, 293, 309, 1985, 869, 281, 341, 786, 291, 603, 669, 3318, 307, 1391, 264, 1151, 3109, 291, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.16911615265740287, "compression_ratio": 1.5320197044334976, "no_speech_prob": 5.6484372180420905e-05}, {"id": 460, "seek": 258160, "start": 2602.2, "end": 2609.7999999999997, "text": " and it works great to this day you'll am fit is probably the best approach you", "tokens": [50364, 15247, 22667, 775, 309, 775, 309, 1319, 498, 291, 600, 658, 709, 3801, 7909, 300, 50592, 50592, 291, 434, 1382, 281, 1466, 293, 14581, 1338, 938, 8512, 293, 291, 603, 669, 3318, 3651, 51032, 51032, 572, 2121, 12381, 370, 21463, 27735, 294, 1186, 575, 4825, 4714, 1349, 3169, 10229, 51394, 51394, 293, 309, 1985, 869, 281, 341, 786, 291, 603, 669, 3318, 307, 1391, 264, 1151, 3109, 291, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.16911615265740287, "compression_ratio": 1.5320197044334976, "no_speech_prob": 5.6484372180420905e-05}, {"id": 461, "seek": 260980, "start": 2609.8, "end": 2617.4, "text": " know for reasonably quickly and easily using large documents otherwise if you", "tokens": [50364, 458, 337, 23551, 2661, 293, 3612, 1228, 2416, 8512, 5911, 498, 291, 50744, 50744, 764, 31782, 2361, 11587, 2416, 8512, 366, 7595, 4682, 51080, 51096, 4088, 433, 575, 281, 1936, 362, 281, 360, 264, 1379, 4166, 412, 1564, 689, 1646, 51318, 51318, 291, 439, 669, 3318, 393, 7472, 309, 666, 3866, 3755, 293, 1401, 309, 13145, 293, 51462, 51462, 370, 300, 1355, 291, 603, 915, 300, 561, 1382, 281, 589, 365, 2416, 8512, 3928, 51684, 51684, 281, 3496, 257, 688, 295, 1460, 322, 18407, 82, 570, 436, 643, 264, 955, 10247, 2306, 365, 3195, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.14241977613799425, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.00012531111133284867}, {"id": 462, "seek": 260980, "start": 2617.4, "end": 2624.1200000000003, "text": " use transformer based approaches large documents are challenging specifically", "tokens": [50364, 458, 337, 23551, 2661, 293, 3612, 1228, 2416, 8512, 5911, 498, 291, 50744, 50744, 764, 31782, 2361, 11587, 2416, 8512, 366, 7595, 4682, 51080, 51096, 4088, 433, 575, 281, 1936, 362, 281, 360, 264, 1379, 4166, 412, 1564, 689, 1646, 51318, 51318, 291, 439, 669, 3318, 393, 7472, 309, 666, 3866, 3755, 293, 1401, 309, 13145, 293, 51462, 51462, 370, 300, 1355, 291, 603, 915, 300, 561, 1382, 281, 589, 365, 2416, 8512, 3928, 51684, 51684, 281, 3496, 257, 688, 295, 1460, 322, 18407, 82, 570, 436, 643, 264, 955, 10247, 2306, 365, 3195, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.14241977613799425, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.00012531111133284867}, {"id": 463, "seek": 260980, "start": 2624.44, "end": 2628.88, "text": " transformers has to basically have to do the whole document at once where else", "tokens": [50364, 458, 337, 23551, 2661, 293, 3612, 1228, 2416, 8512, 5911, 498, 291, 50744, 50744, 764, 31782, 2361, 11587, 2416, 8512, 366, 7595, 4682, 51080, 51096, 4088, 433, 575, 281, 1936, 362, 281, 360, 264, 1379, 4166, 412, 1564, 689, 1646, 51318, 51318, 291, 439, 669, 3318, 393, 7472, 309, 666, 3866, 3755, 293, 1401, 309, 13145, 293, 51462, 51462, 370, 300, 1355, 291, 603, 915, 300, 561, 1382, 281, 589, 365, 2416, 8512, 3928, 51684, 51684, 281, 3496, 257, 688, 295, 1460, 322, 18407, 82, 570, 436, 643, 264, 955, 10247, 2306, 365, 3195, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.14241977613799425, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.00012531111133284867}, {"id": 464, "seek": 260980, "start": 2628.88, "end": 2631.76, "text": " you all am fit can split it into multiple pieces and read it gradually and", "tokens": [50364, 458, 337, 23551, 2661, 293, 3612, 1228, 2416, 8512, 5911, 498, 291, 50744, 50744, 764, 31782, 2361, 11587, 2416, 8512, 366, 7595, 4682, 51080, 51096, 4088, 433, 575, 281, 1936, 362, 281, 360, 264, 1379, 4166, 412, 1564, 689, 1646, 51318, 51318, 291, 439, 669, 3318, 393, 7472, 309, 666, 3866, 3755, 293, 1401, 309, 13145, 293, 51462, 51462, 370, 300, 1355, 291, 603, 915, 300, 561, 1382, 281, 589, 365, 2416, 8512, 3928, 51684, 51684, 281, 3496, 257, 688, 295, 1460, 322, 18407, 82, 570, 436, 643, 264, 955, 10247, 2306, 365, 3195, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.14241977613799425, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.00012531111133284867}, {"id": 465, "seek": 260980, "start": 2631.76, "end": 2636.2000000000003, "text": " so that means you'll find that people trying to work with large documents tend", "tokens": [50364, 458, 337, 23551, 2661, 293, 3612, 1228, 2416, 8512, 5911, 498, 291, 50744, 50744, 764, 31782, 2361, 11587, 2416, 8512, 366, 7595, 4682, 51080, 51096, 4088, 433, 575, 281, 1936, 362, 281, 360, 264, 1379, 4166, 412, 1564, 689, 1646, 51318, 51318, 291, 439, 669, 3318, 393, 7472, 309, 666, 3866, 3755, 293, 1401, 309, 13145, 293, 51462, 51462, 370, 300, 1355, 291, 603, 915, 300, 561, 1382, 281, 589, 365, 2416, 8512, 3928, 51684, 51684, 281, 3496, 257, 688, 295, 1460, 322, 18407, 82, 570, 436, 643, 264, 955, 10247, 2306, 365, 3195, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.14241977613799425, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.00012531111133284867}, {"id": 466, "seek": 260980, "start": 2636.2000000000003, "end": 2639.76, "text": " to spend a lot of money on GPUs because they need the big fancy ones with lots", "tokens": [50364, 458, 337, 23551, 2661, 293, 3612, 1228, 2416, 8512, 5911, 498, 291, 50744, 50744, 764, 31782, 2361, 11587, 2416, 8512, 366, 7595, 4682, 51080, 51096, 4088, 433, 575, 281, 1936, 362, 281, 360, 264, 1379, 4166, 412, 1564, 689, 1646, 51318, 51318, 291, 439, 669, 3318, 393, 7472, 309, 666, 3866, 3755, 293, 1401, 309, 13145, 293, 51462, 51462, 370, 300, 1355, 291, 603, 915, 300, 561, 1382, 281, 589, 365, 2416, 8512, 3928, 51684, 51684, 281, 3496, 257, 688, 295, 1460, 322, 18407, 82, 570, 436, 643, 264, 955, 10247, 2306, 365, 3195, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.14241977613799425, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.00012531111133284867}, {"id": 467, "seek": 263976, "start": 2639.76, "end": 2644.5200000000004, "text": " of memory so yeah generally speaking I would say if you're trying to do stuff", "tokens": [50364, 295, 4675, 370, 1338, 5101, 4124, 286, 576, 584, 498, 291, 434, 1382, 281, 360, 1507, 50602, 50602, 365, 8512, 295, 670, 11169, 12, 10250, 568, 11, 1360, 2283, 291, 1062, 528, 281, 574, 412, 291, 439, 50892, 50892, 669, 3318, 853, 4088, 433, 536, 498, 309, 1985, 337, 291, 457, 291, 458, 286, 1116, 3297, 853, 51166, 51166, 1293, 833, 568, 11, 1360, 2283, 291, 458, 4088, 433, 820, 312, 2489, 5969, 51474, 51474, 291, 600, 658, 257, 291, 458, 1825, 457, 411, 257, 10732, 18407, 420, 746, 365, 406, 709, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.12277165055274963, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00016845711797941476}, {"id": 468, "seek": 263976, "start": 2644.5200000000004, "end": 2650.32, "text": " with documents of over mm-hmm 2,000 words you might want to look at you all", "tokens": [50364, 295, 4675, 370, 1338, 5101, 4124, 286, 576, 584, 498, 291, 434, 1382, 281, 360, 1507, 50602, 50602, 365, 8512, 295, 670, 11169, 12, 10250, 568, 11, 1360, 2283, 291, 1062, 528, 281, 574, 412, 291, 439, 50892, 50892, 669, 3318, 853, 4088, 433, 536, 498, 309, 1985, 337, 291, 457, 291, 458, 286, 1116, 3297, 853, 51166, 51166, 1293, 833, 568, 11, 1360, 2283, 291, 458, 4088, 433, 820, 312, 2489, 5969, 51474, 51474, 291, 600, 658, 257, 291, 458, 1825, 457, 411, 257, 10732, 18407, 420, 746, 365, 406, 709, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.12277165055274963, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00016845711797941476}, {"id": 469, "seek": 263976, "start": 2650.32, "end": 2655.8, "text": " am fit try transformers see if it works for you but you know I'd certainly try", "tokens": [50364, 295, 4675, 370, 1338, 5101, 4124, 286, 576, 584, 498, 291, 434, 1382, 281, 360, 1507, 50602, 50602, 365, 8512, 295, 670, 11169, 12, 10250, 568, 11, 1360, 2283, 291, 1062, 528, 281, 574, 412, 291, 439, 50892, 50892, 669, 3318, 853, 4088, 433, 536, 498, 309, 1985, 337, 291, 457, 291, 458, 286, 1116, 3297, 853, 51166, 51166, 1293, 833, 568, 11, 1360, 2283, 291, 458, 4088, 433, 820, 312, 2489, 5969, 51474, 51474, 291, 600, 658, 257, 291, 458, 1825, 457, 411, 257, 10732, 18407, 420, 746, 365, 406, 709, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.12277165055274963, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00016845711797941476}, {"id": 470, "seek": 263976, "start": 2655.8, "end": 2661.96, "text": " both under 2,000 words you know transformers should be fine unless", "tokens": [50364, 295, 4675, 370, 1338, 5101, 4124, 286, 576, 584, 498, 291, 434, 1382, 281, 360, 1507, 50602, 50602, 365, 8512, 295, 670, 11169, 12, 10250, 568, 11, 1360, 2283, 291, 1062, 528, 281, 574, 412, 291, 439, 50892, 50892, 669, 3318, 853, 4088, 433, 536, 498, 309, 1985, 337, 291, 457, 291, 458, 286, 1116, 3297, 853, 51166, 51166, 1293, 833, 568, 11, 1360, 2283, 291, 458, 4088, 433, 820, 312, 2489, 5969, 51474, 51474, 291, 600, 658, 257, 291, 458, 1825, 457, 411, 257, 10732, 18407, 420, 746, 365, 406, 709, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.12277165055274963, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00016845711797941476}, {"id": 471, "seek": 263976, "start": 2661.96, "end": 2665.88, "text": " you've got a you know nothing but like a laptop GPU or something with not much", "tokens": [50364, 295, 4675, 370, 1338, 5101, 4124, 286, 576, 584, 498, 291, 434, 1382, 281, 360, 1507, 50602, 50602, 365, 8512, 295, 670, 11169, 12, 10250, 568, 11, 1360, 2283, 291, 1062, 528, 281, 574, 412, 291, 439, 50892, 50892, 669, 3318, 853, 4088, 433, 536, 498, 309, 1985, 337, 291, 457, 291, 458, 286, 1116, 3297, 853, 51166, 51166, 1293, 833, 568, 11, 1360, 2283, 291, 458, 4088, 433, 820, 312, 2489, 5969, 51474, 51474, 291, 600, 658, 257, 291, 458, 1825, 457, 411, 257, 10732, 18407, 420, 746, 365, 406, 709, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.12277165055274963, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00016845711797941476}, {"id": 472, "seek": 266588, "start": 2665.88, "end": 2680.12, "text": " memory so hacking face transformers has these you know as I say it right now", "tokens": [50364, 4675, 370, 31422, 1851, 4088, 433, 575, 613, 291, 458, 382, 286, 584, 309, 558, 586, 51076, 51076, 300, 286, 915, 552, 8344, 34443, 293, 406, 4098, 731, 23007, 51224, 51224, 9843, 466, 428, 1412, 300, 291, 733, 295, 362, 281, 2573, 484, 293, 472, 295, 51450, 51450, 729, 307, 300, 309, 33280, 300, 428, 3779, 307, 257, 7738, 1219, 16949, 370, 1564, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.10742351902064992, "compression_ratio": 1.60752688172043, "no_speech_prob": 3.168166585965082e-05}, {"id": 473, "seek": 266588, "start": 2680.12, "end": 2683.08, "text": " that I find them somewhat obscure and not particularly well documented", "tokens": [50364, 4675, 370, 31422, 1851, 4088, 433, 575, 613, 291, 458, 382, 286, 584, 309, 558, 586, 51076, 51076, 300, 286, 915, 552, 8344, 34443, 293, 406, 4098, 731, 23007, 51224, 51224, 9843, 466, 428, 1412, 300, 291, 733, 295, 362, 281, 2573, 484, 293, 472, 295, 51450, 51450, 729, 307, 300, 309, 33280, 300, 428, 3779, 307, 257, 7738, 1219, 16949, 370, 1564, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.10742351902064992, "compression_ratio": 1.60752688172043, "no_speech_prob": 3.168166585965082e-05}, {"id": 474, "seek": 266588, "start": 2683.08, "end": 2687.6, "text": " expectations about your data that you kind of have to figure out and one of", "tokens": [50364, 4675, 370, 31422, 1851, 4088, 433, 575, 613, 291, 458, 382, 286, 584, 309, 558, 586, 51076, 51076, 300, 286, 915, 552, 8344, 34443, 293, 406, 4098, 731, 23007, 51224, 51224, 9843, 466, 428, 1412, 300, 291, 733, 295, 362, 281, 2573, 484, 293, 472, 295, 51450, 51450, 729, 307, 300, 309, 33280, 300, 428, 3779, 307, 257, 7738, 1219, 16949, 370, 1564, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.10742351902064992, "compression_ratio": 1.60752688172043, "no_speech_prob": 3.168166585965082e-05}, {"id": 475, "seek": 266588, "start": 2687.6, "end": 2694.36, "text": " those is that it expects that your target is a column called labels so once", "tokens": [50364, 4675, 370, 31422, 1851, 4088, 433, 575, 613, 291, 458, 382, 286, 584, 309, 558, 586, 51076, 51076, 300, 286, 915, 552, 8344, 34443, 293, 406, 4098, 731, 23007, 51224, 51224, 9843, 466, 428, 1412, 300, 291, 733, 295, 362, 281, 2573, 484, 293, 472, 295, 51450, 51450, 729, 307, 300, 309, 33280, 300, 428, 3779, 307, 257, 7738, 1219, 16949, 370, 1564, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.10742351902064992, "compression_ratio": 1.60752688172043, "no_speech_prob": 3.168166585965082e-05}, {"id": 476, "seek": 269436, "start": 2694.36, "end": 2698.28, "text": " I figured that out I just went got our tokenized data set and renamed our", "tokens": [50364, 286, 8932, 300, 484, 286, 445, 1437, 658, 527, 14862, 1602, 1412, 992, 293, 40949, 527, 50560, 50560, 6175, 7738, 281, 16949, 293, 1203, 1409, 1364, 370, 1391, 307, 291, 458, 50844, 50844, 286, 500, 380, 458, 498, 412, 512, 935, 436, 603, 652, 341, 257, 857, 544, 11358, 457, 50976, 50976, 1391, 1151, 281, 445, 818, 428, 3779, 16949, 293, 993, 486, 312, 1858, 291, 1062, 51360, 51360, 362, 1612, 646, 562, 286, 360, 364, 36657, 3100, 300, 456, 390, 1071, 1412, 992, 456, 1219, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.12344675064086914, "compression_ratio": 1.6394849785407726, "no_speech_prob": 3.943448609788902e-05}, {"id": 477, "seek": 269436, "start": 2698.28, "end": 2703.96, "text": " score column to labels and everything started working so probably is you know", "tokens": [50364, 286, 8932, 300, 484, 286, 445, 1437, 658, 527, 14862, 1602, 1412, 992, 293, 40949, 527, 50560, 50560, 6175, 7738, 281, 16949, 293, 1203, 1409, 1364, 370, 1391, 307, 291, 458, 50844, 50844, 286, 500, 380, 458, 498, 412, 512, 935, 436, 603, 652, 341, 257, 857, 544, 11358, 457, 50976, 50976, 1391, 1151, 281, 445, 818, 428, 3779, 16949, 293, 993, 486, 312, 1858, 291, 1062, 51360, 51360, 362, 1612, 646, 562, 286, 360, 364, 36657, 3100, 300, 456, 390, 1071, 1412, 992, 456, 1219, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.12344675064086914, "compression_ratio": 1.6394849785407726, "no_speech_prob": 3.943448609788902e-05}, {"id": 478, "seek": 269436, "start": 2703.96, "end": 2706.6, "text": " I don't know if at some point they'll make this a bit more flexible but", "tokens": [50364, 286, 8932, 300, 484, 286, 445, 1437, 658, 527, 14862, 1602, 1412, 992, 293, 40949, 527, 50560, 50560, 6175, 7738, 281, 16949, 293, 1203, 1409, 1364, 370, 1391, 307, 291, 458, 50844, 50844, 286, 500, 380, 458, 498, 412, 512, 935, 436, 603, 652, 341, 257, 857, 544, 11358, 457, 50976, 50976, 1391, 1151, 281, 445, 818, 428, 3779, 16949, 293, 993, 486, 312, 1858, 291, 1062, 51360, 51360, 362, 1612, 646, 562, 286, 360, 364, 36657, 3100, 300, 456, 390, 1071, 1412, 992, 456, 1219, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.12344675064086914, "compression_ratio": 1.6394849785407726, "no_speech_prob": 3.943448609788902e-05}, {"id": 479, "seek": 269436, "start": 2706.6, "end": 2714.28, "text": " probably best to just call your target labels and life will be easy you might", "tokens": [50364, 286, 8932, 300, 484, 286, 445, 1437, 658, 527, 14862, 1602, 1412, 992, 293, 40949, 527, 50560, 50560, 6175, 7738, 281, 16949, 293, 1203, 1409, 1364, 370, 1391, 307, 291, 458, 50844, 50844, 286, 500, 380, 458, 498, 412, 512, 935, 436, 603, 652, 341, 257, 857, 544, 11358, 457, 50976, 50976, 1391, 1151, 281, 445, 818, 428, 3779, 16949, 293, 993, 486, 312, 1858, 291, 1062, 51360, 51360, 362, 1612, 646, 562, 286, 360, 364, 36657, 3100, 300, 456, 390, 1071, 1412, 992, 456, 1219, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.12344675064086914, "compression_ratio": 1.6394849785407726, "no_speech_prob": 3.943448609788902e-05}, {"id": 480, "seek": 269436, "start": 2714.28, "end": 2718.1200000000003, "text": " have seen back when I do an LS path that there was another data set there called", "tokens": [50364, 286, 8932, 300, 484, 286, 445, 1437, 658, 527, 14862, 1602, 1412, 992, 293, 40949, 527, 50560, 50560, 6175, 7738, 281, 16949, 293, 1203, 1409, 1364, 370, 1391, 307, 291, 458, 50844, 50844, 286, 500, 380, 458, 498, 412, 512, 935, 436, 603, 652, 341, 257, 857, 544, 11358, 457, 50976, 50976, 1391, 1151, 281, 445, 818, 428, 3779, 16949, 293, 993, 486, 312, 1858, 291, 1062, 51360, 51360, 362, 1612, 646, 562, 286, 360, 364, 36657, 3100, 300, 456, 390, 1071, 1412, 992, 456, 1219, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.12344675064086914, "compression_ratio": 1.6394849785407726, "no_speech_prob": 3.943448609788902e-05}, {"id": 481, "seek": 271812, "start": 2718.12, "end": 2726.6, "text": " test CSV and if you look at it it looks a lot like our training set train our", "tokens": [50364, 1500, 48814, 293, 498, 291, 574, 412, 309, 309, 1542, 257, 688, 411, 527, 3097, 992, 3847, 527, 50788, 50788, 661, 48814, 300, 321, 600, 668, 1364, 365, 457, 309, 311, 5361, 264, 6175, 264, 16949, 51092, 51092, 341, 307, 341, 307, 1219, 257, 1500, 992, 293, 370, 321, 434, 516, 281, 751, 257, 707, 857, 466, 51400, 51400, 300, 586, 570, 452, 3932, 510, 307, 300, 4317, 264, 881, 1021, 1558, 293, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.12188331802169998, "compression_ratio": 1.6263157894736842, "no_speech_prob": 2.7533043976291083e-05}, {"id": 482, "seek": 271812, "start": 2726.6, "end": 2732.68, "text": " other CSV that we've been working with but it's missing the score the labels", "tokens": [50364, 1500, 48814, 293, 498, 291, 574, 412, 309, 309, 1542, 257, 688, 411, 527, 3097, 992, 3847, 527, 50788, 50788, 661, 48814, 300, 321, 600, 668, 1364, 365, 457, 309, 311, 5361, 264, 6175, 264, 16949, 51092, 51092, 341, 307, 341, 307, 1219, 257, 1500, 992, 293, 370, 321, 434, 516, 281, 751, 257, 707, 857, 466, 51400, 51400, 300, 586, 570, 452, 3932, 510, 307, 300, 4317, 264, 881, 1021, 1558, 293, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.12188331802169998, "compression_ratio": 1.6263157894736842, "no_speech_prob": 2.7533043976291083e-05}, {"id": 483, "seek": 271812, "start": 2732.68, "end": 2738.8399999999997, "text": " this is this is called a test set and so we're going to talk a little bit about", "tokens": [50364, 1500, 48814, 293, 498, 291, 574, 412, 309, 309, 1542, 257, 688, 411, 527, 3097, 992, 3847, 527, 50788, 50788, 661, 48814, 300, 321, 600, 668, 1364, 365, 457, 309, 311, 5361, 264, 6175, 264, 16949, 51092, 51092, 341, 307, 341, 307, 1219, 257, 1500, 992, 293, 370, 321, 434, 516, 281, 751, 257, 707, 857, 466, 51400, 51400, 300, 586, 570, 452, 3932, 510, 307, 300, 4317, 264, 881, 1021, 1558, 293, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.12188331802169998, "compression_ratio": 1.6263157894736842, "no_speech_prob": 2.7533043976291083e-05}, {"id": 484, "seek": 271812, "start": 2738.8399999999997, "end": 2744.72, "text": " that now because my claim here is that perhaps the most important idea and", "tokens": [50364, 1500, 48814, 293, 498, 291, 574, 412, 309, 309, 1542, 257, 688, 411, 527, 3097, 992, 3847, 527, 50788, 50788, 661, 48814, 300, 321, 600, 668, 1364, 365, 457, 309, 311, 5361, 264, 6175, 264, 16949, 51092, 51092, 341, 307, 341, 307, 1219, 257, 1500, 992, 293, 370, 321, 434, 516, 281, 751, 257, 707, 857, 466, 51400, 51400, 300, 586, 570, 452, 3932, 510, 307, 300, 4317, 264, 881, 1021, 1558, 293, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.12188331802169998, "compression_ratio": 1.6263157894736842, "no_speech_prob": 2.7533043976291083e-05}, {"id": 485, "seek": 274472, "start": 2744.72, "end": 2750.0, "text": " machine learning is the idea of having separate training validation and test", "tokens": [50364, 3479, 2539, 307, 264, 1558, 295, 1419, 4994, 3097, 24071, 293, 1500, 50628, 50628, 42856, 1338, 50854, 51254, 370, 1500, 293, 24071, 6352, 366, 439, 466, 16696, 293, 14905, 337, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.2578497774460736, "compression_ratio": 1.5185185185185186, "no_speech_prob": 5.063425123807974e-05}, {"id": 486, "seek": 274472, "start": 2750.0, "end": 2754.52, "text": " datasets yeah", "tokens": [50364, 3479, 2539, 307, 264, 1558, 295, 1419, 4994, 3097, 24071, 293, 1500, 50628, 50628, 42856, 1338, 50854, 51254, 370, 1500, 293, 24071, 6352, 366, 439, 466, 16696, 293, 14905, 337, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.2578497774460736, "compression_ratio": 1.5185185185185186, "no_speech_prob": 5.063425123807974e-05}, {"id": 487, "seek": 274472, "start": 2762.52, "end": 2769.6, "text": " so test and validation sets are all about identifying and controlling for", "tokens": [50364, 3479, 2539, 307, 264, 1558, 295, 1419, 4994, 3097, 24071, 293, 1500, 50628, 50628, 42856, 1338, 50854, 51254, 370, 1500, 293, 24071, 6352, 366, 439, 466, 16696, 293, 14905, 337, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.2578497774460736, "compression_ratio": 1.5185185185185186, "no_speech_prob": 5.063425123807974e-05}, {"id": 488, "seek": 276960, "start": 2769.6, "end": 2774.7999999999997, "text": " something called overfitting and we're going to try and learn about this through", "tokens": [50364, 746, 1219, 670, 69, 2414, 293, 321, 434, 516, 281, 853, 293, 1466, 466, 341, 807, 50624, 50624, 1365, 370, 341, 307, 264, 912, 1589, 300, 311, 294, 300, 48751, 22631, 21060, 286, 600, 445, 50948, 50948, 829, 309, 322, 512, 9788, 510, 370, 286, 478, 516, 281, 1884, 257, 2445, 510, 1219, 7542, 51496, 51496, 6754, 293, 286, 478, 767, 516, 281, 764, 264, 912, 1412, 300, 286, 500, 380, 458, 498, 291, 434, 257, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.1039886713027954, "compression_ratio": 1.6736842105263159, "no_speech_prob": 4.985480700270273e-05}, {"id": 489, "seek": 276960, "start": 2774.7999999999997, "end": 2781.2799999999997, "text": " example so this is the same information that's in that Kaggle notebook I've just", "tokens": [50364, 746, 1219, 670, 69, 2414, 293, 321, 434, 516, 281, 853, 293, 1466, 466, 341, 807, 50624, 50624, 1365, 370, 341, 307, 264, 912, 1589, 300, 311, 294, 300, 48751, 22631, 21060, 286, 600, 445, 50948, 50948, 829, 309, 322, 512, 9788, 510, 370, 286, 478, 516, 281, 1884, 257, 2445, 510, 1219, 7542, 51496, 51496, 6754, 293, 286, 478, 767, 516, 281, 764, 264, 912, 1412, 300, 286, 500, 380, 458, 498, 291, 434, 257, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.1039886713027954, "compression_ratio": 1.6736842105263159, "no_speech_prob": 4.985480700270273e-05}, {"id": 490, "seek": 276960, "start": 2781.2799999999997, "end": 2792.24, "text": " put it on some slides here so I'm going to create a function here called plot", "tokens": [50364, 746, 1219, 670, 69, 2414, 293, 321, 434, 516, 281, 853, 293, 1466, 466, 341, 807, 50624, 50624, 1365, 370, 341, 307, 264, 912, 1589, 300, 311, 294, 300, 48751, 22631, 21060, 286, 600, 445, 50948, 50948, 829, 309, 322, 512, 9788, 510, 370, 286, 478, 516, 281, 1884, 257, 2445, 510, 1219, 7542, 51496, 51496, 6754, 293, 286, 478, 767, 516, 281, 764, 264, 912, 1412, 300, 286, 500, 380, 458, 498, 291, 434, 257, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.1039886713027954, "compression_ratio": 1.6736842105263159, "no_speech_prob": 4.985480700270273e-05}, {"id": 491, "seek": 276960, "start": 2792.24, "end": 2796.56, "text": " poly and I'm actually going to use the same data that I don't know if you're a", "tokens": [50364, 746, 1219, 670, 69, 2414, 293, 321, 434, 516, 281, 853, 293, 1466, 466, 341, 807, 50624, 50624, 1365, 370, 341, 307, 264, 912, 1589, 300, 311, 294, 300, 48751, 22631, 21060, 286, 600, 445, 50948, 50948, 829, 309, 322, 512, 9788, 510, 370, 286, 478, 516, 281, 1884, 257, 2445, 510, 1219, 7542, 51496, 51496, 6754, 293, 286, 478, 767, 516, 281, 764, 264, 912, 1412, 300, 286, 500, 380, 458, 498, 291, 434, 257, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.1039886713027954, "compression_ratio": 1.6736842105263159, "no_speech_prob": 4.985480700270273e-05}, {"id": 492, "seek": 279656, "start": 2796.56, "end": 2805.72, "text": " member we used it earlier for trying to fit this quadratic we created a X and", "tokens": [50364, 4006, 321, 1143, 309, 3071, 337, 1382, 281, 3318, 341, 37262, 321, 2942, 257, 1783, 293, 50822, 50822, 512, 1783, 293, 512, 398, 1412, 341, 307, 264, 1412, 321, 434, 516, 281, 764, 293, 321, 434, 516, 281, 50992, 50992, 764, 341, 281, 574, 412, 670, 69, 2414, 370, 264, 4365, 295, 341, 2445, 500, 380, 1871, 51306, 51306, 886, 709, 437, 7001, 307, 437, 321, 360, 365, 309, 597, 307, 300, 309, 4045, 505, 281, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.10332566499710083, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.000131318811327219}, {"id": 493, "seek": 279656, "start": 2805.72, "end": 2809.12, "text": " some X and some Y data this is the data we're going to use and we're going to", "tokens": [50364, 4006, 321, 1143, 309, 3071, 337, 1382, 281, 3318, 341, 37262, 321, 2942, 257, 1783, 293, 50822, 50822, 512, 1783, 293, 512, 398, 1412, 341, 307, 264, 1412, 321, 434, 516, 281, 764, 293, 321, 434, 516, 281, 50992, 50992, 764, 341, 281, 574, 412, 670, 69, 2414, 370, 264, 4365, 295, 341, 2445, 500, 380, 1871, 51306, 51306, 886, 709, 437, 7001, 307, 437, 321, 360, 365, 309, 597, 307, 300, 309, 4045, 505, 281, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.10332566499710083, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.000131318811327219}, {"id": 494, "seek": 279656, "start": 2809.12, "end": 2815.4, "text": " use this to look at overfitting so the details of this function don't matter", "tokens": [50364, 4006, 321, 1143, 309, 3071, 337, 1382, 281, 3318, 341, 37262, 321, 2942, 257, 1783, 293, 50822, 50822, 512, 1783, 293, 512, 398, 1412, 341, 307, 264, 1412, 321, 434, 516, 281, 764, 293, 321, 434, 516, 281, 50992, 50992, 764, 341, 281, 574, 412, 670, 69, 2414, 370, 264, 4365, 295, 341, 2445, 500, 380, 1871, 51306, 51306, 886, 709, 437, 7001, 307, 437, 321, 360, 365, 309, 597, 307, 300, 309, 4045, 505, 281, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.10332566499710083, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.000131318811327219}, {"id": 495, "seek": 279656, "start": 2815.4, "end": 2823.6, "text": " too much what matters is what we do with it which is that it allows us to", "tokens": [50364, 4006, 321, 1143, 309, 3071, 337, 1382, 281, 3318, 341, 37262, 321, 2942, 257, 1783, 293, 50822, 50822, 512, 1783, 293, 512, 398, 1412, 341, 307, 264, 1412, 321, 434, 516, 281, 764, 293, 321, 434, 516, 281, 50992, 50992, 764, 341, 281, 574, 412, 670, 69, 2414, 370, 264, 4365, 295, 341, 2445, 500, 380, 1871, 51306, 51306, 886, 709, 437, 7001, 307, 437, 321, 360, 365, 309, 597, 307, 300, 309, 4045, 505, 281, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.10332566499710083, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.000131318811327219}, {"id": 496, "seek": 282360, "start": 2823.6, "end": 2828.72, "text": " basically pass in the degree of a polynomial so for those of you that", "tokens": [50364, 1936, 1320, 294, 264, 4314, 295, 257, 26110, 370, 337, 729, 295, 291, 300, 50620, 50620, 1604, 257, 700, 4314, 26110, 307, 445, 257, 1622, 309, 311, 288, 6915, 257, 1783, 257, 1150, 50930, 50930, 4314, 26110, 486, 312, 288, 6915, 257, 8889, 1783, 1804, 363, 1783, 1804, 383, 2636, 4314, 51217, 51217, 26110, 486, 362, 257, 28733, 6409, 4314, 291, 458, 20837, 299, 293, 370, 5220, 293, 51470, 51470, 437, 286, 600, 1096, 510, 307, 286, 600, 43288, 437, 2314, 498, 321, 853, 281, 3318, 257, 1622, 281, 527, 51777, 51777], "temperature": 0.0, "avg_logprob": -0.11860096708257148, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.00010887115058721974}, {"id": 497, "seek": 282360, "start": 2828.72, "end": 2834.92, "text": " remember a first degree polynomial is just a line it's y equals a X a second", "tokens": [50364, 1936, 1320, 294, 264, 4314, 295, 257, 26110, 370, 337, 729, 295, 291, 300, 50620, 50620, 1604, 257, 700, 4314, 26110, 307, 445, 257, 1622, 309, 311, 288, 6915, 257, 1783, 257, 1150, 50930, 50930, 4314, 26110, 486, 312, 288, 6915, 257, 8889, 1783, 1804, 363, 1783, 1804, 383, 2636, 4314, 51217, 51217, 26110, 486, 362, 257, 28733, 6409, 4314, 291, 458, 20837, 299, 293, 370, 5220, 293, 51470, 51470, 437, 286, 600, 1096, 510, 307, 286, 600, 43288, 437, 2314, 498, 321, 853, 281, 3318, 257, 1622, 281, 527, 51777, 51777], "temperature": 0.0, "avg_logprob": -0.11860096708257148, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.00010887115058721974}, {"id": 498, "seek": 282360, "start": 2834.92, "end": 2840.66, "text": " degree polynomial will be y equals a squared X plus B X plus C third degree", "tokens": [50364, 1936, 1320, 294, 264, 4314, 295, 257, 26110, 370, 337, 729, 295, 291, 300, 50620, 50620, 1604, 257, 700, 4314, 26110, 307, 445, 257, 1622, 309, 311, 288, 6915, 257, 1783, 257, 1150, 50930, 50930, 4314, 26110, 486, 312, 288, 6915, 257, 8889, 1783, 1804, 363, 1783, 1804, 383, 2636, 4314, 51217, 51217, 26110, 486, 362, 257, 28733, 6409, 4314, 291, 458, 20837, 299, 293, 370, 5220, 293, 51470, 51470, 437, 286, 600, 1096, 510, 307, 286, 600, 43288, 437, 2314, 498, 321, 853, 281, 3318, 257, 1622, 281, 527, 51777, 51777], "temperature": 0.0, "avg_logprob": -0.11860096708257148, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.00010887115058721974}, {"id": 499, "seek": 282360, "start": 2840.66, "end": 2845.72, "text": " polynomial will have a cubic fourth degree you know quartic and so forth and", "tokens": [50364, 1936, 1320, 294, 264, 4314, 295, 257, 26110, 370, 337, 729, 295, 291, 300, 50620, 50620, 1604, 257, 700, 4314, 26110, 307, 445, 257, 1622, 309, 311, 288, 6915, 257, 1783, 257, 1150, 50930, 50930, 4314, 26110, 486, 312, 288, 6915, 257, 8889, 1783, 1804, 363, 1783, 1804, 383, 2636, 4314, 51217, 51217, 26110, 486, 362, 257, 28733, 6409, 4314, 291, 458, 20837, 299, 293, 370, 5220, 293, 51470, 51470, 437, 286, 600, 1096, 510, 307, 286, 600, 43288, 437, 2314, 498, 321, 853, 281, 3318, 257, 1622, 281, 527, 51777, 51777], "temperature": 0.0, "avg_logprob": -0.11860096708257148, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.00010887115058721974}, {"id": 500, "seek": 282360, "start": 2845.72, "end": 2851.86, "text": " what I've done here is I've plotted what happens if we try to fit a line to our", "tokens": [50364, 1936, 1320, 294, 264, 4314, 295, 257, 26110, 370, 337, 729, 295, 291, 300, 50620, 50620, 1604, 257, 700, 4314, 26110, 307, 445, 257, 1622, 309, 311, 288, 6915, 257, 1783, 257, 1150, 50930, 50930, 4314, 26110, 486, 312, 288, 6915, 257, 8889, 1783, 1804, 363, 1783, 1804, 383, 2636, 4314, 51217, 51217, 26110, 486, 362, 257, 28733, 6409, 4314, 291, 458, 20837, 299, 293, 370, 5220, 293, 51470, 51470, 437, 286, 600, 1096, 510, 307, 286, 600, 43288, 437, 2314, 498, 321, 853, 281, 3318, 257, 1622, 281, 527, 51777, 51777], "temperature": 0.0, "avg_logprob": -0.11860096708257148, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.00010887115058721974}, {"id": 501, "seek": 285186, "start": 2851.86, "end": 2861.36, "text": " data it doesn't fit very well so what happened here is we we did a linear", "tokens": [50364, 1412, 309, 1177, 380, 3318, 588, 731, 370, 437, 2011, 510, 307, 321, 321, 630, 257, 8213, 50839, 50839, 24590, 293, 437, 321, 434, 1228, 510, 307, 257, 588, 1627, 6405, 1219, 2180, 22681, 12, 306, 1083, 51077, 51077, 2180, 22681, 12, 306, 1083, 307, 746, 300, 291, 458, 286, 519, 309, 1116, 312, 3143, 281, 584, 309, 311, 8704, 51269, 51269, 4761, 337, 733, 295, 7230, 3479, 2539, 7150, 411, 733, 295, 8213, 51493, 51493, 30268, 293, 1507, 411, 300, 286, 478, 588, 7339, 9606, 295, 613, 721, 457, 51767, 51767], "temperature": 0.0, "avg_logprob": -0.12277867946218937, "compression_ratio": 1.75, "no_speech_prob": 2.5865621864795685e-05}, {"id": 502, "seek": 285186, "start": 2861.36, "end": 2866.1200000000003, "text": " regression and what we're using here is a very cool library called scikit-learn", "tokens": [50364, 1412, 309, 1177, 380, 3318, 588, 731, 370, 437, 2011, 510, 307, 321, 321, 630, 257, 8213, 50839, 50839, 24590, 293, 437, 321, 434, 1228, 510, 307, 257, 588, 1627, 6405, 1219, 2180, 22681, 12, 306, 1083, 51077, 51077, 2180, 22681, 12, 306, 1083, 307, 746, 300, 291, 458, 286, 519, 309, 1116, 312, 3143, 281, 584, 309, 311, 8704, 51269, 51269, 4761, 337, 733, 295, 7230, 3479, 2539, 7150, 411, 733, 295, 8213, 51493, 51493, 30268, 293, 1507, 411, 300, 286, 478, 588, 7339, 9606, 295, 613, 721, 457, 51767, 51767], "temperature": 0.0, "avg_logprob": -0.12277867946218937, "compression_ratio": 1.75, "no_speech_prob": 2.5865621864795685e-05}, {"id": 503, "seek": 285186, "start": 2866.1200000000003, "end": 2869.96, "text": " scikit-learn is something that you know I think it'd be fair to say it's mainly", "tokens": [50364, 1412, 309, 1177, 380, 3318, 588, 731, 370, 437, 2011, 510, 307, 321, 321, 630, 257, 8213, 50839, 50839, 24590, 293, 437, 321, 434, 1228, 510, 307, 257, 588, 1627, 6405, 1219, 2180, 22681, 12, 306, 1083, 51077, 51077, 2180, 22681, 12, 306, 1083, 307, 746, 300, 291, 458, 286, 519, 309, 1116, 312, 3143, 281, 584, 309, 311, 8704, 51269, 51269, 4761, 337, 733, 295, 7230, 3479, 2539, 7150, 411, 733, 295, 8213, 51493, 51493, 30268, 293, 1507, 411, 300, 286, 478, 588, 7339, 9606, 295, 613, 721, 457, 51767, 51767], "temperature": 0.0, "avg_logprob": -0.12277867946218937, "compression_ratio": 1.75, "no_speech_prob": 2.5865621864795685e-05}, {"id": 504, "seek": 285186, "start": 2869.96, "end": 2874.44, "text": " designed for kind of classic machine learning methods like kind of linear", "tokens": [50364, 1412, 309, 1177, 380, 3318, 588, 731, 370, 437, 2011, 510, 307, 321, 321, 630, 257, 8213, 50839, 50839, 24590, 293, 437, 321, 434, 1228, 510, 307, 257, 588, 1627, 6405, 1219, 2180, 22681, 12, 306, 1083, 51077, 51077, 2180, 22681, 12, 306, 1083, 307, 746, 300, 291, 458, 286, 519, 309, 1116, 312, 3143, 281, 584, 309, 311, 8704, 51269, 51269, 4761, 337, 733, 295, 7230, 3479, 2539, 7150, 411, 733, 295, 8213, 51493, 51493, 30268, 293, 1507, 411, 300, 286, 478, 588, 7339, 9606, 295, 613, 721, 457, 51767, 51767], "temperature": 0.0, "avg_logprob": -0.12277867946218937, "compression_ratio": 1.75, "no_speech_prob": 2.5865621864795685e-05}, {"id": 505, "seek": 285186, "start": 2874.44, "end": 2879.92, "text": " aggression and stuff like that I'm very advanced versions of these things but", "tokens": [50364, 1412, 309, 1177, 380, 3318, 588, 731, 370, 437, 2011, 510, 307, 321, 321, 630, 257, 8213, 50839, 50839, 24590, 293, 437, 321, 434, 1228, 510, 307, 257, 588, 1627, 6405, 1219, 2180, 22681, 12, 306, 1083, 51077, 51077, 2180, 22681, 12, 306, 1083, 307, 746, 300, 291, 458, 286, 519, 309, 1116, 312, 3143, 281, 584, 309, 311, 8704, 51269, 51269, 4761, 337, 733, 295, 7230, 3479, 2539, 7150, 411, 733, 295, 8213, 51493, 51493, 30268, 293, 1507, 411, 300, 286, 478, 588, 7339, 9606, 295, 613, 721, 457, 51767, 51767], "temperature": 0.0, "avg_logprob": -0.12277867946218937, "compression_ratio": 1.75, "no_speech_prob": 2.5865621864795685e-05}, {"id": 506, "seek": 287992, "start": 2879.92, "end": 2883.48, "text": " it's also great for doing these quick and dirty things so in this case I wanted", "tokens": [50364, 309, 311, 611, 869, 337, 884, 613, 1702, 293, 9360, 721, 370, 294, 341, 1389, 286, 1415, 50542, 50542, 281, 360, 257, 437, 311, 1219, 257, 26110, 24590, 597, 307, 15669, 428, 50698, 50698, 26110, 281, 1412, 293, 309, 311, 445, 613, 732, 3876, 295, 3089, 309, 311, 257, 1687, 1481, 50898, 50898, 6405, 370, 294, 341, 1389, 257, 4314, 472, 26110, 307, 445, 257, 1622, 370, 286, 3318, 309, 293, 51138, 51138, 550, 286, 855, 309, 365, 264, 1412, 293, 456, 309, 307, 586, 300, 311, 437, 321, 818, 833, 6845, 51398, 51398, 597, 307, 281, 584, 456, 311, 406, 1547, 733, 295, 14024, 294, 341, 2316, 286, 3318, 281, 281, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.09459770958999107, "compression_ratio": 1.873469387755102, "no_speech_prob": 8.218540460802615e-05}, {"id": 507, "seek": 287992, "start": 2883.48, "end": 2886.6, "text": " to do a what's called a polynomial regression which is fitting your", "tokens": [50364, 309, 311, 611, 869, 337, 884, 613, 1702, 293, 9360, 721, 370, 294, 341, 1389, 286, 1415, 50542, 50542, 281, 360, 257, 437, 311, 1219, 257, 26110, 24590, 597, 307, 15669, 428, 50698, 50698, 26110, 281, 1412, 293, 309, 311, 445, 613, 732, 3876, 295, 3089, 309, 311, 257, 1687, 1481, 50898, 50898, 6405, 370, 294, 341, 1389, 257, 4314, 472, 26110, 307, 445, 257, 1622, 370, 286, 3318, 309, 293, 51138, 51138, 550, 286, 855, 309, 365, 264, 1412, 293, 456, 309, 307, 586, 300, 311, 437, 321, 818, 833, 6845, 51398, 51398, 597, 307, 281, 584, 456, 311, 406, 1547, 733, 295, 14024, 294, 341, 2316, 286, 3318, 281, 281, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.09459770958999107, "compression_ratio": 1.873469387755102, "no_speech_prob": 8.218540460802615e-05}, {"id": 508, "seek": 287992, "start": 2886.6, "end": 2890.6, "text": " polynomial to data and it's just these two lines of code it's a super nice", "tokens": [50364, 309, 311, 611, 869, 337, 884, 613, 1702, 293, 9360, 721, 370, 294, 341, 1389, 286, 1415, 50542, 50542, 281, 360, 257, 437, 311, 1219, 257, 26110, 24590, 597, 307, 15669, 428, 50698, 50698, 26110, 281, 1412, 293, 309, 311, 445, 613, 732, 3876, 295, 3089, 309, 311, 257, 1687, 1481, 50898, 50898, 6405, 370, 294, 341, 1389, 257, 4314, 472, 26110, 307, 445, 257, 1622, 370, 286, 3318, 309, 293, 51138, 51138, 550, 286, 855, 309, 365, 264, 1412, 293, 456, 309, 307, 586, 300, 311, 437, 321, 818, 833, 6845, 51398, 51398, 597, 307, 281, 584, 456, 311, 406, 1547, 733, 295, 14024, 294, 341, 2316, 286, 3318, 281, 281, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.09459770958999107, "compression_ratio": 1.873469387755102, "no_speech_prob": 8.218540460802615e-05}, {"id": 509, "seek": 287992, "start": 2890.6, "end": 2895.4, "text": " library so in this case a degree one polynomial is just a line so I fit it and", "tokens": [50364, 309, 311, 611, 869, 337, 884, 613, 1702, 293, 9360, 721, 370, 294, 341, 1389, 286, 1415, 50542, 50542, 281, 360, 257, 437, 311, 1219, 257, 26110, 24590, 597, 307, 15669, 428, 50698, 50698, 26110, 281, 1412, 293, 309, 311, 445, 613, 732, 3876, 295, 3089, 309, 311, 257, 1687, 1481, 50898, 50898, 6405, 370, 294, 341, 1389, 257, 4314, 472, 26110, 307, 445, 257, 1622, 370, 286, 3318, 309, 293, 51138, 51138, 550, 286, 855, 309, 365, 264, 1412, 293, 456, 309, 307, 586, 300, 311, 437, 321, 818, 833, 6845, 51398, 51398, 597, 307, 281, 584, 456, 311, 406, 1547, 733, 295, 14024, 294, 341, 2316, 286, 3318, 281, 281, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.09459770958999107, "compression_ratio": 1.873469387755102, "no_speech_prob": 8.218540460802615e-05}, {"id": 510, "seek": 287992, "start": 2895.4, "end": 2900.6, "text": " then I show it with the data and there it is now that's what we call underfit", "tokens": [50364, 309, 311, 611, 869, 337, 884, 613, 1702, 293, 9360, 721, 370, 294, 341, 1389, 286, 1415, 50542, 50542, 281, 360, 257, 437, 311, 1219, 257, 26110, 24590, 597, 307, 15669, 428, 50698, 50698, 26110, 281, 1412, 293, 309, 311, 445, 613, 732, 3876, 295, 3089, 309, 311, 257, 1687, 1481, 50898, 50898, 6405, 370, 294, 341, 1389, 257, 4314, 472, 26110, 307, 445, 257, 1622, 370, 286, 3318, 309, 293, 51138, 51138, 550, 286, 855, 309, 365, 264, 1412, 293, 456, 309, 307, 586, 300, 311, 437, 321, 818, 833, 6845, 51398, 51398, 597, 307, 281, 584, 456, 311, 406, 1547, 733, 295, 14024, 294, 341, 2316, 286, 3318, 281, 281, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.09459770958999107, "compression_ratio": 1.873469387755102, "no_speech_prob": 8.218540460802615e-05}, {"id": 511, "seek": 287992, "start": 2900.6, "end": 2908.28, "text": " which is to say there's not enough kind of complexity in this model I fit to to", "tokens": [50364, 309, 311, 611, 869, 337, 884, 613, 1702, 293, 9360, 721, 370, 294, 341, 1389, 286, 1415, 50542, 50542, 281, 360, 257, 437, 311, 1219, 257, 26110, 24590, 597, 307, 15669, 428, 50698, 50698, 26110, 281, 1412, 293, 309, 311, 445, 613, 732, 3876, 295, 3089, 309, 311, 257, 1687, 1481, 50898, 50898, 6405, 370, 294, 341, 1389, 257, 4314, 472, 26110, 307, 445, 257, 1622, 370, 286, 3318, 309, 293, 51138, 51138, 550, 286, 855, 309, 365, 264, 1412, 293, 456, 309, 307, 586, 300, 311, 437, 321, 818, 833, 6845, 51398, 51398, 597, 307, 281, 584, 456, 311, 406, 1547, 733, 295, 14024, 294, 341, 2316, 286, 3318, 281, 281, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.09459770958999107, "compression_ratio": 1.873469387755102, "no_speech_prob": 8.218540460802615e-05}, {"id": 512, "seek": 290828, "start": 2908.28, "end": 2916.0, "text": " match the data that's there so an underfit model is a problem it's got to", "tokens": [50364, 2995, 264, 1412, 300, 311, 456, 370, 364, 833, 6845, 2316, 307, 257, 1154, 309, 311, 658, 281, 50750, 50750, 312, 39531, 28035, 291, 458, 439, 264, 1507, 493, 510, 321, 434, 516, 281, 312, 50908, 50908, 32884, 886, 2295, 439, 264, 1507, 760, 510, 321, 434, 32884, 886, 2295, 439, 264, 51068, 51068, 1507, 294, 264, 2808, 576, 312, 32884, 886, 1090, 257, 2689, 29227, 307, 51294, 51294, 300, 411, 18587, 5245, 366, 733, 295, 544, 12924, 294, 512, 636, 457, 5245, 300, 51606, 51606], "temperature": 0.0, "avg_logprob": -0.11112882874228737, "compression_ratio": 1.9095477386934674, "no_speech_prob": 5.390644582803361e-05}, {"id": 513, "seek": 290828, "start": 2916.0, "end": 2919.1600000000003, "text": " be systematically biased you know all the stuff up here we're going to be", "tokens": [50364, 2995, 264, 1412, 300, 311, 456, 370, 364, 833, 6845, 2316, 307, 257, 1154, 309, 311, 658, 281, 50750, 50750, 312, 39531, 28035, 291, 458, 439, 264, 1507, 493, 510, 321, 434, 516, 281, 312, 50908, 50908, 32884, 886, 2295, 439, 264, 1507, 760, 510, 321, 434, 32884, 886, 2295, 439, 264, 51068, 51068, 1507, 294, 264, 2808, 576, 312, 32884, 886, 1090, 257, 2689, 29227, 307, 51294, 51294, 300, 411, 18587, 5245, 366, 733, 295, 544, 12924, 294, 512, 636, 457, 5245, 300, 51606, 51606], "temperature": 0.0, "avg_logprob": -0.11112882874228737, "compression_ratio": 1.9095477386934674, "no_speech_prob": 5.390644582803361e-05}, {"id": 514, "seek": 290828, "start": 2919.1600000000003, "end": 2922.36, "text": " predicting too low all the stuff down here we're predicting too low all the", "tokens": [50364, 2995, 264, 1412, 300, 311, 456, 370, 364, 833, 6845, 2316, 307, 257, 1154, 309, 311, 658, 281, 50750, 50750, 312, 39531, 28035, 291, 458, 439, 264, 1507, 493, 510, 321, 434, 516, 281, 312, 50908, 50908, 32884, 886, 2295, 439, 264, 1507, 760, 510, 321, 434, 32884, 886, 2295, 439, 264, 51068, 51068, 1507, 294, 264, 2808, 576, 312, 32884, 886, 1090, 257, 2689, 29227, 307, 51294, 51294, 300, 411, 18587, 5245, 366, 733, 295, 544, 12924, 294, 512, 636, 457, 5245, 300, 51606, 51606], "temperature": 0.0, "avg_logprob": -0.11112882874228737, "compression_ratio": 1.9095477386934674, "no_speech_prob": 5.390644582803361e-05}, {"id": 515, "seek": 290828, "start": 2922.36, "end": 2926.88, "text": " stuff in the middle would be predicting too high a common misunderstanding is", "tokens": [50364, 2995, 264, 1412, 300, 311, 456, 370, 364, 833, 6845, 2316, 307, 257, 1154, 309, 311, 658, 281, 50750, 50750, 312, 39531, 28035, 291, 458, 439, 264, 1507, 493, 510, 321, 434, 516, 281, 312, 50908, 50908, 32884, 886, 2295, 439, 264, 1507, 760, 510, 321, 434, 32884, 886, 2295, 439, 264, 51068, 51068, 1507, 294, 264, 2808, 576, 312, 32884, 886, 1090, 257, 2689, 29227, 307, 51294, 51294, 300, 411, 18587, 5245, 366, 733, 295, 544, 12924, 294, 512, 636, 457, 5245, 300, 51606, 51606], "temperature": 0.0, "avg_logprob": -0.11112882874228737, "compression_ratio": 1.9095477386934674, "no_speech_prob": 5.390644582803361e-05}, {"id": 516, "seek": 290828, "start": 2926.88, "end": 2933.1200000000003, "text": " that like simpler models are kind of more reliable in some way but models that", "tokens": [50364, 2995, 264, 1412, 300, 311, 456, 370, 364, 833, 6845, 2316, 307, 257, 1154, 309, 311, 658, 281, 50750, 50750, 312, 39531, 28035, 291, 458, 439, 264, 1507, 493, 510, 321, 434, 516, 281, 312, 50908, 50908, 32884, 886, 2295, 439, 264, 1507, 760, 510, 321, 434, 32884, 886, 2295, 439, 264, 51068, 51068, 1507, 294, 264, 2808, 576, 312, 32884, 886, 1090, 257, 2689, 29227, 307, 51294, 51294, 300, 411, 18587, 5245, 366, 733, 295, 544, 12924, 294, 512, 636, 457, 5245, 300, 51606, 51606], "temperature": 0.0, "avg_logprob": -0.11112882874228737, "compression_ratio": 1.9095477386934674, "no_speech_prob": 5.390644582803361e-05}, {"id": 517, "seek": 293312, "start": 2933.12, "end": 2941.56, "text": " are too simple will be systematically incorrect as you see here what happens", "tokens": [50364, 366, 886, 2199, 486, 312, 39531, 18424, 382, 291, 536, 510, 437, 2314, 50786, 50786, 498, 321, 3318, 257, 1266, 4314, 26110, 300, 311, 406, 869, 2139, 294, 341, 1389, 309, 311, 406, 51292, 51292, 534, 4099, 505, 437, 264, 3539, 1604, 341, 390, 7993, 257, 37262, 51540, 51540, 341, 307, 4140, 281, 2995, 558, 4098, 412, 264, 5314, 510, 309, 311, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.11181481679280598, "compression_ratio": 1.5978260869565217, "no_speech_prob": 2.429919913993217e-05}, {"id": 518, "seek": 293312, "start": 2941.56, "end": 2951.68, "text": " if we fit a 10 degree polynomial that's not great either in this case it's not", "tokens": [50364, 366, 886, 2199, 486, 312, 39531, 18424, 382, 291, 536, 510, 437, 2314, 50786, 50786, 498, 321, 3318, 257, 1266, 4314, 26110, 300, 311, 406, 869, 2139, 294, 341, 1389, 309, 311, 406, 51292, 51292, 534, 4099, 505, 437, 264, 3539, 1604, 341, 390, 7993, 257, 37262, 51540, 51540, 341, 307, 4140, 281, 2995, 558, 4098, 412, 264, 5314, 510, 309, 311, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.11181481679280598, "compression_ratio": 1.5978260869565217, "no_speech_prob": 2.429919913993217e-05}, {"id": 519, "seek": 293312, "start": 2951.68, "end": 2956.64, "text": " really showing us what the actual remember this was originally a quadratic", "tokens": [50364, 366, 886, 2199, 486, 312, 39531, 18424, 382, 291, 536, 510, 437, 2314, 50786, 50786, 498, 321, 3318, 257, 1266, 4314, 26110, 300, 311, 406, 869, 2139, 294, 341, 1389, 309, 311, 406, 51292, 51292, 534, 4099, 505, 437, 264, 3539, 1604, 341, 390, 7993, 257, 37262, 51540, 51540, 341, 307, 4140, 281, 2995, 558, 4098, 412, 264, 5314, 510, 309, 311, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.11181481679280598, "compression_ratio": 1.5978260869565217, "no_speech_prob": 2.429919913993217e-05}, {"id": 520, "seek": 293312, "start": 2956.64, "end": 2959.88, "text": " this is meant to match right particularly at the ends here it's", "tokens": [50364, 366, 886, 2199, 486, 312, 39531, 18424, 382, 291, 536, 510, 437, 2314, 50786, 50786, 498, 321, 3318, 257, 1266, 4314, 26110, 300, 311, 406, 869, 2139, 294, 341, 1389, 309, 311, 406, 51292, 51292, 534, 4099, 505, 437, 264, 3539, 1604, 341, 390, 7993, 257, 37262, 51540, 51540, 341, 307, 4140, 281, 2995, 558, 4098, 412, 264, 5314, 510, 309, 311, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.11181481679280598, "compression_ratio": 1.5978260869565217, "no_speech_prob": 2.429919913993217e-05}, {"id": 521, "seek": 295988, "start": 2959.88, "end": 2964.88, "text": " predicting things that are way above what we would expect in real life right", "tokens": [50364, 32884, 721, 300, 366, 636, 3673, 437, 321, 576, 2066, 294, 957, 993, 558, 50614, 50614, 293, 309, 311, 1382, 281, 483, 534, 309, 311, 1382, 534, 1152, 281, 483, 807, 341, 50748, 50748, 935, 457, 4448, 341, 935, 390, 445, 512, 5658, 558, 370, 341, 307, 437, 321, 818, 50970, 50970, 670, 6845, 309, 311, 1096, 257, 665, 1691, 295, 15669, 281, 527, 1900, 1412, 2793, 457, 498, 321, 51320, 51320, 6889, 512, 544, 1412, 2793, 490, 341, 7316, 6095, 321, 1391, 576, 51592, 51592, 9091, 436, 434, 406, 516, 281, 312, 588, 1998, 281, 341, 4098, 498, 436, 434, 257, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.11332507403391712, "compression_ratio": 1.8699186991869918, "no_speech_prob": 2.468127968313638e-05}, {"id": 522, "seek": 295988, "start": 2964.88, "end": 2967.56, "text": " and it's trying to get really it's trying really hard to get through this", "tokens": [50364, 32884, 721, 300, 366, 636, 3673, 437, 321, 576, 2066, 294, 957, 993, 558, 50614, 50614, 293, 309, 311, 1382, 281, 483, 534, 309, 311, 1382, 534, 1152, 281, 483, 807, 341, 50748, 50748, 935, 457, 4448, 341, 935, 390, 445, 512, 5658, 558, 370, 341, 307, 437, 321, 818, 50970, 50970, 670, 6845, 309, 311, 1096, 257, 665, 1691, 295, 15669, 281, 527, 1900, 1412, 2793, 457, 498, 321, 51320, 51320, 6889, 512, 544, 1412, 2793, 490, 341, 7316, 6095, 321, 1391, 576, 51592, 51592, 9091, 436, 434, 406, 516, 281, 312, 588, 1998, 281, 341, 4098, 498, 436, 434, 257, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.11332507403391712, "compression_ratio": 1.8699186991869918, "no_speech_prob": 2.468127968313638e-05}, {"id": 523, "seek": 295988, "start": 2967.56, "end": 2972.0, "text": " point but clearly this point was just some noise right so this is what we call", "tokens": [50364, 32884, 721, 300, 366, 636, 3673, 437, 321, 576, 2066, 294, 957, 993, 558, 50614, 50614, 293, 309, 311, 1382, 281, 483, 534, 309, 311, 1382, 534, 1152, 281, 483, 807, 341, 50748, 50748, 935, 457, 4448, 341, 935, 390, 445, 512, 5658, 558, 370, 341, 307, 437, 321, 818, 50970, 50970, 670, 6845, 309, 311, 1096, 257, 665, 1691, 295, 15669, 281, 527, 1900, 1412, 2793, 457, 498, 321, 51320, 51320, 6889, 512, 544, 1412, 2793, 490, 341, 7316, 6095, 321, 1391, 576, 51592, 51592, 9091, 436, 434, 406, 516, 281, 312, 588, 1998, 281, 341, 4098, 498, 436, 434, 257, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.11332507403391712, "compression_ratio": 1.8699186991869918, "no_speech_prob": 2.468127968313638e-05}, {"id": 524, "seek": 295988, "start": 2972.0, "end": 2979.0, "text": " overfit it's done a good job of fitting to our exact data points but if we", "tokens": [50364, 32884, 721, 300, 366, 636, 3673, 437, 321, 576, 2066, 294, 957, 993, 558, 50614, 50614, 293, 309, 311, 1382, 281, 483, 534, 309, 311, 1382, 534, 1152, 281, 483, 807, 341, 50748, 50748, 935, 457, 4448, 341, 935, 390, 445, 512, 5658, 558, 370, 341, 307, 437, 321, 818, 50970, 50970, 670, 6845, 309, 311, 1096, 257, 665, 1691, 295, 15669, 281, 527, 1900, 1412, 2793, 457, 498, 321, 51320, 51320, 6889, 512, 544, 1412, 2793, 490, 341, 7316, 6095, 321, 1391, 576, 51592, 51592, 9091, 436, 434, 406, 516, 281, 312, 588, 1998, 281, 341, 4098, 498, 436, 434, 257, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.11332507403391712, "compression_ratio": 1.8699186991869918, "no_speech_prob": 2.468127968313638e-05}, {"id": 525, "seek": 295988, "start": 2979.0, "end": 2984.44, "text": " sample some more data points from this distribution honestly we probably would", "tokens": [50364, 32884, 721, 300, 366, 636, 3673, 437, 321, 576, 2066, 294, 957, 993, 558, 50614, 50614, 293, 309, 311, 1382, 281, 483, 534, 309, 311, 1382, 534, 1152, 281, 483, 807, 341, 50748, 50748, 935, 457, 4448, 341, 935, 390, 445, 512, 5658, 558, 370, 341, 307, 437, 321, 818, 50970, 50970, 670, 6845, 309, 311, 1096, 257, 665, 1691, 295, 15669, 281, 527, 1900, 1412, 2793, 457, 498, 321, 51320, 51320, 6889, 512, 544, 1412, 2793, 490, 341, 7316, 6095, 321, 1391, 576, 51592, 51592, 9091, 436, 434, 406, 516, 281, 312, 588, 1998, 281, 341, 4098, 498, 436, 434, 257, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.11332507403391712, "compression_ratio": 1.8699186991869918, "no_speech_prob": 2.468127968313638e-05}, {"id": 526, "seek": 295988, "start": 2984.44, "end": 2987.0, "text": " suspect they're not going to be very close to this particularly if they're a", "tokens": [50364, 32884, 721, 300, 366, 636, 3673, 437, 321, 576, 2066, 294, 957, 993, 558, 50614, 50614, 293, 309, 311, 1382, 281, 483, 534, 309, 311, 1382, 534, 1152, 281, 483, 807, 341, 50748, 50748, 935, 457, 4448, 341, 935, 390, 445, 512, 5658, 558, 370, 341, 307, 437, 321, 818, 50970, 50970, 670, 6845, 309, 311, 1096, 257, 665, 1691, 295, 15669, 281, 527, 1900, 1412, 2793, 457, 498, 321, 51320, 51320, 6889, 512, 544, 1412, 2793, 490, 341, 7316, 6095, 321, 1391, 576, 51592, 51592, 9091, 436, 434, 406, 516, 281, 312, 588, 1998, 281, 341, 4098, 498, 436, 434, 257, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.11332507403391712, "compression_ratio": 1.8699186991869918, "no_speech_prob": 2.468127968313638e-05}, {"id": 527, "seek": 298700, "start": 2987.0, "end": 2991.52, "text": " bit beyond the edges so that's what overfitting looks like we don't want", "tokens": [50364, 857, 4399, 264, 8819, 370, 300, 311, 437, 670, 69, 2414, 1542, 411, 321, 500, 380, 528, 50590, 50590, 833, 15669, 439, 670, 15669, 586, 833, 15669, 307, 767, 1238, 1858, 281, 50824, 50824, 5521, 570, 321, 393, 767, 574, 412, 527, 3097, 1412, 293, 536, 300, 309, 311, 51016, 51016, 406, 588, 1998, 670, 15669, 307, 257, 857, 6081, 281, 5521, 570, 264, 3097, 51342, 51342, 1412, 307, 767, 588, 1998, 586, 322, 264, 661, 1011, 510, 311, 437, 2314, 498, 321, 3318, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.0974510908126831, "compression_ratio": 1.9492385786802031, "no_speech_prob": 5.306808452587575e-05}, {"id": 528, "seek": 298700, "start": 2991.52, "end": 2996.2, "text": " under fitting all over fitting now under fitting is actually pretty easy to", "tokens": [50364, 857, 4399, 264, 8819, 370, 300, 311, 437, 670, 69, 2414, 1542, 411, 321, 500, 380, 528, 50590, 50590, 833, 15669, 439, 670, 15669, 586, 833, 15669, 307, 767, 1238, 1858, 281, 50824, 50824, 5521, 570, 321, 393, 767, 574, 412, 527, 3097, 1412, 293, 536, 300, 309, 311, 51016, 51016, 406, 588, 1998, 670, 15669, 307, 257, 857, 6081, 281, 5521, 570, 264, 3097, 51342, 51342, 1412, 307, 767, 588, 1998, 586, 322, 264, 661, 1011, 510, 311, 437, 2314, 498, 321, 3318, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.0974510908126831, "compression_ratio": 1.9492385786802031, "no_speech_prob": 5.306808452587575e-05}, {"id": 529, "seek": 298700, "start": 2996.2, "end": 3000.04, "text": " recognize because we can actually look at our training data and see that it's", "tokens": [50364, 857, 4399, 264, 8819, 370, 300, 311, 437, 670, 69, 2414, 1542, 411, 321, 500, 380, 528, 50590, 50590, 833, 15669, 439, 670, 15669, 586, 833, 15669, 307, 767, 1238, 1858, 281, 50824, 50824, 5521, 570, 321, 393, 767, 574, 412, 527, 3097, 1412, 293, 536, 300, 309, 311, 51016, 51016, 406, 588, 1998, 670, 15669, 307, 257, 857, 6081, 281, 5521, 570, 264, 3097, 51342, 51342, 1412, 307, 767, 588, 1998, 586, 322, 264, 661, 1011, 510, 311, 437, 2314, 498, 321, 3318, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.0974510908126831, "compression_ratio": 1.9492385786802031, "no_speech_prob": 5.306808452587575e-05}, {"id": 530, "seek": 298700, "start": 3000.04, "end": 3006.56, "text": " not very close over fitting is a bit harder to recognize because the training", "tokens": [50364, 857, 4399, 264, 8819, 370, 300, 311, 437, 670, 69, 2414, 1542, 411, 321, 500, 380, 528, 50590, 50590, 833, 15669, 439, 670, 15669, 586, 833, 15669, 307, 767, 1238, 1858, 281, 50824, 50824, 5521, 570, 321, 393, 767, 574, 412, 527, 3097, 1412, 293, 536, 300, 309, 311, 51016, 51016, 406, 588, 1998, 670, 15669, 307, 257, 857, 6081, 281, 5521, 570, 264, 3097, 51342, 51342, 1412, 307, 767, 588, 1998, 586, 322, 264, 661, 1011, 510, 311, 437, 2314, 498, 321, 3318, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.0974510908126831, "compression_ratio": 1.9492385786802031, "no_speech_prob": 5.306808452587575e-05}, {"id": 531, "seek": 298700, "start": 3006.56, "end": 3016.84, "text": " data is actually very close now on the other hand here's what happens if we fit", "tokens": [50364, 857, 4399, 264, 8819, 370, 300, 311, 437, 670, 69, 2414, 1542, 411, 321, 500, 380, 528, 50590, 50590, 833, 15669, 439, 670, 15669, 586, 833, 15669, 307, 767, 1238, 1858, 281, 50824, 50824, 5521, 570, 321, 393, 767, 574, 412, 527, 3097, 1412, 293, 536, 300, 309, 311, 51016, 51016, 406, 588, 1998, 670, 15669, 307, 257, 857, 6081, 281, 5521, 570, 264, 3097, 51342, 51342, 1412, 307, 767, 588, 1998, 586, 322, 264, 661, 1011, 510, 311, 437, 2314, 498, 321, 3318, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.0974510908126831, "compression_ratio": 1.9492385786802031, "no_speech_prob": 5.306808452587575e-05}, {"id": 532, "seek": 301684, "start": 3016.84, "end": 3024.76, "text": " a quadratic and here I've got both the real line and the fit line and you can", "tokens": [50364, 257, 37262, 293, 510, 286, 600, 658, 1293, 264, 957, 1622, 293, 264, 3318, 1622, 293, 291, 393, 50760, 50760, 536, 436, 434, 1238, 1998, 293, 300, 311, 295, 1164, 437, 321, 767, 528, 370, 577, 360, 51350, 51350, 321, 980, 1968, 321, 362, 746, 544, 411, 341, 420, 746, 544, 411, 341, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.06198552198577346, "compression_ratio": 1.6338028169014085, "no_speech_prob": 4.5394292101264e-05}, {"id": 533, "seek": 301684, "start": 3024.76, "end": 3036.56, "text": " see they're pretty close and that's of course what we actually want so how do", "tokens": [50364, 257, 37262, 293, 510, 286, 600, 658, 1293, 264, 957, 1622, 293, 264, 3318, 1622, 293, 291, 393, 50760, 50760, 536, 436, 434, 1238, 1998, 293, 300, 311, 295, 1164, 437, 321, 767, 528, 370, 577, 360, 51350, 51350, 321, 980, 1968, 321, 362, 746, 544, 411, 341, 420, 746, 544, 411, 341, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.06198552198577346, "compression_ratio": 1.6338028169014085, "no_speech_prob": 4.5394292101264e-05}, {"id": 534, "seek": 301684, "start": 3036.56, "end": 3043.32, "text": " we tell whether we have something more like this or something more like this", "tokens": [50364, 257, 37262, 293, 510, 286, 600, 658, 1293, 264, 957, 1622, 293, 264, 3318, 1622, 293, 291, 393, 50760, 50760, 536, 436, 434, 1238, 1998, 293, 300, 311, 295, 1164, 437, 321, 767, 528, 370, 577, 360, 51350, 51350, 321, 980, 1968, 321, 362, 746, 544, 411, 341, 420, 746, 544, 411, 341, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.06198552198577346, "compression_ratio": 1.6338028169014085, "no_speech_prob": 4.5394292101264e-05}, {"id": 535, "seek": 304332, "start": 3043.32, "end": 3047.84, "text": " well what we do is we do something pretty straightforward is we take our", "tokens": [50364, 731, 437, 321, 360, 307, 321, 360, 746, 1238, 15325, 307, 321, 747, 527, 50590, 50590, 3380, 1412, 992, 613, 2793, 293, 321, 4159, 257, 1326, 295, 552, 370, 718, 311, 584, 945, 4, 295, 50914, 50914, 552, 321, 550, 3318, 527, 2316, 1228, 787, 729, 2793, 321, 2378, 380, 7261, 293, 550, 51360, 51360, 321, 3481, 577, 665, 309, 307, 538, 1237, 412, 787, 264, 2793, 321, 7261, 370, 294, 341, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.0700137552462126, "compression_ratio": 1.6830601092896176, "no_speech_prob": 6.20432838331908e-05}, {"id": 536, "seek": 304332, "start": 3047.84, "end": 3054.32, "text": " original data set these points and we remove a few of them so let's say 20% of", "tokens": [50364, 731, 437, 321, 360, 307, 321, 360, 746, 1238, 15325, 307, 321, 747, 527, 50590, 50590, 3380, 1412, 992, 613, 2793, 293, 321, 4159, 257, 1326, 295, 552, 370, 718, 311, 584, 945, 4, 295, 50914, 50914, 552, 321, 550, 3318, 527, 2316, 1228, 787, 729, 2793, 321, 2378, 380, 7261, 293, 550, 51360, 51360, 321, 3481, 577, 665, 309, 307, 538, 1237, 412, 787, 264, 2793, 321, 7261, 370, 294, 341, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.0700137552462126, "compression_ratio": 1.6830601092896176, "no_speech_prob": 6.20432838331908e-05}, {"id": 537, "seek": 304332, "start": 3054.32, "end": 3063.2400000000002, "text": " them we then fit our model using only those points we haven't removed and then", "tokens": [50364, 731, 437, 321, 360, 307, 321, 360, 746, 1238, 15325, 307, 321, 747, 527, 50590, 50590, 3380, 1412, 992, 613, 2793, 293, 321, 4159, 257, 1326, 295, 552, 370, 718, 311, 584, 945, 4, 295, 50914, 50914, 552, 321, 550, 3318, 527, 2316, 1228, 787, 729, 2793, 321, 2378, 380, 7261, 293, 550, 51360, 51360, 321, 3481, 577, 665, 309, 307, 538, 1237, 412, 787, 264, 2793, 321, 7261, 370, 294, 341, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.0700137552462126, "compression_ratio": 1.6830601092896176, "no_speech_prob": 6.20432838331908e-05}, {"id": 538, "seek": 304332, "start": 3063.2400000000002, "end": 3070.0800000000004, "text": " we measure how good it is by looking at only the points we removed so in this", "tokens": [50364, 731, 437, 321, 360, 307, 321, 360, 746, 1238, 15325, 307, 321, 747, 527, 50590, 50590, 3380, 1412, 992, 613, 2793, 293, 321, 4159, 257, 1326, 295, 552, 370, 718, 311, 584, 945, 4, 295, 50914, 50914, 552, 321, 550, 3318, 527, 2316, 1228, 787, 729, 2793, 321, 2378, 380, 7261, 293, 550, 51360, 51360, 321, 3481, 577, 665, 309, 307, 538, 1237, 412, 787, 264, 2793, 321, 7261, 370, 294, 341, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.0700137552462126, "compression_ratio": 1.6830601092896176, "no_speech_prob": 6.20432838331908e-05}, {"id": 539, "seek": 307008, "start": 3070.08, "end": 3078.52, "text": " case let's say we had removed I'm just trying to think if I had removed this", "tokens": [50364, 1389, 718, 311, 584, 321, 632, 7261, 286, 478, 445, 1382, 281, 519, 498, 286, 632, 7261, 341, 50786, 50786, 935, 510, 558, 550, 309, 1062, 362, 733, 295, 2780, 766, 760, 670, 510, 293, 370, 550, 51080, 51080, 562, 321, 574, 412, 577, 731, 309, 9001, 321, 576, 584, 1954, 341, 472, 311, 6193, 1314, 264, 51412, 51412, 2316, 264, 1412, 300, 321, 747, 1314, 293, 500, 380, 718, 264, 2316, 536, 309, 562, 309, 311, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.06541677757545754, "compression_ratio": 1.6850828729281768, "no_speech_prob": 4.264034942025319e-05}, {"id": 540, "seek": 307008, "start": 3078.52, "end": 3084.4, "text": " point here right then it might have kind of gone off down over here and so then", "tokens": [50364, 1389, 718, 311, 584, 321, 632, 7261, 286, 478, 445, 1382, 281, 519, 498, 286, 632, 7261, 341, 50786, 50786, 935, 510, 558, 550, 309, 1062, 362, 733, 295, 2780, 766, 760, 670, 510, 293, 370, 550, 51080, 51080, 562, 321, 574, 412, 577, 731, 309, 9001, 321, 576, 584, 1954, 341, 472, 311, 6193, 1314, 264, 51412, 51412, 2316, 264, 1412, 300, 321, 747, 1314, 293, 500, 380, 718, 264, 2316, 536, 309, 562, 309, 311, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.06541677757545754, "compression_ratio": 1.6850828729281768, "no_speech_prob": 4.264034942025319e-05}, {"id": 541, "seek": 307008, "start": 3084.4, "end": 3091.04, "text": " when we look at how well it fits we would say oh this one's miles away the", "tokens": [50364, 1389, 718, 311, 584, 321, 632, 7261, 286, 478, 445, 1382, 281, 519, 498, 286, 632, 7261, 341, 50786, 50786, 935, 510, 558, 550, 309, 1062, 362, 733, 295, 2780, 766, 760, 670, 510, 293, 370, 550, 51080, 51080, 562, 321, 574, 412, 577, 731, 309, 9001, 321, 576, 584, 1954, 341, 472, 311, 6193, 1314, 264, 51412, 51412, 2316, 264, 1412, 300, 321, 747, 1314, 293, 500, 380, 718, 264, 2316, 536, 309, 562, 309, 311, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.06541677757545754, "compression_ratio": 1.6850828729281768, "no_speech_prob": 4.264034942025319e-05}, {"id": 542, "seek": 307008, "start": 3091.04, "end": 3096.48, "text": " model the data that we take away and don't let the model see it when it's", "tokens": [50364, 1389, 718, 311, 584, 321, 632, 7261, 286, 478, 445, 1382, 281, 519, 498, 286, 632, 7261, 341, 50786, 50786, 935, 510, 558, 550, 309, 1062, 362, 733, 295, 2780, 766, 760, 670, 510, 293, 370, 550, 51080, 51080, 562, 321, 574, 412, 577, 731, 309, 9001, 321, 576, 584, 1954, 341, 472, 311, 6193, 1314, 264, 51412, 51412, 2316, 264, 1412, 300, 321, 747, 1314, 293, 500, 380, 718, 264, 2316, 536, 309, 562, 309, 311, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.06541677757545754, "compression_ratio": 1.6850828729281768, "no_speech_prob": 4.264034942025319e-05}, {"id": 543, "seek": 309648, "start": 3096.48, "end": 3102.8, "text": " training is called the validation set so in first AI we've seen splitters before", "tokens": [50364, 3097, 307, 1219, 264, 24071, 992, 370, 294, 700, 7318, 321, 600, 1612, 7472, 1559, 949, 50680, 50680, 558, 264, 7472, 1559, 366, 264, 721, 300, 4994, 484, 264, 24071, 992, 2370, 7318, 50888, 50888, 1582, 380, 718, 291, 3847, 257, 2316, 1553, 257, 24071, 992, 2370, 7318, 1009, 3110, 291, 51172, 51172, 428, 16367, 370, 721, 411, 14170, 12690, 787, 322, 264, 24071, 992, 341, 51410, 51410, 307, 534, 10901, 881, 15148, 652, 309, 534, 1858, 281, 3076, 1803, 294, 264, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.11464955085931822, "compression_ratio": 1.8497652582159625, "no_speech_prob": 0.00011411122977733612}, {"id": 544, "seek": 309648, "start": 3102.8, "end": 3106.96, "text": " right the splitters are the things that separate out the validation set fast AI", "tokens": [50364, 3097, 307, 1219, 264, 24071, 992, 370, 294, 700, 7318, 321, 600, 1612, 7472, 1559, 949, 50680, 50680, 558, 264, 7472, 1559, 366, 264, 721, 300, 4994, 484, 264, 24071, 992, 2370, 7318, 50888, 50888, 1582, 380, 718, 291, 3847, 257, 2316, 1553, 257, 24071, 992, 2370, 7318, 1009, 3110, 291, 51172, 51172, 428, 16367, 370, 721, 411, 14170, 12690, 787, 322, 264, 24071, 992, 341, 51410, 51410, 307, 534, 10901, 881, 15148, 652, 309, 534, 1858, 281, 3076, 1803, 294, 264, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.11464955085931822, "compression_ratio": 1.8497652582159625, "no_speech_prob": 0.00011411122977733612}, {"id": 545, "seek": 309648, "start": 3106.96, "end": 3112.64, "text": " won't let you train a model without a validation set fast AI always shows you", "tokens": [50364, 3097, 307, 1219, 264, 24071, 992, 370, 294, 700, 7318, 321, 600, 1612, 7472, 1559, 949, 50680, 50680, 558, 264, 7472, 1559, 366, 264, 721, 300, 4994, 484, 264, 24071, 992, 2370, 7318, 50888, 50888, 1582, 380, 718, 291, 3847, 257, 2316, 1553, 257, 24071, 992, 2370, 7318, 1009, 3110, 291, 51172, 51172, 428, 16367, 370, 721, 411, 14170, 12690, 787, 322, 264, 24071, 992, 341, 51410, 51410, 307, 534, 10901, 881, 15148, 652, 309, 534, 1858, 281, 3076, 1803, 294, 264, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.11464955085931822, "compression_ratio": 1.8497652582159625, "no_speech_prob": 0.00011411122977733612}, {"id": 546, "seek": 309648, "start": 3112.64, "end": 3117.4, "text": " your metrics so things like accuracy measured only on the validation set this", "tokens": [50364, 3097, 307, 1219, 264, 24071, 992, 370, 294, 700, 7318, 321, 600, 1612, 7472, 1559, 949, 50680, 50680, 558, 264, 7472, 1559, 366, 264, 721, 300, 4994, 484, 264, 24071, 992, 2370, 7318, 50888, 50888, 1582, 380, 718, 291, 3847, 257, 2316, 1553, 257, 24071, 992, 2370, 7318, 1009, 3110, 291, 51172, 51172, 428, 16367, 370, 721, 411, 14170, 12690, 787, 322, 264, 24071, 992, 341, 51410, 51410, 307, 534, 10901, 881, 15148, 652, 309, 534, 1858, 281, 3076, 1803, 294, 264, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.11464955085931822, "compression_ratio": 1.8497652582159625, "no_speech_prob": 0.00011411122977733612}, {"id": 547, "seek": 309648, "start": 3117.4, "end": 3122.4, "text": " is really unusual most libraries make it really easy to shoot yourself in the", "tokens": [50364, 3097, 307, 1219, 264, 24071, 992, 370, 294, 700, 7318, 321, 600, 1612, 7472, 1559, 949, 50680, 50680, 558, 264, 7472, 1559, 366, 264, 721, 300, 4994, 484, 264, 24071, 992, 2370, 7318, 50888, 50888, 1582, 380, 718, 291, 3847, 257, 2316, 1553, 257, 24071, 992, 2370, 7318, 1009, 3110, 291, 51172, 51172, 428, 16367, 370, 721, 411, 14170, 12690, 787, 322, 264, 24071, 992, 341, 51410, 51410, 307, 534, 10901, 881, 15148, 652, 309, 534, 1858, 281, 3076, 1803, 294, 264, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.11464955085931822, "compression_ratio": 1.8497652582159625, "no_speech_prob": 0.00011411122977733612}, {"id": 548, "seek": 312240, "start": 3122.4, "end": 3126.56, "text": " foot by not having a validation set or accidentally not using it correctly so", "tokens": [50364, 2671, 538, 406, 1419, 257, 24071, 992, 420, 15715, 406, 1228, 309, 8944, 370, 50572, 50572, 2370, 7318, 1582, 380, 754, 718, 291, 360, 300, 370, 291, 600, 658, 281, 312, 4098, 5026, 50802, 50802, 562, 1228, 661, 15148, 31422, 1851, 4088, 433, 307, 665, 466, 341, 370, 436, 51042, 51042, 652, 988, 300, 436, 360, 855, 291, 428, 16367, 322, 257, 24071, 992, 586, 51630, 51630, 4084, 257, 665, 24071, 992, 307, 406, 5101, 382, 2199, 382, 445, 16979, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.09952913011823382, "compression_ratio": 1.6905829596412556, "no_speech_prob": 4.399822137202136e-05}, {"id": 549, "seek": 312240, "start": 3126.56, "end": 3131.1600000000003, "text": " fast AI won't even let you do that so you've got to be particularly careful", "tokens": [50364, 2671, 538, 406, 1419, 257, 24071, 992, 420, 15715, 406, 1228, 309, 8944, 370, 50572, 50572, 2370, 7318, 1582, 380, 754, 718, 291, 360, 300, 370, 291, 600, 658, 281, 312, 4098, 5026, 50802, 50802, 562, 1228, 661, 15148, 31422, 1851, 4088, 433, 307, 665, 466, 341, 370, 436, 51042, 51042, 652, 988, 300, 436, 360, 855, 291, 428, 16367, 322, 257, 24071, 992, 586, 51630, 51630, 4084, 257, 665, 24071, 992, 307, 406, 5101, 382, 2199, 382, 445, 16979, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.09952913011823382, "compression_ratio": 1.6905829596412556, "no_speech_prob": 4.399822137202136e-05}, {"id": 550, "seek": 312240, "start": 3131.1600000000003, "end": 3135.96, "text": " when using other libraries hacking face transformers is good about this so they", "tokens": [50364, 2671, 538, 406, 1419, 257, 24071, 992, 420, 15715, 406, 1228, 309, 8944, 370, 50572, 50572, 2370, 7318, 1582, 380, 754, 718, 291, 360, 300, 370, 291, 600, 658, 281, 312, 4098, 5026, 50802, 50802, 562, 1228, 661, 15148, 31422, 1851, 4088, 433, 307, 665, 466, 341, 370, 436, 51042, 51042, 652, 988, 300, 436, 360, 855, 291, 428, 16367, 322, 257, 24071, 992, 586, 51630, 51630, 4084, 257, 665, 24071, 992, 307, 406, 5101, 382, 2199, 382, 445, 16979, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.09952913011823382, "compression_ratio": 1.6905829596412556, "no_speech_prob": 4.399822137202136e-05}, {"id": 551, "seek": 312240, "start": 3135.96, "end": 3147.7200000000003, "text": " make sure that they do show you your metrics on a validation set now", "tokens": [50364, 2671, 538, 406, 1419, 257, 24071, 992, 420, 15715, 406, 1228, 309, 8944, 370, 50572, 50572, 2370, 7318, 1582, 380, 754, 718, 291, 360, 300, 370, 291, 600, 658, 281, 312, 4098, 5026, 50802, 50802, 562, 1228, 661, 15148, 31422, 1851, 4088, 433, 307, 665, 466, 341, 370, 436, 51042, 51042, 652, 988, 300, 436, 360, 855, 291, 428, 16367, 322, 257, 24071, 992, 586, 51630, 51630, 4084, 257, 665, 24071, 992, 307, 406, 5101, 382, 2199, 382, 445, 16979, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.09952913011823382, "compression_ratio": 1.6905829596412556, "no_speech_prob": 4.399822137202136e-05}, {"id": 552, "seek": 312240, "start": 3147.7200000000003, "end": 3151.64, "text": " creating a good validation set is not generally as simple as just randomly", "tokens": [50364, 2671, 538, 406, 1419, 257, 24071, 992, 420, 15715, 406, 1228, 309, 8944, 370, 50572, 50572, 2370, 7318, 1582, 380, 754, 718, 291, 360, 300, 370, 291, 600, 658, 281, 312, 4098, 5026, 50802, 50802, 562, 1228, 661, 15148, 31422, 1851, 4088, 433, 307, 665, 466, 341, 370, 436, 51042, 51042, 652, 988, 300, 436, 360, 855, 291, 428, 16367, 322, 257, 24071, 992, 586, 51630, 51630, 4084, 257, 665, 24071, 992, 307, 406, 5101, 382, 2199, 382, 445, 16979, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.09952913011823382, "compression_ratio": 1.6905829596412556, "no_speech_prob": 4.399822137202136e-05}, {"id": 553, "seek": 315164, "start": 3151.64, "end": 3156.52, "text": " pulling some of your data out of your model out of the data that you passed", "tokens": [50364, 8407, 512, 295, 428, 1412, 484, 295, 428, 2316, 484, 295, 264, 1412, 300, 291, 4678, 50608, 50608, 300, 291, 3847, 365, 428, 2316, 264, 1778, 983, 307, 3811, 300, 341, 390, 264, 50988, 50988, 1412, 291, 645, 1382, 281, 3318, 746, 281, 1392, 293, 291, 16979, 4159, 512, 370, 309, 51308, 51308, 1542, 411, 341, 300, 1542, 588, 1858, 1177, 380, 309, 570, 291, 600, 733, 295, 411, 51598, 51598, 920, 658, 439, 264, 1412, 291, 576, 528, 926, 264, 2793, 293, 294, 257, 565, 2638, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.094330934377817, "compression_ratio": 1.8640776699029127, "no_speech_prob": 6.708705041091889e-05}, {"id": 554, "seek": 315164, "start": 3156.52, "end": 3164.12, "text": " that you train with your model the reason why is imagine that this was the", "tokens": [50364, 8407, 512, 295, 428, 1412, 484, 295, 428, 2316, 484, 295, 264, 1412, 300, 291, 4678, 50608, 50608, 300, 291, 3847, 365, 428, 2316, 264, 1778, 983, 307, 3811, 300, 341, 390, 264, 50988, 50988, 1412, 291, 645, 1382, 281, 3318, 746, 281, 1392, 293, 291, 16979, 4159, 512, 370, 309, 51308, 51308, 1542, 411, 341, 300, 1542, 588, 1858, 1177, 380, 309, 570, 291, 600, 733, 295, 411, 51598, 51598, 920, 658, 439, 264, 1412, 291, 576, 528, 926, 264, 2793, 293, 294, 257, 565, 2638, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.094330934377817, "compression_ratio": 1.8640776699029127, "no_speech_prob": 6.708705041091889e-05}, {"id": 555, "seek": 315164, "start": 3164.12, "end": 3170.52, "text": " data you were trying to fit something to okay and you randomly remove some so it", "tokens": [50364, 8407, 512, 295, 428, 1412, 484, 295, 428, 2316, 484, 295, 264, 1412, 300, 291, 4678, 50608, 50608, 300, 291, 3847, 365, 428, 2316, 264, 1778, 983, 307, 3811, 300, 341, 390, 264, 50988, 50988, 1412, 291, 645, 1382, 281, 3318, 746, 281, 1392, 293, 291, 16979, 4159, 512, 370, 309, 51308, 51308, 1542, 411, 341, 300, 1542, 588, 1858, 1177, 380, 309, 570, 291, 600, 733, 295, 411, 51598, 51598, 920, 658, 439, 264, 1412, 291, 576, 528, 926, 264, 2793, 293, 294, 257, 565, 2638, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.094330934377817, "compression_ratio": 1.8640776699029127, "no_speech_prob": 6.708705041091889e-05}, {"id": 556, "seek": 315164, "start": 3170.52, "end": 3176.3199999999997, "text": " looks like this that looks very easy doesn't it because you've kind of like", "tokens": [50364, 8407, 512, 295, 428, 1412, 484, 295, 428, 2316, 484, 295, 264, 1412, 300, 291, 4678, 50608, 50608, 300, 291, 3847, 365, 428, 2316, 264, 1778, 983, 307, 3811, 300, 341, 390, 264, 50988, 50988, 1412, 291, 645, 1382, 281, 3318, 746, 281, 1392, 293, 291, 16979, 4159, 512, 370, 309, 51308, 51308, 1542, 411, 341, 300, 1542, 588, 1858, 1177, 380, 309, 570, 291, 600, 733, 295, 411, 51598, 51598, 920, 658, 439, 264, 1412, 291, 576, 528, 926, 264, 2793, 293, 294, 257, 565, 2638, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.094330934377817, "compression_ratio": 1.8640776699029127, "no_speech_prob": 6.708705041091889e-05}, {"id": 557, "seek": 315164, "start": 3176.3199999999997, "end": 3180.8399999999997, "text": " still got all the data you would want around the points and in a time series", "tokens": [50364, 8407, 512, 295, 428, 1412, 484, 295, 428, 2316, 484, 295, 264, 1412, 300, 291, 4678, 50608, 50608, 300, 291, 3847, 365, 428, 2316, 264, 1778, 983, 307, 3811, 300, 341, 390, 264, 50988, 50988, 1412, 291, 645, 1382, 281, 3318, 746, 281, 1392, 293, 291, 16979, 4159, 512, 370, 309, 51308, 51308, 1542, 411, 341, 300, 1542, 588, 1858, 1177, 380, 309, 570, 291, 600, 733, 295, 411, 51598, 51598, 920, 658, 439, 264, 1412, 291, 576, 528, 926, 264, 2793, 293, 294, 257, 565, 2638, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.094330934377817, "compression_ratio": 1.8640776699029127, "no_speech_prob": 6.708705041091889e-05}, {"id": 558, "seek": 318084, "start": 3180.84, "end": 3184.88, "text": " like this this is dates and sales in real life you're probably going to want", "tokens": [50364, 411, 341, 341, 307, 11691, 293, 5763, 294, 957, 993, 291, 434, 1391, 516, 281, 528, 50566, 50566, 281, 6069, 2027, 11691, 370, 498, 291, 5397, 428, 24071, 992, 538, 16979, 50800, 50800, 12720, 1507, 490, 264, 2808, 309, 311, 406, 534, 257, 665, 18877, 295, 577, 291, 434, 51002, 51002, 516, 281, 312, 1228, 341, 2316, 2602, 291, 820, 504, 409, 66, 473, 293, 4159, 264, 1036, 51242, 51242, 1916, 295, 3259, 370, 498, 341, 390, 428, 24071, 992, 293, 341, 307, 428, 3097, 51498, 51498, 992, 300, 311, 516, 281, 312, 767, 4997, 1968, 291, 393, 764, 341, 281, 6069, 264, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.07512142502258871, "compression_ratio": 1.8780487804878048, "no_speech_prob": 9.459813736611977e-05}, {"id": 559, "seek": 318084, "start": 3184.88, "end": 3189.56, "text": " to predict future dates so if you credit your validation set by randomly", "tokens": [50364, 411, 341, 341, 307, 11691, 293, 5763, 294, 957, 993, 291, 434, 1391, 516, 281, 528, 50566, 50566, 281, 6069, 2027, 11691, 370, 498, 291, 5397, 428, 24071, 992, 538, 16979, 50800, 50800, 12720, 1507, 490, 264, 2808, 309, 311, 406, 534, 257, 665, 18877, 295, 577, 291, 434, 51002, 51002, 516, 281, 312, 1228, 341, 2316, 2602, 291, 820, 504, 409, 66, 473, 293, 4159, 264, 1036, 51242, 51242, 1916, 295, 3259, 370, 498, 341, 390, 428, 24071, 992, 293, 341, 307, 428, 3097, 51498, 51498, 992, 300, 311, 516, 281, 312, 767, 4997, 1968, 291, 393, 764, 341, 281, 6069, 264, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.07512142502258871, "compression_ratio": 1.8780487804878048, "no_speech_prob": 9.459813736611977e-05}, {"id": 560, "seek": 318084, "start": 3189.56, "end": 3193.6000000000004, "text": " removing stuff from the middle it's not really a good indication of how you're", "tokens": [50364, 411, 341, 341, 307, 11691, 293, 5763, 294, 957, 993, 291, 434, 1391, 516, 281, 528, 50566, 50566, 281, 6069, 2027, 11691, 370, 498, 291, 5397, 428, 24071, 992, 538, 16979, 50800, 50800, 12720, 1507, 490, 264, 2808, 309, 311, 406, 534, 257, 665, 18877, 295, 577, 291, 434, 51002, 51002, 516, 281, 312, 1228, 341, 2316, 2602, 291, 820, 504, 409, 66, 473, 293, 4159, 264, 1036, 51242, 51242, 1916, 295, 3259, 370, 498, 341, 390, 428, 24071, 992, 293, 341, 307, 428, 3097, 51498, 51498, 992, 300, 311, 516, 281, 312, 767, 4997, 1968, 291, 393, 764, 341, 281, 6069, 264, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.07512142502258871, "compression_ratio": 1.8780487804878048, "no_speech_prob": 9.459813736611977e-05}, {"id": 561, "seek": 318084, "start": 3193.6000000000004, "end": 3198.4, "text": " going to be using this model instead you should truncate and remove the last", "tokens": [50364, 411, 341, 341, 307, 11691, 293, 5763, 294, 957, 993, 291, 434, 1391, 516, 281, 528, 50566, 50566, 281, 6069, 2027, 11691, 370, 498, 291, 5397, 428, 24071, 992, 538, 16979, 50800, 50800, 12720, 1507, 490, 264, 2808, 309, 311, 406, 534, 257, 665, 18877, 295, 577, 291, 434, 51002, 51002, 516, 281, 312, 1228, 341, 2316, 2602, 291, 820, 504, 409, 66, 473, 293, 4159, 264, 1036, 51242, 51242, 1916, 295, 3259, 370, 498, 341, 390, 428, 24071, 992, 293, 341, 307, 428, 3097, 51498, 51498, 992, 300, 311, 516, 281, 312, 767, 4997, 1968, 291, 393, 764, 341, 281, 6069, 264, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.07512142502258871, "compression_ratio": 1.8780487804878048, "no_speech_prob": 9.459813736611977e-05}, {"id": 562, "seek": 318084, "start": 3198.4, "end": 3203.52, "text": " couple of weeks so if this was your validation set and this is your training", "tokens": [50364, 411, 341, 341, 307, 11691, 293, 5763, 294, 957, 993, 291, 434, 1391, 516, 281, 528, 50566, 50566, 281, 6069, 2027, 11691, 370, 498, 291, 5397, 428, 24071, 992, 538, 16979, 50800, 50800, 12720, 1507, 490, 264, 2808, 309, 311, 406, 534, 257, 665, 18877, 295, 577, 291, 434, 51002, 51002, 516, 281, 312, 1228, 341, 2316, 2602, 291, 820, 504, 409, 66, 473, 293, 4159, 264, 1036, 51242, 51242, 1916, 295, 3259, 370, 498, 341, 390, 428, 24071, 992, 293, 341, 307, 428, 3097, 51498, 51498, 992, 300, 311, 516, 281, 312, 767, 4997, 1968, 291, 393, 764, 341, 281, 6069, 264, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.07512142502258871, "compression_ratio": 1.8780487804878048, "no_speech_prob": 9.459813736611977e-05}, {"id": 563, "seek": 318084, "start": 3203.52, "end": 3208.2400000000002, "text": " set that's going to be actually testing whether you can use this to predict the", "tokens": [50364, 411, 341, 341, 307, 11691, 293, 5763, 294, 957, 993, 291, 434, 1391, 516, 281, 528, 50566, 50566, 281, 6069, 2027, 11691, 370, 498, 291, 5397, 428, 24071, 992, 538, 16979, 50800, 50800, 12720, 1507, 490, 264, 2808, 309, 311, 406, 534, 257, 665, 18877, 295, 577, 291, 434, 51002, 51002, 516, 281, 312, 1228, 341, 2316, 2602, 291, 820, 504, 409, 66, 473, 293, 4159, 264, 1036, 51242, 51242, 1916, 295, 3259, 370, 498, 341, 390, 428, 24071, 992, 293, 341, 307, 428, 3097, 51498, 51498, 992, 300, 311, 516, 281, 312, 767, 4997, 1968, 291, 393, 764, 341, 281, 6069, 264, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.07512142502258871, "compression_ratio": 1.8780487804878048, "no_speech_prob": 9.459813736611977e-05}, {"id": 564, "seek": 320824, "start": 3208.24, "end": 3212.72, "text": " future rather than using it to predict the past", "tokens": [50364, 2027, 2831, 813, 1228, 309, 281, 6069, 264, 1791, 50588, 50606, 48751, 22631, 26185, 366, 257, 5456, 636, 281, 1500, 428, 3485, 281, 1884, 257, 665, 50900, 50900, 24071, 992, 570, 48751, 22631, 26185, 787, 2089, 291, 281, 10315, 51156, 51156, 5101, 257, 1916, 295, 1413, 257, 786, 264, 1412, 992, 300, 291, 366, 18139, 322, 294, 264, 51573, 51573, 5263, 3787, 1830, 300, 565, 307, 767, 787, 257, 1359, 25993, 294, 1186, 307, 257, 3879, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.14409559965133667, "compression_ratio": 1.7425742574257426, "no_speech_prob": 4.1981853428296745e-05}, {"id": 565, "seek": 320824, "start": 3213.08, "end": 3218.9599999999996, "text": " Kaggle competitions are a fantastic way to test your ability to create a good", "tokens": [50364, 2027, 2831, 813, 1228, 309, 281, 6069, 264, 1791, 50588, 50606, 48751, 22631, 26185, 366, 257, 5456, 636, 281, 1500, 428, 3485, 281, 1884, 257, 665, 50900, 50900, 24071, 992, 570, 48751, 22631, 26185, 787, 2089, 291, 281, 10315, 51156, 51156, 5101, 257, 1916, 295, 1413, 257, 786, 264, 1412, 992, 300, 291, 366, 18139, 322, 294, 264, 51573, 51573, 5263, 3787, 1830, 300, 565, 307, 767, 787, 257, 1359, 25993, 294, 1186, 307, 257, 3879, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.14409559965133667, "compression_ratio": 1.7425742574257426, "no_speech_prob": 4.1981853428296745e-05}, {"id": 566, "seek": 320824, "start": 3218.9599999999996, "end": 3224.08, "text": " validation set because Kaggle competitions only allow you to submit", "tokens": [50364, 2027, 2831, 813, 1228, 309, 281, 6069, 264, 1791, 50588, 50606, 48751, 22631, 26185, 366, 257, 5456, 636, 281, 1500, 428, 3485, 281, 1884, 257, 665, 50900, 50900, 24071, 992, 570, 48751, 22631, 26185, 787, 2089, 291, 281, 10315, 51156, 51156, 5101, 257, 1916, 295, 1413, 257, 786, 264, 1412, 992, 300, 291, 366, 18139, 322, 294, 264, 51573, 51573, 5263, 3787, 1830, 300, 565, 307, 767, 787, 257, 1359, 25993, 294, 1186, 307, 257, 3879, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.14409559965133667, "compression_ratio": 1.7425742574257426, "no_speech_prob": 4.1981853428296745e-05}, {"id": 567, "seek": 320824, "start": 3224.08, "end": 3232.4199999999996, "text": " generally a couple of times a day the data set that you are scored on in the", "tokens": [50364, 2027, 2831, 813, 1228, 309, 281, 6069, 264, 1791, 50588, 50606, 48751, 22631, 26185, 366, 257, 5456, 636, 281, 1500, 428, 3485, 281, 1884, 257, 665, 50900, 50900, 24071, 992, 570, 48751, 22631, 26185, 787, 2089, 291, 281, 10315, 51156, 51156, 5101, 257, 1916, 295, 1413, 257, 786, 264, 1412, 992, 300, 291, 366, 18139, 322, 294, 264, 51573, 51573, 5263, 3787, 1830, 300, 565, 307, 767, 787, 257, 1359, 25993, 294, 1186, 307, 257, 3879, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.14409559965133667, "compression_ratio": 1.7425742574257426, "no_speech_prob": 4.1981853428296745e-05}, {"id": 568, "seek": 320824, "start": 3232.4199999999996, "end": 3237.3199999999997, "text": " leaderboard during that time is actually only a small subset in fact is a totally", "tokens": [50364, 2027, 2831, 813, 1228, 309, 281, 6069, 264, 1791, 50588, 50606, 48751, 22631, 26185, 366, 257, 5456, 636, 281, 1500, 428, 3485, 281, 1884, 257, 665, 50900, 50900, 24071, 992, 570, 48751, 22631, 26185, 787, 2089, 291, 281, 10315, 51156, 51156, 5101, 257, 1916, 295, 1413, 257, 786, 264, 1412, 992, 300, 291, 366, 18139, 322, 294, 264, 51573, 51573, 5263, 3787, 1830, 300, 565, 307, 767, 787, 257, 1359, 25993, 294, 1186, 307, 257, 3879, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.14409559965133667, "compression_ratio": 1.7425742574257426, "no_speech_prob": 4.1981853428296745e-05}, {"id": 569, "seek": 323732, "start": 3237.32, "end": 3240.92, "text": " separate subset to the one you'll be scored on on the end of the competition", "tokens": [50364, 4994, 25993, 281, 264, 472, 291, 603, 312, 18139, 322, 322, 264, 917, 295, 264, 6211, 50544, 50544, 293, 370, 881, 26992, 322, 48751, 22631, 670, 6845, 293, 309, 311, 406, 1826, 291, 600, 1096, 309, 300, 50914, 50914, 291, 486, 483, 300, 1452, 47879, 2633, 295, 411, 1954, 452, 3044, 286, 670, 6845, 294, 264, 957, 51190, 51190, 1002, 2380, 295, 48751, 22631, 291, 486, 2049, 406, 754, 458, 300, 291, 670, 6845, 291, 445, 51542, 51542, 5293, 2158, 337, 428, 4475, 40087, 370, 309, 311, 257, 534, 665, 1558, 281, 51786, 51786], "temperature": 0.0, "avg_logprob": -0.10950591166814168, "compression_ratio": 1.7142857142857142, "no_speech_prob": 7.96581880422309e-05}, {"id": 570, "seek": 323732, "start": 3240.92, "end": 3248.32, "text": " and so most beginners on Kaggle overfit and it's not until you've done it that", "tokens": [50364, 4994, 25993, 281, 264, 472, 291, 603, 312, 18139, 322, 322, 264, 917, 295, 264, 6211, 50544, 50544, 293, 370, 881, 26992, 322, 48751, 22631, 670, 6845, 293, 309, 311, 406, 1826, 291, 600, 1096, 309, 300, 50914, 50914, 291, 486, 483, 300, 1452, 47879, 2633, 295, 411, 1954, 452, 3044, 286, 670, 6845, 294, 264, 957, 51190, 51190, 1002, 2380, 295, 48751, 22631, 291, 486, 2049, 406, 754, 458, 300, 291, 670, 6845, 291, 445, 51542, 51542, 5293, 2158, 337, 428, 4475, 40087, 370, 309, 311, 257, 534, 665, 1558, 281, 51786, 51786], "temperature": 0.0, "avg_logprob": -0.10950591166814168, "compression_ratio": 1.7142857142857142, "no_speech_prob": 7.96581880422309e-05}, {"id": 571, "seek": 323732, "start": 3248.32, "end": 3253.84, "text": " you will get that visceral feeling of like oh my god I overfit in the real", "tokens": [50364, 4994, 25993, 281, 264, 472, 291, 603, 312, 18139, 322, 322, 264, 917, 295, 264, 6211, 50544, 50544, 293, 370, 881, 26992, 322, 48751, 22631, 670, 6845, 293, 309, 311, 406, 1826, 291, 600, 1096, 309, 300, 50914, 50914, 291, 486, 483, 300, 1452, 47879, 2633, 295, 411, 1954, 452, 3044, 286, 670, 6845, 294, 264, 957, 51190, 51190, 1002, 2380, 295, 48751, 22631, 291, 486, 2049, 406, 754, 458, 300, 291, 670, 6845, 291, 445, 51542, 51542, 5293, 2158, 337, 428, 4475, 40087, 370, 309, 311, 257, 534, 665, 1558, 281, 51786, 51786], "temperature": 0.0, "avg_logprob": -0.10950591166814168, "compression_ratio": 1.7142857142857142, "no_speech_prob": 7.96581880422309e-05}, {"id": 572, "seek": 323732, "start": 3253.84, "end": 3260.88, "text": " world outside of Kaggle you will often not even know that you overfit you just", "tokens": [50364, 4994, 25993, 281, 264, 472, 291, 603, 312, 18139, 322, 322, 264, 917, 295, 264, 6211, 50544, 50544, 293, 370, 881, 26992, 322, 48751, 22631, 670, 6845, 293, 309, 311, 406, 1826, 291, 600, 1096, 309, 300, 50914, 50914, 291, 486, 483, 300, 1452, 47879, 2633, 295, 411, 1954, 452, 3044, 286, 670, 6845, 294, 264, 957, 51190, 51190, 1002, 2380, 295, 48751, 22631, 291, 486, 2049, 406, 754, 458, 300, 291, 670, 6845, 291, 445, 51542, 51542, 5293, 2158, 337, 428, 4475, 40087, 370, 309, 311, 257, 534, 665, 1558, 281, 51786, 51786], "temperature": 0.0, "avg_logprob": -0.10950591166814168, "compression_ratio": 1.7142857142857142, "no_speech_prob": 7.96581880422309e-05}, {"id": 573, "seek": 323732, "start": 3260.88, "end": 3265.76, "text": " destroy value for your organization silently so it's a really good idea to", "tokens": [50364, 4994, 25993, 281, 264, 472, 291, 603, 312, 18139, 322, 322, 264, 917, 295, 264, 6211, 50544, 50544, 293, 370, 881, 26992, 322, 48751, 22631, 670, 6845, 293, 309, 311, 406, 1826, 291, 600, 1096, 309, 300, 50914, 50914, 291, 486, 483, 300, 1452, 47879, 2633, 295, 411, 1954, 452, 3044, 286, 670, 6845, 294, 264, 957, 51190, 51190, 1002, 2380, 295, 48751, 22631, 291, 486, 2049, 406, 754, 458, 300, 291, 670, 6845, 291, 445, 51542, 51542, 5293, 2158, 337, 428, 4475, 40087, 370, 309, 311, 257, 534, 665, 1558, 281, 51786, 51786], "temperature": 0.0, "avg_logprob": -0.10950591166814168, "compression_ratio": 1.7142857142857142, "no_speech_prob": 7.96581880422309e-05}, {"id": 574, "seek": 326576, "start": 3265.76, "end": 3269.5600000000004, "text": " do this kind of stuff on Kaggle a few times first in real competitions to", "tokens": [50364, 360, 341, 733, 295, 1507, 322, 48751, 22631, 257, 1326, 1413, 700, 294, 957, 26185, 281, 50554, 50554, 534, 652, 988, 300, 291, 366, 6679, 291, 458, 577, 281, 5042, 670, 69, 2414, 577, 281, 50780, 50780, 915, 257, 665, 24071, 992, 293, 577, 281, 7302, 309, 8944, 293, 291, 534, 51012, 51012, 500, 380, 483, 300, 1826, 291, 5630, 309, 493, 257, 1326, 1413, 665, 1365, 295, 341, 390, 456, 51456, 51456, 390, 257, 21658, 6787, 6211, 322, 48751, 22631, 456, 366, 613, 733, 295, 5242, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.08020035251156314, "compression_ratio": 1.7522522522522523, "no_speech_prob": 6.50082147330977e-05}, {"id": 575, "seek": 326576, "start": 3269.5600000000004, "end": 3274.0800000000004, "text": " really make sure that you are confident you know how to avoid overfitting how to", "tokens": [50364, 360, 341, 733, 295, 1507, 322, 48751, 22631, 257, 1326, 1413, 700, 294, 957, 26185, 281, 50554, 50554, 534, 652, 988, 300, 291, 366, 6679, 291, 458, 577, 281, 5042, 670, 69, 2414, 577, 281, 50780, 50780, 915, 257, 665, 24071, 992, 293, 577, 281, 7302, 309, 8944, 293, 291, 534, 51012, 51012, 500, 380, 483, 300, 1826, 291, 5630, 309, 493, 257, 1326, 1413, 665, 1365, 295, 341, 390, 456, 51456, 51456, 390, 257, 21658, 6787, 6211, 322, 48751, 22631, 456, 366, 613, 733, 295, 5242, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.08020035251156314, "compression_ratio": 1.7522522522522523, "no_speech_prob": 6.50082147330977e-05}, {"id": 576, "seek": 326576, "start": 3274.0800000000004, "end": 3278.7200000000003, "text": " find a good validation set and how to interpret it correctly and you really", "tokens": [50364, 360, 341, 733, 295, 1507, 322, 48751, 22631, 257, 1326, 1413, 700, 294, 957, 26185, 281, 50554, 50554, 534, 652, 988, 300, 291, 366, 6679, 291, 458, 577, 281, 5042, 670, 69, 2414, 577, 281, 50780, 50780, 915, 257, 665, 24071, 992, 293, 577, 281, 7302, 309, 8944, 293, 291, 534, 51012, 51012, 500, 380, 483, 300, 1826, 291, 5630, 309, 493, 257, 1326, 1413, 665, 1365, 295, 341, 390, 456, 51456, 51456, 390, 257, 21658, 6787, 6211, 322, 48751, 22631, 456, 366, 613, 733, 295, 5242, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.08020035251156314, "compression_ratio": 1.7522522522522523, "no_speech_prob": 6.50082147330977e-05}, {"id": 577, "seek": 326576, "start": 3278.7200000000003, "end": 3287.6000000000004, "text": " don't get that until you screw it up a few times good example of this was there", "tokens": [50364, 360, 341, 733, 295, 1507, 322, 48751, 22631, 257, 1326, 1413, 700, 294, 957, 26185, 281, 50554, 50554, 534, 652, 988, 300, 291, 366, 6679, 291, 458, 577, 281, 5042, 670, 69, 2414, 577, 281, 50780, 50780, 915, 257, 665, 24071, 992, 293, 577, 281, 7302, 309, 8944, 293, 291, 534, 51012, 51012, 500, 380, 483, 300, 1826, 291, 5630, 309, 493, 257, 1326, 1413, 665, 1365, 295, 341, 390, 456, 51456, 51456, 390, 257, 21658, 6787, 6211, 322, 48751, 22631, 456, 366, 613, 733, 295, 5242, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.08020035251156314, "compression_ratio": 1.7522522522522523, "no_speech_prob": 6.50082147330977e-05}, {"id": 578, "seek": 326576, "start": 3287.6000000000004, "end": 3291.5200000000004, "text": " was a distracted driver competition on Kaggle there are these kind of pictures", "tokens": [50364, 360, 341, 733, 295, 1507, 322, 48751, 22631, 257, 1326, 1413, 700, 294, 957, 26185, 281, 50554, 50554, 534, 652, 988, 300, 291, 366, 6679, 291, 458, 577, 281, 5042, 670, 69, 2414, 577, 281, 50780, 50780, 915, 257, 665, 24071, 992, 293, 577, 281, 7302, 309, 8944, 293, 291, 534, 51012, 51012, 500, 380, 483, 300, 1826, 291, 5630, 309, 493, 257, 1326, 1413, 665, 1365, 295, 341, 390, 456, 51456, 51456, 390, 257, 21658, 6787, 6211, 322, 48751, 22631, 456, 366, 613, 733, 295, 5242, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.08020035251156314, "compression_ratio": 1.7522522522522523, "no_speech_prob": 6.50082147330977e-05}, {"id": 579, "seek": 329152, "start": 3291.52, "end": 3298.6, "text": " from inside a car and the idea was that you had to try and predict whether", "tokens": [50364, 490, 1854, 257, 1032, 293, 264, 1558, 390, 300, 291, 632, 281, 853, 293, 6069, 1968, 50718, 50718, 2618, 390, 4840, 294, 257, 21658, 636, 420, 406, 293, 322, 48751, 22631, 436, 630, 50956, 50956, 746, 1238, 4069, 264, 1500, 992, 370, 264, 551, 300, 436, 18139, 291, 322, 264, 51133, 51133, 5263, 3787, 16212, 561, 300, 994, 380, 2514, 412, 439, 294, 264, 6211, 1412, 51436, 51436, 300, 291, 3847, 264, 2316, 365, 370, 498, 291, 1415, 281, 1884, 364, 4942, 24071, 51652, 51652, 992, 294, 341, 6211, 291, 576, 362, 281, 652, 988, 300, 291, 12005, 264, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.0756732468466157, "compression_ratio": 1.7882352941176471, "no_speech_prob": 3.3734475437086076e-05}, {"id": 580, "seek": 329152, "start": 3298.6, "end": 3303.36, "text": " somebody was driving in a distracted way or not and on Kaggle they did", "tokens": [50364, 490, 1854, 257, 1032, 293, 264, 1558, 390, 300, 291, 632, 281, 853, 293, 6069, 1968, 50718, 50718, 2618, 390, 4840, 294, 257, 21658, 636, 420, 406, 293, 322, 48751, 22631, 436, 630, 50956, 50956, 746, 1238, 4069, 264, 1500, 992, 370, 264, 551, 300, 436, 18139, 291, 322, 264, 51133, 51133, 5263, 3787, 16212, 561, 300, 994, 380, 2514, 412, 439, 294, 264, 6211, 1412, 51436, 51436, 300, 291, 3847, 264, 2316, 365, 370, 498, 291, 1415, 281, 1884, 364, 4942, 24071, 51652, 51652, 992, 294, 341, 6211, 291, 576, 362, 281, 652, 988, 300, 291, 12005, 264, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.0756732468466157, "compression_ratio": 1.7882352941176471, "no_speech_prob": 3.3734475437086076e-05}, {"id": 581, "seek": 329152, "start": 3303.36, "end": 3306.9, "text": " something pretty smart the test set so the thing that they scored you on the", "tokens": [50364, 490, 1854, 257, 1032, 293, 264, 1558, 390, 300, 291, 632, 281, 853, 293, 6069, 1968, 50718, 50718, 2618, 390, 4840, 294, 257, 21658, 636, 420, 406, 293, 322, 48751, 22631, 436, 630, 50956, 50956, 746, 1238, 4069, 264, 1500, 992, 370, 264, 551, 300, 436, 18139, 291, 322, 264, 51133, 51133, 5263, 3787, 16212, 561, 300, 994, 380, 2514, 412, 439, 294, 264, 6211, 1412, 51436, 51436, 300, 291, 3847, 264, 2316, 365, 370, 498, 291, 1415, 281, 1884, 364, 4942, 24071, 51652, 51652, 992, 294, 341, 6211, 291, 576, 362, 281, 652, 988, 300, 291, 12005, 264, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.0756732468466157, "compression_ratio": 1.7882352941176471, "no_speech_prob": 3.3734475437086076e-05}, {"id": 582, "seek": 329152, "start": 3306.9, "end": 3312.96, "text": " leaderboard contained people that didn't exist at all in the competition data", "tokens": [50364, 490, 1854, 257, 1032, 293, 264, 1558, 390, 300, 291, 632, 281, 853, 293, 6069, 1968, 50718, 50718, 2618, 390, 4840, 294, 257, 21658, 636, 420, 406, 293, 322, 48751, 22631, 436, 630, 50956, 50956, 746, 1238, 4069, 264, 1500, 992, 370, 264, 551, 300, 436, 18139, 291, 322, 264, 51133, 51133, 5263, 3787, 16212, 561, 300, 994, 380, 2514, 412, 439, 294, 264, 6211, 1412, 51436, 51436, 300, 291, 3847, 264, 2316, 365, 370, 498, 291, 1415, 281, 1884, 364, 4942, 24071, 51652, 51652, 992, 294, 341, 6211, 291, 576, 362, 281, 652, 988, 300, 291, 12005, 264, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.0756732468466157, "compression_ratio": 1.7882352941176471, "no_speech_prob": 3.3734475437086076e-05}, {"id": 583, "seek": 329152, "start": 3312.96, "end": 3317.28, "text": " that you train the model with so if you wanted to create an effective validation", "tokens": [50364, 490, 1854, 257, 1032, 293, 264, 1558, 390, 300, 291, 632, 281, 853, 293, 6069, 1968, 50718, 50718, 2618, 390, 4840, 294, 257, 21658, 636, 420, 406, 293, 322, 48751, 22631, 436, 630, 50956, 50956, 746, 1238, 4069, 264, 1500, 992, 370, 264, 551, 300, 436, 18139, 291, 322, 264, 51133, 51133, 5263, 3787, 16212, 561, 300, 994, 380, 2514, 412, 439, 294, 264, 6211, 1412, 51436, 51436, 300, 291, 3847, 264, 2316, 365, 370, 498, 291, 1415, 281, 1884, 364, 4942, 24071, 51652, 51652, 992, 294, 341, 6211, 291, 576, 362, 281, 652, 988, 300, 291, 12005, 264, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.0756732468466157, "compression_ratio": 1.7882352941176471, "no_speech_prob": 3.3734475437086076e-05}, {"id": 584, "seek": 329152, "start": 3317.28, "end": 3321.12, "text": " set in this competition you would have to make sure that you separated the", "tokens": [50364, 490, 1854, 257, 1032, 293, 264, 1558, 390, 300, 291, 632, 281, 853, 293, 6069, 1968, 50718, 50718, 2618, 390, 4840, 294, 257, 21658, 636, 420, 406, 293, 322, 48751, 22631, 436, 630, 50956, 50956, 746, 1238, 4069, 264, 1500, 992, 370, 264, 551, 300, 436, 18139, 291, 322, 264, 51133, 51133, 5263, 3787, 16212, 561, 300, 994, 380, 2514, 412, 439, 294, 264, 6211, 1412, 51436, 51436, 300, 291, 3847, 264, 2316, 365, 370, 498, 291, 1415, 281, 1884, 364, 4942, 24071, 51652, 51652, 992, 294, 341, 6211, 291, 576, 362, 281, 652, 988, 300, 291, 12005, 264, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.0756732468466157, "compression_ratio": 1.7882352941176471, "no_speech_prob": 3.3734475437086076e-05}, {"id": 585, "seek": 332112, "start": 3321.12, "end": 3325.68, "text": " photos so that your validation set contained photos of people that aren't in", "tokens": [50364, 5787, 370, 300, 428, 24071, 992, 16212, 5787, 295, 561, 300, 3212, 380, 294, 50592, 50592, 264, 1412, 291, 434, 3097, 428, 2316, 322, 456, 390, 1071, 472, 411, 300, 264, 50923, 50923, 48751, 22631, 20698, 530, 6211, 597, 632, 17772, 300, 994, 380, 4204, 370, 456, 645, 51230, 51230, 1936, 5242, 295, 17772, 291, 434, 4140, 281, 853, 281, 2041, 6069, 437, 3506, 645, 51434, 51434, 294, 264, 5242, 293, 309, 3574, 484, 300, 257, 688, 295, 561, 15715, 8932, 484, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.1140358281690021, "compression_ratio": 1.7844036697247707, "no_speech_prob": 8.613909449195489e-05}, {"id": 586, "seek": 332112, "start": 3325.68, "end": 3332.2999999999997, "text": " the data you're training your model on there was another one like that the", "tokens": [50364, 5787, 370, 300, 428, 24071, 992, 16212, 5787, 295, 561, 300, 3212, 380, 294, 50592, 50592, 264, 1412, 291, 434, 3097, 428, 2316, 322, 456, 390, 1071, 472, 411, 300, 264, 50923, 50923, 48751, 22631, 20698, 530, 6211, 597, 632, 17772, 300, 994, 380, 4204, 370, 456, 645, 51230, 51230, 1936, 5242, 295, 17772, 291, 434, 4140, 281, 853, 281, 2041, 6069, 437, 3506, 645, 51434, 51434, 294, 264, 5242, 293, 309, 3574, 484, 300, 257, 688, 295, 561, 15715, 8932, 484, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.1140358281690021, "compression_ratio": 1.7844036697247707, "no_speech_prob": 8.613909449195489e-05}, {"id": 587, "seek": 332112, "start": 3332.2999999999997, "end": 3338.44, "text": " Kaggle fisheries competition which had boats that didn't appear so there were", "tokens": [50364, 5787, 370, 300, 428, 24071, 992, 16212, 5787, 295, 561, 300, 3212, 380, 294, 50592, 50592, 264, 1412, 291, 434, 3097, 428, 2316, 322, 456, 390, 1071, 472, 411, 300, 264, 50923, 50923, 48751, 22631, 20698, 530, 6211, 597, 632, 17772, 300, 994, 380, 4204, 370, 456, 645, 51230, 51230, 1936, 5242, 295, 17772, 291, 434, 4140, 281, 853, 281, 2041, 6069, 437, 3506, 645, 51434, 51434, 294, 264, 5242, 293, 309, 3574, 484, 300, 257, 688, 295, 561, 15715, 8932, 484, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.1140358281690021, "compression_ratio": 1.7844036697247707, "no_speech_prob": 8.613909449195489e-05}, {"id": 588, "seek": 332112, "start": 3338.44, "end": 3342.52, "text": " basically pictures of boats you're meant to try to guess predict what fish were", "tokens": [50364, 5787, 370, 300, 428, 24071, 992, 16212, 5787, 295, 561, 300, 3212, 380, 294, 50592, 50592, 264, 1412, 291, 434, 3097, 428, 2316, 322, 456, 390, 1071, 472, 411, 300, 264, 50923, 50923, 48751, 22631, 20698, 530, 6211, 597, 632, 17772, 300, 994, 380, 4204, 370, 456, 645, 51230, 51230, 1936, 5242, 295, 17772, 291, 434, 4140, 281, 853, 281, 2041, 6069, 437, 3506, 645, 51434, 51434, 294, 264, 5242, 293, 309, 3574, 484, 300, 257, 688, 295, 561, 15715, 8932, 484, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.1140358281690021, "compression_ratio": 1.7844036697247707, "no_speech_prob": 8.613909449195489e-05}, {"id": 589, "seek": 332112, "start": 3342.52, "end": 3347.48, "text": " in the pictures and it turned out that a lot of people accidentally figured out", "tokens": [50364, 5787, 370, 300, 428, 24071, 992, 16212, 5787, 295, 561, 300, 3212, 380, 294, 50592, 50592, 264, 1412, 291, 434, 3097, 428, 2316, 322, 456, 390, 1071, 472, 411, 300, 264, 50923, 50923, 48751, 22631, 20698, 530, 6211, 597, 632, 17772, 300, 994, 380, 4204, 370, 456, 645, 51230, 51230, 1936, 5242, 295, 17772, 291, 434, 4140, 281, 853, 281, 2041, 6069, 437, 3506, 645, 51434, 51434, 294, 264, 5242, 293, 309, 3574, 484, 300, 257, 688, 295, 561, 15715, 8932, 484, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.1140358281690021, "compression_ratio": 1.7844036697247707, "no_speech_prob": 8.613909449195489e-05}, {"id": 590, "seek": 334748, "start": 3347.48, "end": 3352.08, "text": " what the fish were by looking at the boat because certain boats tended to", "tokens": [50364, 437, 264, 3506, 645, 538, 1237, 412, 264, 6582, 570, 1629, 17772, 34732, 281, 50594, 50594, 3745, 1629, 3685, 295, 3506, 293, 370, 538, 23258, 493, 641, 24071, 992, 436, 50812, 50812, 645, 534, 670, 24697, 1078, 295, 264, 14170, 295, 641, 2316, 286, 603, 2152, 294, 51224, 51224, 8437, 498, 291, 600, 668, 926, 48751, 22631, 257, 857, 291, 603, 536, 561, 751, 466, 3278, 51444, 51444, 24071, 257, 688, 286, 478, 445, 516, 281, 2152, 312, 588, 588, 5026, 3278, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.07092854034068972, "compression_ratio": 1.6926605504587156, "no_speech_prob": 8.34810925880447e-05}, {"id": 591, "seek": 334748, "start": 3352.08, "end": 3356.44, "text": " catch certain kinds of fish and so by messing up their validation set they", "tokens": [50364, 437, 264, 3506, 645, 538, 1237, 412, 264, 6582, 570, 1629, 17772, 34732, 281, 50594, 50594, 3745, 1629, 3685, 295, 3506, 293, 370, 538, 23258, 493, 641, 24071, 992, 436, 50812, 50812, 645, 534, 670, 24697, 1078, 295, 264, 14170, 295, 641, 2316, 286, 603, 2152, 294, 51224, 51224, 8437, 498, 291, 600, 668, 926, 48751, 22631, 257, 857, 291, 603, 536, 561, 751, 466, 3278, 51444, 51444, 24071, 257, 688, 286, 478, 445, 516, 281, 2152, 312, 588, 588, 5026, 3278, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.07092854034068972, "compression_ratio": 1.6926605504587156, "no_speech_prob": 8.34810925880447e-05}, {"id": 592, "seek": 334748, "start": 3356.44, "end": 3364.68, "text": " were really overconfident of the accuracy of their model I'll mention in", "tokens": [50364, 437, 264, 3506, 645, 538, 1237, 412, 264, 6582, 570, 1629, 17772, 34732, 281, 50594, 50594, 3745, 1629, 3685, 295, 3506, 293, 370, 538, 23258, 493, 641, 24071, 992, 436, 50812, 50812, 645, 534, 670, 24697, 1078, 295, 264, 14170, 295, 641, 2316, 286, 603, 2152, 294, 51224, 51224, 8437, 498, 291, 600, 668, 926, 48751, 22631, 257, 857, 291, 603, 536, 561, 751, 466, 3278, 51444, 51444, 24071, 257, 688, 286, 478, 445, 516, 281, 2152, 312, 588, 588, 5026, 3278, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.07092854034068972, "compression_ratio": 1.6926605504587156, "no_speech_prob": 8.34810925880447e-05}, {"id": 593, "seek": 334748, "start": 3364.68, "end": 3369.08, "text": " passing if you've been around Kaggle a bit you'll see people talk about cross", "tokens": [50364, 437, 264, 3506, 645, 538, 1237, 412, 264, 6582, 570, 1629, 17772, 34732, 281, 50594, 50594, 3745, 1629, 3685, 295, 3506, 293, 370, 538, 23258, 493, 641, 24071, 992, 436, 50812, 50812, 645, 534, 670, 24697, 1078, 295, 264, 14170, 295, 641, 2316, 286, 603, 2152, 294, 51224, 51224, 8437, 498, 291, 600, 668, 926, 48751, 22631, 257, 857, 291, 603, 536, 561, 751, 466, 3278, 51444, 51444, 24071, 257, 688, 286, 478, 445, 516, 281, 2152, 312, 588, 588, 5026, 3278, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.07092854034068972, "compression_ratio": 1.6926605504587156, "no_speech_prob": 8.34810925880447e-05}, {"id": 594, "seek": 334748, "start": 3369.08, "end": 3374.64, "text": " validation a lot I'm just going to mention be very very careful cross", "tokens": [50364, 437, 264, 3506, 645, 538, 1237, 412, 264, 6582, 570, 1629, 17772, 34732, 281, 50594, 50594, 3745, 1629, 3685, 295, 3506, 293, 370, 538, 23258, 493, 641, 24071, 992, 436, 50812, 50812, 645, 534, 670, 24697, 1078, 295, 264, 14170, 295, 641, 2316, 286, 603, 2152, 294, 51224, 51224, 8437, 498, 291, 600, 668, 926, 48751, 22631, 257, 857, 291, 603, 536, 561, 751, 466, 3278, 51444, 51444, 24071, 257, 688, 286, 478, 445, 516, 281, 2152, 312, 588, 588, 5026, 3278, 51722, 51722], "temperature": 0.0, "avg_logprob": -0.07092854034068972, "compression_ratio": 1.6926605504587156, "no_speech_prob": 8.34810925880447e-05}, {"id": 595, "seek": 337464, "start": 3374.64, "end": 3380.2799999999997, "text": " validation is explicitly not about building a good validation set so you've", "tokens": [50364, 24071, 307, 20803, 406, 466, 2390, 257, 665, 24071, 992, 370, 291, 600, 50646, 50646, 658, 281, 312, 1687, 1687, 5026, 498, 291, 1562, 360, 300, 1071, 551, 286, 603, 2152, 51010, 51010, 307, 300, 2180, 22681, 12, 306, 1083, 44375, 7736, 746, 1219, 3847, 1500, 7472, 382, 51320, 51320, 775, 41706, 1851, 42856, 382, 775, 2370, 7318, 321, 362, 746, 1219, 4974, 51598, 51598], "temperature": 0.0, "avg_logprob": -0.13071380444427036, "compression_ratio": 1.5968586387434556, "no_speech_prob": 4.3992327846353874e-05}, {"id": 596, "seek": 337464, "start": 3380.2799999999997, "end": 3387.56, "text": " got to be super super careful if you ever do that another thing I'll mention", "tokens": [50364, 24071, 307, 20803, 406, 466, 2390, 257, 665, 24071, 992, 370, 291, 600, 50646, 50646, 658, 281, 312, 1687, 1687, 5026, 498, 291, 1562, 360, 300, 1071, 551, 286, 603, 2152, 51010, 51010, 307, 300, 2180, 22681, 12, 306, 1083, 44375, 7736, 746, 1219, 3847, 1500, 7472, 382, 51320, 51320, 775, 41706, 1851, 42856, 382, 775, 2370, 7318, 321, 362, 746, 1219, 4974, 51598, 51598], "temperature": 0.0, "avg_logprob": -0.13071380444427036, "compression_ratio": 1.5968586387434556, "no_speech_prob": 4.3992327846353874e-05}, {"id": 597, "seek": 337464, "start": 3387.56, "end": 3393.7599999999998, "text": " is that scikit-learn conveniently offers something called train test split as", "tokens": [50364, 24071, 307, 20803, 406, 466, 2390, 257, 665, 24071, 992, 370, 291, 600, 50646, 50646, 658, 281, 312, 1687, 1687, 5026, 498, 291, 1562, 360, 300, 1071, 551, 286, 603, 2152, 51010, 51010, 307, 300, 2180, 22681, 12, 306, 1083, 44375, 7736, 746, 1219, 3847, 1500, 7472, 382, 51320, 51320, 775, 41706, 1851, 42856, 382, 775, 2370, 7318, 321, 362, 746, 1219, 4974, 51598, 51598], "temperature": 0.0, "avg_logprob": -0.13071380444427036, "compression_ratio": 1.5968586387434556, "no_speech_prob": 4.3992327846353874e-05}, {"id": 598, "seek": 337464, "start": 3393.7599999999998, "end": 3399.3199999999997, "text": " does hugging face datasets as does fast AI we have something called random", "tokens": [50364, 24071, 307, 20803, 406, 466, 2390, 257, 665, 24071, 992, 370, 291, 600, 50646, 50646, 658, 281, 312, 1687, 1687, 5026, 498, 291, 1562, 360, 300, 1071, 551, 286, 603, 2152, 51010, 51010, 307, 300, 2180, 22681, 12, 306, 1083, 44375, 7736, 746, 1219, 3847, 1500, 7472, 382, 51320, 51320, 775, 41706, 1851, 42856, 382, 775, 2370, 7318, 321, 362, 746, 1219, 4974, 51598, 51598], "temperature": 0.0, "avg_logprob": -0.13071380444427036, "compression_ratio": 1.5968586387434556, "no_speech_prob": 4.3992327846353874e-05}, {"id": 599, "seek": 339932, "start": 3399.32, "end": 3406.4, "text": " split it can be encouraged it can almost feel like it's encouraging you to use a", "tokens": [50364, 7472, 309, 393, 312, 14658, 309, 393, 1920, 841, 411, 309, 311, 14580, 291, 281, 764, 257, 50718, 50718, 38513, 24071, 992, 570, 456, 366, 613, 7150, 300, 360, 309, 337, 291, 457, 51036, 51036, 1338, 312, 588, 588, 5026, 570, 588, 588, 2049, 300, 311, 406, 437, 291, 528, 1392, 51302, 51302, 370, 498, 291, 528, 437, 257, 24071, 992, 307, 370, 300, 311, 264, 857, 300, 291, 2235, 484, 295, 51480, 51480, 428, 1412, 300, 291, 500, 380, 3847, 365, 457, 291, 360, 3481, 428, 14170, 365, 370, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.12379081198509703, "compression_ratio": 1.8726415094339623, "no_speech_prob": 1.7500031390227377e-05}, {"id": 600, "seek": 339932, "start": 3406.4, "end": 3412.76, "text": " randomized validation set because there are these methods that do it for you but", "tokens": [50364, 7472, 309, 393, 312, 14658, 309, 393, 1920, 841, 411, 309, 311, 14580, 291, 281, 764, 257, 50718, 50718, 38513, 24071, 992, 570, 456, 366, 613, 7150, 300, 360, 309, 337, 291, 457, 51036, 51036, 1338, 312, 588, 588, 5026, 570, 588, 588, 2049, 300, 311, 406, 437, 291, 528, 1392, 51302, 51302, 370, 498, 291, 528, 437, 257, 24071, 992, 307, 370, 300, 311, 264, 857, 300, 291, 2235, 484, 295, 51480, 51480, 428, 1412, 300, 291, 500, 380, 3847, 365, 457, 291, 360, 3481, 428, 14170, 365, 370, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.12379081198509703, "compression_ratio": 1.8726415094339623, "no_speech_prob": 1.7500031390227377e-05}, {"id": 601, "seek": 339932, "start": 3412.76, "end": 3418.0800000000004, "text": " yeah be very very careful because very very often that's not what you want okay", "tokens": [50364, 7472, 309, 393, 312, 14658, 309, 393, 1920, 841, 411, 309, 311, 14580, 291, 281, 764, 257, 50718, 50718, 38513, 24071, 992, 570, 456, 366, 613, 7150, 300, 360, 309, 337, 291, 457, 51036, 51036, 1338, 312, 588, 588, 5026, 570, 588, 588, 2049, 300, 311, 406, 437, 291, 528, 1392, 51302, 51302, 370, 498, 291, 528, 437, 257, 24071, 992, 307, 370, 300, 311, 264, 857, 300, 291, 2235, 484, 295, 51480, 51480, 428, 1412, 300, 291, 500, 380, 3847, 365, 457, 291, 360, 3481, 428, 14170, 365, 370, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.12379081198509703, "compression_ratio": 1.8726415094339623, "no_speech_prob": 1.7500031390227377e-05}, {"id": 602, "seek": 339932, "start": 3418.0800000000004, "end": 3421.6400000000003, "text": " so if you want what a validation set is so that's the bit that you pull out of", "tokens": [50364, 7472, 309, 393, 312, 14658, 309, 393, 1920, 841, 411, 309, 311, 14580, 291, 281, 764, 257, 50718, 50718, 38513, 24071, 992, 570, 456, 366, 613, 7150, 300, 360, 309, 337, 291, 457, 51036, 51036, 1338, 312, 588, 588, 5026, 570, 588, 588, 2049, 300, 311, 406, 437, 291, 528, 1392, 51302, 51302, 370, 498, 291, 528, 437, 257, 24071, 992, 307, 370, 300, 311, 264, 857, 300, 291, 2235, 484, 295, 51480, 51480, 428, 1412, 300, 291, 500, 380, 3847, 365, 457, 291, 360, 3481, 428, 14170, 365, 370, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.12379081198509703, "compression_ratio": 1.8726415094339623, "no_speech_prob": 1.7500031390227377e-05}, {"id": 603, "seek": 339932, "start": 3421.6400000000003, "end": 3427.28, "text": " your data that you don't train with but you do measure your accuracy with so", "tokens": [50364, 7472, 309, 393, 312, 14658, 309, 393, 1920, 841, 411, 309, 311, 14580, 291, 281, 764, 257, 50718, 50718, 38513, 24071, 992, 570, 456, 366, 613, 7150, 300, 360, 309, 337, 291, 457, 51036, 51036, 1338, 312, 588, 588, 5026, 570, 588, 588, 2049, 300, 311, 406, 437, 291, 528, 1392, 51302, 51302, 370, 498, 291, 528, 437, 257, 24071, 992, 307, 370, 300, 311, 264, 857, 300, 291, 2235, 484, 295, 51480, 51480, 428, 1412, 300, 291, 500, 380, 3847, 365, 457, 291, 360, 3481, 428, 14170, 365, 370, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.12379081198509703, "compression_ratio": 1.8726415094339623, "no_speech_prob": 1.7500031390227377e-05}, {"id": 604, "seek": 342728, "start": 3427.28, "end": 3434.92, "text": " what's a test set it's basically another validation set but you don't even use it", "tokens": [50364, 437, 311, 257, 1500, 992, 309, 311, 1936, 1071, 24071, 992, 457, 291, 500, 380, 754, 764, 309, 50746, 50746, 337, 11603, 428, 14170, 1339, 291, 1322, 428, 2316, 983, 406, 731, 3811, 51044, 51044, 291, 3031, 732, 777, 5245, 633, 786, 337, 1045, 2493, 300, 311, 577, 938, 257, 48751, 22631, 51252, 51252, 6211, 1709, 337, 370, 291, 576, 362, 3031, 11971, 5245, 293, 550, 291, 574, 412, 51518, 51518, 264, 14170, 322, 264, 24071, 992, 337, 1184, 472, 512, 295, 729, 5245, 291, 576, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.0929788695441352, "compression_ratio": 1.7366071428571428, "no_speech_prob": 3.0239252737374045e-05}, {"id": 605, "seek": 342728, "start": 3434.92, "end": 3440.88, "text": " for tracking your accuracy while you build your model why not well imagine", "tokens": [50364, 437, 311, 257, 1500, 992, 309, 311, 1936, 1071, 24071, 992, 457, 291, 500, 380, 754, 764, 309, 50746, 50746, 337, 11603, 428, 14170, 1339, 291, 1322, 428, 2316, 983, 406, 731, 3811, 51044, 51044, 291, 3031, 732, 777, 5245, 633, 786, 337, 1045, 2493, 300, 311, 577, 938, 257, 48751, 22631, 51252, 51252, 6211, 1709, 337, 370, 291, 576, 362, 3031, 11971, 5245, 293, 550, 291, 574, 412, 51518, 51518, 264, 14170, 322, 264, 24071, 992, 337, 1184, 472, 512, 295, 729, 5245, 291, 576, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.0929788695441352, "compression_ratio": 1.7366071428571428, "no_speech_prob": 3.0239252737374045e-05}, {"id": 606, "seek": 342728, "start": 3440.88, "end": 3445.0400000000004, "text": " you tried two new models every day for three months that's how long a Kaggle", "tokens": [50364, 437, 311, 257, 1500, 992, 309, 311, 1936, 1071, 24071, 992, 457, 291, 500, 380, 754, 764, 309, 50746, 50746, 337, 11603, 428, 14170, 1339, 291, 1322, 428, 2316, 983, 406, 731, 3811, 51044, 51044, 291, 3031, 732, 777, 5245, 633, 786, 337, 1045, 2493, 300, 311, 577, 938, 257, 48751, 22631, 51252, 51252, 6211, 1709, 337, 370, 291, 576, 362, 3031, 11971, 5245, 293, 550, 291, 574, 412, 51518, 51518, 264, 14170, 322, 264, 24071, 992, 337, 1184, 472, 512, 295, 729, 5245, 291, 576, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.0929788695441352, "compression_ratio": 1.7366071428571428, "no_speech_prob": 3.0239252737374045e-05}, {"id": 607, "seek": 342728, "start": 3445.0400000000004, "end": 3450.36, "text": " competition goes for so you would have tried 180 models and then you look at", "tokens": [50364, 437, 311, 257, 1500, 992, 309, 311, 1936, 1071, 24071, 992, 457, 291, 500, 380, 754, 764, 309, 50746, 50746, 337, 11603, 428, 14170, 1339, 291, 1322, 428, 2316, 983, 406, 731, 3811, 51044, 51044, 291, 3031, 732, 777, 5245, 633, 786, 337, 1045, 2493, 300, 311, 577, 938, 257, 48751, 22631, 51252, 51252, 6211, 1709, 337, 370, 291, 576, 362, 3031, 11971, 5245, 293, 550, 291, 574, 412, 51518, 51518, 264, 14170, 322, 264, 24071, 992, 337, 1184, 472, 512, 295, 729, 5245, 291, 576, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.0929788695441352, "compression_ratio": 1.7366071428571428, "no_speech_prob": 3.0239252737374045e-05}, {"id": 608, "seek": 342728, "start": 3450.36, "end": 3455.1600000000003, "text": " the accuracy on the validation set for each one some of those models you would", "tokens": [50364, 437, 311, 257, 1500, 992, 309, 311, 1936, 1071, 24071, 992, 457, 291, 500, 380, 754, 764, 309, 50746, 50746, 337, 11603, 428, 14170, 1339, 291, 1322, 428, 2316, 983, 406, 731, 3811, 51044, 51044, 291, 3031, 732, 777, 5245, 633, 786, 337, 1045, 2493, 300, 311, 577, 938, 257, 48751, 22631, 51252, 51252, 6211, 1709, 337, 370, 291, 576, 362, 3031, 11971, 5245, 293, 550, 291, 574, 412, 51518, 51518, 264, 14170, 322, 264, 24071, 992, 337, 1184, 472, 512, 295, 729, 5245, 291, 576, 51758, 51758], "temperature": 0.0, "avg_logprob": -0.0929788695441352, "compression_ratio": 1.7366071428571428, "no_speech_prob": 3.0239252737374045e-05}, {"id": 609, "seek": 345516, "start": 3455.16, "end": 3459.08, "text": " have got a good accuracy on the validation set potentially because of", "tokens": [50364, 362, 658, 257, 665, 14170, 322, 264, 24071, 992, 7263, 570, 295, 50560, 50560, 6075, 2931, 445, 257, 22137, 293, 550, 291, 483, 439, 2919, 293, 291, 10315, 300, 50774, 50774, 281, 48751, 22631, 293, 291, 519, 291, 434, 516, 281, 1942, 264, 6211, 293, 291, 2082, 309, 493, 50948, 50948, 293, 300, 311, 570, 291, 767, 670, 6845, 1228, 264, 24071, 992, 370, 291, 767, 51272, 51272, 528, 281, 458, 1968, 291, 600, 534, 1352, 257, 665, 2316, 420, 406, 370, 294, 1186, 322, 48751, 22631, 51594, 51594, 436, 362, 732, 732, 1500, 6352, 436, 600, 658, 264, 472, 300, 2709, 291, 5824, 322, 264, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.09906409870494495, "compression_ratio": 1.8577075098814229, "no_speech_prob": 1.922208502946887e-05}, {"id": 610, "seek": 345516, "start": 3459.08, "end": 3463.3599999999997, "text": " pure chance just a coincidence and then you get all excited and you submit that", "tokens": [50364, 362, 658, 257, 665, 14170, 322, 264, 24071, 992, 7263, 570, 295, 50560, 50560, 6075, 2931, 445, 257, 22137, 293, 550, 291, 483, 439, 2919, 293, 291, 10315, 300, 50774, 50774, 281, 48751, 22631, 293, 291, 519, 291, 434, 516, 281, 1942, 264, 6211, 293, 291, 2082, 309, 493, 50948, 50948, 293, 300, 311, 570, 291, 767, 670, 6845, 1228, 264, 24071, 992, 370, 291, 767, 51272, 51272, 528, 281, 458, 1968, 291, 600, 534, 1352, 257, 665, 2316, 420, 406, 370, 294, 1186, 322, 48751, 22631, 51594, 51594, 436, 362, 732, 732, 1500, 6352, 436, 600, 658, 264, 472, 300, 2709, 291, 5824, 322, 264, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.09906409870494495, "compression_ratio": 1.8577075098814229, "no_speech_prob": 1.922208502946887e-05}, {"id": 611, "seek": 345516, "start": 3463.3599999999997, "end": 3466.8399999999997, "text": " to Kaggle and you think you're going to win the competition and you mess it up", "tokens": [50364, 362, 658, 257, 665, 14170, 322, 264, 24071, 992, 7263, 570, 295, 50560, 50560, 6075, 2931, 445, 257, 22137, 293, 550, 291, 483, 439, 2919, 293, 291, 10315, 300, 50774, 50774, 281, 48751, 22631, 293, 291, 519, 291, 434, 516, 281, 1942, 264, 6211, 293, 291, 2082, 309, 493, 50948, 50948, 293, 300, 311, 570, 291, 767, 670, 6845, 1228, 264, 24071, 992, 370, 291, 767, 51272, 51272, 528, 281, 458, 1968, 291, 600, 534, 1352, 257, 665, 2316, 420, 406, 370, 294, 1186, 322, 48751, 22631, 51594, 51594, 436, 362, 732, 732, 1500, 6352, 436, 600, 658, 264, 472, 300, 2709, 291, 5824, 322, 264, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.09906409870494495, "compression_ratio": 1.8577075098814229, "no_speech_prob": 1.922208502946887e-05}, {"id": 612, "seek": 345516, "start": 3466.8399999999997, "end": 3473.3199999999997, "text": " and that's because you actually overfit using the validation set so you actually", "tokens": [50364, 362, 658, 257, 665, 14170, 322, 264, 24071, 992, 7263, 570, 295, 50560, 50560, 6075, 2931, 445, 257, 22137, 293, 550, 291, 483, 439, 2919, 293, 291, 10315, 300, 50774, 50774, 281, 48751, 22631, 293, 291, 519, 291, 434, 516, 281, 1942, 264, 6211, 293, 291, 2082, 309, 493, 50948, 50948, 293, 300, 311, 570, 291, 767, 670, 6845, 1228, 264, 24071, 992, 370, 291, 767, 51272, 51272, 528, 281, 458, 1968, 291, 600, 534, 1352, 257, 665, 2316, 420, 406, 370, 294, 1186, 322, 48751, 22631, 51594, 51594, 436, 362, 732, 732, 1500, 6352, 436, 600, 658, 264, 472, 300, 2709, 291, 5824, 322, 264, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.09906409870494495, "compression_ratio": 1.8577075098814229, "no_speech_prob": 1.922208502946887e-05}, {"id": 613, "seek": 345516, "start": 3473.3199999999997, "end": 3479.7599999999998, "text": " want to know whether you've really found a good model or not so in fact on Kaggle", "tokens": [50364, 362, 658, 257, 665, 14170, 322, 264, 24071, 992, 7263, 570, 295, 50560, 50560, 6075, 2931, 445, 257, 22137, 293, 550, 291, 483, 439, 2919, 293, 291, 10315, 300, 50774, 50774, 281, 48751, 22631, 293, 291, 519, 291, 434, 516, 281, 1942, 264, 6211, 293, 291, 2082, 309, 493, 50948, 50948, 293, 300, 311, 570, 291, 767, 670, 6845, 1228, 264, 24071, 992, 370, 291, 767, 51272, 51272, 528, 281, 458, 1968, 291, 600, 534, 1352, 257, 665, 2316, 420, 406, 370, 294, 1186, 322, 48751, 22631, 51594, 51594, 436, 362, 732, 732, 1500, 6352, 436, 600, 658, 264, 472, 300, 2709, 291, 5824, 322, 264, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.09906409870494495, "compression_ratio": 1.8577075098814229, "no_speech_prob": 1.922208502946887e-05}, {"id": 614, "seek": 345516, "start": 3479.7599999999998, "end": 3484.08, "text": " they have two two test sets they've got the one that gives you feedback on the", "tokens": [50364, 362, 658, 257, 665, 14170, 322, 264, 24071, 992, 7263, 570, 295, 50560, 50560, 6075, 2931, 445, 257, 22137, 293, 550, 291, 483, 439, 2919, 293, 291, 10315, 300, 50774, 50774, 281, 48751, 22631, 293, 291, 519, 291, 434, 516, 281, 1942, 264, 6211, 293, 291, 2082, 309, 493, 50948, 50948, 293, 300, 311, 570, 291, 767, 670, 6845, 1228, 264, 24071, 992, 370, 291, 767, 51272, 51272, 528, 281, 458, 1968, 291, 600, 534, 1352, 257, 665, 2316, 420, 406, 370, 294, 1186, 322, 48751, 22631, 51594, 51594, 436, 362, 732, 732, 1500, 6352, 436, 600, 658, 264, 472, 300, 2709, 291, 5824, 322, 264, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.09906409870494495, "compression_ratio": 1.8577075098814229, "no_speech_prob": 1.922208502946887e-05}, {"id": 615, "seek": 348408, "start": 3484.08, "end": 3488.84, "text": " leaderboard during the competition and a second test set which you don't get", "tokens": [50364, 5263, 3787, 1830, 264, 6211, 293, 257, 1150, 1500, 992, 597, 291, 500, 380, 483, 50602, 50602, 281, 536, 1826, 934, 264, 6211, 307, 4335, 370, 294, 957, 993, 291, 600, 658, 281, 50896, 50896, 312, 588, 5026, 466, 341, 406, 281, 853, 370, 867, 5245, 1830, 428, 2316, 2390, 51158, 51158, 1399, 300, 291, 15715, 915, 472, 300, 311, 665, 538, 22137, 293, 787, 498, 51406, 51406, 291, 362, 257, 1500, 992, 300, 291, 600, 5167, 484, 420, 291, 458, 300, 586, 300, 6689, 281, 264, 51626, 51626, 6322, 1168, 597, 307, 588, 7595, 307, 291, 4418, 1045, 2493, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.09533993097452018, "compression_ratio": 1.803921568627451, "no_speech_prob": 5.7378114433959126e-05}, {"id": 616, "seek": 348408, "start": 3488.84, "end": 3494.72, "text": " to see until after the competition is finished so in real life you've got to", "tokens": [50364, 5263, 3787, 1830, 264, 6211, 293, 257, 1150, 1500, 992, 597, 291, 500, 380, 483, 50602, 50602, 281, 536, 1826, 934, 264, 6211, 307, 4335, 370, 294, 957, 993, 291, 600, 658, 281, 50896, 50896, 312, 588, 5026, 466, 341, 406, 281, 853, 370, 867, 5245, 1830, 428, 2316, 2390, 51158, 51158, 1399, 300, 291, 15715, 915, 472, 300, 311, 665, 538, 22137, 293, 787, 498, 51406, 51406, 291, 362, 257, 1500, 992, 300, 291, 600, 5167, 484, 420, 291, 458, 300, 586, 300, 6689, 281, 264, 51626, 51626, 6322, 1168, 597, 307, 588, 7595, 307, 291, 4418, 1045, 2493, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.09533993097452018, "compression_ratio": 1.803921568627451, "no_speech_prob": 5.7378114433959126e-05}, {"id": 617, "seek": 348408, "start": 3494.72, "end": 3499.96, "text": " be very careful about this not to try so many models during your model building", "tokens": [50364, 5263, 3787, 1830, 264, 6211, 293, 257, 1150, 1500, 992, 597, 291, 500, 380, 483, 50602, 50602, 281, 536, 1826, 934, 264, 6211, 307, 4335, 370, 294, 957, 993, 291, 600, 658, 281, 50896, 50896, 312, 588, 5026, 466, 341, 406, 281, 853, 370, 867, 5245, 1830, 428, 2316, 2390, 51158, 51158, 1399, 300, 291, 15715, 915, 472, 300, 311, 665, 538, 22137, 293, 787, 498, 51406, 51406, 291, 362, 257, 1500, 992, 300, 291, 600, 5167, 484, 420, 291, 458, 300, 586, 300, 6689, 281, 264, 51626, 51626, 6322, 1168, 597, 307, 588, 7595, 307, 291, 4418, 1045, 2493, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.09533993097452018, "compression_ratio": 1.803921568627451, "no_speech_prob": 5.7378114433959126e-05}, {"id": 618, "seek": 348408, "start": 3499.96, "end": 3504.92, "text": " process that you accidentally find one that's good by coincidence and only if", "tokens": [50364, 5263, 3787, 1830, 264, 6211, 293, 257, 1150, 1500, 992, 597, 291, 500, 380, 483, 50602, 50602, 281, 536, 1826, 934, 264, 6211, 307, 4335, 370, 294, 957, 993, 291, 600, 658, 281, 50896, 50896, 312, 588, 5026, 466, 341, 406, 281, 853, 370, 867, 5245, 1830, 428, 2316, 2390, 51158, 51158, 1399, 300, 291, 15715, 915, 472, 300, 311, 665, 538, 22137, 293, 787, 498, 51406, 51406, 291, 362, 257, 1500, 992, 300, 291, 600, 5167, 484, 420, 291, 458, 300, 586, 300, 6689, 281, 264, 51626, 51626, 6322, 1168, 597, 307, 588, 7595, 307, 291, 4418, 1045, 2493, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.09533993097452018, "compression_ratio": 1.803921568627451, "no_speech_prob": 5.7378114433959126e-05}, {"id": 619, "seek": 348408, "start": 3504.92, "end": 3509.3199999999997, "text": " you have a test set that you've held out or you know that now that leads to the", "tokens": [50364, 5263, 3787, 1830, 264, 6211, 293, 257, 1150, 1500, 992, 597, 291, 500, 380, 483, 50602, 50602, 281, 536, 1826, 934, 264, 6211, 307, 4335, 370, 294, 957, 993, 291, 600, 658, 281, 50896, 50896, 312, 588, 5026, 466, 341, 406, 281, 853, 370, 867, 5245, 1830, 428, 2316, 2390, 51158, 51158, 1399, 300, 291, 15715, 915, 472, 300, 311, 665, 538, 22137, 293, 787, 498, 51406, 51406, 291, 362, 257, 1500, 992, 300, 291, 600, 5167, 484, 420, 291, 458, 300, 586, 300, 6689, 281, 264, 51626, 51626, 6322, 1168, 597, 307, 588, 7595, 307, 291, 4418, 1045, 2493, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.09533993097452018, "compression_ratio": 1.803921568627451, "no_speech_prob": 5.7378114433959126e-05}, {"id": 620, "seek": 348408, "start": 3509.3199999999997, "end": 3512.48, "text": " obvious question which is very challenging is you spent three months", "tokens": [50364, 5263, 3787, 1830, 264, 6211, 293, 257, 1150, 1500, 992, 597, 291, 500, 380, 483, 50602, 50602, 281, 536, 1826, 934, 264, 6211, 307, 4335, 370, 294, 957, 993, 291, 600, 658, 281, 50896, 50896, 312, 588, 5026, 466, 341, 406, 281, 853, 370, 867, 5245, 1830, 428, 2316, 2390, 51158, 51158, 1399, 300, 291, 15715, 915, 472, 300, 311, 665, 538, 22137, 293, 787, 498, 51406, 51406, 291, 362, 257, 1500, 992, 300, 291, 600, 5167, 484, 420, 291, 458, 300, 586, 300, 6689, 281, 264, 51626, 51626, 6322, 1168, 597, 307, 588, 7595, 307, 291, 4418, 1045, 2493, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.09533993097452018, "compression_ratio": 1.803921568627451, "no_speech_prob": 5.7378114433959126e-05}, {"id": 621, "seek": 351248, "start": 3512.48, "end": 3518.0, "text": " working on a model worked well on your validation set you did a good job of", "tokens": [50364, 1364, 322, 257, 2316, 2732, 731, 322, 428, 24071, 992, 291, 630, 257, 665, 1691, 295, 50640, 50640, 23954, 300, 1500, 992, 1314, 294, 257, 3273, 370, 291, 4999, 380, 4350, 281, 764, 309, 293, 412, 264, 50796, 50796, 917, 295, 264, 1045, 2493, 291, 2721, 10033, 294, 322, 264, 1500, 992, 293, 309, 311, 50978, 50978, 6237, 437, 360, 291, 360, 6095, 291, 362, 281, 352, 646, 281, 3732, 472, 291, 458, 51324, 51324, 456, 534, 1943, 380, 604, 3922, 661, 813, 2891, 797, 370, 341, 307, 4930, 457, 309, 311, 51604, 51604, 1101, 281, 458, 558, 1101, 281, 458, 813, 281, 406, 458, 370, 300, 311, 437, 257, 1500, 6352, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09046824225063982, "compression_ratio": 1.8565737051792828, "no_speech_prob": 0.00018809021275956184}, {"id": 622, "seek": 351248, "start": 3518.0, "end": 3521.12, "text": " locking that test set away in a safe so you weren't allowed to use it and at the", "tokens": [50364, 1364, 322, 257, 2316, 2732, 731, 322, 428, 24071, 992, 291, 630, 257, 665, 1691, 295, 50640, 50640, 23954, 300, 1500, 992, 1314, 294, 257, 3273, 370, 291, 4999, 380, 4350, 281, 764, 309, 293, 412, 264, 50796, 50796, 917, 295, 264, 1045, 2493, 291, 2721, 10033, 294, 322, 264, 1500, 992, 293, 309, 311, 50978, 50978, 6237, 437, 360, 291, 360, 6095, 291, 362, 281, 352, 646, 281, 3732, 472, 291, 458, 51324, 51324, 456, 534, 1943, 380, 604, 3922, 661, 813, 2891, 797, 370, 341, 307, 4930, 457, 309, 311, 51604, 51604, 1101, 281, 458, 558, 1101, 281, 458, 813, 281, 406, 458, 370, 300, 311, 437, 257, 1500, 6352, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09046824225063982, "compression_ratio": 1.8565737051792828, "no_speech_prob": 0.00018809021275956184}, {"id": 623, "seek": 351248, "start": 3521.12, "end": 3524.76, "text": " end of the three months you finally checked in on the test set and it's", "tokens": [50364, 1364, 322, 257, 2316, 2732, 731, 322, 428, 24071, 992, 291, 630, 257, 665, 1691, 295, 50640, 50640, 23954, 300, 1500, 992, 1314, 294, 257, 3273, 370, 291, 4999, 380, 4350, 281, 764, 309, 293, 412, 264, 50796, 50796, 917, 295, 264, 1045, 2493, 291, 2721, 10033, 294, 322, 264, 1500, 992, 293, 309, 311, 50978, 50978, 6237, 437, 360, 291, 360, 6095, 291, 362, 281, 352, 646, 281, 3732, 472, 291, 458, 51324, 51324, 456, 534, 1943, 380, 604, 3922, 661, 813, 2891, 797, 370, 341, 307, 4930, 457, 309, 311, 51604, 51604, 1101, 281, 458, 558, 1101, 281, 458, 813, 281, 406, 458, 370, 300, 311, 437, 257, 1500, 6352, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09046824225063982, "compression_ratio": 1.8565737051792828, "no_speech_prob": 0.00018809021275956184}, {"id": 624, "seek": 351248, "start": 3524.76, "end": 3531.68, "text": " terrible what do you do honestly you have to go back to square one you know", "tokens": [50364, 1364, 322, 257, 2316, 2732, 731, 322, 428, 24071, 992, 291, 630, 257, 665, 1691, 295, 50640, 50640, 23954, 300, 1500, 992, 1314, 294, 257, 3273, 370, 291, 4999, 380, 4350, 281, 764, 309, 293, 412, 264, 50796, 50796, 917, 295, 264, 1045, 2493, 291, 2721, 10033, 294, 322, 264, 1500, 992, 293, 309, 311, 50978, 50978, 6237, 437, 360, 291, 360, 6095, 291, 362, 281, 352, 646, 281, 3732, 472, 291, 458, 51324, 51324, 456, 534, 1943, 380, 604, 3922, 661, 813, 2891, 797, 370, 341, 307, 4930, 457, 309, 311, 51604, 51604, 1101, 281, 458, 558, 1101, 281, 458, 813, 281, 406, 458, 370, 300, 311, 437, 257, 1500, 6352, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09046824225063982, "compression_ratio": 1.8565737051792828, "no_speech_prob": 0.00018809021275956184}, {"id": 625, "seek": 351248, "start": 3531.68, "end": 3537.28, "text": " there really isn't any choice other than starting again so this is tough but it's", "tokens": [50364, 1364, 322, 257, 2316, 2732, 731, 322, 428, 24071, 992, 291, 630, 257, 665, 1691, 295, 50640, 50640, 23954, 300, 1500, 992, 1314, 294, 257, 3273, 370, 291, 4999, 380, 4350, 281, 764, 309, 293, 412, 264, 50796, 50796, 917, 295, 264, 1045, 2493, 291, 2721, 10033, 294, 322, 264, 1500, 992, 293, 309, 311, 50978, 50978, 6237, 437, 360, 291, 360, 6095, 291, 362, 281, 352, 646, 281, 3732, 472, 291, 458, 51324, 51324, 456, 534, 1943, 380, 604, 3922, 661, 813, 2891, 797, 370, 341, 307, 4930, 457, 309, 311, 51604, 51604, 1101, 281, 458, 558, 1101, 281, 458, 813, 281, 406, 458, 370, 300, 311, 437, 257, 1500, 6352, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09046824225063982, "compression_ratio": 1.8565737051792828, "no_speech_prob": 0.00018809021275956184}, {"id": 626, "seek": 351248, "start": 3537.28, "end": 3540.8, "text": " better to know right better to know than to not know so that's what a test sets", "tokens": [50364, 1364, 322, 257, 2316, 2732, 731, 322, 428, 24071, 992, 291, 630, 257, 665, 1691, 295, 50640, 50640, 23954, 300, 1500, 992, 1314, 294, 257, 3273, 370, 291, 4999, 380, 4350, 281, 764, 309, 293, 412, 264, 50796, 50796, 917, 295, 264, 1045, 2493, 291, 2721, 10033, 294, 322, 264, 1500, 992, 293, 309, 311, 50978, 50978, 6237, 437, 360, 291, 360, 6095, 291, 362, 281, 352, 646, 281, 3732, 472, 291, 458, 51324, 51324, 456, 534, 1943, 380, 604, 3922, 661, 813, 2891, 797, 370, 341, 307, 4930, 457, 309, 311, 51604, 51604, 1101, 281, 458, 558, 1101, 281, 458, 813, 281, 406, 458, 370, 300, 311, 437, 257, 1500, 6352, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.09046824225063982, "compression_ratio": 1.8565737051792828, "no_speech_prob": 0.00018809021275956184}, {"id": 627, "seek": 354080, "start": 3540.8, "end": 3548.6000000000004, "text": " for so you've got a validation set what are you going to do with it what you're", "tokens": [50364, 337, 370, 291, 600, 658, 257, 24071, 992, 437, 366, 291, 516, 281, 360, 365, 309, 437, 291, 434, 50754, 50754, 516, 281, 360, 365, 257, 24071, 992, 307, 291, 434, 516, 281, 3481, 512, 16367, 370, 51051, 51051, 257, 20678, 307, 746, 411, 14170, 309, 311, 257, 1230, 300, 5112, 291, 577, 665, 307, 428, 51366, 51366, 2316, 586, 322, 48751, 22631, 341, 307, 588, 1858, 437, 16367, 820, 321, 764, 731, 436, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.10057825919909355, "compression_ratio": 1.8304093567251463, "no_speech_prob": 2.5864848794299178e-05}, {"id": 628, "seek": 354080, "start": 3548.6000000000004, "end": 3554.54, "text": " going to do with a validation set is you're going to measure some metrics so", "tokens": [50364, 337, 370, 291, 600, 658, 257, 24071, 992, 437, 366, 291, 516, 281, 360, 365, 309, 437, 291, 434, 50754, 50754, 516, 281, 360, 365, 257, 24071, 992, 307, 291, 434, 516, 281, 3481, 512, 16367, 370, 51051, 51051, 257, 20678, 307, 746, 411, 14170, 309, 311, 257, 1230, 300, 5112, 291, 577, 665, 307, 428, 51366, 51366, 2316, 586, 322, 48751, 22631, 341, 307, 588, 1858, 437, 16367, 820, 321, 764, 731, 436, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.10057825919909355, "compression_ratio": 1.8304093567251463, "no_speech_prob": 2.5864848794299178e-05}, {"id": 629, "seek": 354080, "start": 3554.54, "end": 3560.84, "text": " a metric is something like accuracy it's a number that tells you how good is your", "tokens": [50364, 337, 370, 291, 600, 658, 257, 24071, 992, 437, 366, 291, 516, 281, 360, 365, 309, 437, 291, 434, 50754, 50754, 516, 281, 360, 365, 257, 24071, 992, 307, 291, 434, 516, 281, 3481, 512, 16367, 370, 51051, 51051, 257, 20678, 307, 746, 411, 14170, 309, 311, 257, 1230, 300, 5112, 291, 577, 665, 307, 428, 51366, 51366, 2316, 586, 322, 48751, 22631, 341, 307, 588, 1858, 437, 16367, 820, 321, 764, 731, 436, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.10057825919909355, "compression_ratio": 1.8304093567251463, "no_speech_prob": 2.5864848794299178e-05}, {"id": 630, "seek": 354080, "start": 3560.84, "end": 3568.88, "text": " model now on Kaggle this is very easy what metrics should we use well they", "tokens": [50364, 337, 370, 291, 600, 658, 257, 24071, 992, 437, 366, 291, 516, 281, 360, 365, 309, 437, 291, 434, 50754, 50754, 516, 281, 360, 365, 257, 24071, 992, 307, 291, 434, 516, 281, 3481, 512, 16367, 370, 51051, 51051, 257, 20678, 307, 746, 411, 14170, 309, 311, 257, 1230, 300, 5112, 291, 577, 665, 307, 428, 51366, 51366, 2316, 586, 322, 48751, 22631, 341, 307, 588, 1858, 437, 16367, 820, 321, 764, 731, 436, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.10057825919909355, "compression_ratio": 1.8304093567251463, "no_speech_prob": 2.5864848794299178e-05}, {"id": 631, "seek": 356888, "start": 3568.88, "end": 3576.1600000000003, "text": " tell us go to overview click on evaluation and find out and it says oh we", "tokens": [50364, 980, 505, 352, 281, 12492, 2052, 322, 13344, 293, 915, 484, 293, 309, 1619, 1954, 321, 50728, 50728, 486, 13059, 322, 264, 39041, 20009, 17619, 4412, 341, 307, 264, 20678, 50990, 50990, 291, 1127, 466, 370, 472, 6322, 1168, 307, 307, 341, 264, 912, 382, 264, 4470, 2445, 51520, 51520, 307, 341, 264, 551, 300, 321, 486, 747, 264, 13760, 295, 293, 915, 264, 16235, 293, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.0910745348249163, "compression_ratio": 1.7696629213483146, "no_speech_prob": 1.0288650628353935e-05}, {"id": 632, "seek": 356888, "start": 3576.1600000000003, "end": 3581.4, "text": " will evaluate on the Pearson correlation coefficient therefore this is the metric", "tokens": [50364, 980, 505, 352, 281, 12492, 2052, 322, 13344, 293, 915, 484, 293, 309, 1619, 1954, 321, 50728, 50728, 486, 13059, 322, 264, 39041, 20009, 17619, 4412, 341, 307, 264, 20678, 50990, 50990, 291, 1127, 466, 370, 472, 6322, 1168, 307, 307, 341, 264, 912, 382, 264, 4470, 2445, 51520, 51520, 307, 341, 264, 551, 300, 321, 486, 747, 264, 13760, 295, 293, 915, 264, 16235, 293, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.0910745348249163, "compression_ratio": 1.7696629213483146, "no_speech_prob": 1.0288650628353935e-05}, {"id": 633, "seek": 356888, "start": 3581.4, "end": 3592.0, "text": " you care about so one obvious question is is this the same as the loss function", "tokens": [50364, 980, 505, 352, 281, 12492, 2052, 322, 13344, 293, 915, 484, 293, 309, 1619, 1954, 321, 50728, 50728, 486, 13059, 322, 264, 39041, 20009, 17619, 4412, 341, 307, 264, 20678, 50990, 50990, 291, 1127, 466, 370, 472, 6322, 1168, 307, 307, 341, 264, 912, 382, 264, 4470, 2445, 51520, 51520, 307, 341, 264, 551, 300, 321, 486, 747, 264, 13760, 295, 293, 915, 264, 16235, 293, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.0910745348249163, "compression_ratio": 1.7696629213483146, "no_speech_prob": 1.0288650628353935e-05}, {"id": 634, "seek": 356888, "start": 3592.0, "end": 3597.2000000000003, "text": " is this the thing that we will take the derivative of and find the gradient and", "tokens": [50364, 980, 505, 352, 281, 12492, 2052, 322, 13344, 293, 915, 484, 293, 309, 1619, 1954, 321, 50728, 50728, 486, 13059, 322, 264, 39041, 20009, 17619, 4412, 341, 307, 264, 20678, 50990, 50990, 291, 1127, 466, 370, 472, 6322, 1168, 307, 307, 341, 264, 912, 382, 264, 4470, 2445, 51520, 51520, 307, 341, 264, 551, 300, 321, 486, 747, 264, 13760, 295, 293, 915, 264, 16235, 293, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.0910745348249163, "compression_ratio": 1.7696629213483146, "no_speech_prob": 1.0288650628353935e-05}, {"id": 635, "seek": 359720, "start": 3597.2, "end": 3604.7599999999998, "text": " use that to improve our parameters during training and the answer is maybe", "tokens": [50364, 764, 300, 281, 3470, 527, 9834, 1830, 3097, 293, 264, 1867, 307, 1310, 50742, 50742, 2171, 457, 1391, 406, 337, 1365, 1949, 14170, 586, 498, 321, 645, 1228, 51208, 51208, 14170, 281, 8873, 527, 13760, 293, 483, 264, 16235, 291, 393, 362, 257, 2316, 51480, 51480, 300, 311, 767, 4748, 1101, 291, 458, 309, 311, 4748, 411, 309, 311, 884, 257, 1101, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.08165576963713675, "compression_ratio": 1.6455026455026456, "no_speech_prob": 2.2125057512312196e-05}, {"id": 636, "seek": 359720, "start": 3604.7599999999998, "end": 3614.08, "text": " sometimes but probably not for example consider accuracy now if we were using", "tokens": [50364, 764, 300, 281, 3470, 527, 9834, 1830, 3097, 293, 264, 1867, 307, 1310, 50742, 50742, 2171, 457, 1391, 406, 337, 1365, 1949, 14170, 586, 498, 321, 645, 1228, 51208, 51208, 14170, 281, 8873, 527, 13760, 293, 483, 264, 16235, 291, 393, 362, 257, 2316, 51480, 51480, 300, 311, 767, 4748, 1101, 291, 458, 309, 311, 4748, 411, 309, 311, 884, 257, 1101, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.08165576963713675, "compression_ratio": 1.6455026455026456, "no_speech_prob": 2.2125057512312196e-05}, {"id": 637, "seek": 359720, "start": 3614.08, "end": 3619.52, "text": " accuracy to calculate our derivative and get the gradient you can have a model", "tokens": [50364, 764, 300, 281, 3470, 527, 9834, 1830, 3097, 293, 264, 1867, 307, 1310, 50742, 50742, 2171, 457, 1391, 406, 337, 1365, 1949, 14170, 586, 498, 321, 645, 1228, 51208, 51208, 14170, 281, 8873, 527, 13760, 293, 483, 264, 16235, 291, 393, 362, 257, 2316, 51480, 51480, 300, 311, 767, 4748, 1101, 291, 458, 309, 311, 4748, 411, 309, 311, 884, 257, 1101, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.08165576963713675, "compression_ratio": 1.6455026455026456, "no_speech_prob": 2.2125057512312196e-05}, {"id": 638, "seek": 359720, "start": 3619.52, "end": 3623.24, "text": " that's actually slightly better you know it's slightly like it's doing a better", "tokens": [50364, 764, 300, 281, 3470, 527, 9834, 1830, 3097, 293, 264, 1867, 307, 1310, 50742, 50742, 2171, 457, 1391, 406, 337, 1365, 1949, 14170, 586, 498, 321, 645, 1228, 51208, 51208, 14170, 281, 8873, 527, 13760, 293, 483, 264, 16235, 291, 393, 362, 257, 2316, 51480, 51480, 300, 311, 767, 4748, 1101, 291, 458, 309, 311, 4748, 411, 309, 311, 884, 257, 1101, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.08165576963713675, "compression_ratio": 1.6455026455026456, "no_speech_prob": 2.2125057512312196e-05}, {"id": 639, "seek": 362324, "start": 3623.24, "end": 3627.8799999999997, "text": " job of recognizing dogs and cats but not so much better that it's actually", "tokens": [50364, 1691, 295, 18538, 7197, 293, 11111, 457, 406, 370, 709, 1101, 300, 309, 311, 767, 50596, 50596, 7008, 604, 42892, 20627, 3857, 281, 1813, 257, 3000, 370, 264, 14170, 1177, 380, 50912, 50912, 1319, 412, 439, 370, 264, 16235, 4018, 291, 500, 380, 528, 1507, 411, 300, 291, 500, 380, 51222, 51222, 528, 49400, 6828, 570, 436, 500, 380, 362, 1481, 2771, 2448, 2049, 436, 500, 380, 51448, 51448, 362, 2771, 2448, 412, 439, 436, 434, 1936, 4018, 6217, 5315, 291, 528, 257, 51650, 51650], "temperature": 0.0, "avg_logprob": -0.11717839076601226, "compression_ratio": 1.7877358490566038, "no_speech_prob": 6.013588063069619e-05}, {"id": 640, "seek": 362324, "start": 3627.8799999999997, "end": 3634.2, "text": " caused any incorrectly classified cat to become a dog so the accuracy doesn't", "tokens": [50364, 1691, 295, 18538, 7197, 293, 11111, 457, 406, 370, 709, 1101, 300, 309, 311, 767, 50596, 50596, 7008, 604, 42892, 20627, 3857, 281, 1813, 257, 3000, 370, 264, 14170, 1177, 380, 50912, 50912, 1319, 412, 439, 370, 264, 16235, 4018, 291, 500, 380, 528, 1507, 411, 300, 291, 500, 380, 51222, 51222, 528, 49400, 6828, 570, 436, 500, 380, 362, 1481, 2771, 2448, 2049, 436, 500, 380, 51448, 51448, 362, 2771, 2448, 412, 439, 436, 434, 1936, 4018, 6217, 5315, 291, 528, 257, 51650, 51650], "temperature": 0.0, "avg_logprob": -0.11717839076601226, "compression_ratio": 1.7877358490566038, "no_speech_prob": 6.013588063069619e-05}, {"id": 641, "seek": 362324, "start": 3634.2, "end": 3640.3999999999996, "text": " change at all so the gradient zero you don't want stuff like that you don't", "tokens": [50364, 1691, 295, 18538, 7197, 293, 11111, 457, 406, 370, 709, 1101, 300, 309, 311, 767, 50596, 50596, 7008, 604, 42892, 20627, 3857, 281, 1813, 257, 3000, 370, 264, 14170, 1177, 380, 50912, 50912, 1319, 412, 439, 370, 264, 16235, 4018, 291, 500, 380, 528, 1507, 411, 300, 291, 500, 380, 51222, 51222, 528, 49400, 6828, 570, 436, 500, 380, 362, 1481, 2771, 2448, 2049, 436, 500, 380, 51448, 51448, 362, 2771, 2448, 412, 439, 436, 434, 1936, 4018, 6217, 5315, 291, 528, 257, 51650, 51650], "temperature": 0.0, "avg_logprob": -0.11717839076601226, "compression_ratio": 1.7877358490566038, "no_speech_prob": 6.013588063069619e-05}, {"id": 642, "seek": 362324, "start": 3640.3999999999996, "end": 3644.9199999999996, "text": " want bumpy functions because they don't have nice gradients often they don't", "tokens": [50364, 1691, 295, 18538, 7197, 293, 11111, 457, 406, 370, 709, 1101, 300, 309, 311, 767, 50596, 50596, 7008, 604, 42892, 20627, 3857, 281, 1813, 257, 3000, 370, 264, 14170, 1177, 380, 50912, 50912, 1319, 412, 439, 370, 264, 16235, 4018, 291, 500, 380, 528, 1507, 411, 300, 291, 500, 380, 51222, 51222, 528, 49400, 6828, 570, 436, 500, 380, 362, 1481, 2771, 2448, 2049, 436, 500, 380, 51448, 51448, 362, 2771, 2448, 412, 439, 436, 434, 1936, 4018, 6217, 5315, 291, 528, 257, 51650, 51650], "temperature": 0.0, "avg_logprob": -0.11717839076601226, "compression_ratio": 1.7877358490566038, "no_speech_prob": 6.013588063069619e-05}, {"id": 643, "seek": 362324, "start": 3644.9199999999996, "end": 3648.9599999999996, "text": " have gradients at all they're basically zero nearly everywhere you want a", "tokens": [50364, 1691, 295, 18538, 7197, 293, 11111, 457, 406, 370, 709, 1101, 300, 309, 311, 767, 50596, 50596, 7008, 604, 42892, 20627, 3857, 281, 1813, 257, 3000, 370, 264, 14170, 1177, 380, 50912, 50912, 1319, 412, 439, 370, 264, 16235, 4018, 291, 500, 380, 528, 1507, 411, 300, 291, 500, 380, 51222, 51222, 528, 49400, 6828, 570, 436, 500, 380, 362, 1481, 2771, 2448, 2049, 436, 500, 380, 51448, 51448, 362, 2771, 2448, 412, 439, 436, 434, 1936, 4018, 6217, 5315, 291, 528, 257, 51650, 51650], "temperature": 0.0, "avg_logprob": -0.11717839076601226, "compression_ratio": 1.7877358490566038, "no_speech_prob": 6.013588063069619e-05}, {"id": 644, "seek": 364896, "start": 3648.96, "end": 3655.08, "text": " function that's nice and smooth something like for instance the average", "tokens": [50364, 2445, 300, 311, 1481, 293, 5508, 746, 411, 337, 5197, 264, 4274, 50670, 50670, 8236, 6713, 914, 8236, 6713, 597, 321, 600, 1143, 949, 370, 300, 311, 264, 50951, 50951, 2649, 1296, 428, 16367, 293, 428, 4470, 586, 312, 5026, 558, 570, 562, 51154, 51154, 291, 434, 3097, 428, 2316, 6434, 439, 295, 1080, 565, 1382, 281, 3470, 264, 4470, 51324, 51324, 293, 881, 295, 264, 565, 300, 311, 406, 264, 912, 382, 264, 551, 291, 767, 1127, 466, 51544, 51544, 597, 307, 428, 20678, 370, 291, 658, 281, 1066, 729, 732, 819, 721, 294, 1575, 264, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.07931091761825108, "compression_ratio": 1.84, "no_speech_prob": 3.943964475183748e-05}, {"id": 645, "seek": 364896, "start": 3655.08, "end": 3660.7, "text": " absolute error mean absolute error which we've used before so that's the", "tokens": [50364, 2445, 300, 311, 1481, 293, 5508, 746, 411, 337, 5197, 264, 4274, 50670, 50670, 8236, 6713, 914, 8236, 6713, 597, 321, 600, 1143, 949, 370, 300, 311, 264, 50951, 50951, 2649, 1296, 428, 16367, 293, 428, 4470, 586, 312, 5026, 558, 570, 562, 51154, 51154, 291, 434, 3097, 428, 2316, 6434, 439, 295, 1080, 565, 1382, 281, 3470, 264, 4470, 51324, 51324, 293, 881, 295, 264, 565, 300, 311, 406, 264, 912, 382, 264, 551, 291, 767, 1127, 466, 51544, 51544, 597, 307, 428, 20678, 370, 291, 658, 281, 1066, 729, 732, 819, 721, 294, 1575, 264, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.07931091761825108, "compression_ratio": 1.84, "no_speech_prob": 3.943964475183748e-05}, {"id": 646, "seek": 364896, "start": 3660.7, "end": 3664.76, "text": " difference between your metrics and your loss now be careful right because when", "tokens": [50364, 2445, 300, 311, 1481, 293, 5508, 746, 411, 337, 5197, 264, 4274, 50670, 50670, 8236, 6713, 914, 8236, 6713, 597, 321, 600, 1143, 949, 370, 300, 311, 264, 50951, 50951, 2649, 1296, 428, 16367, 293, 428, 4470, 586, 312, 5026, 558, 570, 562, 51154, 51154, 291, 434, 3097, 428, 2316, 6434, 439, 295, 1080, 565, 1382, 281, 3470, 264, 4470, 51324, 51324, 293, 881, 295, 264, 565, 300, 311, 406, 264, 912, 382, 264, 551, 291, 767, 1127, 466, 51544, 51544, 597, 307, 428, 20678, 370, 291, 658, 281, 1066, 729, 732, 819, 721, 294, 1575, 264, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.07931091761825108, "compression_ratio": 1.84, "no_speech_prob": 3.943964475183748e-05}, {"id": 647, "seek": 364896, "start": 3664.76, "end": 3668.16, "text": " you're training your model spending all of its time trying to improve the loss", "tokens": [50364, 2445, 300, 311, 1481, 293, 5508, 746, 411, 337, 5197, 264, 4274, 50670, 50670, 8236, 6713, 914, 8236, 6713, 597, 321, 600, 1143, 949, 370, 300, 311, 264, 50951, 50951, 2649, 1296, 428, 16367, 293, 428, 4470, 586, 312, 5026, 558, 570, 562, 51154, 51154, 291, 434, 3097, 428, 2316, 6434, 439, 295, 1080, 565, 1382, 281, 3470, 264, 4470, 51324, 51324, 293, 881, 295, 264, 565, 300, 311, 406, 264, 912, 382, 264, 551, 291, 767, 1127, 466, 51544, 51544, 597, 307, 428, 20678, 370, 291, 658, 281, 1066, 729, 732, 819, 721, 294, 1575, 264, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.07931091761825108, "compression_ratio": 1.84, "no_speech_prob": 3.943964475183748e-05}, {"id": 648, "seek": 364896, "start": 3668.16, "end": 3672.56, "text": " and most of the time that's not the same as the thing you actually care about", "tokens": [50364, 2445, 300, 311, 1481, 293, 5508, 746, 411, 337, 5197, 264, 4274, 50670, 50670, 8236, 6713, 914, 8236, 6713, 597, 321, 600, 1143, 949, 370, 300, 311, 264, 50951, 50951, 2649, 1296, 428, 16367, 293, 428, 4470, 586, 312, 5026, 558, 570, 562, 51154, 51154, 291, 434, 3097, 428, 2316, 6434, 439, 295, 1080, 565, 1382, 281, 3470, 264, 4470, 51324, 51324, 293, 881, 295, 264, 565, 300, 311, 406, 264, 912, 382, 264, 551, 291, 767, 1127, 466, 51544, 51544, 597, 307, 428, 20678, 370, 291, 658, 281, 1066, 729, 732, 819, 721, 294, 1575, 264, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.07931091761825108, "compression_ratio": 1.84, "no_speech_prob": 3.943964475183748e-05}, {"id": 649, "seek": 364896, "start": 3672.56, "end": 3677.52, "text": " which is your metric so you got to keep those two different things in mind the", "tokens": [50364, 2445, 300, 311, 1481, 293, 5508, 746, 411, 337, 5197, 264, 4274, 50670, 50670, 8236, 6713, 914, 8236, 6713, 597, 321, 600, 1143, 949, 370, 300, 311, 264, 50951, 50951, 2649, 1296, 428, 16367, 293, 428, 4470, 586, 312, 5026, 558, 570, 562, 51154, 51154, 291, 434, 3097, 428, 2316, 6434, 439, 295, 1080, 565, 1382, 281, 3470, 264, 4470, 51324, 51324, 293, 881, 295, 264, 565, 300, 311, 406, 264, 912, 382, 264, 551, 291, 767, 1127, 466, 51544, 51544, 597, 307, 428, 20678, 370, 291, 658, 281, 1066, 729, 732, 819, 721, 294, 1575, 264, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.07931091761825108, "compression_ratio": 1.84, "no_speech_prob": 3.943964475183748e-05}, {"id": 650, "seek": 367752, "start": 3677.52, "end": 3685.88, "text": " other thing to keep in mind is that in real life you can't go to a website and", "tokens": [50364, 661, 551, 281, 1066, 294, 1575, 307, 300, 294, 957, 993, 291, 393, 380, 352, 281, 257, 3144, 293, 50782, 50782, 312, 1907, 437, 16367, 764, 294, 957, 993, 264, 264, 264, 2316, 300, 291, 2826, 456, 51192, 51192, 1943, 380, 472, 1230, 300, 5112, 291, 1968, 309, 311, 665, 420, 1578, 293, 754, 498, 456, 390, 51404, 51404, 291, 2759, 380, 312, 1075, 281, 915, 309, 484, 2286, 295, 565, 294, 957, 993, 264, 2316, 291, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.11281452649905357, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00010888377437368035}, {"id": 651, "seek": 367752, "start": 3685.88, "end": 3694.08, "text": " be told what metrics use in real life the the the model that you choose there", "tokens": [50364, 661, 551, 281, 1066, 294, 1575, 307, 300, 294, 957, 993, 291, 393, 380, 352, 281, 257, 3144, 293, 50782, 50782, 312, 1907, 437, 16367, 764, 294, 957, 993, 264, 264, 264, 2316, 300, 291, 2826, 456, 51192, 51192, 1943, 380, 472, 1230, 300, 5112, 291, 1968, 309, 311, 665, 420, 1578, 293, 754, 498, 456, 390, 51404, 51404, 291, 2759, 380, 312, 1075, 281, 915, 309, 484, 2286, 295, 565, 294, 957, 993, 264, 2316, 291, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.11281452649905357, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00010888377437368035}, {"id": 652, "seek": 367752, "start": 3694.08, "end": 3698.32, "text": " isn't one number that tells you whether it's good or bad and even if there was", "tokens": [50364, 661, 551, 281, 1066, 294, 1575, 307, 300, 294, 957, 993, 291, 393, 380, 352, 281, 257, 3144, 293, 50782, 50782, 312, 1907, 437, 16367, 764, 294, 957, 993, 264, 264, 264, 2316, 300, 291, 2826, 456, 51192, 51192, 1943, 380, 472, 1230, 300, 5112, 291, 1968, 309, 311, 665, 420, 1578, 293, 754, 498, 456, 390, 51404, 51404, 291, 2759, 380, 312, 1075, 281, 915, 309, 484, 2286, 295, 565, 294, 957, 993, 264, 2316, 291, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.11281452649905357, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00010888377437368035}, {"id": 653, "seek": 367752, "start": 3698.32, "end": 3703.64, "text": " you wouldn't be able to find it out ahead of time in real life the model you", "tokens": [50364, 661, 551, 281, 1066, 294, 1575, 307, 300, 294, 957, 993, 291, 393, 380, 352, 281, 257, 3144, 293, 50782, 50782, 312, 1907, 437, 16367, 764, 294, 957, 993, 264, 264, 264, 2316, 300, 291, 2826, 456, 51192, 51192, 1943, 380, 472, 1230, 300, 5112, 291, 1968, 309, 311, 665, 420, 1578, 293, 754, 498, 456, 390, 51404, 51404, 291, 2759, 380, 312, 1075, 281, 915, 309, 484, 2286, 295, 565, 294, 957, 993, 264, 2316, 291, 51670, 51670], "temperature": 0.0, "avg_logprob": -0.11281452649905357, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00010888377437368035}, {"id": 654, "seek": 370364, "start": 3703.64, "end": 3711.44, "text": " use is a part of a complex process often involving humans both as users or", "tokens": [50364, 764, 307, 257, 644, 295, 257, 3997, 1399, 2049, 17030, 6255, 1293, 382, 5022, 420, 50754, 50754, 4581, 293, 382, 561, 291, 458, 3288, 294, 294, 382, 644, 295, 264, 1399, 51070, 51070, 456, 311, 439, 3685, 295, 721, 300, 366, 4473, 670, 565, 293, 456, 311, 3195, 293, 51300, 51300, 3195, 295, 10070, 295, 5327, 300, 366, 1027, 472, 20678, 307, 406, 1547, 281, 7983, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0785191604069301, "compression_ratio": 1.687150837988827, "no_speech_prob": 2.3550337573396973e-05}, {"id": 655, "seek": 370364, "start": 3711.44, "end": 3717.7599999999998, "text": " customers and as people you know involved in in as part of the process", "tokens": [50364, 764, 307, 257, 644, 295, 257, 3997, 1399, 2049, 17030, 6255, 1293, 382, 5022, 420, 50754, 50754, 4581, 293, 382, 561, 291, 458, 3288, 294, 294, 382, 644, 295, 264, 1399, 51070, 51070, 456, 311, 439, 3685, 295, 721, 300, 366, 4473, 670, 565, 293, 456, 311, 3195, 293, 51300, 51300, 3195, 295, 10070, 295, 5327, 300, 366, 1027, 472, 20678, 307, 406, 1547, 281, 7983, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0785191604069301, "compression_ratio": 1.687150837988827, "no_speech_prob": 2.3550337573396973e-05}, {"id": 656, "seek": 370364, "start": 3717.7599999999998, "end": 3722.3599999999997, "text": " there's all kinds of things that are changing over time and there's lots and", "tokens": [50364, 764, 307, 257, 644, 295, 257, 3997, 1399, 2049, 17030, 6255, 1293, 382, 5022, 420, 50754, 50754, 4581, 293, 382, 561, 291, 458, 3288, 294, 294, 382, 644, 295, 264, 1399, 51070, 51070, 456, 311, 439, 3685, 295, 721, 300, 366, 4473, 670, 565, 293, 456, 311, 3195, 293, 51300, 51300, 3195, 295, 10070, 295, 5327, 300, 366, 1027, 472, 20678, 307, 406, 1547, 281, 7983, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0785191604069301, "compression_ratio": 1.687150837988827, "no_speech_prob": 2.3550337573396973e-05}, {"id": 657, "seek": 370364, "start": 3722.3599999999997, "end": 3729.64, "text": " lots of outcomes of decisions that are made one metric is not enough to capture", "tokens": [50364, 764, 307, 257, 644, 295, 257, 3997, 1399, 2049, 17030, 6255, 1293, 382, 5022, 420, 50754, 50754, 4581, 293, 382, 561, 291, 458, 3288, 294, 294, 382, 644, 295, 264, 1399, 51070, 51070, 456, 311, 439, 3685, 295, 721, 300, 366, 4473, 670, 565, 293, 456, 311, 3195, 293, 51300, 51300, 3195, 295, 10070, 295, 5327, 300, 366, 1027, 472, 20678, 307, 406, 1547, 281, 7983, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.0785191604069301, "compression_ratio": 1.687150837988827, "no_speech_prob": 2.3550337573396973e-05}, {"id": 658, "seek": 372964, "start": 3729.64, "end": 3739.7999999999997, "text": " all of that unfortunately because it's so convenient to pick one metric and use", "tokens": [50364, 439, 295, 300, 7015, 570, 309, 311, 370, 10851, 281, 1888, 472, 20678, 293, 764, 50872, 50872, 300, 281, 584, 286, 600, 658, 257, 665, 2316, 300, 588, 2049, 10704, 1080, 636, 666, 666, 51164, 51164, 3518, 666, 2463, 689, 561, 3373, 484, 613, 721, 300, 366, 665, 322, 51476, 51476, 264, 472, 20678, 300, 2011, 281, 312, 1858, 281, 3481, 293, 797, 293, 797, 321, 1352, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.08406079654962244, "compression_ratio": 1.6756756756756757, "no_speech_prob": 3.0715502361999825e-05}, {"id": 659, "seek": 372964, "start": 3739.7999999999997, "end": 3745.64, "text": " that to say I've got a good model that very often finds its way into into", "tokens": [50364, 439, 295, 300, 7015, 570, 309, 311, 370, 10851, 281, 1888, 472, 20678, 293, 764, 50872, 50872, 300, 281, 584, 286, 600, 658, 257, 665, 2316, 300, 588, 2049, 10704, 1080, 636, 666, 666, 51164, 51164, 3518, 666, 2463, 689, 561, 3373, 484, 613, 721, 300, 366, 665, 322, 51476, 51476, 264, 472, 20678, 300, 2011, 281, 312, 1858, 281, 3481, 293, 797, 293, 797, 321, 1352, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.08406079654962244, "compression_ratio": 1.6756756756756757, "no_speech_prob": 3.0715502361999825e-05}, {"id": 660, "seek": 372964, "start": 3745.64, "end": 3751.8799999999997, "text": " industry into government where people roll out these things that are good on", "tokens": [50364, 439, 295, 300, 7015, 570, 309, 311, 370, 10851, 281, 1888, 472, 20678, 293, 764, 50872, 50872, 300, 281, 584, 286, 600, 658, 257, 665, 2316, 300, 588, 2049, 10704, 1080, 636, 666, 666, 51164, 51164, 3518, 666, 2463, 689, 561, 3373, 484, 613, 721, 300, 366, 665, 322, 51476, 51476, 264, 472, 20678, 300, 2011, 281, 312, 1858, 281, 3481, 293, 797, 293, 797, 321, 1352, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.08406079654962244, "compression_ratio": 1.6756756756756757, "no_speech_prob": 3.0715502361999825e-05}, {"id": 661, "seek": 372964, "start": 3751.8799999999997, "end": 3758.7999999999997, "text": " the one metric that happened to be easy to measure and again and again we found", "tokens": [50364, 439, 295, 300, 7015, 570, 309, 311, 370, 10851, 281, 1888, 472, 20678, 293, 764, 50872, 50872, 300, 281, 584, 286, 600, 658, 257, 665, 2316, 300, 588, 2049, 10704, 1080, 636, 666, 666, 51164, 51164, 3518, 666, 2463, 689, 561, 3373, 484, 613, 721, 300, 366, 665, 322, 51476, 51476, 264, 472, 20678, 300, 2011, 281, 312, 1858, 281, 3481, 293, 797, 293, 797, 321, 1352, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.08406079654962244, "compression_ratio": 1.6756756756756757, "no_speech_prob": 3.0715502361999825e-05}, {"id": 662, "seek": 375880, "start": 3758.8, "end": 3765.0800000000004, "text": " people's lives turned upside down because of how badly they get screwed up", "tokens": [50364, 561, 311, 2909, 3574, 14119, 760, 570, 295, 577, 13425, 436, 483, 20331, 493, 50678, 50678, 538, 5245, 300, 362, 668, 42892, 12690, 1228, 257, 2167, 20678, 370, 452, 50970, 50970, 4975, 14246, 8500, 575, 3720, 341, 7222, 597, 286, 2748, 291, 1401, 466, 51136, 51136, 264, 1154, 365, 16367, 307, 257, 955, 1154, 337, 7318, 309, 311, 406, 445, 364, 7318, 551, 51694, 51694, 456, 311, 767, 341, 551, 1219, 665, 8609, 2101, 300, 4368, 562, 286, 3481, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.12305174678205008, "compression_ratio": 1.59915611814346, "no_speech_prob": 4.6833058149786666e-05}, {"id": 663, "seek": 375880, "start": 3765.0800000000004, "end": 3770.92, "text": " by models that have been incorrectly measured using a single metric so my", "tokens": [50364, 561, 311, 2909, 3574, 14119, 760, 570, 295, 577, 13425, 436, 483, 20331, 493, 50678, 50678, 538, 5245, 300, 362, 668, 42892, 12690, 1228, 257, 2167, 20678, 370, 452, 50970, 50970, 4975, 14246, 8500, 575, 3720, 341, 7222, 597, 286, 2748, 291, 1401, 466, 51136, 51136, 264, 1154, 365, 16367, 307, 257, 955, 1154, 337, 7318, 309, 311, 406, 445, 364, 7318, 551, 51694, 51694, 456, 311, 767, 341, 551, 1219, 665, 8609, 2101, 300, 4368, 562, 286, 3481, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.12305174678205008, "compression_ratio": 1.59915611814346, "no_speech_prob": 4.6833058149786666e-05}, {"id": 664, "seek": 375880, "start": 3770.92, "end": 3774.2400000000002, "text": " partner Rachel Thomas has written this article which I recommend you read about", "tokens": [50364, 561, 311, 2909, 3574, 14119, 760, 570, 295, 577, 13425, 436, 483, 20331, 493, 50678, 50678, 538, 5245, 300, 362, 668, 42892, 12690, 1228, 257, 2167, 20678, 370, 452, 50970, 50970, 4975, 14246, 8500, 575, 3720, 341, 7222, 597, 286, 2748, 291, 1401, 466, 51136, 51136, 264, 1154, 365, 16367, 307, 257, 955, 1154, 337, 7318, 309, 311, 406, 445, 364, 7318, 551, 51694, 51694, 456, 311, 767, 341, 551, 1219, 665, 8609, 2101, 300, 4368, 562, 286, 3481, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.12305174678205008, "compression_ratio": 1.59915611814346, "no_speech_prob": 4.6833058149786666e-05}, {"id": 665, "seek": 375880, "start": 3774.2400000000002, "end": 3785.4, "text": " the problem with metrics is a big problem for AI it's not just an AI thing", "tokens": [50364, 561, 311, 2909, 3574, 14119, 760, 570, 295, 577, 13425, 436, 483, 20331, 493, 50678, 50678, 538, 5245, 300, 362, 668, 42892, 12690, 1228, 257, 2167, 20678, 370, 452, 50970, 50970, 4975, 14246, 8500, 575, 3720, 341, 7222, 597, 286, 2748, 291, 1401, 466, 51136, 51136, 264, 1154, 365, 16367, 307, 257, 955, 1154, 337, 7318, 309, 311, 406, 445, 364, 7318, 551, 51694, 51694, 456, 311, 767, 341, 551, 1219, 665, 8609, 2101, 300, 4368, 562, 286, 3481, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.12305174678205008, "compression_ratio": 1.59915611814346, "no_speech_prob": 4.6833058149786666e-05}, {"id": 666, "seek": 375880, "start": 3785.4, "end": 3788.52, "text": " there's actually this thing called good arts law that states when I measure", "tokens": [50364, 561, 311, 2909, 3574, 14119, 760, 570, 295, 577, 13425, 436, 483, 20331, 493, 50678, 50678, 538, 5245, 300, 362, 668, 42892, 12690, 1228, 257, 2167, 20678, 370, 452, 50970, 50970, 4975, 14246, 8500, 575, 3720, 341, 7222, 597, 286, 2748, 291, 1401, 466, 51136, 51136, 264, 1154, 365, 16367, 307, 257, 955, 1154, 337, 7318, 309, 311, 406, 445, 364, 7318, 551, 51694, 51694, 456, 311, 767, 341, 551, 1219, 665, 8609, 2101, 300, 4368, 562, 286, 3481, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.12305174678205008, "compression_ratio": 1.59915611814346, "no_speech_prob": 4.6833058149786666e-05}, {"id": 667, "seek": 378852, "start": 3788.52, "end": 3795.24, "text": " becomes a target it ceases to be a good measure the thing is so when I was a", "tokens": [50364, 3643, 257, 3779, 309, 1769, 1957, 281, 312, 257, 665, 3481, 264, 551, 307, 370, 562, 286, 390, 257, 50700, 50700, 4592, 24676, 291, 458, 945, 924, 2057, 321, 645, 1009, 733, 295, 644, 295, 613, 51086, 51086, 10924, 721, 1382, 281, 411, 915, 2141, 3389, 22176, 293, 2098, 281, 733, 51344, 51344, 295, 291, 458, 992, 9221, 6846, 337, 5763, 21123, 293, 321, 645, 534, 884, 257, 51520, 51520, 688, 295, 341, 411, 1507, 597, 307, 1936, 466, 8867, 16367, 293, 291, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.08817697393483129, "compression_ratio": 1.6782608695652175, "no_speech_prob": 9.457347186980769e-05}, {"id": 668, "seek": 378852, "start": 3795.24, "end": 3802.96, "text": " management consultant you know 20 years ago we were always kind of part of these", "tokens": [50364, 3643, 257, 3779, 309, 1769, 1957, 281, 312, 257, 665, 3481, 264, 551, 307, 370, 562, 286, 390, 257, 50700, 50700, 4592, 24676, 291, 458, 945, 924, 2057, 321, 645, 1009, 733, 295, 644, 295, 613, 51086, 51086, 10924, 721, 1382, 281, 411, 915, 2141, 3389, 22176, 293, 2098, 281, 733, 51344, 51344, 295, 291, 458, 992, 9221, 6846, 337, 5763, 21123, 293, 321, 645, 534, 884, 257, 51520, 51520, 688, 295, 341, 411, 1507, 597, 307, 1936, 466, 8867, 16367, 293, 291, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.08817697393483129, "compression_ratio": 1.6782608695652175, "no_speech_prob": 9.457347186980769e-05}, {"id": 669, "seek": 378852, "start": 3802.96, "end": 3808.12, "text": " strategic things trying to like find key performance indicators and ways to kind", "tokens": [50364, 3643, 257, 3779, 309, 1769, 1957, 281, 312, 257, 665, 3481, 264, 551, 307, 370, 562, 286, 390, 257, 50700, 50700, 4592, 24676, 291, 458, 945, 924, 2057, 321, 645, 1009, 733, 295, 644, 295, 613, 51086, 51086, 10924, 721, 1382, 281, 411, 915, 2141, 3389, 22176, 293, 2098, 281, 733, 51344, 51344, 295, 291, 458, 992, 9221, 6846, 337, 5763, 21123, 293, 321, 645, 534, 884, 257, 51520, 51520, 688, 295, 341, 411, 1507, 597, 307, 1936, 466, 8867, 16367, 293, 291, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.08817697393483129, "compression_ratio": 1.6782608695652175, "no_speech_prob": 9.457347186980769e-05}, {"id": 670, "seek": 378852, "start": 3808.12, "end": 3811.64, "text": " of you know set commission rates for salespeople and we were really doing a", "tokens": [50364, 3643, 257, 3779, 309, 1769, 1957, 281, 312, 257, 665, 3481, 264, 551, 307, 370, 562, 286, 390, 257, 50700, 50700, 4592, 24676, 291, 458, 945, 924, 2057, 321, 645, 1009, 733, 295, 644, 295, 613, 51086, 51086, 10924, 721, 1382, 281, 411, 915, 2141, 3389, 22176, 293, 2098, 281, 733, 51344, 51344, 295, 291, 458, 992, 9221, 6846, 337, 5763, 21123, 293, 321, 645, 534, 884, 257, 51520, 51520, 688, 295, 341, 411, 1507, 597, 307, 1936, 466, 8867, 16367, 293, 291, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.08817697393483129, "compression_ratio": 1.6782608695652175, "no_speech_prob": 9.457347186980769e-05}, {"id": 671, "seek": 378852, "start": 3811.64, "end": 3816.16, "text": " lot of this like stuff which is basically about picking metrics and you", "tokens": [50364, 3643, 257, 3779, 309, 1769, 1957, 281, 312, 257, 665, 3481, 264, 551, 307, 370, 562, 286, 390, 257, 50700, 50700, 4592, 24676, 291, 458, 945, 924, 2057, 321, 645, 1009, 733, 295, 644, 295, 613, 51086, 51086, 10924, 721, 1382, 281, 411, 915, 2141, 3389, 22176, 293, 2098, 281, 733, 51344, 51344, 295, 291, 458, 992, 9221, 6846, 337, 5763, 21123, 293, 321, 645, 534, 884, 257, 51520, 51520, 688, 295, 341, 411, 1507, 597, 307, 1936, 466, 8867, 16367, 293, 291, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.08817697393483129, "compression_ratio": 1.6782608695652175, "no_speech_prob": 9.457347186980769e-05}, {"id": 672, "seek": 381616, "start": 3816.16, "end": 3823.3999999999996, "text": " know we see that happen go wrong in industry all the time AI is dramatically", "tokens": [50364, 458, 321, 536, 300, 1051, 352, 2085, 294, 3518, 439, 264, 565, 7318, 307, 17548, 50726, 50726, 5324, 570, 7318, 307, 370, 665, 412, 40425, 16367, 293, 370, 300, 311, 983, 291, 50954, 50954, 362, 281, 312, 2857, 2857, 2857, 5026, 466, 16367, 562, 291, 366, 1382, 281, 764, 51220, 51220, 257, 2316, 294, 957, 993, 4033, 382, 286, 848, 294, 48751, 22631, 321, 500, 380, 362, 281, 3292, 466, 604, 51464, 51464, 295, 300, 321, 434, 445, 516, 281, 764, 264, 39041, 20009, 17619, 597, 307, 51688, 51688, 439, 588, 731, 382, 938, 382, 291, 458, 437, 264, 4921, 264, 39041, 20009, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09695418527193159, "compression_ratio": 1.7538461538461538, "no_speech_prob": 1.805721149139572e-05}, {"id": 673, "seek": 381616, "start": 3823.3999999999996, "end": 3827.96, "text": " worse because AI is so good at optimizing metrics and so that's why you", "tokens": [50364, 458, 321, 536, 300, 1051, 352, 2085, 294, 3518, 439, 264, 565, 7318, 307, 17548, 50726, 50726, 5324, 570, 7318, 307, 370, 665, 412, 40425, 16367, 293, 370, 300, 311, 983, 291, 50954, 50954, 362, 281, 312, 2857, 2857, 2857, 5026, 466, 16367, 562, 291, 366, 1382, 281, 764, 51220, 51220, 257, 2316, 294, 957, 993, 4033, 382, 286, 848, 294, 48751, 22631, 321, 500, 380, 362, 281, 3292, 466, 604, 51464, 51464, 295, 300, 321, 434, 445, 516, 281, 764, 264, 39041, 20009, 17619, 597, 307, 51688, 51688, 439, 588, 731, 382, 938, 382, 291, 458, 437, 264, 4921, 264, 39041, 20009, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09695418527193159, "compression_ratio": 1.7538461538461538, "no_speech_prob": 1.805721149139572e-05}, {"id": 674, "seek": 381616, "start": 3827.96, "end": 3833.2799999999997, "text": " have to be extra extra extra careful about metrics when you are trying to use", "tokens": [50364, 458, 321, 536, 300, 1051, 352, 2085, 294, 3518, 439, 264, 565, 7318, 307, 17548, 50726, 50726, 5324, 570, 7318, 307, 370, 665, 412, 40425, 16367, 293, 370, 300, 311, 983, 291, 50954, 50954, 362, 281, 312, 2857, 2857, 2857, 5026, 466, 16367, 562, 291, 366, 1382, 281, 764, 51220, 51220, 257, 2316, 294, 957, 993, 4033, 382, 286, 848, 294, 48751, 22631, 321, 500, 380, 362, 281, 3292, 466, 604, 51464, 51464, 295, 300, 321, 434, 445, 516, 281, 764, 264, 39041, 20009, 17619, 597, 307, 51688, 51688, 439, 588, 731, 382, 938, 382, 291, 458, 437, 264, 4921, 264, 39041, 20009, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09695418527193159, "compression_ratio": 1.7538461538461538, "no_speech_prob": 1.805721149139572e-05}, {"id": 675, "seek": 381616, "start": 3833.2799999999997, "end": 3838.16, "text": " a model in real life anyway as I said in Kaggle we don't have to worry about any", "tokens": [50364, 458, 321, 536, 300, 1051, 352, 2085, 294, 3518, 439, 264, 565, 7318, 307, 17548, 50726, 50726, 5324, 570, 7318, 307, 370, 665, 412, 40425, 16367, 293, 370, 300, 311, 983, 291, 50954, 50954, 362, 281, 312, 2857, 2857, 2857, 5026, 466, 16367, 562, 291, 366, 1382, 281, 764, 51220, 51220, 257, 2316, 294, 957, 993, 4033, 382, 286, 848, 294, 48751, 22631, 321, 500, 380, 362, 281, 3292, 466, 604, 51464, 51464, 295, 300, 321, 434, 445, 516, 281, 764, 264, 39041, 20009, 17619, 597, 307, 51688, 51688, 439, 588, 731, 382, 938, 382, 291, 458, 437, 264, 4921, 264, 39041, 20009, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09695418527193159, "compression_ratio": 1.7538461538461538, "no_speech_prob": 1.805721149139572e-05}, {"id": 676, "seek": 381616, "start": 3838.16, "end": 3842.64, "text": " of that we're just going to use the Pearson correlation coefficient which is", "tokens": [50364, 458, 321, 536, 300, 1051, 352, 2085, 294, 3518, 439, 264, 565, 7318, 307, 17548, 50726, 50726, 5324, 570, 7318, 307, 370, 665, 412, 40425, 16367, 293, 370, 300, 311, 983, 291, 50954, 50954, 362, 281, 312, 2857, 2857, 2857, 5026, 466, 16367, 562, 291, 366, 1382, 281, 764, 51220, 51220, 257, 2316, 294, 957, 993, 4033, 382, 286, 848, 294, 48751, 22631, 321, 500, 380, 362, 281, 3292, 466, 604, 51464, 51464, 295, 300, 321, 434, 445, 516, 281, 764, 264, 39041, 20009, 17619, 597, 307, 51688, 51688, 439, 588, 731, 382, 938, 382, 291, 458, 437, 264, 4921, 264, 39041, 20009, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09695418527193159, "compression_ratio": 1.7538461538461538, "no_speech_prob": 1.805721149139572e-05}, {"id": 677, "seek": 381616, "start": 3842.64, "end": 3846.08, "text": " all very well as long as you know what the hell the Pearson correlation", "tokens": [50364, 458, 321, 536, 300, 1051, 352, 2085, 294, 3518, 439, 264, 565, 7318, 307, 17548, 50726, 50726, 5324, 570, 7318, 307, 370, 665, 412, 40425, 16367, 293, 370, 300, 311, 983, 291, 50954, 50954, 362, 281, 312, 2857, 2857, 2857, 5026, 466, 16367, 562, 291, 366, 1382, 281, 764, 51220, 51220, 257, 2316, 294, 957, 993, 4033, 382, 286, 848, 294, 48751, 22631, 321, 500, 380, 362, 281, 3292, 466, 604, 51464, 51464, 295, 300, 321, 434, 445, 516, 281, 764, 264, 39041, 20009, 17619, 597, 307, 51688, 51688, 439, 588, 731, 382, 938, 382, 291, 458, 437, 264, 4921, 264, 39041, 20009, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09695418527193159, "compression_ratio": 1.7538461538461538, "no_speech_prob": 1.805721149139572e-05}, {"id": 678, "seek": 384608, "start": 3846.08, "end": 3853.88, "text": " coefficient is if you don't let's learn about it so Pearson correlation", "tokens": [50364, 17619, 307, 498, 291, 500, 380, 718, 311, 1466, 466, 309, 370, 39041, 20009, 50754, 50754, 17619, 307, 2673, 35839, 770, 1228, 5063, 497, 293, 309, 311, 264, 881, 13371, 1143, 50996, 50996, 3481, 295, 577, 2531, 732, 9102, 366, 293, 370, 498, 428, 21264, 366, 588, 51292, 51292, 2531, 281, 264, 957, 4190, 550, 264, 39041, 20009, 17619, 486, 312, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.09857124915489783, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.591020868043415e-05}, {"id": 679, "seek": 384608, "start": 3853.88, "end": 3858.72, "text": " coefficient is usually abbreviated using letter R and it's the most widely used", "tokens": [50364, 17619, 307, 498, 291, 500, 380, 718, 311, 1466, 466, 309, 370, 39041, 20009, 50754, 50754, 17619, 307, 2673, 35839, 770, 1228, 5063, 497, 293, 309, 311, 264, 881, 13371, 1143, 50996, 50996, 3481, 295, 577, 2531, 732, 9102, 366, 293, 370, 498, 428, 21264, 366, 588, 51292, 51292, 2531, 281, 264, 957, 4190, 550, 264, 39041, 20009, 17619, 486, 312, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.09857124915489783, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.591020868043415e-05}, {"id": 680, "seek": 384608, "start": 3858.72, "end": 3864.64, "text": " measure of how similar two variables are and so if your predictions are very", "tokens": [50364, 17619, 307, 498, 291, 500, 380, 718, 311, 1466, 466, 309, 370, 39041, 20009, 50754, 50754, 17619, 307, 2673, 35839, 770, 1228, 5063, 497, 293, 309, 311, 264, 881, 13371, 1143, 50996, 50996, 3481, 295, 577, 2531, 732, 9102, 366, 293, 370, 498, 428, 21264, 366, 588, 51292, 51292, 2531, 281, 264, 957, 4190, 550, 264, 39041, 20009, 17619, 486, 312, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.09857124915489783, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.591020868043415e-05}, {"id": 681, "seek": 384608, "start": 3864.64, "end": 3872.3199999999997, "text": " similar to the real values then the Pearson correlation coefficient will be", "tokens": [50364, 17619, 307, 498, 291, 500, 380, 718, 311, 1466, 466, 309, 370, 39041, 20009, 50754, 50754, 17619, 307, 2673, 35839, 770, 1228, 5063, 497, 293, 309, 311, 264, 881, 13371, 1143, 50996, 50996, 3481, 295, 577, 2531, 732, 9102, 366, 293, 370, 498, 428, 21264, 366, 588, 51292, 51292, 2531, 281, 264, 957, 4190, 550, 264, 39041, 20009, 17619, 486, 312, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.09857124915489783, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.591020868043415e-05}, {"id": 682, "seek": 387232, "start": 3872.32, "end": 3881.0, "text": " high and that's what you want R can be between minus one and one minus one", "tokens": [50364, 1090, 293, 300, 311, 437, 291, 528, 497, 393, 312, 1296, 3175, 472, 293, 472, 3175, 472, 50798, 50798, 1355, 291, 19147, 2293, 264, 2085, 1867, 597, 294, 257, 48751, 22631, 6211, 312, 51018, 51018, 869, 570, 550, 291, 393, 445, 9943, 439, 295, 428, 6338, 293, 291, 603, 312, 51174, 51174, 2176, 1804, 472, 1355, 291, 658, 1203, 2293, 3006, 5101, 51534, 51534, 4124, 294, 7712, 420, 33587, 562, 436, 2924, 291, 466, 264, 39041, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.10518221855163574, "compression_ratio": 1.6590909090909092, "no_speech_prob": 6.401272548828274e-05}, {"id": 683, "seek": 387232, "start": 3881.0, "end": 3885.4, "text": " means you predicted exactly the wrong answer which in a Kaggle competition be", "tokens": [50364, 1090, 293, 300, 311, 437, 291, 528, 497, 393, 312, 1296, 3175, 472, 293, 472, 3175, 472, 50798, 50798, 1355, 291, 19147, 2293, 264, 2085, 1867, 597, 294, 257, 48751, 22631, 6211, 312, 51018, 51018, 869, 570, 550, 291, 393, 445, 9943, 439, 295, 428, 6338, 293, 291, 603, 312, 51174, 51174, 2176, 1804, 472, 1355, 291, 658, 1203, 2293, 3006, 5101, 51534, 51534, 4124, 294, 7712, 420, 33587, 562, 436, 2924, 291, 466, 264, 39041, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.10518221855163574, "compression_ratio": 1.6590909090909092, "no_speech_prob": 6.401272548828274e-05}, {"id": 684, "seek": 387232, "start": 3885.4, "end": 3888.52, "text": " great because then you can just reverse all of your answers and you'll be", "tokens": [50364, 1090, 293, 300, 311, 437, 291, 528, 497, 393, 312, 1296, 3175, 472, 293, 472, 3175, 472, 50798, 50798, 1355, 291, 19147, 2293, 264, 2085, 1867, 597, 294, 257, 48751, 22631, 6211, 312, 51018, 51018, 869, 570, 550, 291, 393, 445, 9943, 439, 295, 428, 6338, 293, 291, 603, 312, 51174, 51174, 2176, 1804, 472, 1355, 291, 658, 1203, 2293, 3006, 5101, 51534, 51534, 4124, 294, 7712, 420, 33587, 562, 436, 2924, 291, 466, 264, 39041, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.10518221855163574, "compression_ratio": 1.6590909090909092, "no_speech_prob": 6.401272548828274e-05}, {"id": 685, "seek": 387232, "start": 3888.52, "end": 3895.7200000000003, "text": " perfect plus one means you got everything exactly correct generally", "tokens": [50364, 1090, 293, 300, 311, 437, 291, 528, 497, 393, 312, 1296, 3175, 472, 293, 472, 3175, 472, 50798, 50798, 1355, 291, 19147, 2293, 264, 2085, 1867, 597, 294, 257, 48751, 22631, 6211, 312, 51018, 51018, 869, 570, 550, 291, 393, 445, 9943, 439, 295, 428, 6338, 293, 291, 603, 312, 51174, 51174, 2176, 1804, 472, 1355, 291, 658, 1203, 2293, 3006, 5101, 51534, 51534, 4124, 294, 7712, 420, 33587, 562, 436, 2924, 291, 466, 264, 39041, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.10518221855163574, "compression_ratio": 1.6590909090909092, "no_speech_prob": 6.401272548828274e-05}, {"id": 686, "seek": 387232, "start": 3895.7200000000003, "end": 3899.0800000000004, "text": " speaking in courses or textbooks when they teach you about the Pearson", "tokens": [50364, 1090, 293, 300, 311, 437, 291, 528, 497, 393, 312, 1296, 3175, 472, 293, 472, 3175, 472, 50798, 50798, 1355, 291, 19147, 2293, 264, 2085, 1867, 597, 294, 257, 48751, 22631, 6211, 312, 51018, 51018, 869, 570, 550, 291, 393, 445, 9943, 439, 295, 428, 6338, 293, 291, 603, 312, 51174, 51174, 2176, 1804, 472, 1355, 291, 658, 1203, 2293, 3006, 5101, 51534, 51534, 4124, 294, 7712, 420, 33587, 562, 436, 2924, 291, 466, 264, 39041, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.10518221855163574, "compression_ratio": 1.6590909090909092, "no_speech_prob": 6.401272548828274e-05}, {"id": 687, "seek": 389908, "start": 3899.08, "end": 3902.2, "text": " correlation coefficient at that point at this point they will show you a", "tokens": [50364, 20009, 17619, 412, 300, 935, 412, 341, 935, 436, 486, 855, 291, 257, 50520, 50520, 18894, 2445, 286, 478, 406, 516, 281, 360, 300, 570, 300, 5112, 291, 1825, 50730, 50730, 466, 264, 39041, 20009, 17619, 437, 321, 767, 1127, 466, 50894, 50894, 307, 406, 264, 18894, 2445, 457, 577, 309, 36896, 293, 286, 915, 881, 561, 754, 51252, 51252, 567, 589, 294, 1412, 3497, 362, 406, 767, 2956, 412, 257, 3840, 295, 1412, 6352, 51422, 51422, 281, 1223, 577, 497, 36896, 370, 718, 311, 360, 300, 558, 586, 370, 300, 291, 434, 406, 472, 295, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.09509395599365235, "compression_ratio": 1.887966804979253, "no_speech_prob": 6.707318971166387e-05}, {"id": 688, "seek": 389908, "start": 3902.2, "end": 3906.4, "text": " mathematical function I'm not going to do that because that tells you nothing", "tokens": [50364, 20009, 17619, 412, 300, 935, 412, 341, 935, 436, 486, 855, 291, 257, 50520, 50520, 18894, 2445, 286, 478, 406, 516, 281, 360, 300, 570, 300, 5112, 291, 1825, 50730, 50730, 466, 264, 39041, 20009, 17619, 437, 321, 767, 1127, 466, 50894, 50894, 307, 406, 264, 18894, 2445, 457, 577, 309, 36896, 293, 286, 915, 881, 561, 754, 51252, 51252, 567, 589, 294, 1412, 3497, 362, 406, 767, 2956, 412, 257, 3840, 295, 1412, 6352, 51422, 51422, 281, 1223, 577, 497, 36896, 370, 718, 311, 360, 300, 558, 586, 370, 300, 291, 434, 406, 472, 295, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.09509395599365235, "compression_ratio": 1.887966804979253, "no_speech_prob": 6.707318971166387e-05}, {"id": 689, "seek": 389908, "start": 3906.4, "end": 3909.68, "text": " about the Pearson correlation coefficient what we actually care about", "tokens": [50364, 20009, 17619, 412, 300, 935, 412, 341, 935, 436, 486, 855, 291, 257, 50520, 50520, 18894, 2445, 286, 478, 406, 516, 281, 360, 300, 570, 300, 5112, 291, 1825, 50730, 50730, 466, 264, 39041, 20009, 17619, 437, 321, 767, 1127, 466, 50894, 50894, 307, 406, 264, 18894, 2445, 457, 577, 309, 36896, 293, 286, 915, 881, 561, 754, 51252, 51252, 567, 589, 294, 1412, 3497, 362, 406, 767, 2956, 412, 257, 3840, 295, 1412, 6352, 51422, 51422, 281, 1223, 577, 497, 36896, 370, 718, 311, 360, 300, 558, 586, 370, 300, 291, 434, 406, 472, 295, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.09509395599365235, "compression_ratio": 1.887966804979253, "no_speech_prob": 6.707318971166387e-05}, {"id": 690, "seek": 389908, "start": 3909.68, "end": 3916.84, "text": " is not the mathematical function but how it behaves and I find most people even", "tokens": [50364, 20009, 17619, 412, 300, 935, 412, 341, 935, 436, 486, 855, 291, 257, 50520, 50520, 18894, 2445, 286, 478, 406, 516, 281, 360, 300, 570, 300, 5112, 291, 1825, 50730, 50730, 466, 264, 39041, 20009, 17619, 437, 321, 767, 1127, 466, 50894, 50894, 307, 406, 264, 18894, 2445, 457, 577, 309, 36896, 293, 286, 915, 881, 561, 754, 51252, 51252, 567, 589, 294, 1412, 3497, 362, 406, 767, 2956, 412, 257, 3840, 295, 1412, 6352, 51422, 51422, 281, 1223, 577, 497, 36896, 370, 718, 311, 360, 300, 558, 586, 370, 300, 291, 434, 406, 472, 295, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.09509395599365235, "compression_ratio": 1.887966804979253, "no_speech_prob": 6.707318971166387e-05}, {"id": 691, "seek": 389908, "start": 3916.84, "end": 3920.24, "text": " who work in data science have not actually looked at a bunch of data sets", "tokens": [50364, 20009, 17619, 412, 300, 935, 412, 341, 935, 436, 486, 855, 291, 257, 50520, 50520, 18894, 2445, 286, 478, 406, 516, 281, 360, 300, 570, 300, 5112, 291, 1825, 50730, 50730, 466, 264, 39041, 20009, 17619, 437, 321, 767, 1127, 466, 50894, 50894, 307, 406, 264, 18894, 2445, 457, 577, 309, 36896, 293, 286, 915, 881, 561, 754, 51252, 51252, 567, 589, 294, 1412, 3497, 362, 406, 767, 2956, 412, 257, 3840, 295, 1412, 6352, 51422, 51422, 281, 1223, 577, 497, 36896, 370, 718, 311, 360, 300, 558, 586, 370, 300, 291, 434, 406, 472, 295, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.09509395599365235, "compression_ratio": 1.887966804979253, "no_speech_prob": 6.707318971166387e-05}, {"id": 692, "seek": 389908, "start": 3920.24, "end": 3925.16, "text": " to understand how R behaves so let's do that right now so that you're not one of", "tokens": [50364, 20009, 17619, 412, 300, 935, 412, 341, 935, 436, 486, 855, 291, 257, 50520, 50520, 18894, 2445, 286, 478, 406, 516, 281, 360, 300, 570, 300, 5112, 291, 1825, 50730, 50730, 466, 264, 39041, 20009, 17619, 437, 321, 767, 1127, 466, 50894, 50894, 307, 406, 264, 18894, 2445, 457, 577, 309, 36896, 293, 286, 915, 881, 561, 754, 51252, 51252, 567, 589, 294, 1412, 3497, 362, 406, 767, 2956, 412, 257, 3840, 295, 1412, 6352, 51422, 51422, 281, 1223, 577, 497, 36896, 370, 718, 311, 360, 300, 558, 586, 370, 300, 291, 434, 406, 472, 295, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.09509395599365235, "compression_ratio": 1.887966804979253, "no_speech_prob": 6.707318971166387e-05}, {"id": 693, "seek": 392516, "start": 3925.16, "end": 3930.7999999999997, "text": " those people the best way I find to understand how data behaves in real life", "tokens": [50364, 729, 561, 264, 1151, 636, 286, 915, 281, 1223, 577, 1412, 36896, 294, 957, 993, 50646, 50646, 307, 281, 574, 412, 957, 993, 1412, 370, 456, 311, 257, 1412, 992, 2180, 22681, 12, 306, 1083, 1487, 365, 257, 50916, 50916, 1230, 295, 1412, 6352, 293, 472, 295, 552, 307, 1219, 5384, 6849, 293, 309, 311, 257, 51082, 51082, 1412, 992, 689, 1184, 5386, 307, 257, 6566, 293, 309, 311, 733, 295, 26331, 309, 311, 2597, 51480, 51480, 309, 311, 1589, 512, 26331, 1589, 466, 819, 16815, 293, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1198481675032731, "compression_ratio": 1.8066037735849056, "no_speech_prob": 4.068070848006755e-05}, {"id": 694, "seek": 392516, "start": 3930.7999999999997, "end": 3936.2, "text": " is to look at real life data so there's a data set scikit-learn comes with a", "tokens": [50364, 729, 561, 264, 1151, 636, 286, 915, 281, 1223, 577, 1412, 36896, 294, 957, 993, 50646, 50646, 307, 281, 574, 412, 957, 993, 1412, 370, 456, 311, 257, 1412, 992, 2180, 22681, 12, 306, 1083, 1487, 365, 257, 50916, 50916, 1230, 295, 1412, 6352, 293, 472, 295, 552, 307, 1219, 5384, 6849, 293, 309, 311, 257, 51082, 51082, 1412, 992, 689, 1184, 5386, 307, 257, 6566, 293, 309, 311, 733, 295, 26331, 309, 311, 2597, 51480, 51480, 309, 311, 1589, 512, 26331, 1589, 466, 819, 16815, 293, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1198481675032731, "compression_ratio": 1.8066037735849056, "no_speech_prob": 4.068070848006755e-05}, {"id": 695, "seek": 392516, "start": 3936.2, "end": 3939.52, "text": " number of data sets and one of them is called California housing and it's a", "tokens": [50364, 729, 561, 264, 1151, 636, 286, 915, 281, 1223, 577, 1412, 36896, 294, 957, 993, 50646, 50646, 307, 281, 574, 412, 957, 993, 1412, 370, 456, 311, 257, 1412, 992, 2180, 22681, 12, 306, 1083, 1487, 365, 257, 50916, 50916, 1230, 295, 1412, 6352, 293, 472, 295, 552, 307, 1219, 5384, 6849, 293, 309, 311, 257, 51082, 51082, 1412, 992, 689, 1184, 5386, 307, 257, 6566, 293, 309, 311, 733, 295, 26331, 309, 311, 2597, 51480, 51480, 309, 311, 1589, 512, 26331, 1589, 466, 819, 16815, 293, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1198481675032731, "compression_ratio": 1.8066037735849056, "no_speech_prob": 4.068070848006755e-05}, {"id": 696, "seek": 392516, "start": 3939.52, "end": 3947.48, "text": " data set where each row is a district and it's kind of demographic it's sorry", "tokens": [50364, 729, 561, 264, 1151, 636, 286, 915, 281, 1223, 577, 1412, 36896, 294, 957, 993, 50646, 50646, 307, 281, 574, 412, 957, 993, 1412, 370, 456, 311, 257, 1412, 992, 2180, 22681, 12, 306, 1083, 1487, 365, 257, 50916, 50916, 1230, 295, 1412, 6352, 293, 472, 295, 552, 307, 1219, 5384, 6849, 293, 309, 311, 257, 51082, 51082, 1412, 992, 689, 1184, 5386, 307, 257, 6566, 293, 309, 311, 733, 295, 26331, 309, 311, 2597, 51480, 51480, 309, 311, 1589, 512, 26331, 1589, 466, 819, 16815, 293, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1198481675032731, "compression_ratio": 1.8066037735849056, "no_speech_prob": 4.068070848006755e-05}, {"id": 697, "seek": 392516, "start": 3947.48, "end": 3950.92, "text": " it's information some demographic information about different districts and", "tokens": [50364, 729, 561, 264, 1151, 636, 286, 915, 281, 1223, 577, 1412, 36896, 294, 957, 993, 50646, 50646, 307, 281, 574, 412, 957, 993, 1412, 370, 456, 311, 257, 1412, 992, 2180, 22681, 12, 306, 1083, 1487, 365, 257, 50916, 50916, 1230, 295, 1412, 6352, 293, 472, 295, 552, 307, 1219, 5384, 6849, 293, 309, 311, 257, 51082, 51082, 1412, 992, 689, 1184, 5386, 307, 257, 6566, 293, 309, 311, 733, 295, 26331, 309, 311, 2597, 51480, 51480, 309, 311, 1589, 512, 26331, 1589, 466, 819, 16815, 293, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1198481675032731, "compression_ratio": 1.8066037735849056, "no_speech_prob": 4.068070848006755e-05}, {"id": 698, "seek": 395092, "start": 3950.92, "end": 3960.52, "text": " about the value of houses in that district I'm not going to try to plot", "tokens": [50364, 466, 264, 2158, 295, 8078, 294, 300, 6566, 286, 478, 406, 516, 281, 853, 281, 7542, 50844, 50844, 264, 1379, 721, 309, 311, 886, 955, 293, 341, 307, 257, 588, 2689, 1168, 286, 362, 490, 51026, 51026, 561, 307, 577, 360, 286, 7542, 1412, 6352, 365, 1400, 886, 867, 2793, 264, 1867, 307, 588, 51304, 51304, 2199, 483, 1570, 2793, 370, 286, 445, 16979, 4444, 257, 4714, 2793, 2035, 51634, 51634, 291, 536, 365, 257, 4714, 2793, 309, 311, 516, 281, 312, 264, 912, 382, 437, 291, 536, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.08330107253530751, "compression_ratio": 1.7511737089201878, "no_speech_prob": 2.8855010896222666e-05}, {"id": 699, "seek": 395092, "start": 3960.52, "end": 3964.16, "text": " the whole things it's too big and this is a very common question I have from", "tokens": [50364, 466, 264, 2158, 295, 8078, 294, 300, 6566, 286, 478, 406, 516, 281, 853, 281, 7542, 50844, 50844, 264, 1379, 721, 309, 311, 886, 955, 293, 341, 307, 257, 588, 2689, 1168, 286, 362, 490, 51026, 51026, 561, 307, 577, 360, 286, 7542, 1412, 6352, 365, 1400, 886, 867, 2793, 264, 1867, 307, 588, 51304, 51304, 2199, 483, 1570, 2793, 370, 286, 445, 16979, 4444, 257, 4714, 2793, 2035, 51634, 51634, 291, 536, 365, 257, 4714, 2793, 309, 311, 516, 281, 312, 264, 912, 382, 437, 291, 536, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.08330107253530751, "compression_ratio": 1.7511737089201878, "no_speech_prob": 2.8855010896222666e-05}, {"id": 700, "seek": 395092, "start": 3964.16, "end": 3969.7200000000003, "text": " people is how do I plot data sets with far too many points the answer is very", "tokens": [50364, 466, 264, 2158, 295, 8078, 294, 300, 6566, 286, 478, 406, 516, 281, 853, 281, 7542, 50844, 50844, 264, 1379, 721, 309, 311, 886, 955, 293, 341, 307, 257, 588, 2689, 1168, 286, 362, 490, 51026, 51026, 561, 307, 577, 360, 286, 7542, 1412, 6352, 365, 1400, 886, 867, 2793, 264, 1867, 307, 588, 51304, 51304, 2199, 483, 1570, 2793, 370, 286, 445, 16979, 4444, 257, 4714, 2793, 2035, 51634, 51634, 291, 536, 365, 257, 4714, 2793, 309, 311, 516, 281, 312, 264, 912, 382, 437, 291, 536, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.08330107253530751, "compression_ratio": 1.7511737089201878, "no_speech_prob": 2.8855010896222666e-05}, {"id": 701, "seek": 395092, "start": 3969.7200000000003, "end": 3976.32, "text": " simple get less points so I just randomly grab a thousand points whatever", "tokens": [50364, 466, 264, 2158, 295, 8078, 294, 300, 6566, 286, 478, 406, 516, 281, 853, 281, 7542, 50844, 50844, 264, 1379, 721, 309, 311, 886, 955, 293, 341, 307, 257, 588, 2689, 1168, 286, 362, 490, 51026, 51026, 561, 307, 577, 360, 286, 7542, 1412, 6352, 365, 1400, 886, 867, 2793, 264, 1867, 307, 588, 51304, 51304, 2199, 483, 1570, 2793, 370, 286, 445, 16979, 4444, 257, 4714, 2793, 2035, 51634, 51634, 291, 536, 365, 257, 4714, 2793, 309, 311, 516, 281, 312, 264, 912, 382, 437, 291, 536, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.08330107253530751, "compression_ratio": 1.7511737089201878, "no_speech_prob": 2.8855010896222666e-05}, {"id": 702, "seek": 395092, "start": 3976.32, "end": 3978.56, "text": " you see with a thousand points it's going to be the same as what you see", "tokens": [50364, 466, 264, 2158, 295, 8078, 294, 300, 6566, 286, 478, 406, 516, 281, 853, 281, 7542, 50844, 50844, 264, 1379, 721, 309, 311, 886, 955, 293, 341, 307, 257, 588, 2689, 1168, 286, 362, 490, 51026, 51026, 561, 307, 577, 360, 286, 7542, 1412, 6352, 365, 1400, 886, 867, 2793, 264, 1867, 307, 588, 51304, 51304, 2199, 483, 1570, 2793, 370, 286, 445, 16979, 4444, 257, 4714, 2793, 2035, 51634, 51634, 291, 536, 365, 257, 4714, 2793, 309, 311, 516, 281, 312, 264, 912, 382, 437, 291, 536, 51746, 51746], "temperature": 0.0, "avg_logprob": -0.08330107253530751, "compression_ratio": 1.7511737089201878, "no_speech_prob": 2.8855010896222666e-05}, {"id": 703, "seek": 397856, "start": 3978.56, "end": 3983.0, "text": " with a million points there's no point no reason to plot huge amounts of data", "tokens": [50364, 365, 257, 2459, 2793, 456, 311, 572, 935, 572, 1778, 281, 7542, 2603, 11663, 295, 1412, 50586, 50586, 5101, 445, 4444, 257, 4974, 6889, 586, 1031, 8200, 575, 746, 1219, 4965, 598, 5666, 370, 51014, 51014, 483, 264, 20009, 17619, 1296, 633, 7006, 293, 633, 661, 7006, 51230, 51230, 293, 309, 11247, 257, 8141, 370, 286, 393, 574, 760, 510, 293, 370, 337, 1365, 510, 307, 264, 51494, 51494, 20009, 17619, 1296, 7006, 502, 293, 7006, 502, 597, 295, 1164, 307, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.12084019604851218, "compression_ratio": 1.8110599078341014, "no_speech_prob": 5.306855382514186e-05}, {"id": 704, "seek": 397856, "start": 3983.0, "end": 3991.56, "text": " generally just grab a random sample now numpy has something called core coef so", "tokens": [50364, 365, 257, 2459, 2793, 456, 311, 572, 935, 572, 1778, 281, 7542, 2603, 11663, 295, 1412, 50586, 50586, 5101, 445, 4444, 257, 4974, 6889, 586, 1031, 8200, 575, 746, 1219, 4965, 598, 5666, 370, 51014, 51014, 483, 264, 20009, 17619, 1296, 633, 7006, 293, 633, 661, 7006, 51230, 51230, 293, 309, 11247, 257, 8141, 370, 286, 393, 574, 760, 510, 293, 370, 337, 1365, 510, 307, 264, 51494, 51494, 20009, 17619, 1296, 7006, 502, 293, 7006, 502, 597, 295, 1164, 307, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.12084019604851218, "compression_ratio": 1.8110599078341014, "no_speech_prob": 5.306855382514186e-05}, {"id": 705, "seek": 397856, "start": 3991.56, "end": 3995.88, "text": " get the correlation coefficient between every variable and every other variable", "tokens": [50364, 365, 257, 2459, 2793, 456, 311, 572, 935, 572, 1778, 281, 7542, 2603, 11663, 295, 1412, 50586, 50586, 5101, 445, 4444, 257, 4974, 6889, 586, 1031, 8200, 575, 746, 1219, 4965, 598, 5666, 370, 51014, 51014, 483, 264, 20009, 17619, 1296, 633, 7006, 293, 633, 661, 7006, 51230, 51230, 293, 309, 11247, 257, 8141, 370, 286, 393, 574, 760, 510, 293, 370, 337, 1365, 510, 307, 264, 51494, 51494, 20009, 17619, 1296, 7006, 502, 293, 7006, 502, 597, 295, 1164, 307, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.12084019604851218, "compression_ratio": 1.8110599078341014, "no_speech_prob": 5.306855382514186e-05}, {"id": 706, "seek": 397856, "start": 3995.88, "end": 4001.16, "text": " and it returns a matrix so I can look down here and so for example here is the", "tokens": [50364, 365, 257, 2459, 2793, 456, 311, 572, 935, 572, 1778, 281, 7542, 2603, 11663, 295, 1412, 50586, 50586, 5101, 445, 4444, 257, 4974, 6889, 586, 1031, 8200, 575, 746, 1219, 4965, 598, 5666, 370, 51014, 51014, 483, 264, 20009, 17619, 1296, 633, 7006, 293, 633, 661, 7006, 51230, 51230, 293, 309, 11247, 257, 8141, 370, 286, 393, 574, 760, 510, 293, 370, 337, 1365, 510, 307, 264, 51494, 51494, 20009, 17619, 1296, 7006, 502, 293, 7006, 502, 597, 295, 1164, 307, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.12084019604851218, "compression_ratio": 1.8110599078341014, "no_speech_prob": 5.306855382514186e-05}, {"id": 707, "seek": 397856, "start": 4001.16, "end": 4005.44, "text": " correlation coefficient between variable 1 and variable 1 which of course is", "tokens": [50364, 365, 257, 2459, 2793, 456, 311, 572, 935, 572, 1778, 281, 7542, 2603, 11663, 295, 1412, 50586, 50586, 5101, 445, 4444, 257, 4974, 6889, 586, 1031, 8200, 575, 746, 1219, 4965, 598, 5666, 370, 51014, 51014, 483, 264, 20009, 17619, 1296, 633, 7006, 293, 633, 661, 7006, 51230, 51230, 293, 309, 11247, 257, 8141, 370, 286, 393, 574, 760, 510, 293, 370, 337, 1365, 510, 307, 264, 51494, 51494, 20009, 17619, 1296, 7006, 502, 293, 7006, 502, 597, 295, 1164, 307, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.12084019604851218, "compression_ratio": 1.8110599078341014, "no_speech_prob": 5.306855382514186e-05}, {"id": 708, "seek": 400544, "start": 4005.44, "end": 4009.7200000000003, "text": " exactly perfectly 1.0 right because variable 1 is the same as variable 1", "tokens": [50364, 2293, 6239, 502, 13, 15, 558, 570, 7006, 502, 307, 264, 912, 382, 7006, 502, 50578, 50578, 510, 307, 264, 1359, 17340, 20009, 1296, 7006, 502, 293, 7006, 568, 293, 50888, 50888, 6399, 12, 20614, 3353, 20009, 1296, 7006, 502, 293, 7006, 805, 293, 370, 51090, 51090, 5220, 341, 307, 32330, 466, 264, 21539, 570, 264, 20009, 1296, 51342, 51342, 7006, 502, 293, 7006, 1649, 307, 264, 912, 382, 264, 20009, 1296, 7006, 1649, 293, 51528, 51528], "temperature": 0.0, "avg_logprob": -0.12789826453486575, "compression_ratio": 2.3048780487804876, "no_speech_prob": 3.534874849719927e-05}, {"id": 709, "seek": 400544, "start": 4009.7200000000003, "end": 4015.92, "text": " here is the small inverse correlation between variable 1 and variable 2 and", "tokens": [50364, 2293, 6239, 502, 13, 15, 558, 570, 7006, 502, 307, 264, 912, 382, 7006, 502, 50578, 50578, 510, 307, 264, 1359, 17340, 20009, 1296, 7006, 502, 293, 7006, 568, 293, 50888, 50888, 6399, 12, 20614, 3353, 20009, 1296, 7006, 502, 293, 7006, 805, 293, 370, 51090, 51090, 5220, 341, 307, 32330, 466, 264, 21539, 570, 264, 20009, 1296, 51342, 51342, 7006, 502, 293, 7006, 1649, 307, 264, 912, 382, 264, 20009, 1296, 7006, 1649, 293, 51528, 51528], "temperature": 0.0, "avg_logprob": -0.12789826453486575, "compression_ratio": 2.3048780487804876, "no_speech_prob": 3.534874849719927e-05}, {"id": 710, "seek": 400544, "start": 4015.92, "end": 4019.96, "text": " medium-sized positive correlation between variable 1 and variable 3 and so", "tokens": [50364, 2293, 6239, 502, 13, 15, 558, 570, 7006, 502, 307, 264, 912, 382, 7006, 502, 50578, 50578, 510, 307, 264, 1359, 17340, 20009, 1296, 7006, 502, 293, 7006, 568, 293, 50888, 50888, 6399, 12, 20614, 3353, 20009, 1296, 7006, 502, 293, 7006, 805, 293, 370, 51090, 51090, 5220, 341, 307, 32330, 466, 264, 21539, 570, 264, 20009, 1296, 51342, 51342, 7006, 502, 293, 7006, 1649, 307, 264, 912, 382, 264, 20009, 1296, 7006, 1649, 293, 51528, 51528], "temperature": 0.0, "avg_logprob": -0.12789826453486575, "compression_ratio": 2.3048780487804876, "no_speech_prob": 3.534874849719927e-05}, {"id": 711, "seek": 400544, "start": 4019.96, "end": 4025.0, "text": " forth this is symmetric about the diagonal because the correlation between", "tokens": [50364, 2293, 6239, 502, 13, 15, 558, 570, 7006, 502, 307, 264, 912, 382, 7006, 502, 50578, 50578, 510, 307, 264, 1359, 17340, 20009, 1296, 7006, 502, 293, 7006, 568, 293, 50888, 50888, 6399, 12, 20614, 3353, 20009, 1296, 7006, 502, 293, 7006, 805, 293, 370, 51090, 51090, 5220, 341, 307, 32330, 466, 264, 21539, 570, 264, 20009, 1296, 51342, 51342, 7006, 502, 293, 7006, 1649, 307, 264, 912, 382, 264, 20009, 1296, 7006, 1649, 293, 51528, 51528], "temperature": 0.0, "avg_logprob": -0.12789826453486575, "compression_ratio": 2.3048780487804876, "no_speech_prob": 3.534874849719927e-05}, {"id": 712, "seek": 400544, "start": 4025.0, "end": 4028.7200000000003, "text": " variable 1 and variable 8 is the same as the correlation between variable 8 and", "tokens": [50364, 2293, 6239, 502, 13, 15, 558, 570, 7006, 502, 307, 264, 912, 382, 7006, 502, 50578, 50578, 510, 307, 264, 1359, 17340, 20009, 1296, 7006, 502, 293, 7006, 568, 293, 50888, 50888, 6399, 12, 20614, 3353, 20009, 1296, 7006, 502, 293, 7006, 805, 293, 370, 51090, 51090, 5220, 341, 307, 32330, 466, 264, 21539, 570, 264, 20009, 1296, 51342, 51342, 7006, 502, 293, 7006, 1649, 307, 264, 912, 382, 264, 20009, 1296, 7006, 1649, 293, 51528, 51528], "temperature": 0.0, "avg_logprob": -0.12789826453486575, "compression_ratio": 2.3048780487804876, "no_speech_prob": 3.534874849719927e-05}, {"id": 713, "seek": 402872, "start": 4028.72, "end": 4037.9599999999996, "text": " variable 1 so this is a correlation coefficient matrix so that's great when", "tokens": [50364, 7006, 502, 370, 341, 307, 257, 20009, 17619, 8141, 370, 300, 311, 869, 562, 50826, 50826, 321, 1415, 281, 483, 257, 3840, 295, 4190, 439, 412, 1564, 337, 264, 48751, 22631, 6211, 321, 51022, 51022, 500, 380, 528, 300, 321, 445, 528, 257, 2167, 20009, 1230, 498, 321, 445, 1320, 294, 257, 51222, 51222, 6119, 295, 9102, 321, 920, 483, 257, 8141, 597, 307, 733, 295, 3657, 309, 311, 733, 295, 51516, 51516, 309, 311, 406, 3657, 309, 311, 406, 437, 321, 528, 370, 321, 820, 4444, 472, 295, 613, 370, 562, 286, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.06984220583414294, "compression_ratio": 1.761467889908257, "no_speech_prob": 4.132293179281987e-05}, {"id": 714, "seek": 402872, "start": 4037.9599999999996, "end": 4041.8799999999997, "text": " we wanted to get a bunch of values all at once for the Kaggle competition we", "tokens": [50364, 7006, 502, 370, 341, 307, 257, 20009, 17619, 8141, 370, 300, 311, 869, 562, 50826, 50826, 321, 1415, 281, 483, 257, 3840, 295, 4190, 439, 412, 1564, 337, 264, 48751, 22631, 6211, 321, 51022, 51022, 500, 380, 528, 300, 321, 445, 528, 257, 2167, 20009, 1230, 498, 321, 445, 1320, 294, 257, 51222, 51222, 6119, 295, 9102, 321, 920, 483, 257, 8141, 597, 307, 733, 295, 3657, 309, 311, 733, 295, 51516, 51516, 309, 311, 406, 3657, 309, 311, 406, 437, 321, 528, 370, 321, 820, 4444, 472, 295, 613, 370, 562, 286, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.06984220583414294, "compression_ratio": 1.761467889908257, "no_speech_prob": 4.132293179281987e-05}, {"id": 715, "seek": 402872, "start": 4041.8799999999997, "end": 4045.8799999999997, "text": " don't want that we just want a single correlation number if we just pass in a", "tokens": [50364, 7006, 502, 370, 341, 307, 257, 20009, 17619, 8141, 370, 300, 311, 869, 562, 50826, 50826, 321, 1415, 281, 483, 257, 3840, 295, 4190, 439, 412, 1564, 337, 264, 48751, 22631, 6211, 321, 51022, 51022, 500, 380, 528, 300, 321, 445, 528, 257, 2167, 20009, 1230, 498, 321, 445, 1320, 294, 257, 51222, 51222, 6119, 295, 9102, 321, 920, 483, 257, 8141, 597, 307, 733, 295, 3657, 309, 311, 733, 295, 51516, 51516, 309, 311, 406, 3657, 309, 311, 406, 437, 321, 528, 370, 321, 820, 4444, 472, 295, 613, 370, 562, 286, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.06984220583414294, "compression_ratio": 1.761467889908257, "no_speech_prob": 4.132293179281987e-05}, {"id": 716, "seek": 402872, "start": 4045.8799999999997, "end": 4051.7599999999998, "text": " pair of variables we still get a matrix which is kind of weird it's kind of", "tokens": [50364, 7006, 502, 370, 341, 307, 257, 20009, 17619, 8141, 370, 300, 311, 869, 562, 50826, 50826, 321, 1415, 281, 483, 257, 3840, 295, 4190, 439, 412, 1564, 337, 264, 48751, 22631, 6211, 321, 51022, 51022, 500, 380, 528, 300, 321, 445, 528, 257, 2167, 20009, 1230, 498, 321, 445, 1320, 294, 257, 51222, 51222, 6119, 295, 9102, 321, 920, 483, 257, 8141, 597, 307, 733, 295, 3657, 309, 311, 733, 295, 51516, 51516, 309, 311, 406, 3657, 309, 311, 406, 437, 321, 528, 370, 321, 820, 4444, 472, 295, 613, 370, 562, 286, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.06984220583414294, "compression_ratio": 1.761467889908257, "no_speech_prob": 4.132293179281987e-05}, {"id": 717, "seek": 402872, "start": 4051.7599999999998, "end": 4056.0, "text": " it's not weird it's not what we want so we should grab one of these so when I", "tokens": [50364, 7006, 502, 370, 341, 307, 257, 20009, 17619, 8141, 370, 300, 311, 869, 562, 50826, 50826, 321, 1415, 281, 483, 257, 3840, 295, 4190, 439, 412, 1564, 337, 264, 48751, 22631, 6211, 321, 51022, 51022, 500, 380, 528, 300, 321, 445, 528, 257, 2167, 20009, 1230, 498, 321, 445, 1320, 294, 257, 51222, 51222, 6119, 295, 9102, 321, 920, 483, 257, 8141, 597, 307, 733, 295, 3657, 309, 311, 733, 295, 51516, 51516, 309, 311, 406, 3657, 309, 311, 406, 437, 321, 528, 370, 321, 820, 4444, 472, 295, 613, 370, 562, 286, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.06984220583414294, "compression_ratio": 1.761467889908257, "no_speech_prob": 4.132293179281987e-05}, {"id": 718, "seek": 405600, "start": 4056.0, "end": 4060.48, "text": " want to grab a correlation coefficient I'll just return the zeroth row first", "tokens": [50364, 528, 281, 4444, 257, 20009, 17619, 286, 603, 445, 2736, 264, 44746, 900, 5386, 700, 50588, 50588, 7738, 370, 300, 311, 437, 4965, 307, 300, 311, 516, 281, 312, 527, 2167, 20009, 50792, 50792, 17619, 370, 718, 311, 574, 412, 264, 20009, 1296, 732, 721, 337, 51080, 51080, 1365, 815, 26779, 5742, 293, 6399, 1782, 2158, 1958, 13, 22452, 1392, 307, 300, 1090, 51486, 51486, 6399, 2295, 577, 955, 307, 300, 437, 775, 309, 574, 411, 370, 264, 2135, 551, 321, 643, 281, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.12011514861008217, "compression_ratio": 1.7230046948356808, "no_speech_prob": 1.7502250557299703e-05}, {"id": 719, "seek": 405600, "start": 4060.48, "end": 4064.56, "text": " column so that's what core is that's going to be our single correlation", "tokens": [50364, 528, 281, 4444, 257, 20009, 17619, 286, 603, 445, 2736, 264, 44746, 900, 5386, 700, 50588, 50588, 7738, 370, 300, 311, 437, 4965, 307, 300, 311, 516, 281, 312, 527, 2167, 20009, 50792, 50792, 17619, 370, 718, 311, 574, 412, 264, 20009, 1296, 732, 721, 337, 51080, 51080, 1365, 815, 26779, 5742, 293, 6399, 1782, 2158, 1958, 13, 22452, 1392, 307, 300, 1090, 51486, 51486, 6399, 2295, 577, 955, 307, 300, 437, 775, 309, 574, 411, 370, 264, 2135, 551, 321, 643, 281, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.12011514861008217, "compression_ratio": 1.7230046948356808, "no_speech_prob": 1.7502250557299703e-05}, {"id": 720, "seek": 405600, "start": 4064.56, "end": 4070.32, "text": " coefficient so let's look at the correlation between two things for", "tokens": [50364, 528, 281, 4444, 257, 20009, 17619, 286, 603, 445, 2736, 264, 44746, 900, 5386, 700, 50588, 50588, 7738, 370, 300, 311, 437, 4965, 307, 300, 311, 516, 281, 312, 527, 2167, 20009, 50792, 50792, 17619, 370, 718, 311, 574, 412, 264, 20009, 1296, 732, 721, 337, 51080, 51080, 1365, 815, 26779, 5742, 293, 6399, 1782, 2158, 1958, 13, 22452, 1392, 307, 300, 1090, 51486, 51486, 6399, 2295, 577, 955, 307, 300, 437, 775, 309, 574, 411, 370, 264, 2135, 551, 321, 643, 281, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.12011514861008217, "compression_ratio": 1.7230046948356808, "no_speech_prob": 1.7502250557299703e-05}, {"id": 721, "seek": 405600, "start": 4070.32, "end": 4078.44, "text": " example may median income and medium house value 0.67 okay is that high", "tokens": [50364, 528, 281, 4444, 257, 20009, 17619, 286, 603, 445, 2736, 264, 44746, 900, 5386, 700, 50588, 50588, 7738, 370, 300, 311, 437, 4965, 307, 300, 311, 516, 281, 312, 527, 2167, 20009, 50792, 50792, 17619, 370, 718, 311, 574, 412, 264, 20009, 1296, 732, 721, 337, 51080, 51080, 1365, 815, 26779, 5742, 293, 6399, 1782, 2158, 1958, 13, 22452, 1392, 307, 300, 1090, 51486, 51486, 6399, 2295, 577, 955, 307, 300, 437, 775, 309, 574, 411, 370, 264, 2135, 551, 321, 643, 281, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.12011514861008217, "compression_ratio": 1.7230046948356808, "no_speech_prob": 1.7502250557299703e-05}, {"id": 722, "seek": 405600, "start": 4078.44, "end": 4084.8, "text": " medium low how big is that what does it look like so the main thing we need to", "tokens": [50364, 528, 281, 4444, 257, 20009, 17619, 286, 603, 445, 2736, 264, 44746, 900, 5386, 700, 50588, 50588, 7738, 370, 300, 311, 437, 4965, 307, 300, 311, 516, 281, 312, 527, 2167, 20009, 50792, 50792, 17619, 370, 718, 311, 574, 412, 264, 20009, 1296, 732, 721, 337, 51080, 51080, 1365, 815, 26779, 5742, 293, 6399, 1782, 2158, 1958, 13, 22452, 1392, 307, 300, 1090, 51486, 51486, 6399, 2295, 577, 955, 307, 300, 437, 775, 309, 574, 411, 370, 264, 2135, 551, 321, 643, 281, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.12011514861008217, "compression_ratio": 1.7230046948356808, "no_speech_prob": 1.7502250557299703e-05}, {"id": 723, "seek": 408480, "start": 4084.8, "end": 4088.6400000000003, "text": " understand is what these things look like so what I suggest we do is we're", "tokens": [50364, 1223, 307, 437, 613, 721, 574, 411, 370, 437, 286, 3402, 321, 360, 307, 321, 434, 50556, 50556, 516, 281, 747, 257, 1266, 3456, 1821, 4949, 3456, 1821, 321, 603, 808, 646, 412, 1922, 50780, 50780, 1791, 293, 550, 321, 434, 516, 281, 574, 412, 512, 5110, 295, 20009, 31994, 51068, 51320, 1392, 2928, 646, 370, 437, 286, 600, 1096, 510, 307, 286, 600, 2942, 257, 707, 2445, 1219, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.13540060226231404, "compression_ratio": 1.6524064171122994, "no_speech_prob": 1.3845125067746267e-05}, {"id": 724, "seek": 408480, "start": 4088.6400000000003, "end": 4093.1200000000003, "text": " going to take a 10 minute break nine minute break we'll come back at half", "tokens": [50364, 1223, 307, 437, 613, 721, 574, 411, 370, 437, 286, 3402, 321, 360, 307, 321, 434, 50556, 50556, 516, 281, 747, 257, 1266, 3456, 1821, 4949, 3456, 1821, 321, 603, 808, 646, 412, 1922, 50780, 50780, 1791, 293, 550, 321, 434, 516, 281, 574, 412, 512, 5110, 295, 20009, 31994, 51068, 51320, 1392, 2928, 646, 370, 437, 286, 600, 1096, 510, 307, 286, 600, 2942, 257, 707, 2445, 1219, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.13540060226231404, "compression_ratio": 1.6524064171122994, "no_speech_prob": 1.3845125067746267e-05}, {"id": 725, "seek": 408480, "start": 4093.1200000000003, "end": 4098.88, "text": " past and then we're going to look at some examples of correlation coefficients", "tokens": [50364, 1223, 307, 437, 613, 721, 574, 411, 370, 437, 286, 3402, 321, 360, 307, 321, 434, 50556, 50556, 516, 281, 747, 257, 1266, 3456, 1821, 4949, 3456, 1821, 321, 603, 808, 646, 412, 1922, 50780, 50780, 1791, 293, 550, 321, 434, 516, 281, 574, 412, 512, 5110, 295, 20009, 31994, 51068, 51320, 1392, 2928, 646, 370, 437, 286, 600, 1096, 510, 307, 286, 600, 2942, 257, 707, 2445, 1219, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.13540060226231404, "compression_ratio": 1.6524064171122994, "no_speech_prob": 1.3845125067746267e-05}, {"id": 726, "seek": 408480, "start": 4103.92, "end": 4110.4800000000005, "text": " okay welcome back so what I've done here is I've created a little function called", "tokens": [50364, 1223, 307, 437, 613, 721, 574, 411, 370, 437, 286, 3402, 321, 360, 307, 321, 434, 50556, 50556, 516, 281, 747, 257, 1266, 3456, 1821, 4949, 3456, 1821, 321, 603, 808, 646, 412, 1922, 50780, 50780, 1791, 293, 550, 321, 434, 516, 281, 574, 412, 512, 5110, 295, 20009, 31994, 51068, 51320, 1392, 2928, 646, 370, 437, 286, 600, 1096, 510, 307, 286, 600, 2942, 257, 707, 2445, 1219, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.13540060226231404, "compression_ratio": 1.6524064171122994, "no_speech_prob": 1.3845125067746267e-05}, {"id": 727, "seek": 411048, "start": 4110.48, "end": 4116.28, "text": " show correlations and a passing a data frame and a couple of columns as strings", "tokens": [50364, 855, 13983, 763, 293, 257, 8437, 257, 1412, 3920, 293, 257, 1916, 295, 13766, 382, 13985, 50654, 50654, 516, 281, 4444, 1184, 295, 729, 13766, 382, 2638, 360, 257, 34951, 7542, 293, 550, 855, 50948, 50948, 264, 20009, 370, 321, 1217, 2835, 6399, 5742, 293, 6399, 1782, 51156, 51156, 38546, 295, 1958, 13, 27102, 370, 510, 309, 307, 510, 311, 437, 935, 2309, 3180, 1542, 411, 370, 291, 51476, 51476, 458, 286, 500, 380, 458, 498, 291, 632, 512, 24002, 466, 437, 291, 5176, 457, 382, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.12432889938354492, "compression_ratio": 1.6858407079646018, "no_speech_prob": 4.6107437810860574e-05}, {"id": 728, "seek": 411048, "start": 4116.28, "end": 4122.16, "text": " going to grab each of those columns as series do a scatter plot and then show", "tokens": [50364, 855, 13983, 763, 293, 257, 8437, 257, 1412, 3920, 293, 257, 1916, 295, 13766, 382, 13985, 50654, 50654, 516, 281, 4444, 1184, 295, 729, 13766, 382, 2638, 360, 257, 34951, 7542, 293, 550, 855, 50948, 50948, 264, 20009, 370, 321, 1217, 2835, 6399, 5742, 293, 6399, 1782, 51156, 51156, 38546, 295, 1958, 13, 27102, 370, 510, 309, 307, 510, 311, 437, 935, 2309, 3180, 1542, 411, 370, 291, 51476, 51476, 458, 286, 500, 380, 458, 498, 291, 632, 512, 24002, 466, 437, 291, 5176, 457, 382, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.12432889938354492, "compression_ratio": 1.6858407079646018, "no_speech_prob": 4.6107437810860574e-05}, {"id": 729, "seek": 411048, "start": 4122.16, "end": 4126.32, "text": " the correlation so we already mentioned medium income and medium house", "tokens": [50364, 855, 13983, 763, 293, 257, 8437, 257, 1412, 3920, 293, 257, 1916, 295, 13766, 382, 13985, 50654, 50654, 516, 281, 4444, 1184, 295, 729, 13766, 382, 2638, 360, 257, 34951, 7542, 293, 550, 855, 50948, 50948, 264, 20009, 370, 321, 1217, 2835, 6399, 5742, 293, 6399, 1782, 51156, 51156, 38546, 295, 1958, 13, 27102, 370, 510, 309, 307, 510, 311, 437, 935, 2309, 3180, 1542, 411, 370, 291, 51476, 51476, 458, 286, 500, 380, 458, 498, 291, 632, 512, 24002, 466, 437, 291, 5176, 457, 382, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.12432889938354492, "compression_ratio": 1.6858407079646018, "no_speech_prob": 4.6107437810860574e-05}, {"id": 730, "seek": 411048, "start": 4126.32, "end": 4132.719999999999, "text": " valuation of 0.68 so here it is here's what point six eight looks like so you", "tokens": [50364, 855, 13983, 763, 293, 257, 8437, 257, 1412, 3920, 293, 257, 1916, 295, 13766, 382, 13985, 50654, 50654, 516, 281, 4444, 1184, 295, 729, 13766, 382, 2638, 360, 257, 34951, 7542, 293, 550, 855, 50948, 50948, 264, 20009, 370, 321, 1217, 2835, 6399, 5742, 293, 6399, 1782, 51156, 51156, 38546, 295, 1958, 13, 27102, 370, 510, 309, 307, 510, 311, 437, 935, 2309, 3180, 1542, 411, 370, 291, 51476, 51476, 458, 286, 500, 380, 458, 498, 291, 632, 512, 24002, 466, 437, 291, 5176, 457, 382, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.12432889938354492, "compression_ratio": 1.6858407079646018, "no_speech_prob": 4.6107437810860574e-05}, {"id": 731, "seek": 411048, "start": 4132.719999999999, "end": 4137.12, "text": " know I don't know if you had some intuition about what you expected but as", "tokens": [50364, 855, 13983, 763, 293, 257, 8437, 257, 1412, 3920, 293, 257, 1916, 295, 13766, 382, 13985, 50654, 50654, 516, 281, 4444, 1184, 295, 729, 13766, 382, 2638, 360, 257, 34951, 7542, 293, 550, 855, 50948, 50948, 264, 20009, 370, 321, 1217, 2835, 6399, 5742, 293, 6399, 1782, 51156, 51156, 38546, 295, 1958, 13, 27102, 370, 510, 309, 307, 510, 311, 437, 935, 2309, 3180, 1542, 411, 370, 291, 51476, 51476, 458, 286, 500, 380, 458, 498, 291, 632, 512, 24002, 466, 437, 291, 5176, 457, 382, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.12432889938354492, "compression_ratio": 1.6858407079646018, "no_speech_prob": 4.6107437810860574e-05}, {"id": 732, "seek": 413712, "start": 4137.12, "end": 4142.44, "text": " you can see it's still plenty of variation even at that reasonably high", "tokens": [50364, 291, 393, 536, 309, 311, 920, 7140, 295, 12990, 754, 412, 300, 23551, 1090, 50630, 50630, 20009, 611, 291, 393, 536, 510, 300, 5056, 3319, 428, 1412, 307, 588, 1021, 51154, 51154, 498, 291, 366, 1364, 365, 341, 1412, 992, 570, 291, 393, 4258, 536, 439, 51334, 51334, 613, 15026, 2051, 510, 300, 311, 4448, 504, 409, 46252, 558, 370, 341, 307, 411, 562, 51616, 51616, 309, 311, 406, 1826, 291, 574, 412, 5242, 411, 341, 300, 291, 434, 516, 281, 1888, 1507, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.08370740934350025, "compression_ratio": 1.7136363636363636, "no_speech_prob": 7.842541526770219e-05}, {"id": 733, "seek": 413712, "start": 4142.44, "end": 4152.92, "text": " correlation also you can see here that visualizing your data is very important", "tokens": [50364, 291, 393, 536, 309, 311, 920, 7140, 295, 12990, 754, 412, 300, 23551, 1090, 50630, 50630, 20009, 611, 291, 393, 536, 510, 300, 5056, 3319, 428, 1412, 307, 588, 1021, 51154, 51154, 498, 291, 366, 1364, 365, 341, 1412, 992, 570, 291, 393, 4258, 536, 439, 51334, 51334, 613, 15026, 2051, 510, 300, 311, 4448, 504, 409, 46252, 558, 370, 341, 307, 411, 562, 51616, 51616, 309, 311, 406, 1826, 291, 574, 412, 5242, 411, 341, 300, 291, 434, 516, 281, 1888, 1507, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.08370740934350025, "compression_ratio": 1.7136363636363636, "no_speech_prob": 7.842541526770219e-05}, {"id": 734, "seek": 413712, "start": 4152.92, "end": 4156.5199999999995, "text": " if you are working with this data set because you can immediately see all", "tokens": [50364, 291, 393, 536, 309, 311, 920, 7140, 295, 12990, 754, 412, 300, 23551, 1090, 50630, 50630, 20009, 611, 291, 393, 536, 510, 300, 5056, 3319, 428, 1412, 307, 588, 1021, 51154, 51154, 498, 291, 366, 1364, 365, 341, 1412, 992, 570, 291, 393, 4258, 536, 439, 51334, 51334, 613, 15026, 2051, 510, 300, 311, 4448, 504, 409, 46252, 558, 370, 341, 307, 411, 562, 51616, 51616, 309, 311, 406, 1826, 291, 574, 412, 5242, 411, 341, 300, 291, 434, 516, 281, 1888, 1507, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.08370740934350025, "compression_ratio": 1.7136363636363636, "no_speech_prob": 7.842541526770219e-05}, {"id": 735, "seek": 413712, "start": 4156.5199999999995, "end": 4162.16, "text": " these dots along here that's clearly truncation right so this is like when", "tokens": [50364, 291, 393, 536, 309, 311, 920, 7140, 295, 12990, 754, 412, 300, 23551, 1090, 50630, 50630, 20009, 611, 291, 393, 536, 510, 300, 5056, 3319, 428, 1412, 307, 588, 1021, 51154, 51154, 498, 291, 366, 1364, 365, 341, 1412, 992, 570, 291, 393, 4258, 536, 439, 51334, 51334, 613, 15026, 2051, 510, 300, 311, 4448, 504, 409, 46252, 558, 370, 341, 307, 411, 562, 51616, 51616, 309, 311, 406, 1826, 291, 574, 412, 5242, 411, 341, 300, 291, 434, 516, 281, 1888, 1507, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.08370740934350025, "compression_ratio": 1.7136363636363636, "no_speech_prob": 7.842541526770219e-05}, {"id": 736, "seek": 413712, "start": 4162.16, "end": 4164.8, "text": " it's not until you look at pictures like this that you're going to pick stuff", "tokens": [50364, 291, 393, 536, 309, 311, 920, 7140, 295, 12990, 754, 412, 300, 23551, 1090, 50630, 50630, 20009, 611, 291, 393, 536, 510, 300, 5056, 3319, 428, 1412, 307, 588, 1021, 51154, 51154, 498, 291, 366, 1364, 365, 341, 1412, 992, 570, 291, 393, 4258, 536, 439, 51334, 51334, 613, 15026, 2051, 510, 300, 311, 4448, 504, 409, 46252, 558, 370, 341, 307, 411, 562, 51616, 51616, 309, 311, 406, 1826, 291, 574, 412, 5242, 411, 341, 300, 291, 434, 516, 281, 1888, 1507, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.08370740934350025, "compression_ratio": 1.7136363636363636, "no_speech_prob": 7.842541526770219e-05}, {"id": 737, "seek": 416480, "start": 4164.8, "end": 4172.4800000000005, "text": " like this up pictures are great oh little trick on the scatter plot I put", "tokens": [50364, 411, 341, 493, 5242, 366, 869, 1954, 707, 4282, 322, 264, 34951, 7542, 286, 829, 50748, 50748, 8961, 307, 1958, 13, 20, 300, 7829, 512, 17131, 337, 613, 733, 295, 34951, 50978, 50978, 28609, 300, 534, 3665, 570, 309, 411, 733, 295, 7829, 12741, 3179, 294, 3190, 51238, 51238, 689, 456, 311, 3195, 295, 15026, 370, 1338, 264, 8961, 293, 34951, 28609, 307, 1481, 1392, 51544, 51544, 510, 311, 1071, 6119, 370, 341, 472, 311, 2780, 760, 490, 935, 2309, 3180, 281, 935, 1451, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.13434694030068137, "compression_ratio": 1.7136363636363636, "no_speech_prob": 6.813537038397044e-05}, {"id": 738, "seek": 416480, "start": 4172.4800000000005, "end": 4177.08, "text": " alpha is 0.5 that creates some transparency for these kind of scatter", "tokens": [50364, 411, 341, 493, 5242, 366, 869, 1954, 707, 4282, 322, 264, 34951, 7542, 286, 829, 50748, 50748, 8961, 307, 1958, 13, 20, 300, 7829, 512, 17131, 337, 613, 733, 295, 34951, 50978, 50978, 28609, 300, 534, 3665, 570, 309, 411, 733, 295, 7829, 12741, 3179, 294, 3190, 51238, 51238, 689, 456, 311, 3195, 295, 15026, 370, 1338, 264, 8961, 293, 34951, 28609, 307, 1481, 1392, 51544, 51544, 510, 311, 1071, 6119, 370, 341, 472, 311, 2780, 760, 490, 935, 2309, 3180, 281, 935, 1451, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.13434694030068137, "compression_ratio": 1.7136363636363636, "no_speech_prob": 6.813537038397044e-05}, {"id": 739, "seek": 416480, "start": 4177.08, "end": 4182.28, "text": " plots that really helps because it like kind of creates darker areas in places", "tokens": [50364, 411, 341, 493, 5242, 366, 869, 1954, 707, 4282, 322, 264, 34951, 7542, 286, 829, 50748, 50748, 8961, 307, 1958, 13, 20, 300, 7829, 512, 17131, 337, 613, 733, 295, 34951, 50978, 50978, 28609, 300, 534, 3665, 570, 309, 411, 733, 295, 7829, 12741, 3179, 294, 3190, 51238, 51238, 689, 456, 311, 3195, 295, 15026, 370, 1338, 264, 8961, 293, 34951, 28609, 307, 1481, 1392, 51544, 51544, 510, 311, 1071, 6119, 370, 341, 472, 311, 2780, 760, 490, 935, 2309, 3180, 281, 935, 1451, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.13434694030068137, "compression_ratio": 1.7136363636363636, "no_speech_prob": 6.813537038397044e-05}, {"id": 740, "seek": 416480, "start": 4182.28, "end": 4188.400000000001, "text": " where there's lots of dots so yeah the alpha and scatter plots is nice okay", "tokens": [50364, 411, 341, 493, 5242, 366, 869, 1954, 707, 4282, 322, 264, 34951, 7542, 286, 829, 50748, 50748, 8961, 307, 1958, 13, 20, 300, 7829, 512, 17131, 337, 613, 733, 295, 34951, 50978, 50978, 28609, 300, 534, 3665, 570, 309, 411, 733, 295, 7829, 12741, 3179, 294, 3190, 51238, 51238, 689, 456, 311, 3195, 295, 15026, 370, 1338, 264, 8961, 293, 34951, 28609, 307, 1481, 1392, 51544, 51544, 510, 311, 1071, 6119, 370, 341, 472, 311, 2780, 760, 490, 935, 2309, 3180, 281, 935, 1451, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.13434694030068137, "compression_ratio": 1.7136363636363636, "no_speech_prob": 6.813537038397044e-05}, {"id": 741, "seek": 416480, "start": 4188.400000000001, "end": 4192.88, "text": " here's another pair so this one's gone down from point six eight to point four", "tokens": [50364, 411, 341, 493, 5242, 366, 869, 1954, 707, 4282, 322, 264, 34951, 7542, 286, 829, 50748, 50748, 8961, 307, 1958, 13, 20, 300, 7829, 512, 17131, 337, 613, 733, 295, 34951, 50978, 50978, 28609, 300, 534, 3665, 570, 309, 411, 733, 295, 7829, 12741, 3179, 294, 3190, 51238, 51238, 689, 456, 311, 3195, 295, 15026, 370, 1338, 264, 8961, 293, 34951, 28609, 307, 1481, 1392, 51544, 51544, 510, 311, 1071, 6119, 370, 341, 472, 311, 2780, 760, 490, 935, 2309, 3180, 281, 935, 1451, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.13434694030068137, "compression_ratio": 1.7136363636363636, "no_speech_prob": 6.813537038397044e-05}, {"id": 742, "seek": 419288, "start": 4192.88, "end": 4199.32, "text": " three median income versus the number of rooms per house as you'd expect more", "tokens": [50364, 1045, 26779, 5742, 5717, 264, 1230, 295, 9396, 680, 1782, 382, 291, 1116, 2066, 544, 50686, 50686, 9396, 309, 311, 544, 5742, 457, 341, 307, 257, 588, 3657, 1237, 551, 586, 291, 603, 915, 51106, 51106, 300, 257, 688, 295, 613, 22820, 8000, 411, 20009, 10687, 322, 264, 3732, 295, 51406, 51406, 264, 2649, 293, 562, 291, 362, 955, 484, 23646, 411, 341, 264, 3732, 295, 264, 51660, 51660, 2649, 1709, 3219, 293, 370, 341, 307, 1071, 1081, 321, 1116, 528, 281, 574, 412, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.09370305564966094, "compression_ratio": 1.742081447963801, "no_speech_prob": 3.480625309748575e-05}, {"id": 743, "seek": 419288, "start": 4199.32, "end": 4207.72, "text": " rooms it's more income but this is a very weird looking thing now you'll find", "tokens": [50364, 1045, 26779, 5742, 5717, 264, 1230, 295, 9396, 680, 1782, 382, 291, 1116, 2066, 544, 50686, 50686, 9396, 309, 311, 544, 5742, 457, 341, 307, 257, 588, 3657, 1237, 551, 586, 291, 603, 915, 51106, 51106, 300, 257, 688, 295, 613, 22820, 8000, 411, 20009, 10687, 322, 264, 3732, 295, 51406, 51406, 264, 2649, 293, 562, 291, 362, 955, 484, 23646, 411, 341, 264, 3732, 295, 264, 51660, 51660, 2649, 1709, 3219, 293, 370, 341, 307, 1071, 1081, 321, 1116, 528, 281, 574, 412, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.09370305564966094, "compression_ratio": 1.742081447963801, "no_speech_prob": 3.480625309748575e-05}, {"id": 744, "seek": 419288, "start": 4207.72, "end": 4213.72, "text": " that a lot of these statistical measures like correlation rely on the square of", "tokens": [50364, 1045, 26779, 5742, 5717, 264, 1230, 295, 9396, 680, 1782, 382, 291, 1116, 2066, 544, 50686, 50686, 9396, 309, 311, 544, 5742, 457, 341, 307, 257, 588, 3657, 1237, 551, 586, 291, 603, 915, 51106, 51106, 300, 257, 688, 295, 613, 22820, 8000, 411, 20009, 10687, 322, 264, 3732, 295, 51406, 51406, 264, 2649, 293, 562, 291, 362, 955, 484, 23646, 411, 341, 264, 3732, 295, 264, 51660, 51660, 2649, 1709, 3219, 293, 370, 341, 307, 1071, 1081, 321, 1116, 528, 281, 574, 412, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.09370305564966094, "compression_ratio": 1.742081447963801, "no_speech_prob": 3.480625309748575e-05}, {"id": 745, "seek": 419288, "start": 4213.72, "end": 4218.8, "text": " the difference and when you have big outliers like this the square of the", "tokens": [50364, 1045, 26779, 5742, 5717, 264, 1230, 295, 9396, 680, 1782, 382, 291, 1116, 2066, 544, 50686, 50686, 9396, 309, 311, 544, 5742, 457, 341, 307, 257, 588, 3657, 1237, 551, 586, 291, 603, 915, 51106, 51106, 300, 257, 688, 295, 613, 22820, 8000, 411, 20009, 10687, 322, 264, 3732, 295, 51406, 51406, 264, 2649, 293, 562, 291, 362, 955, 484, 23646, 411, 341, 264, 3732, 295, 264, 51660, 51660, 2649, 1709, 3219, 293, 370, 341, 307, 1071, 1081, 321, 1116, 528, 281, 574, 412, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.09370305564966094, "compression_ratio": 1.742081447963801, "no_speech_prob": 3.480625309748575e-05}, {"id": 746, "seek": 419288, "start": 4218.8, "end": 4222.76, "text": " difference goes crazy and so this is another place we'd want to look at the", "tokens": [50364, 1045, 26779, 5742, 5717, 264, 1230, 295, 9396, 680, 1782, 382, 291, 1116, 2066, 544, 50686, 50686, 9396, 309, 311, 544, 5742, 457, 341, 307, 257, 588, 3657, 1237, 551, 586, 291, 603, 915, 51106, 51106, 300, 257, 688, 295, 613, 22820, 8000, 411, 20009, 10687, 322, 264, 3732, 295, 51406, 51406, 264, 2649, 293, 562, 291, 362, 955, 484, 23646, 411, 341, 264, 3732, 295, 264, 51660, 51660, 2649, 1709, 3219, 293, 370, 341, 307, 1071, 1081, 321, 1116, 528, 281, 574, 412, 264, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.09370305564966094, "compression_ratio": 1.742081447963801, "no_speech_prob": 3.480625309748575e-05}, {"id": 747, "seek": 422276, "start": 4222.76, "end": 4228.280000000001, "text": " data first you say oh that's that's going to be a bit of an issue there's", "tokens": [50364, 1412, 700, 291, 584, 1954, 300, 311, 300, 311, 516, 281, 312, 257, 857, 295, 364, 2734, 456, 311, 50640, 50640, 1391, 544, 20009, 510, 457, 456, 311, 257, 1326, 5110, 295, 512, 8078, 50874, 50874, 365, 3195, 293, 3195, 295, 1808, 689, 561, 300, 3212, 380, 588, 4593, 1621, 1310, 613, 51064, 51064, 366, 512, 733, 295, 5507, 5507, 27363, 420, 746, 370, 497, 307, 588, 51412, 51412, 9477, 281, 484, 23646, 370, 718, 311, 483, 3973, 295, 264, 8078, 264, 9396, 365, 2119, 9396, 264, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.1074960205581162, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.00013132780441083014}, {"id": 748, "seek": 422276, "start": 4228.280000000001, "end": 4232.96, "text": " probably more correlation here but there's a few examples of some houses", "tokens": [50364, 1412, 700, 291, 584, 1954, 300, 311, 300, 311, 516, 281, 312, 257, 857, 295, 364, 2734, 456, 311, 50640, 50640, 1391, 544, 20009, 510, 457, 456, 311, 257, 1326, 5110, 295, 512, 8078, 50874, 50874, 365, 3195, 293, 3195, 295, 1808, 689, 561, 300, 3212, 380, 588, 4593, 1621, 1310, 613, 51064, 51064, 366, 512, 733, 295, 5507, 5507, 27363, 420, 746, 370, 497, 307, 588, 51412, 51412, 9477, 281, 484, 23646, 370, 718, 311, 483, 3973, 295, 264, 8078, 264, 9396, 365, 2119, 9396, 264, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.1074960205581162, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.00013132780441083014}, {"id": 749, "seek": 422276, "start": 4232.96, "end": 4236.76, "text": " with lots and lots of room where people that aren't very rich live maybe these", "tokens": [50364, 1412, 700, 291, 584, 1954, 300, 311, 300, 311, 516, 281, 312, 257, 857, 295, 364, 2734, 456, 311, 50640, 50640, 1391, 544, 20009, 510, 457, 456, 311, 257, 1326, 5110, 295, 512, 8078, 50874, 50874, 365, 3195, 293, 3195, 295, 1808, 689, 561, 300, 3212, 380, 588, 4593, 1621, 1310, 613, 51064, 51064, 366, 512, 733, 295, 5507, 5507, 27363, 420, 746, 370, 497, 307, 588, 51412, 51412, 9477, 281, 484, 23646, 370, 718, 311, 483, 3973, 295, 264, 8078, 264, 9396, 365, 2119, 9396, 264, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.1074960205581162, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.00013132780441083014}, {"id": 750, "seek": 422276, "start": 4236.76, "end": 4243.72, "text": " are some kind of shared shared accommodation or something so R is very", "tokens": [50364, 1412, 700, 291, 584, 1954, 300, 311, 300, 311, 516, 281, 312, 257, 857, 295, 364, 2734, 456, 311, 50640, 50640, 1391, 544, 20009, 510, 457, 456, 311, 257, 1326, 5110, 295, 512, 8078, 50874, 50874, 365, 3195, 293, 3195, 295, 1808, 689, 561, 300, 3212, 380, 588, 4593, 1621, 1310, 613, 51064, 51064, 366, 512, 733, 295, 5507, 5507, 27363, 420, 746, 370, 497, 307, 588, 51412, 51412, 9477, 281, 484, 23646, 370, 718, 311, 483, 3973, 295, 264, 8078, 264, 9396, 365, 2119, 9396, 264, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.1074960205581162, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.00013132780441083014}, {"id": 751, "seek": 422276, "start": 4243.72, "end": 4250.24, "text": " sensitive to outliers so let's get rid of the houses the rooms with 15 rooms the", "tokens": [50364, 1412, 700, 291, 584, 1954, 300, 311, 300, 311, 516, 281, 312, 257, 857, 295, 364, 2734, 456, 311, 50640, 50640, 1391, 544, 20009, 510, 457, 456, 311, 257, 1326, 5110, 295, 512, 8078, 50874, 50874, 365, 3195, 293, 3195, 295, 1808, 689, 561, 300, 3212, 380, 588, 4593, 1621, 1310, 613, 51064, 51064, 366, 512, 733, 295, 5507, 5507, 27363, 420, 746, 370, 497, 307, 588, 51412, 51412, 9477, 281, 484, 23646, 370, 718, 311, 483, 3973, 295, 264, 8078, 264, 9396, 365, 2119, 9396, 264, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.1074960205581162, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.00013132780441083014}, {"id": 752, "seek": 425024, "start": 4250.24, "end": 4256.32, "text": " houses with 15 rooms or more and now you can see it's gone up from point four", "tokens": [50364, 8078, 365, 2119, 9396, 420, 544, 293, 586, 291, 393, 536, 309, 311, 2780, 493, 490, 935, 1451, 50668, 50668, 1045, 281, 935, 2309, 3180, 754, 1673, 321, 1391, 787, 658, 3973, 295, 472, 732, 1045, 50908, 50908, 1451, 1732, 2309, 754, 658, 291, 658, 3973, 295, 3407, 1412, 2793, 370, 321, 434, 516, 281, 312, 51122, 51122, 588, 5026, 295, 484, 23646, 293, 300, 1355, 498, 291, 434, 1382, 281, 1942, 257, 269, 501, 306, 51252, 51252, 6211, 689, 264, 20678, 307, 20009, 293, 291, 445, 483, 257, 1916, 51494, 51494, 295, 13241, 534, 13425, 2085, 550, 300, 311, 516, 281, 312, 257, 11293, 281, 428, 6175, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.1259876319340297, "compression_ratio": 1.7451737451737452, "no_speech_prob": 4.069265560247004e-05}, {"id": 753, "seek": 425024, "start": 4256.32, "end": 4261.12, "text": " three to point six eight even though we probably only got rid of one two three", "tokens": [50364, 8078, 365, 2119, 9396, 420, 544, 293, 586, 291, 393, 536, 309, 311, 2780, 493, 490, 935, 1451, 50668, 50668, 1045, 281, 935, 2309, 3180, 754, 1673, 321, 1391, 787, 658, 3973, 295, 472, 732, 1045, 50908, 50908, 1451, 1732, 2309, 754, 658, 291, 658, 3973, 295, 3407, 1412, 2793, 370, 321, 434, 516, 281, 312, 51122, 51122, 588, 5026, 295, 484, 23646, 293, 300, 1355, 498, 291, 434, 1382, 281, 1942, 257, 269, 501, 306, 51252, 51252, 6211, 689, 264, 20678, 307, 20009, 293, 291, 445, 483, 257, 1916, 51494, 51494, 295, 13241, 534, 13425, 2085, 550, 300, 311, 516, 281, 312, 257, 11293, 281, 428, 6175, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.1259876319340297, "compression_ratio": 1.7451737451737452, "no_speech_prob": 4.069265560247004e-05}, {"id": 754, "seek": 425024, "start": 4261.12, "end": 4265.4, "text": " four five six even got you got rid of seven data points so we're going to be", "tokens": [50364, 8078, 365, 2119, 9396, 420, 544, 293, 586, 291, 393, 536, 309, 311, 2780, 493, 490, 935, 1451, 50668, 50668, 1045, 281, 935, 2309, 3180, 754, 1673, 321, 1391, 787, 658, 3973, 295, 472, 732, 1045, 50908, 50908, 1451, 1732, 2309, 754, 658, 291, 658, 3973, 295, 3407, 1412, 2793, 370, 321, 434, 516, 281, 312, 51122, 51122, 588, 5026, 295, 484, 23646, 293, 300, 1355, 498, 291, 434, 1382, 281, 1942, 257, 269, 501, 306, 51252, 51252, 6211, 689, 264, 20678, 307, 20009, 293, 291, 445, 483, 257, 1916, 51494, 51494, 295, 13241, 534, 13425, 2085, 550, 300, 311, 516, 281, 312, 257, 11293, 281, 428, 6175, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.1259876319340297, "compression_ratio": 1.7451737451737452, "no_speech_prob": 4.069265560247004e-05}, {"id": 755, "seek": 425024, "start": 4265.4, "end": 4268.0, "text": " very careful of outliers and that means if you're trying to win a cackle", "tokens": [50364, 8078, 365, 2119, 9396, 420, 544, 293, 586, 291, 393, 536, 309, 311, 2780, 493, 490, 935, 1451, 50668, 50668, 1045, 281, 935, 2309, 3180, 754, 1673, 321, 1391, 787, 658, 3973, 295, 472, 732, 1045, 50908, 50908, 1451, 1732, 2309, 754, 658, 291, 658, 3973, 295, 3407, 1412, 2793, 370, 321, 434, 516, 281, 312, 51122, 51122, 588, 5026, 295, 484, 23646, 293, 300, 1355, 498, 291, 434, 1382, 281, 1942, 257, 269, 501, 306, 51252, 51252, 6211, 689, 264, 20678, 307, 20009, 293, 291, 445, 483, 257, 1916, 51494, 51494, 295, 13241, 534, 13425, 2085, 550, 300, 311, 516, 281, 312, 257, 11293, 281, 428, 6175, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.1259876319340297, "compression_ratio": 1.7451737451737452, "no_speech_prob": 4.069265560247004e-05}, {"id": 756, "seek": 425024, "start": 4268.0, "end": 4272.84, "text": " competition where the metric is correlation and you just get a couple", "tokens": [50364, 8078, 365, 2119, 9396, 420, 544, 293, 586, 291, 393, 536, 309, 311, 2780, 493, 490, 935, 1451, 50668, 50668, 1045, 281, 935, 2309, 3180, 754, 1673, 321, 1391, 787, 658, 3973, 295, 472, 732, 1045, 50908, 50908, 1451, 1732, 2309, 754, 658, 291, 658, 3973, 295, 3407, 1412, 2793, 370, 321, 434, 516, 281, 312, 51122, 51122, 588, 5026, 295, 484, 23646, 293, 300, 1355, 498, 291, 434, 1382, 281, 1942, 257, 269, 501, 306, 51252, 51252, 6211, 689, 264, 20678, 307, 20009, 293, 291, 445, 483, 257, 1916, 51494, 51494, 295, 13241, 534, 13425, 2085, 550, 300, 311, 516, 281, 312, 257, 11293, 281, 428, 6175, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.1259876319340297, "compression_ratio": 1.7451737451737452, "no_speech_prob": 4.069265560247004e-05}, {"id": 757, "seek": 425024, "start": 4272.84, "end": 4277.28, "text": " of rows really badly wrong then that's going to be a disaster to your score", "tokens": [50364, 8078, 365, 2119, 9396, 420, 544, 293, 586, 291, 393, 536, 309, 311, 2780, 493, 490, 935, 1451, 50668, 50668, 1045, 281, 935, 2309, 3180, 754, 1673, 321, 1391, 787, 658, 3973, 295, 472, 732, 1045, 50908, 50908, 1451, 1732, 2309, 754, 658, 291, 658, 3973, 295, 3407, 1412, 2793, 370, 321, 434, 516, 281, 312, 51122, 51122, 588, 5026, 295, 484, 23646, 293, 300, 1355, 498, 291, 434, 1382, 281, 1942, 257, 269, 501, 306, 51252, 51252, 6211, 689, 264, 20678, 307, 20009, 293, 291, 445, 483, 257, 1916, 51494, 51494, 295, 13241, 534, 13425, 2085, 550, 300, 311, 516, 281, 312, 257, 11293, 281, 428, 6175, 51716, 51716], "temperature": 0.0, "avg_logprob": -0.1259876319340297, "compression_ratio": 1.7451737451737452, "no_speech_prob": 4.069265560247004e-05}, {"id": 758, "seek": 427728, "start": 4277.28, "end": 4282.88, "text": " right so you've got to make sure that you do a pretty good job of every room", "tokens": [50364, 558, 370, 291, 600, 658, 281, 652, 988, 300, 291, 360, 257, 1238, 665, 1691, 295, 633, 1808, 50644, 50644, 370, 456, 311, 437, 257, 20009, 295, 935, 2309, 3180, 1542, 411, 1392, 510, 311, 257, 50964, 50964, 20009, 295, 935, 1045, 1451, 293, 341, 307, 733, 295, 1880, 1943, 380, 309, 570, 51150, 51150, 935, 1045, 1451, 3263, 411, 1596, 257, 665, 2480, 457, 291, 1920, 393, 380, 51414, 51414, 536, 309, 370, 341, 307, 746, 286, 10613, 3402, 307, 498, 291, 434, 1364, 365, 257, 777, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.09780289815819782, "compression_ratio": 1.7625570776255708, "no_speech_prob": 4.469061241252348e-05}, {"id": 759, "seek": 427728, "start": 4282.88, "end": 4289.28, "text": " so there's what a correlation of point six eight looks like okay here's a", "tokens": [50364, 558, 370, 291, 600, 658, 281, 652, 988, 300, 291, 360, 257, 1238, 665, 1691, 295, 633, 1808, 50644, 50644, 370, 456, 311, 437, 257, 20009, 295, 935, 2309, 3180, 1542, 411, 1392, 510, 311, 257, 50964, 50964, 20009, 295, 935, 1045, 1451, 293, 341, 307, 733, 295, 1880, 1943, 380, 309, 570, 51150, 51150, 935, 1045, 1451, 3263, 411, 1596, 257, 665, 2480, 457, 291, 1920, 393, 380, 51414, 51414, 536, 309, 370, 341, 307, 746, 286, 10613, 3402, 307, 498, 291, 434, 1364, 365, 257, 777, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.09780289815819782, "compression_ratio": 1.7625570776255708, "no_speech_prob": 4.469061241252348e-05}, {"id": 760, "seek": 427728, "start": 4289.28, "end": 4293.0, "text": " correlation of point three four and this is kind of interesting isn't it because", "tokens": [50364, 558, 370, 291, 600, 658, 281, 652, 988, 300, 291, 360, 257, 1238, 665, 1691, 295, 633, 1808, 50644, 50644, 370, 456, 311, 437, 257, 20009, 295, 935, 2309, 3180, 1542, 411, 1392, 510, 311, 257, 50964, 50964, 20009, 295, 935, 1045, 1451, 293, 341, 307, 733, 295, 1880, 1943, 380, 309, 570, 51150, 51150, 935, 1045, 1451, 3263, 411, 1596, 257, 665, 2480, 457, 291, 1920, 393, 380, 51414, 51414, 536, 309, 370, 341, 307, 746, 286, 10613, 3402, 307, 498, 291, 434, 1364, 365, 257, 777, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.09780289815819782, "compression_ratio": 1.7625570776255708, "no_speech_prob": 4.469061241252348e-05}, {"id": 761, "seek": 427728, "start": 4293.0, "end": 4298.28, "text": " point three four sounds like quite a good relationship but you almost can't", "tokens": [50364, 558, 370, 291, 600, 658, 281, 652, 988, 300, 291, 360, 257, 1238, 665, 1691, 295, 633, 1808, 50644, 50644, 370, 456, 311, 437, 257, 20009, 295, 935, 2309, 3180, 1542, 411, 1392, 510, 311, 257, 50964, 50964, 20009, 295, 935, 1045, 1451, 293, 341, 307, 733, 295, 1880, 1943, 380, 309, 570, 51150, 51150, 935, 1045, 1451, 3263, 411, 1596, 257, 665, 2480, 457, 291, 1920, 393, 380, 51414, 51414, 536, 309, 370, 341, 307, 746, 286, 10613, 3402, 307, 498, 291, 434, 1364, 365, 257, 777, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.09780289815819782, "compression_ratio": 1.7625570776255708, "no_speech_prob": 4.469061241252348e-05}, {"id": 762, "seek": 427728, "start": 4298.28, "end": 4303.88, "text": " see it so this is something I strongly suggest is if you're working with a new", "tokens": [50364, 558, 370, 291, 600, 658, 281, 652, 988, 300, 291, 360, 257, 1238, 665, 1691, 295, 633, 1808, 50644, 50644, 370, 456, 311, 437, 257, 20009, 295, 935, 2309, 3180, 1542, 411, 1392, 510, 311, 257, 50964, 50964, 20009, 295, 935, 1045, 1451, 293, 341, 307, 733, 295, 1880, 1943, 380, 309, 570, 51150, 51150, 935, 1045, 1451, 3263, 411, 1596, 257, 665, 2480, 457, 291, 1920, 393, 380, 51414, 51414, 536, 309, 370, 341, 307, 746, 286, 10613, 3402, 307, 498, 291, 434, 1364, 365, 257, 777, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.09780289815819782, "compression_ratio": 1.7625570776255708, "no_speech_prob": 4.469061241252348e-05}, {"id": 763, "seek": 430388, "start": 4303.88, "end": 4310.12, "text": " metric is draw some pictures of a few different levels of that metric to kind", "tokens": [50364, 20678, 307, 2642, 512, 5242, 295, 257, 1326, 819, 4358, 295, 300, 20678, 281, 733, 50676, 50676, 295, 853, 281, 483, 257, 841, 337, 411, 437, 775, 309, 914, 291, 458, 437, 775, 935, 2309, 50858, 50858, 574, 411, 437, 775, 935, 1045, 574, 411, 293, 370, 5220, 293, 510, 311, 364, 1365, 51138, 51138, 295, 257, 20009, 295, 3175, 935, 732, 341, 588, 4036, 3671, 13525, 1392, 370, 51596, 51596, 456, 311, 445, 544, 295, 257, 733, 295, 257, 2674, 4125, 295, 746, 286, 411, 281, 360, 562, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.10346732601042717, "compression_ratio": 1.7897196261682242, "no_speech_prob": 4.539193832897581e-05}, {"id": 764, "seek": 430388, "start": 4310.12, "end": 4313.76, "text": " of try to get a feel for like what does it mean you know what does point six", "tokens": [50364, 20678, 307, 2642, 512, 5242, 295, 257, 1326, 819, 4358, 295, 300, 20678, 281, 733, 50676, 50676, 295, 853, 281, 483, 257, 841, 337, 411, 437, 775, 309, 914, 291, 458, 437, 775, 935, 2309, 50858, 50858, 574, 411, 437, 775, 935, 1045, 574, 411, 293, 370, 5220, 293, 510, 311, 364, 1365, 51138, 51138, 295, 257, 20009, 295, 3175, 935, 732, 341, 588, 4036, 3671, 13525, 1392, 370, 51596, 51596, 456, 311, 445, 544, 295, 257, 733, 295, 257, 2674, 4125, 295, 746, 286, 411, 281, 360, 562, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.10346732601042717, "compression_ratio": 1.7897196261682242, "no_speech_prob": 4.539193832897581e-05}, {"id": 765, "seek": 430388, "start": 4313.76, "end": 4319.36, "text": " look like what does point three look like and so forth and here's an example", "tokens": [50364, 20678, 307, 2642, 512, 5242, 295, 257, 1326, 819, 4358, 295, 300, 20678, 281, 733, 50676, 50676, 295, 853, 281, 483, 257, 841, 337, 411, 437, 775, 309, 914, 291, 458, 437, 775, 935, 2309, 50858, 50858, 574, 411, 437, 775, 935, 1045, 574, 411, 293, 370, 5220, 293, 510, 311, 364, 1365, 51138, 51138, 295, 257, 20009, 295, 3175, 935, 732, 341, 588, 4036, 3671, 13525, 1392, 370, 51596, 51596, 456, 311, 445, 544, 295, 257, 733, 295, 257, 2674, 4125, 295, 746, 286, 411, 281, 360, 562, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.10346732601042717, "compression_ratio": 1.7897196261682242, "no_speech_prob": 4.539193832897581e-05}, {"id": 766, "seek": 430388, "start": 4319.36, "end": 4328.52, "text": " of a correlation of minus point two this very slight negative slope okay so", "tokens": [50364, 20678, 307, 2642, 512, 5242, 295, 257, 1326, 819, 4358, 295, 300, 20678, 281, 733, 50676, 50676, 295, 853, 281, 483, 257, 841, 337, 411, 437, 775, 309, 914, 291, 458, 437, 775, 935, 2309, 50858, 50858, 574, 411, 437, 775, 935, 1045, 574, 411, 293, 370, 5220, 293, 510, 311, 364, 1365, 51138, 51138, 295, 257, 20009, 295, 3175, 935, 732, 341, 588, 4036, 3671, 13525, 1392, 370, 51596, 51596, 456, 311, 445, 544, 295, 257, 733, 295, 257, 2674, 4125, 295, 746, 286, 411, 281, 360, 562, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.10346732601042717, "compression_ratio": 1.7897196261682242, "no_speech_prob": 4.539193832897581e-05}, {"id": 767, "seek": 430388, "start": 4328.52, "end": 4331.28, "text": " there's just more of a kind of a general tip of something I like to do when", "tokens": [50364, 20678, 307, 2642, 512, 5242, 295, 257, 1326, 819, 4358, 295, 300, 20678, 281, 733, 50676, 50676, 295, 853, 281, 483, 257, 841, 337, 411, 437, 775, 309, 914, 291, 458, 437, 775, 935, 2309, 50858, 50858, 574, 411, 437, 775, 935, 1045, 574, 411, 293, 370, 5220, 293, 510, 311, 364, 1365, 51138, 51138, 295, 257, 20009, 295, 3175, 935, 732, 341, 588, 4036, 3671, 13525, 1392, 370, 51596, 51596, 456, 311, 445, 544, 295, 257, 733, 295, 257, 2674, 4125, 295, 746, 286, 411, 281, 360, 562, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.10346732601042717, "compression_ratio": 1.7897196261682242, "no_speech_prob": 4.539193832897581e-05}, {"id": 768, "seek": 433128, "start": 4331.28, "end": 4334.5199999999995, "text": " playing with a new metric and I recommend you do as well I think we've", "tokens": [50364, 2433, 365, 257, 777, 20678, 293, 286, 2748, 291, 360, 382, 731, 286, 519, 321, 600, 50526, 50526, 586, 658, 257, 2020, 295, 437, 264, 20009, 3417, 411, 586, 291, 393, 352, 574, 493, 264, 50724, 50724, 5367, 322, 28999, 498, 291, 434, 666, 300, 733, 295, 551, 321, 643, 281, 2275, 264, 51130, 51130, 20009, 934, 1184, 30992, 339, 570, 321, 528, 281, 458, 577, 527, 3097, 307, 516, 51330, 51330, 41706, 1851, 33280, 291, 281, 2736, 257, 25890, 570, 309, 311, 516, 281, 764, 264, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.06226988153143243, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.00020983359718229622}, {"id": 769, "seek": 433128, "start": 4334.5199999999995, "end": 4338.48, "text": " now got a sense of what the correlation feels like now you can go look up the", "tokens": [50364, 2433, 365, 257, 777, 20678, 293, 286, 2748, 291, 360, 382, 731, 286, 519, 321, 600, 50526, 50526, 586, 658, 257, 2020, 295, 437, 264, 20009, 3417, 411, 586, 291, 393, 352, 574, 493, 264, 50724, 50724, 5367, 322, 28999, 498, 291, 434, 666, 300, 733, 295, 551, 321, 643, 281, 2275, 264, 51130, 51130, 20009, 934, 1184, 30992, 339, 570, 321, 528, 281, 458, 577, 527, 3097, 307, 516, 51330, 51330, 41706, 1851, 33280, 291, 281, 2736, 257, 25890, 570, 309, 311, 516, 281, 764, 264, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.06226988153143243, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.00020983359718229622}, {"id": 770, "seek": 433128, "start": 4338.48, "end": 4346.599999999999, "text": " equation on Wikipedia if you're into that kind of thing we need to report the", "tokens": [50364, 2433, 365, 257, 777, 20678, 293, 286, 2748, 291, 360, 382, 731, 286, 519, 321, 600, 50526, 50526, 586, 658, 257, 2020, 295, 437, 264, 20009, 3417, 411, 586, 291, 393, 352, 574, 493, 264, 50724, 50724, 5367, 322, 28999, 498, 291, 434, 666, 300, 733, 295, 551, 321, 643, 281, 2275, 264, 51130, 51130, 20009, 934, 1184, 30992, 339, 570, 321, 528, 281, 458, 577, 527, 3097, 307, 516, 51330, 51330, 41706, 1851, 33280, 291, 281, 2736, 257, 25890, 570, 309, 311, 516, 281, 764, 264, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.06226988153143243, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.00020983359718229622}, {"id": 771, "seek": 433128, "start": 4346.599999999999, "end": 4350.599999999999, "text": " correlation after each epoch because we want to know how our training is going", "tokens": [50364, 2433, 365, 257, 777, 20678, 293, 286, 2748, 291, 360, 382, 731, 286, 519, 321, 600, 50526, 50526, 586, 658, 257, 2020, 295, 437, 264, 20009, 3417, 411, 586, 291, 393, 352, 574, 493, 264, 50724, 50724, 5367, 322, 28999, 498, 291, 434, 666, 300, 733, 295, 551, 321, 643, 281, 2275, 264, 51130, 51130, 20009, 934, 1184, 30992, 339, 570, 321, 528, 281, 458, 577, 527, 3097, 307, 516, 51330, 51330, 41706, 1851, 33280, 291, 281, 2736, 257, 25890, 570, 309, 311, 516, 281, 764, 264, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.06226988153143243, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.00020983359718229622}, {"id": 772, "seek": 433128, "start": 4350.599999999999, "end": 4357.5599999999995, "text": " hugging face expects you to return a dictionary because it's going to use the", "tokens": [50364, 2433, 365, 257, 777, 20678, 293, 286, 2748, 291, 360, 382, 731, 286, 519, 321, 600, 50526, 50526, 586, 658, 257, 2020, 295, 437, 264, 20009, 3417, 411, 586, 291, 393, 352, 574, 493, 264, 50724, 50724, 5367, 322, 28999, 498, 291, 434, 666, 300, 733, 295, 551, 321, 643, 281, 2275, 264, 51130, 51130, 20009, 934, 1184, 30992, 339, 570, 321, 528, 281, 458, 577, 527, 3097, 307, 516, 51330, 51330, 41706, 1851, 33280, 291, 281, 2736, 257, 25890, 570, 309, 311, 516, 281, 764, 264, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.06226988153143243, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.00020983359718229622}, {"id": 773, "seek": 435756, "start": 4357.56, "end": 4362.400000000001, "text": " keys of the dictionary to like label each metric so here's something that", "tokens": [50364, 9317, 295, 264, 25890, 281, 411, 7645, 1184, 20678, 370, 510, 311, 746, 300, 50606, 50606, 2170, 264, 20009, 293, 11247, 309, 382, 257, 25890, 365, 264, 7645, 39041, 1392, 51016, 51016, 370, 321, 600, 1096, 16367, 321, 600, 1096, 527, 3097, 24071, 7472, 1954, 321, 1062, 51400, 51400, 362, 767, 30193, 670, 264, 857, 689, 321, 767, 630, 264, 7472, 965, 286, 630, 370, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.07579855987991112, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.759343811310828e-05}, {"id": 774, "seek": 435756, "start": 4362.400000000001, "end": 4370.6, "text": " gets the correlation and returns it as a dictionary with the label Pearson okay", "tokens": [50364, 9317, 295, 264, 25890, 281, 411, 7645, 1184, 20678, 370, 510, 311, 746, 300, 50606, 50606, 2170, 264, 20009, 293, 11247, 309, 382, 257, 25890, 365, 264, 7645, 39041, 1392, 51016, 51016, 370, 321, 600, 1096, 16367, 321, 600, 1096, 527, 3097, 24071, 7472, 1954, 321, 1062, 51400, 51400, 362, 767, 30193, 670, 264, 857, 689, 321, 767, 630, 264, 7472, 965, 286, 630, 370, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.07579855987991112, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.759343811310828e-05}, {"id": 775, "seek": 435756, "start": 4370.6, "end": 4378.280000000001, "text": " so we've done metrics we've done our training validation split oh we might", "tokens": [50364, 9317, 295, 264, 25890, 281, 411, 7645, 1184, 20678, 370, 510, 311, 746, 300, 50606, 50606, 2170, 264, 20009, 293, 11247, 309, 382, 257, 25890, 365, 264, 7645, 39041, 1392, 51016, 51016, 370, 321, 600, 1096, 16367, 321, 600, 1096, 527, 3097, 24071, 7472, 1954, 321, 1062, 51400, 51400, 362, 767, 30193, 670, 264, 857, 689, 321, 767, 630, 264, 7472, 965, 286, 630, 370, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.07579855987991112, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.759343811310828e-05}, {"id": 776, "seek": 435756, "start": 4378.280000000001, "end": 4383.72, "text": " have actually skipped over the bit where we actually did the split today I did so", "tokens": [50364, 9317, 295, 264, 25890, 281, 411, 7645, 1184, 20678, 370, 510, 311, 746, 300, 50606, 50606, 2170, 264, 20009, 293, 11247, 309, 382, 257, 25890, 365, 264, 7645, 39041, 1392, 51016, 51016, 370, 321, 600, 1096, 16367, 321, 600, 1096, 527, 3097, 24071, 7472, 1954, 321, 1062, 51400, 51400, 362, 767, 30193, 670, 264, 857, 689, 321, 767, 630, 264, 7472, 965, 286, 630, 370, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.07579855987991112, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.759343811310828e-05}, {"id": 777, "seek": 438372, "start": 4383.72, "end": 4391.52, "text": " to actually do the split because in this Kaggle competition I've got another", "tokens": [50364, 281, 767, 360, 264, 7472, 570, 294, 341, 48751, 22631, 6211, 286, 600, 658, 1071, 50754, 50754, 21060, 321, 603, 574, 412, 1780, 689, 321, 767, 7472, 341, 6108, 457, 510, 50954, 50954, 321, 434, 445, 516, 281, 360, 257, 4974, 7472, 445, 281, 1066, 721, 2199, 337, 586, 295, 51154, 51154, 3552, 4, 486, 312, 295, 264, 1412, 486, 312, 257, 24071, 992, 370, 498, 321, 352, 15816, 3847, 1500, 51486, 51486], "temperature": 0.0, "avg_logprob": -0.13581244150797525, "compression_ratio": 1.5538461538461539, "no_speech_prob": 6.0135465901112184e-05}, {"id": 778, "seek": 438372, "start": 4391.52, "end": 4395.52, "text": " notebook we'll look at later where we actually split this properly but here", "tokens": [50364, 281, 767, 360, 264, 7472, 570, 294, 341, 48751, 22631, 6211, 286, 600, 658, 1071, 50754, 50754, 21060, 321, 603, 574, 412, 1780, 689, 321, 767, 7472, 341, 6108, 457, 510, 50954, 50954, 321, 434, 445, 516, 281, 360, 257, 4974, 7472, 445, 281, 1066, 721, 2199, 337, 586, 295, 51154, 51154, 3552, 4, 486, 312, 295, 264, 1412, 486, 312, 257, 24071, 992, 370, 498, 321, 352, 15816, 3847, 1500, 51486, 51486], "temperature": 0.0, "avg_logprob": -0.13581244150797525, "compression_ratio": 1.5538461538461539, "no_speech_prob": 6.0135465901112184e-05}, {"id": 779, "seek": 438372, "start": 4395.52, "end": 4399.52, "text": " we're just going to do a random split just to keep things simple for now of", "tokens": [50364, 281, 767, 360, 264, 7472, 570, 294, 341, 48751, 22631, 6211, 286, 600, 658, 1071, 50754, 50754, 21060, 321, 603, 574, 412, 1780, 689, 321, 767, 7472, 341, 6108, 457, 510, 50954, 50954, 321, 434, 445, 516, 281, 360, 257, 4974, 7472, 445, 281, 1066, 721, 2199, 337, 586, 295, 51154, 51154, 3552, 4, 486, 312, 295, 264, 1412, 486, 312, 257, 24071, 992, 370, 498, 321, 352, 15816, 3847, 1500, 51486, 51486], "temperature": 0.0, "avg_logprob": -0.13581244150797525, "compression_ratio": 1.5538461538461539, "no_speech_prob": 6.0135465901112184e-05}, {"id": 780, "seek": 438372, "start": 4399.52, "end": 4406.16, "text": " 25% will be of the data will be a validation set so if we go DS train test", "tokens": [50364, 281, 767, 360, 264, 7472, 570, 294, 341, 48751, 22631, 6211, 286, 600, 658, 1071, 50754, 50754, 21060, 321, 603, 574, 412, 1780, 689, 321, 767, 7472, 341, 6108, 457, 510, 50954, 50954, 321, 434, 445, 516, 281, 360, 257, 4974, 7472, 445, 281, 1066, 721, 2199, 337, 586, 295, 51154, 51154, 3552, 4, 486, 312, 295, 264, 1412, 486, 312, 257, 24071, 992, 370, 498, 321, 352, 15816, 3847, 1500, 51486, 51486], "temperature": 0.0, "avg_logprob": -0.13581244150797525, "compression_ratio": 1.5538461538461539, "no_speech_prob": 6.0135465901112184e-05}, {"id": 781, "seek": 440616, "start": 4406.16, "end": 4414.24, "text": " split it returns a data set dict which has a train and a test so that looks a", "tokens": [50364, 7472, 309, 11247, 257, 1412, 992, 12569, 597, 575, 257, 3847, 293, 257, 1500, 370, 300, 1542, 257, 50768, 50768, 688, 411, 257, 1412, 6352, 2657, 294, 2370, 7318, 588, 2531, 1558, 370, 341, 486, 312, 264, 51196, 51196, 551, 300, 321, 603, 312, 1075, 281, 3847, 365, 370, 309, 311, 516, 281, 3847, 365, 341, 1412, 992, 51372, 51372, 293, 2736, 264, 16367, 322, 341, 1412, 992, 341, 307, 534, 257, 24071, 992, 457, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.08393909357771089, "compression_ratio": 1.7771428571428571, "no_speech_prob": 4.33157583756838e-05}, {"id": 782, "seek": 440616, "start": 4414.24, "end": 4422.8, "text": " lot like a data sets object in fast AI very similar idea so this will be the", "tokens": [50364, 7472, 309, 11247, 257, 1412, 992, 12569, 597, 575, 257, 3847, 293, 257, 1500, 370, 300, 1542, 257, 50768, 50768, 688, 411, 257, 1412, 6352, 2657, 294, 2370, 7318, 588, 2531, 1558, 370, 341, 486, 312, 264, 51196, 51196, 551, 300, 321, 603, 312, 1075, 281, 3847, 365, 370, 309, 311, 516, 281, 3847, 365, 341, 1412, 992, 51372, 51372, 293, 2736, 264, 16367, 322, 341, 1412, 992, 341, 307, 534, 257, 24071, 992, 457, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.08393909357771089, "compression_ratio": 1.7771428571428571, "no_speech_prob": 4.33157583756838e-05}, {"id": 783, "seek": 440616, "start": 4422.8, "end": 4426.32, "text": " thing that we'll be able to train with so it's going to train with this data set", "tokens": [50364, 7472, 309, 11247, 257, 1412, 992, 12569, 597, 575, 257, 3847, 293, 257, 1500, 370, 300, 1542, 257, 50768, 50768, 688, 411, 257, 1412, 6352, 2657, 294, 2370, 7318, 588, 2531, 1558, 370, 341, 486, 312, 264, 51196, 51196, 551, 300, 321, 603, 312, 1075, 281, 3847, 365, 370, 309, 311, 516, 281, 3847, 365, 341, 1412, 992, 51372, 51372, 293, 2736, 264, 16367, 322, 341, 1412, 992, 341, 307, 534, 257, 24071, 992, 457, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.08393909357771089, "compression_ratio": 1.7771428571428571, "no_speech_prob": 4.33157583756838e-05}, {"id": 784, "seek": 440616, "start": 4426.32, "end": 4432.88, "text": " and return the metrics on this data set this is really a validation set but", "tokens": [50364, 7472, 309, 11247, 257, 1412, 992, 12569, 597, 575, 257, 3847, 293, 257, 1500, 370, 300, 1542, 257, 50768, 50768, 688, 411, 257, 1412, 6352, 2657, 294, 2370, 7318, 588, 2531, 1558, 370, 341, 486, 312, 264, 51196, 51196, 551, 300, 321, 603, 312, 1075, 281, 3847, 365, 370, 309, 311, 516, 281, 3847, 365, 341, 1412, 992, 51372, 51372, 293, 2736, 264, 16367, 322, 341, 1412, 992, 341, 307, 534, 257, 24071, 992, 457, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.08393909357771089, "compression_ratio": 1.7771428571428571, "no_speech_prob": 4.33157583756838e-05}, {"id": 785, "seek": 443288, "start": 4432.88, "end": 4442.8, "text": " hugging face data sets calls it test okay we're now ready to train our model", "tokens": [50364, 41706, 1851, 1412, 6352, 5498, 309, 1500, 1392, 321, 434, 586, 1919, 281, 3847, 527, 2316, 50860, 50860, 294, 2370, 7318, 321, 764, 746, 1219, 257, 33347, 264, 10344, 294, 41706, 1851, 51152, 51152, 4088, 433, 307, 1219, 21110, 370, 321, 603, 1565, 300, 294, 746, 321, 603, 1466, 51462, 51462, 466, 1596, 13392, 307, 264, 1558, 295, 867, 15245, 279, 293, 15245, 11602, 294, 2099, 1184, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.11989174426441461, "compression_ratio": 1.6451612903225807, "no_speech_prob": 5.224613050813787e-05}, {"id": 786, "seek": 443288, "start": 4442.8, "end": 4448.64, "text": " in fast AI we use something called a learner the equivalent in hugging face", "tokens": [50364, 41706, 1851, 1412, 6352, 5498, 309, 1500, 1392, 321, 434, 586, 1919, 281, 3847, 527, 2316, 50860, 50860, 294, 2370, 7318, 321, 764, 746, 1219, 257, 33347, 264, 10344, 294, 41706, 1851, 51152, 51152, 4088, 433, 307, 1219, 21110, 370, 321, 603, 1565, 300, 294, 746, 321, 603, 1466, 51462, 51462, 466, 1596, 13392, 307, 264, 1558, 295, 867, 15245, 279, 293, 15245, 11602, 294, 2099, 1184, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.11989174426441461, "compression_ratio": 1.6451612903225807, "no_speech_prob": 5.224613050813787e-05}, {"id": 787, "seek": 443288, "start": 4448.64, "end": 4454.84, "text": " transformers is called trainer so we'll bring that in something we'll learn", "tokens": [50364, 41706, 1851, 1412, 6352, 5498, 309, 1500, 1392, 321, 434, 586, 1919, 281, 3847, 527, 2316, 50860, 50860, 294, 2370, 7318, 321, 764, 746, 1219, 257, 33347, 264, 10344, 294, 41706, 1851, 51152, 51152, 4088, 433, 307, 1219, 21110, 370, 321, 603, 1565, 300, 294, 746, 321, 603, 1466, 51462, 51462, 466, 1596, 13392, 307, 264, 1558, 295, 867, 15245, 279, 293, 15245, 11602, 294, 2099, 1184, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.11989174426441461, "compression_ratio": 1.6451612903225807, "no_speech_prob": 5.224613050813787e-05}, {"id": 788, "seek": 443288, "start": 4454.84, "end": 4461.400000000001, "text": " about quite shortly is the idea of many batches and batch sizes in short each", "tokens": [50364, 41706, 1851, 1412, 6352, 5498, 309, 1500, 1392, 321, 434, 586, 1919, 281, 3847, 527, 2316, 50860, 50860, 294, 2370, 7318, 321, 764, 746, 1219, 257, 33347, 264, 10344, 294, 41706, 1851, 51152, 51152, 4088, 433, 307, 1219, 21110, 370, 321, 603, 1565, 300, 294, 746, 321, 603, 1466, 51462, 51462, 466, 1596, 13392, 307, 264, 1558, 295, 867, 15245, 279, 293, 15245, 11602, 294, 2099, 1184, 51790, 51790], "temperature": 0.0, "avg_logprob": -0.11989174426441461, "compression_ratio": 1.6451612903225807, "no_speech_prob": 5.224613050813787e-05}, {"id": 789, "seek": 446140, "start": 4461.4, "end": 4465.16, "text": " time we pass some data to our model for training it's going to return it's going", "tokens": [50364, 565, 321, 1320, 512, 1412, 281, 527, 2316, 337, 3097, 309, 311, 516, 281, 2736, 309, 311, 516, 50552, 50552, 281, 2845, 807, 257, 1326, 13241, 412, 257, 565, 281, 264, 18407, 370, 300, 309, 393, 8873, 729, 50810, 50810, 294, 8952, 729, 3840, 295, 13241, 307, 1219, 257, 15245, 420, 257, 8382, 15245, 293, 264, 51142, 51142, 1230, 295, 13241, 307, 1219, 264, 15245, 2744, 370, 510, 321, 434, 516, 281, 992, 264, 15245, 51352, 51352, 2744, 281, 29810, 5101, 4124, 264, 4833, 428, 15245, 2744, 264, 544, 309, 393, 51586, 51586, 360, 294, 8952, 412, 1564, 293, 309, 603, 312, 4663, 457, 498, 291, 652, 309, 886, 955, 291, 434, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.08674320681341763, "compression_ratio": 1.844, "no_speech_prob": 0.0001313129614572972}, {"id": 790, "seek": 446140, "start": 4465.16, "end": 4470.32, "text": " to send through a few rows at a time to the GPU so that it can calculate those", "tokens": [50364, 565, 321, 1320, 512, 1412, 281, 527, 2316, 337, 3097, 309, 311, 516, 281, 2736, 309, 311, 516, 50552, 50552, 281, 2845, 807, 257, 1326, 13241, 412, 257, 565, 281, 264, 18407, 370, 300, 309, 393, 8873, 729, 50810, 50810, 294, 8952, 729, 3840, 295, 13241, 307, 1219, 257, 15245, 420, 257, 8382, 15245, 293, 264, 51142, 51142, 1230, 295, 13241, 307, 1219, 264, 15245, 2744, 370, 510, 321, 434, 516, 281, 992, 264, 15245, 51352, 51352, 2744, 281, 29810, 5101, 4124, 264, 4833, 428, 15245, 2744, 264, 544, 309, 393, 51586, 51586, 360, 294, 8952, 412, 1564, 293, 309, 603, 312, 4663, 457, 498, 291, 652, 309, 886, 955, 291, 434, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.08674320681341763, "compression_ratio": 1.844, "no_speech_prob": 0.0001313129614572972}, {"id": 791, "seek": 446140, "start": 4470.32, "end": 4476.96, "text": " in parallel those bunch of rows is called a batch or a mini batch and the", "tokens": [50364, 565, 321, 1320, 512, 1412, 281, 527, 2316, 337, 3097, 309, 311, 516, 281, 2736, 309, 311, 516, 50552, 50552, 281, 2845, 807, 257, 1326, 13241, 412, 257, 565, 281, 264, 18407, 370, 300, 309, 393, 8873, 729, 50810, 50810, 294, 8952, 729, 3840, 295, 13241, 307, 1219, 257, 15245, 420, 257, 8382, 15245, 293, 264, 51142, 51142, 1230, 295, 13241, 307, 1219, 264, 15245, 2744, 370, 510, 321, 434, 516, 281, 992, 264, 15245, 51352, 51352, 2744, 281, 29810, 5101, 4124, 264, 4833, 428, 15245, 2744, 264, 544, 309, 393, 51586, 51586, 360, 294, 8952, 412, 1564, 293, 309, 603, 312, 4663, 457, 498, 291, 652, 309, 886, 955, 291, 434, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.08674320681341763, "compression_ratio": 1.844, "no_speech_prob": 0.0001313129614572972}, {"id": 792, "seek": 446140, "start": 4476.96, "end": 4481.16, "text": " number of rows is called the batch size so here we're going to set the batch", "tokens": [50364, 565, 321, 1320, 512, 1412, 281, 527, 2316, 337, 3097, 309, 311, 516, 281, 2736, 309, 311, 516, 50552, 50552, 281, 2845, 807, 257, 1326, 13241, 412, 257, 565, 281, 264, 18407, 370, 300, 309, 393, 8873, 729, 50810, 50810, 294, 8952, 729, 3840, 295, 13241, 307, 1219, 257, 15245, 420, 257, 8382, 15245, 293, 264, 51142, 51142, 1230, 295, 13241, 307, 1219, 264, 15245, 2744, 370, 510, 321, 434, 516, 281, 992, 264, 15245, 51352, 51352, 2744, 281, 29810, 5101, 4124, 264, 4833, 428, 15245, 2744, 264, 544, 309, 393, 51586, 51586, 360, 294, 8952, 412, 1564, 293, 309, 603, 312, 4663, 457, 498, 291, 652, 309, 886, 955, 291, 434, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.08674320681341763, "compression_ratio": 1.844, "no_speech_prob": 0.0001313129614572972}, {"id": 793, "seek": 446140, "start": 4481.16, "end": 4485.839999999999, "text": " size to 128 generally speaking the larger your batch size the more it can", "tokens": [50364, 565, 321, 1320, 512, 1412, 281, 527, 2316, 337, 3097, 309, 311, 516, 281, 2736, 309, 311, 516, 50552, 50552, 281, 2845, 807, 257, 1326, 13241, 412, 257, 565, 281, 264, 18407, 370, 300, 309, 393, 8873, 729, 50810, 50810, 294, 8952, 729, 3840, 295, 13241, 307, 1219, 257, 15245, 420, 257, 8382, 15245, 293, 264, 51142, 51142, 1230, 295, 13241, 307, 1219, 264, 15245, 2744, 370, 510, 321, 434, 516, 281, 992, 264, 15245, 51352, 51352, 2744, 281, 29810, 5101, 4124, 264, 4833, 428, 15245, 2744, 264, 544, 309, 393, 51586, 51586, 360, 294, 8952, 412, 1564, 293, 309, 603, 312, 4663, 457, 498, 291, 652, 309, 886, 955, 291, 434, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.08674320681341763, "compression_ratio": 1.844, "no_speech_prob": 0.0001313129614572972}, {"id": 794, "seek": 446140, "start": 4485.839999999999, "end": 4490.799999999999, "text": " do in parallel at once and it'll be faster but if you make it too big you're", "tokens": [50364, 565, 321, 1320, 512, 1412, 281, 527, 2316, 337, 3097, 309, 311, 516, 281, 2736, 309, 311, 516, 50552, 50552, 281, 2845, 807, 257, 1326, 13241, 412, 257, 565, 281, 264, 18407, 370, 300, 309, 393, 8873, 729, 50810, 50810, 294, 8952, 729, 3840, 295, 13241, 307, 1219, 257, 15245, 420, 257, 8382, 15245, 293, 264, 51142, 51142, 1230, 295, 13241, 307, 1219, 264, 15245, 2744, 370, 510, 321, 434, 516, 281, 992, 264, 15245, 51352, 51352, 2744, 281, 29810, 5101, 4124, 264, 4833, 428, 15245, 2744, 264, 544, 309, 393, 51586, 51586, 360, 294, 8952, 412, 1564, 293, 309, 603, 312, 4663, 457, 498, 291, 652, 309, 886, 955, 291, 434, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.08674320681341763, "compression_ratio": 1.844, "no_speech_prob": 0.0001313129614572972}, {"id": 795, "seek": 449080, "start": 4490.8, "end": 4495.92, "text": " going to out of memory error on your GPU so you know it's a bit of trial and error", "tokens": [50364, 516, 281, 484, 295, 4675, 6713, 322, 428, 18407, 370, 291, 458, 309, 311, 257, 857, 295, 7308, 293, 6713, 50620, 50620, 281, 915, 257, 15245, 2744, 300, 1985, 30992, 28346, 321, 600, 1612, 949, 550, 321, 600, 658, 264, 50954, 50954, 2539, 3314, 321, 603, 751, 294, 264, 958, 6898, 5969, 321, 483, 281, 341, 6898, 51292, 51292, 466, 257, 6532, 281, 6772, 915, 257, 420, 12909, 6772, 915, 257, 665, 51520, 51520, 2539, 3314, 321, 1217, 458, 437, 257, 2539, 3314, 307, 490, 264, 1036, 6898, 286, 51662, 51662], "temperature": 0.0, "avg_logprob": -0.14054781390774634, "compression_ratio": 1.8028169014084507, "no_speech_prob": 2.6684969270718284e-05}, {"id": 796, "seek": 449080, "start": 4495.92, "end": 4502.6, "text": " to find a batch size that works epochs we've seen before then we've got the", "tokens": [50364, 516, 281, 484, 295, 4675, 6713, 322, 428, 18407, 370, 291, 458, 309, 311, 257, 857, 295, 7308, 293, 6713, 50620, 50620, 281, 915, 257, 15245, 2744, 300, 1985, 30992, 28346, 321, 600, 1612, 949, 550, 321, 600, 658, 264, 50954, 50954, 2539, 3314, 321, 603, 751, 294, 264, 958, 6898, 5969, 321, 483, 281, 341, 6898, 51292, 51292, 466, 257, 6532, 281, 6772, 915, 257, 420, 12909, 6772, 915, 257, 665, 51520, 51520, 2539, 3314, 321, 1217, 458, 437, 257, 2539, 3314, 307, 490, 264, 1036, 6898, 286, 51662, 51662], "temperature": 0.0, "avg_logprob": -0.14054781390774634, "compression_ratio": 1.8028169014084507, "no_speech_prob": 2.6684969270718284e-05}, {"id": 797, "seek": 449080, "start": 4502.6, "end": 4509.360000000001, "text": " learning rate we'll talk in the next lesson unless we get to this lesson", "tokens": [50364, 516, 281, 484, 295, 4675, 6713, 322, 428, 18407, 370, 291, 458, 309, 311, 257, 857, 295, 7308, 293, 6713, 50620, 50620, 281, 915, 257, 15245, 2744, 300, 1985, 30992, 28346, 321, 600, 1612, 949, 550, 321, 600, 658, 264, 50954, 50954, 2539, 3314, 321, 603, 751, 294, 264, 958, 6898, 5969, 321, 483, 281, 341, 6898, 51292, 51292, 466, 257, 6532, 281, 6772, 915, 257, 420, 12909, 6772, 915, 257, 665, 51520, 51520, 2539, 3314, 321, 1217, 458, 437, 257, 2539, 3314, 307, 490, 264, 1036, 6898, 286, 51662, 51662], "temperature": 0.0, "avg_logprob": -0.14054781390774634, "compression_ratio": 1.8028169014084507, "no_speech_prob": 2.6684969270718284e-05}, {"id": 798, "seek": 449080, "start": 4509.360000000001, "end": 4513.92, "text": " about a technique to automatically find a or semi automatically find a good", "tokens": [50364, 516, 281, 484, 295, 4675, 6713, 322, 428, 18407, 370, 291, 458, 309, 311, 257, 857, 295, 7308, 293, 6713, 50620, 50620, 281, 915, 257, 15245, 2744, 300, 1985, 30992, 28346, 321, 600, 1612, 949, 550, 321, 600, 658, 264, 50954, 50954, 2539, 3314, 321, 603, 751, 294, 264, 958, 6898, 5969, 321, 483, 281, 341, 6898, 51292, 51292, 466, 257, 6532, 281, 6772, 915, 257, 420, 12909, 6772, 915, 257, 665, 51520, 51520, 2539, 3314, 321, 1217, 458, 437, 257, 2539, 3314, 307, 490, 264, 1036, 6898, 286, 51662, 51662], "temperature": 0.0, "avg_logprob": -0.14054781390774634, "compression_ratio": 1.8028169014084507, "no_speech_prob": 2.6684969270718284e-05}, {"id": 799, "seek": 449080, "start": 4513.92, "end": 4516.76, "text": " learning rate we already know what a learning rate is from the last lesson I", "tokens": [50364, 516, 281, 484, 295, 4675, 6713, 322, 428, 18407, 370, 291, 458, 309, 311, 257, 857, 295, 7308, 293, 6713, 50620, 50620, 281, 915, 257, 15245, 2744, 300, 1985, 30992, 28346, 321, 600, 1612, 949, 550, 321, 600, 658, 264, 50954, 50954, 2539, 3314, 321, 603, 751, 294, 264, 958, 6898, 5969, 321, 483, 281, 341, 6898, 51292, 51292, 466, 257, 6532, 281, 6772, 915, 257, 420, 12909, 6772, 915, 257, 665, 51520, 51520, 2539, 3314, 321, 1217, 458, 437, 257, 2539, 3314, 307, 490, 264, 1036, 6898, 286, 51662, 51662], "temperature": 0.0, "avg_logprob": -0.14054781390774634, "compression_ratio": 1.8028169014084507, "no_speech_prob": 2.6684969270718284e-05}, {"id": 800, "seek": 451676, "start": 4516.76, "end": 4520.96, "text": " played around and found one that seems to train quite quickly without falling", "tokens": [50364, 3737, 926, 293, 1352, 472, 300, 2544, 281, 3847, 1596, 2661, 1553, 7440, 50574, 50574, 4936, 370, 286, 445, 3031, 257, 1326, 5101, 286, 733, 295, 291, 458, 498, 286, 498, 286, 500, 380, 362, 257, 50970, 50970, 370, 31422, 1851, 4088, 433, 1177, 380, 362, 746, 281, 854, 291, 915, 264, 51234, 51234, 2539, 3314, 341, 264, 10980, 321, 434, 884, 294, 2370, 7318, 486, 718, 291, 360, 300, 51452, 51452, 457, 498, 291, 434, 1228, 257, 8388, 300, 1177, 380, 362, 300, 291, 393, 445, 722, 51618, 51618, 365, 257, 534, 2295, 2614, 3314, 293, 550, 733, 295, 3834, 309, 293, 1066, 33651, 309, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.13666384436867454, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.00010551040759310126}, {"id": 801, "seek": 451676, "start": 4520.96, "end": 4528.88, "text": " apart so I just tried a few generally I kind of you know if I if I don't have a", "tokens": [50364, 3737, 926, 293, 1352, 472, 300, 2544, 281, 3847, 1596, 2661, 1553, 7440, 50574, 50574, 4936, 370, 286, 445, 3031, 257, 1326, 5101, 286, 733, 295, 291, 458, 498, 286, 498, 286, 500, 380, 362, 257, 50970, 50970, 370, 31422, 1851, 4088, 433, 1177, 380, 362, 746, 281, 854, 291, 915, 264, 51234, 51234, 2539, 3314, 341, 264, 10980, 321, 434, 884, 294, 2370, 7318, 486, 718, 291, 360, 300, 51452, 51452, 457, 498, 291, 434, 1228, 257, 8388, 300, 1177, 380, 362, 300, 291, 393, 445, 722, 51618, 51618, 365, 257, 534, 2295, 2614, 3314, 293, 550, 733, 295, 3834, 309, 293, 1066, 33651, 309, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.13666384436867454, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.00010551040759310126}, {"id": 802, "seek": 451676, "start": 4528.88, "end": 4534.16, "text": " so hacking face transformers doesn't have something to help you find the", "tokens": [50364, 3737, 926, 293, 1352, 472, 300, 2544, 281, 3847, 1596, 2661, 1553, 7440, 50574, 50574, 4936, 370, 286, 445, 3031, 257, 1326, 5101, 286, 733, 295, 291, 458, 498, 286, 498, 286, 500, 380, 362, 257, 50970, 50970, 370, 31422, 1851, 4088, 433, 1177, 380, 362, 746, 281, 854, 291, 915, 264, 51234, 51234, 2539, 3314, 341, 264, 10980, 321, 434, 884, 294, 2370, 7318, 486, 718, 291, 360, 300, 51452, 51452, 457, 498, 291, 434, 1228, 257, 8388, 300, 1177, 380, 362, 300, 291, 393, 445, 722, 51618, 51618, 365, 257, 534, 2295, 2614, 3314, 293, 550, 733, 295, 3834, 309, 293, 1066, 33651, 309, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.13666384436867454, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.00010551040759310126}, {"id": 803, "seek": 451676, "start": 4534.16, "end": 4538.52, "text": " learning rate this the integration we're doing in fast AI will let you do that", "tokens": [50364, 3737, 926, 293, 1352, 472, 300, 2544, 281, 3847, 1596, 2661, 1553, 7440, 50574, 50574, 4936, 370, 286, 445, 3031, 257, 1326, 5101, 286, 733, 295, 291, 458, 498, 286, 498, 286, 500, 380, 362, 257, 50970, 50970, 370, 31422, 1851, 4088, 433, 1177, 380, 362, 746, 281, 854, 291, 915, 264, 51234, 51234, 2539, 3314, 341, 264, 10980, 321, 434, 884, 294, 2370, 7318, 486, 718, 291, 360, 300, 51452, 51452, 457, 498, 291, 434, 1228, 257, 8388, 300, 1177, 380, 362, 300, 291, 393, 445, 722, 51618, 51618, 365, 257, 534, 2295, 2614, 3314, 293, 550, 733, 295, 3834, 309, 293, 1066, 33651, 309, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.13666384436867454, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.00010551040759310126}, {"id": 804, "seek": 451676, "start": 4538.52, "end": 4541.84, "text": " but if you're using a framework that doesn't have that you can just start", "tokens": [50364, 3737, 926, 293, 1352, 472, 300, 2544, 281, 3847, 1596, 2661, 1553, 7440, 50574, 50574, 4936, 370, 286, 445, 3031, 257, 1326, 5101, 286, 733, 295, 291, 458, 498, 286, 498, 286, 500, 380, 362, 257, 50970, 50970, 370, 31422, 1851, 4088, 433, 1177, 380, 362, 746, 281, 854, 291, 915, 264, 51234, 51234, 2539, 3314, 341, 264, 10980, 321, 434, 884, 294, 2370, 7318, 486, 718, 291, 360, 300, 51452, 51452, 457, 498, 291, 434, 1228, 257, 8388, 300, 1177, 380, 362, 300, 291, 393, 445, 722, 51618, 51618, 365, 257, 534, 2295, 2614, 3314, 293, 550, 733, 295, 3834, 309, 293, 1066, 33651, 309, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.13666384436867454, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.00010551040759310126}, {"id": 805, "seek": 451676, "start": 4541.84, "end": 4545.84, "text": " with a really low running rate and then kind of double it and keep doubling it", "tokens": [50364, 3737, 926, 293, 1352, 472, 300, 2544, 281, 3847, 1596, 2661, 1553, 7440, 50574, 50574, 4936, 370, 286, 445, 3031, 257, 1326, 5101, 286, 733, 295, 291, 458, 498, 286, 498, 286, 500, 380, 362, 257, 50970, 50970, 370, 31422, 1851, 4088, 433, 1177, 380, 362, 746, 281, 854, 291, 915, 264, 51234, 51234, 2539, 3314, 341, 264, 10980, 321, 434, 884, 294, 2370, 7318, 486, 718, 291, 360, 300, 51452, 51452, 457, 498, 291, 434, 1228, 257, 8388, 300, 1177, 380, 362, 300, 291, 393, 445, 722, 51618, 51618, 365, 257, 534, 2295, 2614, 3314, 293, 550, 733, 295, 3834, 309, 293, 1066, 33651, 309, 51818, 51818], "temperature": 0.0, "avg_logprob": -0.13666384436867454, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.00010551040759310126}, {"id": 806, "seek": 454584, "start": 4545.84, "end": 4553.360000000001, "text": " until it falls apart hacking face transformers uses this thing called", "tokens": [50364, 1826, 309, 8804, 4936, 31422, 1851, 4088, 433, 4960, 341, 551, 1219, 50740, 50740, 3097, 12869, 597, 307, 257, 1508, 321, 445, 2893, 439, 295, 264, 733, 295, 50896, 50896, 11694, 370, 291, 362, 281, 980, 309, 437, 428, 2539, 3314, 307, 341, 1507, 51364, 51364, 510, 307, 264, 912, 382, 437, 321, 818, 1936, 3318, 472, 6586, 294, 2370, 7318, 291, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.09926548871127042, "compression_ratio": 1.5769230769230769, "no_speech_prob": 5.388888530433178e-05}, {"id": 807, "seek": 454584, "start": 4553.360000000001, "end": 4556.4800000000005, "text": " training arguments which is a class we just provide all of the kind of", "tokens": [50364, 1826, 309, 8804, 4936, 31422, 1851, 4088, 433, 4960, 341, 551, 1219, 50740, 50740, 3097, 12869, 597, 307, 257, 1508, 321, 445, 2893, 439, 295, 264, 733, 295, 50896, 50896, 11694, 370, 291, 362, 281, 980, 309, 437, 428, 2539, 3314, 307, 341, 1507, 51364, 51364, 510, 307, 264, 912, 382, 437, 321, 818, 1936, 3318, 472, 6586, 294, 2370, 7318, 291, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.09926548871127042, "compression_ratio": 1.5769230769230769, "no_speech_prob": 5.388888530433178e-05}, {"id": 808, "seek": 454584, "start": 4556.4800000000005, "end": 4565.84, "text": " configuration so you have to tell it what your learning rate is this stuff", "tokens": [50364, 1826, 309, 8804, 4936, 31422, 1851, 4088, 433, 4960, 341, 551, 1219, 50740, 50740, 3097, 12869, 597, 307, 257, 1508, 321, 445, 2893, 439, 295, 264, 733, 295, 50896, 50896, 11694, 370, 291, 362, 281, 980, 309, 437, 428, 2539, 3314, 307, 341, 1507, 51364, 51364, 510, 307, 264, 912, 382, 437, 321, 818, 1936, 3318, 472, 6586, 294, 2370, 7318, 291, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.09926548871127042, "compression_ratio": 1.5769230769230769, "no_speech_prob": 5.388888530433178e-05}, {"id": 809, "seek": 454584, "start": 4565.84, "end": 4571.0, "text": " here is the same as what we call basically fit one cycle in fast AI you", "tokens": [50364, 1826, 309, 8804, 4936, 31422, 1851, 4088, 433, 4960, 341, 551, 1219, 50740, 50740, 3097, 12869, 597, 307, 257, 1508, 321, 445, 2893, 439, 295, 264, 733, 295, 50896, 50896, 11694, 370, 291, 362, 281, 980, 309, 437, 428, 2539, 3314, 307, 341, 1507, 51364, 51364, 510, 307, 264, 912, 382, 437, 321, 818, 1936, 3318, 472, 6586, 294, 2370, 7318, 291, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.09926548871127042, "compression_ratio": 1.5769230769230769, "no_speech_prob": 5.388888530433178e-05}, {"id": 810, "seek": 457100, "start": 4571.0, "end": 4576.52, "text": " always want this to be true because it's going to be faster pretty much and then", "tokens": [50364, 1009, 528, 341, 281, 312, 2074, 570, 309, 311, 516, 281, 312, 4663, 1238, 709, 293, 550, 50640, 50640, 264, 5593, 341, 1507, 510, 291, 393, 1391, 764, 2293, 264, 912, 633, 565, 50780, 50780, 456, 311, 257, 688, 295, 39228, 37008, 5347, 281, 2370, 7318, 382, 291, 536, 341, 1507, 291, 393, 51184, 51184, 1391, 764, 264, 912, 633, 565, 1392, 370, 321, 586, 643, 281, 1884, 527, 2316, 370, 264, 51528, 51528, 10344, 295, 264, 5201, 33347, 2445, 300, 321, 600, 1143, 281, 6772, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.09572896957397461, "compression_ratio": 1.7798165137614679, "no_speech_prob": 7.367521175183356e-05}, {"id": 811, "seek": 457100, "start": 4576.52, "end": 4579.32, "text": " the root this stuff here you can probably use exactly the same every time", "tokens": [50364, 1009, 528, 341, 281, 312, 2074, 570, 309, 311, 516, 281, 312, 4663, 1238, 709, 293, 550, 50640, 50640, 264, 5593, 341, 1507, 510, 291, 393, 1391, 764, 2293, 264, 912, 633, 565, 50780, 50780, 456, 311, 257, 688, 295, 39228, 37008, 5347, 281, 2370, 7318, 382, 291, 536, 341, 1507, 291, 393, 51184, 51184, 1391, 764, 264, 912, 633, 565, 1392, 370, 321, 586, 643, 281, 1884, 527, 2316, 370, 264, 51528, 51528, 10344, 295, 264, 5201, 33347, 2445, 300, 321, 600, 1143, 281, 6772, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.09572896957397461, "compression_ratio": 1.7798165137614679, "no_speech_prob": 7.367521175183356e-05}, {"id": 812, "seek": 457100, "start": 4579.32, "end": 4587.4, "text": " there's a lot of boilerplate compared to fast AI as you see this stuff you can", "tokens": [50364, 1009, 528, 341, 281, 312, 2074, 570, 309, 311, 516, 281, 312, 4663, 1238, 709, 293, 550, 50640, 50640, 264, 5593, 341, 1507, 510, 291, 393, 1391, 764, 2293, 264, 912, 633, 565, 50780, 50780, 456, 311, 257, 688, 295, 39228, 37008, 5347, 281, 2370, 7318, 382, 291, 536, 341, 1507, 291, 393, 51184, 51184, 1391, 764, 264, 912, 633, 565, 1392, 370, 321, 586, 643, 281, 1884, 527, 2316, 370, 264, 51528, 51528, 10344, 295, 264, 5201, 33347, 2445, 300, 321, 600, 1143, 281, 6772, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.09572896957397461, "compression_ratio": 1.7798165137614679, "no_speech_prob": 7.367521175183356e-05}, {"id": 813, "seek": 457100, "start": 4587.4, "end": 4594.28, "text": " probably use the same every time okay so we now need to create our model so the", "tokens": [50364, 1009, 528, 341, 281, 312, 2074, 570, 309, 311, 516, 281, 312, 4663, 1238, 709, 293, 550, 50640, 50640, 264, 5593, 341, 1507, 510, 291, 393, 1391, 764, 2293, 264, 912, 633, 565, 50780, 50780, 456, 311, 257, 688, 295, 39228, 37008, 5347, 281, 2370, 7318, 382, 291, 536, 341, 1507, 291, 393, 51184, 51184, 1391, 764, 264, 912, 633, 565, 1392, 370, 321, 586, 643, 281, 1884, 527, 2316, 370, 264, 51528, 51528, 10344, 295, 264, 5201, 33347, 2445, 300, 321, 600, 1143, 281, 6772, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.09572896957397461, "compression_ratio": 1.7798165137614679, "no_speech_prob": 7.367521175183356e-05}, {"id": 814, "seek": 457100, "start": 4594.28, "end": 4599.12, "text": " equivalent of the vision learner function that we've used to automatically", "tokens": [50364, 1009, 528, 341, 281, 312, 2074, 570, 309, 311, 516, 281, 312, 4663, 1238, 709, 293, 550, 50640, 50640, 264, 5593, 341, 1507, 510, 291, 393, 1391, 764, 2293, 264, 912, 633, 565, 50780, 50780, 456, 311, 257, 688, 295, 39228, 37008, 5347, 281, 2370, 7318, 382, 291, 536, 341, 1507, 291, 393, 51184, 51184, 1391, 764, 264, 912, 633, 565, 1392, 370, 321, 586, 643, 281, 1884, 527, 2316, 370, 264, 51528, 51528, 10344, 295, 264, 5201, 33347, 2445, 300, 321, 600, 1143, 281, 6772, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.09572896957397461, "compression_ratio": 1.7798165137614679, "no_speech_prob": 7.367521175183356e-05}, {"id": 815, "seek": 459912, "start": 4599.12, "end": 4606.4, "text": " create a reasonable vision model in hacking face transformers they've got", "tokens": [50364, 1884, 257, 10585, 5201, 2316, 294, 31422, 1851, 4088, 433, 436, 600, 658, 50728, 50728, 3195, 295, 819, 2306, 5413, 322, 437, 291, 434, 1382, 281, 360, 370, 321, 434, 1382, 281, 50968, 50968, 360, 21538, 382, 321, 600, 7152, 295, 22978, 370, 498, 321, 818, 8399, 2316, 337, 51246, 51246, 8310, 21538, 309, 486, 1884, 257, 2316, 300, 307, 6854, 337, 51442, 51442, 1508, 5489, 22978, 490, 257, 3847, 659, 12, 17227, 2001, 2316, 293, 341, 307, 264, 1315, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.07203098705836705, "compression_ratio": 1.7699530516431925, "no_speech_prob": 6.401387508958578e-05}, {"id": 816, "seek": 459912, "start": 4606.4, "end": 4611.2, "text": " lots of different ones depending on what you're trying to do so we're trying to", "tokens": [50364, 1884, 257, 10585, 5201, 2316, 294, 31422, 1851, 4088, 433, 436, 600, 658, 50728, 50728, 3195, 295, 819, 2306, 5413, 322, 437, 291, 434, 1382, 281, 360, 370, 321, 434, 1382, 281, 50968, 50968, 360, 21538, 382, 321, 600, 7152, 295, 22978, 370, 498, 321, 818, 8399, 2316, 337, 51246, 51246, 8310, 21538, 309, 486, 1884, 257, 2316, 300, 307, 6854, 337, 51442, 51442, 1508, 5489, 22978, 490, 257, 3847, 659, 12, 17227, 2001, 2316, 293, 341, 307, 264, 1315, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.07203098705836705, "compression_ratio": 1.7699530516431925, "no_speech_prob": 6.401387508958578e-05}, {"id": 817, "seek": 459912, "start": 4611.2, "end": 4616.76, "text": " do classification as we've discussed of sequences so if we call auto model for", "tokens": [50364, 1884, 257, 10585, 5201, 2316, 294, 31422, 1851, 4088, 433, 436, 600, 658, 50728, 50728, 3195, 295, 819, 2306, 5413, 322, 437, 291, 434, 1382, 281, 360, 370, 321, 434, 1382, 281, 50968, 50968, 360, 21538, 382, 321, 600, 7152, 295, 22978, 370, 498, 321, 818, 8399, 2316, 337, 51246, 51246, 8310, 21538, 309, 486, 1884, 257, 2316, 300, 307, 6854, 337, 51442, 51442, 1508, 5489, 22978, 490, 257, 3847, 659, 12, 17227, 2001, 2316, 293, 341, 307, 264, 1315, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.07203098705836705, "compression_ratio": 1.7699530516431925, "no_speech_prob": 6.401387508958578e-05}, {"id": 818, "seek": 459912, "start": 4616.76, "end": 4620.68, "text": " sequence classification it will create a model that is appropriate for", "tokens": [50364, 1884, 257, 10585, 5201, 2316, 294, 31422, 1851, 4088, 433, 436, 600, 658, 50728, 50728, 3195, 295, 819, 2306, 5413, 322, 437, 291, 434, 1382, 281, 360, 370, 321, 434, 1382, 281, 50968, 50968, 360, 21538, 382, 321, 600, 7152, 295, 22978, 370, 498, 321, 818, 8399, 2316, 337, 51246, 51246, 8310, 21538, 309, 486, 1884, 257, 2316, 300, 307, 6854, 337, 51442, 51442, 1508, 5489, 22978, 490, 257, 3847, 659, 12, 17227, 2001, 2316, 293, 341, 307, 264, 1315, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.07203098705836705, "compression_ratio": 1.7699530516431925, "no_speech_prob": 6.401387508958578e-05}, {"id": 819, "seek": 459912, "start": 4620.68, "end": 4625.32, "text": " classifying sequences from a train pre-trained model and this is the name", "tokens": [50364, 1884, 257, 10585, 5201, 2316, 294, 31422, 1851, 4088, 433, 436, 600, 658, 50728, 50728, 3195, 295, 819, 2306, 5413, 322, 437, 291, 434, 1382, 281, 360, 370, 321, 434, 1382, 281, 50968, 50968, 360, 21538, 382, 321, 600, 7152, 295, 22978, 370, 498, 321, 818, 8399, 2316, 337, 51246, 51246, 8310, 21538, 309, 486, 1884, 257, 2316, 300, 307, 6854, 337, 51442, 51442, 1508, 5489, 22978, 490, 257, 3847, 659, 12, 17227, 2001, 2316, 293, 341, 307, 264, 1315, 51674, 51674], "temperature": 0.0, "avg_logprob": -0.07203098705836705, "compression_ratio": 1.7699530516431925, "no_speech_prob": 6.401387508958578e-05}, {"id": 820, "seek": 462532, "start": 4625.32, "end": 4631.799999999999, "text": " of the model that we used dead earlier that the bird of e3 it has to know when", "tokens": [50364, 295, 264, 2316, 300, 321, 1143, 3116, 3071, 300, 264, 5255, 295, 308, 18, 309, 575, 281, 458, 562, 50688, 50688, 309, 10860, 300, 4974, 8141, 281, 264, 917, 577, 867, 23930, 309, 2203, 281, 362, 370, 321, 50894, 50894, 362, 472, 7645, 597, 307, 264, 6175, 370, 300, 311, 516, 281, 1884, 527, 2316, 293, 550, 51203, 51203, 341, 307, 264, 10344, 295, 4084, 257, 33347, 309, 8306, 257, 2316, 293, 264, 1412, 51476, 51476, 264, 3097, 1412, 293, 264, 1500, 1412, 797, 456, 311, 257, 688, 544, 39228, 37008, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11987375962106805, "compression_ratio": 1.7706422018348624, "no_speech_prob": 4.611002077581361e-05}, {"id": 821, "seek": 462532, "start": 4631.799999999999, "end": 4635.92, "text": " it adds that random matrix to the end how many outputs it needs to have so we", "tokens": [50364, 295, 264, 2316, 300, 321, 1143, 3116, 3071, 300, 264, 5255, 295, 308, 18, 309, 575, 281, 458, 562, 50688, 50688, 309, 10860, 300, 4974, 8141, 281, 264, 917, 577, 867, 23930, 309, 2203, 281, 362, 370, 321, 50894, 50894, 362, 472, 7645, 597, 307, 264, 6175, 370, 300, 311, 516, 281, 1884, 527, 2316, 293, 550, 51203, 51203, 341, 307, 264, 10344, 295, 4084, 257, 33347, 309, 8306, 257, 2316, 293, 264, 1412, 51476, 51476, 264, 3097, 1412, 293, 264, 1500, 1412, 797, 456, 311, 257, 688, 544, 39228, 37008, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11987375962106805, "compression_ratio": 1.7706422018348624, "no_speech_prob": 4.611002077581361e-05}, {"id": 822, "seek": 462532, "start": 4635.92, "end": 4642.099999999999, "text": " have one label which is the score so that's going to create our model and then", "tokens": [50364, 295, 264, 2316, 300, 321, 1143, 3116, 3071, 300, 264, 5255, 295, 308, 18, 309, 575, 281, 458, 562, 50688, 50688, 309, 10860, 300, 4974, 8141, 281, 264, 917, 577, 867, 23930, 309, 2203, 281, 362, 370, 321, 50894, 50894, 362, 472, 7645, 597, 307, 264, 6175, 370, 300, 311, 516, 281, 1884, 527, 2316, 293, 550, 51203, 51203, 341, 307, 264, 10344, 295, 4084, 257, 33347, 309, 8306, 257, 2316, 293, 264, 1412, 51476, 51476, 264, 3097, 1412, 293, 264, 1500, 1412, 797, 456, 311, 257, 688, 544, 39228, 37008, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11987375962106805, "compression_ratio": 1.7706422018348624, "no_speech_prob": 4.611002077581361e-05}, {"id": 823, "seek": 462532, "start": 4642.099999999999, "end": 4647.5599999999995, "text": " this is the equivalent of creating a learner it contains a model and the data", "tokens": [50364, 295, 264, 2316, 300, 321, 1143, 3116, 3071, 300, 264, 5255, 295, 308, 18, 309, 575, 281, 458, 562, 50688, 50688, 309, 10860, 300, 4974, 8141, 281, 264, 917, 577, 867, 23930, 309, 2203, 281, 362, 370, 321, 50894, 50894, 362, 472, 7645, 597, 307, 264, 6175, 370, 300, 311, 516, 281, 1884, 527, 2316, 293, 550, 51203, 51203, 341, 307, 264, 10344, 295, 4084, 257, 33347, 309, 8306, 257, 2316, 293, 264, 1412, 51476, 51476, 264, 3097, 1412, 293, 264, 1500, 1412, 797, 456, 311, 257, 688, 544, 39228, 37008, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11987375962106805, "compression_ratio": 1.7706422018348624, "no_speech_prob": 4.611002077581361e-05}, {"id": 824, "seek": 462532, "start": 4647.5599999999995, "end": 4651.719999999999, "text": " the training data and the test data again there's a lot more boilerplate", "tokens": [50364, 295, 264, 2316, 300, 321, 1143, 3116, 3071, 300, 264, 5255, 295, 308, 18, 309, 575, 281, 458, 562, 50688, 50688, 309, 10860, 300, 4974, 8141, 281, 264, 917, 577, 867, 23930, 309, 2203, 281, 362, 370, 321, 50894, 50894, 362, 472, 7645, 597, 307, 264, 6175, 370, 300, 311, 516, 281, 1884, 527, 2316, 293, 550, 51203, 51203, 341, 307, 264, 10344, 295, 4084, 257, 33347, 309, 8306, 257, 2316, 293, 264, 1412, 51476, 51476, 264, 3097, 1412, 293, 264, 1500, 1412, 797, 456, 311, 257, 688, 544, 39228, 37008, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11987375962106805, "compression_ratio": 1.7706422018348624, "no_speech_prob": 4.611002077581361e-05}, {"id": 825, "seek": 465172, "start": 4651.72, "end": 4655.92, "text": " here than fast AI but you can kind of see the same basic steps so here we just", "tokens": [50364, 510, 813, 2370, 7318, 457, 291, 393, 733, 295, 536, 264, 912, 3875, 4439, 370, 510, 321, 445, 50574, 50574, 362, 281, 360, 257, 707, 857, 544, 16945, 457, 309, 311, 406, 291, 458, 456, 311, 1825, 886, 50738, 50738, 3219, 370, 309, 311, 516, 281, 14862, 1125, 309, 337, 505, 1228, 300, 2445, 293, 550, 613, 50982, 50982, 366, 264, 8141, 8141, 8141, 300, 486, 4482, 484, 1184, 565, 300, 311, 300, 707, 51222, 51222, 2445, 321, 2942, 597, 11247, 257, 25890, 412, 264, 1623, 286, 915, 51482, 51482, 41706, 1851, 4088, 433, 588, 9595, 541, 309, 637, 1208, 484, 3195, 293, 3195, 293, 3195, 295, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12529570777137, "compression_ratio": 1.7722007722007722, "no_speech_prob": 8.34883758216165e-05}, {"id": 826, "seek": 465172, "start": 4655.92, "end": 4659.2, "text": " have to do a little bit more manually but it's not you know there's nothing too", "tokens": [50364, 510, 813, 2370, 7318, 457, 291, 393, 733, 295, 536, 264, 912, 3875, 4439, 370, 510, 321, 445, 50574, 50574, 362, 281, 360, 257, 707, 857, 544, 16945, 457, 309, 311, 406, 291, 458, 456, 311, 1825, 886, 50738, 50738, 3219, 370, 309, 311, 516, 281, 14862, 1125, 309, 337, 505, 1228, 300, 2445, 293, 550, 613, 50982, 50982, 366, 264, 8141, 8141, 8141, 300, 486, 4482, 484, 1184, 565, 300, 311, 300, 707, 51222, 51222, 2445, 321, 2942, 597, 11247, 257, 25890, 412, 264, 1623, 286, 915, 51482, 51482, 41706, 1851, 4088, 433, 588, 9595, 541, 309, 637, 1208, 484, 3195, 293, 3195, 293, 3195, 295, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12529570777137, "compression_ratio": 1.7722007722007722, "no_speech_prob": 8.34883758216165e-05}, {"id": 827, "seek": 465172, "start": 4659.2, "end": 4664.08, "text": " crazy so it's going to tokenize it for us using that function and then these", "tokens": [50364, 510, 813, 2370, 7318, 457, 291, 393, 733, 295, 536, 264, 912, 3875, 4439, 370, 510, 321, 445, 50574, 50574, 362, 281, 360, 257, 707, 857, 544, 16945, 457, 309, 311, 406, 291, 458, 456, 311, 1825, 886, 50738, 50738, 3219, 370, 309, 311, 516, 281, 14862, 1125, 309, 337, 505, 1228, 300, 2445, 293, 550, 613, 50982, 50982, 366, 264, 8141, 8141, 8141, 300, 486, 4482, 484, 1184, 565, 300, 311, 300, 707, 51222, 51222, 2445, 321, 2942, 597, 11247, 257, 25890, 412, 264, 1623, 286, 915, 51482, 51482, 41706, 1851, 4088, 433, 588, 9595, 541, 309, 637, 1208, 484, 3195, 293, 3195, 293, 3195, 295, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12529570777137, "compression_ratio": 1.7722007722007722, "no_speech_prob": 8.34883758216165e-05}, {"id": 828, "seek": 465172, "start": 4664.08, "end": 4668.88, "text": " are the matrix matrix matrix that will print out each time that's that little", "tokens": [50364, 510, 813, 2370, 7318, 457, 291, 393, 733, 295, 536, 264, 912, 3875, 4439, 370, 510, 321, 445, 50574, 50574, 362, 281, 360, 257, 707, 857, 544, 16945, 457, 309, 311, 406, 291, 458, 456, 311, 1825, 886, 50738, 50738, 3219, 370, 309, 311, 516, 281, 14862, 1125, 309, 337, 505, 1228, 300, 2445, 293, 550, 613, 50982, 50982, 366, 264, 8141, 8141, 8141, 300, 486, 4482, 484, 1184, 565, 300, 311, 300, 707, 51222, 51222, 2445, 321, 2942, 597, 11247, 257, 25890, 412, 264, 1623, 286, 915, 51482, 51482, 41706, 1851, 4088, 433, 588, 9595, 541, 309, 637, 1208, 484, 3195, 293, 3195, 293, 3195, 295, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12529570777137, "compression_ratio": 1.7722007722007722, "no_speech_prob": 8.34883758216165e-05}, {"id": 829, "seek": 465172, "start": 4668.88, "end": 4674.08, "text": " function we created which returns a dictionary at the moment I find", "tokens": [50364, 510, 813, 2370, 7318, 457, 291, 393, 733, 295, 536, 264, 912, 3875, 4439, 370, 510, 321, 445, 50574, 50574, 362, 281, 360, 257, 707, 857, 544, 16945, 457, 309, 311, 406, 291, 458, 456, 311, 1825, 886, 50738, 50738, 3219, 370, 309, 311, 516, 281, 14862, 1125, 309, 337, 505, 1228, 300, 2445, 293, 550, 613, 50982, 50982, 366, 264, 8141, 8141, 8141, 300, 486, 4482, 484, 1184, 565, 300, 311, 300, 707, 51222, 51222, 2445, 321, 2942, 597, 11247, 257, 25890, 412, 264, 1623, 286, 915, 51482, 51482, 41706, 1851, 4088, 433, 588, 9595, 541, 309, 637, 1208, 484, 3195, 293, 3195, 293, 3195, 295, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12529570777137, "compression_ratio": 1.7722007722007722, "no_speech_prob": 8.34883758216165e-05}, {"id": 830, "seek": 465172, "start": 4674.08, "end": 4677.72, "text": " hugging face transformers very verbose it spits out lots and lots and lots of", "tokens": [50364, 510, 813, 2370, 7318, 457, 291, 393, 733, 295, 536, 264, 912, 3875, 4439, 370, 510, 321, 445, 50574, 50574, 362, 281, 360, 257, 707, 857, 544, 16945, 457, 309, 311, 406, 291, 458, 456, 311, 1825, 886, 50738, 50738, 3219, 370, 309, 311, 516, 281, 14862, 1125, 309, 337, 505, 1228, 300, 2445, 293, 550, 613, 50982, 50982, 366, 264, 8141, 8141, 8141, 300, 486, 4482, 484, 1184, 565, 300, 311, 300, 707, 51222, 51222, 2445, 321, 2942, 597, 11247, 257, 25890, 412, 264, 1623, 286, 915, 51482, 51482, 41706, 1851, 4088, 433, 588, 9595, 541, 309, 637, 1208, 484, 3195, 293, 3195, 293, 3195, 295, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12529570777137, "compression_ratio": 1.7722007722007722, "no_speech_prob": 8.34883758216165e-05}, {"id": 831, "seek": 467772, "start": 4677.72, "end": 4682.84, "text": " text which you can ignore and we can finally call train which will spit out", "tokens": [50364, 2487, 597, 291, 393, 11200, 293, 321, 393, 2721, 818, 3847, 597, 486, 22127, 484, 50620, 50620, 709, 544, 2487, 797, 597, 291, 393, 11200, 293, 382, 291, 393, 536, 382, 309, 16329, 50862, 50862, 309, 311, 14699, 484, 264, 4470, 293, 510, 311, 527, 39041, 20009, 17619, 370, 51236, 51236, 309, 311, 3097, 293, 321, 600, 658, 257, 935, 3180, 1045, 1451, 13983, 763, 300, 311, 51438, 51438, 1238, 1627, 558, 286, 914, 309, 1890, 437, 775, 958, 291, 584, 457, 309, 445, 1890, 257, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1401280403137207, "compression_ratio": 1.7894736842105263, "no_speech_prob": 4.6835775719955564e-05}, {"id": 832, "seek": 467772, "start": 4682.84, "end": 4687.68, "text": " much more text again which you can ignore and as you can see as it trains", "tokens": [50364, 2487, 597, 291, 393, 11200, 293, 321, 393, 2721, 818, 3847, 597, 486, 22127, 484, 50620, 50620, 709, 544, 2487, 797, 597, 291, 393, 11200, 293, 382, 291, 393, 536, 382, 309, 16329, 50862, 50862, 309, 311, 14699, 484, 264, 4470, 293, 510, 311, 527, 39041, 20009, 17619, 370, 51236, 51236, 309, 311, 3097, 293, 321, 600, 658, 257, 935, 3180, 1045, 1451, 13983, 763, 300, 311, 51438, 51438, 1238, 1627, 558, 286, 914, 309, 1890, 437, 775, 958, 291, 584, 457, 309, 445, 1890, 257, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1401280403137207, "compression_ratio": 1.7894736842105263, "no_speech_prob": 4.6835775719955564e-05}, {"id": 833, "seek": 467772, "start": 4687.68, "end": 4695.16, "text": " it's printing out the loss and here's our Pearson correlation coefficient so", "tokens": [50364, 2487, 597, 291, 393, 11200, 293, 321, 393, 2721, 818, 3847, 597, 486, 22127, 484, 50620, 50620, 709, 544, 2487, 797, 597, 291, 393, 11200, 293, 382, 291, 393, 536, 382, 309, 16329, 50862, 50862, 309, 311, 14699, 484, 264, 4470, 293, 510, 311, 527, 39041, 20009, 17619, 370, 51236, 51236, 309, 311, 3097, 293, 321, 600, 658, 257, 935, 3180, 1045, 1451, 13983, 763, 300, 311, 51438, 51438, 1238, 1627, 558, 286, 914, 309, 1890, 437, 775, 958, 291, 584, 457, 309, 445, 1890, 257, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1401280403137207, "compression_ratio": 1.7894736842105263, "no_speech_prob": 4.6835775719955564e-05}, {"id": 834, "seek": 467772, "start": 4695.16, "end": 4699.2, "text": " it's training and we've got a point eight three four correlations that's", "tokens": [50364, 2487, 597, 291, 393, 11200, 293, 321, 393, 2721, 818, 3847, 597, 486, 22127, 484, 50620, 50620, 709, 544, 2487, 797, 597, 291, 393, 11200, 293, 382, 291, 393, 536, 382, 309, 16329, 50862, 50862, 309, 311, 14699, 484, 264, 4470, 293, 510, 311, 527, 39041, 20009, 17619, 370, 51236, 51236, 309, 311, 3097, 293, 321, 600, 658, 257, 935, 3180, 1045, 1451, 13983, 763, 300, 311, 51438, 51438, 1238, 1627, 558, 286, 914, 309, 1890, 437, 775, 958, 291, 584, 457, 309, 445, 1890, 257, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1401280403137207, "compression_ratio": 1.7894736842105263, "no_speech_prob": 4.6835775719955564e-05}, {"id": 835, "seek": 467772, "start": 4699.2, "end": 4703.8, "text": " pretty cool right I mean it took what does next you say but it just took a", "tokens": [50364, 2487, 597, 291, 393, 11200, 293, 321, 393, 2721, 818, 3847, 597, 486, 22127, 484, 50620, 50620, 709, 544, 2487, 797, 597, 291, 393, 11200, 293, 382, 291, 393, 536, 382, 309, 16329, 50862, 50862, 309, 311, 14699, 484, 264, 4470, 293, 510, 311, 527, 39041, 20009, 17619, 370, 51236, 51236, 309, 311, 3097, 293, 321, 600, 658, 257, 935, 3180, 1045, 1451, 13983, 763, 300, 311, 51438, 51438, 1238, 1627, 558, 286, 914, 309, 1890, 437, 775, 958, 291, 584, 457, 309, 445, 1890, 257, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1401280403137207, "compression_ratio": 1.7894736842105263, "no_speech_prob": 4.6835775719955564e-05}, {"id": 836, "seek": 470380, "start": 4703.8, "end": 4708.16, "text": " here we are five minutes to run maybe that's five minutes per epoch on Kaggle", "tokens": [50364, 510, 321, 366, 1732, 2077, 281, 1190, 1310, 300, 311, 1732, 2077, 680, 30992, 339, 322, 48751, 22631, 50582, 50582, 597, 1177, 380, 362, 4098, 869, 18407, 82, 457, 665, 337, 1737, 293, 321, 600, 658, 50838, 50838, 746, 300, 307, 291, 458, 658, 257, 588, 1090, 1496, 295, 20009, 294, 34348, 51140, 51140, 577, 2531, 264, 732, 13766, 366, 293, 264, 787, 1778, 309, 727, 360, 300, 307, 570, 51404, 51404, 309, 1143, 257, 659, 12, 17227, 2001, 2316, 558, 456, 311, 572, 636, 291, 727, 445, 362, 300, 51592, 51592, 5870, 2372, 295, 1589, 293, 2573, 484, 1968, 729, 732, 13766, 366, 588, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.1049100018422538, "compression_ratio": 1.684981684981685, "no_speech_prob": 3.0716764740645885e-05}, {"id": 837, "seek": 470380, "start": 4708.16, "end": 4713.28, "text": " which doesn't have particularly great GPUs but good for free and we've got", "tokens": [50364, 510, 321, 366, 1732, 2077, 281, 1190, 1310, 300, 311, 1732, 2077, 680, 30992, 339, 322, 48751, 22631, 50582, 50582, 597, 1177, 380, 362, 4098, 869, 18407, 82, 457, 665, 337, 1737, 293, 321, 600, 658, 50838, 50838, 746, 300, 307, 291, 458, 658, 257, 588, 1090, 1496, 295, 20009, 294, 34348, 51140, 51140, 577, 2531, 264, 732, 13766, 366, 293, 264, 787, 1778, 309, 727, 360, 300, 307, 570, 51404, 51404, 309, 1143, 257, 659, 12, 17227, 2001, 2316, 558, 456, 311, 572, 636, 291, 727, 445, 362, 300, 51592, 51592, 5870, 2372, 295, 1589, 293, 2573, 484, 1968, 729, 732, 13766, 366, 588, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.1049100018422538, "compression_ratio": 1.684981684981685, "no_speech_prob": 3.0716764740645885e-05}, {"id": 838, "seek": 470380, "start": 4713.28, "end": 4719.320000000001, "text": " something that is you know got a very high level of correlation in assessing", "tokens": [50364, 510, 321, 366, 1732, 2077, 281, 1190, 1310, 300, 311, 1732, 2077, 680, 30992, 339, 322, 48751, 22631, 50582, 50582, 597, 1177, 380, 362, 4098, 869, 18407, 82, 457, 665, 337, 1737, 293, 321, 600, 658, 50838, 50838, 746, 300, 307, 291, 458, 658, 257, 588, 1090, 1496, 295, 20009, 294, 34348, 51140, 51140, 577, 2531, 264, 732, 13766, 366, 293, 264, 787, 1778, 309, 727, 360, 300, 307, 570, 51404, 51404, 309, 1143, 257, 659, 12, 17227, 2001, 2316, 558, 456, 311, 572, 636, 291, 727, 445, 362, 300, 51592, 51592, 5870, 2372, 295, 1589, 293, 2573, 484, 1968, 729, 732, 13766, 366, 588, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.1049100018422538, "compression_ratio": 1.684981684981685, "no_speech_prob": 3.0716764740645885e-05}, {"id": 839, "seek": 470380, "start": 4719.320000000001, "end": 4724.6, "text": " how similar the two columns are and the only reason it could do that is because", "tokens": [50364, 510, 321, 366, 1732, 2077, 281, 1190, 1310, 300, 311, 1732, 2077, 680, 30992, 339, 322, 48751, 22631, 50582, 50582, 597, 1177, 380, 362, 4098, 869, 18407, 82, 457, 665, 337, 1737, 293, 321, 600, 658, 50838, 50838, 746, 300, 307, 291, 458, 658, 257, 588, 1090, 1496, 295, 20009, 294, 34348, 51140, 51140, 577, 2531, 264, 732, 13766, 366, 293, 264, 787, 1778, 309, 727, 360, 300, 307, 570, 51404, 51404, 309, 1143, 257, 659, 12, 17227, 2001, 2316, 558, 456, 311, 572, 636, 291, 727, 445, 362, 300, 51592, 51592, 5870, 2372, 295, 1589, 293, 2573, 484, 1968, 729, 732, 13766, 366, 588, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.1049100018422538, "compression_ratio": 1.684981684981685, "no_speech_prob": 3.0716764740645885e-05}, {"id": 840, "seek": 470380, "start": 4724.6, "end": 4728.360000000001, "text": " it used a pre-trained model right there's no way you could just have that", "tokens": [50364, 510, 321, 366, 1732, 2077, 281, 1190, 1310, 300, 311, 1732, 2077, 680, 30992, 339, 322, 48751, 22631, 50582, 50582, 597, 1177, 380, 362, 4098, 869, 18407, 82, 457, 665, 337, 1737, 293, 321, 600, 658, 50838, 50838, 746, 300, 307, 291, 458, 658, 257, 588, 1090, 1496, 295, 20009, 294, 34348, 51140, 51140, 577, 2531, 264, 732, 13766, 366, 293, 264, 787, 1778, 309, 727, 360, 300, 307, 570, 51404, 51404, 309, 1143, 257, 659, 12, 17227, 2001, 2316, 558, 456, 311, 572, 636, 291, 727, 445, 362, 300, 51592, 51592, 5870, 2372, 295, 1589, 293, 2573, 484, 1968, 729, 732, 13766, 366, 588, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.1049100018422538, "compression_ratio": 1.684981684981685, "no_speech_prob": 3.0716764740645885e-05}, {"id": 841, "seek": 470380, "start": 4728.360000000001, "end": 4731.84, "text": " tiny amount of information and figure out whether those two columns are very", "tokens": [50364, 510, 321, 366, 1732, 2077, 281, 1190, 1310, 300, 311, 1732, 2077, 680, 30992, 339, 322, 48751, 22631, 50582, 50582, 597, 1177, 380, 362, 4098, 869, 18407, 82, 457, 665, 337, 1737, 293, 321, 600, 658, 50838, 50838, 746, 300, 307, 291, 458, 658, 257, 588, 1090, 1496, 295, 20009, 294, 34348, 51140, 51140, 577, 2531, 264, 732, 13766, 366, 293, 264, 787, 1778, 309, 727, 360, 300, 307, 570, 51404, 51404, 309, 1143, 257, 659, 12, 17227, 2001, 2316, 558, 456, 311, 572, 636, 291, 727, 445, 362, 300, 51592, 51592, 5870, 2372, 295, 1589, 293, 2573, 484, 1968, 729, 732, 13766, 366, 588, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.1049100018422538, "compression_ratio": 1.684981684981685, "no_speech_prob": 3.0716764740645885e-05}, {"id": 842, "seek": 473184, "start": 4731.84, "end": 4737.6, "text": " similar this pre-trained model already knows a lot about language it already", "tokens": [50364, 2531, 341, 659, 12, 17227, 2001, 2316, 1217, 3255, 257, 688, 466, 2856, 309, 1217, 50652, 50652, 575, 257, 665, 2020, 295, 1968, 732, 20312, 366, 2531, 420, 406, 293, 321, 600, 445, 50854, 50854, 2489, 12, 83, 43703, 309, 291, 393, 536, 2212, 300, 934, 472, 30992, 339, 309, 390, 1217, 412, 935, 51054, 51054, 3180, 291, 458, 321, 341, 390, 257, 2316, 300, 1217, 630, 746, 1238, 1998, 281, 51332, 51332, 437, 321, 2978, 309, 994, 380, 534, 643, 300, 709, 2857, 15164, 337, 341, 51542, 51542], "temperature": 0.0, "avg_logprob": -0.09678555582905864, "compression_ratio": 1.7155963302752293, "no_speech_prob": 1.5445701137650758e-05}, {"id": 843, "seek": 473184, "start": 4737.6, "end": 4741.64, "text": " has a good sense of whether two phrases are similar or not and we've just", "tokens": [50364, 2531, 341, 659, 12, 17227, 2001, 2316, 1217, 3255, 257, 688, 466, 2856, 309, 1217, 50652, 50652, 575, 257, 665, 2020, 295, 1968, 732, 20312, 366, 2531, 420, 406, 293, 321, 600, 445, 50854, 50854, 2489, 12, 83, 43703, 309, 291, 393, 536, 2212, 300, 934, 472, 30992, 339, 309, 390, 1217, 412, 935, 51054, 51054, 3180, 291, 458, 321, 341, 390, 257, 2316, 300, 1217, 630, 746, 1238, 1998, 281, 51332, 51332, 437, 321, 2978, 309, 994, 380, 534, 643, 300, 709, 2857, 15164, 337, 341, 51542, 51542], "temperature": 0.0, "avg_logprob": -0.09678555582905864, "compression_ratio": 1.7155963302752293, "no_speech_prob": 1.5445701137650758e-05}, {"id": 844, "seek": 473184, "start": 4741.64, "end": 4745.64, "text": " fine-tuned it you can see given that after one epoch it was already at point", "tokens": [50364, 2531, 341, 659, 12, 17227, 2001, 2316, 1217, 3255, 257, 688, 466, 2856, 309, 1217, 50652, 50652, 575, 257, 665, 2020, 295, 1968, 732, 20312, 366, 2531, 420, 406, 293, 321, 600, 445, 50854, 50854, 2489, 12, 83, 43703, 309, 291, 393, 536, 2212, 300, 934, 472, 30992, 339, 309, 390, 1217, 412, 935, 51054, 51054, 3180, 291, 458, 321, 341, 390, 257, 2316, 300, 1217, 630, 746, 1238, 1998, 281, 51332, 51332, 437, 321, 2978, 309, 994, 380, 534, 643, 300, 709, 2857, 15164, 337, 341, 51542, 51542], "temperature": 0.0, "avg_logprob": -0.09678555582905864, "compression_ratio": 1.7155963302752293, "no_speech_prob": 1.5445701137650758e-05}, {"id": 845, "seek": 473184, "start": 4745.64, "end": 4751.2, "text": " eight you know we this was a model that already did something pretty close to", "tokens": [50364, 2531, 341, 659, 12, 17227, 2001, 2316, 1217, 3255, 257, 688, 466, 2856, 309, 1217, 50652, 50652, 575, 257, 665, 2020, 295, 1968, 732, 20312, 366, 2531, 420, 406, 293, 321, 600, 445, 50854, 50854, 2489, 12, 83, 43703, 309, 291, 393, 536, 2212, 300, 934, 472, 30992, 339, 309, 390, 1217, 412, 935, 51054, 51054, 3180, 291, 458, 321, 341, 390, 257, 2316, 300, 1217, 630, 746, 1238, 1998, 281, 51332, 51332, 437, 321, 2978, 309, 994, 380, 534, 643, 300, 709, 2857, 15164, 337, 341, 51542, 51542], "temperature": 0.0, "avg_logprob": -0.09678555582905864, "compression_ratio": 1.7155963302752293, "no_speech_prob": 1.5445701137650758e-05}, {"id": 846, "seek": 473184, "start": 4751.2, "end": 4755.400000000001, "text": " what we needed it didn't really need that much extra tuning for this", "tokens": [50364, 2531, 341, 659, 12, 17227, 2001, 2316, 1217, 3255, 257, 688, 466, 2856, 309, 1217, 50652, 50652, 575, 257, 665, 2020, 295, 1968, 732, 20312, 366, 2531, 420, 406, 293, 321, 600, 445, 50854, 50854, 2489, 12, 83, 43703, 309, 291, 393, 536, 2212, 300, 934, 472, 30992, 339, 309, 390, 1217, 412, 935, 51054, 51054, 3180, 291, 458, 321, 341, 390, 257, 2316, 300, 1217, 630, 746, 1238, 1998, 281, 51332, 51332, 437, 321, 2978, 309, 994, 380, 534, 643, 300, 709, 2857, 15164, 337, 341, 51542, 51542], "temperature": 0.0, "avg_logprob": -0.09678555582905864, "compression_ratio": 1.7155963302752293, "no_speech_prob": 1.5445701137650758e-05}, {"id": 847, "seek": 475540, "start": 4755.4, "end": 4766.599999999999, "text": " particular task I've got any questions there John yeah we do it's actually a bit", "tokens": [50364, 1729, 5633, 286, 600, 658, 604, 1651, 456, 2619, 1338, 321, 360, 309, 311, 767, 257, 857, 50924, 50924, 646, 322, 264, 4829, 949, 689, 291, 645, 4099, 505, 264, 5056, 14174, 295, 51118, 51118, 264, 39041, 17619, 293, 291, 434, 1417, 466, 484, 23646, 1338, 293, 321, 600, 658, 51310, 51310, 257, 1168, 510, 490, 9954, 3365, 577, 360, 291, 4536, 562, 309, 311, 1392, 281, 4159, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.10938027832243177, "compression_ratio": 1.5858585858585859, "no_speech_prob": 0.00011583491141209379}, {"id": 848, "seek": 475540, "start": 4766.599999999999, "end": 4770.48, "text": " back on the topic before where you were showing us the visual interpretation of", "tokens": [50364, 1729, 5633, 286, 600, 658, 604, 1651, 456, 2619, 1338, 321, 360, 309, 311, 767, 257, 857, 50924, 50924, 646, 322, 264, 4829, 949, 689, 291, 645, 4099, 505, 264, 5056, 14174, 295, 51118, 51118, 264, 39041, 17619, 293, 291, 434, 1417, 466, 484, 23646, 1338, 293, 321, 600, 658, 51310, 51310, 257, 1168, 510, 490, 9954, 3365, 577, 360, 291, 4536, 562, 309, 311, 1392, 281, 4159, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.10938027832243177, "compression_ratio": 1.5858585858585859, "no_speech_prob": 0.00011583491141209379}, {"id": 849, "seek": 475540, "start": 4770.48, "end": 4774.32, "text": " the Pearson coefficient and you're talking about outliers yeah and we've got", "tokens": [50364, 1729, 5633, 286, 600, 658, 604, 1651, 456, 2619, 1338, 321, 360, 309, 311, 767, 257, 857, 50924, 50924, 646, 322, 264, 4829, 949, 689, 291, 645, 4099, 505, 264, 5056, 14174, 295, 51118, 51118, 264, 39041, 17619, 293, 291, 434, 1417, 466, 484, 23646, 1338, 293, 321, 600, 658, 51310, 51310, 257, 1168, 510, 490, 9954, 3365, 577, 360, 291, 4536, 562, 309, 311, 1392, 281, 4159, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.10938027832243177, "compression_ratio": 1.5858585858585859, "no_speech_prob": 0.00011583491141209379}, {"id": 850, "seek": 475540, "start": 4774.32, "end": 4780.5199999999995, "text": " a question here from Kevin asking how do you decide when it's okay to remove", "tokens": [50364, 1729, 5633, 286, 600, 658, 604, 1651, 456, 2619, 1338, 321, 360, 309, 311, 767, 257, 857, 50924, 50924, 646, 322, 264, 4829, 949, 689, 291, 645, 4099, 505, 264, 5056, 14174, 295, 51118, 51118, 264, 39041, 17619, 293, 291, 434, 1417, 466, 484, 23646, 1338, 293, 321, 600, 658, 51310, 51310, 257, 1168, 510, 490, 9954, 3365, 577, 360, 291, 4536, 562, 309, 311, 1392, 281, 4159, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.10938027832243177, "compression_ratio": 1.5858585858585859, "no_speech_prob": 0.00011583491141209379}, {"id": 851, "seek": 478052, "start": 4780.52, "end": 4786.76, "text": " outliers like you you you pointed out some in that data set and clearly your", "tokens": [50364, 484, 23646, 411, 291, 291, 291, 10932, 484, 512, 294, 300, 1412, 992, 293, 4448, 428, 50676, 50676, 2316, 307, 516, 281, 3847, 257, 688, 1101, 498, 291, 2541, 300, 493, 457, 293, 286, 519, 50928, 50928, 9954, 311, 935, 510, 307, 291, 458, 729, 3685, 295, 484, 23646, 486, 1391, 2514, 294, 51187, 51187, 264, 1500, 992, 382, 731, 370, 286, 519, 415, 311, 445, 1237, 337, 512, 8496, 5192, 51342, 51342, 322, 577, 291, 4813, 300, 294, 257, 544, 2674, 2020, 370, 484, 23646, 820, 1128, 445, 312, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11721555222856238, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.4792326914612204e-05}, {"id": 852, "seek": 478052, "start": 4786.76, "end": 4791.8, "text": " model is going to train a lot better if you clean that up but and I think", "tokens": [50364, 484, 23646, 411, 291, 291, 291, 10932, 484, 512, 294, 300, 1412, 992, 293, 4448, 428, 50676, 50676, 2316, 307, 516, 281, 3847, 257, 688, 1101, 498, 291, 2541, 300, 493, 457, 293, 286, 519, 50928, 50928, 9954, 311, 935, 510, 307, 291, 458, 729, 3685, 295, 484, 23646, 486, 1391, 2514, 294, 51187, 51187, 264, 1500, 992, 382, 731, 370, 286, 519, 415, 311, 445, 1237, 337, 512, 8496, 5192, 51342, 51342, 322, 577, 291, 4813, 300, 294, 257, 544, 2674, 2020, 370, 484, 23646, 820, 1128, 445, 312, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11721555222856238, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.4792326914612204e-05}, {"id": 853, "seek": 478052, "start": 4791.8, "end": 4796.9800000000005, "text": " Kevin's point here is you know those kinds of outliers will probably exist in", "tokens": [50364, 484, 23646, 411, 291, 291, 291, 10932, 484, 512, 294, 300, 1412, 992, 293, 4448, 428, 50676, 50676, 2316, 307, 516, 281, 3847, 257, 688, 1101, 498, 291, 2541, 300, 493, 457, 293, 286, 519, 50928, 50928, 9954, 311, 935, 510, 307, 291, 458, 729, 3685, 295, 484, 23646, 486, 1391, 2514, 294, 51187, 51187, 264, 1500, 992, 382, 731, 370, 286, 519, 415, 311, 445, 1237, 337, 512, 8496, 5192, 51342, 51342, 322, 577, 291, 4813, 300, 294, 257, 544, 2674, 2020, 370, 484, 23646, 820, 1128, 445, 312, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11721555222856238, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.4792326914612204e-05}, {"id": 854, "seek": 478052, "start": 4796.9800000000005, "end": 4800.080000000001, "text": " the test set as well so I think he's just looking for some practical advice", "tokens": [50364, 484, 23646, 411, 291, 291, 291, 10932, 484, 512, 294, 300, 1412, 992, 293, 4448, 428, 50676, 50676, 2316, 307, 516, 281, 3847, 257, 688, 1101, 498, 291, 2541, 300, 493, 457, 293, 286, 519, 50928, 50928, 9954, 311, 935, 510, 307, 291, 458, 729, 3685, 295, 484, 23646, 486, 1391, 2514, 294, 51187, 51187, 264, 1500, 992, 382, 731, 370, 286, 519, 415, 311, 445, 1237, 337, 512, 8496, 5192, 51342, 51342, 322, 577, 291, 4813, 300, 294, 257, 544, 2674, 2020, 370, 484, 23646, 820, 1128, 445, 312, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11721555222856238, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.4792326914612204e-05}, {"id": 855, "seek": 478052, "start": 4800.080000000001, "end": 4810.400000000001, "text": " on how you handle that in a more general sense so outliers should never just be", "tokens": [50364, 484, 23646, 411, 291, 291, 291, 10932, 484, 512, 294, 300, 1412, 992, 293, 4448, 428, 50676, 50676, 2316, 307, 516, 281, 3847, 257, 688, 1101, 498, 291, 2541, 300, 493, 457, 293, 286, 519, 50928, 50928, 9954, 311, 935, 510, 307, 291, 458, 729, 3685, 295, 484, 23646, 486, 1391, 2514, 294, 51187, 51187, 264, 1500, 992, 382, 731, 370, 286, 519, 415, 311, 445, 1237, 337, 512, 8496, 5192, 51342, 51342, 322, 577, 291, 4813, 300, 294, 257, 544, 2674, 2020, 370, 484, 23646, 820, 1128, 445, 312, 51858, 51858], "temperature": 0.0, "avg_logprob": -0.11721555222856238, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.4792326914612204e-05}, {"id": 856, "seek": 481040, "start": 4810.4, "end": 4818.32, "text": " removed like for modeling so if we take the example of the California date", "tokens": [50364, 7261, 411, 337, 15983, 370, 498, 321, 747, 264, 1365, 295, 264, 5384, 4002, 50760, 50760, 6849, 1412, 992, 291, 458, 498, 286, 390, 534, 1364, 365, 300, 1412, 992, 294, 50956, 50956, 957, 993, 286, 576, 312, 1566, 1954, 300, 311, 1880, 309, 2544, 411, 456, 311, 257, 51156, 51156, 4994, 1594, 295, 16815, 365, 257, 819, 733, 295, 5223, 586, 452, 2041, 51344, 51344, 307, 300, 436, 434, 516, 281, 312, 733, 295, 411, 12521, 82, 420, 746, 411, 300, 291, 458, 51482, 51482, 1391, 2295, 12, 30673, 6849, 293, 370, 286, 576, 312, 1566, 411, 1954, 4448, 490, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.08931344350179037, "compression_ratio": 1.7777777777777777, "no_speech_prob": 9.606462117517367e-05}, {"id": 857, "seek": 481040, "start": 4818.32, "end": 4822.24, "text": " housing data set you know if I was really working with that data set in", "tokens": [50364, 7261, 411, 337, 15983, 370, 498, 321, 747, 264, 1365, 295, 264, 5384, 4002, 50760, 50760, 6849, 1412, 992, 291, 458, 498, 286, 390, 534, 1364, 365, 300, 1412, 992, 294, 50956, 50956, 957, 993, 286, 576, 312, 1566, 1954, 300, 311, 1880, 309, 2544, 411, 456, 311, 257, 51156, 51156, 4994, 1594, 295, 16815, 365, 257, 819, 733, 295, 5223, 586, 452, 2041, 51344, 51344, 307, 300, 436, 434, 516, 281, 312, 733, 295, 411, 12521, 82, 420, 746, 411, 300, 291, 458, 51482, 51482, 1391, 2295, 12, 30673, 6849, 293, 370, 286, 576, 312, 1566, 411, 1954, 4448, 490, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.08931344350179037, "compression_ratio": 1.7777777777777777, "no_speech_prob": 9.606462117517367e-05}, {"id": 858, "seek": 481040, "start": 4822.24, "end": 4826.24, "text": " real life I would be saying oh that's interesting it seems like there's a", "tokens": [50364, 7261, 411, 337, 15983, 370, 498, 321, 747, 264, 1365, 295, 264, 5384, 4002, 50760, 50760, 6849, 1412, 992, 291, 458, 498, 286, 390, 534, 1364, 365, 300, 1412, 992, 294, 50956, 50956, 957, 993, 286, 576, 312, 1566, 1954, 300, 311, 1880, 309, 2544, 411, 456, 311, 257, 51156, 51156, 4994, 1594, 295, 16815, 365, 257, 819, 733, 295, 5223, 586, 452, 2041, 51344, 51344, 307, 300, 436, 434, 516, 281, 312, 733, 295, 411, 12521, 82, 420, 746, 411, 300, 291, 458, 51482, 51482, 1391, 2295, 12, 30673, 6849, 293, 370, 286, 576, 312, 1566, 411, 1954, 4448, 490, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.08931344350179037, "compression_ratio": 1.7777777777777777, "no_speech_prob": 9.606462117517367e-05}, {"id": 859, "seek": 481040, "start": 4826.24, "end": 4830.0, "text": " separate group of districts with a different kind of behavior now my guess", "tokens": [50364, 7261, 411, 337, 15983, 370, 498, 321, 747, 264, 1365, 295, 264, 5384, 4002, 50760, 50760, 6849, 1412, 992, 291, 458, 498, 286, 390, 534, 1364, 365, 300, 1412, 992, 294, 50956, 50956, 957, 993, 286, 576, 312, 1566, 1954, 300, 311, 1880, 309, 2544, 411, 456, 311, 257, 51156, 51156, 4994, 1594, 295, 16815, 365, 257, 819, 733, 295, 5223, 586, 452, 2041, 51344, 51344, 307, 300, 436, 434, 516, 281, 312, 733, 295, 411, 12521, 82, 420, 746, 411, 300, 291, 458, 51482, 51482, 1391, 2295, 12, 30673, 6849, 293, 370, 286, 576, 312, 1566, 411, 1954, 4448, 490, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.08931344350179037, "compression_ratio": 1.7777777777777777, "no_speech_prob": 9.606462117517367e-05}, {"id": 860, "seek": 481040, "start": 4830.0, "end": 4832.759999999999, "text": " is that they're going to be kind of like dorms or something like that you know", "tokens": [50364, 7261, 411, 337, 15983, 370, 498, 321, 747, 264, 1365, 295, 264, 5384, 4002, 50760, 50760, 6849, 1412, 992, 291, 458, 498, 286, 390, 534, 1364, 365, 300, 1412, 992, 294, 50956, 50956, 957, 993, 286, 576, 312, 1566, 1954, 300, 311, 1880, 309, 2544, 411, 456, 311, 257, 51156, 51156, 4994, 1594, 295, 16815, 365, 257, 819, 733, 295, 5223, 586, 452, 2041, 51344, 51344, 307, 300, 436, 434, 516, 281, 312, 733, 295, 411, 12521, 82, 420, 746, 411, 300, 291, 458, 51482, 51482, 1391, 2295, 12, 30673, 6849, 293, 370, 286, 576, 312, 1566, 411, 1954, 4448, 490, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.08931344350179037, "compression_ratio": 1.7777777777777777, "no_speech_prob": 9.606462117517367e-05}, {"id": 861, "seek": 481040, "start": 4832.759999999999, "end": 4838.799999999999, "text": " probably low-income housing and so I would be saying like oh clearly from", "tokens": [50364, 7261, 411, 337, 15983, 370, 498, 321, 747, 264, 1365, 295, 264, 5384, 4002, 50760, 50760, 6849, 1412, 992, 291, 458, 498, 286, 390, 534, 1364, 365, 300, 1412, 992, 294, 50956, 50956, 957, 993, 286, 576, 312, 1566, 1954, 300, 311, 1880, 309, 2544, 411, 456, 311, 257, 51156, 51156, 4994, 1594, 295, 16815, 365, 257, 819, 733, 295, 5223, 586, 452, 2041, 51344, 51344, 307, 300, 436, 434, 516, 281, 312, 733, 295, 411, 12521, 82, 420, 746, 411, 300, 291, 458, 51482, 51482, 1391, 2295, 12, 30673, 6849, 293, 370, 286, 576, 312, 1566, 411, 1954, 4448, 490, 51784, 51784], "temperature": 0.0, "avg_logprob": -0.08931344350179037, "compression_ratio": 1.7777777777777777, "no_speech_prob": 9.606462117517367e-05}, {"id": 862, "seek": 483880, "start": 4838.8, "end": 4842.08, "text": " looking at this data set these two different groups can't be treated the", "tokens": [50364, 1237, 412, 341, 1412, 992, 613, 732, 819, 3935, 393, 380, 312, 8668, 264, 50528, 50528, 912, 636, 436, 362, 588, 819, 15501, 293, 286, 576, 1391, 7472, 50670, 50670, 552, 666, 732, 4994, 37560, 291, 458, 264, 1349, 484, 2753, 294, 309, 733, 295, 8198, 294, 51280, 51280, 257, 22820, 2020, 558, 456, 393, 312, 721, 300, 366, 731, 2380, 527, 2710, 51498, 51498, 7316, 293, 2082, 493, 527, 733, 295, 16367, 293, 721, 309, 1177, 380, 2514, 294, 257, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.08690612456377815, "compression_ratio": 1.6902654867256637, "no_speech_prob": 6.921379826962948e-05}, {"id": 863, "seek": 483880, "start": 4842.08, "end": 4844.92, "text": " same way they have very different behaviors and I would probably split", "tokens": [50364, 1237, 412, 341, 1412, 992, 613, 732, 819, 3935, 393, 380, 312, 8668, 264, 50528, 50528, 912, 636, 436, 362, 588, 819, 15501, 293, 286, 576, 1391, 7472, 50670, 50670, 552, 666, 732, 4994, 37560, 291, 458, 264, 1349, 484, 2753, 294, 309, 733, 295, 8198, 294, 51280, 51280, 257, 22820, 2020, 558, 456, 393, 312, 721, 300, 366, 731, 2380, 527, 2710, 51498, 51498, 7316, 293, 2082, 493, 527, 733, 295, 16367, 293, 721, 309, 1177, 380, 2514, 294, 257, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.08690612456377815, "compression_ratio": 1.6902654867256637, "no_speech_prob": 6.921379826962948e-05}, {"id": 864, "seek": 483880, "start": 4844.92, "end": 4857.12, "text": " them into two separate analyses you know the word outlier in it kind of exists in", "tokens": [50364, 1237, 412, 341, 1412, 992, 613, 732, 819, 3935, 393, 380, 312, 8668, 264, 50528, 50528, 912, 636, 436, 362, 588, 819, 15501, 293, 286, 576, 1391, 7472, 50670, 50670, 552, 666, 732, 4994, 37560, 291, 458, 264, 1349, 484, 2753, 294, 309, 733, 295, 8198, 294, 51280, 51280, 257, 22820, 2020, 558, 456, 393, 312, 721, 300, 366, 731, 2380, 527, 2710, 51498, 51498, 7316, 293, 2082, 493, 527, 733, 295, 16367, 293, 721, 309, 1177, 380, 2514, 294, 257, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.08690612456377815, "compression_ratio": 1.6902654867256637, "no_speech_prob": 6.921379826962948e-05}, {"id": 865, "seek": 483880, "start": 4857.12, "end": 4861.4800000000005, "text": " a statistical sense right there can be things that are well outside our normal", "tokens": [50364, 1237, 412, 341, 1412, 992, 613, 732, 819, 3935, 393, 380, 312, 8668, 264, 50528, 50528, 912, 636, 436, 362, 588, 819, 15501, 293, 286, 576, 1391, 7472, 50670, 50670, 552, 666, 732, 4994, 37560, 291, 458, 264, 1349, 484, 2753, 294, 309, 733, 295, 8198, 294, 51280, 51280, 257, 22820, 2020, 558, 456, 393, 312, 721, 300, 366, 731, 2380, 527, 2710, 51498, 51498, 7316, 293, 2082, 493, 527, 733, 295, 16367, 293, 721, 309, 1177, 380, 2514, 294, 257, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.08690612456377815, "compression_ratio": 1.6902654867256637, "no_speech_prob": 6.921379826962948e-05}, {"id": 866, "seek": 483880, "start": 4861.4800000000005, "end": 4865.400000000001, "text": " distribution and mess up our kind of metrics and things it doesn't exist in a", "tokens": [50364, 1237, 412, 341, 1412, 992, 613, 732, 819, 3935, 393, 380, 312, 8668, 264, 50528, 50528, 912, 636, 436, 362, 588, 819, 15501, 293, 286, 576, 1391, 7472, 50670, 50670, 552, 666, 732, 4994, 37560, 291, 458, 264, 1349, 484, 2753, 294, 309, 733, 295, 8198, 294, 51280, 51280, 257, 22820, 2020, 558, 456, 393, 312, 721, 300, 366, 731, 2380, 527, 2710, 51498, 51498, 7316, 293, 2082, 493, 527, 733, 295, 16367, 293, 721, 309, 1177, 380, 2514, 294, 257, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.08690612456377815, "compression_ratio": 1.6902654867256637, "no_speech_prob": 6.921379826962948e-05}, {"id": 867, "seek": 486540, "start": 4865.4, "end": 4869.0599999999995, "text": " real sense it doesn't exist in a sense of like oh things that we should like", "tokens": [50364, 957, 2020, 309, 1177, 380, 2514, 294, 257, 2020, 295, 411, 1954, 721, 300, 321, 820, 411, 50547, 50547, 11200, 420, 3507, 1314, 291, 458, 512, 295, 264, 881, 4420, 733, 295, 14310, 286, 600, 50904, 50904, 632, 294, 452, 993, 294, 1412, 4455, 575, 668, 538, 17343, 666, 484, 23646, 370, 12, 11880, 51218, 51218, 484, 23646, 293, 3701, 437, 366, 436, 689, 630, 436, 808, 490, 293, 309, 311, 733, 51566, 51566, 295, 2049, 294, 729, 4691, 3331, 300, 291, 4411, 534, 1021, 721, 466, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.07621846356234707, "compression_ratio": 1.6594827586206897, "no_speech_prob": 7.601163815706968e-05}, {"id": 868, "seek": 486540, "start": 4869.0599999999995, "end": 4876.2, "text": " ignore or throw away you know some of the most useful kind of insights I've", "tokens": [50364, 957, 2020, 309, 1177, 380, 2514, 294, 257, 2020, 295, 411, 1954, 721, 300, 321, 820, 411, 50547, 50547, 11200, 420, 3507, 1314, 291, 458, 512, 295, 264, 881, 4420, 733, 295, 14310, 286, 600, 50904, 50904, 632, 294, 452, 993, 294, 1412, 4455, 575, 668, 538, 17343, 666, 484, 23646, 370, 12, 11880, 51218, 51218, 484, 23646, 293, 3701, 437, 366, 436, 689, 630, 436, 808, 490, 293, 309, 311, 733, 51566, 51566, 295, 2049, 294, 729, 4691, 3331, 300, 291, 4411, 534, 1021, 721, 466, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.07621846356234707, "compression_ratio": 1.6594827586206897, "no_speech_prob": 7.601163815706968e-05}, {"id": 869, "seek": 486540, "start": 4876.2, "end": 4882.48, "text": " had in my life in data projects has been by digging into outliers so-called", "tokens": [50364, 957, 2020, 309, 1177, 380, 2514, 294, 257, 2020, 295, 411, 1954, 721, 300, 321, 820, 411, 50547, 50547, 11200, 420, 3507, 1314, 291, 458, 512, 295, 264, 881, 4420, 733, 295, 14310, 286, 600, 50904, 50904, 632, 294, 452, 993, 294, 1412, 4455, 575, 668, 538, 17343, 666, 484, 23646, 370, 12, 11880, 51218, 51218, 484, 23646, 293, 3701, 437, 366, 436, 689, 630, 436, 808, 490, 293, 309, 311, 733, 51566, 51566, 295, 2049, 294, 729, 4691, 3331, 300, 291, 4411, 534, 1021, 721, 466, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.07621846356234707, "compression_ratio": 1.6594827586206897, "no_speech_prob": 7.601163815706968e-05}, {"id": 870, "seek": 486540, "start": 4882.48, "end": 4889.44, "text": " outliers and understanding what are they where did they come from and it's kind", "tokens": [50364, 957, 2020, 309, 1177, 380, 2514, 294, 257, 2020, 295, 411, 1954, 721, 300, 321, 820, 411, 50547, 50547, 11200, 420, 3507, 1314, 291, 458, 512, 295, 264, 881, 4420, 733, 295, 14310, 286, 600, 50904, 50904, 632, 294, 452, 993, 294, 1412, 4455, 575, 668, 538, 17343, 666, 484, 23646, 370, 12, 11880, 51218, 51218, 484, 23646, 293, 3701, 437, 366, 436, 689, 630, 436, 808, 490, 293, 309, 311, 733, 51566, 51566, 295, 2049, 294, 729, 4691, 3331, 300, 291, 4411, 534, 1021, 721, 466, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.07621846356234707, "compression_ratio": 1.6594827586206897, "no_speech_prob": 7.601163815706968e-05}, {"id": 871, "seek": 486540, "start": 4889.44, "end": 4894.0, "text": " of often in those edge cases that you discover really important things about", "tokens": [50364, 957, 2020, 309, 1177, 380, 2514, 294, 257, 2020, 295, 411, 1954, 721, 300, 321, 820, 411, 50547, 50547, 11200, 420, 3507, 1314, 291, 458, 512, 295, 264, 881, 4420, 733, 295, 14310, 286, 600, 50904, 50904, 632, 294, 452, 993, 294, 1412, 4455, 575, 668, 538, 17343, 666, 484, 23646, 370, 12, 11880, 51218, 51218, 484, 23646, 293, 3701, 437, 366, 436, 689, 630, 436, 808, 490, 293, 309, 311, 733, 51566, 51566, 295, 2049, 294, 729, 4691, 3331, 300, 291, 4411, 534, 1021, 721, 466, 51794, 51794], "temperature": 0.0, "avg_logprob": -0.07621846356234707, "compression_ratio": 1.6594827586206897, "no_speech_prob": 7.601163815706968e-05}, {"id": 872, "seek": 489400, "start": 4894.0, "end": 4898.76, "text": " like where processes go wrong or about you know kinds of behaviors you didn't", "tokens": [50364, 411, 689, 7555, 352, 2085, 420, 466, 291, 458, 3685, 295, 15501, 291, 994, 380, 50602, 50602, 754, 458, 13135, 420, 6451, 466, 291, 458, 733, 295, 40244, 2740, 420, 50834, 50834, 1399, 2740, 597, 291, 534, 528, 281, 3191, 552, 412, 264, 4009, 570, 50994, 50994, 5911, 562, 291, 352, 666, 4265, 291, 434, 516, 281, 362, 544, 295, 729, 370, 51184, 51184, 1219, 484, 23646, 370, 1338, 286, 1116, 584, 1128, 12097, 484, 23646, 1553, 22858, 51686, 51686, 552, 293, 1419, 257, 5206, 337, 411, 3701, 689, 436, 1361, 490, 293, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09324494588006403, "compression_ratio": 1.7470817120622568, "no_speech_prob": 4.1984869312727824e-05}, {"id": 873, "seek": 489400, "start": 4898.76, "end": 4903.4, "text": " even know existed or indeed about you know kind of labeling problems or", "tokens": [50364, 411, 689, 7555, 352, 2085, 420, 466, 291, 458, 3685, 295, 15501, 291, 994, 380, 50602, 50602, 754, 458, 13135, 420, 6451, 466, 291, 458, 733, 295, 40244, 2740, 420, 50834, 50834, 1399, 2740, 597, 291, 534, 528, 281, 3191, 552, 412, 264, 4009, 570, 50994, 50994, 5911, 562, 291, 352, 666, 4265, 291, 434, 516, 281, 362, 544, 295, 729, 370, 51184, 51184, 1219, 484, 23646, 370, 1338, 286, 1116, 584, 1128, 12097, 484, 23646, 1553, 22858, 51686, 51686, 552, 293, 1419, 257, 5206, 337, 411, 3701, 689, 436, 1361, 490, 293, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09324494588006403, "compression_ratio": 1.7470817120622568, "no_speech_prob": 4.1984869312727824e-05}, {"id": 874, "seek": 489400, "start": 4903.4, "end": 4906.6, "text": " process problems which you really want to fix them at the source because", "tokens": [50364, 411, 689, 7555, 352, 2085, 420, 466, 291, 458, 3685, 295, 15501, 291, 994, 380, 50602, 50602, 754, 458, 13135, 420, 6451, 466, 291, 458, 733, 295, 40244, 2740, 420, 50834, 50834, 1399, 2740, 597, 291, 534, 528, 281, 3191, 552, 412, 264, 4009, 570, 50994, 50994, 5911, 562, 291, 352, 666, 4265, 291, 434, 516, 281, 362, 544, 295, 729, 370, 51184, 51184, 1219, 484, 23646, 370, 1338, 286, 1116, 584, 1128, 12097, 484, 23646, 1553, 22858, 51686, 51686, 552, 293, 1419, 257, 5206, 337, 411, 3701, 689, 436, 1361, 490, 293, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09324494588006403, "compression_ratio": 1.7470817120622568, "no_speech_prob": 4.1984869312727824e-05}, {"id": 875, "seek": 489400, "start": 4906.6, "end": 4910.4, "text": " otherwise when you go into production you're going to have more of those so", "tokens": [50364, 411, 689, 7555, 352, 2085, 420, 466, 291, 458, 3685, 295, 15501, 291, 994, 380, 50602, 50602, 754, 458, 13135, 420, 6451, 466, 291, 458, 733, 295, 40244, 2740, 420, 50834, 50834, 1399, 2740, 597, 291, 534, 528, 281, 3191, 552, 412, 264, 4009, 570, 50994, 50994, 5911, 562, 291, 352, 666, 4265, 291, 434, 516, 281, 362, 544, 295, 729, 370, 51184, 51184, 1219, 484, 23646, 370, 1338, 286, 1116, 584, 1128, 12097, 484, 23646, 1553, 22858, 51686, 51686, 552, 293, 1419, 257, 5206, 337, 411, 3701, 689, 436, 1361, 490, 293, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09324494588006403, "compression_ratio": 1.7470817120622568, "no_speech_prob": 4.1984869312727824e-05}, {"id": 876, "seek": 489400, "start": 4910.4, "end": 4920.44, "text": " called outliers so yeah I'd say never delete outliers without investigating", "tokens": [50364, 411, 689, 7555, 352, 2085, 420, 466, 291, 458, 3685, 295, 15501, 291, 994, 380, 50602, 50602, 754, 458, 13135, 420, 6451, 466, 291, 458, 733, 295, 40244, 2740, 420, 50834, 50834, 1399, 2740, 597, 291, 534, 528, 281, 3191, 552, 412, 264, 4009, 570, 50994, 50994, 5911, 562, 291, 352, 666, 4265, 291, 434, 516, 281, 362, 544, 295, 729, 370, 51184, 51184, 1219, 484, 23646, 370, 1338, 286, 1116, 584, 1128, 12097, 484, 23646, 1553, 22858, 51686, 51686, 552, 293, 1419, 257, 5206, 337, 411, 3701, 689, 436, 1361, 490, 293, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09324494588006403, "compression_ratio": 1.7470817120622568, "no_speech_prob": 4.1984869312727824e-05}, {"id": 877, "seek": 489400, "start": 4920.44, "end": 4923.92, "text": " them and having a strategy for like understanding where they came from and", "tokens": [50364, 411, 689, 7555, 352, 2085, 420, 466, 291, 458, 3685, 295, 15501, 291, 994, 380, 50602, 50602, 754, 458, 13135, 420, 6451, 466, 291, 458, 733, 295, 40244, 2740, 420, 50834, 50834, 1399, 2740, 597, 291, 534, 528, 281, 3191, 552, 412, 264, 4009, 570, 50994, 50994, 5911, 562, 291, 352, 666, 4265, 291, 434, 516, 281, 362, 544, 295, 729, 370, 51184, 51184, 1219, 484, 23646, 370, 1338, 286, 1116, 584, 1128, 12097, 484, 23646, 1553, 22858, 51686, 51686, 552, 293, 1419, 257, 5206, 337, 411, 3701, 689, 436, 1361, 490, 293, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09324494588006403, "compression_ratio": 1.7470817120622568, "no_speech_prob": 4.1984869312727824e-05}, {"id": 878, "seek": 492392, "start": 4923.92, "end": 4930.12, "text": " like what should you do about them all right so now that we've got a trained", "tokens": [50364, 411, 437, 820, 291, 360, 466, 552, 439, 558, 370, 586, 300, 321, 600, 658, 257, 8895, 50674, 50674, 2316, 291, 603, 536, 300, 309, 767, 36896, 291, 458, 534, 257, 688, 411, 257, 50922, 50922, 2370, 7318, 33347, 293, 291, 458, 4696, 264, 9995, 291, 603, 483, 490, 516, 51114, 51114, 807, 341, 1399, 307, 11611, 257, 2020, 295, 49828, 286, 478, 411, 1954, 1338, 341, 51420, 51420, 1542, 411, 1507, 286, 600, 1612, 949, 291, 458, 411, 257, 857, 544, 1349, 88, 293, 512, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.09096174240112305, "compression_ratio": 1.6846846846846846, "no_speech_prob": 5.6492990552214906e-05}, {"id": 879, "seek": 492392, "start": 4930.12, "end": 4935.08, "text": " model you'll see that it actually behaves you know really a lot like a", "tokens": [50364, 411, 437, 820, 291, 360, 466, 552, 439, 558, 370, 586, 300, 321, 600, 658, 257, 8895, 50674, 50674, 2316, 291, 603, 536, 300, 309, 767, 36896, 291, 458, 534, 257, 688, 411, 257, 50922, 50922, 2370, 7318, 33347, 293, 291, 458, 4696, 264, 9995, 291, 603, 483, 490, 516, 51114, 51114, 807, 341, 1399, 307, 11611, 257, 2020, 295, 49828, 286, 478, 411, 1954, 1338, 341, 51420, 51420, 1542, 411, 1507, 286, 600, 1612, 949, 291, 458, 411, 257, 857, 544, 1349, 88, 293, 512, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.09096174240112305, "compression_ratio": 1.6846846846846846, "no_speech_prob": 5.6492990552214906e-05}, {"id": 880, "seek": 492392, "start": 4935.08, "end": 4938.92, "text": " fast AI learner and you know hopefully the impression you'll get from going", "tokens": [50364, 411, 437, 820, 291, 360, 466, 552, 439, 558, 370, 586, 300, 321, 600, 658, 257, 8895, 50674, 50674, 2316, 291, 603, 536, 300, 309, 767, 36896, 291, 458, 534, 257, 688, 411, 257, 50922, 50922, 2370, 7318, 33347, 293, 291, 458, 4696, 264, 9995, 291, 603, 483, 490, 516, 51114, 51114, 807, 341, 1399, 307, 11611, 257, 2020, 295, 49828, 286, 478, 411, 1954, 1338, 341, 51420, 51420, 1542, 411, 1507, 286, 600, 1612, 949, 291, 458, 411, 257, 857, 544, 1349, 88, 293, 512, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.09096174240112305, "compression_ratio": 1.6846846846846846, "no_speech_prob": 5.6492990552214906e-05}, {"id": 881, "seek": 492392, "start": 4938.92, "end": 4945.04, "text": " through this process is largely a sense of familiarity I'm like oh yeah this", "tokens": [50364, 411, 437, 820, 291, 360, 466, 552, 439, 558, 370, 586, 300, 321, 600, 658, 257, 8895, 50674, 50674, 2316, 291, 603, 536, 300, 309, 767, 36896, 291, 458, 534, 257, 688, 411, 257, 50922, 50922, 2370, 7318, 33347, 293, 291, 458, 4696, 264, 9995, 291, 603, 483, 490, 516, 51114, 51114, 807, 341, 1399, 307, 11611, 257, 2020, 295, 49828, 286, 478, 411, 1954, 1338, 341, 51420, 51420, 1542, 411, 1507, 286, 600, 1612, 949, 291, 458, 411, 257, 857, 544, 1349, 88, 293, 512, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.09096174240112305, "compression_ratio": 1.6846846846846846, "no_speech_prob": 5.6492990552214906e-05}, {"id": 882, "seek": 492392, "start": 4945.04, "end": 4950.28, "text": " looks like stuff I've seen before you know like a bit more wordy and some", "tokens": [50364, 411, 437, 820, 291, 360, 466, 552, 439, 558, 370, 586, 300, 321, 600, 658, 257, 8895, 50674, 50674, 2316, 291, 603, 536, 300, 309, 767, 36896, 291, 458, 534, 257, 688, 411, 257, 50922, 50922, 2370, 7318, 33347, 293, 291, 458, 4696, 264, 9995, 291, 603, 483, 490, 516, 51114, 51114, 807, 341, 1399, 307, 11611, 257, 2020, 295, 49828, 286, 478, 411, 1954, 1338, 341, 51420, 51420, 1542, 411, 1507, 286, 600, 1612, 949, 291, 458, 411, 257, 857, 544, 1349, 88, 293, 512, 51682, 51682], "temperature": 0.0, "avg_logprob": -0.09096174240112305, "compression_ratio": 1.6846846846846846, "no_speech_prob": 5.6492990552214906e-05}, {"id": 883, "seek": 495028, "start": 4950.28, "end": 4954.48, "text": " slight changes but it really is very very similar to the way we've done it", "tokens": [50364, 4036, 2962, 457, 309, 534, 307, 588, 588, 2531, 281, 264, 636, 321, 600, 1096, 309, 50574, 50574, 949, 570, 586, 300, 321, 600, 658, 257, 8895, 21110, 2831, 813, 33347, 321, 50820, 50820, 393, 818, 6069, 293, 586, 321, 434, 516, 281, 1320, 294, 527, 1412, 992, 490, 264, 48751, 22631, 51134, 51134, 1500, 3991, 293, 370, 300, 311, 516, 281, 976, 505, 527, 21264, 597, 321, 393, 4193, 281, 51456, 51456], "temperature": 0.0, "avg_logprob": -0.07839427947998047, "compression_ratio": 1.6888888888888889, "no_speech_prob": 5.22494810866192e-05}, {"id": 884, "seek": 495028, "start": 4954.48, "end": 4959.4, "text": " before because now that we've got a trained trainer rather than learner we", "tokens": [50364, 4036, 2962, 457, 309, 534, 307, 588, 588, 2531, 281, 264, 636, 321, 600, 1096, 309, 50574, 50574, 949, 570, 586, 300, 321, 600, 658, 257, 8895, 21110, 2831, 813, 33347, 321, 50820, 50820, 393, 818, 6069, 293, 586, 321, 434, 516, 281, 1320, 294, 527, 1412, 992, 490, 264, 48751, 22631, 51134, 51134, 1500, 3991, 293, 370, 300, 311, 516, 281, 976, 505, 527, 21264, 597, 321, 393, 4193, 281, 51456, 51456], "temperature": 0.0, "avg_logprob": -0.07839427947998047, "compression_ratio": 1.6888888888888889, "no_speech_prob": 5.22494810866192e-05}, {"id": 885, "seek": 495028, "start": 4959.4, "end": 4965.679999999999, "text": " can call predict and now we're going to pass in our data set from the Kaggle", "tokens": [50364, 4036, 2962, 457, 309, 534, 307, 588, 588, 2531, 281, 264, 636, 321, 600, 1096, 309, 50574, 50574, 949, 570, 586, 300, 321, 600, 658, 257, 8895, 21110, 2831, 813, 33347, 321, 50820, 50820, 393, 818, 6069, 293, 586, 321, 434, 516, 281, 1320, 294, 527, 1412, 992, 490, 264, 48751, 22631, 51134, 51134, 1500, 3991, 293, 370, 300, 311, 516, 281, 976, 505, 527, 21264, 597, 321, 393, 4193, 281, 51456, 51456], "temperature": 0.0, "avg_logprob": -0.07839427947998047, "compression_ratio": 1.6888888888888889, "no_speech_prob": 5.22494810866192e-05}, {"id": 886, "seek": 495028, "start": 4965.679999999999, "end": 4972.12, "text": " test file and so that's going to give us our predictions which we can cast to", "tokens": [50364, 4036, 2962, 457, 309, 534, 307, 588, 588, 2531, 281, 264, 636, 321, 600, 1096, 309, 50574, 50574, 949, 570, 586, 300, 321, 600, 658, 257, 8895, 21110, 2831, 813, 33347, 321, 50820, 50820, 393, 818, 6069, 293, 586, 321, 434, 516, 281, 1320, 294, 527, 1412, 992, 490, 264, 48751, 22631, 51134, 51134, 1500, 3991, 293, 370, 300, 311, 516, 281, 976, 505, 527, 21264, 597, 321, 393, 4193, 281, 51456, 51456], "temperature": 0.0, "avg_logprob": -0.07839427947998047, "compression_ratio": 1.6888888888888889, "no_speech_prob": 5.22494810866192e-05}, {"id": 887, "seek": 497212, "start": 4972.12, "end": 4984.5599999999995, "text": " float and here they are so here are the predictions we made of similarity now", "tokens": [50364, 15706, 293, 510, 436, 366, 370, 510, 366, 264, 21264, 321, 1027, 295, 32194, 586, 50986, 50986, 797, 406, 445, 337, 428, 15743, 457, 611, 498, 428, 23930, 1009, 574, 412, 552, 51218, 51218, 1009, 558, 293, 25873, 286, 2956, 412, 1596, 257, 1326, 48751, 22631, 43782, 490, 51608, 51608, 661, 561, 337, 341, 6211, 293, 6217, 439, 295, 552, 632, 264, 1154, 321, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.08415823123034309, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.535328869475052e-05}, {"id": 888, "seek": 497212, "start": 4984.5599999999995, "end": 4989.2, "text": " again not just for your inputs but also if your outputs always look at them", "tokens": [50364, 15706, 293, 510, 436, 366, 370, 510, 366, 264, 21264, 321, 1027, 295, 32194, 586, 50986, 50986, 797, 406, 445, 337, 428, 15743, 457, 611, 498, 428, 23930, 1009, 574, 412, 552, 51218, 51218, 1009, 558, 293, 25873, 286, 2956, 412, 1596, 257, 1326, 48751, 22631, 43782, 490, 51608, 51608, 661, 561, 337, 341, 6211, 293, 6217, 439, 295, 552, 632, 264, 1154, 321, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.08415823123034309, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.535328869475052e-05}, {"id": 889, "seek": 497212, "start": 4989.2, "end": 4997.0, "text": " always right and interestingly I looked at quite a few Kaggle notebooks from", "tokens": [50364, 15706, 293, 510, 436, 366, 370, 510, 366, 264, 21264, 321, 1027, 295, 32194, 586, 50986, 50986, 797, 406, 445, 337, 428, 15743, 457, 611, 498, 428, 23930, 1009, 574, 412, 552, 51218, 51218, 1009, 558, 293, 25873, 286, 2956, 412, 1596, 257, 1326, 48751, 22631, 43782, 490, 51608, 51608, 661, 561, 337, 341, 6211, 293, 6217, 439, 295, 552, 632, 264, 1154, 321, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.08415823123034309, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.535328869475052e-05}, {"id": 890, "seek": 497212, "start": 4997.0, "end": 5001.76, "text": " other people for this competition and nearly all of them had the problem we", "tokens": [50364, 15706, 293, 510, 436, 366, 370, 510, 366, 264, 21264, 321, 1027, 295, 32194, 586, 50986, 50986, 797, 406, 445, 337, 428, 15743, 457, 611, 498, 428, 23930, 1009, 574, 412, 552, 51218, 51218, 1009, 558, 293, 25873, 286, 2956, 412, 1596, 257, 1326, 48751, 22631, 43782, 490, 51608, 51608, 661, 561, 337, 341, 6211, 293, 6217, 439, 295, 552, 632, 264, 1154, 321, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.08415823123034309, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.535328869475052e-05}, {"id": 891, "seek": 500176, "start": 5001.76, "end": 5008.84, "text": " have right now which is negative predictions and predictions over one so", "tokens": [50364, 362, 558, 586, 597, 307, 3671, 21264, 293, 21264, 670, 472, 370, 50718, 50718, 286, 603, 312, 4099, 291, 577, 281, 3191, 341, 294, 257, 544, 2296, 636, 1310, 4696, 294, 264, 51016, 51016, 958, 6898, 457, 337, 586, 291, 458, 321, 727, 412, 1935, 445, 3098, 613, 766, 51296, 51296, 558, 570, 321, 458, 300, 6022, 295, 264, 13444, 366, 516, 281, 312, 3801, 813, 472, 51520, 51520, 420, 4356, 813, 4018, 370, 527, 20009, 17619, 486, 2138, 3470, 498, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.0859899128184599, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.00015840538253542036}, {"id": 892, "seek": 500176, "start": 5008.84, "end": 5014.8, "text": " I'll be showing you how to fix this in a more proper way maybe hopefully in the", "tokens": [50364, 362, 558, 586, 597, 307, 3671, 21264, 293, 21264, 670, 472, 370, 50718, 50718, 286, 603, 312, 4099, 291, 577, 281, 3191, 341, 294, 257, 544, 2296, 636, 1310, 4696, 294, 264, 51016, 51016, 958, 6898, 457, 337, 586, 291, 458, 321, 727, 412, 1935, 445, 3098, 613, 766, 51296, 51296, 558, 570, 321, 458, 300, 6022, 295, 264, 13444, 366, 516, 281, 312, 3801, 813, 472, 51520, 51520, 420, 4356, 813, 4018, 370, 527, 20009, 17619, 486, 2138, 3470, 498, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.0859899128184599, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.00015840538253542036}, {"id": 893, "seek": 500176, "start": 5014.8, "end": 5020.400000000001, "text": " next lesson but for now you know we could at least just round these off", "tokens": [50364, 362, 558, 586, 597, 307, 3671, 21264, 293, 21264, 670, 472, 370, 50718, 50718, 286, 603, 312, 4099, 291, 577, 281, 3191, 341, 294, 257, 544, 2296, 636, 1310, 4696, 294, 264, 51016, 51016, 958, 6898, 457, 337, 586, 291, 458, 321, 727, 412, 1935, 445, 3098, 613, 766, 51296, 51296, 558, 570, 321, 458, 300, 6022, 295, 264, 13444, 366, 516, 281, 312, 3801, 813, 472, 51520, 51520, 420, 4356, 813, 4018, 370, 527, 20009, 17619, 486, 2138, 3470, 498, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.0859899128184599, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.00015840538253542036}, {"id": 894, "seek": 500176, "start": 5020.400000000001, "end": 5024.88, "text": " right because we know that none of the scores are going to be bigger than one", "tokens": [50364, 362, 558, 586, 597, 307, 3671, 21264, 293, 21264, 670, 472, 370, 50718, 50718, 286, 603, 312, 4099, 291, 577, 281, 3191, 341, 294, 257, 544, 2296, 636, 1310, 4696, 294, 264, 51016, 51016, 958, 6898, 457, 337, 586, 291, 458, 321, 727, 412, 1935, 445, 3098, 613, 766, 51296, 51296, 558, 570, 321, 458, 300, 6022, 295, 264, 13444, 366, 516, 281, 312, 3801, 813, 472, 51520, 51520, 420, 4356, 813, 4018, 370, 527, 20009, 17619, 486, 2138, 3470, 498, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.0859899128184599, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.00015840538253542036}, {"id": 895, "seek": 500176, "start": 5024.88, "end": 5028.52, "text": " or smaller than zero so our correlation coefficient will definitely improve if", "tokens": [50364, 362, 558, 586, 597, 307, 3671, 21264, 293, 21264, 670, 472, 370, 50718, 50718, 286, 603, 312, 4099, 291, 577, 281, 3191, 341, 294, 257, 544, 2296, 636, 1310, 4696, 294, 264, 51016, 51016, 958, 6898, 457, 337, 586, 291, 458, 321, 727, 412, 1935, 445, 3098, 613, 766, 51296, 51296, 558, 570, 321, 458, 300, 6022, 295, 264, 13444, 366, 516, 281, 312, 3801, 813, 472, 51520, 51520, 420, 4356, 813, 4018, 370, 527, 20009, 17619, 486, 2138, 3470, 498, 51702, 51702], "temperature": 0.0, "avg_logprob": -0.0859899128184599, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.00015840538253542036}, {"id": 896, "seek": 502852, "start": 5028.52, "end": 5033.4800000000005, "text": " we at least round this up to zero and round this down to one as I say there", "tokens": [50364, 321, 412, 1935, 3098, 341, 493, 281, 4018, 293, 3098, 341, 760, 281, 472, 382, 286, 584, 456, 50612, 50612, 366, 1101, 2098, 281, 360, 341, 457, 300, 311, 3297, 1101, 813, 1825, 370, 294, 50886, 50886, 25878, 284, 339, 291, 1062, 1604, 490, 562, 321, 2956, 412, 2158, 456, 311, 257, 551, 1219, 51060, 51060, 7353, 293, 300, 486, 7353, 1203, 833, 4018, 281, 4018, 293, 1203, 670, 472, 281, 51280, 51280, 472, 293, 370, 586, 300, 1542, 709, 1101, 370, 510, 311, 527, 21264, 370, 48751, 22631, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.1171946781937794, "compression_ratio": 1.7813953488372094, "no_speech_prob": 4.005396840511821e-05}, {"id": 897, "seek": 502852, "start": 5033.4800000000005, "end": 5038.96, "text": " are better ways to do this but that's certainly better than nothing so in", "tokens": [50364, 321, 412, 1935, 3098, 341, 493, 281, 4018, 293, 3098, 341, 760, 281, 472, 382, 286, 584, 456, 50612, 50612, 366, 1101, 2098, 281, 360, 341, 457, 300, 311, 3297, 1101, 813, 1825, 370, 294, 50886, 50886, 25878, 284, 339, 291, 1062, 1604, 490, 562, 321, 2956, 412, 2158, 456, 311, 257, 551, 1219, 51060, 51060, 7353, 293, 300, 486, 7353, 1203, 833, 4018, 281, 4018, 293, 1203, 670, 472, 281, 51280, 51280, 472, 293, 370, 586, 300, 1542, 709, 1101, 370, 510, 311, 527, 21264, 370, 48751, 22631, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.1171946781937794, "compression_ratio": 1.7813953488372094, "no_speech_prob": 4.005396840511821e-05}, {"id": 898, "seek": 502852, "start": 5038.96, "end": 5042.4400000000005, "text": " pytorch you might remember from when we looked at value there's a thing called", "tokens": [50364, 321, 412, 1935, 3098, 341, 493, 281, 4018, 293, 3098, 341, 760, 281, 472, 382, 286, 584, 456, 50612, 50612, 366, 1101, 2098, 281, 360, 341, 457, 300, 311, 3297, 1101, 813, 1825, 370, 294, 50886, 50886, 25878, 284, 339, 291, 1062, 1604, 490, 562, 321, 2956, 412, 2158, 456, 311, 257, 551, 1219, 51060, 51060, 7353, 293, 300, 486, 7353, 1203, 833, 4018, 281, 4018, 293, 1203, 670, 472, 281, 51280, 51280, 472, 293, 370, 586, 300, 1542, 709, 1101, 370, 510, 311, 527, 21264, 370, 48751, 22631, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.1171946781937794, "compression_ratio": 1.7813953488372094, "no_speech_prob": 4.005396840511821e-05}, {"id": 899, "seek": 502852, "start": 5042.4400000000005, "end": 5046.84, "text": " clip and that will clip everything under zero to zero and everything over one to", "tokens": [50364, 321, 412, 1935, 3098, 341, 493, 281, 4018, 293, 3098, 341, 760, 281, 472, 382, 286, 584, 456, 50612, 50612, 366, 1101, 2098, 281, 360, 341, 457, 300, 311, 3297, 1101, 813, 1825, 370, 294, 50886, 50886, 25878, 284, 339, 291, 1062, 1604, 490, 562, 321, 2956, 412, 2158, 456, 311, 257, 551, 1219, 51060, 51060, 7353, 293, 300, 486, 7353, 1203, 833, 4018, 281, 4018, 293, 1203, 670, 472, 281, 51280, 51280, 472, 293, 370, 586, 300, 1542, 709, 1101, 370, 510, 311, 527, 21264, 370, 48751, 22631, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.1171946781937794, "compression_ratio": 1.7813953488372094, "no_speech_prob": 4.005396840511821e-05}, {"id": 900, "seek": 502852, "start": 5046.84, "end": 5056.120000000001, "text": " one and so now that looks much better so here's our predictions so Kaggle", "tokens": [50364, 321, 412, 1935, 3098, 341, 493, 281, 4018, 293, 3098, 341, 760, 281, 472, 382, 286, 584, 456, 50612, 50612, 366, 1101, 2098, 281, 360, 341, 457, 300, 311, 3297, 1101, 813, 1825, 370, 294, 50886, 50886, 25878, 284, 339, 291, 1062, 1604, 490, 562, 321, 2956, 412, 2158, 456, 311, 257, 551, 1219, 51060, 51060, 7353, 293, 300, 486, 7353, 1203, 833, 4018, 281, 4018, 293, 1203, 670, 472, 281, 51280, 51280, 472, 293, 370, 586, 300, 1542, 709, 1101, 370, 510, 311, 527, 21264, 370, 48751, 22631, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.1171946781937794, "compression_ratio": 1.7813953488372094, "no_speech_prob": 4.005396840511821e-05}, {"id": 901, "seek": 505612, "start": 5056.12, "end": 5062.4, "text": " expects submissions to generally be in a CSV file and hacking face datasets it", "tokens": [50364, 33280, 40429, 281, 5101, 312, 294, 257, 48814, 3991, 293, 31422, 1851, 42856, 309, 50678, 50678, 733, 295, 1542, 257, 688, 411, 4565, 296, 534, 321, 393, 1884, 527, 23689, 3991, 490, 50892, 50892, 365, 527, 732, 13766, 1219, 48814, 293, 456, 321, 352, 300, 311, 1936, 309, 370, 1338, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.14910429495352287, "compression_ratio": 1.4777070063694266, "no_speech_prob": 8.343542140210047e-05}, {"id": 902, "seek": 505612, "start": 5062.4, "end": 5066.68, "text": " kind of looks a lot like pandas really we can create our submission file from", "tokens": [50364, 33280, 40429, 281, 5101, 312, 294, 257, 48814, 3991, 293, 31422, 1851, 42856, 309, 50678, 50678, 733, 295, 1542, 257, 688, 411, 4565, 296, 534, 321, 393, 1884, 527, 23689, 3991, 490, 50892, 50892, 365, 527, 732, 13766, 1219, 48814, 293, 456, 321, 352, 300, 311, 1936, 309, 370, 1338, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.14910429495352287, "compression_ratio": 1.4777070063694266, "no_speech_prob": 8.343542140210047e-05}, {"id": 903, "seek": 505612, "start": 5066.68, "end": 5084.48, "text": " with our two columns called CSV and there we go that's basically it so yeah", "tokens": [50364, 33280, 40429, 281, 5101, 312, 294, 257, 48814, 3991, 293, 31422, 1851, 42856, 309, 50678, 50678, 733, 295, 1542, 257, 688, 411, 4565, 296, 534, 321, 393, 1884, 527, 23689, 3991, 490, 50892, 50892, 365, 527, 732, 13766, 1219, 48814, 293, 456, 321, 352, 300, 311, 1936, 309, 370, 1338, 51782, 51782], "temperature": 0.0, "avg_logprob": -0.14910429495352287, "compression_ratio": 1.4777070063694266, "no_speech_prob": 8.343542140210047e-05}, {"id": 904, "seek": 508448, "start": 5084.48, "end": 5090.5599999999995, "text": " you know it's it's it's kind of nice to see how you know in the sense how far", "tokens": [50364, 291, 458, 309, 311, 309, 311, 309, 311, 733, 295, 1481, 281, 536, 577, 291, 458, 294, 264, 2020, 577, 1400, 50668, 50668, 2452, 2539, 575, 808, 1670, 321, 1409, 341, 1164, 257, 1326, 924, 2057, 300, 300, 50888, 50888, 13434, 291, 458, 456, 366, 3866, 15148, 926, 281, 733, 295, 360, 341, 264, 51126, 51126, 912, 551, 321, 393, 291, 458, 764, 552, 294, 3866, 3861, 3179, 436, 439, 574, 51426, 51426, 733, 295, 1238, 4963, 436, 434, 23551, 22080, 12, 22864, 293, 426, 45196, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.080995421939426, "compression_ratio": 1.731818181818182, "no_speech_prob": 6.920409214217216e-05}, {"id": 905, "seek": 508448, "start": 5090.5599999999995, "end": 5094.959999999999, "text": " deep learning has come since we started this course a few years ago that that", "tokens": [50364, 291, 458, 309, 311, 309, 311, 309, 311, 733, 295, 1481, 281, 536, 577, 291, 458, 294, 264, 2020, 577, 1400, 50668, 50668, 2452, 2539, 575, 808, 1670, 321, 1409, 341, 1164, 257, 1326, 924, 2057, 300, 300, 50888, 50888, 13434, 291, 458, 456, 366, 3866, 15148, 926, 281, 733, 295, 360, 341, 264, 51126, 51126, 912, 551, 321, 393, 291, 458, 764, 552, 294, 3866, 3861, 3179, 436, 439, 574, 51426, 51426, 733, 295, 1238, 4963, 436, 434, 23551, 22080, 12, 22864, 293, 426, 45196, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.080995421939426, "compression_ratio": 1.731818181818182, "no_speech_prob": 6.920409214217216e-05}, {"id": 906, "seek": 508448, "start": 5094.959999999999, "end": 5099.719999999999, "text": " nowadays you know there are multiple libraries around to kind of do this the", "tokens": [50364, 291, 458, 309, 311, 309, 311, 309, 311, 733, 295, 1481, 281, 536, 577, 291, 458, 294, 264, 2020, 577, 1400, 50668, 50668, 2452, 2539, 575, 808, 1670, 321, 1409, 341, 1164, 257, 1326, 924, 2057, 300, 300, 50888, 50888, 13434, 291, 458, 456, 366, 3866, 15148, 926, 281, 733, 295, 360, 341, 264, 51126, 51126, 912, 551, 321, 393, 291, 458, 764, 552, 294, 3866, 3861, 3179, 436, 439, 574, 51426, 51426, 733, 295, 1238, 4963, 436, 434, 23551, 22080, 12, 22864, 293, 426, 45196, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.080995421939426, "compression_ratio": 1.731818181818182, "no_speech_prob": 6.920409214217216e-05}, {"id": 907, "seek": 508448, "start": 5099.719999999999, "end": 5105.719999999999, "text": " same thing we can you know use them in multiple application areas they all look", "tokens": [50364, 291, 458, 309, 311, 309, 311, 309, 311, 733, 295, 1481, 281, 536, 577, 291, 458, 294, 264, 2020, 577, 1400, 50668, 50668, 2452, 2539, 575, 808, 1670, 321, 1409, 341, 1164, 257, 1326, 924, 2057, 300, 300, 50888, 50888, 13434, 291, 458, 456, 366, 3866, 15148, 926, 281, 733, 295, 360, 341, 264, 51126, 51126, 912, 551, 321, 393, 291, 458, 764, 552, 294, 3866, 3861, 3179, 436, 439, 574, 51426, 51426, 733, 295, 1238, 4963, 436, 434, 23551, 22080, 12, 22864, 293, 426, 45196, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.080995421939426, "compression_ratio": 1.731818181818182, "no_speech_prob": 6.920409214217216e-05}, {"id": 908, "seek": 508448, "start": 5105.719999999999, "end": 5111.959999999999, "text": " kind of pretty familiar they're reasonably beginner-friendly and NLP", "tokens": [50364, 291, 458, 309, 311, 309, 311, 309, 311, 733, 295, 1481, 281, 536, 577, 291, 458, 294, 264, 2020, 577, 1400, 50668, 50668, 2452, 2539, 575, 808, 1670, 321, 1409, 341, 1164, 257, 1326, 924, 2057, 300, 300, 50888, 50888, 13434, 291, 458, 456, 366, 3866, 15148, 926, 281, 733, 295, 360, 341, 264, 51126, 51126, 912, 551, 321, 393, 291, 458, 764, 552, 294, 3866, 3861, 3179, 436, 439, 574, 51426, 51426, 733, 295, 1238, 4963, 436, 434, 23551, 22080, 12, 22864, 293, 426, 45196, 51738, 51738], "temperature": 0.0, "avg_logprob": -0.080995421939426, "compression_ratio": 1.731818181818182, "no_speech_prob": 6.920409214217216e-05}, {"id": 909, "seek": 511196, "start": 5111.96, "end": 5118.56, "text": " because it's kind of like the most recent area that's really become", "tokens": [50364, 570, 309, 311, 733, 295, 411, 264, 881, 5162, 1859, 300, 311, 534, 1813, 50694, 50694, 4942, 294, 264, 1036, 1064, 420, 732, 307, 1391, 689, 264, 3880, 50904, 50904, 4786, 366, 337, 291, 458, 955, 10641, 1293, 294, 2132, 293, 51196, 51196, 6841, 2144, 293, 370, 498, 291, 434, 1237, 281, 1322, 257, 18578, 337, 1365, 51544, 51544, 472, 295, 264, 2141, 721, 300, 691, 33290, 574, 337, 291, 458, 300, 436, 603, 1029, 307, 411, 731, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.09439882418004478, "compression_ratio": 1.6273584905660377, "no_speech_prob": 0.000123342324513942}, {"id": 910, "seek": 511196, "start": 5118.56, "end": 5122.76, "text": " effective in the last year or two is probably where the biggest", "tokens": [50364, 570, 309, 311, 733, 295, 411, 264, 881, 5162, 1859, 300, 311, 534, 1813, 50694, 50694, 4942, 294, 264, 1036, 1064, 420, 732, 307, 1391, 689, 264, 3880, 50904, 50904, 4786, 366, 337, 291, 458, 955, 10641, 1293, 294, 2132, 293, 51196, 51196, 6841, 2144, 293, 370, 498, 291, 434, 1237, 281, 1322, 257, 18578, 337, 1365, 51544, 51544, 472, 295, 264, 2141, 721, 300, 691, 33290, 574, 337, 291, 458, 300, 436, 603, 1029, 307, 411, 731, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.09439882418004478, "compression_ratio": 1.6273584905660377, "no_speech_prob": 0.000123342324513942}, {"id": 911, "seek": 511196, "start": 5122.76, "end": 5128.6, "text": " opportunities are for you know big wins both in research and", "tokens": [50364, 570, 309, 311, 733, 295, 411, 264, 881, 5162, 1859, 300, 311, 534, 1813, 50694, 50694, 4942, 294, 264, 1036, 1064, 420, 732, 307, 1391, 689, 264, 3880, 50904, 50904, 4786, 366, 337, 291, 458, 955, 10641, 1293, 294, 2132, 293, 51196, 51196, 6841, 2144, 293, 370, 498, 291, 434, 1237, 281, 1322, 257, 18578, 337, 1365, 51544, 51544, 472, 295, 264, 2141, 721, 300, 691, 33290, 574, 337, 291, 458, 300, 436, 603, 1029, 307, 411, 731, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.09439882418004478, "compression_ratio": 1.6273584905660377, "no_speech_prob": 0.000123342324513942}, {"id": 912, "seek": 511196, "start": 5128.6, "end": 5135.56, "text": " commercialization and so if you're looking to build a startup for example", "tokens": [50364, 570, 309, 311, 733, 295, 411, 264, 881, 5162, 1859, 300, 311, 534, 1813, 50694, 50694, 4942, 294, 264, 1036, 1064, 420, 732, 307, 1391, 689, 264, 3880, 50904, 50904, 4786, 366, 337, 291, 458, 955, 10641, 1293, 294, 2132, 293, 51196, 51196, 6841, 2144, 293, 370, 498, 291, 434, 1237, 281, 1322, 257, 18578, 337, 1365, 51544, 51544, 472, 295, 264, 2141, 721, 300, 691, 33290, 574, 337, 291, 458, 300, 436, 603, 1029, 307, 411, 731, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.09439882418004478, "compression_ratio": 1.6273584905660377, "no_speech_prob": 0.000123342324513942}, {"id": 913, "seek": 511196, "start": 5135.56, "end": 5139.72, "text": " one of the key things that VCs look for you know that they'll ask is like well", "tokens": [50364, 570, 309, 311, 733, 295, 411, 264, 881, 5162, 1859, 300, 311, 534, 1813, 50694, 50694, 4942, 294, 264, 1036, 1064, 420, 732, 307, 1391, 689, 264, 3880, 50904, 50904, 4786, 366, 337, 291, 458, 955, 10641, 1293, 294, 2132, 293, 51196, 51196, 6841, 2144, 293, 370, 498, 291, 434, 1237, 281, 1322, 257, 18578, 337, 1365, 51544, 51544, 472, 295, 264, 2141, 721, 300, 691, 33290, 574, 337, 291, 458, 300, 436, 603, 1029, 307, 411, 731, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.09439882418004478, "compression_ratio": 1.6273584905660377, "no_speech_prob": 0.000123342324513942}, {"id": 914, "seek": 513972, "start": 5139.72, "end": 5144.72, "text": " why now you know why why would you build this company now and of course you know", "tokens": [50364, 983, 586, 291, 458, 983, 983, 576, 291, 1322, 341, 2237, 586, 293, 295, 1164, 291, 458, 50614, 50614, 365, 426, 45196, 264, 1867, 307, 534, 2199, 309, 311, 411, 309, 393, 2049, 312, 411, 731, 50820, 50820, 1826, 1036, 1064, 341, 2067, 380, 1944, 291, 458, 689, 309, 1890, 2064, 1413, 544, 565, 51086, 51086, 420, 309, 1890, 2064, 1413, 544, 1460, 420, 2035, 370, 286, 519, 426, 45196, 307, 257, 2603, 51398, 51398], "temperature": 0.0, "avg_logprob": -0.13390521260050983, "compression_ratio": 1.687150837988827, "no_speech_prob": 9.168576798401773e-05}, {"id": 915, "seek": 513972, "start": 5144.72, "end": 5148.84, "text": " with NLP the answer is really simple it's like it can often be like well", "tokens": [50364, 983, 586, 291, 458, 983, 983, 576, 291, 1322, 341, 2237, 586, 293, 295, 1164, 291, 458, 50614, 50614, 365, 426, 45196, 264, 1867, 307, 534, 2199, 309, 311, 411, 309, 393, 2049, 312, 411, 731, 50820, 50820, 1826, 1036, 1064, 341, 2067, 380, 1944, 291, 458, 689, 309, 1890, 2064, 1413, 544, 565, 51086, 51086, 420, 309, 1890, 2064, 1413, 544, 1460, 420, 2035, 370, 286, 519, 426, 45196, 307, 257, 2603, 51398, 51398], "temperature": 0.0, "avg_logprob": -0.13390521260050983, "compression_ratio": 1.687150837988827, "no_speech_prob": 9.168576798401773e-05}, {"id": 916, "seek": 513972, "start": 5148.84, "end": 5154.16, "text": " until last year this wasn't possible you know where it took ten times more time", "tokens": [50364, 983, 586, 291, 458, 983, 983, 576, 291, 1322, 341, 2237, 586, 293, 295, 1164, 291, 458, 50614, 50614, 365, 426, 45196, 264, 1867, 307, 534, 2199, 309, 311, 411, 309, 393, 2049, 312, 411, 731, 50820, 50820, 1826, 1036, 1064, 341, 2067, 380, 1944, 291, 458, 689, 309, 1890, 2064, 1413, 544, 565, 51086, 51086, 420, 309, 1890, 2064, 1413, 544, 1460, 420, 2035, 370, 286, 519, 426, 45196, 307, 257, 2603, 51398, 51398], "temperature": 0.0, "avg_logprob": -0.13390521260050983, "compression_ratio": 1.687150837988827, "no_speech_prob": 9.168576798401773e-05}, {"id": 917, "seek": 513972, "start": 5154.16, "end": 5160.400000000001, "text": " or it took ten times more money or whatever so I think NLP is a huge", "tokens": [50364, 983, 586, 291, 458, 983, 983, 576, 291, 1322, 341, 2237, 586, 293, 295, 1164, 291, 458, 50614, 50614, 365, 426, 45196, 264, 1867, 307, 534, 2199, 309, 311, 411, 309, 393, 2049, 312, 411, 731, 50820, 50820, 1826, 1036, 1064, 341, 2067, 380, 1944, 291, 458, 689, 309, 1890, 2064, 1413, 544, 565, 51086, 51086, 420, 309, 1890, 2064, 1413, 544, 1460, 420, 2035, 370, 286, 519, 426, 45196, 307, 257, 2603, 51398, 51398], "temperature": 0.0, "avg_logprob": -0.13390521260050983, "compression_ratio": 1.687150837988827, "no_speech_prob": 9.168576798401773e-05}, {"id": 918, "seek": 516040, "start": 5160.4, "end": 5175.5199999999995, "text": " opportunity area okay so it's worth thinking about both use and misuse of", "tokens": [50364, 2650, 1859, 1392, 370, 309, 311, 3163, 1953, 466, 1293, 764, 293, 3346, 438, 295, 51120, 51120, 4363, 426, 45196, 293, 286, 528, 281, 855, 291, 257, 1422, 986, 17975, 510, 307, 257, 3761, 322, 257, 51452, 51452, 1422, 986, 17975, 490, 257, 1916, 295, 924, 2057, 286, 603, 718, 291, 362, 257, 1702, 1401, 295, 309, 51756], "temperature": 0.0, "avg_logprob": -0.09448943932851156, "compression_ratio": 1.4350649350649352, "no_speech_prob": 2.3550019250251353e-05}, {"id": 919, "seek": 516040, "start": 5175.5199999999995, "end": 5182.16, "text": " modern NLP and I want to show you a subreddit here is a conversation on a", "tokens": [50364, 2650, 1859, 1392, 370, 309, 311, 3163, 1953, 466, 1293, 764, 293, 3346, 438, 295, 51120, 51120, 4363, 426, 45196, 293, 286, 528, 281, 855, 291, 257, 1422, 986, 17975, 510, 307, 257, 3761, 322, 257, 51452, 51452, 1422, 986, 17975, 490, 257, 1916, 295, 924, 2057, 286, 603, 718, 291, 362, 257, 1702, 1401, 295, 309, 51756], "temperature": 0.0, "avg_logprob": -0.09448943932851156, "compression_ratio": 1.4350649350649352, "no_speech_prob": 2.3550019250251353e-05}, {"id": 920, "seek": 518216, "start": 5182.16, "end": 5195.04, "text": " subreddit from a couple of years ago I'll let you have a quick read of it so", "tokens": [50364, 1422, 986, 17975, 490, 257, 1916, 295, 924, 2057, 286, 603, 718, 291, 362, 257, 1702, 1401, 295, 309, 370, 51008, 51008, 264, 1168, 286, 528, 291, 281, 312, 1953, 466, 307, 437, 1422, 986, 17975, 360, 291, 519, 51202, 51202, 341, 1487, 490, 341, 341, 7958, 466, 4632, 6434, 293, 264, 1867, 307, 309, 51600, 51600, 1487, 490, 257, 1422, 986, 17975, 300, 12300, 6772, 10833, 7315, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.055871824423472084, "compression_ratio": 1.650273224043716, "no_speech_prob": 8.089984476100653e-05}, {"id": 921, "seek": 518216, "start": 5195.04, "end": 5198.92, "text": " the question I want you to be thinking about is what subreddit do you think", "tokens": [50364, 1422, 986, 17975, 490, 257, 1916, 295, 924, 2057, 286, 603, 718, 291, 362, 257, 1702, 1401, 295, 309, 370, 51008, 51008, 264, 1168, 286, 528, 291, 281, 312, 1953, 466, 307, 437, 1422, 986, 17975, 360, 291, 519, 51202, 51202, 341, 1487, 490, 341, 341, 7958, 466, 4632, 6434, 293, 264, 1867, 307, 309, 51600, 51600, 1487, 490, 257, 1422, 986, 17975, 300, 12300, 6772, 10833, 7315, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.055871824423472084, "compression_ratio": 1.650273224043716, "no_speech_prob": 8.089984476100653e-05}, {"id": 922, "seek": 518216, "start": 5198.92, "end": 5206.88, "text": " this comes from this this debate about military spending and the answer is it", "tokens": [50364, 1422, 986, 17975, 490, 257, 1916, 295, 924, 2057, 286, 603, 718, 291, 362, 257, 1702, 1401, 295, 309, 370, 51008, 51008, 264, 1168, 286, 528, 291, 281, 312, 1953, 466, 307, 437, 1422, 986, 17975, 360, 291, 519, 51202, 51202, 341, 1487, 490, 341, 341, 7958, 466, 4632, 6434, 293, 264, 1867, 307, 309, 51600, 51600, 1487, 490, 257, 1422, 986, 17975, 300, 12300, 6772, 10833, 7315, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.055871824423472084, "compression_ratio": 1.650273224043716, "no_speech_prob": 8.089984476100653e-05}, {"id": 923, "seek": 518216, "start": 5206.88, "end": 5210.24, "text": " comes from a subreddit that posts automatically generated conversations", "tokens": [50364, 1422, 986, 17975, 490, 257, 1916, 295, 924, 2057, 286, 603, 718, 291, 362, 257, 1702, 1401, 295, 309, 370, 51008, 51008, 264, 1168, 286, 528, 291, 281, 312, 1953, 466, 307, 437, 1422, 986, 17975, 360, 291, 519, 51202, 51202, 341, 1487, 490, 341, 341, 7958, 466, 4632, 6434, 293, 264, 1867, 307, 309, 51600, 51600, 1487, 490, 257, 1422, 986, 17975, 300, 12300, 6772, 10833, 7315, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.055871824423472084, "compression_ratio": 1.650273224043716, "no_speech_prob": 8.089984476100653e-05}, {"id": 924, "seek": 521024, "start": 5210.24, "end": 5218.0, "text": " between GPT-2 models now this is a like a totally previous generation of", "tokens": [50364, 1296, 26039, 51, 12, 17, 5245, 586, 341, 307, 257, 411, 257, 3879, 3894, 5125, 295, 50752, 50752, 2316, 436, 434, 709, 709, 1101, 586, 370, 754, 550, 291, 727, 536, 613, 5245, 51006, 51006, 645, 17746, 4319, 6854, 1351, 17915, 6267, 291, 458, 286, 576, 10613, 51538, 51538], "temperature": 0.0, "avg_logprob": -0.14676698983884326, "compression_ratio": 1.4423076923076923, "no_speech_prob": 5.5610355047974735e-05}, {"id": 925, "seek": 521024, "start": 5218.0, "end": 5223.08, "text": " model they're much much better now so even then you could see these models", "tokens": [50364, 1296, 26039, 51, 12, 17, 5245, 586, 341, 307, 257, 411, 257, 3879, 3894, 5125, 295, 50752, 50752, 2316, 436, 434, 709, 709, 1101, 586, 370, 754, 550, 291, 727, 536, 613, 5245, 51006, 51006, 645, 17746, 4319, 6854, 1351, 17915, 6267, 291, 458, 286, 576, 10613, 51538, 51538], "temperature": 0.0, "avg_logprob": -0.14676698983884326, "compression_ratio": 1.4423076923076923, "no_speech_prob": 5.5610355047974735e-05}, {"id": 926, "seek": 521024, "start": 5223.08, "end": 5233.719999999999, "text": " were generating context appropriate believable pros you know I would strongly", "tokens": [50364, 1296, 26039, 51, 12, 17, 5245, 586, 341, 307, 257, 411, 257, 3879, 3894, 5125, 295, 50752, 50752, 2316, 436, 434, 709, 709, 1101, 586, 370, 754, 550, 291, 727, 536, 613, 5245, 51006, 51006, 645, 17746, 4319, 6854, 1351, 17915, 6267, 291, 458, 286, 576, 10613, 51538, 51538], "temperature": 0.0, "avg_logprob": -0.14676698983884326, "compression_ratio": 1.4423076923076923, "no_speech_prob": 5.5610355047974735e-05}, {"id": 927, "seek": 523372, "start": 5233.72, "end": 5241.08, "text": " believe that like any of our kind of like upper tier of competent fast AI", "tokens": [50364, 1697, 300, 411, 604, 295, 527, 733, 295, 411, 6597, 12362, 295, 29998, 2370, 7318, 50732, 50732, 16347, 576, 312, 6457, 3612, 1075, 281, 1884, 257, 10592, 597, 727, 1884, 51006, 51006, 4319, 6854, 6267, 322, 5794, 420, 4384, 3935, 420, 2035, 2360, 291, 458, 51346, 51346, 19697, 337, 257, 1252, 295, 364, 6770, 293, 291, 727, 4373, 300, 493, 1270, 300, 11803, 4, 295, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.14358068549114725, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.00010387649672338739}, {"id": 928, "seek": 523372, "start": 5241.08, "end": 5246.56, "text": " alumni would be fairly easily able to create a bot which could create", "tokens": [50364, 1697, 300, 411, 604, 295, 527, 733, 295, 411, 6597, 12362, 295, 29998, 2370, 7318, 50732, 50732, 16347, 576, 312, 6457, 3612, 1075, 281, 1884, 257, 10592, 597, 727, 1884, 51006, 51006, 4319, 6854, 6267, 322, 5794, 420, 4384, 3935, 420, 2035, 2360, 291, 458, 51346, 51346, 19697, 337, 257, 1252, 295, 364, 6770, 293, 291, 727, 4373, 300, 493, 1270, 300, 11803, 4, 295, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.14358068549114725, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.00010387649672338739}, {"id": 929, "seek": 523372, "start": 5246.56, "end": 5253.360000000001, "text": " context appropriate pros on Twitter or Facebook groups or whatever lay you know", "tokens": [50364, 1697, 300, 411, 604, 295, 527, 733, 295, 411, 6597, 12362, 295, 29998, 2370, 7318, 50732, 50732, 16347, 576, 312, 6457, 3612, 1075, 281, 1884, 257, 10592, 597, 727, 1884, 51006, 51006, 4319, 6854, 6267, 322, 5794, 420, 4384, 3935, 420, 2035, 2360, 291, 458, 51346, 51346, 19697, 337, 257, 1252, 295, 364, 6770, 293, 291, 727, 4373, 300, 493, 1270, 300, 11803, 4, 295, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.14358068549114725, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.00010387649672338739}, {"id": 930, "seek": 523372, "start": 5253.360000000001, "end": 5260.4400000000005, "text": " arguing for a side of an argument and you could scale that up such that 99% of", "tokens": [50364, 1697, 300, 411, 604, 295, 527, 733, 295, 411, 6597, 12362, 295, 29998, 2370, 7318, 50732, 50732, 16347, 576, 312, 6457, 3612, 1075, 281, 1884, 257, 10592, 597, 727, 1884, 51006, 51006, 4319, 6854, 6267, 322, 5794, 420, 4384, 3935, 420, 2035, 2360, 291, 458, 51346, 51346, 19697, 337, 257, 1252, 295, 364, 6770, 293, 291, 727, 4373, 300, 493, 1270, 300, 11803, 4, 295, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.14358068549114725, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.00010387649672338739}, {"id": 931, "seek": 526044, "start": 5260.44, "end": 5265.719999999999, "text": " Twitter was these bots and nobody would know you know nobody would know and", "tokens": [50364, 5794, 390, 613, 35410, 293, 5079, 576, 458, 291, 458, 5079, 576, 458, 293, 50628, 50628, 300, 311, 588, 18788, 281, 385, 570, 257, 688, 295, 291, 458, 257, 688, 295, 733, 295, 264, 636, 51184, 51184, 561, 536, 264, 1002, 307, 586, 534, 1348, 484, 295, 641, 641, 2093, 3021, 51378, 51378, 7315, 597, 412, 341, 935, 436, 434, 436, 434, 45159, 712, 411, 309, 51590, 51590, 576, 406, 312, 300, 1152, 281, 1884, 746, 300, 311, 733, 295, 26941, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.10977177339441636, "compression_ratio": 1.7681159420289856, "no_speech_prob": 5.222896652412601e-05}, {"id": 932, "seek": 526044, "start": 5265.719999999999, "end": 5276.839999999999, "text": " that's very worrying to me because a lot of you know a lot of kind of the way", "tokens": [50364, 5794, 390, 613, 35410, 293, 5079, 576, 458, 291, 458, 5079, 576, 458, 293, 50628, 50628, 300, 311, 588, 18788, 281, 385, 570, 257, 688, 295, 291, 458, 257, 688, 295, 733, 295, 264, 636, 51184, 51184, 561, 536, 264, 1002, 307, 586, 534, 1348, 484, 295, 641, 641, 2093, 3021, 51378, 51378, 7315, 597, 412, 341, 935, 436, 434, 436, 434, 45159, 712, 411, 309, 51590, 51590, 576, 406, 312, 300, 1152, 281, 1884, 746, 300, 311, 733, 295, 26941, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.10977177339441636, "compression_ratio": 1.7681159420289856, "no_speech_prob": 5.222896652412601e-05}, {"id": 933, "seek": 526044, "start": 5276.839999999999, "end": 5280.719999999999, "text": " people see the world is now really coming out of their their social media", "tokens": [50364, 5794, 390, 613, 35410, 293, 5079, 576, 458, 291, 458, 5079, 576, 458, 293, 50628, 50628, 300, 311, 588, 18788, 281, 385, 570, 257, 688, 295, 291, 458, 257, 688, 295, 733, 295, 264, 636, 51184, 51184, 561, 536, 264, 1002, 307, 586, 534, 1348, 484, 295, 641, 641, 2093, 3021, 51378, 51378, 7315, 597, 412, 341, 935, 436, 434, 436, 434, 45159, 712, 411, 309, 51590, 51590, 576, 406, 312, 300, 1152, 281, 1884, 746, 300, 311, 733, 295, 26941, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.10977177339441636, "compression_ratio": 1.7681159420289856, "no_speech_prob": 5.222896652412601e-05}, {"id": 934, "seek": 526044, "start": 5280.719999999999, "end": 5284.96, "text": " conversations which at this point they're they're controllable like it", "tokens": [50364, 5794, 390, 613, 35410, 293, 5079, 576, 458, 291, 458, 5079, 576, 458, 293, 50628, 50628, 300, 311, 588, 18788, 281, 385, 570, 257, 688, 295, 291, 458, 257, 688, 295, 733, 295, 264, 636, 51184, 51184, 561, 536, 264, 1002, 307, 586, 534, 1348, 484, 295, 641, 641, 2093, 3021, 51378, 51378, 7315, 597, 412, 341, 935, 436, 434, 436, 434, 45159, 712, 411, 309, 51590, 51590, 576, 406, 312, 300, 1152, 281, 1884, 746, 300, 311, 733, 295, 26941, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.10977177339441636, "compression_ratio": 1.7681159420289856, "no_speech_prob": 5.222896652412601e-05}, {"id": 935, "seek": 526044, "start": 5284.96, "end": 5288.4, "text": " would not be that hard to create something that's kind of optimized", "tokens": [50364, 5794, 390, 613, 35410, 293, 5079, 576, 458, 291, 458, 5079, 576, 458, 293, 50628, 50628, 300, 311, 588, 18788, 281, 385, 570, 257, 688, 295, 291, 458, 257, 688, 295, 733, 295, 264, 636, 51184, 51184, 561, 536, 264, 1002, 307, 586, 534, 1348, 484, 295, 641, 641, 2093, 3021, 51378, 51378, 7315, 597, 412, 341, 935, 436, 434, 436, 434, 45159, 712, 411, 309, 51590, 51590, 576, 406, 312, 300, 1152, 281, 1884, 746, 300, 311, 733, 295, 26941, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.10977177339441636, "compression_ratio": 1.7681159420289856, "no_speech_prob": 5.222896652412601e-05}, {"id": 936, "seek": 528840, "start": 5288.4, "end": 5295.679999999999, "text": " towards moving a point of view amongst a billion people you know in a very subtle", "tokens": [50364, 3030, 2684, 257, 935, 295, 1910, 12918, 257, 5218, 561, 291, 458, 294, 257, 588, 13743, 50728, 50728, 636, 588, 13145, 670, 257, 938, 2896, 295, 565, 538, 3866, 35410, 1184, 22106, 281, 50934, 50934, 9695, 365, 1184, 661, 293, 472, 295, 552, 1242, 264, 6597, 1011, 293, 370, 5220, 510, 51408, 51408], "temperature": 0.0, "avg_logprob": -0.11889862580732866, "compression_ratio": 1.5612903225806452, "no_speech_prob": 0.000277881626971066}, {"id": 937, "seek": 528840, "start": 5295.679999999999, "end": 5299.799999999999, "text": " way very gradually over a long period of time by multiple bots each pretending to", "tokens": [50364, 3030, 2684, 257, 935, 295, 1910, 12918, 257, 5218, 561, 291, 458, 294, 257, 588, 13743, 50728, 50728, 636, 588, 13145, 670, 257, 938, 2896, 295, 565, 538, 3866, 35410, 1184, 22106, 281, 50934, 50934, 9695, 365, 1184, 661, 293, 472, 295, 552, 1242, 264, 6597, 1011, 293, 370, 5220, 510, 51408, 51408], "temperature": 0.0, "avg_logprob": -0.11889862580732866, "compression_ratio": 1.5612903225806452, "no_speech_prob": 0.000277881626971066}, {"id": 938, "seek": 528840, "start": 5299.799999999999, "end": 5309.28, "text": " argue with each other and one of them getting the upper hand and so forth here", "tokens": [50364, 3030, 2684, 257, 935, 295, 1910, 12918, 257, 5218, 561, 291, 458, 294, 257, 588, 13743, 50728, 50728, 636, 588, 13145, 670, 257, 938, 2896, 295, 565, 538, 3866, 35410, 1184, 22106, 281, 50934, 50934, 9695, 365, 1184, 661, 293, 472, 295, 552, 1242, 264, 6597, 1011, 293, 370, 5220, 510, 51408, 51408], "temperature": 0.0, "avg_logprob": -0.11889862580732866, "compression_ratio": 1.5612903225806452, "no_speech_prob": 0.000277881626971066}, {"id": 939, "seek": 530928, "start": 5309.28, "end": 5333.0, "text": " is the start of a article in the Guardian which I'll let you read this", "tokens": [50364, 307, 264, 722, 295, 257, 7222, 294, 264, 27684, 597, 286, 603, 718, 291, 1401, 341, 51550, 51550, 7222, 390, 291, 458, 1596, 938, 613, 366, 445, 264, 700, 1326, 48910, 293, 412, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.0913348069062104, "compression_ratio": 1.3423423423423424, "no_speech_prob": 3.82240250473842e-05}, {"id": 940, "seek": 530928, "start": 5333.0, "end": 5337.48, "text": " article was you know quite long these are just the first few paragraphs and at", "tokens": [50364, 307, 264, 722, 295, 257, 7222, 294, 264, 27684, 597, 286, 603, 718, 291, 1401, 341, 51550, 51550, 7222, 390, 291, 458, 1596, 938, 613, 366, 445, 264, 700, 1326, 48910, 293, 412, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.0913348069062104, "compression_ratio": 1.3423423423423424, "no_speech_prob": 3.82240250473842e-05}, {"id": 941, "seek": 533748, "start": 5337.48, "end": 5341.959999999999, "text": " the end it explains that this article was written by GPT-3 it was given the", "tokens": [50364, 264, 917, 309, 13948, 300, 341, 7222, 390, 3720, 538, 26039, 51, 12, 18, 309, 390, 2212, 264, 50588, 50588, 10951, 1767, 2464, 257, 2099, 999, 12, 292, 926, 5923, 2283, 1066, 264, 2856, 50766, 50766, 2199, 293, 44882, 1879, 322, 983, 6255, 362, 1825, 281, 4240, 490, 7318, 370, 26039, 51, 12, 18, 51190, 51190, 7126, 3180, 23930, 293, 550, 436, 584, 1936, 264, 31446, 412, 264, 27684, 51408, 51408, 630, 466, 264, 912, 1496, 295, 10000, 300, 436, 576, 360, 337, 6255, 294, 1186, 436, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.07228393869085627, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.00019703419820871204}, {"id": 942, "seek": 533748, "start": 5341.959999999999, "end": 5345.5199999999995, "text": " instruction please write a short op-ed around 500 words keep the language", "tokens": [50364, 264, 917, 309, 13948, 300, 341, 7222, 390, 3720, 538, 26039, 51, 12, 18, 309, 390, 2212, 264, 50588, 50588, 10951, 1767, 2464, 257, 2099, 999, 12, 292, 926, 5923, 2283, 1066, 264, 2856, 50766, 50766, 2199, 293, 44882, 1879, 322, 983, 6255, 362, 1825, 281, 4240, 490, 7318, 370, 26039, 51, 12, 18, 51190, 51190, 7126, 3180, 23930, 293, 550, 436, 584, 1936, 264, 31446, 412, 264, 27684, 51408, 51408, 630, 466, 264, 912, 1496, 295, 10000, 300, 436, 576, 360, 337, 6255, 294, 1186, 436, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.07228393869085627, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.00019703419820871204}, {"id": 943, "seek": 533748, "start": 5345.5199999999995, "end": 5354.0, "text": " simple and concise focus on why humans have nothing to fear from AI so GPT-3", "tokens": [50364, 264, 917, 309, 13948, 300, 341, 7222, 390, 3720, 538, 26039, 51, 12, 18, 309, 390, 2212, 264, 50588, 50588, 10951, 1767, 2464, 257, 2099, 999, 12, 292, 926, 5923, 2283, 1066, 264, 2856, 50766, 50766, 2199, 293, 44882, 1879, 322, 983, 6255, 362, 1825, 281, 4240, 490, 7318, 370, 26039, 51, 12, 18, 51190, 51190, 7126, 3180, 23930, 293, 550, 436, 584, 1936, 264, 31446, 412, 264, 27684, 51408, 51408, 630, 466, 264, 912, 1496, 295, 10000, 300, 436, 576, 360, 337, 6255, 294, 1186, 436, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.07228393869085627, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.00019703419820871204}, {"id": 944, "seek": 533748, "start": 5354.0, "end": 5358.36, "text": " produced eight outputs and then they say basically the editors at the Guardian", "tokens": [50364, 264, 917, 309, 13948, 300, 341, 7222, 390, 3720, 538, 26039, 51, 12, 18, 309, 390, 2212, 264, 50588, 50588, 10951, 1767, 2464, 257, 2099, 999, 12, 292, 926, 5923, 2283, 1066, 264, 2856, 50766, 50766, 2199, 293, 44882, 1879, 322, 983, 6255, 362, 1825, 281, 4240, 490, 7318, 370, 26039, 51, 12, 18, 51190, 51190, 7126, 3180, 23930, 293, 550, 436, 584, 1936, 264, 31446, 412, 264, 27684, 51408, 51408, 630, 466, 264, 912, 1496, 295, 10000, 300, 436, 576, 360, 337, 6255, 294, 1186, 436, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.07228393869085627, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.00019703419820871204}, {"id": 945, "seek": 533748, "start": 5358.36, "end": 5362.2, "text": " did about the same level of editing that they would do for humans in fact they", "tokens": [50364, 264, 917, 309, 13948, 300, 341, 7222, 390, 3720, 538, 26039, 51, 12, 18, 309, 390, 2212, 264, 50588, 50588, 10951, 1767, 2464, 257, 2099, 999, 12, 292, 926, 5923, 2283, 1066, 264, 2856, 50766, 50766, 2199, 293, 44882, 1879, 322, 983, 6255, 362, 1825, 281, 4240, 490, 7318, 370, 26039, 51, 12, 18, 51190, 51190, 7126, 3180, 23930, 293, 550, 436, 584, 1936, 264, 31446, 412, 264, 27684, 51408, 51408, 630, 466, 264, 912, 1496, 295, 10000, 300, 436, 576, 360, 337, 6255, 294, 1186, 436, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.07228393869085627, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.00019703419820871204}, {"id": 946, "seek": 536220, "start": 5362.2, "end": 5369.72, "text": " found it a bit less editing required than a humans so you know again like you", "tokens": [50364, 1352, 309, 257, 857, 1570, 10000, 4739, 813, 257, 6255, 370, 291, 458, 797, 411, 291, 50740, 50740, 393, 1884, 2854, 3755, 295, 4319, 6854, 12505, 4761, 281, 9695, 257, 50964, 50964, 1729, 935, 295, 1910, 437, 733, 295, 721, 1062, 341, 312, 1143, 337, 291, 458, 51274, 51274, 456, 321, 1582, 380, 458, 1391, 337, 7878, 498, 1562, 457, 2171, 321, 483, 257, 13602, 51490, 51490, 2361, 322, 4906, 2899, 510, 311, 746, 490, 646, 6591, 293, 264, 659, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.11269427481151763, "compression_ratio": 1.5756302521008403, "no_speech_prob": 5.474932913784869e-05}, {"id": 947, "seek": 536220, "start": 5369.72, "end": 5374.2, "text": " can create longer pieces of context appropriate prose designed to argue a", "tokens": [50364, 1352, 309, 257, 857, 1570, 10000, 4739, 813, 257, 6255, 370, 291, 458, 797, 411, 291, 50740, 50740, 393, 1884, 2854, 3755, 295, 4319, 6854, 12505, 4761, 281, 9695, 257, 50964, 50964, 1729, 935, 295, 1910, 437, 733, 295, 721, 1062, 341, 312, 1143, 337, 291, 458, 51274, 51274, 456, 321, 1582, 380, 458, 1391, 337, 7878, 498, 1562, 457, 2171, 321, 483, 257, 13602, 51490, 51490, 2361, 322, 4906, 2899, 510, 311, 746, 490, 646, 6591, 293, 264, 659, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.11269427481151763, "compression_ratio": 1.5756302521008403, "no_speech_prob": 5.474932913784869e-05}, {"id": 948, "seek": 536220, "start": 5374.2, "end": 5380.4, "text": " particular point of view what kind of things might this be used for you know", "tokens": [50364, 1352, 309, 257, 857, 1570, 10000, 4739, 813, 257, 6255, 370, 291, 458, 797, 411, 291, 50740, 50740, 393, 1884, 2854, 3755, 295, 4319, 6854, 12505, 4761, 281, 9695, 257, 50964, 50964, 1729, 935, 295, 1910, 437, 733, 295, 721, 1062, 341, 312, 1143, 337, 291, 458, 51274, 51274, 456, 321, 1582, 380, 458, 1391, 337, 7878, 498, 1562, 457, 2171, 321, 483, 257, 13602, 51490, 51490, 2361, 322, 4906, 2899, 510, 311, 746, 490, 646, 6591, 293, 264, 659, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.11269427481151763, "compression_ratio": 1.5756302521008403, "no_speech_prob": 5.474932913784869e-05}, {"id": 949, "seek": 536220, "start": 5380.4, "end": 5384.72, "text": " there we won't know probably for decades if ever but sometimes we get a clue", "tokens": [50364, 1352, 309, 257, 857, 1570, 10000, 4739, 813, 257, 6255, 370, 291, 458, 797, 411, 291, 50740, 50740, 393, 1884, 2854, 3755, 295, 4319, 6854, 12505, 4761, 281, 9695, 257, 50964, 50964, 1729, 935, 295, 1910, 437, 733, 295, 721, 1062, 341, 312, 1143, 337, 291, 458, 51274, 51274, 456, 321, 1582, 380, 458, 1391, 337, 7878, 498, 1562, 457, 2171, 321, 483, 257, 13602, 51490, 51490, 2361, 322, 4906, 2899, 510, 311, 746, 490, 646, 6591, 293, 264, 659, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.11269427481151763, "compression_ratio": 1.5756302521008403, "no_speech_prob": 5.474932913784869e-05}, {"id": 950, "seek": 536220, "start": 5384.72, "end": 5389.32, "text": " based on older technology here's something from back 2017 and the pre", "tokens": [50364, 1352, 309, 257, 857, 1570, 10000, 4739, 813, 257, 6255, 370, 291, 458, 797, 411, 291, 50740, 50740, 393, 1884, 2854, 3755, 295, 4319, 6854, 12505, 4761, 281, 9695, 257, 50964, 50964, 1729, 935, 295, 1910, 437, 733, 295, 721, 1062, 341, 312, 1143, 337, 291, 458, 51274, 51274, 456, 321, 1582, 380, 458, 1391, 337, 7878, 498, 1562, 457, 2171, 321, 483, 257, 13602, 51490, 51490, 2361, 322, 4906, 2899, 510, 311, 746, 490, 646, 6591, 293, 264, 659, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.11269427481151763, "compression_ratio": 1.5756302521008403, "no_speech_prob": 5.474932913784869e-05}, {"id": 951, "seek": 538932, "start": 5389.32, "end": 5398.08, "text": " kind of deep learning NLP days there were millions of submissions to the FTC", "tokens": [50364, 733, 295, 2452, 2539, 426, 45196, 1708, 456, 645, 6803, 295, 40429, 281, 264, 479, 18238, 50802, 50802, 466, 264, 2533, 39913, 1860, 2590, 294, 3374, 588, 588, 10950, 28035, 51150, 51150, 3030, 264, 935, 295, 1910, 295, 1566, 321, 528, 281, 483, 3973, 295, 2533, 39913, 1860, 364, 51478, 51478, 5215, 538, 7506, 21933, 898, 4712, 300, 746, 411, 11803, 4, 295, 552, 293, 294, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.11227338654654366, "compression_ratio": 1.4948979591836735, "no_speech_prob": 0.00023014418547973037}, {"id": 952, "seek": 538932, "start": 5398.08, "end": 5405.04, "text": " about the net neutrality situation in America very very heavily biased", "tokens": [50364, 733, 295, 2452, 2539, 426, 45196, 1708, 456, 645, 6803, 295, 40429, 281, 264, 479, 18238, 50802, 50802, 466, 264, 2533, 39913, 1860, 2590, 294, 3374, 588, 588, 10950, 28035, 51150, 51150, 3030, 264, 935, 295, 1910, 295, 1566, 321, 528, 281, 483, 3973, 295, 2533, 39913, 1860, 364, 51478, 51478, 5215, 538, 7506, 21933, 898, 4712, 300, 746, 411, 11803, 4, 295, 552, 293, 294, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.11227338654654366, "compression_ratio": 1.4948979591836735, "no_speech_prob": 0.00023014418547973037}, {"id": 953, "seek": 538932, "start": 5405.04, "end": 5411.599999999999, "text": " towards the point of view of saying we want to get rid of net neutrality an", "tokens": [50364, 733, 295, 2452, 2539, 426, 45196, 1708, 456, 645, 6803, 295, 40429, 281, 264, 479, 18238, 50802, 50802, 466, 264, 2533, 39913, 1860, 2590, 294, 3374, 588, 588, 10950, 28035, 51150, 51150, 3030, 264, 935, 295, 1910, 295, 1566, 321, 528, 281, 483, 3973, 295, 2533, 39913, 1860, 364, 51478, 51478, 5215, 538, 7506, 21933, 898, 4712, 300, 746, 411, 11803, 4, 295, 552, 293, 294, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.11227338654654366, "compression_ratio": 1.4948979591836735, "no_speech_prob": 0.00023014418547973037}, {"id": 954, "seek": 538932, "start": 5411.599999999999, "end": 5417.44, "text": " analysis by Jeff Cowell showed that something like 99% of them and in", "tokens": [50364, 733, 295, 2452, 2539, 426, 45196, 1708, 456, 645, 6803, 295, 40429, 281, 264, 479, 18238, 50802, 50802, 466, 264, 2533, 39913, 1860, 2590, 294, 3374, 588, 588, 10950, 28035, 51150, 51150, 3030, 264, 935, 295, 1910, 295, 1566, 321, 528, 281, 483, 3973, 295, 2533, 39913, 1860, 364, 51478, 51478, 5215, 538, 7506, 21933, 898, 4712, 300, 746, 411, 11803, 4, 295, 552, 293, 294, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.11227338654654366, "compression_ratio": 1.4948979591836735, "no_speech_prob": 0.00023014418547973037}, {"id": 955, "seek": 541744, "start": 5417.44, "end": 5422.759999999999, "text": " particular nearly all of the ones which were pro removal of net neutrality were", "tokens": [50364, 1729, 6217, 439, 295, 264, 2306, 597, 645, 447, 17933, 295, 2533, 39913, 1860, 645, 50630, 50630, 4448, 8399, 12, 21848, 770, 538, 1936, 498, 291, 574, 412, 264, 3092, 456, 311, 411, 50866, 50866, 18182, 490, 257, 6510, 370, 321, 600, 658, 6280, 382, 8851, 281, 6149, 51090, 51090, 26360, 25869, 9948, 281, 2103, 264, 3328, 436, 7516, 5346, 382, 51264, 51264, 8851, 281, 6149, 26360, 25869, 820, 312, 445, 707, 561, 411, 385, 382, 8851, 51442, 51442, 281, 370, 12, 11880, 8572, 293, 291, 483, 264, 1558, 586, 341, 307, 364, 1365, 295, 257, 588, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.10747318456668666, "compression_ratio": 1.75390625, "no_speech_prob": 0.00015591295959893614}, {"id": 956, "seek": 541744, "start": 5422.759999999999, "end": 5427.48, "text": " clearly auto-generated by basically if you look at the green there's like", "tokens": [50364, 1729, 6217, 439, 295, 264, 2306, 597, 645, 447, 17933, 295, 2533, 39913, 1860, 645, 50630, 50630, 4448, 8399, 12, 21848, 770, 538, 1936, 498, 291, 574, 412, 264, 3092, 456, 311, 411, 50866, 50866, 18182, 490, 257, 6510, 370, 321, 600, 658, 6280, 382, 8851, 281, 6149, 51090, 51090, 26360, 25869, 9948, 281, 2103, 264, 3328, 436, 7516, 5346, 382, 51264, 51264, 8851, 281, 6149, 26360, 25869, 820, 312, 445, 707, 561, 411, 385, 382, 8851, 51442, 51442, 281, 370, 12, 11880, 8572, 293, 291, 483, 264, 1558, 586, 341, 307, 364, 1365, 295, 257, 588, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.10747318456668666, "compression_ratio": 1.75390625, "no_speech_prob": 0.00015591295959893614}, {"id": 957, "seek": 541744, "start": 5427.48, "end": 5431.96, "text": " selecting from a menu so we've got Americans as opposed to Washington", "tokens": [50364, 1729, 6217, 439, 295, 264, 2306, 597, 645, 447, 17933, 295, 2533, 39913, 1860, 645, 50630, 50630, 4448, 8399, 12, 21848, 770, 538, 1936, 498, 291, 574, 412, 264, 3092, 456, 311, 411, 50866, 50866, 18182, 490, 257, 6510, 370, 321, 600, 658, 6280, 382, 8851, 281, 6149, 51090, 51090, 26360, 25869, 9948, 281, 2103, 264, 3328, 436, 7516, 5346, 382, 51264, 51264, 8851, 281, 6149, 26360, 25869, 820, 312, 445, 707, 561, 411, 385, 382, 8851, 51442, 51442, 281, 370, 12, 11880, 8572, 293, 291, 483, 264, 1558, 586, 341, 307, 364, 1365, 295, 257, 588, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.10747318456668666, "compression_ratio": 1.75390625, "no_speech_prob": 0.00015591295959893614}, {"id": 958, "seek": 541744, "start": 5431.96, "end": 5435.44, "text": " bureaucrats deserve to enjoy the services they desire individuals as", "tokens": [50364, 1729, 6217, 439, 295, 264, 2306, 597, 645, 447, 17933, 295, 2533, 39913, 1860, 645, 50630, 50630, 4448, 8399, 12, 21848, 770, 538, 1936, 498, 291, 574, 412, 264, 3092, 456, 311, 411, 50866, 50866, 18182, 490, 257, 6510, 370, 321, 600, 658, 6280, 382, 8851, 281, 6149, 51090, 51090, 26360, 25869, 9948, 281, 2103, 264, 3328, 436, 7516, 5346, 382, 51264, 51264, 8851, 281, 6149, 26360, 25869, 820, 312, 445, 707, 561, 411, 385, 382, 8851, 51442, 51442, 281, 370, 12, 11880, 8572, 293, 291, 483, 264, 1558, 586, 341, 307, 364, 1365, 295, 257, 588, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.10747318456668666, "compression_ratio": 1.75390625, "no_speech_prob": 0.00015591295959893614}, {"id": 959, "seek": 541744, "start": 5435.44, "end": 5439.0, "text": " opposed to Washington bureaucrats should be just little people like me as opposed", "tokens": [50364, 1729, 6217, 439, 295, 264, 2306, 597, 645, 447, 17933, 295, 2533, 39913, 1860, 645, 50630, 50630, 4448, 8399, 12, 21848, 770, 538, 1936, 498, 291, 574, 412, 264, 3092, 456, 311, 411, 50866, 50866, 18182, 490, 257, 6510, 370, 321, 600, 658, 6280, 382, 8851, 281, 6149, 51090, 51090, 26360, 25869, 9948, 281, 2103, 264, 3328, 436, 7516, 5346, 382, 51264, 51264, 8851, 281, 6149, 26360, 25869, 820, 312, 445, 707, 561, 411, 385, 382, 8851, 51442, 51442, 281, 370, 12, 11880, 8572, 293, 291, 483, 264, 1558, 586, 341, 307, 364, 1365, 295, 257, 588, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.10747318456668666, "compression_ratio": 1.75390625, "no_speech_prob": 0.00015591295959893614}, {"id": 960, "seek": 541744, "start": 5439.0, "end": 5444.28, "text": " to so-called experts and you get the idea now this is an example of a very", "tokens": [50364, 1729, 6217, 439, 295, 264, 2306, 597, 645, 447, 17933, 295, 2533, 39913, 1860, 645, 50630, 50630, 4448, 8399, 12, 21848, 770, 538, 1936, 498, 291, 574, 412, 264, 3092, 456, 311, 411, 50866, 50866, 18182, 490, 257, 6510, 370, 321, 600, 658, 6280, 382, 8851, 281, 6149, 51090, 51090, 26360, 25869, 9948, 281, 2103, 264, 3328, 436, 7516, 5346, 382, 51264, 51264, 8851, 281, 6149, 26360, 25869, 820, 312, 445, 707, 561, 411, 385, 382, 8851, 51442, 51442, 281, 370, 12, 11880, 8572, 293, 291, 483, 264, 1558, 586, 341, 307, 364, 1365, 295, 257, 588, 51706, 51706], "temperature": 0.0, "avg_logprob": -0.10747318456668666, "compression_ratio": 1.75390625, "no_speech_prob": 0.00015591295959893614}, {"id": 961, "seek": 544428, "start": 5444.28, "end": 5450.96, "text": " very you know simple approach to auto generating huge amounts of text we don't", "tokens": [50364, 588, 291, 458, 2199, 3109, 281, 8399, 17746, 2603, 11663, 295, 2487, 321, 500, 380, 50698, 50698, 458, 337, 988, 457, 309, 1542, 411, 341, 1062, 362, 668, 4406, 570, 341, 50922, 50922, 1437, 807, 291, 458, 7228, 437, 2544, 281, 312, 767, 13373, 51232, 51232, 38947, 490, 264, 1908, 300, 2201, 1920, 2201, 5902, 2533, 51430, 51430, 39913, 1860, 264, 479, 18238, 658, 3973, 295, 309, 293, 341, 390, 257, 955, 644, 295, 264, 5143, 390, 51688, 51688, 411, 1954, 321, 658, 439, 613, 3053, 490, 264, 1908, 293, 2201, 848, 436, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.0847318786935708, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.00018232580623589456}, {"id": 962, "seek": 544428, "start": 5450.96, "end": 5455.44, "text": " know for sure but it looks like this might have been successful because this", "tokens": [50364, 588, 291, 458, 2199, 3109, 281, 8399, 17746, 2603, 11663, 295, 2487, 321, 500, 380, 50698, 50698, 458, 337, 988, 457, 309, 1542, 411, 341, 1062, 362, 668, 4406, 570, 341, 50922, 50922, 1437, 807, 291, 458, 7228, 437, 2544, 281, 312, 767, 13373, 51232, 51232, 38947, 490, 264, 1908, 300, 2201, 1920, 2201, 5902, 2533, 51430, 51430, 39913, 1860, 264, 479, 18238, 658, 3973, 295, 309, 293, 341, 390, 257, 955, 644, 295, 264, 5143, 390, 51688, 51688, 411, 1954, 321, 658, 439, 613, 3053, 490, 264, 1908, 293, 2201, 848, 436, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.0847318786935708, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.00018232580623589456}, {"id": 963, "seek": 544428, "start": 5455.44, "end": 5461.639999999999, "text": " went through you know despite what seems to be actually overwhelming", "tokens": [50364, 588, 291, 458, 2199, 3109, 281, 8399, 17746, 2603, 11663, 295, 2487, 321, 500, 380, 50698, 50698, 458, 337, 988, 457, 309, 1542, 411, 341, 1062, 362, 668, 4406, 570, 341, 50922, 50922, 1437, 807, 291, 458, 7228, 437, 2544, 281, 312, 767, 13373, 51232, 51232, 38947, 490, 264, 1908, 300, 2201, 1920, 2201, 5902, 2533, 51430, 51430, 39913, 1860, 264, 479, 18238, 658, 3973, 295, 309, 293, 341, 390, 257, 955, 644, 295, 264, 5143, 390, 51688, 51688, 411, 1954, 321, 658, 439, 613, 3053, 490, 264, 1908, 293, 2201, 848, 436, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.0847318786935708, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.00018232580623589456}, {"id": 964, "seek": 544428, "start": 5461.639999999999, "end": 5465.599999999999, "text": " disagreement from the public that everybody almost everybody likes net", "tokens": [50364, 588, 291, 458, 2199, 3109, 281, 8399, 17746, 2603, 11663, 295, 2487, 321, 500, 380, 50698, 50698, 458, 337, 988, 457, 309, 1542, 411, 341, 1062, 362, 668, 4406, 570, 341, 50922, 50922, 1437, 807, 291, 458, 7228, 437, 2544, 281, 312, 767, 13373, 51232, 51232, 38947, 490, 264, 1908, 300, 2201, 1920, 2201, 5902, 2533, 51430, 51430, 39913, 1860, 264, 479, 18238, 658, 3973, 295, 309, 293, 341, 390, 257, 955, 644, 295, 264, 5143, 390, 51688, 51688, 411, 1954, 321, 658, 439, 613, 3053, 490, 264, 1908, 293, 2201, 848, 436, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.0847318786935708, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.00018232580623589456}, {"id": 965, "seek": 544428, "start": 5465.599999999999, "end": 5470.759999999999, "text": " neutrality the FTC got rid of it and this was a big part of the basis was", "tokens": [50364, 588, 291, 458, 2199, 3109, 281, 8399, 17746, 2603, 11663, 295, 2487, 321, 500, 380, 50698, 50698, 458, 337, 988, 457, 309, 1542, 411, 341, 1062, 362, 668, 4406, 570, 341, 50922, 50922, 1437, 807, 291, 458, 7228, 437, 2544, 281, 312, 767, 13373, 51232, 51232, 38947, 490, 264, 1908, 300, 2201, 1920, 2201, 5902, 2533, 51430, 51430, 39913, 1860, 264, 479, 18238, 658, 3973, 295, 309, 293, 341, 390, 257, 955, 644, 295, 264, 5143, 390, 51688, 51688, 411, 1954, 321, 658, 439, 613, 3053, 490, 264, 1908, 293, 2201, 848, 436, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.0847318786935708, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.00018232580623589456}, {"id": 966, "seek": 544428, "start": 5470.759999999999, "end": 5473.84, "text": " like oh we got all these comments from the public and everybody said they", "tokens": [50364, 588, 291, 458, 2199, 3109, 281, 8399, 17746, 2603, 11663, 295, 2487, 321, 500, 380, 50698, 50698, 458, 337, 988, 457, 309, 1542, 411, 341, 1062, 362, 668, 4406, 570, 341, 50922, 50922, 1437, 807, 291, 458, 7228, 437, 2544, 281, 312, 767, 13373, 51232, 51232, 38947, 490, 264, 1908, 300, 2201, 1920, 2201, 5902, 2533, 51430, 51430, 39913, 1860, 264, 479, 18238, 658, 3973, 295, 309, 293, 341, 390, 257, 955, 644, 295, 264, 5143, 390, 51688, 51688, 411, 1954, 321, 658, 439, 613, 3053, 490, 264, 1908, 293, 2201, 848, 436, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.0847318786935708, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.00018232580623589456}, {"id": 967, "seek": 547384, "start": 5473.84, "end": 5480.04, "text": " don't want net neutrality so imagine a similar thing where you absolutely", "tokens": [50364, 500, 380, 528, 2533, 39913, 1860, 370, 3811, 257, 2531, 551, 689, 291, 3122, 50674, 50674, 2809, 380, 360, 341, 291, 2809, 380, 2573, 309, 484, 570, 1518, 390, 534, 588, 50882, 50882, 20050, 293, 588, 819, 300, 311, 291, 458, 309, 311, 733, 295, 18788, 466, 577, 51186, 51186, 321, 2028, 365, 300, 291, 458, 286, 486, 584, 562, 286, 751, 466, 341, 1507, 2049, 51434, 51434, 561, 584, 1954, 572, 16340, 321, 603, 312, 1075, 281, 2316, 281, 5521, 291, 458, 10592, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.1255307305942882, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.00015595961303915828}, {"id": 968, "seek": 547384, "start": 5480.04, "end": 5484.2, "text": " couldn't do this you couldn't figure it out because everyone was really very", "tokens": [50364, 500, 380, 528, 2533, 39913, 1860, 370, 3811, 257, 2531, 551, 689, 291, 3122, 50674, 50674, 2809, 380, 360, 341, 291, 2809, 380, 2573, 309, 484, 570, 1518, 390, 534, 588, 50882, 50882, 20050, 293, 588, 819, 300, 311, 291, 458, 309, 311, 733, 295, 18788, 466, 577, 51186, 51186, 321, 2028, 365, 300, 291, 458, 286, 486, 584, 562, 286, 751, 466, 341, 1507, 2049, 51434, 51434, 561, 584, 1954, 572, 16340, 321, 603, 312, 1075, 281, 2316, 281, 5521, 291, 458, 10592, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.1255307305942882, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.00015595961303915828}, {"id": 969, "seek": 547384, "start": 5484.2, "end": 5490.28, "text": " compelling and very different that's you know it's kind of worrying about how", "tokens": [50364, 500, 380, 528, 2533, 39913, 1860, 370, 3811, 257, 2531, 551, 689, 291, 3122, 50674, 50674, 2809, 380, 360, 341, 291, 2809, 380, 2573, 309, 484, 570, 1518, 390, 534, 588, 50882, 50882, 20050, 293, 588, 819, 300, 311, 291, 458, 309, 311, 733, 295, 18788, 466, 577, 51186, 51186, 321, 2028, 365, 300, 291, 458, 286, 486, 584, 562, 286, 751, 466, 341, 1507, 2049, 51434, 51434, 561, 584, 1954, 572, 16340, 321, 603, 312, 1075, 281, 2316, 281, 5521, 291, 458, 10592, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.1255307305942882, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.00015595961303915828}, {"id": 970, "seek": 547384, "start": 5490.28, "end": 5495.24, "text": " we deal with that you know I will say when I talk about this stuff often", "tokens": [50364, 500, 380, 528, 2533, 39913, 1860, 370, 3811, 257, 2531, 551, 689, 291, 3122, 50674, 50674, 2809, 380, 360, 341, 291, 2809, 380, 2573, 309, 484, 570, 1518, 390, 534, 588, 50882, 50882, 20050, 293, 588, 819, 300, 311, 291, 458, 309, 311, 733, 295, 18788, 466, 577, 51186, 51186, 321, 2028, 365, 300, 291, 458, 286, 486, 584, 562, 286, 751, 466, 341, 1507, 2049, 51434, 51434, 561, 584, 1954, 572, 16340, 321, 603, 312, 1075, 281, 2316, 281, 5521, 291, 458, 10592, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.1255307305942882, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.00015595961303915828}, {"id": 971, "seek": 547384, "start": 5495.24, "end": 5502.46, "text": " people say oh no worries we'll be able to model to recognize you know bot", "tokens": [50364, 500, 380, 528, 2533, 39913, 1860, 370, 3811, 257, 2531, 551, 689, 291, 3122, 50674, 50674, 2809, 380, 360, 341, 291, 2809, 380, 2573, 309, 484, 570, 1518, 390, 534, 588, 50882, 50882, 20050, 293, 588, 819, 300, 311, 291, 458, 309, 311, 733, 295, 18788, 466, 577, 51186, 51186, 321, 2028, 365, 300, 291, 458, 286, 486, 584, 562, 286, 751, 466, 341, 1507, 2049, 51434, 51434, 561, 584, 1954, 572, 16340, 321, 603, 312, 1075, 281, 2316, 281, 5521, 291, 458, 10592, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.1255307305942882, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.00015595961303915828}, {"id": 972, "seek": 550246, "start": 5502.46, "end": 5508.08, "text": " generated content but you know if I put my black hat on I'm like no that's not", "tokens": [50364, 10833, 2701, 457, 291, 458, 498, 286, 829, 452, 2211, 2385, 322, 286, 478, 411, 572, 300, 311, 406, 50645, 50645, 516, 281, 589, 558, 498, 291, 1907, 385, 281, 1322, 746, 300, 16447, 264, 10592, 50901, 50901, 1508, 23463, 286, 1116, 584, 572, 16340, 1858, 291, 458, 286, 486, 747, 264, 3089, 420, 264, 3753, 51195, 51195, 420, 2643, 420, 2035, 300, 775, 264, 10592, 1508, 5489, 293, 286, 486, 4090, 51379, 51379, 13497, 300, 294, 452, 4470, 2445, 293, 286, 486, 2489, 12, 83, 2613, 452, 2316, 1826, 309, 16447, 51599, 51599, 264, 10592, 1508, 9902, 291, 458, 562, 286, 1143, 281, 1190, 364, 3796, 2237, 321, 632, 257, 2531, 51855, 51855], "temperature": 0.0, "avg_logprob": -0.12484489638229897, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.00010069353447761387}, {"id": 973, "seek": 550246, "start": 5508.08, "end": 5513.2, "text": " going to work right if you told me to build something that beats the bot", "tokens": [50364, 10833, 2701, 457, 291, 458, 498, 286, 829, 452, 2211, 2385, 322, 286, 478, 411, 572, 300, 311, 406, 50645, 50645, 516, 281, 589, 558, 498, 291, 1907, 385, 281, 1322, 746, 300, 16447, 264, 10592, 50901, 50901, 1508, 23463, 286, 1116, 584, 572, 16340, 1858, 291, 458, 286, 486, 747, 264, 3089, 420, 264, 3753, 51195, 51195, 420, 2643, 420, 2035, 300, 775, 264, 10592, 1508, 5489, 293, 286, 486, 4090, 51379, 51379, 13497, 300, 294, 452, 4470, 2445, 293, 286, 486, 2489, 12, 83, 2613, 452, 2316, 1826, 309, 16447, 51599, 51599, 264, 10592, 1508, 9902, 291, 458, 562, 286, 1143, 281, 1190, 364, 3796, 2237, 321, 632, 257, 2531, 51855, 51855], "temperature": 0.0, "avg_logprob": -0.12484489638229897, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.00010069353447761387}, {"id": 974, "seek": 550246, "start": 5513.2, "end": 5519.08, "text": " classifiers I'd say no worries easy you know I will take the code or the surface", "tokens": [50364, 10833, 2701, 457, 291, 458, 498, 286, 829, 452, 2211, 2385, 322, 286, 478, 411, 572, 300, 311, 406, 50645, 50645, 516, 281, 589, 558, 498, 291, 1907, 385, 281, 1322, 746, 300, 16447, 264, 10592, 50901, 50901, 1508, 23463, 286, 1116, 584, 572, 16340, 1858, 291, 458, 286, 486, 747, 264, 3089, 420, 264, 3753, 51195, 51195, 420, 2643, 420, 2035, 300, 775, 264, 10592, 1508, 5489, 293, 286, 486, 4090, 51379, 51379, 13497, 300, 294, 452, 4470, 2445, 293, 286, 486, 2489, 12, 83, 2613, 452, 2316, 1826, 309, 16447, 51599, 51599, 264, 10592, 1508, 9902, 291, 458, 562, 286, 1143, 281, 1190, 364, 3796, 2237, 321, 632, 257, 2531, 51855, 51855], "temperature": 0.0, "avg_logprob": -0.12484489638229897, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.00010069353447761387}, {"id": 975, "seek": 550246, "start": 5519.08, "end": 5522.76, "text": " or service or whatever that does the bot classifying and I will include", "tokens": [50364, 10833, 2701, 457, 291, 458, 498, 286, 829, 452, 2211, 2385, 322, 286, 478, 411, 572, 300, 311, 406, 50645, 50645, 516, 281, 589, 558, 498, 291, 1907, 385, 281, 1322, 746, 300, 16447, 264, 10592, 50901, 50901, 1508, 23463, 286, 1116, 584, 572, 16340, 1858, 291, 458, 286, 486, 747, 264, 3089, 420, 264, 3753, 51195, 51195, 420, 2643, 420, 2035, 300, 775, 264, 10592, 1508, 5489, 293, 286, 486, 4090, 51379, 51379, 13497, 300, 294, 452, 4470, 2445, 293, 286, 486, 2489, 12, 83, 2613, 452, 2316, 1826, 309, 16447, 51599, 51599, 264, 10592, 1508, 9902, 291, 458, 562, 286, 1143, 281, 1190, 364, 3796, 2237, 321, 632, 257, 2531, 51855, 51855], "temperature": 0.0, "avg_logprob": -0.12484489638229897, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.00010069353447761387}, {"id": 976, "seek": 550246, "start": 5522.76, "end": 5527.16, "text": " beating that in my loss function and I will fine-tune my model until it beats", "tokens": [50364, 10833, 2701, 457, 291, 458, 498, 286, 829, 452, 2211, 2385, 322, 286, 478, 411, 572, 300, 311, 406, 50645, 50645, 516, 281, 589, 558, 498, 291, 1907, 385, 281, 1322, 746, 300, 16447, 264, 10592, 50901, 50901, 1508, 23463, 286, 1116, 584, 572, 16340, 1858, 291, 458, 286, 486, 747, 264, 3089, 420, 264, 3753, 51195, 51195, 420, 2643, 420, 2035, 300, 775, 264, 10592, 1508, 5489, 293, 286, 486, 4090, 51379, 51379, 13497, 300, 294, 452, 4470, 2445, 293, 286, 486, 2489, 12, 83, 2613, 452, 2316, 1826, 309, 16447, 51599, 51599, 264, 10592, 1508, 9902, 291, 458, 562, 286, 1143, 281, 1190, 364, 3796, 2237, 321, 632, 257, 2531, 51855, 51855], "temperature": 0.0, "avg_logprob": -0.12484489638229897, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.00010069353447761387}, {"id": 977, "seek": 550246, "start": 5527.16, "end": 5532.28, "text": " the bot classifier you know when I used to run an email company we had a similar", "tokens": [50364, 10833, 2701, 457, 291, 458, 498, 286, 829, 452, 2211, 2385, 322, 286, 478, 411, 572, 300, 311, 406, 50645, 50645, 516, 281, 589, 558, 498, 291, 1907, 385, 281, 1322, 746, 300, 16447, 264, 10592, 50901, 50901, 1508, 23463, 286, 1116, 584, 572, 16340, 1858, 291, 458, 286, 486, 747, 264, 3089, 420, 264, 3753, 51195, 51195, 420, 2643, 420, 2035, 300, 775, 264, 10592, 1508, 5489, 293, 286, 486, 4090, 51379, 51379, 13497, 300, 294, 452, 4470, 2445, 293, 286, 486, 2489, 12, 83, 2613, 452, 2316, 1826, 309, 16447, 51599, 51599, 264, 10592, 1508, 9902, 291, 458, 562, 286, 1143, 281, 1190, 364, 3796, 2237, 321, 632, 257, 2531, 51855, 51855], "temperature": 0.0, "avg_logprob": -0.12484489638229897, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.00010069353447761387}, {"id": 978, "seek": 553228, "start": 5532.28, "end": 5537.08, "text": " problem with spam prevention you know spammers could always take a spam", "tokens": [50364, 1154, 365, 24028, 14630, 291, 458, 637, 48414, 727, 1009, 747, 257, 24028, 50604, 50604, 14630, 9284, 293, 1319, 641, 12524, 1826, 309, 994, 380, 483, 264, 24028, 50858, 50858, 14630, 9284, 3602, 337, 1365, 370, 1338, 370, 286, 478, 534, 2919, 466, 264, 51188, 51188, 4786, 337, 337, 1731, 294, 341, 1164, 281, 1322, 291, 458, 286, 519, 588, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.10312250256538391, "compression_ratio": 1.6833333333333333, "no_speech_prob": 3.0239825719036162e-05}, {"id": 979, "seek": 553228, "start": 5537.08, "end": 5542.16, "text": " prevention algorithm and change their emails until it didn't get the spam", "tokens": [50364, 1154, 365, 24028, 14630, 291, 458, 637, 48414, 727, 1009, 747, 257, 24028, 50604, 50604, 14630, 9284, 293, 1319, 641, 12524, 1826, 309, 994, 380, 483, 264, 24028, 50858, 50858, 14630, 9284, 3602, 337, 1365, 370, 1338, 370, 286, 478, 534, 2919, 466, 264, 51188, 51188, 4786, 337, 337, 1731, 294, 341, 1164, 281, 1322, 291, 458, 286, 519, 588, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.10312250256538391, "compression_ratio": 1.6833333333333333, "no_speech_prob": 3.0239825719036162e-05}, {"id": 980, "seek": 553228, "start": 5542.16, "end": 5548.759999999999, "text": " prevention algorithm anymore for example so yeah so I'm really excited about the", "tokens": [50364, 1154, 365, 24028, 14630, 291, 458, 637, 48414, 727, 1009, 747, 257, 24028, 50604, 50604, 14630, 9284, 293, 1319, 641, 12524, 1826, 309, 994, 380, 483, 264, 24028, 50858, 50858, 14630, 9284, 3602, 337, 1365, 370, 1338, 370, 286, 478, 534, 2919, 466, 264, 51188, 51188, 4786, 337, 337, 1731, 294, 341, 1164, 281, 1322, 291, 458, 286, 519, 588, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.10312250256538391, "compression_ratio": 1.6833333333333333, "no_speech_prob": 3.0239825719036162e-05}, {"id": 981, "seek": 553228, "start": 5548.759999999999, "end": 5558.88, "text": " opportunities for for students in this course to build you know I think very", "tokens": [50364, 1154, 365, 24028, 14630, 291, 458, 637, 48414, 727, 1009, 747, 257, 24028, 50604, 50604, 14630, 9284, 293, 1319, 641, 12524, 1826, 309, 994, 380, 483, 264, 24028, 50858, 50858, 14630, 9284, 3602, 337, 1365, 370, 1338, 370, 286, 478, 534, 2919, 466, 264, 51188, 51188, 4786, 337, 337, 1731, 294, 341, 1164, 281, 1322, 291, 458, 286, 519, 588, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.10312250256538391, "compression_ratio": 1.6833333333333333, "no_speech_prob": 3.0239825719036162e-05}, {"id": 982, "seek": 555888, "start": 5558.88, "end": 5565.68, "text": " valuable businesses really cool research and so forth using these pretty new NLP", "tokens": [50364, 8263, 6011, 534, 1627, 2132, 293, 370, 5220, 1228, 613, 1238, 777, 426, 45196, 50704, 50704, 7512, 300, 366, 586, 1238, 9515, 293, 286, 478, 611, 534, 5804, 50866, 50866, 466, 264, 721, 300, 1062, 352, 2085, 286, 360, 519, 1673, 300, 264, 544, 561, 51054, 51054, 300, 1223, 613, 10862, 264, 1570, 2931, 436, 603, 352, 2085, 2619, 390, 51304, 51304, 456, 512, 1651, 1338, 286, 914, 309, 311, 257, 3507, 3207, 281, 264, 281, 264, 589, 2939, 300, 51609, 51609], "temperature": 0.0, "avg_logprob": -0.10610463505699522, "compression_ratio": 1.6565217391304348, "no_speech_prob": 8.747587708057836e-05}, {"id": 983, "seek": 555888, "start": 5565.68, "end": 5568.92, "text": " techniques that are now pretty accessible and I'm also really worried", "tokens": [50364, 8263, 6011, 534, 1627, 2132, 293, 370, 5220, 1228, 613, 1238, 777, 426, 45196, 50704, 50704, 7512, 300, 366, 586, 1238, 9515, 293, 286, 478, 611, 534, 5804, 50866, 50866, 466, 264, 721, 300, 1062, 352, 2085, 286, 360, 519, 1673, 300, 264, 544, 561, 51054, 51054, 300, 1223, 613, 10862, 264, 1570, 2931, 436, 603, 352, 2085, 2619, 390, 51304, 51304, 456, 512, 1651, 1338, 286, 914, 309, 311, 257, 3507, 3207, 281, 264, 281, 264, 589, 2939, 300, 51609, 51609], "temperature": 0.0, "avg_logprob": -0.10610463505699522, "compression_ratio": 1.6565217391304348, "no_speech_prob": 8.747587708057836e-05}, {"id": 984, "seek": 555888, "start": 5568.92, "end": 5572.68, "text": " about the things that might go wrong I do think though that the more people", "tokens": [50364, 8263, 6011, 534, 1627, 2132, 293, 370, 5220, 1228, 613, 1238, 777, 426, 45196, 50704, 50704, 7512, 300, 366, 586, 1238, 9515, 293, 286, 478, 611, 534, 5804, 50866, 50866, 466, 264, 721, 300, 1062, 352, 2085, 286, 360, 519, 1673, 300, 264, 544, 561, 51054, 51054, 300, 1223, 613, 10862, 264, 1570, 2931, 436, 603, 352, 2085, 2619, 390, 51304, 51304, 456, 512, 1651, 1338, 286, 914, 309, 311, 257, 3507, 3207, 281, 264, 281, 264, 589, 2939, 300, 51609, 51609], "temperature": 0.0, "avg_logprob": -0.10610463505699522, "compression_ratio": 1.6565217391304348, "no_speech_prob": 8.747587708057836e-05}, {"id": 985, "seek": 555888, "start": 5572.68, "end": 5577.68, "text": " that understand these capabilities the less chance they'll go wrong John was", "tokens": [50364, 8263, 6011, 534, 1627, 2132, 293, 370, 5220, 1228, 613, 1238, 777, 426, 45196, 50704, 50704, 7512, 300, 366, 586, 1238, 9515, 293, 286, 478, 611, 534, 5804, 50866, 50866, 466, 264, 721, 300, 1062, 352, 2085, 286, 360, 519, 1673, 300, 264, 544, 561, 51054, 51054, 300, 1223, 613, 10862, 264, 1570, 2931, 436, 603, 352, 2085, 2619, 390, 51304, 51304, 456, 512, 1651, 1338, 286, 914, 309, 311, 257, 3507, 3207, 281, 264, 281, 264, 589, 2939, 300, 51609, 51609], "temperature": 0.0, "avg_logprob": -0.10610463505699522, "compression_ratio": 1.6565217391304348, "no_speech_prob": 8.747587708057836e-05}, {"id": 986, "seek": 555888, "start": 5577.68, "end": 5583.78, "text": " there some questions yeah I mean it's a throwback to the to the workbook that", "tokens": [50364, 8263, 6011, 534, 1627, 2132, 293, 370, 5220, 1228, 613, 1238, 777, 426, 45196, 50704, 50704, 7512, 300, 366, 586, 1238, 9515, 293, 286, 478, 611, 534, 5804, 50866, 50866, 466, 264, 721, 300, 1062, 352, 2085, 286, 360, 519, 1673, 300, 264, 544, 561, 51054, 51054, 300, 1223, 613, 10862, 264, 1570, 2931, 436, 603, 352, 2085, 2619, 390, 51304, 51304, 456, 512, 1651, 1338, 286, 914, 309, 311, 257, 3507, 3207, 281, 264, 281, 264, 589, 2939, 300, 51609, 51609], "temperature": 0.0, "avg_logprob": -0.10610463505699522, "compression_ratio": 1.6565217391304348, "no_speech_prob": 8.747587708057836e-05}, {"id": 987, "seek": 558378, "start": 5583.78, "end": 5593.4, "text": " you had before yeah that's the one the question Manikandan is asking shouldn't", "tokens": [50364, 291, 632, 949, 1338, 300, 311, 264, 472, 264, 1168, 2458, 1035, 39762, 307, 3365, 4659, 380, 50845, 50845, 1031, 16949, 312, 1732, 4018, 4018, 935, 732, 1732, 4018, 935, 1732, 4018, 935, 3407, 51125, 51125, 1732, 472, 2602, 295, 472, 1943, 380, 264, 3779, 257, 19250, 804, 420, 366, 321, 8079, 341, 51423, 51423, 382, 257, 24590, 1154, 1338, 309, 311, 257, 665, 1168, 370, 456, 311, 472, 7645, 570, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.15777055104573567, "compression_ratio": 1.6989247311827957, "no_speech_prob": 5.8268546126782894e-05}, {"id": 988, "seek": 558378, "start": 5593.4, "end": 5599.0, "text": " num labels be five zero zero point two five zero point five zero point seven", "tokens": [50364, 291, 632, 949, 1338, 300, 311, 264, 472, 264, 1168, 2458, 1035, 39762, 307, 3365, 4659, 380, 50845, 50845, 1031, 16949, 312, 1732, 4018, 4018, 935, 732, 1732, 4018, 935, 1732, 4018, 935, 3407, 51125, 51125, 1732, 472, 2602, 295, 472, 1943, 380, 264, 3779, 257, 19250, 804, 420, 366, 321, 8079, 341, 51423, 51423, 382, 257, 24590, 1154, 1338, 309, 311, 257, 665, 1168, 370, 456, 311, 472, 7645, 570, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.15777055104573567, "compression_ratio": 1.6989247311827957, "no_speech_prob": 5.8268546126782894e-05}, {"id": 989, "seek": 558378, "start": 5599.0, "end": 5604.96, "text": " five one instead of one isn't the target a categorical or are we considering this", "tokens": [50364, 291, 632, 949, 1338, 300, 311, 264, 472, 264, 1168, 2458, 1035, 39762, 307, 3365, 4659, 380, 50845, 50845, 1031, 16949, 312, 1732, 4018, 4018, 935, 732, 1732, 4018, 935, 1732, 4018, 935, 3407, 51125, 51125, 1732, 472, 2602, 295, 472, 1943, 380, 264, 3779, 257, 19250, 804, 420, 366, 321, 8079, 341, 51423, 51423, 382, 257, 24590, 1154, 1338, 309, 311, 257, 665, 1168, 370, 456, 311, 472, 7645, 570, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.15777055104573567, "compression_ratio": 1.6989247311827957, "no_speech_prob": 5.8268546126782894e-05}, {"id": 990, "seek": 558378, "start": 5604.96, "end": 5610.639999999999, "text": " as a regression problem yeah it's a good question so there's one label because", "tokens": [50364, 291, 632, 949, 1338, 300, 311, 264, 472, 264, 1168, 2458, 1035, 39762, 307, 3365, 4659, 380, 50845, 50845, 1031, 16949, 312, 1732, 4018, 4018, 935, 732, 1732, 4018, 935, 1732, 4018, 935, 3407, 51125, 51125, 1732, 472, 2602, 295, 472, 1943, 380, 264, 3779, 257, 19250, 804, 420, 366, 321, 8079, 341, 51423, 51423, 382, 257, 24590, 1154, 1338, 309, 311, 257, 665, 1168, 370, 456, 311, 472, 7645, 570, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.15777055104573567, "compression_ratio": 1.6989247311827957, "no_speech_prob": 5.8268546126782894e-05}, {"id": 991, "seek": 561064, "start": 5610.64, "end": 5616.400000000001, "text": " there's one column even if this was being treated as a categorical problem", "tokens": [50364, 456, 311, 472, 7738, 754, 498, 341, 390, 885, 8668, 382, 257, 19250, 804, 1154, 50652, 50652, 365, 1732, 10479, 309, 311, 920, 4888, 472, 7645, 294, 341, 1389, 1673, 50968, 50968, 321, 434, 767, 15083, 309, 382, 257, 24590, 1154, 309, 311, 445, 472, 295, 264, 51305, 51305, 721, 309, 311, 257, 857, 12414, 286, 390, 1382, 281, 2573, 341, 484, 445, 264, 661, 786, 309, 311, 51450, 51450, 406, 23007, 382, 1400, 382, 286, 393, 980, 457, 322, 264, 41706, 1851, 4088, 433, 3144, 51650, 51650], "temperature": 0.0, "avg_logprob": -0.09596988889906141, "compression_ratio": 1.7162162162162162, "no_speech_prob": 6.204182136571035e-05}, {"id": 992, "seek": 561064, "start": 5616.400000000001, "end": 5622.72, "text": " with five categories it's still considered one label in this case though", "tokens": [50364, 456, 311, 472, 7738, 754, 498, 341, 390, 885, 8668, 382, 257, 19250, 804, 1154, 50652, 50652, 365, 1732, 10479, 309, 311, 920, 4888, 472, 7645, 294, 341, 1389, 1673, 50968, 50968, 321, 434, 767, 15083, 309, 382, 257, 24590, 1154, 309, 311, 445, 472, 295, 264, 51305, 51305, 721, 309, 311, 257, 857, 12414, 286, 390, 1382, 281, 2573, 341, 484, 445, 264, 661, 786, 309, 311, 51450, 51450, 406, 23007, 382, 1400, 382, 286, 393, 980, 457, 322, 264, 41706, 1851, 4088, 433, 3144, 51650, 51650], "temperature": 0.0, "avg_logprob": -0.09596988889906141, "compression_ratio": 1.7162162162162162, "no_speech_prob": 6.204182136571035e-05}, {"id": 993, "seek": 561064, "start": 5622.72, "end": 5629.46, "text": " we're actually treating it as a regression problem it's just one of the", "tokens": [50364, 456, 311, 472, 7738, 754, 498, 341, 390, 885, 8668, 382, 257, 19250, 804, 1154, 50652, 50652, 365, 1732, 10479, 309, 311, 920, 4888, 472, 7645, 294, 341, 1389, 1673, 50968, 50968, 321, 434, 767, 15083, 309, 382, 257, 24590, 1154, 309, 311, 445, 472, 295, 264, 51305, 51305, 721, 309, 311, 257, 857, 12414, 286, 390, 1382, 281, 2573, 341, 484, 445, 264, 661, 786, 309, 311, 51450, 51450, 406, 23007, 382, 1400, 382, 286, 393, 980, 457, 322, 264, 41706, 1851, 4088, 433, 3144, 51650, 51650], "temperature": 0.0, "avg_logprob": -0.09596988889906141, "compression_ratio": 1.7162162162162162, "no_speech_prob": 6.204182136571035e-05}, {"id": 994, "seek": 561064, "start": 5629.46, "end": 5632.360000000001, "text": " things it's a bit tricky I was trying to figure this out just the other day it's", "tokens": [50364, 456, 311, 472, 7738, 754, 498, 341, 390, 885, 8668, 382, 257, 19250, 804, 1154, 50652, 50652, 365, 1732, 10479, 309, 311, 920, 4888, 472, 7645, 294, 341, 1389, 1673, 50968, 50968, 321, 434, 767, 15083, 309, 382, 257, 24590, 1154, 309, 311, 445, 472, 295, 264, 51305, 51305, 721, 309, 311, 257, 857, 12414, 286, 390, 1382, 281, 2573, 341, 484, 445, 264, 661, 786, 309, 311, 51450, 51450, 406, 23007, 382, 1400, 382, 286, 393, 980, 457, 322, 264, 41706, 1851, 4088, 433, 3144, 51650, 51650], "temperature": 0.0, "avg_logprob": -0.09596988889906141, "compression_ratio": 1.7162162162162162, "no_speech_prob": 6.204182136571035e-05}, {"id": 995, "seek": 561064, "start": 5632.360000000001, "end": 5636.360000000001, "text": " not documented as far as I can tell but on the hugging face transformers website", "tokens": [50364, 456, 311, 472, 7738, 754, 498, 341, 390, 885, 8668, 382, 257, 19250, 804, 1154, 50652, 50652, 365, 1732, 10479, 309, 311, 920, 4888, 472, 7645, 294, 341, 1389, 1673, 50968, 50968, 321, 434, 767, 15083, 309, 382, 257, 24590, 1154, 309, 311, 445, 472, 295, 264, 51305, 51305, 721, 309, 311, 257, 857, 12414, 286, 390, 1382, 281, 2573, 341, 484, 445, 264, 661, 786, 309, 311, 51450, 51450, 406, 23007, 382, 1400, 382, 286, 393, 980, 457, 322, 264, 41706, 1851, 4088, 433, 3144, 51650, 51650], "temperature": 0.0, "avg_logprob": -0.09596988889906141, "compression_ratio": 1.7162162162162162, "no_speech_prob": 6.204182136571035e-05}, {"id": 996, "seek": 563636, "start": 5636.36, "end": 5641.0599999999995, "text": " but if you pass in one label to auto model for sequence classification it", "tokens": [50364, 457, 498, 291, 1320, 294, 472, 7645, 281, 8399, 2316, 337, 8310, 21538, 309, 50599, 50599, 4523, 309, 666, 257, 24590, 1154, 597, 307, 767, 983, 321, 4590, 493, 365, 50802, 50802, 21264, 300, 645, 1570, 813, 4018, 293, 3801, 813, 472, 370, 321, 603, 312, 2539, 51108, 51108, 958, 565, 466, 264, 764, 295, 4556, 3280, 327, 6828, 281, 14151, 341, 1154, 293, 51364, 51364, 300, 820, 300, 820, 3191, 309, 493, 337, 505, 1392, 869, 731, 3231, 2201, 286, 1454, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.10653982606045036, "compression_ratio": 1.6478260869565218, "no_speech_prob": 4.005433584097773e-05}, {"id": 997, "seek": 563636, "start": 5641.0599999999995, "end": 5645.12, "text": " turns it into a regression problem which is actually why we ended up with", "tokens": [50364, 457, 498, 291, 1320, 294, 472, 7645, 281, 8399, 2316, 337, 8310, 21538, 309, 50599, 50599, 4523, 309, 666, 257, 24590, 1154, 597, 307, 767, 983, 321, 4590, 493, 365, 50802, 50802, 21264, 300, 645, 1570, 813, 4018, 293, 3801, 813, 472, 370, 321, 603, 312, 2539, 51108, 51108, 958, 565, 466, 264, 764, 295, 4556, 3280, 327, 6828, 281, 14151, 341, 1154, 293, 51364, 51364, 300, 820, 300, 820, 3191, 309, 493, 337, 505, 1392, 869, 731, 3231, 2201, 286, 1454, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.10653982606045036, "compression_ratio": 1.6478260869565218, "no_speech_prob": 4.005433584097773e-05}, {"id": 998, "seek": 563636, "start": 5645.12, "end": 5651.24, "text": " predictions that were less than zero and bigger than one so we'll be learning", "tokens": [50364, 457, 498, 291, 1320, 294, 472, 7645, 281, 8399, 2316, 337, 8310, 21538, 309, 50599, 50599, 4523, 309, 666, 257, 24590, 1154, 597, 307, 767, 983, 321, 4590, 493, 365, 50802, 50802, 21264, 300, 645, 1570, 813, 4018, 293, 3801, 813, 472, 370, 321, 603, 312, 2539, 51108, 51108, 958, 565, 466, 264, 764, 295, 4556, 3280, 327, 6828, 281, 14151, 341, 1154, 293, 51364, 51364, 300, 820, 300, 820, 3191, 309, 493, 337, 505, 1392, 869, 731, 3231, 2201, 286, 1454, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.10653982606045036, "compression_ratio": 1.6478260869565218, "no_speech_prob": 4.005433584097773e-05}, {"id": 999, "seek": 563636, "start": 5651.24, "end": 5656.36, "text": " next time about the use of sigmoid functions to resolve this problem and", "tokens": [50364, 457, 498, 291, 1320, 294, 472, 7645, 281, 8399, 2316, 337, 8310, 21538, 309, 50599, 50599, 4523, 309, 666, 257, 24590, 1154, 597, 307, 767, 983, 321, 4590, 493, 365, 50802, 50802, 21264, 300, 645, 1570, 813, 4018, 293, 3801, 813, 472, 370, 321, 603, 312, 2539, 51108, 51108, 958, 565, 466, 264, 764, 295, 4556, 3280, 327, 6828, 281, 14151, 341, 1154, 293, 51364, 51364, 300, 820, 300, 820, 3191, 309, 493, 337, 505, 1392, 869, 731, 3231, 2201, 286, 1454, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.10653982606045036, "compression_ratio": 1.6478260869565218, "no_speech_prob": 4.005433584097773e-05}, {"id": 1000, "seek": 563636, "start": 5656.36, "end": 5664.08, "text": " that should that should fix it up for us okay great well thanks everybody I hope", "tokens": [50364, 457, 498, 291, 1320, 294, 472, 7645, 281, 8399, 2316, 337, 8310, 21538, 309, 50599, 50599, 4523, 309, 666, 257, 24590, 1154, 597, 307, 767, 983, 321, 4590, 493, 365, 50802, 50802, 21264, 300, 645, 1570, 813, 4018, 293, 3801, 813, 472, 370, 321, 603, 312, 2539, 51108, 51108, 958, 565, 466, 264, 764, 295, 4556, 3280, 327, 6828, 281, 14151, 341, 1154, 293, 51364, 51364, 300, 820, 300, 820, 3191, 309, 493, 337, 505, 1392, 869, 731, 3231, 2201, 286, 1454, 51750, 51750], "temperature": 0.0, "avg_logprob": -0.10653982606045036, "compression_ratio": 1.6478260869565218, "no_speech_prob": 4.005433584097773e-05}, {"id": 1001, "seek": 566408, "start": 5664.08, "end": 5668.6, "text": " you enjoyed learning about NLP as much as I enjoyed putting this together I'm", "tokens": [50364, 291, 4626, 2539, 466, 426, 45196, 382, 709, 382, 286, 4626, 3372, 341, 1214, 286, 478, 50590, 50590, 534, 2919, 466, 309, 293, 393, 380, 1699, 337, 958, 1243, 311, 6898, 536, 2478, 50926], "temperature": 0.0, "avg_logprob": -0.12348710166083442, "compression_ratio": 1.2695652173913043, "no_speech_prob": 6.576287705684081e-05}, {"id": 1002, "seek": 566860, "start": 5668.6, "end": 5695.88, "text": " really excited about it and can't wait for next week's lesson see ya", "tokens": [50364, 534, 2919, 466, 309, 293, 393, 380, 1699, 337, 958, 1243, 311, 6898, 536, 2478, 51728], "temperature": 0.0, "avg_logprob": -0.2828181849585639, "compression_ratio": 1.0461538461538462, "no_speech_prob": 0.00011170547077199444}], "language": "en", "video_id": "toUgBQv1BT8", "entity": "FastAI"}}