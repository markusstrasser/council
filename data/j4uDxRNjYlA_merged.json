{"video_id": "j4uDxRNjYlA", "title": "2.8 Practical Tips for Linear Regression | | Choosing the learning rate-[MachineLearning|Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 367, "views": 236, "publish_date": "11/04/2022", "timestamp": 1661126400, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " Your learning algorithm will run much better with an appropriate choice of learning rate. If it's too small, it will run very slowly, and if it's too large, it may not even converge. Let's take a look at how you can choose a good learning rate for your model. Concretely, if you plot the cost for a number of iterations and notice that the cost sometimes goes up and sometimes goes down, you should take that as a clear sign that gradient descent is not working properly. This could mean that there's a bug in the code, or sometimes it could mean that your learning rate is too large. So here's an illustration of what might be happening. Here the vertical axis is a cost function J, and the horizontal axis represents a parameter like maybe W1, and if the learning rate is too big, then if you start off here, your update step may overshoot the minimum and end up here, and in the next update step here, you're again overshooting, so you end up here, and so on. And that's why the cost can sometimes go up instead of decreasing. To fix this, you can use a smaller learning rate. So then your updates may start here and go down a little bit and down a bit, and will hopefully consistently decrease until it reaches the global minimum. Sometimes you may see that the cost consistently increases after each iteration, like this curve here. This is also likely due to a learning rate that is too large, and it could be addressed by choosing a smaller learning rate. But learning rates like this could also be a sign of a possible bug in the code. For example, if I wrote my code so that W1 gets updated as W1 plus alpha times this derivative term, this could result in the cost consistently increasing at each iteration. And this is because adding the derivative term moves your cost j further from the global minimum instead of close up. So remember, you want to use a minus sign, so the code should be updated W1, updated by W1 minus alpha times the derivative term. One debugging tip for a correct implementation of gradient descent is that with a small enough learning rate, the cost function should decrease on every single iteration. So if gradient descent isn't working, one thing I will often do, and I hope you find this tip useful too, one thing I'll often do is just set alpha to be a very, very small number and see if that causes the cost to decrease on every iteration. If even with alpha set to a very small number, j doesn't decrease on every single iteration, but instead sometimes increases, then that usually means there's a bug somewhere in the code. Note that setting alpha to be really, really small is meant here as a debugging step, and a very, very small value of alpha is not going to be the most efficient choice for actually training your learning algorithm. One important trade off is that if your learning rate is too small, then gradient descent can take a lot of iterations to converge. So when I am running gradient descent, I will usually try a range of values for the learning rate alpha. So I might start by trying a learning rate of 0.001, and I might also try a learning rate that's 10 times as large, say 0.01 and 0.1 and so on. And for each choice of alpha, you might run gradient descent just for a handful of iterations and plot the cost function j as a function of the number of iterations. And after trying a few different values, you might then pick the value of alpha that seems to decrease the learning rate rapidly, but also consistently. In fact, what I actually do is try a range of values like this. After trying 0.001, I'll then increase the learning rate threefold to 0.003. And after that, I'll try 0.01, which is again about three times as large as 0.003. So these are roughly trying out gradient descent with each value of alpha being roughly three times bigger than the previous value. So what I'll do is try a range of values until I found a value that's too small. And then also make sure I found a value that's too large. And I'll slowly try to pick the largest possible learning rate or just something slightly smaller than the largest reasonable value that I found. And when I do that, it usually gives me a good learning rate for my model. So I hope this technique too will be useful for you to choose a good learning rate for your implementation of gradient descent. In the upcoming optional lab, you can also take a look at how feature scaling is done in code and also see how different choices of the learning rate alpha can lead to either better or worse training of your model. I hope you have fun playing with the value of alpha and seeing the outcomes of different choices of alpha. So please take a look and run the code in the optional lab to gain a deeper intuition about feature scaling as well as the learning rate alpha. Training learning rates is an important part of training many learning algorithms. And I hope that this video gives you intuition about different choices and how to pick a good value for alpha. Now there are a couple more ideas they can use to make multiple linear regression much more powerful. And that is choosing custom features, which will also allow you to fit curves, not just a straight line to your data. Let's take a look at that in the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.76, "text": " Your learning algorithm will run much better with an appropriate choice of learning rate.", "tokens": [50364, 2260, 2539, 9284, 486, 1190, 709, 1101, 365, 364, 6854, 3922, 295, 2539, 3314, 13, 50702, 50702, 759, 309, 311, 886, 1359, 11, 309, 486, 1190, 588, 5692, 11, 293, 498, 309, 311, 886, 2416, 11, 309, 815, 406, 754, 41881, 13, 50996, 50996, 961, 311, 747, 257, 574, 412, 577, 291, 393, 2826, 257, 665, 2539, 3314, 337, 428, 2316, 13, 51200, 51200, 18200, 1505, 736, 11, 498, 291, 7542, 264, 2063, 337, 257, 1230, 295, 36540, 293, 3449, 300, 264, 2063, 2171, 51548, 51548, 1709, 493, 293, 2171, 1709, 760, 11, 291, 820, 747, 300, 382, 257, 1850, 1465, 300, 16235, 23475, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.11805565268905074, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.011327019892632961}, {"id": 1, "seek": 0, "start": 6.76, "end": 12.64, "text": " If it's too small, it will run very slowly, and if it's too large, it may not even converge.", "tokens": [50364, 2260, 2539, 9284, 486, 1190, 709, 1101, 365, 364, 6854, 3922, 295, 2539, 3314, 13, 50702, 50702, 759, 309, 311, 886, 1359, 11, 309, 486, 1190, 588, 5692, 11, 293, 498, 309, 311, 886, 2416, 11, 309, 815, 406, 754, 41881, 13, 50996, 50996, 961, 311, 747, 257, 574, 412, 577, 291, 393, 2826, 257, 665, 2539, 3314, 337, 428, 2316, 13, 51200, 51200, 18200, 1505, 736, 11, 498, 291, 7542, 264, 2063, 337, 257, 1230, 295, 36540, 293, 3449, 300, 264, 2063, 2171, 51548, 51548, 1709, 493, 293, 2171, 1709, 760, 11, 291, 820, 747, 300, 382, 257, 1850, 1465, 300, 16235, 23475, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.11805565268905074, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.011327019892632961}, {"id": 2, "seek": 0, "start": 12.64, "end": 16.72, "text": " Let's take a look at how you can choose a good learning rate for your model.", "tokens": [50364, 2260, 2539, 9284, 486, 1190, 709, 1101, 365, 364, 6854, 3922, 295, 2539, 3314, 13, 50702, 50702, 759, 309, 311, 886, 1359, 11, 309, 486, 1190, 588, 5692, 11, 293, 498, 309, 311, 886, 2416, 11, 309, 815, 406, 754, 41881, 13, 50996, 50996, 961, 311, 747, 257, 574, 412, 577, 291, 393, 2826, 257, 665, 2539, 3314, 337, 428, 2316, 13, 51200, 51200, 18200, 1505, 736, 11, 498, 291, 7542, 264, 2063, 337, 257, 1230, 295, 36540, 293, 3449, 300, 264, 2063, 2171, 51548, 51548, 1709, 493, 293, 2171, 1709, 760, 11, 291, 820, 747, 300, 382, 257, 1850, 1465, 300, 16235, 23475, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.11805565268905074, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.011327019892632961}, {"id": 3, "seek": 0, "start": 16.72, "end": 23.68, "text": " Concretely, if you plot the cost for a number of iterations and notice that the cost sometimes", "tokens": [50364, 2260, 2539, 9284, 486, 1190, 709, 1101, 365, 364, 6854, 3922, 295, 2539, 3314, 13, 50702, 50702, 759, 309, 311, 886, 1359, 11, 309, 486, 1190, 588, 5692, 11, 293, 498, 309, 311, 886, 2416, 11, 309, 815, 406, 754, 41881, 13, 50996, 50996, 961, 311, 747, 257, 574, 412, 577, 291, 393, 2826, 257, 665, 2539, 3314, 337, 428, 2316, 13, 51200, 51200, 18200, 1505, 736, 11, 498, 291, 7542, 264, 2063, 337, 257, 1230, 295, 36540, 293, 3449, 300, 264, 2063, 2171, 51548, 51548, 1709, 493, 293, 2171, 1709, 760, 11, 291, 820, 747, 300, 382, 257, 1850, 1465, 300, 16235, 23475, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.11805565268905074, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.011327019892632961}, {"id": 4, "seek": 0, "start": 23.68, "end": 29.64, "text": " goes up and sometimes goes down, you should take that as a clear sign that gradient descent", "tokens": [50364, 2260, 2539, 9284, 486, 1190, 709, 1101, 365, 364, 6854, 3922, 295, 2539, 3314, 13, 50702, 50702, 759, 309, 311, 886, 1359, 11, 309, 486, 1190, 588, 5692, 11, 293, 498, 309, 311, 886, 2416, 11, 309, 815, 406, 754, 41881, 13, 50996, 50996, 961, 311, 747, 257, 574, 412, 577, 291, 393, 2826, 257, 665, 2539, 3314, 337, 428, 2316, 13, 51200, 51200, 18200, 1505, 736, 11, 498, 291, 7542, 264, 2063, 337, 257, 1230, 295, 36540, 293, 3449, 300, 264, 2063, 2171, 51548, 51548, 1709, 493, 293, 2171, 1709, 760, 11, 291, 820, 747, 300, 382, 257, 1850, 1465, 300, 16235, 23475, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.11805565268905074, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.011327019892632961}, {"id": 5, "seek": 2964, "start": 29.64, "end": 31.76, "text": " is not working properly.", "tokens": [50364, 307, 406, 1364, 6108, 13, 50470, 50470, 639, 727, 914, 300, 456, 311, 257, 7426, 294, 264, 3089, 11, 420, 2171, 309, 727, 914, 300, 428, 50684, 50684, 2539, 3314, 307, 886, 2416, 13, 50804, 50804, 407, 510, 311, 364, 22645, 295, 437, 1062, 312, 2737, 13, 51004, 51004, 1692, 264, 9429, 10298, 307, 257, 2063, 2445, 508, 11, 293, 264, 12750, 10298, 8855, 257, 13075, 51399, 51399, 411, 1310, 343, 16, 11, 293, 498, 264, 2539, 3314, 307, 886, 955, 11, 550, 498, 291, 722, 766, 510, 11, 428, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.12872672588267225, "compression_ratio": 1.6576576576576576, "no_speech_prob": 1.147854436567286e-05}, {"id": 6, "seek": 2964, "start": 31.76, "end": 36.04, "text": " This could mean that there's a bug in the code, or sometimes it could mean that your", "tokens": [50364, 307, 406, 1364, 6108, 13, 50470, 50470, 639, 727, 914, 300, 456, 311, 257, 7426, 294, 264, 3089, 11, 420, 2171, 309, 727, 914, 300, 428, 50684, 50684, 2539, 3314, 307, 886, 2416, 13, 50804, 50804, 407, 510, 311, 364, 22645, 295, 437, 1062, 312, 2737, 13, 51004, 51004, 1692, 264, 9429, 10298, 307, 257, 2063, 2445, 508, 11, 293, 264, 12750, 10298, 8855, 257, 13075, 51399, 51399, 411, 1310, 343, 16, 11, 293, 498, 264, 2539, 3314, 307, 886, 955, 11, 550, 498, 291, 722, 766, 510, 11, 428, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.12872672588267225, "compression_ratio": 1.6576576576576576, "no_speech_prob": 1.147854436567286e-05}, {"id": 7, "seek": 2964, "start": 36.04, "end": 38.44, "text": " learning rate is too large.", "tokens": [50364, 307, 406, 1364, 6108, 13, 50470, 50470, 639, 727, 914, 300, 456, 311, 257, 7426, 294, 264, 3089, 11, 420, 2171, 309, 727, 914, 300, 428, 50684, 50684, 2539, 3314, 307, 886, 2416, 13, 50804, 50804, 407, 510, 311, 364, 22645, 295, 437, 1062, 312, 2737, 13, 51004, 51004, 1692, 264, 9429, 10298, 307, 257, 2063, 2445, 508, 11, 293, 264, 12750, 10298, 8855, 257, 13075, 51399, 51399, 411, 1310, 343, 16, 11, 293, 498, 264, 2539, 3314, 307, 886, 955, 11, 550, 498, 291, 722, 766, 510, 11, 428, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.12872672588267225, "compression_ratio": 1.6576576576576576, "no_speech_prob": 1.147854436567286e-05}, {"id": 8, "seek": 2964, "start": 38.44, "end": 42.44, "text": " So here's an illustration of what might be happening.", "tokens": [50364, 307, 406, 1364, 6108, 13, 50470, 50470, 639, 727, 914, 300, 456, 311, 257, 7426, 294, 264, 3089, 11, 420, 2171, 309, 727, 914, 300, 428, 50684, 50684, 2539, 3314, 307, 886, 2416, 13, 50804, 50804, 407, 510, 311, 364, 22645, 295, 437, 1062, 312, 2737, 13, 51004, 51004, 1692, 264, 9429, 10298, 307, 257, 2063, 2445, 508, 11, 293, 264, 12750, 10298, 8855, 257, 13075, 51399, 51399, 411, 1310, 343, 16, 11, 293, 498, 264, 2539, 3314, 307, 886, 955, 11, 550, 498, 291, 722, 766, 510, 11, 428, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.12872672588267225, "compression_ratio": 1.6576576576576576, "no_speech_prob": 1.147854436567286e-05}, {"id": 9, "seek": 2964, "start": 42.44, "end": 50.34, "text": " Here the vertical axis is a cost function J, and the horizontal axis represents a parameter", "tokens": [50364, 307, 406, 1364, 6108, 13, 50470, 50470, 639, 727, 914, 300, 456, 311, 257, 7426, 294, 264, 3089, 11, 420, 2171, 309, 727, 914, 300, 428, 50684, 50684, 2539, 3314, 307, 886, 2416, 13, 50804, 50804, 407, 510, 311, 364, 22645, 295, 437, 1062, 312, 2737, 13, 51004, 51004, 1692, 264, 9429, 10298, 307, 257, 2063, 2445, 508, 11, 293, 264, 12750, 10298, 8855, 257, 13075, 51399, 51399, 411, 1310, 343, 16, 11, 293, 498, 264, 2539, 3314, 307, 886, 955, 11, 550, 498, 291, 722, 766, 510, 11, 428, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.12872672588267225, "compression_ratio": 1.6576576576576576, "no_speech_prob": 1.147854436567286e-05}, {"id": 10, "seek": 2964, "start": 50.34, "end": 57.88, "text": " like maybe W1, and if the learning rate is too big, then if you start off here, your", "tokens": [50364, 307, 406, 1364, 6108, 13, 50470, 50470, 639, 727, 914, 300, 456, 311, 257, 7426, 294, 264, 3089, 11, 420, 2171, 309, 727, 914, 300, 428, 50684, 50684, 2539, 3314, 307, 886, 2416, 13, 50804, 50804, 407, 510, 311, 364, 22645, 295, 437, 1062, 312, 2737, 13, 51004, 51004, 1692, 264, 9429, 10298, 307, 257, 2063, 2445, 508, 11, 293, 264, 12750, 10298, 8855, 257, 13075, 51399, 51399, 411, 1310, 343, 16, 11, 293, 498, 264, 2539, 3314, 307, 886, 955, 11, 550, 498, 291, 722, 766, 510, 11, 428, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.12872672588267225, "compression_ratio": 1.6576576576576576, "no_speech_prob": 1.147854436567286e-05}, {"id": 11, "seek": 5788, "start": 57.88, "end": 63.92, "text": " update step may overshoot the minimum and end up here, and in the next update step here,", "tokens": [50364, 5623, 1823, 815, 15488, 24467, 264, 7285, 293, 917, 493, 510, 11, 293, 294, 264, 958, 5623, 1823, 510, 11, 50666, 50666, 291, 434, 797, 15488, 47011, 11, 370, 291, 917, 493, 510, 11, 293, 370, 322, 13, 50890, 50890, 400, 300, 311, 983, 264, 2063, 393, 2171, 352, 493, 2602, 295, 23223, 13, 51122, 51122, 1407, 3191, 341, 11, 291, 393, 764, 257, 4356, 2539, 3314, 13, 51274, 51274, 407, 550, 428, 9205, 815, 722, 510, 293, 352, 760, 257, 707, 857, 293, 760, 257, 857, 11, 293, 486, 51550, 51550, 4696, 14961, 11514, 1826, 309, 14235, 264, 4338, 7285, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.09359102429084058, "compression_ratio": 1.7344398340248963, "no_speech_prob": 9.665825018601026e-06}, {"id": 12, "seek": 5788, "start": 63.92, "end": 68.4, "text": " you're again overshooting, so you end up here, and so on.", "tokens": [50364, 5623, 1823, 815, 15488, 24467, 264, 7285, 293, 917, 493, 510, 11, 293, 294, 264, 958, 5623, 1823, 510, 11, 50666, 50666, 291, 434, 797, 15488, 47011, 11, 370, 291, 917, 493, 510, 11, 293, 370, 322, 13, 50890, 50890, 400, 300, 311, 983, 264, 2063, 393, 2171, 352, 493, 2602, 295, 23223, 13, 51122, 51122, 1407, 3191, 341, 11, 291, 393, 764, 257, 4356, 2539, 3314, 13, 51274, 51274, 407, 550, 428, 9205, 815, 722, 510, 293, 352, 760, 257, 707, 857, 293, 760, 257, 857, 11, 293, 486, 51550, 51550, 4696, 14961, 11514, 1826, 309, 14235, 264, 4338, 7285, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.09359102429084058, "compression_ratio": 1.7344398340248963, "no_speech_prob": 9.665825018601026e-06}, {"id": 13, "seek": 5788, "start": 68.4, "end": 73.04, "text": " And that's why the cost can sometimes go up instead of decreasing.", "tokens": [50364, 5623, 1823, 815, 15488, 24467, 264, 7285, 293, 917, 493, 510, 11, 293, 294, 264, 958, 5623, 1823, 510, 11, 50666, 50666, 291, 434, 797, 15488, 47011, 11, 370, 291, 917, 493, 510, 11, 293, 370, 322, 13, 50890, 50890, 400, 300, 311, 983, 264, 2063, 393, 2171, 352, 493, 2602, 295, 23223, 13, 51122, 51122, 1407, 3191, 341, 11, 291, 393, 764, 257, 4356, 2539, 3314, 13, 51274, 51274, 407, 550, 428, 9205, 815, 722, 510, 293, 352, 760, 257, 707, 857, 293, 760, 257, 857, 11, 293, 486, 51550, 51550, 4696, 14961, 11514, 1826, 309, 14235, 264, 4338, 7285, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.09359102429084058, "compression_ratio": 1.7344398340248963, "no_speech_prob": 9.665825018601026e-06}, {"id": 14, "seek": 5788, "start": 73.04, "end": 76.08, "text": " To fix this, you can use a smaller learning rate.", "tokens": [50364, 5623, 1823, 815, 15488, 24467, 264, 7285, 293, 917, 493, 510, 11, 293, 294, 264, 958, 5623, 1823, 510, 11, 50666, 50666, 291, 434, 797, 15488, 47011, 11, 370, 291, 917, 493, 510, 11, 293, 370, 322, 13, 50890, 50890, 400, 300, 311, 983, 264, 2063, 393, 2171, 352, 493, 2602, 295, 23223, 13, 51122, 51122, 1407, 3191, 341, 11, 291, 393, 764, 257, 4356, 2539, 3314, 13, 51274, 51274, 407, 550, 428, 9205, 815, 722, 510, 293, 352, 760, 257, 707, 857, 293, 760, 257, 857, 11, 293, 486, 51550, 51550, 4696, 14961, 11514, 1826, 309, 14235, 264, 4338, 7285, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.09359102429084058, "compression_ratio": 1.7344398340248963, "no_speech_prob": 9.665825018601026e-06}, {"id": 15, "seek": 5788, "start": 76.08, "end": 81.6, "text": " So then your updates may start here and go down a little bit and down a bit, and will", "tokens": [50364, 5623, 1823, 815, 15488, 24467, 264, 7285, 293, 917, 493, 510, 11, 293, 294, 264, 958, 5623, 1823, 510, 11, 50666, 50666, 291, 434, 797, 15488, 47011, 11, 370, 291, 917, 493, 510, 11, 293, 370, 322, 13, 50890, 50890, 400, 300, 311, 983, 264, 2063, 393, 2171, 352, 493, 2602, 295, 23223, 13, 51122, 51122, 1407, 3191, 341, 11, 291, 393, 764, 257, 4356, 2539, 3314, 13, 51274, 51274, 407, 550, 428, 9205, 815, 722, 510, 293, 352, 760, 257, 707, 857, 293, 760, 257, 857, 11, 293, 486, 51550, 51550, 4696, 14961, 11514, 1826, 309, 14235, 264, 4338, 7285, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.09359102429084058, "compression_ratio": 1.7344398340248963, "no_speech_prob": 9.665825018601026e-06}, {"id": 16, "seek": 5788, "start": 81.6, "end": 86.92, "text": " hopefully consistently decrease until it reaches the global minimum.", "tokens": [50364, 5623, 1823, 815, 15488, 24467, 264, 7285, 293, 917, 493, 510, 11, 293, 294, 264, 958, 5623, 1823, 510, 11, 50666, 50666, 291, 434, 797, 15488, 47011, 11, 370, 291, 917, 493, 510, 11, 293, 370, 322, 13, 50890, 50890, 400, 300, 311, 983, 264, 2063, 393, 2171, 352, 493, 2602, 295, 23223, 13, 51122, 51122, 1407, 3191, 341, 11, 291, 393, 764, 257, 4356, 2539, 3314, 13, 51274, 51274, 407, 550, 428, 9205, 815, 722, 510, 293, 352, 760, 257, 707, 857, 293, 760, 257, 857, 11, 293, 486, 51550, 51550, 4696, 14961, 11514, 1826, 309, 14235, 264, 4338, 7285, 13, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.09359102429084058, "compression_ratio": 1.7344398340248963, "no_speech_prob": 9.665825018601026e-06}, {"id": 17, "seek": 8692, "start": 86.92, "end": 92.52, "text": " Sometimes you may see that the cost consistently increases after each iteration, like this", "tokens": [50364, 4803, 291, 815, 536, 300, 264, 2063, 14961, 8637, 934, 1184, 24784, 11, 411, 341, 50644, 50644, 7605, 510, 13, 50716, 50716, 639, 307, 611, 3700, 3462, 281, 257, 2539, 3314, 300, 307, 886, 2416, 11, 293, 309, 727, 312, 13847, 50948, 50948, 538, 10875, 257, 4356, 2539, 3314, 13, 51120, 51120, 583, 2539, 6846, 411, 341, 727, 611, 312, 257, 1465, 295, 257, 1944, 7426, 294, 264, 3089, 13, 51376, 51376, 1171, 1365, 11, 498, 286, 4114, 452, 3089, 370, 300, 343, 16, 2170, 10588, 382, 343, 16, 1804, 8961, 1413, 341, 13760, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.08358964141534299, "compression_ratio": 1.6584362139917694, "no_speech_prob": 1.0845072893062024e-06}, {"id": 18, "seek": 8692, "start": 92.52, "end": 93.96000000000001, "text": " curve here.", "tokens": [50364, 4803, 291, 815, 536, 300, 264, 2063, 14961, 8637, 934, 1184, 24784, 11, 411, 341, 50644, 50644, 7605, 510, 13, 50716, 50716, 639, 307, 611, 3700, 3462, 281, 257, 2539, 3314, 300, 307, 886, 2416, 11, 293, 309, 727, 312, 13847, 50948, 50948, 538, 10875, 257, 4356, 2539, 3314, 13, 51120, 51120, 583, 2539, 6846, 411, 341, 727, 611, 312, 257, 1465, 295, 257, 1944, 7426, 294, 264, 3089, 13, 51376, 51376, 1171, 1365, 11, 498, 286, 4114, 452, 3089, 370, 300, 343, 16, 2170, 10588, 382, 343, 16, 1804, 8961, 1413, 341, 13760, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.08358964141534299, "compression_ratio": 1.6584362139917694, "no_speech_prob": 1.0845072893062024e-06}, {"id": 19, "seek": 8692, "start": 93.96000000000001, "end": 98.6, "text": " This is also likely due to a learning rate that is too large, and it could be addressed", "tokens": [50364, 4803, 291, 815, 536, 300, 264, 2063, 14961, 8637, 934, 1184, 24784, 11, 411, 341, 50644, 50644, 7605, 510, 13, 50716, 50716, 639, 307, 611, 3700, 3462, 281, 257, 2539, 3314, 300, 307, 886, 2416, 11, 293, 309, 727, 312, 13847, 50948, 50948, 538, 10875, 257, 4356, 2539, 3314, 13, 51120, 51120, 583, 2539, 6846, 411, 341, 727, 611, 312, 257, 1465, 295, 257, 1944, 7426, 294, 264, 3089, 13, 51376, 51376, 1171, 1365, 11, 498, 286, 4114, 452, 3089, 370, 300, 343, 16, 2170, 10588, 382, 343, 16, 1804, 8961, 1413, 341, 13760, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.08358964141534299, "compression_ratio": 1.6584362139917694, "no_speech_prob": 1.0845072893062024e-06}, {"id": 20, "seek": 8692, "start": 98.6, "end": 102.04, "text": " by choosing a smaller learning rate.", "tokens": [50364, 4803, 291, 815, 536, 300, 264, 2063, 14961, 8637, 934, 1184, 24784, 11, 411, 341, 50644, 50644, 7605, 510, 13, 50716, 50716, 639, 307, 611, 3700, 3462, 281, 257, 2539, 3314, 300, 307, 886, 2416, 11, 293, 309, 727, 312, 13847, 50948, 50948, 538, 10875, 257, 4356, 2539, 3314, 13, 51120, 51120, 583, 2539, 6846, 411, 341, 727, 611, 312, 257, 1465, 295, 257, 1944, 7426, 294, 264, 3089, 13, 51376, 51376, 1171, 1365, 11, 498, 286, 4114, 452, 3089, 370, 300, 343, 16, 2170, 10588, 382, 343, 16, 1804, 8961, 1413, 341, 13760, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.08358964141534299, "compression_ratio": 1.6584362139917694, "no_speech_prob": 1.0845072893062024e-06}, {"id": 21, "seek": 8692, "start": 102.04, "end": 107.16, "text": " But learning rates like this could also be a sign of a possible bug in the code.", "tokens": [50364, 4803, 291, 815, 536, 300, 264, 2063, 14961, 8637, 934, 1184, 24784, 11, 411, 341, 50644, 50644, 7605, 510, 13, 50716, 50716, 639, 307, 611, 3700, 3462, 281, 257, 2539, 3314, 300, 307, 886, 2416, 11, 293, 309, 727, 312, 13847, 50948, 50948, 538, 10875, 257, 4356, 2539, 3314, 13, 51120, 51120, 583, 2539, 6846, 411, 341, 727, 611, 312, 257, 1465, 295, 257, 1944, 7426, 294, 264, 3089, 13, 51376, 51376, 1171, 1365, 11, 498, 286, 4114, 452, 3089, 370, 300, 343, 16, 2170, 10588, 382, 343, 16, 1804, 8961, 1413, 341, 13760, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.08358964141534299, "compression_ratio": 1.6584362139917694, "no_speech_prob": 1.0845072893062024e-06}, {"id": 22, "seek": 8692, "start": 107.16, "end": 115.84, "text": " For example, if I wrote my code so that W1 gets updated as W1 plus alpha times this derivative", "tokens": [50364, 4803, 291, 815, 536, 300, 264, 2063, 14961, 8637, 934, 1184, 24784, 11, 411, 341, 50644, 50644, 7605, 510, 13, 50716, 50716, 639, 307, 611, 3700, 3462, 281, 257, 2539, 3314, 300, 307, 886, 2416, 11, 293, 309, 727, 312, 13847, 50948, 50948, 538, 10875, 257, 4356, 2539, 3314, 13, 51120, 51120, 583, 2539, 6846, 411, 341, 727, 611, 312, 257, 1465, 295, 257, 1944, 7426, 294, 264, 3089, 13, 51376, 51376, 1171, 1365, 11, 498, 286, 4114, 452, 3089, 370, 300, 343, 16, 2170, 10588, 382, 343, 16, 1804, 8961, 1413, 341, 13760, 51810, 51810], "temperature": 0.0, "avg_logprob": -0.08358964141534299, "compression_ratio": 1.6584362139917694, "no_speech_prob": 1.0845072893062024e-06}, {"id": 23, "seek": 11584, "start": 115.84, "end": 122.08, "text": " term, this could result in the cost consistently increasing at each iteration.", "tokens": [50364, 1433, 11, 341, 727, 1874, 294, 264, 2063, 14961, 5662, 412, 1184, 24784, 13, 50676, 50676, 400, 341, 307, 570, 5127, 264, 13760, 1433, 6067, 428, 2063, 361, 3052, 490, 264, 4338, 50958, 50958, 7285, 2602, 295, 1998, 493, 13, 51078, 51078, 407, 1604, 11, 291, 528, 281, 764, 257, 3175, 1465, 11, 370, 264, 3089, 820, 312, 10588, 343, 16, 11, 10588, 51404, 51404, 538, 343, 16, 3175, 8961, 1413, 264, 13760, 1433, 13, 51683, 51683], "temperature": 0.0, "avg_logprob": -0.12106253225592119, "compression_ratio": 1.6059113300492611, "no_speech_prob": 2.5215272216883022e-06}, {"id": 24, "seek": 11584, "start": 122.08, "end": 127.72, "text": " And this is because adding the derivative term moves your cost j further from the global", "tokens": [50364, 1433, 11, 341, 727, 1874, 294, 264, 2063, 14961, 5662, 412, 1184, 24784, 13, 50676, 50676, 400, 341, 307, 570, 5127, 264, 13760, 1433, 6067, 428, 2063, 361, 3052, 490, 264, 4338, 50958, 50958, 7285, 2602, 295, 1998, 493, 13, 51078, 51078, 407, 1604, 11, 291, 528, 281, 764, 257, 3175, 1465, 11, 370, 264, 3089, 820, 312, 10588, 343, 16, 11, 10588, 51404, 51404, 538, 343, 16, 3175, 8961, 1413, 264, 13760, 1433, 13, 51683, 51683], "temperature": 0.0, "avg_logprob": -0.12106253225592119, "compression_ratio": 1.6059113300492611, "no_speech_prob": 2.5215272216883022e-06}, {"id": 25, "seek": 11584, "start": 127.72, "end": 130.12, "text": " minimum instead of close up.", "tokens": [50364, 1433, 11, 341, 727, 1874, 294, 264, 2063, 14961, 5662, 412, 1184, 24784, 13, 50676, 50676, 400, 341, 307, 570, 5127, 264, 13760, 1433, 6067, 428, 2063, 361, 3052, 490, 264, 4338, 50958, 50958, 7285, 2602, 295, 1998, 493, 13, 51078, 51078, 407, 1604, 11, 291, 528, 281, 764, 257, 3175, 1465, 11, 370, 264, 3089, 820, 312, 10588, 343, 16, 11, 10588, 51404, 51404, 538, 343, 16, 3175, 8961, 1413, 264, 13760, 1433, 13, 51683, 51683], "temperature": 0.0, "avg_logprob": -0.12106253225592119, "compression_ratio": 1.6059113300492611, "no_speech_prob": 2.5215272216883022e-06}, {"id": 26, "seek": 11584, "start": 130.12, "end": 136.64000000000001, "text": " So remember, you want to use a minus sign, so the code should be updated W1, updated", "tokens": [50364, 1433, 11, 341, 727, 1874, 294, 264, 2063, 14961, 5662, 412, 1184, 24784, 13, 50676, 50676, 400, 341, 307, 570, 5127, 264, 13760, 1433, 6067, 428, 2063, 361, 3052, 490, 264, 4338, 50958, 50958, 7285, 2602, 295, 1998, 493, 13, 51078, 51078, 407, 1604, 11, 291, 528, 281, 764, 257, 3175, 1465, 11, 370, 264, 3089, 820, 312, 10588, 343, 16, 11, 10588, 51404, 51404, 538, 343, 16, 3175, 8961, 1413, 264, 13760, 1433, 13, 51683, 51683], "temperature": 0.0, "avg_logprob": -0.12106253225592119, "compression_ratio": 1.6059113300492611, "no_speech_prob": 2.5215272216883022e-06}, {"id": 27, "seek": 11584, "start": 136.64000000000001, "end": 142.22, "text": " by W1 minus alpha times the derivative term.", "tokens": [50364, 1433, 11, 341, 727, 1874, 294, 264, 2063, 14961, 5662, 412, 1184, 24784, 13, 50676, 50676, 400, 341, 307, 570, 5127, 264, 13760, 1433, 6067, 428, 2063, 361, 3052, 490, 264, 4338, 50958, 50958, 7285, 2602, 295, 1998, 493, 13, 51078, 51078, 407, 1604, 11, 291, 528, 281, 764, 257, 3175, 1465, 11, 370, 264, 3089, 820, 312, 10588, 343, 16, 11, 10588, 51404, 51404, 538, 343, 16, 3175, 8961, 1413, 264, 13760, 1433, 13, 51683, 51683], "temperature": 0.0, "avg_logprob": -0.12106253225592119, "compression_ratio": 1.6059113300492611, "no_speech_prob": 2.5215272216883022e-06}, {"id": 28, "seek": 14222, "start": 142.22, "end": 147.72, "text": " One debugging tip for a correct implementation of gradient descent is that with a small enough", "tokens": [50364, 1485, 45592, 4125, 337, 257, 3006, 11420, 295, 16235, 23475, 307, 300, 365, 257, 1359, 1547, 50639, 50639, 2539, 3314, 11, 264, 2063, 2445, 820, 11514, 322, 633, 2167, 24784, 13, 50943, 50943, 407, 498, 16235, 23475, 1943, 380, 1364, 11, 472, 551, 286, 486, 2049, 360, 11, 293, 286, 1454, 291, 915, 51187, 51187, 341, 4125, 4420, 886, 11, 472, 551, 286, 603, 2049, 360, 307, 445, 992, 8961, 281, 312, 257, 588, 11, 588, 1359, 51485, 51485, 1230, 293, 536, 498, 300, 7700, 264, 2063, 281, 11514, 322, 633, 24784, 13, 51841, 51841], "temperature": 0.0, "avg_logprob": -0.08130361124412301, "compression_ratio": 1.7768240343347639, "no_speech_prob": 3.39310145136551e-06}, {"id": 29, "seek": 14222, "start": 147.72, "end": 153.8, "text": " learning rate, the cost function should decrease on every single iteration.", "tokens": [50364, 1485, 45592, 4125, 337, 257, 3006, 11420, 295, 16235, 23475, 307, 300, 365, 257, 1359, 1547, 50639, 50639, 2539, 3314, 11, 264, 2063, 2445, 820, 11514, 322, 633, 2167, 24784, 13, 50943, 50943, 407, 498, 16235, 23475, 1943, 380, 1364, 11, 472, 551, 286, 486, 2049, 360, 11, 293, 286, 1454, 291, 915, 51187, 51187, 341, 4125, 4420, 886, 11, 472, 551, 286, 603, 2049, 360, 307, 445, 992, 8961, 281, 312, 257, 588, 11, 588, 1359, 51485, 51485, 1230, 293, 536, 498, 300, 7700, 264, 2063, 281, 11514, 322, 633, 24784, 13, 51841, 51841], "temperature": 0.0, "avg_logprob": -0.08130361124412301, "compression_ratio": 1.7768240343347639, "no_speech_prob": 3.39310145136551e-06}, {"id": 30, "seek": 14222, "start": 153.8, "end": 158.68, "text": " So if gradient descent isn't working, one thing I will often do, and I hope you find", "tokens": [50364, 1485, 45592, 4125, 337, 257, 3006, 11420, 295, 16235, 23475, 307, 300, 365, 257, 1359, 1547, 50639, 50639, 2539, 3314, 11, 264, 2063, 2445, 820, 11514, 322, 633, 2167, 24784, 13, 50943, 50943, 407, 498, 16235, 23475, 1943, 380, 1364, 11, 472, 551, 286, 486, 2049, 360, 11, 293, 286, 1454, 291, 915, 51187, 51187, 341, 4125, 4420, 886, 11, 472, 551, 286, 603, 2049, 360, 307, 445, 992, 8961, 281, 312, 257, 588, 11, 588, 1359, 51485, 51485, 1230, 293, 536, 498, 300, 7700, 264, 2063, 281, 11514, 322, 633, 24784, 13, 51841, 51841], "temperature": 0.0, "avg_logprob": -0.08130361124412301, "compression_ratio": 1.7768240343347639, "no_speech_prob": 3.39310145136551e-06}, {"id": 31, "seek": 14222, "start": 158.68, "end": 164.64, "text": " this tip useful too, one thing I'll often do is just set alpha to be a very, very small", "tokens": [50364, 1485, 45592, 4125, 337, 257, 3006, 11420, 295, 16235, 23475, 307, 300, 365, 257, 1359, 1547, 50639, 50639, 2539, 3314, 11, 264, 2063, 2445, 820, 11514, 322, 633, 2167, 24784, 13, 50943, 50943, 407, 498, 16235, 23475, 1943, 380, 1364, 11, 472, 551, 286, 486, 2049, 360, 11, 293, 286, 1454, 291, 915, 51187, 51187, 341, 4125, 4420, 886, 11, 472, 551, 286, 603, 2049, 360, 307, 445, 992, 8961, 281, 312, 257, 588, 11, 588, 1359, 51485, 51485, 1230, 293, 536, 498, 300, 7700, 264, 2063, 281, 11514, 322, 633, 24784, 13, 51841, 51841], "temperature": 0.0, "avg_logprob": -0.08130361124412301, "compression_ratio": 1.7768240343347639, "no_speech_prob": 3.39310145136551e-06}, {"id": 32, "seek": 14222, "start": 164.64, "end": 171.76, "text": " number and see if that causes the cost to decrease on every iteration.", "tokens": [50364, 1485, 45592, 4125, 337, 257, 3006, 11420, 295, 16235, 23475, 307, 300, 365, 257, 1359, 1547, 50639, 50639, 2539, 3314, 11, 264, 2063, 2445, 820, 11514, 322, 633, 2167, 24784, 13, 50943, 50943, 407, 498, 16235, 23475, 1943, 380, 1364, 11, 472, 551, 286, 486, 2049, 360, 11, 293, 286, 1454, 291, 915, 51187, 51187, 341, 4125, 4420, 886, 11, 472, 551, 286, 603, 2049, 360, 307, 445, 992, 8961, 281, 312, 257, 588, 11, 588, 1359, 51485, 51485, 1230, 293, 536, 498, 300, 7700, 264, 2063, 281, 11514, 322, 633, 24784, 13, 51841, 51841], "temperature": 0.0, "avg_logprob": -0.08130361124412301, "compression_ratio": 1.7768240343347639, "no_speech_prob": 3.39310145136551e-06}, {"id": 33, "seek": 17176, "start": 171.76, "end": 178.32, "text": " If even with alpha set to a very small number, j doesn't decrease on every single iteration,", "tokens": [50364, 759, 754, 365, 8961, 992, 281, 257, 588, 1359, 1230, 11, 361, 1177, 380, 11514, 322, 633, 2167, 24784, 11, 50692, 50692, 457, 2602, 2171, 8637, 11, 550, 300, 2673, 1355, 456, 311, 257, 7426, 4079, 294, 264, 50910, 50910, 3089, 13, 51018, 51018, 11633, 300, 3287, 8961, 281, 312, 534, 11, 534, 1359, 307, 4140, 510, 382, 257, 45592, 1823, 11, 293, 51308, 51308, 257, 588, 11, 588, 1359, 2158, 295, 8961, 307, 406, 516, 281, 312, 264, 881, 7148, 3922, 337, 767, 51532, 51532, 3097, 428, 2539, 9284, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.09864354384572882, "compression_ratio": 1.6962025316455696, "no_speech_prob": 7.766795533825643e-06}, {"id": 34, "seek": 17176, "start": 178.32, "end": 182.67999999999998, "text": " but instead sometimes increases, then that usually means there's a bug somewhere in the", "tokens": [50364, 759, 754, 365, 8961, 992, 281, 257, 588, 1359, 1230, 11, 361, 1177, 380, 11514, 322, 633, 2167, 24784, 11, 50692, 50692, 457, 2602, 2171, 8637, 11, 550, 300, 2673, 1355, 456, 311, 257, 7426, 4079, 294, 264, 50910, 50910, 3089, 13, 51018, 51018, 11633, 300, 3287, 8961, 281, 312, 534, 11, 534, 1359, 307, 4140, 510, 382, 257, 45592, 1823, 11, 293, 51308, 51308, 257, 588, 11, 588, 1359, 2158, 295, 8961, 307, 406, 516, 281, 312, 264, 881, 7148, 3922, 337, 767, 51532, 51532, 3097, 428, 2539, 9284, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.09864354384572882, "compression_ratio": 1.6962025316455696, "no_speech_prob": 7.766795533825643e-06}, {"id": 35, "seek": 17176, "start": 182.67999999999998, "end": 184.84, "text": " code.", "tokens": [50364, 759, 754, 365, 8961, 992, 281, 257, 588, 1359, 1230, 11, 361, 1177, 380, 11514, 322, 633, 2167, 24784, 11, 50692, 50692, 457, 2602, 2171, 8637, 11, 550, 300, 2673, 1355, 456, 311, 257, 7426, 4079, 294, 264, 50910, 50910, 3089, 13, 51018, 51018, 11633, 300, 3287, 8961, 281, 312, 534, 11, 534, 1359, 307, 4140, 510, 382, 257, 45592, 1823, 11, 293, 51308, 51308, 257, 588, 11, 588, 1359, 2158, 295, 8961, 307, 406, 516, 281, 312, 264, 881, 7148, 3922, 337, 767, 51532, 51532, 3097, 428, 2539, 9284, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.09864354384572882, "compression_ratio": 1.6962025316455696, "no_speech_prob": 7.766795533825643e-06}, {"id": 36, "seek": 17176, "start": 184.84, "end": 190.64, "text": " Note that setting alpha to be really, really small is meant here as a debugging step, and", "tokens": [50364, 759, 754, 365, 8961, 992, 281, 257, 588, 1359, 1230, 11, 361, 1177, 380, 11514, 322, 633, 2167, 24784, 11, 50692, 50692, 457, 2602, 2171, 8637, 11, 550, 300, 2673, 1355, 456, 311, 257, 7426, 4079, 294, 264, 50910, 50910, 3089, 13, 51018, 51018, 11633, 300, 3287, 8961, 281, 312, 534, 11, 534, 1359, 307, 4140, 510, 382, 257, 45592, 1823, 11, 293, 51308, 51308, 257, 588, 11, 588, 1359, 2158, 295, 8961, 307, 406, 516, 281, 312, 264, 881, 7148, 3922, 337, 767, 51532, 51532, 3097, 428, 2539, 9284, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.09864354384572882, "compression_ratio": 1.6962025316455696, "no_speech_prob": 7.766795533825643e-06}, {"id": 37, "seek": 17176, "start": 190.64, "end": 195.12, "text": " a very, very small value of alpha is not going to be the most efficient choice for actually", "tokens": [50364, 759, 754, 365, 8961, 992, 281, 257, 588, 1359, 1230, 11, 361, 1177, 380, 11514, 322, 633, 2167, 24784, 11, 50692, 50692, 457, 2602, 2171, 8637, 11, 550, 300, 2673, 1355, 456, 311, 257, 7426, 4079, 294, 264, 50910, 50910, 3089, 13, 51018, 51018, 11633, 300, 3287, 8961, 281, 312, 534, 11, 534, 1359, 307, 4140, 510, 382, 257, 45592, 1823, 11, 293, 51308, 51308, 257, 588, 11, 588, 1359, 2158, 295, 8961, 307, 406, 516, 281, 312, 264, 881, 7148, 3922, 337, 767, 51532, 51532, 3097, 428, 2539, 9284, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.09864354384572882, "compression_ratio": 1.6962025316455696, "no_speech_prob": 7.766795533825643e-06}, {"id": 38, "seek": 17176, "start": 195.12, "end": 197.6, "text": " training your learning algorithm.", "tokens": [50364, 759, 754, 365, 8961, 992, 281, 257, 588, 1359, 1230, 11, 361, 1177, 380, 11514, 322, 633, 2167, 24784, 11, 50692, 50692, 457, 2602, 2171, 8637, 11, 550, 300, 2673, 1355, 456, 311, 257, 7426, 4079, 294, 264, 50910, 50910, 3089, 13, 51018, 51018, 11633, 300, 3287, 8961, 281, 312, 534, 11, 534, 1359, 307, 4140, 510, 382, 257, 45592, 1823, 11, 293, 51308, 51308, 257, 588, 11, 588, 1359, 2158, 295, 8961, 307, 406, 516, 281, 312, 264, 881, 7148, 3922, 337, 767, 51532, 51532, 3097, 428, 2539, 9284, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.09864354384572882, "compression_ratio": 1.6962025316455696, "no_speech_prob": 7.766795533825643e-06}, {"id": 39, "seek": 19760, "start": 197.6, "end": 202.88, "text": " One important trade off is that if your learning rate is too small, then gradient descent can", "tokens": [50364, 1485, 1021, 4923, 766, 307, 300, 498, 428, 2539, 3314, 307, 886, 1359, 11, 550, 16235, 23475, 393, 50628, 50628, 747, 257, 688, 295, 36540, 281, 41881, 13, 50790, 50790, 407, 562, 286, 669, 2614, 16235, 23475, 11, 286, 486, 2673, 853, 257, 3613, 295, 4190, 337, 264, 2539, 51072, 51072, 3314, 8961, 13, 51122, 51122, 407, 286, 1062, 722, 538, 1382, 257, 2539, 3314, 295, 1958, 13, 628, 16, 11, 293, 286, 1062, 611, 853, 257, 2539, 51434, 51434, 3314, 300, 311, 1266, 1413, 382, 2416, 11, 584, 1958, 13, 10607, 293, 1958, 13, 16, 293, 370, 322, 13, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.09942254653343788, "compression_ratio": 1.7431192660550459, "no_speech_prob": 2.642555273268954e-06}, {"id": 40, "seek": 19760, "start": 202.88, "end": 206.12, "text": " take a lot of iterations to converge.", "tokens": [50364, 1485, 1021, 4923, 766, 307, 300, 498, 428, 2539, 3314, 307, 886, 1359, 11, 550, 16235, 23475, 393, 50628, 50628, 747, 257, 688, 295, 36540, 281, 41881, 13, 50790, 50790, 407, 562, 286, 669, 2614, 16235, 23475, 11, 286, 486, 2673, 853, 257, 3613, 295, 4190, 337, 264, 2539, 51072, 51072, 3314, 8961, 13, 51122, 51122, 407, 286, 1062, 722, 538, 1382, 257, 2539, 3314, 295, 1958, 13, 628, 16, 11, 293, 286, 1062, 611, 853, 257, 2539, 51434, 51434, 3314, 300, 311, 1266, 1413, 382, 2416, 11, 584, 1958, 13, 10607, 293, 1958, 13, 16, 293, 370, 322, 13, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.09942254653343788, "compression_ratio": 1.7431192660550459, "no_speech_prob": 2.642555273268954e-06}, {"id": 41, "seek": 19760, "start": 206.12, "end": 211.76, "text": " So when I am running gradient descent, I will usually try a range of values for the learning", "tokens": [50364, 1485, 1021, 4923, 766, 307, 300, 498, 428, 2539, 3314, 307, 886, 1359, 11, 550, 16235, 23475, 393, 50628, 50628, 747, 257, 688, 295, 36540, 281, 41881, 13, 50790, 50790, 407, 562, 286, 669, 2614, 16235, 23475, 11, 286, 486, 2673, 853, 257, 3613, 295, 4190, 337, 264, 2539, 51072, 51072, 3314, 8961, 13, 51122, 51122, 407, 286, 1062, 722, 538, 1382, 257, 2539, 3314, 295, 1958, 13, 628, 16, 11, 293, 286, 1062, 611, 853, 257, 2539, 51434, 51434, 3314, 300, 311, 1266, 1413, 382, 2416, 11, 584, 1958, 13, 10607, 293, 1958, 13, 16, 293, 370, 322, 13, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.09942254653343788, "compression_ratio": 1.7431192660550459, "no_speech_prob": 2.642555273268954e-06}, {"id": 42, "seek": 19760, "start": 211.76, "end": 212.76, "text": " rate alpha.", "tokens": [50364, 1485, 1021, 4923, 766, 307, 300, 498, 428, 2539, 3314, 307, 886, 1359, 11, 550, 16235, 23475, 393, 50628, 50628, 747, 257, 688, 295, 36540, 281, 41881, 13, 50790, 50790, 407, 562, 286, 669, 2614, 16235, 23475, 11, 286, 486, 2673, 853, 257, 3613, 295, 4190, 337, 264, 2539, 51072, 51072, 3314, 8961, 13, 51122, 51122, 407, 286, 1062, 722, 538, 1382, 257, 2539, 3314, 295, 1958, 13, 628, 16, 11, 293, 286, 1062, 611, 853, 257, 2539, 51434, 51434, 3314, 300, 311, 1266, 1413, 382, 2416, 11, 584, 1958, 13, 10607, 293, 1958, 13, 16, 293, 370, 322, 13, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.09942254653343788, "compression_ratio": 1.7431192660550459, "no_speech_prob": 2.642555273268954e-06}, {"id": 43, "seek": 19760, "start": 212.76, "end": 219.0, "text": " So I might start by trying a learning rate of 0.001, and I might also try a learning", "tokens": [50364, 1485, 1021, 4923, 766, 307, 300, 498, 428, 2539, 3314, 307, 886, 1359, 11, 550, 16235, 23475, 393, 50628, 50628, 747, 257, 688, 295, 36540, 281, 41881, 13, 50790, 50790, 407, 562, 286, 669, 2614, 16235, 23475, 11, 286, 486, 2673, 853, 257, 3613, 295, 4190, 337, 264, 2539, 51072, 51072, 3314, 8961, 13, 51122, 51122, 407, 286, 1062, 722, 538, 1382, 257, 2539, 3314, 295, 1958, 13, 628, 16, 11, 293, 286, 1062, 611, 853, 257, 2539, 51434, 51434, 3314, 300, 311, 1266, 1413, 382, 2416, 11, 584, 1958, 13, 10607, 293, 1958, 13, 16, 293, 370, 322, 13, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.09942254653343788, "compression_ratio": 1.7431192660550459, "no_speech_prob": 2.642555273268954e-06}, {"id": 44, "seek": 19760, "start": 219.0, "end": 225.35999999999999, "text": " rate that's 10 times as large, say 0.01 and 0.1 and so on.", "tokens": [50364, 1485, 1021, 4923, 766, 307, 300, 498, 428, 2539, 3314, 307, 886, 1359, 11, 550, 16235, 23475, 393, 50628, 50628, 747, 257, 688, 295, 36540, 281, 41881, 13, 50790, 50790, 407, 562, 286, 669, 2614, 16235, 23475, 11, 286, 486, 2673, 853, 257, 3613, 295, 4190, 337, 264, 2539, 51072, 51072, 3314, 8961, 13, 51122, 51122, 407, 286, 1062, 722, 538, 1382, 257, 2539, 3314, 295, 1958, 13, 628, 16, 11, 293, 286, 1062, 611, 853, 257, 2539, 51434, 51434, 3314, 300, 311, 1266, 1413, 382, 2416, 11, 584, 1958, 13, 10607, 293, 1958, 13, 16, 293, 370, 322, 13, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.09942254653343788, "compression_ratio": 1.7431192660550459, "no_speech_prob": 2.642555273268954e-06}, {"id": 45, "seek": 22536, "start": 225.36, "end": 231.20000000000002, "text": " And for each choice of alpha, you might run gradient descent just for a handful of iterations", "tokens": [50364, 400, 337, 1184, 3922, 295, 8961, 11, 291, 1062, 1190, 16235, 23475, 445, 337, 257, 16458, 295, 36540, 50656, 50656, 293, 7542, 264, 2063, 2445, 361, 382, 257, 2445, 295, 264, 1230, 295, 36540, 13, 50954, 50954, 400, 934, 1382, 257, 1326, 819, 4190, 11, 291, 1062, 550, 1888, 264, 2158, 295, 8961, 300, 2544, 51220, 51220, 281, 11514, 264, 2539, 3314, 12910, 11, 457, 611, 14961, 13, 51476, 51476, 682, 1186, 11, 437, 286, 767, 360, 307, 853, 257, 3613, 295, 4190, 411, 341, 13, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.08469890488518608, "compression_ratio": 1.6902654867256637, "no_speech_prob": 5.422047252068296e-06}, {"id": 46, "seek": 22536, "start": 231.20000000000002, "end": 237.16000000000003, "text": " and plot the cost function j as a function of the number of iterations.", "tokens": [50364, 400, 337, 1184, 3922, 295, 8961, 11, 291, 1062, 1190, 16235, 23475, 445, 337, 257, 16458, 295, 36540, 50656, 50656, 293, 7542, 264, 2063, 2445, 361, 382, 257, 2445, 295, 264, 1230, 295, 36540, 13, 50954, 50954, 400, 934, 1382, 257, 1326, 819, 4190, 11, 291, 1062, 550, 1888, 264, 2158, 295, 8961, 300, 2544, 51220, 51220, 281, 11514, 264, 2539, 3314, 12910, 11, 457, 611, 14961, 13, 51476, 51476, 682, 1186, 11, 437, 286, 767, 360, 307, 853, 257, 3613, 295, 4190, 411, 341, 13, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.08469890488518608, "compression_ratio": 1.6902654867256637, "no_speech_prob": 5.422047252068296e-06}, {"id": 47, "seek": 22536, "start": 237.16000000000003, "end": 242.48000000000002, "text": " And after trying a few different values, you might then pick the value of alpha that seems", "tokens": [50364, 400, 337, 1184, 3922, 295, 8961, 11, 291, 1062, 1190, 16235, 23475, 445, 337, 257, 16458, 295, 36540, 50656, 50656, 293, 7542, 264, 2063, 2445, 361, 382, 257, 2445, 295, 264, 1230, 295, 36540, 13, 50954, 50954, 400, 934, 1382, 257, 1326, 819, 4190, 11, 291, 1062, 550, 1888, 264, 2158, 295, 8961, 300, 2544, 51220, 51220, 281, 11514, 264, 2539, 3314, 12910, 11, 457, 611, 14961, 13, 51476, 51476, 682, 1186, 11, 437, 286, 767, 360, 307, 853, 257, 3613, 295, 4190, 411, 341, 13, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.08469890488518608, "compression_ratio": 1.6902654867256637, "no_speech_prob": 5.422047252068296e-06}, {"id": 48, "seek": 22536, "start": 242.48000000000002, "end": 247.60000000000002, "text": " to decrease the learning rate rapidly, but also consistently.", "tokens": [50364, 400, 337, 1184, 3922, 295, 8961, 11, 291, 1062, 1190, 16235, 23475, 445, 337, 257, 16458, 295, 36540, 50656, 50656, 293, 7542, 264, 2063, 2445, 361, 382, 257, 2445, 295, 264, 1230, 295, 36540, 13, 50954, 50954, 400, 934, 1382, 257, 1326, 819, 4190, 11, 291, 1062, 550, 1888, 264, 2158, 295, 8961, 300, 2544, 51220, 51220, 281, 11514, 264, 2539, 3314, 12910, 11, 457, 611, 14961, 13, 51476, 51476, 682, 1186, 11, 437, 286, 767, 360, 307, 853, 257, 3613, 295, 4190, 411, 341, 13, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.08469890488518608, "compression_ratio": 1.6902654867256637, "no_speech_prob": 5.422047252068296e-06}, {"id": 49, "seek": 22536, "start": 247.60000000000002, "end": 252.68, "text": " In fact, what I actually do is try a range of values like this.", "tokens": [50364, 400, 337, 1184, 3922, 295, 8961, 11, 291, 1062, 1190, 16235, 23475, 445, 337, 257, 16458, 295, 36540, 50656, 50656, 293, 7542, 264, 2063, 2445, 361, 382, 257, 2445, 295, 264, 1230, 295, 36540, 13, 50954, 50954, 400, 934, 1382, 257, 1326, 819, 4190, 11, 291, 1062, 550, 1888, 264, 2158, 295, 8961, 300, 2544, 51220, 51220, 281, 11514, 264, 2539, 3314, 12910, 11, 457, 611, 14961, 13, 51476, 51476, 682, 1186, 11, 437, 286, 767, 360, 307, 853, 257, 3613, 295, 4190, 411, 341, 13, 51730, 51730], "temperature": 0.0, "avg_logprob": -0.08469890488518608, "compression_ratio": 1.6902654867256637, "no_speech_prob": 5.422047252068296e-06}, {"id": 50, "seek": 25268, "start": 252.68, "end": 260.12, "text": " After trying 0.001, I'll then increase the learning rate threefold to 0.003.", "tokens": [50364, 2381, 1382, 1958, 13, 628, 16, 11, 286, 603, 550, 3488, 264, 2539, 3314, 1045, 18353, 281, 1958, 13, 628, 18, 13, 50736, 50736, 400, 934, 300, 11, 286, 603, 853, 1958, 13, 10607, 11, 597, 307, 797, 466, 1045, 1413, 382, 2416, 382, 1958, 13, 628, 18, 13, 51143, 51143, 407, 613, 366, 9810, 1382, 484, 16235, 23475, 365, 1184, 2158, 295, 8961, 885, 9810, 1045, 51395, 51395, 1413, 3801, 813, 264, 3894, 2158, 13, 51592, 51592, 407, 437, 286, 603, 360, 307, 853, 257, 3613, 295, 4190, 1826, 286, 1352, 257, 2158, 300, 311, 886, 1359, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.11294124195876631, "compression_ratio": 1.6681614349775784, "no_speech_prob": 5.507530659087934e-06}, {"id": 51, "seek": 25268, "start": 260.12, "end": 268.26, "text": " And after that, I'll try 0.01, which is again about three times as large as 0.003.", "tokens": [50364, 2381, 1382, 1958, 13, 628, 16, 11, 286, 603, 550, 3488, 264, 2539, 3314, 1045, 18353, 281, 1958, 13, 628, 18, 13, 50736, 50736, 400, 934, 300, 11, 286, 603, 853, 1958, 13, 10607, 11, 597, 307, 797, 466, 1045, 1413, 382, 2416, 382, 1958, 13, 628, 18, 13, 51143, 51143, 407, 613, 366, 9810, 1382, 484, 16235, 23475, 365, 1184, 2158, 295, 8961, 885, 9810, 1045, 51395, 51395, 1413, 3801, 813, 264, 3894, 2158, 13, 51592, 51592, 407, 437, 286, 603, 360, 307, 853, 257, 3613, 295, 4190, 1826, 286, 1352, 257, 2158, 300, 311, 886, 1359, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.11294124195876631, "compression_ratio": 1.6681614349775784, "no_speech_prob": 5.507530659087934e-06}, {"id": 52, "seek": 25268, "start": 268.26, "end": 273.3, "text": " So these are roughly trying out gradient descent with each value of alpha being roughly three", "tokens": [50364, 2381, 1382, 1958, 13, 628, 16, 11, 286, 603, 550, 3488, 264, 2539, 3314, 1045, 18353, 281, 1958, 13, 628, 18, 13, 50736, 50736, 400, 934, 300, 11, 286, 603, 853, 1958, 13, 10607, 11, 597, 307, 797, 466, 1045, 1413, 382, 2416, 382, 1958, 13, 628, 18, 13, 51143, 51143, 407, 613, 366, 9810, 1382, 484, 16235, 23475, 365, 1184, 2158, 295, 8961, 885, 9810, 1045, 51395, 51395, 1413, 3801, 813, 264, 3894, 2158, 13, 51592, 51592, 407, 437, 286, 603, 360, 307, 853, 257, 3613, 295, 4190, 1826, 286, 1352, 257, 2158, 300, 311, 886, 1359, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.11294124195876631, "compression_ratio": 1.6681614349775784, "no_speech_prob": 5.507530659087934e-06}, {"id": 53, "seek": 25268, "start": 273.3, "end": 277.24, "text": " times bigger than the previous value.", "tokens": [50364, 2381, 1382, 1958, 13, 628, 16, 11, 286, 603, 550, 3488, 264, 2539, 3314, 1045, 18353, 281, 1958, 13, 628, 18, 13, 50736, 50736, 400, 934, 300, 11, 286, 603, 853, 1958, 13, 10607, 11, 597, 307, 797, 466, 1045, 1413, 382, 2416, 382, 1958, 13, 628, 18, 13, 51143, 51143, 407, 613, 366, 9810, 1382, 484, 16235, 23475, 365, 1184, 2158, 295, 8961, 885, 9810, 1045, 51395, 51395, 1413, 3801, 813, 264, 3894, 2158, 13, 51592, 51592, 407, 437, 286, 603, 360, 307, 853, 257, 3613, 295, 4190, 1826, 286, 1352, 257, 2158, 300, 311, 886, 1359, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.11294124195876631, "compression_ratio": 1.6681614349775784, "no_speech_prob": 5.507530659087934e-06}, {"id": 54, "seek": 25268, "start": 277.24, "end": 281.84000000000003, "text": " So what I'll do is try a range of values until I found a value that's too small.", "tokens": [50364, 2381, 1382, 1958, 13, 628, 16, 11, 286, 603, 550, 3488, 264, 2539, 3314, 1045, 18353, 281, 1958, 13, 628, 18, 13, 50736, 50736, 400, 934, 300, 11, 286, 603, 853, 1958, 13, 10607, 11, 597, 307, 797, 466, 1045, 1413, 382, 2416, 382, 1958, 13, 628, 18, 13, 51143, 51143, 407, 613, 366, 9810, 1382, 484, 16235, 23475, 365, 1184, 2158, 295, 8961, 885, 9810, 1045, 51395, 51395, 1413, 3801, 813, 264, 3894, 2158, 13, 51592, 51592, 407, 437, 286, 603, 360, 307, 853, 257, 3613, 295, 4190, 1826, 286, 1352, 257, 2158, 300, 311, 886, 1359, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.11294124195876631, "compression_ratio": 1.6681614349775784, "no_speech_prob": 5.507530659087934e-06}, {"id": 55, "seek": 28184, "start": 281.84, "end": 286.08, "text": " And then also make sure I found a value that's too large.", "tokens": [50364, 400, 550, 611, 652, 988, 286, 1352, 257, 2158, 300, 311, 886, 2416, 13, 50576, 50576, 400, 286, 603, 5692, 853, 281, 1888, 264, 6443, 1944, 2539, 3314, 420, 445, 746, 4748, 4356, 50900, 50900, 813, 264, 6443, 10585, 2158, 300, 286, 1352, 13, 51070, 51070, 400, 562, 286, 360, 300, 11, 309, 2673, 2709, 385, 257, 665, 2539, 3314, 337, 452, 2316, 13, 51305, 51305, 407, 286, 1454, 341, 6532, 886, 486, 312, 4420, 337, 291, 281, 2826, 257, 665, 2539, 3314, 337, 51560, 51560, 428, 11420, 295, 16235, 23475, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.09571651617685954, "compression_ratio": 1.7058823529411764, "no_speech_prob": 4.356786121206824e-06}, {"id": 56, "seek": 28184, "start": 286.08, "end": 292.56, "text": " And I'll slowly try to pick the largest possible learning rate or just something slightly smaller", "tokens": [50364, 400, 550, 611, 652, 988, 286, 1352, 257, 2158, 300, 311, 886, 2416, 13, 50576, 50576, 400, 286, 603, 5692, 853, 281, 1888, 264, 6443, 1944, 2539, 3314, 420, 445, 746, 4748, 4356, 50900, 50900, 813, 264, 6443, 10585, 2158, 300, 286, 1352, 13, 51070, 51070, 400, 562, 286, 360, 300, 11, 309, 2673, 2709, 385, 257, 665, 2539, 3314, 337, 452, 2316, 13, 51305, 51305, 407, 286, 1454, 341, 6532, 886, 486, 312, 4420, 337, 291, 281, 2826, 257, 665, 2539, 3314, 337, 51560, 51560, 428, 11420, 295, 16235, 23475, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.09571651617685954, "compression_ratio": 1.7058823529411764, "no_speech_prob": 4.356786121206824e-06}, {"id": 57, "seek": 28184, "start": 292.56, "end": 295.96, "text": " than the largest reasonable value that I found.", "tokens": [50364, 400, 550, 611, 652, 988, 286, 1352, 257, 2158, 300, 311, 886, 2416, 13, 50576, 50576, 400, 286, 603, 5692, 853, 281, 1888, 264, 6443, 1944, 2539, 3314, 420, 445, 746, 4748, 4356, 50900, 50900, 813, 264, 6443, 10585, 2158, 300, 286, 1352, 13, 51070, 51070, 400, 562, 286, 360, 300, 11, 309, 2673, 2709, 385, 257, 665, 2539, 3314, 337, 452, 2316, 13, 51305, 51305, 407, 286, 1454, 341, 6532, 886, 486, 312, 4420, 337, 291, 281, 2826, 257, 665, 2539, 3314, 337, 51560, 51560, 428, 11420, 295, 16235, 23475, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.09571651617685954, "compression_ratio": 1.7058823529411764, "no_speech_prob": 4.356786121206824e-06}, {"id": 58, "seek": 28184, "start": 295.96, "end": 300.65999999999997, "text": " And when I do that, it usually gives me a good learning rate for my model.", "tokens": [50364, 400, 550, 611, 652, 988, 286, 1352, 257, 2158, 300, 311, 886, 2416, 13, 50576, 50576, 400, 286, 603, 5692, 853, 281, 1888, 264, 6443, 1944, 2539, 3314, 420, 445, 746, 4748, 4356, 50900, 50900, 813, 264, 6443, 10585, 2158, 300, 286, 1352, 13, 51070, 51070, 400, 562, 286, 360, 300, 11, 309, 2673, 2709, 385, 257, 665, 2539, 3314, 337, 452, 2316, 13, 51305, 51305, 407, 286, 1454, 341, 6532, 886, 486, 312, 4420, 337, 291, 281, 2826, 257, 665, 2539, 3314, 337, 51560, 51560, 428, 11420, 295, 16235, 23475, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.09571651617685954, "compression_ratio": 1.7058823529411764, "no_speech_prob": 4.356786121206824e-06}, {"id": 59, "seek": 28184, "start": 300.65999999999997, "end": 305.76, "text": " So I hope this technique too will be useful for you to choose a good learning rate for", "tokens": [50364, 400, 550, 611, 652, 988, 286, 1352, 257, 2158, 300, 311, 886, 2416, 13, 50576, 50576, 400, 286, 603, 5692, 853, 281, 1888, 264, 6443, 1944, 2539, 3314, 420, 445, 746, 4748, 4356, 50900, 50900, 813, 264, 6443, 10585, 2158, 300, 286, 1352, 13, 51070, 51070, 400, 562, 286, 360, 300, 11, 309, 2673, 2709, 385, 257, 665, 2539, 3314, 337, 452, 2316, 13, 51305, 51305, 407, 286, 1454, 341, 6532, 886, 486, 312, 4420, 337, 291, 281, 2826, 257, 665, 2539, 3314, 337, 51560, 51560, 428, 11420, 295, 16235, 23475, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.09571651617685954, "compression_ratio": 1.7058823529411764, "no_speech_prob": 4.356786121206824e-06}, {"id": 60, "seek": 28184, "start": 305.76, "end": 309.91999999999996, "text": " your implementation of gradient descent.", "tokens": [50364, 400, 550, 611, 652, 988, 286, 1352, 257, 2158, 300, 311, 886, 2416, 13, 50576, 50576, 400, 286, 603, 5692, 853, 281, 1888, 264, 6443, 1944, 2539, 3314, 420, 445, 746, 4748, 4356, 50900, 50900, 813, 264, 6443, 10585, 2158, 300, 286, 1352, 13, 51070, 51070, 400, 562, 286, 360, 300, 11, 309, 2673, 2709, 385, 257, 665, 2539, 3314, 337, 452, 2316, 13, 51305, 51305, 407, 286, 1454, 341, 6532, 886, 486, 312, 4420, 337, 291, 281, 2826, 257, 665, 2539, 3314, 337, 51560, 51560, 428, 11420, 295, 16235, 23475, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.09571651617685954, "compression_ratio": 1.7058823529411764, "no_speech_prob": 4.356786121206824e-06}, {"id": 61, "seek": 30992, "start": 309.92, "end": 315.08000000000004, "text": " In the upcoming optional lab, you can also take a look at how feature scaling is done", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 393, 611, 747, 257, 574, 412, 577, 4111, 21589, 307, 1096, 50622, 50622, 294, 3089, 293, 611, 536, 577, 819, 7994, 295, 264, 2539, 3314, 8961, 393, 1477, 281, 2139, 50906, 50906, 1101, 420, 5324, 3097, 295, 428, 2316, 13, 51076, 51076, 286, 1454, 291, 362, 1019, 2433, 365, 264, 2158, 295, 8961, 293, 2577, 264, 10070, 295, 819, 51309, 51309, 7994, 295, 8961, 13, 51403, 51403, 407, 1767, 747, 257, 574, 293, 1190, 264, 3089, 294, 264, 17312, 2715, 281, 6052, 257, 7731, 24002, 51644, 51644, 466, 4111, 21589, 382, 731, 382, 264, 2539, 3314, 8961, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.08481377417888117, "compression_ratio": 1.9135802469135803, "no_speech_prob": 5.1737397370743565e-06}, {"id": 62, "seek": 30992, "start": 315.08000000000004, "end": 320.76, "text": " in code and also see how different choices of the learning rate alpha can lead to either", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 393, 611, 747, 257, 574, 412, 577, 4111, 21589, 307, 1096, 50622, 50622, 294, 3089, 293, 611, 536, 577, 819, 7994, 295, 264, 2539, 3314, 8961, 393, 1477, 281, 2139, 50906, 50906, 1101, 420, 5324, 3097, 295, 428, 2316, 13, 51076, 51076, 286, 1454, 291, 362, 1019, 2433, 365, 264, 2158, 295, 8961, 293, 2577, 264, 10070, 295, 819, 51309, 51309, 7994, 295, 8961, 13, 51403, 51403, 407, 1767, 747, 257, 574, 293, 1190, 264, 3089, 294, 264, 17312, 2715, 281, 6052, 257, 7731, 24002, 51644, 51644, 466, 4111, 21589, 382, 731, 382, 264, 2539, 3314, 8961, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.08481377417888117, "compression_ratio": 1.9135802469135803, "no_speech_prob": 5.1737397370743565e-06}, {"id": 63, "seek": 30992, "start": 320.76, "end": 324.16, "text": " better or worse training of your model.", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 393, 611, 747, 257, 574, 412, 577, 4111, 21589, 307, 1096, 50622, 50622, 294, 3089, 293, 611, 536, 577, 819, 7994, 295, 264, 2539, 3314, 8961, 393, 1477, 281, 2139, 50906, 50906, 1101, 420, 5324, 3097, 295, 428, 2316, 13, 51076, 51076, 286, 1454, 291, 362, 1019, 2433, 365, 264, 2158, 295, 8961, 293, 2577, 264, 10070, 295, 819, 51309, 51309, 7994, 295, 8961, 13, 51403, 51403, 407, 1767, 747, 257, 574, 293, 1190, 264, 3089, 294, 264, 17312, 2715, 281, 6052, 257, 7731, 24002, 51644, 51644, 466, 4111, 21589, 382, 731, 382, 264, 2539, 3314, 8961, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.08481377417888117, "compression_ratio": 1.9135802469135803, "no_speech_prob": 5.1737397370743565e-06}, {"id": 64, "seek": 30992, "start": 324.16, "end": 328.82, "text": " I hope you have fun playing with the value of alpha and seeing the outcomes of different", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 393, 611, 747, 257, 574, 412, 577, 4111, 21589, 307, 1096, 50622, 50622, 294, 3089, 293, 611, 536, 577, 819, 7994, 295, 264, 2539, 3314, 8961, 393, 1477, 281, 2139, 50906, 50906, 1101, 420, 5324, 3097, 295, 428, 2316, 13, 51076, 51076, 286, 1454, 291, 362, 1019, 2433, 365, 264, 2158, 295, 8961, 293, 2577, 264, 10070, 295, 819, 51309, 51309, 7994, 295, 8961, 13, 51403, 51403, 407, 1767, 747, 257, 574, 293, 1190, 264, 3089, 294, 264, 17312, 2715, 281, 6052, 257, 7731, 24002, 51644, 51644, 466, 4111, 21589, 382, 731, 382, 264, 2539, 3314, 8961, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.08481377417888117, "compression_ratio": 1.9135802469135803, "no_speech_prob": 5.1737397370743565e-06}, {"id": 65, "seek": 30992, "start": 328.82, "end": 330.70000000000005, "text": " choices of alpha.", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 393, 611, 747, 257, 574, 412, 577, 4111, 21589, 307, 1096, 50622, 50622, 294, 3089, 293, 611, 536, 577, 819, 7994, 295, 264, 2539, 3314, 8961, 393, 1477, 281, 2139, 50906, 50906, 1101, 420, 5324, 3097, 295, 428, 2316, 13, 51076, 51076, 286, 1454, 291, 362, 1019, 2433, 365, 264, 2158, 295, 8961, 293, 2577, 264, 10070, 295, 819, 51309, 51309, 7994, 295, 8961, 13, 51403, 51403, 407, 1767, 747, 257, 574, 293, 1190, 264, 3089, 294, 264, 17312, 2715, 281, 6052, 257, 7731, 24002, 51644, 51644, 466, 4111, 21589, 382, 731, 382, 264, 2539, 3314, 8961, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.08481377417888117, "compression_ratio": 1.9135802469135803, "no_speech_prob": 5.1737397370743565e-06}, {"id": 66, "seek": 30992, "start": 330.70000000000005, "end": 335.52000000000004, "text": " So please take a look and run the code in the optional lab to gain a deeper intuition", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 393, 611, 747, 257, 574, 412, 577, 4111, 21589, 307, 1096, 50622, 50622, 294, 3089, 293, 611, 536, 577, 819, 7994, 295, 264, 2539, 3314, 8961, 393, 1477, 281, 2139, 50906, 50906, 1101, 420, 5324, 3097, 295, 428, 2316, 13, 51076, 51076, 286, 1454, 291, 362, 1019, 2433, 365, 264, 2158, 295, 8961, 293, 2577, 264, 10070, 295, 819, 51309, 51309, 7994, 295, 8961, 13, 51403, 51403, 407, 1767, 747, 257, 574, 293, 1190, 264, 3089, 294, 264, 17312, 2715, 281, 6052, 257, 7731, 24002, 51644, 51644, 466, 4111, 21589, 382, 731, 382, 264, 2539, 3314, 8961, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.08481377417888117, "compression_ratio": 1.9135802469135803, "no_speech_prob": 5.1737397370743565e-06}, {"id": 67, "seek": 30992, "start": 335.52000000000004, "end": 339.88, "text": " about feature scaling as well as the learning rate alpha.", "tokens": [50364, 682, 264, 11500, 17312, 2715, 11, 291, 393, 611, 747, 257, 574, 412, 577, 4111, 21589, 307, 1096, 50622, 50622, 294, 3089, 293, 611, 536, 577, 819, 7994, 295, 264, 2539, 3314, 8961, 393, 1477, 281, 2139, 50906, 50906, 1101, 420, 5324, 3097, 295, 428, 2316, 13, 51076, 51076, 286, 1454, 291, 362, 1019, 2433, 365, 264, 2158, 295, 8961, 293, 2577, 264, 10070, 295, 819, 51309, 51309, 7994, 295, 8961, 13, 51403, 51403, 407, 1767, 747, 257, 574, 293, 1190, 264, 3089, 294, 264, 17312, 2715, 281, 6052, 257, 7731, 24002, 51644, 51644, 466, 4111, 21589, 382, 731, 382, 264, 2539, 3314, 8961, 13, 51862, 51862], "temperature": 0.0, "avg_logprob": -0.08481377417888117, "compression_ratio": 1.9135802469135803, "no_speech_prob": 5.1737397370743565e-06}, {"id": 68, "seek": 33988, "start": 339.88, "end": 344.04, "text": " Training learning rates is an important part of training many learning algorithms.", "tokens": [50364, 20620, 2539, 6846, 307, 364, 1021, 644, 295, 3097, 867, 2539, 14642, 13, 50572, 50572, 400, 286, 1454, 300, 341, 960, 2709, 291, 24002, 466, 819, 7994, 293, 577, 281, 1888, 257, 50774, 50774, 665, 2158, 337, 8961, 13, 50916, 50916, 823, 456, 366, 257, 1916, 544, 3487, 436, 393, 764, 281, 652, 3866, 8213, 24590, 709, 51142, 51142, 544, 4005, 13, 51216, 51216, 400, 300, 307, 10875, 2375, 4122, 11, 597, 486, 611, 2089, 291, 281, 3318, 19490, 11, 406, 445, 51482, 51482, 257, 2997, 1622, 281, 428, 1412, 13, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.14833857887669613, "compression_ratio": 1.619607843137255, "no_speech_prob": 1.5439123671967536e-05}, {"id": 69, "seek": 33988, "start": 344.04, "end": 348.08, "text": " And I hope that this video gives you intuition about different choices and how to pick a", "tokens": [50364, 20620, 2539, 6846, 307, 364, 1021, 644, 295, 3097, 867, 2539, 14642, 13, 50572, 50572, 400, 286, 1454, 300, 341, 960, 2709, 291, 24002, 466, 819, 7994, 293, 577, 281, 1888, 257, 50774, 50774, 665, 2158, 337, 8961, 13, 50916, 50916, 823, 456, 366, 257, 1916, 544, 3487, 436, 393, 764, 281, 652, 3866, 8213, 24590, 709, 51142, 51142, 544, 4005, 13, 51216, 51216, 400, 300, 307, 10875, 2375, 4122, 11, 597, 486, 611, 2089, 291, 281, 3318, 19490, 11, 406, 445, 51482, 51482, 257, 2997, 1622, 281, 428, 1412, 13, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.14833857887669613, "compression_ratio": 1.619607843137255, "no_speech_prob": 1.5439123671967536e-05}, {"id": 70, "seek": 33988, "start": 348.08, "end": 350.92, "text": " good value for alpha.", "tokens": [50364, 20620, 2539, 6846, 307, 364, 1021, 644, 295, 3097, 867, 2539, 14642, 13, 50572, 50572, 400, 286, 1454, 300, 341, 960, 2709, 291, 24002, 466, 819, 7994, 293, 577, 281, 1888, 257, 50774, 50774, 665, 2158, 337, 8961, 13, 50916, 50916, 823, 456, 366, 257, 1916, 544, 3487, 436, 393, 764, 281, 652, 3866, 8213, 24590, 709, 51142, 51142, 544, 4005, 13, 51216, 51216, 400, 300, 307, 10875, 2375, 4122, 11, 597, 486, 611, 2089, 291, 281, 3318, 19490, 11, 406, 445, 51482, 51482, 257, 2997, 1622, 281, 428, 1412, 13, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.14833857887669613, "compression_ratio": 1.619607843137255, "no_speech_prob": 1.5439123671967536e-05}, {"id": 71, "seek": 33988, "start": 350.92, "end": 355.44, "text": " Now there are a couple more ideas they can use to make multiple linear regression much", "tokens": [50364, 20620, 2539, 6846, 307, 364, 1021, 644, 295, 3097, 867, 2539, 14642, 13, 50572, 50572, 400, 286, 1454, 300, 341, 960, 2709, 291, 24002, 466, 819, 7994, 293, 577, 281, 1888, 257, 50774, 50774, 665, 2158, 337, 8961, 13, 50916, 50916, 823, 456, 366, 257, 1916, 544, 3487, 436, 393, 764, 281, 652, 3866, 8213, 24590, 709, 51142, 51142, 544, 4005, 13, 51216, 51216, 400, 300, 307, 10875, 2375, 4122, 11, 597, 486, 611, 2089, 291, 281, 3318, 19490, 11, 406, 445, 51482, 51482, 257, 2997, 1622, 281, 428, 1412, 13, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.14833857887669613, "compression_ratio": 1.619607843137255, "no_speech_prob": 1.5439123671967536e-05}, {"id": 72, "seek": 33988, "start": 355.44, "end": 356.92, "text": " more powerful.", "tokens": [50364, 20620, 2539, 6846, 307, 364, 1021, 644, 295, 3097, 867, 2539, 14642, 13, 50572, 50572, 400, 286, 1454, 300, 341, 960, 2709, 291, 24002, 466, 819, 7994, 293, 577, 281, 1888, 257, 50774, 50774, 665, 2158, 337, 8961, 13, 50916, 50916, 823, 456, 366, 257, 1916, 544, 3487, 436, 393, 764, 281, 652, 3866, 8213, 24590, 709, 51142, 51142, 544, 4005, 13, 51216, 51216, 400, 300, 307, 10875, 2375, 4122, 11, 597, 486, 611, 2089, 291, 281, 3318, 19490, 11, 406, 445, 51482, 51482, 257, 2997, 1622, 281, 428, 1412, 13, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.14833857887669613, "compression_ratio": 1.619607843137255, "no_speech_prob": 1.5439123671967536e-05}, {"id": 73, "seek": 33988, "start": 356.92, "end": 362.24, "text": " And that is choosing custom features, which will also allow you to fit curves, not just", "tokens": [50364, 20620, 2539, 6846, 307, 364, 1021, 644, 295, 3097, 867, 2539, 14642, 13, 50572, 50572, 400, 286, 1454, 300, 341, 960, 2709, 291, 24002, 466, 819, 7994, 293, 577, 281, 1888, 257, 50774, 50774, 665, 2158, 337, 8961, 13, 50916, 50916, 823, 456, 366, 257, 1916, 544, 3487, 436, 393, 764, 281, 652, 3866, 8213, 24590, 709, 51142, 51142, 544, 4005, 13, 51216, 51216, 400, 300, 307, 10875, 2375, 4122, 11, 597, 486, 611, 2089, 291, 281, 3318, 19490, 11, 406, 445, 51482, 51482, 257, 2997, 1622, 281, 428, 1412, 13, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.14833857887669613, "compression_ratio": 1.619607843137255, "no_speech_prob": 1.5439123671967536e-05}, {"id": 74, "seek": 33988, "start": 362.24, "end": 364.2, "text": " a straight line to your data.", "tokens": [50364, 20620, 2539, 6846, 307, 364, 1021, 644, 295, 3097, 867, 2539, 14642, 13, 50572, 50572, 400, 286, 1454, 300, 341, 960, 2709, 291, 24002, 466, 819, 7994, 293, 577, 281, 1888, 257, 50774, 50774, 665, 2158, 337, 8961, 13, 50916, 50916, 823, 456, 366, 257, 1916, 544, 3487, 436, 393, 764, 281, 652, 3866, 8213, 24590, 709, 51142, 51142, 544, 4005, 13, 51216, 51216, 400, 300, 307, 10875, 2375, 4122, 11, 597, 486, 611, 2089, 291, 281, 3318, 19490, 11, 406, 445, 51482, 51482, 257, 2997, 1622, 281, 428, 1412, 13, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.14833857887669613, "compression_ratio": 1.619607843137255, "no_speech_prob": 1.5439123671967536e-05}, {"id": 75, "seek": 36420, "start": 364.2, "end": 370.64, "text": " Let's take a look at that in the next video.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 300, 294, 264, 958, 960, 13, 50686], "temperature": 0.0, "avg_logprob": -0.36212644577026365, "compression_ratio": 0.8979591836734694, "no_speech_prob": 0.00029965402791276574}], "language": "en", "video_id": "j4uDxRNjYlA", "entity": "ML Specialization, Andrew Ng (2022)"}}