{"video_id": "m0QgVaFS6O4", "title": "6.7 Bias and variance | Learning curves --[Machine Learning | Andrew Ng]", "description": "Second Course:\nAdvanced Learning Algorithms.\n\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 733, "views": 85, "publish_date": "11/04/2022", "timestamp": 1661817600, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " Learning curves are a way to help understand how your learning algorithm is doing as a function of the amount of experience it has, whereby experience I mean, for example, the number of training examples it has. Let's take a look. Let me plot learning curves for a model that fits a second order polynomial quadratic function like so. And I'm going to plot both J C V, the cross-validation error, as well as J train, the training error. So on this figure, the horizontal axis is going to be M train. That is the training set size or the number of examples that the algorithm can learn from. And on the vertical axis, I'm going to plot the error. And by error, I mean either J C V or J train. So let's start by plotting the cross-validation error. It will look something like this. So that's what J C V of W B will look like. And it's maybe no surprise that as M train, the training set size gets bigger, then you learn a better model. And so the cross-validation error goes down. Now, let's plot J train of W B of what the training error looks like as the training set size gets bigger. It turns out that the training error will actually look like this. That as the training set size gets bigger, the training set error actually increases. Let's take a look at why this is the case. We'll start with an example of when you have just a single training example. Well, if you were to fit a quadratic model to this, you can fit, you know, easily a straight line or a curve and your training error will be zero. How about if you have two training examples like this? Well, you can again fit straight line and achieve zero training error. In fact, if you have three training examples, the quadratic function can still fit this very well and get pretty much zero training error. But now, if your training set gets a little bit bigger, say you have four training examples, then it gets a little bit harder to fit all four examples perfectly. And you may get a curve that looks like this, fits it pretty well, but you know, a little bit off in a few places here and there. And so when you have increased the training set size to four, the training error has actually gone up a little bit. How about when you have five training examples? Well, again, you can fit it pretty well, but it gets even a little bit harder to fit all of them perfectly. And when you have an even larger training set, it just gets harder and harder to fit every single one of your training examples perfectly. So to recap, when you have a very small number of training examples, like one or two or even three, it's relatively easy to get zero or very small training error. But when you have a larger training set, it's harder for quadratic function to fit all the training examples perfectly, which is why as the training set gets bigger, the training error increases because it's harder to fit all of the training examples perfectly. Notice one other thing about these curves, which is the cross validation error will be typically higher than the training error because you fit the parameters to the training set and so you expect to do at least a little bit better or when M is small, maybe even a lot better on the training set than on the cross validation set. Let's now take a look at what the learning curves will look like for an algorithm with high bias versus one with high variance. Let's start at the high bias or the under fitting case. Recall that an example of high bias would be if you're fitting a linear function to a curve that looks like this. If you were to plot the training error, then the training error will go up like so, as you'd expect. And in fact, this curve of training error may start to flatten out or we call it plateau, meaning flatten out after a while. And that's because as you get more and more training examples, when you're fitting the simple linear function, your model doesn't actually change that much more. Fitting a straight line and even as you get more and more and more examples, there's just not that much more to change, which is why the average training error flattens out after a while. And similarly, your cross validation error will come down and also flatten out after a while, which is why JCV again is higher than JTrain, but JCV will tend to look like that. And it's because beyond a certain point, even as you get more and more and more examples, not much is going to change about the straight line you're fitting. It's just too simple a model to be fitting to this much data, which is why both of these curves, JCV and JTrain, tend to flatten after a while. And if you had a measure of that baseline level of performance, such as human level performance, then it'll tend to be a value that is lower than your JTrain and your JCV. So human level performance may look like this. And there's a big gap between the baseline level performance and JTrain, which was our indicator for this algorithm having high bias. That is, one could hope to be doing much better if only we could fit a more complex function than just a straight line. Now, one interesting thing about this plot is you can ask, what do you think will happen if you could have a much bigger training set? So what would it look like if we could increase M even further than the right of this plot and go further to the right as follows? Well, you can imagine if you were to extend both of these curves to the right, they both sort of flatten out and both of them will probably just continue to be flat like that. And no matter how far you extend to the right of this plot, these two curves, they will never, you know, somehow find a way to dip down to this human level of performance or just keep on being kind of flat like this pretty much forever, no matter how large the training set gets. So that gives this conclusion, maybe a little bit surprising, that if a learning algorithm has high bias, getting more training data will not by itself help that much. And I know that we're used to thinking that having more data is good. But if your algorithm has high bias, then if the only thing you do is throw more training data at it, that by itself will not ever let you bring down the error rate that much. And it's because of this really, no matter how many more examples you add to this figure, the straight learning fitting just isn't going to get that much better. And that's why before investing a lot of effort into collecting more training data, it's worth checking if your learning algorithm has high bias, because if it does, then you probably need to do some other things other than just throw more training data at it. Let's now take a look at what the learning curve looks like for learning algorithm with high variance. You might remember that if you were to fit a four-fold polynomial with small lambda, say, or even lambda equals zero, then you get a curve that looks like this. And even though it fits the training data very well, it doesn't generalize. Let's now look at what a learning curve might look like in this high variance scenario. JTrain will be going up as the training set size increases, so you get a curve that looks like this, and JCV will be much higher. So your cross-validation error is much higher than your training error. And the fact there's a huge gap here is what can tell you that this high variance is doing much better on the training set than it's doing on your cross-validation set. If you were to plot a baseline level of performance, such as human level performance, you may find that it turns out to be here that JTrain can sometimes be even lower than the human level of performance, or maybe human level performance is a little bit lower than this. But when you're overfitting the training set, you may be able to fit the training set so well to have an unrealistically low error, such as zero error in this example over here, which is actually better than how well humans would actually be able to predict housing prices or whatever the application you're working on. But again, the signal for high variance is whether JCV is much higher than JTrain. And when you have high variance, then increasing the training set size could help a lot. And in particular, if we could extrapolate these curves to the right, increase MTrain, then the training error will continue to go up, but then the cross-validation error hopefully will come down and approach JTrain. And so in this scenario, it might be possible just by increasing the training set size to lower the cross-validation error and to get your algorithm to perform better and better. And this is unlike the high bias case where if the only thing you do is get more training data, that won't actually help your learning algorithm performance much. So to summarize, if a learning algorithm suffers from high variance, then getting more training data is indeed likely to help. Because extrapolating to the right of this curve, you see that you can expect JCV to keep on coming down. And in this example, just by getting more training data allows the algorithm to go from this relatively high cross-validation error to get much closer to human level performance. You can see that if you were to add a lot more training examples and continue to fit a four-folder polynomial, then you can just get a better four-folder polynomial fit to this data than just very weakly curve up on top. So if you're building a machine learning application, you could plot the learning curve if you want. That is, you can take different subsets of your training sets. And even if you have, say, a thousand training examples, you could train a model on just a hundred training examples and look at the training error and the cross-validation error, then train a model on 200 examples, holding out 800 examples and just not using them for now, and plot J train and JCV and so on and repeat and plot out what the learning curve looks like. And if you were to visualize it that way, then that could be another way for you to see if your learning curve looks more like a high bias or high variance one. One downside of plotting learning curves like this is something I've done, but one downside is it is computationally quite expensive to train so many different models using different size subsets of your training set. So in practice, it isn't done that often. But nonetheless, I find that having this mental visual picture in my head of what the training set looks like, sometimes that helps me to think through what I think my learning algorithm is doing and whether it has high bias or high variance. So I know we've gone through a lot about bias and variance. Let's go back to our earlier example of if you have trained a model for housing price prediction, how does bias and variance help you decide what to do next? Let's go back to that earlier example, which I hope will now make a lot more sense to you. Let's do that in the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.4, "text": " Learning curves are a way to help understand how your learning algorithm is doing as a function of the amount of experience it has,", "tokens": [50364, 15205, 19490, 366, 257, 636, 281, 854, 1223, 577, 428, 2539, 9284, 307, 884, 382, 257, 2445, 295, 264, 2372, 295, 1752, 309, 575, 11, 50834, 50834, 36998, 1752, 286, 914, 11, 337, 1365, 11, 264, 1230, 295, 3097, 5110, 309, 575, 13, 51064, 51064, 961, 311, 747, 257, 574, 13, 961, 385, 7542, 2539, 19490, 337, 257, 2316, 300, 9001, 257, 1150, 1668, 26110, 37262, 2445, 411, 370, 13, 51466, 51466, 400, 286, 478, 516, 281, 7542, 1293, 508, 383, 691, 11, 264, 3278, 12, 3337, 327, 399, 6713, 11, 382, 731, 382, 508, 3847, 11, 264, 3097, 6713, 13, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.22264306204659598, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.0028000024612993}, {"id": 1, "seek": 0, "start": 9.4, "end": 14.0, "text": " whereby experience I mean, for example, the number of training examples it has.", "tokens": [50364, 15205, 19490, 366, 257, 636, 281, 854, 1223, 577, 428, 2539, 9284, 307, 884, 382, 257, 2445, 295, 264, 2372, 295, 1752, 309, 575, 11, 50834, 50834, 36998, 1752, 286, 914, 11, 337, 1365, 11, 264, 1230, 295, 3097, 5110, 309, 575, 13, 51064, 51064, 961, 311, 747, 257, 574, 13, 961, 385, 7542, 2539, 19490, 337, 257, 2316, 300, 9001, 257, 1150, 1668, 26110, 37262, 2445, 411, 370, 13, 51466, 51466, 400, 286, 478, 516, 281, 7542, 1293, 508, 383, 691, 11, 264, 3278, 12, 3337, 327, 399, 6713, 11, 382, 731, 382, 508, 3847, 11, 264, 3097, 6713, 13, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.22264306204659598, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.0028000024612993}, {"id": 2, "seek": 0, "start": 14.0, "end": 22.04, "text": " Let's take a look. Let me plot learning curves for a model that fits a second order polynomial quadratic function like so.", "tokens": [50364, 15205, 19490, 366, 257, 636, 281, 854, 1223, 577, 428, 2539, 9284, 307, 884, 382, 257, 2445, 295, 264, 2372, 295, 1752, 309, 575, 11, 50834, 50834, 36998, 1752, 286, 914, 11, 337, 1365, 11, 264, 1230, 295, 3097, 5110, 309, 575, 13, 51064, 51064, 961, 311, 747, 257, 574, 13, 961, 385, 7542, 2539, 19490, 337, 257, 2316, 300, 9001, 257, 1150, 1668, 26110, 37262, 2445, 411, 370, 13, 51466, 51466, 400, 286, 478, 516, 281, 7542, 1293, 508, 383, 691, 11, 264, 3278, 12, 3337, 327, 399, 6713, 11, 382, 731, 382, 508, 3847, 11, 264, 3097, 6713, 13, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.22264306204659598, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.0028000024612993}, {"id": 3, "seek": 0, "start": 22.04, "end": 29.400000000000002, "text": " And I'm going to plot both J C V, the cross-validation error, as well as J train, the training error.", "tokens": [50364, 15205, 19490, 366, 257, 636, 281, 854, 1223, 577, 428, 2539, 9284, 307, 884, 382, 257, 2445, 295, 264, 2372, 295, 1752, 309, 575, 11, 50834, 50834, 36998, 1752, 286, 914, 11, 337, 1365, 11, 264, 1230, 295, 3097, 5110, 309, 575, 13, 51064, 51064, 961, 311, 747, 257, 574, 13, 961, 385, 7542, 2539, 19490, 337, 257, 2316, 300, 9001, 257, 1150, 1668, 26110, 37262, 2445, 411, 370, 13, 51466, 51466, 400, 286, 478, 516, 281, 7542, 1293, 508, 383, 691, 11, 264, 3278, 12, 3337, 327, 399, 6713, 11, 382, 731, 382, 508, 3847, 11, 264, 3097, 6713, 13, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.22264306204659598, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.0028000024612993}, {"id": 4, "seek": 2940, "start": 29.4, "end": 36.64, "text": " So on this figure, the horizontal axis is going to be M train.", "tokens": [50364, 407, 322, 341, 2573, 11, 264, 12750, 10298, 307, 516, 281, 312, 376, 3847, 13, 50726, 50726, 663, 307, 264, 3097, 992, 2744, 420, 264, 1230, 295, 5110, 300, 264, 9284, 393, 1466, 490, 13, 50962, 50962, 400, 322, 264, 9429, 10298, 11, 286, 478, 516, 281, 7542, 264, 6713, 13, 51114, 51114, 400, 538, 6713, 11, 286, 914, 2139, 508, 383, 691, 420, 508, 3847, 13, 51286, 51286, 407, 718, 311, 722, 538, 41178, 264, 3278, 12, 3337, 327, 399, 6713, 13, 51446, 51446, 467, 486, 574, 746, 411, 341, 13, 51572, 51572, 407, 300, 311, 437, 508, 383, 691, 295, 343, 363, 486, 574, 411, 13, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.10685852595738002, "compression_ratio": 1.6538461538461537, "no_speech_prob": 7.296158855751855e-06}, {"id": 5, "seek": 2940, "start": 36.64, "end": 41.36, "text": " That is the training set size or the number of examples that the algorithm can learn from.", "tokens": [50364, 407, 322, 341, 2573, 11, 264, 12750, 10298, 307, 516, 281, 312, 376, 3847, 13, 50726, 50726, 663, 307, 264, 3097, 992, 2744, 420, 264, 1230, 295, 5110, 300, 264, 9284, 393, 1466, 490, 13, 50962, 50962, 400, 322, 264, 9429, 10298, 11, 286, 478, 516, 281, 7542, 264, 6713, 13, 51114, 51114, 400, 538, 6713, 11, 286, 914, 2139, 508, 383, 691, 420, 508, 3847, 13, 51286, 51286, 407, 718, 311, 722, 538, 41178, 264, 3278, 12, 3337, 327, 399, 6713, 13, 51446, 51446, 467, 486, 574, 746, 411, 341, 13, 51572, 51572, 407, 300, 311, 437, 508, 383, 691, 295, 343, 363, 486, 574, 411, 13, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.10685852595738002, "compression_ratio": 1.6538461538461537, "no_speech_prob": 7.296158855751855e-06}, {"id": 6, "seek": 2940, "start": 41.36, "end": 44.4, "text": " And on the vertical axis, I'm going to plot the error.", "tokens": [50364, 407, 322, 341, 2573, 11, 264, 12750, 10298, 307, 516, 281, 312, 376, 3847, 13, 50726, 50726, 663, 307, 264, 3097, 992, 2744, 420, 264, 1230, 295, 5110, 300, 264, 9284, 393, 1466, 490, 13, 50962, 50962, 400, 322, 264, 9429, 10298, 11, 286, 478, 516, 281, 7542, 264, 6713, 13, 51114, 51114, 400, 538, 6713, 11, 286, 914, 2139, 508, 383, 691, 420, 508, 3847, 13, 51286, 51286, 407, 718, 311, 722, 538, 41178, 264, 3278, 12, 3337, 327, 399, 6713, 13, 51446, 51446, 467, 486, 574, 746, 411, 341, 13, 51572, 51572, 407, 300, 311, 437, 508, 383, 691, 295, 343, 363, 486, 574, 411, 13, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.10685852595738002, "compression_ratio": 1.6538461538461537, "no_speech_prob": 7.296158855751855e-06}, {"id": 7, "seek": 2940, "start": 44.4, "end": 47.84, "text": " And by error, I mean either J C V or J train.", "tokens": [50364, 407, 322, 341, 2573, 11, 264, 12750, 10298, 307, 516, 281, 312, 376, 3847, 13, 50726, 50726, 663, 307, 264, 3097, 992, 2744, 420, 264, 1230, 295, 5110, 300, 264, 9284, 393, 1466, 490, 13, 50962, 50962, 400, 322, 264, 9429, 10298, 11, 286, 478, 516, 281, 7542, 264, 6713, 13, 51114, 51114, 400, 538, 6713, 11, 286, 914, 2139, 508, 383, 691, 420, 508, 3847, 13, 51286, 51286, 407, 718, 311, 722, 538, 41178, 264, 3278, 12, 3337, 327, 399, 6713, 13, 51446, 51446, 467, 486, 574, 746, 411, 341, 13, 51572, 51572, 407, 300, 311, 437, 508, 383, 691, 295, 343, 363, 486, 574, 411, 13, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.10685852595738002, "compression_ratio": 1.6538461538461537, "no_speech_prob": 7.296158855751855e-06}, {"id": 8, "seek": 2940, "start": 47.84, "end": 51.04, "text": " So let's start by plotting the cross-validation error.", "tokens": [50364, 407, 322, 341, 2573, 11, 264, 12750, 10298, 307, 516, 281, 312, 376, 3847, 13, 50726, 50726, 663, 307, 264, 3097, 992, 2744, 420, 264, 1230, 295, 5110, 300, 264, 9284, 393, 1466, 490, 13, 50962, 50962, 400, 322, 264, 9429, 10298, 11, 286, 478, 516, 281, 7542, 264, 6713, 13, 51114, 51114, 400, 538, 6713, 11, 286, 914, 2139, 508, 383, 691, 420, 508, 3847, 13, 51286, 51286, 407, 718, 311, 722, 538, 41178, 264, 3278, 12, 3337, 327, 399, 6713, 13, 51446, 51446, 467, 486, 574, 746, 411, 341, 13, 51572, 51572, 407, 300, 311, 437, 508, 383, 691, 295, 343, 363, 486, 574, 411, 13, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.10685852595738002, "compression_ratio": 1.6538461538461537, "no_speech_prob": 7.296158855751855e-06}, {"id": 9, "seek": 2940, "start": 51.04, "end": 53.56, "text": " It will look something like this.", "tokens": [50364, 407, 322, 341, 2573, 11, 264, 12750, 10298, 307, 516, 281, 312, 376, 3847, 13, 50726, 50726, 663, 307, 264, 3097, 992, 2744, 420, 264, 1230, 295, 5110, 300, 264, 9284, 393, 1466, 490, 13, 50962, 50962, 400, 322, 264, 9429, 10298, 11, 286, 478, 516, 281, 7542, 264, 6713, 13, 51114, 51114, 400, 538, 6713, 11, 286, 914, 2139, 508, 383, 691, 420, 508, 3847, 13, 51286, 51286, 407, 718, 311, 722, 538, 41178, 264, 3278, 12, 3337, 327, 399, 6713, 13, 51446, 51446, 467, 486, 574, 746, 411, 341, 13, 51572, 51572, 407, 300, 311, 437, 508, 383, 691, 295, 343, 363, 486, 574, 411, 13, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.10685852595738002, "compression_ratio": 1.6538461538461537, "no_speech_prob": 7.296158855751855e-06}, {"id": 10, "seek": 2940, "start": 53.56, "end": 58.72, "text": " So that's what J C V of W B will look like.", "tokens": [50364, 407, 322, 341, 2573, 11, 264, 12750, 10298, 307, 516, 281, 312, 376, 3847, 13, 50726, 50726, 663, 307, 264, 3097, 992, 2744, 420, 264, 1230, 295, 5110, 300, 264, 9284, 393, 1466, 490, 13, 50962, 50962, 400, 322, 264, 9429, 10298, 11, 286, 478, 516, 281, 7542, 264, 6713, 13, 51114, 51114, 400, 538, 6713, 11, 286, 914, 2139, 508, 383, 691, 420, 508, 3847, 13, 51286, 51286, 407, 718, 311, 722, 538, 41178, 264, 3278, 12, 3337, 327, 399, 6713, 13, 51446, 51446, 467, 486, 574, 746, 411, 341, 13, 51572, 51572, 407, 300, 311, 437, 508, 383, 691, 295, 343, 363, 486, 574, 411, 13, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.10685852595738002, "compression_ratio": 1.6538461538461537, "no_speech_prob": 7.296158855751855e-06}, {"id": 11, "seek": 5872, "start": 58.72, "end": 63.44, "text": " And it's maybe no surprise that as M train, the training set size gets bigger,", "tokens": [50364, 400, 309, 311, 1310, 572, 6365, 300, 382, 376, 3847, 11, 264, 3097, 992, 2744, 2170, 3801, 11, 50600, 50600, 550, 291, 1466, 257, 1101, 2316, 13, 50698, 50698, 400, 370, 264, 3278, 12, 3337, 327, 399, 6713, 1709, 760, 13, 50860, 50860, 823, 11, 718, 311, 7542, 508, 3847, 295, 343, 363, 295, 437, 264, 3097, 6713, 1542, 411, 382, 264, 3097, 992, 2744, 2170, 3801, 13, 51296, 51296, 467, 4523, 484, 300, 264, 3097, 6713, 486, 767, 574, 411, 341, 13, 51538, 51538, 663, 382, 264, 3097, 992, 2744, 2170, 3801, 11, 264, 3097, 992, 6713, 767, 8637, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10862342289515904, "compression_ratio": 1.962085308056872, "no_speech_prob": 1.0615526662149932e-05}, {"id": 12, "seek": 5872, "start": 63.44, "end": 65.4, "text": " then you learn a better model.", "tokens": [50364, 400, 309, 311, 1310, 572, 6365, 300, 382, 376, 3847, 11, 264, 3097, 992, 2744, 2170, 3801, 11, 50600, 50600, 550, 291, 1466, 257, 1101, 2316, 13, 50698, 50698, 400, 370, 264, 3278, 12, 3337, 327, 399, 6713, 1709, 760, 13, 50860, 50860, 823, 11, 718, 311, 7542, 508, 3847, 295, 343, 363, 295, 437, 264, 3097, 6713, 1542, 411, 382, 264, 3097, 992, 2744, 2170, 3801, 13, 51296, 51296, 467, 4523, 484, 300, 264, 3097, 6713, 486, 767, 574, 411, 341, 13, 51538, 51538, 663, 382, 264, 3097, 992, 2744, 2170, 3801, 11, 264, 3097, 992, 6713, 767, 8637, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10862342289515904, "compression_ratio": 1.962085308056872, "no_speech_prob": 1.0615526662149932e-05}, {"id": 13, "seek": 5872, "start": 65.4, "end": 68.64, "text": " And so the cross-validation error goes down.", "tokens": [50364, 400, 309, 311, 1310, 572, 6365, 300, 382, 376, 3847, 11, 264, 3097, 992, 2744, 2170, 3801, 11, 50600, 50600, 550, 291, 1466, 257, 1101, 2316, 13, 50698, 50698, 400, 370, 264, 3278, 12, 3337, 327, 399, 6713, 1709, 760, 13, 50860, 50860, 823, 11, 718, 311, 7542, 508, 3847, 295, 343, 363, 295, 437, 264, 3097, 6713, 1542, 411, 382, 264, 3097, 992, 2744, 2170, 3801, 13, 51296, 51296, 467, 4523, 484, 300, 264, 3097, 6713, 486, 767, 574, 411, 341, 13, 51538, 51538, 663, 382, 264, 3097, 992, 2744, 2170, 3801, 11, 264, 3097, 992, 6713, 767, 8637, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10862342289515904, "compression_ratio": 1.962085308056872, "no_speech_prob": 1.0615526662149932e-05}, {"id": 14, "seek": 5872, "start": 68.64, "end": 77.36, "text": " Now, let's plot J train of W B of what the training error looks like as the training set size gets bigger.", "tokens": [50364, 400, 309, 311, 1310, 572, 6365, 300, 382, 376, 3847, 11, 264, 3097, 992, 2744, 2170, 3801, 11, 50600, 50600, 550, 291, 1466, 257, 1101, 2316, 13, 50698, 50698, 400, 370, 264, 3278, 12, 3337, 327, 399, 6713, 1709, 760, 13, 50860, 50860, 823, 11, 718, 311, 7542, 508, 3847, 295, 343, 363, 295, 437, 264, 3097, 6713, 1542, 411, 382, 264, 3097, 992, 2744, 2170, 3801, 13, 51296, 51296, 467, 4523, 484, 300, 264, 3097, 6713, 486, 767, 574, 411, 341, 13, 51538, 51538, 663, 382, 264, 3097, 992, 2744, 2170, 3801, 11, 264, 3097, 992, 6713, 767, 8637, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10862342289515904, "compression_ratio": 1.962085308056872, "no_speech_prob": 1.0615526662149932e-05}, {"id": 15, "seek": 5872, "start": 77.36, "end": 82.2, "text": " It turns out that the training error will actually look like this.", "tokens": [50364, 400, 309, 311, 1310, 572, 6365, 300, 382, 376, 3847, 11, 264, 3097, 992, 2744, 2170, 3801, 11, 50600, 50600, 550, 291, 1466, 257, 1101, 2316, 13, 50698, 50698, 400, 370, 264, 3278, 12, 3337, 327, 399, 6713, 1709, 760, 13, 50860, 50860, 823, 11, 718, 311, 7542, 508, 3847, 295, 343, 363, 295, 437, 264, 3097, 6713, 1542, 411, 382, 264, 3097, 992, 2744, 2170, 3801, 13, 51296, 51296, 467, 4523, 484, 300, 264, 3097, 6713, 486, 767, 574, 411, 341, 13, 51538, 51538, 663, 382, 264, 3097, 992, 2744, 2170, 3801, 11, 264, 3097, 992, 6713, 767, 8637, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10862342289515904, "compression_ratio": 1.962085308056872, "no_speech_prob": 1.0615526662149932e-05}, {"id": 16, "seek": 5872, "start": 82.2, "end": 88.44, "text": " That as the training set size gets bigger, the training set error actually increases.", "tokens": [50364, 400, 309, 311, 1310, 572, 6365, 300, 382, 376, 3847, 11, 264, 3097, 992, 2744, 2170, 3801, 11, 50600, 50600, 550, 291, 1466, 257, 1101, 2316, 13, 50698, 50698, 400, 370, 264, 3278, 12, 3337, 327, 399, 6713, 1709, 760, 13, 50860, 50860, 823, 11, 718, 311, 7542, 508, 3847, 295, 343, 363, 295, 437, 264, 3097, 6713, 1542, 411, 382, 264, 3097, 992, 2744, 2170, 3801, 13, 51296, 51296, 467, 4523, 484, 300, 264, 3097, 6713, 486, 767, 574, 411, 341, 13, 51538, 51538, 663, 382, 264, 3097, 992, 2744, 2170, 3801, 11, 264, 3097, 992, 6713, 767, 8637, 13, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10862342289515904, "compression_ratio": 1.962085308056872, "no_speech_prob": 1.0615526662149932e-05}, {"id": 17, "seek": 8844, "start": 88.44, "end": 90.39999999999999, "text": " Let's take a look at why this is the case.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 983, 341, 307, 264, 1389, 13, 50462, 50462, 492, 603, 722, 365, 364, 1365, 295, 562, 291, 362, 445, 257, 2167, 3097, 1365, 13, 50712, 50712, 1042, 11, 498, 291, 645, 281, 3318, 257, 37262, 2316, 281, 341, 11, 291, 393, 3318, 11, 291, 458, 11, 50910, 50910, 3612, 257, 2997, 1622, 420, 257, 7605, 293, 428, 3097, 6713, 486, 312, 4018, 13, 51130, 51130, 1012, 466, 498, 291, 362, 732, 3097, 5110, 411, 341, 30, 51276, 51276, 1042, 11, 291, 393, 797, 3318, 2997, 1622, 293, 4584, 4018, 3097, 6713, 13, 51526, 51526, 682, 1186, 11, 498, 291, 362, 1045, 3097, 5110, 11, 264, 37262, 2445, 393, 920, 3318, 341, 588, 731, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.09588012850381494, "compression_ratio": 1.856060606060606, "no_speech_prob": 2.078416400763672e-05}, {"id": 18, "seek": 8844, "start": 90.39999999999999, "end": 95.39999999999999, "text": " We'll start with an example of when you have just a single training example.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 983, 341, 307, 264, 1389, 13, 50462, 50462, 492, 603, 722, 365, 364, 1365, 295, 562, 291, 362, 445, 257, 2167, 3097, 1365, 13, 50712, 50712, 1042, 11, 498, 291, 645, 281, 3318, 257, 37262, 2316, 281, 341, 11, 291, 393, 3318, 11, 291, 458, 11, 50910, 50910, 3612, 257, 2997, 1622, 420, 257, 7605, 293, 428, 3097, 6713, 486, 312, 4018, 13, 51130, 51130, 1012, 466, 498, 291, 362, 732, 3097, 5110, 411, 341, 30, 51276, 51276, 1042, 11, 291, 393, 797, 3318, 2997, 1622, 293, 4584, 4018, 3097, 6713, 13, 51526, 51526, 682, 1186, 11, 498, 291, 362, 1045, 3097, 5110, 11, 264, 37262, 2445, 393, 920, 3318, 341, 588, 731, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.09588012850381494, "compression_ratio": 1.856060606060606, "no_speech_prob": 2.078416400763672e-05}, {"id": 19, "seek": 8844, "start": 95.39999999999999, "end": 99.36, "text": " Well, if you were to fit a quadratic model to this, you can fit, you know,", "tokens": [50364, 961, 311, 747, 257, 574, 412, 983, 341, 307, 264, 1389, 13, 50462, 50462, 492, 603, 722, 365, 364, 1365, 295, 562, 291, 362, 445, 257, 2167, 3097, 1365, 13, 50712, 50712, 1042, 11, 498, 291, 645, 281, 3318, 257, 37262, 2316, 281, 341, 11, 291, 393, 3318, 11, 291, 458, 11, 50910, 50910, 3612, 257, 2997, 1622, 420, 257, 7605, 293, 428, 3097, 6713, 486, 312, 4018, 13, 51130, 51130, 1012, 466, 498, 291, 362, 732, 3097, 5110, 411, 341, 30, 51276, 51276, 1042, 11, 291, 393, 797, 3318, 2997, 1622, 293, 4584, 4018, 3097, 6713, 13, 51526, 51526, 682, 1186, 11, 498, 291, 362, 1045, 3097, 5110, 11, 264, 37262, 2445, 393, 920, 3318, 341, 588, 731, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.09588012850381494, "compression_ratio": 1.856060606060606, "no_speech_prob": 2.078416400763672e-05}, {"id": 20, "seek": 8844, "start": 99.36, "end": 103.75999999999999, "text": " easily a straight line or a curve and your training error will be zero.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 983, 341, 307, 264, 1389, 13, 50462, 50462, 492, 603, 722, 365, 364, 1365, 295, 562, 291, 362, 445, 257, 2167, 3097, 1365, 13, 50712, 50712, 1042, 11, 498, 291, 645, 281, 3318, 257, 37262, 2316, 281, 341, 11, 291, 393, 3318, 11, 291, 458, 11, 50910, 50910, 3612, 257, 2997, 1622, 420, 257, 7605, 293, 428, 3097, 6713, 486, 312, 4018, 13, 51130, 51130, 1012, 466, 498, 291, 362, 732, 3097, 5110, 411, 341, 30, 51276, 51276, 1042, 11, 291, 393, 797, 3318, 2997, 1622, 293, 4584, 4018, 3097, 6713, 13, 51526, 51526, 682, 1186, 11, 498, 291, 362, 1045, 3097, 5110, 11, 264, 37262, 2445, 393, 920, 3318, 341, 588, 731, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.09588012850381494, "compression_ratio": 1.856060606060606, "no_speech_prob": 2.078416400763672e-05}, {"id": 21, "seek": 8844, "start": 103.75999999999999, "end": 106.68, "text": " How about if you have two training examples like this?", "tokens": [50364, 961, 311, 747, 257, 574, 412, 983, 341, 307, 264, 1389, 13, 50462, 50462, 492, 603, 722, 365, 364, 1365, 295, 562, 291, 362, 445, 257, 2167, 3097, 1365, 13, 50712, 50712, 1042, 11, 498, 291, 645, 281, 3318, 257, 37262, 2316, 281, 341, 11, 291, 393, 3318, 11, 291, 458, 11, 50910, 50910, 3612, 257, 2997, 1622, 420, 257, 7605, 293, 428, 3097, 6713, 486, 312, 4018, 13, 51130, 51130, 1012, 466, 498, 291, 362, 732, 3097, 5110, 411, 341, 30, 51276, 51276, 1042, 11, 291, 393, 797, 3318, 2997, 1622, 293, 4584, 4018, 3097, 6713, 13, 51526, 51526, 682, 1186, 11, 498, 291, 362, 1045, 3097, 5110, 11, 264, 37262, 2445, 393, 920, 3318, 341, 588, 731, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.09588012850381494, "compression_ratio": 1.856060606060606, "no_speech_prob": 2.078416400763672e-05}, {"id": 22, "seek": 8844, "start": 106.68, "end": 111.68, "text": " Well, you can again fit straight line and achieve zero training error.", "tokens": [50364, 961, 311, 747, 257, 574, 412, 983, 341, 307, 264, 1389, 13, 50462, 50462, 492, 603, 722, 365, 364, 1365, 295, 562, 291, 362, 445, 257, 2167, 3097, 1365, 13, 50712, 50712, 1042, 11, 498, 291, 645, 281, 3318, 257, 37262, 2316, 281, 341, 11, 291, 393, 3318, 11, 291, 458, 11, 50910, 50910, 3612, 257, 2997, 1622, 420, 257, 7605, 293, 428, 3097, 6713, 486, 312, 4018, 13, 51130, 51130, 1012, 466, 498, 291, 362, 732, 3097, 5110, 411, 341, 30, 51276, 51276, 1042, 11, 291, 393, 797, 3318, 2997, 1622, 293, 4584, 4018, 3097, 6713, 13, 51526, 51526, 682, 1186, 11, 498, 291, 362, 1045, 3097, 5110, 11, 264, 37262, 2445, 393, 920, 3318, 341, 588, 731, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.09588012850381494, "compression_ratio": 1.856060606060606, "no_speech_prob": 2.078416400763672e-05}, {"id": 23, "seek": 8844, "start": 111.68, "end": 116.68, "text": " In fact, if you have three training examples, the quadratic function can still fit this very well", "tokens": [50364, 961, 311, 747, 257, 574, 412, 983, 341, 307, 264, 1389, 13, 50462, 50462, 492, 603, 722, 365, 364, 1365, 295, 562, 291, 362, 445, 257, 2167, 3097, 1365, 13, 50712, 50712, 1042, 11, 498, 291, 645, 281, 3318, 257, 37262, 2316, 281, 341, 11, 291, 393, 3318, 11, 291, 458, 11, 50910, 50910, 3612, 257, 2997, 1622, 420, 257, 7605, 293, 428, 3097, 6713, 486, 312, 4018, 13, 51130, 51130, 1012, 466, 498, 291, 362, 732, 3097, 5110, 411, 341, 30, 51276, 51276, 1042, 11, 291, 393, 797, 3318, 2997, 1622, 293, 4584, 4018, 3097, 6713, 13, 51526, 51526, 682, 1186, 11, 498, 291, 362, 1045, 3097, 5110, 11, 264, 37262, 2445, 393, 920, 3318, 341, 588, 731, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.09588012850381494, "compression_ratio": 1.856060606060606, "no_speech_prob": 2.078416400763672e-05}, {"id": 24, "seek": 11668, "start": 116.68, "end": 119.80000000000001, "text": " and get pretty much zero training error.", "tokens": [50364, 293, 483, 1238, 709, 4018, 3097, 6713, 13, 50520, 50520, 583, 586, 11, 498, 428, 3097, 992, 2170, 257, 707, 857, 3801, 11, 584, 291, 362, 1451, 3097, 5110, 11, 50782, 50782, 550, 309, 2170, 257, 707, 857, 6081, 281, 3318, 439, 1451, 5110, 6239, 13, 51010, 51010, 400, 291, 815, 483, 257, 7605, 300, 1542, 411, 341, 11, 9001, 309, 1238, 731, 11, 51150, 51150, 457, 291, 458, 11, 257, 707, 857, 766, 294, 257, 1326, 3190, 510, 293, 456, 13, 51290, 51290, 400, 370, 562, 291, 362, 6505, 264, 3097, 992, 2744, 281, 1451, 11, 51542, 51542, 264, 3097, 6713, 575, 767, 2780, 493, 257, 707, 857, 13, 51732, 51732, 1012, 466, 562, 291, 362, 1732, 3097, 5110, 30, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10748176574707032, "compression_ratio": 1.9450980392156862, "no_speech_prob": 5.2552081797330175e-06}, {"id": 25, "seek": 11668, "start": 119.80000000000001, "end": 125.04, "text": " But now, if your training set gets a little bit bigger, say you have four training examples,", "tokens": [50364, 293, 483, 1238, 709, 4018, 3097, 6713, 13, 50520, 50520, 583, 586, 11, 498, 428, 3097, 992, 2170, 257, 707, 857, 3801, 11, 584, 291, 362, 1451, 3097, 5110, 11, 50782, 50782, 550, 309, 2170, 257, 707, 857, 6081, 281, 3318, 439, 1451, 5110, 6239, 13, 51010, 51010, 400, 291, 815, 483, 257, 7605, 300, 1542, 411, 341, 11, 9001, 309, 1238, 731, 11, 51150, 51150, 457, 291, 458, 11, 257, 707, 857, 766, 294, 257, 1326, 3190, 510, 293, 456, 13, 51290, 51290, 400, 370, 562, 291, 362, 6505, 264, 3097, 992, 2744, 281, 1451, 11, 51542, 51542, 264, 3097, 6713, 575, 767, 2780, 493, 257, 707, 857, 13, 51732, 51732, 1012, 466, 562, 291, 362, 1732, 3097, 5110, 30, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10748176574707032, "compression_ratio": 1.9450980392156862, "no_speech_prob": 5.2552081797330175e-06}, {"id": 26, "seek": 11668, "start": 125.04, "end": 129.6, "text": " then it gets a little bit harder to fit all four examples perfectly.", "tokens": [50364, 293, 483, 1238, 709, 4018, 3097, 6713, 13, 50520, 50520, 583, 586, 11, 498, 428, 3097, 992, 2170, 257, 707, 857, 3801, 11, 584, 291, 362, 1451, 3097, 5110, 11, 50782, 50782, 550, 309, 2170, 257, 707, 857, 6081, 281, 3318, 439, 1451, 5110, 6239, 13, 51010, 51010, 400, 291, 815, 483, 257, 7605, 300, 1542, 411, 341, 11, 9001, 309, 1238, 731, 11, 51150, 51150, 457, 291, 458, 11, 257, 707, 857, 766, 294, 257, 1326, 3190, 510, 293, 456, 13, 51290, 51290, 400, 370, 562, 291, 362, 6505, 264, 3097, 992, 2744, 281, 1451, 11, 51542, 51542, 264, 3097, 6713, 575, 767, 2780, 493, 257, 707, 857, 13, 51732, 51732, 1012, 466, 562, 291, 362, 1732, 3097, 5110, 30, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10748176574707032, "compression_ratio": 1.9450980392156862, "no_speech_prob": 5.2552081797330175e-06}, {"id": 27, "seek": 11668, "start": 129.6, "end": 132.4, "text": " And you may get a curve that looks like this, fits it pretty well,", "tokens": [50364, 293, 483, 1238, 709, 4018, 3097, 6713, 13, 50520, 50520, 583, 586, 11, 498, 428, 3097, 992, 2170, 257, 707, 857, 3801, 11, 584, 291, 362, 1451, 3097, 5110, 11, 50782, 50782, 550, 309, 2170, 257, 707, 857, 6081, 281, 3318, 439, 1451, 5110, 6239, 13, 51010, 51010, 400, 291, 815, 483, 257, 7605, 300, 1542, 411, 341, 11, 9001, 309, 1238, 731, 11, 51150, 51150, 457, 291, 458, 11, 257, 707, 857, 766, 294, 257, 1326, 3190, 510, 293, 456, 13, 51290, 51290, 400, 370, 562, 291, 362, 6505, 264, 3097, 992, 2744, 281, 1451, 11, 51542, 51542, 264, 3097, 6713, 575, 767, 2780, 493, 257, 707, 857, 13, 51732, 51732, 1012, 466, 562, 291, 362, 1732, 3097, 5110, 30, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10748176574707032, "compression_ratio": 1.9450980392156862, "no_speech_prob": 5.2552081797330175e-06}, {"id": 28, "seek": 11668, "start": 132.4, "end": 135.20000000000002, "text": " but you know, a little bit off in a few places here and there.", "tokens": [50364, 293, 483, 1238, 709, 4018, 3097, 6713, 13, 50520, 50520, 583, 586, 11, 498, 428, 3097, 992, 2170, 257, 707, 857, 3801, 11, 584, 291, 362, 1451, 3097, 5110, 11, 50782, 50782, 550, 309, 2170, 257, 707, 857, 6081, 281, 3318, 439, 1451, 5110, 6239, 13, 51010, 51010, 400, 291, 815, 483, 257, 7605, 300, 1542, 411, 341, 11, 9001, 309, 1238, 731, 11, 51150, 51150, 457, 291, 458, 11, 257, 707, 857, 766, 294, 257, 1326, 3190, 510, 293, 456, 13, 51290, 51290, 400, 370, 562, 291, 362, 6505, 264, 3097, 992, 2744, 281, 1451, 11, 51542, 51542, 264, 3097, 6713, 575, 767, 2780, 493, 257, 707, 857, 13, 51732, 51732, 1012, 466, 562, 291, 362, 1732, 3097, 5110, 30, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10748176574707032, "compression_ratio": 1.9450980392156862, "no_speech_prob": 5.2552081797330175e-06}, {"id": 29, "seek": 11668, "start": 135.20000000000002, "end": 140.24, "text": " And so when you have increased the training set size to four,", "tokens": [50364, 293, 483, 1238, 709, 4018, 3097, 6713, 13, 50520, 50520, 583, 586, 11, 498, 428, 3097, 992, 2170, 257, 707, 857, 3801, 11, 584, 291, 362, 1451, 3097, 5110, 11, 50782, 50782, 550, 309, 2170, 257, 707, 857, 6081, 281, 3318, 439, 1451, 5110, 6239, 13, 51010, 51010, 400, 291, 815, 483, 257, 7605, 300, 1542, 411, 341, 11, 9001, 309, 1238, 731, 11, 51150, 51150, 457, 291, 458, 11, 257, 707, 857, 766, 294, 257, 1326, 3190, 510, 293, 456, 13, 51290, 51290, 400, 370, 562, 291, 362, 6505, 264, 3097, 992, 2744, 281, 1451, 11, 51542, 51542, 264, 3097, 6713, 575, 767, 2780, 493, 257, 707, 857, 13, 51732, 51732, 1012, 466, 562, 291, 362, 1732, 3097, 5110, 30, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10748176574707032, "compression_ratio": 1.9450980392156862, "no_speech_prob": 5.2552081797330175e-06}, {"id": 30, "seek": 11668, "start": 140.24, "end": 144.04000000000002, "text": " the training error has actually gone up a little bit.", "tokens": [50364, 293, 483, 1238, 709, 4018, 3097, 6713, 13, 50520, 50520, 583, 586, 11, 498, 428, 3097, 992, 2170, 257, 707, 857, 3801, 11, 584, 291, 362, 1451, 3097, 5110, 11, 50782, 50782, 550, 309, 2170, 257, 707, 857, 6081, 281, 3318, 439, 1451, 5110, 6239, 13, 51010, 51010, 400, 291, 815, 483, 257, 7605, 300, 1542, 411, 341, 11, 9001, 309, 1238, 731, 11, 51150, 51150, 457, 291, 458, 11, 257, 707, 857, 766, 294, 257, 1326, 3190, 510, 293, 456, 13, 51290, 51290, 400, 370, 562, 291, 362, 6505, 264, 3097, 992, 2744, 281, 1451, 11, 51542, 51542, 264, 3097, 6713, 575, 767, 2780, 493, 257, 707, 857, 13, 51732, 51732, 1012, 466, 562, 291, 362, 1732, 3097, 5110, 30, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10748176574707032, "compression_ratio": 1.9450980392156862, "no_speech_prob": 5.2552081797330175e-06}, {"id": 31, "seek": 11668, "start": 144.04000000000002, "end": 146.4, "text": " How about when you have five training examples?", "tokens": [50364, 293, 483, 1238, 709, 4018, 3097, 6713, 13, 50520, 50520, 583, 586, 11, 498, 428, 3097, 992, 2170, 257, 707, 857, 3801, 11, 584, 291, 362, 1451, 3097, 5110, 11, 50782, 50782, 550, 309, 2170, 257, 707, 857, 6081, 281, 3318, 439, 1451, 5110, 6239, 13, 51010, 51010, 400, 291, 815, 483, 257, 7605, 300, 1542, 411, 341, 11, 9001, 309, 1238, 731, 11, 51150, 51150, 457, 291, 458, 11, 257, 707, 857, 766, 294, 257, 1326, 3190, 510, 293, 456, 13, 51290, 51290, 400, 370, 562, 291, 362, 6505, 264, 3097, 992, 2744, 281, 1451, 11, 51542, 51542, 264, 3097, 6713, 575, 767, 2780, 493, 257, 707, 857, 13, 51732, 51732, 1012, 466, 562, 291, 362, 1732, 3097, 5110, 30, 51850, 51850], "temperature": 0.0, "avg_logprob": -0.10748176574707032, "compression_ratio": 1.9450980392156862, "no_speech_prob": 5.2552081797330175e-06}, {"id": 32, "seek": 14640, "start": 146.4, "end": 152.20000000000002, "text": " Well, again, you can fit it pretty well, but it gets even a little bit harder to fit all of them perfectly.", "tokens": [50364, 1042, 11, 797, 11, 291, 393, 3318, 309, 1238, 731, 11, 457, 309, 2170, 754, 257, 707, 857, 6081, 281, 3318, 439, 295, 552, 6239, 13, 50654, 50654, 400, 562, 291, 362, 364, 754, 4833, 3097, 992, 11, 309, 445, 2170, 6081, 293, 6081, 50858, 50858, 281, 3318, 633, 2167, 472, 295, 428, 3097, 5110, 6239, 13, 51040, 51040, 407, 281, 20928, 11, 562, 291, 362, 257, 588, 1359, 1230, 295, 3097, 5110, 11, 51224, 51224, 411, 472, 420, 732, 420, 754, 1045, 11, 309, 311, 7226, 1858, 281, 483, 4018, 420, 588, 1359, 3097, 6713, 13, 51618, 51618], "temperature": 0.0, "avg_logprob": -0.10647640605964283, "compression_ratio": 1.8506787330316743, "no_speech_prob": 6.962103270780062e-06}, {"id": 33, "seek": 14640, "start": 152.20000000000002, "end": 156.28, "text": " And when you have an even larger training set, it just gets harder and harder", "tokens": [50364, 1042, 11, 797, 11, 291, 393, 3318, 309, 1238, 731, 11, 457, 309, 2170, 754, 257, 707, 857, 6081, 281, 3318, 439, 295, 552, 6239, 13, 50654, 50654, 400, 562, 291, 362, 364, 754, 4833, 3097, 992, 11, 309, 445, 2170, 6081, 293, 6081, 50858, 50858, 281, 3318, 633, 2167, 472, 295, 428, 3097, 5110, 6239, 13, 51040, 51040, 407, 281, 20928, 11, 562, 291, 362, 257, 588, 1359, 1230, 295, 3097, 5110, 11, 51224, 51224, 411, 472, 420, 732, 420, 754, 1045, 11, 309, 311, 7226, 1858, 281, 483, 4018, 420, 588, 1359, 3097, 6713, 13, 51618, 51618], "temperature": 0.0, "avg_logprob": -0.10647640605964283, "compression_ratio": 1.8506787330316743, "no_speech_prob": 6.962103270780062e-06}, {"id": 34, "seek": 14640, "start": 156.28, "end": 159.92000000000002, "text": " to fit every single one of your training examples perfectly.", "tokens": [50364, 1042, 11, 797, 11, 291, 393, 3318, 309, 1238, 731, 11, 457, 309, 2170, 754, 257, 707, 857, 6081, 281, 3318, 439, 295, 552, 6239, 13, 50654, 50654, 400, 562, 291, 362, 364, 754, 4833, 3097, 992, 11, 309, 445, 2170, 6081, 293, 6081, 50858, 50858, 281, 3318, 633, 2167, 472, 295, 428, 3097, 5110, 6239, 13, 51040, 51040, 407, 281, 20928, 11, 562, 291, 362, 257, 588, 1359, 1230, 295, 3097, 5110, 11, 51224, 51224, 411, 472, 420, 732, 420, 754, 1045, 11, 309, 311, 7226, 1858, 281, 483, 4018, 420, 588, 1359, 3097, 6713, 13, 51618, 51618], "temperature": 0.0, "avg_logprob": -0.10647640605964283, "compression_ratio": 1.8506787330316743, "no_speech_prob": 6.962103270780062e-06}, {"id": 35, "seek": 14640, "start": 159.92000000000002, "end": 163.6, "text": " So to recap, when you have a very small number of training examples,", "tokens": [50364, 1042, 11, 797, 11, 291, 393, 3318, 309, 1238, 731, 11, 457, 309, 2170, 754, 257, 707, 857, 6081, 281, 3318, 439, 295, 552, 6239, 13, 50654, 50654, 400, 562, 291, 362, 364, 754, 4833, 3097, 992, 11, 309, 445, 2170, 6081, 293, 6081, 50858, 50858, 281, 3318, 633, 2167, 472, 295, 428, 3097, 5110, 6239, 13, 51040, 51040, 407, 281, 20928, 11, 562, 291, 362, 257, 588, 1359, 1230, 295, 3097, 5110, 11, 51224, 51224, 411, 472, 420, 732, 420, 754, 1045, 11, 309, 311, 7226, 1858, 281, 483, 4018, 420, 588, 1359, 3097, 6713, 13, 51618, 51618], "temperature": 0.0, "avg_logprob": -0.10647640605964283, "compression_ratio": 1.8506787330316743, "no_speech_prob": 6.962103270780062e-06}, {"id": 36, "seek": 14640, "start": 163.6, "end": 171.48000000000002, "text": " like one or two or even three, it's relatively easy to get zero or very small training error.", "tokens": [50364, 1042, 11, 797, 11, 291, 393, 3318, 309, 1238, 731, 11, 457, 309, 2170, 754, 257, 707, 857, 6081, 281, 3318, 439, 295, 552, 6239, 13, 50654, 50654, 400, 562, 291, 362, 364, 754, 4833, 3097, 992, 11, 309, 445, 2170, 6081, 293, 6081, 50858, 50858, 281, 3318, 633, 2167, 472, 295, 428, 3097, 5110, 6239, 13, 51040, 51040, 407, 281, 20928, 11, 562, 291, 362, 257, 588, 1359, 1230, 295, 3097, 5110, 11, 51224, 51224, 411, 472, 420, 732, 420, 754, 1045, 11, 309, 311, 7226, 1858, 281, 483, 4018, 420, 588, 1359, 3097, 6713, 13, 51618, 51618], "temperature": 0.0, "avg_logprob": -0.10647640605964283, "compression_ratio": 1.8506787330316743, "no_speech_prob": 6.962103270780062e-06}, {"id": 37, "seek": 17148, "start": 171.48, "end": 176.67999999999998, "text": " But when you have a larger training set, it's harder for quadratic function", "tokens": [50364, 583, 562, 291, 362, 257, 4833, 3097, 992, 11, 309, 311, 6081, 337, 37262, 2445, 50624, 50624, 281, 3318, 439, 264, 3097, 5110, 6239, 11, 597, 307, 983, 382, 264, 3097, 992, 2170, 3801, 11, 50932, 50932, 264, 3097, 6713, 8637, 570, 309, 311, 6081, 281, 3318, 439, 295, 264, 3097, 5110, 6239, 13, 51278, 51278, 13428, 472, 661, 551, 466, 613, 19490, 11, 597, 307, 264, 3278, 24071, 6713, 51506, 51506, 486, 312, 5850, 2946, 813, 264, 3097, 6713, 570, 291, 3318, 264, 9834, 281, 264, 3097, 992, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.11694191348168158, "compression_ratio": 1.9383259911894273, "no_speech_prob": 3.4464901546016335e-06}, {"id": 38, "seek": 17148, "start": 176.67999999999998, "end": 182.83999999999997, "text": " to fit all the training examples perfectly, which is why as the training set gets bigger,", "tokens": [50364, 583, 562, 291, 362, 257, 4833, 3097, 992, 11, 309, 311, 6081, 337, 37262, 2445, 50624, 50624, 281, 3318, 439, 264, 3097, 5110, 6239, 11, 597, 307, 983, 382, 264, 3097, 992, 2170, 3801, 11, 50932, 50932, 264, 3097, 6713, 8637, 570, 309, 311, 6081, 281, 3318, 439, 295, 264, 3097, 5110, 6239, 13, 51278, 51278, 13428, 472, 661, 551, 466, 613, 19490, 11, 597, 307, 264, 3278, 24071, 6713, 51506, 51506, 486, 312, 5850, 2946, 813, 264, 3097, 6713, 570, 291, 3318, 264, 9834, 281, 264, 3097, 992, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.11694191348168158, "compression_ratio": 1.9383259911894273, "no_speech_prob": 3.4464901546016335e-06}, {"id": 39, "seek": 17148, "start": 182.83999999999997, "end": 189.76, "text": " the training error increases because it's harder to fit all of the training examples perfectly.", "tokens": [50364, 583, 562, 291, 362, 257, 4833, 3097, 992, 11, 309, 311, 6081, 337, 37262, 2445, 50624, 50624, 281, 3318, 439, 264, 3097, 5110, 6239, 11, 597, 307, 983, 382, 264, 3097, 992, 2170, 3801, 11, 50932, 50932, 264, 3097, 6713, 8637, 570, 309, 311, 6081, 281, 3318, 439, 295, 264, 3097, 5110, 6239, 13, 51278, 51278, 13428, 472, 661, 551, 466, 613, 19490, 11, 597, 307, 264, 3278, 24071, 6713, 51506, 51506, 486, 312, 5850, 2946, 813, 264, 3097, 6713, 570, 291, 3318, 264, 9834, 281, 264, 3097, 992, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.11694191348168158, "compression_ratio": 1.9383259911894273, "no_speech_prob": 3.4464901546016335e-06}, {"id": 40, "seek": 17148, "start": 189.76, "end": 194.32, "text": " Notice one other thing about these curves, which is the cross validation error", "tokens": [50364, 583, 562, 291, 362, 257, 4833, 3097, 992, 11, 309, 311, 6081, 337, 37262, 2445, 50624, 50624, 281, 3318, 439, 264, 3097, 5110, 6239, 11, 597, 307, 983, 382, 264, 3097, 992, 2170, 3801, 11, 50932, 50932, 264, 3097, 6713, 8637, 570, 309, 311, 6081, 281, 3318, 439, 295, 264, 3097, 5110, 6239, 13, 51278, 51278, 13428, 472, 661, 551, 466, 613, 19490, 11, 597, 307, 264, 3278, 24071, 6713, 51506, 51506, 486, 312, 5850, 2946, 813, 264, 3097, 6713, 570, 291, 3318, 264, 9834, 281, 264, 3097, 992, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.11694191348168158, "compression_ratio": 1.9383259911894273, "no_speech_prob": 3.4464901546016335e-06}, {"id": 41, "seek": 17148, "start": 194.32, "end": 201.28, "text": " will be typically higher than the training error because you fit the parameters to the training set", "tokens": [50364, 583, 562, 291, 362, 257, 4833, 3097, 992, 11, 309, 311, 6081, 337, 37262, 2445, 50624, 50624, 281, 3318, 439, 264, 3097, 5110, 6239, 11, 597, 307, 983, 382, 264, 3097, 992, 2170, 3801, 11, 50932, 50932, 264, 3097, 6713, 8637, 570, 309, 311, 6081, 281, 3318, 439, 295, 264, 3097, 5110, 6239, 13, 51278, 51278, 13428, 472, 661, 551, 466, 613, 19490, 11, 597, 307, 264, 3278, 24071, 6713, 51506, 51506, 486, 312, 5850, 2946, 813, 264, 3097, 6713, 570, 291, 3318, 264, 9834, 281, 264, 3097, 992, 51854, 51854], "temperature": 0.0, "avg_logprob": -0.11694191348168158, "compression_ratio": 1.9383259911894273, "no_speech_prob": 3.4464901546016335e-06}, {"id": 42, "seek": 20128, "start": 201.28, "end": 205.6, "text": " and so you expect to do at least a little bit better or when M is small,", "tokens": [50364, 293, 370, 291, 2066, 281, 360, 412, 1935, 257, 707, 857, 1101, 420, 562, 376, 307, 1359, 11, 50580, 50580, 1310, 754, 257, 688, 1101, 322, 264, 3097, 992, 813, 322, 264, 3278, 24071, 992, 13, 50876, 50876, 961, 311, 586, 747, 257, 574, 412, 437, 264, 2539, 19490, 486, 574, 411, 337, 364, 9284, 365, 1090, 12577, 5717, 472, 365, 1090, 21977, 13, 51286, 51286, 961, 311, 722, 412, 264, 1090, 12577, 420, 264, 833, 15669, 1389, 13, 51496, 51496, 9647, 336, 300, 364, 1365, 295, 1090, 12577, 576, 312, 498, 291, 434, 15669, 257, 8213, 2445, 281, 257, 7605, 300, 1542, 411, 341, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.09122272838245739, "compression_ratio": 1.7707509881422925, "no_speech_prob": 1.384546521876473e-05}, {"id": 43, "seek": 20128, "start": 205.6, "end": 211.52, "text": " maybe even a lot better on the training set than on the cross validation set.", "tokens": [50364, 293, 370, 291, 2066, 281, 360, 412, 1935, 257, 707, 857, 1101, 420, 562, 376, 307, 1359, 11, 50580, 50580, 1310, 754, 257, 688, 1101, 322, 264, 3097, 992, 813, 322, 264, 3278, 24071, 992, 13, 50876, 50876, 961, 311, 586, 747, 257, 574, 412, 437, 264, 2539, 19490, 486, 574, 411, 337, 364, 9284, 365, 1090, 12577, 5717, 472, 365, 1090, 21977, 13, 51286, 51286, 961, 311, 722, 412, 264, 1090, 12577, 420, 264, 833, 15669, 1389, 13, 51496, 51496, 9647, 336, 300, 364, 1365, 295, 1090, 12577, 576, 312, 498, 291, 434, 15669, 257, 8213, 2445, 281, 257, 7605, 300, 1542, 411, 341, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.09122272838245739, "compression_ratio": 1.7707509881422925, "no_speech_prob": 1.384546521876473e-05}, {"id": 44, "seek": 20128, "start": 211.52, "end": 219.72, "text": " Let's now take a look at what the learning curves will look like for an algorithm with high bias versus one with high variance.", "tokens": [50364, 293, 370, 291, 2066, 281, 360, 412, 1935, 257, 707, 857, 1101, 420, 562, 376, 307, 1359, 11, 50580, 50580, 1310, 754, 257, 688, 1101, 322, 264, 3097, 992, 813, 322, 264, 3278, 24071, 992, 13, 50876, 50876, 961, 311, 586, 747, 257, 574, 412, 437, 264, 2539, 19490, 486, 574, 411, 337, 364, 9284, 365, 1090, 12577, 5717, 472, 365, 1090, 21977, 13, 51286, 51286, 961, 311, 722, 412, 264, 1090, 12577, 420, 264, 833, 15669, 1389, 13, 51496, 51496, 9647, 336, 300, 364, 1365, 295, 1090, 12577, 576, 312, 498, 291, 434, 15669, 257, 8213, 2445, 281, 257, 7605, 300, 1542, 411, 341, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.09122272838245739, "compression_ratio": 1.7707509881422925, "no_speech_prob": 1.384546521876473e-05}, {"id": 45, "seek": 20128, "start": 219.72, "end": 223.92000000000002, "text": " Let's start at the high bias or the under fitting case.", "tokens": [50364, 293, 370, 291, 2066, 281, 360, 412, 1935, 257, 707, 857, 1101, 420, 562, 376, 307, 1359, 11, 50580, 50580, 1310, 754, 257, 688, 1101, 322, 264, 3097, 992, 813, 322, 264, 3278, 24071, 992, 13, 50876, 50876, 961, 311, 586, 747, 257, 574, 412, 437, 264, 2539, 19490, 486, 574, 411, 337, 364, 9284, 365, 1090, 12577, 5717, 472, 365, 1090, 21977, 13, 51286, 51286, 961, 311, 722, 412, 264, 1090, 12577, 420, 264, 833, 15669, 1389, 13, 51496, 51496, 9647, 336, 300, 364, 1365, 295, 1090, 12577, 576, 312, 498, 291, 434, 15669, 257, 8213, 2445, 281, 257, 7605, 300, 1542, 411, 341, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.09122272838245739, "compression_ratio": 1.7707509881422925, "no_speech_prob": 1.384546521876473e-05}, {"id": 46, "seek": 20128, "start": 223.92000000000002, "end": 230.52, "text": " Recall that an example of high bias would be if you're fitting a linear function to a curve that looks like this.", "tokens": [50364, 293, 370, 291, 2066, 281, 360, 412, 1935, 257, 707, 857, 1101, 420, 562, 376, 307, 1359, 11, 50580, 50580, 1310, 754, 257, 688, 1101, 322, 264, 3097, 992, 813, 322, 264, 3278, 24071, 992, 13, 50876, 50876, 961, 311, 586, 747, 257, 574, 412, 437, 264, 2539, 19490, 486, 574, 411, 337, 364, 9284, 365, 1090, 12577, 5717, 472, 365, 1090, 21977, 13, 51286, 51286, 961, 311, 722, 412, 264, 1090, 12577, 420, 264, 833, 15669, 1389, 13, 51496, 51496, 9647, 336, 300, 364, 1365, 295, 1090, 12577, 576, 312, 498, 291, 434, 15669, 257, 8213, 2445, 281, 257, 7605, 300, 1542, 411, 341, 13, 51826, 51826], "temperature": 0.0, "avg_logprob": -0.09122272838245739, "compression_ratio": 1.7707509881422925, "no_speech_prob": 1.384546521876473e-05}, {"id": 47, "seek": 23052, "start": 230.52, "end": 238.96, "text": " If you were to plot the training error, then the training error will go up like so, as you'd expect.", "tokens": [50364, 759, 291, 645, 281, 7542, 264, 3097, 6713, 11, 550, 264, 3097, 6713, 486, 352, 493, 411, 370, 11, 382, 291, 1116, 2066, 13, 50786, 50786, 400, 294, 1186, 11, 341, 7605, 295, 3097, 6713, 815, 722, 281, 24183, 484, 420, 321, 818, 309, 39885, 11, 3620, 24183, 484, 934, 257, 1339, 13, 51284, 51284, 400, 300, 311, 570, 382, 291, 483, 544, 293, 544, 3097, 5110, 11, 562, 291, 434, 15669, 264, 2199, 8213, 2445, 11, 51616, 51616, 428, 2316, 1177, 380, 767, 1319, 300, 709, 544, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.09596803111414756, "compression_ratio": 1.72, "no_speech_prob": 5.862552825419698e-06}, {"id": 48, "seek": 23052, "start": 238.96, "end": 248.92000000000002, "text": " And in fact, this curve of training error may start to flatten out or we call it plateau, meaning flatten out after a while.", "tokens": [50364, 759, 291, 645, 281, 7542, 264, 3097, 6713, 11, 550, 264, 3097, 6713, 486, 352, 493, 411, 370, 11, 382, 291, 1116, 2066, 13, 50786, 50786, 400, 294, 1186, 11, 341, 7605, 295, 3097, 6713, 815, 722, 281, 24183, 484, 420, 321, 818, 309, 39885, 11, 3620, 24183, 484, 934, 257, 1339, 13, 51284, 51284, 400, 300, 311, 570, 382, 291, 483, 544, 293, 544, 3097, 5110, 11, 562, 291, 434, 15669, 264, 2199, 8213, 2445, 11, 51616, 51616, 428, 2316, 1177, 380, 767, 1319, 300, 709, 544, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.09596803111414756, "compression_ratio": 1.72, "no_speech_prob": 5.862552825419698e-06}, {"id": 49, "seek": 23052, "start": 248.92000000000002, "end": 255.56, "text": " And that's because as you get more and more training examples, when you're fitting the simple linear function,", "tokens": [50364, 759, 291, 645, 281, 7542, 264, 3097, 6713, 11, 550, 264, 3097, 6713, 486, 352, 493, 411, 370, 11, 382, 291, 1116, 2066, 13, 50786, 50786, 400, 294, 1186, 11, 341, 7605, 295, 3097, 6713, 815, 722, 281, 24183, 484, 420, 321, 818, 309, 39885, 11, 3620, 24183, 484, 934, 257, 1339, 13, 51284, 51284, 400, 300, 311, 570, 382, 291, 483, 544, 293, 544, 3097, 5110, 11, 562, 291, 434, 15669, 264, 2199, 8213, 2445, 11, 51616, 51616, 428, 2316, 1177, 380, 767, 1319, 300, 709, 544, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.09596803111414756, "compression_ratio": 1.72, "no_speech_prob": 5.862552825419698e-06}, {"id": 50, "seek": 23052, "start": 255.56, "end": 258.44, "text": " your model doesn't actually change that much more.", "tokens": [50364, 759, 291, 645, 281, 7542, 264, 3097, 6713, 11, 550, 264, 3097, 6713, 486, 352, 493, 411, 370, 11, 382, 291, 1116, 2066, 13, 50786, 50786, 400, 294, 1186, 11, 341, 7605, 295, 3097, 6713, 815, 722, 281, 24183, 484, 420, 321, 818, 309, 39885, 11, 3620, 24183, 484, 934, 257, 1339, 13, 51284, 51284, 400, 300, 311, 570, 382, 291, 483, 544, 293, 544, 3097, 5110, 11, 562, 291, 434, 15669, 264, 2199, 8213, 2445, 11, 51616, 51616, 428, 2316, 1177, 380, 767, 1319, 300, 709, 544, 13, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.09596803111414756, "compression_ratio": 1.72, "no_speech_prob": 5.862552825419698e-06}, {"id": 51, "seek": 25844, "start": 258.44, "end": 264.2, "text": " Fitting a straight line and even as you get more and more and more examples, there's just not that much more to change,", "tokens": [50364, 479, 2414, 257, 2997, 1622, 293, 754, 382, 291, 483, 544, 293, 544, 293, 544, 5110, 11, 456, 311, 445, 406, 300, 709, 544, 281, 1319, 11, 50652, 50652, 597, 307, 983, 264, 4274, 3097, 6713, 932, 1591, 694, 484, 934, 257, 1339, 13, 50948, 50948, 400, 14138, 11, 428, 3278, 24071, 6713, 486, 808, 760, 293, 611, 24183, 484, 934, 257, 1339, 11, 51286, 51286, 597, 307, 983, 49802, 53, 797, 307, 2946, 813, 508, 51, 7146, 11, 457, 49802, 53, 486, 3928, 281, 574, 411, 300, 13, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.10096257732760522, "compression_ratio": 1.6545454545454545, "no_speech_prob": 4.936893674312159e-06}, {"id": 52, "seek": 25844, "start": 264.2, "end": 270.12, "text": " which is why the average training error flattens out after a while.", "tokens": [50364, 479, 2414, 257, 2997, 1622, 293, 754, 382, 291, 483, 544, 293, 544, 293, 544, 5110, 11, 456, 311, 445, 406, 300, 709, 544, 281, 1319, 11, 50652, 50652, 597, 307, 983, 264, 4274, 3097, 6713, 932, 1591, 694, 484, 934, 257, 1339, 13, 50948, 50948, 400, 14138, 11, 428, 3278, 24071, 6713, 486, 808, 760, 293, 611, 24183, 484, 934, 257, 1339, 11, 51286, 51286, 597, 307, 983, 49802, 53, 797, 307, 2946, 813, 508, 51, 7146, 11, 457, 49802, 53, 486, 3928, 281, 574, 411, 300, 13, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.10096257732760522, "compression_ratio": 1.6545454545454545, "no_speech_prob": 4.936893674312159e-06}, {"id": 53, "seek": 25844, "start": 270.12, "end": 276.88, "text": " And similarly, your cross validation error will come down and also flatten out after a while,", "tokens": [50364, 479, 2414, 257, 2997, 1622, 293, 754, 382, 291, 483, 544, 293, 544, 293, 544, 5110, 11, 456, 311, 445, 406, 300, 709, 544, 281, 1319, 11, 50652, 50652, 597, 307, 983, 264, 4274, 3097, 6713, 932, 1591, 694, 484, 934, 257, 1339, 13, 50948, 50948, 400, 14138, 11, 428, 3278, 24071, 6713, 486, 808, 760, 293, 611, 24183, 484, 934, 257, 1339, 11, 51286, 51286, 597, 307, 983, 49802, 53, 797, 307, 2946, 813, 508, 51, 7146, 11, 457, 49802, 53, 486, 3928, 281, 574, 411, 300, 13, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.10096257732760522, "compression_ratio": 1.6545454545454545, "no_speech_prob": 4.936893674312159e-06}, {"id": 54, "seek": 25844, "start": 276.88, "end": 284.0, "text": " which is why JCV again is higher than JTrain, but JCV will tend to look like that.", "tokens": [50364, 479, 2414, 257, 2997, 1622, 293, 754, 382, 291, 483, 544, 293, 544, 293, 544, 5110, 11, 456, 311, 445, 406, 300, 709, 544, 281, 1319, 11, 50652, 50652, 597, 307, 983, 264, 4274, 3097, 6713, 932, 1591, 694, 484, 934, 257, 1339, 13, 50948, 50948, 400, 14138, 11, 428, 3278, 24071, 6713, 486, 808, 760, 293, 611, 24183, 484, 934, 257, 1339, 11, 51286, 51286, 597, 307, 983, 49802, 53, 797, 307, 2946, 813, 508, 51, 7146, 11, 457, 49802, 53, 486, 3928, 281, 574, 411, 300, 13, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.10096257732760522, "compression_ratio": 1.6545454545454545, "no_speech_prob": 4.936893674312159e-06}, {"id": 55, "seek": 28400, "start": 284.0, "end": 289.2, "text": " And it's because beyond a certain point, even as you get more and more and more examples,", "tokens": [50364, 400, 309, 311, 570, 4399, 257, 1629, 935, 11, 754, 382, 291, 483, 544, 293, 544, 293, 544, 5110, 11, 50624, 50624, 406, 709, 307, 516, 281, 1319, 466, 264, 2997, 1622, 291, 434, 15669, 13, 50754, 50754, 467, 311, 445, 886, 2199, 257, 2316, 281, 312, 15669, 281, 341, 709, 1412, 11, 597, 307, 983, 1293, 295, 613, 19490, 11, 49802, 53, 293, 508, 51, 7146, 11, 3928, 281, 24183, 934, 257, 1339, 13, 51268, 51268, 400, 498, 291, 632, 257, 3481, 295, 300, 20518, 1496, 295, 3389, 11, 1270, 382, 1952, 1496, 3389, 11, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.08292606353759766, "compression_ratio": 1.6311475409836065, "no_speech_prob": 6.048583600204438e-06}, {"id": 56, "seek": 28400, "start": 289.2, "end": 291.8, "text": " not much is going to change about the straight line you're fitting.", "tokens": [50364, 400, 309, 311, 570, 4399, 257, 1629, 935, 11, 754, 382, 291, 483, 544, 293, 544, 293, 544, 5110, 11, 50624, 50624, 406, 709, 307, 516, 281, 1319, 466, 264, 2997, 1622, 291, 434, 15669, 13, 50754, 50754, 467, 311, 445, 886, 2199, 257, 2316, 281, 312, 15669, 281, 341, 709, 1412, 11, 597, 307, 983, 1293, 295, 613, 19490, 11, 49802, 53, 293, 508, 51, 7146, 11, 3928, 281, 24183, 934, 257, 1339, 13, 51268, 51268, 400, 498, 291, 632, 257, 3481, 295, 300, 20518, 1496, 295, 3389, 11, 1270, 382, 1952, 1496, 3389, 11, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.08292606353759766, "compression_ratio": 1.6311475409836065, "no_speech_prob": 6.048583600204438e-06}, {"id": 57, "seek": 28400, "start": 291.8, "end": 302.08, "text": " It's just too simple a model to be fitting to this much data, which is why both of these curves, JCV and JTrain, tend to flatten after a while.", "tokens": [50364, 400, 309, 311, 570, 4399, 257, 1629, 935, 11, 754, 382, 291, 483, 544, 293, 544, 293, 544, 5110, 11, 50624, 50624, 406, 709, 307, 516, 281, 1319, 466, 264, 2997, 1622, 291, 434, 15669, 13, 50754, 50754, 467, 311, 445, 886, 2199, 257, 2316, 281, 312, 15669, 281, 341, 709, 1412, 11, 597, 307, 983, 1293, 295, 613, 19490, 11, 49802, 53, 293, 508, 51, 7146, 11, 3928, 281, 24183, 934, 257, 1339, 13, 51268, 51268, 400, 498, 291, 632, 257, 3481, 295, 300, 20518, 1496, 295, 3389, 11, 1270, 382, 1952, 1496, 3389, 11, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.08292606353759766, "compression_ratio": 1.6311475409836065, "no_speech_prob": 6.048583600204438e-06}, {"id": 58, "seek": 28400, "start": 302.08, "end": 309.84, "text": " And if you had a measure of that baseline level of performance, such as human level performance,", "tokens": [50364, 400, 309, 311, 570, 4399, 257, 1629, 935, 11, 754, 382, 291, 483, 544, 293, 544, 293, 544, 5110, 11, 50624, 50624, 406, 709, 307, 516, 281, 1319, 466, 264, 2997, 1622, 291, 434, 15669, 13, 50754, 50754, 467, 311, 445, 886, 2199, 257, 2316, 281, 312, 15669, 281, 341, 709, 1412, 11, 597, 307, 983, 1293, 295, 613, 19490, 11, 49802, 53, 293, 508, 51, 7146, 11, 3928, 281, 24183, 934, 257, 1339, 13, 51268, 51268, 400, 498, 291, 632, 257, 3481, 295, 300, 20518, 1496, 295, 3389, 11, 1270, 382, 1952, 1496, 3389, 11, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.08292606353759766, "compression_ratio": 1.6311475409836065, "no_speech_prob": 6.048583600204438e-06}, {"id": 59, "seek": 30984, "start": 309.84, "end": 315.47999999999996, "text": " then it'll tend to be a value that is lower than your JTrain and your JCV.", "tokens": [50364, 550, 309, 603, 3928, 281, 312, 257, 2158, 300, 307, 3126, 813, 428, 508, 51, 7146, 293, 428, 49802, 53, 13, 50646, 50646, 407, 1952, 1496, 3389, 815, 574, 411, 341, 13, 50802, 50802, 400, 456, 311, 257, 955, 7417, 1296, 264, 20518, 1496, 3389, 293, 508, 51, 7146, 11, 51038, 51038, 597, 390, 527, 16961, 337, 341, 9284, 1419, 1090, 12577, 13, 51318, 51318, 663, 307, 11, 472, 727, 1454, 281, 312, 884, 709, 1101, 498, 787, 321, 727, 3318, 257, 544, 3997, 2445, 813, 445, 257, 2997, 1622, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.07587781705354389, "compression_ratio": 1.6025641025641026, "no_speech_prob": 6.577670887963905e-07}, {"id": 60, "seek": 30984, "start": 315.47999999999996, "end": 318.59999999999997, "text": " So human level performance may look like this.", "tokens": [50364, 550, 309, 603, 3928, 281, 312, 257, 2158, 300, 307, 3126, 813, 428, 508, 51, 7146, 293, 428, 49802, 53, 13, 50646, 50646, 407, 1952, 1496, 3389, 815, 574, 411, 341, 13, 50802, 50802, 400, 456, 311, 257, 955, 7417, 1296, 264, 20518, 1496, 3389, 293, 508, 51, 7146, 11, 51038, 51038, 597, 390, 527, 16961, 337, 341, 9284, 1419, 1090, 12577, 13, 51318, 51318, 663, 307, 11, 472, 727, 1454, 281, 312, 884, 709, 1101, 498, 787, 321, 727, 3318, 257, 544, 3997, 2445, 813, 445, 257, 2997, 1622, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.07587781705354389, "compression_ratio": 1.6025641025641026, "no_speech_prob": 6.577670887963905e-07}, {"id": 61, "seek": 30984, "start": 318.59999999999997, "end": 323.32, "text": " And there's a big gap between the baseline level performance and JTrain,", "tokens": [50364, 550, 309, 603, 3928, 281, 312, 257, 2158, 300, 307, 3126, 813, 428, 508, 51, 7146, 293, 428, 49802, 53, 13, 50646, 50646, 407, 1952, 1496, 3389, 815, 574, 411, 341, 13, 50802, 50802, 400, 456, 311, 257, 955, 7417, 1296, 264, 20518, 1496, 3389, 293, 508, 51, 7146, 11, 51038, 51038, 597, 390, 527, 16961, 337, 341, 9284, 1419, 1090, 12577, 13, 51318, 51318, 663, 307, 11, 472, 727, 1454, 281, 312, 884, 709, 1101, 498, 787, 321, 727, 3318, 257, 544, 3997, 2445, 813, 445, 257, 2997, 1622, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.07587781705354389, "compression_ratio": 1.6025641025641026, "no_speech_prob": 6.577670887963905e-07}, {"id": 62, "seek": 30984, "start": 323.32, "end": 328.91999999999996, "text": " which was our indicator for this algorithm having high bias.", "tokens": [50364, 550, 309, 603, 3928, 281, 312, 257, 2158, 300, 307, 3126, 813, 428, 508, 51, 7146, 293, 428, 49802, 53, 13, 50646, 50646, 407, 1952, 1496, 3389, 815, 574, 411, 341, 13, 50802, 50802, 400, 456, 311, 257, 955, 7417, 1296, 264, 20518, 1496, 3389, 293, 508, 51, 7146, 11, 51038, 51038, 597, 390, 527, 16961, 337, 341, 9284, 1419, 1090, 12577, 13, 51318, 51318, 663, 307, 11, 472, 727, 1454, 281, 312, 884, 709, 1101, 498, 787, 321, 727, 3318, 257, 544, 3997, 2445, 813, 445, 257, 2997, 1622, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.07587781705354389, "compression_ratio": 1.6025641025641026, "no_speech_prob": 6.577670887963905e-07}, {"id": 63, "seek": 30984, "start": 328.91999999999996, "end": 337.91999999999996, "text": " That is, one could hope to be doing much better if only we could fit a more complex function than just a straight line.", "tokens": [50364, 550, 309, 603, 3928, 281, 312, 257, 2158, 300, 307, 3126, 813, 428, 508, 51, 7146, 293, 428, 49802, 53, 13, 50646, 50646, 407, 1952, 1496, 3389, 815, 574, 411, 341, 13, 50802, 50802, 400, 456, 311, 257, 955, 7417, 1296, 264, 20518, 1496, 3389, 293, 508, 51, 7146, 11, 51038, 51038, 597, 390, 527, 16961, 337, 341, 9284, 1419, 1090, 12577, 13, 51318, 51318, 663, 307, 11, 472, 727, 1454, 281, 312, 884, 709, 1101, 498, 787, 321, 727, 3318, 257, 544, 3997, 2445, 813, 445, 257, 2997, 1622, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.07587781705354389, "compression_ratio": 1.6025641025641026, "no_speech_prob": 6.577670887963905e-07}, {"id": 64, "seek": 33792, "start": 337.92, "end": 343.76, "text": " Now, one interesting thing about this plot is you can ask,", "tokens": [50364, 823, 11, 472, 1880, 551, 466, 341, 7542, 307, 291, 393, 1029, 11, 50656, 50656, 437, 360, 291, 519, 486, 1051, 498, 291, 727, 362, 257, 709, 3801, 3097, 992, 30, 50934, 50934, 407, 437, 576, 309, 574, 411, 498, 321, 727, 3488, 376, 754, 3052, 813, 264, 558, 295, 341, 7542, 293, 352, 3052, 281, 264, 558, 382, 10002, 30, 51392, 51392, 1042, 11, 291, 393, 3811, 498, 291, 645, 281, 10101, 1293, 295, 613, 19490, 281, 264, 558, 11, 51586, 51586], "temperature": 0.0, "avg_logprob": -0.07571620380177217, "compression_ratio": 1.6780487804878048, "no_speech_prob": 4.565802555589471e-06}, {"id": 65, "seek": 33792, "start": 343.76, "end": 349.32, "text": " what do you think will happen if you could have a much bigger training set?", "tokens": [50364, 823, 11, 472, 1880, 551, 466, 341, 7542, 307, 291, 393, 1029, 11, 50656, 50656, 437, 360, 291, 519, 486, 1051, 498, 291, 727, 362, 257, 709, 3801, 3097, 992, 30, 50934, 50934, 407, 437, 576, 309, 574, 411, 498, 321, 727, 3488, 376, 754, 3052, 813, 264, 558, 295, 341, 7542, 293, 352, 3052, 281, 264, 558, 382, 10002, 30, 51392, 51392, 1042, 11, 291, 393, 3811, 498, 291, 645, 281, 10101, 1293, 295, 613, 19490, 281, 264, 558, 11, 51586, 51586], "temperature": 0.0, "avg_logprob": -0.07571620380177217, "compression_ratio": 1.6780487804878048, "no_speech_prob": 4.565802555589471e-06}, {"id": 66, "seek": 33792, "start": 349.32, "end": 358.48, "text": " So what would it look like if we could increase M even further than the right of this plot and go further to the right as follows?", "tokens": [50364, 823, 11, 472, 1880, 551, 466, 341, 7542, 307, 291, 393, 1029, 11, 50656, 50656, 437, 360, 291, 519, 486, 1051, 498, 291, 727, 362, 257, 709, 3801, 3097, 992, 30, 50934, 50934, 407, 437, 576, 309, 574, 411, 498, 321, 727, 3488, 376, 754, 3052, 813, 264, 558, 295, 341, 7542, 293, 352, 3052, 281, 264, 558, 382, 10002, 30, 51392, 51392, 1042, 11, 291, 393, 3811, 498, 291, 645, 281, 10101, 1293, 295, 613, 19490, 281, 264, 558, 11, 51586, 51586], "temperature": 0.0, "avg_logprob": -0.07571620380177217, "compression_ratio": 1.6780487804878048, "no_speech_prob": 4.565802555589471e-06}, {"id": 67, "seek": 33792, "start": 358.48, "end": 362.36, "text": " Well, you can imagine if you were to extend both of these curves to the right,", "tokens": [50364, 823, 11, 472, 1880, 551, 466, 341, 7542, 307, 291, 393, 1029, 11, 50656, 50656, 437, 360, 291, 519, 486, 1051, 498, 291, 727, 362, 257, 709, 3801, 3097, 992, 30, 50934, 50934, 407, 437, 576, 309, 574, 411, 498, 321, 727, 3488, 376, 754, 3052, 813, 264, 558, 295, 341, 7542, 293, 352, 3052, 281, 264, 558, 382, 10002, 30, 51392, 51392, 1042, 11, 291, 393, 3811, 498, 291, 645, 281, 10101, 1293, 295, 613, 19490, 281, 264, 558, 11, 51586, 51586], "temperature": 0.0, "avg_logprob": -0.07571620380177217, "compression_ratio": 1.6780487804878048, "no_speech_prob": 4.565802555589471e-06}, {"id": 68, "seek": 36236, "start": 362.36, "end": 368.6, "text": " they both sort of flatten out and both of them will probably just continue to be flat like that.", "tokens": [50364, 436, 1293, 1333, 295, 24183, 484, 293, 1293, 295, 552, 486, 1391, 445, 2354, 281, 312, 4962, 411, 300, 13, 50676, 50676, 400, 572, 1871, 577, 1400, 291, 10101, 281, 264, 558, 295, 341, 7542, 11, 613, 732, 19490, 11, 50874, 50874, 436, 486, 1128, 11, 291, 458, 11, 6063, 915, 257, 636, 281, 10460, 760, 281, 341, 1952, 1496, 295, 3389, 420, 445, 1066, 322, 885, 733, 295, 4962, 411, 341, 1238, 709, 5680, 11, 51356, 51356, 572, 1871, 577, 2416, 264, 3097, 992, 2170, 13, 51522, 51522], "temperature": 0.0, "avg_logprob": -0.09891156835870428, "compression_ratio": 1.6936936936936937, "no_speech_prob": 9.422234938938345e-07}, {"id": 69, "seek": 36236, "start": 368.6, "end": 372.56, "text": " And no matter how far you extend to the right of this plot, these two curves,", "tokens": [50364, 436, 1293, 1333, 295, 24183, 484, 293, 1293, 295, 552, 486, 1391, 445, 2354, 281, 312, 4962, 411, 300, 13, 50676, 50676, 400, 572, 1871, 577, 1400, 291, 10101, 281, 264, 558, 295, 341, 7542, 11, 613, 732, 19490, 11, 50874, 50874, 436, 486, 1128, 11, 291, 458, 11, 6063, 915, 257, 636, 281, 10460, 760, 281, 341, 1952, 1496, 295, 3389, 420, 445, 1066, 322, 885, 733, 295, 4962, 411, 341, 1238, 709, 5680, 11, 51356, 51356, 572, 1871, 577, 2416, 264, 3097, 992, 2170, 13, 51522, 51522], "temperature": 0.0, "avg_logprob": -0.09891156835870428, "compression_ratio": 1.6936936936936937, "no_speech_prob": 9.422234938938345e-07}, {"id": 70, "seek": 36236, "start": 372.56, "end": 382.2, "text": " they will never, you know, somehow find a way to dip down to this human level of performance or just keep on being kind of flat like this pretty much forever,", "tokens": [50364, 436, 1293, 1333, 295, 24183, 484, 293, 1293, 295, 552, 486, 1391, 445, 2354, 281, 312, 4962, 411, 300, 13, 50676, 50676, 400, 572, 1871, 577, 1400, 291, 10101, 281, 264, 558, 295, 341, 7542, 11, 613, 732, 19490, 11, 50874, 50874, 436, 486, 1128, 11, 291, 458, 11, 6063, 915, 257, 636, 281, 10460, 760, 281, 341, 1952, 1496, 295, 3389, 420, 445, 1066, 322, 885, 733, 295, 4962, 411, 341, 1238, 709, 5680, 11, 51356, 51356, 572, 1871, 577, 2416, 264, 3097, 992, 2170, 13, 51522, 51522], "temperature": 0.0, "avg_logprob": -0.09891156835870428, "compression_ratio": 1.6936936936936937, "no_speech_prob": 9.422234938938345e-07}, {"id": 71, "seek": 36236, "start": 382.2, "end": 385.52000000000004, "text": " no matter how large the training set gets.", "tokens": [50364, 436, 1293, 1333, 295, 24183, 484, 293, 1293, 295, 552, 486, 1391, 445, 2354, 281, 312, 4962, 411, 300, 13, 50676, 50676, 400, 572, 1871, 577, 1400, 291, 10101, 281, 264, 558, 295, 341, 7542, 11, 613, 732, 19490, 11, 50874, 50874, 436, 486, 1128, 11, 291, 458, 11, 6063, 915, 257, 636, 281, 10460, 760, 281, 341, 1952, 1496, 295, 3389, 420, 445, 1066, 322, 885, 733, 295, 4962, 411, 341, 1238, 709, 5680, 11, 51356, 51356, 572, 1871, 577, 2416, 264, 3097, 992, 2170, 13, 51522, 51522], "temperature": 0.0, "avg_logprob": -0.09891156835870428, "compression_ratio": 1.6936936936936937, "no_speech_prob": 9.422234938938345e-07}, {"id": 72, "seek": 38552, "start": 385.52, "end": 392.71999999999997, "text": " So that gives this conclusion, maybe a little bit surprising, that if a learning algorithm has high bias,", "tokens": [50364, 407, 300, 2709, 341, 10063, 11, 1310, 257, 707, 857, 8830, 11, 300, 498, 257, 2539, 9284, 575, 1090, 12577, 11, 50724, 50724, 1242, 544, 3097, 1412, 486, 406, 538, 2564, 854, 300, 709, 13, 50950, 50950, 400, 286, 458, 300, 321, 434, 1143, 281, 1953, 300, 1419, 544, 1412, 307, 665, 13, 51200, 51200, 583, 498, 428, 9284, 575, 1090, 12577, 11, 550, 498, 264, 787, 551, 291, 360, 307, 3507, 544, 3097, 1412, 412, 309, 11, 51556, 51556, 300, 538, 2564, 486, 406, 1562, 718, 291, 1565, 760, 264, 6713, 3314, 300, 709, 13, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.07359719276428223, "compression_ratio": 1.8114035087719298, "no_speech_prob": 3.966795247833943e-06}, {"id": 73, "seek": 38552, "start": 392.71999999999997, "end": 397.24, "text": " getting more training data will not by itself help that much.", "tokens": [50364, 407, 300, 2709, 341, 10063, 11, 1310, 257, 707, 857, 8830, 11, 300, 498, 257, 2539, 9284, 575, 1090, 12577, 11, 50724, 50724, 1242, 544, 3097, 1412, 486, 406, 538, 2564, 854, 300, 709, 13, 50950, 50950, 400, 286, 458, 300, 321, 434, 1143, 281, 1953, 300, 1419, 544, 1412, 307, 665, 13, 51200, 51200, 583, 498, 428, 9284, 575, 1090, 12577, 11, 550, 498, 264, 787, 551, 291, 360, 307, 3507, 544, 3097, 1412, 412, 309, 11, 51556, 51556, 300, 538, 2564, 486, 406, 1562, 718, 291, 1565, 760, 264, 6713, 3314, 300, 709, 13, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.07359719276428223, "compression_ratio": 1.8114035087719298, "no_speech_prob": 3.966795247833943e-06}, {"id": 74, "seek": 38552, "start": 397.24, "end": 402.24, "text": " And I know that we're used to thinking that having more data is good.", "tokens": [50364, 407, 300, 2709, 341, 10063, 11, 1310, 257, 707, 857, 8830, 11, 300, 498, 257, 2539, 9284, 575, 1090, 12577, 11, 50724, 50724, 1242, 544, 3097, 1412, 486, 406, 538, 2564, 854, 300, 709, 13, 50950, 50950, 400, 286, 458, 300, 321, 434, 1143, 281, 1953, 300, 1419, 544, 1412, 307, 665, 13, 51200, 51200, 583, 498, 428, 9284, 575, 1090, 12577, 11, 550, 498, 264, 787, 551, 291, 360, 307, 3507, 544, 3097, 1412, 412, 309, 11, 51556, 51556, 300, 538, 2564, 486, 406, 1562, 718, 291, 1565, 760, 264, 6713, 3314, 300, 709, 13, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.07359719276428223, "compression_ratio": 1.8114035087719298, "no_speech_prob": 3.966795247833943e-06}, {"id": 75, "seek": 38552, "start": 402.24, "end": 409.35999999999996, "text": " But if your algorithm has high bias, then if the only thing you do is throw more training data at it,", "tokens": [50364, 407, 300, 2709, 341, 10063, 11, 1310, 257, 707, 857, 8830, 11, 300, 498, 257, 2539, 9284, 575, 1090, 12577, 11, 50724, 50724, 1242, 544, 3097, 1412, 486, 406, 538, 2564, 854, 300, 709, 13, 50950, 50950, 400, 286, 458, 300, 321, 434, 1143, 281, 1953, 300, 1419, 544, 1412, 307, 665, 13, 51200, 51200, 583, 498, 428, 9284, 575, 1090, 12577, 11, 550, 498, 264, 787, 551, 291, 360, 307, 3507, 544, 3097, 1412, 412, 309, 11, 51556, 51556, 300, 538, 2564, 486, 406, 1562, 718, 291, 1565, 760, 264, 6713, 3314, 300, 709, 13, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.07359719276428223, "compression_ratio": 1.8114035087719298, "no_speech_prob": 3.966795247833943e-06}, {"id": 76, "seek": 38552, "start": 409.35999999999996, "end": 412.91999999999996, "text": " that by itself will not ever let you bring down the error rate that much.", "tokens": [50364, 407, 300, 2709, 341, 10063, 11, 1310, 257, 707, 857, 8830, 11, 300, 498, 257, 2539, 9284, 575, 1090, 12577, 11, 50724, 50724, 1242, 544, 3097, 1412, 486, 406, 538, 2564, 854, 300, 709, 13, 50950, 50950, 400, 286, 458, 300, 321, 434, 1143, 281, 1953, 300, 1419, 544, 1412, 307, 665, 13, 51200, 51200, 583, 498, 428, 9284, 575, 1090, 12577, 11, 550, 498, 264, 787, 551, 291, 360, 307, 3507, 544, 3097, 1412, 412, 309, 11, 51556, 51556, 300, 538, 2564, 486, 406, 1562, 718, 291, 1565, 760, 264, 6713, 3314, 300, 709, 13, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.07359719276428223, "compression_ratio": 1.8114035087719298, "no_speech_prob": 3.966795247833943e-06}, {"id": 77, "seek": 41292, "start": 412.92, "end": 417.24, "text": " And it's because of this really, no matter how many more examples you add to this figure,", "tokens": [50364, 400, 309, 311, 570, 295, 341, 534, 11, 572, 1871, 577, 867, 544, 5110, 291, 909, 281, 341, 2573, 11, 50580, 50580, 264, 2997, 2539, 15669, 445, 1943, 380, 516, 281, 483, 300, 709, 1101, 13, 50806, 50806, 400, 300, 311, 983, 949, 10978, 257, 688, 295, 4630, 666, 12510, 544, 3097, 1412, 11, 51064, 51064, 309, 311, 3163, 8568, 498, 428, 2539, 9284, 575, 1090, 12577, 11, 570, 498, 309, 775, 11, 51288, 51288, 550, 291, 1391, 643, 281, 360, 512, 661, 721, 661, 813, 445, 3507, 544, 3097, 1412, 412, 309, 13, 51594, 51594], "temperature": 0.0, "avg_logprob": -0.09235460904179787, "compression_ratio": 1.7154471544715446, "no_speech_prob": 7.2961220212164335e-06}, {"id": 78, "seek": 41292, "start": 417.24, "end": 421.76, "text": " the straight learning fitting just isn't going to get that much better.", "tokens": [50364, 400, 309, 311, 570, 295, 341, 534, 11, 572, 1871, 577, 867, 544, 5110, 291, 909, 281, 341, 2573, 11, 50580, 50580, 264, 2997, 2539, 15669, 445, 1943, 380, 516, 281, 483, 300, 709, 1101, 13, 50806, 50806, 400, 300, 311, 983, 949, 10978, 257, 688, 295, 4630, 666, 12510, 544, 3097, 1412, 11, 51064, 51064, 309, 311, 3163, 8568, 498, 428, 2539, 9284, 575, 1090, 12577, 11, 570, 498, 309, 775, 11, 51288, 51288, 550, 291, 1391, 643, 281, 360, 512, 661, 721, 661, 813, 445, 3507, 544, 3097, 1412, 412, 309, 13, 51594, 51594], "temperature": 0.0, "avg_logprob": -0.09235460904179787, "compression_ratio": 1.7154471544715446, "no_speech_prob": 7.2961220212164335e-06}, {"id": 79, "seek": 41292, "start": 421.76, "end": 426.92, "text": " And that's why before investing a lot of effort into collecting more training data,", "tokens": [50364, 400, 309, 311, 570, 295, 341, 534, 11, 572, 1871, 577, 867, 544, 5110, 291, 909, 281, 341, 2573, 11, 50580, 50580, 264, 2997, 2539, 15669, 445, 1943, 380, 516, 281, 483, 300, 709, 1101, 13, 50806, 50806, 400, 300, 311, 983, 949, 10978, 257, 688, 295, 4630, 666, 12510, 544, 3097, 1412, 11, 51064, 51064, 309, 311, 3163, 8568, 498, 428, 2539, 9284, 575, 1090, 12577, 11, 570, 498, 309, 775, 11, 51288, 51288, 550, 291, 1391, 643, 281, 360, 512, 661, 721, 661, 813, 445, 3507, 544, 3097, 1412, 412, 309, 13, 51594, 51594], "temperature": 0.0, "avg_logprob": -0.09235460904179787, "compression_ratio": 1.7154471544715446, "no_speech_prob": 7.2961220212164335e-06}, {"id": 80, "seek": 41292, "start": 426.92, "end": 431.40000000000003, "text": " it's worth checking if your learning algorithm has high bias, because if it does,", "tokens": [50364, 400, 309, 311, 570, 295, 341, 534, 11, 572, 1871, 577, 867, 544, 5110, 291, 909, 281, 341, 2573, 11, 50580, 50580, 264, 2997, 2539, 15669, 445, 1943, 380, 516, 281, 483, 300, 709, 1101, 13, 50806, 50806, 400, 300, 311, 983, 949, 10978, 257, 688, 295, 4630, 666, 12510, 544, 3097, 1412, 11, 51064, 51064, 309, 311, 3163, 8568, 498, 428, 2539, 9284, 575, 1090, 12577, 11, 570, 498, 309, 775, 11, 51288, 51288, 550, 291, 1391, 643, 281, 360, 512, 661, 721, 661, 813, 445, 3507, 544, 3097, 1412, 412, 309, 13, 51594, 51594], "temperature": 0.0, "avg_logprob": -0.09235460904179787, "compression_ratio": 1.7154471544715446, "no_speech_prob": 7.2961220212164335e-06}, {"id": 81, "seek": 41292, "start": 431.40000000000003, "end": 437.52000000000004, "text": " then you probably need to do some other things other than just throw more training data at it.", "tokens": [50364, 400, 309, 311, 570, 295, 341, 534, 11, 572, 1871, 577, 867, 544, 5110, 291, 909, 281, 341, 2573, 11, 50580, 50580, 264, 2997, 2539, 15669, 445, 1943, 380, 516, 281, 483, 300, 709, 1101, 13, 50806, 50806, 400, 300, 311, 983, 949, 10978, 257, 688, 295, 4630, 666, 12510, 544, 3097, 1412, 11, 51064, 51064, 309, 311, 3163, 8568, 498, 428, 2539, 9284, 575, 1090, 12577, 11, 570, 498, 309, 775, 11, 51288, 51288, 550, 291, 1391, 643, 281, 360, 512, 661, 721, 661, 813, 445, 3507, 544, 3097, 1412, 412, 309, 13, 51594, 51594], "temperature": 0.0, "avg_logprob": -0.09235460904179787, "compression_ratio": 1.7154471544715446, "no_speech_prob": 7.2961220212164335e-06}, {"id": 82, "seek": 43752, "start": 437.52, "end": 443.0, "text": " Let's now take a look at what the learning curve looks like for learning algorithm with high variance.", "tokens": [50364, 961, 311, 586, 747, 257, 574, 412, 437, 264, 2539, 7605, 1542, 411, 337, 2539, 9284, 365, 1090, 21977, 13, 50638, 50638, 509, 1062, 1604, 300, 498, 291, 645, 281, 3318, 257, 1451, 12, 18353, 26110, 365, 1359, 13607, 11, 50934, 50934, 584, 11, 420, 754, 13607, 6915, 4018, 11, 550, 291, 483, 257, 7605, 300, 1542, 411, 341, 13, 51158, 51158, 400, 754, 1673, 309, 9001, 264, 3097, 1412, 588, 731, 11, 309, 1177, 380, 2674, 1125, 13, 51432, 51432, 961, 311, 586, 574, 412, 437, 257, 2539, 7605, 1062, 574, 411, 294, 341, 1090, 21977, 9005, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.09772581267125398, "compression_ratio": 1.75, "no_speech_prob": 9.66566312854411e-06}, {"id": 83, "seek": 43752, "start": 443.0, "end": 448.91999999999996, "text": " You might remember that if you were to fit a four-fold polynomial with small lambda,", "tokens": [50364, 961, 311, 586, 747, 257, 574, 412, 437, 264, 2539, 7605, 1542, 411, 337, 2539, 9284, 365, 1090, 21977, 13, 50638, 50638, 509, 1062, 1604, 300, 498, 291, 645, 281, 3318, 257, 1451, 12, 18353, 26110, 365, 1359, 13607, 11, 50934, 50934, 584, 11, 420, 754, 13607, 6915, 4018, 11, 550, 291, 483, 257, 7605, 300, 1542, 411, 341, 13, 51158, 51158, 400, 754, 1673, 309, 9001, 264, 3097, 1412, 588, 731, 11, 309, 1177, 380, 2674, 1125, 13, 51432, 51432, 961, 311, 586, 574, 412, 437, 257, 2539, 7605, 1062, 574, 411, 294, 341, 1090, 21977, 9005, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.09772581267125398, "compression_ratio": 1.75, "no_speech_prob": 9.66566312854411e-06}, {"id": 84, "seek": 43752, "start": 448.91999999999996, "end": 453.4, "text": " say, or even lambda equals zero, then you get a curve that looks like this.", "tokens": [50364, 961, 311, 586, 747, 257, 574, 412, 437, 264, 2539, 7605, 1542, 411, 337, 2539, 9284, 365, 1090, 21977, 13, 50638, 50638, 509, 1062, 1604, 300, 498, 291, 645, 281, 3318, 257, 1451, 12, 18353, 26110, 365, 1359, 13607, 11, 50934, 50934, 584, 11, 420, 754, 13607, 6915, 4018, 11, 550, 291, 483, 257, 7605, 300, 1542, 411, 341, 13, 51158, 51158, 400, 754, 1673, 309, 9001, 264, 3097, 1412, 588, 731, 11, 309, 1177, 380, 2674, 1125, 13, 51432, 51432, 961, 311, 586, 574, 412, 437, 257, 2539, 7605, 1062, 574, 411, 294, 341, 1090, 21977, 9005, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.09772581267125398, "compression_ratio": 1.75, "no_speech_prob": 9.66566312854411e-06}, {"id": 85, "seek": 43752, "start": 453.4, "end": 458.88, "text": " And even though it fits the training data very well, it doesn't generalize.", "tokens": [50364, 961, 311, 586, 747, 257, 574, 412, 437, 264, 2539, 7605, 1542, 411, 337, 2539, 9284, 365, 1090, 21977, 13, 50638, 50638, 509, 1062, 1604, 300, 498, 291, 645, 281, 3318, 257, 1451, 12, 18353, 26110, 365, 1359, 13607, 11, 50934, 50934, 584, 11, 420, 754, 13607, 6915, 4018, 11, 550, 291, 483, 257, 7605, 300, 1542, 411, 341, 13, 51158, 51158, 400, 754, 1673, 309, 9001, 264, 3097, 1412, 588, 731, 11, 309, 1177, 380, 2674, 1125, 13, 51432, 51432, 961, 311, 586, 574, 412, 437, 257, 2539, 7605, 1062, 574, 411, 294, 341, 1090, 21977, 9005, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.09772581267125398, "compression_ratio": 1.75, "no_speech_prob": 9.66566312854411e-06}, {"id": 86, "seek": 43752, "start": 458.88, "end": 464.96, "text": " Let's now look at what a learning curve might look like in this high variance scenario.", "tokens": [50364, 961, 311, 586, 747, 257, 574, 412, 437, 264, 2539, 7605, 1542, 411, 337, 2539, 9284, 365, 1090, 21977, 13, 50638, 50638, 509, 1062, 1604, 300, 498, 291, 645, 281, 3318, 257, 1451, 12, 18353, 26110, 365, 1359, 13607, 11, 50934, 50934, 584, 11, 420, 754, 13607, 6915, 4018, 11, 550, 291, 483, 257, 7605, 300, 1542, 411, 341, 13, 51158, 51158, 400, 754, 1673, 309, 9001, 264, 3097, 1412, 588, 731, 11, 309, 1177, 380, 2674, 1125, 13, 51432, 51432, 961, 311, 586, 574, 412, 437, 257, 2539, 7605, 1062, 574, 411, 294, 341, 1090, 21977, 9005, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.09772581267125398, "compression_ratio": 1.75, "no_speech_prob": 9.66566312854411e-06}, {"id": 87, "seek": 46496, "start": 464.96, "end": 470.2, "text": " JTrain will be going up as the training set size increases,", "tokens": [50364, 508, 51, 7146, 486, 312, 516, 493, 382, 264, 3097, 992, 2744, 8637, 11, 50626, 50626, 370, 291, 483, 257, 7605, 300, 1542, 411, 341, 11, 293, 49802, 53, 486, 312, 709, 2946, 13, 50924, 50924, 407, 428, 3278, 12, 3337, 327, 399, 6713, 307, 709, 2946, 813, 428, 3097, 6713, 13, 51132, 51132, 400, 264, 1186, 456, 311, 257, 2603, 7417, 510, 307, 437, 393, 980, 291, 300, 341, 1090, 21977, 307, 884, 709, 1101, 51468, 51468, 322, 264, 3097, 992, 813, 309, 311, 884, 322, 428, 3278, 12, 3337, 327, 399, 992, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.12200583833636659, "compression_ratio": 1.770334928229665, "no_speech_prob": 1.384574261464877e-05}, {"id": 88, "seek": 46496, "start": 470.2, "end": 476.15999999999997, "text": " so you get a curve that looks like this, and JCV will be much higher.", "tokens": [50364, 508, 51, 7146, 486, 312, 516, 493, 382, 264, 3097, 992, 2744, 8637, 11, 50626, 50626, 370, 291, 483, 257, 7605, 300, 1542, 411, 341, 11, 293, 49802, 53, 486, 312, 709, 2946, 13, 50924, 50924, 407, 428, 3278, 12, 3337, 327, 399, 6713, 307, 709, 2946, 813, 428, 3097, 6713, 13, 51132, 51132, 400, 264, 1186, 456, 311, 257, 2603, 7417, 510, 307, 437, 393, 980, 291, 300, 341, 1090, 21977, 307, 884, 709, 1101, 51468, 51468, 322, 264, 3097, 992, 813, 309, 311, 884, 322, 428, 3278, 12, 3337, 327, 399, 992, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.12200583833636659, "compression_ratio": 1.770334928229665, "no_speech_prob": 1.384574261464877e-05}, {"id": 89, "seek": 46496, "start": 476.15999999999997, "end": 480.32, "text": " So your cross-validation error is much higher than your training error.", "tokens": [50364, 508, 51, 7146, 486, 312, 516, 493, 382, 264, 3097, 992, 2744, 8637, 11, 50626, 50626, 370, 291, 483, 257, 7605, 300, 1542, 411, 341, 11, 293, 49802, 53, 486, 312, 709, 2946, 13, 50924, 50924, 407, 428, 3278, 12, 3337, 327, 399, 6713, 307, 709, 2946, 813, 428, 3097, 6713, 13, 51132, 51132, 400, 264, 1186, 456, 311, 257, 2603, 7417, 510, 307, 437, 393, 980, 291, 300, 341, 1090, 21977, 307, 884, 709, 1101, 51468, 51468, 322, 264, 3097, 992, 813, 309, 311, 884, 322, 428, 3278, 12, 3337, 327, 399, 992, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.12200583833636659, "compression_ratio": 1.770334928229665, "no_speech_prob": 1.384574261464877e-05}, {"id": 90, "seek": 46496, "start": 480.32, "end": 487.03999999999996, "text": " And the fact there's a huge gap here is what can tell you that this high variance is doing much better", "tokens": [50364, 508, 51, 7146, 486, 312, 516, 493, 382, 264, 3097, 992, 2744, 8637, 11, 50626, 50626, 370, 291, 483, 257, 7605, 300, 1542, 411, 341, 11, 293, 49802, 53, 486, 312, 709, 2946, 13, 50924, 50924, 407, 428, 3278, 12, 3337, 327, 399, 6713, 307, 709, 2946, 813, 428, 3097, 6713, 13, 51132, 51132, 400, 264, 1186, 456, 311, 257, 2603, 7417, 510, 307, 437, 393, 980, 291, 300, 341, 1090, 21977, 307, 884, 709, 1101, 51468, 51468, 322, 264, 3097, 992, 813, 309, 311, 884, 322, 428, 3278, 12, 3337, 327, 399, 992, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.12200583833636659, "compression_ratio": 1.770334928229665, "no_speech_prob": 1.384574261464877e-05}, {"id": 91, "seek": 46496, "start": 487.03999999999996, "end": 491.24, "text": " on the training set than it's doing on your cross-validation set.", "tokens": [50364, 508, 51, 7146, 486, 312, 516, 493, 382, 264, 3097, 992, 2744, 8637, 11, 50626, 50626, 370, 291, 483, 257, 7605, 300, 1542, 411, 341, 11, 293, 49802, 53, 486, 312, 709, 2946, 13, 50924, 50924, 407, 428, 3278, 12, 3337, 327, 399, 6713, 307, 709, 2946, 813, 428, 3097, 6713, 13, 51132, 51132, 400, 264, 1186, 456, 311, 257, 2603, 7417, 510, 307, 437, 393, 980, 291, 300, 341, 1090, 21977, 307, 884, 709, 1101, 51468, 51468, 322, 264, 3097, 992, 813, 309, 311, 884, 322, 428, 3278, 12, 3337, 327, 399, 992, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.12200583833636659, "compression_ratio": 1.770334928229665, "no_speech_prob": 1.384574261464877e-05}, {"id": 92, "seek": 49124, "start": 491.24, "end": 496.08, "text": " If you were to plot a baseline level of performance, such as human level performance,", "tokens": [50364, 759, 291, 645, 281, 7542, 257, 20518, 1496, 295, 3389, 11, 1270, 382, 1952, 1496, 3389, 11, 50606, 50606, 291, 815, 915, 300, 309, 4523, 484, 281, 312, 510, 300, 508, 51, 7146, 393, 2171, 312, 754, 3126, 813, 264, 1952, 1496, 295, 3389, 11, 50998, 50998, 420, 1310, 1952, 1496, 3389, 307, 257, 707, 857, 3126, 813, 341, 13, 51180, 51180, 583, 562, 291, 434, 670, 69, 2414, 264, 3097, 992, 11, 291, 815, 312, 1075, 281, 3318, 264, 3097, 992, 370, 731, 51474, 51474, 281, 362, 364, 25754, 20458, 2295, 6713, 11, 1270, 382, 4018, 6713, 294, 341, 1365, 670, 510, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.07554325351008663, "compression_ratio": 1.8983050847457628, "no_speech_prob": 7.690157417528098e-07}, {"id": 93, "seek": 49124, "start": 496.08, "end": 503.92, "text": " you may find that it turns out to be here that JTrain can sometimes be even lower than the human level of performance,", "tokens": [50364, 759, 291, 645, 281, 7542, 257, 20518, 1496, 295, 3389, 11, 1270, 382, 1952, 1496, 3389, 11, 50606, 50606, 291, 815, 915, 300, 309, 4523, 484, 281, 312, 510, 300, 508, 51, 7146, 393, 2171, 312, 754, 3126, 813, 264, 1952, 1496, 295, 3389, 11, 50998, 50998, 420, 1310, 1952, 1496, 3389, 307, 257, 707, 857, 3126, 813, 341, 13, 51180, 51180, 583, 562, 291, 434, 670, 69, 2414, 264, 3097, 992, 11, 291, 815, 312, 1075, 281, 3318, 264, 3097, 992, 370, 731, 51474, 51474, 281, 362, 364, 25754, 20458, 2295, 6713, 11, 1270, 382, 4018, 6713, 294, 341, 1365, 670, 510, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.07554325351008663, "compression_ratio": 1.8983050847457628, "no_speech_prob": 7.690157417528098e-07}, {"id": 94, "seek": 49124, "start": 503.92, "end": 507.56, "text": " or maybe human level performance is a little bit lower than this.", "tokens": [50364, 759, 291, 645, 281, 7542, 257, 20518, 1496, 295, 3389, 11, 1270, 382, 1952, 1496, 3389, 11, 50606, 50606, 291, 815, 915, 300, 309, 4523, 484, 281, 312, 510, 300, 508, 51, 7146, 393, 2171, 312, 754, 3126, 813, 264, 1952, 1496, 295, 3389, 11, 50998, 50998, 420, 1310, 1952, 1496, 3389, 307, 257, 707, 857, 3126, 813, 341, 13, 51180, 51180, 583, 562, 291, 434, 670, 69, 2414, 264, 3097, 992, 11, 291, 815, 312, 1075, 281, 3318, 264, 3097, 992, 370, 731, 51474, 51474, 281, 362, 364, 25754, 20458, 2295, 6713, 11, 1270, 382, 4018, 6713, 294, 341, 1365, 670, 510, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.07554325351008663, "compression_ratio": 1.8983050847457628, "no_speech_prob": 7.690157417528098e-07}, {"id": 95, "seek": 49124, "start": 507.56, "end": 513.44, "text": " But when you're overfitting the training set, you may be able to fit the training set so well", "tokens": [50364, 759, 291, 645, 281, 7542, 257, 20518, 1496, 295, 3389, 11, 1270, 382, 1952, 1496, 3389, 11, 50606, 50606, 291, 815, 915, 300, 309, 4523, 484, 281, 312, 510, 300, 508, 51, 7146, 393, 2171, 312, 754, 3126, 813, 264, 1952, 1496, 295, 3389, 11, 50998, 50998, 420, 1310, 1952, 1496, 3389, 307, 257, 707, 857, 3126, 813, 341, 13, 51180, 51180, 583, 562, 291, 434, 670, 69, 2414, 264, 3097, 992, 11, 291, 815, 312, 1075, 281, 3318, 264, 3097, 992, 370, 731, 51474, 51474, 281, 362, 364, 25754, 20458, 2295, 6713, 11, 1270, 382, 4018, 6713, 294, 341, 1365, 670, 510, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.07554325351008663, "compression_ratio": 1.8983050847457628, "no_speech_prob": 7.690157417528098e-07}, {"id": 96, "seek": 49124, "start": 513.44, "end": 518.64, "text": " to have an unrealistically low error, such as zero error in this example over here,", "tokens": [50364, 759, 291, 645, 281, 7542, 257, 20518, 1496, 295, 3389, 11, 1270, 382, 1952, 1496, 3389, 11, 50606, 50606, 291, 815, 915, 300, 309, 4523, 484, 281, 312, 510, 300, 508, 51, 7146, 393, 2171, 312, 754, 3126, 813, 264, 1952, 1496, 295, 3389, 11, 50998, 50998, 420, 1310, 1952, 1496, 3389, 307, 257, 707, 857, 3126, 813, 341, 13, 51180, 51180, 583, 562, 291, 434, 670, 69, 2414, 264, 3097, 992, 11, 291, 815, 312, 1075, 281, 3318, 264, 3097, 992, 370, 731, 51474, 51474, 281, 362, 364, 25754, 20458, 2295, 6713, 11, 1270, 382, 4018, 6713, 294, 341, 1365, 670, 510, 11, 51734, 51734], "temperature": 0.0, "avg_logprob": -0.07554325351008663, "compression_ratio": 1.8983050847457628, "no_speech_prob": 7.690157417528098e-07}, {"id": 97, "seek": 51864, "start": 518.64, "end": 524.3199999999999, "text": " which is actually better than how well humans would actually be able to predict housing prices", "tokens": [50364, 597, 307, 767, 1101, 813, 577, 731, 6255, 576, 767, 312, 1075, 281, 6069, 6849, 7901, 50648, 50648, 420, 2035, 264, 3861, 291, 434, 1364, 322, 13, 50740, 50740, 583, 797, 11, 264, 6358, 337, 1090, 21977, 307, 1968, 49802, 53, 307, 709, 2946, 813, 508, 51, 7146, 13, 51070, 51070, 400, 562, 291, 362, 1090, 21977, 11, 550, 5662, 264, 3097, 992, 2744, 727, 854, 257, 688, 13, 51424, 51424, 400, 294, 1729, 11, 498, 321, 727, 48224, 473, 613, 19490, 281, 264, 558, 11, 3488, 37333, 7146, 11, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.09439636798615152, "compression_ratio": 1.646090534979424, "no_speech_prob": 8.315050195051299e-07}, {"id": 98, "seek": 51864, "start": 524.3199999999999, "end": 526.16, "text": " or whatever the application you're working on.", "tokens": [50364, 597, 307, 767, 1101, 813, 577, 731, 6255, 576, 767, 312, 1075, 281, 6069, 6849, 7901, 50648, 50648, 420, 2035, 264, 3861, 291, 434, 1364, 322, 13, 50740, 50740, 583, 797, 11, 264, 6358, 337, 1090, 21977, 307, 1968, 49802, 53, 307, 709, 2946, 813, 508, 51, 7146, 13, 51070, 51070, 400, 562, 291, 362, 1090, 21977, 11, 550, 5662, 264, 3097, 992, 2744, 727, 854, 257, 688, 13, 51424, 51424, 400, 294, 1729, 11, 498, 321, 727, 48224, 473, 613, 19490, 281, 264, 558, 11, 3488, 37333, 7146, 11, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.09439636798615152, "compression_ratio": 1.646090534979424, "no_speech_prob": 8.315050195051299e-07}, {"id": 99, "seek": 51864, "start": 526.16, "end": 532.76, "text": " But again, the signal for high variance is whether JCV is much higher than JTrain.", "tokens": [50364, 597, 307, 767, 1101, 813, 577, 731, 6255, 576, 767, 312, 1075, 281, 6069, 6849, 7901, 50648, 50648, 420, 2035, 264, 3861, 291, 434, 1364, 322, 13, 50740, 50740, 583, 797, 11, 264, 6358, 337, 1090, 21977, 307, 1968, 49802, 53, 307, 709, 2946, 813, 508, 51, 7146, 13, 51070, 51070, 400, 562, 291, 362, 1090, 21977, 11, 550, 5662, 264, 3097, 992, 2744, 727, 854, 257, 688, 13, 51424, 51424, 400, 294, 1729, 11, 498, 321, 727, 48224, 473, 613, 19490, 281, 264, 558, 11, 3488, 37333, 7146, 11, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.09439636798615152, "compression_ratio": 1.646090534979424, "no_speech_prob": 8.315050195051299e-07}, {"id": 100, "seek": 51864, "start": 532.76, "end": 539.84, "text": " And when you have high variance, then increasing the training set size could help a lot.", "tokens": [50364, 597, 307, 767, 1101, 813, 577, 731, 6255, 576, 767, 312, 1075, 281, 6069, 6849, 7901, 50648, 50648, 420, 2035, 264, 3861, 291, 434, 1364, 322, 13, 50740, 50740, 583, 797, 11, 264, 6358, 337, 1090, 21977, 307, 1968, 49802, 53, 307, 709, 2946, 813, 508, 51, 7146, 13, 51070, 51070, 400, 562, 291, 362, 1090, 21977, 11, 550, 5662, 264, 3097, 992, 2744, 727, 854, 257, 688, 13, 51424, 51424, 400, 294, 1729, 11, 498, 321, 727, 48224, 473, 613, 19490, 281, 264, 558, 11, 3488, 37333, 7146, 11, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.09439636798615152, "compression_ratio": 1.646090534979424, "no_speech_prob": 8.315050195051299e-07}, {"id": 101, "seek": 51864, "start": 539.84, "end": 545.3199999999999, "text": " And in particular, if we could extrapolate these curves to the right, increase MTrain,", "tokens": [50364, 597, 307, 767, 1101, 813, 577, 731, 6255, 576, 767, 312, 1075, 281, 6069, 6849, 7901, 50648, 50648, 420, 2035, 264, 3861, 291, 434, 1364, 322, 13, 50740, 50740, 583, 797, 11, 264, 6358, 337, 1090, 21977, 307, 1968, 49802, 53, 307, 709, 2946, 813, 508, 51, 7146, 13, 51070, 51070, 400, 562, 291, 362, 1090, 21977, 11, 550, 5662, 264, 3097, 992, 2744, 727, 854, 257, 688, 13, 51424, 51424, 400, 294, 1729, 11, 498, 321, 727, 48224, 473, 613, 19490, 281, 264, 558, 11, 3488, 37333, 7146, 11, 51698, 51698], "temperature": 0.0, "avg_logprob": -0.09439636798615152, "compression_ratio": 1.646090534979424, "no_speech_prob": 8.315050195051299e-07}, {"id": 102, "seek": 54532, "start": 545.32, "end": 551.48, "text": " then the training error will continue to go up, but then the cross-validation error", "tokens": [50364, 550, 264, 3097, 6713, 486, 2354, 281, 352, 493, 11, 457, 550, 264, 3278, 12, 3337, 327, 399, 6713, 50672, 50672, 4696, 486, 808, 760, 293, 3109, 508, 51, 7146, 13, 50908, 50908, 400, 370, 294, 341, 9005, 11, 309, 1062, 312, 1944, 445, 538, 5662, 264, 3097, 992, 2744, 51258, 51258, 281, 3126, 264, 3278, 12, 3337, 327, 399, 6713, 293, 281, 483, 428, 9284, 281, 2042, 1101, 293, 1101, 13, 51536, 51536, 400, 341, 307, 8343, 264, 1090, 12577, 1389, 689, 498, 264, 787, 551, 291, 360, 307, 483, 544, 3097, 1412, 11, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.08935398525661892, "compression_ratio": 1.748917748917749, "no_speech_prob": 8.713932970749738e-07}, {"id": 103, "seek": 54532, "start": 551.48, "end": 556.2, "text": " hopefully will come down and approach JTrain.", "tokens": [50364, 550, 264, 3097, 6713, 486, 2354, 281, 352, 493, 11, 457, 550, 264, 3278, 12, 3337, 327, 399, 6713, 50672, 50672, 4696, 486, 808, 760, 293, 3109, 508, 51, 7146, 13, 50908, 50908, 400, 370, 294, 341, 9005, 11, 309, 1062, 312, 1944, 445, 538, 5662, 264, 3097, 992, 2744, 51258, 51258, 281, 3126, 264, 3278, 12, 3337, 327, 399, 6713, 293, 281, 483, 428, 9284, 281, 2042, 1101, 293, 1101, 13, 51536, 51536, 400, 341, 307, 8343, 264, 1090, 12577, 1389, 689, 498, 264, 787, 551, 291, 360, 307, 483, 544, 3097, 1412, 11, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.08935398525661892, "compression_ratio": 1.748917748917749, "no_speech_prob": 8.713932970749738e-07}, {"id": 104, "seek": 54532, "start": 556.2, "end": 563.2, "text": " And so in this scenario, it might be possible just by increasing the training set size", "tokens": [50364, 550, 264, 3097, 6713, 486, 2354, 281, 352, 493, 11, 457, 550, 264, 3278, 12, 3337, 327, 399, 6713, 50672, 50672, 4696, 486, 808, 760, 293, 3109, 508, 51, 7146, 13, 50908, 50908, 400, 370, 294, 341, 9005, 11, 309, 1062, 312, 1944, 445, 538, 5662, 264, 3097, 992, 2744, 51258, 51258, 281, 3126, 264, 3278, 12, 3337, 327, 399, 6713, 293, 281, 483, 428, 9284, 281, 2042, 1101, 293, 1101, 13, 51536, 51536, 400, 341, 307, 8343, 264, 1090, 12577, 1389, 689, 498, 264, 787, 551, 291, 360, 307, 483, 544, 3097, 1412, 11, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.08935398525661892, "compression_ratio": 1.748917748917749, "no_speech_prob": 8.713932970749738e-07}, {"id": 105, "seek": 54532, "start": 563.2, "end": 568.7600000000001, "text": " to lower the cross-validation error and to get your algorithm to perform better and better.", "tokens": [50364, 550, 264, 3097, 6713, 486, 2354, 281, 352, 493, 11, 457, 550, 264, 3278, 12, 3337, 327, 399, 6713, 50672, 50672, 4696, 486, 808, 760, 293, 3109, 508, 51, 7146, 13, 50908, 50908, 400, 370, 294, 341, 9005, 11, 309, 1062, 312, 1944, 445, 538, 5662, 264, 3097, 992, 2744, 51258, 51258, 281, 3126, 264, 3278, 12, 3337, 327, 399, 6713, 293, 281, 483, 428, 9284, 281, 2042, 1101, 293, 1101, 13, 51536, 51536, 400, 341, 307, 8343, 264, 1090, 12577, 1389, 689, 498, 264, 787, 551, 291, 360, 307, 483, 544, 3097, 1412, 11, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.08935398525661892, "compression_ratio": 1.748917748917749, "no_speech_prob": 8.713932970749738e-07}, {"id": 106, "seek": 54532, "start": 568.7600000000001, "end": 574.72, "text": " And this is unlike the high bias case where if the only thing you do is get more training data,", "tokens": [50364, 550, 264, 3097, 6713, 486, 2354, 281, 352, 493, 11, 457, 550, 264, 3278, 12, 3337, 327, 399, 6713, 50672, 50672, 4696, 486, 808, 760, 293, 3109, 508, 51, 7146, 13, 50908, 50908, 400, 370, 294, 341, 9005, 11, 309, 1062, 312, 1944, 445, 538, 5662, 264, 3097, 992, 2744, 51258, 51258, 281, 3126, 264, 3278, 12, 3337, 327, 399, 6713, 293, 281, 483, 428, 9284, 281, 2042, 1101, 293, 1101, 13, 51536, 51536, 400, 341, 307, 8343, 264, 1090, 12577, 1389, 689, 498, 264, 787, 551, 291, 360, 307, 483, 544, 3097, 1412, 11, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.08935398525661892, "compression_ratio": 1.748917748917749, "no_speech_prob": 8.713932970749738e-07}, {"id": 107, "seek": 57472, "start": 574.72, "end": 578.2, "text": " that won't actually help your learning algorithm performance much.", "tokens": [50364, 300, 1582, 380, 767, 854, 428, 2539, 9284, 3389, 709, 13, 50538, 50538, 407, 281, 20858, 11, 498, 257, 2539, 9284, 33776, 490, 1090, 21977, 11, 50766, 50766, 550, 1242, 544, 3097, 1412, 307, 6451, 3700, 281, 854, 13, 51000, 51000, 1436, 48224, 990, 281, 264, 558, 295, 341, 7605, 11, 291, 536, 300, 291, 393, 2066, 49802, 53, 281, 1066, 322, 1348, 760, 13, 51318, 51318, 400, 294, 341, 1365, 11, 445, 538, 1242, 544, 3097, 1412, 4045, 264, 9284, 281, 352, 490, 341, 51570, 51570], "temperature": 0.0, "avg_logprob": -0.10007322504279319, "compression_ratio": 1.6824034334763949, "no_speech_prob": 7.182639819802716e-06}, {"id": 108, "seek": 57472, "start": 578.2, "end": 582.76, "text": " So to summarize, if a learning algorithm suffers from high variance,", "tokens": [50364, 300, 1582, 380, 767, 854, 428, 2539, 9284, 3389, 709, 13, 50538, 50538, 407, 281, 20858, 11, 498, 257, 2539, 9284, 33776, 490, 1090, 21977, 11, 50766, 50766, 550, 1242, 544, 3097, 1412, 307, 6451, 3700, 281, 854, 13, 51000, 51000, 1436, 48224, 990, 281, 264, 558, 295, 341, 7605, 11, 291, 536, 300, 291, 393, 2066, 49802, 53, 281, 1066, 322, 1348, 760, 13, 51318, 51318, 400, 294, 341, 1365, 11, 445, 538, 1242, 544, 3097, 1412, 4045, 264, 9284, 281, 352, 490, 341, 51570, 51570], "temperature": 0.0, "avg_logprob": -0.10007322504279319, "compression_ratio": 1.6824034334763949, "no_speech_prob": 7.182639819802716e-06}, {"id": 109, "seek": 57472, "start": 582.76, "end": 587.44, "text": " then getting more training data is indeed likely to help.", "tokens": [50364, 300, 1582, 380, 767, 854, 428, 2539, 9284, 3389, 709, 13, 50538, 50538, 407, 281, 20858, 11, 498, 257, 2539, 9284, 33776, 490, 1090, 21977, 11, 50766, 50766, 550, 1242, 544, 3097, 1412, 307, 6451, 3700, 281, 854, 13, 51000, 51000, 1436, 48224, 990, 281, 264, 558, 295, 341, 7605, 11, 291, 536, 300, 291, 393, 2066, 49802, 53, 281, 1066, 322, 1348, 760, 13, 51318, 51318, 400, 294, 341, 1365, 11, 445, 538, 1242, 544, 3097, 1412, 4045, 264, 9284, 281, 352, 490, 341, 51570, 51570], "temperature": 0.0, "avg_logprob": -0.10007322504279319, "compression_ratio": 1.6824034334763949, "no_speech_prob": 7.182639819802716e-06}, {"id": 110, "seek": 57472, "start": 587.44, "end": 593.8000000000001, "text": " Because extrapolating to the right of this curve, you see that you can expect JCV to keep on coming down.", "tokens": [50364, 300, 1582, 380, 767, 854, 428, 2539, 9284, 3389, 709, 13, 50538, 50538, 407, 281, 20858, 11, 498, 257, 2539, 9284, 33776, 490, 1090, 21977, 11, 50766, 50766, 550, 1242, 544, 3097, 1412, 307, 6451, 3700, 281, 854, 13, 51000, 51000, 1436, 48224, 990, 281, 264, 558, 295, 341, 7605, 11, 291, 536, 300, 291, 393, 2066, 49802, 53, 281, 1066, 322, 1348, 760, 13, 51318, 51318, 400, 294, 341, 1365, 11, 445, 538, 1242, 544, 3097, 1412, 4045, 264, 9284, 281, 352, 490, 341, 51570, 51570], "temperature": 0.0, "avg_logprob": -0.10007322504279319, "compression_ratio": 1.6824034334763949, "no_speech_prob": 7.182639819802716e-06}, {"id": 111, "seek": 57472, "start": 593.8000000000001, "end": 598.84, "text": " And in this example, just by getting more training data allows the algorithm to go from this", "tokens": [50364, 300, 1582, 380, 767, 854, 428, 2539, 9284, 3389, 709, 13, 50538, 50538, 407, 281, 20858, 11, 498, 257, 2539, 9284, 33776, 490, 1090, 21977, 11, 50766, 50766, 550, 1242, 544, 3097, 1412, 307, 6451, 3700, 281, 854, 13, 51000, 51000, 1436, 48224, 990, 281, 264, 558, 295, 341, 7605, 11, 291, 536, 300, 291, 393, 2066, 49802, 53, 281, 1066, 322, 1348, 760, 13, 51318, 51318, 400, 294, 341, 1365, 11, 445, 538, 1242, 544, 3097, 1412, 4045, 264, 9284, 281, 352, 490, 341, 51570, 51570], "temperature": 0.0, "avg_logprob": -0.10007322504279319, "compression_ratio": 1.6824034334763949, "no_speech_prob": 7.182639819802716e-06}, {"id": 112, "seek": 59884, "start": 598.84, "end": 604.84, "text": " relatively high cross-validation error to get much closer to human level performance.", "tokens": [50364, 7226, 1090, 3278, 12, 3337, 327, 399, 6713, 281, 483, 709, 4966, 281, 1952, 1496, 3389, 13, 50664, 50664, 509, 393, 536, 300, 498, 291, 645, 281, 909, 257, 688, 544, 3097, 5110, 293, 2354, 281, 3318, 257, 1451, 12, 18353, 260, 26110, 11, 50978, 50978, 550, 291, 393, 445, 483, 257, 1101, 1451, 12, 18353, 260, 26110, 3318, 281, 341, 1412, 813, 445, 588, 5336, 356, 7605, 493, 322, 1192, 13, 51364, 51364, 407, 498, 291, 434, 2390, 257, 3479, 2539, 3861, 11, 291, 727, 7542, 264, 2539, 7605, 498, 291, 528, 13, 51648, 51648, 663, 307, 11, 291, 393, 747, 819, 2090, 1385, 295, 428, 3097, 6352, 13, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.1164914515980503, "compression_ratio": 1.7669172932330828, "no_speech_prob": 3.288620291641564e-06}, {"id": 113, "seek": 59884, "start": 604.84, "end": 611.12, "text": " You can see that if you were to add a lot more training examples and continue to fit a four-folder polynomial,", "tokens": [50364, 7226, 1090, 3278, 12, 3337, 327, 399, 6713, 281, 483, 709, 4966, 281, 1952, 1496, 3389, 13, 50664, 50664, 509, 393, 536, 300, 498, 291, 645, 281, 909, 257, 688, 544, 3097, 5110, 293, 2354, 281, 3318, 257, 1451, 12, 18353, 260, 26110, 11, 50978, 50978, 550, 291, 393, 445, 483, 257, 1101, 1451, 12, 18353, 260, 26110, 3318, 281, 341, 1412, 813, 445, 588, 5336, 356, 7605, 493, 322, 1192, 13, 51364, 51364, 407, 498, 291, 434, 2390, 257, 3479, 2539, 3861, 11, 291, 727, 7542, 264, 2539, 7605, 498, 291, 528, 13, 51648, 51648, 663, 307, 11, 291, 393, 747, 819, 2090, 1385, 295, 428, 3097, 6352, 13, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.1164914515980503, "compression_ratio": 1.7669172932330828, "no_speech_prob": 3.288620291641564e-06}, {"id": 114, "seek": 59884, "start": 611.12, "end": 618.84, "text": " then you can just get a better four-folder polynomial fit to this data than just very weakly curve up on top.", "tokens": [50364, 7226, 1090, 3278, 12, 3337, 327, 399, 6713, 281, 483, 709, 4966, 281, 1952, 1496, 3389, 13, 50664, 50664, 509, 393, 536, 300, 498, 291, 645, 281, 909, 257, 688, 544, 3097, 5110, 293, 2354, 281, 3318, 257, 1451, 12, 18353, 260, 26110, 11, 50978, 50978, 550, 291, 393, 445, 483, 257, 1101, 1451, 12, 18353, 260, 26110, 3318, 281, 341, 1412, 813, 445, 588, 5336, 356, 7605, 493, 322, 1192, 13, 51364, 51364, 407, 498, 291, 434, 2390, 257, 3479, 2539, 3861, 11, 291, 727, 7542, 264, 2539, 7605, 498, 291, 528, 13, 51648, 51648, 663, 307, 11, 291, 393, 747, 819, 2090, 1385, 295, 428, 3097, 6352, 13, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.1164914515980503, "compression_ratio": 1.7669172932330828, "no_speech_prob": 3.288620291641564e-06}, {"id": 115, "seek": 59884, "start": 618.84, "end": 624.52, "text": " So if you're building a machine learning application, you could plot the learning curve if you want.", "tokens": [50364, 7226, 1090, 3278, 12, 3337, 327, 399, 6713, 281, 483, 709, 4966, 281, 1952, 1496, 3389, 13, 50664, 50664, 509, 393, 536, 300, 498, 291, 645, 281, 909, 257, 688, 544, 3097, 5110, 293, 2354, 281, 3318, 257, 1451, 12, 18353, 260, 26110, 11, 50978, 50978, 550, 291, 393, 445, 483, 257, 1101, 1451, 12, 18353, 260, 26110, 3318, 281, 341, 1412, 813, 445, 588, 5336, 356, 7605, 493, 322, 1192, 13, 51364, 51364, 407, 498, 291, 434, 2390, 257, 3479, 2539, 3861, 11, 291, 727, 7542, 264, 2539, 7605, 498, 291, 528, 13, 51648, 51648, 663, 307, 11, 291, 393, 747, 819, 2090, 1385, 295, 428, 3097, 6352, 13, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.1164914515980503, "compression_ratio": 1.7669172932330828, "no_speech_prob": 3.288620291641564e-06}, {"id": 116, "seek": 59884, "start": 624.52, "end": 627.4000000000001, "text": " That is, you can take different subsets of your training sets.", "tokens": [50364, 7226, 1090, 3278, 12, 3337, 327, 399, 6713, 281, 483, 709, 4966, 281, 1952, 1496, 3389, 13, 50664, 50664, 509, 393, 536, 300, 498, 291, 645, 281, 909, 257, 688, 544, 3097, 5110, 293, 2354, 281, 3318, 257, 1451, 12, 18353, 260, 26110, 11, 50978, 50978, 550, 291, 393, 445, 483, 257, 1101, 1451, 12, 18353, 260, 26110, 3318, 281, 341, 1412, 813, 445, 588, 5336, 356, 7605, 493, 322, 1192, 13, 51364, 51364, 407, 498, 291, 434, 2390, 257, 3479, 2539, 3861, 11, 291, 727, 7542, 264, 2539, 7605, 498, 291, 528, 13, 51648, 51648, 663, 307, 11, 291, 393, 747, 819, 2090, 1385, 295, 428, 3097, 6352, 13, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.1164914515980503, "compression_ratio": 1.7669172932330828, "no_speech_prob": 3.288620291641564e-06}, {"id": 117, "seek": 62740, "start": 627.4, "end": 633.6, "text": " And even if you have, say, a thousand training examples, you could train a model on just a hundred training examples", "tokens": [50364, 400, 754, 498, 291, 362, 11, 584, 11, 257, 4714, 3097, 5110, 11, 291, 727, 3847, 257, 2316, 322, 445, 257, 3262, 3097, 5110, 50674, 50674, 293, 574, 412, 264, 3097, 6713, 293, 264, 3278, 12, 3337, 327, 399, 6713, 11, 550, 3847, 257, 2316, 322, 2331, 5110, 11, 50958, 50958, 5061, 484, 13083, 5110, 293, 445, 406, 1228, 552, 337, 586, 11, 293, 7542, 508, 3847, 293, 49802, 53, 293, 370, 322, 293, 7149, 51290, 51290, 293, 7542, 484, 437, 264, 2539, 7605, 1542, 411, 13, 51442, 51442, 400, 498, 291, 645, 281, 23273, 309, 300, 636, 11, 550, 300, 727, 312, 1071, 636, 337, 291, 281, 536, 498, 428, 2539, 7605, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.11576787834493522, "compression_ratio": 1.916, "no_speech_prob": 9.223127563018352e-06}, {"id": 118, "seek": 62740, "start": 633.6, "end": 639.28, "text": " and look at the training error and the cross-validation error, then train a model on 200 examples,", "tokens": [50364, 400, 754, 498, 291, 362, 11, 584, 11, 257, 4714, 3097, 5110, 11, 291, 727, 3847, 257, 2316, 322, 445, 257, 3262, 3097, 5110, 50674, 50674, 293, 574, 412, 264, 3097, 6713, 293, 264, 3278, 12, 3337, 327, 399, 6713, 11, 550, 3847, 257, 2316, 322, 2331, 5110, 11, 50958, 50958, 5061, 484, 13083, 5110, 293, 445, 406, 1228, 552, 337, 586, 11, 293, 7542, 508, 3847, 293, 49802, 53, 293, 370, 322, 293, 7149, 51290, 51290, 293, 7542, 484, 437, 264, 2539, 7605, 1542, 411, 13, 51442, 51442, 400, 498, 291, 645, 281, 23273, 309, 300, 636, 11, 550, 300, 727, 312, 1071, 636, 337, 291, 281, 536, 498, 428, 2539, 7605, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.11576787834493522, "compression_ratio": 1.916, "no_speech_prob": 9.223127563018352e-06}, {"id": 119, "seek": 62740, "start": 639.28, "end": 645.92, "text": " holding out 800 examples and just not using them for now, and plot J train and JCV and so on and repeat", "tokens": [50364, 400, 754, 498, 291, 362, 11, 584, 11, 257, 4714, 3097, 5110, 11, 291, 727, 3847, 257, 2316, 322, 445, 257, 3262, 3097, 5110, 50674, 50674, 293, 574, 412, 264, 3097, 6713, 293, 264, 3278, 12, 3337, 327, 399, 6713, 11, 550, 3847, 257, 2316, 322, 2331, 5110, 11, 50958, 50958, 5061, 484, 13083, 5110, 293, 445, 406, 1228, 552, 337, 586, 11, 293, 7542, 508, 3847, 293, 49802, 53, 293, 370, 322, 293, 7149, 51290, 51290, 293, 7542, 484, 437, 264, 2539, 7605, 1542, 411, 13, 51442, 51442, 400, 498, 291, 645, 281, 23273, 309, 300, 636, 11, 550, 300, 727, 312, 1071, 636, 337, 291, 281, 536, 498, 428, 2539, 7605, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.11576787834493522, "compression_ratio": 1.916, "no_speech_prob": 9.223127563018352e-06}, {"id": 120, "seek": 62740, "start": 645.92, "end": 648.9599999999999, "text": " and plot out what the learning curve looks like.", "tokens": [50364, 400, 754, 498, 291, 362, 11, 584, 11, 257, 4714, 3097, 5110, 11, 291, 727, 3847, 257, 2316, 322, 445, 257, 3262, 3097, 5110, 50674, 50674, 293, 574, 412, 264, 3097, 6713, 293, 264, 3278, 12, 3337, 327, 399, 6713, 11, 550, 3847, 257, 2316, 322, 2331, 5110, 11, 50958, 50958, 5061, 484, 13083, 5110, 293, 445, 406, 1228, 552, 337, 586, 11, 293, 7542, 508, 3847, 293, 49802, 53, 293, 370, 322, 293, 7149, 51290, 51290, 293, 7542, 484, 437, 264, 2539, 7605, 1542, 411, 13, 51442, 51442, 400, 498, 291, 645, 281, 23273, 309, 300, 636, 11, 550, 300, 727, 312, 1071, 636, 337, 291, 281, 536, 498, 428, 2539, 7605, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.11576787834493522, "compression_ratio": 1.916, "no_speech_prob": 9.223127563018352e-06}, {"id": 121, "seek": 62740, "start": 648.9599999999999, "end": 655.0, "text": " And if you were to visualize it that way, then that could be another way for you to see if your learning curve", "tokens": [50364, 400, 754, 498, 291, 362, 11, 584, 11, 257, 4714, 3097, 5110, 11, 291, 727, 3847, 257, 2316, 322, 445, 257, 3262, 3097, 5110, 50674, 50674, 293, 574, 412, 264, 3097, 6713, 293, 264, 3278, 12, 3337, 327, 399, 6713, 11, 550, 3847, 257, 2316, 322, 2331, 5110, 11, 50958, 50958, 5061, 484, 13083, 5110, 293, 445, 406, 1228, 552, 337, 586, 11, 293, 7542, 508, 3847, 293, 49802, 53, 293, 370, 322, 293, 7149, 51290, 51290, 293, 7542, 484, 437, 264, 2539, 7605, 1542, 411, 13, 51442, 51442, 400, 498, 291, 645, 281, 23273, 309, 300, 636, 11, 550, 300, 727, 312, 1071, 636, 337, 291, 281, 536, 498, 428, 2539, 7605, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.11576787834493522, "compression_ratio": 1.916, "no_speech_prob": 9.223127563018352e-06}, {"id": 122, "seek": 65500, "start": 655.0, "end": 658.36, "text": " looks more like a high bias or high variance one.", "tokens": [50364, 1542, 544, 411, 257, 1090, 12577, 420, 1090, 21977, 472, 13, 50532, 50532, 1485, 25060, 295, 41178, 2539, 19490, 411, 341, 307, 746, 286, 600, 1096, 11, 50688, 50688, 457, 472, 25060, 307, 309, 307, 24903, 379, 1596, 5124, 281, 3847, 370, 867, 819, 5245, 50980, 50980, 1228, 819, 2744, 2090, 1385, 295, 428, 3097, 992, 13, 51142, 51142, 407, 294, 3124, 11, 309, 1943, 380, 1096, 300, 2049, 13, 51284, 51284, 583, 26756, 11, 286, 915, 300, 1419, 341, 4973, 5056, 3036, 294, 452, 1378, 295, 437, 264, 3097, 992, 1542, 411, 11, 51584, 51584, 2171, 300, 3665, 385, 281, 519, 807, 437, 286, 519, 452, 2539, 9284, 307, 884, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0827983275703762, "compression_ratio": 1.7003367003367003, "no_speech_prob": 1.816165195123176e-06}, {"id": 123, "seek": 65500, "start": 658.36, "end": 661.48, "text": " One downside of plotting learning curves like this is something I've done,", "tokens": [50364, 1542, 544, 411, 257, 1090, 12577, 420, 1090, 21977, 472, 13, 50532, 50532, 1485, 25060, 295, 41178, 2539, 19490, 411, 341, 307, 746, 286, 600, 1096, 11, 50688, 50688, 457, 472, 25060, 307, 309, 307, 24903, 379, 1596, 5124, 281, 3847, 370, 867, 819, 5245, 50980, 50980, 1228, 819, 2744, 2090, 1385, 295, 428, 3097, 992, 13, 51142, 51142, 407, 294, 3124, 11, 309, 1943, 380, 1096, 300, 2049, 13, 51284, 51284, 583, 26756, 11, 286, 915, 300, 1419, 341, 4973, 5056, 3036, 294, 452, 1378, 295, 437, 264, 3097, 992, 1542, 411, 11, 51584, 51584, 2171, 300, 3665, 385, 281, 519, 807, 437, 286, 519, 452, 2539, 9284, 307, 884, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0827983275703762, "compression_ratio": 1.7003367003367003, "no_speech_prob": 1.816165195123176e-06}, {"id": 124, "seek": 65500, "start": 661.48, "end": 667.32, "text": " but one downside is it is computationally quite expensive to train so many different models", "tokens": [50364, 1542, 544, 411, 257, 1090, 12577, 420, 1090, 21977, 472, 13, 50532, 50532, 1485, 25060, 295, 41178, 2539, 19490, 411, 341, 307, 746, 286, 600, 1096, 11, 50688, 50688, 457, 472, 25060, 307, 309, 307, 24903, 379, 1596, 5124, 281, 3847, 370, 867, 819, 5245, 50980, 50980, 1228, 819, 2744, 2090, 1385, 295, 428, 3097, 992, 13, 51142, 51142, 407, 294, 3124, 11, 309, 1943, 380, 1096, 300, 2049, 13, 51284, 51284, 583, 26756, 11, 286, 915, 300, 1419, 341, 4973, 5056, 3036, 294, 452, 1378, 295, 437, 264, 3097, 992, 1542, 411, 11, 51584, 51584, 2171, 300, 3665, 385, 281, 519, 807, 437, 286, 519, 452, 2539, 9284, 307, 884, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0827983275703762, "compression_ratio": 1.7003367003367003, "no_speech_prob": 1.816165195123176e-06}, {"id": 125, "seek": 65500, "start": 667.32, "end": 670.56, "text": " using different size subsets of your training set.", "tokens": [50364, 1542, 544, 411, 257, 1090, 12577, 420, 1090, 21977, 472, 13, 50532, 50532, 1485, 25060, 295, 41178, 2539, 19490, 411, 341, 307, 746, 286, 600, 1096, 11, 50688, 50688, 457, 472, 25060, 307, 309, 307, 24903, 379, 1596, 5124, 281, 3847, 370, 867, 819, 5245, 50980, 50980, 1228, 819, 2744, 2090, 1385, 295, 428, 3097, 992, 13, 51142, 51142, 407, 294, 3124, 11, 309, 1943, 380, 1096, 300, 2049, 13, 51284, 51284, 583, 26756, 11, 286, 915, 300, 1419, 341, 4973, 5056, 3036, 294, 452, 1378, 295, 437, 264, 3097, 992, 1542, 411, 11, 51584, 51584, 2171, 300, 3665, 385, 281, 519, 807, 437, 286, 519, 452, 2539, 9284, 307, 884, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0827983275703762, "compression_ratio": 1.7003367003367003, "no_speech_prob": 1.816165195123176e-06}, {"id": 126, "seek": 65500, "start": 670.56, "end": 673.4, "text": " So in practice, it isn't done that often.", "tokens": [50364, 1542, 544, 411, 257, 1090, 12577, 420, 1090, 21977, 472, 13, 50532, 50532, 1485, 25060, 295, 41178, 2539, 19490, 411, 341, 307, 746, 286, 600, 1096, 11, 50688, 50688, 457, 472, 25060, 307, 309, 307, 24903, 379, 1596, 5124, 281, 3847, 370, 867, 819, 5245, 50980, 50980, 1228, 819, 2744, 2090, 1385, 295, 428, 3097, 992, 13, 51142, 51142, 407, 294, 3124, 11, 309, 1943, 380, 1096, 300, 2049, 13, 51284, 51284, 583, 26756, 11, 286, 915, 300, 1419, 341, 4973, 5056, 3036, 294, 452, 1378, 295, 437, 264, 3097, 992, 1542, 411, 11, 51584, 51584, 2171, 300, 3665, 385, 281, 519, 807, 437, 286, 519, 452, 2539, 9284, 307, 884, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0827983275703762, "compression_ratio": 1.7003367003367003, "no_speech_prob": 1.816165195123176e-06}, {"id": 127, "seek": 65500, "start": 673.4, "end": 679.4, "text": " But nonetheless, I find that having this mental visual picture in my head of what the training set looks like,", "tokens": [50364, 1542, 544, 411, 257, 1090, 12577, 420, 1090, 21977, 472, 13, 50532, 50532, 1485, 25060, 295, 41178, 2539, 19490, 411, 341, 307, 746, 286, 600, 1096, 11, 50688, 50688, 457, 472, 25060, 307, 309, 307, 24903, 379, 1596, 5124, 281, 3847, 370, 867, 819, 5245, 50980, 50980, 1228, 819, 2744, 2090, 1385, 295, 428, 3097, 992, 13, 51142, 51142, 407, 294, 3124, 11, 309, 1943, 380, 1096, 300, 2049, 13, 51284, 51284, 583, 26756, 11, 286, 915, 300, 1419, 341, 4973, 5056, 3036, 294, 452, 1378, 295, 437, 264, 3097, 992, 1542, 411, 11, 51584, 51584, 2171, 300, 3665, 385, 281, 519, 807, 437, 286, 519, 452, 2539, 9284, 307, 884, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0827983275703762, "compression_ratio": 1.7003367003367003, "no_speech_prob": 1.816165195123176e-06}, {"id": 128, "seek": 65500, "start": 679.4, "end": 684.0, "text": " sometimes that helps me to think through what I think my learning algorithm is doing", "tokens": [50364, 1542, 544, 411, 257, 1090, 12577, 420, 1090, 21977, 472, 13, 50532, 50532, 1485, 25060, 295, 41178, 2539, 19490, 411, 341, 307, 746, 286, 600, 1096, 11, 50688, 50688, 457, 472, 25060, 307, 309, 307, 24903, 379, 1596, 5124, 281, 3847, 370, 867, 819, 5245, 50980, 50980, 1228, 819, 2744, 2090, 1385, 295, 428, 3097, 992, 13, 51142, 51142, 407, 294, 3124, 11, 309, 1943, 380, 1096, 300, 2049, 13, 51284, 51284, 583, 26756, 11, 286, 915, 300, 1419, 341, 4973, 5056, 3036, 294, 452, 1378, 295, 437, 264, 3097, 992, 1542, 411, 11, 51584, 51584, 2171, 300, 3665, 385, 281, 519, 807, 437, 286, 519, 452, 2539, 9284, 307, 884, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0827983275703762, "compression_ratio": 1.7003367003367003, "no_speech_prob": 1.816165195123176e-06}, {"id": 129, "seek": 68400, "start": 684.0, "end": 687.8, "text": " and whether it has high bias or high variance.", "tokens": [50364, 293, 1968, 309, 575, 1090, 12577, 420, 1090, 21977, 13, 50554, 50554, 407, 286, 458, 321, 600, 2780, 807, 257, 688, 466, 12577, 293, 21977, 13, 50758, 50758, 961, 311, 352, 646, 281, 527, 3071, 1365, 295, 498, 291, 362, 8895, 257, 2316, 337, 6849, 3218, 17630, 11, 51038, 51038, 577, 775, 12577, 293, 21977, 854, 291, 4536, 437, 281, 360, 958, 30, 51226, 51226, 961, 311, 352, 646, 281, 300, 3071, 1365, 11, 597, 286, 1454, 486, 586, 652, 257, 688, 544, 2020, 281, 291, 13, 51456, 51456, 961, 311, 360, 300, 294, 264, 958, 960, 13, 51526], "temperature": 0.0, "avg_logprob": -0.10308831281001025, "compression_ratio": 1.7092511013215859, "no_speech_prob": 1.593453998793848e-05}, {"id": 130, "seek": 68400, "start": 687.8, "end": 691.88, "text": " So I know we've gone through a lot about bias and variance.", "tokens": [50364, 293, 1968, 309, 575, 1090, 12577, 420, 1090, 21977, 13, 50554, 50554, 407, 286, 458, 321, 600, 2780, 807, 257, 688, 466, 12577, 293, 21977, 13, 50758, 50758, 961, 311, 352, 646, 281, 527, 3071, 1365, 295, 498, 291, 362, 8895, 257, 2316, 337, 6849, 3218, 17630, 11, 51038, 51038, 577, 775, 12577, 293, 21977, 854, 291, 4536, 437, 281, 360, 958, 30, 51226, 51226, 961, 311, 352, 646, 281, 300, 3071, 1365, 11, 597, 286, 1454, 486, 586, 652, 257, 688, 544, 2020, 281, 291, 13, 51456, 51456, 961, 311, 360, 300, 294, 264, 958, 960, 13, 51526], "temperature": 0.0, "avg_logprob": -0.10308831281001025, "compression_ratio": 1.7092511013215859, "no_speech_prob": 1.593453998793848e-05}, {"id": 131, "seek": 68400, "start": 691.88, "end": 697.48, "text": " Let's go back to our earlier example of if you have trained a model for housing price prediction,", "tokens": [50364, 293, 1968, 309, 575, 1090, 12577, 420, 1090, 21977, 13, 50554, 50554, 407, 286, 458, 321, 600, 2780, 807, 257, 688, 466, 12577, 293, 21977, 13, 50758, 50758, 961, 311, 352, 646, 281, 527, 3071, 1365, 295, 498, 291, 362, 8895, 257, 2316, 337, 6849, 3218, 17630, 11, 51038, 51038, 577, 775, 12577, 293, 21977, 854, 291, 4536, 437, 281, 360, 958, 30, 51226, 51226, 961, 311, 352, 646, 281, 300, 3071, 1365, 11, 597, 286, 1454, 486, 586, 652, 257, 688, 544, 2020, 281, 291, 13, 51456, 51456, 961, 311, 360, 300, 294, 264, 958, 960, 13, 51526], "temperature": 0.0, "avg_logprob": -0.10308831281001025, "compression_ratio": 1.7092511013215859, "no_speech_prob": 1.593453998793848e-05}, {"id": 132, "seek": 68400, "start": 697.48, "end": 701.24, "text": " how does bias and variance help you decide what to do next?", "tokens": [50364, 293, 1968, 309, 575, 1090, 12577, 420, 1090, 21977, 13, 50554, 50554, 407, 286, 458, 321, 600, 2780, 807, 257, 688, 466, 12577, 293, 21977, 13, 50758, 50758, 961, 311, 352, 646, 281, 527, 3071, 1365, 295, 498, 291, 362, 8895, 257, 2316, 337, 6849, 3218, 17630, 11, 51038, 51038, 577, 775, 12577, 293, 21977, 854, 291, 4536, 437, 281, 360, 958, 30, 51226, 51226, 961, 311, 352, 646, 281, 300, 3071, 1365, 11, 597, 286, 1454, 486, 586, 652, 257, 688, 544, 2020, 281, 291, 13, 51456, 51456, 961, 311, 360, 300, 294, 264, 958, 960, 13, 51526], "temperature": 0.0, "avg_logprob": -0.10308831281001025, "compression_ratio": 1.7092511013215859, "no_speech_prob": 1.593453998793848e-05}, {"id": 133, "seek": 68400, "start": 701.24, "end": 705.84, "text": " Let's go back to that earlier example, which I hope will now make a lot more sense to you.", "tokens": [50364, 293, 1968, 309, 575, 1090, 12577, 420, 1090, 21977, 13, 50554, 50554, 407, 286, 458, 321, 600, 2780, 807, 257, 688, 466, 12577, 293, 21977, 13, 50758, 50758, 961, 311, 352, 646, 281, 527, 3071, 1365, 295, 498, 291, 362, 8895, 257, 2316, 337, 6849, 3218, 17630, 11, 51038, 51038, 577, 775, 12577, 293, 21977, 854, 291, 4536, 437, 281, 360, 958, 30, 51226, 51226, 961, 311, 352, 646, 281, 300, 3071, 1365, 11, 597, 286, 1454, 486, 586, 652, 257, 688, 544, 2020, 281, 291, 13, 51456, 51456, 961, 311, 360, 300, 294, 264, 958, 960, 13, 51526], "temperature": 0.0, "avg_logprob": -0.10308831281001025, "compression_ratio": 1.7092511013215859, "no_speech_prob": 1.593453998793848e-05}, {"id": 134, "seek": 70584, "start": 705.84, "end": 715.84, "text": " Let's do that in the next video.", "tokens": [50364, 961, 311, 360, 300, 294, 264, 958, 960, 13, 50864], "temperature": 0.0, "avg_logprob": -0.33779744307200116, "compression_ratio": 0.8421052631578947, "no_speech_prob": 0.00036830067983828485}], "language": "en", "video_id": "m0QgVaFS6O4", "entity": "ML Specialization, Andrew Ng (2022)"}}