{"video_id": "ZzeDtSmrRoU", "title": "1.11 Machine Learning Overview | Cost function formula --[Machine Learning | Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 545, "views": 610, "publish_date": "11/04/2022", "timestamp": 1660953600, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " In order to implement linear regression, the first key step is for us to define something called a cost function. This is something we'll build in this video. And the cost function will tell us how well the model is doing so that we can try to get it to do better. Let's look at what this means. Recall that you have a training set that contains input features x and output targets y. And the model you're going to use to fit this training set is this linear function fwb of x equals w times x plus b. To introduce a little bit more terminology, the w and b are called the parameters of the model. In machine learning, parameters of a model are the variables you can adjust during training in order to improve the model. Sometimes, you also hear the parameters w and b referred to as coefficients or as weights. Now, let's take a look at what these parameters w and b do. Depending on the values you've chosen for w and b, you get a different function f of x, which generates a different line on the graph. And remember that we can write f of x as a shorthand for fwb of x. We're going to take a look at some plots of f of x on a chart. Maybe you're already familiar with drawing lines on charts, but even if this is a review for you, I hope this will help you build intuition on how w and b, the parameters, determine f. When w is equal to 0 and b is equal to 1.5, then f looks like this horizontal line. In this case, the function f of x is 0 times x plus 1.5, so f is always a constant value. It always predicts 1.5 for the estimated value of y. So y hat is always equal to b. And here, b is also called the y-intercept, because that's where it crosses the vertical axis or the y-axis on this graph. As a second example, if w is 0.5 and b is equal to 0, then f of x is 0.5 times x. When x is 0, the prediction is also 0. And when x is 2, then the prediction is 0.5 times 2, which is 1. So you get a line that looks like this. And notice that the slope is 0.5 divided by 1, so the value of w gives you the slope of the line, which is 0.5. And finally, if w equals 0.5 and b equals 1, then f of x is 0.5 times x plus 1. And when x is 0, then f of x equals b, which is 1, so the line intersects the vertical axis at b, the y-intercept. Also when x is 2, then f of x is 2, so the line looks like this. Again, the slope is 0.5 divided by 1, so the value of w gives you the slope, which is 0.5. Recall that you have a training set, like the one shown here, with linear regression, what you want to do is to choose values for the parameters w and b so that the straight line you get from the function f somehow fits the data well, like maybe this line shown here. And when I say that the line fits the data, visually you can think of this to mean that the line defined by f is roughly passing through or somewhat close to the training examples as compared to other possible lines that are not as close to these points. And just to remind you of some notation, a training example like this point here is defined by x super strip i, y super strip i, where y is the target. For a given input x i, the function f also makes a predicted value for y, and the value that it predicts for y is y hat i, shown here. For our choice of a model, f of x i is w times x i plus b. Stated differently, the prediction y hat i is f of w b of x i, where for the model where using f of x i is equal to w x i plus b. So now the question is, how do you find values for w and b so that the prediction y hat i is close to the true target y i for many or maybe all training examples x i y i? To answer that question, let's first take a look at how to measure how well a line fits the training data. To do that, we're going to construct our cost function. The cost function takes the prediction y hat and compares it to the target y by taking y hat minus y. This difference is called the error. We're measuring how far off the prediction is from the target. Next let's compute the square of this error. Also we're going to want to compute this term for different training examples i in the training set. So when measuring the error for example i, we'll compute this squared error term. Finally, we want to measure the error across the entire training set. In particular, let's sum up the squared errors like this. We'll sum from i equals 1, 2, 3, all the way up to m. And remember that m is the number of training examples, which is 47 for this data set. Notice that if we have more training examples, m is larger and your cost function will calculate a bigger number since it's summing over more examples. So to build a cost function that doesn't automatically get bigger as the training set size gets larger, by convention, we will compute the average squared error instead of the total squared error. And we do that by dividing by m like this. Okay, we're nearly there. Just one last thing. By convention, the cost function that machine learning people use actually divides by two times m. The extra division by two is just meant to make some of our later calculations a little bit neater. But the cost function still works whether you include this division by two or not. So this expression right here is the cost function. And we're going to write J of WB to refer to the cost function. This is also called the squared error cost function. And it's called this because you're taking the square of these error terms. In machine learning, different people will use different cost functions for different applications. But the squared error cost function is by far the most commonly used one for linear regression. And for that matter, for all regression problems, where it seems to give good results for many applications. So just as a reminder, the prediction y hat is equal to the output of the model f at x. So we can rewrite the cost function J of WB as one over two m times the sum from i equals one to m of f of x i minus y i, the quantity squared. Eventually, we're going to want to find values of W and B that make the cost function small. But before going there, let's first gain more intuition about what J of WB is really computing. At this point, you might be thinking we've done a whole lot of math to define the cost function. But what exactly is it doing? Let's go on to the next video, where we'll step through one example of what the cost function is really computing. That I hope will help you build intuition about what it means if J of WB is large versus if the cost J is small. Let's go on to the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.32, "text": " In order to implement linear regression, the first key step is for us to define something", "tokens": [50364, 682, 1668, 281, 4445, 8213, 24590, 11, 264, 700, 2141, 1823, 307, 337, 505, 281, 6964, 746, 50680, 50680, 1219, 257, 2063, 2445, 13, 50780, 50780, 639, 307, 746, 321, 603, 1322, 294, 341, 960, 13, 50904, 50904, 400, 264, 2063, 2445, 486, 980, 505, 577, 731, 264, 2316, 307, 884, 370, 300, 321, 393, 853, 281, 483, 51166, 51166, 309, 281, 360, 1101, 13, 51248, 51248, 961, 311, 574, 412, 437, 341, 1355, 13, 51348, 51348, 9647, 336, 300, 291, 362, 257, 3097, 992, 300, 8306, 4846, 4122, 2031, 293, 5598, 12911, 288, 13, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.13483159229008837, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.010324057191610336}, {"id": 1, "seek": 0, "start": 6.32, "end": 8.32, "text": " called a cost function.", "tokens": [50364, 682, 1668, 281, 4445, 8213, 24590, 11, 264, 700, 2141, 1823, 307, 337, 505, 281, 6964, 746, 50680, 50680, 1219, 257, 2063, 2445, 13, 50780, 50780, 639, 307, 746, 321, 603, 1322, 294, 341, 960, 13, 50904, 50904, 400, 264, 2063, 2445, 486, 980, 505, 577, 731, 264, 2316, 307, 884, 370, 300, 321, 393, 853, 281, 483, 51166, 51166, 309, 281, 360, 1101, 13, 51248, 51248, 961, 311, 574, 412, 437, 341, 1355, 13, 51348, 51348, 9647, 336, 300, 291, 362, 257, 3097, 992, 300, 8306, 4846, 4122, 2031, 293, 5598, 12911, 288, 13, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.13483159229008837, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.010324057191610336}, {"id": 2, "seek": 0, "start": 8.32, "end": 10.8, "text": " This is something we'll build in this video.", "tokens": [50364, 682, 1668, 281, 4445, 8213, 24590, 11, 264, 700, 2141, 1823, 307, 337, 505, 281, 6964, 746, 50680, 50680, 1219, 257, 2063, 2445, 13, 50780, 50780, 639, 307, 746, 321, 603, 1322, 294, 341, 960, 13, 50904, 50904, 400, 264, 2063, 2445, 486, 980, 505, 577, 731, 264, 2316, 307, 884, 370, 300, 321, 393, 853, 281, 483, 51166, 51166, 309, 281, 360, 1101, 13, 51248, 51248, 961, 311, 574, 412, 437, 341, 1355, 13, 51348, 51348, 9647, 336, 300, 291, 362, 257, 3097, 992, 300, 8306, 4846, 4122, 2031, 293, 5598, 12911, 288, 13, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.13483159229008837, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.010324057191610336}, {"id": 3, "seek": 0, "start": 10.8, "end": 16.04, "text": " And the cost function will tell us how well the model is doing so that we can try to get", "tokens": [50364, 682, 1668, 281, 4445, 8213, 24590, 11, 264, 700, 2141, 1823, 307, 337, 505, 281, 6964, 746, 50680, 50680, 1219, 257, 2063, 2445, 13, 50780, 50780, 639, 307, 746, 321, 603, 1322, 294, 341, 960, 13, 50904, 50904, 400, 264, 2063, 2445, 486, 980, 505, 577, 731, 264, 2316, 307, 884, 370, 300, 321, 393, 853, 281, 483, 51166, 51166, 309, 281, 360, 1101, 13, 51248, 51248, 961, 311, 574, 412, 437, 341, 1355, 13, 51348, 51348, 9647, 336, 300, 291, 362, 257, 3097, 992, 300, 8306, 4846, 4122, 2031, 293, 5598, 12911, 288, 13, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.13483159229008837, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.010324057191610336}, {"id": 4, "seek": 0, "start": 16.04, "end": 17.68, "text": " it to do better.", "tokens": [50364, 682, 1668, 281, 4445, 8213, 24590, 11, 264, 700, 2141, 1823, 307, 337, 505, 281, 6964, 746, 50680, 50680, 1219, 257, 2063, 2445, 13, 50780, 50780, 639, 307, 746, 321, 603, 1322, 294, 341, 960, 13, 50904, 50904, 400, 264, 2063, 2445, 486, 980, 505, 577, 731, 264, 2316, 307, 884, 370, 300, 321, 393, 853, 281, 483, 51166, 51166, 309, 281, 360, 1101, 13, 51248, 51248, 961, 311, 574, 412, 437, 341, 1355, 13, 51348, 51348, 9647, 336, 300, 291, 362, 257, 3097, 992, 300, 8306, 4846, 4122, 2031, 293, 5598, 12911, 288, 13, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.13483159229008837, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.010324057191610336}, {"id": 5, "seek": 0, "start": 17.68, "end": 19.68, "text": " Let's look at what this means.", "tokens": [50364, 682, 1668, 281, 4445, 8213, 24590, 11, 264, 700, 2141, 1823, 307, 337, 505, 281, 6964, 746, 50680, 50680, 1219, 257, 2063, 2445, 13, 50780, 50780, 639, 307, 746, 321, 603, 1322, 294, 341, 960, 13, 50904, 50904, 400, 264, 2063, 2445, 486, 980, 505, 577, 731, 264, 2316, 307, 884, 370, 300, 321, 393, 853, 281, 483, 51166, 51166, 309, 281, 360, 1101, 13, 51248, 51248, 961, 311, 574, 412, 437, 341, 1355, 13, 51348, 51348, 9647, 336, 300, 291, 362, 257, 3097, 992, 300, 8306, 4846, 4122, 2031, 293, 5598, 12911, 288, 13, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.13483159229008837, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.010324057191610336}, {"id": 6, "seek": 0, "start": 19.68, "end": 26.6, "text": " Recall that you have a training set that contains input features x and output targets y.", "tokens": [50364, 682, 1668, 281, 4445, 8213, 24590, 11, 264, 700, 2141, 1823, 307, 337, 505, 281, 6964, 746, 50680, 50680, 1219, 257, 2063, 2445, 13, 50780, 50780, 639, 307, 746, 321, 603, 1322, 294, 341, 960, 13, 50904, 50904, 400, 264, 2063, 2445, 486, 980, 505, 577, 731, 264, 2316, 307, 884, 370, 300, 321, 393, 853, 281, 483, 51166, 51166, 309, 281, 360, 1101, 13, 51248, 51248, 961, 311, 574, 412, 437, 341, 1355, 13, 51348, 51348, 9647, 336, 300, 291, 362, 257, 3097, 992, 300, 8306, 4846, 4122, 2031, 293, 5598, 12911, 288, 13, 51694, 51694], "temperature": 0.0, "avg_logprob": -0.13483159229008837, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.010324057191610336}, {"id": 7, "seek": 2660, "start": 26.6, "end": 33.68, "text": " And the model you're going to use to fit this training set is this linear function fwb of", "tokens": [50364, 400, 264, 2316, 291, 434, 516, 281, 764, 281, 3318, 341, 3097, 992, 307, 341, 8213, 2445, 283, 86, 65, 295, 50718, 50718, 2031, 6915, 261, 1413, 2031, 1804, 272, 13, 50900, 50900, 1407, 5366, 257, 707, 857, 544, 27575, 11, 264, 261, 293, 272, 366, 1219, 264, 9834, 295, 264, 51208, 51208, 2316, 13, 51278, 51278, 682, 3479, 2539, 11, 9834, 295, 257, 2316, 366, 264, 9102, 291, 393, 4369, 1830, 3097, 51588, 51588, 294, 1668, 281, 3470, 264, 2316, 13, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.13319645371547964, "compression_ratio": 1.6028708133971292, "no_speech_prob": 1.8342154362471774e-05}, {"id": 8, "seek": 2660, "start": 33.68, "end": 37.32, "text": " x equals w times x plus b.", "tokens": [50364, 400, 264, 2316, 291, 434, 516, 281, 764, 281, 3318, 341, 3097, 992, 307, 341, 8213, 2445, 283, 86, 65, 295, 50718, 50718, 2031, 6915, 261, 1413, 2031, 1804, 272, 13, 50900, 50900, 1407, 5366, 257, 707, 857, 544, 27575, 11, 264, 261, 293, 272, 366, 1219, 264, 9834, 295, 264, 51208, 51208, 2316, 13, 51278, 51278, 682, 3479, 2539, 11, 9834, 295, 257, 2316, 366, 264, 9102, 291, 393, 4369, 1830, 3097, 51588, 51588, 294, 1668, 281, 3470, 264, 2316, 13, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.13319645371547964, "compression_ratio": 1.6028708133971292, "no_speech_prob": 1.8342154362471774e-05}, {"id": 9, "seek": 2660, "start": 37.32, "end": 43.480000000000004, "text": " To introduce a little bit more terminology, the w and b are called the parameters of the", "tokens": [50364, 400, 264, 2316, 291, 434, 516, 281, 764, 281, 3318, 341, 3097, 992, 307, 341, 8213, 2445, 283, 86, 65, 295, 50718, 50718, 2031, 6915, 261, 1413, 2031, 1804, 272, 13, 50900, 50900, 1407, 5366, 257, 707, 857, 544, 27575, 11, 264, 261, 293, 272, 366, 1219, 264, 9834, 295, 264, 51208, 51208, 2316, 13, 51278, 51278, 682, 3479, 2539, 11, 9834, 295, 257, 2316, 366, 264, 9102, 291, 393, 4369, 1830, 3097, 51588, 51588, 294, 1668, 281, 3470, 264, 2316, 13, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.13319645371547964, "compression_ratio": 1.6028708133971292, "no_speech_prob": 1.8342154362471774e-05}, {"id": 10, "seek": 2660, "start": 43.480000000000004, "end": 44.88, "text": " model.", "tokens": [50364, 400, 264, 2316, 291, 434, 516, 281, 764, 281, 3318, 341, 3097, 992, 307, 341, 8213, 2445, 283, 86, 65, 295, 50718, 50718, 2031, 6915, 261, 1413, 2031, 1804, 272, 13, 50900, 50900, 1407, 5366, 257, 707, 857, 544, 27575, 11, 264, 261, 293, 272, 366, 1219, 264, 9834, 295, 264, 51208, 51208, 2316, 13, 51278, 51278, 682, 3479, 2539, 11, 9834, 295, 257, 2316, 366, 264, 9102, 291, 393, 4369, 1830, 3097, 51588, 51588, 294, 1668, 281, 3470, 264, 2316, 13, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.13319645371547964, "compression_ratio": 1.6028708133971292, "no_speech_prob": 1.8342154362471774e-05}, {"id": 11, "seek": 2660, "start": 44.88, "end": 51.08, "text": " In machine learning, parameters of a model are the variables you can adjust during training", "tokens": [50364, 400, 264, 2316, 291, 434, 516, 281, 764, 281, 3318, 341, 3097, 992, 307, 341, 8213, 2445, 283, 86, 65, 295, 50718, 50718, 2031, 6915, 261, 1413, 2031, 1804, 272, 13, 50900, 50900, 1407, 5366, 257, 707, 857, 544, 27575, 11, 264, 261, 293, 272, 366, 1219, 264, 9834, 295, 264, 51208, 51208, 2316, 13, 51278, 51278, 682, 3479, 2539, 11, 9834, 295, 257, 2316, 366, 264, 9102, 291, 393, 4369, 1830, 3097, 51588, 51588, 294, 1668, 281, 3470, 264, 2316, 13, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.13319645371547964, "compression_ratio": 1.6028708133971292, "no_speech_prob": 1.8342154362471774e-05}, {"id": 12, "seek": 2660, "start": 51.08, "end": 53.84, "text": " in order to improve the model.", "tokens": [50364, 400, 264, 2316, 291, 434, 516, 281, 764, 281, 3318, 341, 3097, 992, 307, 341, 8213, 2445, 283, 86, 65, 295, 50718, 50718, 2031, 6915, 261, 1413, 2031, 1804, 272, 13, 50900, 50900, 1407, 5366, 257, 707, 857, 544, 27575, 11, 264, 261, 293, 272, 366, 1219, 264, 9834, 295, 264, 51208, 51208, 2316, 13, 51278, 51278, 682, 3479, 2539, 11, 9834, 295, 257, 2316, 366, 264, 9102, 291, 393, 4369, 1830, 3097, 51588, 51588, 294, 1668, 281, 3470, 264, 2316, 13, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.13319645371547964, "compression_ratio": 1.6028708133971292, "no_speech_prob": 1.8342154362471774e-05}, {"id": 13, "seek": 5384, "start": 53.84, "end": 61.64, "text": " Sometimes, you also hear the parameters w and b referred to as coefficients or as weights.", "tokens": [50364, 4803, 11, 291, 611, 1568, 264, 9834, 261, 293, 272, 10839, 281, 382, 31994, 420, 382, 17443, 13, 50754, 50754, 823, 11, 718, 311, 747, 257, 574, 412, 437, 613, 9834, 261, 293, 272, 360, 13, 51086, 51086, 22539, 322, 264, 4190, 291, 600, 8614, 337, 261, 293, 272, 11, 291, 483, 257, 819, 2445, 283, 295, 51350, 51350, 2031, 11, 597, 23815, 257, 819, 1622, 322, 264, 4295, 13, 51550, 51550], "temperature": 0.0, "avg_logprob": -0.12202149468499261, "compression_ratio": 1.5573770491803278, "no_speech_prob": 6.540276899613673e-06}, {"id": 14, "seek": 5384, "start": 61.64, "end": 68.28, "text": " Now, let's take a look at what these parameters w and b do.", "tokens": [50364, 4803, 11, 291, 611, 1568, 264, 9834, 261, 293, 272, 10839, 281, 382, 31994, 420, 382, 17443, 13, 50754, 50754, 823, 11, 718, 311, 747, 257, 574, 412, 437, 613, 9834, 261, 293, 272, 360, 13, 51086, 51086, 22539, 322, 264, 4190, 291, 600, 8614, 337, 261, 293, 272, 11, 291, 483, 257, 819, 2445, 283, 295, 51350, 51350, 2031, 11, 597, 23815, 257, 819, 1622, 322, 264, 4295, 13, 51550, 51550], "temperature": 0.0, "avg_logprob": -0.12202149468499261, "compression_ratio": 1.5573770491803278, "no_speech_prob": 6.540276899613673e-06}, {"id": 15, "seek": 5384, "start": 68.28, "end": 73.56, "text": " Depending on the values you've chosen for w and b, you get a different function f of", "tokens": [50364, 4803, 11, 291, 611, 1568, 264, 9834, 261, 293, 272, 10839, 281, 382, 31994, 420, 382, 17443, 13, 50754, 50754, 823, 11, 718, 311, 747, 257, 574, 412, 437, 613, 9834, 261, 293, 272, 360, 13, 51086, 51086, 22539, 322, 264, 4190, 291, 600, 8614, 337, 261, 293, 272, 11, 291, 483, 257, 819, 2445, 283, 295, 51350, 51350, 2031, 11, 597, 23815, 257, 819, 1622, 322, 264, 4295, 13, 51550, 51550], "temperature": 0.0, "avg_logprob": -0.12202149468499261, "compression_ratio": 1.5573770491803278, "no_speech_prob": 6.540276899613673e-06}, {"id": 16, "seek": 5384, "start": 73.56, "end": 77.56, "text": " x, which generates a different line on the graph.", "tokens": [50364, 4803, 11, 291, 611, 1568, 264, 9834, 261, 293, 272, 10839, 281, 382, 31994, 420, 382, 17443, 13, 50754, 50754, 823, 11, 718, 311, 747, 257, 574, 412, 437, 613, 9834, 261, 293, 272, 360, 13, 51086, 51086, 22539, 322, 264, 4190, 291, 600, 8614, 337, 261, 293, 272, 11, 291, 483, 257, 819, 2445, 283, 295, 51350, 51350, 2031, 11, 597, 23815, 257, 819, 1622, 322, 264, 4295, 13, 51550, 51550], "temperature": 0.0, "avg_logprob": -0.12202149468499261, "compression_ratio": 1.5573770491803278, "no_speech_prob": 6.540276899613673e-06}, {"id": 17, "seek": 7756, "start": 77.56, "end": 84.84, "text": " And remember that we can write f of x as a shorthand for fwb of x.", "tokens": [50364, 400, 1604, 300, 321, 393, 2464, 283, 295, 2031, 382, 257, 402, 2652, 474, 337, 283, 86, 65, 295, 2031, 13, 50728, 50728, 492, 434, 516, 281, 747, 257, 574, 412, 512, 28609, 295, 283, 295, 2031, 322, 257, 6927, 13, 50986, 50986, 2704, 291, 434, 1217, 4963, 365, 6316, 3876, 322, 17767, 11, 457, 754, 498, 341, 307, 257, 3131, 51195, 51195, 337, 291, 11, 286, 1454, 341, 486, 854, 291, 1322, 24002, 322, 577, 261, 293, 272, 11, 264, 9834, 11, 6997, 51506, 51506, 283, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.09111322527346404, "compression_ratio": 1.5096153846153846, "no_speech_prob": 1.8738617200142471e-06}, {"id": 18, "seek": 7756, "start": 84.84, "end": 90.0, "text": " We're going to take a look at some plots of f of x on a chart.", "tokens": [50364, 400, 1604, 300, 321, 393, 2464, 283, 295, 2031, 382, 257, 402, 2652, 474, 337, 283, 86, 65, 295, 2031, 13, 50728, 50728, 492, 434, 516, 281, 747, 257, 574, 412, 512, 28609, 295, 283, 295, 2031, 322, 257, 6927, 13, 50986, 50986, 2704, 291, 434, 1217, 4963, 365, 6316, 3876, 322, 17767, 11, 457, 754, 498, 341, 307, 257, 3131, 51195, 51195, 337, 291, 11, 286, 1454, 341, 486, 854, 291, 1322, 24002, 322, 577, 261, 293, 272, 11, 264, 9834, 11, 6997, 51506, 51506, 283, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.09111322527346404, "compression_ratio": 1.5096153846153846, "no_speech_prob": 1.8738617200142471e-06}, {"id": 19, "seek": 7756, "start": 90.0, "end": 94.18, "text": " Maybe you're already familiar with drawing lines on charts, but even if this is a review", "tokens": [50364, 400, 1604, 300, 321, 393, 2464, 283, 295, 2031, 382, 257, 402, 2652, 474, 337, 283, 86, 65, 295, 2031, 13, 50728, 50728, 492, 434, 516, 281, 747, 257, 574, 412, 512, 28609, 295, 283, 295, 2031, 322, 257, 6927, 13, 50986, 50986, 2704, 291, 434, 1217, 4963, 365, 6316, 3876, 322, 17767, 11, 457, 754, 498, 341, 307, 257, 3131, 51195, 51195, 337, 291, 11, 286, 1454, 341, 486, 854, 291, 1322, 24002, 322, 577, 261, 293, 272, 11, 264, 9834, 11, 6997, 51506, 51506, 283, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.09111322527346404, "compression_ratio": 1.5096153846153846, "no_speech_prob": 1.8738617200142471e-06}, {"id": 20, "seek": 7756, "start": 94.18, "end": 100.4, "text": " for you, I hope this will help you build intuition on how w and b, the parameters, determine", "tokens": [50364, 400, 1604, 300, 321, 393, 2464, 283, 295, 2031, 382, 257, 402, 2652, 474, 337, 283, 86, 65, 295, 2031, 13, 50728, 50728, 492, 434, 516, 281, 747, 257, 574, 412, 512, 28609, 295, 283, 295, 2031, 322, 257, 6927, 13, 50986, 50986, 2704, 291, 434, 1217, 4963, 365, 6316, 3876, 322, 17767, 11, 457, 754, 498, 341, 307, 257, 3131, 51195, 51195, 337, 291, 11, 286, 1454, 341, 486, 854, 291, 1322, 24002, 322, 577, 261, 293, 272, 11, 264, 9834, 11, 6997, 51506, 51506, 283, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.09111322527346404, "compression_ratio": 1.5096153846153846, "no_speech_prob": 1.8738617200142471e-06}, {"id": 21, "seek": 7756, "start": 100.4, "end": 102.72, "text": " f.", "tokens": [50364, 400, 1604, 300, 321, 393, 2464, 283, 295, 2031, 382, 257, 402, 2652, 474, 337, 283, 86, 65, 295, 2031, 13, 50728, 50728, 492, 434, 516, 281, 747, 257, 574, 412, 512, 28609, 295, 283, 295, 2031, 322, 257, 6927, 13, 50986, 50986, 2704, 291, 434, 1217, 4963, 365, 6316, 3876, 322, 17767, 11, 457, 754, 498, 341, 307, 257, 3131, 51195, 51195, 337, 291, 11, 286, 1454, 341, 486, 854, 291, 1322, 24002, 322, 577, 261, 293, 272, 11, 264, 9834, 11, 6997, 51506, 51506, 283, 13, 51622, 51622], "temperature": 0.0, "avg_logprob": -0.09111322527346404, "compression_ratio": 1.5096153846153846, "no_speech_prob": 1.8738617200142471e-06}, {"id": 22, "seek": 10272, "start": 102.72, "end": 111.0, "text": " When w is equal to 0 and b is equal to 1.5, then f looks like this horizontal line.", "tokens": [50364, 1133, 261, 307, 2681, 281, 1958, 293, 272, 307, 2681, 281, 502, 13, 20, 11, 550, 283, 1542, 411, 341, 12750, 1622, 13, 50778, 50778, 682, 341, 1389, 11, 264, 2445, 283, 295, 2031, 307, 1958, 1413, 2031, 1804, 502, 13, 20, 11, 370, 283, 307, 1009, 257, 5754, 2158, 13, 51258, 51258, 467, 1009, 6069, 82, 502, 13, 20, 337, 264, 14109, 2158, 295, 288, 13, 51516, 51516, 407, 288, 2385, 307, 1009, 2681, 281, 272, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11469060618702959, "compression_ratio": 1.5670731707317074, "no_speech_prob": 1.341987808700651e-05}, {"id": 23, "seek": 10272, "start": 111.0, "end": 120.6, "text": " In this case, the function f of x is 0 times x plus 1.5, so f is always a constant value.", "tokens": [50364, 1133, 261, 307, 2681, 281, 1958, 293, 272, 307, 2681, 281, 502, 13, 20, 11, 550, 283, 1542, 411, 341, 12750, 1622, 13, 50778, 50778, 682, 341, 1389, 11, 264, 2445, 283, 295, 2031, 307, 1958, 1413, 2031, 1804, 502, 13, 20, 11, 370, 283, 307, 1009, 257, 5754, 2158, 13, 51258, 51258, 467, 1009, 6069, 82, 502, 13, 20, 337, 264, 14109, 2158, 295, 288, 13, 51516, 51516, 407, 288, 2385, 307, 1009, 2681, 281, 272, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11469060618702959, "compression_ratio": 1.5670731707317074, "no_speech_prob": 1.341987808700651e-05}, {"id": 24, "seek": 10272, "start": 120.6, "end": 125.75999999999999, "text": " It always predicts 1.5 for the estimated value of y.", "tokens": [50364, 1133, 261, 307, 2681, 281, 1958, 293, 272, 307, 2681, 281, 502, 13, 20, 11, 550, 283, 1542, 411, 341, 12750, 1622, 13, 50778, 50778, 682, 341, 1389, 11, 264, 2445, 283, 295, 2031, 307, 1958, 1413, 2031, 1804, 502, 13, 20, 11, 370, 283, 307, 1009, 257, 5754, 2158, 13, 51258, 51258, 467, 1009, 6069, 82, 502, 13, 20, 337, 264, 14109, 2158, 295, 288, 13, 51516, 51516, 407, 288, 2385, 307, 1009, 2681, 281, 272, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11469060618702959, "compression_ratio": 1.5670731707317074, "no_speech_prob": 1.341987808700651e-05}, {"id": 25, "seek": 10272, "start": 125.75999999999999, "end": 129.12, "text": " So y hat is always equal to b.", "tokens": [50364, 1133, 261, 307, 2681, 281, 1958, 293, 272, 307, 2681, 281, 502, 13, 20, 11, 550, 283, 1542, 411, 341, 12750, 1622, 13, 50778, 50778, 682, 341, 1389, 11, 264, 2445, 283, 295, 2031, 307, 1958, 1413, 2031, 1804, 502, 13, 20, 11, 370, 283, 307, 1009, 257, 5754, 2158, 13, 51258, 51258, 467, 1009, 6069, 82, 502, 13, 20, 337, 264, 14109, 2158, 295, 288, 13, 51516, 51516, 407, 288, 2385, 307, 1009, 2681, 281, 272, 13, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11469060618702959, "compression_ratio": 1.5670731707317074, "no_speech_prob": 1.341987808700651e-05}, {"id": 26, "seek": 12912, "start": 129.12, "end": 134.72, "text": " And here, b is also called the y-intercept, because that's where it crosses the vertical", "tokens": [50364, 400, 510, 11, 272, 307, 611, 1219, 264, 288, 12, 5106, 1336, 11, 570, 300, 311, 689, 309, 28467, 264, 9429, 50644, 50644, 10298, 420, 264, 288, 12, 24633, 322, 341, 4295, 13, 50844, 50844, 1018, 257, 1150, 1365, 11, 498, 261, 307, 1958, 13, 20, 293, 272, 307, 2681, 281, 1958, 11, 550, 283, 295, 2031, 307, 1958, 13, 20, 1413, 2031, 13, 51356, 51356, 1133, 2031, 307, 1958, 11, 264, 17630, 307, 611, 1958, 13, 51536, 51536, 400, 562, 2031, 307, 568, 11, 550, 264, 17630, 307, 1958, 13, 20, 1413, 568, 11, 597, 307, 502, 13, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.07801751257146447, "compression_ratio": 1.6470588235294117, "no_speech_prob": 4.222805728204548e-06}, {"id": 27, "seek": 12912, "start": 134.72, "end": 138.72, "text": " axis or the y-axis on this graph.", "tokens": [50364, 400, 510, 11, 272, 307, 611, 1219, 264, 288, 12, 5106, 1336, 11, 570, 300, 311, 689, 309, 28467, 264, 9429, 50644, 50644, 10298, 420, 264, 288, 12, 24633, 322, 341, 4295, 13, 50844, 50844, 1018, 257, 1150, 1365, 11, 498, 261, 307, 1958, 13, 20, 293, 272, 307, 2681, 281, 1958, 11, 550, 283, 295, 2031, 307, 1958, 13, 20, 1413, 2031, 13, 51356, 51356, 1133, 2031, 307, 1958, 11, 264, 17630, 307, 611, 1958, 13, 51536, 51536, 400, 562, 2031, 307, 568, 11, 550, 264, 17630, 307, 1958, 13, 20, 1413, 568, 11, 597, 307, 502, 13, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.07801751257146447, "compression_ratio": 1.6470588235294117, "no_speech_prob": 4.222805728204548e-06}, {"id": 28, "seek": 12912, "start": 138.72, "end": 148.96, "text": " As a second example, if w is 0.5 and b is equal to 0, then f of x is 0.5 times x.", "tokens": [50364, 400, 510, 11, 272, 307, 611, 1219, 264, 288, 12, 5106, 1336, 11, 570, 300, 311, 689, 309, 28467, 264, 9429, 50644, 50644, 10298, 420, 264, 288, 12, 24633, 322, 341, 4295, 13, 50844, 50844, 1018, 257, 1150, 1365, 11, 498, 261, 307, 1958, 13, 20, 293, 272, 307, 2681, 281, 1958, 11, 550, 283, 295, 2031, 307, 1958, 13, 20, 1413, 2031, 13, 51356, 51356, 1133, 2031, 307, 1958, 11, 264, 17630, 307, 611, 1958, 13, 51536, 51536, 400, 562, 2031, 307, 568, 11, 550, 264, 17630, 307, 1958, 13, 20, 1413, 568, 11, 597, 307, 502, 13, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.07801751257146447, "compression_ratio": 1.6470588235294117, "no_speech_prob": 4.222805728204548e-06}, {"id": 29, "seek": 12912, "start": 148.96, "end": 152.56, "text": " When x is 0, the prediction is also 0.", "tokens": [50364, 400, 510, 11, 272, 307, 611, 1219, 264, 288, 12, 5106, 1336, 11, 570, 300, 311, 689, 309, 28467, 264, 9429, 50644, 50644, 10298, 420, 264, 288, 12, 24633, 322, 341, 4295, 13, 50844, 50844, 1018, 257, 1150, 1365, 11, 498, 261, 307, 1958, 13, 20, 293, 272, 307, 2681, 281, 1958, 11, 550, 283, 295, 2031, 307, 1958, 13, 20, 1413, 2031, 13, 51356, 51356, 1133, 2031, 307, 1958, 11, 264, 17630, 307, 611, 1958, 13, 51536, 51536, 400, 562, 2031, 307, 568, 11, 550, 264, 17630, 307, 1958, 13, 20, 1413, 568, 11, 597, 307, 502, 13, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.07801751257146447, "compression_ratio": 1.6470588235294117, "no_speech_prob": 4.222805728204548e-06}, {"id": 30, "seek": 12912, "start": 152.56, "end": 158.52, "text": " And when x is 2, then the prediction is 0.5 times 2, which is 1.", "tokens": [50364, 400, 510, 11, 272, 307, 611, 1219, 264, 288, 12, 5106, 1336, 11, 570, 300, 311, 689, 309, 28467, 264, 9429, 50644, 50644, 10298, 420, 264, 288, 12, 24633, 322, 341, 4295, 13, 50844, 50844, 1018, 257, 1150, 1365, 11, 498, 261, 307, 1958, 13, 20, 293, 272, 307, 2681, 281, 1958, 11, 550, 283, 295, 2031, 307, 1958, 13, 20, 1413, 2031, 13, 51356, 51356, 1133, 2031, 307, 1958, 11, 264, 17630, 307, 611, 1958, 13, 51536, 51536, 400, 562, 2031, 307, 568, 11, 550, 264, 17630, 307, 1958, 13, 20, 1413, 568, 11, 597, 307, 502, 13, 51834, 51834], "temperature": 0.0, "avg_logprob": -0.07801751257146447, "compression_ratio": 1.6470588235294117, "no_speech_prob": 4.222805728204548e-06}, {"id": 31, "seek": 15852, "start": 158.52, "end": 160.88000000000002, "text": " So you get a line that looks like this.", "tokens": [50364, 407, 291, 483, 257, 1622, 300, 1542, 411, 341, 13, 50482, 50482, 400, 3449, 300, 264, 13525, 307, 1958, 13, 20, 6666, 538, 502, 11, 370, 264, 2158, 295, 261, 2709, 291, 264, 13525, 295, 50916, 50916, 264, 1622, 11, 597, 307, 1958, 13, 20, 13, 51110, 51110, 400, 2721, 11, 498, 261, 6915, 1958, 13, 20, 293, 272, 6915, 502, 11, 550, 283, 295, 2031, 307, 1958, 13, 20, 1413, 2031, 1804, 502, 13, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.08353667319575443, "compression_ratio": 1.5298013245033113, "no_speech_prob": 1.2029221579723526e-05}, {"id": 32, "seek": 15852, "start": 160.88000000000002, "end": 169.56, "text": " And notice that the slope is 0.5 divided by 1, so the value of w gives you the slope of", "tokens": [50364, 407, 291, 483, 257, 1622, 300, 1542, 411, 341, 13, 50482, 50482, 400, 3449, 300, 264, 13525, 307, 1958, 13, 20, 6666, 538, 502, 11, 370, 264, 2158, 295, 261, 2709, 291, 264, 13525, 295, 50916, 50916, 264, 1622, 11, 597, 307, 1958, 13, 20, 13, 51110, 51110, 400, 2721, 11, 498, 261, 6915, 1958, 13, 20, 293, 272, 6915, 502, 11, 550, 283, 295, 2031, 307, 1958, 13, 20, 1413, 2031, 1804, 502, 13, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.08353667319575443, "compression_ratio": 1.5298013245033113, "no_speech_prob": 1.2029221579723526e-05}, {"id": 33, "seek": 15852, "start": 169.56, "end": 173.44, "text": " the line, which is 0.5.", "tokens": [50364, 407, 291, 483, 257, 1622, 300, 1542, 411, 341, 13, 50482, 50482, 400, 3449, 300, 264, 13525, 307, 1958, 13, 20, 6666, 538, 502, 11, 370, 264, 2158, 295, 261, 2709, 291, 264, 13525, 295, 50916, 50916, 264, 1622, 11, 597, 307, 1958, 13, 20, 13, 51110, 51110, 400, 2721, 11, 498, 261, 6915, 1958, 13, 20, 293, 272, 6915, 502, 11, 550, 283, 295, 2031, 307, 1958, 13, 20, 1413, 2031, 1804, 502, 13, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.08353667319575443, "compression_ratio": 1.5298013245033113, "no_speech_prob": 1.2029221579723526e-05}, {"id": 34, "seek": 15852, "start": 173.44, "end": 184.68, "text": " And finally, if w equals 0.5 and b equals 1, then f of x is 0.5 times x plus 1.", "tokens": [50364, 407, 291, 483, 257, 1622, 300, 1542, 411, 341, 13, 50482, 50482, 400, 3449, 300, 264, 13525, 307, 1958, 13, 20, 6666, 538, 502, 11, 370, 264, 2158, 295, 261, 2709, 291, 264, 13525, 295, 50916, 50916, 264, 1622, 11, 597, 307, 1958, 13, 20, 13, 51110, 51110, 400, 2721, 11, 498, 261, 6915, 1958, 13, 20, 293, 272, 6915, 502, 11, 550, 283, 295, 2031, 307, 1958, 13, 20, 1413, 2031, 1804, 502, 13, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.08353667319575443, "compression_ratio": 1.5298013245033113, "no_speech_prob": 1.2029221579723526e-05}, {"id": 35, "seek": 18468, "start": 184.68, "end": 191.72, "text": " And when x is 0, then f of x equals b, which is 1, so the line intersects the vertical", "tokens": [50364, 400, 562, 2031, 307, 1958, 11, 550, 283, 295, 2031, 6915, 272, 11, 597, 307, 502, 11, 370, 264, 1622, 27815, 82, 264, 9429, 50716, 50716, 10298, 412, 272, 11, 264, 288, 12, 5106, 1336, 13, 50888, 50888, 2743, 562, 2031, 307, 568, 11, 550, 283, 295, 2031, 307, 568, 11, 370, 264, 1622, 1542, 411, 341, 13, 51182, 51182, 3764, 11, 264, 13525, 307, 1958, 13, 20, 6666, 538, 502, 11, 370, 264, 2158, 295, 261, 2709, 291, 264, 13525, 11, 597, 307, 1958, 13, 20, 13, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.11854355231575343, "compression_ratio": 1.6981132075471699, "no_speech_prob": 3.7266181607265025e-06}, {"id": 36, "seek": 18468, "start": 191.72, "end": 195.16, "text": " axis at b, the y-intercept.", "tokens": [50364, 400, 562, 2031, 307, 1958, 11, 550, 283, 295, 2031, 6915, 272, 11, 597, 307, 502, 11, 370, 264, 1622, 27815, 82, 264, 9429, 50716, 50716, 10298, 412, 272, 11, 264, 288, 12, 5106, 1336, 13, 50888, 50888, 2743, 562, 2031, 307, 568, 11, 550, 283, 295, 2031, 307, 568, 11, 370, 264, 1622, 1542, 411, 341, 13, 51182, 51182, 3764, 11, 264, 13525, 307, 1958, 13, 20, 6666, 538, 502, 11, 370, 264, 2158, 295, 261, 2709, 291, 264, 13525, 11, 597, 307, 1958, 13, 20, 13, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.11854355231575343, "compression_ratio": 1.6981132075471699, "no_speech_prob": 3.7266181607265025e-06}, {"id": 37, "seek": 18468, "start": 195.16, "end": 201.04000000000002, "text": " Also when x is 2, then f of x is 2, so the line looks like this.", "tokens": [50364, 400, 562, 2031, 307, 1958, 11, 550, 283, 295, 2031, 6915, 272, 11, 597, 307, 502, 11, 370, 264, 1622, 27815, 82, 264, 9429, 50716, 50716, 10298, 412, 272, 11, 264, 288, 12, 5106, 1336, 13, 50888, 50888, 2743, 562, 2031, 307, 568, 11, 550, 283, 295, 2031, 307, 568, 11, 370, 264, 1622, 1542, 411, 341, 13, 51182, 51182, 3764, 11, 264, 13525, 307, 1958, 13, 20, 6666, 538, 502, 11, 370, 264, 2158, 295, 261, 2709, 291, 264, 13525, 11, 597, 307, 1958, 13, 20, 13, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.11854355231575343, "compression_ratio": 1.6981132075471699, "no_speech_prob": 3.7266181607265025e-06}, {"id": 38, "seek": 18468, "start": 201.04000000000002, "end": 209.4, "text": " Again, the slope is 0.5 divided by 1, so the value of w gives you the slope, which is 0.5.", "tokens": [50364, 400, 562, 2031, 307, 1958, 11, 550, 283, 295, 2031, 6915, 272, 11, 597, 307, 502, 11, 370, 264, 1622, 27815, 82, 264, 9429, 50716, 50716, 10298, 412, 272, 11, 264, 288, 12, 5106, 1336, 13, 50888, 50888, 2743, 562, 2031, 307, 568, 11, 550, 283, 295, 2031, 307, 568, 11, 370, 264, 1622, 1542, 411, 341, 13, 51182, 51182, 3764, 11, 264, 13525, 307, 1958, 13, 20, 6666, 538, 502, 11, 370, 264, 2158, 295, 261, 2709, 291, 264, 13525, 11, 597, 307, 1958, 13, 20, 13, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.11854355231575343, "compression_ratio": 1.6981132075471699, "no_speech_prob": 3.7266181607265025e-06}, {"id": 39, "seek": 20940, "start": 209.4, "end": 215.36, "text": " Recall that you have a training set, like the one shown here, with linear regression,", "tokens": [50364, 9647, 336, 300, 291, 362, 257, 3097, 992, 11, 411, 264, 472, 4898, 510, 11, 365, 8213, 24590, 11, 50662, 50662, 437, 291, 528, 281, 360, 307, 281, 2826, 4190, 337, 264, 9834, 261, 293, 272, 370, 300, 264, 2997, 50910, 50910, 1622, 291, 483, 490, 264, 2445, 283, 6063, 9001, 264, 1412, 731, 11, 411, 1310, 341, 1622, 4898, 51188, 51188, 510, 13, 51258, 51258, 400, 562, 286, 584, 300, 264, 1622, 9001, 264, 1412, 11, 19622, 291, 393, 519, 295, 341, 281, 914, 300, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.11351250542534722, "compression_ratio": 1.6904761904761905, "no_speech_prob": 9.223297638527583e-06}, {"id": 40, "seek": 20940, "start": 215.36, "end": 220.32, "text": " what you want to do is to choose values for the parameters w and b so that the straight", "tokens": [50364, 9647, 336, 300, 291, 362, 257, 3097, 992, 11, 411, 264, 472, 4898, 510, 11, 365, 8213, 24590, 11, 50662, 50662, 437, 291, 528, 281, 360, 307, 281, 2826, 4190, 337, 264, 9834, 261, 293, 272, 370, 300, 264, 2997, 50910, 50910, 1622, 291, 483, 490, 264, 2445, 283, 6063, 9001, 264, 1412, 731, 11, 411, 1310, 341, 1622, 4898, 51188, 51188, 510, 13, 51258, 51258, 400, 562, 286, 584, 300, 264, 1622, 9001, 264, 1412, 11, 19622, 291, 393, 519, 295, 341, 281, 914, 300, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.11351250542534722, "compression_ratio": 1.6904761904761905, "no_speech_prob": 9.223297638527583e-06}, {"id": 41, "seek": 20940, "start": 220.32, "end": 225.88, "text": " line you get from the function f somehow fits the data well, like maybe this line shown", "tokens": [50364, 9647, 336, 300, 291, 362, 257, 3097, 992, 11, 411, 264, 472, 4898, 510, 11, 365, 8213, 24590, 11, 50662, 50662, 437, 291, 528, 281, 360, 307, 281, 2826, 4190, 337, 264, 9834, 261, 293, 272, 370, 300, 264, 2997, 50910, 50910, 1622, 291, 483, 490, 264, 2445, 283, 6063, 9001, 264, 1412, 731, 11, 411, 1310, 341, 1622, 4898, 51188, 51188, 510, 13, 51258, 51258, 400, 562, 286, 584, 300, 264, 1622, 9001, 264, 1412, 11, 19622, 291, 393, 519, 295, 341, 281, 914, 300, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.11351250542534722, "compression_ratio": 1.6904761904761905, "no_speech_prob": 9.223297638527583e-06}, {"id": 42, "seek": 20940, "start": 225.88, "end": 227.28, "text": " here.", "tokens": [50364, 9647, 336, 300, 291, 362, 257, 3097, 992, 11, 411, 264, 472, 4898, 510, 11, 365, 8213, 24590, 11, 50662, 50662, 437, 291, 528, 281, 360, 307, 281, 2826, 4190, 337, 264, 9834, 261, 293, 272, 370, 300, 264, 2997, 50910, 50910, 1622, 291, 483, 490, 264, 2445, 283, 6063, 9001, 264, 1412, 731, 11, 411, 1310, 341, 1622, 4898, 51188, 51188, 510, 13, 51258, 51258, 400, 562, 286, 584, 300, 264, 1622, 9001, 264, 1412, 11, 19622, 291, 393, 519, 295, 341, 281, 914, 300, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.11351250542534722, "compression_ratio": 1.6904761904761905, "no_speech_prob": 9.223297638527583e-06}, {"id": 43, "seek": 20940, "start": 227.28, "end": 232.76, "text": " And when I say that the line fits the data, visually you can think of this to mean that", "tokens": [50364, 9647, 336, 300, 291, 362, 257, 3097, 992, 11, 411, 264, 472, 4898, 510, 11, 365, 8213, 24590, 11, 50662, 50662, 437, 291, 528, 281, 360, 307, 281, 2826, 4190, 337, 264, 9834, 261, 293, 272, 370, 300, 264, 2997, 50910, 50910, 1622, 291, 483, 490, 264, 2445, 283, 6063, 9001, 264, 1412, 731, 11, 411, 1310, 341, 1622, 4898, 51188, 51188, 510, 13, 51258, 51258, 400, 562, 286, 584, 300, 264, 1622, 9001, 264, 1412, 11, 19622, 291, 393, 519, 295, 341, 281, 914, 300, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.11351250542534722, "compression_ratio": 1.6904761904761905, "no_speech_prob": 9.223297638527583e-06}, {"id": 44, "seek": 23276, "start": 232.76, "end": 239.88, "text": " the line defined by f is roughly passing through or somewhat close to the training examples", "tokens": [50364, 264, 1622, 7642, 538, 283, 307, 9810, 8437, 807, 420, 8344, 1998, 281, 264, 3097, 5110, 50720, 50720, 382, 5347, 281, 661, 1944, 3876, 300, 366, 406, 382, 1998, 281, 613, 2793, 13, 51016, 51016, 400, 445, 281, 4160, 291, 295, 512, 24657, 11, 257, 3097, 1365, 411, 341, 935, 510, 307, 7642, 51368, 51368, 538, 2031, 1687, 12828, 741, 11, 288, 1687, 12828, 741, 11, 689, 288, 307, 264, 3779, 13, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.08202610517802991, "compression_ratio": 1.7569060773480663, "no_speech_prob": 3.844914772344055e-06}, {"id": 45, "seek": 23276, "start": 239.88, "end": 245.79999999999998, "text": " as compared to other possible lines that are not as close to these points.", "tokens": [50364, 264, 1622, 7642, 538, 283, 307, 9810, 8437, 807, 420, 8344, 1998, 281, 264, 3097, 5110, 50720, 50720, 382, 5347, 281, 661, 1944, 3876, 300, 366, 406, 382, 1998, 281, 613, 2793, 13, 51016, 51016, 400, 445, 281, 4160, 291, 295, 512, 24657, 11, 257, 3097, 1365, 411, 341, 935, 510, 307, 7642, 51368, 51368, 538, 2031, 1687, 12828, 741, 11, 288, 1687, 12828, 741, 11, 689, 288, 307, 264, 3779, 13, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.08202610517802991, "compression_ratio": 1.7569060773480663, "no_speech_prob": 3.844914772344055e-06}, {"id": 46, "seek": 23276, "start": 245.79999999999998, "end": 252.84, "text": " And just to remind you of some notation, a training example like this point here is defined", "tokens": [50364, 264, 1622, 7642, 538, 283, 307, 9810, 8437, 807, 420, 8344, 1998, 281, 264, 3097, 5110, 50720, 50720, 382, 5347, 281, 661, 1944, 3876, 300, 366, 406, 382, 1998, 281, 613, 2793, 13, 51016, 51016, 400, 445, 281, 4160, 291, 295, 512, 24657, 11, 257, 3097, 1365, 411, 341, 935, 510, 307, 7642, 51368, 51368, 538, 2031, 1687, 12828, 741, 11, 288, 1687, 12828, 741, 11, 689, 288, 307, 264, 3779, 13, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.08202610517802991, "compression_ratio": 1.7569060773480663, "no_speech_prob": 3.844914772344055e-06}, {"id": 47, "seek": 23276, "start": 252.84, "end": 260.8, "text": " by x super strip i, y super strip i, where y is the target.", "tokens": [50364, 264, 1622, 7642, 538, 283, 307, 9810, 8437, 807, 420, 8344, 1998, 281, 264, 3097, 5110, 50720, 50720, 382, 5347, 281, 661, 1944, 3876, 300, 366, 406, 382, 1998, 281, 613, 2793, 13, 51016, 51016, 400, 445, 281, 4160, 291, 295, 512, 24657, 11, 257, 3097, 1365, 411, 341, 935, 510, 307, 7642, 51368, 51368, 538, 2031, 1687, 12828, 741, 11, 288, 1687, 12828, 741, 11, 689, 288, 307, 264, 3779, 13, 51766, 51766], "temperature": 0.0, "avg_logprob": -0.08202610517802991, "compression_ratio": 1.7569060773480663, "no_speech_prob": 3.844914772344055e-06}, {"id": 48, "seek": 26080, "start": 260.8, "end": 270.44, "text": " For a given input x i, the function f also makes a predicted value for y, and the value", "tokens": [50364, 1171, 257, 2212, 4846, 2031, 741, 11, 264, 2445, 283, 611, 1669, 257, 19147, 2158, 337, 288, 11, 293, 264, 2158, 50846, 50846, 300, 309, 6069, 82, 337, 288, 307, 288, 2385, 741, 11, 4898, 510, 13, 51084, 51084, 1171, 527, 3922, 295, 257, 2316, 11, 283, 295, 2031, 741, 307, 261, 1413, 2031, 741, 1804, 272, 13, 51398, 51398, 745, 770, 7614, 11, 264, 17630, 288, 2385, 741, 307, 283, 295, 261, 272, 295, 2031, 741, 11, 689, 337, 264, 2316, 689, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09518833818106816, "compression_ratio": 1.630057803468208, "no_speech_prob": 2.813001401591464e-06}, {"id": 49, "seek": 26080, "start": 270.44, "end": 275.2, "text": " that it predicts for y is y hat i, shown here.", "tokens": [50364, 1171, 257, 2212, 4846, 2031, 741, 11, 264, 2445, 283, 611, 1669, 257, 19147, 2158, 337, 288, 11, 293, 264, 2158, 50846, 50846, 300, 309, 6069, 82, 337, 288, 307, 288, 2385, 741, 11, 4898, 510, 13, 51084, 51084, 1171, 527, 3922, 295, 257, 2316, 11, 283, 295, 2031, 741, 307, 261, 1413, 2031, 741, 1804, 272, 13, 51398, 51398, 745, 770, 7614, 11, 264, 17630, 288, 2385, 741, 307, 283, 295, 261, 272, 295, 2031, 741, 11, 689, 337, 264, 2316, 689, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09518833818106816, "compression_ratio": 1.630057803468208, "no_speech_prob": 2.813001401591464e-06}, {"id": 50, "seek": 26080, "start": 275.2, "end": 281.48, "text": " For our choice of a model, f of x i is w times x i plus b.", "tokens": [50364, 1171, 257, 2212, 4846, 2031, 741, 11, 264, 2445, 283, 611, 1669, 257, 19147, 2158, 337, 288, 11, 293, 264, 2158, 50846, 50846, 300, 309, 6069, 82, 337, 288, 307, 288, 2385, 741, 11, 4898, 510, 13, 51084, 51084, 1171, 527, 3922, 295, 257, 2316, 11, 283, 295, 2031, 741, 307, 261, 1413, 2031, 741, 1804, 272, 13, 51398, 51398, 745, 770, 7614, 11, 264, 17630, 288, 2385, 741, 307, 283, 295, 261, 272, 295, 2031, 741, 11, 689, 337, 264, 2316, 689, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09518833818106816, "compression_ratio": 1.630057803468208, "no_speech_prob": 2.813001401591464e-06}, {"id": 51, "seek": 26080, "start": 281.48, "end": 290.72, "text": " Stated differently, the prediction y hat i is f of w b of x i, where for the model where", "tokens": [50364, 1171, 257, 2212, 4846, 2031, 741, 11, 264, 2445, 283, 611, 1669, 257, 19147, 2158, 337, 288, 11, 293, 264, 2158, 50846, 50846, 300, 309, 6069, 82, 337, 288, 307, 288, 2385, 741, 11, 4898, 510, 13, 51084, 51084, 1171, 527, 3922, 295, 257, 2316, 11, 283, 295, 2031, 741, 307, 261, 1413, 2031, 741, 1804, 272, 13, 51398, 51398, 745, 770, 7614, 11, 264, 17630, 288, 2385, 741, 307, 283, 295, 261, 272, 295, 2031, 741, 11, 689, 337, 264, 2316, 689, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.09518833818106816, "compression_ratio": 1.630057803468208, "no_speech_prob": 2.813001401591464e-06}, {"id": 52, "seek": 29072, "start": 290.72, "end": 299.76000000000005, "text": " using f of x i is equal to w x i plus b.", "tokens": [50364, 1228, 283, 295, 2031, 741, 307, 2681, 281, 261, 2031, 741, 1804, 272, 13, 50816, 50816, 407, 586, 264, 1168, 307, 11, 577, 360, 291, 915, 4190, 337, 261, 293, 272, 370, 300, 264, 17630, 288, 2385, 741, 51216, 51216, 307, 1998, 281, 264, 2074, 3779, 288, 741, 337, 867, 420, 1310, 439, 3097, 5110, 2031, 741, 288, 741, 30, 51705, 51705], "temperature": 0.0, "avg_logprob": -0.0979757159948349, "compression_ratio": 1.4452054794520548, "no_speech_prob": 2.6841555609280476e-06}, {"id": 53, "seek": 29072, "start": 299.76000000000005, "end": 307.76000000000005, "text": " So now the question is, how do you find values for w and b so that the prediction y hat i", "tokens": [50364, 1228, 283, 295, 2031, 741, 307, 2681, 281, 261, 2031, 741, 1804, 272, 13, 50816, 50816, 407, 586, 264, 1168, 307, 11, 577, 360, 291, 915, 4190, 337, 261, 293, 272, 370, 300, 264, 17630, 288, 2385, 741, 51216, 51216, 307, 1998, 281, 264, 2074, 3779, 288, 741, 337, 867, 420, 1310, 439, 3097, 5110, 2031, 741, 288, 741, 30, 51705, 51705], "temperature": 0.0, "avg_logprob": -0.0979757159948349, "compression_ratio": 1.4452054794520548, "no_speech_prob": 2.6841555609280476e-06}, {"id": 54, "seek": 29072, "start": 307.76000000000005, "end": 317.54, "text": " is close to the true target y i for many or maybe all training examples x i y i?", "tokens": [50364, 1228, 283, 295, 2031, 741, 307, 2681, 281, 261, 2031, 741, 1804, 272, 13, 50816, 50816, 407, 586, 264, 1168, 307, 11, 577, 360, 291, 915, 4190, 337, 261, 293, 272, 370, 300, 264, 17630, 288, 2385, 741, 51216, 51216, 307, 1998, 281, 264, 2074, 3779, 288, 741, 337, 867, 420, 1310, 439, 3097, 5110, 2031, 741, 288, 741, 30, 51705, 51705], "temperature": 0.0, "avg_logprob": -0.0979757159948349, "compression_ratio": 1.4452054794520548, "no_speech_prob": 2.6841555609280476e-06}, {"id": 55, "seek": 31754, "start": 317.54, "end": 323.08000000000004, "text": " To answer that question, let's first take a look at how to measure how well a line fits", "tokens": [50364, 1407, 1867, 300, 1168, 11, 718, 311, 700, 747, 257, 574, 412, 577, 281, 3481, 577, 731, 257, 1622, 9001, 50641, 50641, 264, 3097, 1412, 13, 50739, 50739, 1407, 360, 300, 11, 321, 434, 516, 281, 7690, 527, 2063, 2445, 13, 50941, 50941, 440, 2063, 2445, 2516, 264, 17630, 288, 2385, 293, 38334, 309, 281, 264, 3779, 288, 538, 1940, 51327, 51327, 288, 2385, 3175, 288, 13, 51482, 51482, 639, 2649, 307, 1219, 264, 6713, 13, 51635, 51635], "temperature": 0.0, "avg_logprob": -0.10088934898376464, "compression_ratio": 1.618279569892473, "no_speech_prob": 2.90226216748124e-06}, {"id": 56, "seek": 31754, "start": 323.08000000000004, "end": 325.04, "text": " the training data.", "tokens": [50364, 1407, 1867, 300, 1168, 11, 718, 311, 700, 747, 257, 574, 412, 577, 281, 3481, 577, 731, 257, 1622, 9001, 50641, 50641, 264, 3097, 1412, 13, 50739, 50739, 1407, 360, 300, 11, 321, 434, 516, 281, 7690, 527, 2063, 2445, 13, 50941, 50941, 440, 2063, 2445, 2516, 264, 17630, 288, 2385, 293, 38334, 309, 281, 264, 3779, 288, 538, 1940, 51327, 51327, 288, 2385, 3175, 288, 13, 51482, 51482, 639, 2649, 307, 1219, 264, 6713, 13, 51635, 51635], "temperature": 0.0, "avg_logprob": -0.10088934898376464, "compression_ratio": 1.618279569892473, "no_speech_prob": 2.90226216748124e-06}, {"id": 57, "seek": 31754, "start": 325.04, "end": 329.08000000000004, "text": " To do that, we're going to construct our cost function.", "tokens": [50364, 1407, 1867, 300, 1168, 11, 718, 311, 700, 747, 257, 574, 412, 577, 281, 3481, 577, 731, 257, 1622, 9001, 50641, 50641, 264, 3097, 1412, 13, 50739, 50739, 1407, 360, 300, 11, 321, 434, 516, 281, 7690, 527, 2063, 2445, 13, 50941, 50941, 440, 2063, 2445, 2516, 264, 17630, 288, 2385, 293, 38334, 309, 281, 264, 3779, 288, 538, 1940, 51327, 51327, 288, 2385, 3175, 288, 13, 51482, 51482, 639, 2649, 307, 1219, 264, 6713, 13, 51635, 51635], "temperature": 0.0, "avg_logprob": -0.10088934898376464, "compression_ratio": 1.618279569892473, "no_speech_prob": 2.90226216748124e-06}, {"id": 58, "seek": 31754, "start": 329.08000000000004, "end": 336.8, "text": " The cost function takes the prediction y hat and compares it to the target y by taking", "tokens": [50364, 1407, 1867, 300, 1168, 11, 718, 311, 700, 747, 257, 574, 412, 577, 281, 3481, 577, 731, 257, 1622, 9001, 50641, 50641, 264, 3097, 1412, 13, 50739, 50739, 1407, 360, 300, 11, 321, 434, 516, 281, 7690, 527, 2063, 2445, 13, 50941, 50941, 440, 2063, 2445, 2516, 264, 17630, 288, 2385, 293, 38334, 309, 281, 264, 3779, 288, 538, 1940, 51327, 51327, 288, 2385, 3175, 288, 13, 51482, 51482, 639, 2649, 307, 1219, 264, 6713, 13, 51635, 51635], "temperature": 0.0, "avg_logprob": -0.10088934898376464, "compression_ratio": 1.618279569892473, "no_speech_prob": 2.90226216748124e-06}, {"id": 59, "seek": 31754, "start": 336.8, "end": 339.90000000000003, "text": " y hat minus y.", "tokens": [50364, 1407, 1867, 300, 1168, 11, 718, 311, 700, 747, 257, 574, 412, 577, 281, 3481, 577, 731, 257, 1622, 9001, 50641, 50641, 264, 3097, 1412, 13, 50739, 50739, 1407, 360, 300, 11, 321, 434, 516, 281, 7690, 527, 2063, 2445, 13, 50941, 50941, 440, 2063, 2445, 2516, 264, 17630, 288, 2385, 293, 38334, 309, 281, 264, 3779, 288, 538, 1940, 51327, 51327, 288, 2385, 3175, 288, 13, 51482, 51482, 639, 2649, 307, 1219, 264, 6713, 13, 51635, 51635], "temperature": 0.0, "avg_logprob": -0.10088934898376464, "compression_ratio": 1.618279569892473, "no_speech_prob": 2.90226216748124e-06}, {"id": 60, "seek": 31754, "start": 339.90000000000003, "end": 342.96000000000004, "text": " This difference is called the error.", "tokens": [50364, 1407, 1867, 300, 1168, 11, 718, 311, 700, 747, 257, 574, 412, 577, 281, 3481, 577, 731, 257, 1622, 9001, 50641, 50641, 264, 3097, 1412, 13, 50739, 50739, 1407, 360, 300, 11, 321, 434, 516, 281, 7690, 527, 2063, 2445, 13, 50941, 50941, 440, 2063, 2445, 2516, 264, 17630, 288, 2385, 293, 38334, 309, 281, 264, 3779, 288, 538, 1940, 51327, 51327, 288, 2385, 3175, 288, 13, 51482, 51482, 639, 2649, 307, 1219, 264, 6713, 13, 51635, 51635], "temperature": 0.0, "avg_logprob": -0.10088934898376464, "compression_ratio": 1.618279569892473, "no_speech_prob": 2.90226216748124e-06}, {"id": 61, "seek": 34296, "start": 342.96, "end": 347.96, "text": " We're measuring how far off the prediction is from the target.", "tokens": [50364, 492, 434, 13389, 577, 1400, 766, 264, 17630, 307, 490, 264, 3779, 13, 50614, 50614, 3087, 718, 311, 14722, 264, 3732, 295, 341, 6713, 13, 50876, 50876, 2743, 321, 434, 516, 281, 528, 281, 14722, 341, 1433, 337, 819, 3097, 5110, 741, 294, 264, 3097, 51128, 51128, 992, 13, 51187, 51187, 407, 562, 13389, 264, 6713, 337, 1365, 741, 11, 321, 603, 14722, 341, 8889, 6713, 1433, 13, 51496, 51496, 6288, 11, 321, 528, 281, 3481, 264, 6713, 2108, 264, 2302, 3097, 992, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.15227323228662665, "compression_ratio": 1.8181818181818181, "no_speech_prob": 2.443963921905379e-06}, {"id": 62, "seek": 34296, "start": 347.96, "end": 353.2, "text": " Next let's compute the square of this error.", "tokens": [50364, 492, 434, 13389, 577, 1400, 766, 264, 17630, 307, 490, 264, 3779, 13, 50614, 50614, 3087, 718, 311, 14722, 264, 3732, 295, 341, 6713, 13, 50876, 50876, 2743, 321, 434, 516, 281, 528, 281, 14722, 341, 1433, 337, 819, 3097, 5110, 741, 294, 264, 3097, 51128, 51128, 992, 13, 51187, 51187, 407, 562, 13389, 264, 6713, 337, 1365, 741, 11, 321, 603, 14722, 341, 8889, 6713, 1433, 13, 51496, 51496, 6288, 11, 321, 528, 281, 3481, 264, 6713, 2108, 264, 2302, 3097, 992, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.15227323228662665, "compression_ratio": 1.8181818181818181, "no_speech_prob": 2.443963921905379e-06}, {"id": 63, "seek": 34296, "start": 353.2, "end": 358.24, "text": " Also we're going to want to compute this term for different training examples i in the training", "tokens": [50364, 492, 434, 13389, 577, 1400, 766, 264, 17630, 307, 490, 264, 3779, 13, 50614, 50614, 3087, 718, 311, 14722, 264, 3732, 295, 341, 6713, 13, 50876, 50876, 2743, 321, 434, 516, 281, 528, 281, 14722, 341, 1433, 337, 819, 3097, 5110, 741, 294, 264, 3097, 51128, 51128, 992, 13, 51187, 51187, 407, 562, 13389, 264, 6713, 337, 1365, 741, 11, 321, 603, 14722, 341, 8889, 6713, 1433, 13, 51496, 51496, 6288, 11, 321, 528, 281, 3481, 264, 6713, 2108, 264, 2302, 3097, 992, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.15227323228662665, "compression_ratio": 1.8181818181818181, "no_speech_prob": 2.443963921905379e-06}, {"id": 64, "seek": 34296, "start": 358.24, "end": 359.41999999999996, "text": " set.", "tokens": [50364, 492, 434, 13389, 577, 1400, 766, 264, 17630, 307, 490, 264, 3779, 13, 50614, 50614, 3087, 718, 311, 14722, 264, 3732, 295, 341, 6713, 13, 50876, 50876, 2743, 321, 434, 516, 281, 528, 281, 14722, 341, 1433, 337, 819, 3097, 5110, 741, 294, 264, 3097, 51128, 51128, 992, 13, 51187, 51187, 407, 562, 13389, 264, 6713, 337, 1365, 741, 11, 321, 603, 14722, 341, 8889, 6713, 1433, 13, 51496, 51496, 6288, 11, 321, 528, 281, 3481, 264, 6713, 2108, 264, 2302, 3097, 992, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.15227323228662665, "compression_ratio": 1.8181818181818181, "no_speech_prob": 2.443963921905379e-06}, {"id": 65, "seek": 34296, "start": 359.41999999999996, "end": 365.59999999999997, "text": " So when measuring the error for example i, we'll compute this squared error term.", "tokens": [50364, 492, 434, 13389, 577, 1400, 766, 264, 17630, 307, 490, 264, 3779, 13, 50614, 50614, 3087, 718, 311, 14722, 264, 3732, 295, 341, 6713, 13, 50876, 50876, 2743, 321, 434, 516, 281, 528, 281, 14722, 341, 1433, 337, 819, 3097, 5110, 741, 294, 264, 3097, 51128, 51128, 992, 13, 51187, 51187, 407, 562, 13389, 264, 6713, 337, 1365, 741, 11, 321, 603, 14722, 341, 8889, 6713, 1433, 13, 51496, 51496, 6288, 11, 321, 528, 281, 3481, 264, 6713, 2108, 264, 2302, 3097, 992, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.15227323228662665, "compression_ratio": 1.8181818181818181, "no_speech_prob": 2.443963921905379e-06}, {"id": 66, "seek": 34296, "start": 365.59999999999997, "end": 370.34, "text": " Finally, we want to measure the error across the entire training set.", "tokens": [50364, 492, 434, 13389, 577, 1400, 766, 264, 17630, 307, 490, 264, 3779, 13, 50614, 50614, 3087, 718, 311, 14722, 264, 3732, 295, 341, 6713, 13, 50876, 50876, 2743, 321, 434, 516, 281, 528, 281, 14722, 341, 1433, 337, 819, 3097, 5110, 741, 294, 264, 3097, 51128, 51128, 992, 13, 51187, 51187, 407, 562, 13389, 264, 6713, 337, 1365, 741, 11, 321, 603, 14722, 341, 8889, 6713, 1433, 13, 51496, 51496, 6288, 11, 321, 528, 281, 3481, 264, 6713, 2108, 264, 2302, 3097, 992, 13, 51733, 51733], "temperature": 0.0, "avg_logprob": -0.15227323228662665, "compression_ratio": 1.8181818181818181, "no_speech_prob": 2.443963921905379e-06}, {"id": 67, "seek": 37034, "start": 370.34, "end": 374.11999999999995, "text": " In particular, let's sum up the squared errors like this.", "tokens": [50364, 682, 1729, 11, 718, 311, 2408, 493, 264, 8889, 13603, 411, 341, 13, 50553, 50553, 492, 603, 2408, 490, 741, 6915, 502, 11, 568, 11, 805, 11, 439, 264, 636, 493, 281, 275, 13, 50861, 50861, 400, 1604, 300, 275, 307, 264, 1230, 295, 3097, 5110, 11, 597, 307, 16953, 337, 341, 1412, 992, 13, 51161, 51161, 13428, 300, 498, 321, 362, 544, 3097, 5110, 11, 275, 307, 4833, 293, 428, 2063, 2445, 486, 8873, 51441, 51441, 257, 3801, 1230, 1670, 309, 311, 2408, 2810, 670, 544, 5110, 13, 51643, 51643], "temperature": 0.0, "avg_logprob": -0.135323001492408, "compression_ratio": 1.583710407239819, "no_speech_prob": 1.706167267911951e-06}, {"id": 68, "seek": 37034, "start": 374.11999999999995, "end": 380.28, "text": " We'll sum from i equals 1, 2, 3, all the way up to m.", "tokens": [50364, 682, 1729, 11, 718, 311, 2408, 493, 264, 8889, 13603, 411, 341, 13, 50553, 50553, 492, 603, 2408, 490, 741, 6915, 502, 11, 568, 11, 805, 11, 439, 264, 636, 493, 281, 275, 13, 50861, 50861, 400, 1604, 300, 275, 307, 264, 1230, 295, 3097, 5110, 11, 597, 307, 16953, 337, 341, 1412, 992, 13, 51161, 51161, 13428, 300, 498, 321, 362, 544, 3097, 5110, 11, 275, 307, 4833, 293, 428, 2063, 2445, 486, 8873, 51441, 51441, 257, 3801, 1230, 1670, 309, 311, 2408, 2810, 670, 544, 5110, 13, 51643, 51643], "temperature": 0.0, "avg_logprob": -0.135323001492408, "compression_ratio": 1.583710407239819, "no_speech_prob": 1.706167267911951e-06}, {"id": 69, "seek": 37034, "start": 380.28, "end": 386.28, "text": " And remember that m is the number of training examples, which is 47 for this data set.", "tokens": [50364, 682, 1729, 11, 718, 311, 2408, 493, 264, 8889, 13603, 411, 341, 13, 50553, 50553, 492, 603, 2408, 490, 741, 6915, 502, 11, 568, 11, 805, 11, 439, 264, 636, 493, 281, 275, 13, 50861, 50861, 400, 1604, 300, 275, 307, 264, 1230, 295, 3097, 5110, 11, 597, 307, 16953, 337, 341, 1412, 992, 13, 51161, 51161, 13428, 300, 498, 321, 362, 544, 3097, 5110, 11, 275, 307, 4833, 293, 428, 2063, 2445, 486, 8873, 51441, 51441, 257, 3801, 1230, 1670, 309, 311, 2408, 2810, 670, 544, 5110, 13, 51643, 51643], "temperature": 0.0, "avg_logprob": -0.135323001492408, "compression_ratio": 1.583710407239819, "no_speech_prob": 1.706167267911951e-06}, {"id": 70, "seek": 37034, "start": 386.28, "end": 391.88, "text": " Notice that if we have more training examples, m is larger and your cost function will calculate", "tokens": [50364, 682, 1729, 11, 718, 311, 2408, 493, 264, 8889, 13603, 411, 341, 13, 50553, 50553, 492, 603, 2408, 490, 741, 6915, 502, 11, 568, 11, 805, 11, 439, 264, 636, 493, 281, 275, 13, 50861, 50861, 400, 1604, 300, 275, 307, 264, 1230, 295, 3097, 5110, 11, 597, 307, 16953, 337, 341, 1412, 992, 13, 51161, 51161, 13428, 300, 498, 321, 362, 544, 3097, 5110, 11, 275, 307, 4833, 293, 428, 2063, 2445, 486, 8873, 51441, 51441, 257, 3801, 1230, 1670, 309, 311, 2408, 2810, 670, 544, 5110, 13, 51643, 51643], "temperature": 0.0, "avg_logprob": -0.135323001492408, "compression_ratio": 1.583710407239819, "no_speech_prob": 1.706167267911951e-06}, {"id": 71, "seek": 37034, "start": 391.88, "end": 395.91999999999996, "text": " a bigger number since it's summing over more examples.", "tokens": [50364, 682, 1729, 11, 718, 311, 2408, 493, 264, 8889, 13603, 411, 341, 13, 50553, 50553, 492, 603, 2408, 490, 741, 6915, 502, 11, 568, 11, 805, 11, 439, 264, 636, 493, 281, 275, 13, 50861, 50861, 400, 1604, 300, 275, 307, 264, 1230, 295, 3097, 5110, 11, 597, 307, 16953, 337, 341, 1412, 992, 13, 51161, 51161, 13428, 300, 498, 321, 362, 544, 3097, 5110, 11, 275, 307, 4833, 293, 428, 2063, 2445, 486, 8873, 51441, 51441, 257, 3801, 1230, 1670, 309, 311, 2408, 2810, 670, 544, 5110, 13, 51643, 51643], "temperature": 0.0, "avg_logprob": -0.135323001492408, "compression_ratio": 1.583710407239819, "no_speech_prob": 1.706167267911951e-06}, {"id": 72, "seek": 39592, "start": 395.92, "end": 403.24, "text": " So to build a cost function that doesn't automatically get bigger as the training set size gets larger,", "tokens": [50364, 407, 281, 1322, 257, 2063, 2445, 300, 1177, 380, 6772, 483, 3801, 382, 264, 3097, 992, 2744, 2170, 4833, 11, 50730, 50730, 538, 10286, 11, 321, 486, 14722, 264, 4274, 8889, 6713, 2602, 295, 264, 3217, 8889, 51044, 51044, 6713, 13, 51094, 51094, 400, 321, 360, 300, 538, 26764, 538, 275, 411, 341, 13, 51278, 51278, 1033, 11, 321, 434, 6217, 456, 13, 51444, 51444, 1449, 472, 1036, 551, 13, 51530, 51530, 3146, 10286, 11, 264, 2063, 2445, 300, 3479, 2539, 561, 764, 767, 41347, 538, 732, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.14033667595831903, "compression_ratio": 1.6419213973799127, "no_speech_prob": 1.0187978887188365e-06}, {"id": 73, "seek": 39592, "start": 403.24, "end": 409.52000000000004, "text": " by convention, we will compute the average squared error instead of the total squared", "tokens": [50364, 407, 281, 1322, 257, 2063, 2445, 300, 1177, 380, 6772, 483, 3801, 382, 264, 3097, 992, 2744, 2170, 4833, 11, 50730, 50730, 538, 10286, 11, 321, 486, 14722, 264, 4274, 8889, 6713, 2602, 295, 264, 3217, 8889, 51044, 51044, 6713, 13, 51094, 51094, 400, 321, 360, 300, 538, 26764, 538, 275, 411, 341, 13, 51278, 51278, 1033, 11, 321, 434, 6217, 456, 13, 51444, 51444, 1449, 472, 1036, 551, 13, 51530, 51530, 3146, 10286, 11, 264, 2063, 2445, 300, 3479, 2539, 561, 764, 767, 41347, 538, 732, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.14033667595831903, "compression_ratio": 1.6419213973799127, "no_speech_prob": 1.0187978887188365e-06}, {"id": 74, "seek": 39592, "start": 409.52000000000004, "end": 410.52000000000004, "text": " error.", "tokens": [50364, 407, 281, 1322, 257, 2063, 2445, 300, 1177, 380, 6772, 483, 3801, 382, 264, 3097, 992, 2744, 2170, 4833, 11, 50730, 50730, 538, 10286, 11, 321, 486, 14722, 264, 4274, 8889, 6713, 2602, 295, 264, 3217, 8889, 51044, 51044, 6713, 13, 51094, 51094, 400, 321, 360, 300, 538, 26764, 538, 275, 411, 341, 13, 51278, 51278, 1033, 11, 321, 434, 6217, 456, 13, 51444, 51444, 1449, 472, 1036, 551, 13, 51530, 51530, 3146, 10286, 11, 264, 2063, 2445, 300, 3479, 2539, 561, 764, 767, 41347, 538, 732, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.14033667595831903, "compression_ratio": 1.6419213973799127, "no_speech_prob": 1.0187978887188365e-06}, {"id": 75, "seek": 39592, "start": 410.52000000000004, "end": 414.20000000000005, "text": " And we do that by dividing by m like this.", "tokens": [50364, 407, 281, 1322, 257, 2063, 2445, 300, 1177, 380, 6772, 483, 3801, 382, 264, 3097, 992, 2744, 2170, 4833, 11, 50730, 50730, 538, 10286, 11, 321, 486, 14722, 264, 4274, 8889, 6713, 2602, 295, 264, 3217, 8889, 51044, 51044, 6713, 13, 51094, 51094, 400, 321, 360, 300, 538, 26764, 538, 275, 411, 341, 13, 51278, 51278, 1033, 11, 321, 434, 6217, 456, 13, 51444, 51444, 1449, 472, 1036, 551, 13, 51530, 51530, 3146, 10286, 11, 264, 2063, 2445, 300, 3479, 2539, 561, 764, 767, 41347, 538, 732, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.14033667595831903, "compression_ratio": 1.6419213973799127, "no_speech_prob": 1.0187978887188365e-06}, {"id": 76, "seek": 39592, "start": 414.20000000000005, "end": 417.52000000000004, "text": " Okay, we're nearly there.", "tokens": [50364, 407, 281, 1322, 257, 2063, 2445, 300, 1177, 380, 6772, 483, 3801, 382, 264, 3097, 992, 2744, 2170, 4833, 11, 50730, 50730, 538, 10286, 11, 321, 486, 14722, 264, 4274, 8889, 6713, 2602, 295, 264, 3217, 8889, 51044, 51044, 6713, 13, 51094, 51094, 400, 321, 360, 300, 538, 26764, 538, 275, 411, 341, 13, 51278, 51278, 1033, 11, 321, 434, 6217, 456, 13, 51444, 51444, 1449, 472, 1036, 551, 13, 51530, 51530, 3146, 10286, 11, 264, 2063, 2445, 300, 3479, 2539, 561, 764, 767, 41347, 538, 732, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.14033667595831903, "compression_ratio": 1.6419213973799127, "no_speech_prob": 1.0187978887188365e-06}, {"id": 77, "seek": 39592, "start": 417.52000000000004, "end": 419.24, "text": " Just one last thing.", "tokens": [50364, 407, 281, 1322, 257, 2063, 2445, 300, 1177, 380, 6772, 483, 3801, 382, 264, 3097, 992, 2744, 2170, 4833, 11, 50730, 50730, 538, 10286, 11, 321, 486, 14722, 264, 4274, 8889, 6713, 2602, 295, 264, 3217, 8889, 51044, 51044, 6713, 13, 51094, 51094, 400, 321, 360, 300, 538, 26764, 538, 275, 411, 341, 13, 51278, 51278, 1033, 11, 321, 434, 6217, 456, 13, 51444, 51444, 1449, 472, 1036, 551, 13, 51530, 51530, 3146, 10286, 11, 264, 2063, 2445, 300, 3479, 2539, 561, 764, 767, 41347, 538, 732, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.14033667595831903, "compression_ratio": 1.6419213973799127, "no_speech_prob": 1.0187978887188365e-06}, {"id": 78, "seek": 39592, "start": 419.24, "end": 425.44, "text": " By convention, the cost function that machine learning people use actually divides by two", "tokens": [50364, 407, 281, 1322, 257, 2063, 2445, 300, 1177, 380, 6772, 483, 3801, 382, 264, 3097, 992, 2744, 2170, 4833, 11, 50730, 50730, 538, 10286, 11, 321, 486, 14722, 264, 4274, 8889, 6713, 2602, 295, 264, 3217, 8889, 51044, 51044, 6713, 13, 51094, 51094, 400, 321, 360, 300, 538, 26764, 538, 275, 411, 341, 13, 51278, 51278, 1033, 11, 321, 434, 6217, 456, 13, 51444, 51444, 1449, 472, 1036, 551, 13, 51530, 51530, 3146, 10286, 11, 264, 2063, 2445, 300, 3479, 2539, 561, 764, 767, 41347, 538, 732, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.14033667595831903, "compression_ratio": 1.6419213973799127, "no_speech_prob": 1.0187978887188365e-06}, {"id": 79, "seek": 42544, "start": 425.44, "end": 426.64, "text": " times m.", "tokens": [50364, 1413, 275, 13, 50424, 50424, 440, 2857, 10044, 538, 732, 307, 445, 4140, 281, 652, 512, 295, 527, 1780, 20448, 257, 707, 50678, 50678, 857, 408, 771, 13, 50728, 50728, 583, 264, 2063, 2445, 920, 1985, 1968, 291, 4090, 341, 10044, 538, 732, 420, 406, 13, 50961, 50961, 407, 341, 6114, 558, 510, 307, 264, 2063, 2445, 13, 51126, 51126, 400, 321, 434, 516, 281, 2464, 508, 295, 343, 33, 281, 2864, 281, 264, 2063, 2445, 13, 51512, 51512, 639, 307, 611, 1219, 264, 8889, 6713, 2063, 2445, 13, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.13064047085341587, "compression_ratio": 1.7061611374407584, "no_speech_prob": 6.438953278120607e-06}, {"id": 80, "seek": 42544, "start": 426.64, "end": 431.71999999999997, "text": " The extra division by two is just meant to make some of our later calculations a little", "tokens": [50364, 1413, 275, 13, 50424, 50424, 440, 2857, 10044, 538, 732, 307, 445, 4140, 281, 652, 512, 295, 527, 1780, 20448, 257, 707, 50678, 50678, 857, 408, 771, 13, 50728, 50728, 583, 264, 2063, 2445, 920, 1985, 1968, 291, 4090, 341, 10044, 538, 732, 420, 406, 13, 50961, 50961, 407, 341, 6114, 558, 510, 307, 264, 2063, 2445, 13, 51126, 51126, 400, 321, 434, 516, 281, 2464, 508, 295, 343, 33, 281, 2864, 281, 264, 2063, 2445, 13, 51512, 51512, 639, 307, 611, 1219, 264, 8889, 6713, 2063, 2445, 13, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.13064047085341587, "compression_ratio": 1.7061611374407584, "no_speech_prob": 6.438953278120607e-06}, {"id": 81, "seek": 42544, "start": 431.71999999999997, "end": 432.71999999999997, "text": " bit neater.", "tokens": [50364, 1413, 275, 13, 50424, 50424, 440, 2857, 10044, 538, 732, 307, 445, 4140, 281, 652, 512, 295, 527, 1780, 20448, 257, 707, 50678, 50678, 857, 408, 771, 13, 50728, 50728, 583, 264, 2063, 2445, 920, 1985, 1968, 291, 4090, 341, 10044, 538, 732, 420, 406, 13, 50961, 50961, 407, 341, 6114, 558, 510, 307, 264, 2063, 2445, 13, 51126, 51126, 400, 321, 434, 516, 281, 2464, 508, 295, 343, 33, 281, 2864, 281, 264, 2063, 2445, 13, 51512, 51512, 639, 307, 611, 1219, 264, 8889, 6713, 2063, 2445, 13, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.13064047085341587, "compression_ratio": 1.7061611374407584, "no_speech_prob": 6.438953278120607e-06}, {"id": 82, "seek": 42544, "start": 432.71999999999997, "end": 437.38, "text": " But the cost function still works whether you include this division by two or not.", "tokens": [50364, 1413, 275, 13, 50424, 50424, 440, 2857, 10044, 538, 732, 307, 445, 4140, 281, 652, 512, 295, 527, 1780, 20448, 257, 707, 50678, 50678, 857, 408, 771, 13, 50728, 50728, 583, 264, 2063, 2445, 920, 1985, 1968, 291, 4090, 341, 10044, 538, 732, 420, 406, 13, 50961, 50961, 407, 341, 6114, 558, 510, 307, 264, 2063, 2445, 13, 51126, 51126, 400, 321, 434, 516, 281, 2464, 508, 295, 343, 33, 281, 2864, 281, 264, 2063, 2445, 13, 51512, 51512, 639, 307, 611, 1219, 264, 8889, 6713, 2063, 2445, 13, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.13064047085341587, "compression_ratio": 1.7061611374407584, "no_speech_prob": 6.438953278120607e-06}, {"id": 83, "seek": 42544, "start": 437.38, "end": 440.68, "text": " So this expression right here is the cost function.", "tokens": [50364, 1413, 275, 13, 50424, 50424, 440, 2857, 10044, 538, 732, 307, 445, 4140, 281, 652, 512, 295, 527, 1780, 20448, 257, 707, 50678, 50678, 857, 408, 771, 13, 50728, 50728, 583, 264, 2063, 2445, 920, 1985, 1968, 291, 4090, 341, 10044, 538, 732, 420, 406, 13, 50961, 50961, 407, 341, 6114, 558, 510, 307, 264, 2063, 2445, 13, 51126, 51126, 400, 321, 434, 516, 281, 2464, 508, 295, 343, 33, 281, 2864, 281, 264, 2063, 2445, 13, 51512, 51512, 639, 307, 611, 1219, 264, 8889, 6713, 2063, 2445, 13, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.13064047085341587, "compression_ratio": 1.7061611374407584, "no_speech_prob": 6.438953278120607e-06}, {"id": 84, "seek": 42544, "start": 440.68, "end": 448.4, "text": " And we're going to write J of WB to refer to the cost function.", "tokens": [50364, 1413, 275, 13, 50424, 50424, 440, 2857, 10044, 538, 732, 307, 445, 4140, 281, 652, 512, 295, 527, 1780, 20448, 257, 707, 50678, 50678, 857, 408, 771, 13, 50728, 50728, 583, 264, 2063, 2445, 920, 1985, 1968, 291, 4090, 341, 10044, 538, 732, 420, 406, 13, 50961, 50961, 407, 341, 6114, 558, 510, 307, 264, 2063, 2445, 13, 51126, 51126, 400, 321, 434, 516, 281, 2464, 508, 295, 343, 33, 281, 2864, 281, 264, 2063, 2445, 13, 51512, 51512, 639, 307, 611, 1219, 264, 8889, 6713, 2063, 2445, 13, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.13064047085341587, "compression_ratio": 1.7061611374407584, "no_speech_prob": 6.438953278120607e-06}, {"id": 85, "seek": 42544, "start": 448.4, "end": 452.32, "text": " This is also called the squared error cost function.", "tokens": [50364, 1413, 275, 13, 50424, 50424, 440, 2857, 10044, 538, 732, 307, 445, 4140, 281, 652, 512, 295, 527, 1780, 20448, 257, 707, 50678, 50678, 857, 408, 771, 13, 50728, 50728, 583, 264, 2063, 2445, 920, 1985, 1968, 291, 4090, 341, 10044, 538, 732, 420, 406, 13, 50961, 50961, 407, 341, 6114, 558, 510, 307, 264, 2063, 2445, 13, 51126, 51126, 400, 321, 434, 516, 281, 2464, 508, 295, 343, 33, 281, 2864, 281, 264, 2063, 2445, 13, 51512, 51512, 639, 307, 611, 1219, 264, 8889, 6713, 2063, 2445, 13, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.13064047085341587, "compression_ratio": 1.7061611374407584, "no_speech_prob": 6.438953278120607e-06}, {"id": 86, "seek": 45232, "start": 452.32, "end": 457.44, "text": " And it's called this because you're taking the square of these error terms.", "tokens": [50364, 400, 309, 311, 1219, 341, 570, 291, 434, 1940, 264, 3732, 295, 613, 6713, 2115, 13, 50620, 50620, 682, 3479, 2539, 11, 819, 561, 486, 764, 819, 2063, 6828, 337, 819, 50872, 50872, 5821, 13, 50922, 50922, 583, 264, 8889, 6713, 2063, 2445, 307, 538, 1400, 264, 881, 12719, 1143, 472, 337, 8213, 51184, 51184, 24590, 13, 51254, 51254, 400, 337, 300, 1871, 11, 337, 439, 24590, 2740, 11, 689, 309, 2544, 281, 976, 665, 3542, 337, 867, 51530, 51530, 5821, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.13732676173365393, "compression_ratio": 1.7194570135746607, "no_speech_prob": 1.051132926477294e-06}, {"id": 87, "seek": 45232, "start": 457.44, "end": 462.48, "text": " In machine learning, different people will use different cost functions for different", "tokens": [50364, 400, 309, 311, 1219, 341, 570, 291, 434, 1940, 264, 3732, 295, 613, 6713, 2115, 13, 50620, 50620, 682, 3479, 2539, 11, 819, 561, 486, 764, 819, 2063, 6828, 337, 819, 50872, 50872, 5821, 13, 50922, 50922, 583, 264, 8889, 6713, 2063, 2445, 307, 538, 1400, 264, 881, 12719, 1143, 472, 337, 8213, 51184, 51184, 24590, 13, 51254, 51254, 400, 337, 300, 1871, 11, 337, 439, 24590, 2740, 11, 689, 309, 2544, 281, 976, 665, 3542, 337, 867, 51530, 51530, 5821, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.13732676173365393, "compression_ratio": 1.7194570135746607, "no_speech_prob": 1.051132926477294e-06}, {"id": 88, "seek": 45232, "start": 462.48, "end": 463.48, "text": " applications.", "tokens": [50364, 400, 309, 311, 1219, 341, 570, 291, 434, 1940, 264, 3732, 295, 613, 6713, 2115, 13, 50620, 50620, 682, 3479, 2539, 11, 819, 561, 486, 764, 819, 2063, 6828, 337, 819, 50872, 50872, 5821, 13, 50922, 50922, 583, 264, 8889, 6713, 2063, 2445, 307, 538, 1400, 264, 881, 12719, 1143, 472, 337, 8213, 51184, 51184, 24590, 13, 51254, 51254, 400, 337, 300, 1871, 11, 337, 439, 24590, 2740, 11, 689, 309, 2544, 281, 976, 665, 3542, 337, 867, 51530, 51530, 5821, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.13732676173365393, "compression_ratio": 1.7194570135746607, "no_speech_prob": 1.051132926477294e-06}, {"id": 89, "seek": 45232, "start": 463.48, "end": 468.71999999999997, "text": " But the squared error cost function is by far the most commonly used one for linear", "tokens": [50364, 400, 309, 311, 1219, 341, 570, 291, 434, 1940, 264, 3732, 295, 613, 6713, 2115, 13, 50620, 50620, 682, 3479, 2539, 11, 819, 561, 486, 764, 819, 2063, 6828, 337, 819, 50872, 50872, 5821, 13, 50922, 50922, 583, 264, 8889, 6713, 2063, 2445, 307, 538, 1400, 264, 881, 12719, 1143, 472, 337, 8213, 51184, 51184, 24590, 13, 51254, 51254, 400, 337, 300, 1871, 11, 337, 439, 24590, 2740, 11, 689, 309, 2544, 281, 976, 665, 3542, 337, 867, 51530, 51530, 5821, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.13732676173365393, "compression_ratio": 1.7194570135746607, "no_speech_prob": 1.051132926477294e-06}, {"id": 90, "seek": 45232, "start": 468.71999999999997, "end": 470.12, "text": " regression.", "tokens": [50364, 400, 309, 311, 1219, 341, 570, 291, 434, 1940, 264, 3732, 295, 613, 6713, 2115, 13, 50620, 50620, 682, 3479, 2539, 11, 819, 561, 486, 764, 819, 2063, 6828, 337, 819, 50872, 50872, 5821, 13, 50922, 50922, 583, 264, 8889, 6713, 2063, 2445, 307, 538, 1400, 264, 881, 12719, 1143, 472, 337, 8213, 51184, 51184, 24590, 13, 51254, 51254, 400, 337, 300, 1871, 11, 337, 439, 24590, 2740, 11, 689, 309, 2544, 281, 976, 665, 3542, 337, 867, 51530, 51530, 5821, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.13732676173365393, "compression_ratio": 1.7194570135746607, "no_speech_prob": 1.051132926477294e-06}, {"id": 91, "seek": 45232, "start": 470.12, "end": 475.64, "text": " And for that matter, for all regression problems, where it seems to give good results for many", "tokens": [50364, 400, 309, 311, 1219, 341, 570, 291, 434, 1940, 264, 3732, 295, 613, 6713, 2115, 13, 50620, 50620, 682, 3479, 2539, 11, 819, 561, 486, 764, 819, 2063, 6828, 337, 819, 50872, 50872, 5821, 13, 50922, 50922, 583, 264, 8889, 6713, 2063, 2445, 307, 538, 1400, 264, 881, 12719, 1143, 472, 337, 8213, 51184, 51184, 24590, 13, 51254, 51254, 400, 337, 300, 1871, 11, 337, 439, 24590, 2740, 11, 689, 309, 2544, 281, 976, 665, 3542, 337, 867, 51530, 51530, 5821, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.13732676173365393, "compression_ratio": 1.7194570135746607, "no_speech_prob": 1.051132926477294e-06}, {"id": 92, "seek": 45232, "start": 475.64, "end": 476.8, "text": " applications.", "tokens": [50364, 400, 309, 311, 1219, 341, 570, 291, 434, 1940, 264, 3732, 295, 613, 6713, 2115, 13, 50620, 50620, 682, 3479, 2539, 11, 819, 561, 486, 764, 819, 2063, 6828, 337, 819, 50872, 50872, 5821, 13, 50922, 50922, 583, 264, 8889, 6713, 2063, 2445, 307, 538, 1400, 264, 881, 12719, 1143, 472, 337, 8213, 51184, 51184, 24590, 13, 51254, 51254, 400, 337, 300, 1871, 11, 337, 439, 24590, 2740, 11, 689, 309, 2544, 281, 976, 665, 3542, 337, 867, 51530, 51530, 5821, 13, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.13732676173365393, "compression_ratio": 1.7194570135746607, "no_speech_prob": 1.051132926477294e-06}, {"id": 93, "seek": 47680, "start": 476.8, "end": 485.72, "text": " So just as a reminder, the prediction y hat is equal to the output of the model f at x.", "tokens": [50364, 407, 445, 382, 257, 13548, 11, 264, 17630, 288, 2385, 307, 2681, 281, 264, 5598, 295, 264, 2316, 283, 412, 2031, 13, 50810, 50810, 407, 321, 393, 28132, 264, 2063, 2445, 508, 295, 343, 33, 382, 472, 670, 732, 275, 1413, 264, 2408, 490, 741, 6915, 51312, 51312, 472, 281, 275, 295, 283, 295, 2031, 741, 3175, 288, 741, 11, 264, 11275, 8889, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.13951321209178252, "compression_ratio": 1.4807692307692308, "no_speech_prob": 1.9637684545159573e-06}, {"id": 94, "seek": 47680, "start": 485.72, "end": 495.76, "text": " So we can rewrite the cost function J of WB as one over two m times the sum from i equals", "tokens": [50364, 407, 445, 382, 257, 13548, 11, 264, 17630, 288, 2385, 307, 2681, 281, 264, 5598, 295, 264, 2316, 283, 412, 2031, 13, 50810, 50810, 407, 321, 393, 28132, 264, 2063, 2445, 508, 295, 343, 33, 382, 472, 670, 732, 275, 1413, 264, 2408, 490, 741, 6915, 51312, 51312, 472, 281, 275, 295, 283, 295, 2031, 741, 3175, 288, 741, 11, 264, 11275, 8889, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.13951321209178252, "compression_ratio": 1.4807692307692308, "no_speech_prob": 1.9637684545159573e-06}, {"id": 95, "seek": 47680, "start": 495.76, "end": 503.04, "text": " one to m of f of x i minus y i, the quantity squared.", "tokens": [50364, 407, 445, 382, 257, 13548, 11, 264, 17630, 288, 2385, 307, 2681, 281, 264, 5598, 295, 264, 2316, 283, 412, 2031, 13, 50810, 50810, 407, 321, 393, 28132, 264, 2063, 2445, 508, 295, 343, 33, 382, 472, 670, 732, 275, 1413, 264, 2408, 490, 741, 6915, 51312, 51312, 472, 281, 275, 295, 283, 295, 2031, 741, 3175, 288, 741, 11, 264, 11275, 8889, 13, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.13951321209178252, "compression_ratio": 1.4807692307692308, "no_speech_prob": 1.9637684545159573e-06}, {"id": 96, "seek": 50304, "start": 503.04, "end": 510.36, "text": " Eventually, we're going to want to find values of W and B that make the cost function small.", "tokens": [50364, 17586, 11, 321, 434, 516, 281, 528, 281, 915, 4190, 295, 343, 293, 363, 300, 652, 264, 2063, 2445, 1359, 13, 50730, 50730, 583, 949, 516, 456, 11, 718, 311, 700, 6052, 544, 24002, 466, 437, 508, 295, 343, 33, 307, 534, 15866, 13, 51144, 51144, 1711, 341, 935, 11, 291, 1062, 312, 1953, 321, 600, 1096, 257, 1379, 688, 295, 5221, 281, 6964, 264, 2063, 51376, 51376, 2445, 13, 51432, 51432, 583, 437, 2293, 307, 309, 884, 30, 51556, 51556, 961, 311, 352, 322, 281, 264, 958, 960, 11, 689, 321, 603, 1823, 807, 472, 1365, 295, 437, 264, 2063, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11304160526820592, "compression_ratio": 1.6326530612244898, "no_speech_prob": 3.2377067782363156e-06}, {"id": 97, "seek": 50304, "start": 510.36, "end": 518.64, "text": " But before going there, let's first gain more intuition about what J of WB is really computing.", "tokens": [50364, 17586, 11, 321, 434, 516, 281, 528, 281, 915, 4190, 295, 343, 293, 363, 300, 652, 264, 2063, 2445, 1359, 13, 50730, 50730, 583, 949, 516, 456, 11, 718, 311, 700, 6052, 544, 24002, 466, 437, 508, 295, 343, 33, 307, 534, 15866, 13, 51144, 51144, 1711, 341, 935, 11, 291, 1062, 312, 1953, 321, 600, 1096, 257, 1379, 688, 295, 5221, 281, 6964, 264, 2063, 51376, 51376, 2445, 13, 51432, 51432, 583, 437, 2293, 307, 309, 884, 30, 51556, 51556, 961, 311, 352, 322, 281, 264, 958, 960, 11, 689, 321, 603, 1823, 807, 472, 1365, 295, 437, 264, 2063, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11304160526820592, "compression_ratio": 1.6326530612244898, "no_speech_prob": 3.2377067782363156e-06}, {"id": 98, "seek": 50304, "start": 518.64, "end": 523.28, "text": " At this point, you might be thinking we've done a whole lot of math to define the cost", "tokens": [50364, 17586, 11, 321, 434, 516, 281, 528, 281, 915, 4190, 295, 343, 293, 363, 300, 652, 264, 2063, 2445, 1359, 13, 50730, 50730, 583, 949, 516, 456, 11, 718, 311, 700, 6052, 544, 24002, 466, 437, 508, 295, 343, 33, 307, 534, 15866, 13, 51144, 51144, 1711, 341, 935, 11, 291, 1062, 312, 1953, 321, 600, 1096, 257, 1379, 688, 295, 5221, 281, 6964, 264, 2063, 51376, 51376, 2445, 13, 51432, 51432, 583, 437, 2293, 307, 309, 884, 30, 51556, 51556, 961, 311, 352, 322, 281, 264, 958, 960, 11, 689, 321, 603, 1823, 807, 472, 1365, 295, 437, 264, 2063, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11304160526820592, "compression_ratio": 1.6326530612244898, "no_speech_prob": 3.2377067782363156e-06}, {"id": 99, "seek": 50304, "start": 523.28, "end": 524.4, "text": " function.", "tokens": [50364, 17586, 11, 321, 434, 516, 281, 528, 281, 915, 4190, 295, 343, 293, 363, 300, 652, 264, 2063, 2445, 1359, 13, 50730, 50730, 583, 949, 516, 456, 11, 718, 311, 700, 6052, 544, 24002, 466, 437, 508, 295, 343, 33, 307, 534, 15866, 13, 51144, 51144, 1711, 341, 935, 11, 291, 1062, 312, 1953, 321, 600, 1096, 257, 1379, 688, 295, 5221, 281, 6964, 264, 2063, 51376, 51376, 2445, 13, 51432, 51432, 583, 437, 2293, 307, 309, 884, 30, 51556, 51556, 961, 311, 352, 322, 281, 264, 958, 960, 11, 689, 321, 603, 1823, 807, 472, 1365, 295, 437, 264, 2063, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11304160526820592, "compression_ratio": 1.6326530612244898, "no_speech_prob": 3.2377067782363156e-06}, {"id": 100, "seek": 50304, "start": 524.4, "end": 526.88, "text": " But what exactly is it doing?", "tokens": [50364, 17586, 11, 321, 434, 516, 281, 528, 281, 915, 4190, 295, 343, 293, 363, 300, 652, 264, 2063, 2445, 1359, 13, 50730, 50730, 583, 949, 516, 456, 11, 718, 311, 700, 6052, 544, 24002, 466, 437, 508, 295, 343, 33, 307, 534, 15866, 13, 51144, 51144, 1711, 341, 935, 11, 291, 1062, 312, 1953, 321, 600, 1096, 257, 1379, 688, 295, 5221, 281, 6964, 264, 2063, 51376, 51376, 2445, 13, 51432, 51432, 583, 437, 2293, 307, 309, 884, 30, 51556, 51556, 961, 311, 352, 322, 281, 264, 958, 960, 11, 689, 321, 603, 1823, 807, 472, 1365, 295, 437, 264, 2063, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11304160526820592, "compression_ratio": 1.6326530612244898, "no_speech_prob": 3.2377067782363156e-06}, {"id": 101, "seek": 50304, "start": 526.88, "end": 531.8000000000001, "text": " Let's go on to the next video, where we'll step through one example of what the cost", "tokens": [50364, 17586, 11, 321, 434, 516, 281, 528, 281, 915, 4190, 295, 343, 293, 363, 300, 652, 264, 2063, 2445, 1359, 13, 50730, 50730, 583, 949, 516, 456, 11, 718, 311, 700, 6052, 544, 24002, 466, 437, 508, 295, 343, 33, 307, 534, 15866, 13, 51144, 51144, 1711, 341, 935, 11, 291, 1062, 312, 1953, 321, 600, 1096, 257, 1379, 688, 295, 5221, 281, 6964, 264, 2063, 51376, 51376, 2445, 13, 51432, 51432, 583, 437, 2293, 307, 309, 884, 30, 51556, 51556, 961, 311, 352, 322, 281, 264, 958, 960, 11, 689, 321, 603, 1823, 807, 472, 1365, 295, 437, 264, 2063, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.11304160526820592, "compression_ratio": 1.6326530612244898, "no_speech_prob": 3.2377067782363156e-06}, {"id": 102, "seek": 53180, "start": 531.8, "end": 533.4, "text": " function is really computing.", "tokens": [50364, 2445, 307, 534, 15866, 13, 50444, 50444, 663, 286, 1454, 486, 854, 291, 1322, 24002, 466, 437, 309, 1355, 498, 508, 295, 343, 33, 307, 2416, 5717, 50746, 50746, 498, 264, 2063, 508, 307, 1359, 13, 50876, 50876, 961, 311, 352, 322, 281, 264, 958, 960, 13, 50952], "temperature": 0.0, "avg_logprob": -0.1805354881286621, "compression_ratio": 1.300751879699248, "no_speech_prob": 7.701870345044881e-05}, {"id": 103, "seek": 53180, "start": 533.4, "end": 539.4399999999999, "text": " That I hope will help you build intuition about what it means if J of WB is large versus", "tokens": [50364, 2445, 307, 534, 15866, 13, 50444, 50444, 663, 286, 1454, 486, 854, 291, 1322, 24002, 466, 437, 309, 1355, 498, 508, 295, 343, 33, 307, 2416, 5717, 50746, 50746, 498, 264, 2063, 508, 307, 1359, 13, 50876, 50876, 961, 311, 352, 322, 281, 264, 958, 960, 13, 50952], "temperature": 0.0, "avg_logprob": -0.1805354881286621, "compression_ratio": 1.300751879699248, "no_speech_prob": 7.701870345044881e-05}, {"id": 104, "seek": 53180, "start": 539.4399999999999, "end": 542.04, "text": " if the cost J is small.", "tokens": [50364, 2445, 307, 534, 15866, 13, 50444, 50444, 663, 286, 1454, 486, 854, 291, 1322, 24002, 466, 437, 309, 1355, 498, 508, 295, 343, 33, 307, 2416, 5717, 50746, 50746, 498, 264, 2063, 508, 307, 1359, 13, 50876, 50876, 961, 311, 352, 322, 281, 264, 958, 960, 13, 50952], "temperature": 0.0, "avg_logprob": -0.1805354881286621, "compression_ratio": 1.300751879699248, "no_speech_prob": 7.701870345044881e-05}, {"id": 105, "seek": 54204, "start": 542.04, "end": 562.4399999999999, "text": " Let's go on to the next video.", "tokens": [50364, 961, 311, 352, 322, 281, 264, 958, 960, 13, 51384], "temperature": 0.0, "avg_logprob": -0.4591625928878784, "compression_ratio": 0.7894736842105263, "no_speech_prob": 0.00028110205312259495}], "language": "en", "video_id": "ZzeDtSmrRoU", "entity": "ML Specialization, Andrew Ng (2022)"}}