{"video_id": "vrTHO5zRq6s", "title": "1.10 Machine Learning Overview | Linear regression model part 2 --[Machine Learning | Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart! First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 404, "views": 621, "publish_date": "11/04/2022", "timestamp": 1660953600, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " Let's look in this video at the process of how supervised learning works. Supervised learning algorithm will input a data set and then what exactly does it do and what does it output? Let's find out in this video. Recall that a training set in supervised learning includes both the input features, such as the size of the hulls, and also the output targets, such as the price of the hulls. The output targets are the right answers that the model will learn from. To train the model, you feed the training set, both the input features and the output targets, to your learning algorithm. Then your supervised learning algorithm will produce some function. We'll write this function as lowercase f, where f stands for function. Historically, this function used to be called a hypothesis, but I'm just going to call it a function f in this class. The job of f is to take a new input x and output an estimate or prediction, which I'm going to call y hat, and it's written like the variable y with this little hat symbol on top. In machine learning, the convention is that y hat is the estimate or the prediction for y. The function f is called the model. X is called the input or the input feature, and the output of the model is the prediction y hat. The model's prediction is the estimated value of y. When the symbol is just a letter y, then that refers to the target, which is the actual true value in the training set. In contrast, y hat is an estimate. It may or may not be the actual true value. Well, if you're helping your client to sell their hulls, well, the true price of the hulls is unknown until they sell it. So your model f, given the size, outputs a price which is the estimated, that is the prediction of what the true price will be. Now, when we design a learning algorithm, a key question is, how are we going to represent the function f? Or in other words, what is the math formula we're going to use to compute f? For now, let's stick with f being a straight line. So your function can be written as f subscript w comma b of x equals, I'm going to use w times x plus b. I'll define w and b soon, but for now, just know that w and b are numbers, and the values chosen for w and b will determine the prediction y hat based on the input feature x. So this f w b of x means f is a function that takes x's input, and depending on the values of w and b, f will output some value of a prediction y hat. As an alternative to writing this f w comma b of x, I'll sometimes just write f of x without explicitly including w and b in the subscript. It's just a simple notation, but means exactly the same thing as f w b of x. Let's plot the training set on the graph where the input feature x is on the horizontal axis, and the output target y is on the vertical axis. Remember, the algorithm learns from this data and generates a best fit line like maybe this one here. This straight line is the linear function f w b of x equals w times x plus b. Or more simply, we can drop w and b and just write f of x equals w x plus b. Here's what this function is doing, it's making predictions for the value of y using a straight line function of x. So you may ask, why are we choosing a linear function where linear function is just a fancy term for a straight line, instead of some nonlinear function like a curve or a parabola? Well, sometimes you want to fit more complex nonlinear functions as well, like a curve like this, but since this linear function is relatively simple and easy to work with, let's use a line as a foundation that will eventually help you to get to more complex models that are nonlinear. This particular model has a name, it's called linear regression. More specifically, this is linear regression with one variable, where the phrase one variable means that there's a single input variable or feature x, namely the size of the house. Another name for a linear model with one input variable is univariate linear regression, where uni means one in Latin and where variate means variable. So univariate is just a fancy way of saying one variable. In a later video, you'll also see a variation of regression where you want to make a prediction based not just on the size of a house, but on a bunch of other things that you may know about the house, such as number of bedrooms and other features. And by the way, when you're done with this video, there is another optional lab. You don't need to write any code, just review it, run the code and see what it does. That will show you how to define in Python a straight line function. And the lab will let you choose the values of W and B to try to fit the training data. You don't have to do the lab if you don't want to, but I hope you play of it when you're done watching this video. So that's linear regression. In order for you to make this work, one of the most important things you have to do is construct a cost function. The idea of a cost function is one of the most universal and important ideas in machine learning and is used in both linear regression and in training many of the most advanced AI models in the world. So let's go on to the next video and take a look at how you can construct a cost function.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.38, "text": " Let's look in this video at the process of how supervised learning works.", "tokens": [50364, 961, 311, 574, 294, 341, 960, 412, 264, 1399, 295, 577, 46533, 2539, 1985, 13, 50733, 50733, 4548, 24420, 2539, 9284, 486, 4846, 257, 1412, 992, 293, 550, 437, 2293, 775, 309, 360, 50929, 50929, 293, 437, 775, 309, 5598, 30, 51006, 51006, 961, 311, 915, 484, 294, 341, 960, 13, 51148, 51148, 9647, 336, 300, 257, 3097, 992, 294, 46533, 2539, 5974, 1293, 264, 4846, 4122, 11, 1270, 382, 51414, 51414, 264, 2744, 295, 264, 32335, 82, 11, 293, 611, 264, 5598, 12911, 11, 1270, 382, 264, 3218, 295, 264, 32335, 82, 13, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.14662110075658683, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.008980256505310535}, {"id": 1, "seek": 0, "start": 7.38, "end": 11.3, "text": " Supervised learning algorithm will input a data set and then what exactly does it do", "tokens": [50364, 961, 311, 574, 294, 341, 960, 412, 264, 1399, 295, 577, 46533, 2539, 1985, 13, 50733, 50733, 4548, 24420, 2539, 9284, 486, 4846, 257, 1412, 992, 293, 550, 437, 2293, 775, 309, 360, 50929, 50929, 293, 437, 775, 309, 5598, 30, 51006, 51006, 961, 311, 915, 484, 294, 341, 960, 13, 51148, 51148, 9647, 336, 300, 257, 3097, 992, 294, 46533, 2539, 5974, 1293, 264, 4846, 4122, 11, 1270, 382, 51414, 51414, 264, 2744, 295, 264, 32335, 82, 11, 293, 611, 264, 5598, 12911, 11, 1270, 382, 264, 3218, 295, 264, 32335, 82, 13, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.14662110075658683, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.008980256505310535}, {"id": 2, "seek": 0, "start": 11.3, "end": 12.84, "text": " and what does it output?", "tokens": [50364, 961, 311, 574, 294, 341, 960, 412, 264, 1399, 295, 577, 46533, 2539, 1985, 13, 50733, 50733, 4548, 24420, 2539, 9284, 486, 4846, 257, 1412, 992, 293, 550, 437, 2293, 775, 309, 360, 50929, 50929, 293, 437, 775, 309, 5598, 30, 51006, 51006, 961, 311, 915, 484, 294, 341, 960, 13, 51148, 51148, 9647, 336, 300, 257, 3097, 992, 294, 46533, 2539, 5974, 1293, 264, 4846, 4122, 11, 1270, 382, 51414, 51414, 264, 2744, 295, 264, 32335, 82, 11, 293, 611, 264, 5598, 12911, 11, 1270, 382, 264, 3218, 295, 264, 32335, 82, 13, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.14662110075658683, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.008980256505310535}, {"id": 3, "seek": 0, "start": 12.84, "end": 15.68, "text": " Let's find out in this video.", "tokens": [50364, 961, 311, 574, 294, 341, 960, 412, 264, 1399, 295, 577, 46533, 2539, 1985, 13, 50733, 50733, 4548, 24420, 2539, 9284, 486, 4846, 257, 1412, 992, 293, 550, 437, 2293, 775, 309, 360, 50929, 50929, 293, 437, 775, 309, 5598, 30, 51006, 51006, 961, 311, 915, 484, 294, 341, 960, 13, 51148, 51148, 9647, 336, 300, 257, 3097, 992, 294, 46533, 2539, 5974, 1293, 264, 4846, 4122, 11, 1270, 382, 51414, 51414, 264, 2744, 295, 264, 32335, 82, 11, 293, 611, 264, 5598, 12911, 11, 1270, 382, 264, 3218, 295, 264, 32335, 82, 13, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.14662110075658683, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.008980256505310535}, {"id": 4, "seek": 0, "start": 15.68, "end": 21.0, "text": " Recall that a training set in supervised learning includes both the input features, such as", "tokens": [50364, 961, 311, 574, 294, 341, 960, 412, 264, 1399, 295, 577, 46533, 2539, 1985, 13, 50733, 50733, 4548, 24420, 2539, 9284, 486, 4846, 257, 1412, 992, 293, 550, 437, 2293, 775, 309, 360, 50929, 50929, 293, 437, 775, 309, 5598, 30, 51006, 51006, 961, 311, 915, 484, 294, 341, 960, 13, 51148, 51148, 9647, 336, 300, 257, 3097, 992, 294, 46533, 2539, 5974, 1293, 264, 4846, 4122, 11, 1270, 382, 51414, 51414, 264, 2744, 295, 264, 32335, 82, 11, 293, 611, 264, 5598, 12911, 11, 1270, 382, 264, 3218, 295, 264, 32335, 82, 13, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.14662110075658683, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.008980256505310535}, {"id": 5, "seek": 0, "start": 21.0, "end": 26.16, "text": " the size of the hulls, and also the output targets, such as the price of the hulls.", "tokens": [50364, 961, 311, 574, 294, 341, 960, 412, 264, 1399, 295, 577, 46533, 2539, 1985, 13, 50733, 50733, 4548, 24420, 2539, 9284, 486, 4846, 257, 1412, 992, 293, 550, 437, 2293, 775, 309, 360, 50929, 50929, 293, 437, 775, 309, 5598, 30, 51006, 51006, 961, 311, 915, 484, 294, 341, 960, 13, 51148, 51148, 9647, 336, 300, 257, 3097, 992, 294, 46533, 2539, 5974, 1293, 264, 4846, 4122, 11, 1270, 382, 51414, 51414, 264, 2744, 295, 264, 32335, 82, 11, 293, 611, 264, 5598, 12911, 11, 1270, 382, 264, 3218, 295, 264, 32335, 82, 13, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.14662110075658683, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.008980256505310535}, {"id": 6, "seek": 2616, "start": 26.16, "end": 31.08, "text": " The output targets are the right answers that the model will learn from.", "tokens": [50364, 440, 5598, 12911, 366, 264, 558, 6338, 300, 264, 2316, 486, 1466, 490, 13, 50610, 50610, 1407, 3847, 264, 2316, 11, 291, 3154, 264, 3097, 992, 11, 1293, 264, 4846, 4122, 293, 264, 5598, 50846, 50846, 12911, 11, 281, 428, 2539, 9284, 13, 51068, 51068, 1396, 428, 46533, 2539, 9284, 486, 5258, 512, 2445, 13, 51320, 51320, 492, 603, 2464, 341, 2445, 382, 3126, 9765, 283, 11, 689, 283, 7382, 337, 2445, 13, 51560, 51560, 25108, 984, 11, 341, 2445, 1143, 281, 312, 1219, 257, 17291, 11, 457, 286, 478, 445, 516, 281, 818, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.16019850361103916, "compression_ratio": 1.7991452991452992, "no_speech_prob": 3.1200252124108374e-05}, {"id": 7, "seek": 2616, "start": 31.08, "end": 35.8, "text": " To train the model, you feed the training set, both the input features and the output", "tokens": [50364, 440, 5598, 12911, 366, 264, 558, 6338, 300, 264, 2316, 486, 1466, 490, 13, 50610, 50610, 1407, 3847, 264, 2316, 11, 291, 3154, 264, 3097, 992, 11, 1293, 264, 4846, 4122, 293, 264, 5598, 50846, 50846, 12911, 11, 281, 428, 2539, 9284, 13, 51068, 51068, 1396, 428, 46533, 2539, 9284, 486, 5258, 512, 2445, 13, 51320, 51320, 492, 603, 2464, 341, 2445, 382, 3126, 9765, 283, 11, 689, 283, 7382, 337, 2445, 13, 51560, 51560, 25108, 984, 11, 341, 2445, 1143, 281, 312, 1219, 257, 17291, 11, 457, 286, 478, 445, 516, 281, 818, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.16019850361103916, "compression_ratio": 1.7991452991452992, "no_speech_prob": 3.1200252124108374e-05}, {"id": 8, "seek": 2616, "start": 35.8, "end": 40.24, "text": " targets, to your learning algorithm.", "tokens": [50364, 440, 5598, 12911, 366, 264, 558, 6338, 300, 264, 2316, 486, 1466, 490, 13, 50610, 50610, 1407, 3847, 264, 2316, 11, 291, 3154, 264, 3097, 992, 11, 1293, 264, 4846, 4122, 293, 264, 5598, 50846, 50846, 12911, 11, 281, 428, 2539, 9284, 13, 51068, 51068, 1396, 428, 46533, 2539, 9284, 486, 5258, 512, 2445, 13, 51320, 51320, 492, 603, 2464, 341, 2445, 382, 3126, 9765, 283, 11, 689, 283, 7382, 337, 2445, 13, 51560, 51560, 25108, 984, 11, 341, 2445, 1143, 281, 312, 1219, 257, 17291, 11, 457, 286, 478, 445, 516, 281, 818, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.16019850361103916, "compression_ratio": 1.7991452991452992, "no_speech_prob": 3.1200252124108374e-05}, {"id": 9, "seek": 2616, "start": 40.24, "end": 45.28, "text": " Then your supervised learning algorithm will produce some function.", "tokens": [50364, 440, 5598, 12911, 366, 264, 558, 6338, 300, 264, 2316, 486, 1466, 490, 13, 50610, 50610, 1407, 3847, 264, 2316, 11, 291, 3154, 264, 3097, 992, 11, 1293, 264, 4846, 4122, 293, 264, 5598, 50846, 50846, 12911, 11, 281, 428, 2539, 9284, 13, 51068, 51068, 1396, 428, 46533, 2539, 9284, 486, 5258, 512, 2445, 13, 51320, 51320, 492, 603, 2464, 341, 2445, 382, 3126, 9765, 283, 11, 689, 283, 7382, 337, 2445, 13, 51560, 51560, 25108, 984, 11, 341, 2445, 1143, 281, 312, 1219, 257, 17291, 11, 457, 286, 478, 445, 516, 281, 818, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.16019850361103916, "compression_ratio": 1.7991452991452992, "no_speech_prob": 3.1200252124108374e-05}, {"id": 10, "seek": 2616, "start": 45.28, "end": 50.08, "text": " We'll write this function as lowercase f, where f stands for function.", "tokens": [50364, 440, 5598, 12911, 366, 264, 558, 6338, 300, 264, 2316, 486, 1466, 490, 13, 50610, 50610, 1407, 3847, 264, 2316, 11, 291, 3154, 264, 3097, 992, 11, 1293, 264, 4846, 4122, 293, 264, 5598, 50846, 50846, 12911, 11, 281, 428, 2539, 9284, 13, 51068, 51068, 1396, 428, 46533, 2539, 9284, 486, 5258, 512, 2445, 13, 51320, 51320, 492, 603, 2464, 341, 2445, 382, 3126, 9765, 283, 11, 689, 283, 7382, 337, 2445, 13, 51560, 51560, 25108, 984, 11, 341, 2445, 1143, 281, 312, 1219, 257, 17291, 11, 457, 286, 478, 445, 516, 281, 818, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.16019850361103916, "compression_ratio": 1.7991452991452992, "no_speech_prob": 3.1200252124108374e-05}, {"id": 11, "seek": 2616, "start": 50.08, "end": 55.6, "text": " Historically, this function used to be called a hypothesis, but I'm just going to call", "tokens": [50364, 440, 5598, 12911, 366, 264, 558, 6338, 300, 264, 2316, 486, 1466, 490, 13, 50610, 50610, 1407, 3847, 264, 2316, 11, 291, 3154, 264, 3097, 992, 11, 1293, 264, 4846, 4122, 293, 264, 5598, 50846, 50846, 12911, 11, 281, 428, 2539, 9284, 13, 51068, 51068, 1396, 428, 46533, 2539, 9284, 486, 5258, 512, 2445, 13, 51320, 51320, 492, 603, 2464, 341, 2445, 382, 3126, 9765, 283, 11, 689, 283, 7382, 337, 2445, 13, 51560, 51560, 25108, 984, 11, 341, 2445, 1143, 281, 312, 1219, 257, 17291, 11, 457, 286, 478, 445, 516, 281, 818, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.16019850361103916, "compression_ratio": 1.7991452991452992, "no_speech_prob": 3.1200252124108374e-05}, {"id": 12, "seek": 5560, "start": 55.6, "end": 58.800000000000004, "text": " it a function f in this class.", "tokens": [50364, 309, 257, 2445, 283, 294, 341, 1508, 13, 50524, 50524, 440, 1691, 295, 283, 307, 281, 747, 257, 777, 4846, 2031, 293, 5598, 364, 12539, 420, 17630, 11, 597, 286, 478, 51052, 51052, 516, 281, 818, 288, 2385, 11, 293, 309, 311, 3720, 411, 264, 7006, 288, 365, 341, 707, 2385, 5986, 51406, 51406, 322, 1192, 13, 51510, 51510], "temperature": 0.0, "avg_logprob": -0.1611479618510262, "compression_ratio": 1.4, "no_speech_prob": 1.723102650430519e-05}, {"id": 13, "seek": 5560, "start": 58.800000000000004, "end": 69.36, "text": " The job of f is to take a new input x and output an estimate or prediction, which I'm", "tokens": [50364, 309, 257, 2445, 283, 294, 341, 1508, 13, 50524, 50524, 440, 1691, 295, 283, 307, 281, 747, 257, 777, 4846, 2031, 293, 5598, 364, 12539, 420, 17630, 11, 597, 286, 478, 51052, 51052, 516, 281, 818, 288, 2385, 11, 293, 309, 311, 3720, 411, 264, 7006, 288, 365, 341, 707, 2385, 5986, 51406, 51406, 322, 1192, 13, 51510, 51510], "temperature": 0.0, "avg_logprob": -0.1611479618510262, "compression_ratio": 1.4, "no_speech_prob": 1.723102650430519e-05}, {"id": 14, "seek": 5560, "start": 69.36, "end": 76.44, "text": " going to call y hat, and it's written like the variable y with this little hat symbol", "tokens": [50364, 309, 257, 2445, 283, 294, 341, 1508, 13, 50524, 50524, 440, 1691, 295, 283, 307, 281, 747, 257, 777, 4846, 2031, 293, 5598, 364, 12539, 420, 17630, 11, 597, 286, 478, 51052, 51052, 516, 281, 818, 288, 2385, 11, 293, 309, 311, 3720, 411, 264, 7006, 288, 365, 341, 707, 2385, 5986, 51406, 51406, 322, 1192, 13, 51510, 51510], "temperature": 0.0, "avg_logprob": -0.1611479618510262, "compression_ratio": 1.4, "no_speech_prob": 1.723102650430519e-05}, {"id": 15, "seek": 5560, "start": 76.44, "end": 78.52000000000001, "text": " on top.", "tokens": [50364, 309, 257, 2445, 283, 294, 341, 1508, 13, 50524, 50524, 440, 1691, 295, 283, 307, 281, 747, 257, 777, 4846, 2031, 293, 5598, 364, 12539, 420, 17630, 11, 597, 286, 478, 51052, 51052, 516, 281, 818, 288, 2385, 11, 293, 309, 311, 3720, 411, 264, 7006, 288, 365, 341, 707, 2385, 5986, 51406, 51406, 322, 1192, 13, 51510, 51510], "temperature": 0.0, "avg_logprob": -0.1611479618510262, "compression_ratio": 1.4, "no_speech_prob": 1.723102650430519e-05}, {"id": 16, "seek": 7852, "start": 78.52, "end": 85.84, "text": " In machine learning, the convention is that y hat is the estimate or the prediction for", "tokens": [50364, 682, 3479, 2539, 11, 264, 10286, 307, 300, 288, 2385, 307, 264, 12539, 420, 264, 17630, 337, 50730, 50730, 288, 13, 50838, 50838, 440, 2445, 283, 307, 1219, 264, 2316, 13, 51038, 51038, 1783, 307, 1219, 264, 4846, 420, 264, 4846, 4111, 11, 293, 264, 5598, 295, 264, 2316, 307, 264, 17630, 51394, 51394, 288, 2385, 13, 51494, 51494, 440, 2316, 311, 17630, 307, 264, 14109, 2158, 295, 288, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.12733460761405327, "compression_ratio": 1.8456375838926173, "no_speech_prob": 5.1738602451223414e-06}, {"id": 17, "seek": 7852, "start": 85.84, "end": 88.0, "text": " y.", "tokens": [50364, 682, 3479, 2539, 11, 264, 10286, 307, 300, 288, 2385, 307, 264, 12539, 420, 264, 17630, 337, 50730, 50730, 288, 13, 50838, 50838, 440, 2445, 283, 307, 1219, 264, 2316, 13, 51038, 51038, 1783, 307, 1219, 264, 4846, 420, 264, 4846, 4111, 11, 293, 264, 5598, 295, 264, 2316, 307, 264, 17630, 51394, 51394, 288, 2385, 13, 51494, 51494, 440, 2316, 311, 17630, 307, 264, 14109, 2158, 295, 288, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.12733460761405327, "compression_ratio": 1.8456375838926173, "no_speech_prob": 5.1738602451223414e-06}, {"id": 18, "seek": 7852, "start": 88.0, "end": 92.0, "text": " The function f is called the model.", "tokens": [50364, 682, 3479, 2539, 11, 264, 10286, 307, 300, 288, 2385, 307, 264, 12539, 420, 264, 17630, 337, 50730, 50730, 288, 13, 50838, 50838, 440, 2445, 283, 307, 1219, 264, 2316, 13, 51038, 51038, 1783, 307, 1219, 264, 4846, 420, 264, 4846, 4111, 11, 293, 264, 5598, 295, 264, 2316, 307, 264, 17630, 51394, 51394, 288, 2385, 13, 51494, 51494, 440, 2316, 311, 17630, 307, 264, 14109, 2158, 295, 288, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.12733460761405327, "compression_ratio": 1.8456375838926173, "no_speech_prob": 5.1738602451223414e-06}, {"id": 19, "seek": 7852, "start": 92.0, "end": 99.12, "text": " X is called the input or the input feature, and the output of the model is the prediction", "tokens": [50364, 682, 3479, 2539, 11, 264, 10286, 307, 300, 288, 2385, 307, 264, 12539, 420, 264, 17630, 337, 50730, 50730, 288, 13, 50838, 50838, 440, 2445, 283, 307, 1219, 264, 2316, 13, 51038, 51038, 1783, 307, 1219, 264, 4846, 420, 264, 4846, 4111, 11, 293, 264, 5598, 295, 264, 2316, 307, 264, 17630, 51394, 51394, 288, 2385, 13, 51494, 51494, 440, 2316, 311, 17630, 307, 264, 14109, 2158, 295, 288, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.12733460761405327, "compression_ratio": 1.8456375838926173, "no_speech_prob": 5.1738602451223414e-06}, {"id": 20, "seek": 7852, "start": 99.12, "end": 101.12, "text": " y hat.", "tokens": [50364, 682, 3479, 2539, 11, 264, 10286, 307, 300, 288, 2385, 307, 264, 12539, 420, 264, 17630, 337, 50730, 50730, 288, 13, 50838, 50838, 440, 2445, 283, 307, 1219, 264, 2316, 13, 51038, 51038, 1783, 307, 1219, 264, 4846, 420, 264, 4846, 4111, 11, 293, 264, 5598, 295, 264, 2316, 307, 264, 17630, 51394, 51394, 288, 2385, 13, 51494, 51494, 440, 2316, 311, 17630, 307, 264, 14109, 2158, 295, 288, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.12733460761405327, "compression_ratio": 1.8456375838926173, "no_speech_prob": 5.1738602451223414e-06}, {"id": 21, "seek": 7852, "start": 101.12, "end": 105.88, "text": " The model's prediction is the estimated value of y.", "tokens": [50364, 682, 3479, 2539, 11, 264, 10286, 307, 300, 288, 2385, 307, 264, 12539, 420, 264, 17630, 337, 50730, 50730, 288, 13, 50838, 50838, 440, 2445, 283, 307, 1219, 264, 2316, 13, 51038, 51038, 1783, 307, 1219, 264, 4846, 420, 264, 4846, 4111, 11, 293, 264, 5598, 295, 264, 2316, 307, 264, 17630, 51394, 51394, 288, 2385, 13, 51494, 51494, 440, 2316, 311, 17630, 307, 264, 14109, 2158, 295, 288, 13, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.12733460761405327, "compression_ratio": 1.8456375838926173, "no_speech_prob": 5.1738602451223414e-06}, {"id": 22, "seek": 10588, "start": 105.88, "end": 112.6, "text": " When the symbol is just a letter y, then that refers to the target, which is the actual", "tokens": [50364, 1133, 264, 5986, 307, 445, 257, 5063, 288, 11, 550, 300, 14942, 281, 264, 3779, 11, 597, 307, 264, 3539, 50700, 50700, 2074, 2158, 294, 264, 3097, 992, 13, 50858, 50858, 682, 8712, 11, 288, 2385, 307, 364, 12539, 13, 50998, 50998, 467, 815, 420, 815, 406, 312, 264, 3539, 2074, 2158, 13, 51118, 51118, 1042, 11, 498, 291, 434, 4315, 428, 6423, 281, 3607, 641, 32335, 82, 11, 731, 11, 264, 2074, 3218, 295, 264, 32335, 82, 51400, 51400, 307, 9841, 1826, 436, 3607, 309, 13, 51530, 51530, 407, 428, 2316, 283, 11, 2212, 264, 2744, 11, 23930, 257, 3218, 597, 307, 264, 14109, 11, 300, 307, 264, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.14962616641964532, "compression_ratio": 1.7685589519650655, "no_speech_prob": 2.7967531423200853e-05}, {"id": 23, "seek": 10588, "start": 112.6, "end": 115.75999999999999, "text": " true value in the training set.", "tokens": [50364, 1133, 264, 5986, 307, 445, 257, 5063, 288, 11, 550, 300, 14942, 281, 264, 3779, 11, 597, 307, 264, 3539, 50700, 50700, 2074, 2158, 294, 264, 3097, 992, 13, 50858, 50858, 682, 8712, 11, 288, 2385, 307, 364, 12539, 13, 50998, 50998, 467, 815, 420, 815, 406, 312, 264, 3539, 2074, 2158, 13, 51118, 51118, 1042, 11, 498, 291, 434, 4315, 428, 6423, 281, 3607, 641, 32335, 82, 11, 731, 11, 264, 2074, 3218, 295, 264, 32335, 82, 51400, 51400, 307, 9841, 1826, 436, 3607, 309, 13, 51530, 51530, 407, 428, 2316, 283, 11, 2212, 264, 2744, 11, 23930, 257, 3218, 597, 307, 264, 14109, 11, 300, 307, 264, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.14962616641964532, "compression_ratio": 1.7685589519650655, "no_speech_prob": 2.7967531423200853e-05}, {"id": 24, "seek": 10588, "start": 115.75999999999999, "end": 118.56, "text": " In contrast, y hat is an estimate.", "tokens": [50364, 1133, 264, 5986, 307, 445, 257, 5063, 288, 11, 550, 300, 14942, 281, 264, 3779, 11, 597, 307, 264, 3539, 50700, 50700, 2074, 2158, 294, 264, 3097, 992, 13, 50858, 50858, 682, 8712, 11, 288, 2385, 307, 364, 12539, 13, 50998, 50998, 467, 815, 420, 815, 406, 312, 264, 3539, 2074, 2158, 13, 51118, 51118, 1042, 11, 498, 291, 434, 4315, 428, 6423, 281, 3607, 641, 32335, 82, 11, 731, 11, 264, 2074, 3218, 295, 264, 32335, 82, 51400, 51400, 307, 9841, 1826, 436, 3607, 309, 13, 51530, 51530, 407, 428, 2316, 283, 11, 2212, 264, 2744, 11, 23930, 257, 3218, 597, 307, 264, 14109, 11, 300, 307, 264, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.14962616641964532, "compression_ratio": 1.7685589519650655, "no_speech_prob": 2.7967531423200853e-05}, {"id": 25, "seek": 10588, "start": 118.56, "end": 120.96, "text": " It may or may not be the actual true value.", "tokens": [50364, 1133, 264, 5986, 307, 445, 257, 5063, 288, 11, 550, 300, 14942, 281, 264, 3779, 11, 597, 307, 264, 3539, 50700, 50700, 2074, 2158, 294, 264, 3097, 992, 13, 50858, 50858, 682, 8712, 11, 288, 2385, 307, 364, 12539, 13, 50998, 50998, 467, 815, 420, 815, 406, 312, 264, 3539, 2074, 2158, 13, 51118, 51118, 1042, 11, 498, 291, 434, 4315, 428, 6423, 281, 3607, 641, 32335, 82, 11, 731, 11, 264, 2074, 3218, 295, 264, 32335, 82, 51400, 51400, 307, 9841, 1826, 436, 3607, 309, 13, 51530, 51530, 407, 428, 2316, 283, 11, 2212, 264, 2744, 11, 23930, 257, 3218, 597, 307, 264, 14109, 11, 300, 307, 264, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.14962616641964532, "compression_ratio": 1.7685589519650655, "no_speech_prob": 2.7967531423200853e-05}, {"id": 26, "seek": 10588, "start": 120.96, "end": 126.6, "text": " Well, if you're helping your client to sell their hulls, well, the true price of the hulls", "tokens": [50364, 1133, 264, 5986, 307, 445, 257, 5063, 288, 11, 550, 300, 14942, 281, 264, 3779, 11, 597, 307, 264, 3539, 50700, 50700, 2074, 2158, 294, 264, 3097, 992, 13, 50858, 50858, 682, 8712, 11, 288, 2385, 307, 364, 12539, 13, 50998, 50998, 467, 815, 420, 815, 406, 312, 264, 3539, 2074, 2158, 13, 51118, 51118, 1042, 11, 498, 291, 434, 4315, 428, 6423, 281, 3607, 641, 32335, 82, 11, 731, 11, 264, 2074, 3218, 295, 264, 32335, 82, 51400, 51400, 307, 9841, 1826, 436, 3607, 309, 13, 51530, 51530, 407, 428, 2316, 283, 11, 2212, 264, 2744, 11, 23930, 257, 3218, 597, 307, 264, 14109, 11, 300, 307, 264, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.14962616641964532, "compression_ratio": 1.7685589519650655, "no_speech_prob": 2.7967531423200853e-05}, {"id": 27, "seek": 10588, "start": 126.6, "end": 129.2, "text": " is unknown until they sell it.", "tokens": [50364, 1133, 264, 5986, 307, 445, 257, 5063, 288, 11, 550, 300, 14942, 281, 264, 3779, 11, 597, 307, 264, 3539, 50700, 50700, 2074, 2158, 294, 264, 3097, 992, 13, 50858, 50858, 682, 8712, 11, 288, 2385, 307, 364, 12539, 13, 50998, 50998, 467, 815, 420, 815, 406, 312, 264, 3539, 2074, 2158, 13, 51118, 51118, 1042, 11, 498, 291, 434, 4315, 428, 6423, 281, 3607, 641, 32335, 82, 11, 731, 11, 264, 2074, 3218, 295, 264, 32335, 82, 51400, 51400, 307, 9841, 1826, 436, 3607, 309, 13, 51530, 51530, 407, 428, 2316, 283, 11, 2212, 264, 2744, 11, 23930, 257, 3218, 597, 307, 264, 14109, 11, 300, 307, 264, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.14962616641964532, "compression_ratio": 1.7685589519650655, "no_speech_prob": 2.7967531423200853e-05}, {"id": 28, "seek": 10588, "start": 129.2, "end": 135.2, "text": " So your model f, given the size, outputs a price which is the estimated, that is the", "tokens": [50364, 1133, 264, 5986, 307, 445, 257, 5063, 288, 11, 550, 300, 14942, 281, 264, 3779, 11, 597, 307, 264, 3539, 50700, 50700, 2074, 2158, 294, 264, 3097, 992, 13, 50858, 50858, 682, 8712, 11, 288, 2385, 307, 364, 12539, 13, 50998, 50998, 467, 815, 420, 815, 406, 312, 264, 3539, 2074, 2158, 13, 51118, 51118, 1042, 11, 498, 291, 434, 4315, 428, 6423, 281, 3607, 641, 32335, 82, 11, 731, 11, 264, 2074, 3218, 295, 264, 32335, 82, 51400, 51400, 307, 9841, 1826, 436, 3607, 309, 13, 51530, 51530, 407, 428, 2316, 283, 11, 2212, 264, 2744, 11, 23930, 257, 3218, 597, 307, 264, 14109, 11, 300, 307, 264, 51830, 51830], "temperature": 0.0, "avg_logprob": -0.14962616641964532, "compression_ratio": 1.7685589519650655, "no_speech_prob": 2.7967531423200853e-05}, {"id": 29, "seek": 13520, "start": 135.2, "end": 138.79999999999998, "text": " prediction of what the true price will be.", "tokens": [50364, 17630, 295, 437, 264, 2074, 3218, 486, 312, 13, 50544, 50544, 823, 11, 562, 321, 1715, 257, 2539, 9284, 11, 257, 2141, 1168, 307, 11, 577, 366, 321, 516, 281, 2906, 50938, 50938, 264, 2445, 283, 30, 51026, 51026, 1610, 294, 661, 2283, 11, 437, 307, 264, 5221, 8513, 321, 434, 516, 281, 764, 281, 14722, 283, 30, 51382, 51382, 1171, 586, 11, 718, 311, 2897, 365, 283, 885, 257, 2997, 1622, 13, 51595, 51595], "temperature": 0.0, "avg_logprob": -0.11621221319421546, "compression_ratio": 1.4972972972972973, "no_speech_prob": 6.6433194660930894e-06}, {"id": 30, "seek": 13520, "start": 138.79999999999998, "end": 146.67999999999998, "text": " Now, when we design a learning algorithm, a key question is, how are we going to represent", "tokens": [50364, 17630, 295, 437, 264, 2074, 3218, 486, 312, 13, 50544, 50544, 823, 11, 562, 321, 1715, 257, 2539, 9284, 11, 257, 2141, 1168, 307, 11, 577, 366, 321, 516, 281, 2906, 50938, 50938, 264, 2445, 283, 30, 51026, 51026, 1610, 294, 661, 2283, 11, 437, 307, 264, 5221, 8513, 321, 434, 516, 281, 764, 281, 14722, 283, 30, 51382, 51382, 1171, 586, 11, 718, 311, 2897, 365, 283, 885, 257, 2997, 1622, 13, 51595, 51595], "temperature": 0.0, "avg_logprob": -0.11621221319421546, "compression_ratio": 1.4972972972972973, "no_speech_prob": 6.6433194660930894e-06}, {"id": 31, "seek": 13520, "start": 146.67999999999998, "end": 148.44, "text": " the function f?", "tokens": [50364, 17630, 295, 437, 264, 2074, 3218, 486, 312, 13, 50544, 50544, 823, 11, 562, 321, 1715, 257, 2539, 9284, 11, 257, 2141, 1168, 307, 11, 577, 366, 321, 516, 281, 2906, 50938, 50938, 264, 2445, 283, 30, 51026, 51026, 1610, 294, 661, 2283, 11, 437, 307, 264, 5221, 8513, 321, 434, 516, 281, 764, 281, 14722, 283, 30, 51382, 51382, 1171, 586, 11, 718, 311, 2897, 365, 283, 885, 257, 2997, 1622, 13, 51595, 51595], "temperature": 0.0, "avg_logprob": -0.11621221319421546, "compression_ratio": 1.4972972972972973, "no_speech_prob": 6.6433194660930894e-06}, {"id": 32, "seek": 13520, "start": 148.44, "end": 155.56, "text": " Or in other words, what is the math formula we're going to use to compute f?", "tokens": [50364, 17630, 295, 437, 264, 2074, 3218, 486, 312, 13, 50544, 50544, 823, 11, 562, 321, 1715, 257, 2539, 9284, 11, 257, 2141, 1168, 307, 11, 577, 366, 321, 516, 281, 2906, 50938, 50938, 264, 2445, 283, 30, 51026, 51026, 1610, 294, 661, 2283, 11, 437, 307, 264, 5221, 8513, 321, 434, 516, 281, 764, 281, 14722, 283, 30, 51382, 51382, 1171, 586, 11, 718, 311, 2897, 365, 283, 885, 257, 2997, 1622, 13, 51595, 51595], "temperature": 0.0, "avg_logprob": -0.11621221319421546, "compression_ratio": 1.4972972972972973, "no_speech_prob": 6.6433194660930894e-06}, {"id": 33, "seek": 13520, "start": 155.56, "end": 159.82, "text": " For now, let's stick with f being a straight line.", "tokens": [50364, 17630, 295, 437, 264, 2074, 3218, 486, 312, 13, 50544, 50544, 823, 11, 562, 321, 1715, 257, 2539, 9284, 11, 257, 2141, 1168, 307, 11, 577, 366, 321, 516, 281, 2906, 50938, 50938, 264, 2445, 283, 30, 51026, 51026, 1610, 294, 661, 2283, 11, 437, 307, 264, 5221, 8513, 321, 434, 516, 281, 764, 281, 14722, 283, 30, 51382, 51382, 1171, 586, 11, 718, 311, 2897, 365, 283, 885, 257, 2997, 1622, 13, 51595, 51595], "temperature": 0.0, "avg_logprob": -0.11621221319421546, "compression_ratio": 1.4972972972972973, "no_speech_prob": 6.6433194660930894e-06}, {"id": 34, "seek": 15982, "start": 159.82, "end": 168.88, "text": " So your function can be written as f subscript w comma b of x equals, I'm going to use w", "tokens": [50364, 407, 428, 2445, 393, 312, 3720, 382, 283, 2325, 662, 261, 22117, 272, 295, 2031, 6915, 11, 286, 478, 516, 281, 764, 261, 50817, 50817, 1413, 2031, 1804, 272, 13, 50991, 50991, 286, 603, 6964, 261, 293, 272, 2321, 11, 457, 337, 586, 11, 445, 458, 300, 261, 293, 272, 366, 3547, 11, 293, 264, 4190, 51369, 51369, 8614, 337, 261, 293, 272, 486, 6997, 264, 17630, 288, 2385, 2361, 322, 264, 4846, 4111, 2031, 13, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.11418683528900146, "compression_ratio": 1.508108108108108, "no_speech_prob": 6.438847321987851e-06}, {"id": 35, "seek": 15982, "start": 168.88, "end": 172.35999999999999, "text": " times x plus b.", "tokens": [50364, 407, 428, 2445, 393, 312, 3720, 382, 283, 2325, 662, 261, 22117, 272, 295, 2031, 6915, 11, 286, 478, 516, 281, 764, 261, 50817, 50817, 1413, 2031, 1804, 272, 13, 50991, 50991, 286, 603, 6964, 261, 293, 272, 2321, 11, 457, 337, 586, 11, 445, 458, 300, 261, 293, 272, 366, 3547, 11, 293, 264, 4190, 51369, 51369, 8614, 337, 261, 293, 272, 486, 6997, 264, 17630, 288, 2385, 2361, 322, 264, 4846, 4111, 2031, 13, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.11418683528900146, "compression_ratio": 1.508108108108108, "no_speech_prob": 6.438847321987851e-06}, {"id": 36, "seek": 15982, "start": 172.35999999999999, "end": 179.92, "text": " I'll define w and b soon, but for now, just know that w and b are numbers, and the values", "tokens": [50364, 407, 428, 2445, 393, 312, 3720, 382, 283, 2325, 662, 261, 22117, 272, 295, 2031, 6915, 11, 286, 478, 516, 281, 764, 261, 50817, 50817, 1413, 2031, 1804, 272, 13, 50991, 50991, 286, 603, 6964, 261, 293, 272, 2321, 11, 457, 337, 586, 11, 445, 458, 300, 261, 293, 272, 366, 3547, 11, 293, 264, 4190, 51369, 51369, 8614, 337, 261, 293, 272, 486, 6997, 264, 17630, 288, 2385, 2361, 322, 264, 4846, 4111, 2031, 13, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.11418683528900146, "compression_ratio": 1.508108108108108, "no_speech_prob": 6.438847321987851e-06}, {"id": 37, "seek": 15982, "start": 179.92, "end": 188.44, "text": " chosen for w and b will determine the prediction y hat based on the input feature x.", "tokens": [50364, 407, 428, 2445, 393, 312, 3720, 382, 283, 2325, 662, 261, 22117, 272, 295, 2031, 6915, 11, 286, 478, 516, 281, 764, 261, 50817, 50817, 1413, 2031, 1804, 272, 13, 50991, 50991, 286, 603, 6964, 261, 293, 272, 2321, 11, 457, 337, 586, 11, 445, 458, 300, 261, 293, 272, 366, 3547, 11, 293, 264, 4190, 51369, 51369, 8614, 337, 261, 293, 272, 486, 6997, 264, 17630, 288, 2385, 2361, 322, 264, 4846, 4111, 2031, 13, 51795, 51795], "temperature": 0.0, "avg_logprob": -0.11418683528900146, "compression_ratio": 1.508108108108108, "no_speech_prob": 6.438847321987851e-06}, {"id": 38, "seek": 18844, "start": 188.44, "end": 196.92, "text": " So this f w b of x means f is a function that takes x's input, and depending on the values", "tokens": [50364, 407, 341, 283, 261, 272, 295, 2031, 1355, 283, 307, 257, 2445, 300, 2516, 2031, 311, 4846, 11, 293, 5413, 322, 264, 4190, 50788, 50788, 295, 261, 293, 272, 11, 283, 486, 5598, 512, 2158, 295, 257, 17630, 288, 2385, 13, 51155, 51155, 1018, 364, 8535, 281, 3579, 341, 283, 261, 22117, 272, 295, 2031, 11, 286, 603, 2171, 445, 2464, 283, 295, 2031, 1553, 51562, 51562, 20803, 3009, 261, 293, 272, 294, 264, 2325, 662, 13, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.11628869139117959, "compression_ratio": 1.5591397849462365, "no_speech_prob": 1.706150101199455e-06}, {"id": 39, "seek": 18844, "start": 196.92, "end": 204.26, "text": " of w and b, f will output some value of a prediction y hat.", "tokens": [50364, 407, 341, 283, 261, 272, 295, 2031, 1355, 283, 307, 257, 2445, 300, 2516, 2031, 311, 4846, 11, 293, 5413, 322, 264, 4190, 50788, 50788, 295, 261, 293, 272, 11, 283, 486, 5598, 512, 2158, 295, 257, 17630, 288, 2385, 13, 51155, 51155, 1018, 364, 8535, 281, 3579, 341, 283, 261, 22117, 272, 295, 2031, 11, 286, 603, 2171, 445, 2464, 283, 295, 2031, 1553, 51562, 51562, 20803, 3009, 261, 293, 272, 294, 264, 2325, 662, 13, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.11628869139117959, "compression_ratio": 1.5591397849462365, "no_speech_prob": 1.706150101199455e-06}, {"id": 40, "seek": 18844, "start": 204.26, "end": 212.4, "text": " As an alternative to writing this f w comma b of x, I'll sometimes just write f of x without", "tokens": [50364, 407, 341, 283, 261, 272, 295, 2031, 1355, 283, 307, 257, 2445, 300, 2516, 2031, 311, 4846, 11, 293, 5413, 322, 264, 4190, 50788, 50788, 295, 261, 293, 272, 11, 283, 486, 5598, 512, 2158, 295, 257, 17630, 288, 2385, 13, 51155, 51155, 1018, 364, 8535, 281, 3579, 341, 283, 261, 22117, 272, 295, 2031, 11, 286, 603, 2171, 445, 2464, 283, 295, 2031, 1553, 51562, 51562, 20803, 3009, 261, 293, 272, 294, 264, 2325, 662, 13, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.11628869139117959, "compression_ratio": 1.5591397849462365, "no_speech_prob": 1.706150101199455e-06}, {"id": 41, "seek": 18844, "start": 212.4, "end": 215.68, "text": " explicitly including w and b in the subscript.", "tokens": [50364, 407, 341, 283, 261, 272, 295, 2031, 1355, 283, 307, 257, 2445, 300, 2516, 2031, 311, 4846, 11, 293, 5413, 322, 264, 4190, 50788, 50788, 295, 261, 293, 272, 11, 283, 486, 5598, 512, 2158, 295, 257, 17630, 288, 2385, 13, 51155, 51155, 1018, 364, 8535, 281, 3579, 341, 283, 261, 22117, 272, 295, 2031, 11, 286, 603, 2171, 445, 2464, 283, 295, 2031, 1553, 51562, 51562, 20803, 3009, 261, 293, 272, 294, 264, 2325, 662, 13, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.11628869139117959, "compression_ratio": 1.5591397849462365, "no_speech_prob": 1.706150101199455e-06}, {"id": 42, "seek": 21568, "start": 215.68, "end": 223.24, "text": " It's just a simple notation, but means exactly the same thing as f w b of x.", "tokens": [50364, 467, 311, 445, 257, 2199, 24657, 11, 457, 1355, 2293, 264, 912, 551, 382, 283, 261, 272, 295, 2031, 13, 50742, 50742, 961, 311, 7542, 264, 3097, 992, 322, 264, 4295, 689, 264, 4846, 4111, 2031, 307, 322, 264, 12750, 10298, 11, 51046, 51046, 293, 264, 5598, 3779, 288, 307, 322, 264, 9429, 10298, 13, 51272, 51272, 5459, 11, 264, 9284, 27152, 490, 341, 1412, 293, 23815, 257, 1151, 3318, 1622, 411, 1310, 341, 51622, 51622, 472, 510, 13, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.13633811904723386, "compression_ratio": 1.558252427184466, "no_speech_prob": 1.963788918146747e-06}, {"id": 43, "seek": 21568, "start": 223.24, "end": 229.32, "text": " Let's plot the training set on the graph where the input feature x is on the horizontal axis,", "tokens": [50364, 467, 311, 445, 257, 2199, 24657, 11, 457, 1355, 2293, 264, 912, 551, 382, 283, 261, 272, 295, 2031, 13, 50742, 50742, 961, 311, 7542, 264, 3097, 992, 322, 264, 4295, 689, 264, 4846, 4111, 2031, 307, 322, 264, 12750, 10298, 11, 51046, 51046, 293, 264, 5598, 3779, 288, 307, 322, 264, 9429, 10298, 13, 51272, 51272, 5459, 11, 264, 9284, 27152, 490, 341, 1412, 293, 23815, 257, 1151, 3318, 1622, 411, 1310, 341, 51622, 51622, 472, 510, 13, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.13633811904723386, "compression_ratio": 1.558252427184466, "no_speech_prob": 1.963788918146747e-06}, {"id": 44, "seek": 21568, "start": 229.32, "end": 233.84, "text": " and the output target y is on the vertical axis.", "tokens": [50364, 467, 311, 445, 257, 2199, 24657, 11, 457, 1355, 2293, 264, 912, 551, 382, 283, 261, 272, 295, 2031, 13, 50742, 50742, 961, 311, 7542, 264, 3097, 992, 322, 264, 4295, 689, 264, 4846, 4111, 2031, 307, 322, 264, 12750, 10298, 11, 51046, 51046, 293, 264, 5598, 3779, 288, 307, 322, 264, 9429, 10298, 13, 51272, 51272, 5459, 11, 264, 9284, 27152, 490, 341, 1412, 293, 23815, 257, 1151, 3318, 1622, 411, 1310, 341, 51622, 51622, 472, 510, 13, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.13633811904723386, "compression_ratio": 1.558252427184466, "no_speech_prob": 1.963788918146747e-06}, {"id": 45, "seek": 21568, "start": 233.84, "end": 240.84, "text": " Remember, the algorithm learns from this data and generates a best fit line like maybe this", "tokens": [50364, 467, 311, 445, 257, 2199, 24657, 11, 457, 1355, 2293, 264, 912, 551, 382, 283, 261, 272, 295, 2031, 13, 50742, 50742, 961, 311, 7542, 264, 3097, 992, 322, 264, 4295, 689, 264, 4846, 4111, 2031, 307, 322, 264, 12750, 10298, 11, 51046, 51046, 293, 264, 5598, 3779, 288, 307, 322, 264, 9429, 10298, 13, 51272, 51272, 5459, 11, 264, 9284, 27152, 490, 341, 1412, 293, 23815, 257, 1151, 3318, 1622, 411, 1310, 341, 51622, 51622, 472, 510, 13, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.13633811904723386, "compression_ratio": 1.558252427184466, "no_speech_prob": 1.963788918146747e-06}, {"id": 46, "seek": 21568, "start": 240.84, "end": 242.54000000000002, "text": " one here.", "tokens": [50364, 467, 311, 445, 257, 2199, 24657, 11, 457, 1355, 2293, 264, 912, 551, 382, 283, 261, 272, 295, 2031, 13, 50742, 50742, 961, 311, 7542, 264, 3097, 992, 322, 264, 4295, 689, 264, 4846, 4111, 2031, 307, 322, 264, 12750, 10298, 11, 51046, 51046, 293, 264, 5598, 3779, 288, 307, 322, 264, 9429, 10298, 13, 51272, 51272, 5459, 11, 264, 9284, 27152, 490, 341, 1412, 293, 23815, 257, 1151, 3318, 1622, 411, 1310, 341, 51622, 51622, 472, 510, 13, 51707, 51707], "temperature": 0.0, "avg_logprob": -0.13633811904723386, "compression_ratio": 1.558252427184466, "no_speech_prob": 1.963788918146747e-06}, {"id": 47, "seek": 24254, "start": 242.54, "end": 252.16, "text": " This straight line is the linear function f w b of x equals w times x plus b.", "tokens": [50364, 639, 2997, 1622, 307, 264, 8213, 2445, 283, 261, 272, 295, 2031, 6915, 261, 1413, 2031, 1804, 272, 13, 50845, 50845, 1610, 544, 2935, 11, 321, 393, 3270, 261, 293, 272, 293, 445, 2464, 283, 295, 2031, 6915, 261, 2031, 1804, 272, 13, 51279, 51279, 1692, 311, 437, 341, 2445, 307, 884, 11, 309, 311, 1455, 21264, 337, 264, 2158, 295, 288, 1228, 257, 2997, 51575, 51575, 1622, 2445, 295, 2031, 13, 51671, 51671], "temperature": 0.0, "avg_logprob": -0.12363516656975997, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.4285040379036218e-05}, {"id": 48, "seek": 24254, "start": 252.16, "end": 260.84, "text": " Or more simply, we can drop w and b and just write f of x equals w x plus b.", "tokens": [50364, 639, 2997, 1622, 307, 264, 8213, 2445, 283, 261, 272, 295, 2031, 6915, 261, 1413, 2031, 1804, 272, 13, 50845, 50845, 1610, 544, 2935, 11, 321, 393, 3270, 261, 293, 272, 293, 445, 2464, 283, 295, 2031, 6915, 261, 2031, 1804, 272, 13, 51279, 51279, 1692, 311, 437, 341, 2445, 307, 884, 11, 309, 311, 1455, 21264, 337, 264, 2158, 295, 288, 1228, 257, 2997, 51575, 51575, 1622, 2445, 295, 2031, 13, 51671, 51671], "temperature": 0.0, "avg_logprob": -0.12363516656975997, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.4285040379036218e-05}, {"id": 49, "seek": 24254, "start": 260.84, "end": 266.76, "text": " Here's what this function is doing, it's making predictions for the value of y using a straight", "tokens": [50364, 639, 2997, 1622, 307, 264, 8213, 2445, 283, 261, 272, 295, 2031, 6915, 261, 1413, 2031, 1804, 272, 13, 50845, 50845, 1610, 544, 2935, 11, 321, 393, 3270, 261, 293, 272, 293, 445, 2464, 283, 295, 2031, 6915, 261, 2031, 1804, 272, 13, 51279, 51279, 1692, 311, 437, 341, 2445, 307, 884, 11, 309, 311, 1455, 21264, 337, 264, 2158, 295, 288, 1228, 257, 2997, 51575, 51575, 1622, 2445, 295, 2031, 13, 51671, 51671], "temperature": 0.0, "avg_logprob": -0.12363516656975997, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.4285040379036218e-05}, {"id": 50, "seek": 24254, "start": 266.76, "end": 268.68, "text": " line function of x.", "tokens": [50364, 639, 2997, 1622, 307, 264, 8213, 2445, 283, 261, 272, 295, 2031, 6915, 261, 1413, 2031, 1804, 272, 13, 50845, 50845, 1610, 544, 2935, 11, 321, 393, 3270, 261, 293, 272, 293, 445, 2464, 283, 295, 2031, 6915, 261, 2031, 1804, 272, 13, 51279, 51279, 1692, 311, 437, 341, 2445, 307, 884, 11, 309, 311, 1455, 21264, 337, 264, 2158, 295, 288, 1228, 257, 2997, 51575, 51575, 1622, 2445, 295, 2031, 13, 51671, 51671], "temperature": 0.0, "avg_logprob": -0.12363516656975997, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.4285040379036218e-05}, {"id": 51, "seek": 26868, "start": 268.68, "end": 274.54, "text": " So you may ask, why are we choosing a linear function where linear function is just a fancy", "tokens": [50364, 407, 291, 815, 1029, 11, 983, 366, 321, 10875, 257, 8213, 2445, 689, 8213, 2445, 307, 445, 257, 10247, 50657, 50657, 1433, 337, 257, 2997, 1622, 11, 2602, 295, 512, 2107, 28263, 2445, 411, 257, 7605, 420, 257, 45729, 4711, 30, 50972, 50972, 1042, 11, 2171, 291, 528, 281, 3318, 544, 3997, 2107, 28263, 6828, 382, 731, 11, 411, 257, 7605, 51256, 51256, 411, 341, 11, 457, 1670, 341, 8213, 2445, 307, 7226, 2199, 293, 1858, 281, 589, 365, 11, 51538, 51538, 718, 311, 764, 257, 1622, 382, 257, 7030, 300, 486, 4728, 854, 291, 281, 483, 281, 544, 3997, 51789, 51789], "temperature": 0.0, "avg_logprob": -0.11289734106797439, "compression_ratio": 1.8523206751054853, "no_speech_prob": 2.295902277182904e-06}, {"id": 52, "seek": 26868, "start": 274.54, "end": 280.84000000000003, "text": " term for a straight line, instead of some nonlinear function like a curve or a parabola?", "tokens": [50364, 407, 291, 815, 1029, 11, 983, 366, 321, 10875, 257, 8213, 2445, 689, 8213, 2445, 307, 445, 257, 10247, 50657, 50657, 1433, 337, 257, 2997, 1622, 11, 2602, 295, 512, 2107, 28263, 2445, 411, 257, 7605, 420, 257, 45729, 4711, 30, 50972, 50972, 1042, 11, 2171, 291, 528, 281, 3318, 544, 3997, 2107, 28263, 6828, 382, 731, 11, 411, 257, 7605, 51256, 51256, 411, 341, 11, 457, 1670, 341, 8213, 2445, 307, 7226, 2199, 293, 1858, 281, 589, 365, 11, 51538, 51538, 718, 311, 764, 257, 1622, 382, 257, 7030, 300, 486, 4728, 854, 291, 281, 483, 281, 544, 3997, 51789, 51789], "temperature": 0.0, "avg_logprob": -0.11289734106797439, "compression_ratio": 1.8523206751054853, "no_speech_prob": 2.295902277182904e-06}, {"id": 53, "seek": 26868, "start": 280.84000000000003, "end": 286.52, "text": " Well, sometimes you want to fit more complex nonlinear functions as well, like a curve", "tokens": [50364, 407, 291, 815, 1029, 11, 983, 366, 321, 10875, 257, 8213, 2445, 689, 8213, 2445, 307, 445, 257, 10247, 50657, 50657, 1433, 337, 257, 2997, 1622, 11, 2602, 295, 512, 2107, 28263, 2445, 411, 257, 7605, 420, 257, 45729, 4711, 30, 50972, 50972, 1042, 11, 2171, 291, 528, 281, 3318, 544, 3997, 2107, 28263, 6828, 382, 731, 11, 411, 257, 7605, 51256, 51256, 411, 341, 11, 457, 1670, 341, 8213, 2445, 307, 7226, 2199, 293, 1858, 281, 589, 365, 11, 51538, 51538, 718, 311, 764, 257, 1622, 382, 257, 7030, 300, 486, 4728, 854, 291, 281, 483, 281, 544, 3997, 51789, 51789], "temperature": 0.0, "avg_logprob": -0.11289734106797439, "compression_ratio": 1.8523206751054853, "no_speech_prob": 2.295902277182904e-06}, {"id": 54, "seek": 26868, "start": 286.52, "end": 292.16, "text": " like this, but since this linear function is relatively simple and easy to work with,", "tokens": [50364, 407, 291, 815, 1029, 11, 983, 366, 321, 10875, 257, 8213, 2445, 689, 8213, 2445, 307, 445, 257, 10247, 50657, 50657, 1433, 337, 257, 2997, 1622, 11, 2602, 295, 512, 2107, 28263, 2445, 411, 257, 7605, 420, 257, 45729, 4711, 30, 50972, 50972, 1042, 11, 2171, 291, 528, 281, 3318, 544, 3997, 2107, 28263, 6828, 382, 731, 11, 411, 257, 7605, 51256, 51256, 411, 341, 11, 457, 1670, 341, 8213, 2445, 307, 7226, 2199, 293, 1858, 281, 589, 365, 11, 51538, 51538, 718, 311, 764, 257, 1622, 382, 257, 7030, 300, 486, 4728, 854, 291, 281, 483, 281, 544, 3997, 51789, 51789], "temperature": 0.0, "avg_logprob": -0.11289734106797439, "compression_ratio": 1.8523206751054853, "no_speech_prob": 2.295902277182904e-06}, {"id": 55, "seek": 26868, "start": 292.16, "end": 297.18, "text": " let's use a line as a foundation that will eventually help you to get to more complex", "tokens": [50364, 407, 291, 815, 1029, 11, 983, 366, 321, 10875, 257, 8213, 2445, 689, 8213, 2445, 307, 445, 257, 10247, 50657, 50657, 1433, 337, 257, 2997, 1622, 11, 2602, 295, 512, 2107, 28263, 2445, 411, 257, 7605, 420, 257, 45729, 4711, 30, 50972, 50972, 1042, 11, 2171, 291, 528, 281, 3318, 544, 3997, 2107, 28263, 6828, 382, 731, 11, 411, 257, 7605, 51256, 51256, 411, 341, 11, 457, 1670, 341, 8213, 2445, 307, 7226, 2199, 293, 1858, 281, 589, 365, 11, 51538, 51538, 718, 311, 764, 257, 1622, 382, 257, 7030, 300, 486, 4728, 854, 291, 281, 483, 281, 544, 3997, 51789, 51789], "temperature": 0.0, "avg_logprob": -0.11289734106797439, "compression_ratio": 1.8523206751054853, "no_speech_prob": 2.295902277182904e-06}, {"id": 56, "seek": 29718, "start": 297.18, "end": 300.44, "text": " models that are nonlinear.", "tokens": [50364, 5245, 300, 366, 2107, 28263, 13, 50527, 50527, 639, 1729, 2316, 575, 257, 1315, 11, 309, 311, 1219, 8213, 24590, 13, 50743, 50743, 5048, 4682, 11, 341, 307, 8213, 24590, 365, 472, 7006, 11, 689, 264, 9535, 472, 7006, 51041, 51041, 1355, 300, 456, 311, 257, 2167, 4846, 7006, 420, 4111, 2031, 11, 20926, 264, 2744, 295, 264, 1782, 13, 51391, 51391, 3996, 1315, 337, 257, 8213, 2316, 365, 472, 4846, 7006, 307, 517, 592, 3504, 473, 8213, 24590, 11, 51713, 51713], "temperature": 0.0, "avg_logprob": -0.12338649658929735, "compression_ratio": 1.7960199004975124, "no_speech_prob": 4.565934887068579e-06}, {"id": 57, "seek": 29718, "start": 300.44, "end": 304.76, "text": " This particular model has a name, it's called linear regression.", "tokens": [50364, 5245, 300, 366, 2107, 28263, 13, 50527, 50527, 639, 1729, 2316, 575, 257, 1315, 11, 309, 311, 1219, 8213, 24590, 13, 50743, 50743, 5048, 4682, 11, 341, 307, 8213, 24590, 365, 472, 7006, 11, 689, 264, 9535, 472, 7006, 51041, 51041, 1355, 300, 456, 311, 257, 2167, 4846, 7006, 420, 4111, 2031, 11, 20926, 264, 2744, 295, 264, 1782, 13, 51391, 51391, 3996, 1315, 337, 257, 8213, 2316, 365, 472, 4846, 7006, 307, 517, 592, 3504, 473, 8213, 24590, 11, 51713, 51713], "temperature": 0.0, "avg_logprob": -0.12338649658929735, "compression_ratio": 1.7960199004975124, "no_speech_prob": 4.565934887068579e-06}, {"id": 58, "seek": 29718, "start": 304.76, "end": 310.72, "text": " More specifically, this is linear regression with one variable, where the phrase one variable", "tokens": [50364, 5245, 300, 366, 2107, 28263, 13, 50527, 50527, 639, 1729, 2316, 575, 257, 1315, 11, 309, 311, 1219, 8213, 24590, 13, 50743, 50743, 5048, 4682, 11, 341, 307, 8213, 24590, 365, 472, 7006, 11, 689, 264, 9535, 472, 7006, 51041, 51041, 1355, 300, 456, 311, 257, 2167, 4846, 7006, 420, 4111, 2031, 11, 20926, 264, 2744, 295, 264, 1782, 13, 51391, 51391, 3996, 1315, 337, 257, 8213, 2316, 365, 472, 4846, 7006, 307, 517, 592, 3504, 473, 8213, 24590, 11, 51713, 51713], "temperature": 0.0, "avg_logprob": -0.12338649658929735, "compression_ratio": 1.7960199004975124, "no_speech_prob": 4.565934887068579e-06}, {"id": 59, "seek": 29718, "start": 310.72, "end": 317.72, "text": " means that there's a single input variable or feature x, namely the size of the house.", "tokens": [50364, 5245, 300, 366, 2107, 28263, 13, 50527, 50527, 639, 1729, 2316, 575, 257, 1315, 11, 309, 311, 1219, 8213, 24590, 13, 50743, 50743, 5048, 4682, 11, 341, 307, 8213, 24590, 365, 472, 7006, 11, 689, 264, 9535, 472, 7006, 51041, 51041, 1355, 300, 456, 311, 257, 2167, 4846, 7006, 420, 4111, 2031, 11, 20926, 264, 2744, 295, 264, 1782, 13, 51391, 51391, 3996, 1315, 337, 257, 8213, 2316, 365, 472, 4846, 7006, 307, 517, 592, 3504, 473, 8213, 24590, 11, 51713, 51713], "temperature": 0.0, "avg_logprob": -0.12338649658929735, "compression_ratio": 1.7960199004975124, "no_speech_prob": 4.565934887068579e-06}, {"id": 60, "seek": 29718, "start": 317.72, "end": 324.16, "text": " Another name for a linear model with one input variable is univariate linear regression,", "tokens": [50364, 5245, 300, 366, 2107, 28263, 13, 50527, 50527, 639, 1729, 2316, 575, 257, 1315, 11, 309, 311, 1219, 8213, 24590, 13, 50743, 50743, 5048, 4682, 11, 341, 307, 8213, 24590, 365, 472, 7006, 11, 689, 264, 9535, 472, 7006, 51041, 51041, 1355, 300, 456, 311, 257, 2167, 4846, 7006, 420, 4111, 2031, 11, 20926, 264, 2744, 295, 264, 1782, 13, 51391, 51391, 3996, 1315, 337, 257, 8213, 2316, 365, 472, 4846, 7006, 307, 517, 592, 3504, 473, 8213, 24590, 11, 51713, 51713], "temperature": 0.0, "avg_logprob": -0.12338649658929735, "compression_ratio": 1.7960199004975124, "no_speech_prob": 4.565934887068579e-06}, {"id": 61, "seek": 32416, "start": 324.16, "end": 329.8, "text": " where uni means one in Latin and where variate means variable.", "tokens": [50364, 689, 36435, 1355, 472, 294, 10803, 293, 689, 3034, 473, 1355, 7006, 13, 50646, 50646, 407, 517, 592, 3504, 473, 307, 445, 257, 10247, 636, 295, 1566, 472, 7006, 13, 50896, 50896, 682, 257, 1780, 960, 11, 291, 603, 611, 536, 257, 12990, 295, 24590, 689, 291, 528, 281, 652, 257, 17630, 51184, 51184, 2361, 406, 445, 322, 264, 2744, 295, 257, 1782, 11, 457, 322, 257, 3840, 295, 661, 721, 300, 291, 815, 458, 51432, 51432, 466, 264, 1782, 11, 1270, 382, 1230, 295, 39955, 293, 661, 4122, 13, 51616, 51616, 400, 538, 264, 636, 11, 562, 291, 434, 1096, 365, 341, 960, 11, 456, 307, 1071, 17312, 2715, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.11409266098685887, "compression_ratio": 1.7203065134099618, "no_speech_prob": 8.664467713970225e-06}, {"id": 62, "seek": 32416, "start": 329.8, "end": 334.8, "text": " So univariate is just a fancy way of saying one variable.", "tokens": [50364, 689, 36435, 1355, 472, 294, 10803, 293, 689, 3034, 473, 1355, 7006, 13, 50646, 50646, 407, 517, 592, 3504, 473, 307, 445, 257, 10247, 636, 295, 1566, 472, 7006, 13, 50896, 50896, 682, 257, 1780, 960, 11, 291, 603, 611, 536, 257, 12990, 295, 24590, 689, 291, 528, 281, 652, 257, 17630, 51184, 51184, 2361, 406, 445, 322, 264, 2744, 295, 257, 1782, 11, 457, 322, 257, 3840, 295, 661, 721, 300, 291, 815, 458, 51432, 51432, 466, 264, 1782, 11, 1270, 382, 1230, 295, 39955, 293, 661, 4122, 13, 51616, 51616, 400, 538, 264, 636, 11, 562, 291, 434, 1096, 365, 341, 960, 11, 456, 307, 1071, 17312, 2715, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.11409266098685887, "compression_ratio": 1.7203065134099618, "no_speech_prob": 8.664467713970225e-06}, {"id": 63, "seek": 32416, "start": 334.8, "end": 340.56, "text": " In a later video, you'll also see a variation of regression where you want to make a prediction", "tokens": [50364, 689, 36435, 1355, 472, 294, 10803, 293, 689, 3034, 473, 1355, 7006, 13, 50646, 50646, 407, 517, 592, 3504, 473, 307, 445, 257, 10247, 636, 295, 1566, 472, 7006, 13, 50896, 50896, 682, 257, 1780, 960, 11, 291, 603, 611, 536, 257, 12990, 295, 24590, 689, 291, 528, 281, 652, 257, 17630, 51184, 51184, 2361, 406, 445, 322, 264, 2744, 295, 257, 1782, 11, 457, 322, 257, 3840, 295, 661, 721, 300, 291, 815, 458, 51432, 51432, 466, 264, 1782, 11, 1270, 382, 1230, 295, 39955, 293, 661, 4122, 13, 51616, 51616, 400, 538, 264, 636, 11, 562, 291, 434, 1096, 365, 341, 960, 11, 456, 307, 1071, 17312, 2715, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.11409266098685887, "compression_ratio": 1.7203065134099618, "no_speech_prob": 8.664467713970225e-06}, {"id": 64, "seek": 32416, "start": 340.56, "end": 345.52000000000004, "text": " based not just on the size of a house, but on a bunch of other things that you may know", "tokens": [50364, 689, 36435, 1355, 472, 294, 10803, 293, 689, 3034, 473, 1355, 7006, 13, 50646, 50646, 407, 517, 592, 3504, 473, 307, 445, 257, 10247, 636, 295, 1566, 472, 7006, 13, 50896, 50896, 682, 257, 1780, 960, 11, 291, 603, 611, 536, 257, 12990, 295, 24590, 689, 291, 528, 281, 652, 257, 17630, 51184, 51184, 2361, 406, 445, 322, 264, 2744, 295, 257, 1782, 11, 457, 322, 257, 3840, 295, 661, 721, 300, 291, 815, 458, 51432, 51432, 466, 264, 1782, 11, 1270, 382, 1230, 295, 39955, 293, 661, 4122, 13, 51616, 51616, 400, 538, 264, 636, 11, 562, 291, 434, 1096, 365, 341, 960, 11, 456, 307, 1071, 17312, 2715, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.11409266098685887, "compression_ratio": 1.7203065134099618, "no_speech_prob": 8.664467713970225e-06}, {"id": 65, "seek": 32416, "start": 345.52000000000004, "end": 349.20000000000005, "text": " about the house, such as number of bedrooms and other features.", "tokens": [50364, 689, 36435, 1355, 472, 294, 10803, 293, 689, 3034, 473, 1355, 7006, 13, 50646, 50646, 407, 517, 592, 3504, 473, 307, 445, 257, 10247, 636, 295, 1566, 472, 7006, 13, 50896, 50896, 682, 257, 1780, 960, 11, 291, 603, 611, 536, 257, 12990, 295, 24590, 689, 291, 528, 281, 652, 257, 17630, 51184, 51184, 2361, 406, 445, 322, 264, 2744, 295, 257, 1782, 11, 457, 322, 257, 3840, 295, 661, 721, 300, 291, 815, 458, 51432, 51432, 466, 264, 1782, 11, 1270, 382, 1230, 295, 39955, 293, 661, 4122, 13, 51616, 51616, 400, 538, 264, 636, 11, 562, 291, 434, 1096, 365, 341, 960, 11, 456, 307, 1071, 17312, 2715, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.11409266098685887, "compression_ratio": 1.7203065134099618, "no_speech_prob": 8.664467713970225e-06}, {"id": 66, "seek": 32416, "start": 349.20000000000005, "end": 353.92, "text": " And by the way, when you're done with this video, there is another optional lab.", "tokens": [50364, 689, 36435, 1355, 472, 294, 10803, 293, 689, 3034, 473, 1355, 7006, 13, 50646, 50646, 407, 517, 592, 3504, 473, 307, 445, 257, 10247, 636, 295, 1566, 472, 7006, 13, 50896, 50896, 682, 257, 1780, 960, 11, 291, 603, 611, 536, 257, 12990, 295, 24590, 689, 291, 528, 281, 652, 257, 17630, 51184, 51184, 2361, 406, 445, 322, 264, 2744, 295, 257, 1782, 11, 457, 322, 257, 3840, 295, 661, 721, 300, 291, 815, 458, 51432, 51432, 466, 264, 1782, 11, 1270, 382, 1230, 295, 39955, 293, 661, 4122, 13, 51616, 51616, 400, 538, 264, 636, 11, 562, 291, 434, 1096, 365, 341, 960, 11, 456, 307, 1071, 17312, 2715, 13, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.11409266098685887, "compression_ratio": 1.7203065134099618, "no_speech_prob": 8.664467713970225e-06}, {"id": 67, "seek": 35392, "start": 353.92, "end": 358.92, "text": " You don't need to write any code, just review it, run the code and see what it does.", "tokens": [50364, 509, 500, 380, 643, 281, 2464, 604, 3089, 11, 445, 3131, 309, 11, 1190, 264, 3089, 293, 536, 437, 309, 775, 13, 50614, 50614, 663, 486, 855, 291, 577, 281, 6964, 294, 15329, 257, 2997, 1622, 2445, 13, 50848, 50848, 400, 264, 2715, 486, 718, 291, 2826, 264, 4190, 295, 343, 293, 363, 281, 853, 281, 3318, 264, 3097, 1412, 13, 51196, 51196, 509, 500, 380, 362, 281, 360, 264, 2715, 498, 291, 500, 380, 528, 281, 11, 457, 286, 1454, 291, 862, 295, 309, 562, 291, 434, 51394, 51394, 1096, 1976, 341, 960, 13, 51528, 51528, 407, 300, 311, 8213, 24590, 13, 51635, 51635, 682, 1668, 337, 291, 281, 652, 341, 589, 11, 472, 295, 264, 881, 1021, 721, 291, 362, 281, 360, 307, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.11141312029934669, "compression_ratio": 1.6761565836298933, "no_speech_prob": 4.399429599288851e-05}, {"id": 68, "seek": 35392, "start": 358.92, "end": 363.6, "text": " That will show you how to define in Python a straight line function.", "tokens": [50364, 509, 500, 380, 643, 281, 2464, 604, 3089, 11, 445, 3131, 309, 11, 1190, 264, 3089, 293, 536, 437, 309, 775, 13, 50614, 50614, 663, 486, 855, 291, 577, 281, 6964, 294, 15329, 257, 2997, 1622, 2445, 13, 50848, 50848, 400, 264, 2715, 486, 718, 291, 2826, 264, 4190, 295, 343, 293, 363, 281, 853, 281, 3318, 264, 3097, 1412, 13, 51196, 51196, 509, 500, 380, 362, 281, 360, 264, 2715, 498, 291, 500, 380, 528, 281, 11, 457, 286, 1454, 291, 862, 295, 309, 562, 291, 434, 51394, 51394, 1096, 1976, 341, 960, 13, 51528, 51528, 407, 300, 311, 8213, 24590, 13, 51635, 51635, 682, 1668, 337, 291, 281, 652, 341, 589, 11, 472, 295, 264, 881, 1021, 721, 291, 362, 281, 360, 307, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.11141312029934669, "compression_ratio": 1.6761565836298933, "no_speech_prob": 4.399429599288851e-05}, {"id": 69, "seek": 35392, "start": 363.6, "end": 370.56, "text": " And the lab will let you choose the values of W and B to try to fit the training data.", "tokens": [50364, 509, 500, 380, 643, 281, 2464, 604, 3089, 11, 445, 3131, 309, 11, 1190, 264, 3089, 293, 536, 437, 309, 775, 13, 50614, 50614, 663, 486, 855, 291, 577, 281, 6964, 294, 15329, 257, 2997, 1622, 2445, 13, 50848, 50848, 400, 264, 2715, 486, 718, 291, 2826, 264, 4190, 295, 343, 293, 363, 281, 853, 281, 3318, 264, 3097, 1412, 13, 51196, 51196, 509, 500, 380, 362, 281, 360, 264, 2715, 498, 291, 500, 380, 528, 281, 11, 457, 286, 1454, 291, 862, 295, 309, 562, 291, 434, 51394, 51394, 1096, 1976, 341, 960, 13, 51528, 51528, 407, 300, 311, 8213, 24590, 13, 51635, 51635, 682, 1668, 337, 291, 281, 652, 341, 589, 11, 472, 295, 264, 881, 1021, 721, 291, 362, 281, 360, 307, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.11141312029934669, "compression_ratio": 1.6761565836298933, "no_speech_prob": 4.399429599288851e-05}, {"id": 70, "seek": 35392, "start": 370.56, "end": 374.52000000000004, "text": " You don't have to do the lab if you don't want to, but I hope you play of it when you're", "tokens": [50364, 509, 500, 380, 643, 281, 2464, 604, 3089, 11, 445, 3131, 309, 11, 1190, 264, 3089, 293, 536, 437, 309, 775, 13, 50614, 50614, 663, 486, 855, 291, 577, 281, 6964, 294, 15329, 257, 2997, 1622, 2445, 13, 50848, 50848, 400, 264, 2715, 486, 718, 291, 2826, 264, 4190, 295, 343, 293, 363, 281, 853, 281, 3318, 264, 3097, 1412, 13, 51196, 51196, 509, 500, 380, 362, 281, 360, 264, 2715, 498, 291, 500, 380, 528, 281, 11, 457, 286, 1454, 291, 862, 295, 309, 562, 291, 434, 51394, 51394, 1096, 1976, 341, 960, 13, 51528, 51528, 407, 300, 311, 8213, 24590, 13, 51635, 51635, 682, 1668, 337, 291, 281, 652, 341, 589, 11, 472, 295, 264, 881, 1021, 721, 291, 362, 281, 360, 307, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.11141312029934669, "compression_ratio": 1.6761565836298933, "no_speech_prob": 4.399429599288851e-05}, {"id": 71, "seek": 35392, "start": 374.52000000000004, "end": 377.20000000000005, "text": " done watching this video.", "tokens": [50364, 509, 500, 380, 643, 281, 2464, 604, 3089, 11, 445, 3131, 309, 11, 1190, 264, 3089, 293, 536, 437, 309, 775, 13, 50614, 50614, 663, 486, 855, 291, 577, 281, 6964, 294, 15329, 257, 2997, 1622, 2445, 13, 50848, 50848, 400, 264, 2715, 486, 718, 291, 2826, 264, 4190, 295, 343, 293, 363, 281, 853, 281, 3318, 264, 3097, 1412, 13, 51196, 51196, 509, 500, 380, 362, 281, 360, 264, 2715, 498, 291, 500, 380, 528, 281, 11, 457, 286, 1454, 291, 862, 295, 309, 562, 291, 434, 51394, 51394, 1096, 1976, 341, 960, 13, 51528, 51528, 407, 300, 311, 8213, 24590, 13, 51635, 51635, 682, 1668, 337, 291, 281, 652, 341, 589, 11, 472, 295, 264, 881, 1021, 721, 291, 362, 281, 360, 307, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.11141312029934669, "compression_ratio": 1.6761565836298933, "no_speech_prob": 4.399429599288851e-05}, {"id": 72, "seek": 35392, "start": 377.20000000000005, "end": 379.34000000000003, "text": " So that's linear regression.", "tokens": [50364, 509, 500, 380, 643, 281, 2464, 604, 3089, 11, 445, 3131, 309, 11, 1190, 264, 3089, 293, 536, 437, 309, 775, 13, 50614, 50614, 663, 486, 855, 291, 577, 281, 6964, 294, 15329, 257, 2997, 1622, 2445, 13, 50848, 50848, 400, 264, 2715, 486, 718, 291, 2826, 264, 4190, 295, 343, 293, 363, 281, 853, 281, 3318, 264, 3097, 1412, 13, 51196, 51196, 509, 500, 380, 362, 281, 360, 264, 2715, 498, 291, 500, 380, 528, 281, 11, 457, 286, 1454, 291, 862, 295, 309, 562, 291, 434, 51394, 51394, 1096, 1976, 341, 960, 13, 51528, 51528, 407, 300, 311, 8213, 24590, 13, 51635, 51635, 682, 1668, 337, 291, 281, 652, 341, 589, 11, 472, 295, 264, 881, 1021, 721, 291, 362, 281, 360, 307, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.11141312029934669, "compression_ratio": 1.6761565836298933, "no_speech_prob": 4.399429599288851e-05}, {"id": 73, "seek": 35392, "start": 379.34000000000003, "end": 382.96000000000004, "text": " In order for you to make this work, one of the most important things you have to do is", "tokens": [50364, 509, 500, 380, 643, 281, 2464, 604, 3089, 11, 445, 3131, 309, 11, 1190, 264, 3089, 293, 536, 437, 309, 775, 13, 50614, 50614, 663, 486, 855, 291, 577, 281, 6964, 294, 15329, 257, 2997, 1622, 2445, 13, 50848, 50848, 400, 264, 2715, 486, 718, 291, 2826, 264, 4190, 295, 343, 293, 363, 281, 853, 281, 3318, 264, 3097, 1412, 13, 51196, 51196, 509, 500, 380, 362, 281, 360, 264, 2715, 498, 291, 500, 380, 528, 281, 11, 457, 286, 1454, 291, 862, 295, 309, 562, 291, 434, 51394, 51394, 1096, 1976, 341, 960, 13, 51528, 51528, 407, 300, 311, 8213, 24590, 13, 51635, 51635, 682, 1668, 337, 291, 281, 652, 341, 589, 11, 472, 295, 264, 881, 1021, 721, 291, 362, 281, 360, 307, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.11141312029934669, "compression_ratio": 1.6761565836298933, "no_speech_prob": 4.399429599288851e-05}, {"id": 74, "seek": 38296, "start": 382.96, "end": 385.47999999999996, "text": " construct a cost function.", "tokens": [50364, 7690, 257, 2063, 2445, 13, 50490, 50490, 440, 1558, 295, 257, 2063, 2445, 307, 472, 295, 264, 881, 11455, 293, 1021, 3487, 294, 3479, 50730, 50730, 2539, 293, 307, 1143, 294, 1293, 8213, 24590, 293, 294, 3097, 867, 295, 264, 881, 7339, 51012, 51012, 7318, 5245, 294, 264, 1002, 13, 51106, 51106, 407, 718, 311, 352, 322, 281, 264, 958, 960, 293, 747, 257, 574, 412, 577, 291, 393, 7690, 257, 2063, 2445, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1364894099049754, "compression_ratio": 1.7096774193548387, "no_speech_prob": 2.2457561499322765e-05}, {"id": 75, "seek": 38296, "start": 385.47999999999996, "end": 390.28, "text": " The idea of a cost function is one of the most universal and important ideas in machine", "tokens": [50364, 7690, 257, 2063, 2445, 13, 50490, 50490, 440, 1558, 295, 257, 2063, 2445, 307, 472, 295, 264, 881, 11455, 293, 1021, 3487, 294, 3479, 50730, 50730, 2539, 293, 307, 1143, 294, 1293, 8213, 24590, 293, 294, 3097, 867, 295, 264, 881, 7339, 51012, 51012, 7318, 5245, 294, 264, 1002, 13, 51106, 51106, 407, 718, 311, 352, 322, 281, 264, 958, 960, 293, 747, 257, 574, 412, 577, 291, 393, 7690, 257, 2063, 2445, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1364894099049754, "compression_ratio": 1.7096774193548387, "no_speech_prob": 2.2457561499322765e-05}, {"id": 76, "seek": 38296, "start": 390.28, "end": 395.91999999999996, "text": " learning and is used in both linear regression and in training many of the most advanced", "tokens": [50364, 7690, 257, 2063, 2445, 13, 50490, 50490, 440, 1558, 295, 257, 2063, 2445, 307, 472, 295, 264, 881, 11455, 293, 1021, 3487, 294, 3479, 50730, 50730, 2539, 293, 307, 1143, 294, 1293, 8213, 24590, 293, 294, 3097, 867, 295, 264, 881, 7339, 51012, 51012, 7318, 5245, 294, 264, 1002, 13, 51106, 51106, 407, 718, 311, 352, 322, 281, 264, 958, 960, 293, 747, 257, 574, 412, 577, 291, 393, 7690, 257, 2063, 2445, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1364894099049754, "compression_ratio": 1.7096774193548387, "no_speech_prob": 2.2457561499322765e-05}, {"id": 77, "seek": 38296, "start": 395.91999999999996, "end": 397.79999999999995, "text": " AI models in the world.", "tokens": [50364, 7690, 257, 2063, 2445, 13, 50490, 50490, 440, 1558, 295, 257, 2063, 2445, 307, 472, 295, 264, 881, 11455, 293, 1021, 3487, 294, 3479, 50730, 50730, 2539, 293, 307, 1143, 294, 1293, 8213, 24590, 293, 294, 3097, 867, 295, 264, 881, 7339, 51012, 51012, 7318, 5245, 294, 264, 1002, 13, 51106, 51106, 407, 718, 311, 352, 322, 281, 264, 958, 960, 293, 747, 257, 574, 412, 577, 291, 393, 7690, 257, 2063, 2445, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1364894099049754, "compression_ratio": 1.7096774193548387, "no_speech_prob": 2.2457561499322765e-05}, {"id": 78, "seek": 39780, "start": 397.8, "end": 414.40000000000003, "text": " So let's go on to the next video and take a look at how you can construct a cost function.", "tokens": [50364, 407, 718, 311, 352, 322, 281, 264, 958, 960, 293, 747, 257, 574, 412, 577, 291, 393, 7690, 257, 2063, 2445, 13, 51194, 51194], "temperature": 0.0, "avg_logprob": -0.2051518330207238, "compression_ratio": 1.1111111111111112, "no_speech_prob": 6.632557870034361e-06}], "language": "en", "video_id": "vrTHO5zRq6s", "entity": "ML Specialization, Andrew Ng (2022)"}}