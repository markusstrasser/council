{"video_id": "GY0KyF3h8hA", "title": "1.14 Machine Learning Overview | Visualizing examples --[Machine Learning | Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 360, "views": 374, "publish_date": "11/04/2022", "timestamp": 1660953600, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " Let's look at some more visualizations of w and b. Here's one example. Over here, you have a particular point on the graph J. For this point, w equals about negative 0.15 and b equals about 800. So this point corresponds to one pair of values for w and b that use a particular cost J. And in fact, this particular pair of values for w and b corresponds to this function f of x, which is this line that you can see on the left. This line intersects the vertical axis at 800 because b equals 800 and the slope of the line is negative 0.15 because w equals negative 0.15. Now if you look at the data points in the training set, you may notice that this line is not a good fit to the data. For this function f of x, with these values of w and b, many of the predictions for the value of y are quite far from the actual target value of y that is in the training data. Since this line is not a good fit, if you look at the graph of J, the cost of this line is out here, which is pretty far from the minimum. This is a pretty high cost because this choice of w and b is just not that good a fit to the training set. Now let's look at another example with a different choice of w and b. Now here is another function that is still not a great fit for the data, but maybe slightly less bad. So this point here represents the cost for this particular pair of w and b that creates that line. The value of w is equal to 0 and the value of b is about 360. This pair of parameters corresponds to this function, which is a flat line because f of x equals 0 times x plus 360. I hope that makes sense. Let's look at yet another example. Here's one more choice for w and b, and with these values, you end up with this line f of x. Again, not a great fit to the data. It is actually further away from the minimum compared to the previous example. And remember that the minimum is at the center of that smallest ellipse. This example, if you look at f of x on the left, this looks like a pretty good fit to the training set. You can see on the right, this point representing the cost is very close to the center of the small ellipse. It's not quite exactly the minimum, but it's pretty close. For this value of w and b, you get this line f of x. You can see that if you measure the vertical distances between the data points and the predicted values on the straight line, you get the error for each data point. The sum of squared errors for all of these data points is pretty close to the minimum possible sum of squared errors among all possible straight line fits. I hope that by looking at these figures, you can get a better sense of how different choices of the parameters affect the line f of x and how this corresponds to different values for the cost j. And hopefully, you can see how the better fit lines correspond to points on the graph of j that are closer to the minimum possible cost for this cost function j of w and b. In the optional lab that follows this video, you get to run some code. And remember, all of the code is given, so you just need to hit shift enter to run it and take a look at it. And the lab will show you how the cost function is implemented in code. And given a small training set and different choices for the parameters, you'll be able to see how the cost varies depending on how well the model fits the data. In the optional lab, you also can play with an interactive contour plot. Check this out. You can use your mouse cursor to click anywhere on the contour plot, and you're going to see the straight line defined by the values you chose for the parameters w and b. You see a dot appear also on the 3D surface plot showing the cost. Finally, the optional lab also has a 3D surface plot that you can manually rotate and spin around using your mouse cursor to take a better look at what the cost function looks like. I hope you enjoyed playing with the optional lab. Now, in linear regression, rather than having to manually try to read the contour plot for the best value for w and b, which isn't really a good procedure and also won't work once we get to more complex machine learning models, what you really want is an efficient algorithm that you can write in code for automatically finding the values of parameters w and b that gives you the best fit line that minimizes the cost function j. There is an algorithm for doing this called gradient descent. This algorithm is one of the most important algorithms in machine learning. Gradient descent and variations on gradient descent are used to train not just linear regression, but some of the biggest and most complex models in all of AI. So, let's go to the next video to dive into this really important algorithm called gradient descent.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.6000000000000005, "text": " Let's look at some more visualizations of w and b.", "tokens": [50364, 961, 311, 574, 412, 512, 544, 5056, 14455, 295, 261, 293, 272, 13, 50744, 50744, 1692, 311, 472, 1365, 13, 50854, 50854, 4886, 510, 11, 291, 362, 257, 1729, 935, 322, 264, 4295, 508, 13, 1171, 341, 935, 11, 261, 6915, 466, 51230, 51230, 3671, 1958, 13, 5211, 293, 272, 6915, 466, 13083, 13, 51505, 51505], "temperature": 0.0, "avg_logprob": -0.23569056083416118, "compression_ratio": 1.3197278911564625, "no_speech_prob": 0.0015009979251772165}, {"id": 1, "seek": 0, "start": 7.6000000000000005, "end": 9.8, "text": " Here's one example.", "tokens": [50364, 961, 311, 574, 412, 512, 544, 5056, 14455, 295, 261, 293, 272, 13, 50744, 50744, 1692, 311, 472, 1365, 13, 50854, 50854, 4886, 510, 11, 291, 362, 257, 1729, 935, 322, 264, 4295, 508, 13, 1171, 341, 935, 11, 261, 6915, 466, 51230, 51230, 3671, 1958, 13, 5211, 293, 272, 6915, 466, 13083, 13, 51505, 51505], "temperature": 0.0, "avg_logprob": -0.23569056083416118, "compression_ratio": 1.3197278911564625, "no_speech_prob": 0.0015009979251772165}, {"id": 2, "seek": 0, "start": 9.8, "end": 17.32, "text": " Over here, you have a particular point on the graph J. For this point, w equals about", "tokens": [50364, 961, 311, 574, 412, 512, 544, 5056, 14455, 295, 261, 293, 272, 13, 50744, 50744, 1692, 311, 472, 1365, 13, 50854, 50854, 4886, 510, 11, 291, 362, 257, 1729, 935, 322, 264, 4295, 508, 13, 1171, 341, 935, 11, 261, 6915, 466, 51230, 51230, 3671, 1958, 13, 5211, 293, 272, 6915, 466, 13083, 13, 51505, 51505], "temperature": 0.0, "avg_logprob": -0.23569056083416118, "compression_ratio": 1.3197278911564625, "no_speech_prob": 0.0015009979251772165}, {"id": 3, "seek": 0, "start": 17.32, "end": 22.82, "text": " negative 0.15 and b equals about 800.", "tokens": [50364, 961, 311, 574, 412, 512, 544, 5056, 14455, 295, 261, 293, 272, 13, 50744, 50744, 1692, 311, 472, 1365, 13, 50854, 50854, 4886, 510, 11, 291, 362, 257, 1729, 935, 322, 264, 4295, 508, 13, 1171, 341, 935, 11, 261, 6915, 466, 51230, 51230, 3671, 1958, 13, 5211, 293, 272, 6915, 466, 13083, 13, 51505, 51505], "temperature": 0.0, "avg_logprob": -0.23569056083416118, "compression_ratio": 1.3197278911564625, "no_speech_prob": 0.0015009979251772165}, {"id": 4, "seek": 2282, "start": 22.82, "end": 30.36, "text": " So this point corresponds to one pair of values for w and b that use a particular cost J.", "tokens": [50364, 407, 341, 935, 23249, 281, 472, 6119, 295, 4190, 337, 261, 293, 272, 300, 764, 257, 1729, 2063, 508, 13, 50741, 50741, 400, 294, 1186, 11, 341, 1729, 6119, 295, 4190, 337, 261, 293, 272, 23249, 281, 341, 2445, 283, 51043, 51043, 295, 2031, 11, 597, 307, 341, 1622, 300, 291, 393, 536, 322, 264, 1411, 13, 51284, 51284, 639, 1622, 27815, 82, 264, 9429, 10298, 412, 13083, 570, 272, 6915, 13083, 293, 264, 13525, 295, 51649, 51649], "temperature": 0.0, "avg_logprob": -0.11696596145629883, "compression_ratio": 1.7119565217391304, "no_speech_prob": 5.507491550815757e-06}, {"id": 5, "seek": 2282, "start": 30.36, "end": 36.4, "text": " And in fact, this particular pair of values for w and b corresponds to this function f", "tokens": [50364, 407, 341, 935, 23249, 281, 472, 6119, 295, 4190, 337, 261, 293, 272, 300, 764, 257, 1729, 2063, 508, 13, 50741, 50741, 400, 294, 1186, 11, 341, 1729, 6119, 295, 4190, 337, 261, 293, 272, 23249, 281, 341, 2445, 283, 51043, 51043, 295, 2031, 11, 597, 307, 341, 1622, 300, 291, 393, 536, 322, 264, 1411, 13, 51284, 51284, 639, 1622, 27815, 82, 264, 9429, 10298, 412, 13083, 570, 272, 6915, 13083, 293, 264, 13525, 295, 51649, 51649], "temperature": 0.0, "avg_logprob": -0.11696596145629883, "compression_ratio": 1.7119565217391304, "no_speech_prob": 5.507491550815757e-06}, {"id": 6, "seek": 2282, "start": 36.4, "end": 41.22, "text": " of x, which is this line that you can see on the left.", "tokens": [50364, 407, 341, 935, 23249, 281, 472, 6119, 295, 4190, 337, 261, 293, 272, 300, 764, 257, 1729, 2063, 508, 13, 50741, 50741, 400, 294, 1186, 11, 341, 1729, 6119, 295, 4190, 337, 261, 293, 272, 23249, 281, 341, 2445, 283, 51043, 51043, 295, 2031, 11, 597, 307, 341, 1622, 300, 291, 393, 536, 322, 264, 1411, 13, 51284, 51284, 639, 1622, 27815, 82, 264, 9429, 10298, 412, 13083, 570, 272, 6915, 13083, 293, 264, 13525, 295, 51649, 51649], "temperature": 0.0, "avg_logprob": -0.11696596145629883, "compression_ratio": 1.7119565217391304, "no_speech_prob": 5.507491550815757e-06}, {"id": 7, "seek": 2282, "start": 41.22, "end": 48.519999999999996, "text": " This line intersects the vertical axis at 800 because b equals 800 and the slope of", "tokens": [50364, 407, 341, 935, 23249, 281, 472, 6119, 295, 4190, 337, 261, 293, 272, 300, 764, 257, 1729, 2063, 508, 13, 50741, 50741, 400, 294, 1186, 11, 341, 1729, 6119, 295, 4190, 337, 261, 293, 272, 23249, 281, 341, 2445, 283, 51043, 51043, 295, 2031, 11, 597, 307, 341, 1622, 300, 291, 393, 536, 322, 264, 1411, 13, 51284, 51284, 639, 1622, 27815, 82, 264, 9429, 10298, 412, 13083, 570, 272, 6915, 13083, 293, 264, 13525, 295, 51649, 51649], "temperature": 0.0, "avg_logprob": -0.11696596145629883, "compression_ratio": 1.7119565217391304, "no_speech_prob": 5.507491550815757e-06}, {"id": 8, "seek": 4852, "start": 48.52, "end": 54.56, "text": " the line is negative 0.15 because w equals negative 0.15.", "tokens": [50364, 264, 1622, 307, 3671, 1958, 13, 5211, 570, 261, 6915, 3671, 1958, 13, 5211, 13, 50666, 50666, 823, 498, 291, 574, 412, 264, 1412, 2793, 294, 264, 3097, 992, 11, 291, 815, 3449, 300, 341, 1622, 50884, 50884, 307, 406, 257, 665, 3318, 281, 264, 1412, 13, 51028, 51028, 1171, 341, 2445, 283, 295, 2031, 11, 365, 613, 4190, 295, 261, 293, 272, 11, 867, 295, 264, 21264, 337, 264, 51390, 51390, 2158, 295, 288, 366, 1596, 1400, 490, 264, 3539, 3779, 2158, 295, 288, 300, 307, 294, 264, 3097, 1412, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.09406960010528564, "compression_ratio": 1.7462686567164178, "no_speech_prob": 1.4063278285902925e-05}, {"id": 9, "seek": 4852, "start": 54.56, "end": 58.92, "text": " Now if you look at the data points in the training set, you may notice that this line", "tokens": [50364, 264, 1622, 307, 3671, 1958, 13, 5211, 570, 261, 6915, 3671, 1958, 13, 5211, 13, 50666, 50666, 823, 498, 291, 574, 412, 264, 1412, 2793, 294, 264, 3097, 992, 11, 291, 815, 3449, 300, 341, 1622, 50884, 50884, 307, 406, 257, 665, 3318, 281, 264, 1412, 13, 51028, 51028, 1171, 341, 2445, 283, 295, 2031, 11, 365, 613, 4190, 295, 261, 293, 272, 11, 867, 295, 264, 21264, 337, 264, 51390, 51390, 2158, 295, 288, 366, 1596, 1400, 490, 264, 3539, 3779, 2158, 295, 288, 300, 307, 294, 264, 3097, 1412, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.09406960010528564, "compression_ratio": 1.7462686567164178, "no_speech_prob": 1.4063278285902925e-05}, {"id": 10, "seek": 4852, "start": 58.92, "end": 61.800000000000004, "text": " is not a good fit to the data.", "tokens": [50364, 264, 1622, 307, 3671, 1958, 13, 5211, 570, 261, 6915, 3671, 1958, 13, 5211, 13, 50666, 50666, 823, 498, 291, 574, 412, 264, 1412, 2793, 294, 264, 3097, 992, 11, 291, 815, 3449, 300, 341, 1622, 50884, 50884, 307, 406, 257, 665, 3318, 281, 264, 1412, 13, 51028, 51028, 1171, 341, 2445, 283, 295, 2031, 11, 365, 613, 4190, 295, 261, 293, 272, 11, 867, 295, 264, 21264, 337, 264, 51390, 51390, 2158, 295, 288, 366, 1596, 1400, 490, 264, 3539, 3779, 2158, 295, 288, 300, 307, 294, 264, 3097, 1412, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.09406960010528564, "compression_ratio": 1.7462686567164178, "no_speech_prob": 1.4063278285902925e-05}, {"id": 11, "seek": 4852, "start": 61.800000000000004, "end": 69.04, "text": " For this function f of x, with these values of w and b, many of the predictions for the", "tokens": [50364, 264, 1622, 307, 3671, 1958, 13, 5211, 570, 261, 6915, 3671, 1958, 13, 5211, 13, 50666, 50666, 823, 498, 291, 574, 412, 264, 1412, 2793, 294, 264, 3097, 992, 11, 291, 815, 3449, 300, 341, 1622, 50884, 50884, 307, 406, 257, 665, 3318, 281, 264, 1412, 13, 51028, 51028, 1171, 341, 2445, 283, 295, 2031, 11, 365, 613, 4190, 295, 261, 293, 272, 11, 867, 295, 264, 21264, 337, 264, 51390, 51390, 2158, 295, 288, 366, 1596, 1400, 490, 264, 3539, 3779, 2158, 295, 288, 300, 307, 294, 264, 3097, 1412, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.09406960010528564, "compression_ratio": 1.7462686567164178, "no_speech_prob": 1.4063278285902925e-05}, {"id": 12, "seek": 4852, "start": 69.04, "end": 76.68, "text": " value of y are quite far from the actual target value of y that is in the training data.", "tokens": [50364, 264, 1622, 307, 3671, 1958, 13, 5211, 570, 261, 6915, 3671, 1958, 13, 5211, 13, 50666, 50666, 823, 498, 291, 574, 412, 264, 1412, 2793, 294, 264, 3097, 992, 11, 291, 815, 3449, 300, 341, 1622, 50884, 50884, 307, 406, 257, 665, 3318, 281, 264, 1412, 13, 51028, 51028, 1171, 341, 2445, 283, 295, 2031, 11, 365, 613, 4190, 295, 261, 293, 272, 11, 867, 295, 264, 21264, 337, 264, 51390, 51390, 2158, 295, 288, 366, 1596, 1400, 490, 264, 3539, 3779, 2158, 295, 288, 300, 307, 294, 264, 3097, 1412, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.09406960010528564, "compression_ratio": 1.7462686567164178, "no_speech_prob": 1.4063278285902925e-05}, {"id": 13, "seek": 7668, "start": 76.68, "end": 82.60000000000001, "text": " Since this line is not a good fit, if you look at the graph of J, the cost of this line", "tokens": [50364, 4162, 341, 1622, 307, 406, 257, 665, 3318, 11, 498, 291, 574, 412, 264, 4295, 295, 508, 11, 264, 2063, 295, 341, 1622, 50660, 50660, 307, 484, 510, 11, 597, 307, 1238, 1400, 490, 264, 7285, 13, 50884, 50884, 639, 307, 257, 1238, 1090, 2063, 570, 341, 3922, 295, 261, 293, 272, 307, 445, 406, 300, 665, 257, 3318, 281, 51162, 51162, 264, 3097, 992, 13, 51304, 51304, 823, 718, 311, 574, 412, 1071, 1365, 365, 257, 819, 3922, 295, 261, 293, 272, 13, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.13182489438490433, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.7693795345840044e-06}, {"id": 14, "seek": 7668, "start": 82.60000000000001, "end": 87.08000000000001, "text": " is out here, which is pretty far from the minimum.", "tokens": [50364, 4162, 341, 1622, 307, 406, 257, 665, 3318, 11, 498, 291, 574, 412, 264, 4295, 295, 508, 11, 264, 2063, 295, 341, 1622, 50660, 50660, 307, 484, 510, 11, 597, 307, 1238, 1400, 490, 264, 7285, 13, 50884, 50884, 639, 307, 257, 1238, 1090, 2063, 570, 341, 3922, 295, 261, 293, 272, 307, 445, 406, 300, 665, 257, 3318, 281, 51162, 51162, 264, 3097, 992, 13, 51304, 51304, 823, 718, 311, 574, 412, 1071, 1365, 365, 257, 819, 3922, 295, 261, 293, 272, 13, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.13182489438490433, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.7693795345840044e-06}, {"id": 15, "seek": 7668, "start": 87.08000000000001, "end": 92.64000000000001, "text": " This is a pretty high cost because this choice of w and b is just not that good a fit to", "tokens": [50364, 4162, 341, 1622, 307, 406, 257, 665, 3318, 11, 498, 291, 574, 412, 264, 4295, 295, 508, 11, 264, 2063, 295, 341, 1622, 50660, 50660, 307, 484, 510, 11, 597, 307, 1238, 1400, 490, 264, 7285, 13, 50884, 50884, 639, 307, 257, 1238, 1090, 2063, 570, 341, 3922, 295, 261, 293, 272, 307, 445, 406, 300, 665, 257, 3318, 281, 51162, 51162, 264, 3097, 992, 13, 51304, 51304, 823, 718, 311, 574, 412, 1071, 1365, 365, 257, 819, 3922, 295, 261, 293, 272, 13, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.13182489438490433, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.7693795345840044e-06}, {"id": 16, "seek": 7668, "start": 92.64000000000001, "end": 95.48, "text": " the training set.", "tokens": [50364, 4162, 341, 1622, 307, 406, 257, 665, 3318, 11, 498, 291, 574, 412, 264, 4295, 295, 508, 11, 264, 2063, 295, 341, 1622, 50660, 50660, 307, 484, 510, 11, 597, 307, 1238, 1400, 490, 264, 7285, 13, 50884, 50884, 639, 307, 257, 1238, 1090, 2063, 570, 341, 3922, 295, 261, 293, 272, 307, 445, 406, 300, 665, 257, 3318, 281, 51162, 51162, 264, 3097, 992, 13, 51304, 51304, 823, 718, 311, 574, 412, 1071, 1365, 365, 257, 819, 3922, 295, 261, 293, 272, 13, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.13182489438490433, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.7693795345840044e-06}, {"id": 17, "seek": 7668, "start": 95.48, "end": 102.2, "text": " Now let's look at another example with a different choice of w and b.", "tokens": [50364, 4162, 341, 1622, 307, 406, 257, 665, 3318, 11, 498, 291, 574, 412, 264, 4295, 295, 508, 11, 264, 2063, 295, 341, 1622, 50660, 50660, 307, 484, 510, 11, 597, 307, 1238, 1400, 490, 264, 7285, 13, 50884, 50884, 639, 307, 257, 1238, 1090, 2063, 570, 341, 3922, 295, 261, 293, 272, 307, 445, 406, 300, 665, 257, 3318, 281, 51162, 51162, 264, 3097, 992, 13, 51304, 51304, 823, 718, 311, 574, 412, 1071, 1365, 365, 257, 819, 3922, 295, 261, 293, 272, 13, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.13182489438490433, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.7693795345840044e-06}, {"id": 18, "seek": 10220, "start": 102.2, "end": 107.48, "text": " Now here is another function that is still not a great fit for the data, but maybe slightly", "tokens": [50364, 823, 510, 307, 1071, 2445, 300, 307, 920, 406, 257, 869, 3318, 337, 264, 1412, 11, 457, 1310, 4748, 50628, 50628, 1570, 1578, 13, 50712, 50712, 407, 341, 935, 510, 8855, 264, 2063, 337, 341, 1729, 6119, 295, 261, 293, 272, 300, 7829, 51024, 51024, 300, 1622, 13, 51124, 51124, 440, 2158, 295, 261, 307, 2681, 281, 1958, 293, 264, 2158, 295, 272, 307, 466, 13898, 13, 51470, 51470, 639, 6119, 295, 9834, 23249, 281, 341, 2445, 11, 597, 307, 257, 4962, 1622, 570, 283, 295, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.09497297075059678, "compression_ratio": 1.6355140186915889, "no_speech_prob": 2.947993152702111e-06}, {"id": 19, "seek": 10220, "start": 107.48, "end": 109.16, "text": " less bad.", "tokens": [50364, 823, 510, 307, 1071, 2445, 300, 307, 920, 406, 257, 869, 3318, 337, 264, 1412, 11, 457, 1310, 4748, 50628, 50628, 1570, 1578, 13, 50712, 50712, 407, 341, 935, 510, 8855, 264, 2063, 337, 341, 1729, 6119, 295, 261, 293, 272, 300, 7829, 51024, 51024, 300, 1622, 13, 51124, 51124, 440, 2158, 295, 261, 307, 2681, 281, 1958, 293, 264, 2158, 295, 272, 307, 466, 13898, 13, 51470, 51470, 639, 6119, 295, 9834, 23249, 281, 341, 2445, 11, 597, 307, 257, 4962, 1622, 570, 283, 295, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.09497297075059678, "compression_ratio": 1.6355140186915889, "no_speech_prob": 2.947993152702111e-06}, {"id": 20, "seek": 10220, "start": 109.16, "end": 115.4, "text": " So this point here represents the cost for this particular pair of w and b that creates", "tokens": [50364, 823, 510, 307, 1071, 2445, 300, 307, 920, 406, 257, 869, 3318, 337, 264, 1412, 11, 457, 1310, 4748, 50628, 50628, 1570, 1578, 13, 50712, 50712, 407, 341, 935, 510, 8855, 264, 2063, 337, 341, 1729, 6119, 295, 261, 293, 272, 300, 7829, 51024, 51024, 300, 1622, 13, 51124, 51124, 440, 2158, 295, 261, 307, 2681, 281, 1958, 293, 264, 2158, 295, 272, 307, 466, 13898, 13, 51470, 51470, 639, 6119, 295, 9834, 23249, 281, 341, 2445, 11, 597, 307, 257, 4962, 1622, 570, 283, 295, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.09497297075059678, "compression_ratio": 1.6355140186915889, "no_speech_prob": 2.947993152702111e-06}, {"id": 21, "seek": 10220, "start": 115.4, "end": 117.4, "text": " that line.", "tokens": [50364, 823, 510, 307, 1071, 2445, 300, 307, 920, 406, 257, 869, 3318, 337, 264, 1412, 11, 457, 1310, 4748, 50628, 50628, 1570, 1578, 13, 50712, 50712, 407, 341, 935, 510, 8855, 264, 2063, 337, 341, 1729, 6119, 295, 261, 293, 272, 300, 7829, 51024, 51024, 300, 1622, 13, 51124, 51124, 440, 2158, 295, 261, 307, 2681, 281, 1958, 293, 264, 2158, 295, 272, 307, 466, 13898, 13, 51470, 51470, 639, 6119, 295, 9834, 23249, 281, 341, 2445, 11, 597, 307, 257, 4962, 1622, 570, 283, 295, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.09497297075059678, "compression_ratio": 1.6355140186915889, "no_speech_prob": 2.947993152702111e-06}, {"id": 22, "seek": 10220, "start": 117.4, "end": 124.32000000000001, "text": " The value of w is equal to 0 and the value of b is about 360.", "tokens": [50364, 823, 510, 307, 1071, 2445, 300, 307, 920, 406, 257, 869, 3318, 337, 264, 1412, 11, 457, 1310, 4748, 50628, 50628, 1570, 1578, 13, 50712, 50712, 407, 341, 935, 510, 8855, 264, 2063, 337, 341, 1729, 6119, 295, 261, 293, 272, 300, 7829, 51024, 51024, 300, 1622, 13, 51124, 51124, 440, 2158, 295, 261, 307, 2681, 281, 1958, 293, 264, 2158, 295, 272, 307, 466, 13898, 13, 51470, 51470, 639, 6119, 295, 9834, 23249, 281, 341, 2445, 11, 597, 307, 257, 4962, 1622, 570, 283, 295, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.09497297075059678, "compression_ratio": 1.6355140186915889, "no_speech_prob": 2.947993152702111e-06}, {"id": 23, "seek": 10220, "start": 124.32000000000001, "end": 129.44, "text": " This pair of parameters corresponds to this function, which is a flat line because f of", "tokens": [50364, 823, 510, 307, 1071, 2445, 300, 307, 920, 406, 257, 869, 3318, 337, 264, 1412, 11, 457, 1310, 4748, 50628, 50628, 1570, 1578, 13, 50712, 50712, 407, 341, 935, 510, 8855, 264, 2063, 337, 341, 1729, 6119, 295, 261, 293, 272, 300, 7829, 51024, 51024, 300, 1622, 13, 51124, 51124, 440, 2158, 295, 261, 307, 2681, 281, 1958, 293, 264, 2158, 295, 272, 307, 466, 13898, 13, 51470, 51470, 639, 6119, 295, 9834, 23249, 281, 341, 2445, 11, 597, 307, 257, 4962, 1622, 570, 283, 295, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.09497297075059678, "compression_ratio": 1.6355140186915889, "no_speech_prob": 2.947993152702111e-06}, {"id": 24, "seek": 12944, "start": 129.44, "end": 133.84, "text": " x equals 0 times x plus 360.", "tokens": [50364, 2031, 6915, 1958, 1413, 2031, 1804, 13898, 13, 50584, 50584, 286, 1454, 300, 1669, 2020, 13, 50714, 50714, 961, 311, 574, 412, 1939, 1071, 1365, 13, 50856, 50856, 1692, 311, 472, 544, 3922, 337, 261, 293, 272, 11, 293, 365, 613, 4190, 11, 291, 917, 493, 365, 341, 1622, 283, 51134, 51134, 295, 2031, 13, 51184, 51184, 3764, 11, 406, 257, 869, 3318, 281, 264, 1412, 13, 51274, 51274, 467, 307, 767, 3052, 1314, 490, 264, 7285, 5347, 281, 264, 3894, 1365, 13, 51556, 51556, 400, 1604, 300, 264, 7285, 307, 412, 264, 3056, 295, 300, 16998, 8284, 48041, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1578683119553786, "compression_ratio": 1.5702127659574467, "no_speech_prob": 6.96218921802938e-06}, {"id": 25, "seek": 12944, "start": 133.84, "end": 136.44, "text": " I hope that makes sense.", "tokens": [50364, 2031, 6915, 1958, 1413, 2031, 1804, 13898, 13, 50584, 50584, 286, 1454, 300, 1669, 2020, 13, 50714, 50714, 961, 311, 574, 412, 1939, 1071, 1365, 13, 50856, 50856, 1692, 311, 472, 544, 3922, 337, 261, 293, 272, 11, 293, 365, 613, 4190, 11, 291, 917, 493, 365, 341, 1622, 283, 51134, 51134, 295, 2031, 13, 51184, 51184, 3764, 11, 406, 257, 869, 3318, 281, 264, 1412, 13, 51274, 51274, 467, 307, 767, 3052, 1314, 490, 264, 7285, 5347, 281, 264, 3894, 1365, 13, 51556, 51556, 400, 1604, 300, 264, 7285, 307, 412, 264, 3056, 295, 300, 16998, 8284, 48041, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1578683119553786, "compression_ratio": 1.5702127659574467, "no_speech_prob": 6.96218921802938e-06}, {"id": 26, "seek": 12944, "start": 136.44, "end": 139.28, "text": " Let's look at yet another example.", "tokens": [50364, 2031, 6915, 1958, 1413, 2031, 1804, 13898, 13, 50584, 50584, 286, 1454, 300, 1669, 2020, 13, 50714, 50714, 961, 311, 574, 412, 1939, 1071, 1365, 13, 50856, 50856, 1692, 311, 472, 544, 3922, 337, 261, 293, 272, 11, 293, 365, 613, 4190, 11, 291, 917, 493, 365, 341, 1622, 283, 51134, 51134, 295, 2031, 13, 51184, 51184, 3764, 11, 406, 257, 869, 3318, 281, 264, 1412, 13, 51274, 51274, 467, 307, 767, 3052, 1314, 490, 264, 7285, 5347, 281, 264, 3894, 1365, 13, 51556, 51556, 400, 1604, 300, 264, 7285, 307, 412, 264, 3056, 295, 300, 16998, 8284, 48041, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1578683119553786, "compression_ratio": 1.5702127659574467, "no_speech_prob": 6.96218921802938e-06}, {"id": 27, "seek": 12944, "start": 139.28, "end": 144.84, "text": " Here's one more choice for w and b, and with these values, you end up with this line f", "tokens": [50364, 2031, 6915, 1958, 1413, 2031, 1804, 13898, 13, 50584, 50584, 286, 1454, 300, 1669, 2020, 13, 50714, 50714, 961, 311, 574, 412, 1939, 1071, 1365, 13, 50856, 50856, 1692, 311, 472, 544, 3922, 337, 261, 293, 272, 11, 293, 365, 613, 4190, 11, 291, 917, 493, 365, 341, 1622, 283, 51134, 51134, 295, 2031, 13, 51184, 51184, 3764, 11, 406, 257, 869, 3318, 281, 264, 1412, 13, 51274, 51274, 467, 307, 767, 3052, 1314, 490, 264, 7285, 5347, 281, 264, 3894, 1365, 13, 51556, 51556, 400, 1604, 300, 264, 7285, 307, 412, 264, 3056, 295, 300, 16998, 8284, 48041, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1578683119553786, "compression_ratio": 1.5702127659574467, "no_speech_prob": 6.96218921802938e-06}, {"id": 28, "seek": 12944, "start": 144.84, "end": 145.84, "text": " of x.", "tokens": [50364, 2031, 6915, 1958, 1413, 2031, 1804, 13898, 13, 50584, 50584, 286, 1454, 300, 1669, 2020, 13, 50714, 50714, 961, 311, 574, 412, 1939, 1071, 1365, 13, 50856, 50856, 1692, 311, 472, 544, 3922, 337, 261, 293, 272, 11, 293, 365, 613, 4190, 11, 291, 917, 493, 365, 341, 1622, 283, 51134, 51134, 295, 2031, 13, 51184, 51184, 3764, 11, 406, 257, 869, 3318, 281, 264, 1412, 13, 51274, 51274, 467, 307, 767, 3052, 1314, 490, 264, 7285, 5347, 281, 264, 3894, 1365, 13, 51556, 51556, 400, 1604, 300, 264, 7285, 307, 412, 264, 3056, 295, 300, 16998, 8284, 48041, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1578683119553786, "compression_ratio": 1.5702127659574467, "no_speech_prob": 6.96218921802938e-06}, {"id": 29, "seek": 12944, "start": 145.84, "end": 147.64, "text": " Again, not a great fit to the data.", "tokens": [50364, 2031, 6915, 1958, 1413, 2031, 1804, 13898, 13, 50584, 50584, 286, 1454, 300, 1669, 2020, 13, 50714, 50714, 961, 311, 574, 412, 1939, 1071, 1365, 13, 50856, 50856, 1692, 311, 472, 544, 3922, 337, 261, 293, 272, 11, 293, 365, 613, 4190, 11, 291, 917, 493, 365, 341, 1622, 283, 51134, 51134, 295, 2031, 13, 51184, 51184, 3764, 11, 406, 257, 869, 3318, 281, 264, 1412, 13, 51274, 51274, 467, 307, 767, 3052, 1314, 490, 264, 7285, 5347, 281, 264, 3894, 1365, 13, 51556, 51556, 400, 1604, 300, 264, 7285, 307, 412, 264, 3056, 295, 300, 16998, 8284, 48041, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1578683119553786, "compression_ratio": 1.5702127659574467, "no_speech_prob": 6.96218921802938e-06}, {"id": 30, "seek": 12944, "start": 147.64, "end": 153.28, "text": " It is actually further away from the minimum compared to the previous example.", "tokens": [50364, 2031, 6915, 1958, 1413, 2031, 1804, 13898, 13, 50584, 50584, 286, 1454, 300, 1669, 2020, 13, 50714, 50714, 961, 311, 574, 412, 1939, 1071, 1365, 13, 50856, 50856, 1692, 311, 472, 544, 3922, 337, 261, 293, 272, 11, 293, 365, 613, 4190, 11, 291, 917, 493, 365, 341, 1622, 283, 51134, 51134, 295, 2031, 13, 51184, 51184, 3764, 11, 406, 257, 869, 3318, 281, 264, 1412, 13, 51274, 51274, 467, 307, 767, 3052, 1314, 490, 264, 7285, 5347, 281, 264, 3894, 1365, 13, 51556, 51556, 400, 1604, 300, 264, 7285, 307, 412, 264, 3056, 295, 300, 16998, 8284, 48041, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1578683119553786, "compression_ratio": 1.5702127659574467, "no_speech_prob": 6.96218921802938e-06}, {"id": 31, "seek": 12944, "start": 153.28, "end": 159.12, "text": " And remember that the minimum is at the center of that smallest ellipse.", "tokens": [50364, 2031, 6915, 1958, 1413, 2031, 1804, 13898, 13, 50584, 50584, 286, 1454, 300, 1669, 2020, 13, 50714, 50714, 961, 311, 574, 412, 1939, 1071, 1365, 13, 50856, 50856, 1692, 311, 472, 544, 3922, 337, 261, 293, 272, 11, 293, 365, 613, 4190, 11, 291, 917, 493, 365, 341, 1622, 283, 51134, 51134, 295, 2031, 13, 51184, 51184, 3764, 11, 406, 257, 869, 3318, 281, 264, 1412, 13, 51274, 51274, 467, 307, 767, 3052, 1314, 490, 264, 7285, 5347, 281, 264, 3894, 1365, 13, 51556, 51556, 400, 1604, 300, 264, 7285, 307, 412, 264, 3056, 295, 300, 16998, 8284, 48041, 13, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.1578683119553786, "compression_ratio": 1.5702127659574467, "no_speech_prob": 6.96218921802938e-06}, {"id": 32, "seek": 15912, "start": 159.12, "end": 165.64000000000001, "text": " This example, if you look at f of x on the left, this looks like a pretty good fit to", "tokens": [50364, 639, 1365, 11, 498, 291, 574, 412, 283, 295, 2031, 322, 264, 1411, 11, 341, 1542, 411, 257, 1238, 665, 3318, 281, 50690, 50690, 264, 3097, 992, 13, 50766, 50766, 509, 393, 536, 322, 264, 558, 11, 341, 935, 13460, 264, 2063, 307, 588, 1998, 281, 264, 3056, 295, 264, 51172, 51172, 1359, 8284, 48041, 13, 51222, 51222, 467, 311, 406, 1596, 2293, 264, 7285, 11, 457, 309, 311, 1238, 1998, 13, 51428, 51428, 1171, 341, 2158, 295, 261, 293, 272, 11, 291, 483, 341, 1622, 283, 295, 2031, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.09865509195530668, "compression_ratio": 1.5960591133004927, "no_speech_prob": 6.540383765241131e-06}, {"id": 33, "seek": 15912, "start": 165.64000000000001, "end": 167.16, "text": " the training set.", "tokens": [50364, 639, 1365, 11, 498, 291, 574, 412, 283, 295, 2031, 322, 264, 1411, 11, 341, 1542, 411, 257, 1238, 665, 3318, 281, 50690, 50690, 264, 3097, 992, 13, 50766, 50766, 509, 393, 536, 322, 264, 558, 11, 341, 935, 13460, 264, 2063, 307, 588, 1998, 281, 264, 3056, 295, 264, 51172, 51172, 1359, 8284, 48041, 13, 51222, 51222, 467, 311, 406, 1596, 2293, 264, 7285, 11, 457, 309, 311, 1238, 1998, 13, 51428, 51428, 1171, 341, 2158, 295, 261, 293, 272, 11, 291, 483, 341, 1622, 283, 295, 2031, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.09865509195530668, "compression_ratio": 1.5960591133004927, "no_speech_prob": 6.540383765241131e-06}, {"id": 34, "seek": 15912, "start": 167.16, "end": 175.28, "text": " You can see on the right, this point representing the cost is very close to the center of the", "tokens": [50364, 639, 1365, 11, 498, 291, 574, 412, 283, 295, 2031, 322, 264, 1411, 11, 341, 1542, 411, 257, 1238, 665, 3318, 281, 50690, 50690, 264, 3097, 992, 13, 50766, 50766, 509, 393, 536, 322, 264, 558, 11, 341, 935, 13460, 264, 2063, 307, 588, 1998, 281, 264, 3056, 295, 264, 51172, 51172, 1359, 8284, 48041, 13, 51222, 51222, 467, 311, 406, 1596, 2293, 264, 7285, 11, 457, 309, 311, 1238, 1998, 13, 51428, 51428, 1171, 341, 2158, 295, 261, 293, 272, 11, 291, 483, 341, 1622, 283, 295, 2031, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.09865509195530668, "compression_ratio": 1.5960591133004927, "no_speech_prob": 6.540383765241131e-06}, {"id": 35, "seek": 15912, "start": 175.28, "end": 176.28, "text": " small ellipse.", "tokens": [50364, 639, 1365, 11, 498, 291, 574, 412, 283, 295, 2031, 322, 264, 1411, 11, 341, 1542, 411, 257, 1238, 665, 3318, 281, 50690, 50690, 264, 3097, 992, 13, 50766, 50766, 509, 393, 536, 322, 264, 558, 11, 341, 935, 13460, 264, 2063, 307, 588, 1998, 281, 264, 3056, 295, 264, 51172, 51172, 1359, 8284, 48041, 13, 51222, 51222, 467, 311, 406, 1596, 2293, 264, 7285, 11, 457, 309, 311, 1238, 1998, 13, 51428, 51428, 1171, 341, 2158, 295, 261, 293, 272, 11, 291, 483, 341, 1622, 283, 295, 2031, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.09865509195530668, "compression_ratio": 1.5960591133004927, "no_speech_prob": 6.540383765241131e-06}, {"id": 36, "seek": 15912, "start": 176.28, "end": 180.4, "text": " It's not quite exactly the minimum, but it's pretty close.", "tokens": [50364, 639, 1365, 11, 498, 291, 574, 412, 283, 295, 2031, 322, 264, 1411, 11, 341, 1542, 411, 257, 1238, 665, 3318, 281, 50690, 50690, 264, 3097, 992, 13, 50766, 50766, 509, 393, 536, 322, 264, 558, 11, 341, 935, 13460, 264, 2063, 307, 588, 1998, 281, 264, 3056, 295, 264, 51172, 51172, 1359, 8284, 48041, 13, 51222, 51222, 467, 311, 406, 1596, 2293, 264, 7285, 11, 457, 309, 311, 1238, 1998, 13, 51428, 51428, 1171, 341, 2158, 295, 261, 293, 272, 11, 291, 483, 341, 1622, 283, 295, 2031, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.09865509195530668, "compression_ratio": 1.5960591133004927, "no_speech_prob": 6.540383765241131e-06}, {"id": 37, "seek": 15912, "start": 180.4, "end": 187.08, "text": " For this value of w and b, you get this line f of x.", "tokens": [50364, 639, 1365, 11, 498, 291, 574, 412, 283, 295, 2031, 322, 264, 1411, 11, 341, 1542, 411, 257, 1238, 665, 3318, 281, 50690, 50690, 264, 3097, 992, 13, 50766, 50766, 509, 393, 536, 322, 264, 558, 11, 341, 935, 13460, 264, 2063, 307, 588, 1998, 281, 264, 3056, 295, 264, 51172, 51172, 1359, 8284, 48041, 13, 51222, 51222, 467, 311, 406, 1596, 2293, 264, 7285, 11, 457, 309, 311, 1238, 1998, 13, 51428, 51428, 1171, 341, 2158, 295, 261, 293, 272, 11, 291, 483, 341, 1622, 283, 295, 2031, 13, 51762, 51762], "temperature": 0.0, "avg_logprob": -0.09865509195530668, "compression_ratio": 1.5960591133004927, "no_speech_prob": 6.540383765241131e-06}, {"id": 38, "seek": 18708, "start": 187.08, "end": 191.52, "text": " You can see that if you measure the vertical distances between the data points and the", "tokens": [50364, 509, 393, 536, 300, 498, 291, 3481, 264, 9429, 22182, 1296, 264, 1412, 2793, 293, 264, 50586, 50586, 19147, 4190, 322, 264, 2997, 1622, 11, 291, 483, 264, 6713, 337, 1184, 1412, 935, 13, 50954, 50954, 440, 2408, 295, 8889, 13603, 337, 439, 295, 613, 1412, 2793, 307, 1238, 1998, 281, 264, 7285, 51256, 51256, 1944, 2408, 295, 8889, 13603, 3654, 439, 1944, 2997, 1622, 9001, 13, 51556, 51556, 286, 1454, 300, 538, 1237, 412, 613, 9624, 11, 291, 393, 483, 257, 1101, 2020, 295, 577, 819, 7994, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.05870487378991168, "compression_ratio": 1.8034934497816595, "no_speech_prob": 1.0676840247469954e-06}, {"id": 39, "seek": 18708, "start": 191.52, "end": 198.88000000000002, "text": " predicted values on the straight line, you get the error for each data point.", "tokens": [50364, 509, 393, 536, 300, 498, 291, 3481, 264, 9429, 22182, 1296, 264, 1412, 2793, 293, 264, 50586, 50586, 19147, 4190, 322, 264, 2997, 1622, 11, 291, 483, 264, 6713, 337, 1184, 1412, 935, 13, 50954, 50954, 440, 2408, 295, 8889, 13603, 337, 439, 295, 613, 1412, 2793, 307, 1238, 1998, 281, 264, 7285, 51256, 51256, 1944, 2408, 295, 8889, 13603, 3654, 439, 1944, 2997, 1622, 9001, 13, 51556, 51556, 286, 1454, 300, 538, 1237, 412, 613, 9624, 11, 291, 393, 483, 257, 1101, 2020, 295, 577, 819, 7994, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.05870487378991168, "compression_ratio": 1.8034934497816595, "no_speech_prob": 1.0676840247469954e-06}, {"id": 40, "seek": 18708, "start": 198.88000000000002, "end": 204.92000000000002, "text": " The sum of squared errors for all of these data points is pretty close to the minimum", "tokens": [50364, 509, 393, 536, 300, 498, 291, 3481, 264, 9429, 22182, 1296, 264, 1412, 2793, 293, 264, 50586, 50586, 19147, 4190, 322, 264, 2997, 1622, 11, 291, 483, 264, 6713, 337, 1184, 1412, 935, 13, 50954, 50954, 440, 2408, 295, 8889, 13603, 337, 439, 295, 613, 1412, 2793, 307, 1238, 1998, 281, 264, 7285, 51256, 51256, 1944, 2408, 295, 8889, 13603, 3654, 439, 1944, 2997, 1622, 9001, 13, 51556, 51556, 286, 1454, 300, 538, 1237, 412, 613, 9624, 11, 291, 393, 483, 257, 1101, 2020, 295, 577, 819, 7994, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.05870487378991168, "compression_ratio": 1.8034934497816595, "no_speech_prob": 1.0676840247469954e-06}, {"id": 41, "seek": 18708, "start": 204.92000000000002, "end": 210.92000000000002, "text": " possible sum of squared errors among all possible straight line fits.", "tokens": [50364, 509, 393, 536, 300, 498, 291, 3481, 264, 9429, 22182, 1296, 264, 1412, 2793, 293, 264, 50586, 50586, 19147, 4190, 322, 264, 2997, 1622, 11, 291, 483, 264, 6713, 337, 1184, 1412, 935, 13, 50954, 50954, 440, 2408, 295, 8889, 13603, 337, 439, 295, 613, 1412, 2793, 307, 1238, 1998, 281, 264, 7285, 51256, 51256, 1944, 2408, 295, 8889, 13603, 3654, 439, 1944, 2997, 1622, 9001, 13, 51556, 51556, 286, 1454, 300, 538, 1237, 412, 613, 9624, 11, 291, 393, 483, 257, 1101, 2020, 295, 577, 819, 7994, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.05870487378991168, "compression_ratio": 1.8034934497816595, "no_speech_prob": 1.0676840247469954e-06}, {"id": 42, "seek": 18708, "start": 210.92000000000002, "end": 215.92000000000002, "text": " I hope that by looking at these figures, you can get a better sense of how different choices", "tokens": [50364, 509, 393, 536, 300, 498, 291, 3481, 264, 9429, 22182, 1296, 264, 1412, 2793, 293, 264, 50586, 50586, 19147, 4190, 322, 264, 2997, 1622, 11, 291, 483, 264, 6713, 337, 1184, 1412, 935, 13, 50954, 50954, 440, 2408, 295, 8889, 13603, 337, 439, 295, 613, 1412, 2793, 307, 1238, 1998, 281, 264, 7285, 51256, 51256, 1944, 2408, 295, 8889, 13603, 3654, 439, 1944, 2997, 1622, 9001, 13, 51556, 51556, 286, 1454, 300, 538, 1237, 412, 613, 9624, 11, 291, 393, 483, 257, 1101, 2020, 295, 577, 819, 7994, 51806, 51806], "temperature": 0.0, "avg_logprob": -0.05870487378991168, "compression_ratio": 1.8034934497816595, "no_speech_prob": 1.0676840247469954e-06}, {"id": 43, "seek": 21592, "start": 215.92, "end": 223.28, "text": " of the parameters affect the line f of x and how this corresponds to different values for", "tokens": [50364, 295, 264, 9834, 3345, 264, 1622, 283, 295, 2031, 293, 577, 341, 23249, 281, 819, 4190, 337, 50732, 50732, 264, 2063, 361, 13, 50860, 50860, 400, 4696, 11, 291, 393, 536, 577, 264, 1101, 3318, 3876, 6805, 281, 2793, 322, 264, 4295, 51170, 51170, 295, 361, 300, 366, 4966, 281, 264, 7285, 1944, 2063, 337, 341, 2063, 2445, 361, 295, 261, 293, 272, 13, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10778554748086368, "compression_ratio": 1.5930232558139534, "no_speech_prob": 7.766792805341538e-06}, {"id": 44, "seek": 21592, "start": 223.28, "end": 225.83999999999997, "text": " the cost j.", "tokens": [50364, 295, 264, 9834, 3345, 264, 1622, 283, 295, 2031, 293, 577, 341, 23249, 281, 819, 4190, 337, 50732, 50732, 264, 2063, 361, 13, 50860, 50860, 400, 4696, 11, 291, 393, 536, 577, 264, 1101, 3318, 3876, 6805, 281, 2793, 322, 264, 4295, 51170, 51170, 295, 361, 300, 366, 4966, 281, 264, 7285, 1944, 2063, 337, 341, 2063, 2445, 361, 295, 261, 293, 272, 13, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10778554748086368, "compression_ratio": 1.5930232558139534, "no_speech_prob": 7.766792805341538e-06}, {"id": 45, "seek": 21592, "start": 225.83999999999997, "end": 232.04, "text": " And hopefully, you can see how the better fit lines correspond to points on the graph", "tokens": [50364, 295, 264, 9834, 3345, 264, 1622, 283, 295, 2031, 293, 577, 341, 23249, 281, 819, 4190, 337, 50732, 50732, 264, 2063, 361, 13, 50860, 50860, 400, 4696, 11, 291, 393, 536, 577, 264, 1101, 3318, 3876, 6805, 281, 2793, 322, 264, 4295, 51170, 51170, 295, 361, 300, 366, 4966, 281, 264, 7285, 1944, 2063, 337, 341, 2063, 2445, 361, 295, 261, 293, 272, 13, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10778554748086368, "compression_ratio": 1.5930232558139534, "no_speech_prob": 7.766792805341538e-06}, {"id": 46, "seek": 21592, "start": 232.04, "end": 241.83999999999997, "text": " of j that are closer to the minimum possible cost for this cost function j of w and b.", "tokens": [50364, 295, 264, 9834, 3345, 264, 1622, 283, 295, 2031, 293, 577, 341, 23249, 281, 819, 4190, 337, 50732, 50732, 264, 2063, 361, 13, 50860, 50860, 400, 4696, 11, 291, 393, 536, 577, 264, 1101, 3318, 3876, 6805, 281, 2793, 322, 264, 4295, 51170, 51170, 295, 361, 300, 366, 4966, 281, 264, 7285, 1944, 2063, 337, 341, 2063, 2445, 361, 295, 261, 293, 272, 13, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.10778554748086368, "compression_ratio": 1.5930232558139534, "no_speech_prob": 7.766792805341538e-06}, {"id": 47, "seek": 24184, "start": 241.84, "end": 247.32, "text": " In the optional lab that follows this video, you get to run some code.", "tokens": [50364, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 483, 281, 1190, 512, 3089, 13, 50638, 50638, 400, 1604, 11, 439, 295, 264, 3089, 307, 2212, 11, 370, 291, 445, 643, 281, 2045, 5513, 3242, 281, 1190, 309, 50868, 50868, 293, 747, 257, 574, 412, 309, 13, 50960, 50960, 400, 264, 2715, 486, 855, 291, 577, 264, 2063, 2445, 307, 12270, 294, 3089, 13, 51216, 51216, 400, 2212, 257, 1359, 3097, 992, 293, 819, 7994, 337, 264, 9834, 11, 291, 603, 312, 1075, 51469, 51469, 281, 536, 577, 264, 2063, 21716, 5413, 322, 577, 731, 264, 2316, 9001, 264, 1412, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11842895003984559, "compression_ratio": 1.6926229508196722, "no_speech_prob": 1.8342298062634654e-05}, {"id": 48, "seek": 24184, "start": 247.32, "end": 251.92000000000002, "text": " And remember, all of the code is given, so you just need to hit shift enter to run it", "tokens": [50364, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 483, 281, 1190, 512, 3089, 13, 50638, 50638, 400, 1604, 11, 439, 295, 264, 3089, 307, 2212, 11, 370, 291, 445, 643, 281, 2045, 5513, 3242, 281, 1190, 309, 50868, 50868, 293, 747, 257, 574, 412, 309, 13, 50960, 50960, 400, 264, 2715, 486, 855, 291, 577, 264, 2063, 2445, 307, 12270, 294, 3089, 13, 51216, 51216, 400, 2212, 257, 1359, 3097, 992, 293, 819, 7994, 337, 264, 9834, 11, 291, 603, 312, 1075, 51469, 51469, 281, 536, 577, 264, 2063, 21716, 5413, 322, 577, 731, 264, 2316, 9001, 264, 1412, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11842895003984559, "compression_ratio": 1.6926229508196722, "no_speech_prob": 1.8342298062634654e-05}, {"id": 49, "seek": 24184, "start": 251.92000000000002, "end": 253.76, "text": " and take a look at it.", "tokens": [50364, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 483, 281, 1190, 512, 3089, 13, 50638, 50638, 400, 1604, 11, 439, 295, 264, 3089, 307, 2212, 11, 370, 291, 445, 643, 281, 2045, 5513, 3242, 281, 1190, 309, 50868, 50868, 293, 747, 257, 574, 412, 309, 13, 50960, 50960, 400, 264, 2715, 486, 855, 291, 577, 264, 2063, 2445, 307, 12270, 294, 3089, 13, 51216, 51216, 400, 2212, 257, 1359, 3097, 992, 293, 819, 7994, 337, 264, 9834, 11, 291, 603, 312, 1075, 51469, 51469, 281, 536, 577, 264, 2063, 21716, 5413, 322, 577, 731, 264, 2316, 9001, 264, 1412, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11842895003984559, "compression_ratio": 1.6926229508196722, "no_speech_prob": 1.8342298062634654e-05}, {"id": 50, "seek": 24184, "start": 253.76, "end": 258.88, "text": " And the lab will show you how the cost function is implemented in code.", "tokens": [50364, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 483, 281, 1190, 512, 3089, 13, 50638, 50638, 400, 1604, 11, 439, 295, 264, 3089, 307, 2212, 11, 370, 291, 445, 643, 281, 2045, 5513, 3242, 281, 1190, 309, 50868, 50868, 293, 747, 257, 574, 412, 309, 13, 50960, 50960, 400, 264, 2715, 486, 855, 291, 577, 264, 2063, 2445, 307, 12270, 294, 3089, 13, 51216, 51216, 400, 2212, 257, 1359, 3097, 992, 293, 819, 7994, 337, 264, 9834, 11, 291, 603, 312, 1075, 51469, 51469, 281, 536, 577, 264, 2063, 21716, 5413, 322, 577, 731, 264, 2316, 9001, 264, 1412, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11842895003984559, "compression_ratio": 1.6926229508196722, "no_speech_prob": 1.8342298062634654e-05}, {"id": 51, "seek": 24184, "start": 258.88, "end": 263.94, "text": " And given a small training set and different choices for the parameters, you'll be able", "tokens": [50364, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 483, 281, 1190, 512, 3089, 13, 50638, 50638, 400, 1604, 11, 439, 295, 264, 3089, 307, 2212, 11, 370, 291, 445, 643, 281, 2045, 5513, 3242, 281, 1190, 309, 50868, 50868, 293, 747, 257, 574, 412, 309, 13, 50960, 50960, 400, 264, 2715, 486, 855, 291, 577, 264, 2063, 2445, 307, 12270, 294, 3089, 13, 51216, 51216, 400, 2212, 257, 1359, 3097, 992, 293, 819, 7994, 337, 264, 9834, 11, 291, 603, 312, 1075, 51469, 51469, 281, 536, 577, 264, 2063, 21716, 5413, 322, 577, 731, 264, 2316, 9001, 264, 1412, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11842895003984559, "compression_ratio": 1.6926229508196722, "no_speech_prob": 1.8342298062634654e-05}, {"id": 52, "seek": 24184, "start": 263.94, "end": 269.92, "text": " to see how the cost varies depending on how well the model fits the data.", "tokens": [50364, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 483, 281, 1190, 512, 3089, 13, 50638, 50638, 400, 1604, 11, 439, 295, 264, 3089, 307, 2212, 11, 370, 291, 445, 643, 281, 2045, 5513, 3242, 281, 1190, 309, 50868, 50868, 293, 747, 257, 574, 412, 309, 13, 50960, 50960, 400, 264, 2715, 486, 855, 291, 577, 264, 2063, 2445, 307, 12270, 294, 3089, 13, 51216, 51216, 400, 2212, 257, 1359, 3097, 992, 293, 819, 7994, 337, 264, 9834, 11, 291, 603, 312, 1075, 51469, 51469, 281, 536, 577, 264, 2063, 21716, 5413, 322, 577, 731, 264, 2316, 9001, 264, 1412, 13, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.11842895003984559, "compression_ratio": 1.6926229508196722, "no_speech_prob": 1.8342298062634654e-05}, {"id": 53, "seek": 26992, "start": 269.92, "end": 274.04, "text": " In the optional lab, you also can play with an interactive contour plot.", "tokens": [50364, 682, 264, 17312, 2715, 11, 291, 611, 393, 862, 365, 364, 15141, 21234, 7542, 13, 50570, 50570, 6881, 341, 484, 13, 50644, 50644, 509, 393, 764, 428, 9719, 28169, 281, 2052, 4992, 322, 264, 21234, 7542, 11, 293, 291, 434, 516, 281, 536, 50890, 50890, 264, 2997, 1622, 7642, 538, 264, 4190, 291, 5111, 337, 264, 9834, 261, 293, 272, 13, 51160, 51160, 509, 536, 257, 5893, 4204, 611, 322, 264, 805, 35, 3753, 7542, 4099, 264, 2063, 13, 51452, 51452, 6288, 11, 264, 17312, 2715, 611, 575, 257, 805, 35, 3753, 7542, 300, 291, 393, 16945, 13121, 293, 6060, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.1279059465114887, "compression_ratio": 1.7302904564315353, "no_speech_prob": 2.684136234165635e-06}, {"id": 54, "seek": 26992, "start": 274.04, "end": 275.52000000000004, "text": " Check this out.", "tokens": [50364, 682, 264, 17312, 2715, 11, 291, 611, 393, 862, 365, 364, 15141, 21234, 7542, 13, 50570, 50570, 6881, 341, 484, 13, 50644, 50644, 509, 393, 764, 428, 9719, 28169, 281, 2052, 4992, 322, 264, 21234, 7542, 11, 293, 291, 434, 516, 281, 536, 50890, 50890, 264, 2997, 1622, 7642, 538, 264, 4190, 291, 5111, 337, 264, 9834, 261, 293, 272, 13, 51160, 51160, 509, 536, 257, 5893, 4204, 611, 322, 264, 805, 35, 3753, 7542, 4099, 264, 2063, 13, 51452, 51452, 6288, 11, 264, 17312, 2715, 611, 575, 257, 805, 35, 3753, 7542, 300, 291, 393, 16945, 13121, 293, 6060, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.1279059465114887, "compression_ratio": 1.7302904564315353, "no_speech_prob": 2.684136234165635e-06}, {"id": 55, "seek": 26992, "start": 275.52000000000004, "end": 280.44, "text": " You can use your mouse cursor to click anywhere on the contour plot, and you're going to see", "tokens": [50364, 682, 264, 17312, 2715, 11, 291, 611, 393, 862, 365, 364, 15141, 21234, 7542, 13, 50570, 50570, 6881, 341, 484, 13, 50644, 50644, 509, 393, 764, 428, 9719, 28169, 281, 2052, 4992, 322, 264, 21234, 7542, 11, 293, 291, 434, 516, 281, 536, 50890, 50890, 264, 2997, 1622, 7642, 538, 264, 4190, 291, 5111, 337, 264, 9834, 261, 293, 272, 13, 51160, 51160, 509, 536, 257, 5893, 4204, 611, 322, 264, 805, 35, 3753, 7542, 4099, 264, 2063, 13, 51452, 51452, 6288, 11, 264, 17312, 2715, 611, 575, 257, 805, 35, 3753, 7542, 300, 291, 393, 16945, 13121, 293, 6060, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.1279059465114887, "compression_ratio": 1.7302904564315353, "no_speech_prob": 2.684136234165635e-06}, {"id": 56, "seek": 26992, "start": 280.44, "end": 285.84000000000003, "text": " the straight line defined by the values you chose for the parameters w and b.", "tokens": [50364, 682, 264, 17312, 2715, 11, 291, 611, 393, 862, 365, 364, 15141, 21234, 7542, 13, 50570, 50570, 6881, 341, 484, 13, 50644, 50644, 509, 393, 764, 428, 9719, 28169, 281, 2052, 4992, 322, 264, 21234, 7542, 11, 293, 291, 434, 516, 281, 536, 50890, 50890, 264, 2997, 1622, 7642, 538, 264, 4190, 291, 5111, 337, 264, 9834, 261, 293, 272, 13, 51160, 51160, 509, 536, 257, 5893, 4204, 611, 322, 264, 805, 35, 3753, 7542, 4099, 264, 2063, 13, 51452, 51452, 6288, 11, 264, 17312, 2715, 611, 575, 257, 805, 35, 3753, 7542, 300, 291, 393, 16945, 13121, 293, 6060, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.1279059465114887, "compression_ratio": 1.7302904564315353, "no_speech_prob": 2.684136234165635e-06}, {"id": 57, "seek": 26992, "start": 285.84000000000003, "end": 291.68, "text": " You see a dot appear also on the 3D surface plot showing the cost.", "tokens": [50364, 682, 264, 17312, 2715, 11, 291, 611, 393, 862, 365, 364, 15141, 21234, 7542, 13, 50570, 50570, 6881, 341, 484, 13, 50644, 50644, 509, 393, 764, 428, 9719, 28169, 281, 2052, 4992, 322, 264, 21234, 7542, 11, 293, 291, 434, 516, 281, 536, 50890, 50890, 264, 2997, 1622, 7642, 538, 264, 4190, 291, 5111, 337, 264, 9834, 261, 293, 272, 13, 51160, 51160, 509, 536, 257, 5893, 4204, 611, 322, 264, 805, 35, 3753, 7542, 4099, 264, 2063, 13, 51452, 51452, 6288, 11, 264, 17312, 2715, 611, 575, 257, 805, 35, 3753, 7542, 300, 291, 393, 16945, 13121, 293, 6060, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.1279059465114887, "compression_ratio": 1.7302904564315353, "no_speech_prob": 2.684136234165635e-06}, {"id": 58, "seek": 26992, "start": 291.68, "end": 298.6, "text": " Finally, the optional lab also has a 3D surface plot that you can manually rotate and spin", "tokens": [50364, 682, 264, 17312, 2715, 11, 291, 611, 393, 862, 365, 364, 15141, 21234, 7542, 13, 50570, 50570, 6881, 341, 484, 13, 50644, 50644, 509, 393, 764, 428, 9719, 28169, 281, 2052, 4992, 322, 264, 21234, 7542, 11, 293, 291, 434, 516, 281, 536, 50890, 50890, 264, 2997, 1622, 7642, 538, 264, 4190, 291, 5111, 337, 264, 9834, 261, 293, 272, 13, 51160, 51160, 509, 536, 257, 5893, 4204, 611, 322, 264, 805, 35, 3753, 7542, 4099, 264, 2063, 13, 51452, 51452, 6288, 11, 264, 17312, 2715, 611, 575, 257, 805, 35, 3753, 7542, 300, 291, 393, 16945, 13121, 293, 6060, 51798, 51798], "temperature": 0.0, "avg_logprob": -0.1279059465114887, "compression_ratio": 1.7302904564315353, "no_speech_prob": 2.684136234165635e-06}, {"id": 59, "seek": 29860, "start": 298.6, "end": 304.92, "text": " around using your mouse cursor to take a better look at what the cost function looks like.", "tokens": [50364, 926, 1228, 428, 9719, 28169, 281, 747, 257, 1101, 574, 412, 437, 264, 2063, 2445, 1542, 411, 13, 50680, 50680, 286, 1454, 291, 4626, 2433, 365, 264, 17312, 2715, 13, 50808, 50808, 823, 11, 294, 8213, 24590, 11, 2831, 813, 1419, 281, 16945, 853, 281, 1401, 264, 21234, 7542, 337, 51104, 51104, 264, 1151, 2158, 337, 261, 293, 272, 11, 597, 1943, 380, 534, 257, 665, 10747, 293, 611, 1582, 380, 589, 1564, 51356, 51356, 321, 483, 281, 544, 3997, 3479, 2539, 5245, 11, 437, 291, 534, 528, 307, 364, 7148, 9284, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.10006434718767802, "compression_ratio": 1.6023166023166022, "no_speech_prob": 4.356797035143245e-06}, {"id": 60, "seek": 29860, "start": 304.92, "end": 307.48, "text": " I hope you enjoyed playing with the optional lab.", "tokens": [50364, 926, 1228, 428, 9719, 28169, 281, 747, 257, 1101, 574, 412, 437, 264, 2063, 2445, 1542, 411, 13, 50680, 50680, 286, 1454, 291, 4626, 2433, 365, 264, 17312, 2715, 13, 50808, 50808, 823, 11, 294, 8213, 24590, 11, 2831, 813, 1419, 281, 16945, 853, 281, 1401, 264, 21234, 7542, 337, 51104, 51104, 264, 1151, 2158, 337, 261, 293, 272, 11, 597, 1943, 380, 534, 257, 665, 10747, 293, 611, 1582, 380, 589, 1564, 51356, 51356, 321, 483, 281, 544, 3997, 3479, 2539, 5245, 11, 437, 291, 534, 528, 307, 364, 7148, 9284, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.10006434718767802, "compression_ratio": 1.6023166023166022, "no_speech_prob": 4.356797035143245e-06}, {"id": 61, "seek": 29860, "start": 307.48, "end": 313.40000000000003, "text": " Now, in linear regression, rather than having to manually try to read the contour plot for", "tokens": [50364, 926, 1228, 428, 9719, 28169, 281, 747, 257, 1101, 574, 412, 437, 264, 2063, 2445, 1542, 411, 13, 50680, 50680, 286, 1454, 291, 4626, 2433, 365, 264, 17312, 2715, 13, 50808, 50808, 823, 11, 294, 8213, 24590, 11, 2831, 813, 1419, 281, 16945, 853, 281, 1401, 264, 21234, 7542, 337, 51104, 51104, 264, 1151, 2158, 337, 261, 293, 272, 11, 597, 1943, 380, 534, 257, 665, 10747, 293, 611, 1582, 380, 589, 1564, 51356, 51356, 321, 483, 281, 544, 3997, 3479, 2539, 5245, 11, 437, 291, 534, 528, 307, 364, 7148, 9284, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.10006434718767802, "compression_ratio": 1.6023166023166022, "no_speech_prob": 4.356797035143245e-06}, {"id": 62, "seek": 29860, "start": 313.40000000000003, "end": 318.44, "text": " the best value for w and b, which isn't really a good procedure and also won't work once", "tokens": [50364, 926, 1228, 428, 9719, 28169, 281, 747, 257, 1101, 574, 412, 437, 264, 2063, 2445, 1542, 411, 13, 50680, 50680, 286, 1454, 291, 4626, 2433, 365, 264, 17312, 2715, 13, 50808, 50808, 823, 11, 294, 8213, 24590, 11, 2831, 813, 1419, 281, 16945, 853, 281, 1401, 264, 21234, 7542, 337, 51104, 51104, 264, 1151, 2158, 337, 261, 293, 272, 11, 597, 1943, 380, 534, 257, 665, 10747, 293, 611, 1582, 380, 589, 1564, 51356, 51356, 321, 483, 281, 544, 3997, 3479, 2539, 5245, 11, 437, 291, 534, 528, 307, 364, 7148, 9284, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.10006434718767802, "compression_ratio": 1.6023166023166022, "no_speech_prob": 4.356797035143245e-06}, {"id": 63, "seek": 29860, "start": 318.44, "end": 324.40000000000003, "text": " we get to more complex machine learning models, what you really want is an efficient algorithm", "tokens": [50364, 926, 1228, 428, 9719, 28169, 281, 747, 257, 1101, 574, 412, 437, 264, 2063, 2445, 1542, 411, 13, 50680, 50680, 286, 1454, 291, 4626, 2433, 365, 264, 17312, 2715, 13, 50808, 50808, 823, 11, 294, 8213, 24590, 11, 2831, 813, 1419, 281, 16945, 853, 281, 1401, 264, 21234, 7542, 337, 51104, 51104, 264, 1151, 2158, 337, 261, 293, 272, 11, 597, 1943, 380, 534, 257, 665, 10747, 293, 611, 1582, 380, 589, 1564, 51356, 51356, 321, 483, 281, 544, 3997, 3479, 2539, 5245, 11, 437, 291, 534, 528, 307, 364, 7148, 9284, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.10006434718767802, "compression_ratio": 1.6023166023166022, "no_speech_prob": 4.356797035143245e-06}, {"id": 64, "seek": 32440, "start": 324.4, "end": 329.79999999999995, "text": " that you can write in code for automatically finding the values of parameters w and b that", "tokens": [50364, 300, 291, 393, 2464, 294, 3089, 337, 6772, 5006, 264, 4190, 295, 9834, 261, 293, 272, 300, 50634, 50634, 2709, 291, 264, 1151, 3318, 1622, 300, 4464, 5660, 264, 2063, 2445, 361, 13, 50912, 50912, 821, 307, 364, 9284, 337, 884, 341, 1219, 16235, 23475, 13, 51098, 51098, 639, 9284, 307, 472, 295, 264, 881, 1021, 14642, 294, 3479, 2539, 13, 51324, 51324, 16710, 1196, 23475, 293, 17840, 322, 16235, 23475, 366, 1143, 281, 3847, 406, 445, 8213, 51562, 51562, 24590, 11, 457, 512, 295, 264, 3880, 293, 881, 3997, 5245, 294, 439, 295, 7318, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.10047980308532715, "compression_ratio": 1.751937984496124, "no_speech_prob": 3.9668734643782955e-06}, {"id": 65, "seek": 32440, "start": 329.79999999999995, "end": 335.35999999999996, "text": " gives you the best fit line that minimizes the cost function j.", "tokens": [50364, 300, 291, 393, 2464, 294, 3089, 337, 6772, 5006, 264, 4190, 295, 9834, 261, 293, 272, 300, 50634, 50634, 2709, 291, 264, 1151, 3318, 1622, 300, 4464, 5660, 264, 2063, 2445, 361, 13, 50912, 50912, 821, 307, 364, 9284, 337, 884, 341, 1219, 16235, 23475, 13, 51098, 51098, 639, 9284, 307, 472, 295, 264, 881, 1021, 14642, 294, 3479, 2539, 13, 51324, 51324, 16710, 1196, 23475, 293, 17840, 322, 16235, 23475, 366, 1143, 281, 3847, 406, 445, 8213, 51562, 51562, 24590, 11, 457, 512, 295, 264, 3880, 293, 881, 3997, 5245, 294, 439, 295, 7318, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.10047980308532715, "compression_ratio": 1.751937984496124, "no_speech_prob": 3.9668734643782955e-06}, {"id": 66, "seek": 32440, "start": 335.35999999999996, "end": 339.08, "text": " There is an algorithm for doing this called gradient descent.", "tokens": [50364, 300, 291, 393, 2464, 294, 3089, 337, 6772, 5006, 264, 4190, 295, 9834, 261, 293, 272, 300, 50634, 50634, 2709, 291, 264, 1151, 3318, 1622, 300, 4464, 5660, 264, 2063, 2445, 361, 13, 50912, 50912, 821, 307, 364, 9284, 337, 884, 341, 1219, 16235, 23475, 13, 51098, 51098, 639, 9284, 307, 472, 295, 264, 881, 1021, 14642, 294, 3479, 2539, 13, 51324, 51324, 16710, 1196, 23475, 293, 17840, 322, 16235, 23475, 366, 1143, 281, 3847, 406, 445, 8213, 51562, 51562, 24590, 11, 457, 512, 295, 264, 3880, 293, 881, 3997, 5245, 294, 439, 295, 7318, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.10047980308532715, "compression_ratio": 1.751937984496124, "no_speech_prob": 3.9668734643782955e-06}, {"id": 67, "seek": 32440, "start": 339.08, "end": 343.59999999999997, "text": " This algorithm is one of the most important algorithms in machine learning.", "tokens": [50364, 300, 291, 393, 2464, 294, 3089, 337, 6772, 5006, 264, 4190, 295, 9834, 261, 293, 272, 300, 50634, 50634, 2709, 291, 264, 1151, 3318, 1622, 300, 4464, 5660, 264, 2063, 2445, 361, 13, 50912, 50912, 821, 307, 364, 9284, 337, 884, 341, 1219, 16235, 23475, 13, 51098, 51098, 639, 9284, 307, 472, 295, 264, 881, 1021, 14642, 294, 3479, 2539, 13, 51324, 51324, 16710, 1196, 23475, 293, 17840, 322, 16235, 23475, 366, 1143, 281, 3847, 406, 445, 8213, 51562, 51562, 24590, 11, 457, 512, 295, 264, 3880, 293, 881, 3997, 5245, 294, 439, 295, 7318, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.10047980308532715, "compression_ratio": 1.751937984496124, "no_speech_prob": 3.9668734643782955e-06}, {"id": 68, "seek": 32440, "start": 343.59999999999997, "end": 348.35999999999996, "text": " Gradient descent and variations on gradient descent are used to train not just linear", "tokens": [50364, 300, 291, 393, 2464, 294, 3089, 337, 6772, 5006, 264, 4190, 295, 9834, 261, 293, 272, 300, 50634, 50634, 2709, 291, 264, 1151, 3318, 1622, 300, 4464, 5660, 264, 2063, 2445, 361, 13, 50912, 50912, 821, 307, 364, 9284, 337, 884, 341, 1219, 16235, 23475, 13, 51098, 51098, 639, 9284, 307, 472, 295, 264, 881, 1021, 14642, 294, 3479, 2539, 13, 51324, 51324, 16710, 1196, 23475, 293, 17840, 322, 16235, 23475, 366, 1143, 281, 3847, 406, 445, 8213, 51562, 51562, 24590, 11, 457, 512, 295, 264, 3880, 293, 881, 3997, 5245, 294, 439, 295, 7318, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.10047980308532715, "compression_ratio": 1.751937984496124, "no_speech_prob": 3.9668734643782955e-06}, {"id": 69, "seek": 32440, "start": 348.35999999999996, "end": 353.15999999999997, "text": " regression, but some of the biggest and most complex models in all of AI.", "tokens": [50364, 300, 291, 393, 2464, 294, 3089, 337, 6772, 5006, 264, 4190, 295, 9834, 261, 293, 272, 300, 50634, 50634, 2709, 291, 264, 1151, 3318, 1622, 300, 4464, 5660, 264, 2063, 2445, 361, 13, 50912, 50912, 821, 307, 364, 9284, 337, 884, 341, 1219, 16235, 23475, 13, 51098, 51098, 639, 9284, 307, 472, 295, 264, 881, 1021, 14642, 294, 3479, 2539, 13, 51324, 51324, 16710, 1196, 23475, 293, 17840, 322, 16235, 23475, 366, 1143, 281, 3847, 406, 445, 8213, 51562, 51562, 24590, 11, 457, 512, 295, 264, 3880, 293, 881, 3997, 5245, 294, 439, 295, 7318, 13, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.10047980308532715, "compression_ratio": 1.751937984496124, "no_speech_prob": 3.9668734643782955e-06}, {"id": 70, "seek": 35316, "start": 353.16, "end": 359.08000000000004, "text": " So, let's go to the next video to dive into this really important algorithm called gradient", "tokens": [50364, 407, 11, 718, 311, 352, 281, 264, 958, 960, 281, 9192, 666, 341, 534, 1021, 9284, 1219, 16235, 50660, 50660, 23475, 13, 50686], "temperature": 0.0, "avg_logprob": -0.16167465209960938, "compression_ratio": 1.1627906976744187, "no_speech_prob": 7.655591616639867e-05}, {"id": 71, "seek": 35908, "start": 359.08, "end": 387.79999999999995, "text": " descent.", "tokens": [50364, 23475, 13, 51800], "temperature": 1.0, "avg_logprob": -1.165062713623047, "compression_ratio": 0.5, "no_speech_prob": 0.000652372429613024}], "language": "en", "video_id": "GY0KyF3h8hA", "entity": "ML Specialization, Andrew Ng (2022)"}}