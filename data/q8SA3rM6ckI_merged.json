{"video_id": "q8SA3rM6ckI", "title": "Building makemore Part 4: Becoming a Backprop Ninja", "description": "We take the 2-layer MLP (with BatchNorm) from the previous video and backpropagate through it manually without using PyTorch autograd's loss.backward(): through the cross entropy loss, 2nd linear layer, tanh, batchnorm, 1st linear layer, and the embedding table. Along the way, we get a strong intuitive understanding about how gradients flow backwards through the compute graph and on the level of efficient Tensors, not just individual scalars like in micrograd. This helps build competence and intuition around how neural nets are optimized and sets you up to more confidently innovate on and debug modern neural networks.\n\n!!!!!!!!!!!!\nI recommend you work through the exercise yourself but work with it in tandem and whenever you are stuck unpause the video and see me give away the answer. This video is not super intended to be simply watched. The exercise is here:\nhttps://colab.research.google.com/drive/1WV2oi2fh9XXyldh02wupFQX0wh5ZC-z-?usp=sharing\n!!!!!!!!!!!!\n\nLinks:\n- makemore on github: https://github.com/karpathy/makemore\n- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part4_backprop.ipynb\n- collab notebook: https://colab.research.google.com/drive/1WV2oi2fh9XXyldh02wupFQX0wh5ZC-z-?usp=sharing\n- my website: https://karpathy.ai\n- my twitter: https://twitter.com/karpathy\n- our Discord channel: https://discord.gg/Hp2m3kheJn\n\nSupplementary links:\n- Yes you should understand backprop: https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b\n- BatchNorm paper: https://arxiv.org/abs/1502.03167\n- Bessel\u2019s Correction: http://math.oxford.emory.edu/site/math117/besselCorrection/\n- Bengio et al. 2003 MLP LM https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf \n\nChapters:\n00:00:00 intro: why you should care & fun history\n00:07:26 starter code\n00:13:01 exercise 1: backproping the atomic compute graph\n01:05:17 brief digression: bessel\u2019s correction in batchnorm\n01:26:31 exercise 2: cross entropy loss backward pass\n01:36:37 exercise 3: batch norm layer backward pass\n01:50:02 exercise 4: putting it all together\n01:54:24 outro", "author": "Andrej Karpathy", "keywords": ["deep learning", "backpropagation", "neural network", "language model", "chain rule", "tensors"], "channel_url": "https://www.youtube.com/channel/UCXUPKJO5MZQN11PqgIvyuvQ", "length": 6924, "views": 15965, "publish_date": "11/02/2022", "timestamp": 1665446400, "entity": "Andrew Kaparthy", "transcript": {"text": " Hi everyone. So today we are once again continuing our implementation of Makemore. Now so far we've come up to here, multilayer perceptrons, and our neural net looked like this, and we were implementing this over the last few lectures. Now I'm sure everyone is very excited to go into recurrent neural networks and all of their variants and how they work, and the diagrams look cool and it's very exciting and interesting, and we're going to get a better result. But unfortunately I think we have to remain here for one more lecture. And the reason for that is we've already trained this multilayer perceptron, right, and we are getting pretty good loss, and I think we have a pretty decent understanding of the architecture and how it works. But the line of code here that I take an issue with is here, loss.backward. That is, we are taking PyTorch autograd and using it to calculate all of our gradients along the way. And I would like to remove the use of loss.backward, and I would like us to write our backward pass manually on the level of tensors. And I think that this is a very useful exercise for the following reasons. I actually have an entire blog post on this topic, but I like to call backpropagation a leaky abstraction. And what I mean by that is backpropagation doesn't just make your neural networks just work magically. It's not the case that you can just stack up arbitrary Lego blocks of differentiable functions and just cross your fingers and backpropagate and everything is great. Things don't just work automatically. It is a leaky abstraction in the sense that you can shoot yourself in the foot if you do not understand its internals. It will magically not work or not work optimally, and you will need to understand how it works under the hood if you're hoping to debug it and if you are hoping to address it in your neural net. So this blog post here from a while ago goes into some of those examples. So for example, we've already covered them, some of them already. For example, the flat tails of these functions and how you do not want to saturate them too much because your gradients will die. The case of dead neurons, which I've already covered as well. The case of exploding or vanishing gradients in the case of recurrent neural networks, which we are about to cover. And then also you will often come across some examples in the wild. This is a snippet that I found in a random code base on the internet where they actually have like a very subtle but pretty major bug in their implementation. And the bug points at the fact that the author of this code does not actually understand backpropagation. So what they're trying to do here is they're trying to clip the loss at a certain maximum value. But actually what they're trying to do is they're trying to clip the gradients to have a maximum value instead of trying to clip the loss at a maximum value. And indirectly, they're basically causing some of the outliers to be actually ignored because when you clip a loss of an outlier, you are setting its gradient to zero. And so have a look through this and read through it. But there's basically a bunch of subtle issues that you're going to avoid if you actually know what you're doing. And that's why I don't think it's the case that because PyTorch or other frameworks offer autograd, it is okay for us to ignore how it works. Now we've actually already covered autograd and we wrote micrograd. But micrograd was an autograd engine only on the level of individual scalars. So the atoms were single individual numbers. And I don't think it's enough. And I'd like us to basically think about backpropagation on the level of tensors as well. And so in a summary, I think it's a good exercise. I think it is very, very valuable. You're going to become better at debugging neural networks and making sure that you understand what you're doing. It is going to make everything fully explicit. So you're not going to be nervous about what is hidden away from you. And basically in general, we're going to emerge stronger. And so let's get into it. A bit of a fun historical note here is that today writing your backward pass by hand and manually is not recommended and no one does it except for the purposes of exercise. But about 10 years ago in deep learning, this was fairly standard and in fact pervasive. So at the time everyone used to write their own backward pass by hand manually, including myself, and it's just what you would do. So we used to write backward pass by hand and now everyone just called lost that backward. We've lost something. I want to give you a few examples of this. So here's a 2006 paper from Jeff Hinton and Russell Selectonov in science that was influential at the time. And this was training some architectures called restricted Boltzmann machines. And basically it's an autoencoder trained here. And this is from roughly 2010. I had a library for training restricted Boltzmann machines. And this was at the time written in MATLAB. So Python was not used for deep learning pervasively. It was all MATLAB. And MATLAB was this scientific computing package that everyone would use. So we would write MATLAB, which is barely a programming language as well, but it had a very convenient tensor class. And it was this computing environment and you would run here. It would all run on the CPU, of course, but you would have very nice plots to go with it and a built-in debugger. And it was pretty nice. Now the code in this package in 2010 that I wrote for fitting restricted Boltzmann machines to a large extent is recognizable, but I wanted to show you how you would. Well, I'm creating the data and the XY batches. I'm initializing the neural nut. So it's got weights and biases just like we're used to. And then this is the training loop where we actually do the forward pass. And then here at this time, they didn't even necessarily use back propagation to train neural networks. So this in particular implements contrasted divergence, which estimates a gradient. And then here we take that gradient and use it for a parameter update along the lines that we're used to. Yeah, here. But you can see that basically people are meddling with these gradients directly and inline and themselves. It wasn't that common to use an autograd engine. Here's one more example from a paper of mine from 2014 called the fragment embeddings. And here what I was doing is I was aligning images and text. And so it's kind of like a clip if you're familiar with it, but instead of working on the level of entire images and entire sentences, it was working on the level of individual objects and little pieces of sentences. And I was embedding them and then calculating a very much like a clip like loss. And I dug up the code from 2014 of how I implemented this. And it was already in NumPy and Python. And here I'm implementing the cost function and it was standards to implement not just the cost, but also the backward pass manually. So here I'm calculating the image embeddings, sentence embeddings, the loss function. I calculate the scores. This is the loss function. And then once I have the loss function, I do the backward pass right here. So I backward through the loss function and through the neural net and I append regularization. So everything was done by hand manually. And you will just write out the backward pass and then you would use a gradient checker to make sure that your numerical estimate of the gradient agrees with the one you calculated during back propagation. So this was very standard for a long time, but today of course it is standard to use an undergrad engine. But it was definitely useful. And I think people sort of understood how these neural networks work on a very intuitive level. And so I think it's a good exercise again, and this is where we want to be. Okay. So just as a reminder from our previous lecture, this is the Jupyter notebook that we implemented at the time. And we're going to keep everything the same. So we're still going to have a two layer multi-layer perceptron with a batch normalization layer. So the forward pass will be basically identical to this lecture, but here we're going to get rid of loss.backward. And instead we're going to write the backward pass manually. Now here's the starter code for this lecture. We are becoming a back prop ninja in this notebook. And the first few cells here are identical to what we are used to. So we are doing some imports, loading the dataset and processing the dataset. None of this changed. Now here I'm introducing a utility function that we're going to use later to compare the gradients. So in particular, we are going to have the gradients that we estimate manually ourselves, and we're going to have gradients that PyTorch calculates. And we're going to be checking for correctness, assuming of course that PyTorch is correct. Then here we have the initialization that we are quite used to. So we have our embedding table for the characters, the first layer, second layer, and a batch normalization in between. And here's where we create all the parameters. Now you will note that I changed the initialization a little bit to be small numbers. So normally you would set the biases to be all zero. Here I am setting them to be small random numbers. And I'm doing this because if your variables are initialized to exactly zero, sometimes what can happen is that can mask an incorrect implementation of a gradient. Because when everything is zero, it sort of simplifies and gives you a much simpler expression of the gradient than you would otherwise get. And so by making the small numbers, I'm trying to unmask those potential errors in these calculations. You also notice that I'm using B1 in the first layer. I'm using a bias despite batch normalization right afterwards. So this would typically not be what you do because we talked about the fact that you don't need a bias. But I'm doing this here just for fun because we're going to have a gradient with respect to it and we can check that we are still calculating it correctly even though this bias is spurious. So here I'm calculating a single batch. And then here I am doing a forward pass. Now you'll notice that the forward pass is significantly expanded from what we are used to. Here the forward pass was just here. Now the reason that the forward pass is longer is for two reasons. Number one, here we just had an f dot cross entropy. But here I am bringing back a explicit implementation of the loss function. And number two, I've broken up the implementation into manageable chunks. So we have a lot more intermediate tensors along the way in the forward pass. And that's because we are about to go backwards and calculate the gradients in this back propagation from the bottom to the top. So we're going to go upwards. And just like we have, for example, the log props tensor in a forward pass, in a backward pass we're going to have a d log props, which is going to store the derivative of the loss with respect to the log props tensor. And so we're going to be prepending d to every one of these tensors and calculating it along the way of this back propagation. So as an example, we have a b and raw here. We're going to be calculating a db and raw. So here I'm telling PyTorch that we want to retain the grad of all these intermediate values, because here in exercise one, we're going to calculate the backward pass. So we're going to calculate all these d variables and use the cmp function I've introduced above to check our correctness with respect to what PyTorch is telling us. This is going to be exercise one, where we sort of back propagate through this entire graph. Now, just to give you a very quick preview of what's going to happen in exercise two and below, here we have fully broken up the loss and back propagated through it manually in all the little atomic pieces that make it up. But here we're going to collapse the loss into a single cross entropy call. And instead we're going to analytically derive using math and paper and pencil, the gradient of the loss with respect to the logits. And instead of back propagating through all of its little chunks one at a time, we're just going to analytically derive what that gradient is. And we're going to implement that, which is much more efficient, as we'll see in a bit. Then we're going to do the exact same thing for batch normalization. So instead of breaking up batch norm into all the tiny components, we're going to use pen and paper and mathematics and calculus to derive the gradient through the batch norm layer. So we're going to calculate the backward pass through batch norm layer in a much more efficient expression instead of backward propagating through all of its little pieces independently. So that's going to be exercise three. And then in exercise four, we're going to put it all together. And this is the full code of training this two layer MLP. And we're going to basically insert our manual back prop. We're going to take out loss.backward. And you will basically see that you can get all the same results using fully your own code. And the only thing we're using from PyTorch is the torch.tensor to make the calculations efficient. But otherwise you will understand fully what it means to forward and backward in your net and train it. And I think that will be awesome. So let's get to it. Okay. So I ran all the cells of this notebook all the way up to here, and I'm going to erase this and I'm going to start implementing backward pass, starting with the log props. So we want to understand what should go here to calculate the gradient of the loss with respect to all the elements of the log props tensor. Now I'm going to give away the answer here, but I wanted to put a quick note here that I think will be most pedagogically useful for you is to actually go into the description of this video and find the link to this Jupyter notebook. You can find it both on GitHub, but you can also find Google Colab with it. So you don't have to install anything. You'll just go to a website on Google Colab and you can try to implement these derivatives or gradients yourself. And then if you are not able to come to my video and see me do it. And so work in tandem and try it first yourself and then see me give away the answer. And I think that would be most valuable to you. And that's how I recommend you go through this lecture. So we are starting here with D log props. Now D log props will hold the derivative of the loss with respect to all the elements of log props. What is inside log props? The shape of this is 32 by 27. So it's not going to surprise you that D log props should also be an array of size 32 by 27 because we want the derivative loss with respect to all of its elements. So the sizes of those are always going to be equal. Now, how does log props influence the loss? Okay. Loss is negative log props indexed with range of N and YB and then the mean of that. Now, just as a reminder, YB is just basically an array of all the correct indices. So what we're doing here is we're taking the log props array of size 32 by 27. Right. And then we are going in every single row and in each row, we are plugging out the index eight and then 14 and 15 and so on. So we're going down the rows. That's the iterator range of N. And then we are always plugging out the index at the column specified by this tensor YB. So in the 0th row, we are taking the eighth column. In the first row, we're taking the 14th column, etc. And so log props at this plucks out all those log probabilities of the correct next character in a sequence. So that's what that does. And the shape of this or the size of it is of course 32 because our batch size is 32. So these elements get plucked out and then their mean and the negative of that becomes loss. So I always like to work with simpler examples of how to do that. So let's say I have three examples to understand the numerical form of derivative. What's going on here is once we've plucked out these examples, we're taking the mean and then the negative. So the loss basically, I can write it this way, is the negative of say A plus B plus C. And the mean of those three numbers would be say negative, would divide three. That would be how we achieve the mean of three B C, although we actually have 32 numbers here. And so what is basically the loss by say like D A, right? Well, if we simplify this expression mathematically, this is negative one over three of A and negative plus negative one over three of B plus negative one over three of C. And so what is D loss by D A? It's just negative one over three. And so you can see that if we don't just have A, B and C, but we have 32 numbers, then D loss by D, you know, every one of those numbers is going to be one over N more generally, because N is the size of the batch, 32 in this case. So D loss by D lockprops is negative one over N in all these places. Now, what about the other elements inside lockprops? Because lockprops is larger A, you see that lockprops are shaped as 32 by 27, but only 32 of them participate in the loss calculation. So what's the derivative of all the other, most of the elements that do not get plucked out here? Well, their loss intuitively is zero. Sorry, their gradient intuitively is zero. And that's because they did not participate in the loss. So most of these numbers inside this tensor does not feed into the loss. And so if we were to change these numbers, then the loss doesn't change, which is the equivalent of saying that the derivative of the loss with respect to them is zero. They don't impact it. So here's a way to implement this derivative. Then we start out with torshtot0s of shape 32 by 27. Or let's just say, instead of doing this, because we don't want to hard code numbers, let's do torshtot0s like lockprops. So basically this is going to create an array of zeros exactly in the shape of lockprops. And then we need to set the derivative of negative one over n inside exactly these locations. So here's what we can do. The lockprops indexed in the identical way will be just set to negative one over zero divide n. Right? Just like we derived here. So now let me erase all of these reasoning. And then this is the candidate derivative for the lockprops. Let's uncomment the first line and check that this is correct. Okay. So CMP ran and let's go back to CMP. And you see that what it's doing is it's calculating if the calculated value by us, which is DT, is exactly equal to T dot grad as calculated by PyTorch. And then this is making sure that all the elements are exactly equal and then converting this to a single Boolean value because we don't want to Boolean tensor. We just want to Boolean value. And then here we are making sure that, okay, if they're not exactly equal, maybe they are approximately equal because of some floating point issues, but they're very, very close. So here we are using Torch dot all close, which has a little bit of a wiggle available because sometimes you can get very, very close. But if you use a slightly different calculation because of floating point arithmetic, you can get a slightly different result. So this is checking if you get an approximately close result. And then here we are checking the maximum, basically the value that has the highest difference and what is the difference in the absolute value difference between those two. And so we are printing whether we have an exact equality, an approximate equality, and what is the largest difference. And so here we see that we actually have exact equality. And so therefore, of course, we also have an approximate equality and the maximum difference is exactly zero. So basically our D log props is exactly equal to what PyTorch calculated to be log props dot grad in its back propagation. So, so far we're reading pretty well. Okay, so let's now continue our back propagation. We have that log props depends on props through a log. So all the elements of props are being element wise applied log two. Now, if we want deep props, then, then remember your micro grad training. We have like a log node, it takes in props and creates log props and the props will be the local derivative of that individual operation log times the derivative loss with respect to its output, which in this case is D log props. So what is the local derivative of this operation? Well, we are taking log element wise and we can come here and we can see well from alpha is your friend that D by DX of log of X is just simply one of our X. So therefore in this case, X is props. So we have D by DX is one over X, which is one of our props. And then this is the local derivative and then times we want to chain it. So this is chain rule times D log props. Then let me uncomment this and let me run the cell in place. And we see that the derivative of props as we calculated here is exactly correct. And so notice here how this works. Props that are, props is going to be inverted and then element wise multiplied here. So if your props is very, very close to one, that means you are, your network is currently predicting the character correctly, then this will become one over one and D log props is just gets passed through. But if your probabilities are incorrectly assigned, so if the correct character here is getting a very low probability, then 1.0 dividing by it will boost this and then multiply by the other props. So basically what this line is doing intuitively is it's taking the examples that have a very low probability currently assigned and it's boosting their gradient. You can look at it that way. Next up is count some imv. So we want the derivative of this. Now let me just pause here and kind of introduce what's happening here in general, because I know it's a little bit confusing. We have the logist that come out of the neural net. Here what I'm doing is I'm finding the maximum in each row and I'm subtracting it for the purpose of numerical stability. And we talked about how if you do not do this, you run into numerical issues if some of the logits take on too large values because we end up exponentiating them. So this is done just for safety numerically. Then here's the exponentiation of all the sort of like logits to create our counts. And then we want to take the sum of these counts and normalize so that all the probes sum to one. Now here instead of using one over count sum, I use raised to the power of negative one. Mathematically they are identical. I just found that there's something wrong with the PyTorch implementation of the backward pass of division and it gives like a weird result, but that doesn't happen for star star negative one. So I'm using this formula instead. But basically all that's happening here is we got the logits, we want to exponentiate all of them and we want to normalize the counts to create our probabilities. It's just that it's happening across multiple lines. So now here we want to first take the derivative, we want to backpropagate into count sum and then into counts as well. So what should be the count sum? Now we actually have to be careful here because we have to scrutinize and be careful with the shapes. So counts that shape and then count sum in that shape are different. So in particular counts is 32 by 27, but this count sum is 32 by one. And so in this multiplication here we also have an implicit broadcasting that PyTorch will do because it needs to take this column tensor of 32 numbers and replicate it horizontally 27 times to align these two tensors so it can do an element-wise multiply. So really what this looks like is the following using a toy example again. What we really have here is just props is counts times count sum, so it's c equals a times b, but a is three by three and b is just three by one, a column tensor. And so PyTorch internally replicated this elements of b and it did that across all the columns. So for example b1, which is the first element of b, would be replicated here across all the columns in this multiplication. And now we're trying to backpropagate through this operation to count sum in. So when we are calculating this derivative it's important to realize that these two, this looks like a single operation, but actually is two operations applied sequentially. The first operation that PyTorch did is it took this column tensor and replicated it across all the columns basically 27 times. So that's the first operation, it's a replication. And then the second operation is the multiplication of the two columns. So let's first backprop through the multiplication. If these two arrays were of the same size and we just have a and b, both of them three by three, then how do we backpropagate through a multiplication? So if we just have scalars and not tensors, then if you have c equals a times b, then what is the derivative of c with respect to b? Well, it's just a. And so that's the local derivative. So here in our case, undoing the multiplication and backpropagating through just the multiplication itself, which is element-wise, is going to be the local derivative, which in this case is simply counts, because counts is the a. So this is the local derivative and then times, because the chain rule, dprobs. So this here is the derivative or the gradient, but with respect to replicated b. But we don't have a replicated b, we just have a single b column. So how do we now backpropagate through the replication? And intuitively, this b1 is the same variable and it's just reused multiple times. And so you can look at it as being equivalent to a case with encountered in micrograd. And so here I'm just pulling out a random graph we used in micrograd. We had an example where a single node has its output feeding into two branches of basically the graph until the last function. And we're talking about how the correct thing to do in the backward pass is we need to sum all the gradients that arrive at any one node. So across these different branches, the gradients would sum. So if a node is used multiple times, the gradients for all of its uses sum during backpropagation. So here b1 is used multiple times in all these columns. And therefore the right thing to do here is to sum horizontally across all the rows. So we want to sum in dimension one, but we want to retain this dimension so that count sum nth and its gradient are going to be exactly the same shape. So we want to make sure that we keep them as true so we don't lose this dimension. And this will make the count sum nth be exactly shape 32 by one. So revealing this comparison as well and running this, we see that we get an exact match. So this derivative is exactly correct. And let me erase this. Now let's also back propagate into counts, which is the other variable here to create props. So from props to count sum nth, we just did that. Let's go into counts as well. So d counts will be, d counts is our a, so dc by da is just b. So therefore it's count sum nth and then times chain rule d props. Now count sum nth is 32 by one, d props is 32 by 27. So those will broadcast fine and will give us d counts. There's no additional summation required here. There will be a broadcasting that happens in this multiply here because count sum nth needs to be replicated again to correctly multiply d props. But that's going to give the correct result. So as far as the single operation is concerned, so we've back propagated from props to counts, but we can't actually check the derivative of counts. I have it much later on. And the reason for that is because count sum nth depends on counts. And so there's a second branch here that we have to finish because count sum nth back propagates into count sum and count sum will back propagate into counts. And so counts is a node that is being used twice. It's used right here into props and it goes through this other branch through count sum nth. So even though we've calculated the first contribution of it, we still have to calculate the second contribution of it later. Okay, so we're continuing with this branch. We have the derivative for count sum nth. Now we want the derivative count sum. So d count sum equals, what is the local derivative of this operation? So this is basically an element wise one over counts sum. So count sum raised to the power of negative one is the same as one over count sum. If we go to all from alpha, we see that X is a negative one d by d by d by d X of it is basically negative X to the negative two, right? One negative one over square is the same as negative X to the negative two. So d count sum here will be local derivative is going to be negative counts sum to the negative two. That's the local derivative times chain rule, which is d count sum in. So that's d count sum. Let's uncomment this and check that I am correct. Okay, so we have perfect equality and there's no sketchiness going on here with any shapes because these are of the same shape. Okay, next up we want to backpropagate through this line. We have that count sum is counts dot sum is counts dot sum along the rows. So I wrote out some help here. We have to keep in mind that counts of course is 32 by 27 and count sum is 32 by one. So in this back propagation, we need to take this column of derivatives and transform it into a array of derivatives, two dimensional array. So what is this operation doing? We're taking some kind of an input like say a three by three matrix a and we are summing up the rows into a column tensor b, b1 b2 b3 that is basically this. So now we have the derivatives of the loss with respect to b, all the elements of b and now we want to derivative loss with respect to all these little a's. So how do the b's depend on the a's is basically what we're after. What is the local derivative of this operation? Well we can see here that b1 only depends on these elements here. The derivative of b1 with respect to all of these elements down here is zero but for these elements here like a1 1 a1 2 etc the local derivative is 1 right so db1 by d a1 1 for example is 1 so it's 1 1 and 1. So when we have the derivative of loss with respect to b1 the local derivative of b1 with respect to these inputs is zeros here but it's 1 on these guys. So in the chain rule we have the local derivative times sort of the derivative of b1 and so because the local derivative is 1 on these three elements the local derivative multiplying the derivative of b1 will just be the derivative of b1 and so you can look at it as a router. Basically an addition is a router of gradient whatever gradient comes from above it just gets routed equally to all the elements that participate in that addition. So in this case the derivative of b1 will just flow equally to the derivative of a1 1 a1 2 and a1 3. So if we have a derivative of all the elements of b and in this column tensor which is d counts sum that we've calculated just now we basically see that what that amounts to is all of these are now flowing to all these elements of a and they're doing that horizontally. So basically what we want is we want to take the d counts sum of size 32 by 1 and we just want to replicate it 27 times horizontally to create 32 by 27 array. So there's many ways to implement this operation you could of course just replicate the tensor but I think maybe a one clean one is that d counts is simply torched out once like so just a two-dimensional arrays of ones in the shape of counts so 32 by 27 times d counts sum. So this way we're letting the broadcasting here basically implement the replication you can look at it that way. But then we have to also be careful because d counts was already calculated we calculated earlier here and that was just the first branch and we're now finishing the second branch so we need to make sure that these gradients add so plus equals and then here let's comment out the comparison and let's make sure crossing fingers that we have the correct result so PyTorch agrees with us on this gradient as well. Okay hopefully we're getting a hang of this now counts as an element y exp of norm logits so now we want d norm logits and because it's an element wise operation everything is very simple what is the local derivative of e to the x it's famously just e to the x so this is the local derivative that is the local derivative now we already calculated it and it's inside counts so we may as well potentially just reuse counts that is the local derivative times d counts. Funny as that looks counts times d counts is the derivative on the norm logits and now let's erase this and let's verify and it looks good so that's norm logits okay so we are here on this line now d norm logits we have that and we're trying to calculate d logits and d logit maxes so back propagating through this line now we have to be careful here because the shapes again are not the same and so there's an implicit broadcasting happening here so norm logits has the shape 32 by 27 logits does as well but logit maxes is only 32 by 1 so there's a broadcasting here in the minus now here i try to sort of write out a toy example again we basically have that this is our c equals a minus b and we see that because of the shape these are three by three but this one is just a column and so for example every element of c we have to look at how it came to be and every element of c is just the corresponding element of a minus basically that associated b so it's very clear now that the derivatives of every one of these c's with respect to their inputs are one for the corresponding a and it's a negative one for the corresponding b and so therefore the derivatives on the c will flow equally to the corresponding a's and then also to the corresponding b's but then in addition to that the b's are broadcast so we'll have to do the additional sum just like we did before and of course derivatives for b's will undergo a minus because the local derivative here is a negative one so dc 32 by d b3 is negative one so let's just implement that basically d logits will be exactly copying the derivative on normal logits so d logits equals d norm logits and i'll do a dot clone for safety so we're just making a copy and then we have that d logit maxis will be the negative of d norm logits because of the negative sign and then we have to be careful because logit maxis is a column and so just like we saw before because we keep replicating the same elements across all the columns then in the backward pass because we keep reusing this these are all just like separate branches of use of that one variable and so therefore we have to do a sum along one would keep them equals true so that we don't destroy this dimension and then the logit maxis will be the same shape now we have to be careful because this d logits is not the final d logits and that's because not only do we get gradient signal into logits through here but logit maxis is a function of logits and that's a second branch into logits so this is not yet our final derivative for logits we will come back later for the second branch for now d logit maxis is the final derivative so let me uncomment this cmp here and let's just run this and logit maxis if pytorch agrees with us so that was derivative into through this line now before we move on i want to pause here briefly and i want to look at these logit maxis and especially their gradients we've talked previously in the previous lecture that the only reason we're doing this is for the numerical stability of the softmax that we are implementing here and we talked about how if you take these logits for any one of these examples so one row of this logits tensor if you add or subtract any value equally to all the elements then the value of the probes will be unchanged you're not changing the softmax the only thing that this is doing is it's making sure that x doesn't overflow and the reason we're using a max is because then we are guaranteed that each row of logits the highest number is zero and so this will be safe and so basically what that has repercussions if it is the case that changing logit maxis does not change the probes and therefore does not change the loss then the gradient on logit maxis should be zero right because saying those two things is the same so indeed we hope that this is very very small numbers indeed we hope this is zero now because of floating point sort of wonkiness this doesn't come out exactly zero only in some of the rows it does but we get extremely small values like one e negative nine or ten and so this is telling us that the values of logit maxis are not impacting the loss as they shouldn't it feels kind of weird to back propagate through this branch honestly because if you have any implementation of like f dot cross entropy and pi torch and you you block together all these elements and you're not doing the back propagation piece by piece then you would probably assume that the derivative through here is exactly zero so you would be sort of skipping this branch because it's only done for numerical stability but it's interesting to see that even if you break up everything into the full atoms and you still do the computation as you'd like with respect to numerical stability the correct thing happens and you still get a very very small gradients here basically reflecting the fact that the values of these do not matter uh do not matter with respect to the final loss okay so let's now continue back propagation through this line here we've just calculated the logit maxis and now we want to back prop into logits through this second branch now here of course we took logits and we took the max along all the rows and then we looked at its values here now the way this works is that in pi torch this thing here the max returns both the values and it returns the indices at which those values to count the maximum value now in the forward pass we only used values because that's all we needed but in the backward pass it's extremely useful to know about where those maximum values occurred and we have the indices at which they occurred and this will of course help us to help us do the back propagation because what should the backward pass be here in this case we have the logit tensor which is 32 by 27 and in each row we find the maximum value and then that value gets plucked out into logit maxis and so intuitively basically the derivative flowing through here then should be one times the local derivatives is one for the appropriate entry that was plucked out and then times the global derivative of the logit maxis so really what we're doing here if you think through it is we need to take the delogit maxis and we need to scatter it to the correct positions in these logits from where the maximum values came and so I came up with one line of code sort of that does that let me just erase a bunch of stuff here so the line of you could do it kind of very similar to what we done here where we create a zeros and then we populate the correct elements so we use the indices here and we would set them to be one but you can also use one hot so f dot one hot and then I'm taking the logit max over the first dimension dot indices and I'm telling PyTorch that the dimension of every one of these tensors should be um 27 and so what this is going to do is okay I apologize this is crazy PLT.im show of this it's really just an array of where the maxis came from in each row and that element is one and the all the other elements are zero so it's a one hot vector in each row and these indices are now populating a single one in the proper place and then what I'm doing here is I'm multiplying by the logit maxis and keep in mind that this is a column of 32 by one and so when I'm doing this times the logit maxis the logit maxis will broadcast and that column will you know get replicated and then element wise multiply will ensure that each of these just gets routed to whichever one of these bits is turned on and so that's another way to implement this kind of an this kind of a operation and both of these can be used I just thought I would show an equivalent way to do it and I'm using plus equals because we already calculated the logits here and this is now the second branch so let's look at logits and make sure that this is correct and we see that we have exactly the correct answer next up we want to continue with logits here that is an outcome of a matrix multiplication and a bias offset in this linear layer so I've printed out the shapes of all these intermediate tensors we see that logits is of course 32 by 27 as we've just seen then the h here is 32 by 64 so these are 64 dimensional hidden states and then this w matrix projects those 64 dimensional vectors into 27 dimensions and then there's a 27 dimensional offset which is a one-dimensional vector now we should note that this plus here actually broadcasts because h multiplied by w2 will give us a 32 by 27 and so then this plus b2 is a 27 dimensional vector here now in the rules of broadcasting what's going to happen with this bias vector is that this one dimensional vector 27 will get aligned with an padded dimension of one on the left and it will basically become a row vector and then it will get replicated vertically 32 times to make it 32 by 27 and then there's an element twice multiply now the question is how do we back propagate from logits to the hidden states the weight matrix w2 and the bias b2 and you might think that we need to go to some matrix calculus and then we have to look up the derivative for a matrix multiplication but actually you can look the derivative for a matrix multiplication but actually you don't have to do any of that and you can go back to first principles and derive this yourself on a piece of paper and specifically what i like to do and i what i find works well for me is you find a specific small example that you then fully write out and then in the process of analyzing how that individual small example works you will understand the broader pattern and you'll be able to generalize and write out the full uh general formula for how these derivatives flow in an expression like this so let's try that out so pardon the low budget production here but what i've done here is i'm writing it out on a piece of paper really what we are interested in is we have a multiply b plus c and that creates a d and we have the derivative of the loss with respect to d and we'd like to know what the derivative of the loss is with respect to a b and c now these here are little two-dimensional examples of a matrix multiplication two by two times a two by two plus a two a vector of just two elements c1 and c2 gives me a two by two now notice here that i have a bias vector here called c and the bias vector c1 and c2 but as i described over here that bias vector will become a row vector in the broadcasting and will replicate vertically so that's what's happening here as well c1 c2 is replicated vertically and we see how we have two rows of c1 c2 as a result so now when i say write it out i just mean uh like this basically break up this matrix multiplication into the actual thing that that's going on under the hood so as a result of matrix multiplication and how it works d11 is the result of a dot product between the first row of a and the first column of b so a 11 b 11 plus a 12 b 21 plus c1 and so on so forth for all the other elements of d and once you actually write it out it becomes obvious this is just a bunch of multiplies and um ads and we know from micrograd how to differentiate multiplies and adds and so this is not scary anymore it's not just matrix multiplication it's just uh tedious unfortunately but this is completely tractable we have dl by d for all of these and we want to deal by uh all these little other variables so how do we achieve that and how do we actually get the gradients okay so the low budget production continues here so let's for example derive the derivative of the loss with respect to a 11 we see here that a 11 occurs twice in our simple expression right here right here and influences d 11 and d 12 so this is so what is dl by d a 11 well it's dl by d 11 times the local derivative of d 11 which in this case is just b 11 because that's what's multiplying a 11 here so uh and likewise here the local derivative of d 12 with respect to a 11 is just b 12 and so b 12 will in the chain rule therefore multiply dl by d 12 and then because a 11 is used both to produce d 11 and d 12 we need to add up the contributions of both of those sort of chains that are running in parallel and that's why we get a plus just adding up those two um those two contributions and that gives us dl by d a 11 we can do the exact same analysis for the other one for all the other elements of a and when you simply write it out it's just super simple um taking the gradients on you know expressions like this you find that this matrix dl by d a that we're after right if we just arrange all the all of them in the same shape as a takes so a is just too much matrix so dl by d a here will be also just the same shape tensor with the derivatives now so dl by d a 11 etc and we see that actually we can express what we've written out here as a matrix multiply and so it just so happens that d l by that all of these formulas that we've derived here by taking gradients can actually be expressed as a matrix multiplication and in particular we see that it is the matrix multiplication of these two array matrices so it is the um dl by d and then matrix multiplying b but b transpose actually so you see that b21 and b12 have changed place whereas before we had of course b11 b12 b21 b22 so you see that this other matrix b is transposed and so basically what we have long story short just by doing very simple reasoning here by breaking up the expression in the case of a very simple example is that dl by da is which is this is simply equal to dl by dd matrix multiplied with b transpose so that is what we have so far now we also want the derivatives with respect to um b and c now for b i'm not actually doing the full derivation because honestly it's um it's not deep it's just annoying it's exhausting you can actually do this analysis yourself you'll also find that if you take this these expressions and you differentiate with respect to b instead of a you will find that dl by db is also a matrix multiplication in this case you have to take the matrix a and transpose it and matrix multiply that with dl by dd and that's what gives you a dl by db and then here for the offsets c1 and c2 if you again just differentiate with respect to c1 you will find an expression like this and c2 an expression like this and basically you'll find that dl by dc is simply because they're just offsetting these expressions you just have to take the dl by dd matrix um of the derivatives of d and you just have to sum across the columns and that gives you derivatives for c so long story short the backward path of a matrix multiply is a matrix multiply and instead of just like we had d equals a times b plus c in a scalar case we sort of like arrive at something very very similar but now with a matrix multiplication instead of a scalar multiplication so the derivative of d with respect to a is dl by dd matrix multiply b transpose and here it's a transpose multiply dl by dd but in both cases matrix multiplication with the derivative and the other term in the multiplication and for c it is a sum now i'll tell you a secret i can never remember the formulas that we just arrived for backpropagating for matrix multiplication and i can backpropagate for these expressions just fine and the reason this works is because the dimensions have to work out so let me give you an example say i want to create dh then what should dh be number one i have to know that the shape of dh must be the same as the shape of h and the shape of h is 32 by 64 and then the other piece of information i know is that dh must be some kind of matrix multiplication of d logits with w2 and d logits is 32 by 27 and w2 is 64 by 27 there is only a single way to make the shape work out in this case and it is indeed the correct result in particular here h needs to be 32 by 64 the only way to achieve that is to take a d logits and matrix multiply it with you see how i have to take w2 but i have to transpose it to make the dimensions work out so let's take a matrix multiply and make the dimensions work out so w2 transpose and it's the only way to make these two matrix multiply those two pieces to make the shapes work out and that turns out to be the correct formula so if we come here we want dh which is da and we see that da is dl by dd matrix multiply b transpose so that's d logits multiply and b is w2 so w2 transpose which is exactly what we have here so there's no need to remember these formulas similarly now if i want dw2 well i know that it must be a matrix multiplication of d logits and h and maybe there's a few transpose like there's one transpose in there as well and i don't know which way it is so i have to come to w2 and i see that its shape is 64 by 27 and that has to come from some matrix multiplication of these two and so to get a 64 by 27 i need to take um h i need to transpose it and then i need to matrix multiply it so that will become 64 by 32 and then i need to matrix multiply with the 32 by 27 and that's going to give me a 64 by 27 so i need to matrix multiply this with the logits that shape just like that that's the only way to make the dimensions work out and just use matrix multiplication and if we come here we see that that's exactly what's here so a transpose a for us is h multiplied with the logits so that's w2 and then db2 is just the um vertical sum and actually in the same way there's only one way to make the shapes work out i don't have to remember that it's a vertical sum along the zeroth axis because that's the only way that this makes sense because b2 shape is 27 so in order to get a um d logits here is 32 by 27 so knowing that it's just sum over d logits in some direction that direction must be zero because i need to eliminate this dimension so it's this uh so this is so this kind of like the hacky way let me copy paste and delete that and let me swing over here and this is our backward pass for the linear layer uh hopefully so now let's uncomment these three and we're checking that we um got all the three derivatives correct and uh run and we see that h w2 and b2 are all exactly correct so we back propagate it through a linear layer now next up we have derivative for the h already and we need to back propagate through 10h into h preact so we want to derive dh preact and here we have to back propagate through a 10h and we've already done this in micrograd and we remember that 10h is a very simple backward formula now unfortunately if i just put in d by dx of 10h of x into volt from alpha it lets us down it tells us that it's a hyperbolic secant function squared of x it's not exactly helpful but luckily google image search does not let us down and it gives us the simpler formula in particular if you have that a is equal to 10h of z then da by dz back propagating through 10h is just one minus a square and take note that uh one minus a square a here is the output of the 10h not the input to the 10h z so the da by dz is here formulated in terms of the output of that 10h and here also in google image search we have the full derivation if you want to actually take the actual definition of 10h and work through the math to figure out one minus 10h square of z so one minus a square is the local derivative in our case that is one minus uh the output of 10h square which here is h so it's h square and that is the local derivative and then times the chain rule dh so that is going to be our candidate implementation so if we come here and then uncomment this let's hope for the best and we have the right answer okay next up we have dh preact and we want to back propagate into the gain the b and raw and the b and bias so here this is the batch storm parameters b and gain and bias inside the batch storm that take the b and raw that is exact unit gaussian and they scale it and shift it and these are the parameters of the batch norm now here we have a multiplication but it's worth noting that this multiply is very very different from this matrix multiply here matrix multiply are dot products between rows and columns of these matrices involved this is an element-wise multiply so things are quite a bit simpler now we do have to be careful with some of the broadcasting happening in this line of code though so you see how b and gain and b and bias are 1 by 64 but h preact and b and raw are 32 by 64 so we have to be careful with that and make sure that all the shapes work out fine and that the broadcasting is correctly back propagated so in particular let's start with db and gain so db and gain should be and here this is again element-wise multiply and whenever we have a times b equals c we saw that the local derivative here is just if this is a the local derivative is just the b the other one so the local derivative is just b and raw and then times chain rule so dh preact so this is the candidate gradient now again we have to be careful because b and gain is of size 1 by 64 but this here would be 32 by 64 and so the correct thing to do in this case of course is that b and gain here is a rule vector of 64 numbers it gets replicated vertically in this operation and so therefore the correct thing to do is to sum because it's being replicated and therefore all the gradients in each of the rows that are now flowing backwards need to sum up to that same tensor db and gain so if the sum across all the zero all the examples basically which is the direction which just gets replicated and now we have to be also careful because we um being gain is of shape 1 by 64 so in fact i need to keep them as true otherwise i would just get 64 now i don't actually really remember why the being gained and being biased i made them be 1 by 64 um but the biases b1 and b2 i just made them be one dimensional vectors they're not two dimensional tensors so i can't recall exactly why i left the gain and the bias as two dimensional but it doesn't really matter as long as you are consistent and you're keeping it the same so in this case we want to keep the dimension so that the tensor shapes work next up we have b and raw so db and raw will be b and gain multiplying dh react that's our chain rule now what about the um dimensions of this we have to be careful right so dh preact is 32 by 64 b and gain is 1 by 64 so it will just get replicated and to create this uh multiplication which is the correct thing because in a forward pass it also gets replicated in just the same way so in fact we don't need the brackets here we're done and the shapes are already correct and finally for the bias very similar this bias here is very very similar to the bias we saw in the linear layer and we see that the gradients from h preact will simply flow into the biases and add up because these are just these are just offsets and so basically we want this to be dh preact but it needs to sum along the right dimension and in this case similar to the gain we need to sum across the zeroth dimension the examples because of the way that the bias gets replicated vertically and we also want to have keep them as true and so this will basically take this and sum it up and give us a 1 by 64 so this is the candidate implementation it makes all the shapes work let me bring it up down here and then let me uncomment these three lines to check that we are getting the correct result for all the three tensors and indeed we see that all of that got back propagated correctly so now we get to the batch norm layer we see how here bn gain and bn bias are the parameters so the back propagation ends but bn raw now is the output of the standardization so here what i'm doing of course is i'm breaking up the batch norm into manageable pieces so we can back propagate through each line individually but basically what's happening is bn mean i is the sum so this is the bn mean i i apologize for the variable naming bn diff is x minus mu bn diff 2 is x minus mu squared here inside the variance bn var is the variance so sigma square this is bn var and it's basically the sum of squares so this is the x minus mu squared and then the sum now you'll notice one departure here here it is normalized as one over m which is the number of examples here i am normalizing as one over n minus one instead of n and this is deliberate and i'll come back to that in a bit when we are at this line it is something called the best list correction but this is how i want it in our case bn var in then becomes basically bn var plus epsilon epsilon is one negative five and then it's one over square root is the same as raising to the power of negative 0.5 right because 0.5 is square root and then negative makes it one over square root so bn var n is a one over this denominator here and then we can see that bn raw which is the x hat here is equal to the bn diff the numerator multiplied by the bn var n and this line here that creates h-preact was the last piece we've already back propagated through it so now what we want to do is we are here and we have bn raw and we have to first back propagate into bn diff and bn var and bn var inf so now we're here and we have db and raw and we need to back propagate through this line now i've written out the shapes here and indeed bn var in is a shape one by 64 so there is a broadcasting happening here that we have to be careful with but it is just an element-wise simple multiplication by now we should be pretty comfortable with that to get db and if we know that this is just bn var and multiplied with db and raw and conversely to get db and var we need to take the end if and multiply that by db and raw so this is the candidate but of course we need to make sure that broadcasting is obeyed so in particular bn var and multiplying with db and raw will be okay and give us 32 by 64 as we expect but db and var inf would be taking a 32 by 64 multiplying it by 32 by 64 so this is a 32 by 64 but of course db this bn var inf is only one by 64 so the second line here needs a sum across the examples and because there's this dimension here we need to make sure that keep them is true so this is the candidate let's erase this and let's swing down here and implement it and then let's comment out db and var inf and db and diff now we'll actually notice that db and diff by the way is going to be incorrect so when i run this when i run this bn var inf is correct bn diff is not correct and this is actually expected because we're not done with bn diff so in particular when we slide here we see here that bn raw is a function of bn diff but actually bn var is a function of bn var which is a function of bn diff do which is a function of bn diff so it comes here so bdn diff these variable names are crazy i'm sorry it branches out into two branches and we've only done one branch of it we have to continue our backpropagation and eventually come back to bn diff and then we'll be able to do a plus equals and get the actual current gradient for now it is good to verify that cmp also works it doesn't just lie to us and tell us that everything is always correct it can in fact detect when your gradient is not correct so it's that's good to see as well okay so now we have the derivative here and we're trying to backpropagate through this line and because we're raising to a power of negative point five i brought up the power rule and we see that basically we have that the bm var will now be we bring down the exponent so negative point five times x which is this and now raised to the power of negative point five minus one which is negative one point five now we would have to also apply a small chain rule here in our head because we need to take further derivative of bm var with respect to this expression here inside the bracket but because this is an element-wise operation and everything is fairly simple that's just one and so there's nothing to do there so this is the local derivative and then times the global derivative to create the chain rule this is just times the bm var so this is our candidate let me bring this down and uncomment the check and we see that we have the correct result now before we backpropagate through the next line i wanted to briefly talk about the note here where i'm using the best-less correction dividing by n minus one instead of dividing by n when i normalize here the sum of squares now you'll notice that this is a departure from the paper which uses one over n instead not one over n minus one there m is our n and so it turns out that there are two ways of estimating variance of an array one is the biased estimate which is one over n and the other one is the unbiased estimate which is one over n minus one now confusingly in the paper this is not very clearly described and also it's a detail that kind of matters i think they are using the biased version at training time but later when they are talking about the inference they are mentioning that when they do the inference they are using the unbiased estimate which is the n minus one version in basically for inference and to calibrate the the running mean and the running variance basically and so they they actually introduce a train test mismatch where in training they use the biased version and in the and test time they use the unbiased version i find this extremely confusing you can read more about the best-less correction and why dividing by n minus one gives you a better estimate of the variance in the case where you have population sizes or samples for a population they are very small and that is indeed the case for us because we are dealing with many batches and these mini-matches are a small sample of a larger population which is the entire training set and so it just turns out that if you just estimate it using one over n that actually almost always underestimates the variance and it is a biased estimator and it is advised that you use the unbiased version and divide by n minus one and you can go through this article here that i liked that actually describes the full reasoning and i'll link it in the video description now when you calculate the torshtop variance you'll notice that they take the unbiased flag whether or not you want to divide by n or n minus one confusingly they do not mention what the default is for unbiased but i believe unbiased by default is true i'm not sure why the docs here don't cite that now in the batch norm 1d the documentation again is kind of wrong and confusing it says that the standard deviation is calculated via the biased estimator but this is actually not exactly right and people have pointed out that it is not right in a number of issues since then uh because actually the rabbit hole is deeper and they follow the paper exactly and they use the biased version for training but when they're estimating the running standard deviation we are using the unbiased version so again there's the train test mismatch so long story short i'm not a fan of train test discrepancies i basically kind of consider um the fact that we use the bias version the training time and the unbiased test time i basically consider this to be a bug and i don't think that there's a good reason for that um it's not really they don't really go into the detail of the reasoning behind it in this paper so that's why i basically prefer to use the best list correction in my own work unfortunately batch norm does not take a keyword argument that tells you whether or not um you want to use the unbiased version of the biased version in both training tests and so therefore anyone using batch normalization basically in my view has a bit of a bug in the code um and this turns out to be much less of a problem if your batch many batch sizes are a bit larger but still i just find it kind of uh unpalatable so maybe someone can explain why this is okay but for now i prefer to use the unbiased version consistently both during training and at test time and that's why i'm using one over n minus one here okay so let's now actually back propagate through this line so the first thing that i always like to do is i like to scrutinize the shapes first so in particular here looking at the shapes of what's involved i see that b and var shape is one by 64 so it's a row vector and b and if two dot shape is 32 by 64 so clearly here we're doing a sum over the zeroth axis to squash the first dimension of uh of the shapes here using a sum so that right away actually hints to me that there will be some kind of a replication or broadcasting in the backward pass and maybe you're noticing the pattern here but basically anytime you have a sum in the forward pass that turns into a replication or broadcasting in the backward pass along the same dimension and conversely when we have a replication or a broadcasting in the forward pass that indicates a variable reuse and so in the backward pass that turns into a sum over the exact same dimension and so hopefully you're noticing that duality that those two are kind of like the opposites of each other in the forward and backward pass now once we understand the shapes the next thing i like to do always is i like to look at a toy example in my head to sort of just like understand roughly how the variable the variable dependencies go in the mathematical formula so here we have a two-dimensional array b and def 2 which we are scaling by a constant and then we are summing vertically over the columns so if we have a two by two matrix a and then we sum over the columns and scale we would get a row vector b1 b2 and b1 depends on a in this way whereas just sum their scaled of a and b2 in this way whereas the second column summed and scaled and so looking at this basically what we want to do now is we have the derivatives on b1 and b2 and we want to back propagate them into a's and so it's clear that just differentiating in your head the local derivative here is one over n minus one times one for each one of these a's and basically the derivative of b1 has to flow through the columns of a scaled by one over n minus one and that's roughly what's happening here so intuitively the derivative flow tells us that db and def 2 will be the local derivative of this operation and there are many ways to do this by the way but i like to do something like this torch dot ones like of b and def 2 so i'll create a large array two-dimensional of ones and then i will scale it so 1.0 divide by n minus one so this is a array of um one over n minus one and that's sort of like the local derivative and now for the chain rule i will simply just multiply it by dbm bar and notice here what's going to happen this is 32 by 64 and this is just one by 64 so i'm letting the broadcasting do the replication because internally in pi torch basically dbm bar which is one by 64 row vector will in this multiplication get copied vertically until the two are of the same shape and then there will be an element-wise multiply and so that so that the broadcasting is basically doing the replication and i will end up with the derivatives of dbm diff 2 here so this is the candidate solution let's bring it down here let's uncomment this line where we check it and let's hope for the best and indeed we see that this is the correct formula next up let's differentiate here into b and if so here we have that b and if is element-wise squared to create b and if 2 so this is a relatively simple derivative because the simple element-wise operation so it's kind of like the scalar case and we have that dbm diff should be if this is x squared then the derivative of this is 2x right so it's simply 2 times b and if that's the local derivative and then times chain rule and the shape of these is the same they are of the same shape so times this so that's the backward pass for this variable let me bring it down here and now we have to be careful because we already calculated the bm diff right so this is just the end of the other you know other branch coming back to bm diff because bm diff will already backpropagate it to way over here from bn raw so we now completed the second branch and so that's why i have to do plus equals and if you recall we had an incorrect derivative for bm diff before and i'm hoping that once we append this last missing piece we have the exact correctness so let's run and bm diff to bm diff now actually shows the exact correct derivative so that's comforting okay so let's now back propagate through this line here the first thing we do of course is we check the shapes and i wrote them out here and basically the shape of this is 32 by 64 hpbn is the same shape but b and mean i is a row vector 1 by 64 so this minus here will actually do broadcasting and so we have to be careful with that and as a hint to us again because of the duality a broadcasting in the forward pass means a variable reuse and therefore there will be a sum in the backward pass so let's write out the backward pass here now backpropagate into the hpbn because this is these are the same shape then the local derivative for each one of the elements here is just one for the corresponding element in here so basically what this means is that the gradient just simply copies it's just a variable assignment it's quality so i'm just going to clone this tensor just for safety to create an exact copy of db and diff and then here to back propagate into this one what i'm inclined to do here is the bn mean i will basically be uh what is the local derivative well it's negative torch dot one like of the shape of b and diff right and then times the um derivative here db and diff um derivative here db and diff and this here is the back propagation for the replicated b and mean i so i still have to back propagate through the replication in the broadcasting and i do that by doing a sum so i'm going to take this whole thing and i'm going to do a sum over the zero dimension which was the replication so if you scrutinize this by the way you'll notice that this is the same shape as that and so what i'm doing uh what i'm doing here doesn't actually make that much sense because it's just a array of ones multiplying db and diff so in fact i can just do this um and that is equivalent so this is the candidate backward pass let me copy it here and then let me comment out this one and this one enter and it's wrong damn actually sorry this is supposed to be wrong and it's supposed to be wrong because we are back propagating from a b and diff into h pre-bn and but we're not done because b and mean i depends on h pre-bn and there will be a second portion of that derivative coming from this second branch so we're not done yet and we expect it to be incorrect so there you go so let's now back propagate from b and mean i into h pre-bn and so here again we have to be careful because there's a broadcasting along or there's a sum along the zero dimension so this will turn into broadcasting in the backward pass now and i'm going to go a little bit faster on this line because it is very similar to the line that we had before and multiple lines in the past in fact so d h pre-bn will be the gradient will be scaled by one over n and then basically this gradient here on db and mean i is going to be scaled by one over n and then it's going to flow across all the columns and deposit itself into dh pre-bn so what we want is this thing scaled by one over n when you put the content up front here so scale down the gradient and now we need to replicate it across all the um across all the rows here so we i like to do that by torch dot once like of basically um h pre-bn and i will let the broadcasting do the work of replication so like that so this is the h pre-bn and hopefully we can plus equals that we can plus equals that so this here is broadcasting and then this is the scaling so this should be correct okay so that completes the back propagation of the batch drum layer and we are now here let's back propagate through the linear layer one here now because everything is getting a little vertically crazy i copy pasted the line here and let's just back propagate through this one line so first of course we inspect the shapes and we see that this is 32 by 64 and cat is 32 by 30 w1 is 30 by 64 and b1 is just 64 so as i mentioned back propagating through linear layers is fairly easy just by matching the shapes so let's do that we have that d amp cat should be um some matrix multiplication of dh pre-bn with w1 and one transpose thrown in there so to make uh amp cat be 32 by 30 i need to take dh pre-bn 32 by 64 and multiply it by w1 dot transpose to get dw1 i need to end up with 30 by 64 so to get that i need to take uh amp cat transpose and multiply that by uh dh pre-bn dh pre-bn and finally to get the b1 uh this is a addition uh and we saw that basically i need to just sum the elements in dh pre-bn along some dimension and to make the dimensions work out i need to sum along the zeroth axis here to eliminate uh this dimension and we do not keep dims uh so that we want to just get a single one dimensional vector of 64 so these are the claimed derivatives let me put that here and let me uncomment three lines and cross our fingers everything is great okay so we now continue almost there we have the derivative of amp cat and we want to derivative we want to back propagate into amp so i again copied this line over here uh so this is the forward pass and then this is the shapes so remember that the shape here was 32 by 30 and the original shape of m was 32 by 3 by 10 so this layer in the forward pass as you recall that the concatenation of these three 10 dimensional character vectors and so now we just want to undo that so this is actually relatively straightforward operation because uh the backward pass of the what is the view view is just a representation of the array it's just a logical form of how you interpret the array so let's just reinterpret it to be what it was before so in other words the m is not uh 32 by 30 it is basically dm cat but if you view it as um the original shape so just m dot shape uh you can you can pass some tuples into view and so this should just be okay we just re-represent that view and then we uncomment this line here and hopefully yeah so the derivative of m is correct so in this case we just have to re-represent the shape of those derivatives into the original view so now we are at the final line and the only thing that's left to back propagate through is this um indexing operation here m is c at xp so as i did before i copy pasted xp so as i did before i copy pasted this line here and let's look at the shapes of everything that's involved and remind ourselves how this worked so m dot shape was 32 by 3 by 10 so it's 32 examples and then we have three characters each one of them has a 10-dimensional embedding and this was achieved by taking the lookup table c which have 27 possible characters each of them 10-dimensional and we looked up um you at the rows that were specified inside this tensor xb so xb is 32 by 3 and it's basically giving us for each example the identity or the index of which character uh is part of that example and so here i'm showing the first five rows of three of this um tensor xb and so we can see that for example here it was the first example in this batch is that the first character and the first character and the fourth character comes into the neural net and then we want to predict the next character in a sequence after the character is 114 so basically what's happening here is there are integers inside xb and each one of these integers is specifying which row of c we want to pluck out right and then we arrange those rows that we've plucked out into 32 by 3 by 10 tensor we just package them in we just package them into the sensor and now what's happening is that we have dimp so for every one of these uh basically plucked out rows we have their gradients now but they're arranged inside this 32 by 3 by 10 tensor so all we have to do now is we just need to route this gradient backwards through this assignment so we need to find which row of c that every one of these um 10 dimensional embeddings come from and then we need to deposit them into dc so we just need to undo the indexing and of course if any of these rows of c was used multiple times which almost certainly is the case like the row one and one was used multiple times then we have to remember that the gradients that arrive there have to add so for each occurrence we have to have an addition so let's now write this out and i don't actually know of like a much better way to do this than a for loop unfortunately in python so maybe someone can come up with a vectorized efficient operation but for now let's just use for loops so let me create torsion.zeros like c to initialize just a 27 by 10 tensor of all zeros and then honestly for k in range xb.shape at zero maybe someone has a better way to do this but for j in range xb.shape at one this is going to iterate over all the um all the elements of xb all these integers and then let's get the index at this position so the index is basically xb at kj kj so that an example of that like is 11 or 14 and so on and now in the forward pass we took we basically took um the row of c at index and we deposited it into m at kj that's what happened that's where they are packaged so now we need to go backwards and we just need to route uh dm at the position kj we now have these derivatives for each position and it's 10 dimensional and you just need to go into the correct row of c so dc rather at ix is this but plus equals because there could be multiple occurrences uh like the same row could have been used many many times and so all of those derivatives will just uh go backwards through the index in and they will add so this is my candidate solution let's copy it here let's uncomment this and cross our fingers hey so that's it we've back propagated through this entire beast so there we go totally makes sense so now we come to exercise two it basically turns out that in this first exercise we were doing way too much work we were back propagating way too much and it was all good practice and so on but it's not what you would do in practice and the reason for that is for example here i separated out this loss calculation over multiple lines and i broke it up all all to like its smallest atomic pieces and we back propagated through all of those individually but it turns out that if you just look at the mathematical expression for the loss then actually you can do the differentiation on pen and paper and a lot of terms cancel and simplify and the mathematical expression you end up with can be significantly shorter and easier to implement than back propagating through all the little pieces of everything you've done so before we had this complicated forward pass going from logits to the loss but in pytorch everything can just be glued together into a single call at that cross entropy you just pass in logits and the labels and you get the exact same loss as i verify here so our previous loss and the fast loss coming from the chunk of operations as a single mathematical expression is the same but it's much much faster in a forward pass it's also much much faster in backward pass and the reason for that is if you just look at the mathematical form of this and differentiate again you will end up with a very small and short expression so that's what we want to do here we want to in a single operation or in a single go or like very quickly go directly to d logits and we need to implement d logits as a function of logits and yb's but it will be significantly shorter than whatever we did here where to get to d logits we had to go all the way here so all of this work can be skipped in a much much simpler mathematical expression that you can implement here so you can give it a shot yourself basically look at what exactly is the mathematical expression of loss and differentiate with respect to the logits so let me show you a hint you can of course try it fully yourself but if not i can give you some hint of how to get started mathematically so basically what's happening here is we have logits then there's a softmax that takes the logits and gives you probabilities then we are using the identity of the correct next character to pluck out a row of probabilities take the negative log of it to get our negative log probability and then we average up all the log probabilities or negative log probabilities to get our loss so basically what we have is for a single individual example rather we have that loss is equal to negative log probability where p here is kind of like thought of as a vector of all the probabilities so at the yth position where y is the label and we have that p here of course is the softmax so the i-th component of p of this probability vector is just the softmax function so raising all the logits basically to the power of e and normalizing so everything sums to one now if you write out p of y here you can just write out the softmax and then basically what we're interested in is we're interested in the derivative of the loss with respect to the i-th logit and so basically it's a d by d li of this expression here where we have l indexed with the specific label y and on the bottom we have a sum over j of e to the lj and the negative log of all that so potentially give it a shot pen and paper and see if you can actually derive the expression for the loss by d li and then we're going to implement it here okay so i am going to give away the result here so this is some of the math i did to derive the gradients analytically and so we see here that i'm just applying the rules of calculus from your first or second year of bachelor's degree if you took it and we see that the expressions actually simplify quite a bit you have to separate out the analysis in the case where the i-th index that you're interested in inside logits is either equal to the label or it's not equal to the label and then the expressions simplify and cancel in a slightly different way and what we end up with is something very very simple we either end up with basically p at i where p is again this vector of probabilities after a softmax or p at i minus one where we just simply subtract to one but in any case we just need to calculate the softmax p and then in the correct dimension we need to subtract to one and that's the gradient the form that it takes analytically so let's implement this basically and we have to keep in mind that this is only done for a single example but here we are working with batches of examples so we have to be careful of that and then the loss for a batch is the average loss over all the examples so in other words is the example for all the individual examples is the loss for each individual example summed up and then divided by n and we have to back propagate through that as well and be careful with it so d logits is going to be f dot softmax uh pytorch has a softmax function that you can call and we want to apply the softmax on the logits and we want to go in the dimension that is one so basically we want to do the softmax along the rows of these logits then at the correct positions we need to subtract a one so d logits at iterating over all the rows and indexing into the columns provided by the correct labels inside yb we need to subtract one and then finally it's the average loss that is the loss and in the average there's a one over n of all the losses added up and so we need to also back propagate through that division so the gradient has to be scaled down by by n as well because of the mean but this otherwise should be the result so now if we verify this we see that we don't get an exact match but at the same time the maximum difference from logits from pytorch and rd logits here is on the order of 5e negative 9 so it's a tiny tiny number so because of loading point wonkiness we don't get the exact bitwise result but we basically get the correct answer approximately now i'd like to pause here briefly before we move on to the next exercise because i'd like us to get an intuitive sense of what d logits is because it has a beautiful and very simple explanation honestly so here i'm taking the logits and i'm visualizing it and we can see that we have a batch of 32 examples of 27 characters and what is the logits intuitively right the logits is the probabilities that the probabilities matrix in a forward pass but then here these black squares are the positions of the correct indices where we subtract a one and so what is this doing right these are the derivatives on the logits and so let's look at just the first row here so that's what i'm doing here i'm calculating the probabilities of these logits and then i'm taking just the first row and this is the probability row and then the logits of the first row and multiplying by n just for us so that we don't have the scaling by n in here and everything is more interpretable so we're going to take the probability of the logits and we see that it's exactly equal to the probability of course but then the position of the correct index has a minus equals one so minus one on that position and so notice that if you take the logits at zero and you sum it it actually sums to zero and so you should think of these gradients here at each cell as like a force we are going to be basically pulling down on the probabilities of the incorrect characters and we're going to be pulling up on the probability at the correct index and that's what's basically happening in each row and the the amount of push and pull is exactly equalized because the sum is zero so the amount to which we pull down in the probabilities and then the amount that we push up on the probability of the correct character is equal so sort of the repulsion and the attraction are equal and think of the neural net now as a like a massive pulley system or something like that we're up here on top of the logits and we're pulling up we're pulling down the probabilities of incorrect and pulling up the probability of the correct and in this complicated pulley system because everything is mathematically just determined just think of it as sort of like this tension translating to this complicating pulley mechanism and then eventually we get a tug on the weights and the biases and basically in each update we just kind of like tug in the direction that we like for each of these elements and the parameters are slowly given in to the tug and that's what training a neural net kind of like looks like on a high level and so i think the the forces of push and pull in these gradients are actually uh very intuitive here we're pushing and pulling on the correct answer and the incorrect answers and the amount of force that we're applying is actually proportional to the probabilities that came out in the forward pass and so for example if our probabilities came out exactly correct so they would have had zero everywhere except for one at the correct position then the the logits would be all a row of zeros for that example there would be no push and pull so the amount to which your prediction is incorrect is exactly the amount by which you're going to get a pull or a push in that dimension so if you have for example a very confidently mispredicted element here then what's going to happen is that element is going to be pulled down very heavily and the correct answer is going to be pulled up to the same amount and the other characters are not going to be influenced too much so the amount to which you mispredict is then proportional to the strength of the pull and that's happening independently in all the dimensions of this of this tensor and it's sort of very intuitive and very easy to think through and that's basically the magic of the cross entropy loss and what it's doing dynamically in the backward pass of the neural net so now we get to exercise number three which is a very fun exercise depending on your definition of fun and we are going to do for batch normalization exactly what we did for cross entropy loss in exercise number two that is we are going to consider it as a glued single mathematical expression and back propagate through it in a very efficient manner because we are going to derive a much simpler formula for the backward pass of best formalization and we're going to do that using pen and paper so previously we've broken up pastoralization into all of the little intermediate pieces and all the atomic operations inside it and then we back propagate it through it one by one now we just have a single sort of forward pass of a best term and it's all glued together and we see that we get the exact same result as before now for the bash backward pass we'd like to also implement a single formula basically for back propagating through this entire operation that is the bash formalization so in the forward pass previously we took h pre-bn the hidden states of the pre-batch formalization and created h preact which is the hidden states just before the activation in the bash formalization paper h pre-bn is x and h preact is y so in the backward pass what we'd like to do now is we have dh preact and we'd like to produce dh pre-bien and we'd like to do that in a very efficient manner so that's the name of the game calculate dh pre-bien given dh preact and for the purposes of this exercise we're going to ignore gamma and beta and their derivatives because they take on a very simple form in a very similar way to what we did up above so let's calculate this given that right here so to help you a little bit like i did before i started off the implementation here on pen and paper and i took two sheets of paper to derive the mathematical formulas for the backward pass and basically to set up the problem just write out the mu sigma square variance xi hat and yi exactly as in the paper except for the Bessel correction and then in the backward pass we have the derivative of the loss with respect to all the elements of y and remember that y is a vector there's there's multiple numbers here so we have all the derivatives with respect to all the y's and then there's a dm and a beta and this is kind of like the compute graph the gamma and beta there's the x hat and then the mu and the sigma squared and the x so we have dl by dyi and we want dl by dx i for all the i's in these vectors so this is the compute graph and you have to be careful because i'm trying to note here that these are vectors there's many nodes here inside x x hat and y but mu and sigma sorry sigma square are just individual scalars single numbers so you have to be careful with that you have to imagine there's multiple nodes here or you're going to get your math wrong so as an example i would suggest that you go in the following order one two three four in terms of the back propagation so back propagate into x hat then into sigma square then into mu and then into x just like in a topological sort in micrograd we would go from right to left you're doing the exact same thing except you're doing it with symbols and on a piece of paper so for number one uh i'm not giving away too much if you want dl of the xi hat then we just take dl by dyi and multiply it by gamma because of this expression here where any individual yi is just gamma times xi hat plus beta so uh didn't help you too much there but this gives you basically the derivatives for all the x hats and so now try to go through this computational graph and derive what is the l by d sigma square and then what is dl by d mu and then what is dl by dx eventually so give it a go and i'm going to be revealing the answer one piece at a time okay so to get dl by d sigma square we have to remember again like i mentioned that there are many x's x hats here and remember that sigma square is just a single individual number here so when we look at the expression for dl by d sigma square we have that we have to actually consider all the possible paths that we basically have that there's many x hats and they all feed off from they all depend on sigma square so sigma square has a large fan out there's lots of arrows coming out from sigma square into all the x hats and then there's a back-propagating signal from each x hat into sigma square and that's why we actually need to sum over all those i's from i equal to one to m of the dl by d xi hat which is the global gradient times the xi hat by d sigma square which is the local gradient of this operation here and then mathematically i'm just working it out here and i'm simplifying and you get a certain expression for dl by d sigma square we're going to be using this expression when we back-propagate into mu and then eventually into x so now let's continue our back-propagation into mu so what is dl by d mu now again be careful that mu influences x hat and x hat is actually lots of values so for example if our mini batch size is 32 as it is in our example that we were working on then this is 32 numbers and 32 arrows going back to mu and then mu going to sigma square is just a single arrow because sigma square is a scalar so in total there are 33 arrows going back to mu so in total there are 33 arrows emanating from mu and then all of them have gradients coming into mu and they all need to be summed up and so that's why when we look at the expression for dl by d mu i am summing up over all the gradients of dl by d xi hat times the xi hat by d mu so that's the that's this arrow and that's 32 arrows here and then plus the one arrow from here which is dl by d sigma square times the sigma square by d mu so now we have to work out that expression and let me just reveal the rest of it simplifying here is not complicated the first term and you just get an expression here for the second term though there's something really interesting that happens when we look at the sigma square by d mu and we simplify at one point if we assume that in a special case where mu is actually the average of xi's as it is in this case then if we plug that in then actually the gradient vanishes and becomes exactly zero and that makes the entire second term cancel and so these if you just have a mathematical expression like this and you look at d sigma square by d mu you would get some mathematical formula for how mu impacts sigma square but if it is the special case that mu is actually equal to the average as it is in the case of batch normalization that gradient will actually vanish and become zero so the whole term cancels and we just get a fairly straightforward expression here for dl by d mu okay and now we get to the craziest part which is deriving dl by d xi which is ultimately what we're after now let's count first of all how many numbers are there inside x as i mentioned there are 32 numbers there are 32 little xi's and let's count the number of arrows emanating from each xi there's an arrow going to mu an arrow going to sigma square and then there's an arrow going to x hat but this arrow here let's scrutinize that a little bit each xi hat is just a function of xi and all the other scalars so xi hat only depends on xi and none of the other x's and so therefore there are actually in this single arrow there are 32 arrows but those 32 arrows are going exactly parallel they don't interfere they're just going parallel between x and x hat you can look at it that way and so how many arrows are emanating from each xi there are three arrows mu sigma square and the associated x hat and so in backpropagation we now need to apply the chain rule and we need to add up those three contributions so here's what that looks like if i just write that out we have uh we're going through we're chaining through mu sigma square and through x hat and those three terms are just here now we already have three of these we have dl by d xi hat we have dl by d mu which we derived here and we have dl by d sigma square which we derived here but we need three other terms here the this one this one and this one so i invite you to try to derive them it's not that complicated you're just looking at these expressions here and differentiating with respect to xi so give it a shot but here's the result or at least what i got um yeah i'm just i'm just differentiating with respect to xi for all these expressions and honestly i don't think there's anything too tricky here it's basic calculus now what gets a little bit more tricky is we are now going to plug everything together so all of these terms multiplied with all of these terms and added up according to this formula and that gets a little bit hairy so what ends up happening is you get a large expression and the thing to be very careful with here of course is we are working with a dl by d xi for specific i here but when we are plugging in some of these terms like say this term here dl by d sigma squared you see how dl by d sigma squared i end up with an expression and i'm iterating over little i's here but i can't use i as the variable when i plug in here because this is a different i from this i this i here is just a placeholder like a local variable for a for loop in here so here when i plug that in you notice that i rename the i to a j because i need to make sure that this j is not that this j is not this i this j is like like a little local iterator over 32 terms and so you have to be careful with that when you're plugging in the expressions from here to here you may have to rename i's into j's you have to be very careful what is actually an i with respect to dl by d xi so some of these are j's some of these are i's and then we simplify this expression and i guess like the big thing to notice here is a bunch of terms just kind of come out to the front and you can refactor them there's a sigma squared plus epsilon raised to the power of negative three over two this sigma squared plus epsilon can be actually separated out into three terms each of them are sigma squared plus epsilon to the negative one over two so the three of them multiplied is equal to this and then those three terms can go different places because of the multiplication so one of them actually comes out to the front and will end up here outside one of them joins up with this term and one of them joins up with this other term and then when you simplify the expression you'll notice that some of these terms that are coming out are just the xi hats so you can simplify just by rewriting that and what we actually do is we simply simplify the expression and then we do the same thing with the other terms and what we end up with at the end is a fairly simple mathematical expression over here that i cannot simplify further but basically you'll notice that it only uses the stuff we have and it derives the thing we need so we have dl by dy for all the i's and those are used plenty of times here and also in addition what we're using is these xi hats and xj hats and they just come from the forward pass and otherwise this is a simple expression and it xi for all the i's and that's ultimately what we're interested in so that's the end of a batch norm backward pass analytically let's now implement this final result okay so i implemented the expression into a single line of code here and you can see that the max diff is tiny so this is the correct implementation of this formula now i'll just uh basically tell you that getting this formula here from this mathematical expression was not trivial and there's a lot going on packed into this one formula and this is a whole exercise by itself because you have to consider the fact that this formula here is just for a single neuron and a batch of 32 examples but what i'm doing here is i'm actually we actually have 64 neurons and so this expression has to in parallel evaluate the best from backward pass for all of those 64 neurons in parallel independently so this has to happen basically in every single column of the inputs here and in addition to that you see how there are a bunch of sums here and we need to make sure that when i do those sums that they broadcast correctly onto everything else that's here and so getting this expression is just like highly non trivial and i invite you to basically look through it and step through it and it's a whole exercise to make sure that this this checks out but once all the shapes agree and once you convince yourself that it's correct you can also verify that petros gets the exact same answer as well and so that gives you a lot of peace of mind that this mathematical formula is correctly implemented here and broadcasted correctly and replicated in parallel for all of the 64 neurons inside this batch term layer okay and finally exercise number four asks you to put it all together and here we have a redefinition of the entire problem so you see that we re-initialize the neural net from scratch and everything and then here instead of calling loss that backward we want to have the manual backpropagation here as we derived it up above so go up copy paste all the chunks of code that we've already derived put them here and derive your own gradients and then optimize this neural net basically using your own gradients all the way to the calibration of the batch norm and the evaluation of the loss and i was able to achieve quite a good loss basically the same loss you would achieve before and that shouldn't be surprising because all we've done is we've really gotten to loss that backward and we've pulled out all the code and inserted it here but those gradients are identical and everything is identical and the results are identical it's just that we have full visibility on exactly what goes on under the hood of lot of backward in this specific case okay and this is all of our code this is the full backward pass using basically the simplified backward pass for the cross entropy loss and the batch normalization so backpropagating through cross entropy the second layer the 10h null linearity the batch normalization through the first layer and through the embedding and so you see that this is only maybe what is this 20 lines of code or something like that and that's what gives us gradients and now we can potentially erase loss that backward so the way i have the code set up is you should be able to run this entire cell once you fill this in and this will run for only 100 iterations and then break and it breaks because it gives you an opportunity to check your gradients against pytorch so here our gradients we see are not exactly equal they are approximately equal and the differences are tiny 20 negative 9 or so and i don't exactly know where they're coming from to be honest so once we have some confidence that the gradients are basically correct uh we can take out the gradient checking we can disable this breaking statement and then we can basically disable loss that backward we don't need it anymore feels amazing to see that and then here when we are doing the update we're not going to use p dot grad this is the old way of pytorch we don't have that anymore because we're not doing backward we are going to use this update where we you see that i'm iterating over i've arranged the grads to be in the same order as the parameters and i'm zipping them up the gradients and the parameters into p and grad and then here i'm going to step with just the grad that we derived manually so the last piece is that none of this now requires gradients from pytorch and so one thing you can do here is you can do with torch dot no grad and offset this whole code block and really what you're saying is you're telling pytorch that hey i'm not going to call backward on any of this and this allows pytorch to be a bit more efficient with all of it and then we should be able to just uh run this and it's running and you see that loss backward is commented out and we're optimizing so we're going to leave this run and hopefully we get a good result okay so i allowed the neural net to finish optimization then here i calibrate the batch from parameters because i did not keep track of the running mean and very variance in their training loop then here i ran the loss and you see that we actually obtained a pretty good loss very similar to what we've achieved before and then here i'm sampling from the model and we see some of the values that we've used so basically the model worked and samples uh pretty decent results compared to what we're used to so everything is the same but of course the big deal is that we did not use lots of backward we did not use pytorch autograd and we estimated our gradients ourselves by hand and so hopefully you're looking at this the backward pass of this neural net and you're thinking to yourself actually that's not too complicated um each one of these layers is going to be a little bit more too complicated um each one of these layers is like three lines of code or something like that and most of it is fairly straightforward potentially with the notable exception of the batch normalization backward pass otherwise it's pretty good okay and that's everything i wanted to cover for this lecture so hopefully you found this interesting and what i liked about it honestly is that it gave us a very nice diversity of layers to back propagate through and um i think it gives a pretty nice and comprehensive sense of how these backward passes are implemented and how they work and you'd be able to derive them yourself but of course in practice you probably don't want to and you want to use the pytorch autograd but hopefully you have some intuition about how gradients flow backwards through the neural net starting at the loss and how they flow through all the variables and all the intermediate results and if you understood a good chunk of it and if you have a sense of that then you can count yourself as one of these buff dojis on the left instead of the uh dojis on the right here now in the next lecture we're actually going to go to recurrent neural nets lsdms and all the other variants of rns and we're going to start to complexify the architecture and start to achieve better log likelihoods and so i'm really looking forward to that and i'll see you then", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.32, "text": " Hi everyone. So today we are once again continuing our implementation of Makemore.", "tokens": [50364, 2421, 1518, 13, 407, 965, 321, 366, 1564, 797, 9289, 527, 11420, 295, 4387, 3138, 13, 50580, 50624, 823, 370, 1400, 321, 600, 808, 493, 281, 510, 11, 2120, 388, 11167, 43276, 13270, 11, 293, 527, 18161, 2533, 2956, 411, 341, 11, 50904, 50904, 293, 321, 645, 18114, 341, 670, 264, 1036, 1326, 16564, 13, 823, 286, 478, 988, 1518, 307, 588, 2919, 51112, 51112, 281, 352, 666, 18680, 1753, 18161, 9590, 293, 439, 295, 641, 21669, 293, 577, 436, 589, 11, 293, 264, 36709, 51336, 51336, 574, 1627, 293, 309, 311, 588, 4670, 293, 1880, 11, 293, 321, 434, 516, 281, 483, 257, 1101, 1874, 13, 583, 51508, 51508, 7015, 286, 519, 321, 362, 281, 6222, 510, 337, 472, 544, 7991, 13, 400, 264, 1778, 337, 300, 307, 321, 600, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.14757077252423323, "compression_ratio": 1.7469135802469136, "no_speech_prob": 0.04802422598004341}, {"id": 1, "seek": 0, "start": 5.2, "end": 10.8, "text": " Now so far we've come up to here, multilayer perceptrons, and our neural net looked like this,", "tokens": [50364, 2421, 1518, 13, 407, 965, 321, 366, 1564, 797, 9289, 527, 11420, 295, 4387, 3138, 13, 50580, 50624, 823, 370, 1400, 321, 600, 808, 493, 281, 510, 11, 2120, 388, 11167, 43276, 13270, 11, 293, 527, 18161, 2533, 2956, 411, 341, 11, 50904, 50904, 293, 321, 645, 18114, 341, 670, 264, 1036, 1326, 16564, 13, 823, 286, 478, 988, 1518, 307, 588, 2919, 51112, 51112, 281, 352, 666, 18680, 1753, 18161, 9590, 293, 439, 295, 641, 21669, 293, 577, 436, 589, 11, 293, 264, 36709, 51336, 51336, 574, 1627, 293, 309, 311, 588, 4670, 293, 1880, 11, 293, 321, 434, 516, 281, 483, 257, 1101, 1874, 13, 583, 51508, 51508, 7015, 286, 519, 321, 362, 281, 6222, 510, 337, 472, 544, 7991, 13, 400, 264, 1778, 337, 300, 307, 321, 600, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.14757077252423323, "compression_ratio": 1.7469135802469136, "no_speech_prob": 0.04802422598004341}, {"id": 2, "seek": 0, "start": 10.8, "end": 14.96, "text": " and we were implementing this over the last few lectures. Now I'm sure everyone is very excited", "tokens": [50364, 2421, 1518, 13, 407, 965, 321, 366, 1564, 797, 9289, 527, 11420, 295, 4387, 3138, 13, 50580, 50624, 823, 370, 1400, 321, 600, 808, 493, 281, 510, 11, 2120, 388, 11167, 43276, 13270, 11, 293, 527, 18161, 2533, 2956, 411, 341, 11, 50904, 50904, 293, 321, 645, 18114, 341, 670, 264, 1036, 1326, 16564, 13, 823, 286, 478, 988, 1518, 307, 588, 2919, 51112, 51112, 281, 352, 666, 18680, 1753, 18161, 9590, 293, 439, 295, 641, 21669, 293, 577, 436, 589, 11, 293, 264, 36709, 51336, 51336, 574, 1627, 293, 309, 311, 588, 4670, 293, 1880, 11, 293, 321, 434, 516, 281, 483, 257, 1101, 1874, 13, 583, 51508, 51508, 7015, 286, 519, 321, 362, 281, 6222, 510, 337, 472, 544, 7991, 13, 400, 264, 1778, 337, 300, 307, 321, 600, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.14757077252423323, "compression_ratio": 1.7469135802469136, "no_speech_prob": 0.04802422598004341}, {"id": 3, "seek": 0, "start": 14.96, "end": 19.44, "text": " to go into recurrent neural networks and all of their variants and how they work, and the diagrams", "tokens": [50364, 2421, 1518, 13, 407, 965, 321, 366, 1564, 797, 9289, 527, 11420, 295, 4387, 3138, 13, 50580, 50624, 823, 370, 1400, 321, 600, 808, 493, 281, 510, 11, 2120, 388, 11167, 43276, 13270, 11, 293, 527, 18161, 2533, 2956, 411, 341, 11, 50904, 50904, 293, 321, 645, 18114, 341, 670, 264, 1036, 1326, 16564, 13, 823, 286, 478, 988, 1518, 307, 588, 2919, 51112, 51112, 281, 352, 666, 18680, 1753, 18161, 9590, 293, 439, 295, 641, 21669, 293, 577, 436, 589, 11, 293, 264, 36709, 51336, 51336, 574, 1627, 293, 309, 311, 588, 4670, 293, 1880, 11, 293, 321, 434, 516, 281, 483, 257, 1101, 1874, 13, 583, 51508, 51508, 7015, 286, 519, 321, 362, 281, 6222, 510, 337, 472, 544, 7991, 13, 400, 264, 1778, 337, 300, 307, 321, 600, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.14757077252423323, "compression_ratio": 1.7469135802469136, "no_speech_prob": 0.04802422598004341}, {"id": 4, "seek": 0, "start": 19.44, "end": 22.88, "text": " look cool and it's very exciting and interesting, and we're going to get a better result. But", "tokens": [50364, 2421, 1518, 13, 407, 965, 321, 366, 1564, 797, 9289, 527, 11420, 295, 4387, 3138, 13, 50580, 50624, 823, 370, 1400, 321, 600, 808, 493, 281, 510, 11, 2120, 388, 11167, 43276, 13270, 11, 293, 527, 18161, 2533, 2956, 411, 341, 11, 50904, 50904, 293, 321, 645, 18114, 341, 670, 264, 1036, 1326, 16564, 13, 823, 286, 478, 988, 1518, 307, 588, 2919, 51112, 51112, 281, 352, 666, 18680, 1753, 18161, 9590, 293, 439, 295, 641, 21669, 293, 577, 436, 589, 11, 293, 264, 36709, 51336, 51336, 574, 1627, 293, 309, 311, 588, 4670, 293, 1880, 11, 293, 321, 434, 516, 281, 483, 257, 1101, 1874, 13, 583, 51508, 51508, 7015, 286, 519, 321, 362, 281, 6222, 510, 337, 472, 544, 7991, 13, 400, 264, 1778, 337, 300, 307, 321, 600, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.14757077252423323, "compression_ratio": 1.7469135802469136, "no_speech_prob": 0.04802422598004341}, {"id": 5, "seek": 0, "start": 22.88, "end": 28.96, "text": " unfortunately I think we have to remain here for one more lecture. And the reason for that is we've", "tokens": [50364, 2421, 1518, 13, 407, 965, 321, 366, 1564, 797, 9289, 527, 11420, 295, 4387, 3138, 13, 50580, 50624, 823, 370, 1400, 321, 600, 808, 493, 281, 510, 11, 2120, 388, 11167, 43276, 13270, 11, 293, 527, 18161, 2533, 2956, 411, 341, 11, 50904, 50904, 293, 321, 645, 18114, 341, 670, 264, 1036, 1326, 16564, 13, 823, 286, 478, 988, 1518, 307, 588, 2919, 51112, 51112, 281, 352, 666, 18680, 1753, 18161, 9590, 293, 439, 295, 641, 21669, 293, 577, 436, 589, 11, 293, 264, 36709, 51336, 51336, 574, 1627, 293, 309, 311, 588, 4670, 293, 1880, 11, 293, 321, 434, 516, 281, 483, 257, 1101, 1874, 13, 583, 51508, 51508, 7015, 286, 519, 321, 362, 281, 6222, 510, 337, 472, 544, 7991, 13, 400, 264, 1778, 337, 300, 307, 321, 600, 51812, 51812], "temperature": 0.0, "avg_logprob": -0.14757077252423323, "compression_ratio": 1.7469135802469136, "no_speech_prob": 0.04802422598004341}, {"id": 6, "seek": 2896, "start": 28.96, "end": 33.04, "text": " already trained this multilayer perceptron, right, and we are getting pretty good loss, and I think", "tokens": [50364, 1217, 8895, 341, 2120, 388, 11167, 43276, 2044, 11, 558, 11, 293, 321, 366, 1242, 1238, 665, 4470, 11, 293, 286, 519, 50568, 50568, 321, 362, 257, 1238, 8681, 3701, 295, 264, 9482, 293, 577, 309, 1985, 13, 583, 264, 1622, 295, 50812, 50812, 3089, 510, 300, 286, 747, 364, 2734, 365, 307, 510, 11, 4470, 13, 3207, 1007, 13, 663, 307, 11, 321, 366, 1940, 9953, 51, 284, 339, 51108, 51108, 1476, 664, 6206, 293, 1228, 309, 281, 8873, 439, 295, 527, 2771, 2448, 2051, 264, 636, 13, 400, 286, 576, 411, 281, 4159, 51364, 51364, 264, 764, 295, 4470, 13, 3207, 1007, 11, 293, 286, 576, 411, 505, 281, 2464, 527, 23897, 1320, 16945, 322, 264, 1496, 295, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.0769891200527068, "compression_ratio": 1.7381818181818183, "no_speech_prob": 2.2119240384199657e-05}, {"id": 7, "seek": 2896, "start": 33.04, "end": 37.92, "text": " we have a pretty decent understanding of the architecture and how it works. But the line of", "tokens": [50364, 1217, 8895, 341, 2120, 388, 11167, 43276, 2044, 11, 558, 11, 293, 321, 366, 1242, 1238, 665, 4470, 11, 293, 286, 519, 50568, 50568, 321, 362, 257, 1238, 8681, 3701, 295, 264, 9482, 293, 577, 309, 1985, 13, 583, 264, 1622, 295, 50812, 50812, 3089, 510, 300, 286, 747, 364, 2734, 365, 307, 510, 11, 4470, 13, 3207, 1007, 13, 663, 307, 11, 321, 366, 1940, 9953, 51, 284, 339, 51108, 51108, 1476, 664, 6206, 293, 1228, 309, 281, 8873, 439, 295, 527, 2771, 2448, 2051, 264, 636, 13, 400, 286, 576, 411, 281, 4159, 51364, 51364, 264, 764, 295, 4470, 13, 3207, 1007, 11, 293, 286, 576, 411, 505, 281, 2464, 527, 23897, 1320, 16945, 322, 264, 1496, 295, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.0769891200527068, "compression_ratio": 1.7381818181818183, "no_speech_prob": 2.2119240384199657e-05}, {"id": 8, "seek": 2896, "start": 37.92, "end": 43.84, "text": " code here that I take an issue with is here, loss.backward. That is, we are taking PyTorch", "tokens": [50364, 1217, 8895, 341, 2120, 388, 11167, 43276, 2044, 11, 558, 11, 293, 321, 366, 1242, 1238, 665, 4470, 11, 293, 286, 519, 50568, 50568, 321, 362, 257, 1238, 8681, 3701, 295, 264, 9482, 293, 577, 309, 1985, 13, 583, 264, 1622, 295, 50812, 50812, 3089, 510, 300, 286, 747, 364, 2734, 365, 307, 510, 11, 4470, 13, 3207, 1007, 13, 663, 307, 11, 321, 366, 1940, 9953, 51, 284, 339, 51108, 51108, 1476, 664, 6206, 293, 1228, 309, 281, 8873, 439, 295, 527, 2771, 2448, 2051, 264, 636, 13, 400, 286, 576, 411, 281, 4159, 51364, 51364, 264, 764, 295, 4470, 13, 3207, 1007, 11, 293, 286, 576, 411, 505, 281, 2464, 527, 23897, 1320, 16945, 322, 264, 1496, 295, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.0769891200527068, "compression_ratio": 1.7381818181818183, "no_speech_prob": 2.2119240384199657e-05}, {"id": 9, "seek": 2896, "start": 43.84, "end": 48.96, "text": " autograd and using it to calculate all of our gradients along the way. And I would like to remove", "tokens": [50364, 1217, 8895, 341, 2120, 388, 11167, 43276, 2044, 11, 558, 11, 293, 321, 366, 1242, 1238, 665, 4470, 11, 293, 286, 519, 50568, 50568, 321, 362, 257, 1238, 8681, 3701, 295, 264, 9482, 293, 577, 309, 1985, 13, 583, 264, 1622, 295, 50812, 50812, 3089, 510, 300, 286, 747, 364, 2734, 365, 307, 510, 11, 4470, 13, 3207, 1007, 13, 663, 307, 11, 321, 366, 1940, 9953, 51, 284, 339, 51108, 51108, 1476, 664, 6206, 293, 1228, 309, 281, 8873, 439, 295, 527, 2771, 2448, 2051, 264, 636, 13, 400, 286, 576, 411, 281, 4159, 51364, 51364, 264, 764, 295, 4470, 13, 3207, 1007, 11, 293, 286, 576, 411, 505, 281, 2464, 527, 23897, 1320, 16945, 322, 264, 1496, 295, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.0769891200527068, "compression_ratio": 1.7381818181818183, "no_speech_prob": 2.2119240384199657e-05}, {"id": 10, "seek": 2896, "start": 48.96, "end": 53.52, "text": " the use of loss.backward, and I would like us to write our backward pass manually on the level of", "tokens": [50364, 1217, 8895, 341, 2120, 388, 11167, 43276, 2044, 11, 558, 11, 293, 321, 366, 1242, 1238, 665, 4470, 11, 293, 286, 519, 50568, 50568, 321, 362, 257, 1238, 8681, 3701, 295, 264, 9482, 293, 577, 309, 1985, 13, 583, 264, 1622, 295, 50812, 50812, 3089, 510, 300, 286, 747, 364, 2734, 365, 307, 510, 11, 4470, 13, 3207, 1007, 13, 663, 307, 11, 321, 366, 1940, 9953, 51, 284, 339, 51108, 51108, 1476, 664, 6206, 293, 1228, 309, 281, 8873, 439, 295, 527, 2771, 2448, 2051, 264, 636, 13, 400, 286, 576, 411, 281, 4159, 51364, 51364, 264, 764, 295, 4470, 13, 3207, 1007, 11, 293, 286, 576, 411, 505, 281, 2464, 527, 23897, 1320, 16945, 322, 264, 1496, 295, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.0769891200527068, "compression_ratio": 1.7381818181818183, "no_speech_prob": 2.2119240384199657e-05}, {"id": 11, "seek": 5352, "start": 53.52, "end": 59.2, "text": " tensors. And I think that this is a very useful exercise for the following reasons. I actually", "tokens": [50364, 10688, 830, 13, 400, 286, 519, 300, 341, 307, 257, 588, 4420, 5380, 337, 264, 3480, 4112, 13, 286, 767, 50648, 50648, 362, 364, 2302, 6968, 2183, 322, 341, 4829, 11, 457, 286, 411, 281, 818, 646, 79, 1513, 559, 399, 257, 476, 15681, 37765, 13, 50924, 50964, 400, 437, 286, 914, 538, 300, 307, 646, 79, 1513, 559, 399, 1177, 380, 445, 652, 428, 18161, 9590, 445, 589, 51208, 51208, 39763, 13, 467, 311, 406, 264, 1389, 300, 291, 393, 445, 8630, 493, 23211, 28761, 8474, 295, 819, 9364, 51396, 51396, 6828, 293, 445, 3278, 428, 7350, 293, 646, 79, 1513, 559, 473, 293, 1203, 307, 869, 13, 9514, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.06605214403386701, "compression_ratio": 1.6884057971014492, "no_speech_prob": 9.515645615465473e-06}, {"id": 12, "seek": 5352, "start": 59.2, "end": 64.72, "text": " have an entire blog post on this topic, but I like to call backpropagation a leaky abstraction.", "tokens": [50364, 10688, 830, 13, 400, 286, 519, 300, 341, 307, 257, 588, 4420, 5380, 337, 264, 3480, 4112, 13, 286, 767, 50648, 50648, 362, 364, 2302, 6968, 2183, 322, 341, 4829, 11, 457, 286, 411, 281, 818, 646, 79, 1513, 559, 399, 257, 476, 15681, 37765, 13, 50924, 50964, 400, 437, 286, 914, 538, 300, 307, 646, 79, 1513, 559, 399, 1177, 380, 445, 652, 428, 18161, 9590, 445, 589, 51208, 51208, 39763, 13, 467, 311, 406, 264, 1389, 300, 291, 393, 445, 8630, 493, 23211, 28761, 8474, 295, 819, 9364, 51396, 51396, 6828, 293, 445, 3278, 428, 7350, 293, 646, 79, 1513, 559, 473, 293, 1203, 307, 869, 13, 9514, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.06605214403386701, "compression_ratio": 1.6884057971014492, "no_speech_prob": 9.515645615465473e-06}, {"id": 13, "seek": 5352, "start": 65.52000000000001, "end": 70.4, "text": " And what I mean by that is backpropagation doesn't just make your neural networks just work", "tokens": [50364, 10688, 830, 13, 400, 286, 519, 300, 341, 307, 257, 588, 4420, 5380, 337, 264, 3480, 4112, 13, 286, 767, 50648, 50648, 362, 364, 2302, 6968, 2183, 322, 341, 4829, 11, 457, 286, 411, 281, 818, 646, 79, 1513, 559, 399, 257, 476, 15681, 37765, 13, 50924, 50964, 400, 437, 286, 914, 538, 300, 307, 646, 79, 1513, 559, 399, 1177, 380, 445, 652, 428, 18161, 9590, 445, 589, 51208, 51208, 39763, 13, 467, 311, 406, 264, 1389, 300, 291, 393, 445, 8630, 493, 23211, 28761, 8474, 295, 819, 9364, 51396, 51396, 6828, 293, 445, 3278, 428, 7350, 293, 646, 79, 1513, 559, 473, 293, 1203, 307, 869, 13, 9514, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.06605214403386701, "compression_ratio": 1.6884057971014492, "no_speech_prob": 9.515645615465473e-06}, {"id": 14, "seek": 5352, "start": 70.4, "end": 74.16, "text": " magically. It's not the case that you can just stack up arbitrary Lego blocks of differentiable", "tokens": [50364, 10688, 830, 13, 400, 286, 519, 300, 341, 307, 257, 588, 4420, 5380, 337, 264, 3480, 4112, 13, 286, 767, 50648, 50648, 362, 364, 2302, 6968, 2183, 322, 341, 4829, 11, 457, 286, 411, 281, 818, 646, 79, 1513, 559, 399, 257, 476, 15681, 37765, 13, 50924, 50964, 400, 437, 286, 914, 538, 300, 307, 646, 79, 1513, 559, 399, 1177, 380, 445, 652, 428, 18161, 9590, 445, 589, 51208, 51208, 39763, 13, 467, 311, 406, 264, 1389, 300, 291, 393, 445, 8630, 493, 23211, 28761, 8474, 295, 819, 9364, 51396, 51396, 6828, 293, 445, 3278, 428, 7350, 293, 646, 79, 1513, 559, 473, 293, 1203, 307, 869, 13, 9514, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.06605214403386701, "compression_ratio": 1.6884057971014492, "no_speech_prob": 9.515645615465473e-06}, {"id": 15, "seek": 5352, "start": 74.16, "end": 79.2, "text": " functions and just cross your fingers and backpropagate and everything is great. Things", "tokens": [50364, 10688, 830, 13, 400, 286, 519, 300, 341, 307, 257, 588, 4420, 5380, 337, 264, 3480, 4112, 13, 286, 767, 50648, 50648, 362, 364, 2302, 6968, 2183, 322, 341, 4829, 11, 457, 286, 411, 281, 818, 646, 79, 1513, 559, 399, 257, 476, 15681, 37765, 13, 50924, 50964, 400, 437, 286, 914, 538, 300, 307, 646, 79, 1513, 559, 399, 1177, 380, 445, 652, 428, 18161, 9590, 445, 589, 51208, 51208, 39763, 13, 467, 311, 406, 264, 1389, 300, 291, 393, 445, 8630, 493, 23211, 28761, 8474, 295, 819, 9364, 51396, 51396, 6828, 293, 445, 3278, 428, 7350, 293, 646, 79, 1513, 559, 473, 293, 1203, 307, 869, 13, 9514, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.06605214403386701, "compression_ratio": 1.6884057971014492, "no_speech_prob": 9.515645615465473e-06}, {"id": 16, "seek": 7920, "start": 79.2, "end": 83.52000000000001, "text": " don't just work automatically. It is a leaky abstraction in the sense that you can shoot", "tokens": [50364, 500, 380, 445, 589, 6772, 13, 467, 307, 257, 476, 15681, 37765, 294, 264, 2020, 300, 291, 393, 3076, 50580, 50580, 1803, 294, 264, 2671, 498, 291, 360, 406, 1223, 1080, 2154, 1124, 13, 467, 486, 39763, 406, 589, 420, 406, 589, 50852, 50852, 5028, 379, 11, 293, 291, 486, 643, 281, 1223, 577, 309, 1985, 833, 264, 13376, 498, 291, 434, 7159, 281, 24083, 309, 51132, 51132, 293, 498, 291, 366, 7159, 281, 2985, 309, 294, 428, 18161, 2533, 13, 407, 341, 6968, 2183, 510, 490, 257, 1339, 2057, 51428, 51428, 1709, 666, 512, 295, 729, 5110, 13, 407, 337, 1365, 11, 321, 600, 1217, 5343, 552, 11, 512, 295, 552, 1217, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.07258539684748246, "compression_ratio": 1.8195488721804511, "no_speech_prob": 1.3005903383600526e-05}, {"id": 17, "seek": 7920, "start": 83.52000000000001, "end": 88.96000000000001, "text": " yourself in the foot if you do not understand its internals. It will magically not work or not work", "tokens": [50364, 500, 380, 445, 589, 6772, 13, 467, 307, 257, 476, 15681, 37765, 294, 264, 2020, 300, 291, 393, 3076, 50580, 50580, 1803, 294, 264, 2671, 498, 291, 360, 406, 1223, 1080, 2154, 1124, 13, 467, 486, 39763, 406, 589, 420, 406, 589, 50852, 50852, 5028, 379, 11, 293, 291, 486, 643, 281, 1223, 577, 309, 1985, 833, 264, 13376, 498, 291, 434, 7159, 281, 24083, 309, 51132, 51132, 293, 498, 291, 366, 7159, 281, 2985, 309, 294, 428, 18161, 2533, 13, 407, 341, 6968, 2183, 510, 490, 257, 1339, 2057, 51428, 51428, 1709, 666, 512, 295, 729, 5110, 13, 407, 337, 1365, 11, 321, 600, 1217, 5343, 552, 11, 512, 295, 552, 1217, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.07258539684748246, "compression_ratio": 1.8195488721804511, "no_speech_prob": 1.3005903383600526e-05}, {"id": 18, "seek": 7920, "start": 88.96000000000001, "end": 94.56, "text": " optimally, and you will need to understand how it works under the hood if you're hoping to debug it", "tokens": [50364, 500, 380, 445, 589, 6772, 13, 467, 307, 257, 476, 15681, 37765, 294, 264, 2020, 300, 291, 393, 3076, 50580, 50580, 1803, 294, 264, 2671, 498, 291, 360, 406, 1223, 1080, 2154, 1124, 13, 467, 486, 39763, 406, 589, 420, 406, 589, 50852, 50852, 5028, 379, 11, 293, 291, 486, 643, 281, 1223, 577, 309, 1985, 833, 264, 13376, 498, 291, 434, 7159, 281, 24083, 309, 51132, 51132, 293, 498, 291, 366, 7159, 281, 2985, 309, 294, 428, 18161, 2533, 13, 407, 341, 6968, 2183, 510, 490, 257, 1339, 2057, 51428, 51428, 1709, 666, 512, 295, 729, 5110, 13, 407, 337, 1365, 11, 321, 600, 1217, 5343, 552, 11, 512, 295, 552, 1217, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.07258539684748246, "compression_ratio": 1.8195488721804511, "no_speech_prob": 1.3005903383600526e-05}, {"id": 19, "seek": 7920, "start": 94.56, "end": 100.48, "text": " and if you are hoping to address it in your neural net. So this blog post here from a while ago", "tokens": [50364, 500, 380, 445, 589, 6772, 13, 467, 307, 257, 476, 15681, 37765, 294, 264, 2020, 300, 291, 393, 3076, 50580, 50580, 1803, 294, 264, 2671, 498, 291, 360, 406, 1223, 1080, 2154, 1124, 13, 467, 486, 39763, 406, 589, 420, 406, 589, 50852, 50852, 5028, 379, 11, 293, 291, 486, 643, 281, 1223, 577, 309, 1985, 833, 264, 13376, 498, 291, 434, 7159, 281, 24083, 309, 51132, 51132, 293, 498, 291, 366, 7159, 281, 2985, 309, 294, 428, 18161, 2533, 13, 407, 341, 6968, 2183, 510, 490, 257, 1339, 2057, 51428, 51428, 1709, 666, 512, 295, 729, 5110, 13, 407, 337, 1365, 11, 321, 600, 1217, 5343, 552, 11, 512, 295, 552, 1217, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.07258539684748246, "compression_ratio": 1.8195488721804511, "no_speech_prob": 1.3005903383600526e-05}, {"id": 20, "seek": 7920, "start": 100.48, "end": 105.04, "text": " goes into some of those examples. So for example, we've already covered them, some of them already.", "tokens": [50364, 500, 380, 445, 589, 6772, 13, 467, 307, 257, 476, 15681, 37765, 294, 264, 2020, 300, 291, 393, 3076, 50580, 50580, 1803, 294, 264, 2671, 498, 291, 360, 406, 1223, 1080, 2154, 1124, 13, 467, 486, 39763, 406, 589, 420, 406, 589, 50852, 50852, 5028, 379, 11, 293, 291, 486, 643, 281, 1223, 577, 309, 1985, 833, 264, 13376, 498, 291, 434, 7159, 281, 24083, 309, 51132, 51132, 293, 498, 291, 366, 7159, 281, 2985, 309, 294, 428, 18161, 2533, 13, 407, 341, 6968, 2183, 510, 490, 257, 1339, 2057, 51428, 51428, 1709, 666, 512, 295, 729, 5110, 13, 407, 337, 1365, 11, 321, 600, 1217, 5343, 552, 11, 512, 295, 552, 1217, 13, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.07258539684748246, "compression_ratio": 1.8195488721804511, "no_speech_prob": 1.3005903383600526e-05}, {"id": 21, "seek": 10504, "start": 105.04, "end": 111.68, "text": " For example, the flat tails of these functions and how you do not want to saturate them too much", "tokens": [50364, 1171, 1365, 11, 264, 4962, 28537, 295, 613, 6828, 293, 577, 291, 360, 406, 528, 281, 21160, 473, 552, 886, 709, 50696, 50696, 570, 428, 2771, 2448, 486, 978, 13, 440, 1389, 295, 3116, 22027, 11, 597, 286, 600, 1217, 5343, 382, 731, 13, 50900, 50944, 440, 1389, 295, 35175, 420, 3161, 3807, 2771, 2448, 294, 264, 1389, 295, 18680, 1753, 18161, 9590, 11, 51148, 51148, 597, 321, 366, 466, 281, 2060, 13, 400, 550, 611, 291, 486, 2049, 808, 2108, 512, 5110, 294, 264, 4868, 13, 51456, 51492, 639, 307, 257, 35623, 302, 300, 286, 1352, 294, 257, 4974, 3089, 3096, 322, 264, 4705, 689, 436, 767, 362, 411, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.07985465568408631, "compression_ratio": 1.6989247311827957, "no_speech_prob": 1.6962669178610668e-05}, {"id": 22, "seek": 10504, "start": 111.68, "end": 115.76, "text": " because your gradients will die. The case of dead neurons, which I've already covered as well.", "tokens": [50364, 1171, 1365, 11, 264, 4962, 28537, 295, 613, 6828, 293, 577, 291, 360, 406, 528, 281, 21160, 473, 552, 886, 709, 50696, 50696, 570, 428, 2771, 2448, 486, 978, 13, 440, 1389, 295, 3116, 22027, 11, 597, 286, 600, 1217, 5343, 382, 731, 13, 50900, 50944, 440, 1389, 295, 35175, 420, 3161, 3807, 2771, 2448, 294, 264, 1389, 295, 18680, 1753, 18161, 9590, 11, 51148, 51148, 597, 321, 366, 466, 281, 2060, 13, 400, 550, 611, 291, 486, 2049, 808, 2108, 512, 5110, 294, 264, 4868, 13, 51456, 51492, 639, 307, 257, 35623, 302, 300, 286, 1352, 294, 257, 4974, 3089, 3096, 322, 264, 4705, 689, 436, 767, 362, 411, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.07985465568408631, "compression_ratio": 1.6989247311827957, "no_speech_prob": 1.6962669178610668e-05}, {"id": 23, "seek": 10504, "start": 116.64, "end": 120.72, "text": " The case of exploding or vanishing gradients in the case of recurrent neural networks,", "tokens": [50364, 1171, 1365, 11, 264, 4962, 28537, 295, 613, 6828, 293, 577, 291, 360, 406, 528, 281, 21160, 473, 552, 886, 709, 50696, 50696, 570, 428, 2771, 2448, 486, 978, 13, 440, 1389, 295, 3116, 22027, 11, 597, 286, 600, 1217, 5343, 382, 731, 13, 50900, 50944, 440, 1389, 295, 35175, 420, 3161, 3807, 2771, 2448, 294, 264, 1389, 295, 18680, 1753, 18161, 9590, 11, 51148, 51148, 597, 321, 366, 466, 281, 2060, 13, 400, 550, 611, 291, 486, 2049, 808, 2108, 512, 5110, 294, 264, 4868, 13, 51456, 51492, 639, 307, 257, 35623, 302, 300, 286, 1352, 294, 257, 4974, 3089, 3096, 322, 264, 4705, 689, 436, 767, 362, 411, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.07985465568408631, "compression_ratio": 1.6989247311827957, "no_speech_prob": 1.6962669178610668e-05}, {"id": 24, "seek": 10504, "start": 120.72, "end": 126.88000000000001, "text": " which we are about to cover. And then also you will often come across some examples in the wild.", "tokens": [50364, 1171, 1365, 11, 264, 4962, 28537, 295, 613, 6828, 293, 577, 291, 360, 406, 528, 281, 21160, 473, 552, 886, 709, 50696, 50696, 570, 428, 2771, 2448, 486, 978, 13, 440, 1389, 295, 3116, 22027, 11, 597, 286, 600, 1217, 5343, 382, 731, 13, 50900, 50944, 440, 1389, 295, 35175, 420, 3161, 3807, 2771, 2448, 294, 264, 1389, 295, 18680, 1753, 18161, 9590, 11, 51148, 51148, 597, 321, 366, 466, 281, 2060, 13, 400, 550, 611, 291, 486, 2049, 808, 2108, 512, 5110, 294, 264, 4868, 13, 51456, 51492, 639, 307, 257, 35623, 302, 300, 286, 1352, 294, 257, 4974, 3089, 3096, 322, 264, 4705, 689, 436, 767, 362, 411, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.07985465568408631, "compression_ratio": 1.6989247311827957, "no_speech_prob": 1.6962669178610668e-05}, {"id": 25, "seek": 10504, "start": 127.60000000000001, "end": 132.48000000000002, "text": " This is a snippet that I found in a random code base on the internet where they actually have like", "tokens": [50364, 1171, 1365, 11, 264, 4962, 28537, 295, 613, 6828, 293, 577, 291, 360, 406, 528, 281, 21160, 473, 552, 886, 709, 50696, 50696, 570, 428, 2771, 2448, 486, 978, 13, 440, 1389, 295, 3116, 22027, 11, 597, 286, 600, 1217, 5343, 382, 731, 13, 50900, 50944, 440, 1389, 295, 35175, 420, 3161, 3807, 2771, 2448, 294, 264, 1389, 295, 18680, 1753, 18161, 9590, 11, 51148, 51148, 597, 321, 366, 466, 281, 2060, 13, 400, 550, 611, 291, 486, 2049, 808, 2108, 512, 5110, 294, 264, 4868, 13, 51456, 51492, 639, 307, 257, 35623, 302, 300, 286, 1352, 294, 257, 4974, 3089, 3096, 322, 264, 4705, 689, 436, 767, 362, 411, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.07985465568408631, "compression_ratio": 1.6989247311827957, "no_speech_prob": 1.6962669178610668e-05}, {"id": 26, "seek": 13248, "start": 132.48, "end": 138.72, "text": " a very subtle but pretty major bug in their implementation. And the bug points at the fact", "tokens": [50364, 257, 588, 13743, 457, 1238, 2563, 7426, 294, 641, 11420, 13, 400, 264, 7426, 2793, 412, 264, 1186, 50676, 50676, 300, 264, 3793, 295, 341, 3089, 775, 406, 767, 1223, 646, 79, 1513, 559, 399, 13, 407, 437, 436, 434, 1382, 50856, 50856, 281, 360, 510, 307, 436, 434, 1382, 281, 7353, 264, 4470, 412, 257, 1629, 6674, 2158, 13, 583, 767, 437, 436, 434, 51104, 51104, 1382, 281, 360, 307, 436, 434, 1382, 281, 7353, 264, 2771, 2448, 281, 362, 257, 6674, 2158, 2602, 295, 1382, 281, 7353, 51312, 51312, 264, 4470, 412, 257, 6674, 2158, 13, 400, 37779, 11, 436, 434, 1936, 9853, 512, 295, 264, 484, 23646, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.07022383338526676, "compression_ratio": 2.037974683544304, "no_speech_prob": 3.535100768203847e-05}, {"id": 27, "seek": 13248, "start": 138.72, "end": 142.32, "text": " that the author of this code does not actually understand backpropagation. So what they're trying", "tokens": [50364, 257, 588, 13743, 457, 1238, 2563, 7426, 294, 641, 11420, 13, 400, 264, 7426, 2793, 412, 264, 1186, 50676, 50676, 300, 264, 3793, 295, 341, 3089, 775, 406, 767, 1223, 646, 79, 1513, 559, 399, 13, 407, 437, 436, 434, 1382, 50856, 50856, 281, 360, 510, 307, 436, 434, 1382, 281, 7353, 264, 4470, 412, 257, 1629, 6674, 2158, 13, 583, 767, 437, 436, 434, 51104, 51104, 1382, 281, 360, 307, 436, 434, 1382, 281, 7353, 264, 2771, 2448, 281, 362, 257, 6674, 2158, 2602, 295, 1382, 281, 7353, 51312, 51312, 264, 4470, 412, 257, 6674, 2158, 13, 400, 37779, 11, 436, 434, 1936, 9853, 512, 295, 264, 484, 23646, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.07022383338526676, "compression_ratio": 2.037974683544304, "no_speech_prob": 3.535100768203847e-05}, {"id": 28, "seek": 13248, "start": 142.32, "end": 147.28, "text": " to do here is they're trying to clip the loss at a certain maximum value. But actually what they're", "tokens": [50364, 257, 588, 13743, 457, 1238, 2563, 7426, 294, 641, 11420, 13, 400, 264, 7426, 2793, 412, 264, 1186, 50676, 50676, 300, 264, 3793, 295, 341, 3089, 775, 406, 767, 1223, 646, 79, 1513, 559, 399, 13, 407, 437, 436, 434, 1382, 50856, 50856, 281, 360, 510, 307, 436, 434, 1382, 281, 7353, 264, 4470, 412, 257, 1629, 6674, 2158, 13, 583, 767, 437, 436, 434, 51104, 51104, 1382, 281, 360, 307, 436, 434, 1382, 281, 7353, 264, 2771, 2448, 281, 362, 257, 6674, 2158, 2602, 295, 1382, 281, 7353, 51312, 51312, 264, 4470, 412, 257, 6674, 2158, 13, 400, 37779, 11, 436, 434, 1936, 9853, 512, 295, 264, 484, 23646, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.07022383338526676, "compression_ratio": 2.037974683544304, "no_speech_prob": 3.535100768203847e-05}, {"id": 29, "seek": 13248, "start": 147.28, "end": 151.44, "text": " trying to do is they're trying to clip the gradients to have a maximum value instead of trying to clip", "tokens": [50364, 257, 588, 13743, 457, 1238, 2563, 7426, 294, 641, 11420, 13, 400, 264, 7426, 2793, 412, 264, 1186, 50676, 50676, 300, 264, 3793, 295, 341, 3089, 775, 406, 767, 1223, 646, 79, 1513, 559, 399, 13, 407, 437, 436, 434, 1382, 50856, 50856, 281, 360, 510, 307, 436, 434, 1382, 281, 7353, 264, 4470, 412, 257, 1629, 6674, 2158, 13, 583, 767, 437, 436, 434, 51104, 51104, 1382, 281, 360, 307, 436, 434, 1382, 281, 7353, 264, 2771, 2448, 281, 362, 257, 6674, 2158, 2602, 295, 1382, 281, 7353, 51312, 51312, 264, 4470, 412, 257, 6674, 2158, 13, 400, 37779, 11, 436, 434, 1936, 9853, 512, 295, 264, 484, 23646, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.07022383338526676, "compression_ratio": 2.037974683544304, "no_speech_prob": 3.535100768203847e-05}, {"id": 30, "seek": 13248, "start": 151.44, "end": 157.51999999999998, "text": " the loss at a maximum value. And indirectly, they're basically causing some of the outliers", "tokens": [50364, 257, 588, 13743, 457, 1238, 2563, 7426, 294, 641, 11420, 13, 400, 264, 7426, 2793, 412, 264, 1186, 50676, 50676, 300, 264, 3793, 295, 341, 3089, 775, 406, 767, 1223, 646, 79, 1513, 559, 399, 13, 407, 437, 436, 434, 1382, 50856, 50856, 281, 360, 510, 307, 436, 434, 1382, 281, 7353, 264, 4470, 412, 257, 1629, 6674, 2158, 13, 583, 767, 437, 436, 434, 51104, 51104, 1382, 281, 360, 307, 436, 434, 1382, 281, 7353, 264, 2771, 2448, 281, 362, 257, 6674, 2158, 2602, 295, 1382, 281, 7353, 51312, 51312, 264, 4470, 412, 257, 6674, 2158, 13, 400, 37779, 11, 436, 434, 1936, 9853, 512, 295, 264, 484, 23646, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.07022383338526676, "compression_ratio": 2.037974683544304, "no_speech_prob": 3.535100768203847e-05}, {"id": 31, "seek": 15752, "start": 157.52, "end": 164.16000000000003, "text": " to be actually ignored because when you clip a loss of an outlier, you are setting its gradient to zero.", "tokens": [50364, 281, 312, 767, 19735, 570, 562, 291, 7353, 257, 4470, 295, 364, 484, 2753, 11, 291, 366, 3287, 1080, 16235, 281, 4018, 13, 50696, 50736, 400, 370, 362, 257, 574, 807, 341, 293, 1401, 807, 309, 13, 583, 456, 311, 1936, 257, 3840, 295, 13743, 2663, 50984, 50984, 300, 291, 434, 516, 281, 5042, 498, 291, 767, 458, 437, 291, 434, 884, 13, 400, 300, 311, 983, 286, 500, 380, 519, 51168, 51168, 309, 311, 264, 1389, 300, 570, 9953, 51, 284, 339, 420, 661, 29834, 2626, 1476, 664, 6206, 11, 309, 307, 1392, 337, 505, 51404, 51404, 281, 11200, 577, 309, 1985, 13, 823, 321, 600, 767, 1217, 5343, 1476, 664, 6206, 293, 321, 4114, 4532, 7165, 13, 583, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.08217024803161621, "compression_ratio": 1.6958041958041958, "no_speech_prob": 1.0952434422506485e-05}, {"id": 32, "seek": 15752, "start": 164.96, "end": 169.92000000000002, "text": " And so have a look through this and read through it. But there's basically a bunch of subtle issues", "tokens": [50364, 281, 312, 767, 19735, 570, 562, 291, 7353, 257, 4470, 295, 364, 484, 2753, 11, 291, 366, 3287, 1080, 16235, 281, 4018, 13, 50696, 50736, 400, 370, 362, 257, 574, 807, 341, 293, 1401, 807, 309, 13, 583, 456, 311, 1936, 257, 3840, 295, 13743, 2663, 50984, 50984, 300, 291, 434, 516, 281, 5042, 498, 291, 767, 458, 437, 291, 434, 884, 13, 400, 300, 311, 983, 286, 500, 380, 519, 51168, 51168, 309, 311, 264, 1389, 300, 570, 9953, 51, 284, 339, 420, 661, 29834, 2626, 1476, 664, 6206, 11, 309, 307, 1392, 337, 505, 51404, 51404, 281, 11200, 577, 309, 1985, 13, 823, 321, 600, 767, 1217, 5343, 1476, 664, 6206, 293, 321, 4114, 4532, 7165, 13, 583, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.08217024803161621, "compression_ratio": 1.6958041958041958, "no_speech_prob": 1.0952434422506485e-05}, {"id": 33, "seek": 15752, "start": 169.92000000000002, "end": 173.60000000000002, "text": " that you're going to avoid if you actually know what you're doing. And that's why I don't think", "tokens": [50364, 281, 312, 767, 19735, 570, 562, 291, 7353, 257, 4470, 295, 364, 484, 2753, 11, 291, 366, 3287, 1080, 16235, 281, 4018, 13, 50696, 50736, 400, 370, 362, 257, 574, 807, 341, 293, 1401, 807, 309, 13, 583, 456, 311, 1936, 257, 3840, 295, 13743, 2663, 50984, 50984, 300, 291, 434, 516, 281, 5042, 498, 291, 767, 458, 437, 291, 434, 884, 13, 400, 300, 311, 983, 286, 500, 380, 519, 51168, 51168, 309, 311, 264, 1389, 300, 570, 9953, 51, 284, 339, 420, 661, 29834, 2626, 1476, 664, 6206, 11, 309, 307, 1392, 337, 505, 51404, 51404, 281, 11200, 577, 309, 1985, 13, 823, 321, 600, 767, 1217, 5343, 1476, 664, 6206, 293, 321, 4114, 4532, 7165, 13, 583, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.08217024803161621, "compression_ratio": 1.6958041958041958, "no_speech_prob": 1.0952434422506485e-05}, {"id": 34, "seek": 15752, "start": 173.60000000000002, "end": 178.32000000000002, "text": " it's the case that because PyTorch or other frameworks offer autograd, it is okay for us", "tokens": [50364, 281, 312, 767, 19735, 570, 562, 291, 7353, 257, 4470, 295, 364, 484, 2753, 11, 291, 366, 3287, 1080, 16235, 281, 4018, 13, 50696, 50736, 400, 370, 362, 257, 574, 807, 341, 293, 1401, 807, 309, 13, 583, 456, 311, 1936, 257, 3840, 295, 13743, 2663, 50984, 50984, 300, 291, 434, 516, 281, 5042, 498, 291, 767, 458, 437, 291, 434, 884, 13, 400, 300, 311, 983, 286, 500, 380, 519, 51168, 51168, 309, 311, 264, 1389, 300, 570, 9953, 51, 284, 339, 420, 661, 29834, 2626, 1476, 664, 6206, 11, 309, 307, 1392, 337, 505, 51404, 51404, 281, 11200, 577, 309, 1985, 13, 823, 321, 600, 767, 1217, 5343, 1476, 664, 6206, 293, 321, 4114, 4532, 7165, 13, 583, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.08217024803161621, "compression_ratio": 1.6958041958041958, "no_speech_prob": 1.0952434422506485e-05}, {"id": 35, "seek": 15752, "start": 178.32000000000002, "end": 185.44, "text": " to ignore how it works. Now we've actually already covered autograd and we wrote micrograd. But", "tokens": [50364, 281, 312, 767, 19735, 570, 562, 291, 7353, 257, 4470, 295, 364, 484, 2753, 11, 291, 366, 3287, 1080, 16235, 281, 4018, 13, 50696, 50736, 400, 370, 362, 257, 574, 807, 341, 293, 1401, 807, 309, 13, 583, 456, 311, 1936, 257, 3840, 295, 13743, 2663, 50984, 50984, 300, 291, 434, 516, 281, 5042, 498, 291, 767, 458, 437, 291, 434, 884, 13, 400, 300, 311, 983, 286, 500, 380, 519, 51168, 51168, 309, 311, 264, 1389, 300, 570, 9953, 51, 284, 339, 420, 661, 29834, 2626, 1476, 664, 6206, 11, 309, 307, 1392, 337, 505, 51404, 51404, 281, 11200, 577, 309, 1985, 13, 823, 321, 600, 767, 1217, 5343, 1476, 664, 6206, 293, 321, 4114, 4532, 7165, 13, 583, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.08217024803161621, "compression_ratio": 1.6958041958041958, "no_speech_prob": 1.0952434422506485e-05}, {"id": 36, "seek": 18544, "start": 185.44, "end": 190.24, "text": " micrograd was an autograd engine only on the level of individual scalars. So the atoms were", "tokens": [50364, 4532, 7165, 390, 364, 1476, 664, 6206, 2848, 787, 322, 264, 1496, 295, 2609, 15664, 685, 13, 407, 264, 16871, 645, 50604, 50604, 2167, 2609, 3547, 13, 400, 286, 500, 380, 519, 309, 311, 1547, 13, 400, 286, 1116, 411, 505, 281, 1936, 519, 466, 50852, 50852, 646, 79, 1513, 559, 399, 322, 264, 1496, 295, 10688, 830, 382, 731, 13, 400, 370, 294, 257, 12691, 11, 286, 519, 309, 311, 257, 665, 5380, 13, 51108, 51108, 286, 519, 309, 307, 588, 11, 588, 8263, 13, 509, 434, 516, 281, 1813, 1101, 412, 45592, 18161, 9590, 293, 51376, 51376, 1455, 988, 300, 291, 1223, 437, 291, 434, 884, 13, 467, 307, 516, 281, 652, 1203, 4498, 13691, 13, 407, 51592, 51592, 291, 434, 406, 516, 281, 312, 6296, 466, 437, 307, 7633, 1314, 490, 291, 13, 400, 1936, 294, 2674, 11, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.04661999406485722, "compression_ratio": 1.8074534161490683, "no_speech_prob": 4.222638381179422e-06}, {"id": 37, "seek": 18544, "start": 190.24, "end": 195.2, "text": " single individual numbers. And I don't think it's enough. And I'd like us to basically think about", "tokens": [50364, 4532, 7165, 390, 364, 1476, 664, 6206, 2848, 787, 322, 264, 1496, 295, 2609, 15664, 685, 13, 407, 264, 16871, 645, 50604, 50604, 2167, 2609, 3547, 13, 400, 286, 500, 380, 519, 309, 311, 1547, 13, 400, 286, 1116, 411, 505, 281, 1936, 519, 466, 50852, 50852, 646, 79, 1513, 559, 399, 322, 264, 1496, 295, 10688, 830, 382, 731, 13, 400, 370, 294, 257, 12691, 11, 286, 519, 309, 311, 257, 665, 5380, 13, 51108, 51108, 286, 519, 309, 307, 588, 11, 588, 8263, 13, 509, 434, 516, 281, 1813, 1101, 412, 45592, 18161, 9590, 293, 51376, 51376, 1455, 988, 300, 291, 1223, 437, 291, 434, 884, 13, 467, 307, 516, 281, 652, 1203, 4498, 13691, 13, 407, 51592, 51592, 291, 434, 406, 516, 281, 312, 6296, 466, 437, 307, 7633, 1314, 490, 291, 13, 400, 1936, 294, 2674, 11, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.04661999406485722, "compression_ratio": 1.8074534161490683, "no_speech_prob": 4.222638381179422e-06}, {"id": 38, "seek": 18544, "start": 195.2, "end": 200.32, "text": " backpropagation on the level of tensors as well. And so in a summary, I think it's a good exercise.", "tokens": [50364, 4532, 7165, 390, 364, 1476, 664, 6206, 2848, 787, 322, 264, 1496, 295, 2609, 15664, 685, 13, 407, 264, 16871, 645, 50604, 50604, 2167, 2609, 3547, 13, 400, 286, 500, 380, 519, 309, 311, 1547, 13, 400, 286, 1116, 411, 505, 281, 1936, 519, 466, 50852, 50852, 646, 79, 1513, 559, 399, 322, 264, 1496, 295, 10688, 830, 382, 731, 13, 400, 370, 294, 257, 12691, 11, 286, 519, 309, 311, 257, 665, 5380, 13, 51108, 51108, 286, 519, 309, 307, 588, 11, 588, 8263, 13, 509, 434, 516, 281, 1813, 1101, 412, 45592, 18161, 9590, 293, 51376, 51376, 1455, 988, 300, 291, 1223, 437, 291, 434, 884, 13, 467, 307, 516, 281, 652, 1203, 4498, 13691, 13, 407, 51592, 51592, 291, 434, 406, 516, 281, 312, 6296, 466, 437, 307, 7633, 1314, 490, 291, 13, 400, 1936, 294, 2674, 11, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.04661999406485722, "compression_ratio": 1.8074534161490683, "no_speech_prob": 4.222638381179422e-06}, {"id": 39, "seek": 18544, "start": 200.32, "end": 205.68, "text": " I think it is very, very valuable. You're going to become better at debugging neural networks and", "tokens": [50364, 4532, 7165, 390, 364, 1476, 664, 6206, 2848, 787, 322, 264, 1496, 295, 2609, 15664, 685, 13, 407, 264, 16871, 645, 50604, 50604, 2167, 2609, 3547, 13, 400, 286, 500, 380, 519, 309, 311, 1547, 13, 400, 286, 1116, 411, 505, 281, 1936, 519, 466, 50852, 50852, 646, 79, 1513, 559, 399, 322, 264, 1496, 295, 10688, 830, 382, 731, 13, 400, 370, 294, 257, 12691, 11, 286, 519, 309, 311, 257, 665, 5380, 13, 51108, 51108, 286, 519, 309, 307, 588, 11, 588, 8263, 13, 509, 434, 516, 281, 1813, 1101, 412, 45592, 18161, 9590, 293, 51376, 51376, 1455, 988, 300, 291, 1223, 437, 291, 434, 884, 13, 467, 307, 516, 281, 652, 1203, 4498, 13691, 13, 407, 51592, 51592, 291, 434, 406, 516, 281, 312, 6296, 466, 437, 307, 7633, 1314, 490, 291, 13, 400, 1936, 294, 2674, 11, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.04661999406485722, "compression_ratio": 1.8074534161490683, "no_speech_prob": 4.222638381179422e-06}, {"id": 40, "seek": 18544, "start": 205.68, "end": 210.0, "text": " making sure that you understand what you're doing. It is going to make everything fully explicit. So", "tokens": [50364, 4532, 7165, 390, 364, 1476, 664, 6206, 2848, 787, 322, 264, 1496, 295, 2609, 15664, 685, 13, 407, 264, 16871, 645, 50604, 50604, 2167, 2609, 3547, 13, 400, 286, 500, 380, 519, 309, 311, 1547, 13, 400, 286, 1116, 411, 505, 281, 1936, 519, 466, 50852, 50852, 646, 79, 1513, 559, 399, 322, 264, 1496, 295, 10688, 830, 382, 731, 13, 400, 370, 294, 257, 12691, 11, 286, 519, 309, 311, 257, 665, 5380, 13, 51108, 51108, 286, 519, 309, 307, 588, 11, 588, 8263, 13, 509, 434, 516, 281, 1813, 1101, 412, 45592, 18161, 9590, 293, 51376, 51376, 1455, 988, 300, 291, 1223, 437, 291, 434, 884, 13, 467, 307, 516, 281, 652, 1203, 4498, 13691, 13, 407, 51592, 51592, 291, 434, 406, 516, 281, 312, 6296, 466, 437, 307, 7633, 1314, 490, 291, 13, 400, 1936, 294, 2674, 11, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.04661999406485722, "compression_ratio": 1.8074534161490683, "no_speech_prob": 4.222638381179422e-06}, {"id": 41, "seek": 18544, "start": 210.0, "end": 214.07999999999998, "text": " you're not going to be nervous about what is hidden away from you. And basically in general,", "tokens": [50364, 4532, 7165, 390, 364, 1476, 664, 6206, 2848, 787, 322, 264, 1496, 295, 2609, 15664, 685, 13, 407, 264, 16871, 645, 50604, 50604, 2167, 2609, 3547, 13, 400, 286, 500, 380, 519, 309, 311, 1547, 13, 400, 286, 1116, 411, 505, 281, 1936, 519, 466, 50852, 50852, 646, 79, 1513, 559, 399, 322, 264, 1496, 295, 10688, 830, 382, 731, 13, 400, 370, 294, 257, 12691, 11, 286, 519, 309, 311, 257, 665, 5380, 13, 51108, 51108, 286, 519, 309, 307, 588, 11, 588, 8263, 13, 509, 434, 516, 281, 1813, 1101, 412, 45592, 18161, 9590, 293, 51376, 51376, 1455, 988, 300, 291, 1223, 437, 291, 434, 884, 13, 467, 307, 516, 281, 652, 1203, 4498, 13691, 13, 407, 51592, 51592, 291, 434, 406, 516, 281, 312, 6296, 466, 437, 307, 7633, 1314, 490, 291, 13, 400, 1936, 294, 2674, 11, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.04661999406485722, "compression_ratio": 1.8074534161490683, "no_speech_prob": 4.222638381179422e-06}, {"id": 42, "seek": 21408, "start": 214.08, "end": 219.68, "text": " we're going to emerge stronger. And so let's get into it. A bit of a fun historical note here", "tokens": [50364, 321, 434, 516, 281, 21511, 7249, 13, 400, 370, 718, 311, 483, 666, 309, 13, 316, 857, 295, 257, 1019, 8584, 3637, 510, 50644, 50644, 307, 300, 965, 3579, 428, 23897, 1320, 538, 1011, 293, 16945, 307, 406, 9628, 293, 572, 472, 775, 309, 50892, 50892, 3993, 337, 264, 9932, 295, 5380, 13, 583, 466, 1266, 924, 2057, 294, 2452, 2539, 11, 341, 390, 6457, 51132, 51132, 3832, 293, 294, 1186, 680, 39211, 13, 407, 412, 264, 565, 1518, 1143, 281, 2464, 641, 1065, 23897, 1320, 51348, 51348, 538, 1011, 16945, 11, 3009, 2059, 11, 293, 309, 311, 445, 437, 291, 576, 360, 13, 407, 321, 1143, 281, 2464, 23897, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.05852290203696803, "compression_ratio": 1.6807017543859648, "no_speech_prob": 6.642726020800183e-06}, {"id": 43, "seek": 21408, "start": 219.68, "end": 224.64000000000001, "text": " is that today writing your backward pass by hand and manually is not recommended and no one does it", "tokens": [50364, 321, 434, 516, 281, 21511, 7249, 13, 400, 370, 718, 311, 483, 666, 309, 13, 316, 857, 295, 257, 1019, 8584, 3637, 510, 50644, 50644, 307, 300, 965, 3579, 428, 23897, 1320, 538, 1011, 293, 16945, 307, 406, 9628, 293, 572, 472, 775, 309, 50892, 50892, 3993, 337, 264, 9932, 295, 5380, 13, 583, 466, 1266, 924, 2057, 294, 2452, 2539, 11, 341, 390, 6457, 51132, 51132, 3832, 293, 294, 1186, 680, 39211, 13, 407, 412, 264, 565, 1518, 1143, 281, 2464, 641, 1065, 23897, 1320, 51348, 51348, 538, 1011, 16945, 11, 3009, 2059, 11, 293, 309, 311, 445, 437, 291, 576, 360, 13, 407, 321, 1143, 281, 2464, 23897, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.05852290203696803, "compression_ratio": 1.6807017543859648, "no_speech_prob": 6.642726020800183e-06}, {"id": 44, "seek": 21408, "start": 224.64000000000001, "end": 229.44, "text": " except for the purposes of exercise. But about 10 years ago in deep learning, this was fairly", "tokens": [50364, 321, 434, 516, 281, 21511, 7249, 13, 400, 370, 718, 311, 483, 666, 309, 13, 316, 857, 295, 257, 1019, 8584, 3637, 510, 50644, 50644, 307, 300, 965, 3579, 428, 23897, 1320, 538, 1011, 293, 16945, 307, 406, 9628, 293, 572, 472, 775, 309, 50892, 50892, 3993, 337, 264, 9932, 295, 5380, 13, 583, 466, 1266, 924, 2057, 294, 2452, 2539, 11, 341, 390, 6457, 51132, 51132, 3832, 293, 294, 1186, 680, 39211, 13, 407, 412, 264, 565, 1518, 1143, 281, 2464, 641, 1065, 23897, 1320, 51348, 51348, 538, 1011, 16945, 11, 3009, 2059, 11, 293, 309, 311, 445, 437, 291, 576, 360, 13, 407, 321, 1143, 281, 2464, 23897, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.05852290203696803, "compression_ratio": 1.6807017543859648, "no_speech_prob": 6.642726020800183e-06}, {"id": 45, "seek": 21408, "start": 229.44, "end": 233.76000000000002, "text": " standard and in fact pervasive. So at the time everyone used to write their own backward pass", "tokens": [50364, 321, 434, 516, 281, 21511, 7249, 13, 400, 370, 718, 311, 483, 666, 309, 13, 316, 857, 295, 257, 1019, 8584, 3637, 510, 50644, 50644, 307, 300, 965, 3579, 428, 23897, 1320, 538, 1011, 293, 16945, 307, 406, 9628, 293, 572, 472, 775, 309, 50892, 50892, 3993, 337, 264, 9932, 295, 5380, 13, 583, 466, 1266, 924, 2057, 294, 2452, 2539, 11, 341, 390, 6457, 51132, 51132, 3832, 293, 294, 1186, 680, 39211, 13, 407, 412, 264, 565, 1518, 1143, 281, 2464, 641, 1065, 23897, 1320, 51348, 51348, 538, 1011, 16945, 11, 3009, 2059, 11, 293, 309, 311, 445, 437, 291, 576, 360, 13, 407, 321, 1143, 281, 2464, 23897, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.05852290203696803, "compression_ratio": 1.6807017543859648, "no_speech_prob": 6.642726020800183e-06}, {"id": 46, "seek": 21408, "start": 233.76000000000002, "end": 238.88000000000002, "text": " by hand manually, including myself, and it's just what you would do. So we used to write backward", "tokens": [50364, 321, 434, 516, 281, 21511, 7249, 13, 400, 370, 718, 311, 483, 666, 309, 13, 316, 857, 295, 257, 1019, 8584, 3637, 510, 50644, 50644, 307, 300, 965, 3579, 428, 23897, 1320, 538, 1011, 293, 16945, 307, 406, 9628, 293, 572, 472, 775, 309, 50892, 50892, 3993, 337, 264, 9932, 295, 5380, 13, 583, 466, 1266, 924, 2057, 294, 2452, 2539, 11, 341, 390, 6457, 51132, 51132, 3832, 293, 294, 1186, 680, 39211, 13, 407, 412, 264, 565, 1518, 1143, 281, 2464, 641, 1065, 23897, 1320, 51348, 51348, 538, 1011, 16945, 11, 3009, 2059, 11, 293, 309, 311, 445, 437, 291, 576, 360, 13, 407, 321, 1143, 281, 2464, 23897, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.05852290203696803, "compression_ratio": 1.6807017543859648, "no_speech_prob": 6.642726020800183e-06}, {"id": 47, "seek": 23888, "start": 238.88, "end": 244.4, "text": " pass by hand and now everyone just called lost that backward. We've lost something. I want to", "tokens": [50364, 1320, 538, 1011, 293, 586, 1518, 445, 1219, 2731, 300, 23897, 13, 492, 600, 2731, 746, 13, 286, 528, 281, 50640, 50640, 976, 291, 257, 1326, 5110, 295, 341, 13, 407, 510, 311, 257, 14062, 3035, 490, 7506, 389, 12442, 293, 20937, 1100, 306, 66, 1756, 5179, 51052, 51052, 294, 3497, 300, 390, 22215, 412, 264, 565, 13, 400, 341, 390, 3097, 512, 6331, 1303, 1219, 51316, 51316, 20608, 37884, 89, 14912, 8379, 13, 400, 1936, 309, 311, 364, 8399, 22660, 19866, 8895, 510, 13, 400, 341, 307, 490, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.14678165476809266, "compression_ratio": 1.5303643724696356, "no_speech_prob": 7.296070634765783e-06}, {"id": 48, "seek": 23888, "start": 244.4, "end": 252.64, "text": " give you a few examples of this. So here's a 2006 paper from Jeff Hinton and Russell Selectonov", "tokens": [50364, 1320, 538, 1011, 293, 586, 1518, 445, 1219, 2731, 300, 23897, 13, 492, 600, 2731, 746, 13, 286, 528, 281, 50640, 50640, 976, 291, 257, 1326, 5110, 295, 341, 13, 407, 510, 311, 257, 14062, 3035, 490, 7506, 389, 12442, 293, 20937, 1100, 306, 66, 1756, 5179, 51052, 51052, 294, 3497, 300, 390, 22215, 412, 264, 565, 13, 400, 341, 390, 3097, 512, 6331, 1303, 1219, 51316, 51316, 20608, 37884, 89, 14912, 8379, 13, 400, 1936, 309, 311, 364, 8399, 22660, 19866, 8895, 510, 13, 400, 341, 307, 490, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.14678165476809266, "compression_ratio": 1.5303643724696356, "no_speech_prob": 7.296070634765783e-06}, {"id": 49, "seek": 23888, "start": 252.64, "end": 257.92, "text": " in science that was influential at the time. And this was training some architectures called", "tokens": [50364, 1320, 538, 1011, 293, 586, 1518, 445, 1219, 2731, 300, 23897, 13, 492, 600, 2731, 746, 13, 286, 528, 281, 50640, 50640, 976, 291, 257, 1326, 5110, 295, 341, 13, 407, 510, 311, 257, 14062, 3035, 490, 7506, 389, 12442, 293, 20937, 1100, 306, 66, 1756, 5179, 51052, 51052, 294, 3497, 300, 390, 22215, 412, 264, 565, 13, 400, 341, 390, 3097, 512, 6331, 1303, 1219, 51316, 51316, 20608, 37884, 89, 14912, 8379, 13, 400, 1936, 309, 311, 364, 8399, 22660, 19866, 8895, 510, 13, 400, 341, 307, 490, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.14678165476809266, "compression_ratio": 1.5303643724696356, "no_speech_prob": 7.296070634765783e-06}, {"id": 50, "seek": 23888, "start": 257.92, "end": 264.24, "text": " restricted Boltzmann machines. And basically it's an autoencoder trained here. And this is from", "tokens": [50364, 1320, 538, 1011, 293, 586, 1518, 445, 1219, 2731, 300, 23897, 13, 492, 600, 2731, 746, 13, 286, 528, 281, 50640, 50640, 976, 291, 257, 1326, 5110, 295, 341, 13, 407, 510, 311, 257, 14062, 3035, 490, 7506, 389, 12442, 293, 20937, 1100, 306, 66, 1756, 5179, 51052, 51052, 294, 3497, 300, 390, 22215, 412, 264, 565, 13, 400, 341, 390, 3097, 512, 6331, 1303, 1219, 51316, 51316, 20608, 37884, 89, 14912, 8379, 13, 400, 1936, 309, 311, 364, 8399, 22660, 19866, 8895, 510, 13, 400, 341, 307, 490, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.14678165476809266, "compression_ratio": 1.5303643724696356, "no_speech_prob": 7.296070634765783e-06}, {"id": 51, "seek": 26424, "start": 264.24, "end": 269.84000000000003, "text": " roughly 2010. I had a library for training restricted Boltzmann machines. And this was at", "tokens": [50364, 9810, 9657, 13, 286, 632, 257, 6405, 337, 3097, 20608, 37884, 89, 14912, 8379, 13, 400, 341, 390, 412, 50644, 50644, 264, 565, 3720, 294, 5904, 11435, 33, 13, 407, 15329, 390, 406, 1143, 337, 2452, 2539, 680, 7967, 3413, 13, 467, 390, 439, 5904, 11435, 33, 13, 50896, 50896, 400, 5904, 11435, 33, 390, 341, 8134, 15866, 7372, 300, 1518, 576, 764, 13, 407, 321, 576, 2464, 5904, 11435, 33, 11, 51208, 51208, 597, 307, 10268, 257, 9410, 2856, 382, 731, 11, 457, 309, 632, 257, 588, 10851, 40863, 1508, 13, 400, 51500, 51500, 309, 390, 341, 15866, 2823, 293, 291, 576, 1190, 510, 13, 467, 576, 439, 1190, 322, 264, 13199, 11, 295, 1164, 11, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.09139688349952382, "compression_ratio": 1.654109589041096, "no_speech_prob": 1.1124895536340773e-05}, {"id": 52, "seek": 26424, "start": 269.84000000000003, "end": 274.88, "text": " the time written in MATLAB. So Python was not used for deep learning pervasively. It was all MATLAB.", "tokens": [50364, 9810, 9657, 13, 286, 632, 257, 6405, 337, 3097, 20608, 37884, 89, 14912, 8379, 13, 400, 341, 390, 412, 50644, 50644, 264, 565, 3720, 294, 5904, 11435, 33, 13, 407, 15329, 390, 406, 1143, 337, 2452, 2539, 680, 7967, 3413, 13, 467, 390, 439, 5904, 11435, 33, 13, 50896, 50896, 400, 5904, 11435, 33, 390, 341, 8134, 15866, 7372, 300, 1518, 576, 764, 13, 407, 321, 576, 2464, 5904, 11435, 33, 11, 51208, 51208, 597, 307, 10268, 257, 9410, 2856, 382, 731, 11, 457, 309, 632, 257, 588, 10851, 40863, 1508, 13, 400, 51500, 51500, 309, 390, 341, 15866, 2823, 293, 291, 576, 1190, 510, 13, 467, 576, 439, 1190, 322, 264, 13199, 11, 295, 1164, 11, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.09139688349952382, "compression_ratio": 1.654109589041096, "no_speech_prob": 1.1124895536340773e-05}, {"id": 53, "seek": 26424, "start": 274.88, "end": 281.12, "text": " And MATLAB was this scientific computing package that everyone would use. So we would write MATLAB,", "tokens": [50364, 9810, 9657, 13, 286, 632, 257, 6405, 337, 3097, 20608, 37884, 89, 14912, 8379, 13, 400, 341, 390, 412, 50644, 50644, 264, 565, 3720, 294, 5904, 11435, 33, 13, 407, 15329, 390, 406, 1143, 337, 2452, 2539, 680, 7967, 3413, 13, 467, 390, 439, 5904, 11435, 33, 13, 50896, 50896, 400, 5904, 11435, 33, 390, 341, 8134, 15866, 7372, 300, 1518, 576, 764, 13, 407, 321, 576, 2464, 5904, 11435, 33, 11, 51208, 51208, 597, 307, 10268, 257, 9410, 2856, 382, 731, 11, 457, 309, 632, 257, 588, 10851, 40863, 1508, 13, 400, 51500, 51500, 309, 390, 341, 15866, 2823, 293, 291, 576, 1190, 510, 13, 467, 576, 439, 1190, 322, 264, 13199, 11, 295, 1164, 11, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.09139688349952382, "compression_ratio": 1.654109589041096, "no_speech_prob": 1.1124895536340773e-05}, {"id": 54, "seek": 26424, "start": 281.12, "end": 286.96000000000004, "text": " which is barely a programming language as well, but it had a very convenient tensor class. And", "tokens": [50364, 9810, 9657, 13, 286, 632, 257, 6405, 337, 3097, 20608, 37884, 89, 14912, 8379, 13, 400, 341, 390, 412, 50644, 50644, 264, 565, 3720, 294, 5904, 11435, 33, 13, 407, 15329, 390, 406, 1143, 337, 2452, 2539, 680, 7967, 3413, 13, 467, 390, 439, 5904, 11435, 33, 13, 50896, 50896, 400, 5904, 11435, 33, 390, 341, 8134, 15866, 7372, 300, 1518, 576, 764, 13, 407, 321, 576, 2464, 5904, 11435, 33, 11, 51208, 51208, 597, 307, 10268, 257, 9410, 2856, 382, 731, 11, 457, 309, 632, 257, 588, 10851, 40863, 1508, 13, 400, 51500, 51500, 309, 390, 341, 15866, 2823, 293, 291, 576, 1190, 510, 13, 467, 576, 439, 1190, 322, 264, 13199, 11, 295, 1164, 11, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.09139688349952382, "compression_ratio": 1.654109589041096, "no_speech_prob": 1.1124895536340773e-05}, {"id": 55, "seek": 26424, "start": 286.96000000000004, "end": 291.12, "text": " it was this computing environment and you would run here. It would all run on the CPU, of course,", "tokens": [50364, 9810, 9657, 13, 286, 632, 257, 6405, 337, 3097, 20608, 37884, 89, 14912, 8379, 13, 400, 341, 390, 412, 50644, 50644, 264, 565, 3720, 294, 5904, 11435, 33, 13, 407, 15329, 390, 406, 1143, 337, 2452, 2539, 680, 7967, 3413, 13, 467, 390, 439, 5904, 11435, 33, 13, 50896, 50896, 400, 5904, 11435, 33, 390, 341, 8134, 15866, 7372, 300, 1518, 576, 764, 13, 407, 321, 576, 2464, 5904, 11435, 33, 11, 51208, 51208, 597, 307, 10268, 257, 9410, 2856, 382, 731, 11, 457, 309, 632, 257, 588, 10851, 40863, 1508, 13, 400, 51500, 51500, 309, 390, 341, 15866, 2823, 293, 291, 576, 1190, 510, 13, 467, 576, 439, 1190, 322, 264, 13199, 11, 295, 1164, 11, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.09139688349952382, "compression_ratio": 1.654109589041096, "no_speech_prob": 1.1124895536340773e-05}, {"id": 56, "seek": 29112, "start": 291.12, "end": 295.2, "text": " but you would have very nice plots to go with it and a built-in debugger. And it was pretty nice.", "tokens": [50364, 457, 291, 576, 362, 588, 1481, 28609, 281, 352, 365, 309, 293, 257, 3094, 12, 259, 24083, 1321, 13, 400, 309, 390, 1238, 1481, 13, 50568, 50600, 823, 264, 3089, 294, 341, 7372, 294, 9657, 300, 286, 4114, 337, 15669, 20608, 37884, 89, 14912, 8379, 50888, 50928, 281, 257, 2416, 8396, 307, 40757, 11, 457, 286, 1415, 281, 855, 291, 577, 291, 576, 13, 51116, 51116, 1042, 11, 286, 478, 4084, 264, 1412, 293, 264, 48826, 15245, 279, 13, 286, 478, 5883, 3319, 264, 18161, 5393, 13, 51324, 51372, 407, 309, 311, 658, 17443, 293, 32152, 445, 411, 321, 434, 1143, 281, 13, 400, 550, 341, 307, 264, 3097, 6367, 51560, 51588, 689, 321, 767, 360, 264, 2128, 1320, 13, 400, 550, 510, 412, 341, 565, 11, 436, 994, 380, 754, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.08624712626139323, "compression_ratio": 1.6199376947040498, "no_speech_prob": 3.7264087495714193e-06}, {"id": 57, "seek": 29112, "start": 295.84000000000003, "end": 301.6, "text": " Now the code in this package in 2010 that I wrote for fitting restricted Boltzmann machines", "tokens": [50364, 457, 291, 576, 362, 588, 1481, 28609, 281, 352, 365, 309, 293, 257, 3094, 12, 259, 24083, 1321, 13, 400, 309, 390, 1238, 1481, 13, 50568, 50600, 823, 264, 3089, 294, 341, 7372, 294, 9657, 300, 286, 4114, 337, 15669, 20608, 37884, 89, 14912, 8379, 50888, 50928, 281, 257, 2416, 8396, 307, 40757, 11, 457, 286, 1415, 281, 855, 291, 577, 291, 576, 13, 51116, 51116, 1042, 11, 286, 478, 4084, 264, 1412, 293, 264, 48826, 15245, 279, 13, 286, 478, 5883, 3319, 264, 18161, 5393, 13, 51324, 51372, 407, 309, 311, 658, 17443, 293, 32152, 445, 411, 321, 434, 1143, 281, 13, 400, 550, 341, 307, 264, 3097, 6367, 51560, 51588, 689, 321, 767, 360, 264, 2128, 1320, 13, 400, 550, 510, 412, 341, 565, 11, 436, 994, 380, 754, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.08624712626139323, "compression_ratio": 1.6199376947040498, "no_speech_prob": 3.7264087495714193e-06}, {"id": 58, "seek": 29112, "start": 302.4, "end": 306.16, "text": " to a large extent is recognizable, but I wanted to show you how you would.", "tokens": [50364, 457, 291, 576, 362, 588, 1481, 28609, 281, 352, 365, 309, 293, 257, 3094, 12, 259, 24083, 1321, 13, 400, 309, 390, 1238, 1481, 13, 50568, 50600, 823, 264, 3089, 294, 341, 7372, 294, 9657, 300, 286, 4114, 337, 15669, 20608, 37884, 89, 14912, 8379, 50888, 50928, 281, 257, 2416, 8396, 307, 40757, 11, 457, 286, 1415, 281, 855, 291, 577, 291, 576, 13, 51116, 51116, 1042, 11, 286, 478, 4084, 264, 1412, 293, 264, 48826, 15245, 279, 13, 286, 478, 5883, 3319, 264, 18161, 5393, 13, 51324, 51372, 407, 309, 311, 658, 17443, 293, 32152, 445, 411, 321, 434, 1143, 281, 13, 400, 550, 341, 307, 264, 3097, 6367, 51560, 51588, 689, 321, 767, 360, 264, 2128, 1320, 13, 400, 550, 510, 412, 341, 565, 11, 436, 994, 380, 754, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.08624712626139323, "compression_ratio": 1.6199376947040498, "no_speech_prob": 3.7264087495714193e-06}, {"id": 59, "seek": 29112, "start": 306.16, "end": 310.32, "text": " Well, I'm creating the data and the XY batches. I'm initializing the neural nut.", "tokens": [50364, 457, 291, 576, 362, 588, 1481, 28609, 281, 352, 365, 309, 293, 257, 3094, 12, 259, 24083, 1321, 13, 400, 309, 390, 1238, 1481, 13, 50568, 50600, 823, 264, 3089, 294, 341, 7372, 294, 9657, 300, 286, 4114, 337, 15669, 20608, 37884, 89, 14912, 8379, 50888, 50928, 281, 257, 2416, 8396, 307, 40757, 11, 457, 286, 1415, 281, 855, 291, 577, 291, 576, 13, 51116, 51116, 1042, 11, 286, 478, 4084, 264, 1412, 293, 264, 48826, 15245, 279, 13, 286, 478, 5883, 3319, 264, 18161, 5393, 13, 51324, 51372, 407, 309, 311, 658, 17443, 293, 32152, 445, 411, 321, 434, 1143, 281, 13, 400, 550, 341, 307, 264, 3097, 6367, 51560, 51588, 689, 321, 767, 360, 264, 2128, 1320, 13, 400, 550, 510, 412, 341, 565, 11, 436, 994, 380, 754, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.08624712626139323, "compression_ratio": 1.6199376947040498, "no_speech_prob": 3.7264087495714193e-06}, {"id": 60, "seek": 29112, "start": 311.28000000000003, "end": 315.04, "text": " So it's got weights and biases just like we're used to. And then this is the training loop", "tokens": [50364, 457, 291, 576, 362, 588, 1481, 28609, 281, 352, 365, 309, 293, 257, 3094, 12, 259, 24083, 1321, 13, 400, 309, 390, 1238, 1481, 13, 50568, 50600, 823, 264, 3089, 294, 341, 7372, 294, 9657, 300, 286, 4114, 337, 15669, 20608, 37884, 89, 14912, 8379, 50888, 50928, 281, 257, 2416, 8396, 307, 40757, 11, 457, 286, 1415, 281, 855, 291, 577, 291, 576, 13, 51116, 51116, 1042, 11, 286, 478, 4084, 264, 1412, 293, 264, 48826, 15245, 279, 13, 286, 478, 5883, 3319, 264, 18161, 5393, 13, 51324, 51372, 407, 309, 311, 658, 17443, 293, 32152, 445, 411, 321, 434, 1143, 281, 13, 400, 550, 341, 307, 264, 3097, 6367, 51560, 51588, 689, 321, 767, 360, 264, 2128, 1320, 13, 400, 550, 510, 412, 341, 565, 11, 436, 994, 380, 754, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.08624712626139323, "compression_ratio": 1.6199376947040498, "no_speech_prob": 3.7264087495714193e-06}, {"id": 61, "seek": 29112, "start": 315.6, "end": 320.0, "text": " where we actually do the forward pass. And then here at this time, they didn't even", "tokens": [50364, 457, 291, 576, 362, 588, 1481, 28609, 281, 352, 365, 309, 293, 257, 3094, 12, 259, 24083, 1321, 13, 400, 309, 390, 1238, 1481, 13, 50568, 50600, 823, 264, 3089, 294, 341, 7372, 294, 9657, 300, 286, 4114, 337, 15669, 20608, 37884, 89, 14912, 8379, 50888, 50928, 281, 257, 2416, 8396, 307, 40757, 11, 457, 286, 1415, 281, 855, 291, 577, 291, 576, 13, 51116, 51116, 1042, 11, 286, 478, 4084, 264, 1412, 293, 264, 48826, 15245, 279, 13, 286, 478, 5883, 3319, 264, 18161, 5393, 13, 51324, 51372, 407, 309, 311, 658, 17443, 293, 32152, 445, 411, 321, 434, 1143, 281, 13, 400, 550, 341, 307, 264, 3097, 6367, 51560, 51588, 689, 321, 767, 360, 264, 2128, 1320, 13, 400, 550, 510, 412, 341, 565, 11, 436, 994, 380, 754, 51808, 51808], "temperature": 0.0, "avg_logprob": -0.08624712626139323, "compression_ratio": 1.6199376947040498, "no_speech_prob": 3.7264087495714193e-06}, {"id": 62, "seek": 32000, "start": 320.0, "end": 324.56, "text": " necessarily use back propagation to train neural networks. So this in particular implements", "tokens": [50364, 4725, 764, 646, 38377, 281, 3847, 18161, 9590, 13, 407, 341, 294, 1729, 704, 17988, 50592, 50592, 8712, 292, 47387, 11, 597, 20561, 257, 16235, 13, 400, 550, 510, 321, 747, 300, 16235, 293, 764, 309, 337, 50916, 50916, 257, 13075, 5623, 2051, 264, 3876, 300, 321, 434, 1143, 281, 13, 865, 11, 510, 13, 583, 291, 393, 536, 300, 1936, 51248, 51248, 561, 366, 1205, 35543, 365, 613, 2771, 2448, 3838, 293, 294, 1889, 293, 2969, 13, 467, 2067, 380, 300, 2689, 51484, 51484, 281, 764, 364, 1476, 664, 6206, 2848, 13, 1692, 311, 472, 544, 1365, 490, 257, 3035, 295, 3892, 490, 8227, 1219, 264, 26424, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.09822532960346766, "compression_ratio": 1.607843137254902, "no_speech_prob": 1.9033504941035062e-06}, {"id": 63, "seek": 32000, "start": 324.56, "end": 331.04, "text": " contrasted divergence, which estimates a gradient. And then here we take that gradient and use it for", "tokens": [50364, 4725, 764, 646, 38377, 281, 3847, 18161, 9590, 13, 407, 341, 294, 1729, 704, 17988, 50592, 50592, 8712, 292, 47387, 11, 597, 20561, 257, 16235, 13, 400, 550, 510, 321, 747, 300, 16235, 293, 764, 309, 337, 50916, 50916, 257, 13075, 5623, 2051, 264, 3876, 300, 321, 434, 1143, 281, 13, 865, 11, 510, 13, 583, 291, 393, 536, 300, 1936, 51248, 51248, 561, 366, 1205, 35543, 365, 613, 2771, 2448, 3838, 293, 294, 1889, 293, 2969, 13, 467, 2067, 380, 300, 2689, 51484, 51484, 281, 764, 364, 1476, 664, 6206, 2848, 13, 1692, 311, 472, 544, 1365, 490, 257, 3035, 295, 3892, 490, 8227, 1219, 264, 26424, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.09822532960346766, "compression_ratio": 1.607843137254902, "no_speech_prob": 1.9033504941035062e-06}, {"id": 64, "seek": 32000, "start": 331.04, "end": 337.68, "text": " a parameter update along the lines that we're used to. Yeah, here. But you can see that basically", "tokens": [50364, 4725, 764, 646, 38377, 281, 3847, 18161, 9590, 13, 407, 341, 294, 1729, 704, 17988, 50592, 50592, 8712, 292, 47387, 11, 597, 20561, 257, 16235, 13, 400, 550, 510, 321, 747, 300, 16235, 293, 764, 309, 337, 50916, 50916, 257, 13075, 5623, 2051, 264, 3876, 300, 321, 434, 1143, 281, 13, 865, 11, 510, 13, 583, 291, 393, 536, 300, 1936, 51248, 51248, 561, 366, 1205, 35543, 365, 613, 2771, 2448, 3838, 293, 294, 1889, 293, 2969, 13, 467, 2067, 380, 300, 2689, 51484, 51484, 281, 764, 364, 1476, 664, 6206, 2848, 13, 1692, 311, 472, 544, 1365, 490, 257, 3035, 295, 3892, 490, 8227, 1219, 264, 26424, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.09822532960346766, "compression_ratio": 1.607843137254902, "no_speech_prob": 1.9033504941035062e-06}, {"id": 65, "seek": 32000, "start": 337.68, "end": 342.4, "text": " people are meddling with these gradients directly and inline and themselves. It wasn't that common", "tokens": [50364, 4725, 764, 646, 38377, 281, 3847, 18161, 9590, 13, 407, 341, 294, 1729, 704, 17988, 50592, 50592, 8712, 292, 47387, 11, 597, 20561, 257, 16235, 13, 400, 550, 510, 321, 747, 300, 16235, 293, 764, 309, 337, 50916, 50916, 257, 13075, 5623, 2051, 264, 3876, 300, 321, 434, 1143, 281, 13, 865, 11, 510, 13, 583, 291, 393, 536, 300, 1936, 51248, 51248, 561, 366, 1205, 35543, 365, 613, 2771, 2448, 3838, 293, 294, 1889, 293, 2969, 13, 467, 2067, 380, 300, 2689, 51484, 51484, 281, 764, 364, 1476, 664, 6206, 2848, 13, 1692, 311, 472, 544, 1365, 490, 257, 3035, 295, 3892, 490, 8227, 1219, 264, 26424, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.09822532960346766, "compression_ratio": 1.607843137254902, "no_speech_prob": 1.9033504941035062e-06}, {"id": 66, "seek": 32000, "start": 342.4, "end": 348.8, "text": " to use an autograd engine. Here's one more example from a paper of mine from 2014 called the fragment", "tokens": [50364, 4725, 764, 646, 38377, 281, 3847, 18161, 9590, 13, 407, 341, 294, 1729, 704, 17988, 50592, 50592, 8712, 292, 47387, 11, 597, 20561, 257, 16235, 13, 400, 550, 510, 321, 747, 300, 16235, 293, 764, 309, 337, 50916, 50916, 257, 13075, 5623, 2051, 264, 3876, 300, 321, 434, 1143, 281, 13, 865, 11, 510, 13, 583, 291, 393, 536, 300, 1936, 51248, 51248, 561, 366, 1205, 35543, 365, 613, 2771, 2448, 3838, 293, 294, 1889, 293, 2969, 13, 467, 2067, 380, 300, 2689, 51484, 51484, 281, 764, 364, 1476, 664, 6206, 2848, 13, 1692, 311, 472, 544, 1365, 490, 257, 3035, 295, 3892, 490, 8227, 1219, 264, 26424, 51804, 51804], "temperature": 0.0, "avg_logprob": -0.09822532960346766, "compression_ratio": 1.607843137254902, "no_speech_prob": 1.9033504941035062e-06}, {"id": 67, "seek": 34880, "start": 348.8, "end": 354.48, "text": " embeddings. And here what I was doing is I was aligning images and text. And so it's kind of like", "tokens": [50364, 12240, 29432, 13, 400, 510, 437, 286, 390, 884, 307, 286, 390, 419, 9676, 5267, 293, 2487, 13, 400, 370, 309, 311, 733, 295, 411, 50648, 50648, 257, 7353, 498, 291, 434, 4963, 365, 309, 11, 457, 2602, 295, 1364, 322, 264, 1496, 295, 2302, 5267, 293, 2302, 50860, 50860, 16579, 11, 309, 390, 1364, 322, 264, 1496, 295, 2609, 6565, 293, 707, 3755, 295, 16579, 13, 51080, 51080, 400, 286, 390, 12240, 3584, 552, 293, 550, 28258, 257, 588, 709, 411, 257, 7353, 411, 4470, 13, 400, 286, 22954, 493, 264, 51320, 51320, 3089, 490, 8227, 295, 577, 286, 12270, 341, 13, 400, 309, 390, 1217, 294, 22592, 47, 88, 293, 15329, 13, 400, 510, 286, 478, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.09666065309868484, "compression_ratio": 1.8052434456928839, "no_speech_prob": 4.86025828649872e-06}, {"id": 68, "seek": 34880, "start": 354.48, "end": 358.72, "text": " a clip if you're familiar with it, but instead of working on the level of entire images and entire", "tokens": [50364, 12240, 29432, 13, 400, 510, 437, 286, 390, 884, 307, 286, 390, 419, 9676, 5267, 293, 2487, 13, 400, 370, 309, 311, 733, 295, 411, 50648, 50648, 257, 7353, 498, 291, 434, 4963, 365, 309, 11, 457, 2602, 295, 1364, 322, 264, 1496, 295, 2302, 5267, 293, 2302, 50860, 50860, 16579, 11, 309, 390, 1364, 322, 264, 1496, 295, 2609, 6565, 293, 707, 3755, 295, 16579, 13, 51080, 51080, 400, 286, 390, 12240, 3584, 552, 293, 550, 28258, 257, 588, 709, 411, 257, 7353, 411, 4470, 13, 400, 286, 22954, 493, 264, 51320, 51320, 3089, 490, 8227, 295, 577, 286, 12270, 341, 13, 400, 309, 390, 1217, 294, 22592, 47, 88, 293, 15329, 13, 400, 510, 286, 478, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.09666065309868484, "compression_ratio": 1.8052434456928839, "no_speech_prob": 4.86025828649872e-06}, {"id": 69, "seek": 34880, "start": 358.72, "end": 363.12, "text": " sentences, it was working on the level of individual objects and little pieces of sentences.", "tokens": [50364, 12240, 29432, 13, 400, 510, 437, 286, 390, 884, 307, 286, 390, 419, 9676, 5267, 293, 2487, 13, 400, 370, 309, 311, 733, 295, 411, 50648, 50648, 257, 7353, 498, 291, 434, 4963, 365, 309, 11, 457, 2602, 295, 1364, 322, 264, 1496, 295, 2302, 5267, 293, 2302, 50860, 50860, 16579, 11, 309, 390, 1364, 322, 264, 1496, 295, 2609, 6565, 293, 707, 3755, 295, 16579, 13, 51080, 51080, 400, 286, 390, 12240, 3584, 552, 293, 550, 28258, 257, 588, 709, 411, 257, 7353, 411, 4470, 13, 400, 286, 22954, 493, 264, 51320, 51320, 3089, 490, 8227, 295, 577, 286, 12270, 341, 13, 400, 309, 390, 1217, 294, 22592, 47, 88, 293, 15329, 13, 400, 510, 286, 478, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.09666065309868484, "compression_ratio": 1.8052434456928839, "no_speech_prob": 4.86025828649872e-06}, {"id": 70, "seek": 34880, "start": 363.12, "end": 367.92, "text": " And I was embedding them and then calculating a very much like a clip like loss. And I dug up the", "tokens": [50364, 12240, 29432, 13, 400, 510, 437, 286, 390, 884, 307, 286, 390, 419, 9676, 5267, 293, 2487, 13, 400, 370, 309, 311, 733, 295, 411, 50648, 50648, 257, 7353, 498, 291, 434, 4963, 365, 309, 11, 457, 2602, 295, 1364, 322, 264, 1496, 295, 2302, 5267, 293, 2302, 50860, 50860, 16579, 11, 309, 390, 1364, 322, 264, 1496, 295, 2609, 6565, 293, 707, 3755, 295, 16579, 13, 51080, 51080, 400, 286, 390, 12240, 3584, 552, 293, 550, 28258, 257, 588, 709, 411, 257, 7353, 411, 4470, 13, 400, 286, 22954, 493, 264, 51320, 51320, 3089, 490, 8227, 295, 577, 286, 12270, 341, 13, 400, 309, 390, 1217, 294, 22592, 47, 88, 293, 15329, 13, 400, 510, 286, 478, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.09666065309868484, "compression_ratio": 1.8052434456928839, "no_speech_prob": 4.86025828649872e-06}, {"id": 71, "seek": 34880, "start": 367.92, "end": 375.2, "text": " code from 2014 of how I implemented this. And it was already in NumPy and Python. And here I'm", "tokens": [50364, 12240, 29432, 13, 400, 510, 437, 286, 390, 884, 307, 286, 390, 419, 9676, 5267, 293, 2487, 13, 400, 370, 309, 311, 733, 295, 411, 50648, 50648, 257, 7353, 498, 291, 434, 4963, 365, 309, 11, 457, 2602, 295, 1364, 322, 264, 1496, 295, 2302, 5267, 293, 2302, 50860, 50860, 16579, 11, 309, 390, 1364, 322, 264, 1496, 295, 2609, 6565, 293, 707, 3755, 295, 16579, 13, 51080, 51080, 400, 286, 390, 12240, 3584, 552, 293, 550, 28258, 257, 588, 709, 411, 257, 7353, 411, 4470, 13, 400, 286, 22954, 493, 264, 51320, 51320, 3089, 490, 8227, 295, 577, 286, 12270, 341, 13, 400, 309, 390, 1217, 294, 22592, 47, 88, 293, 15329, 13, 400, 510, 286, 478, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.09666065309868484, "compression_ratio": 1.8052434456928839, "no_speech_prob": 4.86025828649872e-06}, {"id": 72, "seek": 37520, "start": 375.2, "end": 380.15999999999997, "text": " implementing the cost function and it was standards to implement not just the cost, but also the", "tokens": [50364, 18114, 264, 2063, 2445, 293, 309, 390, 7787, 281, 4445, 406, 445, 264, 2063, 11, 457, 611, 264, 50612, 50612, 23897, 1320, 16945, 13, 407, 510, 286, 478, 28258, 264, 3256, 12240, 29432, 11, 8174, 12240, 29432, 11, 50888, 50888, 264, 4470, 2445, 13, 286, 8873, 264, 13444, 13, 639, 307, 264, 4470, 2445, 13, 400, 550, 1564, 286, 362, 264, 51172, 51172, 4470, 2445, 11, 286, 360, 264, 23897, 1320, 558, 510, 13, 407, 286, 23897, 807, 264, 4470, 2445, 51412, 51412, 293, 807, 264, 18161, 2533, 293, 286, 34116, 3890, 2144, 13, 407, 1203, 390, 1096, 538, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07868909372866732, "compression_ratio": 1.9319148936170212, "no_speech_prob": 4.936880031891633e-06}, {"id": 73, "seek": 37520, "start": 380.15999999999997, "end": 385.68, "text": " backward pass manually. So here I'm calculating the image embeddings, sentence embeddings,", "tokens": [50364, 18114, 264, 2063, 2445, 293, 309, 390, 7787, 281, 4445, 406, 445, 264, 2063, 11, 457, 611, 264, 50612, 50612, 23897, 1320, 16945, 13, 407, 510, 286, 478, 28258, 264, 3256, 12240, 29432, 11, 8174, 12240, 29432, 11, 50888, 50888, 264, 4470, 2445, 13, 286, 8873, 264, 13444, 13, 639, 307, 264, 4470, 2445, 13, 400, 550, 1564, 286, 362, 264, 51172, 51172, 4470, 2445, 11, 286, 360, 264, 23897, 1320, 558, 510, 13, 407, 286, 23897, 807, 264, 4470, 2445, 51412, 51412, 293, 807, 264, 18161, 2533, 293, 286, 34116, 3890, 2144, 13, 407, 1203, 390, 1096, 538, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07868909372866732, "compression_ratio": 1.9319148936170212, "no_speech_prob": 4.936880031891633e-06}, {"id": 74, "seek": 37520, "start": 385.68, "end": 391.36, "text": " the loss function. I calculate the scores. This is the loss function. And then once I have the", "tokens": [50364, 18114, 264, 2063, 2445, 293, 309, 390, 7787, 281, 4445, 406, 445, 264, 2063, 11, 457, 611, 264, 50612, 50612, 23897, 1320, 16945, 13, 407, 510, 286, 478, 28258, 264, 3256, 12240, 29432, 11, 8174, 12240, 29432, 11, 50888, 50888, 264, 4470, 2445, 13, 286, 8873, 264, 13444, 13, 639, 307, 264, 4470, 2445, 13, 400, 550, 1564, 286, 362, 264, 51172, 51172, 4470, 2445, 11, 286, 360, 264, 23897, 1320, 558, 510, 13, 407, 286, 23897, 807, 264, 4470, 2445, 51412, 51412, 293, 807, 264, 18161, 2533, 293, 286, 34116, 3890, 2144, 13, 407, 1203, 390, 1096, 538, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07868909372866732, "compression_ratio": 1.9319148936170212, "no_speech_prob": 4.936880031891633e-06}, {"id": 75, "seek": 37520, "start": 391.36, "end": 396.15999999999997, "text": " loss function, I do the backward pass right here. So I backward through the loss function", "tokens": [50364, 18114, 264, 2063, 2445, 293, 309, 390, 7787, 281, 4445, 406, 445, 264, 2063, 11, 457, 611, 264, 50612, 50612, 23897, 1320, 16945, 13, 407, 510, 286, 478, 28258, 264, 3256, 12240, 29432, 11, 8174, 12240, 29432, 11, 50888, 50888, 264, 4470, 2445, 13, 286, 8873, 264, 13444, 13, 639, 307, 264, 4470, 2445, 13, 400, 550, 1564, 286, 362, 264, 51172, 51172, 4470, 2445, 11, 286, 360, 264, 23897, 1320, 558, 510, 13, 407, 286, 23897, 807, 264, 4470, 2445, 51412, 51412, 293, 807, 264, 18161, 2533, 293, 286, 34116, 3890, 2144, 13, 407, 1203, 390, 1096, 538, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07868909372866732, "compression_ratio": 1.9319148936170212, "no_speech_prob": 4.936880031891633e-06}, {"id": 76, "seek": 37520, "start": 396.15999999999997, "end": 401.2, "text": " and through the neural net and I append regularization. So everything was done by", "tokens": [50364, 18114, 264, 2063, 2445, 293, 309, 390, 7787, 281, 4445, 406, 445, 264, 2063, 11, 457, 611, 264, 50612, 50612, 23897, 1320, 16945, 13, 407, 510, 286, 478, 28258, 264, 3256, 12240, 29432, 11, 8174, 12240, 29432, 11, 50888, 50888, 264, 4470, 2445, 13, 286, 8873, 264, 13444, 13, 639, 307, 264, 4470, 2445, 13, 400, 550, 1564, 286, 362, 264, 51172, 51172, 4470, 2445, 11, 286, 360, 264, 23897, 1320, 558, 510, 13, 407, 286, 23897, 807, 264, 4470, 2445, 51412, 51412, 293, 807, 264, 18161, 2533, 293, 286, 34116, 3890, 2144, 13, 407, 1203, 390, 1096, 538, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.07868909372866732, "compression_ratio": 1.9319148936170212, "no_speech_prob": 4.936880031891633e-06}, {"id": 77, "seek": 40120, "start": 401.2, "end": 405.12, "text": " hand manually. And you will just write out the backward pass and then you would use a gradient", "tokens": [50364, 1011, 16945, 13, 400, 291, 486, 445, 2464, 484, 264, 23897, 1320, 293, 550, 291, 576, 764, 257, 16235, 50560, 50560, 1520, 260, 281, 652, 988, 300, 428, 29054, 12539, 295, 264, 16235, 26383, 365, 264, 472, 291, 15598, 50772, 50772, 1830, 646, 38377, 13, 407, 341, 390, 588, 3832, 337, 257, 938, 565, 11, 457, 965, 295, 1164, 309, 307, 3832, 51020, 51020, 281, 764, 364, 14295, 2848, 13, 583, 309, 390, 2138, 4420, 13, 400, 286, 519, 561, 1333, 295, 7320, 577, 51288, 51288, 613, 18161, 9590, 589, 322, 257, 588, 21769, 1496, 13, 400, 370, 286, 519, 309, 311, 257, 665, 5380, 797, 11, 51496, 51496, 293, 341, 307, 689, 321, 528, 281, 312, 13, 1033, 13, 407, 445, 382, 257, 13548, 490, 527, 3894, 7991, 11, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.08906382233349246, "compression_ratio": 1.6997084548104957, "no_speech_prob": 1.384509596391581e-05}, {"id": 78, "seek": 40120, "start": 405.12, "end": 409.36, "text": " checker to make sure that your numerical estimate of the gradient agrees with the one you calculated", "tokens": [50364, 1011, 16945, 13, 400, 291, 486, 445, 2464, 484, 264, 23897, 1320, 293, 550, 291, 576, 764, 257, 16235, 50560, 50560, 1520, 260, 281, 652, 988, 300, 428, 29054, 12539, 295, 264, 16235, 26383, 365, 264, 472, 291, 15598, 50772, 50772, 1830, 646, 38377, 13, 407, 341, 390, 588, 3832, 337, 257, 938, 565, 11, 457, 965, 295, 1164, 309, 307, 3832, 51020, 51020, 281, 764, 364, 14295, 2848, 13, 583, 309, 390, 2138, 4420, 13, 400, 286, 519, 561, 1333, 295, 7320, 577, 51288, 51288, 613, 18161, 9590, 589, 322, 257, 588, 21769, 1496, 13, 400, 370, 286, 519, 309, 311, 257, 665, 5380, 797, 11, 51496, 51496, 293, 341, 307, 689, 321, 528, 281, 312, 13, 1033, 13, 407, 445, 382, 257, 13548, 490, 527, 3894, 7991, 11, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.08906382233349246, "compression_ratio": 1.6997084548104957, "no_speech_prob": 1.384509596391581e-05}, {"id": 79, "seek": 40120, "start": 409.36, "end": 414.32, "text": " during back propagation. So this was very standard for a long time, but today of course it is standard", "tokens": [50364, 1011, 16945, 13, 400, 291, 486, 445, 2464, 484, 264, 23897, 1320, 293, 550, 291, 576, 764, 257, 16235, 50560, 50560, 1520, 260, 281, 652, 988, 300, 428, 29054, 12539, 295, 264, 16235, 26383, 365, 264, 472, 291, 15598, 50772, 50772, 1830, 646, 38377, 13, 407, 341, 390, 588, 3832, 337, 257, 938, 565, 11, 457, 965, 295, 1164, 309, 307, 3832, 51020, 51020, 281, 764, 364, 14295, 2848, 13, 583, 309, 390, 2138, 4420, 13, 400, 286, 519, 561, 1333, 295, 7320, 577, 51288, 51288, 613, 18161, 9590, 589, 322, 257, 588, 21769, 1496, 13, 400, 370, 286, 519, 309, 311, 257, 665, 5380, 797, 11, 51496, 51496, 293, 341, 307, 689, 321, 528, 281, 312, 13, 1033, 13, 407, 445, 382, 257, 13548, 490, 527, 3894, 7991, 11, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.08906382233349246, "compression_ratio": 1.6997084548104957, "no_speech_prob": 1.384509596391581e-05}, {"id": 80, "seek": 40120, "start": 414.32, "end": 419.68, "text": " to use an undergrad engine. But it was definitely useful. And I think people sort of understood how", "tokens": [50364, 1011, 16945, 13, 400, 291, 486, 445, 2464, 484, 264, 23897, 1320, 293, 550, 291, 576, 764, 257, 16235, 50560, 50560, 1520, 260, 281, 652, 988, 300, 428, 29054, 12539, 295, 264, 16235, 26383, 365, 264, 472, 291, 15598, 50772, 50772, 1830, 646, 38377, 13, 407, 341, 390, 588, 3832, 337, 257, 938, 565, 11, 457, 965, 295, 1164, 309, 307, 3832, 51020, 51020, 281, 764, 364, 14295, 2848, 13, 583, 309, 390, 2138, 4420, 13, 400, 286, 519, 561, 1333, 295, 7320, 577, 51288, 51288, 613, 18161, 9590, 589, 322, 257, 588, 21769, 1496, 13, 400, 370, 286, 519, 309, 311, 257, 665, 5380, 797, 11, 51496, 51496, 293, 341, 307, 689, 321, 528, 281, 312, 13, 1033, 13, 407, 445, 382, 257, 13548, 490, 527, 3894, 7991, 11, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.08906382233349246, "compression_ratio": 1.6997084548104957, "no_speech_prob": 1.384509596391581e-05}, {"id": 81, "seek": 40120, "start": 419.68, "end": 423.84, "text": " these neural networks work on a very intuitive level. And so I think it's a good exercise again,", "tokens": [50364, 1011, 16945, 13, 400, 291, 486, 445, 2464, 484, 264, 23897, 1320, 293, 550, 291, 576, 764, 257, 16235, 50560, 50560, 1520, 260, 281, 652, 988, 300, 428, 29054, 12539, 295, 264, 16235, 26383, 365, 264, 472, 291, 15598, 50772, 50772, 1830, 646, 38377, 13, 407, 341, 390, 588, 3832, 337, 257, 938, 565, 11, 457, 965, 295, 1164, 309, 307, 3832, 51020, 51020, 281, 764, 364, 14295, 2848, 13, 583, 309, 390, 2138, 4420, 13, 400, 286, 519, 561, 1333, 295, 7320, 577, 51288, 51288, 613, 18161, 9590, 589, 322, 257, 588, 21769, 1496, 13, 400, 370, 286, 519, 309, 311, 257, 665, 5380, 797, 11, 51496, 51496, 293, 341, 307, 689, 321, 528, 281, 312, 13, 1033, 13, 407, 445, 382, 257, 13548, 490, 527, 3894, 7991, 11, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.08906382233349246, "compression_ratio": 1.6997084548104957, "no_speech_prob": 1.384509596391581e-05}, {"id": 82, "seek": 40120, "start": 423.84, "end": 427.68, "text": " and this is where we want to be. Okay. So just as a reminder from our previous lecture,", "tokens": [50364, 1011, 16945, 13, 400, 291, 486, 445, 2464, 484, 264, 23897, 1320, 293, 550, 291, 576, 764, 257, 16235, 50560, 50560, 1520, 260, 281, 652, 988, 300, 428, 29054, 12539, 295, 264, 16235, 26383, 365, 264, 472, 291, 15598, 50772, 50772, 1830, 646, 38377, 13, 407, 341, 390, 588, 3832, 337, 257, 938, 565, 11, 457, 965, 295, 1164, 309, 307, 3832, 51020, 51020, 281, 764, 364, 14295, 2848, 13, 583, 309, 390, 2138, 4420, 13, 400, 286, 519, 561, 1333, 295, 7320, 577, 51288, 51288, 613, 18161, 9590, 589, 322, 257, 588, 21769, 1496, 13, 400, 370, 286, 519, 309, 311, 257, 665, 5380, 797, 11, 51496, 51496, 293, 341, 307, 689, 321, 528, 281, 312, 13, 1033, 13, 407, 445, 382, 257, 13548, 490, 527, 3894, 7991, 11, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.08906382233349246, "compression_ratio": 1.6997084548104957, "no_speech_prob": 1.384509596391581e-05}, {"id": 83, "seek": 42768, "start": 427.68, "end": 432.88, "text": " this is the Jupyter notebook that we implemented at the time. And we're going to keep everything", "tokens": [50364, 341, 307, 264, 22125, 88, 391, 21060, 300, 321, 12270, 412, 264, 565, 13, 400, 321, 434, 516, 281, 1066, 1203, 50624, 50624, 264, 912, 13, 407, 321, 434, 920, 516, 281, 362, 257, 732, 4583, 4825, 12, 8376, 260, 43276, 2044, 365, 257, 15245, 50808, 50808, 2710, 2144, 4583, 13, 407, 264, 2128, 1320, 486, 312, 1936, 14800, 281, 341, 7991, 11, 457, 510, 51040, 51040, 321, 434, 516, 281, 483, 3973, 295, 4470, 13, 3207, 1007, 13, 400, 2602, 321, 434, 516, 281, 2464, 264, 23897, 1320, 51220, 51220, 16945, 13, 823, 510, 311, 264, 22465, 3089, 337, 341, 7991, 13, 492, 366, 5617, 257, 646, 2365, 31604, 294, 51484, 51484, 341, 21060, 13, 400, 264, 700, 1326, 5438, 510, 366, 14800, 281, 437, 321, 366, 1143, 281, 13, 407, 321, 366, 884, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.08775733014662489, "compression_ratio": 1.8453947368421053, "no_speech_prob": 1.2410896488290746e-05}, {"id": 84, "seek": 42768, "start": 432.88, "end": 436.56, "text": " the same. So we're still going to have a two layer multi-layer perceptron with a batch", "tokens": [50364, 341, 307, 264, 22125, 88, 391, 21060, 300, 321, 12270, 412, 264, 565, 13, 400, 321, 434, 516, 281, 1066, 1203, 50624, 50624, 264, 912, 13, 407, 321, 434, 920, 516, 281, 362, 257, 732, 4583, 4825, 12, 8376, 260, 43276, 2044, 365, 257, 15245, 50808, 50808, 2710, 2144, 4583, 13, 407, 264, 2128, 1320, 486, 312, 1936, 14800, 281, 341, 7991, 11, 457, 510, 51040, 51040, 321, 434, 516, 281, 483, 3973, 295, 4470, 13, 3207, 1007, 13, 400, 2602, 321, 434, 516, 281, 2464, 264, 23897, 1320, 51220, 51220, 16945, 13, 823, 510, 311, 264, 22465, 3089, 337, 341, 7991, 13, 492, 366, 5617, 257, 646, 2365, 31604, 294, 51484, 51484, 341, 21060, 13, 400, 264, 700, 1326, 5438, 510, 366, 14800, 281, 437, 321, 366, 1143, 281, 13, 407, 321, 366, 884, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.08775733014662489, "compression_ratio": 1.8453947368421053, "no_speech_prob": 1.2410896488290746e-05}, {"id": 85, "seek": 42768, "start": 436.56, "end": 441.2, "text": " normalization layer. So the forward pass will be basically identical to this lecture, but here", "tokens": [50364, 341, 307, 264, 22125, 88, 391, 21060, 300, 321, 12270, 412, 264, 565, 13, 400, 321, 434, 516, 281, 1066, 1203, 50624, 50624, 264, 912, 13, 407, 321, 434, 920, 516, 281, 362, 257, 732, 4583, 4825, 12, 8376, 260, 43276, 2044, 365, 257, 15245, 50808, 50808, 2710, 2144, 4583, 13, 407, 264, 2128, 1320, 486, 312, 1936, 14800, 281, 341, 7991, 11, 457, 510, 51040, 51040, 321, 434, 516, 281, 483, 3973, 295, 4470, 13, 3207, 1007, 13, 400, 2602, 321, 434, 516, 281, 2464, 264, 23897, 1320, 51220, 51220, 16945, 13, 823, 510, 311, 264, 22465, 3089, 337, 341, 7991, 13, 492, 366, 5617, 257, 646, 2365, 31604, 294, 51484, 51484, 341, 21060, 13, 400, 264, 700, 1326, 5438, 510, 366, 14800, 281, 437, 321, 366, 1143, 281, 13, 407, 321, 366, 884, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.08775733014662489, "compression_ratio": 1.8453947368421053, "no_speech_prob": 1.2410896488290746e-05}, {"id": 86, "seek": 42768, "start": 441.2, "end": 444.8, "text": " we're going to get rid of loss.backward. And instead we're going to write the backward pass", "tokens": [50364, 341, 307, 264, 22125, 88, 391, 21060, 300, 321, 12270, 412, 264, 565, 13, 400, 321, 434, 516, 281, 1066, 1203, 50624, 50624, 264, 912, 13, 407, 321, 434, 920, 516, 281, 362, 257, 732, 4583, 4825, 12, 8376, 260, 43276, 2044, 365, 257, 15245, 50808, 50808, 2710, 2144, 4583, 13, 407, 264, 2128, 1320, 486, 312, 1936, 14800, 281, 341, 7991, 11, 457, 510, 51040, 51040, 321, 434, 516, 281, 483, 3973, 295, 4470, 13, 3207, 1007, 13, 400, 2602, 321, 434, 516, 281, 2464, 264, 23897, 1320, 51220, 51220, 16945, 13, 823, 510, 311, 264, 22465, 3089, 337, 341, 7991, 13, 492, 366, 5617, 257, 646, 2365, 31604, 294, 51484, 51484, 341, 21060, 13, 400, 264, 700, 1326, 5438, 510, 366, 14800, 281, 437, 321, 366, 1143, 281, 13, 407, 321, 366, 884, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.08775733014662489, "compression_ratio": 1.8453947368421053, "no_speech_prob": 1.2410896488290746e-05}, {"id": 87, "seek": 42768, "start": 444.8, "end": 450.08, "text": " manually. Now here's the starter code for this lecture. We are becoming a back prop ninja in", "tokens": [50364, 341, 307, 264, 22125, 88, 391, 21060, 300, 321, 12270, 412, 264, 565, 13, 400, 321, 434, 516, 281, 1066, 1203, 50624, 50624, 264, 912, 13, 407, 321, 434, 920, 516, 281, 362, 257, 732, 4583, 4825, 12, 8376, 260, 43276, 2044, 365, 257, 15245, 50808, 50808, 2710, 2144, 4583, 13, 407, 264, 2128, 1320, 486, 312, 1936, 14800, 281, 341, 7991, 11, 457, 510, 51040, 51040, 321, 434, 516, 281, 483, 3973, 295, 4470, 13, 3207, 1007, 13, 400, 2602, 321, 434, 516, 281, 2464, 264, 23897, 1320, 51220, 51220, 16945, 13, 823, 510, 311, 264, 22465, 3089, 337, 341, 7991, 13, 492, 366, 5617, 257, 646, 2365, 31604, 294, 51484, 51484, 341, 21060, 13, 400, 264, 700, 1326, 5438, 510, 366, 14800, 281, 437, 321, 366, 1143, 281, 13, 407, 321, 366, 884, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.08775733014662489, "compression_ratio": 1.8453947368421053, "no_speech_prob": 1.2410896488290746e-05}, {"id": 88, "seek": 42768, "start": 450.08, "end": 456.24, "text": " this notebook. And the first few cells here are identical to what we are used to. So we are doing", "tokens": [50364, 341, 307, 264, 22125, 88, 391, 21060, 300, 321, 12270, 412, 264, 565, 13, 400, 321, 434, 516, 281, 1066, 1203, 50624, 50624, 264, 912, 13, 407, 321, 434, 920, 516, 281, 362, 257, 732, 4583, 4825, 12, 8376, 260, 43276, 2044, 365, 257, 15245, 50808, 50808, 2710, 2144, 4583, 13, 407, 264, 2128, 1320, 486, 312, 1936, 14800, 281, 341, 7991, 11, 457, 510, 51040, 51040, 321, 434, 516, 281, 483, 3973, 295, 4470, 13, 3207, 1007, 13, 400, 2602, 321, 434, 516, 281, 2464, 264, 23897, 1320, 51220, 51220, 16945, 13, 823, 510, 311, 264, 22465, 3089, 337, 341, 7991, 13, 492, 366, 5617, 257, 646, 2365, 31604, 294, 51484, 51484, 341, 21060, 13, 400, 264, 700, 1326, 5438, 510, 366, 14800, 281, 437, 321, 366, 1143, 281, 13, 407, 321, 366, 884, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.08775733014662489, "compression_ratio": 1.8453947368421053, "no_speech_prob": 1.2410896488290746e-05}, {"id": 89, "seek": 45624, "start": 456.24, "end": 462.24, "text": " some imports, loading the dataset and processing the dataset. None of this changed. Now here I'm", "tokens": [50364, 512, 41596, 11, 15114, 264, 28872, 293, 9007, 264, 28872, 13, 14492, 295, 341, 3105, 13, 823, 510, 286, 478, 50664, 50664, 15424, 257, 14877, 2445, 300, 321, 434, 516, 281, 764, 1780, 281, 6794, 264, 2771, 2448, 13, 407, 294, 50872, 50872, 1729, 11, 321, 366, 516, 281, 362, 264, 2771, 2448, 300, 321, 12539, 16945, 4175, 11, 293, 321, 434, 51064, 51064, 516, 281, 362, 2771, 2448, 300, 9953, 51, 284, 339, 4322, 1024, 13, 400, 321, 434, 516, 281, 312, 8568, 337, 3006, 1287, 11, 51300, 51300, 11926, 295, 1164, 300, 9953, 51, 284, 339, 307, 3006, 13, 1396, 510, 321, 362, 264, 5883, 2144, 300, 321, 366, 1596, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.061392623802711224, "compression_ratio": 1.8932806324110671, "no_speech_prob": 7.071537311276188e-06}, {"id": 90, "seek": 45624, "start": 462.24, "end": 466.40000000000003, "text": " introducing a utility function that we're going to use later to compare the gradients. So in", "tokens": [50364, 512, 41596, 11, 15114, 264, 28872, 293, 9007, 264, 28872, 13, 14492, 295, 341, 3105, 13, 823, 510, 286, 478, 50664, 50664, 15424, 257, 14877, 2445, 300, 321, 434, 516, 281, 764, 1780, 281, 6794, 264, 2771, 2448, 13, 407, 294, 50872, 50872, 1729, 11, 321, 366, 516, 281, 362, 264, 2771, 2448, 300, 321, 12539, 16945, 4175, 11, 293, 321, 434, 51064, 51064, 516, 281, 362, 2771, 2448, 300, 9953, 51, 284, 339, 4322, 1024, 13, 400, 321, 434, 516, 281, 312, 8568, 337, 3006, 1287, 11, 51300, 51300, 11926, 295, 1164, 300, 9953, 51, 284, 339, 307, 3006, 13, 1396, 510, 321, 362, 264, 5883, 2144, 300, 321, 366, 1596, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.061392623802711224, "compression_ratio": 1.8932806324110671, "no_speech_prob": 7.071537311276188e-06}, {"id": 91, "seek": 45624, "start": 466.40000000000003, "end": 470.24, "text": " particular, we are going to have the gradients that we estimate manually ourselves, and we're", "tokens": [50364, 512, 41596, 11, 15114, 264, 28872, 293, 9007, 264, 28872, 13, 14492, 295, 341, 3105, 13, 823, 510, 286, 478, 50664, 50664, 15424, 257, 14877, 2445, 300, 321, 434, 516, 281, 764, 1780, 281, 6794, 264, 2771, 2448, 13, 407, 294, 50872, 50872, 1729, 11, 321, 366, 516, 281, 362, 264, 2771, 2448, 300, 321, 12539, 16945, 4175, 11, 293, 321, 434, 51064, 51064, 516, 281, 362, 2771, 2448, 300, 9953, 51, 284, 339, 4322, 1024, 13, 400, 321, 434, 516, 281, 312, 8568, 337, 3006, 1287, 11, 51300, 51300, 11926, 295, 1164, 300, 9953, 51, 284, 339, 307, 3006, 13, 1396, 510, 321, 362, 264, 5883, 2144, 300, 321, 366, 1596, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.061392623802711224, "compression_ratio": 1.8932806324110671, "no_speech_prob": 7.071537311276188e-06}, {"id": 92, "seek": 45624, "start": 470.24, "end": 474.96000000000004, "text": " going to have gradients that PyTorch calculates. And we're going to be checking for correctness,", "tokens": [50364, 512, 41596, 11, 15114, 264, 28872, 293, 9007, 264, 28872, 13, 14492, 295, 341, 3105, 13, 823, 510, 286, 478, 50664, 50664, 15424, 257, 14877, 2445, 300, 321, 434, 516, 281, 764, 1780, 281, 6794, 264, 2771, 2448, 13, 407, 294, 50872, 50872, 1729, 11, 321, 366, 516, 281, 362, 264, 2771, 2448, 300, 321, 12539, 16945, 4175, 11, 293, 321, 434, 51064, 51064, 516, 281, 362, 2771, 2448, 300, 9953, 51, 284, 339, 4322, 1024, 13, 400, 321, 434, 516, 281, 312, 8568, 337, 3006, 1287, 11, 51300, 51300, 11926, 295, 1164, 300, 9953, 51, 284, 339, 307, 3006, 13, 1396, 510, 321, 362, 264, 5883, 2144, 300, 321, 366, 1596, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.061392623802711224, "compression_ratio": 1.8932806324110671, "no_speech_prob": 7.071537311276188e-06}, {"id": 93, "seek": 45624, "start": 474.96000000000004, "end": 480.96000000000004, "text": " assuming of course that PyTorch is correct. Then here we have the initialization that we are quite", "tokens": [50364, 512, 41596, 11, 15114, 264, 28872, 293, 9007, 264, 28872, 13, 14492, 295, 341, 3105, 13, 823, 510, 286, 478, 50664, 50664, 15424, 257, 14877, 2445, 300, 321, 434, 516, 281, 764, 1780, 281, 6794, 264, 2771, 2448, 13, 407, 294, 50872, 50872, 1729, 11, 321, 366, 516, 281, 362, 264, 2771, 2448, 300, 321, 12539, 16945, 4175, 11, 293, 321, 434, 51064, 51064, 516, 281, 362, 2771, 2448, 300, 9953, 51, 284, 339, 4322, 1024, 13, 400, 321, 434, 516, 281, 312, 8568, 337, 3006, 1287, 11, 51300, 51300, 11926, 295, 1164, 300, 9953, 51, 284, 339, 307, 3006, 13, 1396, 510, 321, 362, 264, 5883, 2144, 300, 321, 366, 1596, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.061392623802711224, "compression_ratio": 1.8932806324110671, "no_speech_prob": 7.071537311276188e-06}, {"id": 94, "seek": 48096, "start": 480.96, "end": 486.4, "text": " used to. So we have our embedding table for the characters, the first layer, second layer, and a", "tokens": [50364, 1143, 281, 13, 407, 321, 362, 527, 12240, 3584, 3199, 337, 264, 4342, 11, 264, 700, 4583, 11, 1150, 4583, 11, 293, 257, 50636, 50636, 15245, 2710, 2144, 294, 1296, 13, 400, 510, 311, 689, 321, 1884, 439, 264, 9834, 13, 823, 291, 486, 3637, 50888, 50888, 300, 286, 3105, 264, 5883, 2144, 257, 707, 857, 281, 312, 1359, 3547, 13, 407, 5646, 291, 576, 992, 264, 51140, 51140, 32152, 281, 312, 439, 4018, 13, 1692, 286, 669, 3287, 552, 281, 312, 1359, 4974, 3547, 13, 400, 286, 478, 884, 341, 570, 51408, 51460, 498, 428, 9102, 366, 5883, 1602, 281, 2293, 4018, 11, 2171, 437, 393, 1051, 307, 300, 393, 6094, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.0576651301877252, "compression_ratio": 1.7052631578947368, "no_speech_prob": 1.670047640800476e-05}, {"id": 95, "seek": 48096, "start": 486.4, "end": 491.44, "text": " batch normalization in between. And here's where we create all the parameters. Now you will note", "tokens": [50364, 1143, 281, 13, 407, 321, 362, 527, 12240, 3584, 3199, 337, 264, 4342, 11, 264, 700, 4583, 11, 1150, 4583, 11, 293, 257, 50636, 50636, 15245, 2710, 2144, 294, 1296, 13, 400, 510, 311, 689, 321, 1884, 439, 264, 9834, 13, 823, 291, 486, 3637, 50888, 50888, 300, 286, 3105, 264, 5883, 2144, 257, 707, 857, 281, 312, 1359, 3547, 13, 407, 5646, 291, 576, 992, 264, 51140, 51140, 32152, 281, 312, 439, 4018, 13, 1692, 286, 669, 3287, 552, 281, 312, 1359, 4974, 3547, 13, 400, 286, 478, 884, 341, 570, 51408, 51460, 498, 428, 9102, 366, 5883, 1602, 281, 2293, 4018, 11, 2171, 437, 393, 1051, 307, 300, 393, 6094, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.0576651301877252, "compression_ratio": 1.7052631578947368, "no_speech_prob": 1.670047640800476e-05}, {"id": 96, "seek": 48096, "start": 491.44, "end": 496.47999999999996, "text": " that I changed the initialization a little bit to be small numbers. So normally you would set the", "tokens": [50364, 1143, 281, 13, 407, 321, 362, 527, 12240, 3584, 3199, 337, 264, 4342, 11, 264, 700, 4583, 11, 1150, 4583, 11, 293, 257, 50636, 50636, 15245, 2710, 2144, 294, 1296, 13, 400, 510, 311, 689, 321, 1884, 439, 264, 9834, 13, 823, 291, 486, 3637, 50888, 50888, 300, 286, 3105, 264, 5883, 2144, 257, 707, 857, 281, 312, 1359, 3547, 13, 407, 5646, 291, 576, 992, 264, 51140, 51140, 32152, 281, 312, 439, 4018, 13, 1692, 286, 669, 3287, 552, 281, 312, 1359, 4974, 3547, 13, 400, 286, 478, 884, 341, 570, 51408, 51460, 498, 428, 9102, 366, 5883, 1602, 281, 2293, 4018, 11, 2171, 437, 393, 1051, 307, 300, 393, 6094, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.0576651301877252, "compression_ratio": 1.7052631578947368, "no_speech_prob": 1.670047640800476e-05}, {"id": 97, "seek": 48096, "start": 496.47999999999996, "end": 501.84, "text": " biases to be all zero. Here I am setting them to be small random numbers. And I'm doing this because", "tokens": [50364, 1143, 281, 13, 407, 321, 362, 527, 12240, 3584, 3199, 337, 264, 4342, 11, 264, 700, 4583, 11, 1150, 4583, 11, 293, 257, 50636, 50636, 15245, 2710, 2144, 294, 1296, 13, 400, 510, 311, 689, 321, 1884, 439, 264, 9834, 13, 823, 291, 486, 3637, 50888, 50888, 300, 286, 3105, 264, 5883, 2144, 257, 707, 857, 281, 312, 1359, 3547, 13, 407, 5646, 291, 576, 992, 264, 51140, 51140, 32152, 281, 312, 439, 4018, 13, 1692, 286, 669, 3287, 552, 281, 312, 1359, 4974, 3547, 13, 400, 286, 478, 884, 341, 570, 51408, 51460, 498, 428, 9102, 366, 5883, 1602, 281, 2293, 4018, 11, 2171, 437, 393, 1051, 307, 300, 393, 6094, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.0576651301877252, "compression_ratio": 1.7052631578947368, "no_speech_prob": 1.670047640800476e-05}, {"id": 98, "seek": 48096, "start": 502.88, "end": 507.28, "text": " if your variables are initialized to exactly zero, sometimes what can happen is that can mask", "tokens": [50364, 1143, 281, 13, 407, 321, 362, 527, 12240, 3584, 3199, 337, 264, 4342, 11, 264, 700, 4583, 11, 1150, 4583, 11, 293, 257, 50636, 50636, 15245, 2710, 2144, 294, 1296, 13, 400, 510, 311, 689, 321, 1884, 439, 264, 9834, 13, 823, 291, 486, 3637, 50888, 50888, 300, 286, 3105, 264, 5883, 2144, 257, 707, 857, 281, 312, 1359, 3547, 13, 407, 5646, 291, 576, 992, 264, 51140, 51140, 32152, 281, 312, 439, 4018, 13, 1692, 286, 669, 3287, 552, 281, 312, 1359, 4974, 3547, 13, 400, 286, 478, 884, 341, 570, 51408, 51460, 498, 428, 9102, 366, 5883, 1602, 281, 2293, 4018, 11, 2171, 437, 393, 1051, 307, 300, 393, 6094, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.0576651301877252, "compression_ratio": 1.7052631578947368, "no_speech_prob": 1.670047640800476e-05}, {"id": 99, "seek": 50728, "start": 507.28, "end": 513.4399999999999, "text": " an incorrect implementation of a gradient. Because when everything is zero, it sort of simplifies and", "tokens": [50364, 364, 18424, 11420, 295, 257, 16235, 13, 1436, 562, 1203, 307, 4018, 11, 309, 1333, 295, 6883, 11221, 293, 50672, 50672, 2709, 291, 257, 709, 18587, 6114, 295, 264, 16235, 813, 291, 576, 5911, 483, 13, 400, 370, 538, 1455, 264, 50880, 50880, 1359, 3547, 11, 286, 478, 1382, 281, 517, 3799, 74, 729, 3995, 13603, 294, 613, 20448, 13, 509, 611, 3449, 300, 51208, 51208, 286, 478, 1228, 363, 16, 294, 264, 700, 4583, 13, 286, 478, 1228, 257, 12577, 7228, 15245, 2710, 2144, 558, 10543, 13, 51500, 51548, 407, 341, 576, 5850, 406, 312, 437, 291, 360, 570, 321, 2825, 466, 264, 1186, 300, 291, 500, 380, 643, 257, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.07859946541164232, "compression_ratio": 1.6644518272425248, "no_speech_prob": 3.340415787533857e-06}, {"id": 100, "seek": 50728, "start": 513.4399999999999, "end": 517.6, "text": " gives you a much simpler expression of the gradient than you would otherwise get. And so by making the", "tokens": [50364, 364, 18424, 11420, 295, 257, 16235, 13, 1436, 562, 1203, 307, 4018, 11, 309, 1333, 295, 6883, 11221, 293, 50672, 50672, 2709, 291, 257, 709, 18587, 6114, 295, 264, 16235, 813, 291, 576, 5911, 483, 13, 400, 370, 538, 1455, 264, 50880, 50880, 1359, 3547, 11, 286, 478, 1382, 281, 517, 3799, 74, 729, 3995, 13603, 294, 613, 20448, 13, 509, 611, 3449, 300, 51208, 51208, 286, 478, 1228, 363, 16, 294, 264, 700, 4583, 13, 286, 478, 1228, 257, 12577, 7228, 15245, 2710, 2144, 558, 10543, 13, 51500, 51548, 407, 341, 576, 5850, 406, 312, 437, 291, 360, 570, 321, 2825, 466, 264, 1186, 300, 291, 500, 380, 643, 257, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.07859946541164232, "compression_ratio": 1.6644518272425248, "no_speech_prob": 3.340415787533857e-06}, {"id": 101, "seek": 50728, "start": 517.6, "end": 524.16, "text": " small numbers, I'm trying to unmask those potential errors in these calculations. You also notice that", "tokens": [50364, 364, 18424, 11420, 295, 257, 16235, 13, 1436, 562, 1203, 307, 4018, 11, 309, 1333, 295, 6883, 11221, 293, 50672, 50672, 2709, 291, 257, 709, 18587, 6114, 295, 264, 16235, 813, 291, 576, 5911, 483, 13, 400, 370, 538, 1455, 264, 50880, 50880, 1359, 3547, 11, 286, 478, 1382, 281, 517, 3799, 74, 729, 3995, 13603, 294, 613, 20448, 13, 509, 611, 3449, 300, 51208, 51208, 286, 478, 1228, 363, 16, 294, 264, 700, 4583, 13, 286, 478, 1228, 257, 12577, 7228, 15245, 2710, 2144, 558, 10543, 13, 51500, 51548, 407, 341, 576, 5850, 406, 312, 437, 291, 360, 570, 321, 2825, 466, 264, 1186, 300, 291, 500, 380, 643, 257, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.07859946541164232, "compression_ratio": 1.6644518272425248, "no_speech_prob": 3.340415787533857e-06}, {"id": 102, "seek": 50728, "start": 524.16, "end": 530.0, "text": " I'm using B1 in the first layer. I'm using a bias despite batch normalization right afterwards.", "tokens": [50364, 364, 18424, 11420, 295, 257, 16235, 13, 1436, 562, 1203, 307, 4018, 11, 309, 1333, 295, 6883, 11221, 293, 50672, 50672, 2709, 291, 257, 709, 18587, 6114, 295, 264, 16235, 813, 291, 576, 5911, 483, 13, 400, 370, 538, 1455, 264, 50880, 50880, 1359, 3547, 11, 286, 478, 1382, 281, 517, 3799, 74, 729, 3995, 13603, 294, 613, 20448, 13, 509, 611, 3449, 300, 51208, 51208, 286, 478, 1228, 363, 16, 294, 264, 700, 4583, 13, 286, 478, 1228, 257, 12577, 7228, 15245, 2710, 2144, 558, 10543, 13, 51500, 51548, 407, 341, 576, 5850, 406, 312, 437, 291, 360, 570, 321, 2825, 466, 264, 1186, 300, 291, 500, 380, 643, 257, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.07859946541164232, "compression_ratio": 1.6644518272425248, "no_speech_prob": 3.340415787533857e-06}, {"id": 103, "seek": 50728, "start": 530.9599999999999, "end": 534.8, "text": " So this would typically not be what you do because we talked about the fact that you don't need a", "tokens": [50364, 364, 18424, 11420, 295, 257, 16235, 13, 1436, 562, 1203, 307, 4018, 11, 309, 1333, 295, 6883, 11221, 293, 50672, 50672, 2709, 291, 257, 709, 18587, 6114, 295, 264, 16235, 813, 291, 576, 5911, 483, 13, 400, 370, 538, 1455, 264, 50880, 50880, 1359, 3547, 11, 286, 478, 1382, 281, 517, 3799, 74, 729, 3995, 13603, 294, 613, 20448, 13, 509, 611, 3449, 300, 51208, 51208, 286, 478, 1228, 363, 16, 294, 264, 700, 4583, 13, 286, 478, 1228, 257, 12577, 7228, 15245, 2710, 2144, 558, 10543, 13, 51500, 51548, 407, 341, 576, 5850, 406, 312, 437, 291, 360, 570, 321, 2825, 466, 264, 1186, 300, 291, 500, 380, 643, 257, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.07859946541164232, "compression_ratio": 1.6644518272425248, "no_speech_prob": 3.340415787533857e-06}, {"id": 104, "seek": 53480, "start": 534.8, "end": 539.8399999999999, "text": " bias. But I'm doing this here just for fun because we're going to have a gradient with respect to it", "tokens": [50364, 12577, 13, 583, 286, 478, 884, 341, 510, 445, 337, 1019, 570, 321, 434, 516, 281, 362, 257, 16235, 365, 3104, 281, 309, 50616, 50616, 293, 321, 393, 1520, 300, 321, 366, 920, 28258, 309, 8944, 754, 1673, 341, 12577, 307, 637, 24274, 13, 50820, 50884, 407, 510, 286, 478, 28258, 257, 2167, 15245, 13, 400, 550, 510, 286, 669, 884, 257, 2128, 1320, 13, 51080, 51140, 823, 291, 603, 3449, 300, 264, 2128, 1320, 307, 10591, 14342, 490, 437, 321, 366, 1143, 281, 13, 51344, 51344, 1692, 264, 2128, 1320, 390, 445, 510, 13, 823, 264, 1778, 300, 264, 2128, 1320, 307, 2854, 307, 337, 732, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06822481241312113, "compression_ratio": 1.801556420233463, "no_speech_prob": 5.014615453546867e-06}, {"id": 105, "seek": 53480, "start": 539.8399999999999, "end": 543.92, "text": " and we can check that we are still calculating it correctly even though this bias is spurious.", "tokens": [50364, 12577, 13, 583, 286, 478, 884, 341, 510, 445, 337, 1019, 570, 321, 434, 516, 281, 362, 257, 16235, 365, 3104, 281, 309, 50616, 50616, 293, 321, 393, 1520, 300, 321, 366, 920, 28258, 309, 8944, 754, 1673, 341, 12577, 307, 637, 24274, 13, 50820, 50884, 407, 510, 286, 478, 28258, 257, 2167, 15245, 13, 400, 550, 510, 286, 669, 884, 257, 2128, 1320, 13, 51080, 51140, 823, 291, 603, 3449, 300, 264, 2128, 1320, 307, 10591, 14342, 490, 437, 321, 366, 1143, 281, 13, 51344, 51344, 1692, 264, 2128, 1320, 390, 445, 510, 13, 823, 264, 1778, 300, 264, 2128, 1320, 307, 2854, 307, 337, 732, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06822481241312113, "compression_ratio": 1.801556420233463, "no_speech_prob": 5.014615453546867e-06}, {"id": 106, "seek": 53480, "start": 545.1999999999999, "end": 549.12, "text": " So here I'm calculating a single batch. And then here I am doing a forward pass.", "tokens": [50364, 12577, 13, 583, 286, 478, 884, 341, 510, 445, 337, 1019, 570, 321, 434, 516, 281, 362, 257, 16235, 365, 3104, 281, 309, 50616, 50616, 293, 321, 393, 1520, 300, 321, 366, 920, 28258, 309, 8944, 754, 1673, 341, 12577, 307, 637, 24274, 13, 50820, 50884, 407, 510, 286, 478, 28258, 257, 2167, 15245, 13, 400, 550, 510, 286, 669, 884, 257, 2128, 1320, 13, 51080, 51140, 823, 291, 603, 3449, 300, 264, 2128, 1320, 307, 10591, 14342, 490, 437, 321, 366, 1143, 281, 13, 51344, 51344, 1692, 264, 2128, 1320, 390, 445, 510, 13, 823, 264, 1778, 300, 264, 2128, 1320, 307, 2854, 307, 337, 732, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06822481241312113, "compression_ratio": 1.801556420233463, "no_speech_prob": 5.014615453546867e-06}, {"id": 107, "seek": 53480, "start": 550.3199999999999, "end": 554.4, "text": " Now you'll notice that the forward pass is significantly expanded from what we are used to.", "tokens": [50364, 12577, 13, 583, 286, 478, 884, 341, 510, 445, 337, 1019, 570, 321, 434, 516, 281, 362, 257, 16235, 365, 3104, 281, 309, 50616, 50616, 293, 321, 393, 1520, 300, 321, 366, 920, 28258, 309, 8944, 754, 1673, 341, 12577, 307, 637, 24274, 13, 50820, 50884, 407, 510, 286, 478, 28258, 257, 2167, 15245, 13, 400, 550, 510, 286, 669, 884, 257, 2128, 1320, 13, 51080, 51140, 823, 291, 603, 3449, 300, 264, 2128, 1320, 307, 10591, 14342, 490, 437, 321, 366, 1143, 281, 13, 51344, 51344, 1692, 264, 2128, 1320, 390, 445, 510, 13, 823, 264, 1778, 300, 264, 2128, 1320, 307, 2854, 307, 337, 732, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06822481241312113, "compression_ratio": 1.801556420233463, "no_speech_prob": 5.014615453546867e-06}, {"id": 108, "seek": 53480, "start": 554.4, "end": 560.8, "text": " Here the forward pass was just here. Now the reason that the forward pass is longer is for two", "tokens": [50364, 12577, 13, 583, 286, 478, 884, 341, 510, 445, 337, 1019, 570, 321, 434, 516, 281, 362, 257, 16235, 365, 3104, 281, 309, 50616, 50616, 293, 321, 393, 1520, 300, 321, 366, 920, 28258, 309, 8944, 754, 1673, 341, 12577, 307, 637, 24274, 13, 50820, 50884, 407, 510, 286, 478, 28258, 257, 2167, 15245, 13, 400, 550, 510, 286, 669, 884, 257, 2128, 1320, 13, 51080, 51140, 823, 291, 603, 3449, 300, 264, 2128, 1320, 307, 10591, 14342, 490, 437, 321, 366, 1143, 281, 13, 51344, 51344, 1692, 264, 2128, 1320, 390, 445, 510, 13, 823, 264, 1778, 300, 264, 2128, 1320, 307, 2854, 307, 337, 732, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.06822481241312113, "compression_ratio": 1.801556420233463, "no_speech_prob": 5.014615453546867e-06}, {"id": 109, "seek": 56080, "start": 560.8, "end": 565.92, "text": " reasons. Number one, here we just had an f dot cross entropy. But here I am bringing back a", "tokens": [50364, 4112, 13, 5118, 472, 11, 510, 321, 445, 632, 364, 283, 5893, 3278, 30867, 13, 583, 510, 286, 669, 5062, 646, 257, 50620, 50620, 13691, 11420, 295, 264, 4470, 2445, 13, 400, 1230, 732, 11, 286, 600, 5463, 493, 264, 11420, 666, 50932, 50932, 38798, 24004, 13, 407, 321, 362, 257, 688, 544, 19376, 10688, 830, 2051, 264, 636, 294, 264, 51188, 51188, 2128, 1320, 13, 400, 300, 311, 570, 321, 366, 466, 281, 352, 12204, 293, 8873, 264, 2771, 2448, 51400, 51440, 294, 341, 646, 38377, 490, 264, 2767, 281, 264, 1192, 13, 407, 321, 434, 516, 281, 352, 22167, 13, 400, 445, 411, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.08494940591514658, "compression_ratio": 1.68, "no_speech_prob": 2.6841678391065216e-06}, {"id": 110, "seek": 56080, "start": 565.92, "end": 572.16, "text": " explicit implementation of the loss function. And number two, I've broken up the implementation into", "tokens": [50364, 4112, 13, 5118, 472, 11, 510, 321, 445, 632, 364, 283, 5893, 3278, 30867, 13, 583, 510, 286, 669, 5062, 646, 257, 50620, 50620, 13691, 11420, 295, 264, 4470, 2445, 13, 400, 1230, 732, 11, 286, 600, 5463, 493, 264, 11420, 666, 50932, 50932, 38798, 24004, 13, 407, 321, 362, 257, 688, 544, 19376, 10688, 830, 2051, 264, 636, 294, 264, 51188, 51188, 2128, 1320, 13, 400, 300, 311, 570, 321, 366, 466, 281, 352, 12204, 293, 8873, 264, 2771, 2448, 51400, 51440, 294, 341, 646, 38377, 490, 264, 2767, 281, 264, 1192, 13, 407, 321, 434, 516, 281, 352, 22167, 13, 400, 445, 411, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.08494940591514658, "compression_ratio": 1.68, "no_speech_prob": 2.6841678391065216e-06}, {"id": 111, "seek": 56080, "start": 572.16, "end": 577.28, "text": " manageable chunks. So we have a lot more intermediate tensors along the way in the", "tokens": [50364, 4112, 13, 5118, 472, 11, 510, 321, 445, 632, 364, 283, 5893, 3278, 30867, 13, 583, 510, 286, 669, 5062, 646, 257, 50620, 50620, 13691, 11420, 295, 264, 4470, 2445, 13, 400, 1230, 732, 11, 286, 600, 5463, 493, 264, 11420, 666, 50932, 50932, 38798, 24004, 13, 407, 321, 362, 257, 688, 544, 19376, 10688, 830, 2051, 264, 636, 294, 264, 51188, 51188, 2128, 1320, 13, 400, 300, 311, 570, 321, 366, 466, 281, 352, 12204, 293, 8873, 264, 2771, 2448, 51400, 51440, 294, 341, 646, 38377, 490, 264, 2767, 281, 264, 1192, 13, 407, 321, 434, 516, 281, 352, 22167, 13, 400, 445, 411, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.08494940591514658, "compression_ratio": 1.68, "no_speech_prob": 2.6841678391065216e-06}, {"id": 112, "seek": 56080, "start": 577.28, "end": 581.52, "text": " forward pass. And that's because we are about to go backwards and calculate the gradients", "tokens": [50364, 4112, 13, 5118, 472, 11, 510, 321, 445, 632, 364, 283, 5893, 3278, 30867, 13, 583, 510, 286, 669, 5062, 646, 257, 50620, 50620, 13691, 11420, 295, 264, 4470, 2445, 13, 400, 1230, 732, 11, 286, 600, 5463, 493, 264, 11420, 666, 50932, 50932, 38798, 24004, 13, 407, 321, 362, 257, 688, 544, 19376, 10688, 830, 2051, 264, 636, 294, 264, 51188, 51188, 2128, 1320, 13, 400, 300, 311, 570, 321, 366, 466, 281, 352, 12204, 293, 8873, 264, 2771, 2448, 51400, 51440, 294, 341, 646, 38377, 490, 264, 2767, 281, 264, 1192, 13, 407, 321, 434, 516, 281, 352, 22167, 13, 400, 445, 411, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.08494940591514658, "compression_ratio": 1.68, "no_speech_prob": 2.6841678391065216e-06}, {"id": 113, "seek": 56080, "start": 582.3199999999999, "end": 588.3199999999999, "text": " in this back propagation from the bottom to the top. So we're going to go upwards. And just like", "tokens": [50364, 4112, 13, 5118, 472, 11, 510, 321, 445, 632, 364, 283, 5893, 3278, 30867, 13, 583, 510, 286, 669, 5062, 646, 257, 50620, 50620, 13691, 11420, 295, 264, 4470, 2445, 13, 400, 1230, 732, 11, 286, 600, 5463, 493, 264, 11420, 666, 50932, 50932, 38798, 24004, 13, 407, 321, 362, 257, 688, 544, 19376, 10688, 830, 2051, 264, 636, 294, 264, 51188, 51188, 2128, 1320, 13, 400, 300, 311, 570, 321, 366, 466, 281, 352, 12204, 293, 8873, 264, 2771, 2448, 51400, 51440, 294, 341, 646, 38377, 490, 264, 2767, 281, 264, 1192, 13, 407, 321, 434, 516, 281, 352, 22167, 13, 400, 445, 411, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.08494940591514658, "compression_ratio": 1.68, "no_speech_prob": 2.6841678391065216e-06}, {"id": 114, "seek": 58832, "start": 588.32, "end": 592.4000000000001, "text": " we have, for example, the log props tensor in a forward pass, in a backward pass we're going to", "tokens": [50364, 321, 362, 11, 337, 1365, 11, 264, 3565, 26173, 40863, 294, 257, 2128, 1320, 11, 294, 257, 23897, 1320, 321, 434, 516, 281, 50568, 50568, 362, 257, 274, 3565, 26173, 11, 597, 307, 516, 281, 3531, 264, 13760, 295, 264, 4470, 365, 3104, 281, 264, 3565, 50772, 50772, 26173, 40863, 13, 400, 370, 321, 434, 516, 281, 312, 2666, 2029, 274, 281, 633, 472, 295, 613, 10688, 830, 293, 28258, 51048, 51048, 309, 2051, 264, 636, 295, 341, 646, 38377, 13, 407, 382, 364, 1365, 11, 321, 362, 257, 272, 293, 8936, 510, 13, 492, 434, 516, 51344, 51344, 281, 312, 28258, 257, 274, 65, 293, 8936, 13, 407, 510, 286, 478, 3585, 9953, 51, 284, 339, 300, 321, 528, 281, 18340, 264, 2771, 295, 439, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.09266512210552509, "compression_ratio": 1.9133858267716535, "no_speech_prob": 5.422052254289156e-06}, {"id": 115, "seek": 58832, "start": 592.4000000000001, "end": 596.48, "text": " have a d log props, which is going to store the derivative of the loss with respect to the log", "tokens": [50364, 321, 362, 11, 337, 1365, 11, 264, 3565, 26173, 40863, 294, 257, 2128, 1320, 11, 294, 257, 23897, 1320, 321, 434, 516, 281, 50568, 50568, 362, 257, 274, 3565, 26173, 11, 597, 307, 516, 281, 3531, 264, 13760, 295, 264, 4470, 365, 3104, 281, 264, 3565, 50772, 50772, 26173, 40863, 13, 400, 370, 321, 434, 516, 281, 312, 2666, 2029, 274, 281, 633, 472, 295, 613, 10688, 830, 293, 28258, 51048, 51048, 309, 2051, 264, 636, 295, 341, 646, 38377, 13, 407, 382, 364, 1365, 11, 321, 362, 257, 272, 293, 8936, 510, 13, 492, 434, 516, 51344, 51344, 281, 312, 28258, 257, 274, 65, 293, 8936, 13, 407, 510, 286, 478, 3585, 9953, 51, 284, 339, 300, 321, 528, 281, 18340, 264, 2771, 295, 439, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.09266512210552509, "compression_ratio": 1.9133858267716535, "no_speech_prob": 5.422052254289156e-06}, {"id": 116, "seek": 58832, "start": 596.48, "end": 602.0, "text": " props tensor. And so we're going to be prepending d to every one of these tensors and calculating", "tokens": [50364, 321, 362, 11, 337, 1365, 11, 264, 3565, 26173, 40863, 294, 257, 2128, 1320, 11, 294, 257, 23897, 1320, 321, 434, 516, 281, 50568, 50568, 362, 257, 274, 3565, 26173, 11, 597, 307, 516, 281, 3531, 264, 13760, 295, 264, 4470, 365, 3104, 281, 264, 3565, 50772, 50772, 26173, 40863, 13, 400, 370, 321, 434, 516, 281, 312, 2666, 2029, 274, 281, 633, 472, 295, 613, 10688, 830, 293, 28258, 51048, 51048, 309, 2051, 264, 636, 295, 341, 646, 38377, 13, 407, 382, 364, 1365, 11, 321, 362, 257, 272, 293, 8936, 510, 13, 492, 434, 516, 51344, 51344, 281, 312, 28258, 257, 274, 65, 293, 8936, 13, 407, 510, 286, 478, 3585, 9953, 51, 284, 339, 300, 321, 528, 281, 18340, 264, 2771, 295, 439, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.09266512210552509, "compression_ratio": 1.9133858267716535, "no_speech_prob": 5.422052254289156e-06}, {"id": 117, "seek": 58832, "start": 602.0, "end": 607.9200000000001, "text": " it along the way of this back propagation. So as an example, we have a b and raw here. We're going", "tokens": [50364, 321, 362, 11, 337, 1365, 11, 264, 3565, 26173, 40863, 294, 257, 2128, 1320, 11, 294, 257, 23897, 1320, 321, 434, 516, 281, 50568, 50568, 362, 257, 274, 3565, 26173, 11, 597, 307, 516, 281, 3531, 264, 13760, 295, 264, 4470, 365, 3104, 281, 264, 3565, 50772, 50772, 26173, 40863, 13, 400, 370, 321, 434, 516, 281, 312, 2666, 2029, 274, 281, 633, 472, 295, 613, 10688, 830, 293, 28258, 51048, 51048, 309, 2051, 264, 636, 295, 341, 646, 38377, 13, 407, 382, 364, 1365, 11, 321, 362, 257, 272, 293, 8936, 510, 13, 492, 434, 516, 51344, 51344, 281, 312, 28258, 257, 274, 65, 293, 8936, 13, 407, 510, 286, 478, 3585, 9953, 51, 284, 339, 300, 321, 528, 281, 18340, 264, 2771, 295, 439, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.09266512210552509, "compression_ratio": 1.9133858267716535, "no_speech_prob": 5.422052254289156e-06}, {"id": 118, "seek": 58832, "start": 607.9200000000001, "end": 614.24, "text": " to be calculating a db and raw. So here I'm telling PyTorch that we want to retain the grad of all", "tokens": [50364, 321, 362, 11, 337, 1365, 11, 264, 3565, 26173, 40863, 294, 257, 2128, 1320, 11, 294, 257, 23897, 1320, 321, 434, 516, 281, 50568, 50568, 362, 257, 274, 3565, 26173, 11, 597, 307, 516, 281, 3531, 264, 13760, 295, 264, 4470, 365, 3104, 281, 264, 3565, 50772, 50772, 26173, 40863, 13, 400, 370, 321, 434, 516, 281, 312, 2666, 2029, 274, 281, 633, 472, 295, 613, 10688, 830, 293, 28258, 51048, 51048, 309, 2051, 264, 636, 295, 341, 646, 38377, 13, 407, 382, 364, 1365, 11, 321, 362, 257, 272, 293, 8936, 510, 13, 492, 434, 516, 51344, 51344, 281, 312, 28258, 257, 274, 65, 293, 8936, 13, 407, 510, 286, 478, 3585, 9953, 51, 284, 339, 300, 321, 528, 281, 18340, 264, 2771, 295, 439, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.09266512210552509, "compression_ratio": 1.9133858267716535, "no_speech_prob": 5.422052254289156e-06}, {"id": 119, "seek": 61424, "start": 614.24, "end": 619.6, "text": " these intermediate values, because here in exercise one, we're going to calculate the backward pass.", "tokens": [50364, 613, 19376, 4190, 11, 570, 510, 294, 5380, 472, 11, 321, 434, 516, 281, 8873, 264, 23897, 1320, 13, 50632, 50632, 407, 321, 434, 516, 281, 8873, 439, 613, 274, 9102, 293, 764, 264, 269, 2455, 2445, 286, 600, 7268, 3673, 50928, 50928, 281, 1520, 527, 3006, 1287, 365, 3104, 281, 437, 9953, 51, 284, 339, 307, 3585, 505, 13, 639, 307, 516, 281, 312, 5380, 51172, 51172, 472, 11, 689, 321, 1333, 295, 646, 48256, 807, 341, 2302, 4295, 13, 823, 11, 445, 281, 976, 291, 257, 588, 1702, 51460, 51460, 14281, 295, 437, 311, 516, 281, 1051, 294, 5380, 732, 293, 2507, 11, 510, 321, 362, 4498, 5463, 493, 264, 4470, 51744, 51772], "temperature": 0.0, "avg_logprob": -0.07646889971871661, "compression_ratio": 1.7359154929577465, "no_speech_prob": 6.747737188561587e-06}, {"id": 120, "seek": 61424, "start": 619.6, "end": 625.52, "text": " So we're going to calculate all these d variables and use the cmp function I've introduced above", "tokens": [50364, 613, 19376, 4190, 11, 570, 510, 294, 5380, 472, 11, 321, 434, 516, 281, 8873, 264, 23897, 1320, 13, 50632, 50632, 407, 321, 434, 516, 281, 8873, 439, 613, 274, 9102, 293, 764, 264, 269, 2455, 2445, 286, 600, 7268, 3673, 50928, 50928, 281, 1520, 527, 3006, 1287, 365, 3104, 281, 437, 9953, 51, 284, 339, 307, 3585, 505, 13, 639, 307, 516, 281, 312, 5380, 51172, 51172, 472, 11, 689, 321, 1333, 295, 646, 48256, 807, 341, 2302, 4295, 13, 823, 11, 445, 281, 976, 291, 257, 588, 1702, 51460, 51460, 14281, 295, 437, 311, 516, 281, 1051, 294, 5380, 732, 293, 2507, 11, 510, 321, 362, 4498, 5463, 493, 264, 4470, 51744, 51772], "temperature": 0.0, "avg_logprob": -0.07646889971871661, "compression_ratio": 1.7359154929577465, "no_speech_prob": 6.747737188561587e-06}, {"id": 121, "seek": 61424, "start": 625.52, "end": 630.4, "text": " to check our correctness with respect to what PyTorch is telling us. This is going to be exercise", "tokens": [50364, 613, 19376, 4190, 11, 570, 510, 294, 5380, 472, 11, 321, 434, 516, 281, 8873, 264, 23897, 1320, 13, 50632, 50632, 407, 321, 434, 516, 281, 8873, 439, 613, 274, 9102, 293, 764, 264, 269, 2455, 2445, 286, 600, 7268, 3673, 50928, 50928, 281, 1520, 527, 3006, 1287, 365, 3104, 281, 437, 9953, 51, 284, 339, 307, 3585, 505, 13, 639, 307, 516, 281, 312, 5380, 51172, 51172, 472, 11, 689, 321, 1333, 295, 646, 48256, 807, 341, 2302, 4295, 13, 823, 11, 445, 281, 976, 291, 257, 588, 1702, 51460, 51460, 14281, 295, 437, 311, 516, 281, 1051, 294, 5380, 732, 293, 2507, 11, 510, 321, 362, 4498, 5463, 493, 264, 4470, 51744, 51772], "temperature": 0.0, "avg_logprob": -0.07646889971871661, "compression_ratio": 1.7359154929577465, "no_speech_prob": 6.747737188561587e-06}, {"id": 122, "seek": 61424, "start": 630.4, "end": 636.16, "text": " one, where we sort of back propagate through this entire graph. Now, just to give you a very quick", "tokens": [50364, 613, 19376, 4190, 11, 570, 510, 294, 5380, 472, 11, 321, 434, 516, 281, 8873, 264, 23897, 1320, 13, 50632, 50632, 407, 321, 434, 516, 281, 8873, 439, 613, 274, 9102, 293, 764, 264, 269, 2455, 2445, 286, 600, 7268, 3673, 50928, 50928, 281, 1520, 527, 3006, 1287, 365, 3104, 281, 437, 9953, 51, 284, 339, 307, 3585, 505, 13, 639, 307, 516, 281, 312, 5380, 51172, 51172, 472, 11, 689, 321, 1333, 295, 646, 48256, 807, 341, 2302, 4295, 13, 823, 11, 445, 281, 976, 291, 257, 588, 1702, 51460, 51460, 14281, 295, 437, 311, 516, 281, 1051, 294, 5380, 732, 293, 2507, 11, 510, 321, 362, 4498, 5463, 493, 264, 4470, 51744, 51772], "temperature": 0.0, "avg_logprob": -0.07646889971871661, "compression_ratio": 1.7359154929577465, "no_speech_prob": 6.747737188561587e-06}, {"id": 123, "seek": 61424, "start": 636.16, "end": 641.84, "text": " preview of what's going to happen in exercise two and below, here we have fully broken up the loss", "tokens": [50364, 613, 19376, 4190, 11, 570, 510, 294, 5380, 472, 11, 321, 434, 516, 281, 8873, 264, 23897, 1320, 13, 50632, 50632, 407, 321, 434, 516, 281, 8873, 439, 613, 274, 9102, 293, 764, 264, 269, 2455, 2445, 286, 600, 7268, 3673, 50928, 50928, 281, 1520, 527, 3006, 1287, 365, 3104, 281, 437, 9953, 51, 284, 339, 307, 3585, 505, 13, 639, 307, 516, 281, 312, 5380, 51172, 51172, 472, 11, 689, 321, 1333, 295, 646, 48256, 807, 341, 2302, 4295, 13, 823, 11, 445, 281, 976, 291, 257, 588, 1702, 51460, 51460, 14281, 295, 437, 311, 516, 281, 1051, 294, 5380, 732, 293, 2507, 11, 510, 321, 362, 4498, 5463, 493, 264, 4470, 51744, 51772], "temperature": 0.0, "avg_logprob": -0.07646889971871661, "compression_ratio": 1.7359154929577465, "no_speech_prob": 6.747737188561587e-06}, {"id": 124, "seek": 64184, "start": 641.84, "end": 646.8000000000001, "text": " and back propagated through it manually in all the little atomic pieces that make it up.", "tokens": [50364, 293, 646, 12425, 770, 807, 309, 16945, 294, 439, 264, 707, 22275, 3755, 300, 652, 309, 493, 13, 50612, 50644, 583, 510, 321, 434, 516, 281, 15584, 264, 4470, 666, 257, 2167, 3278, 30867, 818, 13, 400, 2602, 321, 434, 516, 50872, 50872, 281, 10783, 984, 28446, 1228, 5221, 293, 3035, 293, 10985, 11, 264, 16235, 295, 264, 4470, 365, 3104, 281, 51220, 51220, 264, 3565, 1208, 13, 400, 2602, 295, 646, 12425, 990, 807, 439, 295, 1080, 707, 24004, 472, 412, 257, 565, 11, 51436, 51436, 321, 434, 445, 516, 281, 10783, 984, 28446, 437, 300, 16235, 307, 13, 400, 321, 434, 516, 281, 4445, 300, 11, 51628, 51628, 597, 307, 709, 544, 7148, 11, 382, 321, 603, 536, 294, 257, 857, 13, 1396, 321, 434, 516, 281, 360, 264, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.15288735848885995, "compression_ratio": 1.9680851063829787, "no_speech_prob": 2.282618333993014e-05}, {"id": 125, "seek": 64184, "start": 647.44, "end": 652.0, "text": " But here we're going to collapse the loss into a single cross entropy call. And instead we're going", "tokens": [50364, 293, 646, 12425, 770, 807, 309, 16945, 294, 439, 264, 707, 22275, 3755, 300, 652, 309, 493, 13, 50612, 50644, 583, 510, 321, 434, 516, 281, 15584, 264, 4470, 666, 257, 2167, 3278, 30867, 818, 13, 400, 2602, 321, 434, 516, 50872, 50872, 281, 10783, 984, 28446, 1228, 5221, 293, 3035, 293, 10985, 11, 264, 16235, 295, 264, 4470, 365, 3104, 281, 51220, 51220, 264, 3565, 1208, 13, 400, 2602, 295, 646, 12425, 990, 807, 439, 295, 1080, 707, 24004, 472, 412, 257, 565, 11, 51436, 51436, 321, 434, 445, 516, 281, 10783, 984, 28446, 437, 300, 16235, 307, 13, 400, 321, 434, 516, 281, 4445, 300, 11, 51628, 51628, 597, 307, 709, 544, 7148, 11, 382, 321, 603, 536, 294, 257, 857, 13, 1396, 321, 434, 516, 281, 360, 264, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.15288735848885995, "compression_ratio": 1.9680851063829787, "no_speech_prob": 2.282618333993014e-05}, {"id": 126, "seek": 64184, "start": 652.0, "end": 658.96, "text": " to analytically derive using math and paper and pencil, the gradient of the loss with respect to", "tokens": [50364, 293, 646, 12425, 770, 807, 309, 16945, 294, 439, 264, 707, 22275, 3755, 300, 652, 309, 493, 13, 50612, 50644, 583, 510, 321, 434, 516, 281, 15584, 264, 4470, 666, 257, 2167, 3278, 30867, 818, 13, 400, 2602, 321, 434, 516, 50872, 50872, 281, 10783, 984, 28446, 1228, 5221, 293, 3035, 293, 10985, 11, 264, 16235, 295, 264, 4470, 365, 3104, 281, 51220, 51220, 264, 3565, 1208, 13, 400, 2602, 295, 646, 12425, 990, 807, 439, 295, 1080, 707, 24004, 472, 412, 257, 565, 11, 51436, 51436, 321, 434, 445, 516, 281, 10783, 984, 28446, 437, 300, 16235, 307, 13, 400, 321, 434, 516, 281, 4445, 300, 11, 51628, 51628, 597, 307, 709, 544, 7148, 11, 382, 321, 603, 536, 294, 257, 857, 13, 1396, 321, 434, 516, 281, 360, 264, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.15288735848885995, "compression_ratio": 1.9680851063829787, "no_speech_prob": 2.282618333993014e-05}, {"id": 127, "seek": 64184, "start": 658.96, "end": 663.2800000000001, "text": " the logits. And instead of back propagating through all of its little chunks one at a time,", "tokens": [50364, 293, 646, 12425, 770, 807, 309, 16945, 294, 439, 264, 707, 22275, 3755, 300, 652, 309, 493, 13, 50612, 50644, 583, 510, 321, 434, 516, 281, 15584, 264, 4470, 666, 257, 2167, 3278, 30867, 818, 13, 400, 2602, 321, 434, 516, 50872, 50872, 281, 10783, 984, 28446, 1228, 5221, 293, 3035, 293, 10985, 11, 264, 16235, 295, 264, 4470, 365, 3104, 281, 51220, 51220, 264, 3565, 1208, 13, 400, 2602, 295, 646, 12425, 990, 807, 439, 295, 1080, 707, 24004, 472, 412, 257, 565, 11, 51436, 51436, 321, 434, 445, 516, 281, 10783, 984, 28446, 437, 300, 16235, 307, 13, 400, 321, 434, 516, 281, 4445, 300, 11, 51628, 51628, 597, 307, 709, 544, 7148, 11, 382, 321, 603, 536, 294, 257, 857, 13, 1396, 321, 434, 516, 281, 360, 264, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.15288735848885995, "compression_ratio": 1.9680851063829787, "no_speech_prob": 2.282618333993014e-05}, {"id": 128, "seek": 64184, "start": 663.2800000000001, "end": 667.12, "text": " we're just going to analytically derive what that gradient is. And we're going to implement that,", "tokens": [50364, 293, 646, 12425, 770, 807, 309, 16945, 294, 439, 264, 707, 22275, 3755, 300, 652, 309, 493, 13, 50612, 50644, 583, 510, 321, 434, 516, 281, 15584, 264, 4470, 666, 257, 2167, 3278, 30867, 818, 13, 400, 2602, 321, 434, 516, 50872, 50872, 281, 10783, 984, 28446, 1228, 5221, 293, 3035, 293, 10985, 11, 264, 16235, 295, 264, 4470, 365, 3104, 281, 51220, 51220, 264, 3565, 1208, 13, 400, 2602, 295, 646, 12425, 990, 807, 439, 295, 1080, 707, 24004, 472, 412, 257, 565, 11, 51436, 51436, 321, 434, 445, 516, 281, 10783, 984, 28446, 437, 300, 16235, 307, 13, 400, 321, 434, 516, 281, 4445, 300, 11, 51628, 51628, 597, 307, 709, 544, 7148, 11, 382, 321, 603, 536, 294, 257, 857, 13, 1396, 321, 434, 516, 281, 360, 264, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.15288735848885995, "compression_ratio": 1.9680851063829787, "no_speech_prob": 2.282618333993014e-05}, {"id": 129, "seek": 64184, "start": 667.12, "end": 671.2800000000001, "text": " which is much more efficient, as we'll see in a bit. Then we're going to do the", "tokens": [50364, 293, 646, 12425, 770, 807, 309, 16945, 294, 439, 264, 707, 22275, 3755, 300, 652, 309, 493, 13, 50612, 50644, 583, 510, 321, 434, 516, 281, 15584, 264, 4470, 666, 257, 2167, 3278, 30867, 818, 13, 400, 2602, 321, 434, 516, 50872, 50872, 281, 10783, 984, 28446, 1228, 5221, 293, 3035, 293, 10985, 11, 264, 16235, 295, 264, 4470, 365, 3104, 281, 51220, 51220, 264, 3565, 1208, 13, 400, 2602, 295, 646, 12425, 990, 807, 439, 295, 1080, 707, 24004, 472, 412, 257, 565, 11, 51436, 51436, 321, 434, 445, 516, 281, 10783, 984, 28446, 437, 300, 16235, 307, 13, 400, 321, 434, 516, 281, 4445, 300, 11, 51628, 51628, 597, 307, 709, 544, 7148, 11, 382, 321, 603, 536, 294, 257, 857, 13, 1396, 321, 434, 516, 281, 360, 264, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.15288735848885995, "compression_ratio": 1.9680851063829787, "no_speech_prob": 2.282618333993014e-05}, {"id": 130, "seek": 67128, "start": 671.28, "end": 676.3199999999999, "text": " exact same thing for batch normalization. So instead of breaking up batch norm into all the", "tokens": [50364, 1900, 912, 551, 337, 15245, 2710, 2144, 13, 407, 2602, 295, 7697, 493, 15245, 2026, 666, 439, 264, 50616, 50616, 5870, 6677, 11, 321, 434, 516, 281, 764, 3435, 293, 3035, 293, 18666, 293, 33400, 281, 28446, 264, 16235, 50932, 50932, 807, 264, 15245, 2026, 4583, 13, 407, 321, 434, 516, 281, 8873, 264, 23897, 1320, 807, 15245, 2026, 4583, 51196, 51196, 294, 257, 709, 544, 7148, 6114, 2602, 295, 23897, 12425, 990, 807, 439, 295, 1080, 707, 51388, 51388, 3755, 21761, 13, 407, 300, 311, 516, 281, 312, 5380, 1045, 13, 400, 550, 294, 5380, 1451, 11, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.12067662033380247, "compression_ratio": 1.8410852713178294, "no_speech_prob": 7.410944363073213e-06}, {"id": 131, "seek": 67128, "start": 676.3199999999999, "end": 682.64, "text": " tiny components, we're going to use pen and paper and mathematics and calculus to derive the gradient", "tokens": [50364, 1900, 912, 551, 337, 15245, 2710, 2144, 13, 407, 2602, 295, 7697, 493, 15245, 2026, 666, 439, 264, 50616, 50616, 5870, 6677, 11, 321, 434, 516, 281, 764, 3435, 293, 3035, 293, 18666, 293, 33400, 281, 28446, 264, 16235, 50932, 50932, 807, 264, 15245, 2026, 4583, 13, 407, 321, 434, 516, 281, 8873, 264, 23897, 1320, 807, 15245, 2026, 4583, 51196, 51196, 294, 257, 709, 544, 7148, 6114, 2602, 295, 23897, 12425, 990, 807, 439, 295, 1080, 707, 51388, 51388, 3755, 21761, 13, 407, 300, 311, 516, 281, 312, 5380, 1045, 13, 400, 550, 294, 5380, 1451, 11, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.12067662033380247, "compression_ratio": 1.8410852713178294, "no_speech_prob": 7.410944363073213e-06}, {"id": 132, "seek": 67128, "start": 682.64, "end": 687.92, "text": " through the batch norm layer. So we're going to calculate the backward pass through batch norm layer", "tokens": [50364, 1900, 912, 551, 337, 15245, 2710, 2144, 13, 407, 2602, 295, 7697, 493, 15245, 2026, 666, 439, 264, 50616, 50616, 5870, 6677, 11, 321, 434, 516, 281, 764, 3435, 293, 3035, 293, 18666, 293, 33400, 281, 28446, 264, 16235, 50932, 50932, 807, 264, 15245, 2026, 4583, 13, 407, 321, 434, 516, 281, 8873, 264, 23897, 1320, 807, 15245, 2026, 4583, 51196, 51196, 294, 257, 709, 544, 7148, 6114, 2602, 295, 23897, 12425, 990, 807, 439, 295, 1080, 707, 51388, 51388, 3755, 21761, 13, 407, 300, 311, 516, 281, 312, 5380, 1045, 13, 400, 550, 294, 5380, 1451, 11, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.12067662033380247, "compression_ratio": 1.8410852713178294, "no_speech_prob": 7.410944363073213e-06}, {"id": 133, "seek": 67128, "start": 687.92, "end": 691.76, "text": " in a much more efficient expression instead of backward propagating through all of its little", "tokens": [50364, 1900, 912, 551, 337, 15245, 2710, 2144, 13, 407, 2602, 295, 7697, 493, 15245, 2026, 666, 439, 264, 50616, 50616, 5870, 6677, 11, 321, 434, 516, 281, 764, 3435, 293, 3035, 293, 18666, 293, 33400, 281, 28446, 264, 16235, 50932, 50932, 807, 264, 15245, 2026, 4583, 13, 407, 321, 434, 516, 281, 8873, 264, 23897, 1320, 807, 15245, 2026, 4583, 51196, 51196, 294, 257, 709, 544, 7148, 6114, 2602, 295, 23897, 12425, 990, 807, 439, 295, 1080, 707, 51388, 51388, 3755, 21761, 13, 407, 300, 311, 516, 281, 312, 5380, 1045, 13, 400, 550, 294, 5380, 1451, 11, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.12067662033380247, "compression_ratio": 1.8410852713178294, "no_speech_prob": 7.410944363073213e-06}, {"id": 134, "seek": 67128, "start": 691.76, "end": 697.4399999999999, "text": " pieces independently. So that's going to be exercise three. And then in exercise four,", "tokens": [50364, 1900, 912, 551, 337, 15245, 2710, 2144, 13, 407, 2602, 295, 7697, 493, 15245, 2026, 666, 439, 264, 50616, 50616, 5870, 6677, 11, 321, 434, 516, 281, 764, 3435, 293, 3035, 293, 18666, 293, 33400, 281, 28446, 264, 16235, 50932, 50932, 807, 264, 15245, 2026, 4583, 13, 407, 321, 434, 516, 281, 8873, 264, 23897, 1320, 807, 15245, 2026, 4583, 51196, 51196, 294, 257, 709, 544, 7148, 6114, 2602, 295, 23897, 12425, 990, 807, 439, 295, 1080, 707, 51388, 51388, 3755, 21761, 13, 407, 300, 311, 516, 281, 312, 5380, 1045, 13, 400, 550, 294, 5380, 1451, 11, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.12067662033380247, "compression_ratio": 1.8410852713178294, "no_speech_prob": 7.410944363073213e-06}, {"id": 135, "seek": 69744, "start": 697.44, "end": 702.4000000000001, "text": " we're going to put it all together. And this is the full code of training this two layer MLP.", "tokens": [50364, 321, 434, 516, 281, 829, 309, 439, 1214, 13, 400, 341, 307, 264, 1577, 3089, 295, 3097, 341, 732, 4583, 21601, 47, 13, 50612, 50612, 400, 321, 434, 516, 281, 1936, 8969, 527, 9688, 646, 2365, 13, 492, 434, 516, 281, 747, 484, 4470, 13, 3207, 1007, 13, 50872, 50872, 400, 291, 486, 1936, 536, 300, 291, 393, 483, 439, 264, 912, 3542, 1228, 4498, 428, 1065, 3089, 13, 400, 51204, 51236, 264, 787, 551, 321, 434, 1228, 490, 9953, 51, 284, 339, 307, 264, 27822, 13, 83, 23153, 281, 652, 264, 20448, 7148, 13, 51512, 51512, 583, 5911, 291, 486, 1223, 4498, 437, 309, 1355, 281, 2128, 293, 23897, 294, 428, 2533, 293, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.12073604939347607, "compression_ratio": 1.7761194029850746, "no_speech_prob": 1.0451236448716372e-05}, {"id": 136, "seek": 69744, "start": 702.4000000000001, "end": 707.6, "text": " And we're going to basically insert our manual back prop. We're going to take out loss.backward.", "tokens": [50364, 321, 434, 516, 281, 829, 309, 439, 1214, 13, 400, 341, 307, 264, 1577, 3089, 295, 3097, 341, 732, 4583, 21601, 47, 13, 50612, 50612, 400, 321, 434, 516, 281, 1936, 8969, 527, 9688, 646, 2365, 13, 492, 434, 516, 281, 747, 484, 4470, 13, 3207, 1007, 13, 50872, 50872, 400, 291, 486, 1936, 536, 300, 291, 393, 483, 439, 264, 912, 3542, 1228, 4498, 428, 1065, 3089, 13, 400, 51204, 51236, 264, 787, 551, 321, 434, 1228, 490, 9953, 51, 284, 339, 307, 264, 27822, 13, 83, 23153, 281, 652, 264, 20448, 7148, 13, 51512, 51512, 583, 5911, 291, 486, 1223, 4498, 437, 309, 1355, 281, 2128, 293, 23897, 294, 428, 2533, 293, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.12073604939347607, "compression_ratio": 1.7761194029850746, "no_speech_prob": 1.0451236448716372e-05}, {"id": 137, "seek": 69744, "start": 707.6, "end": 714.24, "text": " And you will basically see that you can get all the same results using fully your own code. And", "tokens": [50364, 321, 434, 516, 281, 829, 309, 439, 1214, 13, 400, 341, 307, 264, 1577, 3089, 295, 3097, 341, 732, 4583, 21601, 47, 13, 50612, 50612, 400, 321, 434, 516, 281, 1936, 8969, 527, 9688, 646, 2365, 13, 492, 434, 516, 281, 747, 484, 4470, 13, 3207, 1007, 13, 50872, 50872, 400, 291, 486, 1936, 536, 300, 291, 393, 483, 439, 264, 912, 3542, 1228, 4498, 428, 1065, 3089, 13, 400, 51204, 51236, 264, 787, 551, 321, 434, 1228, 490, 9953, 51, 284, 339, 307, 264, 27822, 13, 83, 23153, 281, 652, 264, 20448, 7148, 13, 51512, 51512, 583, 5911, 291, 486, 1223, 4498, 437, 309, 1355, 281, 2128, 293, 23897, 294, 428, 2533, 293, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.12073604939347607, "compression_ratio": 1.7761194029850746, "no_speech_prob": 1.0451236448716372e-05}, {"id": 138, "seek": 69744, "start": 714.8800000000001, "end": 720.4000000000001, "text": " the only thing we're using from PyTorch is the torch.tensor to make the calculations efficient.", "tokens": [50364, 321, 434, 516, 281, 829, 309, 439, 1214, 13, 400, 341, 307, 264, 1577, 3089, 295, 3097, 341, 732, 4583, 21601, 47, 13, 50612, 50612, 400, 321, 434, 516, 281, 1936, 8969, 527, 9688, 646, 2365, 13, 492, 434, 516, 281, 747, 484, 4470, 13, 3207, 1007, 13, 50872, 50872, 400, 291, 486, 1936, 536, 300, 291, 393, 483, 439, 264, 912, 3542, 1228, 4498, 428, 1065, 3089, 13, 400, 51204, 51236, 264, 787, 551, 321, 434, 1228, 490, 9953, 51, 284, 339, 307, 264, 27822, 13, 83, 23153, 281, 652, 264, 20448, 7148, 13, 51512, 51512, 583, 5911, 291, 486, 1223, 4498, 437, 309, 1355, 281, 2128, 293, 23897, 294, 428, 2533, 293, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.12073604939347607, "compression_ratio": 1.7761194029850746, "no_speech_prob": 1.0451236448716372e-05}, {"id": 139, "seek": 69744, "start": 720.4000000000001, "end": 724.5600000000001, "text": " But otherwise you will understand fully what it means to forward and backward in your net and", "tokens": [50364, 321, 434, 516, 281, 829, 309, 439, 1214, 13, 400, 341, 307, 264, 1577, 3089, 295, 3097, 341, 732, 4583, 21601, 47, 13, 50612, 50612, 400, 321, 434, 516, 281, 1936, 8969, 527, 9688, 646, 2365, 13, 492, 434, 516, 281, 747, 484, 4470, 13, 3207, 1007, 13, 50872, 50872, 400, 291, 486, 1936, 536, 300, 291, 393, 483, 439, 264, 912, 3542, 1228, 4498, 428, 1065, 3089, 13, 400, 51204, 51236, 264, 787, 551, 321, 434, 1228, 490, 9953, 51, 284, 339, 307, 264, 27822, 13, 83, 23153, 281, 652, 264, 20448, 7148, 13, 51512, 51512, 583, 5911, 291, 486, 1223, 4498, 437, 309, 1355, 281, 2128, 293, 23897, 294, 428, 2533, 293, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.12073604939347607, "compression_ratio": 1.7761194029850746, "no_speech_prob": 1.0451236448716372e-05}, {"id": 140, "seek": 72456, "start": 724.56, "end": 729.52, "text": " train it. And I think that will be awesome. So let's get to it. Okay. So I ran all the cells", "tokens": [50364, 3847, 309, 13, 400, 286, 519, 300, 486, 312, 3476, 13, 407, 718, 311, 483, 281, 309, 13, 1033, 13, 407, 286, 5872, 439, 264, 5438, 50612, 50612, 295, 341, 21060, 439, 264, 636, 493, 281, 510, 11, 293, 286, 478, 516, 281, 23525, 341, 293, 286, 478, 516, 281, 722, 50876, 50876, 18114, 23897, 1320, 11, 2891, 365, 264, 3565, 26173, 13, 407, 321, 528, 281, 1223, 437, 820, 352, 510, 51144, 51144, 281, 8873, 264, 16235, 295, 264, 4470, 365, 3104, 281, 439, 264, 4959, 295, 264, 3565, 26173, 40863, 13, 51356, 51400, 823, 286, 478, 516, 281, 976, 1314, 264, 1867, 510, 11, 457, 286, 1415, 281, 829, 257, 1702, 3637, 510, 300, 51576, 51604, 286, 519, 486, 312, 881, 5670, 31599, 984, 4420, 337, 291, 307, 281, 767, 352, 666, 264, 3855, 295, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.08561558419085563, "compression_ratio": 1.8214285714285714, "no_speech_prob": 6.339118499454344e-06}, {"id": 141, "seek": 72456, "start": 729.52, "end": 734.8, "text": " of this notebook all the way up to here, and I'm going to erase this and I'm going to start", "tokens": [50364, 3847, 309, 13, 400, 286, 519, 300, 486, 312, 3476, 13, 407, 718, 311, 483, 281, 309, 13, 1033, 13, 407, 286, 5872, 439, 264, 5438, 50612, 50612, 295, 341, 21060, 439, 264, 636, 493, 281, 510, 11, 293, 286, 478, 516, 281, 23525, 341, 293, 286, 478, 516, 281, 722, 50876, 50876, 18114, 23897, 1320, 11, 2891, 365, 264, 3565, 26173, 13, 407, 321, 528, 281, 1223, 437, 820, 352, 510, 51144, 51144, 281, 8873, 264, 16235, 295, 264, 4470, 365, 3104, 281, 439, 264, 4959, 295, 264, 3565, 26173, 40863, 13, 51356, 51400, 823, 286, 478, 516, 281, 976, 1314, 264, 1867, 510, 11, 457, 286, 1415, 281, 829, 257, 1702, 3637, 510, 300, 51576, 51604, 286, 519, 486, 312, 881, 5670, 31599, 984, 4420, 337, 291, 307, 281, 767, 352, 666, 264, 3855, 295, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.08561558419085563, "compression_ratio": 1.8214285714285714, "no_speech_prob": 6.339118499454344e-06}, {"id": 142, "seek": 72456, "start": 734.8, "end": 740.16, "text": " implementing backward pass, starting with the log props. So we want to understand what should go here", "tokens": [50364, 3847, 309, 13, 400, 286, 519, 300, 486, 312, 3476, 13, 407, 718, 311, 483, 281, 309, 13, 1033, 13, 407, 286, 5872, 439, 264, 5438, 50612, 50612, 295, 341, 21060, 439, 264, 636, 493, 281, 510, 11, 293, 286, 478, 516, 281, 23525, 341, 293, 286, 478, 516, 281, 722, 50876, 50876, 18114, 23897, 1320, 11, 2891, 365, 264, 3565, 26173, 13, 407, 321, 528, 281, 1223, 437, 820, 352, 510, 51144, 51144, 281, 8873, 264, 16235, 295, 264, 4470, 365, 3104, 281, 439, 264, 4959, 295, 264, 3565, 26173, 40863, 13, 51356, 51400, 823, 286, 478, 516, 281, 976, 1314, 264, 1867, 510, 11, 457, 286, 1415, 281, 829, 257, 1702, 3637, 510, 300, 51576, 51604, 286, 519, 486, 312, 881, 5670, 31599, 984, 4420, 337, 291, 307, 281, 767, 352, 666, 264, 3855, 295, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.08561558419085563, "compression_ratio": 1.8214285714285714, "no_speech_prob": 6.339118499454344e-06}, {"id": 143, "seek": 72456, "start": 740.16, "end": 744.4, "text": " to calculate the gradient of the loss with respect to all the elements of the log props tensor.", "tokens": [50364, 3847, 309, 13, 400, 286, 519, 300, 486, 312, 3476, 13, 407, 718, 311, 483, 281, 309, 13, 1033, 13, 407, 286, 5872, 439, 264, 5438, 50612, 50612, 295, 341, 21060, 439, 264, 636, 493, 281, 510, 11, 293, 286, 478, 516, 281, 23525, 341, 293, 286, 478, 516, 281, 722, 50876, 50876, 18114, 23897, 1320, 11, 2891, 365, 264, 3565, 26173, 13, 407, 321, 528, 281, 1223, 437, 820, 352, 510, 51144, 51144, 281, 8873, 264, 16235, 295, 264, 4470, 365, 3104, 281, 439, 264, 4959, 295, 264, 3565, 26173, 40863, 13, 51356, 51400, 823, 286, 478, 516, 281, 976, 1314, 264, 1867, 510, 11, 457, 286, 1415, 281, 829, 257, 1702, 3637, 510, 300, 51576, 51604, 286, 519, 486, 312, 881, 5670, 31599, 984, 4420, 337, 291, 307, 281, 767, 352, 666, 264, 3855, 295, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.08561558419085563, "compression_ratio": 1.8214285714285714, "no_speech_prob": 6.339118499454344e-06}, {"id": 144, "seek": 72456, "start": 745.28, "end": 748.8, "text": " Now I'm going to give away the answer here, but I wanted to put a quick note here that", "tokens": [50364, 3847, 309, 13, 400, 286, 519, 300, 486, 312, 3476, 13, 407, 718, 311, 483, 281, 309, 13, 1033, 13, 407, 286, 5872, 439, 264, 5438, 50612, 50612, 295, 341, 21060, 439, 264, 636, 493, 281, 510, 11, 293, 286, 478, 516, 281, 23525, 341, 293, 286, 478, 516, 281, 722, 50876, 50876, 18114, 23897, 1320, 11, 2891, 365, 264, 3565, 26173, 13, 407, 321, 528, 281, 1223, 437, 820, 352, 510, 51144, 51144, 281, 8873, 264, 16235, 295, 264, 4470, 365, 3104, 281, 439, 264, 4959, 295, 264, 3565, 26173, 40863, 13, 51356, 51400, 823, 286, 478, 516, 281, 976, 1314, 264, 1867, 510, 11, 457, 286, 1415, 281, 829, 257, 1702, 3637, 510, 300, 51576, 51604, 286, 519, 486, 312, 881, 5670, 31599, 984, 4420, 337, 291, 307, 281, 767, 352, 666, 264, 3855, 295, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.08561558419085563, "compression_ratio": 1.8214285714285714, "no_speech_prob": 6.339118499454344e-06}, {"id": 145, "seek": 72456, "start": 749.3599999999999, "end": 754.2399999999999, "text": " I think will be most pedagogically useful for you is to actually go into the description of", "tokens": [50364, 3847, 309, 13, 400, 286, 519, 300, 486, 312, 3476, 13, 407, 718, 311, 483, 281, 309, 13, 1033, 13, 407, 286, 5872, 439, 264, 5438, 50612, 50612, 295, 341, 21060, 439, 264, 636, 493, 281, 510, 11, 293, 286, 478, 516, 281, 23525, 341, 293, 286, 478, 516, 281, 722, 50876, 50876, 18114, 23897, 1320, 11, 2891, 365, 264, 3565, 26173, 13, 407, 321, 528, 281, 1223, 437, 820, 352, 510, 51144, 51144, 281, 8873, 264, 16235, 295, 264, 4470, 365, 3104, 281, 439, 264, 4959, 295, 264, 3565, 26173, 40863, 13, 51356, 51400, 823, 286, 478, 516, 281, 976, 1314, 264, 1867, 510, 11, 457, 286, 1415, 281, 829, 257, 1702, 3637, 510, 300, 51576, 51604, 286, 519, 486, 312, 881, 5670, 31599, 984, 4420, 337, 291, 307, 281, 767, 352, 666, 264, 3855, 295, 51848, 51848], "temperature": 0.0, "avg_logprob": -0.08561558419085563, "compression_ratio": 1.8214285714285714, "no_speech_prob": 6.339118499454344e-06}, {"id": 146, "seek": 75424, "start": 754.24, "end": 758.4, "text": " this video and find the link to this Jupyter notebook. You can find it both on GitHub,", "tokens": [50364, 341, 960, 293, 915, 264, 2113, 281, 341, 22125, 88, 391, 21060, 13, 509, 393, 915, 309, 1293, 322, 23331, 11, 50572, 50572, 457, 291, 393, 611, 915, 3329, 4004, 455, 365, 309, 13, 407, 291, 500, 380, 362, 281, 3625, 1340, 13, 509, 603, 50732, 50732, 445, 352, 281, 257, 3144, 322, 3329, 4004, 455, 293, 291, 393, 853, 281, 4445, 613, 33733, 420, 2771, 2448, 51024, 51024, 1803, 13, 400, 550, 498, 291, 366, 406, 1075, 281, 808, 281, 452, 960, 293, 536, 385, 360, 309, 13, 400, 370, 589, 294, 48120, 51336, 51336, 293, 853, 309, 700, 1803, 293, 550, 536, 385, 976, 1314, 264, 1867, 13, 400, 286, 519, 300, 576, 312, 881, 51572, 51572, 8263, 281, 291, 13, 400, 300, 311, 577, 286, 2748, 291, 352, 807, 341, 7991, 13, 407, 321, 366, 2891, 510, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07379436492919922, "compression_ratio": 1.7680250783699059, "no_speech_prob": 1.2804051948478445e-05}, {"id": 147, "seek": 75424, "start": 758.4, "end": 761.6, "text": " but you can also find Google Colab with it. So you don't have to install anything. You'll", "tokens": [50364, 341, 960, 293, 915, 264, 2113, 281, 341, 22125, 88, 391, 21060, 13, 509, 393, 915, 309, 1293, 322, 23331, 11, 50572, 50572, 457, 291, 393, 611, 915, 3329, 4004, 455, 365, 309, 13, 407, 291, 500, 380, 362, 281, 3625, 1340, 13, 509, 603, 50732, 50732, 445, 352, 281, 257, 3144, 322, 3329, 4004, 455, 293, 291, 393, 853, 281, 4445, 613, 33733, 420, 2771, 2448, 51024, 51024, 1803, 13, 400, 550, 498, 291, 366, 406, 1075, 281, 808, 281, 452, 960, 293, 536, 385, 360, 309, 13, 400, 370, 589, 294, 48120, 51336, 51336, 293, 853, 309, 700, 1803, 293, 550, 536, 385, 976, 1314, 264, 1867, 13, 400, 286, 519, 300, 576, 312, 881, 51572, 51572, 8263, 281, 291, 13, 400, 300, 311, 577, 286, 2748, 291, 352, 807, 341, 7991, 13, 407, 321, 366, 2891, 510, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07379436492919922, "compression_ratio": 1.7680250783699059, "no_speech_prob": 1.2804051948478445e-05}, {"id": 148, "seek": 75424, "start": 761.6, "end": 767.44, "text": " just go to a website on Google Colab and you can try to implement these derivatives or gradients", "tokens": [50364, 341, 960, 293, 915, 264, 2113, 281, 341, 22125, 88, 391, 21060, 13, 509, 393, 915, 309, 1293, 322, 23331, 11, 50572, 50572, 457, 291, 393, 611, 915, 3329, 4004, 455, 365, 309, 13, 407, 291, 500, 380, 362, 281, 3625, 1340, 13, 509, 603, 50732, 50732, 445, 352, 281, 257, 3144, 322, 3329, 4004, 455, 293, 291, 393, 853, 281, 4445, 613, 33733, 420, 2771, 2448, 51024, 51024, 1803, 13, 400, 550, 498, 291, 366, 406, 1075, 281, 808, 281, 452, 960, 293, 536, 385, 360, 309, 13, 400, 370, 589, 294, 48120, 51336, 51336, 293, 853, 309, 700, 1803, 293, 550, 536, 385, 976, 1314, 264, 1867, 13, 400, 286, 519, 300, 576, 312, 881, 51572, 51572, 8263, 281, 291, 13, 400, 300, 311, 577, 286, 2748, 291, 352, 807, 341, 7991, 13, 407, 321, 366, 2891, 510, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07379436492919922, "compression_ratio": 1.7680250783699059, "no_speech_prob": 1.2804051948478445e-05}, {"id": 149, "seek": 75424, "start": 767.44, "end": 773.6800000000001, "text": " yourself. And then if you are not able to come to my video and see me do it. And so work in tandem", "tokens": [50364, 341, 960, 293, 915, 264, 2113, 281, 341, 22125, 88, 391, 21060, 13, 509, 393, 915, 309, 1293, 322, 23331, 11, 50572, 50572, 457, 291, 393, 611, 915, 3329, 4004, 455, 365, 309, 13, 407, 291, 500, 380, 362, 281, 3625, 1340, 13, 509, 603, 50732, 50732, 445, 352, 281, 257, 3144, 322, 3329, 4004, 455, 293, 291, 393, 853, 281, 4445, 613, 33733, 420, 2771, 2448, 51024, 51024, 1803, 13, 400, 550, 498, 291, 366, 406, 1075, 281, 808, 281, 452, 960, 293, 536, 385, 360, 309, 13, 400, 370, 589, 294, 48120, 51336, 51336, 293, 853, 309, 700, 1803, 293, 550, 536, 385, 976, 1314, 264, 1867, 13, 400, 286, 519, 300, 576, 312, 881, 51572, 51572, 8263, 281, 291, 13, 400, 300, 311, 577, 286, 2748, 291, 352, 807, 341, 7991, 13, 407, 321, 366, 2891, 510, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07379436492919922, "compression_ratio": 1.7680250783699059, "no_speech_prob": 1.2804051948478445e-05}, {"id": 150, "seek": 75424, "start": 773.6800000000001, "end": 778.4, "text": " and try it first yourself and then see me give away the answer. And I think that would be most", "tokens": [50364, 341, 960, 293, 915, 264, 2113, 281, 341, 22125, 88, 391, 21060, 13, 509, 393, 915, 309, 1293, 322, 23331, 11, 50572, 50572, 457, 291, 393, 611, 915, 3329, 4004, 455, 365, 309, 13, 407, 291, 500, 380, 362, 281, 3625, 1340, 13, 509, 603, 50732, 50732, 445, 352, 281, 257, 3144, 322, 3329, 4004, 455, 293, 291, 393, 853, 281, 4445, 613, 33733, 420, 2771, 2448, 51024, 51024, 1803, 13, 400, 550, 498, 291, 366, 406, 1075, 281, 808, 281, 452, 960, 293, 536, 385, 360, 309, 13, 400, 370, 589, 294, 48120, 51336, 51336, 293, 853, 309, 700, 1803, 293, 550, 536, 385, 976, 1314, 264, 1867, 13, 400, 286, 519, 300, 576, 312, 881, 51572, 51572, 8263, 281, 291, 13, 400, 300, 311, 577, 286, 2748, 291, 352, 807, 341, 7991, 13, 407, 321, 366, 2891, 510, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07379436492919922, "compression_ratio": 1.7680250783699059, "no_speech_prob": 1.2804051948478445e-05}, {"id": 151, "seek": 75424, "start": 778.4, "end": 782.24, "text": " valuable to you. And that's how I recommend you go through this lecture. So we are starting here", "tokens": [50364, 341, 960, 293, 915, 264, 2113, 281, 341, 22125, 88, 391, 21060, 13, 509, 393, 915, 309, 1293, 322, 23331, 11, 50572, 50572, 457, 291, 393, 611, 915, 3329, 4004, 455, 365, 309, 13, 407, 291, 500, 380, 362, 281, 3625, 1340, 13, 509, 603, 50732, 50732, 445, 352, 281, 257, 3144, 322, 3329, 4004, 455, 293, 291, 393, 853, 281, 4445, 613, 33733, 420, 2771, 2448, 51024, 51024, 1803, 13, 400, 550, 498, 291, 366, 406, 1075, 281, 808, 281, 452, 960, 293, 536, 385, 360, 309, 13, 400, 370, 589, 294, 48120, 51336, 51336, 293, 853, 309, 700, 1803, 293, 550, 536, 385, 976, 1314, 264, 1867, 13, 400, 286, 519, 300, 576, 312, 881, 51572, 51572, 8263, 281, 291, 13, 400, 300, 311, 577, 286, 2748, 291, 352, 807, 341, 7991, 13, 407, 321, 366, 2891, 510, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07379436492919922, "compression_ratio": 1.7680250783699059, "no_speech_prob": 1.2804051948478445e-05}, {"id": 152, "seek": 78224, "start": 782.24, "end": 788.8, "text": " with D log props. Now D log props will hold the derivative of the loss with respect to all the", "tokens": [50364, 365, 413, 3565, 26173, 13, 823, 413, 3565, 26173, 486, 1797, 264, 13760, 295, 264, 4470, 365, 3104, 281, 439, 264, 50692, 50692, 4959, 295, 3565, 26173, 13, 708, 307, 1854, 3565, 26173, 30, 440, 3909, 295, 341, 307, 8858, 538, 7634, 13, 407, 309, 311, 406, 516, 51148, 51148, 281, 6365, 291, 300, 413, 3565, 26173, 820, 611, 312, 364, 10225, 295, 2744, 8858, 538, 7634, 570, 321, 528, 264, 51392, 51392, 13760, 4470, 365, 3104, 281, 439, 295, 1080, 4959, 13, 407, 264, 11602, 295, 729, 366, 1009, 516, 281, 312, 2681, 13, 51644, 51736], "temperature": 0.0, "avg_logprob": -0.07569887662174726, "compression_ratio": 1.8186046511627907, "no_speech_prob": 1.3419327842711937e-05}, {"id": 153, "seek": 78224, "start": 788.8, "end": 797.92, "text": " elements of log props. What is inside log props? The shape of this is 32 by 27. So it's not going", "tokens": [50364, 365, 413, 3565, 26173, 13, 823, 413, 3565, 26173, 486, 1797, 264, 13760, 295, 264, 4470, 365, 3104, 281, 439, 264, 50692, 50692, 4959, 295, 3565, 26173, 13, 708, 307, 1854, 3565, 26173, 30, 440, 3909, 295, 341, 307, 8858, 538, 7634, 13, 407, 309, 311, 406, 516, 51148, 51148, 281, 6365, 291, 300, 413, 3565, 26173, 820, 611, 312, 364, 10225, 295, 2744, 8858, 538, 7634, 570, 321, 528, 264, 51392, 51392, 13760, 4470, 365, 3104, 281, 439, 295, 1080, 4959, 13, 407, 264, 11602, 295, 729, 366, 1009, 516, 281, 312, 2681, 13, 51644, 51736], "temperature": 0.0, "avg_logprob": -0.07569887662174726, "compression_ratio": 1.8186046511627907, "no_speech_prob": 1.3419327842711937e-05}, {"id": 154, "seek": 78224, "start": 797.92, "end": 802.8, "text": " to surprise you that D log props should also be an array of size 32 by 27 because we want the", "tokens": [50364, 365, 413, 3565, 26173, 13, 823, 413, 3565, 26173, 486, 1797, 264, 13760, 295, 264, 4470, 365, 3104, 281, 439, 264, 50692, 50692, 4959, 295, 3565, 26173, 13, 708, 307, 1854, 3565, 26173, 30, 440, 3909, 295, 341, 307, 8858, 538, 7634, 13, 407, 309, 311, 406, 516, 51148, 51148, 281, 6365, 291, 300, 413, 3565, 26173, 820, 611, 312, 364, 10225, 295, 2744, 8858, 538, 7634, 570, 321, 528, 264, 51392, 51392, 13760, 4470, 365, 3104, 281, 439, 295, 1080, 4959, 13, 407, 264, 11602, 295, 729, 366, 1009, 516, 281, 312, 2681, 13, 51644, 51736], "temperature": 0.0, "avg_logprob": -0.07569887662174726, "compression_ratio": 1.8186046511627907, "no_speech_prob": 1.3419327842711937e-05}, {"id": 155, "seek": 78224, "start": 802.8, "end": 807.84, "text": " derivative loss with respect to all of its elements. So the sizes of those are always going to be equal.", "tokens": [50364, 365, 413, 3565, 26173, 13, 823, 413, 3565, 26173, 486, 1797, 264, 13760, 295, 264, 4470, 365, 3104, 281, 439, 264, 50692, 50692, 4959, 295, 3565, 26173, 13, 708, 307, 1854, 3565, 26173, 30, 440, 3909, 295, 341, 307, 8858, 538, 7634, 13, 407, 309, 311, 406, 516, 51148, 51148, 281, 6365, 291, 300, 413, 3565, 26173, 820, 611, 312, 364, 10225, 295, 2744, 8858, 538, 7634, 570, 321, 528, 264, 51392, 51392, 13760, 4470, 365, 3104, 281, 439, 295, 1080, 4959, 13, 407, 264, 11602, 295, 729, 366, 1009, 516, 281, 312, 2681, 13, 51644, 51736], "temperature": 0.0, "avg_logprob": -0.07569887662174726, "compression_ratio": 1.8186046511627907, "no_speech_prob": 1.3419327842711937e-05}, {"id": 156, "seek": 80784, "start": 807.84, "end": 817.0400000000001, "text": " Now, how does log props influence the loss? Okay. Loss is negative log props indexed with range of", "tokens": [50364, 823, 11, 577, 775, 3565, 26173, 6503, 264, 4470, 30, 1033, 13, 441, 772, 307, 3671, 3565, 26173, 8186, 292, 365, 3613, 295, 50824, 50824, 426, 293, 398, 33, 293, 550, 264, 914, 295, 300, 13, 823, 11, 445, 382, 257, 13548, 11, 398, 33, 307, 445, 1936, 364, 10225, 295, 439, 264, 51288, 51332, 3006, 43840, 13, 407, 437, 321, 434, 884, 510, 307, 321, 434, 1940, 264, 3565, 26173, 10225, 295, 2744, 8858, 538, 7634, 13, 51644, 51760], "temperature": 0.0, "avg_logprob": -0.20805970633902202, "compression_ratio": 1.4874371859296482, "no_speech_prob": 2.190728537243558e-06}, {"id": 157, "seek": 80784, "start": 817.0400000000001, "end": 826.32, "text": " N and YB and then the mean of that. Now, just as a reminder, YB is just basically an array of all the", "tokens": [50364, 823, 11, 577, 775, 3565, 26173, 6503, 264, 4470, 30, 1033, 13, 441, 772, 307, 3671, 3565, 26173, 8186, 292, 365, 3613, 295, 50824, 50824, 426, 293, 398, 33, 293, 550, 264, 914, 295, 300, 13, 823, 11, 445, 382, 257, 13548, 11, 398, 33, 307, 445, 1936, 364, 10225, 295, 439, 264, 51288, 51332, 3006, 43840, 13, 407, 437, 321, 434, 884, 510, 307, 321, 434, 1940, 264, 3565, 26173, 10225, 295, 2744, 8858, 538, 7634, 13, 51644, 51760], "temperature": 0.0, "avg_logprob": -0.20805970633902202, "compression_ratio": 1.4874371859296482, "no_speech_prob": 2.190728537243558e-06}, {"id": 158, "seek": 80784, "start": 827.2, "end": 833.44, "text": " correct indices. So what we're doing here is we're taking the log props array of size 32 by 27.", "tokens": [50364, 823, 11, 577, 775, 3565, 26173, 6503, 264, 4470, 30, 1033, 13, 441, 772, 307, 3671, 3565, 26173, 8186, 292, 365, 3613, 295, 50824, 50824, 426, 293, 398, 33, 293, 550, 264, 914, 295, 300, 13, 823, 11, 445, 382, 257, 13548, 11, 398, 33, 307, 445, 1936, 364, 10225, 295, 439, 264, 51288, 51332, 3006, 43840, 13, 407, 437, 321, 434, 884, 510, 307, 321, 434, 1940, 264, 3565, 26173, 10225, 295, 2744, 8858, 538, 7634, 13, 51644, 51760], "temperature": 0.0, "avg_logprob": -0.20805970633902202, "compression_ratio": 1.4874371859296482, "no_speech_prob": 2.190728537243558e-06}, {"id": 159, "seek": 83344, "start": 833.44, "end": 842.32, "text": " Right. And then we are going in every single row and in each row, we are plugging out the index", "tokens": [50364, 1779, 13, 400, 550, 321, 366, 516, 294, 633, 2167, 5386, 293, 294, 1184, 5386, 11, 321, 366, 42975, 484, 264, 8186, 50808, 50808, 3180, 293, 550, 3499, 293, 2119, 293, 370, 322, 13, 407, 321, 434, 516, 760, 264, 13241, 13, 663, 311, 264, 17138, 1639, 3613, 295, 426, 13, 51056, 51084, 400, 550, 321, 366, 1009, 42975, 484, 264, 8186, 412, 264, 7738, 22206, 538, 341, 40863, 398, 33, 13, 407, 294, 264, 51376, 51376, 1958, 392, 5386, 11, 321, 366, 1940, 264, 19495, 7738, 13, 682, 264, 700, 5386, 11, 321, 434, 1940, 264, 3499, 392, 7738, 11, 5183, 13, 400, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.19040943075109412, "compression_ratio": 1.8186046511627907, "no_speech_prob": 2.9943489607830998e-06}, {"id": 160, "seek": 83344, "start": 842.32, "end": 847.2800000000001, "text": " eight and then 14 and 15 and so on. So we're going down the rows. That's the iterator range of N.", "tokens": [50364, 1779, 13, 400, 550, 321, 366, 516, 294, 633, 2167, 5386, 293, 294, 1184, 5386, 11, 321, 366, 42975, 484, 264, 8186, 50808, 50808, 3180, 293, 550, 3499, 293, 2119, 293, 370, 322, 13, 407, 321, 434, 516, 760, 264, 13241, 13, 663, 311, 264, 17138, 1639, 3613, 295, 426, 13, 51056, 51084, 400, 550, 321, 366, 1009, 42975, 484, 264, 8186, 412, 264, 7738, 22206, 538, 341, 40863, 398, 33, 13, 407, 294, 264, 51376, 51376, 1958, 392, 5386, 11, 321, 366, 1940, 264, 19495, 7738, 13, 682, 264, 700, 5386, 11, 321, 434, 1940, 264, 3499, 392, 7738, 11, 5183, 13, 400, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.19040943075109412, "compression_ratio": 1.8186046511627907, "no_speech_prob": 2.9943489607830998e-06}, {"id": 161, "seek": 83344, "start": 847.84, "end": 853.6800000000001, "text": " And then we are always plugging out the index at the column specified by this tensor YB. So in the", "tokens": [50364, 1779, 13, 400, 550, 321, 366, 516, 294, 633, 2167, 5386, 293, 294, 1184, 5386, 11, 321, 366, 42975, 484, 264, 8186, 50808, 50808, 3180, 293, 550, 3499, 293, 2119, 293, 370, 322, 13, 407, 321, 434, 516, 760, 264, 13241, 13, 663, 311, 264, 17138, 1639, 3613, 295, 426, 13, 51056, 51084, 400, 550, 321, 366, 1009, 42975, 484, 264, 8186, 412, 264, 7738, 22206, 538, 341, 40863, 398, 33, 13, 407, 294, 264, 51376, 51376, 1958, 392, 5386, 11, 321, 366, 1940, 264, 19495, 7738, 13, 682, 264, 700, 5386, 11, 321, 434, 1940, 264, 3499, 392, 7738, 11, 5183, 13, 400, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.19040943075109412, "compression_ratio": 1.8186046511627907, "no_speech_prob": 2.9943489607830998e-06}, {"id": 162, "seek": 83344, "start": 853.6800000000001, "end": 860.08, "text": " 0th row, we are taking the eighth column. In the first row, we're taking the 14th column, etc. And", "tokens": [50364, 1779, 13, 400, 550, 321, 366, 516, 294, 633, 2167, 5386, 293, 294, 1184, 5386, 11, 321, 366, 42975, 484, 264, 8186, 50808, 50808, 3180, 293, 550, 3499, 293, 2119, 293, 370, 322, 13, 407, 321, 434, 516, 760, 264, 13241, 13, 663, 311, 264, 17138, 1639, 3613, 295, 426, 13, 51056, 51084, 400, 550, 321, 366, 1009, 42975, 484, 264, 8186, 412, 264, 7738, 22206, 538, 341, 40863, 398, 33, 13, 407, 294, 264, 51376, 51376, 1958, 392, 5386, 11, 321, 366, 1940, 264, 19495, 7738, 13, 682, 264, 700, 5386, 11, 321, 434, 1940, 264, 3499, 392, 7738, 11, 5183, 13, 400, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.19040943075109412, "compression_ratio": 1.8186046511627907, "no_speech_prob": 2.9943489607830998e-06}, {"id": 163, "seek": 86008, "start": 860.08, "end": 868.4000000000001, "text": " so log props at this plucks out all those log probabilities of the correct next character in a", "tokens": [50364, 370, 3565, 26173, 412, 341, 499, 15493, 484, 439, 729, 3565, 33783, 295, 264, 3006, 958, 2517, 294, 257, 50780, 50780, 8310, 13, 407, 300, 311, 437, 300, 775, 13, 400, 264, 3909, 295, 341, 420, 264, 2744, 295, 309, 307, 295, 1164, 8858, 570, 51080, 51080, 527, 15245, 2744, 307, 8858, 13, 407, 613, 4959, 483, 41514, 292, 484, 293, 550, 641, 914, 293, 264, 3671, 295, 300, 51484, 51484, 3643, 4470, 13, 407, 286, 1009, 411, 281, 589, 365, 18587, 5110, 295, 577, 281, 360, 300, 13, 407, 718, 311, 584, 286, 362, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.26591774911591504, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.425444330991013e-06}, {"id": 164, "seek": 86008, "start": 868.4000000000001, "end": 874.4000000000001, "text": " sequence. So that's what that does. And the shape of this or the size of it is of course 32 because", "tokens": [50364, 370, 3565, 26173, 412, 341, 499, 15493, 484, 439, 729, 3565, 33783, 295, 264, 3006, 958, 2517, 294, 257, 50780, 50780, 8310, 13, 407, 300, 311, 437, 300, 775, 13, 400, 264, 3909, 295, 341, 420, 264, 2744, 295, 309, 307, 295, 1164, 8858, 570, 51080, 51080, 527, 15245, 2744, 307, 8858, 13, 407, 613, 4959, 483, 41514, 292, 484, 293, 550, 641, 914, 293, 264, 3671, 295, 300, 51484, 51484, 3643, 4470, 13, 407, 286, 1009, 411, 281, 589, 365, 18587, 5110, 295, 577, 281, 360, 300, 13, 407, 718, 311, 584, 286, 362, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.26591774911591504, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.425444330991013e-06}, {"id": 165, "seek": 86008, "start": 874.4000000000001, "end": 882.48, "text": " our batch size is 32. So these elements get plucked out and then their mean and the negative of that", "tokens": [50364, 370, 3565, 26173, 412, 341, 499, 15493, 484, 439, 729, 3565, 33783, 295, 264, 3006, 958, 2517, 294, 257, 50780, 50780, 8310, 13, 407, 300, 311, 437, 300, 775, 13, 400, 264, 3909, 295, 341, 420, 264, 2744, 295, 309, 307, 295, 1164, 8858, 570, 51080, 51080, 527, 15245, 2744, 307, 8858, 13, 407, 613, 4959, 483, 41514, 292, 484, 293, 550, 641, 914, 293, 264, 3671, 295, 300, 51484, 51484, 3643, 4470, 13, 407, 286, 1009, 411, 281, 589, 365, 18587, 5110, 295, 577, 281, 360, 300, 13, 407, 718, 311, 584, 286, 362, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.26591774911591504, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.425444330991013e-06}, {"id": 166, "seek": 86008, "start": 882.48, "end": 890.0, "text": " becomes loss. So I always like to work with simpler examples of how to do that. So let's say I have", "tokens": [50364, 370, 3565, 26173, 412, 341, 499, 15493, 484, 439, 729, 3565, 33783, 295, 264, 3006, 958, 2517, 294, 257, 50780, 50780, 8310, 13, 407, 300, 311, 437, 300, 775, 13, 400, 264, 3909, 295, 341, 420, 264, 2744, 295, 309, 307, 295, 1164, 8858, 570, 51080, 51080, 527, 15245, 2744, 307, 8858, 13, 407, 613, 4959, 483, 41514, 292, 484, 293, 550, 641, 914, 293, 264, 3671, 295, 300, 51484, 51484, 3643, 4470, 13, 407, 286, 1009, 411, 281, 589, 365, 18587, 5110, 295, 577, 281, 360, 300, 13, 407, 718, 311, 584, 286, 362, 51860, 51860], "temperature": 0.0, "avg_logprob": -0.26591774911591504, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.425444330991013e-06}, {"id": 167, "seek": 89000, "start": 890.0, "end": 896.48, "text": " three examples to understand the numerical form of derivative. What's going on here is once we've", "tokens": [50364, 1045, 5110, 281, 1223, 264, 29054, 1254, 295, 13760, 13, 708, 311, 516, 322, 510, 307, 1564, 321, 600, 50688, 50688, 41514, 292, 484, 613, 5110, 11, 321, 434, 1940, 264, 914, 293, 550, 264, 3671, 13, 407, 264, 4470, 1936, 11, 50996, 51048, 286, 393, 2464, 309, 341, 636, 11, 307, 264, 3671, 295, 584, 316, 1804, 363, 1804, 383, 13, 400, 264, 914, 295, 729, 1045, 51344, 51344, 3547, 576, 312, 584, 3671, 11, 576, 9845, 1045, 13, 663, 576, 312, 577, 321, 4584, 264, 914, 295, 1045, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.19356613970817405, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.3736676414264366e-05}, {"id": 168, "seek": 89000, "start": 896.48, "end": 902.64, "text": " plucked out these examples, we're taking the mean and then the negative. So the loss basically,", "tokens": [50364, 1045, 5110, 281, 1223, 264, 29054, 1254, 295, 13760, 13, 708, 311, 516, 322, 510, 307, 1564, 321, 600, 50688, 50688, 41514, 292, 484, 613, 5110, 11, 321, 434, 1940, 264, 914, 293, 550, 264, 3671, 13, 407, 264, 4470, 1936, 11, 50996, 51048, 286, 393, 2464, 309, 341, 636, 11, 307, 264, 3671, 295, 584, 316, 1804, 363, 1804, 383, 13, 400, 264, 914, 295, 729, 1045, 51344, 51344, 3547, 576, 312, 584, 3671, 11, 576, 9845, 1045, 13, 663, 576, 312, 577, 321, 4584, 264, 914, 295, 1045, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.19356613970817405, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.3736676414264366e-05}, {"id": 169, "seek": 89000, "start": 903.68, "end": 909.6, "text": " I can write it this way, is the negative of say A plus B plus C. And the mean of those three", "tokens": [50364, 1045, 5110, 281, 1223, 264, 29054, 1254, 295, 13760, 13, 708, 311, 516, 322, 510, 307, 1564, 321, 600, 50688, 50688, 41514, 292, 484, 613, 5110, 11, 321, 434, 1940, 264, 914, 293, 550, 264, 3671, 13, 407, 264, 4470, 1936, 11, 50996, 51048, 286, 393, 2464, 309, 341, 636, 11, 307, 264, 3671, 295, 584, 316, 1804, 363, 1804, 383, 13, 400, 264, 914, 295, 729, 1045, 51344, 51344, 3547, 576, 312, 584, 3671, 11, 576, 9845, 1045, 13, 663, 576, 312, 577, 321, 4584, 264, 914, 295, 1045, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.19356613970817405, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.3736676414264366e-05}, {"id": 170, "seek": 89000, "start": 909.6, "end": 914.0, "text": " numbers would be say negative, would divide three. That would be how we achieve the mean of three", "tokens": [50364, 1045, 5110, 281, 1223, 264, 29054, 1254, 295, 13760, 13, 708, 311, 516, 322, 510, 307, 1564, 321, 600, 50688, 50688, 41514, 292, 484, 613, 5110, 11, 321, 434, 1940, 264, 914, 293, 550, 264, 3671, 13, 407, 264, 4470, 1936, 11, 50996, 51048, 286, 393, 2464, 309, 341, 636, 11, 307, 264, 3671, 295, 584, 316, 1804, 363, 1804, 383, 13, 400, 264, 914, 295, 729, 1045, 51344, 51344, 3547, 576, 312, 584, 3671, 11, 576, 9845, 1045, 13, 663, 576, 312, 577, 321, 4584, 264, 914, 295, 1045, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.19356613970817405, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.3736676414264366e-05}, {"id": 171, "seek": 91400, "start": 914.0, "end": 921.44, "text": " B C, although we actually have 32 numbers here. And so what is basically the loss by say like D A,", "tokens": [50364, 363, 383, 11, 4878, 321, 767, 362, 8858, 3547, 510, 13, 400, 370, 437, 307, 1936, 264, 4470, 538, 584, 411, 413, 316, 11, 50736, 50736, 558, 30, 1042, 11, 498, 321, 20460, 341, 6114, 44003, 11, 341, 307, 3671, 472, 670, 1045, 50980, 50980, 295, 316, 293, 3671, 1804, 3671, 472, 670, 1045, 295, 363, 1804, 3671, 472, 670, 1045, 295, 383, 13, 400, 370, 437, 51344, 51344, 307, 413, 4470, 538, 413, 316, 30, 467, 311, 445, 3671, 472, 670, 1045, 13, 400, 370, 291, 393, 536, 300, 498, 321, 500, 380, 445, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.11217651945171934, "compression_ratio": 1.8774509803921569, "no_speech_prob": 7.2962220656336285e-06}, {"id": 172, "seek": 91400, "start": 921.44, "end": 926.32, "text": " right? Well, if we simplify this expression mathematically, this is negative one over three", "tokens": [50364, 363, 383, 11, 4878, 321, 767, 362, 8858, 3547, 510, 13, 400, 370, 437, 307, 1936, 264, 4470, 538, 584, 411, 413, 316, 11, 50736, 50736, 558, 30, 1042, 11, 498, 321, 20460, 341, 6114, 44003, 11, 341, 307, 3671, 472, 670, 1045, 50980, 50980, 295, 316, 293, 3671, 1804, 3671, 472, 670, 1045, 295, 363, 1804, 3671, 472, 670, 1045, 295, 383, 13, 400, 370, 437, 51344, 51344, 307, 413, 4470, 538, 413, 316, 30, 467, 311, 445, 3671, 472, 670, 1045, 13, 400, 370, 291, 393, 536, 300, 498, 321, 500, 380, 445, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.11217651945171934, "compression_ratio": 1.8774509803921569, "no_speech_prob": 7.2962220656336285e-06}, {"id": 173, "seek": 91400, "start": 926.32, "end": 933.6, "text": " of A and negative plus negative one over three of B plus negative one over three of C. And so what", "tokens": [50364, 363, 383, 11, 4878, 321, 767, 362, 8858, 3547, 510, 13, 400, 370, 437, 307, 1936, 264, 4470, 538, 584, 411, 413, 316, 11, 50736, 50736, 558, 30, 1042, 11, 498, 321, 20460, 341, 6114, 44003, 11, 341, 307, 3671, 472, 670, 1045, 50980, 50980, 295, 316, 293, 3671, 1804, 3671, 472, 670, 1045, 295, 363, 1804, 3671, 472, 670, 1045, 295, 383, 13, 400, 370, 437, 51344, 51344, 307, 413, 4470, 538, 413, 316, 30, 467, 311, 445, 3671, 472, 670, 1045, 13, 400, 370, 291, 393, 536, 300, 498, 321, 500, 380, 445, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.11217651945171934, "compression_ratio": 1.8774509803921569, "no_speech_prob": 7.2962220656336285e-06}, {"id": 174, "seek": 91400, "start": 933.6, "end": 938.32, "text": " is D loss by D A? It's just negative one over three. And so you can see that if we don't just", "tokens": [50364, 363, 383, 11, 4878, 321, 767, 362, 8858, 3547, 510, 13, 400, 370, 437, 307, 1936, 264, 4470, 538, 584, 411, 413, 316, 11, 50736, 50736, 558, 30, 1042, 11, 498, 321, 20460, 341, 6114, 44003, 11, 341, 307, 3671, 472, 670, 1045, 50980, 50980, 295, 316, 293, 3671, 1804, 3671, 472, 670, 1045, 295, 363, 1804, 3671, 472, 670, 1045, 295, 383, 13, 400, 370, 437, 51344, 51344, 307, 413, 4470, 538, 413, 316, 30, 467, 311, 445, 3671, 472, 670, 1045, 13, 400, 370, 291, 393, 536, 300, 498, 321, 500, 380, 445, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.11217651945171934, "compression_ratio": 1.8774509803921569, "no_speech_prob": 7.2962220656336285e-06}, {"id": 175, "seek": 93832, "start": 938.32, "end": 945.2800000000001, "text": " have A, B and C, but we have 32 numbers, then D loss by D, you know, every one of those numbers", "tokens": [50364, 362, 316, 11, 363, 293, 383, 11, 457, 321, 362, 8858, 3547, 11, 550, 413, 4470, 538, 413, 11, 291, 458, 11, 633, 472, 295, 729, 3547, 50712, 50712, 307, 516, 281, 312, 472, 670, 426, 544, 5101, 11, 570, 426, 307, 264, 2744, 295, 264, 15245, 11, 8858, 294, 341, 1389, 13, 51056, 51104, 407, 413, 4470, 538, 413, 4017, 79, 49715, 307, 3671, 472, 670, 426, 294, 439, 613, 3190, 13, 823, 11, 437, 466, 264, 661, 51616, 51616, 4959, 1854, 4017, 79, 49715, 30, 1436, 4017, 79, 49715, 307, 4833, 316, 11, 291, 536, 300, 4017, 79, 49715, 366, 13475, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.11517674455018802, "compression_ratio": 1.6607929515418502, "no_speech_prob": 3.5559207844926277e-06}, {"id": 176, "seek": 93832, "start": 945.2800000000001, "end": 952.1600000000001, "text": " is going to be one over N more generally, because N is the size of the batch, 32 in this case.", "tokens": [50364, 362, 316, 11, 363, 293, 383, 11, 457, 321, 362, 8858, 3547, 11, 550, 413, 4470, 538, 413, 11, 291, 458, 11, 633, 472, 295, 729, 3547, 50712, 50712, 307, 516, 281, 312, 472, 670, 426, 544, 5101, 11, 570, 426, 307, 264, 2744, 295, 264, 15245, 11, 8858, 294, 341, 1389, 13, 51056, 51104, 407, 413, 4470, 538, 413, 4017, 79, 49715, 307, 3671, 472, 670, 426, 294, 439, 613, 3190, 13, 823, 11, 437, 466, 264, 661, 51616, 51616, 4959, 1854, 4017, 79, 49715, 30, 1436, 4017, 79, 49715, 307, 4833, 316, 11, 291, 536, 300, 4017, 79, 49715, 366, 13475, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.11517674455018802, "compression_ratio": 1.6607929515418502, "no_speech_prob": 3.5559207844926277e-06}, {"id": 177, "seek": 93832, "start": 953.12, "end": 963.36, "text": " So D loss by D lockprops is negative one over N in all these places. Now, what about the other", "tokens": [50364, 362, 316, 11, 363, 293, 383, 11, 457, 321, 362, 8858, 3547, 11, 550, 413, 4470, 538, 413, 11, 291, 458, 11, 633, 472, 295, 729, 3547, 50712, 50712, 307, 516, 281, 312, 472, 670, 426, 544, 5101, 11, 570, 426, 307, 264, 2744, 295, 264, 15245, 11, 8858, 294, 341, 1389, 13, 51056, 51104, 407, 413, 4470, 538, 413, 4017, 79, 49715, 307, 3671, 472, 670, 426, 294, 439, 613, 3190, 13, 823, 11, 437, 466, 264, 661, 51616, 51616, 4959, 1854, 4017, 79, 49715, 30, 1436, 4017, 79, 49715, 307, 4833, 316, 11, 291, 536, 300, 4017, 79, 49715, 366, 13475, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.11517674455018802, "compression_ratio": 1.6607929515418502, "no_speech_prob": 3.5559207844926277e-06}, {"id": 178, "seek": 93832, "start": 963.36, "end": 968.1600000000001, "text": " elements inside lockprops? Because lockprops is larger A, you see that lockprops are shaped", "tokens": [50364, 362, 316, 11, 363, 293, 383, 11, 457, 321, 362, 8858, 3547, 11, 550, 413, 4470, 538, 413, 11, 291, 458, 11, 633, 472, 295, 729, 3547, 50712, 50712, 307, 516, 281, 312, 472, 670, 426, 544, 5101, 11, 570, 426, 307, 264, 2744, 295, 264, 15245, 11, 8858, 294, 341, 1389, 13, 51056, 51104, 407, 413, 4470, 538, 413, 4017, 79, 49715, 307, 3671, 472, 670, 426, 294, 439, 613, 3190, 13, 823, 11, 437, 466, 264, 661, 51616, 51616, 4959, 1854, 4017, 79, 49715, 30, 1436, 4017, 79, 49715, 307, 4833, 316, 11, 291, 536, 300, 4017, 79, 49715, 366, 13475, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.11517674455018802, "compression_ratio": 1.6607929515418502, "no_speech_prob": 3.5559207844926277e-06}, {"id": 179, "seek": 96816, "start": 968.16, "end": 975.36, "text": " as 32 by 27, but only 32 of them participate in the loss calculation. So what's the derivative", "tokens": [50364, 382, 8858, 538, 7634, 11, 457, 787, 8858, 295, 552, 8197, 294, 264, 4470, 17108, 13, 407, 437, 311, 264, 13760, 50724, 50724, 295, 439, 264, 661, 11, 881, 295, 264, 4959, 300, 360, 406, 483, 41514, 292, 484, 510, 30, 1042, 11, 641, 4470, 46506, 51032, 51032, 307, 4018, 13, 4919, 11, 641, 16235, 46506, 307, 4018, 13, 400, 300, 311, 570, 436, 630, 406, 8197, 294, 264, 51280, 51280, 4470, 13, 407, 881, 295, 613, 3547, 1854, 341, 40863, 775, 406, 3154, 666, 264, 4470, 13, 400, 370, 498, 321, 645, 281, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.07972805354059959, "compression_ratio": 1.75, "no_speech_prob": 4.425425686349627e-06}, {"id": 180, "seek": 96816, "start": 975.36, "end": 981.52, "text": " of all the other, most of the elements that do not get plucked out here? Well, their loss intuitively", "tokens": [50364, 382, 8858, 538, 7634, 11, 457, 787, 8858, 295, 552, 8197, 294, 264, 4470, 17108, 13, 407, 437, 311, 264, 13760, 50724, 50724, 295, 439, 264, 661, 11, 881, 295, 264, 4959, 300, 360, 406, 483, 41514, 292, 484, 510, 30, 1042, 11, 641, 4470, 46506, 51032, 51032, 307, 4018, 13, 4919, 11, 641, 16235, 46506, 307, 4018, 13, 400, 300, 311, 570, 436, 630, 406, 8197, 294, 264, 51280, 51280, 4470, 13, 407, 881, 295, 613, 3547, 1854, 341, 40863, 775, 406, 3154, 666, 264, 4470, 13, 400, 370, 498, 321, 645, 281, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.07972805354059959, "compression_ratio": 1.75, "no_speech_prob": 4.425425686349627e-06}, {"id": 181, "seek": 96816, "start": 981.52, "end": 986.48, "text": " is zero. Sorry, their gradient intuitively is zero. And that's because they did not participate in the", "tokens": [50364, 382, 8858, 538, 7634, 11, 457, 787, 8858, 295, 552, 8197, 294, 264, 4470, 17108, 13, 407, 437, 311, 264, 13760, 50724, 50724, 295, 439, 264, 661, 11, 881, 295, 264, 4959, 300, 360, 406, 483, 41514, 292, 484, 510, 30, 1042, 11, 641, 4470, 46506, 51032, 51032, 307, 4018, 13, 4919, 11, 641, 16235, 46506, 307, 4018, 13, 400, 300, 311, 570, 436, 630, 406, 8197, 294, 264, 51280, 51280, 4470, 13, 407, 881, 295, 613, 3547, 1854, 341, 40863, 775, 406, 3154, 666, 264, 4470, 13, 400, 370, 498, 321, 645, 281, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.07972805354059959, "compression_ratio": 1.75, "no_speech_prob": 4.425425686349627e-06}, {"id": 182, "seek": 96816, "start": 986.48, "end": 992.72, "text": " loss. So most of these numbers inside this tensor does not feed into the loss. And so if we were to", "tokens": [50364, 382, 8858, 538, 7634, 11, 457, 787, 8858, 295, 552, 8197, 294, 264, 4470, 17108, 13, 407, 437, 311, 264, 13760, 50724, 50724, 295, 439, 264, 661, 11, 881, 295, 264, 4959, 300, 360, 406, 483, 41514, 292, 484, 510, 30, 1042, 11, 641, 4470, 46506, 51032, 51032, 307, 4018, 13, 4919, 11, 641, 16235, 46506, 307, 4018, 13, 400, 300, 311, 570, 436, 630, 406, 8197, 294, 264, 51280, 51280, 4470, 13, 407, 881, 295, 613, 3547, 1854, 341, 40863, 775, 406, 3154, 666, 264, 4470, 13, 400, 370, 498, 321, 645, 281, 51592, 51592], "temperature": 0.0, "avg_logprob": -0.07972805354059959, "compression_ratio": 1.75, "no_speech_prob": 4.425425686349627e-06}, {"id": 183, "seek": 99272, "start": 992.72, "end": 998.5600000000001, "text": " change these numbers, then the loss doesn't change, which is the equivalent of saying that the", "tokens": [50364, 1319, 613, 3547, 11, 550, 264, 4470, 1177, 380, 1319, 11, 597, 307, 264, 10344, 295, 1566, 300, 264, 50656, 50656, 13760, 295, 264, 4470, 365, 3104, 281, 552, 307, 4018, 13, 814, 500, 380, 2712, 309, 13, 407, 510, 311, 257, 636, 281, 50952, 50952, 4445, 341, 13760, 13, 1396, 321, 722, 484, 365, 3930, 82, 357, 310, 15, 82, 295, 3909, 8858, 538, 7634, 13, 1610, 718, 311, 445, 584, 11, 51284, 51312, 2602, 295, 884, 341, 11, 570, 321, 500, 380, 528, 281, 1152, 3089, 3547, 11, 718, 311, 360, 3930, 82, 357, 310, 15, 82, 411, 51536, 51580, 4017, 79, 49715, 13, 407, 1936, 11, 341, 307, 516, 281, 1884, 364, 10225, 295, 35193, 2293, 294, 264, 3909, 295, 4017, 79, 49715, 13, 51808], "temperature": 0.0, "avg_logprob": -0.15234552530141976, "compression_ratio": 1.7302158273381294, "no_speech_prob": 1.5534873227807111e-06}, {"id": 184, "seek": 99272, "start": 998.5600000000001, "end": 1004.48, "text": " derivative of the loss with respect to them is zero. They don't impact it. So here's a way to", "tokens": [50364, 1319, 613, 3547, 11, 550, 264, 4470, 1177, 380, 1319, 11, 597, 307, 264, 10344, 295, 1566, 300, 264, 50656, 50656, 13760, 295, 264, 4470, 365, 3104, 281, 552, 307, 4018, 13, 814, 500, 380, 2712, 309, 13, 407, 510, 311, 257, 636, 281, 50952, 50952, 4445, 341, 13760, 13, 1396, 321, 722, 484, 365, 3930, 82, 357, 310, 15, 82, 295, 3909, 8858, 538, 7634, 13, 1610, 718, 311, 445, 584, 11, 51284, 51312, 2602, 295, 884, 341, 11, 570, 321, 500, 380, 528, 281, 1152, 3089, 3547, 11, 718, 311, 360, 3930, 82, 357, 310, 15, 82, 411, 51536, 51580, 4017, 79, 49715, 13, 407, 1936, 11, 341, 307, 516, 281, 1884, 364, 10225, 295, 35193, 2293, 294, 264, 3909, 295, 4017, 79, 49715, 13, 51808], "temperature": 0.0, "avg_logprob": -0.15234552530141976, "compression_ratio": 1.7302158273381294, "no_speech_prob": 1.5534873227807111e-06}, {"id": 185, "seek": 99272, "start": 1004.48, "end": 1011.12, "text": " implement this derivative. Then we start out with torshtot0s of shape 32 by 27. Or let's just say,", "tokens": [50364, 1319, 613, 3547, 11, 550, 264, 4470, 1177, 380, 1319, 11, 597, 307, 264, 10344, 295, 1566, 300, 264, 50656, 50656, 13760, 295, 264, 4470, 365, 3104, 281, 552, 307, 4018, 13, 814, 500, 380, 2712, 309, 13, 407, 510, 311, 257, 636, 281, 50952, 50952, 4445, 341, 13760, 13, 1396, 321, 722, 484, 365, 3930, 82, 357, 310, 15, 82, 295, 3909, 8858, 538, 7634, 13, 1610, 718, 311, 445, 584, 11, 51284, 51312, 2602, 295, 884, 341, 11, 570, 321, 500, 380, 528, 281, 1152, 3089, 3547, 11, 718, 311, 360, 3930, 82, 357, 310, 15, 82, 411, 51536, 51580, 4017, 79, 49715, 13, 407, 1936, 11, 341, 307, 516, 281, 1884, 364, 10225, 295, 35193, 2293, 294, 264, 3909, 295, 4017, 79, 49715, 13, 51808], "temperature": 0.0, "avg_logprob": -0.15234552530141976, "compression_ratio": 1.7302158273381294, "no_speech_prob": 1.5534873227807111e-06}, {"id": 186, "seek": 99272, "start": 1011.6800000000001, "end": 1016.1600000000001, "text": " instead of doing this, because we don't want to hard code numbers, let's do torshtot0s like", "tokens": [50364, 1319, 613, 3547, 11, 550, 264, 4470, 1177, 380, 1319, 11, 597, 307, 264, 10344, 295, 1566, 300, 264, 50656, 50656, 13760, 295, 264, 4470, 365, 3104, 281, 552, 307, 4018, 13, 814, 500, 380, 2712, 309, 13, 407, 510, 311, 257, 636, 281, 50952, 50952, 4445, 341, 13760, 13, 1396, 321, 722, 484, 365, 3930, 82, 357, 310, 15, 82, 295, 3909, 8858, 538, 7634, 13, 1610, 718, 311, 445, 584, 11, 51284, 51312, 2602, 295, 884, 341, 11, 570, 321, 500, 380, 528, 281, 1152, 3089, 3547, 11, 718, 311, 360, 3930, 82, 357, 310, 15, 82, 411, 51536, 51580, 4017, 79, 49715, 13, 407, 1936, 11, 341, 307, 516, 281, 1884, 364, 10225, 295, 35193, 2293, 294, 264, 3909, 295, 4017, 79, 49715, 13, 51808], "temperature": 0.0, "avg_logprob": -0.15234552530141976, "compression_ratio": 1.7302158273381294, "no_speech_prob": 1.5534873227807111e-06}, {"id": 187, "seek": 101616, "start": 1016.16, "end": 1022.24, "text": " lockprops. So basically this is going to create an array of zeros exactly in the shape of lockprops.", "tokens": [50364, 4017, 79, 49715, 13, 407, 1936, 341, 307, 516, 281, 1884, 364, 10225, 295, 35193, 2293, 294, 264, 3909, 295, 4017, 79, 49715, 13, 50668, 50668, 400, 550, 321, 643, 281, 992, 264, 13760, 295, 3671, 472, 670, 297, 1854, 2293, 613, 9253, 13, 50968, 50968, 407, 510, 311, 437, 321, 393, 360, 13, 440, 4017, 79, 49715, 8186, 292, 294, 264, 14800, 636, 486, 312, 445, 992, 281, 3671, 472, 670, 4018, 51368, 51368], "temperature": 0.0, "avg_logprob": -0.15703814370291574, "compression_ratio": 1.654054054054054, "no_speech_prob": 6.438967375288485e-06}, {"id": 188, "seek": 101616, "start": 1022.24, "end": 1028.24, "text": " And then we need to set the derivative of negative one over n inside exactly these locations.", "tokens": [50364, 4017, 79, 49715, 13, 407, 1936, 341, 307, 516, 281, 1884, 364, 10225, 295, 35193, 2293, 294, 264, 3909, 295, 4017, 79, 49715, 13, 50668, 50668, 400, 550, 321, 643, 281, 992, 264, 13760, 295, 3671, 472, 670, 297, 1854, 2293, 613, 9253, 13, 50968, 50968, 407, 510, 311, 437, 321, 393, 360, 13, 440, 4017, 79, 49715, 8186, 292, 294, 264, 14800, 636, 486, 312, 445, 992, 281, 3671, 472, 670, 4018, 51368, 51368], "temperature": 0.0, "avg_logprob": -0.15703814370291574, "compression_ratio": 1.654054054054054, "no_speech_prob": 6.438967375288485e-06}, {"id": 189, "seek": 101616, "start": 1028.24, "end": 1036.24, "text": " So here's what we can do. The lockprops indexed in the identical way will be just set to negative one over zero", "tokens": [50364, 4017, 79, 49715, 13, 407, 1936, 341, 307, 516, 281, 1884, 364, 10225, 295, 35193, 2293, 294, 264, 3909, 295, 4017, 79, 49715, 13, 50668, 50668, 400, 550, 321, 643, 281, 992, 264, 13760, 295, 3671, 472, 670, 297, 1854, 2293, 613, 9253, 13, 50968, 50968, 407, 510, 311, 437, 321, 393, 360, 13, 440, 4017, 79, 49715, 8186, 292, 294, 264, 14800, 636, 486, 312, 445, 992, 281, 3671, 472, 670, 4018, 51368, 51368], "temperature": 0.0, "avg_logprob": -0.15703814370291574, "compression_ratio": 1.654054054054054, "no_speech_prob": 6.438967375288485e-06}, {"id": 190, "seek": 103624, "start": 1036.24, "end": 1046.32, "text": " divide n. Right? Just like we derived here. So now let me erase all of these reasoning. And then this", "tokens": [50364, 9845, 297, 13, 1779, 30, 1449, 411, 321, 18949, 510, 13, 407, 586, 718, 385, 23525, 439, 295, 613, 21577, 13, 400, 550, 341, 50868, 50868, 307, 264, 11532, 13760, 337, 264, 4017, 79, 49715, 13, 961, 311, 8585, 518, 264, 700, 1622, 293, 1520, 300, 341, 307, 3006, 13, 51168, 51264, 1033, 13, 407, 383, 12224, 5872, 293, 718, 311, 352, 646, 281, 383, 12224, 13, 400, 291, 536, 300, 437, 309, 311, 884, 307, 309, 311, 28258, 498, 51656, 51700], "temperature": 0.0, "avg_logprob": -0.15086231912885392, "compression_ratio": 1.5270935960591132, "no_speech_prob": 2.1907619611738482e-06}, {"id": 191, "seek": 103624, "start": 1046.32, "end": 1052.32, "text": " is the candidate derivative for the lockprops. Let's uncomment the first line and check that this is correct.", "tokens": [50364, 9845, 297, 13, 1779, 30, 1449, 411, 321, 18949, 510, 13, 407, 586, 718, 385, 23525, 439, 295, 613, 21577, 13, 400, 550, 341, 50868, 50868, 307, 264, 11532, 13760, 337, 264, 4017, 79, 49715, 13, 961, 311, 8585, 518, 264, 700, 1622, 293, 1520, 300, 341, 307, 3006, 13, 51168, 51264, 1033, 13, 407, 383, 12224, 5872, 293, 718, 311, 352, 646, 281, 383, 12224, 13, 400, 291, 536, 300, 437, 309, 311, 884, 307, 309, 311, 28258, 498, 51656, 51700], "temperature": 0.0, "avg_logprob": -0.15086231912885392, "compression_ratio": 1.5270935960591132, "no_speech_prob": 2.1907619611738482e-06}, {"id": 192, "seek": 103624, "start": 1054.24, "end": 1062.08, "text": " Okay. So CMP ran and let's go back to CMP. And you see that what it's doing is it's calculating if", "tokens": [50364, 9845, 297, 13, 1779, 30, 1449, 411, 321, 18949, 510, 13, 407, 586, 718, 385, 23525, 439, 295, 613, 21577, 13, 400, 550, 341, 50868, 50868, 307, 264, 11532, 13760, 337, 264, 4017, 79, 49715, 13, 961, 311, 8585, 518, 264, 700, 1622, 293, 1520, 300, 341, 307, 3006, 13, 51168, 51264, 1033, 13, 407, 383, 12224, 5872, 293, 718, 311, 352, 646, 281, 383, 12224, 13, 400, 291, 536, 300, 437, 309, 311, 884, 307, 309, 311, 28258, 498, 51656, 51700], "temperature": 0.0, "avg_logprob": -0.15086231912885392, "compression_ratio": 1.5270935960591132, "no_speech_prob": 2.1907619611738482e-06}, {"id": 193, "seek": 106208, "start": 1062.08, "end": 1068.24, "text": " the calculated value by us, which is DT, is exactly equal to T dot grad as calculated by PyTorch.", "tokens": [50364, 264, 15598, 2158, 538, 505, 11, 597, 307, 413, 51, 11, 307, 2293, 2681, 281, 314, 5893, 2771, 382, 15598, 538, 9953, 51, 284, 339, 13, 50672, 50712, 400, 550, 341, 307, 1455, 988, 300, 439, 264, 4959, 366, 2293, 2681, 293, 550, 29942, 341, 281, 257, 50996, 50996, 2167, 23351, 28499, 2158, 570, 321, 500, 380, 528, 281, 23351, 28499, 40863, 13, 492, 445, 528, 281, 23351, 28499, 2158, 13, 51188, 51240, 400, 550, 510, 321, 366, 1455, 988, 300, 11, 1392, 11, 498, 436, 434, 406, 2293, 2681, 11, 1310, 436, 366, 51464, 51464, 10447, 2681, 570, 295, 512, 12607, 935, 2663, 11, 457, 436, 434, 588, 11, 588, 1998, 13, 51644, 51688], "temperature": 0.0, "avg_logprob": -0.17341087210891593, "compression_ratio": 1.821011673151751, "no_speech_prob": 5.3380822464532685e-06}, {"id": 194, "seek": 106208, "start": 1069.04, "end": 1074.72, "text": " And then this is making sure that all the elements are exactly equal and then converting this to a", "tokens": [50364, 264, 15598, 2158, 538, 505, 11, 597, 307, 413, 51, 11, 307, 2293, 2681, 281, 314, 5893, 2771, 382, 15598, 538, 9953, 51, 284, 339, 13, 50672, 50712, 400, 550, 341, 307, 1455, 988, 300, 439, 264, 4959, 366, 2293, 2681, 293, 550, 29942, 341, 281, 257, 50996, 50996, 2167, 23351, 28499, 2158, 570, 321, 500, 380, 528, 281, 23351, 28499, 40863, 13, 492, 445, 528, 281, 23351, 28499, 2158, 13, 51188, 51240, 400, 550, 510, 321, 366, 1455, 988, 300, 11, 1392, 11, 498, 436, 434, 406, 2293, 2681, 11, 1310, 436, 366, 51464, 51464, 10447, 2681, 570, 295, 512, 12607, 935, 2663, 11, 457, 436, 434, 588, 11, 588, 1998, 13, 51644, 51688], "temperature": 0.0, "avg_logprob": -0.17341087210891593, "compression_ratio": 1.821011673151751, "no_speech_prob": 5.3380822464532685e-06}, {"id": 195, "seek": 106208, "start": 1074.72, "end": 1078.56, "text": " single Boolean value because we don't want to Boolean tensor. We just want to Boolean value.", "tokens": [50364, 264, 15598, 2158, 538, 505, 11, 597, 307, 413, 51, 11, 307, 2293, 2681, 281, 314, 5893, 2771, 382, 15598, 538, 9953, 51, 284, 339, 13, 50672, 50712, 400, 550, 341, 307, 1455, 988, 300, 439, 264, 4959, 366, 2293, 2681, 293, 550, 29942, 341, 281, 257, 50996, 50996, 2167, 23351, 28499, 2158, 570, 321, 500, 380, 528, 281, 23351, 28499, 40863, 13, 492, 445, 528, 281, 23351, 28499, 2158, 13, 51188, 51240, 400, 550, 510, 321, 366, 1455, 988, 300, 11, 1392, 11, 498, 436, 434, 406, 2293, 2681, 11, 1310, 436, 366, 51464, 51464, 10447, 2681, 570, 295, 512, 12607, 935, 2663, 11, 457, 436, 434, 588, 11, 588, 1998, 13, 51644, 51688], "temperature": 0.0, "avg_logprob": -0.17341087210891593, "compression_ratio": 1.821011673151751, "no_speech_prob": 5.3380822464532685e-06}, {"id": 196, "seek": 106208, "start": 1079.6, "end": 1084.08, "text": " And then here we are making sure that, okay, if they're not exactly equal, maybe they are", "tokens": [50364, 264, 15598, 2158, 538, 505, 11, 597, 307, 413, 51, 11, 307, 2293, 2681, 281, 314, 5893, 2771, 382, 15598, 538, 9953, 51, 284, 339, 13, 50672, 50712, 400, 550, 341, 307, 1455, 988, 300, 439, 264, 4959, 366, 2293, 2681, 293, 550, 29942, 341, 281, 257, 50996, 50996, 2167, 23351, 28499, 2158, 570, 321, 500, 380, 528, 281, 23351, 28499, 40863, 13, 492, 445, 528, 281, 23351, 28499, 2158, 13, 51188, 51240, 400, 550, 510, 321, 366, 1455, 988, 300, 11, 1392, 11, 498, 436, 434, 406, 2293, 2681, 11, 1310, 436, 366, 51464, 51464, 10447, 2681, 570, 295, 512, 12607, 935, 2663, 11, 457, 436, 434, 588, 11, 588, 1998, 13, 51644, 51688], "temperature": 0.0, "avg_logprob": -0.17341087210891593, "compression_ratio": 1.821011673151751, "no_speech_prob": 5.3380822464532685e-06}, {"id": 197, "seek": 106208, "start": 1084.08, "end": 1087.6799999999998, "text": " approximately equal because of some floating point issues, but they're very, very close.", "tokens": [50364, 264, 15598, 2158, 538, 505, 11, 597, 307, 413, 51, 11, 307, 2293, 2681, 281, 314, 5893, 2771, 382, 15598, 538, 9953, 51, 284, 339, 13, 50672, 50712, 400, 550, 341, 307, 1455, 988, 300, 439, 264, 4959, 366, 2293, 2681, 293, 550, 29942, 341, 281, 257, 50996, 50996, 2167, 23351, 28499, 2158, 570, 321, 500, 380, 528, 281, 23351, 28499, 40863, 13, 492, 445, 528, 281, 23351, 28499, 2158, 13, 51188, 51240, 400, 550, 510, 321, 366, 1455, 988, 300, 11, 1392, 11, 498, 436, 434, 406, 2293, 2681, 11, 1310, 436, 366, 51464, 51464, 10447, 2681, 570, 295, 512, 12607, 935, 2663, 11, 457, 436, 434, 588, 11, 588, 1998, 13, 51644, 51688], "temperature": 0.0, "avg_logprob": -0.17341087210891593, "compression_ratio": 1.821011673151751, "no_speech_prob": 5.3380822464532685e-06}, {"id": 198, "seek": 108768, "start": 1087.68, "end": 1093.3600000000001, "text": " So here we are using Torch dot all close, which has a little bit of a wiggle available", "tokens": [50364, 407, 510, 321, 366, 1228, 7160, 339, 5893, 439, 1998, 11, 597, 575, 257, 707, 857, 295, 257, 33377, 2435, 50648, 50680, 570, 2171, 291, 393, 483, 588, 11, 588, 1998, 13, 583, 498, 291, 764, 257, 4748, 819, 17108, 50924, 50924, 570, 295, 12607, 935, 42973, 11, 291, 393, 483, 257, 4748, 819, 1874, 13, 51172, 51172, 407, 341, 307, 8568, 498, 291, 483, 364, 10447, 1998, 1874, 13, 400, 550, 510, 321, 366, 8568, 264, 51408, 51408, 6674, 11, 1936, 264, 2158, 300, 575, 264, 6343, 2649, 293, 437, 307, 264, 2649, 294, 264, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.10095278739929199, "compression_ratio": 1.8448979591836734, "no_speech_prob": 3.04155446428922e-06}, {"id": 199, "seek": 108768, "start": 1094.0, "end": 1098.88, "text": " because sometimes you can get very, very close. But if you use a slightly different calculation", "tokens": [50364, 407, 510, 321, 366, 1228, 7160, 339, 5893, 439, 1998, 11, 597, 575, 257, 707, 857, 295, 257, 33377, 2435, 50648, 50680, 570, 2171, 291, 393, 483, 588, 11, 588, 1998, 13, 583, 498, 291, 764, 257, 4748, 819, 17108, 50924, 50924, 570, 295, 12607, 935, 42973, 11, 291, 393, 483, 257, 4748, 819, 1874, 13, 51172, 51172, 407, 341, 307, 8568, 498, 291, 483, 364, 10447, 1998, 1874, 13, 400, 550, 510, 321, 366, 8568, 264, 51408, 51408, 6674, 11, 1936, 264, 2158, 300, 575, 264, 6343, 2649, 293, 437, 307, 264, 2649, 294, 264, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.10095278739929199, "compression_ratio": 1.8448979591836734, "no_speech_prob": 3.04155446428922e-06}, {"id": 200, "seek": 108768, "start": 1098.88, "end": 1103.8400000000001, "text": " because of floating point arithmetic, you can get a slightly different result.", "tokens": [50364, 407, 510, 321, 366, 1228, 7160, 339, 5893, 439, 1998, 11, 597, 575, 257, 707, 857, 295, 257, 33377, 2435, 50648, 50680, 570, 2171, 291, 393, 483, 588, 11, 588, 1998, 13, 583, 498, 291, 764, 257, 4748, 819, 17108, 50924, 50924, 570, 295, 12607, 935, 42973, 11, 291, 393, 483, 257, 4748, 819, 1874, 13, 51172, 51172, 407, 341, 307, 8568, 498, 291, 483, 364, 10447, 1998, 1874, 13, 400, 550, 510, 321, 366, 8568, 264, 51408, 51408, 6674, 11, 1936, 264, 2158, 300, 575, 264, 6343, 2649, 293, 437, 307, 264, 2649, 294, 264, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.10095278739929199, "compression_ratio": 1.8448979591836734, "no_speech_prob": 3.04155446428922e-06}, {"id": 201, "seek": 108768, "start": 1103.8400000000001, "end": 1108.5600000000002, "text": " So this is checking if you get an approximately close result. And then here we are checking the", "tokens": [50364, 407, 510, 321, 366, 1228, 7160, 339, 5893, 439, 1998, 11, 597, 575, 257, 707, 857, 295, 257, 33377, 2435, 50648, 50680, 570, 2171, 291, 393, 483, 588, 11, 588, 1998, 13, 583, 498, 291, 764, 257, 4748, 819, 17108, 50924, 50924, 570, 295, 12607, 935, 42973, 11, 291, 393, 483, 257, 4748, 819, 1874, 13, 51172, 51172, 407, 341, 307, 8568, 498, 291, 483, 364, 10447, 1998, 1874, 13, 400, 550, 510, 321, 366, 8568, 264, 51408, 51408, 6674, 11, 1936, 264, 2158, 300, 575, 264, 6343, 2649, 293, 437, 307, 264, 2649, 294, 264, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.10095278739929199, "compression_ratio": 1.8448979591836734, "no_speech_prob": 3.04155446428922e-06}, {"id": 202, "seek": 108768, "start": 1108.5600000000002, "end": 1115.2, "text": " maximum, basically the value that has the highest difference and what is the difference in the", "tokens": [50364, 407, 510, 321, 366, 1228, 7160, 339, 5893, 439, 1998, 11, 597, 575, 257, 707, 857, 295, 257, 33377, 2435, 50648, 50680, 570, 2171, 291, 393, 483, 588, 11, 588, 1998, 13, 583, 498, 291, 764, 257, 4748, 819, 17108, 50924, 50924, 570, 295, 12607, 935, 42973, 11, 291, 393, 483, 257, 4748, 819, 1874, 13, 51172, 51172, 407, 341, 307, 8568, 498, 291, 483, 364, 10447, 1998, 1874, 13, 400, 550, 510, 321, 366, 8568, 264, 51408, 51408, 6674, 11, 1936, 264, 2158, 300, 575, 264, 6343, 2649, 293, 437, 307, 264, 2649, 294, 264, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.10095278739929199, "compression_ratio": 1.8448979591836734, "no_speech_prob": 3.04155446428922e-06}, {"id": 203, "seek": 111520, "start": 1115.2, "end": 1120.0, "text": " absolute value difference between those two. And so we are printing whether we have an exact equality,", "tokens": [50364, 8236, 2158, 2649, 1296, 729, 732, 13, 400, 370, 321, 366, 14699, 1968, 321, 362, 364, 1900, 14949, 11, 50604, 50604, 364, 30874, 14949, 11, 293, 437, 307, 264, 6443, 2649, 13, 400, 370, 510, 321, 536, 300, 321, 767, 50992, 50992, 362, 1900, 14949, 13, 400, 370, 4412, 11, 295, 1164, 11, 321, 611, 362, 364, 30874, 14949, 293, 264, 51240, 51240, 6674, 2649, 307, 2293, 4018, 13, 407, 1936, 527, 413, 3565, 26173, 307, 2293, 2681, 281, 437, 9953, 51, 284, 339, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.08979897607456554, "compression_ratio": 1.8148148148148149, "no_speech_prob": 6.083519679123128e-07}, {"id": 204, "seek": 111520, "start": 1120.0, "end": 1127.76, "text": " an approximate equality, and what is the largest difference. And so here we see that we actually", "tokens": [50364, 8236, 2158, 2649, 1296, 729, 732, 13, 400, 370, 321, 366, 14699, 1968, 321, 362, 364, 1900, 14949, 11, 50604, 50604, 364, 30874, 14949, 11, 293, 437, 307, 264, 6443, 2649, 13, 400, 370, 510, 321, 536, 300, 321, 767, 50992, 50992, 362, 1900, 14949, 13, 400, 370, 4412, 11, 295, 1164, 11, 321, 611, 362, 364, 30874, 14949, 293, 264, 51240, 51240, 6674, 2649, 307, 2293, 4018, 13, 407, 1936, 527, 413, 3565, 26173, 307, 2293, 2681, 281, 437, 9953, 51, 284, 339, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.08979897607456554, "compression_ratio": 1.8148148148148149, "no_speech_prob": 6.083519679123128e-07}, {"id": 205, "seek": 111520, "start": 1127.76, "end": 1132.72, "text": " have exact equality. And so therefore, of course, we also have an approximate equality and the", "tokens": [50364, 8236, 2158, 2649, 1296, 729, 732, 13, 400, 370, 321, 366, 14699, 1968, 321, 362, 364, 1900, 14949, 11, 50604, 50604, 364, 30874, 14949, 11, 293, 437, 307, 264, 6443, 2649, 13, 400, 370, 510, 321, 536, 300, 321, 767, 50992, 50992, 362, 1900, 14949, 13, 400, 370, 4412, 11, 295, 1164, 11, 321, 611, 362, 364, 30874, 14949, 293, 264, 51240, 51240, 6674, 2649, 307, 2293, 4018, 13, 407, 1936, 527, 413, 3565, 26173, 307, 2293, 2681, 281, 437, 9953, 51, 284, 339, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.08979897607456554, "compression_ratio": 1.8148148148148149, "no_speech_prob": 6.083519679123128e-07}, {"id": 206, "seek": 111520, "start": 1132.72, "end": 1139.52, "text": " maximum difference is exactly zero. So basically our D log props is exactly equal to what PyTorch", "tokens": [50364, 8236, 2158, 2649, 1296, 729, 732, 13, 400, 370, 321, 366, 14699, 1968, 321, 362, 364, 1900, 14949, 11, 50604, 50604, 364, 30874, 14949, 11, 293, 437, 307, 264, 6443, 2649, 13, 400, 370, 510, 321, 536, 300, 321, 767, 50992, 50992, 362, 1900, 14949, 13, 400, 370, 4412, 11, 295, 1164, 11, 321, 611, 362, 364, 30874, 14949, 293, 264, 51240, 51240, 6674, 2649, 307, 2293, 4018, 13, 407, 1936, 527, 413, 3565, 26173, 307, 2293, 2681, 281, 437, 9953, 51, 284, 339, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.08979897607456554, "compression_ratio": 1.8148148148148149, "no_speech_prob": 6.083519679123128e-07}, {"id": 207, "seek": 113952, "start": 1139.52, "end": 1145.6, "text": " calculated to be log props dot grad in its back propagation. So, so far we're reading pretty well.", "tokens": [50364, 15598, 281, 312, 3565, 26173, 5893, 2771, 294, 1080, 646, 38377, 13, 407, 11, 370, 1400, 321, 434, 3760, 1238, 731, 13, 50668, 50696, 1033, 11, 370, 718, 311, 586, 2354, 527, 646, 38377, 13, 492, 362, 300, 3565, 26173, 5946, 322, 26173, 807, 257, 50968, 50968, 3565, 13, 407, 439, 264, 4959, 295, 26173, 366, 885, 4478, 10829, 6456, 3565, 732, 13, 823, 11, 498, 321, 528, 2452, 51324, 51324, 26173, 11, 550, 11, 550, 1604, 428, 4532, 2771, 3097, 13, 492, 362, 411, 257, 3565, 9984, 11, 309, 2516, 294, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.17632311582565308, "compression_ratio": 1.6872246696035242, "no_speech_prob": 2.2602785065828357e-06}, {"id": 208, "seek": 113952, "start": 1146.16, "end": 1151.6, "text": " Okay, so let's now continue our back propagation. We have that log props depends on props through a", "tokens": [50364, 15598, 281, 312, 3565, 26173, 5893, 2771, 294, 1080, 646, 38377, 13, 407, 11, 370, 1400, 321, 434, 3760, 1238, 731, 13, 50668, 50696, 1033, 11, 370, 718, 311, 586, 2354, 527, 646, 38377, 13, 492, 362, 300, 3565, 26173, 5946, 322, 26173, 807, 257, 50968, 50968, 3565, 13, 407, 439, 264, 4959, 295, 26173, 366, 885, 4478, 10829, 6456, 3565, 732, 13, 823, 11, 498, 321, 528, 2452, 51324, 51324, 26173, 11, 550, 11, 550, 1604, 428, 4532, 2771, 3097, 13, 492, 362, 411, 257, 3565, 9984, 11, 309, 2516, 294, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.17632311582565308, "compression_ratio": 1.6872246696035242, "no_speech_prob": 2.2602785065828357e-06}, {"id": 209, "seek": 113952, "start": 1151.6, "end": 1158.72, "text": " log. So all the elements of props are being element wise applied log two. Now, if we want deep", "tokens": [50364, 15598, 281, 312, 3565, 26173, 5893, 2771, 294, 1080, 646, 38377, 13, 407, 11, 370, 1400, 321, 434, 3760, 1238, 731, 13, 50668, 50696, 1033, 11, 370, 718, 311, 586, 2354, 527, 646, 38377, 13, 492, 362, 300, 3565, 26173, 5946, 322, 26173, 807, 257, 50968, 50968, 3565, 13, 407, 439, 264, 4959, 295, 26173, 366, 885, 4478, 10829, 6456, 3565, 732, 13, 823, 11, 498, 321, 528, 2452, 51324, 51324, 26173, 11, 550, 11, 550, 1604, 428, 4532, 2771, 3097, 13, 492, 362, 411, 257, 3565, 9984, 11, 309, 2516, 294, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.17632311582565308, "compression_ratio": 1.6872246696035242, "no_speech_prob": 2.2602785065828357e-06}, {"id": 210, "seek": 113952, "start": 1158.72, "end": 1164.48, "text": " props, then, then remember your micro grad training. We have like a log node, it takes in", "tokens": [50364, 15598, 281, 312, 3565, 26173, 5893, 2771, 294, 1080, 646, 38377, 13, 407, 11, 370, 1400, 321, 434, 3760, 1238, 731, 13, 50668, 50696, 1033, 11, 370, 718, 311, 586, 2354, 527, 646, 38377, 13, 492, 362, 300, 3565, 26173, 5946, 322, 26173, 807, 257, 50968, 50968, 3565, 13, 407, 439, 264, 4959, 295, 26173, 366, 885, 4478, 10829, 6456, 3565, 732, 13, 823, 11, 498, 321, 528, 2452, 51324, 51324, 26173, 11, 550, 11, 550, 1604, 428, 4532, 2771, 3097, 13, 492, 362, 411, 257, 3565, 9984, 11, 309, 2516, 294, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.17632311582565308, "compression_ratio": 1.6872246696035242, "no_speech_prob": 2.2602785065828357e-06}, {"id": 211, "seek": 116448, "start": 1164.48, "end": 1171.2, "text": " props and creates log props and the props will be the local derivative of that individual operation", "tokens": [50364, 26173, 293, 7829, 3565, 26173, 293, 264, 26173, 486, 312, 264, 2654, 13760, 295, 300, 2609, 6916, 50700, 50700, 3565, 1413, 264, 13760, 4470, 365, 3104, 281, 1080, 5598, 11, 597, 294, 341, 1389, 307, 413, 3565, 26173, 13, 50984, 51020, 407, 437, 307, 264, 2654, 13760, 295, 341, 6916, 30, 1042, 11, 321, 366, 1940, 3565, 4478, 10829, 293, 321, 393, 51272, 51272, 808, 510, 293, 321, 393, 536, 731, 490, 8961, 307, 428, 1277, 300, 413, 538, 48817, 295, 3565, 295, 1783, 307, 445, 2935, 472, 51520, 51520], "temperature": 0.0, "avg_logprob": -0.11732345042021378, "compression_ratio": 1.7455357142857142, "no_speech_prob": 3.0893997973180376e-06}, {"id": 212, "seek": 116448, "start": 1171.2, "end": 1176.88, "text": " log times the derivative loss with respect to its output, which in this case is D log props.", "tokens": [50364, 26173, 293, 7829, 3565, 26173, 293, 264, 26173, 486, 312, 264, 2654, 13760, 295, 300, 2609, 6916, 50700, 50700, 3565, 1413, 264, 13760, 4470, 365, 3104, 281, 1080, 5598, 11, 597, 294, 341, 1389, 307, 413, 3565, 26173, 13, 50984, 51020, 407, 437, 307, 264, 2654, 13760, 295, 341, 6916, 30, 1042, 11, 321, 366, 1940, 3565, 4478, 10829, 293, 321, 393, 51272, 51272, 808, 510, 293, 321, 393, 536, 731, 490, 8961, 307, 428, 1277, 300, 413, 538, 48817, 295, 3565, 295, 1783, 307, 445, 2935, 472, 51520, 51520], "temperature": 0.0, "avg_logprob": -0.11732345042021378, "compression_ratio": 1.7455357142857142, "no_speech_prob": 3.0893997973180376e-06}, {"id": 213, "seek": 116448, "start": 1177.6, "end": 1182.64, "text": " So what is the local derivative of this operation? Well, we are taking log element wise and we can", "tokens": [50364, 26173, 293, 7829, 3565, 26173, 293, 264, 26173, 486, 312, 264, 2654, 13760, 295, 300, 2609, 6916, 50700, 50700, 3565, 1413, 264, 13760, 4470, 365, 3104, 281, 1080, 5598, 11, 597, 294, 341, 1389, 307, 413, 3565, 26173, 13, 50984, 51020, 407, 437, 307, 264, 2654, 13760, 295, 341, 6916, 30, 1042, 11, 321, 366, 1940, 3565, 4478, 10829, 293, 321, 393, 51272, 51272, 808, 510, 293, 321, 393, 536, 731, 490, 8961, 307, 428, 1277, 300, 413, 538, 48817, 295, 3565, 295, 1783, 307, 445, 2935, 472, 51520, 51520], "temperature": 0.0, "avg_logprob": -0.11732345042021378, "compression_ratio": 1.7455357142857142, "no_speech_prob": 3.0893997973180376e-06}, {"id": 214, "seek": 116448, "start": 1182.64, "end": 1187.6, "text": " come here and we can see well from alpha is your friend that D by DX of log of X is just simply one", "tokens": [50364, 26173, 293, 7829, 3565, 26173, 293, 264, 26173, 486, 312, 264, 2654, 13760, 295, 300, 2609, 6916, 50700, 50700, 3565, 1413, 264, 13760, 4470, 365, 3104, 281, 1080, 5598, 11, 597, 294, 341, 1389, 307, 413, 3565, 26173, 13, 50984, 51020, 407, 437, 307, 264, 2654, 13760, 295, 341, 6916, 30, 1042, 11, 321, 366, 1940, 3565, 4478, 10829, 293, 321, 393, 51272, 51272, 808, 510, 293, 321, 393, 536, 731, 490, 8961, 307, 428, 1277, 300, 413, 538, 48817, 295, 3565, 295, 1783, 307, 445, 2935, 472, 51520, 51520], "temperature": 0.0, "avg_logprob": -0.11732345042021378, "compression_ratio": 1.7455357142857142, "no_speech_prob": 3.0893997973180376e-06}, {"id": 215, "seek": 118760, "start": 1187.6, "end": 1195.1999999999998, "text": " of our X. So therefore in this case, X is props. So we have D by DX is one over X, which is one of", "tokens": [50364, 295, 527, 1783, 13, 407, 4412, 294, 341, 1389, 11, 1783, 307, 26173, 13, 407, 321, 362, 413, 538, 48817, 307, 472, 670, 1783, 11, 597, 307, 472, 295, 50744, 50744, 527, 26173, 13, 400, 550, 341, 307, 264, 2654, 13760, 293, 550, 1413, 321, 528, 281, 5021, 309, 13, 407, 341, 307, 51016, 51016, 5021, 4978, 1413, 413, 3565, 26173, 13, 1396, 718, 385, 8585, 518, 341, 293, 718, 385, 1190, 264, 2815, 294, 1081, 13, 400, 321, 536, 51372, 51372, 300, 264, 13760, 295, 26173, 382, 321, 15598, 510, 307, 2293, 3006, 13, 400, 370, 3449, 510, 577, 341, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.12197242010207403, "compression_ratio": 1.7792792792792793, "no_speech_prob": 1.2679105338975205e-06}, {"id": 216, "seek": 118760, "start": 1195.1999999999998, "end": 1200.6399999999999, "text": " our props. And then this is the local derivative and then times we want to chain it. So this is", "tokens": [50364, 295, 527, 1783, 13, 407, 4412, 294, 341, 1389, 11, 1783, 307, 26173, 13, 407, 321, 362, 413, 538, 48817, 307, 472, 670, 1783, 11, 597, 307, 472, 295, 50744, 50744, 527, 26173, 13, 400, 550, 341, 307, 264, 2654, 13760, 293, 550, 1413, 321, 528, 281, 5021, 309, 13, 407, 341, 307, 51016, 51016, 5021, 4978, 1413, 413, 3565, 26173, 13, 1396, 718, 385, 8585, 518, 341, 293, 718, 385, 1190, 264, 2815, 294, 1081, 13, 400, 321, 536, 51372, 51372, 300, 264, 13760, 295, 26173, 382, 321, 15598, 510, 307, 2293, 3006, 13, 400, 370, 3449, 510, 577, 341, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.12197242010207403, "compression_ratio": 1.7792792792792793, "no_speech_prob": 1.2679105338975205e-06}, {"id": 217, "seek": 118760, "start": 1200.6399999999999, "end": 1207.76, "text": " chain rule times D log props. Then let me uncomment this and let me run the cell in place. And we see", "tokens": [50364, 295, 527, 1783, 13, 407, 4412, 294, 341, 1389, 11, 1783, 307, 26173, 13, 407, 321, 362, 413, 538, 48817, 307, 472, 670, 1783, 11, 597, 307, 472, 295, 50744, 50744, 527, 26173, 13, 400, 550, 341, 307, 264, 2654, 13760, 293, 550, 1413, 321, 528, 281, 5021, 309, 13, 407, 341, 307, 51016, 51016, 5021, 4978, 1413, 413, 3565, 26173, 13, 1396, 718, 385, 8585, 518, 341, 293, 718, 385, 1190, 264, 2815, 294, 1081, 13, 400, 321, 536, 51372, 51372, 300, 264, 13760, 295, 26173, 382, 321, 15598, 510, 307, 2293, 3006, 13, 400, 370, 3449, 510, 577, 341, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.12197242010207403, "compression_ratio": 1.7792792792792793, "no_speech_prob": 1.2679105338975205e-06}, {"id": 218, "seek": 118760, "start": 1207.76, "end": 1214.0, "text": " that the derivative of props as we calculated here is exactly correct. And so notice here how this", "tokens": [50364, 295, 527, 1783, 13, 407, 4412, 294, 341, 1389, 11, 1783, 307, 26173, 13, 407, 321, 362, 413, 538, 48817, 307, 472, 670, 1783, 11, 597, 307, 472, 295, 50744, 50744, 527, 26173, 13, 400, 550, 341, 307, 264, 2654, 13760, 293, 550, 1413, 321, 528, 281, 5021, 309, 13, 407, 341, 307, 51016, 51016, 5021, 4978, 1413, 413, 3565, 26173, 13, 1396, 718, 385, 8585, 518, 341, 293, 718, 385, 1190, 264, 2815, 294, 1081, 13, 400, 321, 536, 51372, 51372, 300, 264, 13760, 295, 26173, 382, 321, 15598, 510, 307, 2293, 3006, 13, 400, 370, 3449, 510, 577, 341, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.12197242010207403, "compression_ratio": 1.7792792792792793, "no_speech_prob": 1.2679105338975205e-06}, {"id": 219, "seek": 121400, "start": 1214.0, "end": 1219.92, "text": " works. Props that are, props is going to be inverted and then element wise multiplied here.", "tokens": [50364, 1985, 13, 21944, 82, 300, 366, 11, 26173, 307, 516, 281, 312, 38969, 293, 550, 4478, 10829, 17207, 510, 13, 50660, 50704, 407, 498, 428, 26173, 307, 588, 11, 588, 1998, 281, 472, 11, 300, 1355, 291, 366, 11, 428, 3209, 307, 4362, 50924, 50924, 32884, 264, 2517, 8944, 11, 550, 341, 486, 1813, 472, 670, 472, 293, 413, 3565, 26173, 307, 445, 51164, 51164, 2170, 4678, 807, 13, 583, 498, 428, 33783, 366, 42892, 13279, 11, 370, 498, 264, 3006, 51428, 51428, 2517, 510, 307, 1242, 257, 588, 2295, 8482, 11, 550, 502, 13, 15, 26764, 538, 309, 486, 9194, 341, 293, 550, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10461181181448477, "compression_ratio": 1.810077519379845, "no_speech_prob": 6.540228696394479e-06}, {"id": 220, "seek": 121400, "start": 1220.8, "end": 1225.2, "text": " So if your props is very, very close to one, that means you are, your network is currently", "tokens": [50364, 1985, 13, 21944, 82, 300, 366, 11, 26173, 307, 516, 281, 312, 38969, 293, 550, 4478, 10829, 17207, 510, 13, 50660, 50704, 407, 498, 428, 26173, 307, 588, 11, 588, 1998, 281, 472, 11, 300, 1355, 291, 366, 11, 428, 3209, 307, 4362, 50924, 50924, 32884, 264, 2517, 8944, 11, 550, 341, 486, 1813, 472, 670, 472, 293, 413, 3565, 26173, 307, 445, 51164, 51164, 2170, 4678, 807, 13, 583, 498, 428, 33783, 366, 42892, 13279, 11, 370, 498, 264, 3006, 51428, 51428, 2517, 510, 307, 1242, 257, 588, 2295, 8482, 11, 550, 502, 13, 15, 26764, 538, 309, 486, 9194, 341, 293, 550, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10461181181448477, "compression_ratio": 1.810077519379845, "no_speech_prob": 6.540228696394479e-06}, {"id": 221, "seek": 121400, "start": 1225.2, "end": 1230.0, "text": " predicting the character correctly, then this will become one over one and D log props is just", "tokens": [50364, 1985, 13, 21944, 82, 300, 366, 11, 26173, 307, 516, 281, 312, 38969, 293, 550, 4478, 10829, 17207, 510, 13, 50660, 50704, 407, 498, 428, 26173, 307, 588, 11, 588, 1998, 281, 472, 11, 300, 1355, 291, 366, 11, 428, 3209, 307, 4362, 50924, 50924, 32884, 264, 2517, 8944, 11, 550, 341, 486, 1813, 472, 670, 472, 293, 413, 3565, 26173, 307, 445, 51164, 51164, 2170, 4678, 807, 13, 583, 498, 428, 33783, 366, 42892, 13279, 11, 370, 498, 264, 3006, 51428, 51428, 2517, 510, 307, 1242, 257, 588, 2295, 8482, 11, 550, 502, 13, 15, 26764, 538, 309, 486, 9194, 341, 293, 550, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10461181181448477, "compression_ratio": 1.810077519379845, "no_speech_prob": 6.540228696394479e-06}, {"id": 222, "seek": 121400, "start": 1230.0, "end": 1235.28, "text": " gets passed through. But if your probabilities are incorrectly assigned, so if the correct", "tokens": [50364, 1985, 13, 21944, 82, 300, 366, 11, 26173, 307, 516, 281, 312, 38969, 293, 550, 4478, 10829, 17207, 510, 13, 50660, 50704, 407, 498, 428, 26173, 307, 588, 11, 588, 1998, 281, 472, 11, 300, 1355, 291, 366, 11, 428, 3209, 307, 4362, 50924, 50924, 32884, 264, 2517, 8944, 11, 550, 341, 486, 1813, 472, 670, 472, 293, 413, 3565, 26173, 307, 445, 51164, 51164, 2170, 4678, 807, 13, 583, 498, 428, 33783, 366, 42892, 13279, 11, 370, 498, 264, 3006, 51428, 51428, 2517, 510, 307, 1242, 257, 588, 2295, 8482, 11, 550, 502, 13, 15, 26764, 538, 309, 486, 9194, 341, 293, 550, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10461181181448477, "compression_ratio": 1.810077519379845, "no_speech_prob": 6.540228696394479e-06}, {"id": 223, "seek": 121400, "start": 1235.28, "end": 1243.76, "text": " character here is getting a very low probability, then 1.0 dividing by it will boost this and then", "tokens": [50364, 1985, 13, 21944, 82, 300, 366, 11, 26173, 307, 516, 281, 312, 38969, 293, 550, 4478, 10829, 17207, 510, 13, 50660, 50704, 407, 498, 428, 26173, 307, 588, 11, 588, 1998, 281, 472, 11, 300, 1355, 291, 366, 11, 428, 3209, 307, 4362, 50924, 50924, 32884, 264, 2517, 8944, 11, 550, 341, 486, 1813, 472, 670, 472, 293, 413, 3565, 26173, 307, 445, 51164, 51164, 2170, 4678, 807, 13, 583, 498, 428, 33783, 366, 42892, 13279, 11, 370, 498, 264, 3006, 51428, 51428, 2517, 510, 307, 1242, 257, 588, 2295, 8482, 11, 550, 502, 13, 15, 26764, 538, 309, 486, 9194, 341, 293, 550, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.10461181181448477, "compression_ratio": 1.810077519379845, "no_speech_prob": 6.540228696394479e-06}, {"id": 224, "seek": 124376, "start": 1243.76, "end": 1248.48, "text": " multiply by the other props. So basically what this line is doing intuitively is it's taking", "tokens": [50364, 12972, 538, 264, 661, 26173, 13, 407, 1936, 437, 341, 1622, 307, 884, 46506, 307, 309, 311, 1940, 50600, 50600, 264, 5110, 300, 362, 257, 588, 2295, 8482, 4362, 13279, 293, 309, 311, 43117, 641, 16235, 13, 50844, 50884, 509, 393, 574, 412, 309, 300, 636, 13, 3087, 493, 307, 1207, 512, 566, 85, 13, 407, 321, 528, 264, 13760, 295, 341, 13, 51256, 51284, 823, 718, 385, 445, 10465, 510, 293, 733, 295, 5366, 437, 311, 2737, 510, 294, 2674, 11, 570, 286, 458, 51528, 51528, 309, 311, 257, 707, 857, 13181, 13, 492, 362, 264, 3565, 468, 300, 808, 484, 295, 264, 18161, 2533, 13, 1692, 437, 286, 478, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.12470533122187076, "compression_ratio": 1.6267123287671232, "no_speech_prob": 7.766820090182591e-06}, {"id": 225, "seek": 124376, "start": 1248.48, "end": 1253.36, "text": " the examples that have a very low probability currently assigned and it's boosting their gradient.", "tokens": [50364, 12972, 538, 264, 661, 26173, 13, 407, 1936, 437, 341, 1622, 307, 884, 46506, 307, 309, 311, 1940, 50600, 50600, 264, 5110, 300, 362, 257, 588, 2295, 8482, 4362, 13279, 293, 309, 311, 43117, 641, 16235, 13, 50844, 50884, 509, 393, 574, 412, 309, 300, 636, 13, 3087, 493, 307, 1207, 512, 566, 85, 13, 407, 321, 528, 264, 13760, 295, 341, 13, 51256, 51284, 823, 718, 385, 445, 10465, 510, 293, 733, 295, 5366, 437, 311, 2737, 510, 294, 2674, 11, 570, 286, 458, 51528, 51528, 309, 311, 257, 707, 857, 13181, 13, 492, 362, 264, 3565, 468, 300, 808, 484, 295, 264, 18161, 2533, 13, 1692, 437, 286, 478, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.12470533122187076, "compression_ratio": 1.6267123287671232, "no_speech_prob": 7.766820090182591e-06}, {"id": 226, "seek": 124376, "start": 1254.16, "end": 1261.6, "text": " You can look at it that way. Next up is count some imv. So we want the derivative of this.", "tokens": [50364, 12972, 538, 264, 661, 26173, 13, 407, 1936, 437, 341, 1622, 307, 884, 46506, 307, 309, 311, 1940, 50600, 50600, 264, 5110, 300, 362, 257, 588, 2295, 8482, 4362, 13279, 293, 309, 311, 43117, 641, 16235, 13, 50844, 50884, 509, 393, 574, 412, 309, 300, 636, 13, 3087, 493, 307, 1207, 512, 566, 85, 13, 407, 321, 528, 264, 13760, 295, 341, 13, 51256, 51284, 823, 718, 385, 445, 10465, 510, 293, 733, 295, 5366, 437, 311, 2737, 510, 294, 2674, 11, 570, 286, 458, 51528, 51528, 309, 311, 257, 707, 857, 13181, 13, 492, 362, 264, 3565, 468, 300, 808, 484, 295, 264, 18161, 2533, 13, 1692, 437, 286, 478, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.12470533122187076, "compression_ratio": 1.6267123287671232, "no_speech_prob": 7.766820090182591e-06}, {"id": 227, "seek": 124376, "start": 1262.16, "end": 1267.04, "text": " Now let me just pause here and kind of introduce what's happening here in general, because I know", "tokens": [50364, 12972, 538, 264, 661, 26173, 13, 407, 1936, 437, 341, 1622, 307, 884, 46506, 307, 309, 311, 1940, 50600, 50600, 264, 5110, 300, 362, 257, 588, 2295, 8482, 4362, 13279, 293, 309, 311, 43117, 641, 16235, 13, 50844, 50884, 509, 393, 574, 412, 309, 300, 636, 13, 3087, 493, 307, 1207, 512, 566, 85, 13, 407, 321, 528, 264, 13760, 295, 341, 13, 51256, 51284, 823, 718, 385, 445, 10465, 510, 293, 733, 295, 5366, 437, 311, 2737, 510, 294, 2674, 11, 570, 286, 458, 51528, 51528, 309, 311, 257, 707, 857, 13181, 13, 492, 362, 264, 3565, 468, 300, 808, 484, 295, 264, 18161, 2533, 13, 1692, 437, 286, 478, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.12470533122187076, "compression_ratio": 1.6267123287671232, "no_speech_prob": 7.766820090182591e-06}, {"id": 228, "seek": 124376, "start": 1267.04, "end": 1271.2, "text": " it's a little bit confusing. We have the logist that come out of the neural net. Here what I'm", "tokens": [50364, 12972, 538, 264, 661, 26173, 13, 407, 1936, 437, 341, 1622, 307, 884, 46506, 307, 309, 311, 1940, 50600, 50600, 264, 5110, 300, 362, 257, 588, 2295, 8482, 4362, 13279, 293, 309, 311, 43117, 641, 16235, 13, 50844, 50884, 509, 393, 574, 412, 309, 300, 636, 13, 3087, 493, 307, 1207, 512, 566, 85, 13, 407, 321, 528, 264, 13760, 295, 341, 13, 51256, 51284, 823, 718, 385, 445, 10465, 510, 293, 733, 295, 5366, 437, 311, 2737, 510, 294, 2674, 11, 570, 286, 458, 51528, 51528, 309, 311, 257, 707, 857, 13181, 13, 492, 362, 264, 3565, 468, 300, 808, 484, 295, 264, 18161, 2533, 13, 1692, 437, 286, 478, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.12470533122187076, "compression_ratio": 1.6267123287671232, "no_speech_prob": 7.766820090182591e-06}, {"id": 229, "seek": 127120, "start": 1271.2, "end": 1276.72, "text": " doing is I'm finding the maximum in each row and I'm subtracting it for the purpose of numerical", "tokens": [50364, 884, 307, 286, 478, 5006, 264, 6674, 294, 1184, 5386, 293, 286, 478, 16390, 278, 309, 337, 264, 4334, 295, 29054, 50640, 50640, 11826, 13, 400, 321, 2825, 466, 577, 498, 291, 360, 406, 360, 341, 11, 291, 1190, 666, 29054, 2663, 498, 512, 295, 50880, 50880, 264, 3565, 1208, 747, 322, 886, 2416, 4190, 570, 321, 917, 493, 12680, 23012, 990, 552, 13, 407, 341, 307, 1096, 445, 337, 51188, 51188, 4514, 7866, 984, 13, 1396, 510, 311, 264, 37871, 6642, 295, 439, 264, 1333, 295, 411, 3565, 1208, 281, 1884, 527, 51508, 51508, 14893, 13, 400, 550, 321, 528, 281, 747, 264, 2408, 295, 613, 14893, 293, 2710, 1125, 370, 300, 439, 264, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.08439193052404068, "compression_ratio": 1.7445255474452555, "no_speech_prob": 2.3090196066277713e-07}, {"id": 230, "seek": 127120, "start": 1276.72, "end": 1281.52, "text": " stability. And we talked about how if you do not do this, you run into numerical issues if some of", "tokens": [50364, 884, 307, 286, 478, 5006, 264, 6674, 294, 1184, 5386, 293, 286, 478, 16390, 278, 309, 337, 264, 4334, 295, 29054, 50640, 50640, 11826, 13, 400, 321, 2825, 466, 577, 498, 291, 360, 406, 360, 341, 11, 291, 1190, 666, 29054, 2663, 498, 512, 295, 50880, 50880, 264, 3565, 1208, 747, 322, 886, 2416, 4190, 570, 321, 917, 493, 12680, 23012, 990, 552, 13, 407, 341, 307, 1096, 445, 337, 51188, 51188, 4514, 7866, 984, 13, 1396, 510, 311, 264, 37871, 6642, 295, 439, 264, 1333, 295, 411, 3565, 1208, 281, 1884, 527, 51508, 51508, 14893, 13, 400, 550, 321, 528, 281, 747, 264, 2408, 295, 613, 14893, 293, 2710, 1125, 370, 300, 439, 264, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.08439193052404068, "compression_ratio": 1.7445255474452555, "no_speech_prob": 2.3090196066277713e-07}, {"id": 231, "seek": 127120, "start": 1281.52, "end": 1287.68, "text": " the logits take on too large values because we end up exponentiating them. So this is done just for", "tokens": [50364, 884, 307, 286, 478, 5006, 264, 6674, 294, 1184, 5386, 293, 286, 478, 16390, 278, 309, 337, 264, 4334, 295, 29054, 50640, 50640, 11826, 13, 400, 321, 2825, 466, 577, 498, 291, 360, 406, 360, 341, 11, 291, 1190, 666, 29054, 2663, 498, 512, 295, 50880, 50880, 264, 3565, 1208, 747, 322, 886, 2416, 4190, 570, 321, 917, 493, 12680, 23012, 990, 552, 13, 407, 341, 307, 1096, 445, 337, 51188, 51188, 4514, 7866, 984, 13, 1396, 510, 311, 264, 37871, 6642, 295, 439, 264, 1333, 295, 411, 3565, 1208, 281, 1884, 527, 51508, 51508, 14893, 13, 400, 550, 321, 528, 281, 747, 264, 2408, 295, 613, 14893, 293, 2710, 1125, 370, 300, 439, 264, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.08439193052404068, "compression_ratio": 1.7445255474452555, "no_speech_prob": 2.3090196066277713e-07}, {"id": 232, "seek": 127120, "start": 1287.68, "end": 1294.0800000000002, "text": " safety numerically. Then here's the exponentiation of all the sort of like logits to create our", "tokens": [50364, 884, 307, 286, 478, 5006, 264, 6674, 294, 1184, 5386, 293, 286, 478, 16390, 278, 309, 337, 264, 4334, 295, 29054, 50640, 50640, 11826, 13, 400, 321, 2825, 466, 577, 498, 291, 360, 406, 360, 341, 11, 291, 1190, 666, 29054, 2663, 498, 512, 295, 50880, 50880, 264, 3565, 1208, 747, 322, 886, 2416, 4190, 570, 321, 917, 493, 12680, 23012, 990, 552, 13, 407, 341, 307, 1096, 445, 337, 51188, 51188, 4514, 7866, 984, 13, 1396, 510, 311, 264, 37871, 6642, 295, 439, 264, 1333, 295, 411, 3565, 1208, 281, 1884, 527, 51508, 51508, 14893, 13, 400, 550, 321, 528, 281, 747, 264, 2408, 295, 613, 14893, 293, 2710, 1125, 370, 300, 439, 264, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.08439193052404068, "compression_ratio": 1.7445255474452555, "no_speech_prob": 2.3090196066277713e-07}, {"id": 233, "seek": 127120, "start": 1294.0800000000002, "end": 1299.76, "text": " counts. And then we want to take the sum of these counts and normalize so that all the", "tokens": [50364, 884, 307, 286, 478, 5006, 264, 6674, 294, 1184, 5386, 293, 286, 478, 16390, 278, 309, 337, 264, 4334, 295, 29054, 50640, 50640, 11826, 13, 400, 321, 2825, 466, 577, 498, 291, 360, 406, 360, 341, 11, 291, 1190, 666, 29054, 2663, 498, 512, 295, 50880, 50880, 264, 3565, 1208, 747, 322, 886, 2416, 4190, 570, 321, 917, 493, 12680, 23012, 990, 552, 13, 407, 341, 307, 1096, 445, 337, 51188, 51188, 4514, 7866, 984, 13, 1396, 510, 311, 264, 37871, 6642, 295, 439, 264, 1333, 295, 411, 3565, 1208, 281, 1884, 527, 51508, 51508, 14893, 13, 400, 550, 321, 528, 281, 747, 264, 2408, 295, 613, 14893, 293, 2710, 1125, 370, 300, 439, 264, 51792, 51792], "temperature": 0.0, "avg_logprob": -0.08439193052404068, "compression_ratio": 1.7445255474452555, "no_speech_prob": 2.3090196066277713e-07}, {"id": 234, "seek": 129976, "start": 1299.76, "end": 1306.16, "text": " probes sum to one. Now here instead of using one over count sum, I use raised to the power of negative", "tokens": [50364, 1239, 279, 2408, 281, 472, 13, 823, 510, 2602, 295, 1228, 472, 670, 1207, 2408, 11, 286, 764, 6005, 281, 264, 1347, 295, 3671, 50684, 50684, 472, 13, 15776, 40197, 436, 366, 14800, 13, 286, 445, 1352, 300, 456, 311, 746, 2085, 365, 264, 50872, 50872, 9953, 51, 284, 339, 11420, 295, 264, 23897, 1320, 295, 10044, 293, 309, 2709, 411, 257, 3657, 1874, 11, 51160, 51160, 457, 300, 1177, 380, 1051, 337, 3543, 3543, 3671, 472, 13, 407, 286, 478, 1228, 341, 8513, 2602, 13, 583, 1936, 51448, 51448, 439, 300, 311, 2737, 510, 307, 321, 658, 264, 3565, 1208, 11, 321, 528, 281, 37871, 13024, 439, 295, 552, 293, 321, 528, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10666863531128973, "compression_ratio": 1.708185053380783, "no_speech_prob": 6.74788770993473e-06}, {"id": 235, "seek": 129976, "start": 1306.16, "end": 1309.92, "text": " one. Mathematically they are identical. I just found that there's something wrong with the", "tokens": [50364, 1239, 279, 2408, 281, 472, 13, 823, 510, 2602, 295, 1228, 472, 670, 1207, 2408, 11, 286, 764, 6005, 281, 264, 1347, 295, 3671, 50684, 50684, 472, 13, 15776, 40197, 436, 366, 14800, 13, 286, 445, 1352, 300, 456, 311, 746, 2085, 365, 264, 50872, 50872, 9953, 51, 284, 339, 11420, 295, 264, 23897, 1320, 295, 10044, 293, 309, 2709, 411, 257, 3657, 1874, 11, 51160, 51160, 457, 300, 1177, 380, 1051, 337, 3543, 3543, 3671, 472, 13, 407, 286, 478, 1228, 341, 8513, 2602, 13, 583, 1936, 51448, 51448, 439, 300, 311, 2737, 510, 307, 321, 658, 264, 3565, 1208, 11, 321, 528, 281, 37871, 13024, 439, 295, 552, 293, 321, 528, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10666863531128973, "compression_ratio": 1.708185053380783, "no_speech_prob": 6.74788770993473e-06}, {"id": 236, "seek": 129976, "start": 1309.92, "end": 1315.68, "text": " PyTorch implementation of the backward pass of division and it gives like a weird result,", "tokens": [50364, 1239, 279, 2408, 281, 472, 13, 823, 510, 2602, 295, 1228, 472, 670, 1207, 2408, 11, 286, 764, 6005, 281, 264, 1347, 295, 3671, 50684, 50684, 472, 13, 15776, 40197, 436, 366, 14800, 13, 286, 445, 1352, 300, 456, 311, 746, 2085, 365, 264, 50872, 50872, 9953, 51, 284, 339, 11420, 295, 264, 23897, 1320, 295, 10044, 293, 309, 2709, 411, 257, 3657, 1874, 11, 51160, 51160, 457, 300, 1177, 380, 1051, 337, 3543, 3543, 3671, 472, 13, 407, 286, 478, 1228, 341, 8513, 2602, 13, 583, 1936, 51448, 51448, 439, 300, 311, 2737, 510, 307, 321, 658, 264, 3565, 1208, 11, 321, 528, 281, 37871, 13024, 439, 295, 552, 293, 321, 528, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10666863531128973, "compression_ratio": 1.708185053380783, "no_speech_prob": 6.74788770993473e-06}, {"id": 237, "seek": 129976, "start": 1315.68, "end": 1321.44, "text": " but that doesn't happen for star star negative one. So I'm using this formula instead. But basically", "tokens": [50364, 1239, 279, 2408, 281, 472, 13, 823, 510, 2602, 295, 1228, 472, 670, 1207, 2408, 11, 286, 764, 6005, 281, 264, 1347, 295, 3671, 50684, 50684, 472, 13, 15776, 40197, 436, 366, 14800, 13, 286, 445, 1352, 300, 456, 311, 746, 2085, 365, 264, 50872, 50872, 9953, 51, 284, 339, 11420, 295, 264, 23897, 1320, 295, 10044, 293, 309, 2709, 411, 257, 3657, 1874, 11, 51160, 51160, 457, 300, 1177, 380, 1051, 337, 3543, 3543, 3671, 472, 13, 407, 286, 478, 1228, 341, 8513, 2602, 13, 583, 1936, 51448, 51448, 439, 300, 311, 2737, 510, 307, 321, 658, 264, 3565, 1208, 11, 321, 528, 281, 37871, 13024, 439, 295, 552, 293, 321, 528, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10666863531128973, "compression_ratio": 1.708185053380783, "no_speech_prob": 6.74788770993473e-06}, {"id": 238, "seek": 129976, "start": 1321.44, "end": 1325.76, "text": " all that's happening here is we got the logits, we want to exponentiate all of them and we want", "tokens": [50364, 1239, 279, 2408, 281, 472, 13, 823, 510, 2602, 295, 1228, 472, 670, 1207, 2408, 11, 286, 764, 6005, 281, 264, 1347, 295, 3671, 50684, 50684, 472, 13, 15776, 40197, 436, 366, 14800, 13, 286, 445, 1352, 300, 456, 311, 746, 2085, 365, 264, 50872, 50872, 9953, 51, 284, 339, 11420, 295, 264, 23897, 1320, 295, 10044, 293, 309, 2709, 411, 257, 3657, 1874, 11, 51160, 51160, 457, 300, 1177, 380, 1051, 337, 3543, 3543, 3671, 472, 13, 407, 286, 478, 1228, 341, 8513, 2602, 13, 583, 1936, 51448, 51448, 439, 300, 311, 2737, 510, 307, 321, 658, 264, 3565, 1208, 11, 321, 528, 281, 37871, 13024, 439, 295, 552, 293, 321, 528, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.10666863531128973, "compression_ratio": 1.708185053380783, "no_speech_prob": 6.74788770993473e-06}, {"id": 239, "seek": 132576, "start": 1325.76, "end": 1330.96, "text": " to normalize the counts to create our probabilities. It's just that it's happening across multiple", "tokens": [50364, 281, 2710, 1125, 264, 14893, 281, 1884, 527, 33783, 13, 467, 311, 445, 300, 309, 311, 2737, 2108, 3866, 50624, 50624, 3876, 13, 407, 586, 510, 321, 528, 281, 700, 747, 264, 13760, 11, 321, 528, 281, 646, 79, 1513, 559, 473, 666, 51156, 51156, 1207, 2408, 293, 550, 666, 14893, 382, 731, 13, 407, 437, 820, 312, 264, 1207, 2408, 30, 823, 321, 767, 362, 281, 51520, 51520, 312, 5026, 510, 570, 321, 362, 281, 28949, 259, 1125, 293, 312, 5026, 365, 264, 10854, 13, 407, 14893, 300, 3909, 51816, 51856], "temperature": 0.0, "avg_logprob": -0.09493690855959629, "compression_ratio": 1.7162162162162162, "no_speech_prob": 5.862719262950122e-06}, {"id": 240, "seek": 132576, "start": 1330.96, "end": 1341.6, "text": " lines. So now here we want to first take the derivative, we want to backpropagate into", "tokens": [50364, 281, 2710, 1125, 264, 14893, 281, 1884, 527, 33783, 13, 467, 311, 445, 300, 309, 311, 2737, 2108, 3866, 50624, 50624, 3876, 13, 407, 586, 510, 321, 528, 281, 700, 747, 264, 13760, 11, 321, 528, 281, 646, 79, 1513, 559, 473, 666, 51156, 51156, 1207, 2408, 293, 550, 666, 14893, 382, 731, 13, 407, 437, 820, 312, 264, 1207, 2408, 30, 823, 321, 767, 362, 281, 51520, 51520, 312, 5026, 510, 570, 321, 362, 281, 28949, 259, 1125, 293, 312, 5026, 365, 264, 10854, 13, 407, 14893, 300, 3909, 51816, 51856], "temperature": 0.0, "avg_logprob": -0.09493690855959629, "compression_ratio": 1.7162162162162162, "no_speech_prob": 5.862719262950122e-06}, {"id": 241, "seek": 132576, "start": 1341.6, "end": 1348.8799999999999, "text": " count sum and then into counts as well. So what should be the count sum? Now we actually have to", "tokens": [50364, 281, 2710, 1125, 264, 14893, 281, 1884, 527, 33783, 13, 467, 311, 445, 300, 309, 311, 2737, 2108, 3866, 50624, 50624, 3876, 13, 407, 586, 510, 321, 528, 281, 700, 747, 264, 13760, 11, 321, 528, 281, 646, 79, 1513, 559, 473, 666, 51156, 51156, 1207, 2408, 293, 550, 666, 14893, 382, 731, 13, 407, 437, 820, 312, 264, 1207, 2408, 30, 823, 321, 767, 362, 281, 51520, 51520, 312, 5026, 510, 570, 321, 362, 281, 28949, 259, 1125, 293, 312, 5026, 365, 264, 10854, 13, 407, 14893, 300, 3909, 51816, 51856], "temperature": 0.0, "avg_logprob": -0.09493690855959629, "compression_ratio": 1.7162162162162162, "no_speech_prob": 5.862719262950122e-06}, {"id": 242, "seek": 132576, "start": 1348.8799999999999, "end": 1354.8, "text": " be careful here because we have to scrutinize and be careful with the shapes. So counts that shape", "tokens": [50364, 281, 2710, 1125, 264, 14893, 281, 1884, 527, 33783, 13, 467, 311, 445, 300, 309, 311, 2737, 2108, 3866, 50624, 50624, 3876, 13, 407, 586, 510, 321, 528, 281, 700, 747, 264, 13760, 11, 321, 528, 281, 646, 79, 1513, 559, 473, 666, 51156, 51156, 1207, 2408, 293, 550, 666, 14893, 382, 731, 13, 407, 437, 820, 312, 264, 1207, 2408, 30, 823, 321, 767, 362, 281, 51520, 51520, 312, 5026, 510, 570, 321, 362, 281, 28949, 259, 1125, 293, 312, 5026, 365, 264, 10854, 13, 407, 14893, 300, 3909, 51816, 51856], "temperature": 0.0, "avg_logprob": -0.09493690855959629, "compression_ratio": 1.7162162162162162, "no_speech_prob": 5.862719262950122e-06}, {"id": 243, "seek": 135480, "start": 1354.8, "end": 1363.04, "text": " and then count sum in that shape are different. So in particular counts is 32 by 27, but this count", "tokens": [50364, 293, 550, 1207, 2408, 294, 300, 3909, 366, 819, 13, 407, 294, 1729, 14893, 307, 8858, 538, 7634, 11, 457, 341, 1207, 50776, 50776, 2408, 307, 8858, 538, 472, 13, 400, 370, 294, 341, 27290, 510, 321, 611, 362, 364, 26947, 30024, 51100, 51128, 300, 9953, 51, 284, 339, 486, 360, 570, 309, 2203, 281, 747, 341, 7738, 40863, 295, 8858, 3547, 293, 25356, 51364, 51364, 309, 33796, 7634, 1413, 281, 7975, 613, 732, 10688, 830, 370, 309, 393, 360, 364, 4478, 12, 3711, 12972, 13, 51636, 51676], "temperature": 0.0, "avg_logprob": -0.18182476891411675, "compression_ratio": 1.5914893617021277, "no_speech_prob": 6.540143658639863e-06}, {"id": 244, "seek": 135480, "start": 1363.04, "end": 1369.52, "text": " sum is 32 by one. And so in this multiplication here we also have an implicit broadcasting", "tokens": [50364, 293, 550, 1207, 2408, 294, 300, 3909, 366, 819, 13, 407, 294, 1729, 14893, 307, 8858, 538, 7634, 11, 457, 341, 1207, 50776, 50776, 2408, 307, 8858, 538, 472, 13, 400, 370, 294, 341, 27290, 510, 321, 611, 362, 364, 26947, 30024, 51100, 51128, 300, 9953, 51, 284, 339, 486, 360, 570, 309, 2203, 281, 747, 341, 7738, 40863, 295, 8858, 3547, 293, 25356, 51364, 51364, 309, 33796, 7634, 1413, 281, 7975, 613, 732, 10688, 830, 370, 309, 393, 360, 364, 4478, 12, 3711, 12972, 13, 51636, 51676], "temperature": 0.0, "avg_logprob": -0.18182476891411675, "compression_ratio": 1.5914893617021277, "no_speech_prob": 6.540143658639863e-06}, {"id": 245, "seek": 135480, "start": 1370.08, "end": 1374.8, "text": " that PyTorch will do because it needs to take this column tensor of 32 numbers and replicate", "tokens": [50364, 293, 550, 1207, 2408, 294, 300, 3909, 366, 819, 13, 407, 294, 1729, 14893, 307, 8858, 538, 7634, 11, 457, 341, 1207, 50776, 50776, 2408, 307, 8858, 538, 472, 13, 400, 370, 294, 341, 27290, 510, 321, 611, 362, 364, 26947, 30024, 51100, 51128, 300, 9953, 51, 284, 339, 486, 360, 570, 309, 2203, 281, 747, 341, 7738, 40863, 295, 8858, 3547, 293, 25356, 51364, 51364, 309, 33796, 7634, 1413, 281, 7975, 613, 732, 10688, 830, 370, 309, 393, 360, 364, 4478, 12, 3711, 12972, 13, 51636, 51676], "temperature": 0.0, "avg_logprob": -0.18182476891411675, "compression_ratio": 1.5914893617021277, "no_speech_prob": 6.540143658639863e-06}, {"id": 246, "seek": 135480, "start": 1374.8, "end": 1380.24, "text": " it horizontally 27 times to align these two tensors so it can do an element-wise multiply.", "tokens": [50364, 293, 550, 1207, 2408, 294, 300, 3909, 366, 819, 13, 407, 294, 1729, 14893, 307, 8858, 538, 7634, 11, 457, 341, 1207, 50776, 50776, 2408, 307, 8858, 538, 472, 13, 400, 370, 294, 341, 27290, 510, 321, 611, 362, 364, 26947, 30024, 51100, 51128, 300, 9953, 51, 284, 339, 486, 360, 570, 309, 2203, 281, 747, 341, 7738, 40863, 295, 8858, 3547, 293, 25356, 51364, 51364, 309, 33796, 7634, 1413, 281, 7975, 613, 732, 10688, 830, 370, 309, 393, 360, 364, 4478, 12, 3711, 12972, 13, 51636, 51676], "temperature": 0.0, "avg_logprob": -0.18182476891411675, "compression_ratio": 1.5914893617021277, "no_speech_prob": 6.540143658639863e-06}, {"id": 247, "seek": 138024, "start": 1380.24, "end": 1385.52, "text": " So really what this looks like is the following using a toy example again. What we really have", "tokens": [50364, 407, 534, 437, 341, 1542, 411, 307, 264, 3480, 1228, 257, 12058, 1365, 797, 13, 708, 321, 534, 362, 50628, 50628, 510, 307, 445, 26173, 307, 14893, 1413, 1207, 2408, 11, 370, 309, 311, 269, 6915, 257, 1413, 272, 11, 457, 257, 307, 1045, 538, 1045, 50972, 50972, 293, 272, 307, 445, 1045, 538, 472, 11, 257, 7738, 40863, 13, 400, 370, 9953, 51, 284, 339, 19501, 46365, 341, 51220, 51220, 4959, 295, 272, 293, 309, 630, 300, 2108, 439, 264, 13766, 13, 407, 337, 1365, 272, 16, 11, 597, 307, 264, 700, 51508, 51508, 4478, 295, 272, 11, 576, 312, 46365, 510, 2108, 439, 264, 13766, 294, 341, 27290, 13, 400, 586, 321, 434, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.16712071715282792, "compression_ratio": 1.7773584905660378, "no_speech_prob": 3.1380718610307667e-06}, {"id": 248, "seek": 138024, "start": 1385.52, "end": 1392.4, "text": " here is just props is counts times count sum, so it's c equals a times b, but a is three by three", "tokens": [50364, 407, 534, 437, 341, 1542, 411, 307, 264, 3480, 1228, 257, 12058, 1365, 797, 13, 708, 321, 534, 362, 50628, 50628, 510, 307, 445, 26173, 307, 14893, 1413, 1207, 2408, 11, 370, 309, 311, 269, 6915, 257, 1413, 272, 11, 457, 257, 307, 1045, 538, 1045, 50972, 50972, 293, 272, 307, 445, 1045, 538, 472, 11, 257, 7738, 40863, 13, 400, 370, 9953, 51, 284, 339, 19501, 46365, 341, 51220, 51220, 4959, 295, 272, 293, 309, 630, 300, 2108, 439, 264, 13766, 13, 407, 337, 1365, 272, 16, 11, 597, 307, 264, 700, 51508, 51508, 4478, 295, 272, 11, 576, 312, 46365, 510, 2108, 439, 264, 13766, 294, 341, 27290, 13, 400, 586, 321, 434, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.16712071715282792, "compression_ratio": 1.7773584905660378, "no_speech_prob": 3.1380718610307667e-06}, {"id": 249, "seek": 138024, "start": 1392.4, "end": 1397.36, "text": " and b is just three by one, a column tensor. And so PyTorch internally replicated this", "tokens": [50364, 407, 534, 437, 341, 1542, 411, 307, 264, 3480, 1228, 257, 12058, 1365, 797, 13, 708, 321, 534, 362, 50628, 50628, 510, 307, 445, 26173, 307, 14893, 1413, 1207, 2408, 11, 370, 309, 311, 269, 6915, 257, 1413, 272, 11, 457, 257, 307, 1045, 538, 1045, 50972, 50972, 293, 272, 307, 445, 1045, 538, 472, 11, 257, 7738, 40863, 13, 400, 370, 9953, 51, 284, 339, 19501, 46365, 341, 51220, 51220, 4959, 295, 272, 293, 309, 630, 300, 2108, 439, 264, 13766, 13, 407, 337, 1365, 272, 16, 11, 597, 307, 264, 700, 51508, 51508, 4478, 295, 272, 11, 576, 312, 46365, 510, 2108, 439, 264, 13766, 294, 341, 27290, 13, 400, 586, 321, 434, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.16712071715282792, "compression_ratio": 1.7773584905660378, "no_speech_prob": 3.1380718610307667e-06}, {"id": 250, "seek": 138024, "start": 1397.36, "end": 1403.1200000000001, "text": " elements of b and it did that across all the columns. So for example b1, which is the first", "tokens": [50364, 407, 534, 437, 341, 1542, 411, 307, 264, 3480, 1228, 257, 12058, 1365, 797, 13, 708, 321, 534, 362, 50628, 50628, 510, 307, 445, 26173, 307, 14893, 1413, 1207, 2408, 11, 370, 309, 311, 269, 6915, 257, 1413, 272, 11, 457, 257, 307, 1045, 538, 1045, 50972, 50972, 293, 272, 307, 445, 1045, 538, 472, 11, 257, 7738, 40863, 13, 400, 370, 9953, 51, 284, 339, 19501, 46365, 341, 51220, 51220, 4959, 295, 272, 293, 309, 630, 300, 2108, 439, 264, 13766, 13, 407, 337, 1365, 272, 16, 11, 597, 307, 264, 700, 51508, 51508, 4478, 295, 272, 11, 576, 312, 46365, 510, 2108, 439, 264, 13766, 294, 341, 27290, 13, 400, 586, 321, 434, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.16712071715282792, "compression_ratio": 1.7773584905660378, "no_speech_prob": 3.1380718610307667e-06}, {"id": 251, "seek": 138024, "start": 1403.1200000000001, "end": 1408.56, "text": " element of b, would be replicated here across all the columns in this multiplication. And now we're", "tokens": [50364, 407, 534, 437, 341, 1542, 411, 307, 264, 3480, 1228, 257, 12058, 1365, 797, 13, 708, 321, 534, 362, 50628, 50628, 510, 307, 445, 26173, 307, 14893, 1413, 1207, 2408, 11, 370, 309, 311, 269, 6915, 257, 1413, 272, 11, 457, 257, 307, 1045, 538, 1045, 50972, 50972, 293, 272, 307, 445, 1045, 538, 472, 11, 257, 7738, 40863, 13, 400, 370, 9953, 51, 284, 339, 19501, 46365, 341, 51220, 51220, 4959, 295, 272, 293, 309, 630, 300, 2108, 439, 264, 13766, 13, 407, 337, 1365, 272, 16, 11, 597, 307, 264, 700, 51508, 51508, 4478, 295, 272, 11, 576, 312, 46365, 510, 2108, 439, 264, 13766, 294, 341, 27290, 13, 400, 586, 321, 434, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.16712071715282792, "compression_ratio": 1.7773584905660378, "no_speech_prob": 3.1380718610307667e-06}, {"id": 252, "seek": 140856, "start": 1408.56, "end": 1414.32, "text": " trying to backpropagate through this operation to count sum in. So when we are calculating this", "tokens": [50364, 1382, 281, 646, 79, 1513, 559, 473, 807, 341, 6916, 281, 1207, 2408, 294, 13, 407, 562, 321, 366, 28258, 341, 50652, 50652, 13760, 309, 311, 1021, 281, 4325, 300, 613, 732, 11, 341, 1542, 411, 257, 2167, 6916, 11, 457, 50944, 50944, 767, 307, 732, 7705, 6456, 5123, 3137, 13, 440, 700, 6916, 300, 9953, 51, 284, 339, 630, 307, 309, 1890, 51244, 51244, 341, 7738, 40863, 293, 46365, 309, 2108, 439, 264, 13766, 1936, 7634, 1413, 13, 407, 300, 311, 264, 51612, 51612, 700, 6916, 11, 309, 311, 257, 39911, 13, 400, 550, 264, 1150, 6916, 307, 264, 27290, 295, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.1735329178144347, "compression_ratio": 1.757462686567164, "no_speech_prob": 3.966909844166366e-06}, {"id": 253, "seek": 140856, "start": 1414.32, "end": 1420.1599999999999, "text": " derivative it's important to realize that these two, this looks like a single operation, but", "tokens": [50364, 1382, 281, 646, 79, 1513, 559, 473, 807, 341, 6916, 281, 1207, 2408, 294, 13, 407, 562, 321, 366, 28258, 341, 50652, 50652, 13760, 309, 311, 1021, 281, 4325, 300, 613, 732, 11, 341, 1542, 411, 257, 2167, 6916, 11, 457, 50944, 50944, 767, 307, 732, 7705, 6456, 5123, 3137, 13, 440, 700, 6916, 300, 9953, 51, 284, 339, 630, 307, 309, 1890, 51244, 51244, 341, 7738, 40863, 293, 46365, 309, 2108, 439, 264, 13766, 1936, 7634, 1413, 13, 407, 300, 311, 264, 51612, 51612, 700, 6916, 11, 309, 311, 257, 39911, 13, 400, 550, 264, 1150, 6916, 307, 264, 27290, 295, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.1735329178144347, "compression_ratio": 1.757462686567164, "no_speech_prob": 3.966909844166366e-06}, {"id": 254, "seek": 140856, "start": 1420.1599999999999, "end": 1426.1599999999999, "text": " actually is two operations applied sequentially. The first operation that PyTorch did is it took", "tokens": [50364, 1382, 281, 646, 79, 1513, 559, 473, 807, 341, 6916, 281, 1207, 2408, 294, 13, 407, 562, 321, 366, 28258, 341, 50652, 50652, 13760, 309, 311, 1021, 281, 4325, 300, 613, 732, 11, 341, 1542, 411, 257, 2167, 6916, 11, 457, 50944, 50944, 767, 307, 732, 7705, 6456, 5123, 3137, 13, 440, 700, 6916, 300, 9953, 51, 284, 339, 630, 307, 309, 1890, 51244, 51244, 341, 7738, 40863, 293, 46365, 309, 2108, 439, 264, 13766, 1936, 7634, 1413, 13, 407, 300, 311, 264, 51612, 51612, 700, 6916, 11, 309, 311, 257, 39911, 13, 400, 550, 264, 1150, 6916, 307, 264, 27290, 295, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.1735329178144347, "compression_ratio": 1.757462686567164, "no_speech_prob": 3.966909844166366e-06}, {"id": 255, "seek": 140856, "start": 1426.1599999999999, "end": 1433.52, "text": " this column tensor and replicated it across all the columns basically 27 times. So that's the", "tokens": [50364, 1382, 281, 646, 79, 1513, 559, 473, 807, 341, 6916, 281, 1207, 2408, 294, 13, 407, 562, 321, 366, 28258, 341, 50652, 50652, 13760, 309, 311, 1021, 281, 4325, 300, 613, 732, 11, 341, 1542, 411, 257, 2167, 6916, 11, 457, 50944, 50944, 767, 307, 732, 7705, 6456, 5123, 3137, 13, 440, 700, 6916, 300, 9953, 51, 284, 339, 630, 307, 309, 1890, 51244, 51244, 341, 7738, 40863, 293, 46365, 309, 2108, 439, 264, 13766, 1936, 7634, 1413, 13, 407, 300, 311, 264, 51612, 51612, 700, 6916, 11, 309, 311, 257, 39911, 13, 400, 550, 264, 1150, 6916, 307, 264, 27290, 295, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.1735329178144347, "compression_ratio": 1.757462686567164, "no_speech_prob": 3.966909844166366e-06}, {"id": 256, "seek": 140856, "start": 1433.52, "end": 1437.76, "text": " first operation, it's a replication. And then the second operation is the multiplication of", "tokens": [50364, 1382, 281, 646, 79, 1513, 559, 473, 807, 341, 6916, 281, 1207, 2408, 294, 13, 407, 562, 321, 366, 28258, 341, 50652, 50652, 13760, 309, 311, 1021, 281, 4325, 300, 613, 732, 11, 341, 1542, 411, 257, 2167, 6916, 11, 457, 50944, 50944, 767, 307, 732, 7705, 6456, 5123, 3137, 13, 440, 700, 6916, 300, 9953, 51, 284, 339, 630, 307, 309, 1890, 51244, 51244, 341, 7738, 40863, 293, 46365, 309, 2108, 439, 264, 13766, 1936, 7634, 1413, 13, 407, 300, 311, 264, 51612, 51612, 700, 6916, 11, 309, 311, 257, 39911, 13, 400, 550, 264, 1150, 6916, 307, 264, 27290, 295, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.1735329178144347, "compression_ratio": 1.757462686567164, "no_speech_prob": 3.966909844166366e-06}, {"id": 257, "seek": 143776, "start": 1437.76, "end": 1444.08, "text": " the two columns. So let's first backprop through the multiplication. If these two arrays were of", "tokens": [50364, 264, 732, 13766, 13, 407, 718, 311, 700, 646, 79, 1513, 807, 264, 27290, 13, 759, 613, 732, 41011, 645, 295, 50680, 50680, 264, 912, 2744, 293, 321, 445, 362, 257, 293, 272, 11, 1293, 295, 552, 1045, 538, 1045, 11, 550, 577, 360, 321, 646, 79, 1513, 559, 473, 51016, 51016, 807, 257, 27290, 30, 407, 498, 321, 445, 362, 15664, 685, 293, 406, 10688, 830, 11, 550, 498, 291, 362, 269, 6915, 257, 51256, 51256, 1413, 272, 11, 550, 437, 307, 264, 13760, 295, 269, 365, 3104, 281, 272, 30, 1042, 11, 309, 311, 445, 257, 13, 400, 370, 300, 311, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.2099381562705352, "compression_ratio": 1.685589519650655, "no_speech_prob": 9.080052222998347e-06}, {"id": 258, "seek": 143776, "start": 1444.08, "end": 1450.8, "text": " the same size and we just have a and b, both of them three by three, then how do we backpropagate", "tokens": [50364, 264, 732, 13766, 13, 407, 718, 311, 700, 646, 79, 1513, 807, 264, 27290, 13, 759, 613, 732, 41011, 645, 295, 50680, 50680, 264, 912, 2744, 293, 321, 445, 362, 257, 293, 272, 11, 1293, 295, 552, 1045, 538, 1045, 11, 550, 577, 360, 321, 646, 79, 1513, 559, 473, 51016, 51016, 807, 257, 27290, 30, 407, 498, 321, 445, 362, 15664, 685, 293, 406, 10688, 830, 11, 550, 498, 291, 362, 269, 6915, 257, 51256, 51256, 1413, 272, 11, 550, 437, 307, 264, 13760, 295, 269, 365, 3104, 281, 272, 30, 1042, 11, 309, 311, 445, 257, 13, 400, 370, 300, 311, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.2099381562705352, "compression_ratio": 1.685589519650655, "no_speech_prob": 9.080052222998347e-06}, {"id": 259, "seek": 143776, "start": 1450.8, "end": 1455.6, "text": " through a multiplication? So if we just have scalars and not tensors, then if you have c equals a", "tokens": [50364, 264, 732, 13766, 13, 407, 718, 311, 700, 646, 79, 1513, 807, 264, 27290, 13, 759, 613, 732, 41011, 645, 295, 50680, 50680, 264, 912, 2744, 293, 321, 445, 362, 257, 293, 272, 11, 1293, 295, 552, 1045, 538, 1045, 11, 550, 577, 360, 321, 646, 79, 1513, 559, 473, 51016, 51016, 807, 257, 27290, 30, 407, 498, 321, 445, 362, 15664, 685, 293, 406, 10688, 830, 11, 550, 498, 291, 362, 269, 6915, 257, 51256, 51256, 1413, 272, 11, 550, 437, 307, 264, 13760, 295, 269, 365, 3104, 281, 272, 30, 1042, 11, 309, 311, 445, 257, 13, 400, 370, 300, 311, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.2099381562705352, "compression_ratio": 1.685589519650655, "no_speech_prob": 9.080052222998347e-06}, {"id": 260, "seek": 143776, "start": 1455.6, "end": 1462.24, "text": " times b, then what is the derivative of c with respect to b? Well, it's just a. And so that's", "tokens": [50364, 264, 732, 13766, 13, 407, 718, 311, 700, 646, 79, 1513, 807, 264, 27290, 13, 759, 613, 732, 41011, 645, 295, 50680, 50680, 264, 912, 2744, 293, 321, 445, 362, 257, 293, 272, 11, 1293, 295, 552, 1045, 538, 1045, 11, 550, 577, 360, 321, 646, 79, 1513, 559, 473, 51016, 51016, 807, 257, 27290, 30, 407, 498, 321, 445, 362, 15664, 685, 293, 406, 10688, 830, 11, 550, 498, 291, 362, 269, 6915, 257, 51256, 51256, 1413, 272, 11, 550, 437, 307, 264, 13760, 295, 269, 365, 3104, 281, 272, 30, 1042, 11, 309, 311, 445, 257, 13, 400, 370, 300, 311, 51588, 51588], "temperature": 0.0, "avg_logprob": -0.2099381562705352, "compression_ratio": 1.685589519650655, "no_speech_prob": 9.080052222998347e-06}, {"id": 261, "seek": 146224, "start": 1462.24, "end": 1468.16, "text": " the local derivative. So here in our case, undoing the multiplication and backpropagating through", "tokens": [50364, 264, 2654, 13760, 13, 407, 510, 294, 527, 1389, 11, 23779, 278, 264, 27290, 293, 646, 79, 1513, 559, 990, 807, 50660, 50660, 445, 264, 27290, 2564, 11, 597, 307, 4478, 12, 3711, 11, 307, 516, 281, 312, 264, 2654, 13760, 11, 50892, 50892, 597, 294, 341, 1389, 307, 2935, 14893, 11, 570, 14893, 307, 264, 257, 13, 407, 341, 307, 264, 2654, 13760, 51264, 51264, 293, 550, 1413, 11, 570, 264, 5021, 4978, 11, 274, 4318, 929, 13, 407, 341, 510, 307, 264, 13760, 420, 264, 16235, 11, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.1509861279559392, "compression_ratio": 1.9045226130653266, "no_speech_prob": 7.296241165022366e-06}, {"id": 262, "seek": 146224, "start": 1468.16, "end": 1472.8, "text": " just the multiplication itself, which is element-wise, is going to be the local derivative,", "tokens": [50364, 264, 2654, 13760, 13, 407, 510, 294, 527, 1389, 11, 23779, 278, 264, 27290, 293, 646, 79, 1513, 559, 990, 807, 50660, 50660, 445, 264, 27290, 2564, 11, 597, 307, 4478, 12, 3711, 11, 307, 516, 281, 312, 264, 2654, 13760, 11, 50892, 50892, 597, 294, 341, 1389, 307, 2935, 14893, 11, 570, 14893, 307, 264, 257, 13, 407, 341, 307, 264, 2654, 13760, 51264, 51264, 293, 550, 1413, 11, 570, 264, 5021, 4978, 11, 274, 4318, 929, 13, 407, 341, 510, 307, 264, 13760, 420, 264, 16235, 11, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.1509861279559392, "compression_ratio": 1.9045226130653266, "no_speech_prob": 7.296241165022366e-06}, {"id": 263, "seek": 146224, "start": 1472.8, "end": 1480.24, "text": " which in this case is simply counts, because counts is the a. So this is the local derivative", "tokens": [50364, 264, 2654, 13760, 13, 407, 510, 294, 527, 1389, 11, 23779, 278, 264, 27290, 293, 646, 79, 1513, 559, 990, 807, 50660, 50660, 445, 264, 27290, 2564, 11, 597, 307, 4478, 12, 3711, 11, 307, 516, 281, 312, 264, 2654, 13760, 11, 50892, 50892, 597, 294, 341, 1389, 307, 2935, 14893, 11, 570, 14893, 307, 264, 257, 13, 407, 341, 307, 264, 2654, 13760, 51264, 51264, 293, 550, 1413, 11, 570, 264, 5021, 4978, 11, 274, 4318, 929, 13, 407, 341, 510, 307, 264, 13760, 420, 264, 16235, 11, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.1509861279559392, "compression_ratio": 1.9045226130653266, "no_speech_prob": 7.296241165022366e-06}, {"id": 264, "seek": 146224, "start": 1480.24, "end": 1488.08, "text": " and then times, because the chain rule, dprobs. So this here is the derivative or the gradient,", "tokens": [50364, 264, 2654, 13760, 13, 407, 510, 294, 527, 1389, 11, 23779, 278, 264, 27290, 293, 646, 79, 1513, 559, 990, 807, 50660, 50660, 445, 264, 27290, 2564, 11, 597, 307, 4478, 12, 3711, 11, 307, 516, 281, 312, 264, 2654, 13760, 11, 50892, 50892, 597, 294, 341, 1389, 307, 2935, 14893, 11, 570, 14893, 307, 264, 257, 13, 407, 341, 307, 264, 2654, 13760, 51264, 51264, 293, 550, 1413, 11, 570, 264, 5021, 4978, 11, 274, 4318, 929, 13, 407, 341, 510, 307, 264, 13760, 420, 264, 16235, 11, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.1509861279559392, "compression_ratio": 1.9045226130653266, "no_speech_prob": 7.296241165022366e-06}, {"id": 265, "seek": 148808, "start": 1488.08, "end": 1495.6799999999998, "text": " but with respect to replicated b. But we don't have a replicated b, we just have a single b column.", "tokens": [50364, 457, 365, 3104, 281, 46365, 272, 13, 583, 321, 500, 380, 362, 257, 46365, 272, 11, 321, 445, 362, 257, 2167, 272, 7738, 13, 50744, 50744, 407, 577, 360, 321, 586, 646, 79, 1513, 559, 473, 807, 264, 39911, 30, 400, 46506, 11, 341, 272, 16, 307, 264, 51056, 51056, 912, 7006, 293, 309, 311, 445, 319, 4717, 3866, 1413, 13, 400, 370, 291, 393, 574, 412, 309, 382, 885, 10344, 281, 51388, 51388, 257, 1389, 365, 20381, 294, 4532, 7165, 13, 400, 370, 510, 286, 478, 445, 8407, 484, 257, 4974, 4295, 321, 1143, 294, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.11378810882568359, "compression_ratio": 1.6134453781512605, "no_speech_prob": 1.577921239004354e-06}, {"id": 266, "seek": 148808, "start": 1495.6799999999998, "end": 1501.9199999999998, "text": " So how do we now backpropagate through the replication? And intuitively, this b1 is the", "tokens": [50364, 457, 365, 3104, 281, 46365, 272, 13, 583, 321, 500, 380, 362, 257, 46365, 272, 11, 321, 445, 362, 257, 2167, 272, 7738, 13, 50744, 50744, 407, 577, 360, 321, 586, 646, 79, 1513, 559, 473, 807, 264, 39911, 30, 400, 46506, 11, 341, 272, 16, 307, 264, 51056, 51056, 912, 7006, 293, 309, 311, 445, 319, 4717, 3866, 1413, 13, 400, 370, 291, 393, 574, 412, 309, 382, 885, 10344, 281, 51388, 51388, 257, 1389, 365, 20381, 294, 4532, 7165, 13, 400, 370, 510, 286, 478, 445, 8407, 484, 257, 4974, 4295, 321, 1143, 294, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.11378810882568359, "compression_ratio": 1.6134453781512605, "no_speech_prob": 1.577921239004354e-06}, {"id": 267, "seek": 148808, "start": 1501.9199999999998, "end": 1508.56, "text": " same variable and it's just reused multiple times. And so you can look at it as being equivalent to", "tokens": [50364, 457, 365, 3104, 281, 46365, 272, 13, 583, 321, 500, 380, 362, 257, 46365, 272, 11, 321, 445, 362, 257, 2167, 272, 7738, 13, 50744, 50744, 407, 577, 360, 321, 586, 646, 79, 1513, 559, 473, 807, 264, 39911, 30, 400, 46506, 11, 341, 272, 16, 307, 264, 51056, 51056, 912, 7006, 293, 309, 311, 445, 319, 4717, 3866, 1413, 13, 400, 370, 291, 393, 574, 412, 309, 382, 885, 10344, 281, 51388, 51388, 257, 1389, 365, 20381, 294, 4532, 7165, 13, 400, 370, 510, 286, 478, 445, 8407, 484, 257, 4974, 4295, 321, 1143, 294, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.11378810882568359, "compression_ratio": 1.6134453781512605, "no_speech_prob": 1.577921239004354e-06}, {"id": 268, "seek": 148808, "start": 1508.56, "end": 1513.4399999999998, "text": " a case with encountered in micrograd. And so here I'm just pulling out a random graph we used in", "tokens": [50364, 457, 365, 3104, 281, 46365, 272, 13, 583, 321, 500, 380, 362, 257, 46365, 272, 11, 321, 445, 362, 257, 2167, 272, 7738, 13, 50744, 50744, 407, 577, 360, 321, 586, 646, 79, 1513, 559, 473, 807, 264, 39911, 30, 400, 46506, 11, 341, 272, 16, 307, 264, 51056, 51056, 912, 7006, 293, 309, 311, 445, 319, 4717, 3866, 1413, 13, 400, 370, 291, 393, 574, 412, 309, 382, 885, 10344, 281, 51388, 51388, 257, 1389, 365, 20381, 294, 4532, 7165, 13, 400, 370, 510, 286, 478, 445, 8407, 484, 257, 4974, 4295, 321, 1143, 294, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.11378810882568359, "compression_ratio": 1.6134453781512605, "no_speech_prob": 1.577921239004354e-06}, {"id": 269, "seek": 151344, "start": 1513.44, "end": 1520.0800000000002, "text": " micrograd. We had an example where a single node has its output feeding into two branches of", "tokens": [50364, 4532, 7165, 13, 492, 632, 364, 1365, 689, 257, 2167, 9984, 575, 1080, 5598, 12919, 666, 732, 14770, 295, 50696, 50748, 1936, 264, 4295, 1826, 264, 1036, 2445, 13, 400, 321, 434, 1417, 466, 577, 264, 3006, 551, 281, 360, 50968, 50968, 294, 264, 23897, 1320, 307, 321, 643, 281, 2408, 439, 264, 2771, 2448, 300, 8881, 412, 604, 472, 9984, 13, 51228, 51228, 407, 2108, 613, 819, 14770, 11, 264, 2771, 2448, 576, 2408, 13, 407, 498, 257, 9984, 307, 1143, 3866, 1413, 11, 51540, 51540, 264, 2771, 2448, 337, 439, 295, 1080, 4960, 2408, 1830, 646, 79, 1513, 559, 399, 13, 407, 510, 272, 16, 307, 1143, 3866, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.06235709943269428, "compression_ratio": 1.7471698113207548, "no_speech_prob": 5.01439126310288e-06}, {"id": 270, "seek": 151344, "start": 1521.1200000000001, "end": 1525.52, "text": " basically the graph until the last function. And we're talking about how the correct thing to do", "tokens": [50364, 4532, 7165, 13, 492, 632, 364, 1365, 689, 257, 2167, 9984, 575, 1080, 5598, 12919, 666, 732, 14770, 295, 50696, 50748, 1936, 264, 4295, 1826, 264, 1036, 2445, 13, 400, 321, 434, 1417, 466, 577, 264, 3006, 551, 281, 360, 50968, 50968, 294, 264, 23897, 1320, 307, 321, 643, 281, 2408, 439, 264, 2771, 2448, 300, 8881, 412, 604, 472, 9984, 13, 51228, 51228, 407, 2108, 613, 819, 14770, 11, 264, 2771, 2448, 576, 2408, 13, 407, 498, 257, 9984, 307, 1143, 3866, 1413, 11, 51540, 51540, 264, 2771, 2448, 337, 439, 295, 1080, 4960, 2408, 1830, 646, 79, 1513, 559, 399, 13, 407, 510, 272, 16, 307, 1143, 3866, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.06235709943269428, "compression_ratio": 1.7471698113207548, "no_speech_prob": 5.01439126310288e-06}, {"id": 271, "seek": 151344, "start": 1525.52, "end": 1530.72, "text": " in the backward pass is we need to sum all the gradients that arrive at any one node.", "tokens": [50364, 4532, 7165, 13, 492, 632, 364, 1365, 689, 257, 2167, 9984, 575, 1080, 5598, 12919, 666, 732, 14770, 295, 50696, 50748, 1936, 264, 4295, 1826, 264, 1036, 2445, 13, 400, 321, 434, 1417, 466, 577, 264, 3006, 551, 281, 360, 50968, 50968, 294, 264, 23897, 1320, 307, 321, 643, 281, 2408, 439, 264, 2771, 2448, 300, 8881, 412, 604, 472, 9984, 13, 51228, 51228, 407, 2108, 613, 819, 14770, 11, 264, 2771, 2448, 576, 2408, 13, 407, 498, 257, 9984, 307, 1143, 3866, 1413, 11, 51540, 51540, 264, 2771, 2448, 337, 439, 295, 1080, 4960, 2408, 1830, 646, 79, 1513, 559, 399, 13, 407, 510, 272, 16, 307, 1143, 3866, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.06235709943269428, "compression_ratio": 1.7471698113207548, "no_speech_prob": 5.01439126310288e-06}, {"id": 272, "seek": 151344, "start": 1530.72, "end": 1536.96, "text": " So across these different branches, the gradients would sum. So if a node is used multiple times,", "tokens": [50364, 4532, 7165, 13, 492, 632, 364, 1365, 689, 257, 2167, 9984, 575, 1080, 5598, 12919, 666, 732, 14770, 295, 50696, 50748, 1936, 264, 4295, 1826, 264, 1036, 2445, 13, 400, 321, 434, 1417, 466, 577, 264, 3006, 551, 281, 360, 50968, 50968, 294, 264, 23897, 1320, 307, 321, 643, 281, 2408, 439, 264, 2771, 2448, 300, 8881, 412, 604, 472, 9984, 13, 51228, 51228, 407, 2108, 613, 819, 14770, 11, 264, 2771, 2448, 576, 2408, 13, 407, 498, 257, 9984, 307, 1143, 3866, 1413, 11, 51540, 51540, 264, 2771, 2448, 337, 439, 295, 1080, 4960, 2408, 1830, 646, 79, 1513, 559, 399, 13, 407, 510, 272, 16, 307, 1143, 3866, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.06235709943269428, "compression_ratio": 1.7471698113207548, "no_speech_prob": 5.01439126310288e-06}, {"id": 273, "seek": 151344, "start": 1536.96, "end": 1543.2, "text": " the gradients for all of its uses sum during backpropagation. So here b1 is used multiple", "tokens": [50364, 4532, 7165, 13, 492, 632, 364, 1365, 689, 257, 2167, 9984, 575, 1080, 5598, 12919, 666, 732, 14770, 295, 50696, 50748, 1936, 264, 4295, 1826, 264, 1036, 2445, 13, 400, 321, 434, 1417, 466, 577, 264, 3006, 551, 281, 360, 50968, 50968, 294, 264, 23897, 1320, 307, 321, 643, 281, 2408, 439, 264, 2771, 2448, 300, 8881, 412, 604, 472, 9984, 13, 51228, 51228, 407, 2108, 613, 819, 14770, 11, 264, 2771, 2448, 576, 2408, 13, 407, 498, 257, 9984, 307, 1143, 3866, 1413, 11, 51540, 51540, 264, 2771, 2448, 337, 439, 295, 1080, 4960, 2408, 1830, 646, 79, 1513, 559, 399, 13, 407, 510, 272, 16, 307, 1143, 3866, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.06235709943269428, "compression_ratio": 1.7471698113207548, "no_speech_prob": 5.01439126310288e-06}, {"id": 274, "seek": 154320, "start": 1543.2, "end": 1549.76, "text": " times in all these columns. And therefore the right thing to do here is to sum horizontally across", "tokens": [50364, 1413, 294, 439, 613, 13766, 13, 400, 4412, 264, 558, 551, 281, 360, 510, 307, 281, 2408, 33796, 2108, 50692, 50692, 439, 264, 13241, 13, 407, 321, 528, 281, 2408, 294, 10139, 472, 11, 457, 321, 528, 281, 18340, 341, 10139, 370, 300, 51028, 51096, 1207, 2408, 297, 392, 293, 1080, 16235, 366, 516, 281, 312, 2293, 264, 912, 3909, 13, 407, 321, 528, 281, 652, 988, 51332, 51332, 300, 321, 1066, 552, 382, 2074, 370, 321, 500, 380, 3624, 341, 10139, 13, 400, 341, 486, 652, 264, 1207, 2408, 297, 392, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.14013630151748657, "compression_ratio": 1.8113207547169812, "no_speech_prob": 1.2606506061274558e-05}, {"id": 275, "seek": 154320, "start": 1549.76, "end": 1556.48, "text": " all the rows. So we want to sum in dimension one, but we want to retain this dimension so that", "tokens": [50364, 1413, 294, 439, 613, 13766, 13, 400, 4412, 264, 558, 551, 281, 360, 510, 307, 281, 2408, 33796, 2108, 50692, 50692, 439, 264, 13241, 13, 407, 321, 528, 281, 2408, 294, 10139, 472, 11, 457, 321, 528, 281, 18340, 341, 10139, 370, 300, 51028, 51096, 1207, 2408, 297, 392, 293, 1080, 16235, 366, 516, 281, 312, 2293, 264, 912, 3909, 13, 407, 321, 528, 281, 652, 988, 51332, 51332, 300, 321, 1066, 552, 382, 2074, 370, 321, 500, 380, 3624, 341, 10139, 13, 400, 341, 486, 652, 264, 1207, 2408, 297, 392, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.14013630151748657, "compression_ratio": 1.8113207547169812, "no_speech_prob": 1.2606506061274558e-05}, {"id": 276, "seek": 154320, "start": 1557.8400000000001, "end": 1562.56, "text": " count sum nth and its gradient are going to be exactly the same shape. So we want to make sure", "tokens": [50364, 1413, 294, 439, 613, 13766, 13, 400, 4412, 264, 558, 551, 281, 360, 510, 307, 281, 2408, 33796, 2108, 50692, 50692, 439, 264, 13241, 13, 407, 321, 528, 281, 2408, 294, 10139, 472, 11, 457, 321, 528, 281, 18340, 341, 10139, 370, 300, 51028, 51096, 1207, 2408, 297, 392, 293, 1080, 16235, 366, 516, 281, 312, 2293, 264, 912, 3909, 13, 407, 321, 528, 281, 652, 988, 51332, 51332, 300, 321, 1066, 552, 382, 2074, 370, 321, 500, 380, 3624, 341, 10139, 13, 400, 341, 486, 652, 264, 1207, 2408, 297, 392, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.14013630151748657, "compression_ratio": 1.8113207547169812, "no_speech_prob": 1.2606506061274558e-05}, {"id": 277, "seek": 154320, "start": 1562.56, "end": 1568.16, "text": " that we keep them as true so we don't lose this dimension. And this will make the count sum nth", "tokens": [50364, 1413, 294, 439, 613, 13766, 13, 400, 4412, 264, 558, 551, 281, 360, 510, 307, 281, 2408, 33796, 2108, 50692, 50692, 439, 264, 13241, 13, 407, 321, 528, 281, 2408, 294, 10139, 472, 11, 457, 321, 528, 281, 18340, 341, 10139, 370, 300, 51028, 51096, 1207, 2408, 297, 392, 293, 1080, 16235, 366, 516, 281, 312, 2293, 264, 912, 3909, 13, 407, 321, 528, 281, 652, 988, 51332, 51332, 300, 321, 1066, 552, 382, 2074, 370, 321, 500, 380, 3624, 341, 10139, 13, 400, 341, 486, 652, 264, 1207, 2408, 297, 392, 51612, 51612], "temperature": 0.0, "avg_logprob": -0.14013630151748657, "compression_ratio": 1.8113207547169812, "no_speech_prob": 1.2606506061274558e-05}, {"id": 278, "seek": 156816, "start": 1568.16, "end": 1575.68, "text": " be exactly shape 32 by one. So revealing this comparison as well and running this,", "tokens": [50364, 312, 2293, 3909, 8858, 538, 472, 13, 407, 23983, 341, 9660, 382, 731, 293, 2614, 341, 11, 50740, 50740, 321, 536, 300, 321, 483, 364, 1900, 2995, 13, 407, 341, 13760, 307, 2293, 3006, 13, 50980, 51064, 400, 718, 385, 23525, 341, 13, 823, 718, 311, 611, 646, 48256, 666, 14893, 11, 597, 307, 264, 661, 51396, 51396, 7006, 510, 281, 1884, 26173, 13, 407, 490, 26173, 281, 1207, 2408, 297, 392, 11, 321, 445, 630, 300, 13, 961, 311, 352, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.0746184461257037, "compression_ratio": 1.5761904761904761, "no_speech_prob": 3.96694258597563e-06}, {"id": 279, "seek": 156816, "start": 1575.68, "end": 1580.48, "text": " we see that we get an exact match. So this derivative is exactly correct.", "tokens": [50364, 312, 2293, 3909, 8858, 538, 472, 13, 407, 23983, 341, 9660, 382, 731, 293, 2614, 341, 11, 50740, 50740, 321, 536, 300, 321, 483, 364, 1900, 2995, 13, 407, 341, 13760, 307, 2293, 3006, 13, 50980, 51064, 400, 718, 385, 23525, 341, 13, 823, 718, 311, 611, 646, 48256, 666, 14893, 11, 597, 307, 264, 661, 51396, 51396, 7006, 510, 281, 1884, 26173, 13, 407, 490, 26173, 281, 1207, 2408, 297, 392, 11, 321, 445, 630, 300, 13, 961, 311, 352, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.0746184461257037, "compression_ratio": 1.5761904761904761, "no_speech_prob": 3.96694258597563e-06}, {"id": 280, "seek": 156816, "start": 1582.16, "end": 1588.8000000000002, "text": " And let me erase this. Now let's also back propagate into counts, which is the other", "tokens": [50364, 312, 2293, 3909, 8858, 538, 472, 13, 407, 23983, 341, 9660, 382, 731, 293, 2614, 341, 11, 50740, 50740, 321, 536, 300, 321, 483, 364, 1900, 2995, 13, 407, 341, 13760, 307, 2293, 3006, 13, 50980, 51064, 400, 718, 385, 23525, 341, 13, 823, 718, 311, 611, 646, 48256, 666, 14893, 11, 597, 307, 264, 661, 51396, 51396, 7006, 510, 281, 1884, 26173, 13, 407, 490, 26173, 281, 1207, 2408, 297, 392, 11, 321, 445, 630, 300, 13, 961, 311, 352, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.0746184461257037, "compression_ratio": 1.5761904761904761, "no_speech_prob": 3.96694258597563e-06}, {"id": 281, "seek": 156816, "start": 1588.8000000000002, "end": 1593.6000000000001, "text": " variable here to create props. So from props to count sum nth, we just did that. Let's go", "tokens": [50364, 312, 2293, 3909, 8858, 538, 472, 13, 407, 23983, 341, 9660, 382, 731, 293, 2614, 341, 11, 50740, 50740, 321, 536, 300, 321, 483, 364, 1900, 2995, 13, 407, 341, 13760, 307, 2293, 3006, 13, 50980, 51064, 400, 718, 385, 23525, 341, 13, 823, 718, 311, 611, 646, 48256, 666, 14893, 11, 597, 307, 264, 661, 51396, 51396, 7006, 510, 281, 1884, 26173, 13, 407, 490, 26173, 281, 1207, 2408, 297, 392, 11, 321, 445, 630, 300, 13, 961, 311, 352, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.0746184461257037, "compression_ratio": 1.5761904761904761, "no_speech_prob": 3.96694258597563e-06}, {"id": 282, "seek": 159360, "start": 1593.6, "end": 1603.4399999999998, "text": " into counts as well. So d counts will be, d counts is our a, so dc by da is just b.", "tokens": [50364, 666, 14893, 382, 731, 13, 407, 274, 14893, 486, 312, 11, 274, 14893, 307, 527, 257, 11, 370, 274, 66, 538, 1120, 307, 445, 272, 13, 50856, 50888, 407, 4412, 309, 311, 1207, 2408, 297, 392, 293, 550, 1413, 5021, 4978, 274, 26173, 13, 823, 1207, 2408, 297, 392, 307, 8858, 538, 472, 11, 51356, 51388], "temperature": 0.0, "avg_logprob": -0.1918010218390103, "compression_ratio": 1.4330708661417322, "no_speech_prob": 6.961839972063899e-06}, {"id": 283, "seek": 159360, "start": 1604.08, "end": 1613.4399999999998, "text": " So therefore it's count sum nth and then times chain rule d props. Now count sum nth is 32 by one,", "tokens": [50364, 666, 14893, 382, 731, 13, 407, 274, 14893, 486, 312, 11, 274, 14893, 307, 527, 257, 11, 370, 274, 66, 538, 1120, 307, 445, 272, 13, 50856, 50888, 407, 4412, 309, 311, 1207, 2408, 297, 392, 293, 550, 1413, 5021, 4978, 274, 26173, 13, 823, 1207, 2408, 297, 392, 307, 8858, 538, 472, 11, 51356, 51388], "temperature": 0.0, "avg_logprob": -0.1918010218390103, "compression_ratio": 1.4330708661417322, "no_speech_prob": 6.961839972063899e-06}, {"id": 284, "seek": 161344, "start": 1613.44, "end": 1623.92, "text": " d props is 32 by 27. So those will broadcast fine and will give us d counts. There's no additional", "tokens": [50364, 274, 26173, 307, 8858, 538, 7634, 13, 407, 729, 486, 9975, 2489, 293, 486, 976, 505, 274, 14893, 13, 821, 311, 572, 4497, 50888, 50888, 28811, 4739, 510, 13, 821, 486, 312, 257, 30024, 300, 2314, 294, 341, 12972, 510, 51200, 51200, 570, 1207, 2408, 297, 392, 2203, 281, 312, 46365, 797, 281, 8944, 12972, 274, 26173, 13, 583, 300, 311, 51492, 51492, 516, 281, 976, 264, 3006, 1874, 13, 407, 382, 1400, 382, 264, 2167, 6916, 307, 5922, 11, 370, 321, 600, 646, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.13094192201440985, "compression_ratio": 1.6277056277056277, "no_speech_prob": 1.4367332141773659e-06}, {"id": 285, "seek": 161344, "start": 1623.92, "end": 1630.16, "text": " summation required here. There will be a broadcasting that happens in this multiply here", "tokens": [50364, 274, 26173, 307, 8858, 538, 7634, 13, 407, 729, 486, 9975, 2489, 293, 486, 976, 505, 274, 14893, 13, 821, 311, 572, 4497, 50888, 50888, 28811, 4739, 510, 13, 821, 486, 312, 257, 30024, 300, 2314, 294, 341, 12972, 510, 51200, 51200, 570, 1207, 2408, 297, 392, 2203, 281, 312, 46365, 797, 281, 8944, 12972, 274, 26173, 13, 583, 300, 311, 51492, 51492, 516, 281, 976, 264, 3006, 1874, 13, 407, 382, 1400, 382, 264, 2167, 6916, 307, 5922, 11, 370, 321, 600, 646, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.13094192201440985, "compression_ratio": 1.6277056277056277, "no_speech_prob": 1.4367332141773659e-06}, {"id": 286, "seek": 161344, "start": 1630.16, "end": 1636.0, "text": " because count sum nth needs to be replicated again to correctly multiply d props. But that's", "tokens": [50364, 274, 26173, 307, 8858, 538, 7634, 13, 407, 729, 486, 9975, 2489, 293, 486, 976, 505, 274, 14893, 13, 821, 311, 572, 4497, 50888, 50888, 28811, 4739, 510, 13, 821, 486, 312, 257, 30024, 300, 2314, 294, 341, 12972, 510, 51200, 51200, 570, 1207, 2408, 297, 392, 2203, 281, 312, 46365, 797, 281, 8944, 12972, 274, 26173, 13, 583, 300, 311, 51492, 51492, 516, 281, 976, 264, 3006, 1874, 13, 407, 382, 1400, 382, 264, 2167, 6916, 307, 5922, 11, 370, 321, 600, 646, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.13094192201440985, "compression_ratio": 1.6277056277056277, "no_speech_prob": 1.4367332141773659e-06}, {"id": 287, "seek": 161344, "start": 1636.0, "end": 1642.0800000000002, "text": " going to give the correct result. So as far as the single operation is concerned, so we've back", "tokens": [50364, 274, 26173, 307, 8858, 538, 7634, 13, 407, 729, 486, 9975, 2489, 293, 486, 976, 505, 274, 14893, 13, 821, 311, 572, 4497, 50888, 50888, 28811, 4739, 510, 13, 821, 486, 312, 257, 30024, 300, 2314, 294, 341, 12972, 510, 51200, 51200, 570, 1207, 2408, 297, 392, 2203, 281, 312, 46365, 797, 281, 8944, 12972, 274, 26173, 13, 583, 300, 311, 51492, 51492, 516, 281, 976, 264, 3006, 1874, 13, 407, 382, 1400, 382, 264, 2167, 6916, 307, 5922, 11, 370, 321, 600, 646, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.13094192201440985, "compression_ratio": 1.6277056277056277, "no_speech_prob": 1.4367332141773659e-06}, {"id": 288, "seek": 164208, "start": 1642.08, "end": 1648.8799999999999, "text": " propagated from props to counts, but we can't actually check the derivative of counts. I have", "tokens": [50364, 12425, 770, 490, 26173, 281, 14893, 11, 457, 321, 393, 380, 767, 1520, 264, 13760, 295, 14893, 13, 286, 362, 50704, 50704, 309, 709, 1780, 322, 13, 400, 264, 1778, 337, 300, 307, 570, 1207, 2408, 297, 392, 5946, 322, 14893, 13, 400, 370, 51000, 51000, 456, 311, 257, 1150, 9819, 510, 300, 321, 362, 281, 2413, 570, 1207, 2408, 297, 392, 646, 12425, 1024, 666, 1207, 51228, 51228, 2408, 293, 1207, 2408, 486, 646, 48256, 666, 14893, 13, 400, 370, 14893, 307, 257, 9984, 300, 307, 885, 1143, 6091, 13, 51532, 51532, 467, 311, 1143, 558, 510, 666, 26173, 293, 309, 1709, 807, 341, 661, 9819, 807, 1207, 2408, 297, 392, 13, 51736, 51780], "temperature": 0.0, "avg_logprob": -0.074433351174379, "compression_ratio": 1.9473684210526316, "no_speech_prob": 3.726572685991414e-06}, {"id": 289, "seek": 164208, "start": 1648.8799999999999, "end": 1654.8, "text": " it much later on. And the reason for that is because count sum nth depends on counts. And so", "tokens": [50364, 12425, 770, 490, 26173, 281, 14893, 11, 457, 321, 393, 380, 767, 1520, 264, 13760, 295, 14893, 13, 286, 362, 50704, 50704, 309, 709, 1780, 322, 13, 400, 264, 1778, 337, 300, 307, 570, 1207, 2408, 297, 392, 5946, 322, 14893, 13, 400, 370, 51000, 51000, 456, 311, 257, 1150, 9819, 510, 300, 321, 362, 281, 2413, 570, 1207, 2408, 297, 392, 646, 12425, 1024, 666, 1207, 51228, 51228, 2408, 293, 1207, 2408, 486, 646, 48256, 666, 14893, 13, 400, 370, 14893, 307, 257, 9984, 300, 307, 885, 1143, 6091, 13, 51532, 51532, 467, 311, 1143, 558, 510, 666, 26173, 293, 309, 1709, 807, 341, 661, 9819, 807, 1207, 2408, 297, 392, 13, 51736, 51780], "temperature": 0.0, "avg_logprob": -0.074433351174379, "compression_ratio": 1.9473684210526316, "no_speech_prob": 3.726572685991414e-06}, {"id": 290, "seek": 164208, "start": 1654.8, "end": 1659.36, "text": " there's a second branch here that we have to finish because count sum nth back propagates into count", "tokens": [50364, 12425, 770, 490, 26173, 281, 14893, 11, 457, 321, 393, 380, 767, 1520, 264, 13760, 295, 14893, 13, 286, 362, 50704, 50704, 309, 709, 1780, 322, 13, 400, 264, 1778, 337, 300, 307, 570, 1207, 2408, 297, 392, 5946, 322, 14893, 13, 400, 370, 51000, 51000, 456, 311, 257, 1150, 9819, 510, 300, 321, 362, 281, 2413, 570, 1207, 2408, 297, 392, 646, 12425, 1024, 666, 1207, 51228, 51228, 2408, 293, 1207, 2408, 486, 646, 48256, 666, 14893, 13, 400, 370, 14893, 307, 257, 9984, 300, 307, 885, 1143, 6091, 13, 51532, 51532, 467, 311, 1143, 558, 510, 666, 26173, 293, 309, 1709, 807, 341, 661, 9819, 807, 1207, 2408, 297, 392, 13, 51736, 51780], "temperature": 0.0, "avg_logprob": -0.074433351174379, "compression_ratio": 1.9473684210526316, "no_speech_prob": 3.726572685991414e-06}, {"id": 291, "seek": 164208, "start": 1659.36, "end": 1665.4399999999998, "text": " sum and count sum will back propagate into counts. And so counts is a node that is being used twice.", "tokens": [50364, 12425, 770, 490, 26173, 281, 14893, 11, 457, 321, 393, 380, 767, 1520, 264, 13760, 295, 14893, 13, 286, 362, 50704, 50704, 309, 709, 1780, 322, 13, 400, 264, 1778, 337, 300, 307, 570, 1207, 2408, 297, 392, 5946, 322, 14893, 13, 400, 370, 51000, 51000, 456, 311, 257, 1150, 9819, 510, 300, 321, 362, 281, 2413, 570, 1207, 2408, 297, 392, 646, 12425, 1024, 666, 1207, 51228, 51228, 2408, 293, 1207, 2408, 486, 646, 48256, 666, 14893, 13, 400, 370, 14893, 307, 257, 9984, 300, 307, 885, 1143, 6091, 13, 51532, 51532, 467, 311, 1143, 558, 510, 666, 26173, 293, 309, 1709, 807, 341, 661, 9819, 807, 1207, 2408, 297, 392, 13, 51736, 51780], "temperature": 0.0, "avg_logprob": -0.074433351174379, "compression_ratio": 1.9473684210526316, "no_speech_prob": 3.726572685991414e-06}, {"id": 292, "seek": 164208, "start": 1665.4399999999998, "end": 1669.52, "text": " It's used right here into props and it goes through this other branch through count sum nth.", "tokens": [50364, 12425, 770, 490, 26173, 281, 14893, 11, 457, 321, 393, 380, 767, 1520, 264, 13760, 295, 14893, 13, 286, 362, 50704, 50704, 309, 709, 1780, 322, 13, 400, 264, 1778, 337, 300, 307, 570, 1207, 2408, 297, 392, 5946, 322, 14893, 13, 400, 370, 51000, 51000, 456, 311, 257, 1150, 9819, 510, 300, 321, 362, 281, 2413, 570, 1207, 2408, 297, 392, 646, 12425, 1024, 666, 1207, 51228, 51228, 2408, 293, 1207, 2408, 486, 646, 48256, 666, 14893, 13, 400, 370, 14893, 307, 257, 9984, 300, 307, 885, 1143, 6091, 13, 51532, 51532, 467, 311, 1143, 558, 510, 666, 26173, 293, 309, 1709, 807, 341, 661, 9819, 807, 1207, 2408, 297, 392, 13, 51736, 51780], "temperature": 0.0, "avg_logprob": -0.074433351174379, "compression_ratio": 1.9473684210526316, "no_speech_prob": 3.726572685991414e-06}, {"id": 293, "seek": 166952, "start": 1669.52, "end": 1673.6, "text": " So even though we've calculated the first contribution of it, we still have to calculate", "tokens": [50364, 407, 754, 1673, 321, 600, 15598, 264, 700, 13150, 295, 309, 11, 321, 920, 362, 281, 8873, 50568, 50568, 264, 1150, 13150, 295, 309, 1780, 13, 1033, 11, 370, 321, 434, 9289, 365, 341, 9819, 13, 492, 362, 264, 50828, 50828, 13760, 337, 1207, 2408, 297, 392, 13, 823, 321, 528, 264, 13760, 1207, 2408, 13, 407, 274, 1207, 2408, 6915, 11, 51108, 51136, 437, 307, 264, 2654, 13760, 295, 341, 6916, 30, 407, 341, 307, 1936, 364, 4478, 10829, 472, 670, 14893, 51388, 51388, 2408, 13, 407, 1207, 2408, 6005, 281, 264, 1347, 295, 3671, 472, 307, 264, 912, 382, 472, 670, 1207, 2408, 13, 759, 321, 352, 281, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.1620215700383772, "compression_ratio": 1.880952380952381, "no_speech_prob": 1.112528934754664e-05}, {"id": 294, "seek": 166952, "start": 1673.6, "end": 1678.8, "text": " the second contribution of it later. Okay, so we're continuing with this branch. We have the", "tokens": [50364, 407, 754, 1673, 321, 600, 15598, 264, 700, 13150, 295, 309, 11, 321, 920, 362, 281, 8873, 50568, 50568, 264, 1150, 13150, 295, 309, 1780, 13, 1033, 11, 370, 321, 434, 9289, 365, 341, 9819, 13, 492, 362, 264, 50828, 50828, 13760, 337, 1207, 2408, 297, 392, 13, 823, 321, 528, 264, 13760, 1207, 2408, 13, 407, 274, 1207, 2408, 6915, 11, 51108, 51136, 437, 307, 264, 2654, 13760, 295, 341, 6916, 30, 407, 341, 307, 1936, 364, 4478, 10829, 472, 670, 14893, 51388, 51388, 2408, 13, 407, 1207, 2408, 6005, 281, 264, 1347, 295, 3671, 472, 307, 264, 912, 382, 472, 670, 1207, 2408, 13, 759, 321, 352, 281, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.1620215700383772, "compression_ratio": 1.880952380952381, "no_speech_prob": 1.112528934754664e-05}, {"id": 295, "seek": 166952, "start": 1678.8, "end": 1684.4, "text": " derivative for count sum nth. Now we want the derivative count sum. So d count sum equals,", "tokens": [50364, 407, 754, 1673, 321, 600, 15598, 264, 700, 13150, 295, 309, 11, 321, 920, 362, 281, 8873, 50568, 50568, 264, 1150, 13150, 295, 309, 1780, 13, 1033, 11, 370, 321, 434, 9289, 365, 341, 9819, 13, 492, 362, 264, 50828, 50828, 13760, 337, 1207, 2408, 297, 392, 13, 823, 321, 528, 264, 13760, 1207, 2408, 13, 407, 274, 1207, 2408, 6915, 11, 51108, 51136, 437, 307, 264, 2654, 13760, 295, 341, 6916, 30, 407, 341, 307, 1936, 364, 4478, 10829, 472, 670, 14893, 51388, 51388, 2408, 13, 407, 1207, 2408, 6005, 281, 264, 1347, 295, 3671, 472, 307, 264, 912, 382, 472, 670, 1207, 2408, 13, 759, 321, 352, 281, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.1620215700383772, "compression_ratio": 1.880952380952381, "no_speech_prob": 1.112528934754664e-05}, {"id": 296, "seek": 166952, "start": 1684.96, "end": 1690.0, "text": " what is the local derivative of this operation? So this is basically an element wise one over counts", "tokens": [50364, 407, 754, 1673, 321, 600, 15598, 264, 700, 13150, 295, 309, 11, 321, 920, 362, 281, 8873, 50568, 50568, 264, 1150, 13150, 295, 309, 1780, 13, 1033, 11, 370, 321, 434, 9289, 365, 341, 9819, 13, 492, 362, 264, 50828, 50828, 13760, 337, 1207, 2408, 297, 392, 13, 823, 321, 528, 264, 13760, 1207, 2408, 13, 407, 274, 1207, 2408, 6915, 11, 51108, 51136, 437, 307, 264, 2654, 13760, 295, 341, 6916, 30, 407, 341, 307, 1936, 364, 4478, 10829, 472, 670, 14893, 51388, 51388, 2408, 13, 407, 1207, 2408, 6005, 281, 264, 1347, 295, 3671, 472, 307, 264, 912, 382, 472, 670, 1207, 2408, 13, 759, 321, 352, 281, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.1620215700383772, "compression_ratio": 1.880952380952381, "no_speech_prob": 1.112528934754664e-05}, {"id": 297, "seek": 166952, "start": 1690.0, "end": 1695.92, "text": " sum. So count sum raised to the power of negative one is the same as one over count sum. If we go to", "tokens": [50364, 407, 754, 1673, 321, 600, 15598, 264, 700, 13150, 295, 309, 11, 321, 920, 362, 281, 8873, 50568, 50568, 264, 1150, 13150, 295, 309, 1780, 13, 1033, 11, 370, 321, 434, 9289, 365, 341, 9819, 13, 492, 362, 264, 50828, 50828, 13760, 337, 1207, 2408, 297, 392, 13, 823, 321, 528, 264, 13760, 1207, 2408, 13, 407, 274, 1207, 2408, 6915, 11, 51108, 51136, 437, 307, 264, 2654, 13760, 295, 341, 6916, 30, 407, 341, 307, 1936, 364, 4478, 10829, 472, 670, 14893, 51388, 51388, 2408, 13, 407, 1207, 2408, 6005, 281, 264, 1347, 295, 3671, 472, 307, 264, 912, 382, 472, 670, 1207, 2408, 13, 759, 321, 352, 281, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.1620215700383772, "compression_ratio": 1.880952380952381, "no_speech_prob": 1.112528934754664e-05}, {"id": 298, "seek": 169592, "start": 1695.92, "end": 1702.0800000000002, "text": " all from alpha, we see that X is a negative one d by d by d by d X of it is basically negative X", "tokens": [50364, 439, 490, 8961, 11, 321, 536, 300, 1783, 307, 257, 3671, 472, 274, 538, 274, 538, 274, 538, 274, 1783, 295, 309, 307, 1936, 3671, 1783, 50672, 50672, 281, 264, 3671, 732, 11, 558, 30, 1485, 3671, 472, 670, 3732, 307, 264, 912, 382, 3671, 1783, 281, 264, 3671, 50924, 50924, 732, 13, 407, 274, 1207, 2408, 510, 486, 312, 2654, 13760, 307, 516, 281, 312, 3671, 14893, 2408, 281, 264, 3671, 51452, 51452], "temperature": 0.0, "avg_logprob": -0.22041305742765727, "compression_ratio": 1.8385093167701863, "no_speech_prob": 1.983079891942907e-05}, {"id": 299, "seek": 169592, "start": 1702.0800000000002, "end": 1707.1200000000001, "text": " to the negative two, right? One negative one over square is the same as negative X to the negative", "tokens": [50364, 439, 490, 8961, 11, 321, 536, 300, 1783, 307, 257, 3671, 472, 274, 538, 274, 538, 274, 538, 274, 1783, 295, 309, 307, 1936, 3671, 1783, 50672, 50672, 281, 264, 3671, 732, 11, 558, 30, 1485, 3671, 472, 670, 3732, 307, 264, 912, 382, 3671, 1783, 281, 264, 3671, 50924, 50924, 732, 13, 407, 274, 1207, 2408, 510, 486, 312, 2654, 13760, 307, 516, 281, 312, 3671, 14893, 2408, 281, 264, 3671, 51452, 51452], "temperature": 0.0, "avg_logprob": -0.22041305742765727, "compression_ratio": 1.8385093167701863, "no_speech_prob": 1.983079891942907e-05}, {"id": 300, "seek": 169592, "start": 1707.1200000000001, "end": 1717.68, "text": " two. So d count sum here will be local derivative is going to be negative counts sum to the negative", "tokens": [50364, 439, 490, 8961, 11, 321, 536, 300, 1783, 307, 257, 3671, 472, 274, 538, 274, 538, 274, 538, 274, 1783, 295, 309, 307, 1936, 3671, 1783, 50672, 50672, 281, 264, 3671, 732, 11, 558, 30, 1485, 3671, 472, 670, 3732, 307, 264, 912, 382, 3671, 1783, 281, 264, 3671, 50924, 50924, 732, 13, 407, 274, 1207, 2408, 510, 486, 312, 2654, 13760, 307, 516, 281, 312, 3671, 14893, 2408, 281, 264, 3671, 51452, 51452], "temperature": 0.0, "avg_logprob": -0.22041305742765727, "compression_ratio": 1.8385093167701863, "no_speech_prob": 1.983079891942907e-05}, {"id": 301, "seek": 171768, "start": 1717.68, "end": 1726.96, "text": " two. That's the local derivative times chain rule, which is d count sum in. So that's d count sum.", "tokens": [50364, 732, 13, 663, 311, 264, 2654, 13760, 1413, 5021, 4978, 11, 597, 307, 274, 1207, 2408, 294, 13, 407, 300, 311, 274, 1207, 2408, 13, 50828, 50900, 961, 311, 8585, 518, 341, 293, 1520, 300, 286, 669, 3006, 13, 1033, 11, 370, 321, 362, 2176, 14949, 51128, 51204, 293, 456, 311, 572, 12325, 1324, 516, 322, 510, 365, 604, 10854, 570, 613, 366, 295, 264, 912, 3909, 13, 51436, 51468, 1033, 11, 958, 493, 321, 528, 281, 646, 79, 1513, 559, 473, 807, 341, 1622, 13, 492, 362, 300, 1207, 2408, 307, 14893, 5893, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.22332465891935388, "compression_ratio": 1.5991379310344827, "no_speech_prob": 4.8323719965992495e-05}, {"id": 302, "seek": 171768, "start": 1728.4, "end": 1732.96, "text": " Let's uncomment this and check that I am correct. Okay, so we have perfect equality", "tokens": [50364, 732, 13, 663, 311, 264, 2654, 13760, 1413, 5021, 4978, 11, 597, 307, 274, 1207, 2408, 294, 13, 407, 300, 311, 274, 1207, 2408, 13, 50828, 50900, 961, 311, 8585, 518, 341, 293, 1520, 300, 286, 669, 3006, 13, 1033, 11, 370, 321, 362, 2176, 14949, 51128, 51204, 293, 456, 311, 572, 12325, 1324, 516, 322, 510, 365, 604, 10854, 570, 613, 366, 295, 264, 912, 3909, 13, 51436, 51468, 1033, 11, 958, 493, 321, 528, 281, 646, 79, 1513, 559, 473, 807, 341, 1622, 13, 492, 362, 300, 1207, 2408, 307, 14893, 5893, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.22332465891935388, "compression_ratio": 1.5991379310344827, "no_speech_prob": 4.8323719965992495e-05}, {"id": 303, "seek": 171768, "start": 1734.48, "end": 1739.1200000000001, "text": " and there's no sketchiness going on here with any shapes because these are of the same shape.", "tokens": [50364, 732, 13, 663, 311, 264, 2654, 13760, 1413, 5021, 4978, 11, 597, 307, 274, 1207, 2408, 294, 13, 407, 300, 311, 274, 1207, 2408, 13, 50828, 50900, 961, 311, 8585, 518, 341, 293, 1520, 300, 286, 669, 3006, 13, 1033, 11, 370, 321, 362, 2176, 14949, 51128, 51204, 293, 456, 311, 572, 12325, 1324, 516, 322, 510, 365, 604, 10854, 570, 613, 366, 295, 264, 912, 3909, 13, 51436, 51468, 1033, 11, 958, 493, 321, 528, 281, 646, 79, 1513, 559, 473, 807, 341, 1622, 13, 492, 362, 300, 1207, 2408, 307, 14893, 5893, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.22332465891935388, "compression_ratio": 1.5991379310344827, "no_speech_prob": 4.8323719965992495e-05}, {"id": 304, "seek": 171768, "start": 1739.76, "end": 1744.3200000000002, "text": " Okay, next up we want to backpropagate through this line. We have that count sum is counts dot", "tokens": [50364, 732, 13, 663, 311, 264, 2654, 13760, 1413, 5021, 4978, 11, 597, 307, 274, 1207, 2408, 294, 13, 407, 300, 311, 274, 1207, 2408, 13, 50828, 50900, 961, 311, 8585, 518, 341, 293, 1520, 300, 286, 669, 3006, 13, 1033, 11, 370, 321, 362, 2176, 14949, 51128, 51204, 293, 456, 311, 572, 12325, 1324, 516, 322, 510, 365, 604, 10854, 570, 613, 366, 295, 264, 912, 3909, 13, 51436, 51468, 1033, 11, 958, 493, 321, 528, 281, 646, 79, 1513, 559, 473, 807, 341, 1622, 13, 492, 362, 300, 1207, 2408, 307, 14893, 5893, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.22332465891935388, "compression_ratio": 1.5991379310344827, "no_speech_prob": 4.8323719965992495e-05}, {"id": 305, "seek": 174432, "start": 1744.32, "end": 1751.9199999999998, "text": " sum is counts dot sum along the rows. So I wrote out some help here. We have to keep in mind that", "tokens": [50364, 2408, 307, 14893, 5893, 2408, 2051, 264, 13241, 13, 407, 286, 4114, 484, 512, 854, 510, 13, 492, 362, 281, 1066, 294, 1575, 300, 50744, 50744, 14893, 295, 1164, 307, 8858, 538, 7634, 293, 1207, 2408, 307, 8858, 538, 472, 13, 407, 294, 341, 646, 38377, 11, 321, 643, 281, 51052, 51052, 747, 341, 7738, 295, 33733, 293, 4088, 309, 666, 257, 10225, 295, 33733, 11, 732, 18795, 10225, 13, 51424, 51484, 407, 437, 307, 341, 6916, 884, 30, 492, 434, 1940, 512, 733, 295, 364, 4846, 411, 584, 257, 1045, 538, 1045, 8141, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.09933936839201013, "compression_ratio": 1.6431535269709543, "no_speech_prob": 9.276284345105523e-07}, {"id": 306, "seek": 174432, "start": 1751.9199999999998, "end": 1758.08, "text": " counts of course is 32 by 27 and count sum is 32 by one. So in this back propagation, we need to", "tokens": [50364, 2408, 307, 14893, 5893, 2408, 2051, 264, 13241, 13, 407, 286, 4114, 484, 512, 854, 510, 13, 492, 362, 281, 1066, 294, 1575, 300, 50744, 50744, 14893, 295, 1164, 307, 8858, 538, 7634, 293, 1207, 2408, 307, 8858, 538, 472, 13, 407, 294, 341, 646, 38377, 11, 321, 643, 281, 51052, 51052, 747, 341, 7738, 295, 33733, 293, 4088, 309, 666, 257, 10225, 295, 33733, 11, 732, 18795, 10225, 13, 51424, 51484, 407, 437, 307, 341, 6916, 884, 30, 492, 434, 1940, 512, 733, 295, 364, 4846, 411, 584, 257, 1045, 538, 1045, 8141, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.09933936839201013, "compression_ratio": 1.6431535269709543, "no_speech_prob": 9.276284345105523e-07}, {"id": 307, "seek": 174432, "start": 1758.08, "end": 1765.52, "text": " take this column of derivatives and transform it into a array of derivatives, two dimensional array.", "tokens": [50364, 2408, 307, 14893, 5893, 2408, 2051, 264, 13241, 13, 407, 286, 4114, 484, 512, 854, 510, 13, 492, 362, 281, 1066, 294, 1575, 300, 50744, 50744, 14893, 295, 1164, 307, 8858, 538, 7634, 293, 1207, 2408, 307, 8858, 538, 472, 13, 407, 294, 341, 646, 38377, 11, 321, 643, 281, 51052, 51052, 747, 341, 7738, 295, 33733, 293, 4088, 309, 666, 257, 10225, 295, 33733, 11, 732, 18795, 10225, 13, 51424, 51484, 407, 437, 307, 341, 6916, 884, 30, 492, 434, 1940, 512, 733, 295, 364, 4846, 411, 584, 257, 1045, 538, 1045, 8141, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.09933936839201013, "compression_ratio": 1.6431535269709543, "no_speech_prob": 9.276284345105523e-07}, {"id": 308, "seek": 174432, "start": 1766.72, "end": 1771.9199999999998, "text": " So what is this operation doing? We're taking some kind of an input like say a three by three matrix", "tokens": [50364, 2408, 307, 14893, 5893, 2408, 2051, 264, 13241, 13, 407, 286, 4114, 484, 512, 854, 510, 13, 492, 362, 281, 1066, 294, 1575, 300, 50744, 50744, 14893, 295, 1164, 307, 8858, 538, 7634, 293, 1207, 2408, 307, 8858, 538, 472, 13, 407, 294, 341, 646, 38377, 11, 321, 643, 281, 51052, 51052, 747, 341, 7738, 295, 33733, 293, 4088, 309, 666, 257, 10225, 295, 33733, 11, 732, 18795, 10225, 13, 51424, 51484, 407, 437, 307, 341, 6916, 884, 30, 492, 434, 1940, 512, 733, 295, 364, 4846, 411, 584, 257, 1045, 538, 1045, 8141, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.09933936839201013, "compression_ratio": 1.6431535269709543, "no_speech_prob": 9.276284345105523e-07}, {"id": 309, "seek": 177192, "start": 1771.92, "end": 1779.04, "text": " a and we are summing up the rows into a column tensor b, b1 b2 b3 that is basically this.", "tokens": [50364, 257, 293, 321, 366, 2408, 2810, 493, 264, 13241, 666, 257, 7738, 40863, 272, 11, 272, 16, 272, 17, 272, 18, 300, 307, 1936, 341, 13, 50720, 50764, 407, 586, 321, 362, 264, 33733, 295, 264, 4470, 365, 3104, 281, 272, 11, 439, 264, 4959, 295, 272, 293, 586, 321, 51048, 51048, 528, 281, 13760, 4470, 365, 3104, 281, 439, 613, 707, 257, 311, 13, 407, 577, 360, 264, 272, 311, 5672, 322, 264, 257, 311, 51396, 51396, 307, 1936, 437, 321, 434, 934, 13, 708, 307, 264, 2654, 13760, 295, 341, 6916, 30, 1042, 321, 393, 536, 510, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.1190555998422567, "compression_ratio": 1.76036866359447, "no_speech_prob": 5.862665148015367e-06}, {"id": 310, "seek": 177192, "start": 1779.92, "end": 1785.6000000000001, "text": " So now we have the derivatives of the loss with respect to b, all the elements of b and now we", "tokens": [50364, 257, 293, 321, 366, 2408, 2810, 493, 264, 13241, 666, 257, 7738, 40863, 272, 11, 272, 16, 272, 17, 272, 18, 300, 307, 1936, 341, 13, 50720, 50764, 407, 586, 321, 362, 264, 33733, 295, 264, 4470, 365, 3104, 281, 272, 11, 439, 264, 4959, 295, 272, 293, 586, 321, 51048, 51048, 528, 281, 13760, 4470, 365, 3104, 281, 439, 613, 707, 257, 311, 13, 407, 577, 360, 264, 272, 311, 5672, 322, 264, 257, 311, 51396, 51396, 307, 1936, 437, 321, 434, 934, 13, 708, 307, 264, 2654, 13760, 295, 341, 6916, 30, 1042, 321, 393, 536, 510, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.1190555998422567, "compression_ratio": 1.76036866359447, "no_speech_prob": 5.862665148015367e-06}, {"id": 311, "seek": 177192, "start": 1785.6000000000001, "end": 1792.5600000000002, "text": " want to derivative loss with respect to all these little a's. So how do the b's depend on the a's", "tokens": [50364, 257, 293, 321, 366, 2408, 2810, 493, 264, 13241, 666, 257, 7738, 40863, 272, 11, 272, 16, 272, 17, 272, 18, 300, 307, 1936, 341, 13, 50720, 50764, 407, 586, 321, 362, 264, 33733, 295, 264, 4470, 365, 3104, 281, 272, 11, 439, 264, 4959, 295, 272, 293, 586, 321, 51048, 51048, 528, 281, 13760, 4470, 365, 3104, 281, 439, 613, 707, 257, 311, 13, 407, 577, 360, 264, 272, 311, 5672, 322, 264, 257, 311, 51396, 51396, 307, 1936, 437, 321, 434, 934, 13, 708, 307, 264, 2654, 13760, 295, 341, 6916, 30, 1042, 321, 393, 536, 510, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.1190555998422567, "compression_ratio": 1.76036866359447, "no_speech_prob": 5.862665148015367e-06}, {"id": 312, "seek": 177192, "start": 1792.5600000000002, "end": 1797.04, "text": " is basically what we're after. What is the local derivative of this operation? Well we can see here", "tokens": [50364, 257, 293, 321, 366, 2408, 2810, 493, 264, 13241, 666, 257, 7738, 40863, 272, 11, 272, 16, 272, 17, 272, 18, 300, 307, 1936, 341, 13, 50720, 50764, 407, 586, 321, 362, 264, 33733, 295, 264, 4470, 365, 3104, 281, 272, 11, 439, 264, 4959, 295, 272, 293, 586, 321, 51048, 51048, 528, 281, 13760, 4470, 365, 3104, 281, 439, 613, 707, 257, 311, 13, 407, 577, 360, 264, 272, 311, 5672, 322, 264, 257, 311, 51396, 51396, 307, 1936, 437, 321, 434, 934, 13, 708, 307, 264, 2654, 13760, 295, 341, 6916, 30, 1042, 321, 393, 536, 510, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.1190555998422567, "compression_ratio": 1.76036866359447, "no_speech_prob": 5.862665148015367e-06}, {"id": 313, "seek": 179704, "start": 1797.04, "end": 1803.28, "text": " that b1 only depends on these elements here. The derivative of b1 with respect to all of these", "tokens": [50364, 300, 272, 16, 787, 5946, 322, 613, 4959, 510, 13, 440, 13760, 295, 272, 16, 365, 3104, 281, 439, 295, 613, 50676, 50676, 4959, 760, 510, 307, 4018, 457, 337, 613, 4959, 510, 411, 257, 16, 502, 257, 16, 568, 5183, 264, 2654, 13760, 307, 502, 51048, 51048, 558, 370, 274, 65, 16, 538, 274, 257, 16, 502, 337, 1365, 307, 502, 370, 309, 311, 502, 502, 293, 502, 13, 407, 562, 321, 362, 264, 13760, 295, 4470, 51496, 51496, 365, 3104, 281, 272, 16, 264, 2654, 13760, 295, 272, 16, 365, 3104, 281, 613, 15743, 307, 35193, 510, 457, 309, 311, 502, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.12101152901337525, "compression_ratio": 2.0, "no_speech_prob": 5.338084065442672e-06}, {"id": 314, "seek": 179704, "start": 1803.28, "end": 1810.72, "text": " elements down here is zero but for these elements here like a1 1 a1 2 etc the local derivative is 1", "tokens": [50364, 300, 272, 16, 787, 5946, 322, 613, 4959, 510, 13, 440, 13760, 295, 272, 16, 365, 3104, 281, 439, 295, 613, 50676, 50676, 4959, 760, 510, 307, 4018, 457, 337, 613, 4959, 510, 411, 257, 16, 502, 257, 16, 568, 5183, 264, 2654, 13760, 307, 502, 51048, 51048, 558, 370, 274, 65, 16, 538, 274, 257, 16, 502, 337, 1365, 307, 502, 370, 309, 311, 502, 502, 293, 502, 13, 407, 562, 321, 362, 264, 13760, 295, 4470, 51496, 51496, 365, 3104, 281, 272, 16, 264, 2654, 13760, 295, 272, 16, 365, 3104, 281, 613, 15743, 307, 35193, 510, 457, 309, 311, 502, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.12101152901337525, "compression_ratio": 2.0, "no_speech_prob": 5.338084065442672e-06}, {"id": 315, "seek": 179704, "start": 1810.72, "end": 1819.68, "text": " right so db1 by d a1 1 for example is 1 so it's 1 1 and 1. So when we have the derivative of loss", "tokens": [50364, 300, 272, 16, 787, 5946, 322, 613, 4959, 510, 13, 440, 13760, 295, 272, 16, 365, 3104, 281, 439, 295, 613, 50676, 50676, 4959, 760, 510, 307, 4018, 457, 337, 613, 4959, 510, 411, 257, 16, 502, 257, 16, 568, 5183, 264, 2654, 13760, 307, 502, 51048, 51048, 558, 370, 274, 65, 16, 538, 274, 257, 16, 502, 337, 1365, 307, 502, 370, 309, 311, 502, 502, 293, 502, 13, 407, 562, 321, 362, 264, 13760, 295, 4470, 51496, 51496, 365, 3104, 281, 272, 16, 264, 2654, 13760, 295, 272, 16, 365, 3104, 281, 613, 15743, 307, 35193, 510, 457, 309, 311, 502, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.12101152901337525, "compression_ratio": 2.0, "no_speech_prob": 5.338084065442672e-06}, {"id": 316, "seek": 179704, "start": 1819.68, "end": 1826.3999999999999, "text": " with respect to b1 the local derivative of b1 with respect to these inputs is zeros here but it's 1", "tokens": [50364, 300, 272, 16, 787, 5946, 322, 613, 4959, 510, 13, 440, 13760, 295, 272, 16, 365, 3104, 281, 439, 295, 613, 50676, 50676, 4959, 760, 510, 307, 4018, 457, 337, 613, 4959, 510, 411, 257, 16, 502, 257, 16, 568, 5183, 264, 2654, 13760, 307, 502, 51048, 51048, 558, 370, 274, 65, 16, 538, 274, 257, 16, 502, 337, 1365, 307, 502, 370, 309, 311, 502, 502, 293, 502, 13, 407, 562, 321, 362, 264, 13760, 295, 4470, 51496, 51496, 365, 3104, 281, 272, 16, 264, 2654, 13760, 295, 272, 16, 365, 3104, 281, 613, 15743, 307, 35193, 510, 457, 309, 311, 502, 51832, 51832], "temperature": 0.0, "avg_logprob": -0.12101152901337525, "compression_ratio": 2.0, "no_speech_prob": 5.338084065442672e-06}, {"id": 317, "seek": 182640, "start": 1826.4, "end": 1834.0800000000002, "text": " on these guys. So in the chain rule we have the local derivative times sort of the derivative of", "tokens": [50364, 322, 613, 1074, 13, 407, 294, 264, 5021, 4978, 321, 362, 264, 2654, 13760, 1413, 1333, 295, 264, 13760, 295, 50748, 50748, 272, 16, 293, 370, 570, 264, 2654, 13760, 307, 502, 322, 613, 1045, 4959, 264, 2654, 13760, 30955, 51072, 51072, 264, 13760, 295, 272, 16, 486, 445, 312, 264, 13760, 295, 272, 16, 293, 370, 291, 393, 574, 412, 309, 382, 257, 22492, 13, 51396, 51396, 8537, 364, 4500, 307, 257, 22492, 295, 16235, 2035, 16235, 1487, 490, 3673, 309, 445, 2170, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.05750204216350208, "compression_ratio": 1.9743589743589745, "no_speech_prob": 9.818216312851291e-06}, {"id": 318, "seek": 182640, "start": 1834.0800000000002, "end": 1840.5600000000002, "text": " b1 and so because the local derivative is 1 on these three elements the local derivative multiplying", "tokens": [50364, 322, 613, 1074, 13, 407, 294, 264, 5021, 4978, 321, 362, 264, 2654, 13760, 1413, 1333, 295, 264, 13760, 295, 50748, 50748, 272, 16, 293, 370, 570, 264, 2654, 13760, 307, 502, 322, 613, 1045, 4959, 264, 2654, 13760, 30955, 51072, 51072, 264, 13760, 295, 272, 16, 486, 445, 312, 264, 13760, 295, 272, 16, 293, 370, 291, 393, 574, 412, 309, 382, 257, 22492, 13, 51396, 51396, 8537, 364, 4500, 307, 257, 22492, 295, 16235, 2035, 16235, 1487, 490, 3673, 309, 445, 2170, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.05750204216350208, "compression_ratio": 1.9743589743589745, "no_speech_prob": 9.818216312851291e-06}, {"id": 319, "seek": 182640, "start": 1840.5600000000002, "end": 1847.0400000000002, "text": " the derivative of b1 will just be the derivative of b1 and so you can look at it as a router.", "tokens": [50364, 322, 613, 1074, 13, 407, 294, 264, 5021, 4978, 321, 362, 264, 2654, 13760, 1413, 1333, 295, 264, 13760, 295, 50748, 50748, 272, 16, 293, 370, 570, 264, 2654, 13760, 307, 502, 322, 613, 1045, 4959, 264, 2654, 13760, 30955, 51072, 51072, 264, 13760, 295, 272, 16, 486, 445, 312, 264, 13760, 295, 272, 16, 293, 370, 291, 393, 574, 412, 309, 382, 257, 22492, 13, 51396, 51396, 8537, 364, 4500, 307, 257, 22492, 295, 16235, 2035, 16235, 1487, 490, 3673, 309, 445, 2170, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.05750204216350208, "compression_ratio": 1.9743589743589745, "no_speech_prob": 9.818216312851291e-06}, {"id": 320, "seek": 182640, "start": 1847.0400000000002, "end": 1852.16, "text": " Basically an addition is a router of gradient whatever gradient comes from above it just gets", "tokens": [50364, 322, 613, 1074, 13, 407, 294, 264, 5021, 4978, 321, 362, 264, 2654, 13760, 1413, 1333, 295, 264, 13760, 295, 50748, 50748, 272, 16, 293, 370, 570, 264, 2654, 13760, 307, 502, 322, 613, 1045, 4959, 264, 2654, 13760, 30955, 51072, 51072, 264, 13760, 295, 272, 16, 486, 445, 312, 264, 13760, 295, 272, 16, 293, 370, 291, 393, 574, 412, 309, 382, 257, 22492, 13, 51396, 51396, 8537, 364, 4500, 307, 257, 22492, 295, 16235, 2035, 16235, 1487, 490, 3673, 309, 445, 2170, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.05750204216350208, "compression_ratio": 1.9743589743589745, "no_speech_prob": 9.818216312851291e-06}, {"id": 321, "seek": 185216, "start": 1852.16, "end": 1857.6000000000001, "text": " routed equally to all the elements that participate in that addition. So in this case the derivative", "tokens": [50364, 4020, 292, 12309, 281, 439, 264, 4959, 300, 8197, 294, 300, 4500, 13, 407, 294, 341, 1389, 264, 13760, 50636, 50636, 295, 272, 16, 486, 445, 3095, 12309, 281, 264, 13760, 295, 257, 16, 502, 257, 16, 568, 293, 257, 16, 805, 13, 407, 498, 321, 362, 257, 13760, 295, 50988, 50988, 439, 264, 4959, 295, 272, 293, 294, 341, 7738, 40863, 597, 307, 274, 14893, 2408, 300, 321, 600, 15598, 445, 51304, 51304, 586, 321, 1936, 536, 300, 437, 300, 11663, 281, 307, 439, 295, 613, 366, 586, 13974, 281, 439, 613, 4959, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.04971702731385523, "compression_ratio": 1.8952380952380952, "no_speech_prob": 2.726416369114304e-06}, {"id": 322, "seek": 185216, "start": 1857.6000000000001, "end": 1864.64, "text": " of b1 will just flow equally to the derivative of a1 1 a1 2 and a1 3. So if we have a derivative of", "tokens": [50364, 4020, 292, 12309, 281, 439, 264, 4959, 300, 8197, 294, 300, 4500, 13, 407, 294, 341, 1389, 264, 13760, 50636, 50636, 295, 272, 16, 486, 445, 3095, 12309, 281, 264, 13760, 295, 257, 16, 502, 257, 16, 568, 293, 257, 16, 805, 13, 407, 498, 321, 362, 257, 13760, 295, 50988, 50988, 439, 264, 4959, 295, 272, 293, 294, 341, 7738, 40863, 597, 307, 274, 14893, 2408, 300, 321, 600, 15598, 445, 51304, 51304, 586, 321, 1936, 536, 300, 437, 300, 11663, 281, 307, 439, 295, 613, 366, 586, 13974, 281, 439, 613, 4959, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.04971702731385523, "compression_ratio": 1.8952380952380952, "no_speech_prob": 2.726416369114304e-06}, {"id": 323, "seek": 185216, "start": 1864.64, "end": 1870.96, "text": " all the elements of b and in this column tensor which is d counts sum that we've calculated just", "tokens": [50364, 4020, 292, 12309, 281, 439, 264, 4959, 300, 8197, 294, 300, 4500, 13, 407, 294, 341, 1389, 264, 13760, 50636, 50636, 295, 272, 16, 486, 445, 3095, 12309, 281, 264, 13760, 295, 257, 16, 502, 257, 16, 568, 293, 257, 16, 805, 13, 407, 498, 321, 362, 257, 13760, 295, 50988, 50988, 439, 264, 4959, 295, 272, 293, 294, 341, 7738, 40863, 597, 307, 274, 14893, 2408, 300, 321, 600, 15598, 445, 51304, 51304, 586, 321, 1936, 536, 300, 437, 300, 11663, 281, 307, 439, 295, 613, 366, 586, 13974, 281, 439, 613, 4959, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.04971702731385523, "compression_ratio": 1.8952380952380952, "no_speech_prob": 2.726416369114304e-06}, {"id": 324, "seek": 185216, "start": 1870.96, "end": 1877.92, "text": " now we basically see that what that amounts to is all of these are now flowing to all these elements", "tokens": [50364, 4020, 292, 12309, 281, 439, 264, 4959, 300, 8197, 294, 300, 4500, 13, 407, 294, 341, 1389, 264, 13760, 50636, 50636, 295, 272, 16, 486, 445, 3095, 12309, 281, 264, 13760, 295, 257, 16, 502, 257, 16, 568, 293, 257, 16, 805, 13, 407, 498, 321, 362, 257, 13760, 295, 50988, 50988, 439, 264, 4959, 295, 272, 293, 294, 341, 7738, 40863, 597, 307, 274, 14893, 2408, 300, 321, 600, 15598, 445, 51304, 51304, 586, 321, 1936, 536, 300, 437, 300, 11663, 281, 307, 439, 295, 613, 366, 586, 13974, 281, 439, 613, 4959, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.04971702731385523, "compression_ratio": 1.8952380952380952, "no_speech_prob": 2.726416369114304e-06}, {"id": 325, "seek": 187792, "start": 1877.92, "end": 1883.68, "text": " of a and they're doing that horizontally. So basically what we want is we want to take the", "tokens": [50364, 295, 257, 293, 436, 434, 884, 300, 33796, 13, 407, 1936, 437, 321, 528, 307, 321, 528, 281, 747, 264, 50652, 50652, 274, 14893, 2408, 295, 2744, 8858, 538, 502, 293, 321, 445, 528, 281, 25356, 309, 7634, 1413, 33796, 281, 1884, 8858, 50988, 50988, 538, 7634, 10225, 13, 407, 456, 311, 867, 2098, 281, 4445, 341, 6916, 291, 727, 295, 1164, 445, 25356, 51240, 51240, 264, 40863, 457, 286, 519, 1310, 257, 472, 2541, 472, 307, 300, 274, 14893, 307, 2935, 3930, 19318, 484, 1564, 411, 51600, 51652], "temperature": 0.0, "avg_logprob": -0.07068829484038301, "compression_ratio": 1.6725663716814159, "no_speech_prob": 2.2958886347623775e-06}, {"id": 326, "seek": 187792, "start": 1883.68, "end": 1890.4, "text": " d counts sum of size 32 by 1 and we just want to replicate it 27 times horizontally to create 32", "tokens": [50364, 295, 257, 293, 436, 434, 884, 300, 33796, 13, 407, 1936, 437, 321, 528, 307, 321, 528, 281, 747, 264, 50652, 50652, 274, 14893, 2408, 295, 2744, 8858, 538, 502, 293, 321, 445, 528, 281, 25356, 309, 7634, 1413, 33796, 281, 1884, 8858, 50988, 50988, 538, 7634, 10225, 13, 407, 456, 311, 867, 2098, 281, 4445, 341, 6916, 291, 727, 295, 1164, 445, 25356, 51240, 51240, 264, 40863, 457, 286, 519, 1310, 257, 472, 2541, 472, 307, 300, 274, 14893, 307, 2935, 3930, 19318, 484, 1564, 411, 51600, 51652], "temperature": 0.0, "avg_logprob": -0.07068829484038301, "compression_ratio": 1.6725663716814159, "no_speech_prob": 2.2958886347623775e-06}, {"id": 327, "seek": 187792, "start": 1890.4, "end": 1895.44, "text": " by 27 array. So there's many ways to implement this operation you could of course just replicate", "tokens": [50364, 295, 257, 293, 436, 434, 884, 300, 33796, 13, 407, 1936, 437, 321, 528, 307, 321, 528, 281, 747, 264, 50652, 50652, 274, 14893, 2408, 295, 2744, 8858, 538, 502, 293, 321, 445, 528, 281, 25356, 309, 7634, 1413, 33796, 281, 1884, 8858, 50988, 50988, 538, 7634, 10225, 13, 407, 456, 311, 867, 2098, 281, 4445, 341, 6916, 291, 727, 295, 1164, 445, 25356, 51240, 51240, 264, 40863, 457, 286, 519, 1310, 257, 472, 2541, 472, 307, 300, 274, 14893, 307, 2935, 3930, 19318, 484, 1564, 411, 51600, 51652], "temperature": 0.0, "avg_logprob": -0.07068829484038301, "compression_ratio": 1.6725663716814159, "no_speech_prob": 2.2958886347623775e-06}, {"id": 328, "seek": 187792, "start": 1895.44, "end": 1902.64, "text": " the tensor but I think maybe a one clean one is that d counts is simply torched out once like", "tokens": [50364, 295, 257, 293, 436, 434, 884, 300, 33796, 13, 407, 1936, 437, 321, 528, 307, 321, 528, 281, 747, 264, 50652, 50652, 274, 14893, 2408, 295, 2744, 8858, 538, 502, 293, 321, 445, 528, 281, 25356, 309, 7634, 1413, 33796, 281, 1884, 8858, 50988, 50988, 538, 7634, 10225, 13, 407, 456, 311, 867, 2098, 281, 4445, 341, 6916, 291, 727, 295, 1164, 445, 25356, 51240, 51240, 264, 40863, 457, 286, 519, 1310, 257, 472, 2541, 472, 307, 300, 274, 14893, 307, 2935, 3930, 19318, 484, 1564, 411, 51600, 51652], "temperature": 0.0, "avg_logprob": -0.07068829484038301, "compression_ratio": 1.6725663716814159, "no_speech_prob": 2.2958886347623775e-06}, {"id": 329, "seek": 190264, "start": 1902.64, "end": 1911.76, "text": " so just a two-dimensional arrays of ones in the shape of counts so 32 by 27 times d counts sum.", "tokens": [50364, 370, 445, 257, 732, 12, 18759, 41011, 295, 2306, 294, 264, 3909, 295, 14893, 370, 8858, 538, 7634, 1413, 274, 14893, 2408, 13, 50820, 50820, 407, 341, 636, 321, 434, 8295, 264, 30024, 510, 1936, 4445, 264, 39911, 291, 393, 574, 51096, 51096, 412, 309, 300, 636, 13, 583, 550, 321, 362, 281, 611, 312, 5026, 570, 274, 14893, 390, 1217, 15598, 51444, 51444, 321, 15598, 3071, 510, 293, 300, 390, 445, 264, 700, 9819, 293, 321, 434, 586, 12693, 264, 1150, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.18502827577812728, "compression_ratio": 1.6565217391304348, "no_speech_prob": 4.289273874746868e-06}, {"id": 330, "seek": 190264, "start": 1911.76, "end": 1917.2800000000002, "text": " So this way we're letting the broadcasting here basically implement the replication you can look", "tokens": [50364, 370, 445, 257, 732, 12, 18759, 41011, 295, 2306, 294, 264, 3909, 295, 14893, 370, 8858, 538, 7634, 1413, 274, 14893, 2408, 13, 50820, 50820, 407, 341, 636, 321, 434, 8295, 264, 30024, 510, 1936, 4445, 264, 39911, 291, 393, 574, 51096, 51096, 412, 309, 300, 636, 13, 583, 550, 321, 362, 281, 611, 312, 5026, 570, 274, 14893, 390, 1217, 15598, 51444, 51444, 321, 15598, 3071, 510, 293, 300, 390, 445, 264, 700, 9819, 293, 321, 434, 586, 12693, 264, 1150, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.18502827577812728, "compression_ratio": 1.6565217391304348, "no_speech_prob": 4.289273874746868e-06}, {"id": 331, "seek": 190264, "start": 1917.2800000000002, "end": 1924.24, "text": " at it that way. But then we have to also be careful because d counts was already calculated", "tokens": [50364, 370, 445, 257, 732, 12, 18759, 41011, 295, 2306, 294, 264, 3909, 295, 14893, 370, 8858, 538, 7634, 1413, 274, 14893, 2408, 13, 50820, 50820, 407, 341, 636, 321, 434, 8295, 264, 30024, 510, 1936, 4445, 264, 39911, 291, 393, 574, 51096, 51096, 412, 309, 300, 636, 13, 583, 550, 321, 362, 281, 611, 312, 5026, 570, 274, 14893, 390, 1217, 15598, 51444, 51444, 321, 15598, 3071, 510, 293, 300, 390, 445, 264, 700, 9819, 293, 321, 434, 586, 12693, 264, 1150, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.18502827577812728, "compression_ratio": 1.6565217391304348, "no_speech_prob": 4.289273874746868e-06}, {"id": 332, "seek": 190264, "start": 1924.24, "end": 1929.1200000000001, "text": " we calculated earlier here and that was just the first branch and we're now finishing the second", "tokens": [50364, 370, 445, 257, 732, 12, 18759, 41011, 295, 2306, 294, 264, 3909, 295, 14893, 370, 8858, 538, 7634, 1413, 274, 14893, 2408, 13, 50820, 50820, 407, 341, 636, 321, 434, 8295, 264, 30024, 510, 1936, 4445, 264, 39911, 291, 393, 574, 51096, 51096, 412, 309, 300, 636, 13, 583, 550, 321, 362, 281, 611, 312, 5026, 570, 274, 14893, 390, 1217, 15598, 51444, 51444, 321, 15598, 3071, 510, 293, 300, 390, 445, 264, 700, 9819, 293, 321, 434, 586, 12693, 264, 1150, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.18502827577812728, "compression_ratio": 1.6565217391304348, "no_speech_prob": 4.289273874746868e-06}, {"id": 333, "seek": 192912, "start": 1929.12, "end": 1934.6399999999999, "text": " branch so we need to make sure that these gradients add so plus equals and then here", "tokens": [50364, 9819, 370, 321, 643, 281, 652, 988, 300, 613, 2771, 2448, 909, 370, 1804, 6915, 293, 550, 510, 50640, 50676, 718, 311, 2871, 484, 264, 9660, 293, 718, 311, 652, 988, 14712, 7350, 300, 321, 362, 264, 3006, 51040, 51040, 1874, 370, 9953, 51, 284, 339, 26383, 365, 505, 322, 341, 16235, 382, 731, 13, 1033, 4696, 321, 434, 1242, 257, 3967, 295, 51332, 51332, 341, 586, 14893, 382, 364, 4478, 288, 1278, 295, 2026, 3565, 1208, 370, 586, 321, 528, 274, 2026, 3565, 1208, 293, 570, 309, 311, 364, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.17057303164867646, "compression_ratio": 1.6755555555555555, "no_speech_prob": 2.5612378067307873e-06}, {"id": 334, "seek": 192912, "start": 1935.36, "end": 1942.6399999999999, "text": " let's comment out the comparison and let's make sure crossing fingers that we have the correct", "tokens": [50364, 9819, 370, 321, 643, 281, 652, 988, 300, 613, 2771, 2448, 909, 370, 1804, 6915, 293, 550, 510, 50640, 50676, 718, 311, 2871, 484, 264, 9660, 293, 718, 311, 652, 988, 14712, 7350, 300, 321, 362, 264, 3006, 51040, 51040, 1874, 370, 9953, 51, 284, 339, 26383, 365, 505, 322, 341, 16235, 382, 731, 13, 1033, 4696, 321, 434, 1242, 257, 3967, 295, 51332, 51332, 341, 586, 14893, 382, 364, 4478, 288, 1278, 295, 2026, 3565, 1208, 370, 586, 321, 528, 274, 2026, 3565, 1208, 293, 570, 309, 311, 364, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.17057303164867646, "compression_ratio": 1.6755555555555555, "no_speech_prob": 2.5612378067307873e-06}, {"id": 335, "seek": 192912, "start": 1942.6399999999999, "end": 1948.4799999999998, "text": " result so PyTorch agrees with us on this gradient as well. Okay hopefully we're getting a hang of", "tokens": [50364, 9819, 370, 321, 643, 281, 652, 988, 300, 613, 2771, 2448, 909, 370, 1804, 6915, 293, 550, 510, 50640, 50676, 718, 311, 2871, 484, 264, 9660, 293, 718, 311, 652, 988, 14712, 7350, 300, 321, 362, 264, 3006, 51040, 51040, 1874, 370, 9953, 51, 284, 339, 26383, 365, 505, 322, 341, 16235, 382, 731, 13, 1033, 4696, 321, 434, 1242, 257, 3967, 295, 51332, 51332, 341, 586, 14893, 382, 364, 4478, 288, 1278, 295, 2026, 3565, 1208, 370, 586, 321, 528, 274, 2026, 3565, 1208, 293, 570, 309, 311, 364, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.17057303164867646, "compression_ratio": 1.6755555555555555, "no_speech_prob": 2.5612378067307873e-06}, {"id": 336, "seek": 192912, "start": 1948.4799999999998, "end": 1956.2399999999998, "text": " this now counts as an element y exp of norm logits so now we want d norm logits and because it's an", "tokens": [50364, 9819, 370, 321, 643, 281, 652, 988, 300, 613, 2771, 2448, 909, 370, 1804, 6915, 293, 550, 510, 50640, 50676, 718, 311, 2871, 484, 264, 9660, 293, 718, 311, 652, 988, 14712, 7350, 300, 321, 362, 264, 3006, 51040, 51040, 1874, 370, 9953, 51, 284, 339, 26383, 365, 505, 322, 341, 16235, 382, 731, 13, 1033, 4696, 321, 434, 1242, 257, 3967, 295, 51332, 51332, 341, 586, 14893, 382, 364, 4478, 288, 1278, 295, 2026, 3565, 1208, 370, 586, 321, 528, 274, 2026, 3565, 1208, 293, 570, 309, 311, 364, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.17057303164867646, "compression_ratio": 1.6755555555555555, "no_speech_prob": 2.5612378067307873e-06}, {"id": 337, "seek": 195624, "start": 1956.24, "end": 1960.8, "text": " element wise operation everything is very simple what is the local derivative of e to the x it's", "tokens": [50364, 4478, 10829, 6916, 1203, 307, 588, 2199, 437, 307, 264, 2654, 13760, 295, 308, 281, 264, 2031, 309, 311, 50592, 50592, 34360, 445, 308, 281, 264, 2031, 370, 341, 307, 264, 2654, 13760, 300, 307, 264, 2654, 13760, 586, 321, 51000, 51000, 1217, 15598, 309, 293, 309, 311, 1854, 14893, 370, 321, 815, 382, 731, 7263, 445, 26225, 14893, 300, 51236, 51236, 307, 264, 2654, 13760, 1413, 274, 14893, 13, 36484, 382, 300, 1542, 14893, 1413, 274, 14893, 307, 264, 13760, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1764938803280101, "compression_ratio": 2.091891891891892, "no_speech_prob": 8.800827345112339e-06}, {"id": 338, "seek": 195624, "start": 1960.8, "end": 1968.96, "text": " famously just e to the x so this is the local derivative that is the local derivative now we", "tokens": [50364, 4478, 10829, 6916, 1203, 307, 588, 2199, 437, 307, 264, 2654, 13760, 295, 308, 281, 264, 2031, 309, 311, 50592, 50592, 34360, 445, 308, 281, 264, 2031, 370, 341, 307, 264, 2654, 13760, 300, 307, 264, 2654, 13760, 586, 321, 51000, 51000, 1217, 15598, 309, 293, 309, 311, 1854, 14893, 370, 321, 815, 382, 731, 7263, 445, 26225, 14893, 300, 51236, 51236, 307, 264, 2654, 13760, 1413, 274, 14893, 13, 36484, 382, 300, 1542, 14893, 1413, 274, 14893, 307, 264, 13760, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1764938803280101, "compression_ratio": 2.091891891891892, "no_speech_prob": 8.800827345112339e-06}, {"id": 339, "seek": 195624, "start": 1968.96, "end": 1973.68, "text": " already calculated it and it's inside counts so we may as well potentially just reuse counts that", "tokens": [50364, 4478, 10829, 6916, 1203, 307, 588, 2199, 437, 307, 264, 2654, 13760, 295, 308, 281, 264, 2031, 309, 311, 50592, 50592, 34360, 445, 308, 281, 264, 2031, 370, 341, 307, 264, 2654, 13760, 300, 307, 264, 2654, 13760, 586, 321, 51000, 51000, 1217, 15598, 309, 293, 309, 311, 1854, 14893, 370, 321, 815, 382, 731, 7263, 445, 26225, 14893, 300, 51236, 51236, 307, 264, 2654, 13760, 1413, 274, 14893, 13, 36484, 382, 300, 1542, 14893, 1413, 274, 14893, 307, 264, 13760, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1764938803280101, "compression_ratio": 2.091891891891892, "no_speech_prob": 8.800827345112339e-06}, {"id": 340, "seek": 195624, "start": 1973.68, "end": 1983.44, "text": " is the local derivative times d counts. Funny as that looks counts times d counts is the derivative", "tokens": [50364, 4478, 10829, 6916, 1203, 307, 588, 2199, 437, 307, 264, 2654, 13760, 295, 308, 281, 264, 2031, 309, 311, 50592, 50592, 34360, 445, 308, 281, 264, 2031, 370, 341, 307, 264, 2654, 13760, 300, 307, 264, 2654, 13760, 586, 321, 51000, 51000, 1217, 15598, 309, 293, 309, 311, 1854, 14893, 370, 321, 815, 382, 731, 7263, 445, 26225, 14893, 300, 51236, 51236, 307, 264, 2654, 13760, 1413, 274, 14893, 13, 36484, 382, 300, 1542, 14893, 1413, 274, 14893, 307, 264, 13760, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.1764938803280101, "compression_ratio": 2.091891891891892, "no_speech_prob": 8.800827345112339e-06}, {"id": 341, "seek": 198344, "start": 1983.44, "end": 1990.16, "text": " on the norm logits and now let's erase this and let's verify and it looks good", "tokens": [50364, 322, 264, 2026, 3565, 1208, 293, 586, 718, 311, 23525, 341, 293, 718, 311, 16888, 293, 309, 1542, 665, 50700, 50812, 370, 300, 311, 2026, 3565, 1208, 1392, 370, 321, 366, 510, 322, 341, 1622, 586, 274, 2026, 3565, 1208, 321, 362, 300, 293, 321, 434, 51192, 51192, 1382, 281, 8873, 274, 3565, 1208, 293, 274, 3565, 270, 11469, 279, 370, 646, 12425, 990, 807, 341, 1622, 586, 321, 362, 281, 51484, 51484, 312, 5026, 510, 570, 264, 10854, 797, 366, 406, 264, 912, 293, 370, 456, 311, 364, 26947, 30024, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.10564696663304379, "compression_ratio": 1.8693467336683418, "no_speech_prob": 2.9943898880446795e-06}, {"id": 342, "seek": 198344, "start": 1992.4, "end": 2000.0, "text": " so that's norm logits okay so we are here on this line now d norm logits we have that and we're", "tokens": [50364, 322, 264, 2026, 3565, 1208, 293, 586, 718, 311, 23525, 341, 293, 718, 311, 16888, 293, 309, 1542, 665, 50700, 50812, 370, 300, 311, 2026, 3565, 1208, 1392, 370, 321, 366, 510, 322, 341, 1622, 586, 274, 2026, 3565, 1208, 321, 362, 300, 293, 321, 434, 51192, 51192, 1382, 281, 8873, 274, 3565, 1208, 293, 274, 3565, 270, 11469, 279, 370, 646, 12425, 990, 807, 341, 1622, 586, 321, 362, 281, 51484, 51484, 312, 5026, 510, 570, 264, 10854, 797, 366, 406, 264, 912, 293, 370, 456, 311, 364, 26947, 30024, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.10564696663304379, "compression_ratio": 1.8693467336683418, "no_speech_prob": 2.9943898880446795e-06}, {"id": 343, "seek": 198344, "start": 2000.0, "end": 2005.8400000000001, "text": " trying to calculate d logits and d logit maxes so back propagating through this line now we have to", "tokens": [50364, 322, 264, 2026, 3565, 1208, 293, 586, 718, 311, 23525, 341, 293, 718, 311, 16888, 293, 309, 1542, 665, 50700, 50812, 370, 300, 311, 2026, 3565, 1208, 1392, 370, 321, 366, 510, 322, 341, 1622, 586, 274, 2026, 3565, 1208, 321, 362, 300, 293, 321, 434, 51192, 51192, 1382, 281, 8873, 274, 3565, 1208, 293, 274, 3565, 270, 11469, 279, 370, 646, 12425, 990, 807, 341, 1622, 586, 321, 362, 281, 51484, 51484, 312, 5026, 510, 570, 264, 10854, 797, 366, 406, 264, 912, 293, 370, 456, 311, 364, 26947, 30024, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.10564696663304379, "compression_ratio": 1.8693467336683418, "no_speech_prob": 2.9943898880446795e-06}, {"id": 344, "seek": 198344, "start": 2005.8400000000001, "end": 2010.8, "text": " be careful here because the shapes again are not the same and so there's an implicit broadcasting", "tokens": [50364, 322, 264, 2026, 3565, 1208, 293, 586, 718, 311, 23525, 341, 293, 718, 311, 16888, 293, 309, 1542, 665, 50700, 50812, 370, 300, 311, 2026, 3565, 1208, 1392, 370, 321, 366, 510, 322, 341, 1622, 586, 274, 2026, 3565, 1208, 321, 362, 300, 293, 321, 434, 51192, 51192, 1382, 281, 8873, 274, 3565, 1208, 293, 274, 3565, 270, 11469, 279, 370, 646, 12425, 990, 807, 341, 1622, 586, 321, 362, 281, 51484, 51484, 312, 5026, 510, 570, 264, 10854, 797, 366, 406, 264, 912, 293, 370, 456, 311, 364, 26947, 30024, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.10564696663304379, "compression_ratio": 1.8693467336683418, "no_speech_prob": 2.9943898880446795e-06}, {"id": 345, "seek": 201080, "start": 2010.8, "end": 2018.24, "text": " happening here so norm logits has the shape 32 by 27 logits does as well but logit maxes is only 32", "tokens": [50364, 2737, 510, 370, 2026, 3565, 1208, 575, 264, 3909, 8858, 538, 7634, 3565, 1208, 775, 382, 731, 457, 3565, 270, 11469, 279, 307, 787, 8858, 50736, 50736, 538, 502, 370, 456, 311, 257, 30024, 510, 294, 264, 3175, 586, 510, 741, 853, 281, 1333, 295, 2464, 484, 257, 12058, 1365, 51136, 51136, 797, 321, 1936, 362, 300, 341, 307, 527, 269, 6915, 257, 3175, 272, 293, 321, 536, 300, 570, 295, 264, 3909, 51440, 51440, 613, 366, 1045, 538, 1045, 457, 341, 472, 307, 445, 257, 7738, 293, 370, 337, 1365, 633, 4478, 295, 269, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.06321181672992128, "compression_ratio": 1.7399103139013452, "no_speech_prob": 4.495080702326959e-06}, {"id": 346, "seek": 201080, "start": 2018.24, "end": 2026.24, "text": " by 1 so there's a broadcasting here in the minus now here i try to sort of write out a toy example", "tokens": [50364, 2737, 510, 370, 2026, 3565, 1208, 575, 264, 3909, 8858, 538, 7634, 3565, 1208, 775, 382, 731, 457, 3565, 270, 11469, 279, 307, 787, 8858, 50736, 50736, 538, 502, 370, 456, 311, 257, 30024, 510, 294, 264, 3175, 586, 510, 741, 853, 281, 1333, 295, 2464, 484, 257, 12058, 1365, 51136, 51136, 797, 321, 1936, 362, 300, 341, 307, 527, 269, 6915, 257, 3175, 272, 293, 321, 536, 300, 570, 295, 264, 3909, 51440, 51440, 613, 366, 1045, 538, 1045, 457, 341, 472, 307, 445, 257, 7738, 293, 370, 337, 1365, 633, 4478, 295, 269, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.06321181672992128, "compression_ratio": 1.7399103139013452, "no_speech_prob": 4.495080702326959e-06}, {"id": 347, "seek": 201080, "start": 2026.24, "end": 2032.32, "text": " again we basically have that this is our c equals a minus b and we see that because of the shape", "tokens": [50364, 2737, 510, 370, 2026, 3565, 1208, 575, 264, 3909, 8858, 538, 7634, 3565, 1208, 775, 382, 731, 457, 3565, 270, 11469, 279, 307, 787, 8858, 50736, 50736, 538, 502, 370, 456, 311, 257, 30024, 510, 294, 264, 3175, 586, 510, 741, 853, 281, 1333, 295, 2464, 484, 257, 12058, 1365, 51136, 51136, 797, 321, 1936, 362, 300, 341, 307, 527, 269, 6915, 257, 3175, 272, 293, 321, 536, 300, 570, 295, 264, 3909, 51440, 51440, 613, 366, 1045, 538, 1045, 457, 341, 472, 307, 445, 257, 7738, 293, 370, 337, 1365, 633, 4478, 295, 269, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.06321181672992128, "compression_ratio": 1.7399103139013452, "no_speech_prob": 4.495080702326959e-06}, {"id": 348, "seek": 201080, "start": 2032.32, "end": 2037.36, "text": " these are three by three but this one is just a column and so for example every element of c", "tokens": [50364, 2737, 510, 370, 2026, 3565, 1208, 575, 264, 3909, 8858, 538, 7634, 3565, 1208, 775, 382, 731, 457, 3565, 270, 11469, 279, 307, 787, 8858, 50736, 50736, 538, 502, 370, 456, 311, 257, 30024, 510, 294, 264, 3175, 586, 510, 741, 853, 281, 1333, 295, 2464, 484, 257, 12058, 1365, 51136, 51136, 797, 321, 1936, 362, 300, 341, 307, 527, 269, 6915, 257, 3175, 272, 293, 321, 536, 300, 570, 295, 264, 3909, 51440, 51440, 613, 366, 1045, 538, 1045, 457, 341, 472, 307, 445, 257, 7738, 293, 370, 337, 1365, 633, 4478, 295, 269, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.06321181672992128, "compression_ratio": 1.7399103139013452, "no_speech_prob": 4.495080702326959e-06}, {"id": 349, "seek": 203736, "start": 2037.36, "end": 2042.56, "text": " we have to look at how it came to be and every element of c is just the corresponding element of", "tokens": [50364, 321, 362, 281, 574, 412, 577, 309, 1361, 281, 312, 293, 633, 4478, 295, 269, 307, 445, 264, 11760, 4478, 295, 50624, 50624, 257, 3175, 1936, 300, 6615, 272, 370, 309, 311, 588, 1850, 586, 300, 264, 33733, 295, 633, 472, 295, 51108, 51108, 613, 269, 311, 365, 3104, 281, 641, 15743, 366, 472, 337, 264, 11760, 257, 293, 309, 311, 257, 3671, 472, 337, 51508, 51508], "temperature": 0.0, "avg_logprob": -0.046755279319873756, "compression_ratio": 1.7117647058823529, "no_speech_prob": 2.4439668777631596e-06}, {"id": 350, "seek": 203736, "start": 2042.56, "end": 2052.24, "text": " a minus basically that associated b so it's very clear now that the derivatives of every one of", "tokens": [50364, 321, 362, 281, 574, 412, 577, 309, 1361, 281, 312, 293, 633, 4478, 295, 269, 307, 445, 264, 11760, 4478, 295, 50624, 50624, 257, 3175, 1936, 300, 6615, 272, 370, 309, 311, 588, 1850, 586, 300, 264, 33733, 295, 633, 472, 295, 51108, 51108, 613, 269, 311, 365, 3104, 281, 641, 15743, 366, 472, 337, 264, 11760, 257, 293, 309, 311, 257, 3671, 472, 337, 51508, 51508], "temperature": 0.0, "avg_logprob": -0.046755279319873756, "compression_ratio": 1.7117647058823529, "no_speech_prob": 2.4439668777631596e-06}, {"id": 351, "seek": 203736, "start": 2052.24, "end": 2060.24, "text": " these c's with respect to their inputs are one for the corresponding a and it's a negative one for", "tokens": [50364, 321, 362, 281, 574, 412, 577, 309, 1361, 281, 312, 293, 633, 4478, 295, 269, 307, 445, 264, 11760, 4478, 295, 50624, 50624, 257, 3175, 1936, 300, 6615, 272, 370, 309, 311, 588, 1850, 586, 300, 264, 33733, 295, 633, 472, 295, 51108, 51108, 613, 269, 311, 365, 3104, 281, 641, 15743, 366, 472, 337, 264, 11760, 257, 293, 309, 311, 257, 3671, 472, 337, 51508, 51508], "temperature": 0.0, "avg_logprob": -0.046755279319873756, "compression_ratio": 1.7117647058823529, "no_speech_prob": 2.4439668777631596e-06}, {"id": 352, "seek": 206024, "start": 2060.24, "end": 2069.68, "text": " the corresponding b and so therefore the derivatives on the c will flow equally to the corresponding", "tokens": [50364, 264, 11760, 272, 293, 370, 4412, 264, 33733, 322, 264, 269, 486, 3095, 12309, 281, 264, 11760, 50836, 50836, 257, 311, 293, 550, 611, 281, 264, 11760, 272, 311, 457, 550, 294, 4500, 281, 300, 264, 272, 311, 366, 9975, 51128, 51128, 370, 321, 603, 362, 281, 360, 264, 4497, 2408, 445, 411, 321, 630, 949, 293, 295, 1164, 33733, 337, 272, 311, 51396, 51396, 486, 26426, 257, 3175, 570, 264, 2654, 13760, 510, 307, 257, 3671, 472, 370, 274, 66, 8858, 538, 274, 272, 18, 307, 3671, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.05144449420597242, "compression_ratio": 1.8714285714285714, "no_speech_prob": 1.0511228083487367e-06}, {"id": 353, "seek": 206024, "start": 2069.68, "end": 2075.52, "text": " a's and then also to the corresponding b's but then in addition to that the b's are broadcast", "tokens": [50364, 264, 11760, 272, 293, 370, 4412, 264, 33733, 322, 264, 269, 486, 3095, 12309, 281, 264, 11760, 50836, 50836, 257, 311, 293, 550, 611, 281, 264, 11760, 272, 311, 457, 550, 294, 4500, 281, 300, 264, 272, 311, 366, 9975, 51128, 51128, 370, 321, 603, 362, 281, 360, 264, 4497, 2408, 445, 411, 321, 630, 949, 293, 295, 1164, 33733, 337, 272, 311, 51396, 51396, 486, 26426, 257, 3175, 570, 264, 2654, 13760, 510, 307, 257, 3671, 472, 370, 274, 66, 8858, 538, 274, 272, 18, 307, 3671, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.05144449420597242, "compression_ratio": 1.8714285714285714, "no_speech_prob": 1.0511228083487367e-06}, {"id": 354, "seek": 206024, "start": 2075.52, "end": 2080.8799999999997, "text": " so we'll have to do the additional sum just like we did before and of course derivatives for b's", "tokens": [50364, 264, 11760, 272, 293, 370, 4412, 264, 33733, 322, 264, 269, 486, 3095, 12309, 281, 264, 11760, 50836, 50836, 257, 311, 293, 550, 611, 281, 264, 11760, 272, 311, 457, 550, 294, 4500, 281, 300, 264, 272, 311, 366, 9975, 51128, 51128, 370, 321, 603, 362, 281, 360, 264, 4497, 2408, 445, 411, 321, 630, 949, 293, 295, 1164, 33733, 337, 272, 311, 51396, 51396, 486, 26426, 257, 3175, 570, 264, 2654, 13760, 510, 307, 257, 3671, 472, 370, 274, 66, 8858, 538, 274, 272, 18, 307, 3671, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.05144449420597242, "compression_ratio": 1.8714285714285714, "no_speech_prob": 1.0511228083487367e-06}, {"id": 355, "seek": 206024, "start": 2080.8799999999997, "end": 2089.3599999999997, "text": " will undergo a minus because the local derivative here is a negative one so dc 32 by d b3 is negative", "tokens": [50364, 264, 11760, 272, 293, 370, 4412, 264, 33733, 322, 264, 269, 486, 3095, 12309, 281, 264, 11760, 50836, 50836, 257, 311, 293, 550, 611, 281, 264, 11760, 272, 311, 457, 550, 294, 4500, 281, 300, 264, 272, 311, 366, 9975, 51128, 51128, 370, 321, 603, 362, 281, 360, 264, 4497, 2408, 445, 411, 321, 630, 949, 293, 295, 1164, 33733, 337, 272, 311, 51396, 51396, 486, 26426, 257, 3175, 570, 264, 2654, 13760, 510, 307, 257, 3671, 472, 370, 274, 66, 8858, 538, 274, 272, 18, 307, 3671, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.05144449420597242, "compression_ratio": 1.8714285714285714, "no_speech_prob": 1.0511228083487367e-06}, {"id": 356, "seek": 208936, "start": 2089.36, "end": 2096.48, "text": " one so let's just implement that basically d logits will be exactly copying the derivative on", "tokens": [50364, 472, 370, 718, 311, 445, 4445, 300, 1936, 274, 3565, 1208, 486, 312, 2293, 27976, 264, 13760, 322, 50720, 50720, 2710, 3565, 1208, 370, 274, 3565, 1208, 6915, 274, 2026, 3565, 1208, 293, 741, 603, 360, 257, 5893, 26506, 337, 4514, 370, 321, 434, 445, 51148, 51148, 1455, 257, 5055, 293, 550, 321, 362, 300, 274, 3565, 270, 11469, 271, 486, 312, 264, 3671, 295, 274, 2026, 3565, 1208, 51556, 51592], "temperature": 0.0, "avg_logprob": -0.10799858668079115, "compression_ratio": 1.7098765432098766, "no_speech_prob": 4.425364750204608e-06}, {"id": 357, "seek": 208936, "start": 2096.48, "end": 2105.04, "text": " normal logits so d logits equals d norm logits and i'll do a dot clone for safety so we're just", "tokens": [50364, 472, 370, 718, 311, 445, 4445, 300, 1936, 274, 3565, 1208, 486, 312, 2293, 27976, 264, 13760, 322, 50720, 50720, 2710, 3565, 1208, 370, 274, 3565, 1208, 6915, 274, 2026, 3565, 1208, 293, 741, 603, 360, 257, 5893, 26506, 337, 4514, 370, 321, 434, 445, 51148, 51148, 1455, 257, 5055, 293, 550, 321, 362, 300, 274, 3565, 270, 11469, 271, 486, 312, 264, 3671, 295, 274, 2026, 3565, 1208, 51556, 51592], "temperature": 0.0, "avg_logprob": -0.10799858668079115, "compression_ratio": 1.7098765432098766, "no_speech_prob": 4.425364750204608e-06}, {"id": 358, "seek": 208936, "start": 2105.04, "end": 2113.2000000000003, "text": " making a copy and then we have that d logit maxis will be the negative of d norm logits", "tokens": [50364, 472, 370, 718, 311, 445, 4445, 300, 1936, 274, 3565, 1208, 486, 312, 2293, 27976, 264, 13760, 322, 50720, 50720, 2710, 3565, 1208, 370, 274, 3565, 1208, 6915, 274, 2026, 3565, 1208, 293, 741, 603, 360, 257, 5893, 26506, 337, 4514, 370, 321, 434, 445, 51148, 51148, 1455, 257, 5055, 293, 550, 321, 362, 300, 274, 3565, 270, 11469, 271, 486, 312, 264, 3671, 295, 274, 2026, 3565, 1208, 51556, 51592], "temperature": 0.0, "avg_logprob": -0.10799858668079115, "compression_ratio": 1.7098765432098766, "no_speech_prob": 4.425364750204608e-06}, {"id": 359, "seek": 211320, "start": 2113.2, "end": 2121.6, "text": " because of the negative sign and then we have to be careful because logit maxis is a column and so", "tokens": [50364, 570, 295, 264, 3671, 1465, 293, 550, 321, 362, 281, 312, 5026, 570, 3565, 270, 11469, 271, 307, 257, 7738, 293, 370, 50784, 50784, 445, 411, 321, 1866, 949, 570, 321, 1066, 3248, 30541, 264, 912, 4959, 2108, 439, 264, 13766, 51104, 51152, 550, 294, 264, 23897, 1320, 570, 321, 1066, 319, 7981, 341, 613, 366, 439, 445, 411, 4994, 14770, 51416, 51416, 295, 764, 295, 300, 472, 7006, 293, 370, 4412, 321, 362, 281, 360, 257, 2408, 2051, 472, 576, 1066, 552, 6915, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.0626032677563754, "compression_ratio": 1.8557692307692308, "no_speech_prob": 3.138088914056425e-06}, {"id": 360, "seek": 211320, "start": 2121.6, "end": 2128.0, "text": " just like we saw before because we keep replicating the same elements across all the columns", "tokens": [50364, 570, 295, 264, 3671, 1465, 293, 550, 321, 362, 281, 312, 5026, 570, 3565, 270, 11469, 271, 307, 257, 7738, 293, 370, 50784, 50784, 445, 411, 321, 1866, 949, 570, 321, 1066, 3248, 30541, 264, 912, 4959, 2108, 439, 264, 13766, 51104, 51152, 550, 294, 264, 23897, 1320, 570, 321, 1066, 319, 7981, 341, 613, 366, 439, 445, 411, 4994, 14770, 51416, 51416, 295, 764, 295, 300, 472, 7006, 293, 370, 4412, 321, 362, 281, 360, 257, 2408, 2051, 472, 576, 1066, 552, 6915, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.0626032677563754, "compression_ratio": 1.8557692307692308, "no_speech_prob": 3.138088914056425e-06}, {"id": 361, "seek": 211320, "start": 2128.96, "end": 2134.24, "text": " then in the backward pass because we keep reusing this these are all just like separate branches", "tokens": [50364, 570, 295, 264, 3671, 1465, 293, 550, 321, 362, 281, 312, 5026, 570, 3565, 270, 11469, 271, 307, 257, 7738, 293, 370, 50784, 50784, 445, 411, 321, 1866, 949, 570, 321, 1066, 3248, 30541, 264, 912, 4959, 2108, 439, 264, 13766, 51104, 51152, 550, 294, 264, 23897, 1320, 570, 321, 1066, 319, 7981, 341, 613, 366, 439, 445, 411, 4994, 14770, 51416, 51416, 295, 764, 295, 300, 472, 7006, 293, 370, 4412, 321, 362, 281, 360, 257, 2408, 2051, 472, 576, 1066, 552, 6915, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.0626032677563754, "compression_ratio": 1.8557692307692308, "no_speech_prob": 3.138088914056425e-06}, {"id": 362, "seek": 211320, "start": 2134.24, "end": 2140.08, "text": " of use of that one variable and so therefore we have to do a sum along one would keep them equals", "tokens": [50364, 570, 295, 264, 3671, 1465, 293, 550, 321, 362, 281, 312, 5026, 570, 3565, 270, 11469, 271, 307, 257, 7738, 293, 370, 50784, 50784, 445, 411, 321, 1866, 949, 570, 321, 1066, 3248, 30541, 264, 912, 4959, 2108, 439, 264, 13766, 51104, 51152, 550, 294, 264, 23897, 1320, 570, 321, 1066, 319, 7981, 341, 613, 366, 439, 445, 411, 4994, 14770, 51416, 51416, 295, 764, 295, 300, 472, 7006, 293, 370, 4412, 321, 362, 281, 360, 257, 2408, 2051, 472, 576, 1066, 552, 6915, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.0626032677563754, "compression_ratio": 1.8557692307692308, "no_speech_prob": 3.138088914056425e-06}, {"id": 363, "seek": 214008, "start": 2140.08, "end": 2146.48, "text": " true so that we don't destroy this dimension and then the logit maxis will be the same shape now", "tokens": [50364, 2074, 370, 300, 321, 500, 380, 5293, 341, 10139, 293, 550, 264, 3565, 270, 11469, 271, 486, 312, 264, 912, 3909, 586, 50684, 50684, 321, 362, 281, 312, 5026, 570, 341, 274, 3565, 1208, 307, 406, 264, 2572, 274, 3565, 1208, 293, 300, 311, 570, 406, 787, 360, 50980, 50980, 321, 483, 16235, 6358, 666, 3565, 1208, 807, 510, 457, 3565, 270, 11469, 271, 307, 257, 2445, 295, 3565, 1208, 293, 300, 311, 51280, 51280, 257, 1150, 9819, 666, 3565, 1208, 370, 341, 307, 406, 1939, 527, 2572, 13760, 337, 3565, 1208, 321, 486, 808, 646, 51548, 51548, 1780, 337, 264, 1150, 9819, 337, 586, 274, 3565, 270, 11469, 271, 307, 264, 2572, 13760, 370, 718, 385, 8585, 518, 341, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.05583755047090592, "compression_ratio": 2.04149377593361, "no_speech_prob": 5.1738361435127445e-06}, {"id": 364, "seek": 214008, "start": 2146.48, "end": 2152.4, "text": " we have to be careful because this d logits is not the final d logits and that's because not only do", "tokens": [50364, 2074, 370, 300, 321, 500, 380, 5293, 341, 10139, 293, 550, 264, 3565, 270, 11469, 271, 486, 312, 264, 912, 3909, 586, 50684, 50684, 321, 362, 281, 312, 5026, 570, 341, 274, 3565, 1208, 307, 406, 264, 2572, 274, 3565, 1208, 293, 300, 311, 570, 406, 787, 360, 50980, 50980, 321, 483, 16235, 6358, 666, 3565, 1208, 807, 510, 457, 3565, 270, 11469, 271, 307, 257, 2445, 295, 3565, 1208, 293, 300, 311, 51280, 51280, 257, 1150, 9819, 666, 3565, 1208, 370, 341, 307, 406, 1939, 527, 2572, 13760, 337, 3565, 1208, 321, 486, 808, 646, 51548, 51548, 1780, 337, 264, 1150, 9819, 337, 586, 274, 3565, 270, 11469, 271, 307, 264, 2572, 13760, 370, 718, 385, 8585, 518, 341, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.05583755047090592, "compression_ratio": 2.04149377593361, "no_speech_prob": 5.1738361435127445e-06}, {"id": 365, "seek": 214008, "start": 2152.4, "end": 2158.4, "text": " we get gradient signal into logits through here but logit maxis is a function of logits and that's", "tokens": [50364, 2074, 370, 300, 321, 500, 380, 5293, 341, 10139, 293, 550, 264, 3565, 270, 11469, 271, 486, 312, 264, 912, 3909, 586, 50684, 50684, 321, 362, 281, 312, 5026, 570, 341, 274, 3565, 1208, 307, 406, 264, 2572, 274, 3565, 1208, 293, 300, 311, 570, 406, 787, 360, 50980, 50980, 321, 483, 16235, 6358, 666, 3565, 1208, 807, 510, 457, 3565, 270, 11469, 271, 307, 257, 2445, 295, 3565, 1208, 293, 300, 311, 51280, 51280, 257, 1150, 9819, 666, 3565, 1208, 370, 341, 307, 406, 1939, 527, 2572, 13760, 337, 3565, 1208, 321, 486, 808, 646, 51548, 51548, 1780, 337, 264, 1150, 9819, 337, 586, 274, 3565, 270, 11469, 271, 307, 264, 2572, 13760, 370, 718, 385, 8585, 518, 341, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.05583755047090592, "compression_ratio": 2.04149377593361, "no_speech_prob": 5.1738361435127445e-06}, {"id": 366, "seek": 214008, "start": 2158.4, "end": 2163.7599999999998, "text": " a second branch into logits so this is not yet our final derivative for logits we will come back", "tokens": [50364, 2074, 370, 300, 321, 500, 380, 5293, 341, 10139, 293, 550, 264, 3565, 270, 11469, 271, 486, 312, 264, 912, 3909, 586, 50684, 50684, 321, 362, 281, 312, 5026, 570, 341, 274, 3565, 1208, 307, 406, 264, 2572, 274, 3565, 1208, 293, 300, 311, 570, 406, 787, 360, 50980, 50980, 321, 483, 16235, 6358, 666, 3565, 1208, 807, 510, 457, 3565, 270, 11469, 271, 307, 257, 2445, 295, 3565, 1208, 293, 300, 311, 51280, 51280, 257, 1150, 9819, 666, 3565, 1208, 370, 341, 307, 406, 1939, 527, 2572, 13760, 337, 3565, 1208, 321, 486, 808, 646, 51548, 51548, 1780, 337, 264, 1150, 9819, 337, 586, 274, 3565, 270, 11469, 271, 307, 264, 2572, 13760, 370, 718, 385, 8585, 518, 341, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.05583755047090592, "compression_ratio": 2.04149377593361, "no_speech_prob": 5.1738361435127445e-06}, {"id": 367, "seek": 214008, "start": 2163.7599999999998, "end": 2169.6, "text": " later for the second branch for now d logit maxis is the final derivative so let me uncomment this", "tokens": [50364, 2074, 370, 300, 321, 500, 380, 5293, 341, 10139, 293, 550, 264, 3565, 270, 11469, 271, 486, 312, 264, 912, 3909, 586, 50684, 50684, 321, 362, 281, 312, 5026, 570, 341, 274, 3565, 1208, 307, 406, 264, 2572, 274, 3565, 1208, 293, 300, 311, 570, 406, 787, 360, 50980, 50980, 321, 483, 16235, 6358, 666, 3565, 1208, 807, 510, 457, 3565, 270, 11469, 271, 307, 257, 2445, 295, 3565, 1208, 293, 300, 311, 51280, 51280, 257, 1150, 9819, 666, 3565, 1208, 370, 341, 307, 406, 1939, 527, 2572, 13760, 337, 3565, 1208, 321, 486, 808, 646, 51548, 51548, 1780, 337, 264, 1150, 9819, 337, 586, 274, 3565, 270, 11469, 271, 307, 264, 2572, 13760, 370, 718, 385, 8585, 518, 341, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.05583755047090592, "compression_ratio": 2.04149377593361, "no_speech_prob": 5.1738361435127445e-06}, {"id": 368, "seek": 216960, "start": 2169.6, "end": 2177.68, "text": " cmp here and let's just run this and logit maxis if pytorch agrees with us so that was derivative", "tokens": [50364, 269, 2455, 510, 293, 718, 311, 445, 1190, 341, 293, 3565, 270, 11469, 271, 498, 25878, 284, 339, 26383, 365, 505, 370, 300, 390, 13760, 50768, 50768, 666, 807, 341, 1622, 586, 949, 321, 1286, 322, 741, 528, 281, 10465, 510, 10515, 293, 741, 528, 281, 574, 412, 51092, 51092, 613, 3565, 270, 11469, 271, 293, 2318, 641, 2771, 2448, 321, 600, 2825, 8046, 294, 264, 3894, 7991, 51380, 51380, 300, 264, 787, 1778, 321, 434, 884, 341, 307, 337, 264, 29054, 11826, 295, 264, 2787, 41167, 300, 321, 366, 51592, 51592, 18114, 510, 293, 321, 2825, 466, 577, 498, 291, 747, 613, 3565, 1208, 337, 604, 472, 295, 613, 5110, 51856], "temperature": 0.0, "avg_logprob": -0.052452518228898966, "compression_ratio": 1.8226415094339623, "no_speech_prob": 1.9222712580813095e-05}, {"id": 369, "seek": 216960, "start": 2177.68, "end": 2184.16, "text": " into through this line now before we move on i want to pause here briefly and i want to look at", "tokens": [50364, 269, 2455, 510, 293, 718, 311, 445, 1190, 341, 293, 3565, 270, 11469, 271, 498, 25878, 284, 339, 26383, 365, 505, 370, 300, 390, 13760, 50768, 50768, 666, 807, 341, 1622, 586, 949, 321, 1286, 322, 741, 528, 281, 10465, 510, 10515, 293, 741, 528, 281, 574, 412, 51092, 51092, 613, 3565, 270, 11469, 271, 293, 2318, 641, 2771, 2448, 321, 600, 2825, 8046, 294, 264, 3894, 7991, 51380, 51380, 300, 264, 787, 1778, 321, 434, 884, 341, 307, 337, 264, 29054, 11826, 295, 264, 2787, 41167, 300, 321, 366, 51592, 51592, 18114, 510, 293, 321, 2825, 466, 577, 498, 291, 747, 613, 3565, 1208, 337, 604, 472, 295, 613, 5110, 51856], "temperature": 0.0, "avg_logprob": -0.052452518228898966, "compression_ratio": 1.8226415094339623, "no_speech_prob": 1.9222712580813095e-05}, {"id": 370, "seek": 216960, "start": 2184.16, "end": 2189.92, "text": " these logit maxis and especially their gradients we've talked previously in the previous lecture", "tokens": [50364, 269, 2455, 510, 293, 718, 311, 445, 1190, 341, 293, 3565, 270, 11469, 271, 498, 25878, 284, 339, 26383, 365, 505, 370, 300, 390, 13760, 50768, 50768, 666, 807, 341, 1622, 586, 949, 321, 1286, 322, 741, 528, 281, 10465, 510, 10515, 293, 741, 528, 281, 574, 412, 51092, 51092, 613, 3565, 270, 11469, 271, 293, 2318, 641, 2771, 2448, 321, 600, 2825, 8046, 294, 264, 3894, 7991, 51380, 51380, 300, 264, 787, 1778, 321, 434, 884, 341, 307, 337, 264, 29054, 11826, 295, 264, 2787, 41167, 300, 321, 366, 51592, 51592, 18114, 510, 293, 321, 2825, 466, 577, 498, 291, 747, 613, 3565, 1208, 337, 604, 472, 295, 613, 5110, 51856], "temperature": 0.0, "avg_logprob": -0.052452518228898966, "compression_ratio": 1.8226415094339623, "no_speech_prob": 1.9222712580813095e-05}, {"id": 371, "seek": 216960, "start": 2189.92, "end": 2194.16, "text": " that the only reason we're doing this is for the numerical stability of the softmax that we are", "tokens": [50364, 269, 2455, 510, 293, 718, 311, 445, 1190, 341, 293, 3565, 270, 11469, 271, 498, 25878, 284, 339, 26383, 365, 505, 370, 300, 390, 13760, 50768, 50768, 666, 807, 341, 1622, 586, 949, 321, 1286, 322, 741, 528, 281, 10465, 510, 10515, 293, 741, 528, 281, 574, 412, 51092, 51092, 613, 3565, 270, 11469, 271, 293, 2318, 641, 2771, 2448, 321, 600, 2825, 8046, 294, 264, 3894, 7991, 51380, 51380, 300, 264, 787, 1778, 321, 434, 884, 341, 307, 337, 264, 29054, 11826, 295, 264, 2787, 41167, 300, 321, 366, 51592, 51592, 18114, 510, 293, 321, 2825, 466, 577, 498, 291, 747, 613, 3565, 1208, 337, 604, 472, 295, 613, 5110, 51856], "temperature": 0.0, "avg_logprob": -0.052452518228898966, "compression_ratio": 1.8226415094339623, "no_speech_prob": 1.9222712580813095e-05}, {"id": 372, "seek": 219416, "start": 2194.16, "end": 2199.68, "text": " implementing here and we talked about how if you take these logits for any one of these examples so", "tokens": [50364, 18114, 510, 293, 321, 2825, 466, 577, 498, 291, 747, 613, 3565, 1208, 337, 604, 472, 295, 613, 5110, 370, 50640, 50640, 472, 5386, 295, 341, 3565, 1208, 40863, 498, 291, 909, 420, 16390, 604, 2158, 12309, 281, 439, 264, 4959, 50936, 50968, 550, 264, 2158, 295, 264, 1239, 279, 486, 312, 44553, 291, 434, 406, 4473, 264, 2787, 41167, 264, 787, 551, 51224, 51224, 300, 341, 307, 884, 307, 309, 311, 1455, 988, 300, 2031, 1177, 380, 37772, 293, 264, 1778, 321, 434, 1228, 257, 11469, 51480, 51480, 307, 570, 550, 321, 366, 18031, 300, 1184, 5386, 295, 3565, 1208, 264, 6343, 1230, 307, 4018, 293, 370, 341, 486, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.044141777774743866, "compression_ratio": 1.832699619771863, "no_speech_prob": 2.6424806947034085e-06}, {"id": 373, "seek": 219416, "start": 2199.68, "end": 2205.6, "text": " one row of this logits tensor if you add or subtract any value equally to all the elements", "tokens": [50364, 18114, 510, 293, 321, 2825, 466, 577, 498, 291, 747, 613, 3565, 1208, 337, 604, 472, 295, 613, 5110, 370, 50640, 50640, 472, 5386, 295, 341, 3565, 1208, 40863, 498, 291, 909, 420, 16390, 604, 2158, 12309, 281, 439, 264, 4959, 50936, 50968, 550, 264, 2158, 295, 264, 1239, 279, 486, 312, 44553, 291, 434, 406, 4473, 264, 2787, 41167, 264, 787, 551, 51224, 51224, 300, 341, 307, 884, 307, 309, 311, 1455, 988, 300, 2031, 1177, 380, 37772, 293, 264, 1778, 321, 434, 1228, 257, 11469, 51480, 51480, 307, 570, 550, 321, 366, 18031, 300, 1184, 5386, 295, 3565, 1208, 264, 6343, 1230, 307, 4018, 293, 370, 341, 486, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.044141777774743866, "compression_ratio": 1.832699619771863, "no_speech_prob": 2.6424806947034085e-06}, {"id": 374, "seek": 219416, "start": 2206.24, "end": 2211.3599999999997, "text": " then the value of the probes will be unchanged you're not changing the softmax the only thing", "tokens": [50364, 18114, 510, 293, 321, 2825, 466, 577, 498, 291, 747, 613, 3565, 1208, 337, 604, 472, 295, 613, 5110, 370, 50640, 50640, 472, 5386, 295, 341, 3565, 1208, 40863, 498, 291, 909, 420, 16390, 604, 2158, 12309, 281, 439, 264, 4959, 50936, 50968, 550, 264, 2158, 295, 264, 1239, 279, 486, 312, 44553, 291, 434, 406, 4473, 264, 2787, 41167, 264, 787, 551, 51224, 51224, 300, 341, 307, 884, 307, 309, 311, 1455, 988, 300, 2031, 1177, 380, 37772, 293, 264, 1778, 321, 434, 1228, 257, 11469, 51480, 51480, 307, 570, 550, 321, 366, 18031, 300, 1184, 5386, 295, 3565, 1208, 264, 6343, 1230, 307, 4018, 293, 370, 341, 486, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.044141777774743866, "compression_ratio": 1.832699619771863, "no_speech_prob": 2.6424806947034085e-06}, {"id": 375, "seek": 219416, "start": 2211.3599999999997, "end": 2216.48, "text": " that this is doing is it's making sure that x doesn't overflow and the reason we're using a max", "tokens": [50364, 18114, 510, 293, 321, 2825, 466, 577, 498, 291, 747, 613, 3565, 1208, 337, 604, 472, 295, 613, 5110, 370, 50640, 50640, 472, 5386, 295, 341, 3565, 1208, 40863, 498, 291, 909, 420, 16390, 604, 2158, 12309, 281, 439, 264, 4959, 50936, 50968, 550, 264, 2158, 295, 264, 1239, 279, 486, 312, 44553, 291, 434, 406, 4473, 264, 2787, 41167, 264, 787, 551, 51224, 51224, 300, 341, 307, 884, 307, 309, 311, 1455, 988, 300, 2031, 1177, 380, 37772, 293, 264, 1778, 321, 434, 1228, 257, 11469, 51480, 51480, 307, 570, 550, 321, 366, 18031, 300, 1184, 5386, 295, 3565, 1208, 264, 6343, 1230, 307, 4018, 293, 370, 341, 486, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.044141777774743866, "compression_ratio": 1.832699619771863, "no_speech_prob": 2.6424806947034085e-06}, {"id": 376, "seek": 219416, "start": 2216.48, "end": 2222.24, "text": " is because then we are guaranteed that each row of logits the highest number is zero and so this will", "tokens": [50364, 18114, 510, 293, 321, 2825, 466, 577, 498, 291, 747, 613, 3565, 1208, 337, 604, 472, 295, 613, 5110, 370, 50640, 50640, 472, 5386, 295, 341, 3565, 1208, 40863, 498, 291, 909, 420, 16390, 604, 2158, 12309, 281, 439, 264, 4959, 50936, 50968, 550, 264, 2158, 295, 264, 1239, 279, 486, 312, 44553, 291, 434, 406, 4473, 264, 2787, 41167, 264, 787, 551, 51224, 51224, 300, 341, 307, 884, 307, 309, 311, 1455, 988, 300, 2031, 1177, 380, 37772, 293, 264, 1778, 321, 434, 1228, 257, 11469, 51480, 51480, 307, 570, 550, 321, 366, 18031, 300, 1184, 5386, 295, 3565, 1208, 264, 6343, 1230, 307, 4018, 293, 370, 341, 486, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.044141777774743866, "compression_ratio": 1.832699619771863, "no_speech_prob": 2.6424806947034085e-06}, {"id": 377, "seek": 222224, "start": 2222.24, "end": 2232.0, "text": " be safe and so basically what that has repercussions if it is the case that changing logit maxis", "tokens": [50364, 312, 3273, 293, 370, 1936, 437, 300, 575, 28946, 38899, 498, 309, 307, 264, 1389, 300, 4473, 3565, 270, 11469, 271, 50852, 50880, 775, 406, 1319, 264, 1239, 279, 293, 4412, 775, 406, 1319, 264, 4470, 550, 264, 16235, 322, 3565, 270, 11469, 271, 51116, 51116, 820, 312, 4018, 558, 570, 1566, 729, 732, 721, 307, 264, 912, 370, 6451, 321, 1454, 300, 341, 51408, 51408, 307, 588, 588, 1359, 3547, 6451, 321, 1454, 341, 307, 4018, 586, 570, 295, 12607, 935, 1333, 295, 1582, 74, 1324, 51696, 51752], "temperature": 0.0, "avg_logprob": -0.0641471789433406, "compression_ratio": 1.8888888888888888, "no_speech_prob": 3.3075568239837594e-07}, {"id": 378, "seek": 222224, "start": 2232.56, "end": 2237.2799999999997, "text": " does not change the probes and therefore does not change the loss then the gradient on logit maxis", "tokens": [50364, 312, 3273, 293, 370, 1936, 437, 300, 575, 28946, 38899, 498, 309, 307, 264, 1389, 300, 4473, 3565, 270, 11469, 271, 50852, 50880, 775, 406, 1319, 264, 1239, 279, 293, 4412, 775, 406, 1319, 264, 4470, 550, 264, 16235, 322, 3565, 270, 11469, 271, 51116, 51116, 820, 312, 4018, 558, 570, 1566, 729, 732, 721, 307, 264, 912, 370, 6451, 321, 1454, 300, 341, 51408, 51408, 307, 588, 588, 1359, 3547, 6451, 321, 1454, 341, 307, 4018, 586, 570, 295, 12607, 935, 1333, 295, 1582, 74, 1324, 51696, 51752], "temperature": 0.0, "avg_logprob": -0.0641471789433406, "compression_ratio": 1.8888888888888888, "no_speech_prob": 3.3075568239837594e-07}, {"id": 379, "seek": 222224, "start": 2237.2799999999997, "end": 2243.12, "text": " should be zero right because saying those two things is the same so indeed we hope that this", "tokens": [50364, 312, 3273, 293, 370, 1936, 437, 300, 575, 28946, 38899, 498, 309, 307, 264, 1389, 300, 4473, 3565, 270, 11469, 271, 50852, 50880, 775, 406, 1319, 264, 1239, 279, 293, 4412, 775, 406, 1319, 264, 4470, 550, 264, 16235, 322, 3565, 270, 11469, 271, 51116, 51116, 820, 312, 4018, 558, 570, 1566, 729, 732, 721, 307, 264, 912, 370, 6451, 321, 1454, 300, 341, 51408, 51408, 307, 588, 588, 1359, 3547, 6451, 321, 1454, 341, 307, 4018, 586, 570, 295, 12607, 935, 1333, 295, 1582, 74, 1324, 51696, 51752], "temperature": 0.0, "avg_logprob": -0.0641471789433406, "compression_ratio": 1.8888888888888888, "no_speech_prob": 3.3075568239837594e-07}, {"id": 380, "seek": 222224, "start": 2243.12, "end": 2248.8799999999997, "text": " is very very small numbers indeed we hope this is zero now because of floating point sort of wonkiness", "tokens": [50364, 312, 3273, 293, 370, 1936, 437, 300, 575, 28946, 38899, 498, 309, 307, 264, 1389, 300, 4473, 3565, 270, 11469, 271, 50852, 50880, 775, 406, 1319, 264, 1239, 279, 293, 4412, 775, 406, 1319, 264, 4470, 550, 264, 16235, 322, 3565, 270, 11469, 271, 51116, 51116, 820, 312, 4018, 558, 570, 1566, 729, 732, 721, 307, 264, 912, 370, 6451, 321, 1454, 300, 341, 51408, 51408, 307, 588, 588, 1359, 3547, 6451, 321, 1454, 341, 307, 4018, 586, 570, 295, 12607, 935, 1333, 295, 1582, 74, 1324, 51696, 51752], "temperature": 0.0, "avg_logprob": -0.0641471789433406, "compression_ratio": 1.8888888888888888, "no_speech_prob": 3.3075568239837594e-07}, {"id": 381, "seek": 224888, "start": 2248.88, "end": 2253.6, "text": " this doesn't come out exactly zero only in some of the rows it does but we get extremely small", "tokens": [50364, 341, 1177, 380, 808, 484, 2293, 4018, 787, 294, 512, 295, 264, 13241, 309, 775, 457, 321, 483, 4664, 1359, 50600, 50600, 4190, 411, 472, 308, 3671, 4949, 420, 2064, 293, 370, 341, 307, 3585, 505, 300, 264, 4190, 295, 3565, 270, 11469, 271, 366, 50876, 50876, 406, 29963, 264, 4470, 382, 436, 4659, 380, 309, 3417, 733, 295, 3657, 281, 646, 48256, 807, 341, 9819, 51140, 51140, 6095, 570, 498, 291, 362, 604, 11420, 295, 411, 283, 5893, 3278, 30867, 293, 3895, 27822, 293, 291, 51484, 51484, 291, 3461, 1214, 439, 613, 4959, 293, 291, 434, 406, 884, 264, 646, 38377, 2522, 538, 2522, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.14942471915428793, "compression_ratio": 1.781021897810219, "no_speech_prob": 6.438822310883552e-06}, {"id": 382, "seek": 224888, "start": 2253.6, "end": 2259.12, "text": " values like one e negative nine or ten and so this is telling us that the values of logit maxis are", "tokens": [50364, 341, 1177, 380, 808, 484, 2293, 4018, 787, 294, 512, 295, 264, 13241, 309, 775, 457, 321, 483, 4664, 1359, 50600, 50600, 4190, 411, 472, 308, 3671, 4949, 420, 2064, 293, 370, 341, 307, 3585, 505, 300, 264, 4190, 295, 3565, 270, 11469, 271, 366, 50876, 50876, 406, 29963, 264, 4470, 382, 436, 4659, 380, 309, 3417, 733, 295, 3657, 281, 646, 48256, 807, 341, 9819, 51140, 51140, 6095, 570, 498, 291, 362, 604, 11420, 295, 411, 283, 5893, 3278, 30867, 293, 3895, 27822, 293, 291, 51484, 51484, 291, 3461, 1214, 439, 613, 4959, 293, 291, 434, 406, 884, 264, 646, 38377, 2522, 538, 2522, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.14942471915428793, "compression_ratio": 1.781021897810219, "no_speech_prob": 6.438822310883552e-06}, {"id": 383, "seek": 224888, "start": 2259.12, "end": 2264.4, "text": " not impacting the loss as they shouldn't it feels kind of weird to back propagate through this branch", "tokens": [50364, 341, 1177, 380, 808, 484, 2293, 4018, 787, 294, 512, 295, 264, 13241, 309, 775, 457, 321, 483, 4664, 1359, 50600, 50600, 4190, 411, 472, 308, 3671, 4949, 420, 2064, 293, 370, 341, 307, 3585, 505, 300, 264, 4190, 295, 3565, 270, 11469, 271, 366, 50876, 50876, 406, 29963, 264, 4470, 382, 436, 4659, 380, 309, 3417, 733, 295, 3657, 281, 646, 48256, 807, 341, 9819, 51140, 51140, 6095, 570, 498, 291, 362, 604, 11420, 295, 411, 283, 5893, 3278, 30867, 293, 3895, 27822, 293, 291, 51484, 51484, 291, 3461, 1214, 439, 613, 4959, 293, 291, 434, 406, 884, 264, 646, 38377, 2522, 538, 2522, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.14942471915428793, "compression_ratio": 1.781021897810219, "no_speech_prob": 6.438822310883552e-06}, {"id": 384, "seek": 224888, "start": 2264.4, "end": 2271.28, "text": " honestly because if you have any implementation of like f dot cross entropy and pi torch and you", "tokens": [50364, 341, 1177, 380, 808, 484, 2293, 4018, 787, 294, 512, 295, 264, 13241, 309, 775, 457, 321, 483, 4664, 1359, 50600, 50600, 4190, 411, 472, 308, 3671, 4949, 420, 2064, 293, 370, 341, 307, 3585, 505, 300, 264, 4190, 295, 3565, 270, 11469, 271, 366, 50876, 50876, 406, 29963, 264, 4470, 382, 436, 4659, 380, 309, 3417, 733, 295, 3657, 281, 646, 48256, 807, 341, 9819, 51140, 51140, 6095, 570, 498, 291, 362, 604, 11420, 295, 411, 283, 5893, 3278, 30867, 293, 3895, 27822, 293, 291, 51484, 51484, 291, 3461, 1214, 439, 613, 4959, 293, 291, 434, 406, 884, 264, 646, 38377, 2522, 538, 2522, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.14942471915428793, "compression_ratio": 1.781021897810219, "no_speech_prob": 6.438822310883552e-06}, {"id": 385, "seek": 224888, "start": 2271.28, "end": 2275.6, "text": " you block together all these elements and you're not doing the back propagation piece by piece", "tokens": [50364, 341, 1177, 380, 808, 484, 2293, 4018, 787, 294, 512, 295, 264, 13241, 309, 775, 457, 321, 483, 4664, 1359, 50600, 50600, 4190, 411, 472, 308, 3671, 4949, 420, 2064, 293, 370, 341, 307, 3585, 505, 300, 264, 4190, 295, 3565, 270, 11469, 271, 366, 50876, 50876, 406, 29963, 264, 4470, 382, 436, 4659, 380, 309, 3417, 733, 295, 3657, 281, 646, 48256, 807, 341, 9819, 51140, 51140, 6095, 570, 498, 291, 362, 604, 11420, 295, 411, 283, 5893, 3278, 30867, 293, 3895, 27822, 293, 291, 51484, 51484, 291, 3461, 1214, 439, 613, 4959, 293, 291, 434, 406, 884, 264, 646, 38377, 2522, 538, 2522, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.14942471915428793, "compression_ratio": 1.781021897810219, "no_speech_prob": 6.438822310883552e-06}, {"id": 386, "seek": 227560, "start": 2275.6, "end": 2281.44, "text": " then you would probably assume that the derivative through here is exactly zero so you would be sort of", "tokens": [50364, 550, 291, 576, 1391, 6552, 300, 264, 13760, 807, 510, 307, 2293, 4018, 370, 291, 576, 312, 1333, 295, 50656, 50732, 31533, 341, 9819, 570, 309, 311, 787, 1096, 337, 29054, 11826, 457, 309, 311, 1880, 281, 536, 51032, 51032, 300, 754, 498, 291, 1821, 493, 1203, 666, 264, 1577, 16871, 293, 291, 920, 360, 264, 24903, 382, 291, 1116, 51264, 51264, 411, 365, 3104, 281, 29054, 11826, 264, 3006, 551, 2314, 293, 291, 920, 483, 257, 588, 51504, 51504, 588, 1359, 2771, 2448, 510, 1936, 23543, 264, 1186, 300, 264, 4190, 295, 613, 360, 406, 1871, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.14464337754957746, "compression_ratio": 1.830188679245283, "no_speech_prob": 1.1842604180856142e-05}, {"id": 387, "seek": 227560, "start": 2282.96, "end": 2288.96, "text": " skipping this branch because it's only done for numerical stability but it's interesting to see", "tokens": [50364, 550, 291, 576, 1391, 6552, 300, 264, 13760, 807, 510, 307, 2293, 4018, 370, 291, 576, 312, 1333, 295, 50656, 50732, 31533, 341, 9819, 570, 309, 311, 787, 1096, 337, 29054, 11826, 457, 309, 311, 1880, 281, 536, 51032, 51032, 300, 754, 498, 291, 1821, 493, 1203, 666, 264, 1577, 16871, 293, 291, 920, 360, 264, 24903, 382, 291, 1116, 51264, 51264, 411, 365, 3104, 281, 29054, 11826, 264, 3006, 551, 2314, 293, 291, 920, 483, 257, 588, 51504, 51504, 588, 1359, 2771, 2448, 510, 1936, 23543, 264, 1186, 300, 264, 4190, 295, 613, 360, 406, 1871, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.14464337754957746, "compression_ratio": 1.830188679245283, "no_speech_prob": 1.1842604180856142e-05}, {"id": 388, "seek": 227560, "start": 2288.96, "end": 2293.6, "text": " that even if you break up everything into the full atoms and you still do the computation as you'd", "tokens": [50364, 550, 291, 576, 1391, 6552, 300, 264, 13760, 807, 510, 307, 2293, 4018, 370, 291, 576, 312, 1333, 295, 50656, 50732, 31533, 341, 9819, 570, 309, 311, 787, 1096, 337, 29054, 11826, 457, 309, 311, 1880, 281, 536, 51032, 51032, 300, 754, 498, 291, 1821, 493, 1203, 666, 264, 1577, 16871, 293, 291, 920, 360, 264, 24903, 382, 291, 1116, 51264, 51264, 411, 365, 3104, 281, 29054, 11826, 264, 3006, 551, 2314, 293, 291, 920, 483, 257, 588, 51504, 51504, 588, 1359, 2771, 2448, 510, 1936, 23543, 264, 1186, 300, 264, 4190, 295, 613, 360, 406, 1871, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.14464337754957746, "compression_ratio": 1.830188679245283, "no_speech_prob": 1.1842604180856142e-05}, {"id": 389, "seek": 227560, "start": 2293.6, "end": 2298.4, "text": " like with respect to numerical stability the correct thing happens and you still get a very", "tokens": [50364, 550, 291, 576, 1391, 6552, 300, 264, 13760, 807, 510, 307, 2293, 4018, 370, 291, 576, 312, 1333, 295, 50656, 50732, 31533, 341, 9819, 570, 309, 311, 787, 1096, 337, 29054, 11826, 457, 309, 311, 1880, 281, 536, 51032, 51032, 300, 754, 498, 291, 1821, 493, 1203, 666, 264, 1577, 16871, 293, 291, 920, 360, 264, 24903, 382, 291, 1116, 51264, 51264, 411, 365, 3104, 281, 29054, 11826, 264, 3006, 551, 2314, 293, 291, 920, 483, 257, 588, 51504, 51504, 588, 1359, 2771, 2448, 510, 1936, 23543, 264, 1186, 300, 264, 4190, 295, 613, 360, 406, 1871, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.14464337754957746, "compression_ratio": 1.830188679245283, "no_speech_prob": 1.1842604180856142e-05}, {"id": 390, "seek": 227560, "start": 2298.4, "end": 2304.3199999999997, "text": " very small gradients here basically reflecting the fact that the values of these do not matter", "tokens": [50364, 550, 291, 576, 1391, 6552, 300, 264, 13760, 807, 510, 307, 2293, 4018, 370, 291, 576, 312, 1333, 295, 50656, 50732, 31533, 341, 9819, 570, 309, 311, 787, 1096, 337, 29054, 11826, 457, 309, 311, 1880, 281, 536, 51032, 51032, 300, 754, 498, 291, 1821, 493, 1203, 666, 264, 1577, 16871, 293, 291, 920, 360, 264, 24903, 382, 291, 1116, 51264, 51264, 411, 365, 3104, 281, 29054, 11826, 264, 3006, 551, 2314, 293, 291, 920, 483, 257, 588, 51504, 51504, 588, 1359, 2771, 2448, 510, 1936, 23543, 264, 1186, 300, 264, 4190, 295, 613, 360, 406, 1871, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.14464337754957746, "compression_ratio": 1.830188679245283, "no_speech_prob": 1.1842604180856142e-05}, {"id": 391, "seek": 230432, "start": 2304.32, "end": 2309.04, "text": " uh do not matter with respect to the final loss okay so let's now continue back propagation through", "tokens": [50364, 2232, 360, 406, 1871, 365, 3104, 281, 264, 2572, 4470, 1392, 370, 718, 311, 586, 2354, 646, 38377, 807, 50600, 50600, 341, 1622, 510, 321, 600, 445, 15598, 264, 3565, 270, 11469, 271, 293, 586, 321, 528, 281, 646, 2365, 666, 3565, 1208, 50848, 50848, 807, 341, 1150, 9819, 586, 510, 295, 1164, 321, 1890, 3565, 1208, 293, 321, 1890, 264, 11469, 2051, 439, 264, 13241, 51136, 51164, 293, 550, 321, 2956, 412, 1080, 4190, 510, 586, 264, 636, 341, 1985, 307, 300, 294, 3895, 27822, 51388, 51512, 341, 551, 510, 264, 11469, 11247, 1293, 264, 4190, 293, 309, 11247, 264, 43840, 412, 597, 729, 4190, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.1282058455727317, "compression_ratio": 1.891566265060241, "no_speech_prob": 3.0415355922741583e-06}, {"id": 392, "seek": 230432, "start": 2309.04, "end": 2314.0, "text": " this line here we've just calculated the logit maxis and now we want to back prop into logits", "tokens": [50364, 2232, 360, 406, 1871, 365, 3104, 281, 264, 2572, 4470, 1392, 370, 718, 311, 586, 2354, 646, 38377, 807, 50600, 50600, 341, 1622, 510, 321, 600, 445, 15598, 264, 3565, 270, 11469, 271, 293, 586, 321, 528, 281, 646, 2365, 666, 3565, 1208, 50848, 50848, 807, 341, 1150, 9819, 586, 510, 295, 1164, 321, 1890, 3565, 1208, 293, 321, 1890, 264, 11469, 2051, 439, 264, 13241, 51136, 51164, 293, 550, 321, 2956, 412, 1080, 4190, 510, 586, 264, 636, 341, 1985, 307, 300, 294, 3895, 27822, 51388, 51512, 341, 551, 510, 264, 11469, 11247, 1293, 264, 4190, 293, 309, 11247, 264, 43840, 412, 597, 729, 4190, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.1282058455727317, "compression_ratio": 1.891566265060241, "no_speech_prob": 3.0415355922741583e-06}, {"id": 393, "seek": 230432, "start": 2314.0, "end": 2319.76, "text": " through this second branch now here of course we took logits and we took the max along all the rows", "tokens": [50364, 2232, 360, 406, 1871, 365, 3104, 281, 264, 2572, 4470, 1392, 370, 718, 311, 586, 2354, 646, 38377, 807, 50600, 50600, 341, 1622, 510, 321, 600, 445, 15598, 264, 3565, 270, 11469, 271, 293, 586, 321, 528, 281, 646, 2365, 666, 3565, 1208, 50848, 50848, 807, 341, 1150, 9819, 586, 510, 295, 1164, 321, 1890, 3565, 1208, 293, 321, 1890, 264, 11469, 2051, 439, 264, 13241, 51136, 51164, 293, 550, 321, 2956, 412, 1080, 4190, 510, 586, 264, 636, 341, 1985, 307, 300, 294, 3895, 27822, 51388, 51512, 341, 551, 510, 264, 11469, 11247, 1293, 264, 4190, 293, 309, 11247, 264, 43840, 412, 597, 729, 4190, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.1282058455727317, "compression_ratio": 1.891566265060241, "no_speech_prob": 3.0415355922741583e-06}, {"id": 394, "seek": 230432, "start": 2320.32, "end": 2324.8, "text": " and then we looked at its values here now the way this works is that in pi torch", "tokens": [50364, 2232, 360, 406, 1871, 365, 3104, 281, 264, 2572, 4470, 1392, 370, 718, 311, 586, 2354, 646, 38377, 807, 50600, 50600, 341, 1622, 510, 321, 600, 445, 15598, 264, 3565, 270, 11469, 271, 293, 586, 321, 528, 281, 646, 2365, 666, 3565, 1208, 50848, 50848, 807, 341, 1150, 9819, 586, 510, 295, 1164, 321, 1890, 3565, 1208, 293, 321, 1890, 264, 11469, 2051, 439, 264, 13241, 51136, 51164, 293, 550, 321, 2956, 412, 1080, 4190, 510, 586, 264, 636, 341, 1985, 307, 300, 294, 3895, 27822, 51388, 51512, 341, 551, 510, 264, 11469, 11247, 1293, 264, 4190, 293, 309, 11247, 264, 43840, 412, 597, 729, 4190, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.1282058455727317, "compression_ratio": 1.891566265060241, "no_speech_prob": 3.0415355922741583e-06}, {"id": 395, "seek": 230432, "start": 2327.28, "end": 2333.04, "text": " this thing here the max returns both the values and it returns the indices at which those values", "tokens": [50364, 2232, 360, 406, 1871, 365, 3104, 281, 264, 2572, 4470, 1392, 370, 718, 311, 586, 2354, 646, 38377, 807, 50600, 50600, 341, 1622, 510, 321, 600, 445, 15598, 264, 3565, 270, 11469, 271, 293, 586, 321, 528, 281, 646, 2365, 666, 3565, 1208, 50848, 50848, 807, 341, 1150, 9819, 586, 510, 295, 1164, 321, 1890, 3565, 1208, 293, 321, 1890, 264, 11469, 2051, 439, 264, 13241, 51136, 51164, 293, 550, 321, 2956, 412, 1080, 4190, 510, 586, 264, 636, 341, 1985, 307, 300, 294, 3895, 27822, 51388, 51512, 341, 551, 510, 264, 11469, 11247, 1293, 264, 4190, 293, 309, 11247, 264, 43840, 412, 597, 729, 4190, 51800, 51800], "temperature": 0.0, "avg_logprob": -0.1282058455727317, "compression_ratio": 1.891566265060241, "no_speech_prob": 3.0415355922741583e-06}, {"id": 396, "seek": 233304, "start": 2333.04, "end": 2338.96, "text": " to count the maximum value now in the forward pass we only used values because that's all we needed", "tokens": [50364, 281, 1207, 264, 6674, 2158, 586, 294, 264, 2128, 1320, 321, 787, 1143, 4190, 570, 300, 311, 439, 321, 2978, 50660, 50660, 457, 294, 264, 23897, 1320, 309, 311, 4664, 4420, 281, 458, 466, 689, 729, 6674, 4190, 11068, 50924, 50956, 293, 321, 362, 264, 43840, 412, 597, 436, 11068, 293, 341, 486, 295, 1164, 854, 505, 281, 854, 505, 360, 264, 51176, 51176, 646, 38377, 570, 437, 820, 264, 23897, 1320, 312, 510, 294, 341, 1389, 321, 362, 264, 3565, 270, 40863, 51440, 51440, 597, 307, 8858, 538, 7634, 293, 294, 1184, 5386, 321, 915, 264, 6674, 2158, 293, 550, 300, 2158, 2170, 41514, 292, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.1472243915904652, "compression_ratio": 1.948, "no_speech_prob": 1.9142235885283299e-07}, {"id": 397, "seek": 233304, "start": 2338.96, "end": 2344.24, "text": " but in the backward pass it's extremely useful to know about where those maximum values occurred", "tokens": [50364, 281, 1207, 264, 6674, 2158, 586, 294, 264, 2128, 1320, 321, 787, 1143, 4190, 570, 300, 311, 439, 321, 2978, 50660, 50660, 457, 294, 264, 23897, 1320, 309, 311, 4664, 4420, 281, 458, 466, 689, 729, 6674, 4190, 11068, 50924, 50956, 293, 321, 362, 264, 43840, 412, 597, 436, 11068, 293, 341, 486, 295, 1164, 854, 505, 281, 854, 505, 360, 264, 51176, 51176, 646, 38377, 570, 437, 820, 264, 23897, 1320, 312, 510, 294, 341, 1389, 321, 362, 264, 3565, 270, 40863, 51440, 51440, 597, 307, 8858, 538, 7634, 293, 294, 1184, 5386, 321, 915, 264, 6674, 2158, 293, 550, 300, 2158, 2170, 41514, 292, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.1472243915904652, "compression_ratio": 1.948, "no_speech_prob": 1.9142235885283299e-07}, {"id": 398, "seek": 233304, "start": 2344.88, "end": 2349.2799999999997, "text": " and we have the indices at which they occurred and this will of course help us to help us do the", "tokens": [50364, 281, 1207, 264, 6674, 2158, 586, 294, 264, 2128, 1320, 321, 787, 1143, 4190, 570, 300, 311, 439, 321, 2978, 50660, 50660, 457, 294, 264, 23897, 1320, 309, 311, 4664, 4420, 281, 458, 466, 689, 729, 6674, 4190, 11068, 50924, 50956, 293, 321, 362, 264, 43840, 412, 597, 436, 11068, 293, 341, 486, 295, 1164, 854, 505, 281, 854, 505, 360, 264, 51176, 51176, 646, 38377, 570, 437, 820, 264, 23897, 1320, 312, 510, 294, 341, 1389, 321, 362, 264, 3565, 270, 40863, 51440, 51440, 597, 307, 8858, 538, 7634, 293, 294, 1184, 5386, 321, 915, 264, 6674, 2158, 293, 550, 300, 2158, 2170, 41514, 292, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.1472243915904652, "compression_ratio": 1.948, "no_speech_prob": 1.9142235885283299e-07}, {"id": 399, "seek": 233304, "start": 2349.2799999999997, "end": 2354.56, "text": " back propagation because what should the backward pass be here in this case we have the logit tensor", "tokens": [50364, 281, 1207, 264, 6674, 2158, 586, 294, 264, 2128, 1320, 321, 787, 1143, 4190, 570, 300, 311, 439, 321, 2978, 50660, 50660, 457, 294, 264, 23897, 1320, 309, 311, 4664, 4420, 281, 458, 466, 689, 729, 6674, 4190, 11068, 50924, 50956, 293, 321, 362, 264, 43840, 412, 597, 436, 11068, 293, 341, 486, 295, 1164, 854, 505, 281, 854, 505, 360, 264, 51176, 51176, 646, 38377, 570, 437, 820, 264, 23897, 1320, 312, 510, 294, 341, 1389, 321, 362, 264, 3565, 270, 40863, 51440, 51440, 597, 307, 8858, 538, 7634, 293, 294, 1184, 5386, 321, 915, 264, 6674, 2158, 293, 550, 300, 2158, 2170, 41514, 292, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.1472243915904652, "compression_ratio": 1.948, "no_speech_prob": 1.9142235885283299e-07}, {"id": 400, "seek": 233304, "start": 2354.56, "end": 2359.44, "text": " which is 32 by 27 and in each row we find the maximum value and then that value gets plucked", "tokens": [50364, 281, 1207, 264, 6674, 2158, 586, 294, 264, 2128, 1320, 321, 787, 1143, 4190, 570, 300, 311, 439, 321, 2978, 50660, 50660, 457, 294, 264, 23897, 1320, 309, 311, 4664, 4420, 281, 458, 466, 689, 729, 6674, 4190, 11068, 50924, 50956, 293, 321, 362, 264, 43840, 412, 597, 436, 11068, 293, 341, 486, 295, 1164, 854, 505, 281, 854, 505, 360, 264, 51176, 51176, 646, 38377, 570, 437, 820, 264, 23897, 1320, 312, 510, 294, 341, 1389, 321, 362, 264, 3565, 270, 40863, 51440, 51440, 597, 307, 8858, 538, 7634, 293, 294, 1184, 5386, 321, 915, 264, 6674, 2158, 293, 550, 300, 2158, 2170, 41514, 292, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.1472243915904652, "compression_ratio": 1.948, "no_speech_prob": 1.9142235885283299e-07}, {"id": 401, "seek": 235944, "start": 2359.44, "end": 2368.08, "text": " out into logit maxis and so intuitively basically the derivative flowing through here then", "tokens": [50364, 484, 666, 3565, 270, 11469, 271, 293, 370, 46506, 1936, 264, 13760, 13974, 807, 510, 550, 50796, 50796, 820, 312, 472, 1413, 264, 2654, 33733, 307, 472, 337, 264, 6854, 8729, 300, 390, 41514, 292, 484, 51136, 51172, 293, 550, 1413, 264, 4338, 13760, 295, 264, 3565, 270, 11469, 271, 370, 534, 437, 321, 434, 884, 510, 498, 291, 519, 51492, 51492, 807, 309, 307, 321, 643, 281, 747, 264, 1103, 664, 270, 11469, 271, 293, 321, 643, 281, 34951, 309, 281, 264, 3006, 8432, 51772, 51828], "temperature": 0.0, "avg_logprob": -0.19081705071952906, "compression_ratio": 1.8737864077669903, "no_speech_prob": 2.1907442260271637e-06}, {"id": 402, "seek": 235944, "start": 2368.08, "end": 2374.88, "text": " should be one times the local derivatives is one for the appropriate entry that was plucked out", "tokens": [50364, 484, 666, 3565, 270, 11469, 271, 293, 370, 46506, 1936, 264, 13760, 13974, 807, 510, 550, 50796, 50796, 820, 312, 472, 1413, 264, 2654, 33733, 307, 472, 337, 264, 6854, 8729, 300, 390, 41514, 292, 484, 51136, 51172, 293, 550, 1413, 264, 4338, 13760, 295, 264, 3565, 270, 11469, 271, 370, 534, 437, 321, 434, 884, 510, 498, 291, 519, 51492, 51492, 807, 309, 307, 321, 643, 281, 747, 264, 1103, 664, 270, 11469, 271, 293, 321, 643, 281, 34951, 309, 281, 264, 3006, 8432, 51772, 51828], "temperature": 0.0, "avg_logprob": -0.19081705071952906, "compression_ratio": 1.8737864077669903, "no_speech_prob": 2.1907442260271637e-06}, {"id": 403, "seek": 235944, "start": 2375.6, "end": 2382.0, "text": " and then times the global derivative of the logit maxis so really what we're doing here if you think", "tokens": [50364, 484, 666, 3565, 270, 11469, 271, 293, 370, 46506, 1936, 264, 13760, 13974, 807, 510, 550, 50796, 50796, 820, 312, 472, 1413, 264, 2654, 33733, 307, 472, 337, 264, 6854, 8729, 300, 390, 41514, 292, 484, 51136, 51172, 293, 550, 1413, 264, 4338, 13760, 295, 264, 3565, 270, 11469, 271, 370, 534, 437, 321, 434, 884, 510, 498, 291, 519, 51492, 51492, 807, 309, 307, 321, 643, 281, 747, 264, 1103, 664, 270, 11469, 271, 293, 321, 643, 281, 34951, 309, 281, 264, 3006, 8432, 51772, 51828], "temperature": 0.0, "avg_logprob": -0.19081705071952906, "compression_ratio": 1.8737864077669903, "no_speech_prob": 2.1907442260271637e-06}, {"id": 404, "seek": 235944, "start": 2382.0, "end": 2387.6, "text": " through it is we need to take the delogit maxis and we need to scatter it to the correct positions", "tokens": [50364, 484, 666, 3565, 270, 11469, 271, 293, 370, 46506, 1936, 264, 13760, 13974, 807, 510, 550, 50796, 50796, 820, 312, 472, 1413, 264, 2654, 33733, 307, 472, 337, 264, 6854, 8729, 300, 390, 41514, 292, 484, 51136, 51172, 293, 550, 1413, 264, 4338, 13760, 295, 264, 3565, 270, 11469, 271, 370, 534, 437, 321, 434, 884, 510, 498, 291, 519, 51492, 51492, 807, 309, 307, 321, 643, 281, 747, 264, 1103, 664, 270, 11469, 271, 293, 321, 643, 281, 34951, 309, 281, 264, 3006, 8432, 51772, 51828], "temperature": 0.0, "avg_logprob": -0.19081705071952906, "compression_ratio": 1.8737864077669903, "no_speech_prob": 2.1907442260271637e-06}, {"id": 405, "seek": 238760, "start": 2387.6, "end": 2392.88, "text": " in these logits from where the maximum values came and so", "tokens": [50364, 294, 613, 3565, 1208, 490, 689, 264, 6674, 4190, 1361, 293, 370, 50628, 50704, 286, 1361, 493, 365, 472, 1622, 295, 3089, 1333, 295, 300, 775, 300, 718, 385, 445, 23525, 257, 3840, 295, 1507, 510, 50916, 50916, 370, 264, 1622, 295, 291, 727, 360, 309, 733, 295, 588, 2531, 281, 437, 321, 1096, 510, 689, 321, 1884, 51116, 51116, 257, 35193, 293, 550, 321, 1665, 5256, 264, 3006, 4959, 370, 321, 764, 264, 43840, 510, 293, 321, 576, 992, 552, 51456, 51456], "temperature": 0.0, "avg_logprob": -0.1710827995749081, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.393051201783237e-06}, {"id": 406, "seek": 238760, "start": 2394.4, "end": 2398.64, "text": " I came up with one line of code sort of that does that let me just erase a bunch of stuff here", "tokens": [50364, 294, 613, 3565, 1208, 490, 689, 264, 6674, 4190, 1361, 293, 370, 50628, 50704, 286, 1361, 493, 365, 472, 1622, 295, 3089, 1333, 295, 300, 775, 300, 718, 385, 445, 23525, 257, 3840, 295, 1507, 510, 50916, 50916, 370, 264, 1622, 295, 291, 727, 360, 309, 733, 295, 588, 2531, 281, 437, 321, 1096, 510, 689, 321, 1884, 51116, 51116, 257, 35193, 293, 550, 321, 1665, 5256, 264, 3006, 4959, 370, 321, 764, 264, 43840, 510, 293, 321, 576, 992, 552, 51456, 51456], "temperature": 0.0, "avg_logprob": -0.1710827995749081, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.393051201783237e-06}, {"id": 407, "seek": 238760, "start": 2398.64, "end": 2402.64, "text": " so the line of you could do it kind of very similar to what we done here where we create", "tokens": [50364, 294, 613, 3565, 1208, 490, 689, 264, 6674, 4190, 1361, 293, 370, 50628, 50704, 286, 1361, 493, 365, 472, 1622, 295, 3089, 1333, 295, 300, 775, 300, 718, 385, 445, 23525, 257, 3840, 295, 1507, 510, 50916, 50916, 370, 264, 1622, 295, 291, 727, 360, 309, 733, 295, 588, 2531, 281, 437, 321, 1096, 510, 689, 321, 1884, 51116, 51116, 257, 35193, 293, 550, 321, 1665, 5256, 264, 3006, 4959, 370, 321, 764, 264, 43840, 510, 293, 321, 576, 992, 552, 51456, 51456], "temperature": 0.0, "avg_logprob": -0.1710827995749081, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.393051201783237e-06}, {"id": 408, "seek": 238760, "start": 2402.64, "end": 2409.44, "text": " a zeros and then we populate the correct elements so we use the indices here and we would set them", "tokens": [50364, 294, 613, 3565, 1208, 490, 689, 264, 6674, 4190, 1361, 293, 370, 50628, 50704, 286, 1361, 493, 365, 472, 1622, 295, 3089, 1333, 295, 300, 775, 300, 718, 385, 445, 23525, 257, 3840, 295, 1507, 510, 50916, 50916, 370, 264, 1622, 295, 291, 727, 360, 309, 733, 295, 588, 2531, 281, 437, 321, 1096, 510, 689, 321, 1884, 51116, 51116, 257, 35193, 293, 550, 321, 1665, 5256, 264, 3006, 4959, 370, 321, 764, 264, 43840, 510, 293, 321, 576, 992, 552, 51456, 51456], "temperature": 0.0, "avg_logprob": -0.1710827995749081, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.393051201783237e-06}, {"id": 409, "seek": 240944, "start": 2409.44, "end": 2418.08, "text": " to be one but you can also use one hot so f dot one hot and then I'm taking the logit max over", "tokens": [50364, 281, 312, 472, 457, 291, 393, 611, 764, 472, 2368, 370, 283, 5893, 472, 2368, 293, 550, 286, 478, 1940, 264, 3565, 270, 11469, 670, 50796, 50796, 264, 700, 10139, 5893, 43840, 293, 286, 478, 3585, 9953, 51, 284, 339, 300, 264, 10139, 295, 633, 472, 295, 613, 51160, 51160, 10688, 830, 820, 312, 1105, 7634, 293, 370, 437, 341, 307, 516, 281, 360, 307, 1392, 286, 12328, 341, 307, 3219, 51612, 51644], "temperature": 0.0, "avg_logprob": -0.18499006907145182, "compression_ratio": 1.5875706214689265, "no_speech_prob": 4.637752681446727e-06}, {"id": 410, "seek": 240944, "start": 2418.08, "end": 2425.36, "text": " the first dimension dot indices and I'm telling PyTorch that the dimension of every one of these", "tokens": [50364, 281, 312, 472, 457, 291, 393, 611, 764, 472, 2368, 370, 283, 5893, 472, 2368, 293, 550, 286, 478, 1940, 264, 3565, 270, 11469, 670, 50796, 50796, 264, 700, 10139, 5893, 43840, 293, 286, 478, 3585, 9953, 51, 284, 339, 300, 264, 10139, 295, 633, 472, 295, 613, 51160, 51160, 10688, 830, 820, 312, 1105, 7634, 293, 370, 437, 341, 307, 516, 281, 360, 307, 1392, 286, 12328, 341, 307, 3219, 51612, 51644], "temperature": 0.0, "avg_logprob": -0.18499006907145182, "compression_ratio": 1.5875706214689265, "no_speech_prob": 4.637752681446727e-06}, {"id": 411, "seek": 240944, "start": 2425.36, "end": 2434.4, "text": " tensors should be um 27 and so what this is going to do is okay I apologize this is crazy", "tokens": [50364, 281, 312, 472, 457, 291, 393, 611, 764, 472, 2368, 370, 283, 5893, 472, 2368, 293, 550, 286, 478, 1940, 264, 3565, 270, 11469, 670, 50796, 50796, 264, 700, 10139, 5893, 43840, 293, 286, 478, 3585, 9953, 51, 284, 339, 300, 264, 10139, 295, 633, 472, 295, 613, 51160, 51160, 10688, 830, 820, 312, 1105, 7634, 293, 370, 437, 341, 307, 516, 281, 360, 307, 1392, 286, 12328, 341, 307, 3219, 51612, 51644], "temperature": 0.0, "avg_logprob": -0.18499006907145182, "compression_ratio": 1.5875706214689265, "no_speech_prob": 4.637752681446727e-06}, {"id": 412, "seek": 243440, "start": 2434.4, "end": 2441.76, "text": " PLT.im show of this it's really just an array of where the maxis came from in each row and that", "tokens": [50364, 6999, 51, 13, 332, 855, 295, 341, 309, 311, 534, 445, 364, 10225, 295, 689, 264, 11469, 271, 1361, 490, 294, 1184, 5386, 293, 300, 50732, 50732, 4478, 307, 472, 293, 264, 439, 264, 661, 4959, 366, 4018, 370, 309, 311, 257, 472, 2368, 8062, 294, 1184, 5386, 50972, 50972, 293, 613, 43840, 366, 586, 1665, 12162, 257, 2167, 472, 294, 264, 2296, 1081, 293, 550, 437, 286, 478, 884, 510, 51280, 51280, 307, 286, 478, 30955, 538, 264, 3565, 270, 11469, 271, 293, 1066, 294, 1575, 300, 341, 307, 257, 7738, 295, 8858, 538, 472, 293, 370, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1560970287696988, "compression_ratio": 1.7098214285714286, "no_speech_prob": 1.873843416433374e-06}, {"id": 413, "seek": 243440, "start": 2441.76, "end": 2446.56, "text": " element is one and the all the other elements are zero so it's a one hot vector in each row", "tokens": [50364, 6999, 51, 13, 332, 855, 295, 341, 309, 311, 534, 445, 364, 10225, 295, 689, 264, 11469, 271, 1361, 490, 294, 1184, 5386, 293, 300, 50732, 50732, 4478, 307, 472, 293, 264, 439, 264, 661, 4959, 366, 4018, 370, 309, 311, 257, 472, 2368, 8062, 294, 1184, 5386, 50972, 50972, 293, 613, 43840, 366, 586, 1665, 12162, 257, 2167, 472, 294, 264, 2296, 1081, 293, 550, 437, 286, 478, 884, 510, 51280, 51280, 307, 286, 478, 30955, 538, 264, 3565, 270, 11469, 271, 293, 1066, 294, 1575, 300, 341, 307, 257, 7738, 295, 8858, 538, 472, 293, 370, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1560970287696988, "compression_ratio": 1.7098214285714286, "no_speech_prob": 1.873843416433374e-06}, {"id": 414, "seek": 243440, "start": 2446.56, "end": 2452.7200000000003, "text": " and these indices are now populating a single one in the proper place and then what I'm doing here", "tokens": [50364, 6999, 51, 13, 332, 855, 295, 341, 309, 311, 534, 445, 364, 10225, 295, 689, 264, 11469, 271, 1361, 490, 294, 1184, 5386, 293, 300, 50732, 50732, 4478, 307, 472, 293, 264, 439, 264, 661, 4959, 366, 4018, 370, 309, 311, 257, 472, 2368, 8062, 294, 1184, 5386, 50972, 50972, 293, 613, 43840, 366, 586, 1665, 12162, 257, 2167, 472, 294, 264, 2296, 1081, 293, 550, 437, 286, 478, 884, 510, 51280, 51280, 307, 286, 478, 30955, 538, 264, 3565, 270, 11469, 271, 293, 1066, 294, 1575, 300, 341, 307, 257, 7738, 295, 8858, 538, 472, 293, 370, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1560970287696988, "compression_ratio": 1.7098214285714286, "no_speech_prob": 1.873843416433374e-06}, {"id": 415, "seek": 243440, "start": 2452.7200000000003, "end": 2460.7200000000003, "text": " is I'm multiplying by the logit maxis and keep in mind that this is a column of 32 by one and so", "tokens": [50364, 6999, 51, 13, 332, 855, 295, 341, 309, 311, 534, 445, 364, 10225, 295, 689, 264, 11469, 271, 1361, 490, 294, 1184, 5386, 293, 300, 50732, 50732, 4478, 307, 472, 293, 264, 439, 264, 661, 4959, 366, 4018, 370, 309, 311, 257, 472, 2368, 8062, 294, 1184, 5386, 50972, 50972, 293, 613, 43840, 366, 586, 1665, 12162, 257, 2167, 472, 294, 264, 2296, 1081, 293, 550, 437, 286, 478, 884, 510, 51280, 51280, 307, 286, 478, 30955, 538, 264, 3565, 270, 11469, 271, 293, 1066, 294, 1575, 300, 341, 307, 257, 7738, 295, 8858, 538, 472, 293, 370, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.1560970287696988, "compression_ratio": 1.7098214285714286, "no_speech_prob": 1.873843416433374e-06}, {"id": 416, "seek": 246072, "start": 2460.72, "end": 2467.68, "text": " when I'm doing this times the logit maxis the logit maxis will broadcast and that column will", "tokens": [50364, 562, 286, 478, 884, 341, 1413, 264, 3565, 270, 11469, 271, 264, 3565, 270, 11469, 271, 486, 9975, 293, 300, 7738, 486, 50712, 50712, 291, 458, 483, 46365, 293, 550, 4478, 10829, 12972, 486, 5586, 300, 1184, 295, 613, 445, 2170, 50960, 50960, 4020, 292, 281, 24123, 472, 295, 613, 9239, 307, 3574, 322, 293, 370, 300, 311, 1071, 636, 281, 4445, 341, 51284, 51284, 733, 295, 364, 341, 733, 295, 257, 6916, 293, 1293, 295, 613, 393, 312, 1143, 286, 445, 1194, 286, 576, 855, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.1422345373365614, "compression_ratio": 1.7830188679245282, "no_speech_prob": 3.2886548524402315e-06}, {"id": 417, "seek": 246072, "start": 2467.68, "end": 2472.64, "text": " you know get replicated and then element wise multiply will ensure that each of these just gets", "tokens": [50364, 562, 286, 478, 884, 341, 1413, 264, 3565, 270, 11469, 271, 264, 3565, 270, 11469, 271, 486, 9975, 293, 300, 7738, 486, 50712, 50712, 291, 458, 483, 46365, 293, 550, 4478, 10829, 12972, 486, 5586, 300, 1184, 295, 613, 445, 2170, 50960, 50960, 4020, 292, 281, 24123, 472, 295, 613, 9239, 307, 3574, 322, 293, 370, 300, 311, 1071, 636, 281, 4445, 341, 51284, 51284, 733, 295, 364, 341, 733, 295, 257, 6916, 293, 1293, 295, 613, 393, 312, 1143, 286, 445, 1194, 286, 576, 855, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.1422345373365614, "compression_ratio": 1.7830188679245282, "no_speech_prob": 3.2886548524402315e-06}, {"id": 418, "seek": 246072, "start": 2472.64, "end": 2479.12, "text": " routed to whichever one of these bits is turned on and so that's another way to implement this", "tokens": [50364, 562, 286, 478, 884, 341, 1413, 264, 3565, 270, 11469, 271, 264, 3565, 270, 11469, 271, 486, 9975, 293, 300, 7738, 486, 50712, 50712, 291, 458, 483, 46365, 293, 550, 4478, 10829, 12972, 486, 5586, 300, 1184, 295, 613, 445, 2170, 50960, 50960, 4020, 292, 281, 24123, 472, 295, 613, 9239, 307, 3574, 322, 293, 370, 300, 311, 1071, 636, 281, 4445, 341, 51284, 51284, 733, 295, 364, 341, 733, 295, 257, 6916, 293, 1293, 295, 613, 393, 312, 1143, 286, 445, 1194, 286, 576, 855, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.1422345373365614, "compression_ratio": 1.7830188679245282, "no_speech_prob": 3.2886548524402315e-06}, {"id": 419, "seek": 246072, "start": 2479.12, "end": 2485.04, "text": " kind of an this kind of a operation and both of these can be used I just thought I would show", "tokens": [50364, 562, 286, 478, 884, 341, 1413, 264, 3565, 270, 11469, 271, 264, 3565, 270, 11469, 271, 486, 9975, 293, 300, 7738, 486, 50712, 50712, 291, 458, 483, 46365, 293, 550, 4478, 10829, 12972, 486, 5586, 300, 1184, 295, 613, 445, 2170, 50960, 50960, 4020, 292, 281, 24123, 472, 295, 613, 9239, 307, 3574, 322, 293, 370, 300, 311, 1071, 636, 281, 4445, 341, 51284, 51284, 733, 295, 364, 341, 733, 295, 257, 6916, 293, 1293, 295, 613, 393, 312, 1143, 286, 445, 1194, 286, 576, 855, 51580, 51580], "temperature": 0.0, "avg_logprob": -0.1422345373365614, "compression_ratio": 1.7830188679245282, "no_speech_prob": 3.2886548524402315e-06}, {"id": 420, "seek": 248504, "start": 2485.04, "end": 2490.96, "text": " an equivalent way to do it and I'm using plus equals because we already calculated the logits here", "tokens": [50364, 364, 10344, 636, 281, 360, 309, 293, 286, 478, 1228, 1804, 6915, 570, 321, 1217, 15598, 264, 3565, 1208, 510, 50660, 50660, 293, 341, 307, 586, 264, 1150, 9819, 370, 718, 311, 574, 412, 3565, 1208, 293, 652, 988, 300, 341, 307, 3006, 50984, 51020, 293, 321, 536, 300, 321, 362, 2293, 264, 3006, 1867, 958, 493, 321, 528, 281, 2354, 365, 3565, 1208, 510, 51364, 51364, 300, 307, 364, 9700, 295, 257, 8141, 27290, 293, 257, 12577, 18687, 294, 341, 8213, 4583, 370, 286, 600, 51692, 51744], "temperature": 0.0, "avg_logprob": -0.13391026390923394, "compression_ratio": 1.6888888888888889, "no_speech_prob": 1.2288904827073566e-06}, {"id": 421, "seek": 248504, "start": 2490.96, "end": 2497.44, "text": " and this is now the second branch so let's look at logits and make sure that this is correct", "tokens": [50364, 364, 10344, 636, 281, 360, 309, 293, 286, 478, 1228, 1804, 6915, 570, 321, 1217, 15598, 264, 3565, 1208, 510, 50660, 50660, 293, 341, 307, 586, 264, 1150, 9819, 370, 718, 311, 574, 412, 3565, 1208, 293, 652, 988, 300, 341, 307, 3006, 50984, 51020, 293, 321, 536, 300, 321, 362, 2293, 264, 3006, 1867, 958, 493, 321, 528, 281, 2354, 365, 3565, 1208, 510, 51364, 51364, 300, 307, 364, 9700, 295, 257, 8141, 27290, 293, 257, 12577, 18687, 294, 341, 8213, 4583, 370, 286, 600, 51692, 51744], "temperature": 0.0, "avg_logprob": -0.13391026390923394, "compression_ratio": 1.6888888888888889, "no_speech_prob": 1.2288904827073566e-06}, {"id": 422, "seek": 248504, "start": 2498.16, "end": 2505.04, "text": " and we see that we have exactly the correct answer next up we want to continue with logits here", "tokens": [50364, 364, 10344, 636, 281, 360, 309, 293, 286, 478, 1228, 1804, 6915, 570, 321, 1217, 15598, 264, 3565, 1208, 510, 50660, 50660, 293, 341, 307, 586, 264, 1150, 9819, 370, 718, 311, 574, 412, 3565, 1208, 293, 652, 988, 300, 341, 307, 3006, 50984, 51020, 293, 321, 536, 300, 321, 362, 2293, 264, 3006, 1867, 958, 493, 321, 528, 281, 2354, 365, 3565, 1208, 510, 51364, 51364, 300, 307, 364, 9700, 295, 257, 8141, 27290, 293, 257, 12577, 18687, 294, 341, 8213, 4583, 370, 286, 600, 51692, 51744], "temperature": 0.0, "avg_logprob": -0.13391026390923394, "compression_ratio": 1.6888888888888889, "no_speech_prob": 1.2288904827073566e-06}, {"id": 423, "seek": 248504, "start": 2505.04, "end": 2511.6, "text": " that is an outcome of a matrix multiplication and a bias offset in this linear layer so I've", "tokens": [50364, 364, 10344, 636, 281, 360, 309, 293, 286, 478, 1228, 1804, 6915, 570, 321, 1217, 15598, 264, 3565, 1208, 510, 50660, 50660, 293, 341, 307, 586, 264, 1150, 9819, 370, 718, 311, 574, 412, 3565, 1208, 293, 652, 988, 300, 341, 307, 3006, 50984, 51020, 293, 321, 536, 300, 321, 362, 2293, 264, 3006, 1867, 958, 493, 321, 528, 281, 2354, 365, 3565, 1208, 510, 51364, 51364, 300, 307, 364, 9700, 295, 257, 8141, 27290, 293, 257, 12577, 18687, 294, 341, 8213, 4583, 370, 286, 600, 51692, 51744], "temperature": 0.0, "avg_logprob": -0.13391026390923394, "compression_ratio": 1.6888888888888889, "no_speech_prob": 1.2288904827073566e-06}, {"id": 424, "seek": 251160, "start": 2511.6, "end": 2517.04, "text": " printed out the shapes of all these intermediate tensors we see that logits is of course 32 by 27", "tokens": [50364, 13567, 484, 264, 10854, 295, 439, 613, 19376, 10688, 830, 321, 536, 300, 3565, 1208, 307, 295, 1164, 8858, 538, 7634, 50636, 50636, 382, 321, 600, 445, 1612, 550, 264, 276, 510, 307, 8858, 538, 12145, 370, 613, 366, 12145, 18795, 7633, 4368, 50976, 51000, 293, 550, 341, 261, 8141, 4455, 729, 12145, 18795, 18875, 666, 7634, 12819, 51260, 51260, 293, 550, 456, 311, 257, 7634, 18795, 18687, 597, 307, 257, 472, 12, 18759, 8062, 586, 321, 820, 3637, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.11598373321165521, "compression_ratio": 1.8, "no_speech_prob": 1.4823200444880058e-06}, {"id": 425, "seek": 251160, "start": 2517.04, "end": 2523.8399999999997, "text": " as we've just seen then the h here is 32 by 64 so these are 64 dimensional hidden states", "tokens": [50364, 13567, 484, 264, 10854, 295, 439, 613, 19376, 10688, 830, 321, 536, 300, 3565, 1208, 307, 295, 1164, 8858, 538, 7634, 50636, 50636, 382, 321, 600, 445, 1612, 550, 264, 276, 510, 307, 8858, 538, 12145, 370, 613, 366, 12145, 18795, 7633, 4368, 50976, 51000, 293, 550, 341, 261, 8141, 4455, 729, 12145, 18795, 18875, 666, 7634, 12819, 51260, 51260, 293, 550, 456, 311, 257, 7634, 18795, 18687, 597, 307, 257, 472, 12, 18759, 8062, 586, 321, 820, 3637, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.11598373321165521, "compression_ratio": 1.8, "no_speech_prob": 1.4823200444880058e-06}, {"id": 426, "seek": 251160, "start": 2524.3199999999997, "end": 2529.52, "text": " and then this w matrix projects those 64 dimensional vectors into 27 dimensions", "tokens": [50364, 13567, 484, 264, 10854, 295, 439, 613, 19376, 10688, 830, 321, 536, 300, 3565, 1208, 307, 295, 1164, 8858, 538, 7634, 50636, 50636, 382, 321, 600, 445, 1612, 550, 264, 276, 510, 307, 8858, 538, 12145, 370, 613, 366, 12145, 18795, 7633, 4368, 50976, 51000, 293, 550, 341, 261, 8141, 4455, 729, 12145, 18795, 18875, 666, 7634, 12819, 51260, 51260, 293, 550, 456, 311, 257, 7634, 18795, 18687, 597, 307, 257, 472, 12, 18759, 8062, 586, 321, 820, 3637, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.11598373321165521, "compression_ratio": 1.8, "no_speech_prob": 1.4823200444880058e-06}, {"id": 427, "seek": 251160, "start": 2529.52, "end": 2535.8399999999997, "text": " and then there's a 27 dimensional offset which is a one-dimensional vector now we should note", "tokens": [50364, 13567, 484, 264, 10854, 295, 439, 613, 19376, 10688, 830, 321, 536, 300, 3565, 1208, 307, 295, 1164, 8858, 538, 7634, 50636, 50636, 382, 321, 600, 445, 1612, 550, 264, 276, 510, 307, 8858, 538, 12145, 370, 613, 366, 12145, 18795, 7633, 4368, 50976, 51000, 293, 550, 341, 261, 8141, 4455, 729, 12145, 18795, 18875, 666, 7634, 12819, 51260, 51260, 293, 550, 456, 311, 257, 7634, 18795, 18687, 597, 307, 257, 472, 12, 18759, 8062, 586, 321, 820, 3637, 51576, 51576], "temperature": 0.0, "avg_logprob": -0.11598373321165521, "compression_ratio": 1.8, "no_speech_prob": 1.4823200444880058e-06}, {"id": 428, "seek": 253584, "start": 2535.84, "end": 2542.96, "text": " that this plus here actually broadcasts because h multiplied by w2 will give us a 32 by 27", "tokens": [50364, 300, 341, 1804, 510, 767, 9975, 82, 570, 276, 17207, 538, 261, 17, 486, 976, 505, 257, 8858, 538, 7634, 50720, 50760, 293, 370, 550, 341, 1804, 272, 17, 307, 257, 7634, 18795, 8062, 510, 586, 294, 264, 4474, 295, 30024, 51076, 51076, 437, 311, 516, 281, 1051, 365, 341, 12577, 8062, 307, 300, 341, 472, 18795, 8062, 7634, 486, 483, 51340, 51340, 17962, 365, 364, 6887, 9207, 10139, 295, 472, 322, 264, 1411, 293, 309, 486, 1936, 1813, 257, 5386, 8062, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.12926483154296875, "compression_ratio": 1.7815533980582525, "no_speech_prob": 1.7330172568108537e-06}, {"id": 429, "seek": 253584, "start": 2543.76, "end": 2550.08, "text": " and so then this plus b2 is a 27 dimensional vector here now in the rules of broadcasting", "tokens": [50364, 300, 341, 1804, 510, 767, 9975, 82, 570, 276, 17207, 538, 261, 17, 486, 976, 505, 257, 8858, 538, 7634, 50720, 50760, 293, 370, 550, 341, 1804, 272, 17, 307, 257, 7634, 18795, 8062, 510, 586, 294, 264, 4474, 295, 30024, 51076, 51076, 437, 311, 516, 281, 1051, 365, 341, 12577, 8062, 307, 300, 341, 472, 18795, 8062, 7634, 486, 483, 51340, 51340, 17962, 365, 364, 6887, 9207, 10139, 295, 472, 322, 264, 1411, 293, 309, 486, 1936, 1813, 257, 5386, 8062, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.12926483154296875, "compression_ratio": 1.7815533980582525, "no_speech_prob": 1.7330172568108537e-06}, {"id": 430, "seek": 253584, "start": 2550.08, "end": 2555.36, "text": " what's going to happen with this bias vector is that this one dimensional vector 27 will get", "tokens": [50364, 300, 341, 1804, 510, 767, 9975, 82, 570, 276, 17207, 538, 261, 17, 486, 976, 505, 257, 8858, 538, 7634, 50720, 50760, 293, 370, 550, 341, 1804, 272, 17, 307, 257, 7634, 18795, 8062, 510, 586, 294, 264, 4474, 295, 30024, 51076, 51076, 437, 311, 516, 281, 1051, 365, 341, 12577, 8062, 307, 300, 341, 472, 18795, 8062, 7634, 486, 483, 51340, 51340, 17962, 365, 364, 6887, 9207, 10139, 295, 472, 322, 264, 1411, 293, 309, 486, 1936, 1813, 257, 5386, 8062, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.12926483154296875, "compression_ratio": 1.7815533980582525, "no_speech_prob": 1.7330172568108537e-06}, {"id": 431, "seek": 253584, "start": 2555.36, "end": 2561.44, "text": " aligned with an padded dimension of one on the left and it will basically become a row vector", "tokens": [50364, 300, 341, 1804, 510, 767, 9975, 82, 570, 276, 17207, 538, 261, 17, 486, 976, 505, 257, 8858, 538, 7634, 50720, 50760, 293, 370, 550, 341, 1804, 272, 17, 307, 257, 7634, 18795, 8062, 510, 586, 294, 264, 4474, 295, 30024, 51076, 51076, 437, 311, 516, 281, 1051, 365, 341, 12577, 8062, 307, 300, 341, 472, 18795, 8062, 7634, 486, 483, 51340, 51340, 17962, 365, 364, 6887, 9207, 10139, 295, 472, 322, 264, 1411, 293, 309, 486, 1936, 1813, 257, 5386, 8062, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.12926483154296875, "compression_ratio": 1.7815533980582525, "no_speech_prob": 1.7330172568108537e-06}, {"id": 432, "seek": 256144, "start": 2561.44, "end": 2567.12, "text": " and then it will get replicated vertically 32 times to make it 32 by 27 and then there's an", "tokens": [50364, 293, 550, 309, 486, 483, 46365, 28450, 8858, 1413, 281, 652, 309, 8858, 538, 7634, 293, 550, 456, 311, 364, 50648, 50648, 4478, 6091, 12972, 586, 264, 1168, 307, 577, 360, 321, 646, 48256, 490, 3565, 1208, 281, 264, 7633, 4368, 51064, 51064, 264, 3364, 8141, 261, 17, 293, 264, 12577, 272, 17, 293, 291, 1062, 519, 300, 321, 643, 281, 352, 281, 512, 8141, 33400, 51420, 51476, 293, 550, 321, 362, 281, 574, 493, 264, 13760, 337, 257, 8141, 27290, 457, 767, 291, 393, 574, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.19465899997287325, "compression_ratio": 1.691304347826087, "no_speech_prob": 2.295834974574973e-06}, {"id": 433, "seek": 256144, "start": 2567.12, "end": 2575.44, "text": " element twice multiply now the question is how do we back propagate from logits to the hidden states", "tokens": [50364, 293, 550, 309, 486, 483, 46365, 28450, 8858, 1413, 281, 652, 309, 8858, 538, 7634, 293, 550, 456, 311, 364, 50648, 50648, 4478, 6091, 12972, 586, 264, 1168, 307, 577, 360, 321, 646, 48256, 490, 3565, 1208, 281, 264, 7633, 4368, 51064, 51064, 264, 3364, 8141, 261, 17, 293, 264, 12577, 272, 17, 293, 291, 1062, 519, 300, 321, 643, 281, 352, 281, 512, 8141, 33400, 51420, 51476, 293, 550, 321, 362, 281, 574, 493, 264, 13760, 337, 257, 8141, 27290, 457, 767, 291, 393, 574, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.19465899997287325, "compression_ratio": 1.691304347826087, "no_speech_prob": 2.295834974574973e-06}, {"id": 434, "seek": 256144, "start": 2575.44, "end": 2582.56, "text": " the weight matrix w2 and the bias b2 and you might think that we need to go to some matrix calculus", "tokens": [50364, 293, 550, 309, 486, 483, 46365, 28450, 8858, 1413, 281, 652, 309, 8858, 538, 7634, 293, 550, 456, 311, 364, 50648, 50648, 4478, 6091, 12972, 586, 264, 1168, 307, 577, 360, 321, 646, 48256, 490, 3565, 1208, 281, 264, 7633, 4368, 51064, 51064, 264, 3364, 8141, 261, 17, 293, 264, 12577, 272, 17, 293, 291, 1062, 519, 300, 321, 643, 281, 352, 281, 512, 8141, 33400, 51420, 51476, 293, 550, 321, 362, 281, 574, 493, 264, 13760, 337, 257, 8141, 27290, 457, 767, 291, 393, 574, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.19465899997287325, "compression_ratio": 1.691304347826087, "no_speech_prob": 2.295834974574973e-06}, {"id": 435, "seek": 256144, "start": 2583.68, "end": 2588.2400000000002, "text": " and then we have to look up the derivative for a matrix multiplication but actually you can look", "tokens": [50364, 293, 550, 309, 486, 483, 46365, 28450, 8858, 1413, 281, 652, 309, 8858, 538, 7634, 293, 550, 456, 311, 364, 50648, 50648, 4478, 6091, 12972, 586, 264, 1168, 307, 577, 360, 321, 646, 48256, 490, 3565, 1208, 281, 264, 7633, 4368, 51064, 51064, 264, 3364, 8141, 261, 17, 293, 264, 12577, 272, 17, 293, 291, 1062, 519, 300, 321, 643, 281, 352, 281, 512, 8141, 33400, 51420, 51476, 293, 550, 321, 362, 281, 574, 493, 264, 13760, 337, 257, 8141, 27290, 457, 767, 291, 393, 574, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.19465899997287325, "compression_ratio": 1.691304347826087, "no_speech_prob": 2.295834974574973e-06}, {"id": 436, "seek": 258824, "start": 2588.24, "end": 2592.3999999999996, "text": " the derivative for a matrix multiplication but actually you don't have to do any of that and", "tokens": [50364, 264, 13760, 337, 257, 8141, 27290, 457, 767, 291, 500, 380, 362, 281, 360, 604, 295, 300, 293, 50572, 50572, 291, 393, 352, 646, 281, 700, 9156, 293, 28446, 341, 1803, 322, 257, 2522, 295, 3035, 293, 4682, 50832, 50832, 437, 741, 411, 281, 360, 293, 741, 437, 741, 915, 1985, 731, 337, 385, 307, 291, 915, 257, 2685, 1359, 1365, 300, 51100, 51100, 291, 550, 4498, 2464, 484, 293, 550, 294, 264, 1399, 295, 23663, 577, 300, 2609, 1359, 1365, 51336, 51336, 1985, 291, 486, 1223, 264, 13227, 5102, 293, 291, 603, 312, 1075, 281, 2674, 1125, 293, 2464, 484, 264, 1577, 51560, 51600], "temperature": 0.0, "avg_logprob": -0.05416399519020152, "compression_ratio": 1.8403041825095057, "no_speech_prob": 1.406289084115997e-05}, {"id": 437, "seek": 258824, "start": 2592.3999999999996, "end": 2597.6, "text": " you can go back to first principles and derive this yourself on a piece of paper and specifically", "tokens": [50364, 264, 13760, 337, 257, 8141, 27290, 457, 767, 291, 500, 380, 362, 281, 360, 604, 295, 300, 293, 50572, 50572, 291, 393, 352, 646, 281, 700, 9156, 293, 28446, 341, 1803, 322, 257, 2522, 295, 3035, 293, 4682, 50832, 50832, 437, 741, 411, 281, 360, 293, 741, 437, 741, 915, 1985, 731, 337, 385, 307, 291, 915, 257, 2685, 1359, 1365, 300, 51100, 51100, 291, 550, 4498, 2464, 484, 293, 550, 294, 264, 1399, 295, 23663, 577, 300, 2609, 1359, 1365, 51336, 51336, 1985, 291, 486, 1223, 264, 13227, 5102, 293, 291, 603, 312, 1075, 281, 2674, 1125, 293, 2464, 484, 264, 1577, 51560, 51600], "temperature": 0.0, "avg_logprob": -0.05416399519020152, "compression_ratio": 1.8403041825095057, "no_speech_prob": 1.406289084115997e-05}, {"id": 438, "seek": 258824, "start": 2597.6, "end": 2602.9599999999996, "text": " what i like to do and i what i find works well for me is you find a specific small example that", "tokens": [50364, 264, 13760, 337, 257, 8141, 27290, 457, 767, 291, 500, 380, 362, 281, 360, 604, 295, 300, 293, 50572, 50572, 291, 393, 352, 646, 281, 700, 9156, 293, 28446, 341, 1803, 322, 257, 2522, 295, 3035, 293, 4682, 50832, 50832, 437, 741, 411, 281, 360, 293, 741, 437, 741, 915, 1985, 731, 337, 385, 307, 291, 915, 257, 2685, 1359, 1365, 300, 51100, 51100, 291, 550, 4498, 2464, 484, 293, 550, 294, 264, 1399, 295, 23663, 577, 300, 2609, 1359, 1365, 51336, 51336, 1985, 291, 486, 1223, 264, 13227, 5102, 293, 291, 603, 312, 1075, 281, 2674, 1125, 293, 2464, 484, 264, 1577, 51560, 51600], "temperature": 0.0, "avg_logprob": -0.05416399519020152, "compression_ratio": 1.8403041825095057, "no_speech_prob": 1.406289084115997e-05}, {"id": 439, "seek": 258824, "start": 2602.9599999999996, "end": 2607.68, "text": " you then fully write out and then in the process of analyzing how that individual small example", "tokens": [50364, 264, 13760, 337, 257, 8141, 27290, 457, 767, 291, 500, 380, 362, 281, 360, 604, 295, 300, 293, 50572, 50572, 291, 393, 352, 646, 281, 700, 9156, 293, 28446, 341, 1803, 322, 257, 2522, 295, 3035, 293, 4682, 50832, 50832, 437, 741, 411, 281, 360, 293, 741, 437, 741, 915, 1985, 731, 337, 385, 307, 291, 915, 257, 2685, 1359, 1365, 300, 51100, 51100, 291, 550, 4498, 2464, 484, 293, 550, 294, 264, 1399, 295, 23663, 577, 300, 2609, 1359, 1365, 51336, 51336, 1985, 291, 486, 1223, 264, 13227, 5102, 293, 291, 603, 312, 1075, 281, 2674, 1125, 293, 2464, 484, 264, 1577, 51560, 51600], "temperature": 0.0, "avg_logprob": -0.05416399519020152, "compression_ratio": 1.8403041825095057, "no_speech_prob": 1.406289084115997e-05}, {"id": 440, "seek": 258824, "start": 2607.68, "end": 2612.16, "text": " works you will understand the broader pattern and you'll be able to generalize and write out the full", "tokens": [50364, 264, 13760, 337, 257, 8141, 27290, 457, 767, 291, 500, 380, 362, 281, 360, 604, 295, 300, 293, 50572, 50572, 291, 393, 352, 646, 281, 700, 9156, 293, 28446, 341, 1803, 322, 257, 2522, 295, 3035, 293, 4682, 50832, 50832, 437, 741, 411, 281, 360, 293, 741, 437, 741, 915, 1985, 731, 337, 385, 307, 291, 915, 257, 2685, 1359, 1365, 300, 51100, 51100, 291, 550, 4498, 2464, 484, 293, 550, 294, 264, 1399, 295, 23663, 577, 300, 2609, 1359, 1365, 51336, 51336, 1985, 291, 486, 1223, 264, 13227, 5102, 293, 291, 603, 312, 1075, 281, 2674, 1125, 293, 2464, 484, 264, 1577, 51560, 51600], "temperature": 0.0, "avg_logprob": -0.05416399519020152, "compression_ratio": 1.8403041825095057, "no_speech_prob": 1.406289084115997e-05}, {"id": 441, "seek": 261216, "start": 2612.16, "end": 2618.56, "text": " uh general formula for how these derivatives flow in an expression like this so let's try that out", "tokens": [50364, 2232, 2674, 8513, 337, 577, 613, 33733, 3095, 294, 364, 6114, 411, 341, 370, 718, 311, 853, 300, 484, 50684, 50720, 370, 22440, 264, 2295, 4706, 4265, 510, 457, 437, 741, 600, 1096, 510, 307, 741, 478, 3579, 309, 484, 322, 257, 2522, 50952, 50952, 295, 3035, 534, 437, 321, 366, 3102, 294, 307, 321, 362, 257, 12972, 272, 1804, 269, 293, 300, 7829, 257, 274, 51280, 51320, 293, 321, 362, 264, 13760, 295, 264, 4470, 365, 3104, 281, 274, 293, 321, 1116, 411, 281, 458, 437, 264, 51500, 51500, 13760, 295, 264, 4470, 307, 365, 3104, 281, 257, 272, 293, 269, 586, 613, 510, 366, 707, 732, 12, 18759, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.06655374744482208, "compression_ratio": 1.8804780876494025, "no_speech_prob": 5.0144444685429335e-06}, {"id": 442, "seek": 261216, "start": 2619.2799999999997, "end": 2623.92, "text": " so pardon the low budget production here but what i've done here is i'm writing it out on a piece", "tokens": [50364, 2232, 2674, 8513, 337, 577, 613, 33733, 3095, 294, 364, 6114, 411, 341, 370, 718, 311, 853, 300, 484, 50684, 50720, 370, 22440, 264, 2295, 4706, 4265, 510, 457, 437, 741, 600, 1096, 510, 307, 741, 478, 3579, 309, 484, 322, 257, 2522, 50952, 50952, 295, 3035, 534, 437, 321, 366, 3102, 294, 307, 321, 362, 257, 12972, 272, 1804, 269, 293, 300, 7829, 257, 274, 51280, 51320, 293, 321, 362, 264, 13760, 295, 264, 4470, 365, 3104, 281, 274, 293, 321, 1116, 411, 281, 458, 437, 264, 51500, 51500, 13760, 295, 264, 4470, 307, 365, 3104, 281, 257, 272, 293, 269, 586, 613, 510, 366, 707, 732, 12, 18759, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.06655374744482208, "compression_ratio": 1.8804780876494025, "no_speech_prob": 5.0144444685429335e-06}, {"id": 443, "seek": 261216, "start": 2623.92, "end": 2630.48, "text": " of paper really what we are interested in is we have a multiply b plus c and that creates a d", "tokens": [50364, 2232, 2674, 8513, 337, 577, 613, 33733, 3095, 294, 364, 6114, 411, 341, 370, 718, 311, 853, 300, 484, 50684, 50720, 370, 22440, 264, 2295, 4706, 4265, 510, 457, 437, 741, 600, 1096, 510, 307, 741, 478, 3579, 309, 484, 322, 257, 2522, 50952, 50952, 295, 3035, 534, 437, 321, 366, 3102, 294, 307, 321, 362, 257, 12972, 272, 1804, 269, 293, 300, 7829, 257, 274, 51280, 51320, 293, 321, 362, 264, 13760, 295, 264, 4470, 365, 3104, 281, 274, 293, 321, 1116, 411, 281, 458, 437, 264, 51500, 51500, 13760, 295, 264, 4470, 307, 365, 3104, 281, 257, 272, 293, 269, 586, 613, 510, 366, 707, 732, 12, 18759, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.06655374744482208, "compression_ratio": 1.8804780876494025, "no_speech_prob": 5.0144444685429335e-06}, {"id": 444, "seek": 261216, "start": 2631.2799999999997, "end": 2634.8799999999997, "text": " and we have the derivative of the loss with respect to d and we'd like to know what the", "tokens": [50364, 2232, 2674, 8513, 337, 577, 613, 33733, 3095, 294, 364, 6114, 411, 341, 370, 718, 311, 853, 300, 484, 50684, 50720, 370, 22440, 264, 2295, 4706, 4265, 510, 457, 437, 741, 600, 1096, 510, 307, 741, 478, 3579, 309, 484, 322, 257, 2522, 50952, 50952, 295, 3035, 534, 437, 321, 366, 3102, 294, 307, 321, 362, 257, 12972, 272, 1804, 269, 293, 300, 7829, 257, 274, 51280, 51320, 293, 321, 362, 264, 13760, 295, 264, 4470, 365, 3104, 281, 274, 293, 321, 1116, 411, 281, 458, 437, 264, 51500, 51500, 13760, 295, 264, 4470, 307, 365, 3104, 281, 257, 272, 293, 269, 586, 613, 510, 366, 707, 732, 12, 18759, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.06655374744482208, "compression_ratio": 1.8804780876494025, "no_speech_prob": 5.0144444685429335e-06}, {"id": 445, "seek": 261216, "start": 2634.8799999999997, "end": 2640.3999999999996, "text": " derivative of the loss is with respect to a b and c now these here are little two-dimensional", "tokens": [50364, 2232, 2674, 8513, 337, 577, 613, 33733, 3095, 294, 364, 6114, 411, 341, 370, 718, 311, 853, 300, 484, 50684, 50720, 370, 22440, 264, 2295, 4706, 4265, 510, 457, 437, 741, 600, 1096, 510, 307, 741, 478, 3579, 309, 484, 322, 257, 2522, 50952, 50952, 295, 3035, 534, 437, 321, 366, 3102, 294, 307, 321, 362, 257, 12972, 272, 1804, 269, 293, 300, 7829, 257, 274, 51280, 51320, 293, 321, 362, 264, 13760, 295, 264, 4470, 365, 3104, 281, 274, 293, 321, 1116, 411, 281, 458, 437, 264, 51500, 51500, 13760, 295, 264, 4470, 307, 365, 3104, 281, 257, 272, 293, 269, 586, 613, 510, 366, 707, 732, 12, 18759, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.06655374744482208, "compression_ratio": 1.8804780876494025, "no_speech_prob": 5.0144444685429335e-06}, {"id": 446, "seek": 264040, "start": 2640.4, "end": 2647.28, "text": " examples of a matrix multiplication two by two times a two by two plus a two a vector of just", "tokens": [50364, 5110, 295, 257, 8141, 27290, 732, 538, 732, 1413, 257, 732, 538, 732, 1804, 257, 732, 257, 8062, 295, 445, 50708, 50708, 732, 4959, 269, 16, 293, 269, 17, 2709, 385, 257, 732, 538, 732, 586, 3449, 510, 300, 741, 362, 257, 12577, 8062, 510, 1219, 51096, 51096, 269, 293, 264, 12577, 8062, 269, 16, 293, 269, 17, 457, 382, 741, 7619, 670, 510, 300, 12577, 8062, 486, 1813, 257, 5386, 8062, 51436, 51436, 294, 264, 30024, 293, 486, 25356, 28450, 370, 300, 311, 437, 311, 2737, 510, 382, 731, 269, 16, 269, 17, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.02699211665562221, "compression_ratio": 1.8980582524271845, "no_speech_prob": 7.182887202361599e-06}, {"id": 447, "seek": 264040, "start": 2647.28, "end": 2655.04, "text": " two elements c1 and c2 gives me a two by two now notice here that i have a bias vector here called", "tokens": [50364, 5110, 295, 257, 8141, 27290, 732, 538, 732, 1413, 257, 732, 538, 732, 1804, 257, 732, 257, 8062, 295, 445, 50708, 50708, 732, 4959, 269, 16, 293, 269, 17, 2709, 385, 257, 732, 538, 732, 586, 3449, 510, 300, 741, 362, 257, 12577, 8062, 510, 1219, 51096, 51096, 269, 293, 264, 12577, 8062, 269, 16, 293, 269, 17, 457, 382, 741, 7619, 670, 510, 300, 12577, 8062, 486, 1813, 257, 5386, 8062, 51436, 51436, 294, 264, 30024, 293, 486, 25356, 28450, 370, 300, 311, 437, 311, 2737, 510, 382, 731, 269, 16, 269, 17, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.02699211665562221, "compression_ratio": 1.8980582524271845, "no_speech_prob": 7.182887202361599e-06}, {"id": 448, "seek": 264040, "start": 2655.04, "end": 2661.84, "text": " c and the bias vector c1 and c2 but as i described over here that bias vector will become a row vector", "tokens": [50364, 5110, 295, 257, 8141, 27290, 732, 538, 732, 1413, 257, 732, 538, 732, 1804, 257, 732, 257, 8062, 295, 445, 50708, 50708, 732, 4959, 269, 16, 293, 269, 17, 2709, 385, 257, 732, 538, 732, 586, 3449, 510, 300, 741, 362, 257, 12577, 8062, 510, 1219, 51096, 51096, 269, 293, 264, 12577, 8062, 269, 16, 293, 269, 17, 457, 382, 741, 7619, 670, 510, 300, 12577, 8062, 486, 1813, 257, 5386, 8062, 51436, 51436, 294, 264, 30024, 293, 486, 25356, 28450, 370, 300, 311, 437, 311, 2737, 510, 382, 731, 269, 16, 269, 17, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.02699211665562221, "compression_ratio": 1.8980582524271845, "no_speech_prob": 7.182887202361599e-06}, {"id": 449, "seek": 264040, "start": 2661.84, "end": 2666.96, "text": " in the broadcasting and will replicate vertically so that's what's happening here as well c1 c2", "tokens": [50364, 5110, 295, 257, 8141, 27290, 732, 538, 732, 1413, 257, 732, 538, 732, 1804, 257, 732, 257, 8062, 295, 445, 50708, 50708, 732, 4959, 269, 16, 293, 269, 17, 2709, 385, 257, 732, 538, 732, 586, 3449, 510, 300, 741, 362, 257, 12577, 8062, 510, 1219, 51096, 51096, 269, 293, 264, 12577, 8062, 269, 16, 293, 269, 17, 457, 382, 741, 7619, 670, 510, 300, 12577, 8062, 486, 1813, 257, 5386, 8062, 51436, 51436, 294, 264, 30024, 293, 486, 25356, 28450, 370, 300, 311, 437, 311, 2737, 510, 382, 731, 269, 16, 269, 17, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.02699211665562221, "compression_ratio": 1.8980582524271845, "no_speech_prob": 7.182887202361599e-06}, {"id": 450, "seek": 266696, "start": 2666.96, "end": 2674.16, "text": " is replicated vertically and we see how we have two rows of c1 c2 as a result so now when i say", "tokens": [50364, 307, 46365, 28450, 293, 321, 536, 577, 321, 362, 732, 13241, 295, 269, 16, 269, 17, 382, 257, 1874, 370, 586, 562, 741, 584, 50724, 50724, 2464, 309, 484, 741, 445, 914, 2232, 411, 341, 1936, 1821, 493, 341, 8141, 27290, 666, 264, 51000, 51000, 3539, 551, 300, 300, 311, 516, 322, 833, 264, 13376, 370, 382, 257, 1874, 295, 8141, 27290, 293, 577, 51264, 51264, 309, 1985, 274, 5348, 307, 264, 1874, 295, 257, 5893, 1674, 1296, 264, 700, 5386, 295, 257, 293, 264, 700, 7738, 295, 272, 370, 257, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.037894136027285925, "compression_ratio": 1.775229357798165, "no_speech_prob": 8.013065780687612e-06}, {"id": 451, "seek": 266696, "start": 2674.16, "end": 2679.68, "text": " write it out i just mean uh like this basically break up this matrix multiplication into the", "tokens": [50364, 307, 46365, 28450, 293, 321, 536, 577, 321, 362, 732, 13241, 295, 269, 16, 269, 17, 382, 257, 1874, 370, 586, 562, 741, 584, 50724, 50724, 2464, 309, 484, 741, 445, 914, 2232, 411, 341, 1936, 1821, 493, 341, 8141, 27290, 666, 264, 51000, 51000, 3539, 551, 300, 300, 311, 516, 322, 833, 264, 13376, 370, 382, 257, 1874, 295, 8141, 27290, 293, 577, 51264, 51264, 309, 1985, 274, 5348, 307, 264, 1874, 295, 257, 5893, 1674, 1296, 264, 700, 5386, 295, 257, 293, 264, 700, 7738, 295, 272, 370, 257, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.037894136027285925, "compression_ratio": 1.775229357798165, "no_speech_prob": 8.013065780687612e-06}, {"id": 452, "seek": 266696, "start": 2679.68, "end": 2684.96, "text": " actual thing that that's going on under the hood so as a result of matrix multiplication and how", "tokens": [50364, 307, 46365, 28450, 293, 321, 536, 577, 321, 362, 732, 13241, 295, 269, 16, 269, 17, 382, 257, 1874, 370, 586, 562, 741, 584, 50724, 50724, 2464, 309, 484, 741, 445, 914, 2232, 411, 341, 1936, 1821, 493, 341, 8141, 27290, 666, 264, 51000, 51000, 3539, 551, 300, 300, 311, 516, 322, 833, 264, 13376, 370, 382, 257, 1874, 295, 8141, 27290, 293, 577, 51264, 51264, 309, 1985, 274, 5348, 307, 264, 1874, 295, 257, 5893, 1674, 1296, 264, 700, 5386, 295, 257, 293, 264, 700, 7738, 295, 272, 370, 257, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.037894136027285925, "compression_ratio": 1.775229357798165, "no_speech_prob": 8.013065780687612e-06}, {"id": 453, "seek": 266696, "start": 2684.96, "end": 2691.84, "text": " it works d11 is the result of a dot product between the first row of a and the first column of b so a", "tokens": [50364, 307, 46365, 28450, 293, 321, 536, 577, 321, 362, 732, 13241, 295, 269, 16, 269, 17, 382, 257, 1874, 370, 586, 562, 741, 584, 50724, 50724, 2464, 309, 484, 741, 445, 914, 2232, 411, 341, 1936, 1821, 493, 341, 8141, 27290, 666, 264, 51000, 51000, 3539, 551, 300, 300, 311, 516, 322, 833, 264, 13376, 370, 382, 257, 1874, 295, 8141, 27290, 293, 577, 51264, 51264, 309, 1985, 274, 5348, 307, 264, 1874, 295, 257, 5893, 1674, 1296, 264, 700, 5386, 295, 257, 293, 264, 700, 7738, 295, 272, 370, 257, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.037894136027285925, "compression_ratio": 1.775229357798165, "no_speech_prob": 8.013065780687612e-06}, {"id": 454, "seek": 269184, "start": 2691.84, "end": 2702.0, "text": " 11 b 11 plus a 12 b 21 plus c1 and so on so forth for all the other elements of d and once you", "tokens": [50364, 2975, 272, 2975, 1804, 257, 2272, 272, 5080, 1804, 269, 16, 293, 370, 322, 370, 5220, 337, 439, 264, 661, 4959, 295, 274, 293, 1564, 291, 50872, 50872, 767, 2464, 309, 484, 309, 3643, 6322, 341, 307, 445, 257, 3840, 295, 12788, 530, 293, 1105, 10342, 293, 321, 458, 51156, 51156, 490, 4532, 7165, 577, 281, 23203, 12788, 530, 293, 10860, 293, 370, 341, 307, 406, 6958, 3602, 309, 311, 51404, 51404, 406, 445, 8141, 27290, 309, 311, 445, 2232, 38284, 7015, 457, 341, 307, 2584, 24207, 712, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.07475868515346361, "compression_ratio": 1.7309417040358743, "no_speech_prob": 1.505651994193613e-06}, {"id": 455, "seek": 269184, "start": 2702.0, "end": 2707.6800000000003, "text": " actually write it out it becomes obvious this is just a bunch of multiplies and um ads and we know", "tokens": [50364, 2975, 272, 2975, 1804, 257, 2272, 272, 5080, 1804, 269, 16, 293, 370, 322, 370, 5220, 337, 439, 264, 661, 4959, 295, 274, 293, 1564, 291, 50872, 50872, 767, 2464, 309, 484, 309, 3643, 6322, 341, 307, 445, 257, 3840, 295, 12788, 530, 293, 1105, 10342, 293, 321, 458, 51156, 51156, 490, 4532, 7165, 577, 281, 23203, 12788, 530, 293, 10860, 293, 370, 341, 307, 406, 6958, 3602, 309, 311, 51404, 51404, 406, 445, 8141, 27290, 309, 311, 445, 2232, 38284, 7015, 457, 341, 307, 2584, 24207, 712, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.07475868515346361, "compression_ratio": 1.7309417040358743, "no_speech_prob": 1.505651994193613e-06}, {"id": 456, "seek": 269184, "start": 2707.6800000000003, "end": 2712.6400000000003, "text": " from micrograd how to differentiate multiplies and adds and so this is not scary anymore it's", "tokens": [50364, 2975, 272, 2975, 1804, 257, 2272, 272, 5080, 1804, 269, 16, 293, 370, 322, 370, 5220, 337, 439, 264, 661, 4959, 295, 274, 293, 1564, 291, 50872, 50872, 767, 2464, 309, 484, 309, 3643, 6322, 341, 307, 445, 257, 3840, 295, 12788, 530, 293, 1105, 10342, 293, 321, 458, 51156, 51156, 490, 4532, 7165, 577, 281, 23203, 12788, 530, 293, 10860, 293, 370, 341, 307, 406, 6958, 3602, 309, 311, 51404, 51404, 406, 445, 8141, 27290, 309, 311, 445, 2232, 38284, 7015, 457, 341, 307, 2584, 24207, 712, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.07475868515346361, "compression_ratio": 1.7309417040358743, "no_speech_prob": 1.505651994193613e-06}, {"id": 457, "seek": 269184, "start": 2712.6400000000003, "end": 2718.56, "text": " not just matrix multiplication it's just uh tedious unfortunately but this is completely tractable", "tokens": [50364, 2975, 272, 2975, 1804, 257, 2272, 272, 5080, 1804, 269, 16, 293, 370, 322, 370, 5220, 337, 439, 264, 661, 4959, 295, 274, 293, 1564, 291, 50872, 50872, 767, 2464, 309, 484, 309, 3643, 6322, 341, 307, 445, 257, 3840, 295, 12788, 530, 293, 1105, 10342, 293, 321, 458, 51156, 51156, 490, 4532, 7165, 577, 281, 23203, 12788, 530, 293, 10860, 293, 370, 341, 307, 406, 6958, 3602, 309, 311, 51404, 51404, 406, 445, 8141, 27290, 309, 311, 445, 2232, 38284, 7015, 457, 341, 307, 2584, 24207, 712, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.07475868515346361, "compression_ratio": 1.7309417040358743, "no_speech_prob": 1.505651994193613e-06}, {"id": 458, "seek": 271856, "start": 2718.56, "end": 2724.96, "text": " we have dl by d for all of these and we want to deal by uh all these little other variables", "tokens": [50364, 321, 362, 37873, 538, 274, 337, 439, 295, 613, 293, 321, 528, 281, 2028, 538, 2232, 439, 613, 707, 661, 9102, 50684, 50684, 370, 577, 360, 321, 4584, 300, 293, 577, 360, 321, 767, 483, 264, 2771, 2448, 1392, 370, 264, 2295, 4706, 4265, 50908, 50908, 6515, 510, 370, 718, 311, 337, 1365, 28446, 264, 13760, 295, 264, 4470, 365, 3104, 281, 257, 2975, 51188, 51240, 321, 536, 510, 300, 257, 2975, 11843, 6091, 294, 527, 2199, 6114, 558, 510, 558, 510, 293, 21222, 274, 51516, 51516], "temperature": 0.0, "avg_logprob": -0.08193522892641218, "compression_ratio": 1.76036866359447, "no_speech_prob": 1.3081447605145513e-06}, {"id": 459, "seek": 271856, "start": 2724.96, "end": 2729.44, "text": " so how do we achieve that and how do we actually get the gradients okay so the low budget production", "tokens": [50364, 321, 362, 37873, 538, 274, 337, 439, 295, 613, 293, 321, 528, 281, 2028, 538, 2232, 439, 613, 707, 661, 9102, 50684, 50684, 370, 577, 360, 321, 4584, 300, 293, 577, 360, 321, 767, 483, 264, 2771, 2448, 1392, 370, 264, 2295, 4706, 4265, 50908, 50908, 6515, 510, 370, 718, 311, 337, 1365, 28446, 264, 13760, 295, 264, 4470, 365, 3104, 281, 257, 2975, 51188, 51240, 321, 536, 510, 300, 257, 2975, 11843, 6091, 294, 527, 2199, 6114, 558, 510, 558, 510, 293, 21222, 274, 51516, 51516], "temperature": 0.0, "avg_logprob": -0.08193522892641218, "compression_ratio": 1.76036866359447, "no_speech_prob": 1.3081447605145513e-06}, {"id": 460, "seek": 271856, "start": 2729.44, "end": 2735.04, "text": " continues here so let's for example derive the derivative of the loss with respect to a 11", "tokens": [50364, 321, 362, 37873, 538, 274, 337, 439, 295, 613, 293, 321, 528, 281, 2028, 538, 2232, 439, 613, 707, 661, 9102, 50684, 50684, 370, 577, 360, 321, 4584, 300, 293, 577, 360, 321, 767, 483, 264, 2771, 2448, 1392, 370, 264, 2295, 4706, 4265, 50908, 50908, 6515, 510, 370, 718, 311, 337, 1365, 28446, 264, 13760, 295, 264, 4470, 365, 3104, 281, 257, 2975, 51188, 51240, 321, 536, 510, 300, 257, 2975, 11843, 6091, 294, 527, 2199, 6114, 558, 510, 558, 510, 293, 21222, 274, 51516, 51516], "temperature": 0.0, "avg_logprob": -0.08193522892641218, "compression_ratio": 1.76036866359447, "no_speech_prob": 1.3081447605145513e-06}, {"id": 461, "seek": 271856, "start": 2736.08, "end": 2741.6, "text": " we see here that a 11 occurs twice in our simple expression right here right here and influences d", "tokens": [50364, 321, 362, 37873, 538, 274, 337, 439, 295, 613, 293, 321, 528, 281, 2028, 538, 2232, 439, 613, 707, 661, 9102, 50684, 50684, 370, 577, 360, 321, 4584, 300, 293, 577, 360, 321, 767, 483, 264, 2771, 2448, 1392, 370, 264, 2295, 4706, 4265, 50908, 50908, 6515, 510, 370, 718, 311, 337, 1365, 28446, 264, 13760, 295, 264, 4470, 365, 3104, 281, 257, 2975, 51188, 51240, 321, 536, 510, 300, 257, 2975, 11843, 6091, 294, 527, 2199, 6114, 558, 510, 558, 510, 293, 21222, 274, 51516, 51516], "temperature": 0.0, "avg_logprob": -0.08193522892641218, "compression_ratio": 1.76036866359447, "no_speech_prob": 1.3081447605145513e-06}, {"id": 462, "seek": 274160, "start": 2741.6, "end": 2751.8399999999997, "text": " 11 and d 12 so this is so what is dl by d a 11 well it's dl by d 11 times the local derivative", "tokens": [50364, 2975, 293, 274, 2272, 370, 341, 307, 370, 437, 307, 37873, 538, 274, 257, 2975, 731, 309, 311, 37873, 538, 274, 2975, 1413, 264, 2654, 13760, 50876, 50876, 295, 274, 2975, 597, 294, 341, 1389, 307, 445, 272, 2975, 570, 300, 311, 437, 311, 30955, 257, 2975, 510, 370, 2232, 293, 51252, 51252, 32407, 510, 264, 2654, 13760, 295, 274, 2272, 365, 3104, 281, 257, 2975, 307, 445, 272, 2272, 293, 370, 272, 2272, 486, 294, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.044458478689193726, "compression_ratio": 1.7592592592592593, "no_speech_prob": 3.5558903164201183e-06}, {"id": 463, "seek": 274160, "start": 2751.8399999999997, "end": 2759.36, "text": " of d 11 which in this case is just b 11 because that's what's multiplying a 11 here so uh and", "tokens": [50364, 2975, 293, 274, 2272, 370, 341, 307, 370, 437, 307, 37873, 538, 274, 257, 2975, 731, 309, 311, 37873, 538, 274, 2975, 1413, 264, 2654, 13760, 50876, 50876, 295, 274, 2975, 597, 294, 341, 1389, 307, 445, 272, 2975, 570, 300, 311, 437, 311, 30955, 257, 2975, 510, 370, 2232, 293, 51252, 51252, 32407, 510, 264, 2654, 13760, 295, 274, 2272, 365, 3104, 281, 257, 2975, 307, 445, 272, 2272, 293, 370, 272, 2272, 486, 294, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.044458478689193726, "compression_ratio": 1.7592592592592593, "no_speech_prob": 3.5558903164201183e-06}, {"id": 464, "seek": 274160, "start": 2759.36, "end": 2765.12, "text": " likewise here the local derivative of d 12 with respect to a 11 is just b 12 and so b 12 will in", "tokens": [50364, 2975, 293, 274, 2272, 370, 341, 307, 370, 437, 307, 37873, 538, 274, 257, 2975, 731, 309, 311, 37873, 538, 274, 2975, 1413, 264, 2654, 13760, 50876, 50876, 295, 274, 2975, 597, 294, 341, 1389, 307, 445, 272, 2975, 570, 300, 311, 437, 311, 30955, 257, 2975, 510, 370, 2232, 293, 51252, 51252, 32407, 510, 264, 2654, 13760, 295, 274, 2272, 365, 3104, 281, 257, 2975, 307, 445, 272, 2272, 293, 370, 272, 2272, 486, 294, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.044458478689193726, "compression_ratio": 1.7592592592592593, "no_speech_prob": 3.5558903164201183e-06}, {"id": 465, "seek": 276512, "start": 2765.12, "end": 2772.72, "text": " the chain rule therefore multiply dl by d 12 and then because a 11 is used both to produce d 11 and", "tokens": [50364, 264, 5021, 4978, 4412, 12972, 37873, 538, 274, 2272, 293, 550, 570, 257, 2975, 307, 1143, 1293, 281, 5258, 274, 2975, 293, 50744, 50744, 274, 2272, 321, 643, 281, 909, 493, 264, 15725, 295, 1293, 295, 729, 1333, 295, 12626, 300, 366, 2614, 294, 8952, 51120, 51120, 293, 300, 311, 983, 321, 483, 257, 1804, 445, 5127, 493, 729, 732, 1105, 729, 732, 15725, 293, 300, 2709, 505, 51424, 51424, 37873, 538, 274, 257, 2975, 321, 393, 360, 264, 1900, 912, 5215, 337, 264, 661, 472, 337, 439, 264, 661, 4959, 295, 257, 51764, 51800], "temperature": 0.0, "avg_logprob": -0.05304616259545395, "compression_ratio": 1.7802690582959642, "no_speech_prob": 3.3930386962310877e-06}, {"id": 466, "seek": 276512, "start": 2772.72, "end": 2780.24, "text": " d 12 we need to add up the contributions of both of those sort of chains that are running in parallel", "tokens": [50364, 264, 5021, 4978, 4412, 12972, 37873, 538, 274, 2272, 293, 550, 570, 257, 2975, 307, 1143, 1293, 281, 5258, 274, 2975, 293, 50744, 50744, 274, 2272, 321, 643, 281, 909, 493, 264, 15725, 295, 1293, 295, 729, 1333, 295, 12626, 300, 366, 2614, 294, 8952, 51120, 51120, 293, 300, 311, 983, 321, 483, 257, 1804, 445, 5127, 493, 729, 732, 1105, 729, 732, 15725, 293, 300, 2709, 505, 51424, 51424, 37873, 538, 274, 257, 2975, 321, 393, 360, 264, 1900, 912, 5215, 337, 264, 661, 472, 337, 439, 264, 661, 4959, 295, 257, 51764, 51800], "temperature": 0.0, "avg_logprob": -0.05304616259545395, "compression_ratio": 1.7802690582959642, "no_speech_prob": 3.3930386962310877e-06}, {"id": 467, "seek": 276512, "start": 2780.24, "end": 2786.3199999999997, "text": " and that's why we get a plus just adding up those two um those two contributions and that gives us", "tokens": [50364, 264, 5021, 4978, 4412, 12972, 37873, 538, 274, 2272, 293, 550, 570, 257, 2975, 307, 1143, 1293, 281, 5258, 274, 2975, 293, 50744, 50744, 274, 2272, 321, 643, 281, 909, 493, 264, 15725, 295, 1293, 295, 729, 1333, 295, 12626, 300, 366, 2614, 294, 8952, 51120, 51120, 293, 300, 311, 983, 321, 483, 257, 1804, 445, 5127, 493, 729, 732, 1105, 729, 732, 15725, 293, 300, 2709, 505, 51424, 51424, 37873, 538, 274, 257, 2975, 321, 393, 360, 264, 1900, 912, 5215, 337, 264, 661, 472, 337, 439, 264, 661, 4959, 295, 257, 51764, 51800], "temperature": 0.0, "avg_logprob": -0.05304616259545395, "compression_ratio": 1.7802690582959642, "no_speech_prob": 3.3930386962310877e-06}, {"id": 468, "seek": 276512, "start": 2786.3199999999997, "end": 2793.12, "text": " dl by d a 11 we can do the exact same analysis for the other one for all the other elements of a", "tokens": [50364, 264, 5021, 4978, 4412, 12972, 37873, 538, 274, 2272, 293, 550, 570, 257, 2975, 307, 1143, 1293, 281, 5258, 274, 2975, 293, 50744, 50744, 274, 2272, 321, 643, 281, 909, 493, 264, 15725, 295, 1293, 295, 729, 1333, 295, 12626, 300, 366, 2614, 294, 8952, 51120, 51120, 293, 300, 311, 983, 321, 483, 257, 1804, 445, 5127, 493, 729, 732, 1105, 729, 732, 15725, 293, 300, 2709, 505, 51424, 51424, 37873, 538, 274, 257, 2975, 321, 393, 360, 264, 1900, 912, 5215, 337, 264, 661, 472, 337, 439, 264, 661, 4959, 295, 257, 51764, 51800], "temperature": 0.0, "avg_logprob": -0.05304616259545395, "compression_ratio": 1.7802690582959642, "no_speech_prob": 3.3930386962310877e-06}, {"id": 469, "seek": 279312, "start": 2793.12, "end": 2799.2799999999997, "text": " and when you simply write it out it's just super simple um taking the gradients on you know", "tokens": [50364, 293, 562, 291, 2935, 2464, 309, 484, 309, 311, 445, 1687, 2199, 1105, 1940, 264, 2771, 2448, 322, 291, 458, 50672, 50672, 15277, 411, 341, 291, 915, 300, 341, 8141, 37873, 538, 274, 257, 300, 321, 434, 934, 558, 498, 321, 445, 9424, 51108, 51108, 439, 264, 439, 295, 552, 294, 264, 912, 3909, 382, 257, 2516, 370, 257, 307, 445, 886, 709, 8141, 370, 37873, 538, 274, 257, 510, 51444, 51444], "temperature": 0.0, "avg_logprob": -0.11463330243084882, "compression_ratio": 1.6494252873563218, "no_speech_prob": 2.5214890229108278e-06}, {"id": 470, "seek": 279312, "start": 2799.2799999999997, "end": 2808.0, "text": " expressions like this you find that this matrix dl by d a that we're after right if we just arrange", "tokens": [50364, 293, 562, 291, 2935, 2464, 309, 484, 309, 311, 445, 1687, 2199, 1105, 1940, 264, 2771, 2448, 322, 291, 458, 50672, 50672, 15277, 411, 341, 291, 915, 300, 341, 8141, 37873, 538, 274, 257, 300, 321, 434, 934, 558, 498, 321, 445, 9424, 51108, 51108, 439, 264, 439, 295, 552, 294, 264, 912, 3909, 382, 257, 2516, 370, 257, 307, 445, 886, 709, 8141, 370, 37873, 538, 274, 257, 510, 51444, 51444], "temperature": 0.0, "avg_logprob": -0.11463330243084882, "compression_ratio": 1.6494252873563218, "no_speech_prob": 2.5214890229108278e-06}, {"id": 471, "seek": 279312, "start": 2808.0, "end": 2814.72, "text": " all the all of them in the same shape as a takes so a is just too much matrix so dl by d a here", "tokens": [50364, 293, 562, 291, 2935, 2464, 309, 484, 309, 311, 445, 1687, 2199, 1105, 1940, 264, 2771, 2448, 322, 291, 458, 50672, 50672, 15277, 411, 341, 291, 915, 300, 341, 8141, 37873, 538, 274, 257, 300, 321, 434, 934, 558, 498, 321, 445, 9424, 51108, 51108, 439, 264, 439, 295, 552, 294, 264, 912, 3909, 382, 257, 2516, 370, 257, 307, 445, 886, 709, 8141, 370, 37873, 538, 274, 257, 510, 51444, 51444], "temperature": 0.0, "avg_logprob": -0.11463330243084882, "compression_ratio": 1.6494252873563218, "no_speech_prob": 2.5214890229108278e-06}, {"id": 472, "seek": 281472, "start": 2814.72, "end": 2824.56, "text": " will be also just the same shape tensor with the derivatives now so dl by d a 11 etc", "tokens": [50364, 486, 312, 611, 445, 264, 912, 3909, 40863, 365, 264, 33733, 586, 370, 37873, 538, 274, 257, 2975, 5183, 50856, 50888, 293, 321, 536, 300, 767, 321, 393, 5109, 437, 321, 600, 3720, 484, 510, 382, 257, 8141, 12972, 293, 370, 309, 51216, 51216, 445, 370, 2314, 300, 274, 287, 538, 300, 439, 295, 613, 30546, 300, 321, 600, 18949, 510, 538, 1940, 51452, 51452, 2771, 2448, 393, 767, 312, 12675, 382, 257, 8141, 27290, 293, 294, 1729, 321, 536, 300, 309, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08518296618794285, "compression_ratio": 1.7607655502392345, "no_speech_prob": 2.5214744709955994e-06}, {"id": 473, "seek": 281472, "start": 2825.2, "end": 2831.7599999999998, "text": " and we see that actually we can express what we've written out here as a matrix multiply and so it", "tokens": [50364, 486, 312, 611, 445, 264, 912, 3909, 40863, 365, 264, 33733, 586, 370, 37873, 538, 274, 257, 2975, 5183, 50856, 50888, 293, 321, 536, 300, 767, 321, 393, 5109, 437, 321, 600, 3720, 484, 510, 382, 257, 8141, 12972, 293, 370, 309, 51216, 51216, 445, 370, 2314, 300, 274, 287, 538, 300, 439, 295, 613, 30546, 300, 321, 600, 18949, 510, 538, 1940, 51452, 51452, 2771, 2448, 393, 767, 312, 12675, 382, 257, 8141, 27290, 293, 294, 1729, 321, 536, 300, 309, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08518296618794285, "compression_ratio": 1.7607655502392345, "no_speech_prob": 2.5214744709955994e-06}, {"id": 474, "seek": 281472, "start": 2831.7599999999998, "end": 2836.48, "text": " just so happens that d l by that all of these formulas that we've derived here by taking", "tokens": [50364, 486, 312, 611, 445, 264, 912, 3909, 40863, 365, 264, 33733, 586, 370, 37873, 538, 274, 257, 2975, 5183, 50856, 50888, 293, 321, 536, 300, 767, 321, 393, 5109, 437, 321, 600, 3720, 484, 510, 382, 257, 8141, 12972, 293, 370, 309, 51216, 51216, 445, 370, 2314, 300, 274, 287, 538, 300, 439, 295, 613, 30546, 300, 321, 600, 18949, 510, 538, 1940, 51452, 51452, 2771, 2448, 393, 767, 312, 12675, 382, 257, 8141, 27290, 293, 294, 1729, 321, 536, 300, 309, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08518296618794285, "compression_ratio": 1.7607655502392345, "no_speech_prob": 2.5214744709955994e-06}, {"id": 475, "seek": 281472, "start": 2836.48, "end": 2841.52, "text": " gradients can actually be expressed as a matrix multiplication and in particular we see that it", "tokens": [50364, 486, 312, 611, 445, 264, 912, 3909, 40863, 365, 264, 33733, 586, 370, 37873, 538, 274, 257, 2975, 5183, 50856, 50888, 293, 321, 536, 300, 767, 321, 393, 5109, 437, 321, 600, 3720, 484, 510, 382, 257, 8141, 12972, 293, 370, 309, 51216, 51216, 445, 370, 2314, 300, 274, 287, 538, 300, 439, 295, 613, 30546, 300, 321, 600, 18949, 510, 538, 1940, 51452, 51452, 2771, 2448, 393, 767, 312, 12675, 382, 257, 8141, 27290, 293, 294, 1729, 321, 536, 300, 309, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.08518296618794285, "compression_ratio": 1.7607655502392345, "no_speech_prob": 2.5214744709955994e-06}, {"id": 476, "seek": 284152, "start": 2841.52, "end": 2850.0, "text": " is the matrix multiplication of these two array matrices so it is the um dl by d and then matrix", "tokens": [50364, 307, 264, 8141, 27290, 295, 613, 732, 10225, 32284, 370, 309, 307, 264, 1105, 37873, 538, 274, 293, 550, 8141, 50788, 50788, 30955, 272, 457, 272, 25167, 767, 370, 291, 536, 300, 272, 4436, 293, 272, 4762, 362, 3105, 1081, 9735, 949, 51252, 51252, 321, 632, 295, 1164, 272, 5348, 272, 4762, 272, 4436, 272, 7490, 370, 291, 536, 300, 341, 661, 8141, 272, 307, 7132, 1744, 293, 370, 1936, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.05188417756879652, "compression_ratio": 1.6875, "no_speech_prob": 2.0261086319806054e-06}, {"id": 477, "seek": 284152, "start": 2850.0, "end": 2859.28, "text": " multiplying b but b transpose actually so you see that b21 and b12 have changed place whereas before", "tokens": [50364, 307, 264, 8141, 27290, 295, 613, 732, 10225, 32284, 370, 309, 307, 264, 1105, 37873, 538, 274, 293, 550, 8141, 50788, 50788, 30955, 272, 457, 272, 25167, 767, 370, 291, 536, 300, 272, 4436, 293, 272, 4762, 362, 3105, 1081, 9735, 949, 51252, 51252, 321, 632, 295, 1164, 272, 5348, 272, 4762, 272, 4436, 272, 7490, 370, 291, 536, 300, 341, 661, 8141, 272, 307, 7132, 1744, 293, 370, 1936, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.05188417756879652, "compression_ratio": 1.6875, "no_speech_prob": 2.0261086319806054e-06}, {"id": 478, "seek": 284152, "start": 2859.28, "end": 2868.16, "text": " we had of course b11 b12 b21 b22 so you see that this other matrix b is transposed and so basically", "tokens": [50364, 307, 264, 8141, 27290, 295, 613, 732, 10225, 32284, 370, 309, 307, 264, 1105, 37873, 538, 274, 293, 550, 8141, 50788, 50788, 30955, 272, 457, 272, 25167, 767, 370, 291, 536, 300, 272, 4436, 293, 272, 4762, 362, 3105, 1081, 9735, 949, 51252, 51252, 321, 632, 295, 1164, 272, 5348, 272, 4762, 272, 4436, 272, 7490, 370, 291, 536, 300, 341, 661, 8141, 272, 307, 7132, 1744, 293, 370, 1936, 51696, 51696], "temperature": 0.0, "avg_logprob": -0.05188417756879652, "compression_ratio": 1.6875, "no_speech_prob": 2.0261086319806054e-06}, {"id": 479, "seek": 286816, "start": 2868.16, "end": 2873.04, "text": " what we have long story short just by doing very simple reasoning here by breaking up the expression", "tokens": [50364, 437, 321, 362, 938, 1657, 2099, 445, 538, 884, 588, 2199, 21577, 510, 538, 7697, 493, 264, 6114, 50608, 50608, 294, 264, 1389, 295, 257, 588, 2199, 1365, 307, 300, 37873, 538, 1120, 307, 597, 307, 341, 307, 2935, 2681, 281, 37873, 538, 274, 67, 51032, 51032, 8141, 17207, 365, 272, 25167, 370, 300, 307, 437, 321, 362, 370, 1400, 586, 321, 611, 528, 264, 33733, 51408, 51408, 365, 3104, 281, 1105, 272, 293, 269, 586, 337, 272, 741, 478, 406, 767, 884, 264, 1577, 10151, 399, 570, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.049678351568139115, "compression_ratio": 1.7625570776255708, "no_speech_prob": 3.5007628866878804e-06}, {"id": 480, "seek": 286816, "start": 2873.04, "end": 2881.52, "text": " in the case of a very simple example is that dl by da is which is this is simply equal to dl by dd", "tokens": [50364, 437, 321, 362, 938, 1657, 2099, 445, 538, 884, 588, 2199, 21577, 510, 538, 7697, 493, 264, 6114, 50608, 50608, 294, 264, 1389, 295, 257, 588, 2199, 1365, 307, 300, 37873, 538, 1120, 307, 597, 307, 341, 307, 2935, 2681, 281, 37873, 538, 274, 67, 51032, 51032, 8141, 17207, 365, 272, 25167, 370, 300, 307, 437, 321, 362, 370, 1400, 586, 321, 611, 528, 264, 33733, 51408, 51408, 365, 3104, 281, 1105, 272, 293, 269, 586, 337, 272, 741, 478, 406, 767, 884, 264, 1577, 10151, 399, 570, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.049678351568139115, "compression_ratio": 1.7625570776255708, "no_speech_prob": 3.5007628866878804e-06}, {"id": 481, "seek": 286816, "start": 2881.52, "end": 2889.04, "text": " matrix multiplied with b transpose so that is what we have so far now we also want the derivatives", "tokens": [50364, 437, 321, 362, 938, 1657, 2099, 445, 538, 884, 588, 2199, 21577, 510, 538, 7697, 493, 264, 6114, 50608, 50608, 294, 264, 1389, 295, 257, 588, 2199, 1365, 307, 300, 37873, 538, 1120, 307, 597, 307, 341, 307, 2935, 2681, 281, 37873, 538, 274, 67, 51032, 51032, 8141, 17207, 365, 272, 25167, 370, 300, 307, 437, 321, 362, 370, 1400, 586, 321, 611, 528, 264, 33733, 51408, 51408, 365, 3104, 281, 1105, 272, 293, 269, 586, 337, 272, 741, 478, 406, 767, 884, 264, 1577, 10151, 399, 570, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.049678351568139115, "compression_ratio": 1.7625570776255708, "no_speech_prob": 3.5007628866878804e-06}, {"id": 482, "seek": 286816, "start": 2889.04, "end": 2896.48, "text": " with respect to um b and c now for b i'm not actually doing the full derivation because", "tokens": [50364, 437, 321, 362, 938, 1657, 2099, 445, 538, 884, 588, 2199, 21577, 510, 538, 7697, 493, 264, 6114, 50608, 50608, 294, 264, 1389, 295, 257, 588, 2199, 1365, 307, 300, 37873, 538, 1120, 307, 597, 307, 341, 307, 2935, 2681, 281, 37873, 538, 274, 67, 51032, 51032, 8141, 17207, 365, 272, 25167, 370, 300, 307, 437, 321, 362, 370, 1400, 586, 321, 611, 528, 264, 33733, 51408, 51408, 365, 3104, 281, 1105, 272, 293, 269, 586, 337, 272, 741, 478, 406, 767, 884, 264, 1577, 10151, 399, 570, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.049678351568139115, "compression_ratio": 1.7625570776255708, "no_speech_prob": 3.5007628866878804e-06}, {"id": 483, "seek": 289648, "start": 2896.48, "end": 2902.8, "text": " honestly it's um it's not deep it's just annoying it's exhausting you can actually do this analysis", "tokens": [50364, 6095, 309, 311, 1105, 309, 311, 406, 2452, 309, 311, 445, 11304, 309, 311, 34076, 291, 393, 767, 360, 341, 5215, 50680, 50680, 1803, 291, 603, 611, 915, 300, 498, 291, 747, 341, 613, 15277, 293, 291, 23203, 365, 50884, 50884, 3104, 281, 272, 2602, 295, 257, 291, 486, 915, 300, 37873, 538, 274, 65, 307, 611, 257, 8141, 27290, 294, 341, 1389, 51208, 51208, 291, 362, 281, 747, 264, 8141, 257, 293, 25167, 309, 293, 8141, 12972, 300, 365, 37873, 538, 274, 67, 51460, 51528], "temperature": 0.0, "avg_logprob": -0.03669195012612776, "compression_ratio": 1.8125, "no_speech_prob": 3.3404592159058666e-06}, {"id": 484, "seek": 289648, "start": 2902.8, "end": 2906.88, "text": " yourself you'll also find that if you take this these expressions and you differentiate with", "tokens": [50364, 6095, 309, 311, 1105, 309, 311, 406, 2452, 309, 311, 445, 11304, 309, 311, 34076, 291, 393, 767, 360, 341, 5215, 50680, 50680, 1803, 291, 603, 611, 915, 300, 498, 291, 747, 341, 613, 15277, 293, 291, 23203, 365, 50884, 50884, 3104, 281, 272, 2602, 295, 257, 291, 486, 915, 300, 37873, 538, 274, 65, 307, 611, 257, 8141, 27290, 294, 341, 1389, 51208, 51208, 291, 362, 281, 747, 264, 8141, 257, 293, 25167, 309, 293, 8141, 12972, 300, 365, 37873, 538, 274, 67, 51460, 51528], "temperature": 0.0, "avg_logprob": -0.03669195012612776, "compression_ratio": 1.8125, "no_speech_prob": 3.3404592159058666e-06}, {"id": 485, "seek": 289648, "start": 2906.88, "end": 2913.36, "text": " respect to b instead of a you will find that dl by db is also a matrix multiplication in this case", "tokens": [50364, 6095, 309, 311, 1105, 309, 311, 406, 2452, 309, 311, 445, 11304, 309, 311, 34076, 291, 393, 767, 360, 341, 5215, 50680, 50680, 1803, 291, 603, 611, 915, 300, 498, 291, 747, 341, 613, 15277, 293, 291, 23203, 365, 50884, 50884, 3104, 281, 272, 2602, 295, 257, 291, 486, 915, 300, 37873, 538, 274, 65, 307, 611, 257, 8141, 27290, 294, 341, 1389, 51208, 51208, 291, 362, 281, 747, 264, 8141, 257, 293, 25167, 309, 293, 8141, 12972, 300, 365, 37873, 538, 274, 67, 51460, 51528], "temperature": 0.0, "avg_logprob": -0.03669195012612776, "compression_ratio": 1.8125, "no_speech_prob": 3.3404592159058666e-06}, {"id": 486, "seek": 289648, "start": 2913.36, "end": 2918.4, "text": " you have to take the matrix a and transpose it and matrix multiply that with dl by dd", "tokens": [50364, 6095, 309, 311, 1105, 309, 311, 406, 2452, 309, 311, 445, 11304, 309, 311, 34076, 291, 393, 767, 360, 341, 5215, 50680, 50680, 1803, 291, 603, 611, 915, 300, 498, 291, 747, 341, 613, 15277, 293, 291, 23203, 365, 50884, 50884, 3104, 281, 272, 2602, 295, 257, 291, 486, 915, 300, 37873, 538, 274, 65, 307, 611, 257, 8141, 27290, 294, 341, 1389, 51208, 51208, 291, 362, 281, 747, 264, 8141, 257, 293, 25167, 309, 293, 8141, 12972, 300, 365, 37873, 538, 274, 67, 51460, 51528], "temperature": 0.0, "avg_logprob": -0.03669195012612776, "compression_ratio": 1.8125, "no_speech_prob": 3.3404592159058666e-06}, {"id": 487, "seek": 291840, "start": 2918.4, "end": 2926.2400000000002, "text": " and that's what gives you a dl by db and then here for the offsets c1 and c2 if you again just", "tokens": [50364, 293, 300, 311, 437, 2709, 291, 257, 37873, 538, 274, 65, 293, 550, 510, 337, 264, 39457, 1385, 269, 16, 293, 269, 17, 498, 291, 797, 445, 50756, 50756, 23203, 365, 3104, 281, 269, 16, 291, 486, 915, 364, 6114, 411, 341, 293, 269, 17, 364, 6114, 411, 341, 51124, 51160, 293, 1936, 291, 603, 915, 300, 37873, 538, 274, 66, 307, 2935, 570, 436, 434, 445, 18687, 783, 613, 15277, 51436, 51436, 291, 445, 362, 281, 747, 264, 37873, 538, 274, 67, 8141, 1105, 295, 264, 33733, 295, 274, 293, 291, 445, 362, 281, 2408, 51804, 51856], "temperature": 0.0, "avg_logprob": -0.16032392501831055, "compression_ratio": 1.8840579710144927, "no_speech_prob": 3.2377026855101576e-06}, {"id": 488, "seek": 291840, "start": 2926.2400000000002, "end": 2933.6, "text": " differentiate with respect to c1 you will find an expression like this and c2 an expression like this", "tokens": [50364, 293, 300, 311, 437, 2709, 291, 257, 37873, 538, 274, 65, 293, 550, 510, 337, 264, 39457, 1385, 269, 16, 293, 269, 17, 498, 291, 797, 445, 50756, 50756, 23203, 365, 3104, 281, 269, 16, 291, 486, 915, 364, 6114, 411, 341, 293, 269, 17, 364, 6114, 411, 341, 51124, 51160, 293, 1936, 291, 603, 915, 300, 37873, 538, 274, 66, 307, 2935, 570, 436, 434, 445, 18687, 783, 613, 15277, 51436, 51436, 291, 445, 362, 281, 747, 264, 37873, 538, 274, 67, 8141, 1105, 295, 264, 33733, 295, 274, 293, 291, 445, 362, 281, 2408, 51804, 51856], "temperature": 0.0, "avg_logprob": -0.16032392501831055, "compression_ratio": 1.8840579710144927, "no_speech_prob": 3.2377026855101576e-06}, {"id": 489, "seek": 291840, "start": 2934.32, "end": 2939.84, "text": " and basically you'll find that dl by dc is simply because they're just offsetting these expressions", "tokens": [50364, 293, 300, 311, 437, 2709, 291, 257, 37873, 538, 274, 65, 293, 550, 510, 337, 264, 39457, 1385, 269, 16, 293, 269, 17, 498, 291, 797, 445, 50756, 50756, 23203, 365, 3104, 281, 269, 16, 291, 486, 915, 364, 6114, 411, 341, 293, 269, 17, 364, 6114, 411, 341, 51124, 51160, 293, 1936, 291, 603, 915, 300, 37873, 538, 274, 66, 307, 2935, 570, 436, 434, 445, 18687, 783, 613, 15277, 51436, 51436, 291, 445, 362, 281, 747, 264, 37873, 538, 274, 67, 8141, 1105, 295, 264, 33733, 295, 274, 293, 291, 445, 362, 281, 2408, 51804, 51856], "temperature": 0.0, "avg_logprob": -0.16032392501831055, "compression_ratio": 1.8840579710144927, "no_speech_prob": 3.2377026855101576e-06}, {"id": 490, "seek": 291840, "start": 2939.84, "end": 2947.2000000000003, "text": " you just have to take the dl by dd matrix um of the derivatives of d and you just have to sum", "tokens": [50364, 293, 300, 311, 437, 2709, 291, 257, 37873, 538, 274, 65, 293, 550, 510, 337, 264, 39457, 1385, 269, 16, 293, 269, 17, 498, 291, 797, 445, 50756, 50756, 23203, 365, 3104, 281, 269, 16, 291, 486, 915, 364, 6114, 411, 341, 293, 269, 17, 364, 6114, 411, 341, 51124, 51160, 293, 1936, 291, 603, 915, 300, 37873, 538, 274, 66, 307, 2935, 570, 436, 434, 445, 18687, 783, 613, 15277, 51436, 51436, 291, 445, 362, 281, 747, 264, 37873, 538, 274, 67, 8141, 1105, 295, 264, 33733, 295, 274, 293, 291, 445, 362, 281, 2408, 51804, 51856], "temperature": 0.0, "avg_logprob": -0.16032392501831055, "compression_ratio": 1.8840579710144927, "no_speech_prob": 3.2377026855101576e-06}, {"id": 491, "seek": 294720, "start": 2947.2, "end": 2955.7599999999998, "text": " across the columns and that gives you derivatives for c so long story short the backward path of a", "tokens": [50364, 2108, 264, 13766, 293, 300, 2709, 291, 33733, 337, 269, 370, 938, 1657, 2099, 264, 23897, 3100, 295, 257, 50792, 50792, 8141, 12972, 307, 257, 8141, 12972, 293, 2602, 295, 445, 411, 321, 632, 274, 6915, 257, 1413, 272, 1804, 269, 51084, 51124, 294, 257, 39684, 1389, 321, 1333, 295, 411, 8881, 412, 746, 588, 588, 2531, 457, 586, 51340, 51340, 365, 257, 8141, 27290, 2602, 295, 257, 39684, 27290, 370, 264, 13760, 295, 274, 365, 3104, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.194727850549015, "compression_ratio": 1.8415841584158417, "no_speech_prob": 5.896265520277666e-07}, {"id": 492, "seek": 294720, "start": 2955.7599999999998, "end": 2961.6, "text": " matrix multiply is a matrix multiply and instead of just like we had d equals a times b plus c", "tokens": [50364, 2108, 264, 13766, 293, 300, 2709, 291, 33733, 337, 269, 370, 938, 1657, 2099, 264, 23897, 3100, 295, 257, 50792, 50792, 8141, 12972, 307, 257, 8141, 12972, 293, 2602, 295, 445, 411, 321, 632, 274, 6915, 257, 1413, 272, 1804, 269, 51084, 51124, 294, 257, 39684, 1389, 321, 1333, 295, 411, 8881, 412, 746, 588, 588, 2531, 457, 586, 51340, 51340, 365, 257, 8141, 27290, 2602, 295, 257, 39684, 27290, 370, 264, 13760, 295, 274, 365, 3104, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.194727850549015, "compression_ratio": 1.8415841584158417, "no_speech_prob": 5.896265520277666e-07}, {"id": 493, "seek": 294720, "start": 2962.3999999999996, "end": 2966.72, "text": " in a scalar case we sort of like arrive at something very very similar but now", "tokens": [50364, 2108, 264, 13766, 293, 300, 2709, 291, 33733, 337, 269, 370, 938, 1657, 2099, 264, 23897, 3100, 295, 257, 50792, 50792, 8141, 12972, 307, 257, 8141, 12972, 293, 2602, 295, 445, 411, 321, 632, 274, 6915, 257, 1413, 272, 1804, 269, 51084, 51124, 294, 257, 39684, 1389, 321, 1333, 295, 411, 8881, 412, 746, 588, 588, 2531, 457, 586, 51340, 51340, 365, 257, 8141, 27290, 2602, 295, 257, 39684, 27290, 370, 264, 13760, 295, 274, 365, 3104, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.194727850549015, "compression_ratio": 1.8415841584158417, "no_speech_prob": 5.896265520277666e-07}, {"id": 494, "seek": 294720, "start": 2966.72, "end": 2973.7599999999998, "text": " with a matrix multiplication instead of a scalar multiplication so the derivative of d with respect", "tokens": [50364, 2108, 264, 13766, 293, 300, 2709, 291, 33733, 337, 269, 370, 938, 1657, 2099, 264, 23897, 3100, 295, 257, 50792, 50792, 8141, 12972, 307, 257, 8141, 12972, 293, 2602, 295, 445, 411, 321, 632, 274, 6915, 257, 1413, 272, 1804, 269, 51084, 51124, 294, 257, 39684, 1389, 321, 1333, 295, 411, 8881, 412, 746, 588, 588, 2531, 457, 586, 51340, 51340, 365, 257, 8141, 27290, 2602, 295, 257, 39684, 27290, 370, 264, 13760, 295, 274, 365, 3104, 51692, 51692], "temperature": 0.0, "avg_logprob": -0.194727850549015, "compression_ratio": 1.8415841584158417, "no_speech_prob": 5.896265520277666e-07}, {"id": 495, "seek": 297376, "start": 2973.76, "end": 2983.36, "text": " to a is dl by dd matrix multiply b transpose and here it's a transpose multiply dl by dd but in", "tokens": [50364, 281, 257, 307, 37873, 538, 274, 67, 8141, 12972, 272, 25167, 293, 510, 309, 311, 257, 25167, 12972, 37873, 538, 274, 67, 457, 294, 50844, 50844, 1293, 3331, 8141, 27290, 365, 264, 13760, 293, 264, 661, 1433, 294, 264, 27290, 51192, 51244, 293, 337, 269, 309, 307, 257, 2408, 586, 741, 603, 980, 291, 257, 4054, 741, 393, 1128, 1604, 264, 30546, 300, 321, 445, 51568, 51568, 6678, 337, 646, 79, 1513, 559, 990, 337, 8141, 27290, 293, 741, 393, 646, 79, 1513, 559, 473, 337, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.17843621571858723, "compression_ratio": 1.9015544041450778, "no_speech_prob": 4.4950265873922035e-06}, {"id": 496, "seek": 297376, "start": 2983.36, "end": 2990.32, "text": " both cases matrix multiplication with the derivative and the other term in the multiplication", "tokens": [50364, 281, 257, 307, 37873, 538, 274, 67, 8141, 12972, 272, 25167, 293, 510, 309, 311, 257, 25167, 12972, 37873, 538, 274, 67, 457, 294, 50844, 50844, 1293, 3331, 8141, 27290, 365, 264, 13760, 293, 264, 661, 1433, 294, 264, 27290, 51192, 51244, 293, 337, 269, 309, 307, 257, 2408, 586, 741, 603, 980, 291, 257, 4054, 741, 393, 1128, 1604, 264, 30546, 300, 321, 445, 51568, 51568, 6678, 337, 646, 79, 1513, 559, 990, 337, 8141, 27290, 293, 741, 393, 646, 79, 1513, 559, 473, 337, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.17843621571858723, "compression_ratio": 1.9015544041450778, "no_speech_prob": 4.4950265873922035e-06}, {"id": 497, "seek": 297376, "start": 2991.36, "end": 2997.84, "text": " and for c it is a sum now i'll tell you a secret i can never remember the formulas that we just", "tokens": [50364, 281, 257, 307, 37873, 538, 274, 67, 8141, 12972, 272, 25167, 293, 510, 309, 311, 257, 25167, 12972, 37873, 538, 274, 67, 457, 294, 50844, 50844, 1293, 3331, 8141, 27290, 365, 264, 13760, 293, 264, 661, 1433, 294, 264, 27290, 51192, 51244, 293, 337, 269, 309, 307, 257, 2408, 586, 741, 603, 980, 291, 257, 4054, 741, 393, 1128, 1604, 264, 30546, 300, 321, 445, 51568, 51568, 6678, 337, 646, 79, 1513, 559, 990, 337, 8141, 27290, 293, 741, 393, 646, 79, 1513, 559, 473, 337, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.17843621571858723, "compression_ratio": 1.9015544041450778, "no_speech_prob": 4.4950265873922035e-06}, {"id": 498, "seek": 297376, "start": 2997.84, "end": 3002.0800000000004, "text": " arrived for backpropagating for matrix multiplication and i can backpropagate for", "tokens": [50364, 281, 257, 307, 37873, 538, 274, 67, 8141, 12972, 272, 25167, 293, 510, 309, 311, 257, 25167, 12972, 37873, 538, 274, 67, 457, 294, 50844, 50844, 1293, 3331, 8141, 27290, 365, 264, 13760, 293, 264, 661, 1433, 294, 264, 27290, 51192, 51244, 293, 337, 269, 309, 307, 257, 2408, 586, 741, 603, 980, 291, 257, 4054, 741, 393, 1128, 1604, 264, 30546, 300, 321, 445, 51568, 51568, 6678, 337, 646, 79, 1513, 559, 990, 337, 8141, 27290, 293, 741, 393, 646, 79, 1513, 559, 473, 337, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.17843621571858723, "compression_ratio": 1.9015544041450778, "no_speech_prob": 4.4950265873922035e-06}, {"id": 499, "seek": 300208, "start": 3002.08, "end": 3006.72, "text": " these expressions just fine and the reason this works is because the dimensions have to work out", "tokens": [50364, 613, 15277, 445, 2489, 293, 264, 1778, 341, 1985, 307, 570, 264, 12819, 362, 281, 589, 484, 50596, 50628, 370, 718, 385, 976, 291, 364, 1365, 584, 741, 528, 281, 1884, 274, 71, 550, 437, 820, 274, 71, 312, 1230, 472, 741, 362, 281, 458, 51020, 51020, 300, 264, 3909, 295, 274, 71, 1633, 312, 264, 912, 382, 264, 3909, 295, 276, 293, 264, 3909, 295, 276, 307, 8858, 538, 12145, 293, 550, 264, 51368, 51368, 661, 2522, 295, 1589, 741, 458, 307, 300, 274, 71, 1633, 312, 512, 733, 295, 8141, 27290, 295, 274, 3565, 1208, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12582241662658086, "compression_ratio": 1.78125, "no_speech_prob": 6.338918410619954e-06}, {"id": 500, "seek": 300208, "start": 3007.36, "end": 3015.2, "text": " so let me give you an example say i want to create dh then what should dh be number one i have to know", "tokens": [50364, 613, 15277, 445, 2489, 293, 264, 1778, 341, 1985, 307, 570, 264, 12819, 362, 281, 589, 484, 50596, 50628, 370, 718, 385, 976, 291, 364, 1365, 584, 741, 528, 281, 1884, 274, 71, 550, 437, 820, 274, 71, 312, 1230, 472, 741, 362, 281, 458, 51020, 51020, 300, 264, 3909, 295, 274, 71, 1633, 312, 264, 912, 382, 264, 3909, 295, 276, 293, 264, 3909, 295, 276, 307, 8858, 538, 12145, 293, 550, 264, 51368, 51368, 661, 2522, 295, 1589, 741, 458, 307, 300, 274, 71, 1633, 312, 512, 733, 295, 8141, 27290, 295, 274, 3565, 1208, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12582241662658086, "compression_ratio": 1.78125, "no_speech_prob": 6.338918410619954e-06}, {"id": 501, "seek": 300208, "start": 3015.2, "end": 3022.16, "text": " that the shape of dh must be the same as the shape of h and the shape of h is 32 by 64 and then the", "tokens": [50364, 613, 15277, 445, 2489, 293, 264, 1778, 341, 1985, 307, 570, 264, 12819, 362, 281, 589, 484, 50596, 50628, 370, 718, 385, 976, 291, 364, 1365, 584, 741, 528, 281, 1884, 274, 71, 550, 437, 820, 274, 71, 312, 1230, 472, 741, 362, 281, 458, 51020, 51020, 300, 264, 3909, 295, 274, 71, 1633, 312, 264, 912, 382, 264, 3909, 295, 276, 293, 264, 3909, 295, 276, 307, 8858, 538, 12145, 293, 550, 264, 51368, 51368, 661, 2522, 295, 1589, 741, 458, 307, 300, 274, 71, 1633, 312, 512, 733, 295, 8141, 27290, 295, 274, 3565, 1208, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12582241662658086, "compression_ratio": 1.78125, "no_speech_prob": 6.338918410619954e-06}, {"id": 502, "seek": 300208, "start": 3022.16, "end": 3028.88, "text": " other piece of information i know is that dh must be some kind of matrix multiplication of d logits", "tokens": [50364, 613, 15277, 445, 2489, 293, 264, 1778, 341, 1985, 307, 570, 264, 12819, 362, 281, 589, 484, 50596, 50628, 370, 718, 385, 976, 291, 364, 1365, 584, 741, 528, 281, 1884, 274, 71, 550, 437, 820, 274, 71, 312, 1230, 472, 741, 362, 281, 458, 51020, 51020, 300, 264, 3909, 295, 274, 71, 1633, 312, 264, 912, 382, 264, 3909, 295, 276, 293, 264, 3909, 295, 276, 307, 8858, 538, 12145, 293, 550, 264, 51368, 51368, 661, 2522, 295, 1589, 741, 458, 307, 300, 274, 71, 1633, 312, 512, 733, 295, 8141, 27290, 295, 274, 3565, 1208, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.12582241662658086, "compression_ratio": 1.78125, "no_speech_prob": 6.338918410619954e-06}, {"id": 503, "seek": 302888, "start": 3028.88, "end": 3038.6400000000003, "text": " with w2 and d logits is 32 by 27 and w2 is 64 by 27 there is only a single way to make the shape", "tokens": [50364, 365, 261, 17, 293, 274, 3565, 1208, 307, 8858, 538, 7634, 293, 261, 17, 307, 12145, 538, 7634, 456, 307, 787, 257, 2167, 636, 281, 652, 264, 3909, 50852, 50852, 589, 484, 294, 341, 1389, 293, 309, 307, 6451, 264, 3006, 1874, 294, 1729, 510, 276, 2203, 281, 312, 8858, 538, 12145, 51240, 51240, 264, 787, 636, 281, 4584, 300, 307, 281, 747, 257, 274, 3565, 1208, 293, 8141, 12972, 309, 365, 291, 536, 577, 741, 362, 281, 51604, 51604, 747, 261, 17, 457, 741, 362, 281, 25167, 309, 281, 652, 264, 12819, 589, 484, 370, 718, 311, 747, 257, 8141, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.1851837158203125, "compression_ratio": 1.8, "no_speech_prob": 5.422043159342138e-06}, {"id": 504, "seek": 302888, "start": 3038.6400000000003, "end": 3046.4, "text": " work out in this case and it is indeed the correct result in particular here h needs to be 32 by 64", "tokens": [50364, 365, 261, 17, 293, 274, 3565, 1208, 307, 8858, 538, 7634, 293, 261, 17, 307, 12145, 538, 7634, 456, 307, 787, 257, 2167, 636, 281, 652, 264, 3909, 50852, 50852, 589, 484, 294, 341, 1389, 293, 309, 307, 6451, 264, 3006, 1874, 294, 1729, 510, 276, 2203, 281, 312, 8858, 538, 12145, 51240, 51240, 264, 787, 636, 281, 4584, 300, 307, 281, 747, 257, 274, 3565, 1208, 293, 8141, 12972, 309, 365, 291, 536, 577, 741, 362, 281, 51604, 51604, 747, 261, 17, 457, 741, 362, 281, 25167, 309, 281, 652, 264, 12819, 589, 484, 370, 718, 311, 747, 257, 8141, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.1851837158203125, "compression_ratio": 1.8, "no_speech_prob": 5.422043159342138e-06}, {"id": 505, "seek": 302888, "start": 3046.4, "end": 3053.6800000000003, "text": " the only way to achieve that is to take a d logits and matrix multiply it with you see how i have to", "tokens": [50364, 365, 261, 17, 293, 274, 3565, 1208, 307, 8858, 538, 7634, 293, 261, 17, 307, 12145, 538, 7634, 456, 307, 787, 257, 2167, 636, 281, 652, 264, 3909, 50852, 50852, 589, 484, 294, 341, 1389, 293, 309, 307, 6451, 264, 3006, 1874, 294, 1729, 510, 276, 2203, 281, 312, 8858, 538, 12145, 51240, 51240, 264, 787, 636, 281, 4584, 300, 307, 281, 747, 257, 274, 3565, 1208, 293, 8141, 12972, 309, 365, 291, 536, 577, 741, 362, 281, 51604, 51604, 747, 261, 17, 457, 741, 362, 281, 25167, 309, 281, 652, 264, 12819, 589, 484, 370, 718, 311, 747, 257, 8141, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.1851837158203125, "compression_ratio": 1.8, "no_speech_prob": 5.422043159342138e-06}, {"id": 506, "seek": 302888, "start": 3053.6800000000003, "end": 3058.1600000000003, "text": " take w2 but i have to transpose it to make the dimensions work out so let's take a matrix", "tokens": [50364, 365, 261, 17, 293, 274, 3565, 1208, 307, 8858, 538, 7634, 293, 261, 17, 307, 12145, 538, 7634, 456, 307, 787, 257, 2167, 636, 281, 652, 264, 3909, 50852, 50852, 589, 484, 294, 341, 1389, 293, 309, 307, 6451, 264, 3006, 1874, 294, 1729, 510, 276, 2203, 281, 312, 8858, 538, 12145, 51240, 51240, 264, 787, 636, 281, 4584, 300, 307, 281, 747, 257, 274, 3565, 1208, 293, 8141, 12972, 309, 365, 291, 536, 577, 741, 362, 281, 51604, 51604, 747, 261, 17, 457, 741, 362, 281, 25167, 309, 281, 652, 264, 12819, 589, 484, 370, 718, 311, 747, 257, 8141, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.1851837158203125, "compression_ratio": 1.8, "no_speech_prob": 5.422043159342138e-06}, {"id": 507, "seek": 305816, "start": 3058.16, "end": 3063.68, "text": " multiply and make the dimensions work out so w2 transpose and it's the only way to make these two", "tokens": [50364, 12972, 293, 652, 264, 12819, 589, 484, 370, 261, 17, 25167, 293, 309, 311, 264, 787, 636, 281, 652, 613, 732, 50640, 50640, 8141, 12972, 729, 732, 3755, 281, 652, 264, 10854, 589, 484, 293, 300, 4523, 484, 281, 312, 264, 50840, 50840, 3006, 8513, 370, 498, 321, 808, 510, 321, 528, 274, 71, 597, 307, 1120, 293, 321, 536, 300, 1120, 307, 37873, 538, 274, 67, 51252, 51252, 8141, 12972, 272, 25167, 370, 300, 311, 274, 3565, 1208, 12972, 293, 272, 307, 261, 17, 370, 261, 17, 25167, 597, 307, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.11429250616776315, "compression_ratio": 1.9523809523809523, "no_speech_prob": 5.173763383936603e-06}, {"id": 508, "seek": 305816, "start": 3063.68, "end": 3067.68, "text": " matrix multiply those two pieces to make the shapes work out and that turns out to be the", "tokens": [50364, 12972, 293, 652, 264, 12819, 589, 484, 370, 261, 17, 25167, 293, 309, 311, 264, 787, 636, 281, 652, 613, 732, 50640, 50640, 8141, 12972, 729, 732, 3755, 281, 652, 264, 10854, 589, 484, 293, 300, 4523, 484, 281, 312, 264, 50840, 50840, 3006, 8513, 370, 498, 321, 808, 510, 321, 528, 274, 71, 597, 307, 1120, 293, 321, 536, 300, 1120, 307, 37873, 538, 274, 67, 51252, 51252, 8141, 12972, 272, 25167, 370, 300, 311, 274, 3565, 1208, 12972, 293, 272, 307, 261, 17, 370, 261, 17, 25167, 597, 307, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.11429250616776315, "compression_ratio": 1.9523809523809523, "no_speech_prob": 5.173763383936603e-06}, {"id": 509, "seek": 305816, "start": 3067.68, "end": 3075.92, "text": " correct formula so if we come here we want dh which is da and we see that da is dl by dd", "tokens": [50364, 12972, 293, 652, 264, 12819, 589, 484, 370, 261, 17, 25167, 293, 309, 311, 264, 787, 636, 281, 652, 613, 732, 50640, 50640, 8141, 12972, 729, 732, 3755, 281, 652, 264, 10854, 589, 484, 293, 300, 4523, 484, 281, 312, 264, 50840, 50840, 3006, 8513, 370, 498, 321, 808, 510, 321, 528, 274, 71, 597, 307, 1120, 293, 321, 536, 300, 1120, 307, 37873, 538, 274, 67, 51252, 51252, 8141, 12972, 272, 25167, 370, 300, 311, 274, 3565, 1208, 12972, 293, 272, 307, 261, 17, 370, 261, 17, 25167, 597, 307, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.11429250616776315, "compression_ratio": 1.9523809523809523, "no_speech_prob": 5.173763383936603e-06}, {"id": 510, "seek": 305816, "start": 3075.92, "end": 3083.2, "text": " matrix multiply b transpose so that's d logits multiply and b is w2 so w2 transpose which is", "tokens": [50364, 12972, 293, 652, 264, 12819, 589, 484, 370, 261, 17, 25167, 293, 309, 311, 264, 787, 636, 281, 652, 613, 732, 50640, 50640, 8141, 12972, 729, 732, 3755, 281, 652, 264, 10854, 589, 484, 293, 300, 4523, 484, 281, 312, 264, 50840, 50840, 3006, 8513, 370, 498, 321, 808, 510, 321, 528, 274, 71, 597, 307, 1120, 293, 321, 536, 300, 1120, 307, 37873, 538, 274, 67, 51252, 51252, 8141, 12972, 272, 25167, 370, 300, 311, 274, 3565, 1208, 12972, 293, 272, 307, 261, 17, 370, 261, 17, 25167, 597, 307, 51616, 51616], "temperature": 0.0, "avg_logprob": -0.11429250616776315, "compression_ratio": 1.9523809523809523, "no_speech_prob": 5.173763383936603e-06}, {"id": 511, "seek": 308320, "start": 3083.2, "end": 3090.08, "text": " exactly what we have here so there's no need to remember these formulas similarly now if i want", "tokens": [50364, 2293, 437, 321, 362, 510, 370, 456, 311, 572, 643, 281, 1604, 613, 30546, 14138, 586, 498, 741, 528, 50708, 50708, 27379, 17, 731, 741, 458, 300, 309, 1633, 312, 257, 8141, 27290, 295, 274, 3565, 1208, 293, 276, 293, 1310, 51104, 51104, 456, 311, 257, 1326, 25167, 411, 456, 311, 472, 25167, 294, 456, 382, 731, 293, 741, 500, 380, 458, 597, 636, 309, 307, 51304, 51304, 370, 741, 362, 281, 808, 281, 261, 17, 293, 741, 536, 300, 1080, 3909, 307, 12145, 538, 7634, 293, 300, 575, 281, 808, 490, 512, 8141, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.04132542804795868, "compression_ratio": 1.730593607305936, "no_speech_prob": 1.2098494153178763e-06}, {"id": 512, "seek": 308320, "start": 3090.08, "end": 3098.0, "text": " dw2 well i know that it must be a matrix multiplication of d logits and h and maybe", "tokens": [50364, 2293, 437, 321, 362, 510, 370, 456, 311, 572, 643, 281, 1604, 613, 30546, 14138, 586, 498, 741, 528, 50708, 50708, 27379, 17, 731, 741, 458, 300, 309, 1633, 312, 257, 8141, 27290, 295, 274, 3565, 1208, 293, 276, 293, 1310, 51104, 51104, 456, 311, 257, 1326, 25167, 411, 456, 311, 472, 25167, 294, 456, 382, 731, 293, 741, 500, 380, 458, 597, 636, 309, 307, 51304, 51304, 370, 741, 362, 281, 808, 281, 261, 17, 293, 741, 536, 300, 1080, 3909, 307, 12145, 538, 7634, 293, 300, 575, 281, 808, 490, 512, 8141, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.04132542804795868, "compression_ratio": 1.730593607305936, "no_speech_prob": 1.2098494153178763e-06}, {"id": 513, "seek": 308320, "start": 3098.0, "end": 3102.0, "text": " there's a few transpose like there's one transpose in there as well and i don't know which way it is", "tokens": [50364, 2293, 437, 321, 362, 510, 370, 456, 311, 572, 643, 281, 1604, 613, 30546, 14138, 586, 498, 741, 528, 50708, 50708, 27379, 17, 731, 741, 458, 300, 309, 1633, 312, 257, 8141, 27290, 295, 274, 3565, 1208, 293, 276, 293, 1310, 51104, 51104, 456, 311, 257, 1326, 25167, 411, 456, 311, 472, 25167, 294, 456, 382, 731, 293, 741, 500, 380, 458, 597, 636, 309, 307, 51304, 51304, 370, 741, 362, 281, 808, 281, 261, 17, 293, 741, 536, 300, 1080, 3909, 307, 12145, 538, 7634, 293, 300, 575, 281, 808, 490, 512, 8141, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.04132542804795868, "compression_ratio": 1.730593607305936, "no_speech_prob": 1.2098494153178763e-06}, {"id": 514, "seek": 308320, "start": 3102.0, "end": 3109.12, "text": " so i have to come to w2 and i see that its shape is 64 by 27 and that has to come from some matrix", "tokens": [50364, 2293, 437, 321, 362, 510, 370, 456, 311, 572, 643, 281, 1604, 613, 30546, 14138, 586, 498, 741, 528, 50708, 50708, 27379, 17, 731, 741, 458, 300, 309, 1633, 312, 257, 8141, 27290, 295, 274, 3565, 1208, 293, 276, 293, 1310, 51104, 51104, 456, 311, 257, 1326, 25167, 411, 456, 311, 472, 25167, 294, 456, 382, 731, 293, 741, 500, 380, 458, 597, 636, 309, 307, 51304, 51304, 370, 741, 362, 281, 808, 281, 261, 17, 293, 741, 536, 300, 1080, 3909, 307, 12145, 538, 7634, 293, 300, 575, 281, 808, 490, 512, 8141, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.04132542804795868, "compression_ratio": 1.730593607305936, "no_speech_prob": 1.2098494153178763e-06}, {"id": 515, "seek": 310912, "start": 3109.12, "end": 3118.4, "text": " multiplication of these two and so to get a 64 by 27 i need to take um h i need to transpose it", "tokens": [50364, 27290, 295, 613, 732, 293, 370, 281, 483, 257, 12145, 538, 7634, 741, 643, 281, 747, 1105, 276, 741, 643, 281, 25167, 309, 50828, 50872, 293, 550, 741, 643, 281, 8141, 12972, 309, 370, 300, 486, 1813, 12145, 538, 8858, 293, 550, 741, 643, 281, 8141, 51136, 51136, 12972, 365, 264, 8858, 538, 7634, 293, 300, 311, 516, 281, 976, 385, 257, 12145, 538, 7634, 370, 741, 643, 281, 8141, 12972, 51408, 51408, 341, 365, 264, 3565, 1208, 300, 3909, 445, 411, 300, 300, 311, 264, 787, 636, 281, 652, 264, 12819, 589, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.055583762139389196, "compression_ratio": 2.021390374331551, "no_speech_prob": 2.07845641853055e-05}, {"id": 516, "seek": 310912, "start": 3119.2799999999997, "end": 3124.56, "text": " and then i need to matrix multiply it so that will become 64 by 32 and then i need to matrix", "tokens": [50364, 27290, 295, 613, 732, 293, 370, 281, 483, 257, 12145, 538, 7634, 741, 643, 281, 747, 1105, 276, 741, 643, 281, 25167, 309, 50828, 50872, 293, 550, 741, 643, 281, 8141, 12972, 309, 370, 300, 486, 1813, 12145, 538, 8858, 293, 550, 741, 643, 281, 8141, 51136, 51136, 12972, 365, 264, 8858, 538, 7634, 293, 300, 311, 516, 281, 976, 385, 257, 12145, 538, 7634, 370, 741, 643, 281, 8141, 12972, 51408, 51408, 341, 365, 264, 3565, 1208, 300, 3909, 445, 411, 300, 300, 311, 264, 787, 636, 281, 652, 264, 12819, 589, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.055583762139389196, "compression_ratio": 2.021390374331551, "no_speech_prob": 2.07845641853055e-05}, {"id": 517, "seek": 310912, "start": 3124.56, "end": 3130.0, "text": " multiply with the 32 by 27 and that's going to give me a 64 by 27 so i need to matrix multiply", "tokens": [50364, 27290, 295, 613, 732, 293, 370, 281, 483, 257, 12145, 538, 7634, 741, 643, 281, 747, 1105, 276, 741, 643, 281, 25167, 309, 50828, 50872, 293, 550, 741, 643, 281, 8141, 12972, 309, 370, 300, 486, 1813, 12145, 538, 8858, 293, 550, 741, 643, 281, 8141, 51136, 51136, 12972, 365, 264, 8858, 538, 7634, 293, 300, 311, 516, 281, 976, 385, 257, 12145, 538, 7634, 370, 741, 643, 281, 8141, 12972, 51408, 51408, 341, 365, 264, 3565, 1208, 300, 3909, 445, 411, 300, 300, 311, 264, 787, 636, 281, 652, 264, 12819, 589, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.055583762139389196, "compression_ratio": 2.021390374331551, "no_speech_prob": 2.07845641853055e-05}, {"id": 518, "seek": 310912, "start": 3130.0, "end": 3134.3199999999997, "text": " this with the logits that shape just like that that's the only way to make the dimensions work", "tokens": [50364, 27290, 295, 613, 732, 293, 370, 281, 483, 257, 12145, 538, 7634, 741, 643, 281, 747, 1105, 276, 741, 643, 281, 25167, 309, 50828, 50872, 293, 550, 741, 643, 281, 8141, 12972, 309, 370, 300, 486, 1813, 12145, 538, 8858, 293, 550, 741, 643, 281, 8141, 51136, 51136, 12972, 365, 264, 8858, 538, 7634, 293, 300, 311, 516, 281, 976, 385, 257, 12145, 538, 7634, 370, 741, 643, 281, 8141, 12972, 51408, 51408, 341, 365, 264, 3565, 1208, 300, 3909, 445, 411, 300, 300, 311, 264, 787, 636, 281, 652, 264, 12819, 589, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.055583762139389196, "compression_ratio": 2.021390374331551, "no_speech_prob": 2.07845641853055e-05}, {"id": 519, "seek": 313432, "start": 3134.32, "end": 3140.32, "text": " out and just use matrix multiplication and if we come here we see that that's exactly what's here", "tokens": [50364, 484, 293, 445, 764, 8141, 27290, 293, 498, 321, 808, 510, 321, 536, 300, 300, 311, 2293, 437, 311, 510, 50664, 50664, 370, 257, 25167, 257, 337, 505, 307, 276, 17207, 365, 264, 3565, 1208, 370, 300, 311, 261, 17, 293, 550, 274, 65, 17, 307, 445, 264, 1105, 51256, 51328, 9429, 2408, 293, 767, 294, 264, 912, 636, 456, 311, 787, 472, 636, 281, 652, 264, 10854, 589, 484, 51556, 51556, 741, 500, 380, 362, 281, 1604, 300, 309, 311, 257, 9429, 2408, 2051, 264, 44746, 900, 10298, 570, 300, 311, 264, 787, 636, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.05310550843826448, "compression_ratio": 1.75, "no_speech_prob": 6.962100542295957e-06}, {"id": 520, "seek": 313432, "start": 3140.32, "end": 3152.1600000000003, "text": " so a transpose a for us is h multiplied with the logits so that's w2 and then db2 is just the um", "tokens": [50364, 484, 293, 445, 764, 8141, 27290, 293, 498, 321, 808, 510, 321, 536, 300, 300, 311, 2293, 437, 311, 510, 50664, 50664, 370, 257, 25167, 257, 337, 505, 307, 276, 17207, 365, 264, 3565, 1208, 370, 300, 311, 261, 17, 293, 550, 274, 65, 17, 307, 445, 264, 1105, 51256, 51328, 9429, 2408, 293, 767, 294, 264, 912, 636, 456, 311, 787, 472, 636, 281, 652, 264, 10854, 589, 484, 51556, 51556, 741, 500, 380, 362, 281, 1604, 300, 309, 311, 257, 9429, 2408, 2051, 264, 44746, 900, 10298, 570, 300, 311, 264, 787, 636, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.05310550843826448, "compression_ratio": 1.75, "no_speech_prob": 6.962100542295957e-06}, {"id": 521, "seek": 313432, "start": 3153.6000000000004, "end": 3158.1600000000003, "text": " vertical sum and actually in the same way there's only one way to make the shapes work out", "tokens": [50364, 484, 293, 445, 764, 8141, 27290, 293, 498, 321, 808, 510, 321, 536, 300, 300, 311, 2293, 437, 311, 510, 50664, 50664, 370, 257, 25167, 257, 337, 505, 307, 276, 17207, 365, 264, 3565, 1208, 370, 300, 311, 261, 17, 293, 550, 274, 65, 17, 307, 445, 264, 1105, 51256, 51328, 9429, 2408, 293, 767, 294, 264, 912, 636, 456, 311, 787, 472, 636, 281, 652, 264, 10854, 589, 484, 51556, 51556, 741, 500, 380, 362, 281, 1604, 300, 309, 311, 257, 9429, 2408, 2051, 264, 44746, 900, 10298, 570, 300, 311, 264, 787, 636, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.05310550843826448, "compression_ratio": 1.75, "no_speech_prob": 6.962100542295957e-06}, {"id": 522, "seek": 313432, "start": 3158.1600000000003, "end": 3162.48, "text": " i don't have to remember that it's a vertical sum along the zeroth axis because that's the only way", "tokens": [50364, 484, 293, 445, 764, 8141, 27290, 293, 498, 321, 808, 510, 321, 536, 300, 300, 311, 2293, 437, 311, 510, 50664, 50664, 370, 257, 25167, 257, 337, 505, 307, 276, 17207, 365, 264, 3565, 1208, 370, 300, 311, 261, 17, 293, 550, 274, 65, 17, 307, 445, 264, 1105, 51256, 51328, 9429, 2408, 293, 767, 294, 264, 912, 636, 456, 311, 787, 472, 636, 281, 652, 264, 10854, 589, 484, 51556, 51556, 741, 500, 380, 362, 281, 1604, 300, 309, 311, 257, 9429, 2408, 2051, 264, 44746, 900, 10298, 570, 300, 311, 264, 787, 636, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.05310550843826448, "compression_ratio": 1.75, "no_speech_prob": 6.962100542295957e-06}, {"id": 523, "seek": 316248, "start": 3162.48, "end": 3172.0, "text": " that this makes sense because b2 shape is 27 so in order to get a um d logits here is 32 by 27", "tokens": [50364, 300, 341, 1669, 2020, 570, 272, 17, 3909, 307, 7634, 370, 294, 1668, 281, 483, 257, 1105, 274, 3565, 1208, 510, 307, 8858, 538, 7634, 50840, 50872, 370, 5276, 300, 309, 311, 445, 2408, 670, 274, 3565, 1208, 294, 512, 3513, 51068, 51236, 300, 3513, 1633, 312, 4018, 570, 741, 643, 281, 13819, 341, 10139, 370, 309, 311, 341, 2232, 370, 341, 51576, 51576, 307, 370, 341, 733, 295, 411, 264, 10339, 88, 636, 718, 385, 5055, 9163, 293, 12097, 300, 293, 718, 385, 11173, 670, 510, 51856], "temperature": 0.0, "avg_logprob": -0.0892576429578993, "compression_ratio": 1.7073170731707317, "no_speech_prob": 1.6536675957468105e-06}, {"id": 524, "seek": 316248, "start": 3172.64, "end": 3176.56, "text": " so knowing that it's just sum over d logits in some direction", "tokens": [50364, 300, 341, 1669, 2020, 570, 272, 17, 3909, 307, 7634, 370, 294, 1668, 281, 483, 257, 1105, 274, 3565, 1208, 510, 307, 8858, 538, 7634, 50840, 50872, 370, 5276, 300, 309, 311, 445, 2408, 670, 274, 3565, 1208, 294, 512, 3513, 51068, 51236, 300, 3513, 1633, 312, 4018, 570, 741, 643, 281, 13819, 341, 10139, 370, 309, 311, 341, 2232, 370, 341, 51576, 51576, 307, 370, 341, 733, 295, 411, 264, 10339, 88, 636, 718, 385, 5055, 9163, 293, 12097, 300, 293, 718, 385, 11173, 670, 510, 51856], "temperature": 0.0, "avg_logprob": -0.0892576429578993, "compression_ratio": 1.7073170731707317, "no_speech_prob": 1.6536675957468105e-06}, {"id": 525, "seek": 316248, "start": 3179.92, "end": 3186.72, "text": " that direction must be zero because i need to eliminate this dimension so it's this uh so this", "tokens": [50364, 300, 341, 1669, 2020, 570, 272, 17, 3909, 307, 7634, 370, 294, 1668, 281, 483, 257, 1105, 274, 3565, 1208, 510, 307, 8858, 538, 7634, 50840, 50872, 370, 5276, 300, 309, 311, 445, 2408, 670, 274, 3565, 1208, 294, 512, 3513, 51068, 51236, 300, 3513, 1633, 312, 4018, 570, 741, 643, 281, 13819, 341, 10139, 370, 309, 311, 341, 2232, 370, 341, 51576, 51576, 307, 370, 341, 733, 295, 411, 264, 10339, 88, 636, 718, 385, 5055, 9163, 293, 12097, 300, 293, 718, 385, 11173, 670, 510, 51856], "temperature": 0.0, "avg_logprob": -0.0892576429578993, "compression_ratio": 1.7073170731707317, "no_speech_prob": 1.6536675957468105e-06}, {"id": 526, "seek": 318672, "start": 3186.72, "end": 3192.3999999999996, "text": " is so this kind of like the hacky way let me copy paste and delete that and let me swing over here", "tokens": [50364, 307, 370, 341, 733, 295, 411, 264, 10339, 88, 636, 718, 385, 5055, 9163, 293, 12097, 300, 293, 718, 385, 11173, 670, 510, 50648, 50676, 293, 341, 307, 527, 23897, 1320, 337, 264, 8213, 4583, 2232, 4696, 370, 586, 718, 311, 8585, 518, 613, 1045, 51036, 51036, 293, 321, 434, 8568, 300, 321, 1105, 658, 439, 264, 1045, 33733, 3006, 293, 2232, 1190, 293, 321, 536, 300, 276, 51436, 51476, 261, 17, 293, 272, 17, 366, 439, 2293, 3006, 370, 321, 646, 48256, 309, 807, 257, 8213, 4583, 51724], "temperature": 0.0, "avg_logprob": -0.09216182834499485, "compression_ratio": 1.7201834862385321, "no_speech_prob": 6.179220690682996e-07}, {"id": 527, "seek": 318672, "start": 3192.9599999999996, "end": 3200.16, "text": " and this is our backward pass for the linear layer uh hopefully so now let's uncomment these three", "tokens": [50364, 307, 370, 341, 733, 295, 411, 264, 10339, 88, 636, 718, 385, 5055, 9163, 293, 12097, 300, 293, 718, 385, 11173, 670, 510, 50648, 50676, 293, 341, 307, 527, 23897, 1320, 337, 264, 8213, 4583, 2232, 4696, 370, 586, 718, 311, 8585, 518, 613, 1045, 51036, 51036, 293, 321, 434, 8568, 300, 321, 1105, 658, 439, 264, 1045, 33733, 3006, 293, 2232, 1190, 293, 321, 536, 300, 276, 51436, 51476, 261, 17, 293, 272, 17, 366, 439, 2293, 3006, 370, 321, 646, 48256, 309, 807, 257, 8213, 4583, 51724], "temperature": 0.0, "avg_logprob": -0.09216182834499485, "compression_ratio": 1.7201834862385321, "no_speech_prob": 6.179220690682996e-07}, {"id": 528, "seek": 318672, "start": 3200.16, "end": 3208.16, "text": " and we're checking that we um got all the three derivatives correct and uh run and we see that h", "tokens": [50364, 307, 370, 341, 733, 295, 411, 264, 10339, 88, 636, 718, 385, 5055, 9163, 293, 12097, 300, 293, 718, 385, 11173, 670, 510, 50648, 50676, 293, 341, 307, 527, 23897, 1320, 337, 264, 8213, 4583, 2232, 4696, 370, 586, 718, 311, 8585, 518, 613, 1045, 51036, 51036, 293, 321, 434, 8568, 300, 321, 1105, 658, 439, 264, 1045, 33733, 3006, 293, 2232, 1190, 293, 321, 536, 300, 276, 51436, 51476, 261, 17, 293, 272, 17, 366, 439, 2293, 3006, 370, 321, 646, 48256, 309, 807, 257, 8213, 4583, 51724], "temperature": 0.0, "avg_logprob": -0.09216182834499485, "compression_ratio": 1.7201834862385321, "no_speech_prob": 6.179220690682996e-07}, {"id": 529, "seek": 320816, "start": 3208.16, "end": 3217.68, "text": " w2 and b2 are all exactly correct so we back propagate it through a linear layer now next up", "tokens": [50364, 261, 17, 293, 272, 17, 366, 439, 2293, 3006, 370, 321, 646, 48256, 309, 807, 257, 8213, 4583, 586, 958, 493, 50840, 50840, 321, 362, 13760, 337, 264, 276, 1217, 293, 321, 643, 281, 646, 48256, 807, 1266, 71, 666, 276, 659, 578, 370, 321, 51168, 51168, 528, 281, 28446, 274, 71, 659, 578, 293, 510, 321, 362, 281, 646, 48256, 807, 257, 1266, 71, 293, 321, 600, 1217, 1096, 341, 51472, 51472, 294, 4532, 7165, 293, 321, 1604, 300, 1266, 71, 307, 257, 588, 2199, 23897, 8513, 586, 7015, 498, 741, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.09123238921165466, "compression_ratio": 1.830188679245283, "no_speech_prob": 1.4063232811167836e-05}, {"id": 530, "seek": 320816, "start": 3217.68, "end": 3224.24, "text": " we have derivative for the h already and we need to back propagate through 10h into h preact so we", "tokens": [50364, 261, 17, 293, 272, 17, 366, 439, 2293, 3006, 370, 321, 646, 48256, 309, 807, 257, 8213, 4583, 586, 958, 493, 50840, 50840, 321, 362, 13760, 337, 264, 276, 1217, 293, 321, 643, 281, 646, 48256, 807, 1266, 71, 666, 276, 659, 578, 370, 321, 51168, 51168, 528, 281, 28446, 274, 71, 659, 578, 293, 510, 321, 362, 281, 646, 48256, 807, 257, 1266, 71, 293, 321, 600, 1217, 1096, 341, 51472, 51472, 294, 4532, 7165, 293, 321, 1604, 300, 1266, 71, 307, 257, 588, 2199, 23897, 8513, 586, 7015, 498, 741, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.09123238921165466, "compression_ratio": 1.830188679245283, "no_speech_prob": 1.4063232811167836e-05}, {"id": 531, "seek": 320816, "start": 3224.24, "end": 3230.3199999999997, "text": " want to derive dh preact and here we have to back propagate through a 10h and we've already done this", "tokens": [50364, 261, 17, 293, 272, 17, 366, 439, 2293, 3006, 370, 321, 646, 48256, 309, 807, 257, 8213, 4583, 586, 958, 493, 50840, 50840, 321, 362, 13760, 337, 264, 276, 1217, 293, 321, 643, 281, 646, 48256, 807, 1266, 71, 666, 276, 659, 578, 370, 321, 51168, 51168, 528, 281, 28446, 274, 71, 659, 578, 293, 510, 321, 362, 281, 646, 48256, 807, 257, 1266, 71, 293, 321, 600, 1217, 1096, 341, 51472, 51472, 294, 4532, 7165, 293, 321, 1604, 300, 1266, 71, 307, 257, 588, 2199, 23897, 8513, 586, 7015, 498, 741, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.09123238921165466, "compression_ratio": 1.830188679245283, "no_speech_prob": 1.4063232811167836e-05}, {"id": 532, "seek": 320816, "start": 3230.3199999999997, "end": 3236.0, "text": " in micrograd and we remember that 10h is a very simple backward formula now unfortunately if i", "tokens": [50364, 261, 17, 293, 272, 17, 366, 439, 2293, 3006, 370, 321, 646, 48256, 309, 807, 257, 8213, 4583, 586, 958, 493, 50840, 50840, 321, 362, 13760, 337, 264, 276, 1217, 293, 321, 643, 281, 646, 48256, 807, 1266, 71, 666, 276, 659, 578, 370, 321, 51168, 51168, 528, 281, 28446, 274, 71, 659, 578, 293, 510, 321, 362, 281, 646, 48256, 807, 257, 1266, 71, 293, 321, 600, 1217, 1096, 341, 51472, 51472, 294, 4532, 7165, 293, 321, 1604, 300, 1266, 71, 307, 257, 588, 2199, 23897, 8513, 586, 7015, 498, 741, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.09123238921165466, "compression_ratio": 1.830188679245283, "no_speech_prob": 1.4063232811167836e-05}, {"id": 533, "seek": 323600, "start": 3236.0, "end": 3241.04, "text": " just put in d by dx of 10h of x into volt from alpha it lets us down it tells us that it's a", "tokens": [50364, 445, 829, 294, 274, 538, 30017, 295, 1266, 71, 295, 2031, 666, 5962, 490, 8961, 309, 6653, 505, 760, 309, 5112, 505, 300, 309, 311, 257, 50616, 50616, 9848, 65, 7940, 907, 394, 2445, 8889, 295, 2031, 309, 311, 406, 2293, 4961, 457, 22880, 20742, 3256, 3164, 50924, 50924, 775, 406, 718, 505, 760, 293, 309, 2709, 505, 264, 18587, 8513, 294, 1729, 498, 291, 362, 300, 257, 307, 2681, 51188, 51188, 281, 1266, 71, 295, 710, 550, 1120, 538, 9758, 646, 12425, 990, 807, 1266, 71, 307, 445, 472, 3175, 257, 3732, 293, 747, 3637, 300, 51568, 51596], "temperature": 0.0, "avg_logprob": -0.06251566480882097, "compression_ratio": 1.641350210970464, "no_speech_prob": 2.1567805106315063e-06}, {"id": 534, "seek": 323600, "start": 3241.04, "end": 3247.2, "text": " hyperbolic secant function squared of x it's not exactly helpful but luckily google image search", "tokens": [50364, 445, 829, 294, 274, 538, 30017, 295, 1266, 71, 295, 2031, 666, 5962, 490, 8961, 309, 6653, 505, 760, 309, 5112, 505, 300, 309, 311, 257, 50616, 50616, 9848, 65, 7940, 907, 394, 2445, 8889, 295, 2031, 309, 311, 406, 2293, 4961, 457, 22880, 20742, 3256, 3164, 50924, 50924, 775, 406, 718, 505, 760, 293, 309, 2709, 505, 264, 18587, 8513, 294, 1729, 498, 291, 362, 300, 257, 307, 2681, 51188, 51188, 281, 1266, 71, 295, 710, 550, 1120, 538, 9758, 646, 12425, 990, 807, 1266, 71, 307, 445, 472, 3175, 257, 3732, 293, 747, 3637, 300, 51568, 51596], "temperature": 0.0, "avg_logprob": -0.06251566480882097, "compression_ratio": 1.641350210970464, "no_speech_prob": 2.1567805106315063e-06}, {"id": 535, "seek": 323600, "start": 3247.2, "end": 3252.48, "text": " does not let us down and it gives us the simpler formula in particular if you have that a is equal", "tokens": [50364, 445, 829, 294, 274, 538, 30017, 295, 1266, 71, 295, 2031, 666, 5962, 490, 8961, 309, 6653, 505, 760, 309, 5112, 505, 300, 309, 311, 257, 50616, 50616, 9848, 65, 7940, 907, 394, 2445, 8889, 295, 2031, 309, 311, 406, 2293, 4961, 457, 22880, 20742, 3256, 3164, 50924, 50924, 775, 406, 718, 505, 760, 293, 309, 2709, 505, 264, 18587, 8513, 294, 1729, 498, 291, 362, 300, 257, 307, 2681, 51188, 51188, 281, 1266, 71, 295, 710, 550, 1120, 538, 9758, 646, 12425, 990, 807, 1266, 71, 307, 445, 472, 3175, 257, 3732, 293, 747, 3637, 300, 51568, 51596], "temperature": 0.0, "avg_logprob": -0.06251566480882097, "compression_ratio": 1.641350210970464, "no_speech_prob": 2.1567805106315063e-06}, {"id": 536, "seek": 323600, "start": 3252.48, "end": 3260.08, "text": " to 10h of z then da by dz back propagating through 10h is just one minus a square and take note that", "tokens": [50364, 445, 829, 294, 274, 538, 30017, 295, 1266, 71, 295, 2031, 666, 5962, 490, 8961, 309, 6653, 505, 760, 309, 5112, 505, 300, 309, 311, 257, 50616, 50616, 9848, 65, 7940, 907, 394, 2445, 8889, 295, 2031, 309, 311, 406, 2293, 4961, 457, 22880, 20742, 3256, 3164, 50924, 50924, 775, 406, 718, 505, 760, 293, 309, 2709, 505, 264, 18587, 8513, 294, 1729, 498, 291, 362, 300, 257, 307, 2681, 51188, 51188, 281, 1266, 71, 295, 710, 550, 1120, 538, 9758, 646, 12425, 990, 807, 1266, 71, 307, 445, 472, 3175, 257, 3732, 293, 747, 3637, 300, 51568, 51596], "temperature": 0.0, "avg_logprob": -0.06251566480882097, "compression_ratio": 1.641350210970464, "no_speech_prob": 2.1567805106315063e-06}, {"id": 537, "seek": 326008, "start": 3260.08, "end": 3267.68, "text": " uh one minus a square a here is the output of the 10h not the input to the 10h z so the da by dz", "tokens": [50364, 2232, 472, 3175, 257, 3732, 257, 510, 307, 264, 5598, 295, 264, 1266, 71, 406, 264, 4846, 281, 264, 1266, 71, 710, 370, 264, 1120, 538, 9758, 50744, 50744, 307, 510, 48936, 294, 2115, 295, 264, 5598, 295, 300, 1266, 71, 293, 510, 611, 294, 20742, 3256, 3164, 321, 362, 51040, 51040, 264, 1577, 10151, 399, 498, 291, 528, 281, 767, 747, 264, 3539, 7123, 295, 1266, 71, 293, 589, 807, 264, 51292, 51292, 5221, 281, 2573, 484, 472, 3175, 1266, 71, 3732, 295, 710, 370, 472, 3175, 257, 3732, 307, 264, 2654, 13760, 294, 527, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.12024968147277831, "compression_ratio": 1.893719806763285, "no_speech_prob": 1.6796334421087522e-06}, {"id": 538, "seek": 326008, "start": 3267.68, "end": 3273.6, "text": " is here formulated in terms of the output of that 10h and here also in google image search we have", "tokens": [50364, 2232, 472, 3175, 257, 3732, 257, 510, 307, 264, 5598, 295, 264, 1266, 71, 406, 264, 4846, 281, 264, 1266, 71, 710, 370, 264, 1120, 538, 9758, 50744, 50744, 307, 510, 48936, 294, 2115, 295, 264, 5598, 295, 300, 1266, 71, 293, 510, 611, 294, 20742, 3256, 3164, 321, 362, 51040, 51040, 264, 1577, 10151, 399, 498, 291, 528, 281, 767, 747, 264, 3539, 7123, 295, 1266, 71, 293, 589, 807, 264, 51292, 51292, 5221, 281, 2573, 484, 472, 3175, 1266, 71, 3732, 295, 710, 370, 472, 3175, 257, 3732, 307, 264, 2654, 13760, 294, 527, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.12024968147277831, "compression_ratio": 1.893719806763285, "no_speech_prob": 1.6796334421087522e-06}, {"id": 539, "seek": 326008, "start": 3273.6, "end": 3278.64, "text": " the full derivation if you want to actually take the actual definition of 10h and work through the", "tokens": [50364, 2232, 472, 3175, 257, 3732, 257, 510, 307, 264, 5598, 295, 264, 1266, 71, 406, 264, 4846, 281, 264, 1266, 71, 710, 370, 264, 1120, 538, 9758, 50744, 50744, 307, 510, 48936, 294, 2115, 295, 264, 5598, 295, 300, 1266, 71, 293, 510, 611, 294, 20742, 3256, 3164, 321, 362, 51040, 51040, 264, 1577, 10151, 399, 498, 291, 528, 281, 767, 747, 264, 3539, 7123, 295, 1266, 71, 293, 589, 807, 264, 51292, 51292, 5221, 281, 2573, 484, 472, 3175, 1266, 71, 3732, 295, 710, 370, 472, 3175, 257, 3732, 307, 264, 2654, 13760, 294, 527, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.12024968147277831, "compression_ratio": 1.893719806763285, "no_speech_prob": 1.6796334421087522e-06}, {"id": 540, "seek": 326008, "start": 3278.64, "end": 3285.92, "text": " math to figure out one minus 10h square of z so one minus a square is the local derivative in our", "tokens": [50364, 2232, 472, 3175, 257, 3732, 257, 510, 307, 264, 5598, 295, 264, 1266, 71, 406, 264, 4846, 281, 264, 1266, 71, 710, 370, 264, 1120, 538, 9758, 50744, 50744, 307, 510, 48936, 294, 2115, 295, 264, 5598, 295, 300, 1266, 71, 293, 510, 611, 294, 20742, 3256, 3164, 321, 362, 51040, 51040, 264, 1577, 10151, 399, 498, 291, 528, 281, 767, 747, 264, 3539, 7123, 295, 1266, 71, 293, 589, 807, 264, 51292, 51292, 5221, 281, 2573, 484, 472, 3175, 1266, 71, 3732, 295, 710, 370, 472, 3175, 257, 3732, 307, 264, 2654, 13760, 294, 527, 51656, 51656], "temperature": 0.0, "avg_logprob": -0.12024968147277831, "compression_ratio": 1.893719806763285, "no_speech_prob": 1.6796334421087522e-06}, {"id": 541, "seek": 328592, "start": 3285.92, "end": 3295.28, "text": " case that is one minus uh the output of 10h square which here is h so it's h square and that is the", "tokens": [50364, 1389, 300, 307, 472, 3175, 2232, 264, 5598, 295, 1266, 71, 3732, 597, 510, 307, 276, 370, 309, 311, 276, 3732, 293, 300, 307, 264, 50832, 50832, 2654, 13760, 293, 550, 1413, 264, 5021, 4978, 274, 71, 370, 300, 307, 516, 281, 312, 527, 11532, 11420, 51212, 51212, 370, 498, 321, 808, 510, 293, 550, 8585, 518, 341, 718, 311, 1454, 337, 264, 1151, 293, 321, 362, 264, 558, 1867, 51608, 51668], "temperature": 0.0, "avg_logprob": -0.09629747029897329, "compression_ratio": 1.716763005780347, "no_speech_prob": 6.14402642895584e-06}, {"id": 542, "seek": 328592, "start": 3295.28, "end": 3302.88, "text": " local derivative and then times the chain rule dh so that is going to be our candidate implementation", "tokens": [50364, 1389, 300, 307, 472, 3175, 2232, 264, 5598, 295, 1266, 71, 3732, 597, 510, 307, 276, 370, 309, 311, 276, 3732, 293, 300, 307, 264, 50832, 50832, 2654, 13760, 293, 550, 1413, 264, 5021, 4978, 274, 71, 370, 300, 307, 516, 281, 312, 527, 11532, 11420, 51212, 51212, 370, 498, 321, 808, 510, 293, 550, 8585, 518, 341, 718, 311, 1454, 337, 264, 1151, 293, 321, 362, 264, 558, 1867, 51608, 51668], "temperature": 0.0, "avg_logprob": -0.09629747029897329, "compression_ratio": 1.716763005780347, "no_speech_prob": 6.14402642895584e-06}, {"id": 543, "seek": 328592, "start": 3302.88, "end": 3310.8, "text": " so if we come here and then uncomment this let's hope for the best and we have the right answer", "tokens": [50364, 1389, 300, 307, 472, 3175, 2232, 264, 5598, 295, 1266, 71, 3732, 597, 510, 307, 276, 370, 309, 311, 276, 3732, 293, 300, 307, 264, 50832, 50832, 2654, 13760, 293, 550, 1413, 264, 5021, 4978, 274, 71, 370, 300, 307, 516, 281, 312, 527, 11532, 11420, 51212, 51212, 370, 498, 321, 808, 510, 293, 550, 8585, 518, 341, 718, 311, 1454, 337, 264, 1151, 293, 321, 362, 264, 558, 1867, 51608, 51668], "temperature": 0.0, "avg_logprob": -0.09629747029897329, "compression_ratio": 1.716763005780347, "no_speech_prob": 6.14402642895584e-06}, {"id": 544, "seek": 331080, "start": 3310.8, "end": 3318.1600000000003, "text": " okay next up we have dh preact and we want to back propagate into the gain the b and raw and the b and", "tokens": [50364, 1392, 958, 493, 321, 362, 274, 71, 659, 578, 293, 321, 528, 281, 646, 48256, 666, 264, 6052, 264, 272, 293, 8936, 293, 264, 272, 293, 50732, 50732, 12577, 370, 510, 341, 307, 264, 15245, 7679, 9834, 272, 293, 6052, 293, 12577, 1854, 264, 15245, 7679, 300, 51024, 51024, 747, 264, 272, 293, 8936, 300, 307, 1900, 4985, 5959, 21948, 293, 436, 4373, 309, 293, 5513, 309, 293, 613, 366, 264, 51332, 51332, 9834, 295, 264, 15245, 2026, 586, 510, 321, 362, 257, 27290, 457, 309, 311, 3163, 26801, 300, 341, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.13546032152677837, "compression_ratio": 1.9257425742574257, "no_speech_prob": 1.76030516740866e-06}, {"id": 545, "seek": 331080, "start": 3318.1600000000003, "end": 3324.0, "text": " bias so here this is the batch storm parameters b and gain and bias inside the batch storm that", "tokens": [50364, 1392, 958, 493, 321, 362, 274, 71, 659, 578, 293, 321, 528, 281, 646, 48256, 666, 264, 6052, 264, 272, 293, 8936, 293, 264, 272, 293, 50732, 50732, 12577, 370, 510, 341, 307, 264, 15245, 7679, 9834, 272, 293, 6052, 293, 12577, 1854, 264, 15245, 7679, 300, 51024, 51024, 747, 264, 272, 293, 8936, 300, 307, 1900, 4985, 5959, 21948, 293, 436, 4373, 309, 293, 5513, 309, 293, 613, 366, 264, 51332, 51332, 9834, 295, 264, 15245, 2026, 586, 510, 321, 362, 257, 27290, 457, 309, 311, 3163, 26801, 300, 341, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.13546032152677837, "compression_ratio": 1.9257425742574257, "no_speech_prob": 1.76030516740866e-06}, {"id": 546, "seek": 331080, "start": 3324.0, "end": 3330.1600000000003, "text": " take the b and raw that is exact unit gaussian and they scale it and shift it and these are the", "tokens": [50364, 1392, 958, 493, 321, 362, 274, 71, 659, 578, 293, 321, 528, 281, 646, 48256, 666, 264, 6052, 264, 272, 293, 8936, 293, 264, 272, 293, 50732, 50732, 12577, 370, 510, 341, 307, 264, 15245, 7679, 9834, 272, 293, 6052, 293, 12577, 1854, 264, 15245, 7679, 300, 51024, 51024, 747, 264, 272, 293, 8936, 300, 307, 1900, 4985, 5959, 21948, 293, 436, 4373, 309, 293, 5513, 309, 293, 613, 366, 264, 51332, 51332, 9834, 295, 264, 15245, 2026, 586, 510, 321, 362, 257, 27290, 457, 309, 311, 3163, 26801, 300, 341, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.13546032152677837, "compression_ratio": 1.9257425742574257, "no_speech_prob": 1.76030516740866e-06}, {"id": 547, "seek": 331080, "start": 3330.1600000000003, "end": 3335.6000000000004, "text": " parameters of the batch norm now here we have a multiplication but it's worth noting that this", "tokens": [50364, 1392, 958, 493, 321, 362, 274, 71, 659, 578, 293, 321, 528, 281, 646, 48256, 666, 264, 6052, 264, 272, 293, 8936, 293, 264, 272, 293, 50732, 50732, 12577, 370, 510, 341, 307, 264, 15245, 7679, 9834, 272, 293, 6052, 293, 12577, 1854, 264, 15245, 7679, 300, 51024, 51024, 747, 264, 272, 293, 8936, 300, 307, 1900, 4985, 5959, 21948, 293, 436, 4373, 309, 293, 5513, 309, 293, 613, 366, 264, 51332, 51332, 9834, 295, 264, 15245, 2026, 586, 510, 321, 362, 257, 27290, 457, 309, 311, 3163, 26801, 300, 341, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.13546032152677837, "compression_ratio": 1.9257425742574257, "no_speech_prob": 1.76030516740866e-06}, {"id": 548, "seek": 333560, "start": 3335.6, "end": 3341.12, "text": " multiply is very very different from this matrix multiply here matrix multiply are dot products", "tokens": [50364, 12972, 307, 588, 588, 819, 490, 341, 8141, 12972, 510, 8141, 12972, 366, 5893, 3383, 50640, 50640, 1296, 13241, 293, 13766, 295, 613, 32284, 3288, 341, 307, 364, 4478, 12, 3711, 12972, 370, 721, 366, 50900, 50900, 1596, 257, 857, 18587, 586, 321, 360, 362, 281, 312, 5026, 365, 512, 295, 264, 30024, 2737, 294, 341, 51144, 51144, 1622, 295, 3089, 1673, 370, 291, 536, 577, 272, 293, 6052, 293, 272, 293, 12577, 366, 502, 538, 12145, 457, 276, 659, 578, 293, 272, 293, 8936, 51556, 51556, 366, 8858, 538, 12145, 370, 321, 362, 281, 312, 5026, 365, 300, 293, 652, 988, 300, 439, 264, 10854, 589, 484, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.062227998461042135, "compression_ratio": 1.8396946564885497, "no_speech_prob": 3.844644197670277e-06}, {"id": 549, "seek": 333560, "start": 3341.12, "end": 3346.3199999999997, "text": " between rows and columns of these matrices involved this is an element-wise multiply so things are", "tokens": [50364, 12972, 307, 588, 588, 819, 490, 341, 8141, 12972, 510, 8141, 12972, 366, 5893, 3383, 50640, 50640, 1296, 13241, 293, 13766, 295, 613, 32284, 3288, 341, 307, 364, 4478, 12, 3711, 12972, 370, 721, 366, 50900, 50900, 1596, 257, 857, 18587, 586, 321, 360, 362, 281, 312, 5026, 365, 512, 295, 264, 30024, 2737, 294, 341, 51144, 51144, 1622, 295, 3089, 1673, 370, 291, 536, 577, 272, 293, 6052, 293, 272, 293, 12577, 366, 502, 538, 12145, 457, 276, 659, 578, 293, 272, 293, 8936, 51556, 51556, 366, 8858, 538, 12145, 370, 321, 362, 281, 312, 5026, 365, 300, 293, 652, 988, 300, 439, 264, 10854, 589, 484, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.062227998461042135, "compression_ratio": 1.8396946564885497, "no_speech_prob": 3.844644197670277e-06}, {"id": 550, "seek": 333560, "start": 3346.3199999999997, "end": 3351.2, "text": " quite a bit simpler now we do have to be careful with some of the broadcasting happening in this", "tokens": [50364, 12972, 307, 588, 588, 819, 490, 341, 8141, 12972, 510, 8141, 12972, 366, 5893, 3383, 50640, 50640, 1296, 13241, 293, 13766, 295, 613, 32284, 3288, 341, 307, 364, 4478, 12, 3711, 12972, 370, 721, 366, 50900, 50900, 1596, 257, 857, 18587, 586, 321, 360, 362, 281, 312, 5026, 365, 512, 295, 264, 30024, 2737, 294, 341, 51144, 51144, 1622, 295, 3089, 1673, 370, 291, 536, 577, 272, 293, 6052, 293, 272, 293, 12577, 366, 502, 538, 12145, 457, 276, 659, 578, 293, 272, 293, 8936, 51556, 51556, 366, 8858, 538, 12145, 370, 321, 362, 281, 312, 5026, 365, 300, 293, 652, 988, 300, 439, 264, 10854, 589, 484, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.062227998461042135, "compression_ratio": 1.8396946564885497, "no_speech_prob": 3.844644197670277e-06}, {"id": 551, "seek": 333560, "start": 3351.2, "end": 3359.44, "text": " line of code though so you see how b and gain and b and bias are 1 by 64 but h preact and b and raw", "tokens": [50364, 12972, 307, 588, 588, 819, 490, 341, 8141, 12972, 510, 8141, 12972, 366, 5893, 3383, 50640, 50640, 1296, 13241, 293, 13766, 295, 613, 32284, 3288, 341, 307, 364, 4478, 12, 3711, 12972, 370, 721, 366, 50900, 50900, 1596, 257, 857, 18587, 586, 321, 360, 362, 281, 312, 5026, 365, 512, 295, 264, 30024, 2737, 294, 341, 51144, 51144, 1622, 295, 3089, 1673, 370, 291, 536, 577, 272, 293, 6052, 293, 272, 293, 12577, 366, 502, 538, 12145, 457, 276, 659, 578, 293, 272, 293, 8936, 51556, 51556, 366, 8858, 538, 12145, 370, 321, 362, 281, 312, 5026, 365, 300, 293, 652, 988, 300, 439, 264, 10854, 589, 484, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.062227998461042135, "compression_ratio": 1.8396946564885497, "no_speech_prob": 3.844644197670277e-06}, {"id": 552, "seek": 333560, "start": 3359.44, "end": 3365.44, "text": " are 32 by 64 so we have to be careful with that and make sure that all the shapes work out", "tokens": [50364, 12972, 307, 588, 588, 819, 490, 341, 8141, 12972, 510, 8141, 12972, 366, 5893, 3383, 50640, 50640, 1296, 13241, 293, 13766, 295, 613, 32284, 3288, 341, 307, 364, 4478, 12, 3711, 12972, 370, 721, 366, 50900, 50900, 1596, 257, 857, 18587, 586, 321, 360, 362, 281, 312, 5026, 365, 512, 295, 264, 30024, 2737, 294, 341, 51144, 51144, 1622, 295, 3089, 1673, 370, 291, 536, 577, 272, 293, 6052, 293, 272, 293, 12577, 366, 502, 538, 12145, 457, 276, 659, 578, 293, 272, 293, 8936, 51556, 51556, 366, 8858, 538, 12145, 370, 321, 362, 281, 312, 5026, 365, 300, 293, 652, 988, 300, 439, 264, 10854, 589, 484, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.062227998461042135, "compression_ratio": 1.8396946564885497, "no_speech_prob": 3.844644197670277e-06}, {"id": 553, "seek": 336544, "start": 3365.44, "end": 3370.16, "text": " fine and that the broadcasting is correctly back propagated so in particular let's start with db", "tokens": [50364, 2489, 293, 300, 264, 30024, 307, 8944, 646, 12425, 770, 370, 294, 1729, 718, 311, 722, 365, 274, 65, 50600, 50600, 293, 6052, 370, 274, 65, 293, 6052, 820, 312, 293, 510, 341, 307, 797, 4478, 12, 3711, 12972, 293, 5699, 321, 362, 51012, 51012, 257, 1413, 272, 6915, 269, 321, 1866, 300, 264, 2654, 13760, 510, 307, 445, 498, 341, 307, 257, 264, 2654, 13760, 307, 51300, 51300, 445, 264, 272, 264, 661, 472, 370, 264, 2654, 13760, 307, 445, 272, 293, 8936, 293, 550, 1413, 5021, 4978, 370, 274, 71, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.04566812018553416, "compression_ratio": 1.9086538461538463, "no_speech_prob": 5.954954758635722e-06}, {"id": 554, "seek": 336544, "start": 3370.16, "end": 3378.4, "text": " and gain so db and gain should be and here this is again element-wise multiply and whenever we have", "tokens": [50364, 2489, 293, 300, 264, 30024, 307, 8944, 646, 12425, 770, 370, 294, 1729, 718, 311, 722, 365, 274, 65, 50600, 50600, 293, 6052, 370, 274, 65, 293, 6052, 820, 312, 293, 510, 341, 307, 797, 4478, 12, 3711, 12972, 293, 5699, 321, 362, 51012, 51012, 257, 1413, 272, 6915, 269, 321, 1866, 300, 264, 2654, 13760, 510, 307, 445, 498, 341, 307, 257, 264, 2654, 13760, 307, 51300, 51300, 445, 264, 272, 264, 661, 472, 370, 264, 2654, 13760, 307, 445, 272, 293, 8936, 293, 550, 1413, 5021, 4978, 370, 274, 71, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.04566812018553416, "compression_ratio": 1.9086538461538463, "no_speech_prob": 5.954954758635722e-06}, {"id": 555, "seek": 336544, "start": 3378.4, "end": 3384.16, "text": " a times b equals c we saw that the local derivative here is just if this is a the local derivative is", "tokens": [50364, 2489, 293, 300, 264, 30024, 307, 8944, 646, 12425, 770, 370, 294, 1729, 718, 311, 722, 365, 274, 65, 50600, 50600, 293, 6052, 370, 274, 65, 293, 6052, 820, 312, 293, 510, 341, 307, 797, 4478, 12, 3711, 12972, 293, 5699, 321, 362, 51012, 51012, 257, 1413, 272, 6915, 269, 321, 1866, 300, 264, 2654, 13760, 510, 307, 445, 498, 341, 307, 257, 264, 2654, 13760, 307, 51300, 51300, 445, 264, 272, 264, 661, 472, 370, 264, 2654, 13760, 307, 445, 272, 293, 8936, 293, 550, 1413, 5021, 4978, 370, 274, 71, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.04566812018553416, "compression_ratio": 1.9086538461538463, "no_speech_prob": 5.954954758635722e-06}, {"id": 556, "seek": 336544, "start": 3384.16, "end": 3392.32, "text": " just the b the other one so the local derivative is just b and raw and then times chain rule so dh", "tokens": [50364, 2489, 293, 300, 264, 30024, 307, 8944, 646, 12425, 770, 370, 294, 1729, 718, 311, 722, 365, 274, 65, 50600, 50600, 293, 6052, 370, 274, 65, 293, 6052, 820, 312, 293, 510, 341, 307, 797, 4478, 12, 3711, 12972, 293, 5699, 321, 362, 51012, 51012, 257, 1413, 272, 6915, 269, 321, 1866, 300, 264, 2654, 13760, 510, 307, 445, 498, 341, 307, 257, 264, 2654, 13760, 307, 51300, 51300, 445, 264, 272, 264, 661, 472, 370, 264, 2654, 13760, 307, 445, 272, 293, 8936, 293, 550, 1413, 5021, 4978, 370, 274, 71, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.04566812018553416, "compression_ratio": 1.9086538461538463, "no_speech_prob": 5.954954758635722e-06}, {"id": 557, "seek": 339232, "start": 3392.32, "end": 3401.44, "text": " preact so this is the candidate gradient now again we have to be careful because b and gain", "tokens": [50364, 659, 578, 370, 341, 307, 264, 11532, 16235, 586, 797, 321, 362, 281, 312, 5026, 570, 272, 293, 6052, 50820, 50820, 307, 295, 2744, 502, 538, 12145, 457, 341, 510, 576, 312, 8858, 538, 12145, 293, 370, 264, 3006, 551, 281, 360, 294, 341, 1389, 295, 51316, 51316, 1164, 307, 300, 272, 293, 6052, 510, 307, 257, 4978, 8062, 295, 12145, 3547, 309, 2170, 46365, 28450, 294, 341, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.055051343308554754, "compression_ratio": 1.6235955056179776, "no_speech_prob": 2.1567759631579975e-06}, {"id": 558, "seek": 339232, "start": 3401.44, "end": 3411.36, "text": " is of size 1 by 64 but this here would be 32 by 64 and so the correct thing to do in this case of", "tokens": [50364, 659, 578, 370, 341, 307, 264, 11532, 16235, 586, 797, 321, 362, 281, 312, 5026, 570, 272, 293, 6052, 50820, 50820, 307, 295, 2744, 502, 538, 12145, 457, 341, 510, 576, 312, 8858, 538, 12145, 293, 370, 264, 3006, 551, 281, 360, 294, 341, 1389, 295, 51316, 51316, 1164, 307, 300, 272, 293, 6052, 510, 307, 257, 4978, 8062, 295, 12145, 3547, 309, 2170, 46365, 28450, 294, 341, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.055051343308554754, "compression_ratio": 1.6235955056179776, "no_speech_prob": 2.1567759631579975e-06}, {"id": 559, "seek": 339232, "start": 3411.36, "end": 3417.44, "text": " course is that b and gain here is a rule vector of 64 numbers it gets replicated vertically in this", "tokens": [50364, 659, 578, 370, 341, 307, 264, 11532, 16235, 586, 797, 321, 362, 281, 312, 5026, 570, 272, 293, 6052, 50820, 50820, 307, 295, 2744, 502, 538, 12145, 457, 341, 510, 576, 312, 8858, 538, 12145, 293, 370, 264, 3006, 551, 281, 360, 294, 341, 1389, 295, 51316, 51316, 1164, 307, 300, 272, 293, 6052, 510, 307, 257, 4978, 8062, 295, 12145, 3547, 309, 2170, 46365, 28450, 294, 341, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.055051343308554754, "compression_ratio": 1.6235955056179776, "no_speech_prob": 2.1567759631579975e-06}, {"id": 560, "seek": 341744, "start": 3417.44, "end": 3423.12, "text": " operation and so therefore the correct thing to do is to sum because it's being replicated", "tokens": [50364, 6916, 293, 370, 4412, 264, 3006, 551, 281, 360, 307, 281, 2408, 570, 309, 311, 885, 46365, 50648, 50700, 293, 4412, 439, 264, 2771, 2448, 294, 1184, 295, 264, 13241, 300, 366, 586, 13974, 12204, 643, 281, 2408, 50948, 50948, 493, 281, 300, 912, 40863, 274, 65, 293, 6052, 370, 498, 264, 2408, 2108, 439, 264, 4018, 439, 264, 5110, 1936, 51352, 51384, 597, 307, 264, 3513, 597, 445, 2170, 46365, 293, 586, 321, 362, 281, 312, 611, 5026, 570, 51596, 51632], "temperature": 0.0, "avg_logprob": -0.05819702716100784, "compression_ratio": 1.7641509433962264, "no_speech_prob": 1.349676495010499e-06}, {"id": 561, "seek": 341744, "start": 3424.16, "end": 3429.12, "text": " and therefore all the gradients in each of the rows that are now flowing backwards need to sum", "tokens": [50364, 6916, 293, 370, 4412, 264, 3006, 551, 281, 360, 307, 281, 2408, 570, 309, 311, 885, 46365, 50648, 50700, 293, 4412, 439, 264, 2771, 2448, 294, 1184, 295, 264, 13241, 300, 366, 586, 13974, 12204, 643, 281, 2408, 50948, 50948, 493, 281, 300, 912, 40863, 274, 65, 293, 6052, 370, 498, 264, 2408, 2108, 439, 264, 4018, 439, 264, 5110, 1936, 51352, 51384, 597, 307, 264, 3513, 597, 445, 2170, 46365, 293, 586, 321, 362, 281, 312, 611, 5026, 570, 51596, 51632], "temperature": 0.0, "avg_logprob": -0.05819702716100784, "compression_ratio": 1.7641509433962264, "no_speech_prob": 1.349676495010499e-06}, {"id": 562, "seek": 341744, "start": 3429.12, "end": 3437.2000000000003, "text": " up to that same tensor db and gain so if the sum across all the zero all the examples basically", "tokens": [50364, 6916, 293, 370, 4412, 264, 3006, 551, 281, 360, 307, 281, 2408, 570, 309, 311, 885, 46365, 50648, 50700, 293, 4412, 439, 264, 2771, 2448, 294, 1184, 295, 264, 13241, 300, 366, 586, 13974, 12204, 643, 281, 2408, 50948, 50948, 493, 281, 300, 912, 40863, 274, 65, 293, 6052, 370, 498, 264, 2408, 2108, 439, 264, 4018, 439, 264, 5110, 1936, 51352, 51384, 597, 307, 264, 3513, 597, 445, 2170, 46365, 293, 586, 321, 362, 281, 312, 611, 5026, 570, 51596, 51632], "temperature": 0.0, "avg_logprob": -0.05819702716100784, "compression_ratio": 1.7641509433962264, "no_speech_prob": 1.349676495010499e-06}, {"id": 563, "seek": 341744, "start": 3437.84, "end": 3442.08, "text": " which is the direction which just gets replicated and now we have to be also careful because", "tokens": [50364, 6916, 293, 370, 4412, 264, 3006, 551, 281, 360, 307, 281, 2408, 570, 309, 311, 885, 46365, 50648, 50700, 293, 4412, 439, 264, 2771, 2448, 294, 1184, 295, 264, 13241, 300, 366, 586, 13974, 12204, 643, 281, 2408, 50948, 50948, 493, 281, 300, 912, 40863, 274, 65, 293, 6052, 370, 498, 264, 2408, 2108, 439, 264, 4018, 439, 264, 5110, 1936, 51352, 51384, 597, 307, 264, 3513, 597, 445, 2170, 46365, 293, 586, 321, 362, 281, 312, 611, 5026, 570, 51596, 51632], "temperature": 0.0, "avg_logprob": -0.05819702716100784, "compression_ratio": 1.7641509433962264, "no_speech_prob": 1.349676495010499e-06}, {"id": 564, "seek": 344208, "start": 3442.08, "end": 3450.24, "text": " we um being gain is of shape 1 by 64 so in fact i need to keep them as true otherwise i would just", "tokens": [50364, 321, 1105, 885, 6052, 307, 295, 3909, 502, 538, 12145, 370, 294, 1186, 741, 643, 281, 1066, 552, 382, 2074, 5911, 741, 576, 445, 50772, 50772, 483, 12145, 586, 741, 500, 380, 767, 534, 1604, 983, 264, 885, 12634, 293, 885, 28035, 741, 1027, 552, 312, 502, 51144, 51144, 538, 12145, 1105, 457, 264, 32152, 272, 16, 293, 272, 17, 741, 445, 1027, 552, 312, 472, 18795, 18875, 436, 434, 406, 732, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.13152828969453512, "compression_ratio": 1.6348314606741574, "no_speech_prob": 1.130054715758888e-05}, {"id": 565, "seek": 344208, "start": 3450.24, "end": 3457.68, "text": " get 64 now i don't actually really remember why the being gained and being biased i made them be 1", "tokens": [50364, 321, 1105, 885, 6052, 307, 295, 3909, 502, 538, 12145, 370, 294, 1186, 741, 643, 281, 1066, 552, 382, 2074, 5911, 741, 576, 445, 50772, 50772, 483, 12145, 586, 741, 500, 380, 767, 534, 1604, 983, 264, 885, 12634, 293, 885, 28035, 741, 1027, 552, 312, 502, 51144, 51144, 538, 12145, 1105, 457, 264, 32152, 272, 16, 293, 272, 17, 741, 445, 1027, 552, 312, 472, 18795, 18875, 436, 434, 406, 732, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.13152828969453512, "compression_ratio": 1.6348314606741574, "no_speech_prob": 1.130054715758888e-05}, {"id": 566, "seek": 344208, "start": 3457.68, "end": 3465.84, "text": " by 64 um but the biases b1 and b2 i just made them be one dimensional vectors they're not two", "tokens": [50364, 321, 1105, 885, 6052, 307, 295, 3909, 502, 538, 12145, 370, 294, 1186, 741, 643, 281, 1066, 552, 382, 2074, 5911, 741, 576, 445, 50772, 50772, 483, 12145, 586, 741, 500, 380, 767, 534, 1604, 983, 264, 885, 12634, 293, 885, 28035, 741, 1027, 552, 312, 502, 51144, 51144, 538, 12145, 1105, 457, 264, 32152, 272, 16, 293, 272, 17, 741, 445, 1027, 552, 312, 472, 18795, 18875, 436, 434, 406, 732, 51552, 51552], "temperature": 0.0, "avg_logprob": -0.13152828969453512, "compression_ratio": 1.6348314606741574, "no_speech_prob": 1.130054715758888e-05}, {"id": 567, "seek": 346584, "start": 3465.84, "end": 3472.7200000000003, "text": " dimensional tensors so i can't recall exactly why i left the gain and the bias as two dimensional", "tokens": [50364, 18795, 10688, 830, 370, 741, 393, 380, 9901, 2293, 983, 741, 1411, 264, 6052, 293, 264, 12577, 382, 732, 18795, 50708, 50708, 457, 309, 1177, 380, 534, 1871, 382, 938, 382, 291, 366, 8398, 293, 291, 434, 5145, 309, 264, 912, 50868, 50900, 370, 294, 341, 1389, 321, 528, 281, 1066, 264, 10139, 370, 300, 264, 40863, 10854, 589, 51052, 51140, 958, 493, 321, 362, 272, 293, 8936, 370, 274, 65, 293, 8936, 486, 312, 272, 293, 6052, 30955, 274, 71, 4515, 300, 311, 527, 5021, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.07449367311265734, "compression_ratio": 1.7524271844660195, "no_speech_prob": 2.7693590709532145e-06}, {"id": 568, "seek": 346584, "start": 3472.7200000000003, "end": 3475.92, "text": " but it doesn't really matter as long as you are consistent and you're keeping it the same", "tokens": [50364, 18795, 10688, 830, 370, 741, 393, 380, 9901, 2293, 983, 741, 1411, 264, 6052, 293, 264, 12577, 382, 732, 18795, 50708, 50708, 457, 309, 1177, 380, 534, 1871, 382, 938, 382, 291, 366, 8398, 293, 291, 434, 5145, 309, 264, 912, 50868, 50900, 370, 294, 341, 1389, 321, 528, 281, 1066, 264, 10139, 370, 300, 264, 40863, 10854, 589, 51052, 51140, 958, 493, 321, 362, 272, 293, 8936, 370, 274, 65, 293, 8936, 486, 312, 272, 293, 6052, 30955, 274, 71, 4515, 300, 311, 527, 5021, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.07449367311265734, "compression_ratio": 1.7524271844660195, "no_speech_prob": 2.7693590709532145e-06}, {"id": 569, "seek": 346584, "start": 3476.56, "end": 3479.6000000000004, "text": " so in this case we want to keep the dimension so that the tensor shapes work", "tokens": [50364, 18795, 10688, 830, 370, 741, 393, 380, 9901, 2293, 983, 741, 1411, 264, 6052, 293, 264, 12577, 382, 732, 18795, 50708, 50708, 457, 309, 1177, 380, 534, 1871, 382, 938, 382, 291, 366, 8398, 293, 291, 434, 5145, 309, 264, 912, 50868, 50900, 370, 294, 341, 1389, 321, 528, 281, 1066, 264, 10139, 370, 300, 264, 40863, 10854, 589, 51052, 51140, 958, 493, 321, 362, 272, 293, 8936, 370, 274, 65, 293, 8936, 486, 312, 272, 293, 6052, 30955, 274, 71, 4515, 300, 311, 527, 5021, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.07449367311265734, "compression_ratio": 1.7524271844660195, "no_speech_prob": 2.7693590709532145e-06}, {"id": 570, "seek": 346584, "start": 3481.36, "end": 3493.92, "text": " next up we have b and raw so db and raw will be b and gain multiplying dh react that's our chain", "tokens": [50364, 18795, 10688, 830, 370, 741, 393, 380, 9901, 2293, 983, 741, 1411, 264, 6052, 293, 264, 12577, 382, 732, 18795, 50708, 50708, 457, 309, 1177, 380, 534, 1871, 382, 938, 382, 291, 366, 8398, 293, 291, 434, 5145, 309, 264, 912, 50868, 50900, 370, 294, 341, 1389, 321, 528, 281, 1066, 264, 10139, 370, 300, 264, 40863, 10854, 589, 51052, 51140, 958, 493, 321, 362, 272, 293, 8936, 370, 274, 65, 293, 8936, 486, 312, 272, 293, 6052, 30955, 274, 71, 4515, 300, 311, 527, 5021, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.07449367311265734, "compression_ratio": 1.7524271844660195, "no_speech_prob": 2.7693590709532145e-06}, {"id": 571, "seek": 349392, "start": 3493.92, "end": 3503.52, "text": " rule now what about the um dimensions of this we have to be careful right so dh preact is 32 by 64", "tokens": [50364, 4978, 586, 437, 466, 264, 1105, 12819, 295, 341, 321, 362, 281, 312, 5026, 558, 370, 274, 71, 659, 578, 307, 8858, 538, 12145, 50844, 50872, 272, 293, 6052, 307, 502, 538, 12145, 370, 309, 486, 445, 483, 46365, 293, 281, 1884, 341, 2232, 27290, 51216, 51216, 597, 307, 264, 3006, 551, 570, 294, 257, 2128, 1320, 309, 611, 2170, 46365, 294, 445, 264, 912, 636, 51428, 51460, 370, 294, 1186, 321, 500, 380, 643, 264, 26179, 510, 321, 434, 1096, 293, 264, 10854, 366, 1217, 3006, 293, 2721, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.08394313627673734, "compression_ratio": 1.668103448275862, "no_speech_prob": 1.6280278032354545e-06}, {"id": 572, "seek": 349392, "start": 3504.08, "end": 3510.96, "text": " b and gain is 1 by 64 so it will just get replicated and to create this uh multiplication", "tokens": [50364, 4978, 586, 437, 466, 264, 1105, 12819, 295, 341, 321, 362, 281, 312, 5026, 558, 370, 274, 71, 659, 578, 307, 8858, 538, 12145, 50844, 50872, 272, 293, 6052, 307, 502, 538, 12145, 370, 309, 486, 445, 483, 46365, 293, 281, 1884, 341, 2232, 27290, 51216, 51216, 597, 307, 264, 3006, 551, 570, 294, 257, 2128, 1320, 309, 611, 2170, 46365, 294, 445, 264, 912, 636, 51428, 51460, 370, 294, 1186, 321, 500, 380, 643, 264, 26179, 510, 321, 434, 1096, 293, 264, 10854, 366, 1217, 3006, 293, 2721, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.08394313627673734, "compression_ratio": 1.668103448275862, "no_speech_prob": 1.6280278032354545e-06}, {"id": 573, "seek": 349392, "start": 3510.96, "end": 3515.2000000000003, "text": " which is the correct thing because in a forward pass it also gets replicated in just the same way", "tokens": [50364, 4978, 586, 437, 466, 264, 1105, 12819, 295, 341, 321, 362, 281, 312, 5026, 558, 370, 274, 71, 659, 578, 307, 8858, 538, 12145, 50844, 50872, 272, 293, 6052, 307, 502, 538, 12145, 370, 309, 486, 445, 483, 46365, 293, 281, 1884, 341, 2232, 27290, 51216, 51216, 597, 307, 264, 3006, 551, 570, 294, 257, 2128, 1320, 309, 611, 2170, 46365, 294, 445, 264, 912, 636, 51428, 51460, 370, 294, 1186, 321, 500, 380, 643, 264, 26179, 510, 321, 434, 1096, 293, 264, 10854, 366, 1217, 3006, 293, 2721, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.08394313627673734, "compression_ratio": 1.668103448275862, "no_speech_prob": 1.6280278032354545e-06}, {"id": 574, "seek": 349392, "start": 3515.84, "end": 3521.44, "text": " so in fact we don't need the brackets here we're done and the shapes are already correct and finally", "tokens": [50364, 4978, 586, 437, 466, 264, 1105, 12819, 295, 341, 321, 362, 281, 312, 5026, 558, 370, 274, 71, 659, 578, 307, 8858, 538, 12145, 50844, 50872, 272, 293, 6052, 307, 502, 538, 12145, 370, 309, 486, 445, 483, 46365, 293, 281, 1884, 341, 2232, 27290, 51216, 51216, 597, 307, 264, 3006, 551, 570, 294, 257, 2128, 1320, 309, 611, 2170, 46365, 294, 445, 264, 912, 636, 51428, 51460, 370, 294, 1186, 321, 500, 380, 643, 264, 26179, 510, 321, 434, 1096, 293, 264, 10854, 366, 1217, 3006, 293, 2721, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.08394313627673734, "compression_ratio": 1.668103448275862, "no_speech_prob": 1.6280278032354545e-06}, {"id": 575, "seek": 352144, "start": 3521.44, "end": 3528.88, "text": " for the bias very similar this bias here is very very similar to the bias we saw in the linear layer", "tokens": [50364, 337, 264, 12577, 588, 2531, 341, 12577, 510, 307, 588, 588, 2531, 281, 264, 12577, 321, 1866, 294, 264, 8213, 4583, 50736, 50736, 293, 321, 536, 300, 264, 2771, 2448, 490, 276, 659, 578, 486, 2935, 3095, 666, 264, 32152, 293, 909, 493, 570, 51016, 51016, 613, 366, 445, 613, 366, 445, 39457, 1385, 293, 370, 1936, 321, 528, 341, 281, 312, 274, 71, 659, 578, 457, 309, 51316, 51316, 2203, 281, 2408, 2051, 264, 558, 10139, 293, 294, 341, 1389, 2531, 281, 264, 6052, 321, 643, 281, 2408, 2108, 264, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.04936839655825966, "compression_ratio": 1.935323383084577, "no_speech_prob": 4.936862296744948e-06}, {"id": 576, "seek": 352144, "start": 3528.88, "end": 3534.48, "text": " and we see that the gradients from h preact will simply flow into the biases and add up because", "tokens": [50364, 337, 264, 12577, 588, 2531, 341, 12577, 510, 307, 588, 588, 2531, 281, 264, 12577, 321, 1866, 294, 264, 8213, 4583, 50736, 50736, 293, 321, 536, 300, 264, 2771, 2448, 490, 276, 659, 578, 486, 2935, 3095, 666, 264, 32152, 293, 909, 493, 570, 51016, 51016, 613, 366, 445, 613, 366, 445, 39457, 1385, 293, 370, 1936, 321, 528, 341, 281, 312, 274, 71, 659, 578, 457, 309, 51316, 51316, 2203, 281, 2408, 2051, 264, 558, 10139, 293, 294, 341, 1389, 2531, 281, 264, 6052, 321, 643, 281, 2408, 2108, 264, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.04936839655825966, "compression_ratio": 1.935323383084577, "no_speech_prob": 4.936862296744948e-06}, {"id": 577, "seek": 352144, "start": 3534.48, "end": 3540.48, "text": " these are just these are just offsets and so basically we want this to be dh preact but it", "tokens": [50364, 337, 264, 12577, 588, 2531, 341, 12577, 510, 307, 588, 588, 2531, 281, 264, 12577, 321, 1866, 294, 264, 8213, 4583, 50736, 50736, 293, 321, 536, 300, 264, 2771, 2448, 490, 276, 659, 578, 486, 2935, 3095, 666, 264, 32152, 293, 909, 493, 570, 51016, 51016, 613, 366, 445, 613, 366, 445, 39457, 1385, 293, 370, 1936, 321, 528, 341, 281, 312, 274, 71, 659, 578, 457, 309, 51316, 51316, 2203, 281, 2408, 2051, 264, 558, 10139, 293, 294, 341, 1389, 2531, 281, 264, 6052, 321, 643, 281, 2408, 2108, 264, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.04936839655825966, "compression_ratio": 1.935323383084577, "no_speech_prob": 4.936862296744948e-06}, {"id": 578, "seek": 352144, "start": 3540.48, "end": 3546.2400000000002, "text": " needs to sum along the right dimension and in this case similar to the gain we need to sum across the", "tokens": [50364, 337, 264, 12577, 588, 2531, 341, 12577, 510, 307, 588, 588, 2531, 281, 264, 12577, 321, 1866, 294, 264, 8213, 4583, 50736, 50736, 293, 321, 536, 300, 264, 2771, 2448, 490, 276, 659, 578, 486, 2935, 3095, 666, 264, 32152, 293, 909, 493, 570, 51016, 51016, 613, 366, 445, 613, 366, 445, 39457, 1385, 293, 370, 1936, 321, 528, 341, 281, 312, 274, 71, 659, 578, 457, 309, 51316, 51316, 2203, 281, 2408, 2051, 264, 558, 10139, 293, 294, 341, 1389, 2531, 281, 264, 6052, 321, 643, 281, 2408, 2108, 264, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.04936839655825966, "compression_ratio": 1.935323383084577, "no_speech_prob": 4.936862296744948e-06}, {"id": 579, "seek": 354624, "start": 3546.24, "end": 3552.24, "text": " zeroth dimension the examples because of the way that the bias gets replicated vertically and we", "tokens": [50364, 44746, 900, 10139, 264, 5110, 570, 295, 264, 636, 300, 264, 12577, 2170, 46365, 28450, 293, 321, 50664, 50664, 611, 528, 281, 362, 1066, 552, 382, 2074, 293, 370, 341, 486, 1936, 747, 341, 293, 2408, 309, 493, 293, 976, 50976, 50976, 505, 257, 502, 538, 12145, 370, 341, 307, 264, 11532, 11420, 309, 1669, 439, 264, 10854, 589, 718, 385, 1565, 309, 493, 51376, 51408, 760, 510, 293, 550, 718, 385, 8585, 518, 613, 1045, 3876, 281, 1520, 300, 321, 366, 1242, 264, 3006, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.04936667238728384, "compression_ratio": 1.6869565217391305, "no_speech_prob": 2.5215292680513812e-06}, {"id": 580, "seek": 354624, "start": 3552.24, "end": 3558.4799999999996, "text": " also want to have keep them as true and so this will basically take this and sum it up and give", "tokens": [50364, 44746, 900, 10139, 264, 5110, 570, 295, 264, 636, 300, 264, 12577, 2170, 46365, 28450, 293, 321, 50664, 50664, 611, 528, 281, 362, 1066, 552, 382, 2074, 293, 370, 341, 486, 1936, 747, 341, 293, 2408, 309, 493, 293, 976, 50976, 50976, 505, 257, 502, 538, 12145, 370, 341, 307, 264, 11532, 11420, 309, 1669, 439, 264, 10854, 589, 718, 385, 1565, 309, 493, 51376, 51408, 760, 510, 293, 550, 718, 385, 8585, 518, 613, 1045, 3876, 281, 1520, 300, 321, 366, 1242, 264, 3006, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.04936667238728384, "compression_ratio": 1.6869565217391305, "no_speech_prob": 2.5215292680513812e-06}, {"id": 581, "seek": 354624, "start": 3558.4799999999996, "end": 3566.4799999999996, "text": " us a 1 by 64 so this is the candidate implementation it makes all the shapes work let me bring it up", "tokens": [50364, 44746, 900, 10139, 264, 5110, 570, 295, 264, 636, 300, 264, 12577, 2170, 46365, 28450, 293, 321, 50664, 50664, 611, 528, 281, 362, 1066, 552, 382, 2074, 293, 370, 341, 486, 1936, 747, 341, 293, 2408, 309, 493, 293, 976, 50976, 50976, 505, 257, 502, 538, 12145, 370, 341, 307, 264, 11532, 11420, 309, 1669, 439, 264, 10854, 589, 718, 385, 1565, 309, 493, 51376, 51408, 760, 510, 293, 550, 718, 385, 8585, 518, 613, 1045, 3876, 281, 1520, 300, 321, 366, 1242, 264, 3006, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.04936667238728384, "compression_ratio": 1.6869565217391305, "no_speech_prob": 2.5215292680513812e-06}, {"id": 582, "seek": 354624, "start": 3567.12, "end": 3573.4399999999996, "text": " down here and then let me uncomment these three lines to check that we are getting the correct", "tokens": [50364, 44746, 900, 10139, 264, 5110, 570, 295, 264, 636, 300, 264, 12577, 2170, 46365, 28450, 293, 321, 50664, 50664, 611, 528, 281, 362, 1066, 552, 382, 2074, 293, 370, 341, 486, 1936, 747, 341, 293, 2408, 309, 493, 293, 976, 50976, 50976, 505, 257, 502, 538, 12145, 370, 341, 307, 264, 11532, 11420, 309, 1669, 439, 264, 10854, 589, 718, 385, 1565, 309, 493, 51376, 51408, 760, 510, 293, 550, 718, 385, 8585, 518, 613, 1045, 3876, 281, 1520, 300, 321, 366, 1242, 264, 3006, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.04936667238728384, "compression_ratio": 1.6869565217391305, "no_speech_prob": 2.5215292680513812e-06}, {"id": 583, "seek": 357344, "start": 3573.44, "end": 3579.68, "text": " result for all the three tensors and indeed we see that all of that got back propagated correctly", "tokens": [50364, 1874, 337, 439, 264, 1045, 10688, 830, 293, 6451, 321, 536, 300, 439, 295, 300, 658, 646, 12425, 770, 8944, 50676, 50676, 370, 586, 321, 483, 281, 264, 15245, 2026, 4583, 321, 536, 577, 510, 272, 77, 6052, 293, 272, 77, 12577, 366, 264, 9834, 370, 264, 50956, 50956, 646, 38377, 5314, 457, 272, 77, 8936, 586, 307, 264, 5598, 295, 264, 3832, 2144, 370, 510, 437, 741, 478, 884, 295, 51332, 51332, 1164, 307, 741, 478, 7697, 493, 264, 15245, 2026, 666, 38798, 3755, 370, 321, 393, 646, 48256, 807, 51516, 51516], "temperature": 0.0, "avg_logprob": -0.08952962975752982, "compression_ratio": 1.8325581395348838, "no_speech_prob": 5.771410087618278e-06}, {"id": 584, "seek": 357344, "start": 3579.68, "end": 3585.28, "text": " so now we get to the batch norm layer we see how here bn gain and bn bias are the parameters so the", "tokens": [50364, 1874, 337, 439, 264, 1045, 10688, 830, 293, 6451, 321, 536, 300, 439, 295, 300, 658, 646, 12425, 770, 8944, 50676, 50676, 370, 586, 321, 483, 281, 264, 15245, 2026, 4583, 321, 536, 577, 510, 272, 77, 6052, 293, 272, 77, 12577, 366, 264, 9834, 370, 264, 50956, 50956, 646, 38377, 5314, 457, 272, 77, 8936, 586, 307, 264, 5598, 295, 264, 3832, 2144, 370, 510, 437, 741, 478, 884, 295, 51332, 51332, 1164, 307, 741, 478, 7697, 493, 264, 15245, 2026, 666, 38798, 3755, 370, 321, 393, 646, 48256, 807, 51516, 51516], "temperature": 0.0, "avg_logprob": -0.08952962975752982, "compression_ratio": 1.8325581395348838, "no_speech_prob": 5.771410087618278e-06}, {"id": 585, "seek": 357344, "start": 3585.28, "end": 3592.8, "text": " back propagation ends but bn raw now is the output of the standardization so here what i'm doing of", "tokens": [50364, 1874, 337, 439, 264, 1045, 10688, 830, 293, 6451, 321, 536, 300, 439, 295, 300, 658, 646, 12425, 770, 8944, 50676, 50676, 370, 586, 321, 483, 281, 264, 15245, 2026, 4583, 321, 536, 577, 510, 272, 77, 6052, 293, 272, 77, 12577, 366, 264, 9834, 370, 264, 50956, 50956, 646, 38377, 5314, 457, 272, 77, 8936, 586, 307, 264, 5598, 295, 264, 3832, 2144, 370, 510, 437, 741, 478, 884, 295, 51332, 51332, 1164, 307, 741, 478, 7697, 493, 264, 15245, 2026, 666, 38798, 3755, 370, 321, 393, 646, 48256, 807, 51516, 51516], "temperature": 0.0, "avg_logprob": -0.08952962975752982, "compression_ratio": 1.8325581395348838, "no_speech_prob": 5.771410087618278e-06}, {"id": 586, "seek": 357344, "start": 3592.8, "end": 3596.48, "text": " course is i'm breaking up the batch norm into manageable pieces so we can back propagate through", "tokens": [50364, 1874, 337, 439, 264, 1045, 10688, 830, 293, 6451, 321, 536, 300, 439, 295, 300, 658, 646, 12425, 770, 8944, 50676, 50676, 370, 586, 321, 483, 281, 264, 15245, 2026, 4583, 321, 536, 577, 510, 272, 77, 6052, 293, 272, 77, 12577, 366, 264, 9834, 370, 264, 50956, 50956, 646, 38377, 5314, 457, 272, 77, 8936, 586, 307, 264, 5598, 295, 264, 3832, 2144, 370, 510, 437, 741, 478, 884, 295, 51332, 51332, 1164, 307, 741, 478, 7697, 493, 264, 15245, 2026, 666, 38798, 3755, 370, 321, 393, 646, 48256, 807, 51516, 51516], "temperature": 0.0, "avg_logprob": -0.08952962975752982, "compression_ratio": 1.8325581395348838, "no_speech_prob": 5.771410087618278e-06}, {"id": 587, "seek": 359648, "start": 3596.48, "end": 3604.56, "text": " each line individually but basically what's happening is bn mean i is the sum so this is", "tokens": [50364, 1184, 1622, 16652, 457, 1936, 437, 311, 2737, 307, 272, 77, 914, 741, 307, 264, 2408, 370, 341, 307, 50768, 50768, 264, 272, 77, 914, 741, 741, 12328, 337, 264, 7006, 25290, 272, 77, 7593, 307, 2031, 3175, 2992, 272, 77, 7593, 568, 307, 2031, 3175, 2992, 51240, 51240, 8889, 510, 1854, 264, 21977, 272, 77, 1374, 307, 264, 21977, 370, 12771, 3732, 341, 307, 272, 77, 1374, 293, 309, 311, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.061995232899983724, "compression_ratio": 1.794871794871795, "no_speech_prob": 5.862533726030961e-06}, {"id": 588, "seek": 359648, "start": 3604.56, "end": 3614.0, "text": " the bn mean i i apologize for the variable naming bn diff is x minus mu bn diff 2 is x minus mu", "tokens": [50364, 1184, 1622, 16652, 457, 1936, 437, 311, 2737, 307, 272, 77, 914, 741, 307, 264, 2408, 370, 341, 307, 50768, 50768, 264, 272, 77, 914, 741, 741, 12328, 337, 264, 7006, 25290, 272, 77, 7593, 307, 2031, 3175, 2992, 272, 77, 7593, 568, 307, 2031, 3175, 2992, 51240, 51240, 8889, 510, 1854, 264, 21977, 272, 77, 1374, 307, 264, 21977, 370, 12771, 3732, 341, 307, 272, 77, 1374, 293, 309, 311, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.061995232899983724, "compression_ratio": 1.794871794871795, "no_speech_prob": 5.862533726030961e-06}, {"id": 589, "seek": 359648, "start": 3614.0, "end": 3622.56, "text": " squared here inside the variance bn var is the variance so sigma square this is bn var and it's", "tokens": [50364, 1184, 1622, 16652, 457, 1936, 437, 311, 2737, 307, 272, 77, 914, 741, 307, 264, 2408, 370, 341, 307, 50768, 50768, 264, 272, 77, 914, 741, 741, 12328, 337, 264, 7006, 25290, 272, 77, 7593, 307, 2031, 3175, 2992, 272, 77, 7593, 568, 307, 2031, 3175, 2992, 51240, 51240, 8889, 510, 1854, 264, 21977, 272, 77, 1374, 307, 264, 21977, 370, 12771, 3732, 341, 307, 272, 77, 1374, 293, 309, 311, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.061995232899983724, "compression_ratio": 1.794871794871795, "no_speech_prob": 5.862533726030961e-06}, {"id": 590, "seek": 362256, "start": 3622.56, "end": 3630.0, "text": " basically the sum of squares so this is the x minus mu squared and then the sum now you'll", "tokens": [50364, 1936, 264, 2408, 295, 19368, 370, 341, 307, 264, 2031, 3175, 2992, 8889, 293, 550, 264, 2408, 586, 291, 603, 50736, 50736, 3449, 472, 25866, 510, 510, 309, 307, 48704, 382, 472, 670, 275, 597, 307, 264, 1230, 295, 5110, 510, 51112, 51112, 741, 669, 2710, 3319, 382, 472, 670, 297, 3175, 472, 2602, 295, 297, 293, 341, 307, 30515, 293, 741, 603, 808, 646, 51348, 51348, 281, 300, 294, 257, 857, 562, 321, 366, 412, 341, 1622, 309, 307, 746, 1219, 264, 1151, 1329, 19984, 51588, 51628], "temperature": 0.0, "avg_logprob": -0.07126128408643935, "compression_ratio": 1.7942583732057416, "no_speech_prob": 3.7265674563968787e-06}, {"id": 591, "seek": 362256, "start": 3630.0, "end": 3637.52, "text": " notice one departure here here it is normalized as one over m which is the number of examples here", "tokens": [50364, 1936, 264, 2408, 295, 19368, 370, 341, 307, 264, 2031, 3175, 2992, 8889, 293, 550, 264, 2408, 586, 291, 603, 50736, 50736, 3449, 472, 25866, 510, 510, 309, 307, 48704, 382, 472, 670, 275, 597, 307, 264, 1230, 295, 5110, 510, 51112, 51112, 741, 669, 2710, 3319, 382, 472, 670, 297, 3175, 472, 2602, 295, 297, 293, 341, 307, 30515, 293, 741, 603, 808, 646, 51348, 51348, 281, 300, 294, 257, 857, 562, 321, 366, 412, 341, 1622, 309, 307, 746, 1219, 264, 1151, 1329, 19984, 51588, 51628], "temperature": 0.0, "avg_logprob": -0.07126128408643935, "compression_ratio": 1.7942583732057416, "no_speech_prob": 3.7265674563968787e-06}, {"id": 592, "seek": 362256, "start": 3637.52, "end": 3642.24, "text": " i am normalizing as one over n minus one instead of n and this is deliberate and i'll come back", "tokens": [50364, 1936, 264, 2408, 295, 19368, 370, 341, 307, 264, 2031, 3175, 2992, 8889, 293, 550, 264, 2408, 586, 291, 603, 50736, 50736, 3449, 472, 25866, 510, 510, 309, 307, 48704, 382, 472, 670, 275, 597, 307, 264, 1230, 295, 5110, 510, 51112, 51112, 741, 669, 2710, 3319, 382, 472, 670, 297, 3175, 472, 2602, 295, 297, 293, 341, 307, 30515, 293, 741, 603, 808, 646, 51348, 51348, 281, 300, 294, 257, 857, 562, 321, 366, 412, 341, 1622, 309, 307, 746, 1219, 264, 1151, 1329, 19984, 51588, 51628], "temperature": 0.0, "avg_logprob": -0.07126128408643935, "compression_ratio": 1.7942583732057416, "no_speech_prob": 3.7265674563968787e-06}, {"id": 593, "seek": 362256, "start": 3642.24, "end": 3647.04, "text": " to that in a bit when we are at this line it is something called the best list correction", "tokens": [50364, 1936, 264, 2408, 295, 19368, 370, 341, 307, 264, 2031, 3175, 2992, 8889, 293, 550, 264, 2408, 586, 291, 603, 50736, 50736, 3449, 472, 25866, 510, 510, 309, 307, 48704, 382, 472, 670, 275, 597, 307, 264, 1230, 295, 5110, 510, 51112, 51112, 741, 669, 2710, 3319, 382, 472, 670, 297, 3175, 472, 2602, 295, 297, 293, 341, 307, 30515, 293, 741, 603, 808, 646, 51348, 51348, 281, 300, 294, 257, 857, 562, 321, 366, 412, 341, 1622, 309, 307, 746, 1219, 264, 1151, 1329, 19984, 51588, 51628], "temperature": 0.0, "avg_logprob": -0.07126128408643935, "compression_ratio": 1.7942583732057416, "no_speech_prob": 3.7265674563968787e-06}, {"id": 594, "seek": 364704, "start": 3647.04, "end": 3654.96, "text": " but this is how i want it in our case bn var in then becomes basically bn var plus epsilon", "tokens": [50364, 457, 341, 307, 577, 741, 528, 309, 294, 527, 1389, 272, 77, 1374, 294, 550, 3643, 1936, 272, 77, 1374, 1804, 17889, 50760, 50760, 17889, 307, 472, 3671, 1732, 293, 550, 309, 311, 472, 670, 3732, 5593, 307, 264, 912, 382, 11225, 281, 264, 51052, 51052, 1347, 295, 3671, 1958, 13, 20, 558, 570, 1958, 13, 20, 307, 3732, 5593, 293, 550, 3671, 1669, 309, 472, 670, 3732, 51372, 51372, 5593, 370, 272, 77, 1374, 297, 307, 257, 472, 670, 341, 20687, 510, 293, 550, 321, 393, 536, 300, 272, 77, 8936, 597, 307, 264, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.16722711890634864, "compression_ratio": 1.8719211822660098, "no_speech_prob": 3.2376640319853323e-06}, {"id": 595, "seek": 364704, "start": 3654.96, "end": 3660.8, "text": " epsilon is one negative five and then it's one over square root is the same as raising to the", "tokens": [50364, 457, 341, 307, 577, 741, 528, 309, 294, 527, 1389, 272, 77, 1374, 294, 550, 3643, 1936, 272, 77, 1374, 1804, 17889, 50760, 50760, 17889, 307, 472, 3671, 1732, 293, 550, 309, 311, 472, 670, 3732, 5593, 307, 264, 912, 382, 11225, 281, 264, 51052, 51052, 1347, 295, 3671, 1958, 13, 20, 558, 570, 1958, 13, 20, 307, 3732, 5593, 293, 550, 3671, 1669, 309, 472, 670, 3732, 51372, 51372, 5593, 370, 272, 77, 1374, 297, 307, 257, 472, 670, 341, 20687, 510, 293, 550, 321, 393, 536, 300, 272, 77, 8936, 597, 307, 264, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.16722711890634864, "compression_ratio": 1.8719211822660098, "no_speech_prob": 3.2376640319853323e-06}, {"id": 596, "seek": 364704, "start": 3660.8, "end": 3667.2, "text": " power of negative 0.5 right because 0.5 is square root and then negative makes it one over square", "tokens": [50364, 457, 341, 307, 577, 741, 528, 309, 294, 527, 1389, 272, 77, 1374, 294, 550, 3643, 1936, 272, 77, 1374, 1804, 17889, 50760, 50760, 17889, 307, 472, 3671, 1732, 293, 550, 309, 311, 472, 670, 3732, 5593, 307, 264, 912, 382, 11225, 281, 264, 51052, 51052, 1347, 295, 3671, 1958, 13, 20, 558, 570, 1958, 13, 20, 307, 3732, 5593, 293, 550, 3671, 1669, 309, 472, 670, 3732, 51372, 51372, 5593, 370, 272, 77, 1374, 297, 307, 257, 472, 670, 341, 20687, 510, 293, 550, 321, 393, 536, 300, 272, 77, 8936, 597, 307, 264, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.16722711890634864, "compression_ratio": 1.8719211822660098, "no_speech_prob": 3.2376640319853323e-06}, {"id": 597, "seek": 364704, "start": 3667.2, "end": 3674.64, "text": " root so bn var n is a one over this denominator here and then we can see that bn raw which is the", "tokens": [50364, 457, 341, 307, 577, 741, 528, 309, 294, 527, 1389, 272, 77, 1374, 294, 550, 3643, 1936, 272, 77, 1374, 1804, 17889, 50760, 50760, 17889, 307, 472, 3671, 1732, 293, 550, 309, 311, 472, 670, 3732, 5593, 307, 264, 912, 382, 11225, 281, 264, 51052, 51052, 1347, 295, 3671, 1958, 13, 20, 558, 570, 1958, 13, 20, 307, 3732, 5593, 293, 550, 3671, 1669, 309, 472, 670, 3732, 51372, 51372, 5593, 370, 272, 77, 1374, 297, 307, 257, 472, 670, 341, 20687, 510, 293, 550, 321, 393, 536, 300, 272, 77, 8936, 597, 307, 264, 51744, 51744], "temperature": 0.0, "avg_logprob": -0.16722711890634864, "compression_ratio": 1.8719211822660098, "no_speech_prob": 3.2376640319853323e-06}, {"id": 598, "seek": 367464, "start": 3674.64, "end": 3685.3599999999997, "text": " x hat here is equal to the bn diff the numerator multiplied by the bn var n and this line here that", "tokens": [50364, 2031, 2385, 510, 307, 2681, 281, 264, 272, 77, 7593, 264, 30380, 17207, 538, 264, 272, 77, 1374, 297, 293, 341, 1622, 510, 300, 50900, 50900, 7829, 276, 12, 3712, 578, 390, 264, 1036, 2522, 321, 600, 1217, 646, 12425, 770, 807, 309, 370, 586, 437, 321, 528, 281, 51192, 51192, 360, 307, 321, 366, 510, 293, 321, 362, 272, 77, 8936, 293, 321, 362, 281, 700, 646, 48256, 666, 272, 77, 7593, 293, 272, 77, 1374, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.1799391876032323, "compression_ratio": 1.7109826589595376, "no_speech_prob": 4.7849116526776925e-06}, {"id": 599, "seek": 367464, "start": 3685.3599999999997, "end": 3691.2, "text": " creates h-preact was the last piece we've already back propagated through it so now what we want to", "tokens": [50364, 2031, 2385, 510, 307, 2681, 281, 264, 272, 77, 7593, 264, 30380, 17207, 538, 264, 272, 77, 1374, 297, 293, 341, 1622, 510, 300, 50900, 50900, 7829, 276, 12, 3712, 578, 390, 264, 1036, 2522, 321, 600, 1217, 646, 12425, 770, 807, 309, 370, 586, 437, 321, 528, 281, 51192, 51192, 360, 307, 321, 366, 510, 293, 321, 362, 272, 77, 8936, 293, 321, 362, 281, 700, 646, 48256, 666, 272, 77, 7593, 293, 272, 77, 1374, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.1799391876032323, "compression_ratio": 1.7109826589595376, "no_speech_prob": 4.7849116526776925e-06}, {"id": 600, "seek": 367464, "start": 3691.2, "end": 3698.16, "text": " do is we are here and we have bn raw and we have to first back propagate into bn diff and bn var", "tokens": [50364, 2031, 2385, 510, 307, 2681, 281, 264, 272, 77, 7593, 264, 30380, 17207, 538, 264, 272, 77, 1374, 297, 293, 341, 1622, 510, 300, 50900, 50900, 7829, 276, 12, 3712, 578, 390, 264, 1036, 2522, 321, 600, 1217, 646, 12425, 770, 807, 309, 370, 586, 437, 321, 528, 281, 51192, 51192, 360, 307, 321, 366, 510, 293, 321, 362, 272, 77, 8936, 293, 321, 362, 281, 700, 646, 48256, 666, 272, 77, 7593, 293, 272, 77, 1374, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.1799391876032323, "compression_ratio": 1.7109826589595376, "no_speech_prob": 4.7849116526776925e-06}, {"id": 601, "seek": 369816, "start": 3698.16, "end": 3705.12, "text": " and bn var inf so now we're here and we have db and raw and we need to back propagate through this line", "tokens": [50364, 293, 272, 77, 1374, 1536, 370, 586, 321, 434, 510, 293, 321, 362, 274, 65, 293, 8936, 293, 321, 643, 281, 646, 48256, 807, 341, 1622, 50712, 50756, 586, 741, 600, 3720, 484, 264, 10854, 510, 293, 6451, 272, 77, 1374, 294, 307, 257, 3909, 472, 538, 12145, 370, 456, 307, 257, 51116, 51116, 30024, 2737, 510, 300, 321, 362, 281, 312, 5026, 365, 457, 309, 307, 445, 364, 4478, 12, 3711, 2199, 51348, 51348, 27290, 538, 586, 321, 820, 312, 1238, 4619, 365, 300, 281, 483, 274, 65, 293, 498, 321, 458, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.20336080334850193, "compression_ratio": 1.6946902654867257, "no_speech_prob": 3.138060264973319e-06}, {"id": 602, "seek": 369816, "start": 3706.0, "end": 3713.2, "text": " now i've written out the shapes here and indeed bn var in is a shape one by 64 so there is a", "tokens": [50364, 293, 272, 77, 1374, 1536, 370, 586, 321, 434, 510, 293, 321, 362, 274, 65, 293, 8936, 293, 321, 643, 281, 646, 48256, 807, 341, 1622, 50712, 50756, 586, 741, 600, 3720, 484, 264, 10854, 510, 293, 6451, 272, 77, 1374, 294, 307, 257, 3909, 472, 538, 12145, 370, 456, 307, 257, 51116, 51116, 30024, 2737, 510, 300, 321, 362, 281, 312, 5026, 365, 457, 309, 307, 445, 364, 4478, 12, 3711, 2199, 51348, 51348, 27290, 538, 586, 321, 820, 312, 1238, 4619, 365, 300, 281, 483, 274, 65, 293, 498, 321, 458, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.20336080334850193, "compression_ratio": 1.6946902654867257, "no_speech_prob": 3.138060264973319e-06}, {"id": 603, "seek": 369816, "start": 3713.2, "end": 3717.8399999999997, "text": " broadcasting happening here that we have to be careful with but it is just an element-wise simple", "tokens": [50364, 293, 272, 77, 1374, 1536, 370, 586, 321, 434, 510, 293, 321, 362, 274, 65, 293, 8936, 293, 321, 643, 281, 646, 48256, 807, 341, 1622, 50712, 50756, 586, 741, 600, 3720, 484, 264, 10854, 510, 293, 6451, 272, 77, 1374, 294, 307, 257, 3909, 472, 538, 12145, 370, 456, 307, 257, 51116, 51116, 30024, 2737, 510, 300, 321, 362, 281, 312, 5026, 365, 457, 309, 307, 445, 364, 4478, 12, 3711, 2199, 51348, 51348, 27290, 538, 586, 321, 820, 312, 1238, 4619, 365, 300, 281, 483, 274, 65, 293, 498, 321, 458, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.20336080334850193, "compression_ratio": 1.6946902654867257, "no_speech_prob": 3.138060264973319e-06}, {"id": 604, "seek": 369816, "start": 3717.8399999999997, "end": 3722.96, "text": " multiplication by now we should be pretty comfortable with that to get db and if we know", "tokens": [50364, 293, 272, 77, 1374, 1536, 370, 586, 321, 434, 510, 293, 321, 362, 274, 65, 293, 8936, 293, 321, 643, 281, 646, 48256, 807, 341, 1622, 50712, 50756, 586, 741, 600, 3720, 484, 264, 10854, 510, 293, 6451, 272, 77, 1374, 294, 307, 257, 3909, 472, 538, 12145, 370, 456, 307, 257, 51116, 51116, 30024, 2737, 510, 300, 321, 362, 281, 312, 5026, 365, 457, 309, 307, 445, 364, 4478, 12, 3711, 2199, 51348, 51348, 27290, 538, 586, 321, 820, 312, 1238, 4619, 365, 300, 281, 483, 274, 65, 293, 498, 321, 458, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.20336080334850193, "compression_ratio": 1.6946902654867257, "no_speech_prob": 3.138060264973319e-06}, {"id": 605, "seek": 372296, "start": 3722.96, "end": 3733.52, "text": " that this is just bn var and multiplied with db and raw and conversely to get db and var", "tokens": [50364, 300, 341, 307, 445, 272, 77, 1374, 293, 17207, 365, 274, 65, 293, 8936, 293, 2615, 736, 281, 483, 274, 65, 293, 1374, 50892, 50892, 321, 643, 281, 747, 264, 917, 498, 293, 12972, 300, 538, 274, 65, 293, 8936, 370, 341, 307, 264, 11532, 457, 295, 1164, 51420, 51420, 321, 643, 281, 652, 988, 300, 30024, 307, 19297, 292, 370, 294, 1729, 272, 77, 1374, 293, 30955, 365, 274, 65, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.21341293334960937, "compression_ratio": 1.814102564102564, "no_speech_prob": 5.507257810677402e-06}, {"id": 606, "seek": 372296, "start": 3733.52, "end": 3744.08, "text": " we need to take the end if and multiply that by db and raw so this is the candidate but of course", "tokens": [50364, 300, 341, 307, 445, 272, 77, 1374, 293, 17207, 365, 274, 65, 293, 8936, 293, 2615, 736, 281, 483, 274, 65, 293, 1374, 50892, 50892, 321, 643, 281, 747, 264, 917, 498, 293, 12972, 300, 538, 274, 65, 293, 8936, 370, 341, 307, 264, 11532, 457, 295, 1164, 51420, 51420, 321, 643, 281, 652, 988, 300, 30024, 307, 19297, 292, 370, 294, 1729, 272, 77, 1374, 293, 30955, 365, 274, 65, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.21341293334960937, "compression_ratio": 1.814102564102564, "no_speech_prob": 5.507257810677402e-06}, {"id": 607, "seek": 372296, "start": 3744.08, "end": 3749.84, "text": " we need to make sure that broadcasting is obeyed so in particular bn var and multiplying with db", "tokens": [50364, 300, 341, 307, 445, 272, 77, 1374, 293, 17207, 365, 274, 65, 293, 8936, 293, 2615, 736, 281, 483, 274, 65, 293, 1374, 50892, 50892, 321, 643, 281, 747, 264, 917, 498, 293, 12972, 300, 538, 274, 65, 293, 8936, 370, 341, 307, 264, 11532, 457, 295, 1164, 51420, 51420, 321, 643, 281, 652, 988, 300, 30024, 307, 19297, 292, 370, 294, 1729, 272, 77, 1374, 293, 30955, 365, 274, 65, 51708, 51708], "temperature": 0.0, "avg_logprob": -0.21341293334960937, "compression_ratio": 1.814102564102564, "no_speech_prob": 5.507257810677402e-06}, {"id": 608, "seek": 374984, "start": 3749.84, "end": 3759.52, "text": " and raw will be okay and give us 32 by 64 as we expect but db and var inf would be taking a 32 by", "tokens": [50364, 293, 8936, 486, 312, 1392, 293, 976, 505, 8858, 538, 12145, 382, 321, 2066, 457, 274, 65, 293, 1374, 1536, 576, 312, 1940, 257, 8858, 538, 50848, 50848, 12145, 30955, 309, 538, 8858, 538, 12145, 370, 341, 307, 257, 8858, 538, 12145, 457, 295, 1164, 274, 65, 341, 272, 77, 1374, 1536, 307, 787, 472, 51372, 51372, 538, 12145, 370, 264, 1150, 1622, 510, 2203, 257, 2408, 2108, 264, 5110, 293, 570, 456, 311, 341, 10139, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.12420254945755005, "compression_ratio": 1.6628571428571428, "no_speech_prob": 1.5293729802579037e-06}, {"id": 609, "seek": 374984, "start": 3759.52, "end": 3770.0, "text": " 64 multiplying it by 32 by 64 so this is a 32 by 64 but of course db this bn var inf is only one", "tokens": [50364, 293, 8936, 486, 312, 1392, 293, 976, 505, 8858, 538, 12145, 382, 321, 2066, 457, 274, 65, 293, 1374, 1536, 576, 312, 1940, 257, 8858, 538, 50848, 50848, 12145, 30955, 309, 538, 8858, 538, 12145, 370, 341, 307, 257, 8858, 538, 12145, 457, 295, 1164, 274, 65, 341, 272, 77, 1374, 1536, 307, 787, 472, 51372, 51372, 538, 12145, 370, 264, 1150, 1622, 510, 2203, 257, 2408, 2108, 264, 5110, 293, 570, 456, 311, 341, 10139, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.12420254945755005, "compression_ratio": 1.6628571428571428, "no_speech_prob": 1.5293729802579037e-06}, {"id": 610, "seek": 374984, "start": 3770.0, "end": 3777.6000000000004, "text": " by 64 so the second line here needs a sum across the examples and because there's this dimension", "tokens": [50364, 293, 8936, 486, 312, 1392, 293, 976, 505, 8858, 538, 12145, 382, 321, 2066, 457, 274, 65, 293, 1374, 1536, 576, 312, 1940, 257, 8858, 538, 50848, 50848, 12145, 30955, 309, 538, 8858, 538, 12145, 370, 341, 307, 257, 8858, 538, 12145, 457, 295, 1164, 274, 65, 341, 272, 77, 1374, 1536, 307, 787, 472, 51372, 51372, 538, 12145, 370, 264, 1150, 1622, 510, 2203, 257, 2408, 2108, 264, 5110, 293, 570, 456, 311, 341, 10139, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.12420254945755005, "compression_ratio": 1.6628571428571428, "no_speech_prob": 1.5293729802579037e-06}, {"id": 611, "seek": 377760, "start": 3777.6, "end": 3786.0, "text": " here we need to make sure that keep them is true so this is the candidate let's erase this and let's", "tokens": [50364, 510, 321, 643, 281, 652, 988, 300, 1066, 552, 307, 2074, 370, 341, 307, 264, 11532, 718, 311, 23525, 341, 293, 718, 311, 50784, 50784, 11173, 760, 510, 293, 4445, 309, 293, 550, 718, 311, 2871, 484, 274, 65, 293, 1374, 1536, 293, 274, 65, 293, 7593, 586, 51276, 51308, 321, 603, 767, 3449, 300, 274, 65, 293, 7593, 538, 264, 636, 307, 516, 281, 312, 18424, 370, 562, 741, 1190, 341, 51604, 51656], "temperature": 0.0, "avg_logprob": -0.1753486708590859, "compression_ratio": 1.6900584795321638, "no_speech_prob": 3.28872079080611e-06}, {"id": 612, "seek": 377760, "start": 3786.0, "end": 3795.8399999999997, "text": " swing down here and implement it and then let's comment out db and var inf and db and diff now", "tokens": [50364, 510, 321, 643, 281, 652, 988, 300, 1066, 552, 307, 2074, 370, 341, 307, 264, 11532, 718, 311, 23525, 341, 293, 718, 311, 50784, 50784, 11173, 760, 510, 293, 4445, 309, 293, 550, 718, 311, 2871, 484, 274, 65, 293, 1374, 1536, 293, 274, 65, 293, 7593, 586, 51276, 51308, 321, 603, 767, 3449, 300, 274, 65, 293, 7593, 538, 264, 636, 307, 516, 281, 312, 18424, 370, 562, 741, 1190, 341, 51604, 51656], "temperature": 0.0, "avg_logprob": -0.1753486708590859, "compression_ratio": 1.6900584795321638, "no_speech_prob": 3.28872079080611e-06}, {"id": 613, "seek": 377760, "start": 3796.48, "end": 3802.4, "text": " we'll actually notice that db and diff by the way is going to be incorrect so when i run this", "tokens": [50364, 510, 321, 643, 281, 652, 988, 300, 1066, 552, 307, 2074, 370, 341, 307, 264, 11532, 718, 311, 23525, 341, 293, 718, 311, 50784, 50784, 11173, 760, 510, 293, 4445, 309, 293, 550, 718, 311, 2871, 484, 274, 65, 293, 1374, 1536, 293, 274, 65, 293, 7593, 586, 51276, 51308, 321, 603, 767, 3449, 300, 274, 65, 293, 7593, 538, 264, 636, 307, 516, 281, 312, 18424, 370, 562, 741, 1190, 341, 51604, 51656], "temperature": 0.0, "avg_logprob": -0.1753486708590859, "compression_ratio": 1.6900584795321638, "no_speech_prob": 3.28872079080611e-06}, {"id": 614, "seek": 380240, "start": 3802.4, "end": 3810.56, "text": " when i run this bn var inf is correct bn diff is not correct and this is actually expected", "tokens": [50364, 562, 741, 1190, 341, 272, 77, 1374, 1536, 307, 3006, 272, 77, 7593, 307, 406, 3006, 293, 341, 307, 767, 5176, 50772, 50772, 570, 321, 434, 406, 1096, 365, 272, 77, 7593, 370, 294, 1729, 562, 321, 4137, 510, 321, 536, 510, 300, 51072, 51072, 272, 77, 8936, 307, 257, 2445, 295, 272, 77, 7593, 457, 767, 272, 77, 1374, 307, 257, 2445, 295, 272, 77, 1374, 597, 307, 257, 2445, 51368, 51368, 295, 272, 77, 7593, 360, 597, 307, 257, 2445, 295, 272, 77, 7593, 370, 309, 1487, 510, 370, 272, 67, 77, 7593, 613, 7006, 5288, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.07475863718519024, "compression_ratio": 2.102272727272727, "no_speech_prob": 2.0260886230971664e-06}, {"id": 615, "seek": 380240, "start": 3810.56, "end": 3816.56, "text": " because we're not done with bn diff so in particular when we slide here we see here that", "tokens": [50364, 562, 741, 1190, 341, 272, 77, 1374, 1536, 307, 3006, 272, 77, 7593, 307, 406, 3006, 293, 341, 307, 767, 5176, 50772, 50772, 570, 321, 434, 406, 1096, 365, 272, 77, 7593, 370, 294, 1729, 562, 321, 4137, 510, 321, 536, 510, 300, 51072, 51072, 272, 77, 8936, 307, 257, 2445, 295, 272, 77, 7593, 457, 767, 272, 77, 1374, 307, 257, 2445, 295, 272, 77, 1374, 597, 307, 257, 2445, 51368, 51368, 295, 272, 77, 7593, 360, 597, 307, 257, 2445, 295, 272, 77, 7593, 370, 309, 1487, 510, 370, 272, 67, 77, 7593, 613, 7006, 5288, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.07475863718519024, "compression_ratio": 2.102272727272727, "no_speech_prob": 2.0260886230971664e-06}, {"id": 616, "seek": 380240, "start": 3816.56, "end": 3822.48, "text": " bn raw is a function of bn diff but actually bn var is a function of bn var which is a function", "tokens": [50364, 562, 741, 1190, 341, 272, 77, 1374, 1536, 307, 3006, 272, 77, 7593, 307, 406, 3006, 293, 341, 307, 767, 5176, 50772, 50772, 570, 321, 434, 406, 1096, 365, 272, 77, 7593, 370, 294, 1729, 562, 321, 4137, 510, 321, 536, 510, 300, 51072, 51072, 272, 77, 8936, 307, 257, 2445, 295, 272, 77, 7593, 457, 767, 272, 77, 1374, 307, 257, 2445, 295, 272, 77, 1374, 597, 307, 257, 2445, 51368, 51368, 295, 272, 77, 7593, 360, 597, 307, 257, 2445, 295, 272, 77, 7593, 370, 309, 1487, 510, 370, 272, 67, 77, 7593, 613, 7006, 5288, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.07475863718519024, "compression_ratio": 2.102272727272727, "no_speech_prob": 2.0260886230971664e-06}, {"id": 617, "seek": 380240, "start": 3822.48, "end": 3830.1600000000003, "text": " of bn diff do which is a function of bn diff so it comes here so bdn diff these variable names", "tokens": [50364, 562, 741, 1190, 341, 272, 77, 1374, 1536, 307, 3006, 272, 77, 7593, 307, 406, 3006, 293, 341, 307, 767, 5176, 50772, 50772, 570, 321, 434, 406, 1096, 365, 272, 77, 7593, 370, 294, 1729, 562, 321, 4137, 510, 321, 536, 510, 300, 51072, 51072, 272, 77, 8936, 307, 257, 2445, 295, 272, 77, 7593, 457, 767, 272, 77, 1374, 307, 257, 2445, 295, 272, 77, 1374, 597, 307, 257, 2445, 51368, 51368, 295, 272, 77, 7593, 360, 597, 307, 257, 2445, 295, 272, 77, 7593, 370, 309, 1487, 510, 370, 272, 67, 77, 7593, 613, 7006, 5288, 51752, 51752], "temperature": 0.0, "avg_logprob": -0.07475863718519024, "compression_ratio": 2.102272727272727, "no_speech_prob": 2.0260886230971664e-06}, {"id": 618, "seek": 383016, "start": 3830.16, "end": 3835.3599999999997, "text": " are crazy i'm sorry it branches out into two branches and we've only done one branch of it", "tokens": [50364, 366, 3219, 741, 478, 2597, 309, 14770, 484, 666, 732, 14770, 293, 321, 600, 787, 1096, 472, 9819, 295, 309, 50624, 50624, 321, 362, 281, 2354, 527, 646, 79, 1513, 559, 399, 293, 4728, 808, 646, 281, 272, 77, 7593, 293, 550, 321, 603, 50816, 50816, 312, 1075, 281, 360, 257, 1804, 6915, 293, 483, 264, 3539, 2190, 16235, 337, 586, 309, 307, 665, 281, 16888, 300, 51076, 51076, 269, 2455, 611, 1985, 309, 1177, 380, 445, 4544, 281, 505, 293, 980, 505, 300, 1203, 307, 1009, 3006, 309, 393, 294, 51356, 51356, 1186, 5531, 562, 428, 16235, 307, 406, 3006, 370, 309, 311, 300, 311, 665, 281, 536, 382, 731, 1392, 370, 586, 321, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.03978288474203158, "compression_ratio": 1.7811320754716982, "no_speech_prob": 4.28925432061078e-06}, {"id": 619, "seek": 383016, "start": 3835.3599999999997, "end": 3839.2, "text": " we have to continue our backpropagation and eventually come back to bn diff and then we'll", "tokens": [50364, 366, 3219, 741, 478, 2597, 309, 14770, 484, 666, 732, 14770, 293, 321, 600, 787, 1096, 472, 9819, 295, 309, 50624, 50624, 321, 362, 281, 2354, 527, 646, 79, 1513, 559, 399, 293, 4728, 808, 646, 281, 272, 77, 7593, 293, 550, 321, 603, 50816, 50816, 312, 1075, 281, 360, 257, 1804, 6915, 293, 483, 264, 3539, 2190, 16235, 337, 586, 309, 307, 665, 281, 16888, 300, 51076, 51076, 269, 2455, 611, 1985, 309, 1177, 380, 445, 4544, 281, 505, 293, 980, 505, 300, 1203, 307, 1009, 3006, 309, 393, 294, 51356, 51356, 1186, 5531, 562, 428, 16235, 307, 406, 3006, 370, 309, 311, 300, 311, 665, 281, 536, 382, 731, 1392, 370, 586, 321, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.03978288474203158, "compression_ratio": 1.7811320754716982, "no_speech_prob": 4.28925432061078e-06}, {"id": 620, "seek": 383016, "start": 3839.2, "end": 3844.3999999999996, "text": " be able to do a plus equals and get the actual current gradient for now it is good to verify that", "tokens": [50364, 366, 3219, 741, 478, 2597, 309, 14770, 484, 666, 732, 14770, 293, 321, 600, 787, 1096, 472, 9819, 295, 309, 50624, 50624, 321, 362, 281, 2354, 527, 646, 79, 1513, 559, 399, 293, 4728, 808, 646, 281, 272, 77, 7593, 293, 550, 321, 603, 50816, 50816, 312, 1075, 281, 360, 257, 1804, 6915, 293, 483, 264, 3539, 2190, 16235, 337, 586, 309, 307, 665, 281, 16888, 300, 51076, 51076, 269, 2455, 611, 1985, 309, 1177, 380, 445, 4544, 281, 505, 293, 980, 505, 300, 1203, 307, 1009, 3006, 309, 393, 294, 51356, 51356, 1186, 5531, 562, 428, 16235, 307, 406, 3006, 370, 309, 311, 300, 311, 665, 281, 536, 382, 731, 1392, 370, 586, 321, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.03978288474203158, "compression_ratio": 1.7811320754716982, "no_speech_prob": 4.28925432061078e-06}, {"id": 621, "seek": 383016, "start": 3844.3999999999996, "end": 3850.0, "text": " cmp also works it doesn't just lie to us and tell us that everything is always correct it can in", "tokens": [50364, 366, 3219, 741, 478, 2597, 309, 14770, 484, 666, 732, 14770, 293, 321, 600, 787, 1096, 472, 9819, 295, 309, 50624, 50624, 321, 362, 281, 2354, 527, 646, 79, 1513, 559, 399, 293, 4728, 808, 646, 281, 272, 77, 7593, 293, 550, 321, 603, 50816, 50816, 312, 1075, 281, 360, 257, 1804, 6915, 293, 483, 264, 3539, 2190, 16235, 337, 586, 309, 307, 665, 281, 16888, 300, 51076, 51076, 269, 2455, 611, 1985, 309, 1177, 380, 445, 4544, 281, 505, 293, 980, 505, 300, 1203, 307, 1009, 3006, 309, 393, 294, 51356, 51356, 1186, 5531, 562, 428, 16235, 307, 406, 3006, 370, 309, 311, 300, 311, 665, 281, 536, 382, 731, 1392, 370, 586, 321, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.03978288474203158, "compression_ratio": 1.7811320754716982, "no_speech_prob": 4.28925432061078e-06}, {"id": 622, "seek": 383016, "start": 3850.0, "end": 3855.52, "text": " fact detect when your gradient is not correct so it's that's good to see as well okay so now we", "tokens": [50364, 366, 3219, 741, 478, 2597, 309, 14770, 484, 666, 732, 14770, 293, 321, 600, 787, 1096, 472, 9819, 295, 309, 50624, 50624, 321, 362, 281, 2354, 527, 646, 79, 1513, 559, 399, 293, 4728, 808, 646, 281, 272, 77, 7593, 293, 550, 321, 603, 50816, 50816, 312, 1075, 281, 360, 257, 1804, 6915, 293, 483, 264, 3539, 2190, 16235, 337, 586, 309, 307, 665, 281, 16888, 300, 51076, 51076, 269, 2455, 611, 1985, 309, 1177, 380, 445, 4544, 281, 505, 293, 980, 505, 300, 1203, 307, 1009, 3006, 309, 393, 294, 51356, 51356, 1186, 5531, 562, 428, 16235, 307, 406, 3006, 370, 309, 311, 300, 311, 665, 281, 536, 382, 731, 1392, 370, 586, 321, 51632, 51632], "temperature": 0.0, "avg_logprob": -0.03978288474203158, "compression_ratio": 1.7811320754716982, "no_speech_prob": 4.28925432061078e-06}, {"id": 623, "seek": 385552, "start": 3855.52, "end": 3860.16, "text": " have the derivative here and we're trying to backpropagate through this line and because we're", "tokens": [50364, 362, 264, 13760, 510, 293, 321, 434, 1382, 281, 646, 79, 1513, 559, 473, 807, 341, 1622, 293, 570, 321, 434, 50596, 50596, 11225, 281, 257, 1347, 295, 3671, 935, 1732, 741, 3038, 493, 264, 1347, 4978, 293, 321, 536, 300, 1936, 321, 50820, 50820, 362, 300, 264, 272, 76, 1374, 486, 586, 312, 321, 1565, 760, 264, 37871, 370, 3671, 935, 1732, 1413, 51100, 51220, 2031, 597, 307, 341, 293, 586, 6005, 281, 264, 1347, 295, 3671, 935, 1732, 3175, 472, 597, 307, 3671, 51492, 51492, 472, 935, 1732, 586, 321, 576, 362, 281, 611, 3079, 257, 1359, 5021, 4978, 510, 294, 527, 1378, 51780, 51808], "temperature": 0.0, "avg_logprob": -0.05261734866221017, "compression_ratio": 1.9574468085106382, "no_speech_prob": 6.85407667333493e-06}, {"id": 624, "seek": 385552, "start": 3860.16, "end": 3864.64, "text": " raising to a power of negative point five i brought up the power rule and we see that basically we", "tokens": [50364, 362, 264, 13760, 510, 293, 321, 434, 1382, 281, 646, 79, 1513, 559, 473, 807, 341, 1622, 293, 570, 321, 434, 50596, 50596, 11225, 281, 257, 1347, 295, 3671, 935, 1732, 741, 3038, 493, 264, 1347, 4978, 293, 321, 536, 300, 1936, 321, 50820, 50820, 362, 300, 264, 272, 76, 1374, 486, 586, 312, 321, 1565, 760, 264, 37871, 370, 3671, 935, 1732, 1413, 51100, 51220, 2031, 597, 307, 341, 293, 586, 6005, 281, 264, 1347, 295, 3671, 935, 1732, 3175, 472, 597, 307, 3671, 51492, 51492, 472, 935, 1732, 586, 321, 576, 362, 281, 611, 3079, 257, 1359, 5021, 4978, 510, 294, 527, 1378, 51780, 51808], "temperature": 0.0, "avg_logprob": -0.05261734866221017, "compression_ratio": 1.9574468085106382, "no_speech_prob": 6.85407667333493e-06}, {"id": 625, "seek": 385552, "start": 3864.64, "end": 3870.24, "text": " have that the bm var will now be we bring down the exponent so negative point five times", "tokens": [50364, 362, 264, 13760, 510, 293, 321, 434, 1382, 281, 646, 79, 1513, 559, 473, 807, 341, 1622, 293, 570, 321, 434, 50596, 50596, 11225, 281, 257, 1347, 295, 3671, 935, 1732, 741, 3038, 493, 264, 1347, 4978, 293, 321, 536, 300, 1936, 321, 50820, 50820, 362, 300, 264, 272, 76, 1374, 486, 586, 312, 321, 1565, 760, 264, 37871, 370, 3671, 935, 1732, 1413, 51100, 51220, 2031, 597, 307, 341, 293, 586, 6005, 281, 264, 1347, 295, 3671, 935, 1732, 3175, 472, 597, 307, 3671, 51492, 51492, 472, 935, 1732, 586, 321, 576, 362, 281, 611, 3079, 257, 1359, 5021, 4978, 510, 294, 527, 1378, 51780, 51808], "temperature": 0.0, "avg_logprob": -0.05261734866221017, "compression_ratio": 1.9574468085106382, "no_speech_prob": 6.85407667333493e-06}, {"id": 626, "seek": 385552, "start": 3872.64, "end": 3878.08, "text": " x which is this and now raised to the power of negative point five minus one which is negative", "tokens": [50364, 362, 264, 13760, 510, 293, 321, 434, 1382, 281, 646, 79, 1513, 559, 473, 807, 341, 1622, 293, 570, 321, 434, 50596, 50596, 11225, 281, 257, 1347, 295, 3671, 935, 1732, 741, 3038, 493, 264, 1347, 4978, 293, 321, 536, 300, 1936, 321, 50820, 50820, 362, 300, 264, 272, 76, 1374, 486, 586, 312, 321, 1565, 760, 264, 37871, 370, 3671, 935, 1732, 1413, 51100, 51220, 2031, 597, 307, 341, 293, 586, 6005, 281, 264, 1347, 295, 3671, 935, 1732, 3175, 472, 597, 307, 3671, 51492, 51492, 472, 935, 1732, 586, 321, 576, 362, 281, 611, 3079, 257, 1359, 5021, 4978, 510, 294, 527, 1378, 51780, 51808], "temperature": 0.0, "avg_logprob": -0.05261734866221017, "compression_ratio": 1.9574468085106382, "no_speech_prob": 6.85407667333493e-06}, {"id": 627, "seek": 385552, "start": 3878.08, "end": 3883.84, "text": " one point five now we would have to also apply a small chain rule here in our head", "tokens": [50364, 362, 264, 13760, 510, 293, 321, 434, 1382, 281, 646, 79, 1513, 559, 473, 807, 341, 1622, 293, 570, 321, 434, 50596, 50596, 11225, 281, 257, 1347, 295, 3671, 935, 1732, 741, 3038, 493, 264, 1347, 4978, 293, 321, 536, 300, 1936, 321, 50820, 50820, 362, 300, 264, 272, 76, 1374, 486, 586, 312, 321, 1565, 760, 264, 37871, 370, 3671, 935, 1732, 1413, 51100, 51220, 2031, 597, 307, 341, 293, 586, 6005, 281, 264, 1347, 295, 3671, 935, 1732, 3175, 472, 597, 307, 3671, 51492, 51492, 472, 935, 1732, 586, 321, 576, 362, 281, 611, 3079, 257, 1359, 5021, 4978, 510, 294, 527, 1378, 51780, 51808], "temperature": 0.0, "avg_logprob": -0.05261734866221017, "compression_ratio": 1.9574468085106382, "no_speech_prob": 6.85407667333493e-06}, {"id": 628, "seek": 388384, "start": 3883.84, "end": 3889.6800000000003, "text": " because we need to take further derivative of bm var with respect to this expression here inside", "tokens": [50364, 570, 321, 643, 281, 747, 3052, 13760, 295, 272, 76, 1374, 365, 3104, 281, 341, 6114, 510, 1854, 50656, 50656, 264, 16904, 457, 570, 341, 307, 364, 4478, 12, 3711, 6916, 293, 1203, 307, 6457, 2199, 300, 311, 50876, 50876, 445, 472, 293, 370, 456, 311, 1825, 281, 360, 456, 370, 341, 307, 264, 2654, 13760, 293, 550, 1413, 264, 51176, 51176, 4338, 13760, 281, 1884, 264, 5021, 4978, 341, 307, 445, 1413, 264, 272, 76, 1374, 370, 341, 307, 527, 11532, 51516, 51516], "temperature": 0.0, "avg_logprob": -0.11623363716657771, "compression_ratio": 1.838095238095238, "no_speech_prob": 4.56583802588284e-06}, {"id": 629, "seek": 388384, "start": 3889.6800000000003, "end": 3894.08, "text": " the bracket but because this is an element-wise operation and everything is fairly simple that's", "tokens": [50364, 570, 321, 643, 281, 747, 3052, 13760, 295, 272, 76, 1374, 365, 3104, 281, 341, 6114, 510, 1854, 50656, 50656, 264, 16904, 457, 570, 341, 307, 364, 4478, 12, 3711, 6916, 293, 1203, 307, 6457, 2199, 300, 311, 50876, 50876, 445, 472, 293, 370, 456, 311, 1825, 281, 360, 456, 370, 341, 307, 264, 2654, 13760, 293, 550, 1413, 264, 51176, 51176, 4338, 13760, 281, 1884, 264, 5021, 4978, 341, 307, 445, 1413, 264, 272, 76, 1374, 370, 341, 307, 527, 11532, 51516, 51516], "temperature": 0.0, "avg_logprob": -0.11623363716657771, "compression_ratio": 1.838095238095238, "no_speech_prob": 4.56583802588284e-06}, {"id": 630, "seek": 388384, "start": 3894.08, "end": 3900.08, "text": " just one and so there's nothing to do there so this is the local derivative and then times the", "tokens": [50364, 570, 321, 643, 281, 747, 3052, 13760, 295, 272, 76, 1374, 365, 3104, 281, 341, 6114, 510, 1854, 50656, 50656, 264, 16904, 457, 570, 341, 307, 364, 4478, 12, 3711, 6916, 293, 1203, 307, 6457, 2199, 300, 311, 50876, 50876, 445, 472, 293, 370, 456, 311, 1825, 281, 360, 456, 370, 341, 307, 264, 2654, 13760, 293, 550, 1413, 264, 51176, 51176, 4338, 13760, 281, 1884, 264, 5021, 4978, 341, 307, 445, 1413, 264, 272, 76, 1374, 370, 341, 307, 527, 11532, 51516, 51516], "temperature": 0.0, "avg_logprob": -0.11623363716657771, "compression_ratio": 1.838095238095238, "no_speech_prob": 4.56583802588284e-06}, {"id": 631, "seek": 388384, "start": 3900.08, "end": 3906.88, "text": " global derivative to create the chain rule this is just times the bm var so this is our candidate", "tokens": [50364, 570, 321, 643, 281, 747, 3052, 13760, 295, 272, 76, 1374, 365, 3104, 281, 341, 6114, 510, 1854, 50656, 50656, 264, 16904, 457, 570, 341, 307, 364, 4478, 12, 3711, 6916, 293, 1203, 307, 6457, 2199, 300, 311, 50876, 50876, 445, 472, 293, 370, 456, 311, 1825, 281, 360, 456, 370, 341, 307, 264, 2654, 13760, 293, 550, 1413, 264, 51176, 51176, 4338, 13760, 281, 1884, 264, 5021, 4978, 341, 307, 445, 1413, 264, 272, 76, 1374, 370, 341, 307, 527, 11532, 51516, 51516], "temperature": 0.0, "avg_logprob": -0.11623363716657771, "compression_ratio": 1.838095238095238, "no_speech_prob": 4.56583802588284e-06}, {"id": 632, "seek": 390688, "start": 3906.88, "end": 3917.44, "text": " let me bring this down and uncomment the check and we see that we have the correct result now before", "tokens": [50364, 718, 385, 1565, 341, 760, 293, 8585, 518, 264, 1520, 293, 321, 536, 300, 321, 362, 264, 3006, 1874, 586, 949, 50892, 50892, 321, 646, 79, 1513, 559, 473, 807, 264, 958, 1622, 741, 1415, 281, 10515, 751, 466, 264, 3637, 510, 689, 741, 478, 1228, 51088, 51088, 264, 1151, 12, 1832, 19984, 26764, 538, 297, 3175, 472, 2602, 295, 26764, 538, 297, 562, 741, 2710, 1125, 510, 264, 51396, 51396, 2408, 295, 19368, 586, 291, 603, 3449, 300, 341, 307, 257, 25866, 490, 264, 3035, 597, 4960, 472, 670, 297, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1397368682058234, "compression_ratio": 1.6952789699570816, "no_speech_prob": 5.771787073172163e-06}, {"id": 633, "seek": 390688, "start": 3917.44, "end": 3921.36, "text": " we backpropagate through the next line i wanted to briefly talk about the note here where i'm using", "tokens": [50364, 718, 385, 1565, 341, 760, 293, 8585, 518, 264, 1520, 293, 321, 536, 300, 321, 362, 264, 3006, 1874, 586, 949, 50892, 50892, 321, 646, 79, 1513, 559, 473, 807, 264, 958, 1622, 741, 1415, 281, 10515, 751, 466, 264, 3637, 510, 689, 741, 478, 1228, 51088, 51088, 264, 1151, 12, 1832, 19984, 26764, 538, 297, 3175, 472, 2602, 295, 26764, 538, 297, 562, 741, 2710, 1125, 510, 264, 51396, 51396, 2408, 295, 19368, 586, 291, 603, 3449, 300, 341, 307, 257, 25866, 490, 264, 3035, 597, 4960, 472, 670, 297, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1397368682058234, "compression_ratio": 1.6952789699570816, "no_speech_prob": 5.771787073172163e-06}, {"id": 634, "seek": 390688, "start": 3921.36, "end": 3927.52, "text": " the best-less correction dividing by n minus one instead of dividing by n when i normalize here the", "tokens": [50364, 718, 385, 1565, 341, 760, 293, 8585, 518, 264, 1520, 293, 321, 536, 300, 321, 362, 264, 3006, 1874, 586, 949, 50892, 50892, 321, 646, 79, 1513, 559, 473, 807, 264, 958, 1622, 741, 1415, 281, 10515, 751, 466, 264, 3637, 510, 689, 741, 478, 1228, 51088, 51088, 264, 1151, 12, 1832, 19984, 26764, 538, 297, 3175, 472, 2602, 295, 26764, 538, 297, 562, 741, 2710, 1125, 510, 264, 51396, 51396, 2408, 295, 19368, 586, 291, 603, 3449, 300, 341, 307, 257, 25866, 490, 264, 3035, 597, 4960, 472, 670, 297, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1397368682058234, "compression_ratio": 1.6952789699570816, "no_speech_prob": 5.771787073172163e-06}, {"id": 635, "seek": 390688, "start": 3927.52, "end": 3932.96, "text": " sum of squares now you'll notice that this is a departure from the paper which uses one over n", "tokens": [50364, 718, 385, 1565, 341, 760, 293, 8585, 518, 264, 1520, 293, 321, 536, 300, 321, 362, 264, 3006, 1874, 586, 949, 50892, 50892, 321, 646, 79, 1513, 559, 473, 807, 264, 958, 1622, 741, 1415, 281, 10515, 751, 466, 264, 3637, 510, 689, 741, 478, 1228, 51088, 51088, 264, 1151, 12, 1832, 19984, 26764, 538, 297, 3175, 472, 2602, 295, 26764, 538, 297, 562, 741, 2710, 1125, 510, 264, 51396, 51396, 2408, 295, 19368, 586, 291, 603, 3449, 300, 341, 307, 257, 25866, 490, 264, 3035, 597, 4960, 472, 670, 297, 51668, 51668], "temperature": 0.0, "avg_logprob": -0.1397368682058234, "compression_ratio": 1.6952789699570816, "no_speech_prob": 5.771787073172163e-06}, {"id": 636, "seek": 393296, "start": 3932.96, "end": 3940.8, "text": " instead not one over n minus one there m is our n and so it turns out that there are two ways of", "tokens": [50364, 2602, 406, 472, 670, 297, 3175, 472, 456, 275, 307, 527, 297, 293, 370, 309, 4523, 484, 300, 456, 366, 732, 2098, 295, 50756, 50756, 8017, 990, 21977, 295, 364, 10225, 472, 307, 264, 28035, 12539, 597, 307, 472, 670, 297, 293, 264, 661, 472, 307, 51124, 51124, 264, 517, 5614, 1937, 12539, 597, 307, 472, 670, 297, 3175, 472, 586, 13181, 356, 294, 264, 3035, 341, 307, 406, 588, 51444, 51444, 4448, 7619, 293, 611, 309, 311, 257, 2607, 300, 733, 295, 7001, 741, 519, 436, 366, 1228, 264, 28035, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.07486766012091385, "compression_ratio": 1.95, "no_speech_prob": 6.143910468381364e-06}, {"id": 637, "seek": 393296, "start": 3940.8, "end": 3948.16, "text": " estimating variance of an array one is the biased estimate which is one over n and the other one is", "tokens": [50364, 2602, 406, 472, 670, 297, 3175, 472, 456, 275, 307, 527, 297, 293, 370, 309, 4523, 484, 300, 456, 366, 732, 2098, 295, 50756, 50756, 8017, 990, 21977, 295, 364, 10225, 472, 307, 264, 28035, 12539, 597, 307, 472, 670, 297, 293, 264, 661, 472, 307, 51124, 51124, 264, 517, 5614, 1937, 12539, 597, 307, 472, 670, 297, 3175, 472, 586, 13181, 356, 294, 264, 3035, 341, 307, 406, 588, 51444, 51444, 4448, 7619, 293, 611, 309, 311, 257, 2607, 300, 733, 295, 7001, 741, 519, 436, 366, 1228, 264, 28035, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.07486766012091385, "compression_ratio": 1.95, "no_speech_prob": 6.143910468381364e-06}, {"id": 638, "seek": 393296, "start": 3948.16, "end": 3954.56, "text": " the unbiased estimate which is one over n minus one now confusingly in the paper this is not very", "tokens": [50364, 2602, 406, 472, 670, 297, 3175, 472, 456, 275, 307, 527, 297, 293, 370, 309, 4523, 484, 300, 456, 366, 732, 2098, 295, 50756, 50756, 8017, 990, 21977, 295, 364, 10225, 472, 307, 264, 28035, 12539, 597, 307, 472, 670, 297, 293, 264, 661, 472, 307, 51124, 51124, 264, 517, 5614, 1937, 12539, 597, 307, 472, 670, 297, 3175, 472, 586, 13181, 356, 294, 264, 3035, 341, 307, 406, 588, 51444, 51444, 4448, 7619, 293, 611, 309, 311, 257, 2607, 300, 733, 295, 7001, 741, 519, 436, 366, 1228, 264, 28035, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.07486766012091385, "compression_ratio": 1.95, "no_speech_prob": 6.143910468381364e-06}, {"id": 639, "seek": 393296, "start": 3954.56, "end": 3959.92, "text": " clearly described and also it's a detail that kind of matters i think they are using the biased", "tokens": [50364, 2602, 406, 472, 670, 297, 3175, 472, 456, 275, 307, 527, 297, 293, 370, 309, 4523, 484, 300, 456, 366, 732, 2098, 295, 50756, 50756, 8017, 990, 21977, 295, 364, 10225, 472, 307, 264, 28035, 12539, 597, 307, 472, 670, 297, 293, 264, 661, 472, 307, 51124, 51124, 264, 517, 5614, 1937, 12539, 597, 307, 472, 670, 297, 3175, 472, 586, 13181, 356, 294, 264, 3035, 341, 307, 406, 588, 51444, 51444, 4448, 7619, 293, 611, 309, 311, 257, 2607, 300, 733, 295, 7001, 741, 519, 436, 366, 1228, 264, 28035, 51712, 51712], "temperature": 0.0, "avg_logprob": -0.07486766012091385, "compression_ratio": 1.95, "no_speech_prob": 6.143910468381364e-06}, {"id": 640, "seek": 395992, "start": 3959.92, "end": 3964.7200000000003, "text": " version at training time but later when they are talking about the inference they are mentioning", "tokens": [50364, 3037, 412, 3097, 565, 457, 1780, 562, 436, 366, 1417, 466, 264, 38253, 436, 366, 18315, 50604, 50604, 300, 562, 436, 360, 264, 38253, 436, 366, 1228, 264, 517, 5614, 1937, 12539, 597, 307, 264, 297, 3175, 472, 3037, 50896, 50916, 294, 1936, 337, 38253, 293, 281, 21583, 4404, 264, 264, 2614, 914, 293, 264, 2614, 21977, 1936, 51364, 51364, 293, 370, 436, 436, 767, 5366, 257, 3847, 1500, 23220, 852, 689, 294, 3097, 436, 764, 264, 28035, 51592, 51592, 3037, 293, 294, 264, 293, 1500, 565, 436, 764, 264, 517, 5614, 1937, 3037, 741, 915, 341, 4664, 13181, 51844], "temperature": 0.0, "avg_logprob": -0.10050058364868164, "compression_ratio": 2.1077586206896552, "no_speech_prob": 5.507412424776703e-06}, {"id": 641, "seek": 395992, "start": 3964.7200000000003, "end": 3970.56, "text": " that when they do the inference they are using the unbiased estimate which is the n minus one version", "tokens": [50364, 3037, 412, 3097, 565, 457, 1780, 562, 436, 366, 1417, 466, 264, 38253, 436, 366, 18315, 50604, 50604, 300, 562, 436, 360, 264, 38253, 436, 366, 1228, 264, 517, 5614, 1937, 12539, 597, 307, 264, 297, 3175, 472, 3037, 50896, 50916, 294, 1936, 337, 38253, 293, 281, 21583, 4404, 264, 264, 2614, 914, 293, 264, 2614, 21977, 1936, 51364, 51364, 293, 370, 436, 436, 767, 5366, 257, 3847, 1500, 23220, 852, 689, 294, 3097, 436, 764, 264, 28035, 51592, 51592, 3037, 293, 294, 264, 293, 1500, 565, 436, 764, 264, 517, 5614, 1937, 3037, 741, 915, 341, 4664, 13181, 51844], "temperature": 0.0, "avg_logprob": -0.10050058364868164, "compression_ratio": 2.1077586206896552, "no_speech_prob": 5.507412424776703e-06}, {"id": 642, "seek": 395992, "start": 3970.96, "end": 3979.92, "text": " in basically for inference and to calibrate the the running mean and the running variance basically", "tokens": [50364, 3037, 412, 3097, 565, 457, 1780, 562, 436, 366, 1417, 466, 264, 38253, 436, 366, 18315, 50604, 50604, 300, 562, 436, 360, 264, 38253, 436, 366, 1228, 264, 517, 5614, 1937, 12539, 597, 307, 264, 297, 3175, 472, 3037, 50896, 50916, 294, 1936, 337, 38253, 293, 281, 21583, 4404, 264, 264, 2614, 914, 293, 264, 2614, 21977, 1936, 51364, 51364, 293, 370, 436, 436, 767, 5366, 257, 3847, 1500, 23220, 852, 689, 294, 3097, 436, 764, 264, 28035, 51592, 51592, 3037, 293, 294, 264, 293, 1500, 565, 436, 764, 264, 517, 5614, 1937, 3037, 741, 915, 341, 4664, 13181, 51844], "temperature": 0.0, "avg_logprob": -0.10050058364868164, "compression_ratio": 2.1077586206896552, "no_speech_prob": 5.507412424776703e-06}, {"id": 643, "seek": 395992, "start": 3979.92, "end": 3984.48, "text": " and so they they actually introduce a train test mismatch where in training they use the biased", "tokens": [50364, 3037, 412, 3097, 565, 457, 1780, 562, 436, 366, 1417, 466, 264, 38253, 436, 366, 18315, 50604, 50604, 300, 562, 436, 360, 264, 38253, 436, 366, 1228, 264, 517, 5614, 1937, 12539, 597, 307, 264, 297, 3175, 472, 3037, 50896, 50916, 294, 1936, 337, 38253, 293, 281, 21583, 4404, 264, 264, 2614, 914, 293, 264, 2614, 21977, 1936, 51364, 51364, 293, 370, 436, 436, 767, 5366, 257, 3847, 1500, 23220, 852, 689, 294, 3097, 436, 764, 264, 28035, 51592, 51592, 3037, 293, 294, 264, 293, 1500, 565, 436, 764, 264, 517, 5614, 1937, 3037, 741, 915, 341, 4664, 13181, 51844], "temperature": 0.0, "avg_logprob": -0.10050058364868164, "compression_ratio": 2.1077586206896552, "no_speech_prob": 5.507412424776703e-06}, {"id": 644, "seek": 398448, "start": 3984.48, "end": 3990.08, "text": " version and in the and test time they use the unbiased version i find this extremely confusing", "tokens": [50364, 3037, 293, 294, 264, 293, 1500, 565, 436, 764, 264, 517, 5614, 1937, 3037, 741, 915, 341, 4664, 13181, 50644, 50644, 291, 393, 1401, 544, 466, 264, 1151, 12, 1832, 19984, 293, 983, 26764, 538, 297, 3175, 472, 2709, 291, 257, 1101, 50928, 50928, 12539, 295, 264, 21977, 294, 264, 1389, 689, 291, 362, 4415, 11602, 420, 10938, 337, 257, 4415, 51164, 51216, 436, 366, 588, 1359, 293, 300, 307, 6451, 264, 1389, 337, 505, 570, 321, 366, 6260, 365, 867, 51460, 51460, 15245, 279, 293, 613, 8382, 12, 76, 852, 279, 366, 257, 1359, 6889, 295, 257, 4833, 4415, 597, 307, 264, 2302, 3097, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.07349365129383333, "compression_ratio": 1.821969696969697, "no_speech_prob": 3.0893613711668877e-06}, {"id": 645, "seek": 398448, "start": 3990.08, "end": 3995.76, "text": " you can read more about the best-less correction and why dividing by n minus one gives you a better", "tokens": [50364, 3037, 293, 294, 264, 293, 1500, 565, 436, 764, 264, 517, 5614, 1937, 3037, 741, 915, 341, 4664, 13181, 50644, 50644, 291, 393, 1401, 544, 466, 264, 1151, 12, 1832, 19984, 293, 983, 26764, 538, 297, 3175, 472, 2709, 291, 257, 1101, 50928, 50928, 12539, 295, 264, 21977, 294, 264, 1389, 689, 291, 362, 4415, 11602, 420, 10938, 337, 257, 4415, 51164, 51216, 436, 366, 588, 1359, 293, 300, 307, 6451, 264, 1389, 337, 505, 570, 321, 366, 6260, 365, 867, 51460, 51460, 15245, 279, 293, 613, 8382, 12, 76, 852, 279, 366, 257, 1359, 6889, 295, 257, 4833, 4415, 597, 307, 264, 2302, 3097, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.07349365129383333, "compression_ratio": 1.821969696969697, "no_speech_prob": 3.0893613711668877e-06}, {"id": 646, "seek": 398448, "start": 3995.76, "end": 4000.48, "text": " estimate of the variance in the case where you have population sizes or samples for a population", "tokens": [50364, 3037, 293, 294, 264, 293, 1500, 565, 436, 764, 264, 517, 5614, 1937, 3037, 741, 915, 341, 4664, 13181, 50644, 50644, 291, 393, 1401, 544, 466, 264, 1151, 12, 1832, 19984, 293, 983, 26764, 538, 297, 3175, 472, 2709, 291, 257, 1101, 50928, 50928, 12539, 295, 264, 21977, 294, 264, 1389, 689, 291, 362, 4415, 11602, 420, 10938, 337, 257, 4415, 51164, 51216, 436, 366, 588, 1359, 293, 300, 307, 6451, 264, 1389, 337, 505, 570, 321, 366, 6260, 365, 867, 51460, 51460, 15245, 279, 293, 613, 8382, 12, 76, 852, 279, 366, 257, 1359, 6889, 295, 257, 4833, 4415, 597, 307, 264, 2302, 3097, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.07349365129383333, "compression_ratio": 1.821969696969697, "no_speech_prob": 3.0893613711668877e-06}, {"id": 647, "seek": 398448, "start": 4001.52, "end": 4006.4, "text": " they are very small and that is indeed the case for us because we are dealing with many", "tokens": [50364, 3037, 293, 294, 264, 293, 1500, 565, 436, 764, 264, 517, 5614, 1937, 3037, 741, 915, 341, 4664, 13181, 50644, 50644, 291, 393, 1401, 544, 466, 264, 1151, 12, 1832, 19984, 293, 983, 26764, 538, 297, 3175, 472, 2709, 291, 257, 1101, 50928, 50928, 12539, 295, 264, 21977, 294, 264, 1389, 689, 291, 362, 4415, 11602, 420, 10938, 337, 257, 4415, 51164, 51216, 436, 366, 588, 1359, 293, 300, 307, 6451, 264, 1389, 337, 505, 570, 321, 366, 6260, 365, 867, 51460, 51460, 15245, 279, 293, 613, 8382, 12, 76, 852, 279, 366, 257, 1359, 6889, 295, 257, 4833, 4415, 597, 307, 264, 2302, 3097, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.07349365129383333, "compression_ratio": 1.821969696969697, "no_speech_prob": 3.0893613711668877e-06}, {"id": 648, "seek": 398448, "start": 4006.4, "end": 4011.84, "text": " batches and these mini-matches are a small sample of a larger population which is the entire training", "tokens": [50364, 3037, 293, 294, 264, 293, 1500, 565, 436, 764, 264, 517, 5614, 1937, 3037, 741, 915, 341, 4664, 13181, 50644, 50644, 291, 393, 1401, 544, 466, 264, 1151, 12, 1832, 19984, 293, 983, 26764, 538, 297, 3175, 472, 2709, 291, 257, 1101, 50928, 50928, 12539, 295, 264, 21977, 294, 264, 1389, 689, 291, 362, 4415, 11602, 420, 10938, 337, 257, 4415, 51164, 51216, 436, 366, 588, 1359, 293, 300, 307, 6451, 264, 1389, 337, 505, 570, 321, 366, 6260, 365, 867, 51460, 51460, 15245, 279, 293, 613, 8382, 12, 76, 852, 279, 366, 257, 1359, 6889, 295, 257, 4833, 4415, 597, 307, 264, 2302, 3097, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.07349365129383333, "compression_ratio": 1.821969696969697, "no_speech_prob": 3.0893613711668877e-06}, {"id": 649, "seek": 401184, "start": 4011.84, "end": 4017.6000000000004, "text": " set and so it just turns out that if you just estimate it using one over n that actually almost", "tokens": [50364, 992, 293, 370, 309, 445, 4523, 484, 300, 498, 291, 445, 12539, 309, 1228, 472, 670, 297, 300, 767, 1920, 50652, 50652, 1009, 24612, 332, 1024, 264, 21977, 293, 309, 307, 257, 28035, 8017, 1639, 293, 309, 307, 26269, 300, 291, 764, 50908, 50908, 264, 517, 5614, 1937, 3037, 293, 9845, 538, 297, 3175, 472, 293, 291, 393, 352, 807, 341, 7222, 510, 300, 741, 4501, 51164, 51164, 300, 767, 15626, 264, 1577, 21577, 293, 741, 603, 2113, 309, 294, 264, 960, 3855, 51332, 51388, 586, 562, 291, 8873, 264, 3930, 82, 357, 404, 21977, 291, 603, 3449, 300, 436, 747, 264, 517, 5614, 1937, 7166, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.05246370488947088, "compression_ratio": 1.9180327868852458, "no_speech_prob": 9.222749213222414e-06}, {"id": 650, "seek": 401184, "start": 4017.6000000000004, "end": 4022.7200000000003, "text": " always underestimates the variance and it is a biased estimator and it is advised that you use", "tokens": [50364, 992, 293, 370, 309, 445, 4523, 484, 300, 498, 291, 445, 12539, 309, 1228, 472, 670, 297, 300, 767, 1920, 50652, 50652, 1009, 24612, 332, 1024, 264, 21977, 293, 309, 307, 257, 28035, 8017, 1639, 293, 309, 307, 26269, 300, 291, 764, 50908, 50908, 264, 517, 5614, 1937, 3037, 293, 9845, 538, 297, 3175, 472, 293, 291, 393, 352, 807, 341, 7222, 510, 300, 741, 4501, 51164, 51164, 300, 767, 15626, 264, 1577, 21577, 293, 741, 603, 2113, 309, 294, 264, 960, 3855, 51332, 51388, 586, 562, 291, 8873, 264, 3930, 82, 357, 404, 21977, 291, 603, 3449, 300, 436, 747, 264, 517, 5614, 1937, 7166, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.05246370488947088, "compression_ratio": 1.9180327868852458, "no_speech_prob": 9.222749213222414e-06}, {"id": 651, "seek": 401184, "start": 4022.7200000000003, "end": 4027.84, "text": " the unbiased version and divide by n minus one and you can go through this article here that i liked", "tokens": [50364, 992, 293, 370, 309, 445, 4523, 484, 300, 498, 291, 445, 12539, 309, 1228, 472, 670, 297, 300, 767, 1920, 50652, 50652, 1009, 24612, 332, 1024, 264, 21977, 293, 309, 307, 257, 28035, 8017, 1639, 293, 309, 307, 26269, 300, 291, 764, 50908, 50908, 264, 517, 5614, 1937, 3037, 293, 9845, 538, 297, 3175, 472, 293, 291, 393, 352, 807, 341, 7222, 510, 300, 741, 4501, 51164, 51164, 300, 767, 15626, 264, 1577, 21577, 293, 741, 603, 2113, 309, 294, 264, 960, 3855, 51332, 51388, 586, 562, 291, 8873, 264, 3930, 82, 357, 404, 21977, 291, 603, 3449, 300, 436, 747, 264, 517, 5614, 1937, 7166, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.05246370488947088, "compression_ratio": 1.9180327868852458, "no_speech_prob": 9.222749213222414e-06}, {"id": 652, "seek": 401184, "start": 4027.84, "end": 4031.2000000000003, "text": " that actually describes the full reasoning and i'll link it in the video description", "tokens": [50364, 992, 293, 370, 309, 445, 4523, 484, 300, 498, 291, 445, 12539, 309, 1228, 472, 670, 297, 300, 767, 1920, 50652, 50652, 1009, 24612, 332, 1024, 264, 21977, 293, 309, 307, 257, 28035, 8017, 1639, 293, 309, 307, 26269, 300, 291, 764, 50908, 50908, 264, 517, 5614, 1937, 3037, 293, 9845, 538, 297, 3175, 472, 293, 291, 393, 352, 807, 341, 7222, 510, 300, 741, 4501, 51164, 51164, 300, 767, 15626, 264, 1577, 21577, 293, 741, 603, 2113, 309, 294, 264, 960, 3855, 51332, 51388, 586, 562, 291, 8873, 264, 3930, 82, 357, 404, 21977, 291, 603, 3449, 300, 436, 747, 264, 517, 5614, 1937, 7166, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.05246370488947088, "compression_ratio": 1.9180327868852458, "no_speech_prob": 9.222749213222414e-06}, {"id": 653, "seek": 401184, "start": 4032.32, "end": 4037.44, "text": " now when you calculate the torshtop variance you'll notice that they take the unbiased flag", "tokens": [50364, 992, 293, 370, 309, 445, 4523, 484, 300, 498, 291, 445, 12539, 309, 1228, 472, 670, 297, 300, 767, 1920, 50652, 50652, 1009, 24612, 332, 1024, 264, 21977, 293, 309, 307, 257, 28035, 8017, 1639, 293, 309, 307, 26269, 300, 291, 764, 50908, 50908, 264, 517, 5614, 1937, 3037, 293, 9845, 538, 297, 3175, 472, 293, 291, 393, 352, 807, 341, 7222, 510, 300, 741, 4501, 51164, 51164, 300, 767, 15626, 264, 1577, 21577, 293, 741, 603, 2113, 309, 294, 264, 960, 3855, 51332, 51388, 586, 562, 291, 8873, 264, 3930, 82, 357, 404, 21977, 291, 603, 3449, 300, 436, 747, 264, 517, 5614, 1937, 7166, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.05246370488947088, "compression_ratio": 1.9180327868852458, "no_speech_prob": 9.222749213222414e-06}, {"id": 654, "seek": 403744, "start": 4037.44, "end": 4043.28, "text": " whether or not you want to divide by n or n minus one confusingly they do not mention what the", "tokens": [50364, 1968, 420, 406, 291, 528, 281, 9845, 538, 297, 420, 297, 3175, 472, 13181, 356, 436, 360, 406, 2152, 437, 264, 50656, 50656, 7576, 307, 337, 517, 5614, 1937, 457, 741, 1697, 517, 5614, 1937, 538, 7576, 307, 2074, 741, 478, 406, 988, 983, 264, 45623, 510, 50976, 50976, 500, 380, 37771, 300, 586, 294, 264, 15245, 2026, 502, 67, 264, 14333, 797, 307, 733, 295, 2085, 293, 13181, 51324, 51324, 309, 1619, 300, 264, 3832, 25163, 307, 15598, 5766, 264, 28035, 8017, 1639, 457, 341, 307, 767, 406, 51608, 51608, 2293, 558, 293, 561, 362, 10932, 484, 300, 309, 307, 406, 558, 294, 257, 1230, 295, 2663, 1670, 550, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.0460868425536574, "compression_ratio": 1.795539033457249, "no_speech_prob": 6.643188044108683e-06}, {"id": 655, "seek": 403744, "start": 4043.28, "end": 4049.68, "text": " default is for unbiased but i believe unbiased by default is true i'm not sure why the docs here", "tokens": [50364, 1968, 420, 406, 291, 528, 281, 9845, 538, 297, 420, 297, 3175, 472, 13181, 356, 436, 360, 406, 2152, 437, 264, 50656, 50656, 7576, 307, 337, 517, 5614, 1937, 457, 741, 1697, 517, 5614, 1937, 538, 7576, 307, 2074, 741, 478, 406, 988, 983, 264, 45623, 510, 50976, 50976, 500, 380, 37771, 300, 586, 294, 264, 15245, 2026, 502, 67, 264, 14333, 797, 307, 733, 295, 2085, 293, 13181, 51324, 51324, 309, 1619, 300, 264, 3832, 25163, 307, 15598, 5766, 264, 28035, 8017, 1639, 457, 341, 307, 767, 406, 51608, 51608, 2293, 558, 293, 561, 362, 10932, 484, 300, 309, 307, 406, 558, 294, 257, 1230, 295, 2663, 1670, 550, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.0460868425536574, "compression_ratio": 1.795539033457249, "no_speech_prob": 6.643188044108683e-06}, {"id": 656, "seek": 403744, "start": 4049.68, "end": 4056.64, "text": " don't cite that now in the batch norm 1d the documentation again is kind of wrong and confusing", "tokens": [50364, 1968, 420, 406, 291, 528, 281, 9845, 538, 297, 420, 297, 3175, 472, 13181, 356, 436, 360, 406, 2152, 437, 264, 50656, 50656, 7576, 307, 337, 517, 5614, 1937, 457, 741, 1697, 517, 5614, 1937, 538, 7576, 307, 2074, 741, 478, 406, 988, 983, 264, 45623, 510, 50976, 50976, 500, 380, 37771, 300, 586, 294, 264, 15245, 2026, 502, 67, 264, 14333, 797, 307, 733, 295, 2085, 293, 13181, 51324, 51324, 309, 1619, 300, 264, 3832, 25163, 307, 15598, 5766, 264, 28035, 8017, 1639, 457, 341, 307, 767, 406, 51608, 51608, 2293, 558, 293, 561, 362, 10932, 484, 300, 309, 307, 406, 558, 294, 257, 1230, 295, 2663, 1670, 550, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.0460868425536574, "compression_ratio": 1.795539033457249, "no_speech_prob": 6.643188044108683e-06}, {"id": 657, "seek": 403744, "start": 4056.64, "end": 4062.32, "text": " it says that the standard deviation is calculated via the biased estimator but this is actually not", "tokens": [50364, 1968, 420, 406, 291, 528, 281, 9845, 538, 297, 420, 297, 3175, 472, 13181, 356, 436, 360, 406, 2152, 437, 264, 50656, 50656, 7576, 307, 337, 517, 5614, 1937, 457, 741, 1697, 517, 5614, 1937, 538, 7576, 307, 2074, 741, 478, 406, 988, 983, 264, 45623, 510, 50976, 50976, 500, 380, 37771, 300, 586, 294, 264, 15245, 2026, 502, 67, 264, 14333, 797, 307, 733, 295, 2085, 293, 13181, 51324, 51324, 309, 1619, 300, 264, 3832, 25163, 307, 15598, 5766, 264, 28035, 8017, 1639, 457, 341, 307, 767, 406, 51608, 51608, 2293, 558, 293, 561, 362, 10932, 484, 300, 309, 307, 406, 558, 294, 257, 1230, 295, 2663, 1670, 550, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.0460868425536574, "compression_ratio": 1.795539033457249, "no_speech_prob": 6.643188044108683e-06}, {"id": 658, "seek": 403744, "start": 4062.32, "end": 4067.28, "text": " exactly right and people have pointed out that it is not right in a number of issues since then", "tokens": [50364, 1968, 420, 406, 291, 528, 281, 9845, 538, 297, 420, 297, 3175, 472, 13181, 356, 436, 360, 406, 2152, 437, 264, 50656, 50656, 7576, 307, 337, 517, 5614, 1937, 457, 741, 1697, 517, 5614, 1937, 538, 7576, 307, 2074, 741, 478, 406, 988, 983, 264, 45623, 510, 50976, 50976, 500, 380, 37771, 300, 586, 294, 264, 15245, 2026, 502, 67, 264, 14333, 797, 307, 733, 295, 2085, 293, 13181, 51324, 51324, 309, 1619, 300, 264, 3832, 25163, 307, 15598, 5766, 264, 28035, 8017, 1639, 457, 341, 307, 767, 406, 51608, 51608, 2293, 558, 293, 561, 362, 10932, 484, 300, 309, 307, 406, 558, 294, 257, 1230, 295, 2663, 1670, 550, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.0460868425536574, "compression_ratio": 1.795539033457249, "no_speech_prob": 6.643188044108683e-06}, {"id": 659, "seek": 406728, "start": 4067.28, "end": 4072.88, "text": " uh because actually the rabbit hole is deeper and they follow the paper exactly and they use the", "tokens": [50364, 2232, 570, 767, 264, 19509, 5458, 307, 7731, 293, 436, 1524, 264, 3035, 2293, 293, 436, 764, 264, 50644, 50644, 28035, 3037, 337, 3097, 457, 562, 436, 434, 8017, 990, 264, 2614, 3832, 25163, 50876, 50876, 321, 366, 1228, 264, 517, 5614, 1937, 3037, 370, 797, 456, 311, 264, 3847, 1500, 23220, 852, 370, 938, 1657, 2099, 741, 478, 51176, 51176, 406, 257, 3429, 295, 3847, 1500, 2983, 19919, 32286, 741, 1936, 733, 295, 1949, 1105, 264, 1186, 300, 321, 764, 264, 12577, 51528, 51528, 3037, 264, 3097, 565, 293, 264, 517, 5614, 1937, 1500, 565, 741, 1936, 1949, 341, 281, 312, 257, 7426, 293, 741, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.05014758977023038, "compression_ratio": 1.9551020408163264, "no_speech_prob": 9.36816923058359e-06}, {"id": 660, "seek": 406728, "start": 4072.88, "end": 4077.52, "text": " biased version for training but when they're estimating the running standard deviation", "tokens": [50364, 2232, 570, 767, 264, 19509, 5458, 307, 7731, 293, 436, 1524, 264, 3035, 2293, 293, 436, 764, 264, 50644, 50644, 28035, 3037, 337, 3097, 457, 562, 436, 434, 8017, 990, 264, 2614, 3832, 25163, 50876, 50876, 321, 366, 1228, 264, 517, 5614, 1937, 3037, 370, 797, 456, 311, 264, 3847, 1500, 23220, 852, 370, 938, 1657, 2099, 741, 478, 51176, 51176, 406, 257, 3429, 295, 3847, 1500, 2983, 19919, 32286, 741, 1936, 733, 295, 1949, 1105, 264, 1186, 300, 321, 764, 264, 12577, 51528, 51528, 3037, 264, 3097, 565, 293, 264, 517, 5614, 1937, 1500, 565, 741, 1936, 1949, 341, 281, 312, 257, 7426, 293, 741, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.05014758977023038, "compression_ratio": 1.9551020408163264, "no_speech_prob": 9.36816923058359e-06}, {"id": 661, "seek": 406728, "start": 4077.52, "end": 4083.52, "text": " we are using the unbiased version so again there's the train test mismatch so long story short i'm", "tokens": [50364, 2232, 570, 767, 264, 19509, 5458, 307, 7731, 293, 436, 1524, 264, 3035, 2293, 293, 436, 764, 264, 50644, 50644, 28035, 3037, 337, 3097, 457, 562, 436, 434, 8017, 990, 264, 2614, 3832, 25163, 50876, 50876, 321, 366, 1228, 264, 517, 5614, 1937, 3037, 370, 797, 456, 311, 264, 3847, 1500, 23220, 852, 370, 938, 1657, 2099, 741, 478, 51176, 51176, 406, 257, 3429, 295, 3847, 1500, 2983, 19919, 32286, 741, 1936, 733, 295, 1949, 1105, 264, 1186, 300, 321, 764, 264, 12577, 51528, 51528, 3037, 264, 3097, 565, 293, 264, 517, 5614, 1937, 1500, 565, 741, 1936, 1949, 341, 281, 312, 257, 7426, 293, 741, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.05014758977023038, "compression_ratio": 1.9551020408163264, "no_speech_prob": 9.36816923058359e-06}, {"id": 662, "seek": 406728, "start": 4083.52, "end": 4090.5600000000004, "text": " not a fan of train test discrepancies i basically kind of consider um the fact that we use the bias", "tokens": [50364, 2232, 570, 767, 264, 19509, 5458, 307, 7731, 293, 436, 1524, 264, 3035, 2293, 293, 436, 764, 264, 50644, 50644, 28035, 3037, 337, 3097, 457, 562, 436, 434, 8017, 990, 264, 2614, 3832, 25163, 50876, 50876, 321, 366, 1228, 264, 517, 5614, 1937, 3037, 370, 797, 456, 311, 264, 3847, 1500, 23220, 852, 370, 938, 1657, 2099, 741, 478, 51176, 51176, 406, 257, 3429, 295, 3847, 1500, 2983, 19919, 32286, 741, 1936, 733, 295, 1949, 1105, 264, 1186, 300, 321, 764, 264, 12577, 51528, 51528, 3037, 264, 3097, 565, 293, 264, 517, 5614, 1937, 1500, 565, 741, 1936, 1949, 341, 281, 312, 257, 7426, 293, 741, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.05014758977023038, "compression_ratio": 1.9551020408163264, "no_speech_prob": 9.36816923058359e-06}, {"id": 663, "seek": 406728, "start": 4090.5600000000004, "end": 4095.52, "text": " version the training time and the unbiased test time i basically consider this to be a bug and i", "tokens": [50364, 2232, 570, 767, 264, 19509, 5458, 307, 7731, 293, 436, 1524, 264, 3035, 2293, 293, 436, 764, 264, 50644, 50644, 28035, 3037, 337, 3097, 457, 562, 436, 434, 8017, 990, 264, 2614, 3832, 25163, 50876, 50876, 321, 366, 1228, 264, 517, 5614, 1937, 3037, 370, 797, 456, 311, 264, 3847, 1500, 23220, 852, 370, 938, 1657, 2099, 741, 478, 51176, 51176, 406, 257, 3429, 295, 3847, 1500, 2983, 19919, 32286, 741, 1936, 733, 295, 1949, 1105, 264, 1186, 300, 321, 764, 264, 12577, 51528, 51528, 3037, 264, 3097, 565, 293, 264, 517, 5614, 1937, 1500, 565, 741, 1936, 1949, 341, 281, 312, 257, 7426, 293, 741, 51776, 51776], "temperature": 0.0, "avg_logprob": -0.05014758977023038, "compression_ratio": 1.9551020408163264, "no_speech_prob": 9.36816923058359e-06}, {"id": 664, "seek": 409552, "start": 4095.52, "end": 4099.2, "text": " don't think that there's a good reason for that um it's not really they don't really go into the", "tokens": [50364, 500, 380, 519, 300, 456, 311, 257, 665, 1778, 337, 300, 1105, 309, 311, 406, 534, 436, 500, 380, 534, 352, 666, 264, 50548, 50548, 2607, 295, 264, 21577, 2261, 309, 294, 341, 3035, 370, 300, 311, 983, 741, 1936, 4382, 281, 764, 264, 1151, 50816, 50816, 1329, 19984, 294, 452, 1065, 589, 7015, 15245, 2026, 775, 406, 747, 257, 20428, 6770, 300, 5112, 51088, 51088, 291, 1968, 420, 406, 1105, 291, 528, 281, 764, 264, 517, 5614, 1937, 3037, 295, 264, 28035, 3037, 294, 1293, 3097, 51332, 51332, 6921, 293, 370, 4412, 2878, 1228, 15245, 2710, 2144, 1936, 294, 452, 1910, 575, 257, 857, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.06546334170420236, "compression_ratio": 1.817490494296578, "no_speech_prob": 4.1810613993220613e-07}, {"id": 665, "seek": 409552, "start": 4099.2, "end": 4104.56, "text": " detail of the reasoning behind it in this paper so that's why i basically prefer to use the best", "tokens": [50364, 500, 380, 519, 300, 456, 311, 257, 665, 1778, 337, 300, 1105, 309, 311, 406, 534, 436, 500, 380, 534, 352, 666, 264, 50548, 50548, 2607, 295, 264, 21577, 2261, 309, 294, 341, 3035, 370, 300, 311, 983, 741, 1936, 4382, 281, 764, 264, 1151, 50816, 50816, 1329, 19984, 294, 452, 1065, 589, 7015, 15245, 2026, 775, 406, 747, 257, 20428, 6770, 300, 5112, 51088, 51088, 291, 1968, 420, 406, 1105, 291, 528, 281, 764, 264, 517, 5614, 1937, 3037, 295, 264, 28035, 3037, 294, 1293, 3097, 51332, 51332, 6921, 293, 370, 4412, 2878, 1228, 15245, 2710, 2144, 1936, 294, 452, 1910, 575, 257, 857, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.06546334170420236, "compression_ratio": 1.817490494296578, "no_speech_prob": 4.1810613993220613e-07}, {"id": 666, "seek": 409552, "start": 4104.56, "end": 4110.0, "text": " list correction in my own work unfortunately batch norm does not take a keyword argument that tells", "tokens": [50364, 500, 380, 519, 300, 456, 311, 257, 665, 1778, 337, 300, 1105, 309, 311, 406, 534, 436, 500, 380, 534, 352, 666, 264, 50548, 50548, 2607, 295, 264, 21577, 2261, 309, 294, 341, 3035, 370, 300, 311, 983, 741, 1936, 4382, 281, 764, 264, 1151, 50816, 50816, 1329, 19984, 294, 452, 1065, 589, 7015, 15245, 2026, 775, 406, 747, 257, 20428, 6770, 300, 5112, 51088, 51088, 291, 1968, 420, 406, 1105, 291, 528, 281, 764, 264, 517, 5614, 1937, 3037, 295, 264, 28035, 3037, 294, 1293, 3097, 51332, 51332, 6921, 293, 370, 4412, 2878, 1228, 15245, 2710, 2144, 1936, 294, 452, 1910, 575, 257, 857, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.06546334170420236, "compression_ratio": 1.817490494296578, "no_speech_prob": 4.1810613993220613e-07}, {"id": 667, "seek": 409552, "start": 4110.0, "end": 4114.88, "text": " you whether or not um you want to use the unbiased version of the biased version in both training", "tokens": [50364, 500, 380, 519, 300, 456, 311, 257, 665, 1778, 337, 300, 1105, 309, 311, 406, 534, 436, 500, 380, 534, 352, 666, 264, 50548, 50548, 2607, 295, 264, 21577, 2261, 309, 294, 341, 3035, 370, 300, 311, 983, 741, 1936, 4382, 281, 764, 264, 1151, 50816, 50816, 1329, 19984, 294, 452, 1065, 589, 7015, 15245, 2026, 775, 406, 747, 257, 20428, 6770, 300, 5112, 51088, 51088, 291, 1968, 420, 406, 1105, 291, 528, 281, 764, 264, 517, 5614, 1937, 3037, 295, 264, 28035, 3037, 294, 1293, 3097, 51332, 51332, 6921, 293, 370, 4412, 2878, 1228, 15245, 2710, 2144, 1936, 294, 452, 1910, 575, 257, 857, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.06546334170420236, "compression_ratio": 1.817490494296578, "no_speech_prob": 4.1810613993220613e-07}, {"id": 668, "seek": 409552, "start": 4114.88, "end": 4119.2, "text": " tests and so therefore anyone using batch normalization basically in my view has a bit", "tokens": [50364, 500, 380, 519, 300, 456, 311, 257, 665, 1778, 337, 300, 1105, 309, 311, 406, 534, 436, 500, 380, 534, 352, 666, 264, 50548, 50548, 2607, 295, 264, 21577, 2261, 309, 294, 341, 3035, 370, 300, 311, 983, 741, 1936, 4382, 281, 764, 264, 1151, 50816, 50816, 1329, 19984, 294, 452, 1065, 589, 7015, 15245, 2026, 775, 406, 747, 257, 20428, 6770, 300, 5112, 51088, 51088, 291, 1968, 420, 406, 1105, 291, 528, 281, 764, 264, 517, 5614, 1937, 3037, 295, 264, 28035, 3037, 294, 1293, 3097, 51332, 51332, 6921, 293, 370, 4412, 2878, 1228, 15245, 2710, 2144, 1936, 294, 452, 1910, 575, 257, 857, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.06546334170420236, "compression_ratio": 1.817490494296578, "no_speech_prob": 4.1810613993220613e-07}, {"id": 669, "seek": 411920, "start": 4119.2, "end": 4126.24, "text": " of a bug in the code um and this turns out to be much less of a problem if your batch many batch", "tokens": [50364, 295, 257, 7426, 294, 264, 3089, 1105, 293, 341, 4523, 484, 281, 312, 709, 1570, 295, 257, 1154, 498, 428, 15245, 867, 15245, 50716, 50716, 11602, 366, 257, 857, 4833, 457, 920, 741, 445, 915, 309, 733, 295, 2232, 20994, 304, 31415, 370, 1310, 1580, 393, 2903, 50988, 50988, 983, 341, 307, 1392, 457, 337, 586, 741, 4382, 281, 764, 264, 517, 5614, 1937, 3037, 14961, 1293, 1830, 3097, 51268, 51268, 293, 412, 1500, 565, 293, 300, 311, 983, 741, 478, 1228, 472, 670, 297, 3175, 472, 510, 1392, 370, 718, 311, 586, 767, 646, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.054781740361993965, "compression_ratio": 1.65, "no_speech_prob": 2.443911625960027e-06}, {"id": 670, "seek": 411920, "start": 4126.24, "end": 4131.679999999999, "text": " sizes are a bit larger but still i just find it kind of uh unpalatable so maybe someone can explain", "tokens": [50364, 295, 257, 7426, 294, 264, 3089, 1105, 293, 341, 4523, 484, 281, 312, 709, 1570, 295, 257, 1154, 498, 428, 15245, 867, 15245, 50716, 50716, 11602, 366, 257, 857, 4833, 457, 920, 741, 445, 915, 309, 733, 295, 2232, 20994, 304, 31415, 370, 1310, 1580, 393, 2903, 50988, 50988, 983, 341, 307, 1392, 457, 337, 586, 741, 4382, 281, 764, 264, 517, 5614, 1937, 3037, 14961, 1293, 1830, 3097, 51268, 51268, 293, 412, 1500, 565, 293, 300, 311, 983, 741, 478, 1228, 472, 670, 297, 3175, 472, 510, 1392, 370, 718, 311, 586, 767, 646, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.054781740361993965, "compression_ratio": 1.65, "no_speech_prob": 2.443911625960027e-06}, {"id": 671, "seek": 411920, "start": 4131.679999999999, "end": 4137.28, "text": " why this is okay but for now i prefer to use the unbiased version consistently both during training", "tokens": [50364, 295, 257, 7426, 294, 264, 3089, 1105, 293, 341, 4523, 484, 281, 312, 709, 1570, 295, 257, 1154, 498, 428, 15245, 867, 15245, 50716, 50716, 11602, 366, 257, 857, 4833, 457, 920, 741, 445, 915, 309, 733, 295, 2232, 20994, 304, 31415, 370, 1310, 1580, 393, 2903, 50988, 50988, 983, 341, 307, 1392, 457, 337, 586, 741, 4382, 281, 764, 264, 517, 5614, 1937, 3037, 14961, 1293, 1830, 3097, 51268, 51268, 293, 412, 1500, 565, 293, 300, 311, 983, 741, 478, 1228, 472, 670, 297, 3175, 472, 510, 1392, 370, 718, 311, 586, 767, 646, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.054781740361993965, "compression_ratio": 1.65, "no_speech_prob": 2.443911625960027e-06}, {"id": 672, "seek": 411920, "start": 4137.28, "end": 4143.04, "text": " and at test time and that's why i'm using one over n minus one here okay so let's now actually back", "tokens": [50364, 295, 257, 7426, 294, 264, 3089, 1105, 293, 341, 4523, 484, 281, 312, 709, 1570, 295, 257, 1154, 498, 428, 15245, 867, 15245, 50716, 50716, 11602, 366, 257, 857, 4833, 457, 920, 741, 445, 915, 309, 733, 295, 2232, 20994, 304, 31415, 370, 1310, 1580, 393, 2903, 50988, 50988, 983, 341, 307, 1392, 457, 337, 586, 741, 4382, 281, 764, 264, 517, 5614, 1937, 3037, 14961, 1293, 1830, 3097, 51268, 51268, 293, 412, 1500, 565, 293, 300, 311, 983, 741, 478, 1228, 472, 670, 297, 3175, 472, 510, 1392, 370, 718, 311, 586, 767, 646, 51556, 51556], "temperature": 0.0, "avg_logprob": -0.054781740361993965, "compression_ratio": 1.65, "no_speech_prob": 2.443911625960027e-06}, {"id": 673, "seek": 414304, "start": 4143.04, "end": 4149.44, "text": " propagate through this line so the first thing that i always like to do is i like to scrutinize", "tokens": [50364, 48256, 807, 341, 1622, 370, 264, 700, 551, 300, 741, 1009, 411, 281, 360, 307, 741, 411, 281, 28949, 259, 1125, 50684, 50684, 264, 10854, 700, 370, 294, 1729, 510, 1237, 412, 264, 10854, 295, 437, 311, 3288, 741, 536, 300, 272, 293, 1374, 50976, 51008, 3909, 307, 472, 538, 12145, 370, 309, 311, 257, 5386, 8062, 293, 272, 293, 498, 732, 5893, 3909, 307, 8858, 538, 12145, 370, 4448, 510, 321, 434, 51412, 51412], "temperature": 0.0, "avg_logprob": -0.06868429307813768, "compression_ratio": 1.6875, "no_speech_prob": 6.438768195948796e-06}, {"id": 674, "seek": 414304, "start": 4149.44, "end": 4155.28, "text": " the shapes first so in particular here looking at the shapes of what's involved i see that b and var", "tokens": [50364, 48256, 807, 341, 1622, 370, 264, 700, 551, 300, 741, 1009, 411, 281, 360, 307, 741, 411, 281, 28949, 259, 1125, 50684, 50684, 264, 10854, 700, 370, 294, 1729, 510, 1237, 412, 264, 10854, 295, 437, 311, 3288, 741, 536, 300, 272, 293, 1374, 50976, 51008, 3909, 307, 472, 538, 12145, 370, 309, 311, 257, 5386, 8062, 293, 272, 293, 498, 732, 5893, 3909, 307, 8858, 538, 12145, 370, 4448, 510, 321, 434, 51412, 51412], "temperature": 0.0, "avg_logprob": -0.06868429307813768, "compression_ratio": 1.6875, "no_speech_prob": 6.438768195948796e-06}, {"id": 675, "seek": 414304, "start": 4155.92, "end": 4164.0, "text": " shape is one by 64 so it's a row vector and b and if two dot shape is 32 by 64 so clearly here we're", "tokens": [50364, 48256, 807, 341, 1622, 370, 264, 700, 551, 300, 741, 1009, 411, 281, 360, 307, 741, 411, 281, 28949, 259, 1125, 50684, 50684, 264, 10854, 700, 370, 294, 1729, 510, 1237, 412, 264, 10854, 295, 437, 311, 3288, 741, 536, 300, 272, 293, 1374, 50976, 51008, 3909, 307, 472, 538, 12145, 370, 309, 311, 257, 5386, 8062, 293, 272, 293, 498, 732, 5893, 3909, 307, 8858, 538, 12145, 370, 4448, 510, 321, 434, 51412, 51412], "temperature": 0.0, "avg_logprob": -0.06868429307813768, "compression_ratio": 1.6875, "no_speech_prob": 6.438768195948796e-06}, {"id": 676, "seek": 416400, "start": 4164.0, "end": 4172.8, "text": " doing a sum over the zeroth axis to squash the first dimension of uh of the shapes here using a sum", "tokens": [50364, 884, 257, 2408, 670, 264, 44746, 900, 10298, 281, 30725, 264, 700, 10139, 295, 2232, 295, 264, 10854, 510, 1228, 257, 2408, 50804, 50836, 370, 300, 558, 1314, 767, 27271, 281, 385, 300, 456, 486, 312, 512, 733, 295, 257, 39911, 420, 30024, 51080, 51080, 294, 264, 23897, 1320, 293, 1310, 291, 434, 21814, 264, 5102, 510, 457, 1936, 13038, 291, 362, 257, 2408, 51348, 51348, 294, 264, 2128, 1320, 300, 4523, 666, 257, 39911, 420, 30024, 294, 264, 23897, 1320, 2051, 264, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.05063088186855974, "compression_ratio": 1.9275362318840579, "no_speech_prob": 5.173780664335936e-06}, {"id": 677, "seek": 416400, "start": 4173.44, "end": 4178.32, "text": " so that right away actually hints to me that there will be some kind of a replication or broadcasting", "tokens": [50364, 884, 257, 2408, 670, 264, 44746, 900, 10298, 281, 30725, 264, 700, 10139, 295, 2232, 295, 264, 10854, 510, 1228, 257, 2408, 50804, 50836, 370, 300, 558, 1314, 767, 27271, 281, 385, 300, 456, 486, 312, 512, 733, 295, 257, 39911, 420, 30024, 51080, 51080, 294, 264, 23897, 1320, 293, 1310, 291, 434, 21814, 264, 5102, 510, 457, 1936, 13038, 291, 362, 257, 2408, 51348, 51348, 294, 264, 2128, 1320, 300, 4523, 666, 257, 39911, 420, 30024, 294, 264, 23897, 1320, 2051, 264, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.05063088186855974, "compression_ratio": 1.9275362318840579, "no_speech_prob": 5.173780664335936e-06}, {"id": 678, "seek": 416400, "start": 4178.32, "end": 4183.68, "text": " in the backward pass and maybe you're noticing the pattern here but basically anytime you have a sum", "tokens": [50364, 884, 257, 2408, 670, 264, 44746, 900, 10298, 281, 30725, 264, 700, 10139, 295, 2232, 295, 264, 10854, 510, 1228, 257, 2408, 50804, 50836, 370, 300, 558, 1314, 767, 27271, 281, 385, 300, 456, 486, 312, 512, 733, 295, 257, 39911, 420, 30024, 51080, 51080, 294, 264, 23897, 1320, 293, 1310, 291, 434, 21814, 264, 5102, 510, 457, 1936, 13038, 291, 362, 257, 2408, 51348, 51348, 294, 264, 2128, 1320, 300, 4523, 666, 257, 39911, 420, 30024, 294, 264, 23897, 1320, 2051, 264, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.05063088186855974, "compression_ratio": 1.9275362318840579, "no_speech_prob": 5.173780664335936e-06}, {"id": 679, "seek": 416400, "start": 4183.68, "end": 4188.8, "text": " in the forward pass that turns into a replication or broadcasting in the backward pass along the", "tokens": [50364, 884, 257, 2408, 670, 264, 44746, 900, 10298, 281, 30725, 264, 700, 10139, 295, 2232, 295, 264, 10854, 510, 1228, 257, 2408, 50804, 50836, 370, 300, 558, 1314, 767, 27271, 281, 385, 300, 456, 486, 312, 512, 733, 295, 257, 39911, 420, 30024, 51080, 51080, 294, 264, 23897, 1320, 293, 1310, 291, 434, 21814, 264, 5102, 510, 457, 1936, 13038, 291, 362, 257, 2408, 51348, 51348, 294, 264, 2128, 1320, 300, 4523, 666, 257, 39911, 420, 30024, 294, 264, 23897, 1320, 2051, 264, 51604, 51604], "temperature": 0.0, "avg_logprob": -0.05063088186855974, "compression_ratio": 1.9275362318840579, "no_speech_prob": 5.173780664335936e-06}, {"id": 680, "seek": 418880, "start": 4188.8, "end": 4194.96, "text": " same dimension and conversely when we have a replication or a broadcasting in the forward", "tokens": [50364, 912, 10139, 293, 2615, 736, 562, 321, 362, 257, 39911, 420, 257, 30024, 294, 264, 2128, 50672, 50672, 1320, 300, 16203, 257, 7006, 26225, 293, 370, 294, 264, 23897, 1320, 300, 4523, 666, 257, 2408, 670, 264, 50984, 50984, 1900, 912, 10139, 293, 370, 4696, 291, 434, 21814, 300, 11848, 507, 300, 729, 732, 366, 733, 51208, 51208, 295, 411, 264, 4665, 3324, 295, 1184, 661, 294, 264, 2128, 293, 23897, 1320, 586, 1564, 321, 1223, 264, 10854, 51480, 51480, 264, 958, 551, 741, 411, 281, 360, 1009, 307, 741, 411, 281, 574, 412, 257, 12058, 1365, 294, 452, 1378, 281, 1333, 295, 445, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.03564152673438743, "compression_ratio": 1.874015748031496, "no_speech_prob": 2.4438770651613595e-06}, {"id": 681, "seek": 418880, "start": 4194.96, "end": 4201.2, "text": " pass that indicates a variable reuse and so in the backward pass that turns into a sum over the", "tokens": [50364, 912, 10139, 293, 2615, 736, 562, 321, 362, 257, 39911, 420, 257, 30024, 294, 264, 2128, 50672, 50672, 1320, 300, 16203, 257, 7006, 26225, 293, 370, 294, 264, 23897, 1320, 300, 4523, 666, 257, 2408, 670, 264, 50984, 50984, 1900, 912, 10139, 293, 370, 4696, 291, 434, 21814, 300, 11848, 507, 300, 729, 732, 366, 733, 51208, 51208, 295, 411, 264, 4665, 3324, 295, 1184, 661, 294, 264, 2128, 293, 23897, 1320, 586, 1564, 321, 1223, 264, 10854, 51480, 51480, 264, 958, 551, 741, 411, 281, 360, 1009, 307, 741, 411, 281, 574, 412, 257, 12058, 1365, 294, 452, 1378, 281, 1333, 295, 445, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.03564152673438743, "compression_ratio": 1.874015748031496, "no_speech_prob": 2.4438770651613595e-06}, {"id": 682, "seek": 418880, "start": 4201.2, "end": 4205.68, "text": " exact same dimension and so hopefully you're noticing that duality that those two are kind", "tokens": [50364, 912, 10139, 293, 2615, 736, 562, 321, 362, 257, 39911, 420, 257, 30024, 294, 264, 2128, 50672, 50672, 1320, 300, 16203, 257, 7006, 26225, 293, 370, 294, 264, 23897, 1320, 300, 4523, 666, 257, 2408, 670, 264, 50984, 50984, 1900, 912, 10139, 293, 370, 4696, 291, 434, 21814, 300, 11848, 507, 300, 729, 732, 366, 733, 51208, 51208, 295, 411, 264, 4665, 3324, 295, 1184, 661, 294, 264, 2128, 293, 23897, 1320, 586, 1564, 321, 1223, 264, 10854, 51480, 51480, 264, 958, 551, 741, 411, 281, 360, 1009, 307, 741, 411, 281, 574, 412, 257, 12058, 1365, 294, 452, 1378, 281, 1333, 295, 445, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.03564152673438743, "compression_ratio": 1.874015748031496, "no_speech_prob": 2.4438770651613595e-06}, {"id": 683, "seek": 418880, "start": 4205.68, "end": 4211.12, "text": " of like the opposites of each other in the forward and backward pass now once we understand the shapes", "tokens": [50364, 912, 10139, 293, 2615, 736, 562, 321, 362, 257, 39911, 420, 257, 30024, 294, 264, 2128, 50672, 50672, 1320, 300, 16203, 257, 7006, 26225, 293, 370, 294, 264, 23897, 1320, 300, 4523, 666, 257, 2408, 670, 264, 50984, 50984, 1900, 912, 10139, 293, 370, 4696, 291, 434, 21814, 300, 11848, 507, 300, 729, 732, 366, 733, 51208, 51208, 295, 411, 264, 4665, 3324, 295, 1184, 661, 294, 264, 2128, 293, 23897, 1320, 586, 1564, 321, 1223, 264, 10854, 51480, 51480, 264, 958, 551, 741, 411, 281, 360, 1009, 307, 741, 411, 281, 574, 412, 257, 12058, 1365, 294, 452, 1378, 281, 1333, 295, 445, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.03564152673438743, "compression_ratio": 1.874015748031496, "no_speech_prob": 2.4438770651613595e-06}, {"id": 684, "seek": 418880, "start": 4211.12, "end": 4215.2, "text": " the next thing i like to do always is i like to look at a toy example in my head to sort of just", "tokens": [50364, 912, 10139, 293, 2615, 736, 562, 321, 362, 257, 39911, 420, 257, 30024, 294, 264, 2128, 50672, 50672, 1320, 300, 16203, 257, 7006, 26225, 293, 370, 294, 264, 23897, 1320, 300, 4523, 666, 257, 2408, 670, 264, 50984, 50984, 1900, 912, 10139, 293, 370, 4696, 291, 434, 21814, 300, 11848, 507, 300, 729, 732, 366, 733, 51208, 51208, 295, 411, 264, 4665, 3324, 295, 1184, 661, 294, 264, 2128, 293, 23897, 1320, 586, 1564, 321, 1223, 264, 10854, 51480, 51480, 264, 958, 551, 741, 411, 281, 360, 1009, 307, 741, 411, 281, 574, 412, 257, 12058, 1365, 294, 452, 1378, 281, 1333, 295, 445, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.03564152673438743, "compression_ratio": 1.874015748031496, "no_speech_prob": 2.4438770651613595e-06}, {"id": 685, "seek": 421520, "start": 4215.2, "end": 4220.32, "text": " like understand roughly how the variable the variable dependencies go in the mathematical formula", "tokens": [50364, 411, 1223, 9810, 577, 264, 7006, 264, 7006, 36606, 352, 294, 264, 18894, 8513, 50620, 50676, 370, 510, 321, 362, 257, 732, 12, 18759, 10225, 272, 293, 1060, 568, 597, 321, 366, 21589, 538, 257, 5754, 293, 550, 51020, 51020, 321, 366, 2408, 2810, 28450, 670, 264, 13766, 370, 498, 321, 362, 257, 732, 538, 732, 8141, 257, 293, 550, 321, 2408, 670, 51316, 51316, 264, 13766, 293, 4373, 321, 576, 483, 257, 5386, 8062, 272, 16, 272, 17, 293, 272, 16, 5946, 322, 257, 294, 341, 636, 9735, 445, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.09303843721430352, "compression_ratio": 1.798165137614679, "no_speech_prob": 6.1792803762728e-07}, {"id": 686, "seek": 421520, "start": 4221.44, "end": 4228.32, "text": " so here we have a two-dimensional array b and def 2 which we are scaling by a constant and then", "tokens": [50364, 411, 1223, 9810, 577, 264, 7006, 264, 7006, 36606, 352, 294, 264, 18894, 8513, 50620, 50676, 370, 510, 321, 362, 257, 732, 12, 18759, 10225, 272, 293, 1060, 568, 597, 321, 366, 21589, 538, 257, 5754, 293, 550, 51020, 51020, 321, 366, 2408, 2810, 28450, 670, 264, 13766, 370, 498, 321, 362, 257, 732, 538, 732, 8141, 257, 293, 550, 321, 2408, 670, 51316, 51316, 264, 13766, 293, 4373, 321, 576, 483, 257, 5386, 8062, 272, 16, 272, 17, 293, 272, 16, 5946, 322, 257, 294, 341, 636, 9735, 445, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.09303843721430352, "compression_ratio": 1.798165137614679, "no_speech_prob": 6.1792803762728e-07}, {"id": 687, "seek": 421520, "start": 4228.32, "end": 4234.24, "text": " we are summing vertically over the columns so if we have a two by two matrix a and then we sum over", "tokens": [50364, 411, 1223, 9810, 577, 264, 7006, 264, 7006, 36606, 352, 294, 264, 18894, 8513, 50620, 50676, 370, 510, 321, 362, 257, 732, 12, 18759, 10225, 272, 293, 1060, 568, 597, 321, 366, 21589, 538, 257, 5754, 293, 550, 51020, 51020, 321, 366, 2408, 2810, 28450, 670, 264, 13766, 370, 498, 321, 362, 257, 732, 538, 732, 8141, 257, 293, 550, 321, 2408, 670, 51316, 51316, 264, 13766, 293, 4373, 321, 576, 483, 257, 5386, 8062, 272, 16, 272, 17, 293, 272, 16, 5946, 322, 257, 294, 341, 636, 9735, 445, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.09303843721430352, "compression_ratio": 1.798165137614679, "no_speech_prob": 6.1792803762728e-07}, {"id": 688, "seek": 421520, "start": 4234.24, "end": 4241.5199999999995, "text": " the columns and scale we would get a row vector b1 b2 and b1 depends on a in this way whereas just", "tokens": [50364, 411, 1223, 9810, 577, 264, 7006, 264, 7006, 36606, 352, 294, 264, 18894, 8513, 50620, 50676, 370, 510, 321, 362, 257, 732, 12, 18759, 10225, 272, 293, 1060, 568, 597, 321, 366, 21589, 538, 257, 5754, 293, 550, 51020, 51020, 321, 366, 2408, 2810, 28450, 670, 264, 13766, 370, 498, 321, 362, 257, 732, 538, 732, 8141, 257, 293, 550, 321, 2408, 670, 51316, 51316, 264, 13766, 293, 4373, 321, 576, 483, 257, 5386, 8062, 272, 16, 272, 17, 293, 272, 16, 5946, 322, 257, 294, 341, 636, 9735, 445, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.09303843721430352, "compression_ratio": 1.798165137614679, "no_speech_prob": 6.1792803762728e-07}, {"id": 689, "seek": 424152, "start": 4241.52, "end": 4250.240000000001, "text": " sum their scaled of a and b2 in this way whereas the second column summed and scaled and so looking", "tokens": [50364, 2408, 641, 36039, 295, 257, 293, 272, 17, 294, 341, 636, 9735, 264, 1150, 7738, 2408, 1912, 293, 36039, 293, 370, 1237, 50800, 50800, 412, 341, 1936, 437, 321, 528, 281, 360, 586, 307, 321, 362, 264, 33733, 322, 272, 16, 293, 272, 17, 293, 321, 528, 281, 51068, 51068, 646, 48256, 552, 666, 257, 311, 293, 370, 309, 311, 1850, 300, 445, 27372, 990, 294, 428, 1378, 264, 2654, 51336, 51336, 13760, 510, 307, 472, 670, 297, 3175, 472, 1413, 472, 337, 1184, 472, 295, 613, 257, 311, 293, 1936, 264, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.08269978066285451, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.7330152104477747e-06}, {"id": 690, "seek": 424152, "start": 4250.240000000001, "end": 4255.6, "text": " at this basically what we want to do now is we have the derivatives on b1 and b2 and we want to", "tokens": [50364, 2408, 641, 36039, 295, 257, 293, 272, 17, 294, 341, 636, 9735, 264, 1150, 7738, 2408, 1912, 293, 36039, 293, 370, 1237, 50800, 50800, 412, 341, 1936, 437, 321, 528, 281, 360, 586, 307, 321, 362, 264, 33733, 322, 272, 16, 293, 272, 17, 293, 321, 528, 281, 51068, 51068, 646, 48256, 552, 666, 257, 311, 293, 370, 309, 311, 1850, 300, 445, 27372, 990, 294, 428, 1378, 264, 2654, 51336, 51336, 13760, 510, 307, 472, 670, 297, 3175, 472, 1413, 472, 337, 1184, 472, 295, 613, 257, 311, 293, 1936, 264, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.08269978066285451, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.7330152104477747e-06}, {"id": 691, "seek": 424152, "start": 4255.6, "end": 4260.96, "text": " back propagate them into a's and so it's clear that just differentiating in your head the local", "tokens": [50364, 2408, 641, 36039, 295, 257, 293, 272, 17, 294, 341, 636, 9735, 264, 1150, 7738, 2408, 1912, 293, 36039, 293, 370, 1237, 50800, 50800, 412, 341, 1936, 437, 321, 528, 281, 360, 586, 307, 321, 362, 264, 33733, 322, 272, 16, 293, 272, 17, 293, 321, 528, 281, 51068, 51068, 646, 48256, 552, 666, 257, 311, 293, 370, 309, 311, 1850, 300, 445, 27372, 990, 294, 428, 1378, 264, 2654, 51336, 51336, 13760, 510, 307, 472, 670, 297, 3175, 472, 1413, 472, 337, 1184, 472, 295, 613, 257, 311, 293, 1936, 264, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.08269978066285451, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.7330152104477747e-06}, {"id": 692, "seek": 424152, "start": 4260.96, "end": 4270.0, "text": " derivative here is one over n minus one times one for each one of these a's and basically the", "tokens": [50364, 2408, 641, 36039, 295, 257, 293, 272, 17, 294, 341, 636, 9735, 264, 1150, 7738, 2408, 1912, 293, 36039, 293, 370, 1237, 50800, 50800, 412, 341, 1936, 437, 321, 528, 281, 360, 586, 307, 321, 362, 264, 33733, 322, 272, 16, 293, 272, 17, 293, 321, 528, 281, 51068, 51068, 646, 48256, 552, 666, 257, 311, 293, 370, 309, 311, 1850, 300, 445, 27372, 990, 294, 428, 1378, 264, 2654, 51336, 51336, 13760, 510, 307, 472, 670, 297, 3175, 472, 1413, 472, 337, 1184, 472, 295, 613, 257, 311, 293, 1936, 264, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.08269978066285451, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.7330152104477747e-06}, {"id": 693, "seek": 427000, "start": 4270.0, "end": 4276.96, "text": " derivative of b1 has to flow through the columns of a scaled by one over n minus one and that's", "tokens": [50364, 13760, 295, 272, 16, 575, 281, 3095, 807, 264, 13766, 295, 257, 36039, 538, 472, 670, 297, 3175, 472, 293, 300, 311, 50712, 50712, 9810, 437, 311, 2737, 510, 370, 46506, 264, 13760, 3095, 5112, 505, 300, 274, 65, 293, 1060, 568, 51036, 51116, 486, 312, 264, 2654, 13760, 295, 341, 6916, 293, 456, 366, 867, 2098, 281, 360, 341, 538, 264, 636, 457, 741, 51388, 51388, 411, 281, 360, 746, 411, 341, 27822, 5893, 2306, 411, 295, 272, 293, 1060, 568, 370, 741, 603, 1884, 257, 2416, 10225, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.0716335440194735, "compression_ratio": 1.7638888888888888, "no_speech_prob": 9.87451585388044e-07}, {"id": 694, "seek": 427000, "start": 4276.96, "end": 4283.44, "text": " roughly what's happening here so intuitively the derivative flow tells us that db and def 2", "tokens": [50364, 13760, 295, 272, 16, 575, 281, 3095, 807, 264, 13766, 295, 257, 36039, 538, 472, 670, 297, 3175, 472, 293, 300, 311, 50712, 50712, 9810, 437, 311, 2737, 510, 370, 46506, 264, 13760, 3095, 5112, 505, 300, 274, 65, 293, 1060, 568, 51036, 51116, 486, 312, 264, 2654, 13760, 295, 341, 6916, 293, 456, 366, 867, 2098, 281, 360, 341, 538, 264, 636, 457, 741, 51388, 51388, 411, 281, 360, 746, 411, 341, 27822, 5893, 2306, 411, 295, 272, 293, 1060, 568, 370, 741, 603, 1884, 257, 2416, 10225, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.0716335440194735, "compression_ratio": 1.7638888888888888, "no_speech_prob": 9.87451585388044e-07}, {"id": 695, "seek": 427000, "start": 4285.04, "end": 4290.48, "text": " will be the local derivative of this operation and there are many ways to do this by the way but i", "tokens": [50364, 13760, 295, 272, 16, 575, 281, 3095, 807, 264, 13766, 295, 257, 36039, 538, 472, 670, 297, 3175, 472, 293, 300, 311, 50712, 50712, 9810, 437, 311, 2737, 510, 370, 46506, 264, 13760, 3095, 5112, 505, 300, 274, 65, 293, 1060, 568, 51036, 51116, 486, 312, 264, 2654, 13760, 295, 341, 6916, 293, 456, 366, 867, 2098, 281, 360, 341, 538, 264, 636, 457, 741, 51388, 51388, 411, 281, 360, 746, 411, 341, 27822, 5893, 2306, 411, 295, 272, 293, 1060, 568, 370, 741, 603, 1884, 257, 2416, 10225, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.0716335440194735, "compression_ratio": 1.7638888888888888, "no_speech_prob": 9.87451585388044e-07}, {"id": 696, "seek": 427000, "start": 4290.48, "end": 4297.36, "text": " like to do something like this torch dot ones like of b and def 2 so i'll create a large array", "tokens": [50364, 13760, 295, 272, 16, 575, 281, 3095, 807, 264, 13766, 295, 257, 36039, 538, 472, 670, 297, 3175, 472, 293, 300, 311, 50712, 50712, 9810, 437, 311, 2737, 510, 370, 46506, 264, 13760, 3095, 5112, 505, 300, 274, 65, 293, 1060, 568, 51036, 51116, 486, 312, 264, 2654, 13760, 295, 341, 6916, 293, 456, 366, 867, 2098, 281, 360, 341, 538, 264, 636, 457, 741, 51388, 51388, 411, 281, 360, 746, 411, 341, 27822, 5893, 2306, 411, 295, 272, 293, 1060, 568, 370, 741, 603, 1884, 257, 2416, 10225, 51732, 51732], "temperature": 0.0, "avg_logprob": -0.0716335440194735, "compression_ratio": 1.7638888888888888, "no_speech_prob": 9.87451585388044e-07}, {"id": 697, "seek": 429736, "start": 4297.36, "end": 4302.96, "text": " two-dimensional of ones and then i will scale it so 1.0 divide by n minus one", "tokens": [50364, 732, 12, 18759, 295, 2306, 293, 550, 741, 486, 4373, 309, 370, 502, 13, 15, 9845, 538, 297, 3175, 472, 50644, 50728, 370, 341, 307, 257, 10225, 295, 1105, 472, 670, 297, 3175, 472, 293, 300, 311, 1333, 295, 411, 264, 2654, 13760, 293, 586, 51068, 51068, 337, 264, 5021, 4978, 741, 486, 2935, 445, 12972, 309, 538, 274, 65, 76, 2159, 51292, 51424, 293, 3449, 510, 437, 311, 516, 281, 1051, 341, 307, 8858, 538, 12145, 293, 341, 307, 445, 472, 538, 12145, 370, 741, 478, 8295, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.07133824410645859, "compression_ratio": 1.675, "no_speech_prob": 1.4144619626677013e-06}, {"id": 698, "seek": 429736, "start": 4304.639999999999, "end": 4311.44, "text": " so this is a array of um one over n minus one and that's sort of like the local derivative and now", "tokens": [50364, 732, 12, 18759, 295, 2306, 293, 550, 741, 486, 4373, 309, 370, 502, 13, 15, 9845, 538, 297, 3175, 472, 50644, 50728, 370, 341, 307, 257, 10225, 295, 1105, 472, 670, 297, 3175, 472, 293, 300, 311, 1333, 295, 411, 264, 2654, 13760, 293, 586, 51068, 51068, 337, 264, 5021, 4978, 741, 486, 2935, 445, 12972, 309, 538, 274, 65, 76, 2159, 51292, 51424, 293, 3449, 510, 437, 311, 516, 281, 1051, 341, 307, 8858, 538, 12145, 293, 341, 307, 445, 472, 538, 12145, 370, 741, 478, 8295, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.07133824410645859, "compression_ratio": 1.675, "no_speech_prob": 1.4144619626677013e-06}, {"id": 699, "seek": 429736, "start": 4311.44, "end": 4315.92, "text": " for the chain rule i will simply just multiply it by dbm bar", "tokens": [50364, 732, 12, 18759, 295, 2306, 293, 550, 741, 486, 4373, 309, 370, 502, 13, 15, 9845, 538, 297, 3175, 472, 50644, 50728, 370, 341, 307, 257, 10225, 295, 1105, 472, 670, 297, 3175, 472, 293, 300, 311, 1333, 295, 411, 264, 2654, 13760, 293, 586, 51068, 51068, 337, 264, 5021, 4978, 741, 486, 2935, 445, 12972, 309, 538, 274, 65, 76, 2159, 51292, 51424, 293, 3449, 510, 437, 311, 516, 281, 1051, 341, 307, 8858, 538, 12145, 293, 341, 307, 445, 472, 538, 12145, 370, 741, 478, 8295, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.07133824410645859, "compression_ratio": 1.675, "no_speech_prob": 1.4144619626677013e-06}, {"id": 700, "seek": 429736, "start": 4318.5599999999995, "end": 4324.5599999999995, "text": " and notice here what's going to happen this is 32 by 64 and this is just one by 64 so i'm letting", "tokens": [50364, 732, 12, 18759, 295, 2306, 293, 550, 741, 486, 4373, 309, 370, 502, 13, 15, 9845, 538, 297, 3175, 472, 50644, 50728, 370, 341, 307, 257, 10225, 295, 1105, 472, 670, 297, 3175, 472, 293, 300, 311, 1333, 295, 411, 264, 2654, 13760, 293, 586, 51068, 51068, 337, 264, 5021, 4978, 741, 486, 2935, 445, 12972, 309, 538, 274, 65, 76, 2159, 51292, 51424, 293, 3449, 510, 437, 311, 516, 281, 1051, 341, 307, 8858, 538, 12145, 293, 341, 307, 445, 472, 538, 12145, 370, 741, 478, 8295, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.07133824410645859, "compression_ratio": 1.675, "no_speech_prob": 1.4144619626677013e-06}, {"id": 701, "seek": 432456, "start": 4324.56, "end": 4331.04, "text": " the broadcasting do the replication because internally in pi torch basically dbm bar which", "tokens": [50364, 264, 30024, 360, 264, 39911, 570, 19501, 294, 3895, 27822, 1936, 274, 65, 76, 2159, 597, 50688, 50688, 307, 472, 538, 12145, 5386, 8062, 486, 294, 341, 27290, 483, 25365, 28450, 1826, 264, 732, 366, 295, 264, 912, 51060, 51060, 3909, 293, 550, 456, 486, 312, 364, 4478, 12, 3711, 12972, 293, 370, 300, 370, 300, 264, 30024, 51296, 51296, 307, 1936, 884, 264, 39911, 293, 741, 486, 917, 493, 365, 264, 33733, 295, 274, 65, 76, 7593, 568, 510, 51600, 51640], "temperature": 0.0, "avg_logprob": -0.10727099009922572, "compression_ratio": 1.8, "no_speech_prob": 3.3930909921764396e-06}, {"id": 702, "seek": 432456, "start": 4331.04, "end": 4338.4800000000005, "text": " is one by 64 row vector will in this multiplication get copied vertically until the two are of the same", "tokens": [50364, 264, 30024, 360, 264, 39911, 570, 19501, 294, 3895, 27822, 1936, 274, 65, 76, 2159, 597, 50688, 50688, 307, 472, 538, 12145, 5386, 8062, 486, 294, 341, 27290, 483, 25365, 28450, 1826, 264, 732, 366, 295, 264, 912, 51060, 51060, 3909, 293, 550, 456, 486, 312, 364, 4478, 12, 3711, 12972, 293, 370, 300, 370, 300, 264, 30024, 51296, 51296, 307, 1936, 884, 264, 39911, 293, 741, 486, 917, 493, 365, 264, 33733, 295, 274, 65, 76, 7593, 568, 510, 51600, 51640], "temperature": 0.0, "avg_logprob": -0.10727099009922572, "compression_ratio": 1.8, "no_speech_prob": 3.3930909921764396e-06}, {"id": 703, "seek": 432456, "start": 4338.4800000000005, "end": 4343.200000000001, "text": " shape and then there will be an element-wise multiply and so that so that the broadcasting", "tokens": [50364, 264, 30024, 360, 264, 39911, 570, 19501, 294, 3895, 27822, 1936, 274, 65, 76, 2159, 597, 50688, 50688, 307, 472, 538, 12145, 5386, 8062, 486, 294, 341, 27290, 483, 25365, 28450, 1826, 264, 732, 366, 295, 264, 912, 51060, 51060, 3909, 293, 550, 456, 486, 312, 364, 4478, 12, 3711, 12972, 293, 370, 300, 370, 300, 264, 30024, 51296, 51296, 307, 1936, 884, 264, 39911, 293, 741, 486, 917, 493, 365, 264, 33733, 295, 274, 65, 76, 7593, 568, 510, 51600, 51640], "temperature": 0.0, "avg_logprob": -0.10727099009922572, "compression_ratio": 1.8, "no_speech_prob": 3.3930909921764396e-06}, {"id": 704, "seek": 432456, "start": 4343.200000000001, "end": 4349.280000000001, "text": " is basically doing the replication and i will end up with the derivatives of dbm diff 2 here", "tokens": [50364, 264, 30024, 360, 264, 39911, 570, 19501, 294, 3895, 27822, 1936, 274, 65, 76, 2159, 597, 50688, 50688, 307, 472, 538, 12145, 5386, 8062, 486, 294, 341, 27290, 483, 25365, 28450, 1826, 264, 732, 366, 295, 264, 912, 51060, 51060, 3909, 293, 550, 456, 486, 312, 364, 4478, 12, 3711, 12972, 293, 370, 300, 370, 300, 264, 30024, 51296, 51296, 307, 1936, 884, 264, 39911, 293, 741, 486, 917, 493, 365, 264, 33733, 295, 274, 65, 76, 7593, 568, 510, 51600, 51640], "temperature": 0.0, "avg_logprob": -0.10727099009922572, "compression_ratio": 1.8, "no_speech_prob": 3.3930909921764396e-06}, {"id": 705, "seek": 434928, "start": 4349.28, "end": 4355.84, "text": " so this is the candidate solution let's bring it down here let's uncomment this line where we check", "tokens": [50364, 370, 341, 307, 264, 11532, 3827, 718, 311, 1565, 309, 760, 510, 718, 311, 8585, 518, 341, 1622, 689, 321, 1520, 50692, 50692, 309, 293, 718, 311, 1454, 337, 264, 1151, 293, 6451, 321, 536, 300, 341, 307, 264, 3006, 8513, 958, 493, 718, 311, 51032, 51032, 23203, 510, 666, 272, 293, 498, 370, 510, 321, 362, 300, 272, 293, 498, 307, 4478, 12, 3711, 8889, 281, 1884, 51348, 51348, 272, 293, 498, 568, 370, 341, 307, 257, 7226, 2199, 13760, 570, 264, 2199, 4478, 12, 3711, 6916, 370, 309, 311, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1377024198833265, "compression_ratio": 1.8457943925233644, "no_speech_prob": 5.422133199317614e-06}, {"id": 706, "seek": 434928, "start": 4355.84, "end": 4362.639999999999, "text": " it and let's hope for the best and indeed we see that this is the correct formula next up let's", "tokens": [50364, 370, 341, 307, 264, 11532, 3827, 718, 311, 1565, 309, 760, 510, 718, 311, 8585, 518, 341, 1622, 689, 321, 1520, 50692, 50692, 309, 293, 718, 311, 1454, 337, 264, 1151, 293, 6451, 321, 536, 300, 341, 307, 264, 3006, 8513, 958, 493, 718, 311, 51032, 51032, 23203, 510, 666, 272, 293, 498, 370, 510, 321, 362, 300, 272, 293, 498, 307, 4478, 12, 3711, 8889, 281, 1884, 51348, 51348, 272, 293, 498, 568, 370, 341, 307, 257, 7226, 2199, 13760, 570, 264, 2199, 4478, 12, 3711, 6916, 370, 309, 311, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1377024198833265, "compression_ratio": 1.8457943925233644, "no_speech_prob": 5.422133199317614e-06}, {"id": 707, "seek": 434928, "start": 4362.639999999999, "end": 4368.96, "text": " differentiate here into b and if so here we have that b and if is element-wise squared to create", "tokens": [50364, 370, 341, 307, 264, 11532, 3827, 718, 311, 1565, 309, 760, 510, 718, 311, 8585, 518, 341, 1622, 689, 321, 1520, 50692, 50692, 309, 293, 718, 311, 1454, 337, 264, 1151, 293, 6451, 321, 536, 300, 341, 307, 264, 3006, 8513, 958, 493, 718, 311, 51032, 51032, 23203, 510, 666, 272, 293, 498, 370, 510, 321, 362, 300, 272, 293, 498, 307, 4478, 12, 3711, 8889, 281, 1884, 51348, 51348, 272, 293, 498, 568, 370, 341, 307, 257, 7226, 2199, 13760, 570, 264, 2199, 4478, 12, 3711, 6916, 370, 309, 311, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1377024198833265, "compression_ratio": 1.8457943925233644, "no_speech_prob": 5.422133199317614e-06}, {"id": 708, "seek": 434928, "start": 4368.96, "end": 4375.04, "text": " b and if 2 so this is a relatively simple derivative because the simple element-wise operation so it's", "tokens": [50364, 370, 341, 307, 264, 11532, 3827, 718, 311, 1565, 309, 760, 510, 718, 311, 8585, 518, 341, 1622, 689, 321, 1520, 50692, 50692, 309, 293, 718, 311, 1454, 337, 264, 1151, 293, 6451, 321, 536, 300, 341, 307, 264, 3006, 8513, 958, 493, 718, 311, 51032, 51032, 23203, 510, 666, 272, 293, 498, 370, 510, 321, 362, 300, 272, 293, 498, 307, 4478, 12, 3711, 8889, 281, 1884, 51348, 51348, 272, 293, 498, 568, 370, 341, 307, 257, 7226, 2199, 13760, 570, 264, 2199, 4478, 12, 3711, 6916, 370, 309, 311, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.1377024198833265, "compression_ratio": 1.8457943925233644, "no_speech_prob": 5.422133199317614e-06}, {"id": 709, "seek": 437504, "start": 4375.04, "end": 4381.6, "text": " kind of like the scalar case and we have that dbm diff should be if this is x squared then the", "tokens": [50364, 733, 295, 411, 264, 39684, 1389, 293, 321, 362, 300, 274, 65, 76, 7593, 820, 312, 498, 341, 307, 2031, 8889, 550, 264, 50692, 50692, 13760, 295, 341, 307, 568, 87, 558, 370, 309, 311, 2935, 568, 1413, 272, 293, 498, 300, 311, 264, 2654, 13760, 50980, 51028, 293, 550, 1413, 5021, 4978, 293, 264, 3909, 295, 613, 307, 264, 912, 436, 366, 295, 264, 912, 3909, 370, 1413, 51324, 51324, 341, 370, 300, 311, 264, 23897, 1320, 337, 341, 7006, 718, 385, 1565, 309, 760, 510, 293, 586, 321, 362, 281, 312, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.12785624474594273, "compression_ratio": 1.8317307692307692, "no_speech_prob": 1.3709427548747044e-06}, {"id": 710, "seek": 437504, "start": 4381.6, "end": 4387.36, "text": " derivative of this is 2x right so it's simply 2 times b and if that's the local derivative", "tokens": [50364, 733, 295, 411, 264, 39684, 1389, 293, 321, 362, 300, 274, 65, 76, 7593, 820, 312, 498, 341, 307, 2031, 8889, 550, 264, 50692, 50692, 13760, 295, 341, 307, 568, 87, 558, 370, 309, 311, 2935, 568, 1413, 272, 293, 498, 300, 311, 264, 2654, 13760, 50980, 51028, 293, 550, 1413, 5021, 4978, 293, 264, 3909, 295, 613, 307, 264, 912, 436, 366, 295, 264, 912, 3909, 370, 1413, 51324, 51324, 341, 370, 300, 311, 264, 23897, 1320, 337, 341, 7006, 718, 385, 1565, 309, 760, 510, 293, 586, 321, 362, 281, 312, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.12785624474594273, "compression_ratio": 1.8317307692307692, "no_speech_prob": 1.3709427548747044e-06}, {"id": 711, "seek": 437504, "start": 4388.32, "end": 4394.24, "text": " and then times chain rule and the shape of these is the same they are of the same shape so times", "tokens": [50364, 733, 295, 411, 264, 39684, 1389, 293, 321, 362, 300, 274, 65, 76, 7593, 820, 312, 498, 341, 307, 2031, 8889, 550, 264, 50692, 50692, 13760, 295, 341, 307, 568, 87, 558, 370, 309, 311, 2935, 568, 1413, 272, 293, 498, 300, 311, 264, 2654, 13760, 50980, 51028, 293, 550, 1413, 5021, 4978, 293, 264, 3909, 295, 613, 307, 264, 912, 436, 366, 295, 264, 912, 3909, 370, 1413, 51324, 51324, 341, 370, 300, 311, 264, 23897, 1320, 337, 341, 7006, 718, 385, 1565, 309, 760, 510, 293, 586, 321, 362, 281, 312, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.12785624474594273, "compression_ratio": 1.8317307692307692, "no_speech_prob": 1.3709427548747044e-06}, {"id": 712, "seek": 437504, "start": 4394.24, "end": 4400.48, "text": " this so that's the backward pass for this variable let me bring it down here and now we have to be", "tokens": [50364, 733, 295, 411, 264, 39684, 1389, 293, 321, 362, 300, 274, 65, 76, 7593, 820, 312, 498, 341, 307, 2031, 8889, 550, 264, 50692, 50692, 13760, 295, 341, 307, 568, 87, 558, 370, 309, 311, 2935, 568, 1413, 272, 293, 498, 300, 311, 264, 2654, 13760, 50980, 51028, 293, 550, 1413, 5021, 4978, 293, 264, 3909, 295, 613, 307, 264, 912, 436, 366, 295, 264, 912, 3909, 370, 1413, 51324, 51324, 341, 370, 300, 311, 264, 23897, 1320, 337, 341, 7006, 718, 385, 1565, 309, 760, 510, 293, 586, 321, 362, 281, 312, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.12785624474594273, "compression_ratio": 1.8317307692307692, "no_speech_prob": 1.3709427548747044e-06}, {"id": 713, "seek": 440048, "start": 4400.48, "end": 4405.759999999999, "text": " careful because we already calculated the bm diff right so this is just the end of the other", "tokens": [50364, 5026, 570, 321, 1217, 15598, 264, 272, 76, 7593, 558, 370, 341, 307, 445, 264, 917, 295, 264, 661, 50628, 50676, 291, 458, 661, 9819, 1348, 646, 281, 272, 76, 7593, 570, 272, 76, 7593, 486, 1217, 646, 79, 1513, 559, 473, 309, 281, 50972, 50972, 636, 670, 510, 490, 272, 77, 8936, 370, 321, 586, 7365, 264, 1150, 9819, 293, 370, 300, 311, 983, 741, 362, 281, 360, 1804, 51304, 51304, 6915, 293, 498, 291, 9901, 321, 632, 364, 18424, 13760, 337, 272, 76, 7593, 949, 293, 741, 478, 7159, 300, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.1775353948275248, "compression_ratio": 1.7477064220183487, "no_speech_prob": 1.4823147012066329e-06}, {"id": 714, "seek": 440048, "start": 4406.719999999999, "end": 4412.639999999999, "text": " you know other branch coming back to bm diff because bm diff will already backpropagate it to", "tokens": [50364, 5026, 570, 321, 1217, 15598, 264, 272, 76, 7593, 558, 370, 341, 307, 445, 264, 917, 295, 264, 661, 50628, 50676, 291, 458, 661, 9819, 1348, 646, 281, 272, 76, 7593, 570, 272, 76, 7593, 486, 1217, 646, 79, 1513, 559, 473, 309, 281, 50972, 50972, 636, 670, 510, 490, 272, 77, 8936, 370, 321, 586, 7365, 264, 1150, 9819, 293, 370, 300, 311, 983, 741, 362, 281, 360, 1804, 51304, 51304, 6915, 293, 498, 291, 9901, 321, 632, 364, 18424, 13760, 337, 272, 76, 7593, 949, 293, 741, 478, 7159, 300, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.1775353948275248, "compression_ratio": 1.7477064220183487, "no_speech_prob": 1.4823147012066329e-06}, {"id": 715, "seek": 440048, "start": 4412.639999999999, "end": 4419.28, "text": " way over here from bn raw so we now completed the second branch and so that's why i have to do plus", "tokens": [50364, 5026, 570, 321, 1217, 15598, 264, 272, 76, 7593, 558, 370, 341, 307, 445, 264, 917, 295, 264, 661, 50628, 50676, 291, 458, 661, 9819, 1348, 646, 281, 272, 76, 7593, 570, 272, 76, 7593, 486, 1217, 646, 79, 1513, 559, 473, 309, 281, 50972, 50972, 636, 670, 510, 490, 272, 77, 8936, 370, 321, 586, 7365, 264, 1150, 9819, 293, 370, 300, 311, 983, 741, 362, 281, 360, 1804, 51304, 51304, 6915, 293, 498, 291, 9901, 321, 632, 364, 18424, 13760, 337, 272, 76, 7593, 949, 293, 741, 478, 7159, 300, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.1775353948275248, "compression_ratio": 1.7477064220183487, "no_speech_prob": 1.4823147012066329e-06}, {"id": 716, "seek": 440048, "start": 4419.28, "end": 4424.639999999999, "text": " equals and if you recall we had an incorrect derivative for bm diff before and i'm hoping that", "tokens": [50364, 5026, 570, 321, 1217, 15598, 264, 272, 76, 7593, 558, 370, 341, 307, 445, 264, 917, 295, 264, 661, 50628, 50676, 291, 458, 661, 9819, 1348, 646, 281, 272, 76, 7593, 570, 272, 76, 7593, 486, 1217, 646, 79, 1513, 559, 473, 309, 281, 50972, 50972, 636, 670, 510, 490, 272, 77, 8936, 370, 321, 586, 7365, 264, 1150, 9819, 293, 370, 300, 311, 983, 741, 362, 281, 360, 1804, 51304, 51304, 6915, 293, 498, 291, 9901, 321, 632, 364, 18424, 13760, 337, 272, 76, 7593, 949, 293, 741, 478, 7159, 300, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.1775353948275248, "compression_ratio": 1.7477064220183487, "no_speech_prob": 1.4823147012066329e-06}, {"id": 717, "seek": 442464, "start": 4424.64, "end": 4432.0, "text": " once we append this last missing piece we have the exact correctness so let's run and bm diff", "tokens": [50364, 1564, 321, 34116, 341, 1036, 5361, 2522, 321, 362, 264, 1900, 3006, 1287, 370, 718, 311, 1190, 293, 272, 76, 7593, 50732, 50732, 281, 272, 76, 7593, 586, 767, 3110, 264, 1900, 3006, 13760, 370, 300, 311, 38439, 1392, 370, 718, 311, 586, 51088, 51088, 646, 48256, 807, 341, 1622, 510, 264, 700, 551, 321, 360, 295, 1164, 307, 321, 1520, 264, 10854, 51364, 51364, 293, 741, 4114, 552, 484, 510, 293, 1936, 264, 3909, 295, 341, 307, 8858, 538, 12145, 276, 79, 19404, 307, 264, 912, 3909, 51676, 51708], "temperature": 0.0, "avg_logprob": -0.17081934472788934, "compression_ratio": 1.737327188940092, "no_speech_prob": 8.530137165507767e-06}, {"id": 718, "seek": 442464, "start": 4432.0, "end": 4439.12, "text": " to bm diff now actually shows the exact correct derivative so that's comforting okay so let's now", "tokens": [50364, 1564, 321, 34116, 341, 1036, 5361, 2522, 321, 362, 264, 1900, 3006, 1287, 370, 718, 311, 1190, 293, 272, 76, 7593, 50732, 50732, 281, 272, 76, 7593, 586, 767, 3110, 264, 1900, 3006, 13760, 370, 300, 311, 38439, 1392, 370, 718, 311, 586, 51088, 51088, 646, 48256, 807, 341, 1622, 510, 264, 700, 551, 321, 360, 295, 1164, 307, 321, 1520, 264, 10854, 51364, 51364, 293, 741, 4114, 552, 484, 510, 293, 1936, 264, 3909, 295, 341, 307, 8858, 538, 12145, 276, 79, 19404, 307, 264, 912, 3909, 51676, 51708], "temperature": 0.0, "avg_logprob": -0.17081934472788934, "compression_ratio": 1.737327188940092, "no_speech_prob": 8.530137165507767e-06}, {"id": 719, "seek": 442464, "start": 4439.12, "end": 4444.64, "text": " back propagate through this line here the first thing we do of course is we check the shapes", "tokens": [50364, 1564, 321, 34116, 341, 1036, 5361, 2522, 321, 362, 264, 1900, 3006, 1287, 370, 718, 311, 1190, 293, 272, 76, 7593, 50732, 50732, 281, 272, 76, 7593, 586, 767, 3110, 264, 1900, 3006, 13760, 370, 300, 311, 38439, 1392, 370, 718, 311, 586, 51088, 51088, 646, 48256, 807, 341, 1622, 510, 264, 700, 551, 321, 360, 295, 1164, 307, 321, 1520, 264, 10854, 51364, 51364, 293, 741, 4114, 552, 484, 510, 293, 1936, 264, 3909, 295, 341, 307, 8858, 538, 12145, 276, 79, 19404, 307, 264, 912, 3909, 51676, 51708], "temperature": 0.0, "avg_logprob": -0.17081934472788934, "compression_ratio": 1.737327188940092, "no_speech_prob": 8.530137165507767e-06}, {"id": 720, "seek": 442464, "start": 4444.64, "end": 4450.88, "text": " and i wrote them out here and basically the shape of this is 32 by 64 hpbn is the same shape", "tokens": [50364, 1564, 321, 34116, 341, 1036, 5361, 2522, 321, 362, 264, 1900, 3006, 1287, 370, 718, 311, 1190, 293, 272, 76, 7593, 50732, 50732, 281, 272, 76, 7593, 586, 767, 3110, 264, 1900, 3006, 13760, 370, 300, 311, 38439, 1392, 370, 718, 311, 586, 51088, 51088, 646, 48256, 807, 341, 1622, 510, 264, 700, 551, 321, 360, 295, 1164, 307, 321, 1520, 264, 10854, 51364, 51364, 293, 741, 4114, 552, 484, 510, 293, 1936, 264, 3909, 295, 341, 307, 8858, 538, 12145, 276, 79, 19404, 307, 264, 912, 3909, 51676, 51708], "temperature": 0.0, "avg_logprob": -0.17081934472788934, "compression_ratio": 1.737327188940092, "no_speech_prob": 8.530137165507767e-06}, {"id": 721, "seek": 445088, "start": 4450.88, "end": 4456.8, "text": " but b and mean i is a row vector 1 by 64 so this minus here will actually do broadcasting", "tokens": [50364, 457, 272, 293, 914, 741, 307, 257, 5386, 8062, 502, 538, 12145, 370, 341, 3175, 510, 486, 767, 360, 30024, 50660, 50660, 293, 370, 321, 362, 281, 312, 5026, 365, 300, 293, 382, 257, 12075, 281, 505, 797, 570, 295, 264, 11848, 507, 50876, 50876, 257, 30024, 294, 264, 2128, 1320, 1355, 257, 7006, 26225, 293, 4412, 456, 486, 312, 257, 2408, 51128, 51128, 294, 264, 23897, 1320, 370, 718, 311, 2464, 484, 264, 23897, 1320, 510, 586, 51308, 51436, 646, 79, 1513, 559, 473, 666, 264, 276, 79, 19404, 570, 341, 307, 613, 366, 264, 912, 3909, 550, 264, 2654, 13760, 337, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.15588646291572356, "compression_ratio": 1.8049792531120332, "no_speech_prob": 3.6118985917710233e-06}, {"id": 722, "seek": 445088, "start": 4456.8, "end": 4461.12, "text": " and so we have to be careful with that and as a hint to us again because of the duality", "tokens": [50364, 457, 272, 293, 914, 741, 307, 257, 5386, 8062, 502, 538, 12145, 370, 341, 3175, 510, 486, 767, 360, 30024, 50660, 50660, 293, 370, 321, 362, 281, 312, 5026, 365, 300, 293, 382, 257, 12075, 281, 505, 797, 570, 295, 264, 11848, 507, 50876, 50876, 257, 30024, 294, 264, 2128, 1320, 1355, 257, 7006, 26225, 293, 4412, 456, 486, 312, 257, 2408, 51128, 51128, 294, 264, 23897, 1320, 370, 718, 311, 2464, 484, 264, 23897, 1320, 510, 586, 51308, 51436, 646, 79, 1513, 559, 473, 666, 264, 276, 79, 19404, 570, 341, 307, 613, 366, 264, 912, 3909, 550, 264, 2654, 13760, 337, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.15588646291572356, "compression_ratio": 1.8049792531120332, "no_speech_prob": 3.6118985917710233e-06}, {"id": 723, "seek": 445088, "start": 4461.12, "end": 4466.16, "text": " a broadcasting in the forward pass means a variable reuse and therefore there will be a sum", "tokens": [50364, 457, 272, 293, 914, 741, 307, 257, 5386, 8062, 502, 538, 12145, 370, 341, 3175, 510, 486, 767, 360, 30024, 50660, 50660, 293, 370, 321, 362, 281, 312, 5026, 365, 300, 293, 382, 257, 12075, 281, 505, 797, 570, 295, 264, 11848, 507, 50876, 50876, 257, 30024, 294, 264, 2128, 1320, 1355, 257, 7006, 26225, 293, 4412, 456, 486, 312, 257, 2408, 51128, 51128, 294, 264, 23897, 1320, 370, 718, 311, 2464, 484, 264, 23897, 1320, 510, 586, 51308, 51436, 646, 79, 1513, 559, 473, 666, 264, 276, 79, 19404, 570, 341, 307, 613, 366, 264, 912, 3909, 550, 264, 2654, 13760, 337, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.15588646291572356, "compression_ratio": 1.8049792531120332, "no_speech_prob": 3.6118985917710233e-06}, {"id": 724, "seek": 445088, "start": 4466.16, "end": 4469.76, "text": " in the backward pass so let's write out the backward pass here now", "tokens": [50364, 457, 272, 293, 914, 741, 307, 257, 5386, 8062, 502, 538, 12145, 370, 341, 3175, 510, 486, 767, 360, 30024, 50660, 50660, 293, 370, 321, 362, 281, 312, 5026, 365, 300, 293, 382, 257, 12075, 281, 505, 797, 570, 295, 264, 11848, 507, 50876, 50876, 257, 30024, 294, 264, 2128, 1320, 1355, 257, 7006, 26225, 293, 4412, 456, 486, 312, 257, 2408, 51128, 51128, 294, 264, 23897, 1320, 370, 718, 311, 2464, 484, 264, 23897, 1320, 510, 586, 51308, 51436, 646, 79, 1513, 559, 473, 666, 264, 276, 79, 19404, 570, 341, 307, 613, 366, 264, 912, 3909, 550, 264, 2654, 13760, 337, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.15588646291572356, "compression_ratio": 1.8049792531120332, "no_speech_prob": 3.6118985917710233e-06}, {"id": 725, "seek": 445088, "start": 4472.32, "end": 4478.32, "text": " backpropagate into the hpbn because this is these are the same shape then the local derivative for", "tokens": [50364, 457, 272, 293, 914, 741, 307, 257, 5386, 8062, 502, 538, 12145, 370, 341, 3175, 510, 486, 767, 360, 30024, 50660, 50660, 293, 370, 321, 362, 281, 312, 5026, 365, 300, 293, 382, 257, 12075, 281, 505, 797, 570, 295, 264, 11848, 507, 50876, 50876, 257, 30024, 294, 264, 2128, 1320, 1355, 257, 7006, 26225, 293, 4412, 456, 486, 312, 257, 2408, 51128, 51128, 294, 264, 23897, 1320, 370, 718, 311, 2464, 484, 264, 23897, 1320, 510, 586, 51308, 51436, 646, 79, 1513, 559, 473, 666, 264, 276, 79, 19404, 570, 341, 307, 613, 366, 264, 912, 3909, 550, 264, 2654, 13760, 337, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.15588646291572356, "compression_ratio": 1.8049792531120332, "no_speech_prob": 3.6118985917710233e-06}, {"id": 726, "seek": 447832, "start": 4478.32, "end": 4483.84, "text": " each one of the elements here is just one for the corresponding element in here so basically what", "tokens": [50364, 1184, 472, 295, 264, 4959, 510, 307, 445, 472, 337, 264, 11760, 4478, 294, 510, 370, 1936, 437, 50640, 50640, 341, 1355, 307, 300, 264, 16235, 445, 2935, 14341, 309, 311, 445, 257, 7006, 15187, 309, 311, 3125, 50936, 50936, 370, 741, 478, 445, 516, 281, 26506, 341, 40863, 445, 337, 4514, 281, 1884, 364, 1900, 5055, 295, 274, 65, 293, 7593, 51288, 51340, 293, 550, 510, 281, 646, 48256, 666, 341, 472, 437, 741, 478, 28173, 281, 360, 510, 307, 51564, 51636], "temperature": 0.0, "avg_logprob": -0.18537559509277343, "compression_ratio": 1.7718446601941749, "no_speech_prob": 3.6687902138510253e-06}, {"id": 727, "seek": 447832, "start": 4483.84, "end": 4489.759999999999, "text": " this means is that the gradient just simply copies it's just a variable assignment it's quality", "tokens": [50364, 1184, 472, 295, 264, 4959, 510, 307, 445, 472, 337, 264, 11760, 4478, 294, 510, 370, 1936, 437, 50640, 50640, 341, 1355, 307, 300, 264, 16235, 445, 2935, 14341, 309, 311, 445, 257, 7006, 15187, 309, 311, 3125, 50936, 50936, 370, 741, 478, 445, 516, 281, 26506, 341, 40863, 445, 337, 4514, 281, 1884, 364, 1900, 5055, 295, 274, 65, 293, 7593, 51288, 51340, 293, 550, 510, 281, 646, 48256, 666, 341, 472, 437, 741, 478, 28173, 281, 360, 510, 307, 51564, 51636], "temperature": 0.0, "avg_logprob": -0.18537559509277343, "compression_ratio": 1.7718446601941749, "no_speech_prob": 3.6687902138510253e-06}, {"id": 728, "seek": 447832, "start": 4489.759999999999, "end": 4496.799999999999, "text": " so i'm just going to clone this tensor just for safety to create an exact copy of db and diff", "tokens": [50364, 1184, 472, 295, 264, 4959, 510, 307, 445, 472, 337, 264, 11760, 4478, 294, 510, 370, 1936, 437, 50640, 50640, 341, 1355, 307, 300, 264, 16235, 445, 2935, 14341, 309, 311, 445, 257, 7006, 15187, 309, 311, 3125, 50936, 50936, 370, 741, 478, 445, 516, 281, 26506, 341, 40863, 445, 337, 4514, 281, 1884, 364, 1900, 5055, 295, 274, 65, 293, 7593, 51288, 51340, 293, 550, 510, 281, 646, 48256, 666, 341, 472, 437, 741, 478, 28173, 281, 360, 510, 307, 51564, 51636], "temperature": 0.0, "avg_logprob": -0.18537559509277343, "compression_ratio": 1.7718446601941749, "no_speech_prob": 3.6687902138510253e-06}, {"id": 729, "seek": 447832, "start": 4497.84, "end": 4502.32, "text": " and then here to back propagate into this one what i'm inclined to do here is", "tokens": [50364, 1184, 472, 295, 264, 4959, 510, 307, 445, 472, 337, 264, 11760, 4478, 294, 510, 370, 1936, 437, 50640, 50640, 341, 1355, 307, 300, 264, 16235, 445, 2935, 14341, 309, 311, 445, 257, 7006, 15187, 309, 311, 3125, 50936, 50936, 370, 741, 478, 445, 516, 281, 26506, 341, 40863, 445, 337, 4514, 281, 1884, 364, 1900, 5055, 295, 274, 65, 293, 7593, 51288, 51340, 293, 550, 510, 281, 646, 48256, 666, 341, 472, 437, 741, 478, 28173, 281, 360, 510, 307, 51564, 51636], "temperature": 0.0, "avg_logprob": -0.18537559509277343, "compression_ratio": 1.7718446601941749, "no_speech_prob": 3.6687902138510253e-06}, {"id": 730, "seek": 450232, "start": 4502.32, "end": 4510.639999999999, "text": " the bn mean i will basically be uh what is the local derivative well it's negative torch dot", "tokens": [50364, 264, 272, 77, 914, 741, 486, 1936, 312, 2232, 437, 307, 264, 2654, 13760, 731, 309, 311, 3671, 27822, 5893, 50780, 50780, 472, 411, 295, 264, 3909, 295, 272, 293, 7593, 558, 293, 550, 1413, 264, 1105, 13760, 510, 274, 65, 293, 7593, 51540, 51780], "temperature": 0.0, "avg_logprob": -0.35864935529992936, "compression_ratio": 1.5333333333333334, "no_speech_prob": 4.637646725313971e-06}, {"id": 731, "seek": 450232, "start": 4510.639999999999, "end": 4525.84, "text": " one like of the shape of b and diff right and then times the um derivative here db and diff", "tokens": [50364, 264, 272, 77, 914, 741, 486, 1936, 312, 2232, 437, 307, 264, 2654, 13760, 731, 309, 311, 3671, 27822, 5893, 50780, 50780, 472, 411, 295, 264, 3909, 295, 272, 293, 7593, 558, 293, 550, 1413, 264, 1105, 13760, 510, 274, 65, 293, 7593, 51540, 51780], "temperature": 0.0, "avg_logprob": -0.35864935529992936, "compression_ratio": 1.5333333333333334, "no_speech_prob": 4.637646725313971e-06}, {"id": 732, "seek": 452584, "start": 4525.84, "end": 4529.6, "text": " um derivative here db and diff", "tokens": [50364, 1105, 13760, 510, 274, 65, 293, 7593, 50552, 50724, 293, 341, 510, 307, 264, 646, 38377, 337, 264, 46365, 272, 293, 914, 741, 370, 741, 920, 362, 281, 51012, 51012, 646, 48256, 807, 264, 39911, 294, 264, 30024, 293, 741, 360, 300, 538, 884, 257, 2408, 51264, 51292, 370, 741, 478, 516, 281, 747, 341, 1379, 551, 293, 741, 478, 516, 281, 360, 257, 2408, 670, 264, 4018, 10139, 597, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.1063602808359507, "compression_ratio": 1.8132530120481927, "no_speech_prob": 4.092701601621229e-06}, {"id": 733, "seek": 452584, "start": 4533.04, "end": 4538.8, "text": " and this here is the back propagation for the replicated b and mean i so i still have to", "tokens": [50364, 1105, 13760, 510, 274, 65, 293, 7593, 50552, 50724, 293, 341, 510, 307, 264, 646, 38377, 337, 264, 46365, 272, 293, 914, 741, 370, 741, 920, 362, 281, 51012, 51012, 646, 48256, 807, 264, 39911, 294, 264, 30024, 293, 741, 360, 300, 538, 884, 257, 2408, 51264, 51292, 370, 741, 478, 516, 281, 747, 341, 1379, 551, 293, 741, 478, 516, 281, 360, 257, 2408, 670, 264, 4018, 10139, 597, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.1063602808359507, "compression_ratio": 1.8132530120481927, "no_speech_prob": 4.092701601621229e-06}, {"id": 734, "seek": 452584, "start": 4538.8, "end": 4543.84, "text": " back propagate through the replication in the broadcasting and i do that by doing a sum", "tokens": [50364, 1105, 13760, 510, 274, 65, 293, 7593, 50552, 50724, 293, 341, 510, 307, 264, 646, 38377, 337, 264, 46365, 272, 293, 914, 741, 370, 741, 920, 362, 281, 51012, 51012, 646, 48256, 807, 264, 39911, 294, 264, 30024, 293, 741, 360, 300, 538, 884, 257, 2408, 51264, 51292, 370, 741, 478, 516, 281, 747, 341, 1379, 551, 293, 741, 478, 516, 281, 360, 257, 2408, 670, 264, 4018, 10139, 597, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.1063602808359507, "compression_ratio": 1.8132530120481927, "no_speech_prob": 4.092701601621229e-06}, {"id": 735, "seek": 452584, "start": 4544.400000000001, "end": 4549.2, "text": " so i'm going to take this whole thing and i'm going to do a sum over the zero dimension which", "tokens": [50364, 1105, 13760, 510, 274, 65, 293, 7593, 50552, 50724, 293, 341, 510, 307, 264, 646, 38377, 337, 264, 46365, 272, 293, 914, 741, 370, 741, 920, 362, 281, 51012, 51012, 646, 48256, 807, 264, 39911, 294, 264, 30024, 293, 741, 360, 300, 538, 884, 257, 2408, 51264, 51292, 370, 741, 478, 516, 281, 747, 341, 1379, 551, 293, 741, 478, 516, 281, 360, 257, 2408, 670, 264, 4018, 10139, 597, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.1063602808359507, "compression_ratio": 1.8132530120481927, "no_speech_prob": 4.092701601621229e-06}, {"id": 736, "seek": 454920, "start": 4549.2, "end": 4557.84, "text": " was the replication so if you scrutinize this by the way you'll notice that this is the same shape", "tokens": [50364, 390, 264, 39911, 370, 498, 291, 28949, 259, 1125, 341, 538, 264, 636, 291, 603, 3449, 300, 341, 307, 264, 912, 3909, 50796, 50796, 382, 300, 293, 370, 437, 741, 478, 884, 2232, 437, 741, 478, 884, 510, 1177, 380, 767, 652, 300, 709, 2020, 570, 51012, 51012, 309, 311, 445, 257, 10225, 295, 2306, 30955, 274, 65, 293, 7593, 370, 294, 1186, 741, 393, 445, 360, 341, 1105, 293, 300, 51424, 51424, 307, 10344, 370, 341, 307, 264, 11532, 23897, 1320, 718, 385, 5055, 309, 510, 293, 550, 718, 385, 2871, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.04980128010114034, "compression_ratio": 1.787037037037037, "no_speech_prob": 1.0676898227757192e-06}, {"id": 737, "seek": 454920, "start": 4557.84, "end": 4562.16, "text": " as that and so what i'm doing uh what i'm doing here doesn't actually make that much sense because", "tokens": [50364, 390, 264, 39911, 370, 498, 291, 28949, 259, 1125, 341, 538, 264, 636, 291, 603, 3449, 300, 341, 307, 264, 912, 3909, 50796, 50796, 382, 300, 293, 370, 437, 741, 478, 884, 2232, 437, 741, 478, 884, 510, 1177, 380, 767, 652, 300, 709, 2020, 570, 51012, 51012, 309, 311, 445, 257, 10225, 295, 2306, 30955, 274, 65, 293, 7593, 370, 294, 1186, 741, 393, 445, 360, 341, 1105, 293, 300, 51424, 51424, 307, 10344, 370, 341, 307, 264, 11532, 23897, 1320, 718, 385, 5055, 309, 510, 293, 550, 718, 385, 2871, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.04980128010114034, "compression_ratio": 1.787037037037037, "no_speech_prob": 1.0676898227757192e-06}, {"id": 738, "seek": 454920, "start": 4562.16, "end": 4570.4, "text": " it's just a array of ones multiplying db and diff so in fact i can just do this um and that", "tokens": [50364, 390, 264, 39911, 370, 498, 291, 28949, 259, 1125, 341, 538, 264, 636, 291, 603, 3449, 300, 341, 307, 264, 912, 3909, 50796, 50796, 382, 300, 293, 370, 437, 741, 478, 884, 2232, 437, 741, 478, 884, 510, 1177, 380, 767, 652, 300, 709, 2020, 570, 51012, 51012, 309, 311, 445, 257, 10225, 295, 2306, 30955, 274, 65, 293, 7593, 370, 294, 1186, 741, 393, 445, 360, 341, 1105, 293, 300, 51424, 51424, 307, 10344, 370, 341, 307, 264, 11532, 23897, 1320, 718, 385, 5055, 309, 510, 293, 550, 718, 385, 2871, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.04980128010114034, "compression_ratio": 1.787037037037037, "no_speech_prob": 1.0676898227757192e-06}, {"id": 739, "seek": 454920, "start": 4570.4, "end": 4578.639999999999, "text": " is equivalent so this is the candidate backward pass let me copy it here and then let me comment", "tokens": [50364, 390, 264, 39911, 370, 498, 291, 28949, 259, 1125, 341, 538, 264, 636, 291, 603, 3449, 300, 341, 307, 264, 912, 3909, 50796, 50796, 382, 300, 293, 370, 437, 741, 478, 884, 2232, 437, 741, 478, 884, 510, 1177, 380, 767, 652, 300, 709, 2020, 570, 51012, 51012, 309, 311, 445, 257, 10225, 295, 2306, 30955, 274, 65, 293, 7593, 370, 294, 1186, 741, 393, 445, 360, 341, 1105, 293, 300, 51424, 51424, 307, 10344, 370, 341, 307, 264, 11532, 23897, 1320, 718, 385, 5055, 309, 510, 293, 550, 718, 385, 2871, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.04980128010114034, "compression_ratio": 1.787037037037037, "no_speech_prob": 1.0676898227757192e-06}, {"id": 740, "seek": 457864, "start": 4578.64, "end": 4591.6, "text": " out this one and this one enter and it's wrong damn actually sorry this is supposed to be wrong", "tokens": [50364, 484, 341, 472, 293, 341, 472, 3242, 293, 309, 311, 2085, 8151, 767, 2597, 341, 307, 3442, 281, 312, 2085, 51012, 51012, 293, 309, 311, 3442, 281, 312, 2085, 570, 321, 366, 646, 12425, 990, 490, 257, 272, 293, 7593, 666, 276, 659, 12, 65, 77, 51328, 51356, 293, 457, 321, 434, 406, 1096, 570, 272, 293, 914, 741, 5946, 322, 276, 659, 12, 65, 77, 293, 456, 486, 312, 257, 1150, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.1155793917806525, "compression_ratio": 1.75, "no_speech_prob": 5.09360233991174e-06}, {"id": 741, "seek": 457864, "start": 4591.6, "end": 4597.92, "text": " and it's supposed to be wrong because we are back propagating from a b and diff into h pre-bn", "tokens": [50364, 484, 341, 472, 293, 341, 472, 3242, 293, 309, 311, 2085, 8151, 767, 2597, 341, 307, 3442, 281, 312, 2085, 51012, 51012, 293, 309, 311, 3442, 281, 312, 2085, 570, 321, 366, 646, 12425, 990, 490, 257, 272, 293, 7593, 666, 276, 659, 12, 65, 77, 51328, 51356, 293, 457, 321, 434, 406, 1096, 570, 272, 293, 914, 741, 5946, 322, 276, 659, 12, 65, 77, 293, 456, 486, 312, 257, 1150, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.1155793917806525, "compression_ratio": 1.75, "no_speech_prob": 5.09360233991174e-06}, {"id": 742, "seek": 457864, "start": 4598.4800000000005, "end": 4604.160000000001, "text": " and but we're not done because b and mean i depends on h pre-bn and there will be a second", "tokens": [50364, 484, 341, 472, 293, 341, 472, 3242, 293, 309, 311, 2085, 8151, 767, 2597, 341, 307, 3442, 281, 312, 2085, 51012, 51012, 293, 309, 311, 3442, 281, 312, 2085, 570, 321, 366, 646, 12425, 990, 490, 257, 272, 293, 7593, 666, 276, 659, 12, 65, 77, 51328, 51356, 293, 457, 321, 434, 406, 1096, 570, 272, 293, 914, 741, 5946, 322, 276, 659, 12, 65, 77, 293, 456, 486, 312, 257, 1150, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.1155793917806525, "compression_ratio": 1.75, "no_speech_prob": 5.09360233991174e-06}, {"id": 743, "seek": 460416, "start": 4604.16, "end": 4608.72, "text": " portion of that derivative coming from this second branch so we're not done yet and we expect it to", "tokens": [50364, 8044, 295, 300, 13760, 1348, 490, 341, 1150, 9819, 370, 321, 434, 406, 1096, 1939, 293, 321, 2066, 309, 281, 50592, 50592, 312, 18424, 370, 456, 291, 352, 370, 718, 311, 586, 646, 48256, 490, 272, 293, 914, 741, 666, 276, 659, 12, 65, 77, 50884, 51024, 293, 370, 510, 797, 321, 362, 281, 312, 5026, 570, 456, 311, 257, 30024, 2051, 51176, 51228, 420, 456, 311, 257, 2408, 2051, 264, 4018, 10139, 370, 341, 486, 1261, 666, 30024, 294, 264, 23897, 1320, 51472, 51472, 586, 293, 741, 478, 516, 281, 352, 257, 707, 857, 4663, 322, 341, 1622, 570, 309, 307, 588, 2531, 281, 264, 1622, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.05082277349523596, "compression_ratio": 1.7624521072796935, "no_speech_prob": 4.222743882564828e-06}, {"id": 744, "seek": 460416, "start": 4608.72, "end": 4614.5599999999995, "text": " be incorrect so there you go so let's now back propagate from b and mean i into h pre-bn", "tokens": [50364, 8044, 295, 300, 13760, 1348, 490, 341, 1150, 9819, 370, 321, 434, 406, 1096, 1939, 293, 321, 2066, 309, 281, 50592, 50592, 312, 18424, 370, 456, 291, 352, 370, 718, 311, 586, 646, 48256, 490, 272, 293, 914, 741, 666, 276, 659, 12, 65, 77, 50884, 51024, 293, 370, 510, 797, 321, 362, 281, 312, 5026, 570, 456, 311, 257, 30024, 2051, 51176, 51228, 420, 456, 311, 257, 2408, 2051, 264, 4018, 10139, 370, 341, 486, 1261, 666, 30024, 294, 264, 23897, 1320, 51472, 51472, 586, 293, 741, 478, 516, 281, 352, 257, 707, 857, 4663, 322, 341, 1622, 570, 309, 307, 588, 2531, 281, 264, 1622, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.05082277349523596, "compression_ratio": 1.7624521072796935, "no_speech_prob": 4.222743882564828e-06}, {"id": 745, "seek": 460416, "start": 4617.36, "end": 4620.4, "text": " and so here again we have to be careful because there's a broadcasting along", "tokens": [50364, 8044, 295, 300, 13760, 1348, 490, 341, 1150, 9819, 370, 321, 434, 406, 1096, 1939, 293, 321, 2066, 309, 281, 50592, 50592, 312, 18424, 370, 456, 291, 352, 370, 718, 311, 586, 646, 48256, 490, 272, 293, 914, 741, 666, 276, 659, 12, 65, 77, 50884, 51024, 293, 370, 510, 797, 321, 362, 281, 312, 5026, 570, 456, 311, 257, 30024, 2051, 51176, 51228, 420, 456, 311, 257, 2408, 2051, 264, 4018, 10139, 370, 341, 486, 1261, 666, 30024, 294, 264, 23897, 1320, 51472, 51472, 586, 293, 741, 478, 516, 281, 352, 257, 707, 857, 4663, 322, 341, 1622, 570, 309, 307, 588, 2531, 281, 264, 1622, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.05082277349523596, "compression_ratio": 1.7624521072796935, "no_speech_prob": 4.222743882564828e-06}, {"id": 746, "seek": 460416, "start": 4621.44, "end": 4626.32, "text": " or there's a sum along the zero dimension so this will turn into broadcasting in the backward pass", "tokens": [50364, 8044, 295, 300, 13760, 1348, 490, 341, 1150, 9819, 370, 321, 434, 406, 1096, 1939, 293, 321, 2066, 309, 281, 50592, 50592, 312, 18424, 370, 456, 291, 352, 370, 718, 311, 586, 646, 48256, 490, 272, 293, 914, 741, 666, 276, 659, 12, 65, 77, 50884, 51024, 293, 370, 510, 797, 321, 362, 281, 312, 5026, 570, 456, 311, 257, 30024, 2051, 51176, 51228, 420, 456, 311, 257, 2408, 2051, 264, 4018, 10139, 370, 341, 486, 1261, 666, 30024, 294, 264, 23897, 1320, 51472, 51472, 586, 293, 741, 478, 516, 281, 352, 257, 707, 857, 4663, 322, 341, 1622, 570, 309, 307, 588, 2531, 281, 264, 1622, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.05082277349523596, "compression_ratio": 1.7624521072796935, "no_speech_prob": 4.222743882564828e-06}, {"id": 747, "seek": 460416, "start": 4626.32, "end": 4630.96, "text": " now and i'm going to go a little bit faster on this line because it is very similar to the line", "tokens": [50364, 8044, 295, 300, 13760, 1348, 490, 341, 1150, 9819, 370, 321, 434, 406, 1096, 1939, 293, 321, 2066, 309, 281, 50592, 50592, 312, 18424, 370, 456, 291, 352, 370, 718, 311, 586, 646, 48256, 490, 272, 293, 914, 741, 666, 276, 659, 12, 65, 77, 50884, 51024, 293, 370, 510, 797, 321, 362, 281, 312, 5026, 570, 456, 311, 257, 30024, 2051, 51176, 51228, 420, 456, 311, 257, 2408, 2051, 264, 4018, 10139, 370, 341, 486, 1261, 666, 30024, 294, 264, 23897, 1320, 51472, 51472, 586, 293, 741, 478, 516, 281, 352, 257, 707, 857, 4663, 322, 341, 1622, 570, 309, 307, 588, 2531, 281, 264, 1622, 51704, 51704], "temperature": 0.0, "avg_logprob": -0.05082277349523596, "compression_ratio": 1.7624521072796935, "no_speech_prob": 4.222743882564828e-06}, {"id": 748, "seek": 463096, "start": 4630.96, "end": 4641.36, "text": " that we had before and multiple lines in the past in fact so d h pre-bn will be the gradient will", "tokens": [50364, 300, 321, 632, 949, 293, 3866, 3876, 294, 264, 1791, 294, 1186, 370, 274, 276, 659, 12, 65, 77, 486, 312, 264, 16235, 486, 50884, 50884, 312, 36039, 538, 472, 670, 297, 293, 550, 1936, 341, 16235, 510, 322, 274, 65, 293, 914, 741, 307, 516, 281, 312, 51244, 51244, 36039, 538, 472, 670, 297, 293, 550, 309, 311, 516, 281, 3095, 2108, 439, 264, 13766, 293, 19107, 2564, 666, 274, 71, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.09462380409240723, "compression_ratio": 1.7791411042944785, "no_speech_prob": 2.0904078610328725e-06}, {"id": 749, "seek": 463096, "start": 4641.36, "end": 4648.56, "text": " be scaled by one over n and then basically this gradient here on db and mean i is going to be", "tokens": [50364, 300, 321, 632, 949, 293, 3866, 3876, 294, 264, 1791, 294, 1186, 370, 274, 276, 659, 12, 65, 77, 486, 312, 264, 16235, 486, 50884, 50884, 312, 36039, 538, 472, 670, 297, 293, 550, 1936, 341, 16235, 510, 322, 274, 65, 293, 914, 741, 307, 516, 281, 312, 51244, 51244, 36039, 538, 472, 670, 297, 293, 550, 309, 311, 516, 281, 3095, 2108, 439, 264, 13766, 293, 19107, 2564, 666, 274, 71, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.09462380409240723, "compression_ratio": 1.7791411042944785, "no_speech_prob": 2.0904078610328725e-06}, {"id": 750, "seek": 463096, "start": 4648.56, "end": 4654.4, "text": " scaled by one over n and then it's going to flow across all the columns and deposit itself into dh", "tokens": [50364, 300, 321, 632, 949, 293, 3866, 3876, 294, 264, 1791, 294, 1186, 370, 274, 276, 659, 12, 65, 77, 486, 312, 264, 16235, 486, 50884, 50884, 312, 36039, 538, 472, 670, 297, 293, 550, 1936, 341, 16235, 510, 322, 274, 65, 293, 914, 741, 307, 516, 281, 312, 51244, 51244, 36039, 538, 472, 670, 297, 293, 550, 309, 311, 516, 281, 3095, 2108, 439, 264, 13766, 293, 19107, 2564, 666, 274, 71, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.09462380409240723, "compression_ratio": 1.7791411042944785, "no_speech_prob": 2.0904078610328725e-06}, {"id": 751, "seek": 465440, "start": 4654.4, "end": 4661.2, "text": " pre-bn so what we want is this thing scaled by one over n when you put the content up front here", "tokens": [50364, 659, 12, 65, 77, 370, 437, 321, 528, 307, 341, 551, 36039, 538, 472, 670, 297, 562, 291, 829, 264, 2701, 493, 1868, 510, 50704, 50912, 370, 4373, 760, 264, 16235, 293, 586, 321, 643, 281, 25356, 309, 2108, 439, 264, 1105, 2108, 439, 264, 51268, 51268, 13241, 510, 370, 321, 741, 411, 281, 360, 300, 538, 27822, 5893, 1564, 411, 295, 1936, 1105, 276, 659, 12, 65, 77, 51740, 51840], "temperature": 0.0, "avg_logprob": -0.12840086793246336, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.771658379671862e-06}, {"id": 752, "seek": 465440, "start": 4665.36, "end": 4672.48, "text": " so scale down the gradient and now we need to replicate it across all the um across all the", "tokens": [50364, 659, 12, 65, 77, 370, 437, 321, 528, 307, 341, 551, 36039, 538, 472, 670, 297, 562, 291, 829, 264, 2701, 493, 1868, 510, 50704, 50912, 370, 4373, 760, 264, 16235, 293, 586, 321, 643, 281, 25356, 309, 2108, 439, 264, 1105, 2108, 439, 264, 51268, 51268, 13241, 510, 370, 321, 741, 411, 281, 360, 300, 538, 27822, 5893, 1564, 411, 295, 1936, 1105, 276, 659, 12, 65, 77, 51740, 51840], "temperature": 0.0, "avg_logprob": -0.12840086793246336, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.771658379671862e-06}, {"id": 753, "seek": 465440, "start": 4672.48, "end": 4681.92, "text": " rows here so we i like to do that by torch dot once like of basically um h pre-bn", "tokens": [50364, 659, 12, 65, 77, 370, 437, 321, 528, 307, 341, 551, 36039, 538, 472, 670, 297, 562, 291, 829, 264, 2701, 493, 1868, 510, 50704, 50912, 370, 4373, 760, 264, 16235, 293, 586, 321, 643, 281, 25356, 309, 2108, 439, 264, 1105, 2108, 439, 264, 51268, 51268, 13241, 510, 370, 321, 741, 411, 281, 360, 300, 538, 27822, 5893, 1564, 411, 295, 1936, 1105, 276, 659, 12, 65, 77, 51740, 51840], "temperature": 0.0, "avg_logprob": -0.12840086793246336, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.771658379671862e-06}, {"id": 754, "seek": 468192, "start": 4681.92, "end": 4688.72, "text": " and i will let the broadcasting do the work of replication so", "tokens": [50364, 293, 741, 486, 718, 264, 30024, 360, 264, 589, 295, 39911, 370, 50704, 50908, 411, 300, 370, 341, 307, 264, 276, 659, 12, 65, 77, 293, 4696, 321, 393, 1804, 6915, 300, 51336, 51556], "temperature": 0.0, "avg_logprob": -0.37191033363342285, "compression_ratio": 1.2912621359223302, "no_speech_prob": 4.860138687945437e-06}, {"id": 755, "seek": 468192, "start": 4692.8, "end": 4701.36, "text": " like that so this is the h pre-bn and hopefully we can plus equals that", "tokens": [50364, 293, 741, 486, 718, 264, 30024, 360, 264, 589, 295, 39911, 370, 50704, 50908, 411, 300, 370, 341, 307, 264, 276, 659, 12, 65, 77, 293, 4696, 321, 393, 1804, 6915, 300, 51336, 51556], "temperature": 0.0, "avg_logprob": -0.37191033363342285, "compression_ratio": 1.2912621359223302, "no_speech_prob": 4.860138687945437e-06}, {"id": 756, "seek": 470136, "start": 4701.36, "end": 4708.88, "text": " we can plus equals that so this here is broadcasting", "tokens": [50364, 321, 393, 1804, 6915, 300, 370, 341, 510, 307, 30024, 50740, 50812, 293, 550, 341, 307, 264, 21589, 370, 341, 820, 312, 3006, 1392, 370, 300, 36362, 264, 646, 51124, 51124, 38377, 295, 264, 15245, 10206, 4583, 293, 321, 366, 586, 510, 718, 311, 646, 48256, 807, 264, 8213, 51332, 51332, 4583, 472, 510, 586, 570, 1203, 307, 1242, 257, 707, 28450, 3219, 741, 5055, 1791, 292, 264, 1622, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.07007537476003986, "compression_ratio": 1.7164948453608246, "no_speech_prob": 4.637727215595078e-06}, {"id": 757, "seek": 470136, "start": 4710.32, "end": 4716.5599999999995, "text": " and then this is the scaling so this should be correct okay so that completes the back", "tokens": [50364, 321, 393, 1804, 6915, 300, 370, 341, 510, 307, 30024, 50740, 50812, 293, 550, 341, 307, 264, 21589, 370, 341, 820, 312, 3006, 1392, 370, 300, 36362, 264, 646, 51124, 51124, 38377, 295, 264, 15245, 10206, 4583, 293, 321, 366, 586, 510, 718, 311, 646, 48256, 807, 264, 8213, 51332, 51332, 4583, 472, 510, 586, 570, 1203, 307, 1242, 257, 707, 28450, 3219, 741, 5055, 1791, 292, 264, 1622, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.07007537476003986, "compression_ratio": 1.7164948453608246, "no_speech_prob": 4.637727215595078e-06}, {"id": 758, "seek": 470136, "start": 4716.5599999999995, "end": 4720.719999999999, "text": " propagation of the batch drum layer and we are now here let's back propagate through the linear", "tokens": [50364, 321, 393, 1804, 6915, 300, 370, 341, 510, 307, 30024, 50740, 50812, 293, 550, 341, 307, 264, 21589, 370, 341, 820, 312, 3006, 1392, 370, 300, 36362, 264, 646, 51124, 51124, 38377, 295, 264, 15245, 10206, 4583, 293, 321, 366, 586, 510, 718, 311, 646, 48256, 807, 264, 8213, 51332, 51332, 4583, 472, 510, 586, 570, 1203, 307, 1242, 257, 707, 28450, 3219, 741, 5055, 1791, 292, 264, 1622, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.07007537476003986, "compression_ratio": 1.7164948453608246, "no_speech_prob": 4.637727215595078e-06}, {"id": 759, "seek": 470136, "start": 4720.719999999999, "end": 4726.96, "text": " layer one here now because everything is getting a little vertically crazy i copy pasted the line", "tokens": [50364, 321, 393, 1804, 6915, 300, 370, 341, 510, 307, 30024, 50740, 50812, 293, 550, 341, 307, 264, 21589, 370, 341, 820, 312, 3006, 1392, 370, 300, 36362, 264, 646, 51124, 51124, 38377, 295, 264, 15245, 10206, 4583, 293, 321, 366, 586, 510, 718, 311, 646, 48256, 807, 264, 8213, 51332, 51332, 4583, 472, 510, 586, 570, 1203, 307, 1242, 257, 707, 28450, 3219, 741, 5055, 1791, 292, 264, 1622, 51644, 51644], "temperature": 0.0, "avg_logprob": -0.07007537476003986, "compression_ratio": 1.7164948453608246, "no_speech_prob": 4.637727215595078e-06}, {"id": 760, "seek": 472696, "start": 4726.96, "end": 4732.24, "text": " here and let's just back propagate through this one line so first of course we inspect the shapes", "tokens": [50364, 510, 293, 718, 311, 445, 646, 48256, 807, 341, 472, 1622, 370, 700, 295, 1164, 321, 15018, 264, 10854, 50628, 50628, 293, 321, 536, 300, 341, 307, 8858, 538, 12145, 293, 3857, 307, 8858, 538, 2217, 261, 16, 307, 2217, 538, 12145, 293, 272, 16, 307, 445, 12145, 370, 382, 741, 51268, 51268, 2835, 646, 12425, 990, 807, 8213, 7914, 307, 6457, 1858, 445, 538, 14324, 264, 10854, 370, 718, 311, 51508, 51508], "temperature": 0.0, "avg_logprob": -0.08341630935668945, "compression_ratio": 1.6878612716763006, "no_speech_prob": 1.8738569451670628e-06}, {"id": 761, "seek": 472696, "start": 4732.24, "end": 4745.04, "text": " and we see that this is 32 by 64 and cat is 32 by 30 w1 is 30 by 64 and b1 is just 64 so as i", "tokens": [50364, 510, 293, 718, 311, 445, 646, 48256, 807, 341, 472, 1622, 370, 700, 295, 1164, 321, 15018, 264, 10854, 50628, 50628, 293, 321, 536, 300, 341, 307, 8858, 538, 12145, 293, 3857, 307, 8858, 538, 2217, 261, 16, 307, 2217, 538, 12145, 293, 272, 16, 307, 445, 12145, 370, 382, 741, 51268, 51268, 2835, 646, 12425, 990, 807, 8213, 7914, 307, 6457, 1858, 445, 538, 14324, 264, 10854, 370, 718, 311, 51508, 51508], "temperature": 0.0, "avg_logprob": -0.08341630935668945, "compression_ratio": 1.6878612716763006, "no_speech_prob": 1.8738569451670628e-06}, {"id": 762, "seek": 472696, "start": 4745.04, "end": 4749.84, "text": " mentioned back propagating through linear layers is fairly easy just by matching the shapes so let's", "tokens": [50364, 510, 293, 718, 311, 445, 646, 48256, 807, 341, 472, 1622, 370, 700, 295, 1164, 321, 15018, 264, 10854, 50628, 50628, 293, 321, 536, 300, 341, 307, 8858, 538, 12145, 293, 3857, 307, 8858, 538, 2217, 261, 16, 307, 2217, 538, 12145, 293, 272, 16, 307, 445, 12145, 370, 382, 741, 51268, 51268, 2835, 646, 12425, 990, 807, 8213, 7914, 307, 6457, 1858, 445, 538, 14324, 264, 10854, 370, 718, 311, 51508, 51508], "temperature": 0.0, "avg_logprob": -0.08341630935668945, "compression_ratio": 1.6878612716763006, "no_speech_prob": 1.8738569451670628e-06}, {"id": 763, "seek": 474984, "start": 4749.84, "end": 4760.400000000001, "text": " do that we have that d amp cat should be um some matrix multiplication of dh pre-bn with w1 and", "tokens": [50364, 360, 300, 321, 362, 300, 274, 18648, 3857, 820, 312, 1105, 512, 8141, 27290, 295, 274, 71, 659, 12, 65, 77, 365, 261, 16, 293, 50892, 50892, 472, 25167, 11732, 294, 456, 370, 281, 652, 2232, 18648, 3857, 312, 8858, 538, 2217, 741, 643, 281, 747, 274, 71, 659, 12, 65, 77, 51424, 51480, 8858, 538, 12145, 293, 12972, 309, 538, 261, 16, 5893, 25167, 51700], "temperature": 0.0, "avg_logprob": -0.1424850015079274, "compression_ratio": 1.5684931506849316, "no_speech_prob": 1.3081603356113192e-06}, {"id": 764, "seek": 474984, "start": 4760.400000000001, "end": 4771.04, "text": " one transpose thrown in there so to make uh amp cat be 32 by 30 i need to take dh pre-bn", "tokens": [50364, 360, 300, 321, 362, 300, 274, 18648, 3857, 820, 312, 1105, 512, 8141, 27290, 295, 274, 71, 659, 12, 65, 77, 365, 261, 16, 293, 50892, 50892, 472, 25167, 11732, 294, 456, 370, 281, 652, 2232, 18648, 3857, 312, 8858, 538, 2217, 741, 643, 281, 747, 274, 71, 659, 12, 65, 77, 51424, 51480, 8858, 538, 12145, 293, 12972, 309, 538, 261, 16, 5893, 25167, 51700], "temperature": 0.0, "avg_logprob": -0.1424850015079274, "compression_ratio": 1.5684931506849316, "no_speech_prob": 1.3081603356113192e-06}, {"id": 765, "seek": 477104, "start": 4771.04, "end": 4784.0, "text": " 32 by 64 and multiply it by w1 dot transpose to get dw1 i need to end up with 30 by 64", "tokens": [50364, 8858, 538, 12145, 293, 12972, 309, 538, 261, 16, 5893, 25167, 281, 483, 27379, 16, 741, 643, 281, 917, 493, 365, 2217, 538, 12145, 51012, 51052, 370, 281, 483, 300, 741, 643, 281, 747, 2232, 18648, 3857, 25167, 293, 12972, 300, 538, 2232, 274, 71, 659, 12, 65, 77, 51540, 51684], "temperature": 0.0, "avg_logprob": -0.20201098244145232, "compression_ratio": 1.5132743362831858, "no_speech_prob": 2.026117044806597e-06}, {"id": 766, "seek": 477104, "start": 4784.8, "end": 4794.56, "text": " so to get that i need to take uh amp cat transpose and multiply that by uh dh pre-bn", "tokens": [50364, 8858, 538, 12145, 293, 12972, 309, 538, 261, 16, 5893, 25167, 281, 483, 27379, 16, 741, 643, 281, 917, 493, 365, 2217, 538, 12145, 51012, 51052, 370, 281, 483, 300, 741, 643, 281, 747, 2232, 18648, 3857, 25167, 293, 12972, 300, 538, 2232, 274, 71, 659, 12, 65, 77, 51540, 51684], "temperature": 0.0, "avg_logprob": -0.20201098244145232, "compression_ratio": 1.5132743362831858, "no_speech_prob": 2.026117044806597e-06}, {"id": 767, "seek": 479456, "start": 4794.56, "end": 4805.52, "text": " dh pre-bn and finally to get the b1 uh this is a addition uh and we saw that basically i need to", "tokens": [50364, 274, 71, 659, 12, 65, 77, 293, 2721, 281, 483, 264, 272, 16, 2232, 341, 307, 257, 4500, 2232, 293, 321, 1866, 300, 1936, 741, 643, 281, 50912, 50912, 445, 2408, 264, 4959, 294, 274, 71, 659, 12, 65, 77, 2051, 512, 10139, 293, 281, 652, 264, 12819, 589, 484, 741, 51224, 51224, 643, 281, 2408, 2051, 264, 44746, 900, 10298, 510, 281, 13819, 2232, 341, 10139, 293, 321, 360, 406, 1066, 5013, 82, 2232, 370, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.09703235626220703, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.340465582368779e-06}, {"id": 768, "seek": 479456, "start": 4805.52, "end": 4811.76, "text": " just sum the elements in dh pre-bn along some dimension and to make the dimensions work out i", "tokens": [50364, 274, 71, 659, 12, 65, 77, 293, 2721, 281, 483, 264, 272, 16, 2232, 341, 307, 257, 4500, 2232, 293, 321, 1866, 300, 1936, 741, 643, 281, 50912, 50912, 445, 2408, 264, 4959, 294, 274, 71, 659, 12, 65, 77, 2051, 512, 10139, 293, 281, 652, 264, 12819, 589, 484, 741, 51224, 51224, 643, 281, 2408, 2051, 264, 44746, 900, 10298, 510, 281, 13819, 2232, 341, 10139, 293, 321, 360, 406, 1066, 5013, 82, 2232, 370, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.09703235626220703, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.340465582368779e-06}, {"id": 769, "seek": 479456, "start": 4811.76, "end": 4819.68, "text": " need to sum along the zeroth axis here to eliminate uh this dimension and we do not keep dims uh so", "tokens": [50364, 274, 71, 659, 12, 65, 77, 293, 2721, 281, 483, 264, 272, 16, 2232, 341, 307, 257, 4500, 2232, 293, 321, 1866, 300, 1936, 741, 643, 281, 50912, 50912, 445, 2408, 264, 4959, 294, 274, 71, 659, 12, 65, 77, 2051, 512, 10139, 293, 281, 652, 264, 12819, 589, 484, 741, 51224, 51224, 643, 281, 2408, 2051, 264, 44746, 900, 10298, 510, 281, 13819, 2232, 341, 10139, 293, 321, 360, 406, 1066, 5013, 82, 2232, 370, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.09703235626220703, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.340465582368779e-06}, {"id": 770, "seek": 481968, "start": 4819.68, "end": 4825.6, "text": " that we want to just get a single one dimensional vector of 64 so these are the claimed derivatives", "tokens": [50364, 300, 321, 528, 281, 445, 483, 257, 2167, 472, 18795, 8062, 295, 12145, 370, 613, 366, 264, 12941, 33733, 50660, 50748, 718, 385, 829, 300, 510, 293, 718, 385, 8585, 518, 1045, 3876, 293, 3278, 527, 7350, 1203, 307, 869, 51116, 51144, 1392, 370, 321, 586, 2354, 1920, 456, 321, 362, 264, 13760, 295, 18648, 3857, 293, 321, 528, 281, 13760, 51408, 51408, 321, 528, 281, 646, 48256, 666, 18648, 370, 741, 797, 25365, 341, 1622, 670, 510, 2232, 370, 341, 307, 264, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.06726343878384294, "compression_ratio": 1.740909090909091, "no_speech_prob": 2.6841689759748988e-06}, {"id": 771, "seek": 481968, "start": 4827.360000000001, "end": 4834.72, "text": " let me put that here and let me uncomment three lines and cross our fingers everything is great", "tokens": [50364, 300, 321, 528, 281, 445, 483, 257, 2167, 472, 18795, 8062, 295, 12145, 370, 613, 366, 264, 12941, 33733, 50660, 50748, 718, 385, 829, 300, 510, 293, 718, 385, 8585, 518, 1045, 3876, 293, 3278, 527, 7350, 1203, 307, 869, 51116, 51144, 1392, 370, 321, 586, 2354, 1920, 456, 321, 362, 264, 13760, 295, 18648, 3857, 293, 321, 528, 281, 13760, 51408, 51408, 321, 528, 281, 646, 48256, 666, 18648, 370, 741, 797, 25365, 341, 1622, 670, 510, 2232, 370, 341, 307, 264, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.06726343878384294, "compression_ratio": 1.740909090909091, "no_speech_prob": 2.6841689759748988e-06}, {"id": 772, "seek": 481968, "start": 4835.280000000001, "end": 4840.56, "text": " okay so we now continue almost there we have the derivative of amp cat and we want to derivative", "tokens": [50364, 300, 321, 528, 281, 445, 483, 257, 2167, 472, 18795, 8062, 295, 12145, 370, 613, 366, 264, 12941, 33733, 50660, 50748, 718, 385, 829, 300, 510, 293, 718, 385, 8585, 518, 1045, 3876, 293, 3278, 527, 7350, 1203, 307, 869, 51116, 51144, 1392, 370, 321, 586, 2354, 1920, 456, 321, 362, 264, 13760, 295, 18648, 3857, 293, 321, 528, 281, 13760, 51408, 51408, 321, 528, 281, 646, 48256, 666, 18648, 370, 741, 797, 25365, 341, 1622, 670, 510, 2232, 370, 341, 307, 264, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.06726343878384294, "compression_ratio": 1.740909090909091, "no_speech_prob": 2.6841689759748988e-06}, {"id": 773, "seek": 481968, "start": 4840.56, "end": 4847.12, "text": " we want to back propagate into amp so i again copied this line over here uh so this is the", "tokens": [50364, 300, 321, 528, 281, 445, 483, 257, 2167, 472, 18795, 8062, 295, 12145, 370, 613, 366, 264, 12941, 33733, 50660, 50748, 718, 385, 829, 300, 510, 293, 718, 385, 8585, 518, 1045, 3876, 293, 3278, 527, 7350, 1203, 307, 869, 51116, 51144, 1392, 370, 321, 586, 2354, 1920, 456, 321, 362, 264, 13760, 295, 18648, 3857, 293, 321, 528, 281, 13760, 51408, 51408, 321, 528, 281, 646, 48256, 666, 18648, 370, 741, 797, 25365, 341, 1622, 670, 510, 2232, 370, 341, 307, 264, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.06726343878384294, "compression_ratio": 1.740909090909091, "no_speech_prob": 2.6841689759748988e-06}, {"id": 774, "seek": 484712, "start": 4847.12, "end": 4852.72, "text": " forward pass and then this is the shapes so remember that the shape here was 32 by 30", "tokens": [50364, 2128, 1320, 293, 550, 341, 307, 264, 10854, 370, 1604, 300, 264, 3909, 510, 390, 8858, 538, 2217, 50644, 50644, 293, 264, 3380, 3909, 295, 275, 390, 8858, 538, 805, 538, 1266, 370, 341, 4583, 294, 264, 2128, 1320, 382, 291, 9901, 300, 50940, 50940, 264, 1588, 7186, 399, 295, 613, 1045, 1266, 18795, 2517, 18875, 293, 370, 586, 321, 445, 528, 281, 51272, 51272, 23779, 300, 370, 341, 307, 767, 7226, 15325, 6916, 570, 2232, 264, 23897, 51568, 51568, 1320, 295, 264, 437, 307, 264, 1910, 1910, 307, 445, 257, 10290, 295, 264, 10225, 309, 311, 445, 257, 14978, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.06148761510848999, "compression_ratio": 1.8852459016393444, "no_speech_prob": 2.1567857402260415e-06}, {"id": 775, "seek": 484712, "start": 4852.72, "end": 4858.64, "text": " and the original shape of m was 32 by 3 by 10 so this layer in the forward pass as you recall that", "tokens": [50364, 2128, 1320, 293, 550, 341, 307, 264, 10854, 370, 1604, 300, 264, 3909, 510, 390, 8858, 538, 2217, 50644, 50644, 293, 264, 3380, 3909, 295, 275, 390, 8858, 538, 805, 538, 1266, 370, 341, 4583, 294, 264, 2128, 1320, 382, 291, 9901, 300, 50940, 50940, 264, 1588, 7186, 399, 295, 613, 1045, 1266, 18795, 2517, 18875, 293, 370, 586, 321, 445, 528, 281, 51272, 51272, 23779, 300, 370, 341, 307, 767, 7226, 15325, 6916, 570, 2232, 264, 23897, 51568, 51568, 1320, 295, 264, 437, 307, 264, 1910, 1910, 307, 445, 257, 10290, 295, 264, 10225, 309, 311, 445, 257, 14978, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.06148761510848999, "compression_ratio": 1.8852459016393444, "no_speech_prob": 2.1567857402260415e-06}, {"id": 776, "seek": 484712, "start": 4858.64, "end": 4865.28, "text": " the concatenation of these three 10 dimensional character vectors and so now we just want to", "tokens": [50364, 2128, 1320, 293, 550, 341, 307, 264, 10854, 370, 1604, 300, 264, 3909, 510, 390, 8858, 538, 2217, 50644, 50644, 293, 264, 3380, 3909, 295, 275, 390, 8858, 538, 805, 538, 1266, 370, 341, 4583, 294, 264, 2128, 1320, 382, 291, 9901, 300, 50940, 50940, 264, 1588, 7186, 399, 295, 613, 1045, 1266, 18795, 2517, 18875, 293, 370, 586, 321, 445, 528, 281, 51272, 51272, 23779, 300, 370, 341, 307, 767, 7226, 15325, 6916, 570, 2232, 264, 23897, 51568, 51568, 1320, 295, 264, 437, 307, 264, 1910, 1910, 307, 445, 257, 10290, 295, 264, 10225, 309, 311, 445, 257, 14978, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.06148761510848999, "compression_ratio": 1.8852459016393444, "no_speech_prob": 2.1567857402260415e-06}, {"id": 777, "seek": 484712, "start": 4865.28, "end": 4871.2, "text": " undo that so this is actually relatively straightforward operation because uh the backward", "tokens": [50364, 2128, 1320, 293, 550, 341, 307, 264, 10854, 370, 1604, 300, 264, 3909, 510, 390, 8858, 538, 2217, 50644, 50644, 293, 264, 3380, 3909, 295, 275, 390, 8858, 538, 805, 538, 1266, 370, 341, 4583, 294, 264, 2128, 1320, 382, 291, 9901, 300, 50940, 50940, 264, 1588, 7186, 399, 295, 613, 1045, 1266, 18795, 2517, 18875, 293, 370, 586, 321, 445, 528, 281, 51272, 51272, 23779, 300, 370, 341, 307, 767, 7226, 15325, 6916, 570, 2232, 264, 23897, 51568, 51568, 1320, 295, 264, 437, 307, 264, 1910, 1910, 307, 445, 257, 10290, 295, 264, 10225, 309, 311, 445, 257, 14978, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.06148761510848999, "compression_ratio": 1.8852459016393444, "no_speech_prob": 2.1567857402260415e-06}, {"id": 778, "seek": 484712, "start": 4871.2, "end": 4876.72, "text": " pass of the what is the view view is just a representation of the array it's just a logical", "tokens": [50364, 2128, 1320, 293, 550, 341, 307, 264, 10854, 370, 1604, 300, 264, 3909, 510, 390, 8858, 538, 2217, 50644, 50644, 293, 264, 3380, 3909, 295, 275, 390, 8858, 538, 805, 538, 1266, 370, 341, 4583, 294, 264, 2128, 1320, 382, 291, 9901, 300, 50940, 50940, 264, 1588, 7186, 399, 295, 613, 1045, 1266, 18795, 2517, 18875, 293, 370, 586, 321, 445, 528, 281, 51272, 51272, 23779, 300, 370, 341, 307, 767, 7226, 15325, 6916, 570, 2232, 264, 23897, 51568, 51568, 1320, 295, 264, 437, 307, 264, 1910, 1910, 307, 445, 257, 10290, 295, 264, 10225, 309, 311, 445, 257, 14978, 51844, 51844], "temperature": 0.0, "avg_logprob": -0.06148761510848999, "compression_ratio": 1.8852459016393444, "no_speech_prob": 2.1567857402260415e-06}, {"id": 779, "seek": 487672, "start": 4876.72, "end": 4882.16, "text": " form of how you interpret the array so let's just reinterpret it to be what it was before so in other", "tokens": [50364, 1254, 295, 577, 291, 7302, 264, 10225, 370, 718, 311, 445, 319, 41935, 309, 281, 312, 437, 309, 390, 949, 370, 294, 661, 50636, 50636, 2283, 264, 275, 307, 406, 2232, 8858, 538, 2217, 309, 307, 1936, 274, 76, 3857, 457, 498, 291, 1910, 309, 382, 1105, 264, 3380, 3909, 51208, 51240, 370, 445, 275, 5893, 3909, 2232, 291, 393, 291, 393, 1320, 512, 2604, 2622, 666, 1910, 293, 370, 341, 820, 445, 312, 1392, 51648, 51760], "temperature": 0.0, "avg_logprob": -0.10789279696307605, "compression_ratio": 1.6229508196721312, "no_speech_prob": 5.955032065685373e-06}, {"id": 780, "seek": 487672, "start": 4882.16, "end": 4893.6, "text": " words the m is not uh 32 by 30 it is basically dm cat but if you view it as um the original shape", "tokens": [50364, 1254, 295, 577, 291, 7302, 264, 10225, 370, 718, 311, 445, 319, 41935, 309, 281, 312, 437, 309, 390, 949, 370, 294, 661, 50636, 50636, 2283, 264, 275, 307, 406, 2232, 8858, 538, 2217, 309, 307, 1936, 274, 76, 3857, 457, 498, 291, 1910, 309, 382, 1105, 264, 3380, 3909, 51208, 51240, 370, 445, 275, 5893, 3909, 2232, 291, 393, 291, 393, 1320, 512, 2604, 2622, 666, 1910, 293, 370, 341, 820, 445, 312, 1392, 51648, 51760], "temperature": 0.0, "avg_logprob": -0.10789279696307605, "compression_ratio": 1.6229508196721312, "no_speech_prob": 5.955032065685373e-06}, {"id": 781, "seek": 487672, "start": 4894.240000000001, "end": 4902.400000000001, "text": " so just m dot shape uh you can you can pass some tuples into view and so this should just be okay", "tokens": [50364, 1254, 295, 577, 291, 7302, 264, 10225, 370, 718, 311, 445, 319, 41935, 309, 281, 312, 437, 309, 390, 949, 370, 294, 661, 50636, 50636, 2283, 264, 275, 307, 406, 2232, 8858, 538, 2217, 309, 307, 1936, 274, 76, 3857, 457, 498, 291, 1910, 309, 382, 1105, 264, 3380, 3909, 51208, 51240, 370, 445, 275, 5893, 3909, 2232, 291, 393, 291, 393, 1320, 512, 2604, 2622, 666, 1910, 293, 370, 341, 820, 445, 312, 1392, 51648, 51760], "temperature": 0.0, "avg_logprob": -0.10789279696307605, "compression_ratio": 1.6229508196721312, "no_speech_prob": 5.955032065685373e-06}, {"id": 782, "seek": 490240, "start": 4902.4, "end": 4909.5199999999995, "text": " we just re-represent that view and then we uncomment this line here and hopefully yeah so the", "tokens": [50364, 321, 445, 319, 12, 19919, 11662, 300, 1910, 293, 550, 321, 8585, 518, 341, 1622, 510, 293, 4696, 1338, 370, 264, 50720, 50720, 13760, 295, 275, 307, 3006, 370, 294, 341, 1389, 321, 445, 362, 281, 319, 12, 19919, 11662, 264, 3909, 295, 729, 33733, 51020, 51020, 666, 264, 3380, 1910, 370, 586, 321, 366, 412, 264, 2572, 1622, 293, 264, 787, 551, 300, 311, 1411, 281, 646, 51244, 51244, 48256, 807, 307, 341, 1105, 8186, 278, 6916, 510, 275, 307, 269, 412, 2031, 79, 370, 382, 741, 630, 949, 741, 5055, 1791, 292, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.14704334492586096, "compression_ratio": 1.768181818181818, "no_speech_prob": 2.2602948774874676e-06}, {"id": 783, "seek": 490240, "start": 4909.5199999999995, "end": 4915.5199999999995, "text": " derivative of m is correct so in this case we just have to re-represent the shape of those derivatives", "tokens": [50364, 321, 445, 319, 12, 19919, 11662, 300, 1910, 293, 550, 321, 8585, 518, 341, 1622, 510, 293, 4696, 1338, 370, 264, 50720, 50720, 13760, 295, 275, 307, 3006, 370, 294, 341, 1389, 321, 445, 362, 281, 319, 12, 19919, 11662, 264, 3909, 295, 729, 33733, 51020, 51020, 666, 264, 3380, 1910, 370, 586, 321, 366, 412, 264, 2572, 1622, 293, 264, 787, 551, 300, 311, 1411, 281, 646, 51244, 51244, 48256, 807, 307, 341, 1105, 8186, 278, 6916, 510, 275, 307, 269, 412, 2031, 79, 370, 382, 741, 630, 949, 741, 5055, 1791, 292, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.14704334492586096, "compression_ratio": 1.768181818181818, "no_speech_prob": 2.2602948774874676e-06}, {"id": 784, "seek": 490240, "start": 4915.5199999999995, "end": 4920.0, "text": " into the original view so now we are at the final line and the only thing that's left to back", "tokens": [50364, 321, 445, 319, 12, 19919, 11662, 300, 1910, 293, 550, 321, 8585, 518, 341, 1622, 510, 293, 4696, 1338, 370, 264, 50720, 50720, 13760, 295, 275, 307, 3006, 370, 294, 341, 1389, 321, 445, 362, 281, 319, 12, 19919, 11662, 264, 3909, 295, 729, 33733, 51020, 51020, 666, 264, 3380, 1910, 370, 586, 321, 366, 412, 264, 2572, 1622, 293, 264, 787, 551, 300, 311, 1411, 281, 646, 51244, 51244, 48256, 807, 307, 341, 1105, 8186, 278, 6916, 510, 275, 307, 269, 412, 2031, 79, 370, 382, 741, 630, 949, 741, 5055, 1791, 292, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.14704334492586096, "compression_ratio": 1.768181818181818, "no_speech_prob": 2.2602948774874676e-06}, {"id": 785, "seek": 490240, "start": 4920.0, "end": 4927.04, "text": " propagate through is this um indexing operation here m is c at xp so as i did before i copy pasted", "tokens": [50364, 321, 445, 319, 12, 19919, 11662, 300, 1910, 293, 550, 321, 8585, 518, 341, 1622, 510, 293, 4696, 1338, 370, 264, 50720, 50720, 13760, 295, 275, 307, 3006, 370, 294, 341, 1389, 321, 445, 362, 281, 319, 12, 19919, 11662, 264, 3909, 295, 729, 33733, 51020, 51020, 666, 264, 3380, 1910, 370, 586, 321, 366, 412, 264, 2572, 1622, 293, 264, 787, 551, 300, 311, 1411, 281, 646, 51244, 51244, 48256, 807, 307, 341, 1105, 8186, 278, 6916, 510, 275, 307, 269, 412, 2031, 79, 370, 382, 741, 630, 949, 741, 5055, 1791, 292, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.14704334492586096, "compression_ratio": 1.768181818181818, "no_speech_prob": 2.2602948774874676e-06}, {"id": 786, "seek": 492704, "start": 4927.04, "end": 4932.4, "text": " xp so as i did before i copy pasted this line here and let's look at the shapes of everything that's", "tokens": [50364, 2031, 79, 370, 382, 741, 630, 949, 741, 5055, 1791, 292, 341, 1622, 510, 293, 718, 311, 574, 412, 264, 10854, 295, 1203, 300, 311, 50632, 50632, 3288, 293, 4160, 4175, 577, 341, 2732, 370, 275, 5893, 3909, 390, 8858, 538, 805, 538, 1266, 370, 309, 311, 8858, 5110, 51080, 51080, 293, 550, 321, 362, 1045, 4342, 1184, 472, 295, 552, 575, 257, 1266, 12, 18759, 12240, 3584, 51292, 51352, 293, 341, 390, 11042, 538, 1940, 264, 574, 1010, 3199, 269, 597, 362, 7634, 1944, 4342, 1184, 295, 552, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.03964295438540879, "compression_ratio": 1.6478260869565218, "no_speech_prob": 2.902252390413196e-06}, {"id": 787, "seek": 492704, "start": 4932.4, "end": 4941.36, "text": " involved and remind ourselves how this worked so m dot shape was 32 by 3 by 10 so it's 32 examples", "tokens": [50364, 2031, 79, 370, 382, 741, 630, 949, 741, 5055, 1791, 292, 341, 1622, 510, 293, 718, 311, 574, 412, 264, 10854, 295, 1203, 300, 311, 50632, 50632, 3288, 293, 4160, 4175, 577, 341, 2732, 370, 275, 5893, 3909, 390, 8858, 538, 805, 538, 1266, 370, 309, 311, 8858, 5110, 51080, 51080, 293, 550, 321, 362, 1045, 4342, 1184, 472, 295, 552, 575, 257, 1266, 12, 18759, 12240, 3584, 51292, 51352, 293, 341, 390, 11042, 538, 1940, 264, 574, 1010, 3199, 269, 597, 362, 7634, 1944, 4342, 1184, 295, 552, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.03964295438540879, "compression_ratio": 1.6478260869565218, "no_speech_prob": 2.902252390413196e-06}, {"id": 788, "seek": 492704, "start": 4941.36, "end": 4945.6, "text": " and then we have three characters each one of them has a 10-dimensional embedding", "tokens": [50364, 2031, 79, 370, 382, 741, 630, 949, 741, 5055, 1791, 292, 341, 1622, 510, 293, 718, 311, 574, 412, 264, 10854, 295, 1203, 300, 311, 50632, 50632, 3288, 293, 4160, 4175, 577, 341, 2732, 370, 275, 5893, 3909, 390, 8858, 538, 805, 538, 1266, 370, 309, 311, 8858, 5110, 51080, 51080, 293, 550, 321, 362, 1045, 4342, 1184, 472, 295, 552, 575, 257, 1266, 12, 18759, 12240, 3584, 51292, 51352, 293, 341, 390, 11042, 538, 1940, 264, 574, 1010, 3199, 269, 597, 362, 7634, 1944, 4342, 1184, 295, 552, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.03964295438540879, "compression_ratio": 1.6478260869565218, "no_speech_prob": 2.902252390413196e-06}, {"id": 789, "seek": 492704, "start": 4946.8, "end": 4953.2, "text": " and this was achieved by taking the lookup table c which have 27 possible characters each of them", "tokens": [50364, 2031, 79, 370, 382, 741, 630, 949, 741, 5055, 1791, 292, 341, 1622, 510, 293, 718, 311, 574, 412, 264, 10854, 295, 1203, 300, 311, 50632, 50632, 3288, 293, 4160, 4175, 577, 341, 2732, 370, 275, 5893, 3909, 390, 8858, 538, 805, 538, 1266, 370, 309, 311, 8858, 5110, 51080, 51080, 293, 550, 321, 362, 1045, 4342, 1184, 472, 295, 552, 575, 257, 1266, 12, 18759, 12240, 3584, 51292, 51352, 293, 341, 390, 11042, 538, 1940, 264, 574, 1010, 3199, 269, 597, 362, 7634, 1944, 4342, 1184, 295, 552, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.03964295438540879, "compression_ratio": 1.6478260869565218, "no_speech_prob": 2.902252390413196e-06}, {"id": 790, "seek": 495320, "start": 4953.2, "end": 4960.16, "text": " 10-dimensional and we looked up um you at the rows that were specified inside this tensor xb", "tokens": [50364, 1266, 12, 18759, 293, 321, 2956, 493, 1105, 291, 412, 264, 13241, 300, 645, 22206, 1854, 341, 40863, 2031, 65, 50712, 50760, 370, 2031, 65, 307, 8858, 538, 805, 293, 309, 311, 1936, 2902, 505, 337, 1184, 1365, 264, 6575, 420, 264, 8186, 51048, 51048, 295, 597, 2517, 2232, 307, 644, 295, 300, 1365, 293, 370, 510, 741, 478, 4099, 264, 700, 1732, 13241, 295, 51404, 51404, 1045, 295, 341, 1105, 40863, 2031, 65, 293, 370, 321, 393, 536, 300, 337, 1365, 510, 309, 390, 264, 700, 1365, 294, 341, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.06041101699179791, "compression_ratio": 1.7155963302752293, "no_speech_prob": 4.936872301186668e-06}, {"id": 791, "seek": 495320, "start": 4961.12, "end": 4966.88, "text": " so xb is 32 by 3 and it's basically giving us for each example the identity or the index", "tokens": [50364, 1266, 12, 18759, 293, 321, 2956, 493, 1105, 291, 412, 264, 13241, 300, 645, 22206, 1854, 341, 40863, 2031, 65, 50712, 50760, 370, 2031, 65, 307, 8858, 538, 805, 293, 309, 311, 1936, 2902, 505, 337, 1184, 1365, 264, 6575, 420, 264, 8186, 51048, 51048, 295, 597, 2517, 2232, 307, 644, 295, 300, 1365, 293, 370, 510, 741, 478, 4099, 264, 700, 1732, 13241, 295, 51404, 51404, 1045, 295, 341, 1105, 40863, 2031, 65, 293, 370, 321, 393, 536, 300, 337, 1365, 510, 309, 390, 264, 700, 1365, 294, 341, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.06041101699179791, "compression_ratio": 1.7155963302752293, "no_speech_prob": 4.936872301186668e-06}, {"id": 792, "seek": 495320, "start": 4966.88, "end": 4974.0, "text": " of which character uh is part of that example and so here i'm showing the first five rows of", "tokens": [50364, 1266, 12, 18759, 293, 321, 2956, 493, 1105, 291, 412, 264, 13241, 300, 645, 22206, 1854, 341, 40863, 2031, 65, 50712, 50760, 370, 2031, 65, 307, 8858, 538, 805, 293, 309, 311, 1936, 2902, 505, 337, 1184, 1365, 264, 6575, 420, 264, 8186, 51048, 51048, 295, 597, 2517, 2232, 307, 644, 295, 300, 1365, 293, 370, 510, 741, 478, 4099, 264, 700, 1732, 13241, 295, 51404, 51404, 1045, 295, 341, 1105, 40863, 2031, 65, 293, 370, 321, 393, 536, 300, 337, 1365, 510, 309, 390, 264, 700, 1365, 294, 341, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.06041101699179791, "compression_ratio": 1.7155963302752293, "no_speech_prob": 4.936872301186668e-06}, {"id": 793, "seek": 495320, "start": 4974.0, "end": 4980.32, "text": " three of this um tensor xb and so we can see that for example here it was the first example in this", "tokens": [50364, 1266, 12, 18759, 293, 321, 2956, 493, 1105, 291, 412, 264, 13241, 300, 645, 22206, 1854, 341, 40863, 2031, 65, 50712, 50760, 370, 2031, 65, 307, 8858, 538, 805, 293, 309, 311, 1936, 2902, 505, 337, 1184, 1365, 264, 6575, 420, 264, 8186, 51048, 51048, 295, 597, 2517, 2232, 307, 644, 295, 300, 1365, 293, 370, 510, 741, 478, 4099, 264, 700, 1732, 13241, 295, 51404, 51404, 1045, 295, 341, 1105, 40863, 2031, 65, 293, 370, 321, 393, 536, 300, 337, 1365, 510, 309, 390, 264, 700, 1365, 294, 341, 51720, 51720], "temperature": 0.0, "avg_logprob": -0.06041101699179791, "compression_ratio": 1.7155963302752293, "no_speech_prob": 4.936872301186668e-06}, {"id": 794, "seek": 498032, "start": 4980.32, "end": 4985.04, "text": " batch is that the first character and the first character and the fourth character comes into the", "tokens": [50364, 15245, 307, 300, 264, 700, 2517, 293, 264, 700, 2517, 293, 264, 6409, 2517, 1487, 666, 264, 50600, 50600, 18161, 2533, 293, 550, 321, 528, 281, 6069, 264, 958, 2517, 294, 257, 8310, 934, 264, 2517, 307, 2975, 19, 50896, 50964, 370, 1936, 437, 311, 2737, 510, 307, 456, 366, 41674, 1854, 2031, 65, 293, 1184, 472, 295, 613, 51260, 51260, 41674, 307, 1608, 5489, 597, 5386, 295, 269, 321, 528, 281, 41514, 484, 558, 293, 550, 321, 9424, 729, 13241, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.053584715899299175, "compression_ratio": 1.9242424242424243, "no_speech_prob": 4.425358838489046e-06}, {"id": 795, "seek": 498032, "start": 4985.04, "end": 4990.96, "text": " neural net and then we want to predict the next character in a sequence after the character is 114", "tokens": [50364, 15245, 307, 300, 264, 700, 2517, 293, 264, 700, 2517, 293, 264, 6409, 2517, 1487, 666, 264, 50600, 50600, 18161, 2533, 293, 550, 321, 528, 281, 6069, 264, 958, 2517, 294, 257, 8310, 934, 264, 2517, 307, 2975, 19, 50896, 50964, 370, 1936, 437, 311, 2737, 510, 307, 456, 366, 41674, 1854, 2031, 65, 293, 1184, 472, 295, 613, 51260, 51260, 41674, 307, 1608, 5489, 597, 5386, 295, 269, 321, 528, 281, 41514, 484, 558, 293, 550, 321, 9424, 729, 13241, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.053584715899299175, "compression_ratio": 1.9242424242424243, "no_speech_prob": 4.425358838489046e-06}, {"id": 796, "seek": 498032, "start": 4992.32, "end": 4998.24, "text": " so basically what's happening here is there are integers inside xb and each one of these", "tokens": [50364, 15245, 307, 300, 264, 700, 2517, 293, 264, 700, 2517, 293, 264, 6409, 2517, 1487, 666, 264, 50600, 50600, 18161, 2533, 293, 550, 321, 528, 281, 6069, 264, 958, 2517, 294, 257, 8310, 934, 264, 2517, 307, 2975, 19, 50896, 50964, 370, 1936, 437, 311, 2737, 510, 307, 456, 366, 41674, 1854, 2031, 65, 293, 1184, 472, 295, 613, 51260, 51260, 41674, 307, 1608, 5489, 597, 5386, 295, 269, 321, 528, 281, 41514, 484, 558, 293, 550, 321, 9424, 729, 13241, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.053584715899299175, "compression_ratio": 1.9242424242424243, "no_speech_prob": 4.425358838489046e-06}, {"id": 797, "seek": 498032, "start": 4998.24, "end": 5005.44, "text": " integers is specifying which row of c we want to pluck out right and then we arrange those rows", "tokens": [50364, 15245, 307, 300, 264, 700, 2517, 293, 264, 700, 2517, 293, 264, 6409, 2517, 1487, 666, 264, 50600, 50600, 18161, 2533, 293, 550, 321, 528, 281, 6069, 264, 958, 2517, 294, 257, 8310, 934, 264, 2517, 307, 2975, 19, 50896, 50964, 370, 1936, 437, 311, 2737, 510, 307, 456, 366, 41674, 1854, 2031, 65, 293, 1184, 472, 295, 613, 51260, 51260, 41674, 307, 1608, 5489, 597, 5386, 295, 269, 321, 528, 281, 41514, 484, 558, 293, 550, 321, 9424, 729, 13241, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.053584715899299175, "compression_ratio": 1.9242424242424243, "no_speech_prob": 4.425358838489046e-06}, {"id": 798, "seek": 500544, "start": 5005.44, "end": 5011.679999999999, "text": " that we've plucked out into 32 by 3 by 10 tensor we just package them in we just package them into", "tokens": [50364, 300, 321, 600, 41514, 292, 484, 666, 8858, 538, 805, 538, 1266, 40863, 321, 445, 7372, 552, 294, 321, 445, 7372, 552, 666, 50676, 50708, 264, 10200, 293, 586, 437, 311, 2737, 307, 300, 321, 362, 274, 8814, 370, 337, 633, 472, 295, 613, 2232, 1936, 51040, 51040, 41514, 292, 484, 13241, 321, 362, 641, 2771, 2448, 586, 457, 436, 434, 18721, 1854, 341, 8858, 538, 805, 538, 1266, 40863, 51392, 51424, 370, 439, 321, 362, 281, 360, 586, 307, 321, 445, 643, 281, 7955, 341, 16235, 12204, 807, 341, 15187, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.07572601217972605, "compression_ratio": 1.835680751173709, "no_speech_prob": 3.4464972031855723e-06}, {"id": 799, "seek": 500544, "start": 5012.32, "end": 5018.96, "text": " the sensor and now what's happening is that we have dimp so for every one of these uh basically", "tokens": [50364, 300, 321, 600, 41514, 292, 484, 666, 8858, 538, 805, 538, 1266, 40863, 321, 445, 7372, 552, 294, 321, 445, 7372, 552, 666, 50676, 50708, 264, 10200, 293, 586, 437, 311, 2737, 307, 300, 321, 362, 274, 8814, 370, 337, 633, 472, 295, 613, 2232, 1936, 51040, 51040, 41514, 292, 484, 13241, 321, 362, 641, 2771, 2448, 586, 457, 436, 434, 18721, 1854, 341, 8858, 538, 805, 538, 1266, 40863, 51392, 51424, 370, 439, 321, 362, 281, 360, 586, 307, 321, 445, 643, 281, 7955, 341, 16235, 12204, 807, 341, 15187, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.07572601217972605, "compression_ratio": 1.835680751173709, "no_speech_prob": 3.4464972031855723e-06}, {"id": 800, "seek": 500544, "start": 5018.96, "end": 5026.0, "text": " plucked out rows we have their gradients now but they're arranged inside this 32 by 3 by 10 tensor", "tokens": [50364, 300, 321, 600, 41514, 292, 484, 666, 8858, 538, 805, 538, 1266, 40863, 321, 445, 7372, 552, 294, 321, 445, 7372, 552, 666, 50676, 50708, 264, 10200, 293, 586, 437, 311, 2737, 307, 300, 321, 362, 274, 8814, 370, 337, 633, 472, 295, 613, 2232, 1936, 51040, 51040, 41514, 292, 484, 13241, 321, 362, 641, 2771, 2448, 586, 457, 436, 434, 18721, 1854, 341, 8858, 538, 805, 538, 1266, 40863, 51392, 51424, 370, 439, 321, 362, 281, 360, 586, 307, 321, 445, 643, 281, 7955, 341, 16235, 12204, 807, 341, 15187, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.07572601217972605, "compression_ratio": 1.835680751173709, "no_speech_prob": 3.4464972031855723e-06}, {"id": 801, "seek": 500544, "start": 5026.639999999999, "end": 5031.759999999999, "text": " so all we have to do now is we just need to route this gradient backwards through this assignment", "tokens": [50364, 300, 321, 600, 41514, 292, 484, 666, 8858, 538, 805, 538, 1266, 40863, 321, 445, 7372, 552, 294, 321, 445, 7372, 552, 666, 50676, 50708, 264, 10200, 293, 586, 437, 311, 2737, 307, 300, 321, 362, 274, 8814, 370, 337, 633, 472, 295, 613, 2232, 1936, 51040, 51040, 41514, 292, 484, 13241, 321, 362, 641, 2771, 2448, 586, 457, 436, 434, 18721, 1854, 341, 8858, 538, 805, 538, 1266, 40863, 51392, 51424, 370, 439, 321, 362, 281, 360, 586, 307, 321, 445, 643, 281, 7955, 341, 16235, 12204, 807, 341, 15187, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.07572601217972605, "compression_ratio": 1.835680751173709, "no_speech_prob": 3.4464972031855723e-06}, {"id": 802, "seek": 503176, "start": 5031.76, "end": 5038.16, "text": " so we need to find which row of c that every one of these um 10 dimensional embeddings come from", "tokens": [50364, 370, 321, 643, 281, 915, 597, 5386, 295, 269, 300, 633, 472, 295, 613, 1105, 1266, 18795, 12240, 29432, 808, 490, 50684, 50732, 293, 550, 321, 643, 281, 19107, 552, 666, 274, 66, 370, 321, 445, 643, 281, 23779, 264, 8186, 278, 293, 295, 1164, 498, 51100, 51100, 604, 295, 613, 13241, 295, 269, 390, 1143, 3866, 1413, 597, 1920, 3297, 307, 264, 1389, 411, 264, 5386, 51352, 51352, 472, 293, 472, 390, 1143, 3866, 1413, 550, 321, 362, 281, 1604, 300, 264, 2771, 2448, 300, 8881, 456, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.04648648137631624, "compression_ratio": 1.8113207547169812, "no_speech_prob": 1.482317884438089e-06}, {"id": 803, "seek": 503176, "start": 5039.12, "end": 5046.4800000000005, "text": " and then we need to deposit them into dc so we just need to undo the indexing and of course if", "tokens": [50364, 370, 321, 643, 281, 915, 597, 5386, 295, 269, 300, 633, 472, 295, 613, 1105, 1266, 18795, 12240, 29432, 808, 490, 50684, 50732, 293, 550, 321, 643, 281, 19107, 552, 666, 274, 66, 370, 321, 445, 643, 281, 23779, 264, 8186, 278, 293, 295, 1164, 498, 51100, 51100, 604, 295, 613, 13241, 295, 269, 390, 1143, 3866, 1413, 597, 1920, 3297, 307, 264, 1389, 411, 264, 5386, 51352, 51352, 472, 293, 472, 390, 1143, 3866, 1413, 550, 321, 362, 281, 1604, 300, 264, 2771, 2448, 300, 8881, 456, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.04648648137631624, "compression_ratio": 1.8113207547169812, "no_speech_prob": 1.482317884438089e-06}, {"id": 804, "seek": 503176, "start": 5046.4800000000005, "end": 5051.52, "text": " any of these rows of c was used multiple times which almost certainly is the case like the row", "tokens": [50364, 370, 321, 643, 281, 915, 597, 5386, 295, 269, 300, 633, 472, 295, 613, 1105, 1266, 18795, 12240, 29432, 808, 490, 50684, 50732, 293, 550, 321, 643, 281, 19107, 552, 666, 274, 66, 370, 321, 445, 643, 281, 23779, 264, 8186, 278, 293, 295, 1164, 498, 51100, 51100, 604, 295, 613, 13241, 295, 269, 390, 1143, 3866, 1413, 597, 1920, 3297, 307, 264, 1389, 411, 264, 5386, 51352, 51352, 472, 293, 472, 390, 1143, 3866, 1413, 550, 321, 362, 281, 1604, 300, 264, 2771, 2448, 300, 8881, 456, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.04648648137631624, "compression_ratio": 1.8113207547169812, "no_speech_prob": 1.482317884438089e-06}, {"id": 805, "seek": 503176, "start": 5051.52, "end": 5056.16, "text": " one and one was used multiple times then we have to remember that the gradients that arrive there", "tokens": [50364, 370, 321, 643, 281, 915, 597, 5386, 295, 269, 300, 633, 472, 295, 613, 1105, 1266, 18795, 12240, 29432, 808, 490, 50684, 50732, 293, 550, 321, 643, 281, 19107, 552, 666, 274, 66, 370, 321, 445, 643, 281, 23779, 264, 8186, 278, 293, 295, 1164, 498, 51100, 51100, 604, 295, 613, 13241, 295, 269, 390, 1143, 3866, 1413, 597, 1920, 3297, 307, 264, 1389, 411, 264, 5386, 51352, 51352, 472, 293, 472, 390, 1143, 3866, 1413, 550, 321, 362, 281, 1604, 300, 264, 2771, 2448, 300, 8881, 456, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.04648648137631624, "compression_ratio": 1.8113207547169812, "no_speech_prob": 1.482317884438089e-06}, {"id": 806, "seek": 505616, "start": 5056.16, "end": 5063.28, "text": " have to add so for each occurrence we have to have an addition so let's now write this out and i don't", "tokens": [50364, 362, 281, 909, 370, 337, 1184, 36122, 321, 362, 281, 362, 364, 4500, 370, 718, 311, 586, 2464, 341, 484, 293, 741, 500, 380, 50720, 50720, 767, 458, 295, 411, 257, 709, 1101, 636, 281, 360, 341, 813, 257, 337, 6367, 7015, 294, 38797, 50904, 50952, 370, 1310, 1580, 393, 808, 493, 365, 257, 8062, 1602, 7148, 6916, 457, 337, 586, 718, 311, 445, 764, 51204, 51204, 337, 16121, 370, 718, 385, 1884, 3930, 82, 313, 13, 4527, 329, 411, 269, 281, 5883, 1125, 445, 257, 7634, 538, 1266, 40863, 295, 439, 35193, 51684, 51716], "temperature": 0.0, "avg_logprob": -0.09705581861672942, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.936830009683035e-06}, {"id": 807, "seek": 505616, "start": 5063.28, "end": 5066.96, "text": " actually know of like a much better way to do this than a for loop unfortunately in python", "tokens": [50364, 362, 281, 909, 370, 337, 1184, 36122, 321, 362, 281, 362, 364, 4500, 370, 718, 311, 586, 2464, 341, 484, 293, 741, 500, 380, 50720, 50720, 767, 458, 295, 411, 257, 709, 1101, 636, 281, 360, 341, 813, 257, 337, 6367, 7015, 294, 38797, 50904, 50952, 370, 1310, 1580, 393, 808, 493, 365, 257, 8062, 1602, 7148, 6916, 457, 337, 586, 718, 311, 445, 764, 51204, 51204, 337, 16121, 370, 718, 385, 1884, 3930, 82, 313, 13, 4527, 329, 411, 269, 281, 5883, 1125, 445, 257, 7634, 538, 1266, 40863, 295, 439, 35193, 51684, 51716], "temperature": 0.0, "avg_logprob": -0.09705581861672942, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.936830009683035e-06}, {"id": 808, "seek": 505616, "start": 5067.92, "end": 5072.96, "text": " so maybe someone can come up with a vectorized efficient operation but for now let's just use", "tokens": [50364, 362, 281, 909, 370, 337, 1184, 36122, 321, 362, 281, 362, 364, 4500, 370, 718, 311, 586, 2464, 341, 484, 293, 741, 500, 380, 50720, 50720, 767, 458, 295, 411, 257, 709, 1101, 636, 281, 360, 341, 813, 257, 337, 6367, 7015, 294, 38797, 50904, 50952, 370, 1310, 1580, 393, 808, 493, 365, 257, 8062, 1602, 7148, 6916, 457, 337, 586, 718, 311, 445, 764, 51204, 51204, 337, 16121, 370, 718, 385, 1884, 3930, 82, 313, 13, 4527, 329, 411, 269, 281, 5883, 1125, 445, 257, 7634, 538, 1266, 40863, 295, 439, 35193, 51684, 51716], "temperature": 0.0, "avg_logprob": -0.09705581861672942, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.936830009683035e-06}, {"id": 809, "seek": 505616, "start": 5072.96, "end": 5082.5599999999995, "text": " for loops so let me create torsion.zeros like c to initialize just a 27 by 10 tensor of all zeros", "tokens": [50364, 362, 281, 909, 370, 337, 1184, 36122, 321, 362, 281, 362, 364, 4500, 370, 718, 311, 586, 2464, 341, 484, 293, 741, 500, 380, 50720, 50720, 767, 458, 295, 411, 257, 709, 1101, 636, 281, 360, 341, 813, 257, 337, 6367, 7015, 294, 38797, 50904, 50952, 370, 1310, 1580, 393, 808, 493, 365, 257, 8062, 1602, 7148, 6916, 457, 337, 586, 718, 311, 445, 764, 51204, 51204, 337, 16121, 370, 718, 385, 1884, 3930, 82, 313, 13, 4527, 329, 411, 269, 281, 5883, 1125, 445, 257, 7634, 538, 1266, 40863, 295, 439, 35193, 51684, 51716], "temperature": 0.0, "avg_logprob": -0.09705581861672942, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.936830009683035e-06}, {"id": 810, "seek": 508256, "start": 5082.56, "end": 5086.96, "text": " and then honestly for k in range xb.shape at zero", "tokens": [50364, 293, 550, 6095, 337, 350, 294, 3613, 2031, 65, 13, 82, 42406, 412, 4018, 50584, 50684, 1310, 1580, 575, 257, 1101, 636, 281, 360, 341, 457, 337, 361, 294, 3613, 2031, 65, 13, 82, 42406, 412, 472, 50928, 50996, 341, 307, 516, 281, 44497, 670, 439, 264, 1105, 439, 264, 4959, 295, 2031, 65, 439, 613, 41674, 51312, 51372, 293, 550, 718, 311, 483, 264, 8186, 412, 341, 2535, 370, 264, 8186, 307, 1936, 2031, 65, 412, 350, 73, 51720, 51788], "temperature": 0.0, "avg_logprob": -0.16975560245743718, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.6119331525696907e-06}, {"id": 811, "seek": 508256, "start": 5088.96, "end": 5093.84, "text": " maybe someone has a better way to do this but for j in range xb.shape at one", "tokens": [50364, 293, 550, 6095, 337, 350, 294, 3613, 2031, 65, 13, 82, 42406, 412, 4018, 50584, 50684, 1310, 1580, 575, 257, 1101, 636, 281, 360, 341, 457, 337, 361, 294, 3613, 2031, 65, 13, 82, 42406, 412, 472, 50928, 50996, 341, 307, 516, 281, 44497, 670, 439, 264, 1105, 439, 264, 4959, 295, 2031, 65, 439, 613, 41674, 51312, 51372, 293, 550, 718, 311, 483, 264, 8186, 412, 341, 2535, 370, 264, 8186, 307, 1936, 2031, 65, 412, 350, 73, 51720, 51788], "temperature": 0.0, "avg_logprob": -0.16975560245743718, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.6119331525696907e-06}, {"id": 812, "seek": 508256, "start": 5095.200000000001, "end": 5101.52, "text": " this is going to iterate over all the um all the elements of xb all these integers", "tokens": [50364, 293, 550, 6095, 337, 350, 294, 3613, 2031, 65, 13, 82, 42406, 412, 4018, 50584, 50684, 1310, 1580, 575, 257, 1101, 636, 281, 360, 341, 457, 337, 361, 294, 3613, 2031, 65, 13, 82, 42406, 412, 472, 50928, 50996, 341, 307, 516, 281, 44497, 670, 439, 264, 1105, 439, 264, 4959, 295, 2031, 65, 439, 613, 41674, 51312, 51372, 293, 550, 718, 311, 483, 264, 8186, 412, 341, 2535, 370, 264, 8186, 307, 1936, 2031, 65, 412, 350, 73, 51720, 51788], "temperature": 0.0, "avg_logprob": -0.16975560245743718, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.6119331525696907e-06}, {"id": 813, "seek": 508256, "start": 5102.72, "end": 5109.68, "text": " and then let's get the index at this position so the index is basically xb at kj", "tokens": [50364, 293, 550, 6095, 337, 350, 294, 3613, 2031, 65, 13, 82, 42406, 412, 4018, 50584, 50684, 1310, 1580, 575, 257, 1101, 636, 281, 360, 341, 457, 337, 361, 294, 3613, 2031, 65, 13, 82, 42406, 412, 472, 50928, 50996, 341, 307, 516, 281, 44497, 670, 439, 264, 1105, 439, 264, 4959, 295, 2031, 65, 439, 613, 41674, 51312, 51372, 293, 550, 718, 311, 483, 264, 8186, 412, 341, 2535, 370, 264, 8186, 307, 1936, 2031, 65, 412, 350, 73, 51720, 51788], "temperature": 0.0, "avg_logprob": -0.16975560245743718, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.6119331525696907e-06}, {"id": 814, "seek": 510968, "start": 5109.68, "end": 5118.56, "text": " kj so that an example of that like is 11 or 14 and so on and now in the forward pass we took", "tokens": [50364, 350, 73, 370, 300, 364, 1365, 295, 300, 411, 307, 2975, 420, 3499, 293, 370, 322, 293, 586, 294, 264, 2128, 1320, 321, 1890, 50808, 50864, 321, 1936, 1890, 1105, 264, 5386, 295, 269, 412, 8186, 293, 321, 42002, 309, 666, 275, 412, 350, 73, 300, 311, 437, 2011, 51468, 51468, 300, 311, 689, 436, 366, 38162, 370, 586, 321, 643, 281, 352, 12204, 293, 321, 445, 643, 281, 7955, 51652, 51728], "temperature": 0.0, "avg_logprob": -0.0939797388540732, "compression_ratio": 1.6488095238095237, "no_speech_prob": 3.237634246033849e-06}, {"id": 815, "seek": 510968, "start": 5119.68, "end": 5131.76, "text": " we basically took um the row of c at index and we deposited it into m at kj that's what happened", "tokens": [50364, 350, 73, 370, 300, 364, 1365, 295, 300, 411, 307, 2975, 420, 3499, 293, 370, 322, 293, 586, 294, 264, 2128, 1320, 321, 1890, 50808, 50864, 321, 1936, 1890, 1105, 264, 5386, 295, 269, 412, 8186, 293, 321, 42002, 309, 666, 275, 412, 350, 73, 300, 311, 437, 2011, 51468, 51468, 300, 311, 689, 436, 366, 38162, 370, 586, 321, 643, 281, 352, 12204, 293, 321, 445, 643, 281, 7955, 51652, 51728], "temperature": 0.0, "avg_logprob": -0.0939797388540732, "compression_ratio": 1.6488095238095237, "no_speech_prob": 3.237634246033849e-06}, {"id": 816, "seek": 510968, "start": 5131.76, "end": 5135.4400000000005, "text": " that's where they are packaged so now we need to go backwards and we just need to route", "tokens": [50364, 350, 73, 370, 300, 364, 1365, 295, 300, 411, 307, 2975, 420, 3499, 293, 370, 322, 293, 586, 294, 264, 2128, 1320, 321, 1890, 50808, 50864, 321, 1936, 1890, 1105, 264, 5386, 295, 269, 412, 8186, 293, 321, 42002, 309, 666, 275, 412, 350, 73, 300, 311, 437, 2011, 51468, 51468, 300, 311, 689, 436, 366, 38162, 370, 586, 321, 643, 281, 352, 12204, 293, 321, 445, 643, 281, 7955, 51652, 51728], "temperature": 0.0, "avg_logprob": -0.0939797388540732, "compression_ratio": 1.6488095238095237, "no_speech_prob": 3.237634246033849e-06}, {"id": 817, "seek": 513544, "start": 5135.44, "end": 5144.08, "text": " uh dm at the position kj we now have these derivatives for each position and it's 10", "tokens": [50364, 2232, 274, 76, 412, 264, 2535, 350, 73, 321, 586, 362, 613, 33733, 337, 1184, 2535, 293, 309, 311, 1266, 50796, 50796, 18795, 293, 291, 445, 643, 281, 352, 666, 264, 3006, 5386, 295, 269, 370, 274, 66, 2831, 412, 741, 87, 307, 341, 51228, 51228, 457, 1804, 6915, 570, 456, 727, 312, 3866, 5160, 38983, 2232, 411, 264, 912, 5386, 727, 362, 668, 51492, 51492, 1143, 867, 867, 1413, 293, 370, 439, 295, 729, 33733, 486, 445, 2232, 352, 12204, 807, 264, 8186, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.15502932938662442, "compression_ratio": 1.6681818181818182, "no_speech_prob": 2.6015698040282587e-06}, {"id": 818, "seek": 513544, "start": 5144.08, "end": 5152.719999999999, "text": " dimensional and you just need to go into the correct row of c so dc rather at ix is this", "tokens": [50364, 2232, 274, 76, 412, 264, 2535, 350, 73, 321, 586, 362, 613, 33733, 337, 1184, 2535, 293, 309, 311, 1266, 50796, 50796, 18795, 293, 291, 445, 643, 281, 352, 666, 264, 3006, 5386, 295, 269, 370, 274, 66, 2831, 412, 741, 87, 307, 341, 51228, 51228, 457, 1804, 6915, 570, 456, 727, 312, 3866, 5160, 38983, 2232, 411, 264, 912, 5386, 727, 362, 668, 51492, 51492, 1143, 867, 867, 1413, 293, 370, 439, 295, 729, 33733, 486, 445, 2232, 352, 12204, 807, 264, 8186, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.15502932938662442, "compression_ratio": 1.6681818181818182, "no_speech_prob": 2.6015698040282587e-06}, {"id": 819, "seek": 513544, "start": 5152.719999999999, "end": 5158.0, "text": " but plus equals because there could be multiple occurrences uh like the same row could have been", "tokens": [50364, 2232, 274, 76, 412, 264, 2535, 350, 73, 321, 586, 362, 613, 33733, 337, 1184, 2535, 293, 309, 311, 1266, 50796, 50796, 18795, 293, 291, 445, 643, 281, 352, 666, 264, 3006, 5386, 295, 269, 370, 274, 66, 2831, 412, 741, 87, 307, 341, 51228, 51228, 457, 1804, 6915, 570, 456, 727, 312, 3866, 5160, 38983, 2232, 411, 264, 912, 5386, 727, 362, 668, 51492, 51492, 1143, 867, 867, 1413, 293, 370, 439, 295, 729, 33733, 486, 445, 2232, 352, 12204, 807, 264, 8186, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.15502932938662442, "compression_ratio": 1.6681818181818182, "no_speech_prob": 2.6015698040282587e-06}, {"id": 820, "seek": 513544, "start": 5158.0, "end": 5164.639999999999, "text": " used many many times and so all of those derivatives will just uh go backwards through the index", "tokens": [50364, 2232, 274, 76, 412, 264, 2535, 350, 73, 321, 586, 362, 613, 33733, 337, 1184, 2535, 293, 309, 311, 1266, 50796, 50796, 18795, 293, 291, 445, 643, 281, 352, 666, 264, 3006, 5386, 295, 269, 370, 274, 66, 2831, 412, 741, 87, 307, 341, 51228, 51228, 457, 1804, 6915, 570, 456, 727, 312, 3866, 5160, 38983, 2232, 411, 264, 912, 5386, 727, 362, 668, 51492, 51492, 1143, 867, 867, 1413, 293, 370, 439, 295, 729, 33733, 486, 445, 2232, 352, 12204, 807, 264, 8186, 51824, 51824], "temperature": 0.0, "avg_logprob": -0.15502932938662442, "compression_ratio": 1.6681818181818182, "no_speech_prob": 2.6015698040282587e-06}, {"id": 821, "seek": 516464, "start": 5164.64, "end": 5173.6, "text": " in and they will add so this is my candidate solution let's copy it here", "tokens": [50364, 294, 293, 436, 486, 909, 370, 341, 307, 452, 11532, 3827, 718, 311, 5055, 309, 510, 50812, 50920, 718, 311, 8585, 518, 341, 293, 3278, 527, 7350, 4177, 370, 300, 311, 309, 321, 600, 646, 12425, 770, 807, 51336, 51380, 341, 2302, 13464, 370, 456, 321, 352, 3879, 1669, 2020, 370, 586, 321, 808, 281, 5380, 732, 309, 1936, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.19142166016593812, "compression_ratio": 1.5696969696969696, "no_speech_prob": 8.059362812673498e-07}, {"id": 822, "seek": 516464, "start": 5175.76, "end": 5184.08, "text": " let's uncomment this and cross our fingers hey so that's it we've back propagated through", "tokens": [50364, 294, 293, 436, 486, 909, 370, 341, 307, 452, 11532, 3827, 718, 311, 5055, 309, 510, 50812, 50920, 718, 311, 8585, 518, 341, 293, 3278, 527, 7350, 4177, 370, 300, 311, 309, 321, 600, 646, 12425, 770, 807, 51336, 51380, 341, 2302, 13464, 370, 456, 321, 352, 3879, 1669, 2020, 370, 586, 321, 808, 281, 5380, 732, 309, 1936, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.19142166016593812, "compression_ratio": 1.5696969696969696, "no_speech_prob": 8.059362812673498e-07}, {"id": 823, "seek": 516464, "start": 5184.96, "end": 5192.96, "text": " this entire beast so there we go totally makes sense so now we come to exercise two it basically", "tokens": [50364, 294, 293, 436, 486, 909, 370, 341, 307, 452, 11532, 3827, 718, 311, 5055, 309, 510, 50812, 50920, 718, 311, 8585, 518, 341, 293, 3278, 527, 7350, 4177, 370, 300, 311, 309, 321, 600, 646, 12425, 770, 807, 51336, 51380, 341, 2302, 13464, 370, 456, 321, 352, 3879, 1669, 2020, 370, 586, 321, 808, 281, 5380, 732, 309, 1936, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.19142166016593812, "compression_ratio": 1.5696969696969696, "no_speech_prob": 8.059362812673498e-07}, {"id": 824, "seek": 519296, "start": 5192.96, "end": 5197.52, "text": " turns out that in this first exercise we were doing way too much work we were back propagating", "tokens": [50364, 4523, 484, 300, 294, 341, 700, 5380, 321, 645, 884, 636, 886, 709, 589, 321, 645, 646, 12425, 990, 50592, 50592, 636, 886, 709, 293, 309, 390, 439, 665, 3124, 293, 370, 322, 457, 309, 311, 406, 437, 291, 576, 360, 294, 3124, 50808, 50832, 293, 264, 1778, 337, 300, 307, 337, 1365, 510, 741, 12005, 484, 341, 4470, 17108, 670, 3866, 51088, 51088, 3876, 293, 741, 6902, 309, 493, 439, 439, 281, 411, 1080, 16998, 22275, 3755, 293, 321, 646, 12425, 770, 51340, 51340, 807, 439, 295, 729, 16652, 457, 309, 4523, 484, 300, 498, 291, 445, 574, 412, 264, 18894, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.09874344771763063, "compression_ratio": 1.8571428571428572, "no_speech_prob": 8.664368579047732e-06}, {"id": 825, "seek": 519296, "start": 5197.52, "end": 5201.84, "text": " way too much and it was all good practice and so on but it's not what you would do in practice", "tokens": [50364, 4523, 484, 300, 294, 341, 700, 5380, 321, 645, 884, 636, 886, 709, 589, 321, 645, 646, 12425, 990, 50592, 50592, 636, 886, 709, 293, 309, 390, 439, 665, 3124, 293, 370, 322, 457, 309, 311, 406, 437, 291, 576, 360, 294, 3124, 50808, 50832, 293, 264, 1778, 337, 300, 307, 337, 1365, 510, 741, 12005, 484, 341, 4470, 17108, 670, 3866, 51088, 51088, 3876, 293, 741, 6902, 309, 493, 439, 439, 281, 411, 1080, 16998, 22275, 3755, 293, 321, 646, 12425, 770, 51340, 51340, 807, 439, 295, 729, 16652, 457, 309, 4523, 484, 300, 498, 291, 445, 574, 412, 264, 18894, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.09874344771763063, "compression_ratio": 1.8571428571428572, "no_speech_prob": 8.664368579047732e-06}, {"id": 826, "seek": 519296, "start": 5202.32, "end": 5207.44, "text": " and the reason for that is for example here i separated out this loss calculation over multiple", "tokens": [50364, 4523, 484, 300, 294, 341, 700, 5380, 321, 645, 884, 636, 886, 709, 589, 321, 645, 646, 12425, 990, 50592, 50592, 636, 886, 709, 293, 309, 390, 439, 665, 3124, 293, 370, 322, 457, 309, 311, 406, 437, 291, 576, 360, 294, 3124, 50808, 50832, 293, 264, 1778, 337, 300, 307, 337, 1365, 510, 741, 12005, 484, 341, 4470, 17108, 670, 3866, 51088, 51088, 3876, 293, 741, 6902, 309, 493, 439, 439, 281, 411, 1080, 16998, 22275, 3755, 293, 321, 646, 12425, 770, 51340, 51340, 807, 439, 295, 729, 16652, 457, 309, 4523, 484, 300, 498, 291, 445, 574, 412, 264, 18894, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.09874344771763063, "compression_ratio": 1.8571428571428572, "no_speech_prob": 8.664368579047732e-06}, {"id": 827, "seek": 519296, "start": 5207.44, "end": 5212.4800000000005, "text": " lines and i broke it up all all to like its smallest atomic pieces and we back propagated", "tokens": [50364, 4523, 484, 300, 294, 341, 700, 5380, 321, 645, 884, 636, 886, 709, 589, 321, 645, 646, 12425, 990, 50592, 50592, 636, 886, 709, 293, 309, 390, 439, 665, 3124, 293, 370, 322, 457, 309, 311, 406, 437, 291, 576, 360, 294, 3124, 50808, 50832, 293, 264, 1778, 337, 300, 307, 337, 1365, 510, 741, 12005, 484, 341, 4470, 17108, 670, 3866, 51088, 51088, 3876, 293, 741, 6902, 309, 493, 439, 439, 281, 411, 1080, 16998, 22275, 3755, 293, 321, 646, 12425, 770, 51340, 51340, 807, 439, 295, 729, 16652, 457, 309, 4523, 484, 300, 498, 291, 445, 574, 412, 264, 18894, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.09874344771763063, "compression_ratio": 1.8571428571428572, "no_speech_prob": 8.664368579047732e-06}, {"id": 828, "seek": 519296, "start": 5212.4800000000005, "end": 5216.64, "text": " through all of those individually but it turns out that if you just look at the mathematical", "tokens": [50364, 4523, 484, 300, 294, 341, 700, 5380, 321, 645, 884, 636, 886, 709, 589, 321, 645, 646, 12425, 990, 50592, 50592, 636, 886, 709, 293, 309, 390, 439, 665, 3124, 293, 370, 322, 457, 309, 311, 406, 437, 291, 576, 360, 294, 3124, 50808, 50832, 293, 264, 1778, 337, 300, 307, 337, 1365, 510, 741, 12005, 484, 341, 4470, 17108, 670, 3866, 51088, 51088, 3876, 293, 741, 6902, 309, 493, 439, 439, 281, 411, 1080, 16998, 22275, 3755, 293, 321, 646, 12425, 770, 51340, 51340, 807, 439, 295, 729, 16652, 457, 309, 4523, 484, 300, 498, 291, 445, 574, 412, 264, 18894, 51548, 51548], "temperature": 0.0, "avg_logprob": -0.09874344771763063, "compression_ratio": 1.8571428571428572, "no_speech_prob": 8.664368579047732e-06}, {"id": 829, "seek": 521664, "start": 5216.64, "end": 5223.4400000000005, "text": " expression for the loss then actually you can do the differentiation on pen and paper and a lot of", "tokens": [50364, 6114, 337, 264, 4470, 550, 767, 291, 393, 360, 264, 38902, 322, 3435, 293, 3035, 293, 257, 688, 295, 50704, 50704, 2115, 10373, 293, 20460, 293, 264, 18894, 6114, 291, 917, 493, 365, 393, 312, 10591, 50912, 50912, 11639, 293, 3571, 281, 4445, 813, 646, 12425, 990, 807, 439, 264, 707, 3755, 295, 51112, 51112, 1203, 291, 600, 1096, 370, 949, 321, 632, 341, 6179, 2128, 1320, 516, 490, 3565, 1208, 281, 51396, 51396, 264, 4470, 457, 294, 25878, 284, 339, 1203, 393, 445, 312, 28008, 1214, 666, 257, 2167, 818, 412, 300, 3278, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.13961142909770108, "compression_ratio": 1.7718631178707225, "no_speech_prob": 1.4823227729721111e-06}, {"id": 830, "seek": 521664, "start": 5223.4400000000005, "end": 5227.6, "text": " terms cancel and simplify and the mathematical expression you end up with can be significantly", "tokens": [50364, 6114, 337, 264, 4470, 550, 767, 291, 393, 360, 264, 38902, 322, 3435, 293, 3035, 293, 257, 688, 295, 50704, 50704, 2115, 10373, 293, 20460, 293, 264, 18894, 6114, 291, 917, 493, 365, 393, 312, 10591, 50912, 50912, 11639, 293, 3571, 281, 4445, 813, 646, 12425, 990, 807, 439, 264, 707, 3755, 295, 51112, 51112, 1203, 291, 600, 1096, 370, 949, 321, 632, 341, 6179, 2128, 1320, 516, 490, 3565, 1208, 281, 51396, 51396, 264, 4470, 457, 294, 25878, 284, 339, 1203, 393, 445, 312, 28008, 1214, 666, 257, 2167, 818, 412, 300, 3278, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.13961142909770108, "compression_ratio": 1.7718631178707225, "no_speech_prob": 1.4823227729721111e-06}, {"id": 831, "seek": 521664, "start": 5227.6, "end": 5231.6, "text": " shorter and easier to implement than back propagating through all the little pieces of", "tokens": [50364, 6114, 337, 264, 4470, 550, 767, 291, 393, 360, 264, 38902, 322, 3435, 293, 3035, 293, 257, 688, 295, 50704, 50704, 2115, 10373, 293, 20460, 293, 264, 18894, 6114, 291, 917, 493, 365, 393, 312, 10591, 50912, 50912, 11639, 293, 3571, 281, 4445, 813, 646, 12425, 990, 807, 439, 264, 707, 3755, 295, 51112, 51112, 1203, 291, 600, 1096, 370, 949, 321, 632, 341, 6179, 2128, 1320, 516, 490, 3565, 1208, 281, 51396, 51396, 264, 4470, 457, 294, 25878, 284, 339, 1203, 393, 445, 312, 28008, 1214, 666, 257, 2167, 818, 412, 300, 3278, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.13961142909770108, "compression_ratio": 1.7718631178707225, "no_speech_prob": 1.4823227729721111e-06}, {"id": 832, "seek": 521664, "start": 5231.6, "end": 5237.280000000001, "text": " everything you've done so before we had this complicated forward pass going from logits to", "tokens": [50364, 6114, 337, 264, 4470, 550, 767, 291, 393, 360, 264, 38902, 322, 3435, 293, 3035, 293, 257, 688, 295, 50704, 50704, 2115, 10373, 293, 20460, 293, 264, 18894, 6114, 291, 917, 493, 365, 393, 312, 10591, 50912, 50912, 11639, 293, 3571, 281, 4445, 813, 646, 12425, 990, 807, 439, 264, 707, 3755, 295, 51112, 51112, 1203, 291, 600, 1096, 370, 949, 321, 632, 341, 6179, 2128, 1320, 516, 490, 3565, 1208, 281, 51396, 51396, 264, 4470, 457, 294, 25878, 284, 339, 1203, 393, 445, 312, 28008, 1214, 666, 257, 2167, 818, 412, 300, 3278, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.13961142909770108, "compression_ratio": 1.7718631178707225, "no_speech_prob": 1.4823227729721111e-06}, {"id": 833, "seek": 521664, "start": 5237.280000000001, "end": 5242.320000000001, "text": " the loss but in pytorch everything can just be glued together into a single call at that cross", "tokens": [50364, 6114, 337, 264, 4470, 550, 767, 291, 393, 360, 264, 38902, 322, 3435, 293, 3035, 293, 257, 688, 295, 50704, 50704, 2115, 10373, 293, 20460, 293, 264, 18894, 6114, 291, 917, 493, 365, 393, 312, 10591, 50912, 50912, 11639, 293, 3571, 281, 4445, 813, 646, 12425, 990, 807, 439, 264, 707, 3755, 295, 51112, 51112, 1203, 291, 600, 1096, 370, 949, 321, 632, 341, 6179, 2128, 1320, 516, 490, 3565, 1208, 281, 51396, 51396, 264, 4470, 457, 294, 25878, 284, 339, 1203, 393, 445, 312, 28008, 1214, 666, 257, 2167, 818, 412, 300, 3278, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.13961142909770108, "compression_ratio": 1.7718631178707225, "no_speech_prob": 1.4823227729721111e-06}, {"id": 834, "seek": 524232, "start": 5242.32, "end": 5248.48, "text": " entropy you just pass in logits and the labels and you get the exact same loss as i verify here", "tokens": [50364, 30867, 291, 445, 1320, 294, 3565, 1208, 293, 264, 16949, 293, 291, 483, 264, 1900, 912, 4470, 382, 741, 16888, 510, 50672, 50672, 370, 527, 3894, 4470, 293, 264, 2370, 4470, 1348, 490, 264, 16635, 295, 7705, 382, 257, 2167, 18894, 50936, 50936, 6114, 307, 264, 912, 457, 309, 311, 709, 709, 4663, 294, 257, 2128, 1320, 309, 311, 611, 709, 709, 4663, 294, 51248, 51248, 23897, 1320, 293, 264, 1778, 337, 300, 307, 498, 291, 445, 574, 412, 264, 18894, 1254, 295, 341, 293, 51444, 51444, 23203, 797, 291, 486, 917, 493, 365, 257, 588, 1359, 293, 2099, 6114, 370, 300, 311, 437, 321, 528, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.05062748302112926, "compression_ratio": 1.89453125, "no_speech_prob": 1.7061066728274454e-06}, {"id": 835, "seek": 524232, "start": 5248.48, "end": 5253.759999999999, "text": " so our previous loss and the fast loss coming from the chunk of operations as a single mathematical", "tokens": [50364, 30867, 291, 445, 1320, 294, 3565, 1208, 293, 264, 16949, 293, 291, 483, 264, 1900, 912, 4470, 382, 741, 16888, 510, 50672, 50672, 370, 527, 3894, 4470, 293, 264, 2370, 4470, 1348, 490, 264, 16635, 295, 7705, 382, 257, 2167, 18894, 50936, 50936, 6114, 307, 264, 912, 457, 309, 311, 709, 709, 4663, 294, 257, 2128, 1320, 309, 311, 611, 709, 709, 4663, 294, 51248, 51248, 23897, 1320, 293, 264, 1778, 337, 300, 307, 498, 291, 445, 574, 412, 264, 18894, 1254, 295, 341, 293, 51444, 51444, 23203, 797, 291, 486, 917, 493, 365, 257, 588, 1359, 293, 2099, 6114, 370, 300, 311, 437, 321, 528, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.05062748302112926, "compression_ratio": 1.89453125, "no_speech_prob": 1.7061066728274454e-06}, {"id": 836, "seek": 524232, "start": 5253.759999999999, "end": 5260.0, "text": " expression is the same but it's much much faster in a forward pass it's also much much faster in", "tokens": [50364, 30867, 291, 445, 1320, 294, 3565, 1208, 293, 264, 16949, 293, 291, 483, 264, 1900, 912, 4470, 382, 741, 16888, 510, 50672, 50672, 370, 527, 3894, 4470, 293, 264, 2370, 4470, 1348, 490, 264, 16635, 295, 7705, 382, 257, 2167, 18894, 50936, 50936, 6114, 307, 264, 912, 457, 309, 311, 709, 709, 4663, 294, 257, 2128, 1320, 309, 311, 611, 709, 709, 4663, 294, 51248, 51248, 23897, 1320, 293, 264, 1778, 337, 300, 307, 498, 291, 445, 574, 412, 264, 18894, 1254, 295, 341, 293, 51444, 51444, 23203, 797, 291, 486, 917, 493, 365, 257, 588, 1359, 293, 2099, 6114, 370, 300, 311, 437, 321, 528, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.05062748302112926, "compression_ratio": 1.89453125, "no_speech_prob": 1.7061066728274454e-06}, {"id": 837, "seek": 524232, "start": 5260.0, "end": 5263.92, "text": " backward pass and the reason for that is if you just look at the mathematical form of this and", "tokens": [50364, 30867, 291, 445, 1320, 294, 3565, 1208, 293, 264, 16949, 293, 291, 483, 264, 1900, 912, 4470, 382, 741, 16888, 510, 50672, 50672, 370, 527, 3894, 4470, 293, 264, 2370, 4470, 1348, 490, 264, 16635, 295, 7705, 382, 257, 2167, 18894, 50936, 50936, 6114, 307, 264, 912, 457, 309, 311, 709, 709, 4663, 294, 257, 2128, 1320, 309, 311, 611, 709, 709, 4663, 294, 51248, 51248, 23897, 1320, 293, 264, 1778, 337, 300, 307, 498, 291, 445, 574, 412, 264, 18894, 1254, 295, 341, 293, 51444, 51444, 23203, 797, 291, 486, 917, 493, 365, 257, 588, 1359, 293, 2099, 6114, 370, 300, 311, 437, 321, 528, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.05062748302112926, "compression_ratio": 1.89453125, "no_speech_prob": 1.7061066728274454e-06}, {"id": 838, "seek": 524232, "start": 5263.92, "end": 5268.5599999999995, "text": " differentiate again you will end up with a very small and short expression so that's what we want", "tokens": [50364, 30867, 291, 445, 1320, 294, 3565, 1208, 293, 264, 16949, 293, 291, 483, 264, 1900, 912, 4470, 382, 741, 16888, 510, 50672, 50672, 370, 527, 3894, 4470, 293, 264, 2370, 4470, 1348, 490, 264, 16635, 295, 7705, 382, 257, 2167, 18894, 50936, 50936, 6114, 307, 264, 912, 457, 309, 311, 709, 709, 4663, 294, 257, 2128, 1320, 309, 311, 611, 709, 709, 4663, 294, 51248, 51248, 23897, 1320, 293, 264, 1778, 337, 300, 307, 498, 291, 445, 574, 412, 264, 18894, 1254, 295, 341, 293, 51444, 51444, 23203, 797, 291, 486, 917, 493, 365, 257, 588, 1359, 293, 2099, 6114, 370, 300, 311, 437, 321, 528, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.05062748302112926, "compression_ratio": 1.89453125, "no_speech_prob": 1.7061066728274454e-06}, {"id": 839, "seek": 526856, "start": 5268.56, "end": 5275.04, "text": " to do here we want to in a single operation or in a single go or like very quickly go directly to", "tokens": [50364, 281, 360, 510, 321, 528, 281, 294, 257, 2167, 6916, 420, 294, 257, 2167, 352, 420, 411, 588, 2661, 352, 3838, 281, 50688, 50688, 274, 3565, 1208, 293, 321, 643, 281, 4445, 274, 3565, 1208, 382, 257, 2445, 295, 3565, 1208, 293, 288, 65, 311, 457, 309, 486, 312, 51088, 51088, 10591, 11639, 813, 2035, 321, 630, 510, 689, 281, 483, 281, 274, 3565, 1208, 321, 632, 281, 352, 439, 264, 636, 51376, 51376, 510, 370, 439, 295, 341, 589, 393, 312, 30193, 294, 257, 709, 709, 18587, 18894, 6114, 300, 291, 393, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.05686054033102449, "compression_ratio": 1.7788018433179724, "no_speech_prob": 3.137996145596844e-06}, {"id": 840, "seek": 526856, "start": 5275.04, "end": 5283.04, "text": " d logits and we need to implement d logits as a function of logits and yb's but it will be", "tokens": [50364, 281, 360, 510, 321, 528, 281, 294, 257, 2167, 6916, 420, 294, 257, 2167, 352, 420, 411, 588, 2661, 352, 3838, 281, 50688, 50688, 274, 3565, 1208, 293, 321, 643, 281, 4445, 274, 3565, 1208, 382, 257, 2445, 295, 3565, 1208, 293, 288, 65, 311, 457, 309, 486, 312, 51088, 51088, 10591, 11639, 813, 2035, 321, 630, 510, 689, 281, 483, 281, 274, 3565, 1208, 321, 632, 281, 352, 439, 264, 636, 51376, 51376, 510, 370, 439, 295, 341, 589, 393, 312, 30193, 294, 257, 709, 709, 18587, 18894, 6114, 300, 291, 393, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.05686054033102449, "compression_ratio": 1.7788018433179724, "no_speech_prob": 3.137996145596844e-06}, {"id": 841, "seek": 526856, "start": 5283.04, "end": 5288.8, "text": " significantly shorter than whatever we did here where to get to d logits we had to go all the way", "tokens": [50364, 281, 360, 510, 321, 528, 281, 294, 257, 2167, 6916, 420, 294, 257, 2167, 352, 420, 411, 588, 2661, 352, 3838, 281, 50688, 50688, 274, 3565, 1208, 293, 321, 643, 281, 4445, 274, 3565, 1208, 382, 257, 2445, 295, 3565, 1208, 293, 288, 65, 311, 457, 309, 486, 312, 51088, 51088, 10591, 11639, 813, 2035, 321, 630, 510, 689, 281, 483, 281, 274, 3565, 1208, 321, 632, 281, 352, 439, 264, 636, 51376, 51376, 510, 370, 439, 295, 341, 589, 393, 312, 30193, 294, 257, 709, 709, 18587, 18894, 6114, 300, 291, 393, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.05686054033102449, "compression_ratio": 1.7788018433179724, "no_speech_prob": 3.137996145596844e-06}, {"id": 842, "seek": 526856, "start": 5288.8, "end": 5294.96, "text": " here so all of this work can be skipped in a much much simpler mathematical expression that you can", "tokens": [50364, 281, 360, 510, 321, 528, 281, 294, 257, 2167, 6916, 420, 294, 257, 2167, 352, 420, 411, 588, 2661, 352, 3838, 281, 50688, 50688, 274, 3565, 1208, 293, 321, 643, 281, 4445, 274, 3565, 1208, 382, 257, 2445, 295, 3565, 1208, 293, 288, 65, 311, 457, 309, 486, 312, 51088, 51088, 10591, 11639, 813, 2035, 321, 630, 510, 689, 281, 483, 281, 274, 3565, 1208, 321, 632, 281, 352, 439, 264, 636, 51376, 51376, 510, 370, 439, 295, 341, 589, 393, 312, 30193, 294, 257, 709, 709, 18587, 18894, 6114, 300, 291, 393, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.05686054033102449, "compression_ratio": 1.7788018433179724, "no_speech_prob": 3.137996145596844e-06}, {"id": 843, "seek": 529496, "start": 5294.96, "end": 5302.16, "text": " implement here so you can give it a shot yourself basically look at what exactly is the mathematical", "tokens": [50364, 4445, 510, 370, 291, 393, 976, 309, 257, 3347, 1803, 1936, 574, 412, 437, 2293, 307, 264, 18894, 50724, 50724, 6114, 295, 4470, 293, 23203, 365, 3104, 281, 264, 3565, 1208, 370, 718, 385, 855, 291, 257, 12075, 291, 393, 51088, 51088, 295, 1164, 853, 309, 4498, 1803, 457, 498, 406, 741, 393, 976, 291, 512, 12075, 295, 577, 281, 483, 1409, 44003, 51332, 51448, 370, 1936, 437, 311, 2737, 510, 307, 321, 362, 3565, 1208, 550, 456, 311, 257, 2787, 41167, 300, 2516, 264, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.039349577400121794, "compression_ratio": 1.790909090909091, "no_speech_prob": 2.057971869362518e-06}, {"id": 844, "seek": 529496, "start": 5302.16, "end": 5309.44, "text": " expression of loss and differentiate with respect to the logits so let me show you a hint you can", "tokens": [50364, 4445, 510, 370, 291, 393, 976, 309, 257, 3347, 1803, 1936, 574, 412, 437, 2293, 307, 264, 18894, 50724, 50724, 6114, 295, 4470, 293, 23203, 365, 3104, 281, 264, 3565, 1208, 370, 718, 385, 855, 291, 257, 12075, 291, 393, 51088, 51088, 295, 1164, 853, 309, 4498, 1803, 457, 498, 406, 741, 393, 976, 291, 512, 12075, 295, 577, 281, 483, 1409, 44003, 51332, 51448, 370, 1936, 437, 311, 2737, 510, 307, 321, 362, 3565, 1208, 550, 456, 311, 257, 2787, 41167, 300, 2516, 264, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.039349577400121794, "compression_ratio": 1.790909090909091, "no_speech_prob": 2.057971869362518e-06}, {"id": 845, "seek": 529496, "start": 5309.44, "end": 5314.32, "text": " of course try it fully yourself but if not i can give you some hint of how to get started mathematically", "tokens": [50364, 4445, 510, 370, 291, 393, 976, 309, 257, 3347, 1803, 1936, 574, 412, 437, 2293, 307, 264, 18894, 50724, 50724, 6114, 295, 4470, 293, 23203, 365, 3104, 281, 264, 3565, 1208, 370, 718, 385, 855, 291, 257, 12075, 291, 393, 51088, 51088, 295, 1164, 853, 309, 4498, 1803, 457, 498, 406, 741, 393, 976, 291, 512, 12075, 295, 577, 281, 483, 1409, 44003, 51332, 51448, 370, 1936, 437, 311, 2737, 510, 307, 321, 362, 3565, 1208, 550, 456, 311, 257, 2787, 41167, 300, 2516, 264, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.039349577400121794, "compression_ratio": 1.790909090909091, "no_speech_prob": 2.057971869362518e-06}, {"id": 846, "seek": 529496, "start": 5316.64, "end": 5321.2, "text": " so basically what's happening here is we have logits then there's a softmax that takes the", "tokens": [50364, 4445, 510, 370, 291, 393, 976, 309, 257, 3347, 1803, 1936, 574, 412, 437, 2293, 307, 264, 18894, 50724, 50724, 6114, 295, 4470, 293, 23203, 365, 3104, 281, 264, 3565, 1208, 370, 718, 385, 855, 291, 257, 12075, 291, 393, 51088, 51088, 295, 1164, 853, 309, 4498, 1803, 457, 498, 406, 741, 393, 976, 291, 512, 12075, 295, 577, 281, 483, 1409, 44003, 51332, 51448, 370, 1936, 437, 311, 2737, 510, 307, 321, 362, 3565, 1208, 550, 456, 311, 257, 2787, 41167, 300, 2516, 264, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.039349577400121794, "compression_ratio": 1.790909090909091, "no_speech_prob": 2.057971869362518e-06}, {"id": 847, "seek": 532120, "start": 5321.2, "end": 5326.72, "text": " logits and gives you probabilities then we are using the identity of the correct next character", "tokens": [50364, 3565, 1208, 293, 2709, 291, 33783, 550, 321, 366, 1228, 264, 6575, 295, 264, 3006, 958, 2517, 50640, 50640, 281, 41514, 484, 257, 5386, 295, 33783, 747, 264, 3671, 3565, 295, 309, 281, 483, 527, 3671, 3565, 50912, 50912, 8482, 293, 550, 321, 4274, 493, 439, 264, 3565, 33783, 420, 3671, 3565, 33783, 281, 483, 51200, 51200, 527, 4470, 370, 1936, 437, 321, 362, 307, 337, 257, 2167, 2609, 1365, 2831, 321, 362, 300, 4470, 307, 51536, 51536, 2681, 281, 3671, 3565, 8482, 689, 280, 510, 307, 733, 295, 411, 1194, 295, 382, 257, 8062, 295, 439, 264, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.03239720241696227, "compression_ratio": 2.0695652173913044, "no_speech_prob": 7.296032435988309e-06}, {"id": 848, "seek": 532120, "start": 5326.72, "end": 5332.16, "text": " to pluck out a row of probabilities take the negative log of it to get our negative log", "tokens": [50364, 3565, 1208, 293, 2709, 291, 33783, 550, 321, 366, 1228, 264, 6575, 295, 264, 3006, 958, 2517, 50640, 50640, 281, 41514, 484, 257, 5386, 295, 33783, 747, 264, 3671, 3565, 295, 309, 281, 483, 527, 3671, 3565, 50912, 50912, 8482, 293, 550, 321, 4274, 493, 439, 264, 3565, 33783, 420, 3671, 3565, 33783, 281, 483, 51200, 51200, 527, 4470, 370, 1936, 437, 321, 362, 307, 337, 257, 2167, 2609, 1365, 2831, 321, 362, 300, 4470, 307, 51536, 51536, 2681, 281, 3671, 3565, 8482, 689, 280, 510, 307, 733, 295, 411, 1194, 295, 382, 257, 8062, 295, 439, 264, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.03239720241696227, "compression_ratio": 2.0695652173913044, "no_speech_prob": 7.296032435988309e-06}, {"id": 849, "seek": 532120, "start": 5332.16, "end": 5337.92, "text": " probability and then we average up all the log probabilities or negative log probabilities to get", "tokens": [50364, 3565, 1208, 293, 2709, 291, 33783, 550, 321, 366, 1228, 264, 6575, 295, 264, 3006, 958, 2517, 50640, 50640, 281, 41514, 484, 257, 5386, 295, 33783, 747, 264, 3671, 3565, 295, 309, 281, 483, 527, 3671, 3565, 50912, 50912, 8482, 293, 550, 321, 4274, 493, 439, 264, 3565, 33783, 420, 3671, 3565, 33783, 281, 483, 51200, 51200, 527, 4470, 370, 1936, 437, 321, 362, 307, 337, 257, 2167, 2609, 1365, 2831, 321, 362, 300, 4470, 307, 51536, 51536, 2681, 281, 3671, 3565, 8482, 689, 280, 510, 307, 733, 295, 411, 1194, 295, 382, 257, 8062, 295, 439, 264, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.03239720241696227, "compression_ratio": 2.0695652173913044, "no_speech_prob": 7.296032435988309e-06}, {"id": 850, "seek": 532120, "start": 5337.92, "end": 5344.639999999999, "text": " our loss so basically what we have is for a single individual example rather we have that loss is", "tokens": [50364, 3565, 1208, 293, 2709, 291, 33783, 550, 321, 366, 1228, 264, 6575, 295, 264, 3006, 958, 2517, 50640, 50640, 281, 41514, 484, 257, 5386, 295, 33783, 747, 264, 3671, 3565, 295, 309, 281, 483, 527, 3671, 3565, 50912, 50912, 8482, 293, 550, 321, 4274, 493, 439, 264, 3565, 33783, 420, 3671, 3565, 33783, 281, 483, 51200, 51200, 527, 4470, 370, 1936, 437, 321, 362, 307, 337, 257, 2167, 2609, 1365, 2831, 321, 362, 300, 4470, 307, 51536, 51536, 2681, 281, 3671, 3565, 8482, 689, 280, 510, 307, 733, 295, 411, 1194, 295, 382, 257, 8062, 295, 439, 264, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.03239720241696227, "compression_ratio": 2.0695652173913044, "no_speech_prob": 7.296032435988309e-06}, {"id": 851, "seek": 532120, "start": 5344.639999999999, "end": 5350.96, "text": " equal to negative log probability where p here is kind of like thought of as a vector of all the", "tokens": [50364, 3565, 1208, 293, 2709, 291, 33783, 550, 321, 366, 1228, 264, 6575, 295, 264, 3006, 958, 2517, 50640, 50640, 281, 41514, 484, 257, 5386, 295, 33783, 747, 264, 3671, 3565, 295, 309, 281, 483, 527, 3671, 3565, 50912, 50912, 8482, 293, 550, 321, 4274, 493, 439, 264, 3565, 33783, 420, 3671, 3565, 33783, 281, 483, 51200, 51200, 527, 4470, 370, 1936, 437, 321, 362, 307, 337, 257, 2167, 2609, 1365, 2831, 321, 362, 300, 4470, 307, 51536, 51536, 2681, 281, 3671, 3565, 8482, 689, 280, 510, 307, 733, 295, 411, 1194, 295, 382, 257, 8062, 295, 439, 264, 51852, 51852], "temperature": 0.0, "avg_logprob": -0.03239720241696227, "compression_ratio": 2.0695652173913044, "no_speech_prob": 7.296032435988309e-06}, {"id": 852, "seek": 535096, "start": 5350.96, "end": 5359.44, "text": " probabilities so at the yth position where y is the label and we have that p here of course is the", "tokens": [50364, 33783, 370, 412, 264, 288, 392, 2535, 689, 288, 307, 264, 7645, 293, 321, 362, 300, 280, 510, 295, 1164, 307, 264, 50788, 50788, 2787, 41167, 370, 264, 741, 12, 392, 6542, 295, 280, 295, 341, 8482, 8062, 307, 445, 264, 2787, 41167, 2445, 51148, 51148, 370, 11225, 439, 264, 3565, 1208, 1936, 281, 264, 1347, 295, 308, 293, 2710, 3319, 370, 1203, 34499, 281, 472, 51536, 51588, 586, 498, 291, 2464, 484, 280, 295, 288, 510, 291, 393, 445, 2464, 484, 264, 2787, 41167, 293, 550, 1936, 437, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.06000233721989457, "compression_ratio": 1.821256038647343, "no_speech_prob": 3.905364337697392e-06}, {"id": 853, "seek": 535096, "start": 5359.44, "end": 5366.64, "text": " softmax so the i-th component of p of this probability vector is just the softmax function", "tokens": [50364, 33783, 370, 412, 264, 288, 392, 2535, 689, 288, 307, 264, 7645, 293, 321, 362, 300, 280, 510, 295, 1164, 307, 264, 50788, 50788, 2787, 41167, 370, 264, 741, 12, 392, 6542, 295, 280, 295, 341, 8482, 8062, 307, 445, 264, 2787, 41167, 2445, 51148, 51148, 370, 11225, 439, 264, 3565, 1208, 1936, 281, 264, 1347, 295, 308, 293, 2710, 3319, 370, 1203, 34499, 281, 472, 51536, 51588, 586, 498, 291, 2464, 484, 280, 295, 288, 510, 291, 393, 445, 2464, 484, 264, 2787, 41167, 293, 550, 1936, 437, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.06000233721989457, "compression_ratio": 1.821256038647343, "no_speech_prob": 3.905364337697392e-06}, {"id": 854, "seek": 535096, "start": 5366.64, "end": 5374.4, "text": " so raising all the logits basically to the power of e and normalizing so everything sums to one", "tokens": [50364, 33783, 370, 412, 264, 288, 392, 2535, 689, 288, 307, 264, 7645, 293, 321, 362, 300, 280, 510, 295, 1164, 307, 264, 50788, 50788, 2787, 41167, 370, 264, 741, 12, 392, 6542, 295, 280, 295, 341, 8482, 8062, 307, 445, 264, 2787, 41167, 2445, 51148, 51148, 370, 11225, 439, 264, 3565, 1208, 1936, 281, 264, 1347, 295, 308, 293, 2710, 3319, 370, 1203, 34499, 281, 472, 51536, 51588, 586, 498, 291, 2464, 484, 280, 295, 288, 510, 291, 393, 445, 2464, 484, 264, 2787, 41167, 293, 550, 1936, 437, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.06000233721989457, "compression_ratio": 1.821256038647343, "no_speech_prob": 3.905364337697392e-06}, {"id": 855, "seek": 535096, "start": 5375.44, "end": 5380.4800000000005, "text": " now if you write out p of y here you can just write out the softmax and then basically what", "tokens": [50364, 33783, 370, 412, 264, 288, 392, 2535, 689, 288, 307, 264, 7645, 293, 321, 362, 300, 280, 510, 295, 1164, 307, 264, 50788, 50788, 2787, 41167, 370, 264, 741, 12, 392, 6542, 295, 280, 295, 341, 8482, 8062, 307, 445, 264, 2787, 41167, 2445, 51148, 51148, 370, 11225, 439, 264, 3565, 1208, 1936, 281, 264, 1347, 295, 308, 293, 2710, 3319, 370, 1203, 34499, 281, 472, 51536, 51588, 586, 498, 291, 2464, 484, 280, 295, 288, 510, 291, 393, 445, 2464, 484, 264, 2787, 41167, 293, 550, 1936, 437, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.06000233721989457, "compression_ratio": 1.821256038647343, "no_speech_prob": 3.905364337697392e-06}, {"id": 856, "seek": 538048, "start": 5380.48, "end": 5386.0, "text": " we're interested in is we're interested in the derivative of the loss with respect to the i-th", "tokens": [50364, 321, 434, 3102, 294, 307, 321, 434, 3102, 294, 264, 13760, 295, 264, 4470, 365, 3104, 281, 264, 741, 12, 392, 50640, 50640, 3565, 270, 293, 370, 1936, 309, 311, 257, 274, 538, 274, 375, 295, 341, 6114, 510, 689, 321, 362, 287, 8186, 292, 365, 264, 51044, 51044, 2685, 7645, 288, 293, 322, 264, 2767, 321, 362, 257, 2408, 670, 361, 295, 308, 281, 264, 287, 73, 293, 264, 3671, 3565, 295, 439, 51316, 51316, 300, 370, 7263, 976, 309, 257, 3347, 3435, 293, 3035, 293, 536, 498, 291, 393, 767, 28446, 264, 6114, 51580, 51580, 337, 264, 4470, 538, 274, 375, 293, 550, 321, 434, 516, 281, 4445, 309, 510, 1392, 370, 741, 669, 516, 281, 976, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.048896471659342446, "compression_ratio": 1.9007936507936507, "no_speech_prob": 8.139493729686365e-06}, {"id": 857, "seek": 538048, "start": 5386.0, "end": 5394.08, "text": " logit and so basically it's a d by d li of this expression here where we have l indexed with the", "tokens": [50364, 321, 434, 3102, 294, 307, 321, 434, 3102, 294, 264, 13760, 295, 264, 4470, 365, 3104, 281, 264, 741, 12, 392, 50640, 50640, 3565, 270, 293, 370, 1936, 309, 311, 257, 274, 538, 274, 375, 295, 341, 6114, 510, 689, 321, 362, 287, 8186, 292, 365, 264, 51044, 51044, 2685, 7645, 288, 293, 322, 264, 2767, 321, 362, 257, 2408, 670, 361, 295, 308, 281, 264, 287, 73, 293, 264, 3671, 3565, 295, 439, 51316, 51316, 300, 370, 7263, 976, 309, 257, 3347, 3435, 293, 3035, 293, 536, 498, 291, 393, 767, 28446, 264, 6114, 51580, 51580, 337, 264, 4470, 538, 274, 375, 293, 550, 321, 434, 516, 281, 4445, 309, 510, 1392, 370, 741, 669, 516, 281, 976, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.048896471659342446, "compression_ratio": 1.9007936507936507, "no_speech_prob": 8.139493729686365e-06}, {"id": 858, "seek": 538048, "start": 5394.08, "end": 5399.5199999999995, "text": " specific label y and on the bottom we have a sum over j of e to the lj and the negative log of all", "tokens": [50364, 321, 434, 3102, 294, 307, 321, 434, 3102, 294, 264, 13760, 295, 264, 4470, 365, 3104, 281, 264, 741, 12, 392, 50640, 50640, 3565, 270, 293, 370, 1936, 309, 311, 257, 274, 538, 274, 375, 295, 341, 6114, 510, 689, 321, 362, 287, 8186, 292, 365, 264, 51044, 51044, 2685, 7645, 288, 293, 322, 264, 2767, 321, 362, 257, 2408, 670, 361, 295, 308, 281, 264, 287, 73, 293, 264, 3671, 3565, 295, 439, 51316, 51316, 300, 370, 7263, 976, 309, 257, 3347, 3435, 293, 3035, 293, 536, 498, 291, 393, 767, 28446, 264, 6114, 51580, 51580, 337, 264, 4470, 538, 274, 375, 293, 550, 321, 434, 516, 281, 4445, 309, 510, 1392, 370, 741, 669, 516, 281, 976, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.048896471659342446, "compression_ratio": 1.9007936507936507, "no_speech_prob": 8.139493729686365e-06}, {"id": 859, "seek": 538048, "start": 5399.5199999999995, "end": 5404.799999999999, "text": " that so potentially give it a shot pen and paper and see if you can actually derive the expression", "tokens": [50364, 321, 434, 3102, 294, 307, 321, 434, 3102, 294, 264, 13760, 295, 264, 4470, 365, 3104, 281, 264, 741, 12, 392, 50640, 50640, 3565, 270, 293, 370, 1936, 309, 311, 257, 274, 538, 274, 375, 295, 341, 6114, 510, 689, 321, 362, 287, 8186, 292, 365, 264, 51044, 51044, 2685, 7645, 288, 293, 322, 264, 2767, 321, 362, 257, 2408, 670, 361, 295, 308, 281, 264, 287, 73, 293, 264, 3671, 3565, 295, 439, 51316, 51316, 300, 370, 7263, 976, 309, 257, 3347, 3435, 293, 3035, 293, 536, 498, 291, 393, 767, 28446, 264, 6114, 51580, 51580, 337, 264, 4470, 538, 274, 375, 293, 550, 321, 434, 516, 281, 4445, 309, 510, 1392, 370, 741, 669, 516, 281, 976, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.048896471659342446, "compression_ratio": 1.9007936507936507, "no_speech_prob": 8.139493729686365e-06}, {"id": 860, "seek": 538048, "start": 5404.799999999999, "end": 5410.0, "text": " for the loss by d li and then we're going to implement it here okay so i am going to give", "tokens": [50364, 321, 434, 3102, 294, 307, 321, 434, 3102, 294, 264, 13760, 295, 264, 4470, 365, 3104, 281, 264, 741, 12, 392, 50640, 50640, 3565, 270, 293, 370, 1936, 309, 311, 257, 274, 538, 274, 375, 295, 341, 6114, 510, 689, 321, 362, 287, 8186, 292, 365, 264, 51044, 51044, 2685, 7645, 288, 293, 322, 264, 2767, 321, 362, 257, 2408, 670, 361, 295, 308, 281, 264, 287, 73, 293, 264, 3671, 3565, 295, 439, 51316, 51316, 300, 370, 7263, 976, 309, 257, 3347, 3435, 293, 3035, 293, 536, 498, 291, 393, 767, 28446, 264, 6114, 51580, 51580, 337, 264, 4470, 538, 274, 375, 293, 550, 321, 434, 516, 281, 4445, 309, 510, 1392, 370, 741, 669, 516, 281, 976, 51840, 51840], "temperature": 0.0, "avg_logprob": -0.048896471659342446, "compression_ratio": 1.9007936507936507, "no_speech_prob": 8.139493729686365e-06}, {"id": 861, "seek": 541000, "start": 5410.0, "end": 5417.2, "text": " away the result here so this is some of the math i did to derive the gradients analytically and so", "tokens": [50364, 1314, 264, 1874, 510, 370, 341, 307, 512, 295, 264, 5221, 741, 630, 281, 28446, 264, 2771, 2448, 10783, 984, 293, 370, 50724, 50724, 321, 536, 510, 300, 741, 478, 445, 9275, 264, 4474, 295, 33400, 490, 428, 700, 420, 1150, 1064, 295, 25947, 311, 50932, 50932, 4314, 498, 291, 1890, 309, 293, 321, 536, 300, 264, 15277, 767, 20460, 1596, 257, 857, 51164, 51164, 291, 362, 281, 4994, 484, 264, 5215, 294, 264, 1389, 689, 264, 741, 12, 392, 8186, 300, 291, 434, 3102, 294, 51392, 51392, 1854, 3565, 1208, 307, 2139, 2681, 281, 264, 7645, 420, 309, 311, 406, 2681, 281, 264, 7645, 293, 550, 264, 15277, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.025199752993288294, "compression_ratio": 1.8045112781954886, "no_speech_prob": 3.5558755371312145e-06}, {"id": 862, "seek": 541000, "start": 5417.2, "end": 5421.36, "text": " we see here that i'm just applying the rules of calculus from your first or second year of bachelor's", "tokens": [50364, 1314, 264, 1874, 510, 370, 341, 307, 512, 295, 264, 5221, 741, 630, 281, 28446, 264, 2771, 2448, 10783, 984, 293, 370, 50724, 50724, 321, 536, 510, 300, 741, 478, 445, 9275, 264, 4474, 295, 33400, 490, 428, 700, 420, 1150, 1064, 295, 25947, 311, 50932, 50932, 4314, 498, 291, 1890, 309, 293, 321, 536, 300, 264, 15277, 767, 20460, 1596, 257, 857, 51164, 51164, 291, 362, 281, 4994, 484, 264, 5215, 294, 264, 1389, 689, 264, 741, 12, 392, 8186, 300, 291, 434, 3102, 294, 51392, 51392, 1854, 3565, 1208, 307, 2139, 2681, 281, 264, 7645, 420, 309, 311, 406, 2681, 281, 264, 7645, 293, 550, 264, 15277, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.025199752993288294, "compression_ratio": 1.8045112781954886, "no_speech_prob": 3.5558755371312145e-06}, {"id": 863, "seek": 541000, "start": 5421.36, "end": 5426.0, "text": " degree if you took it and we see that the expressions actually simplify quite a bit", "tokens": [50364, 1314, 264, 1874, 510, 370, 341, 307, 512, 295, 264, 5221, 741, 630, 281, 28446, 264, 2771, 2448, 10783, 984, 293, 370, 50724, 50724, 321, 536, 510, 300, 741, 478, 445, 9275, 264, 4474, 295, 33400, 490, 428, 700, 420, 1150, 1064, 295, 25947, 311, 50932, 50932, 4314, 498, 291, 1890, 309, 293, 321, 536, 300, 264, 15277, 767, 20460, 1596, 257, 857, 51164, 51164, 291, 362, 281, 4994, 484, 264, 5215, 294, 264, 1389, 689, 264, 741, 12, 392, 8186, 300, 291, 434, 3102, 294, 51392, 51392, 1854, 3565, 1208, 307, 2139, 2681, 281, 264, 7645, 420, 309, 311, 406, 2681, 281, 264, 7645, 293, 550, 264, 15277, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.025199752993288294, "compression_ratio": 1.8045112781954886, "no_speech_prob": 3.5558755371312145e-06}, {"id": 864, "seek": 541000, "start": 5426.0, "end": 5430.56, "text": " you have to separate out the analysis in the case where the i-th index that you're interested in", "tokens": [50364, 1314, 264, 1874, 510, 370, 341, 307, 512, 295, 264, 5221, 741, 630, 281, 28446, 264, 2771, 2448, 10783, 984, 293, 370, 50724, 50724, 321, 536, 510, 300, 741, 478, 445, 9275, 264, 4474, 295, 33400, 490, 428, 700, 420, 1150, 1064, 295, 25947, 311, 50932, 50932, 4314, 498, 291, 1890, 309, 293, 321, 536, 300, 264, 15277, 767, 20460, 1596, 257, 857, 51164, 51164, 291, 362, 281, 4994, 484, 264, 5215, 294, 264, 1389, 689, 264, 741, 12, 392, 8186, 300, 291, 434, 3102, 294, 51392, 51392, 1854, 3565, 1208, 307, 2139, 2681, 281, 264, 7645, 420, 309, 311, 406, 2681, 281, 264, 7645, 293, 550, 264, 15277, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.025199752993288294, "compression_ratio": 1.8045112781954886, "no_speech_prob": 3.5558755371312145e-06}, {"id": 865, "seek": 541000, "start": 5430.56, "end": 5435.68, "text": " inside logits is either equal to the label or it's not equal to the label and then the expressions", "tokens": [50364, 1314, 264, 1874, 510, 370, 341, 307, 512, 295, 264, 5221, 741, 630, 281, 28446, 264, 2771, 2448, 10783, 984, 293, 370, 50724, 50724, 321, 536, 510, 300, 741, 478, 445, 9275, 264, 4474, 295, 33400, 490, 428, 700, 420, 1150, 1064, 295, 25947, 311, 50932, 50932, 4314, 498, 291, 1890, 309, 293, 321, 536, 300, 264, 15277, 767, 20460, 1596, 257, 857, 51164, 51164, 291, 362, 281, 4994, 484, 264, 5215, 294, 264, 1389, 689, 264, 741, 12, 392, 8186, 300, 291, 434, 3102, 294, 51392, 51392, 1854, 3565, 1208, 307, 2139, 2681, 281, 264, 7645, 420, 309, 311, 406, 2681, 281, 264, 7645, 293, 550, 264, 15277, 51648, 51648], "temperature": 0.0, "avg_logprob": -0.025199752993288294, "compression_ratio": 1.8045112781954886, "no_speech_prob": 3.5558755371312145e-06}, {"id": 866, "seek": 543568, "start": 5435.68, "end": 5440.56, "text": " simplify and cancel in a slightly different way and what we end up with is something very very simple", "tokens": [50364, 20460, 293, 10373, 294, 257, 4748, 819, 636, 293, 437, 321, 917, 493, 365, 307, 746, 588, 588, 2199, 50608, 50652, 321, 2139, 917, 493, 365, 1936, 280, 412, 741, 689, 280, 307, 797, 341, 8062, 295, 33783, 934, 257, 2787, 41167, 51008, 51008, 420, 280, 412, 741, 3175, 472, 689, 321, 445, 2935, 16390, 281, 472, 457, 294, 604, 1389, 321, 445, 643, 281, 8873, 51280, 51280, 264, 2787, 41167, 280, 293, 550, 294, 264, 3006, 10139, 321, 643, 281, 16390, 281, 472, 293, 300, 311, 264, 16235, 51584, 51584, 264, 1254, 300, 309, 2516, 10783, 984, 370, 718, 311, 4445, 341, 1936, 293, 321, 362, 281, 1066, 294, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.057110610761140525, "compression_ratio": 1.881679389312977, "no_speech_prob": 2.994261876665405e-06}, {"id": 867, "seek": 543568, "start": 5441.4400000000005, "end": 5448.56, "text": " we either end up with basically p at i where p is again this vector of probabilities after a softmax", "tokens": [50364, 20460, 293, 10373, 294, 257, 4748, 819, 636, 293, 437, 321, 917, 493, 365, 307, 746, 588, 588, 2199, 50608, 50652, 321, 2139, 917, 493, 365, 1936, 280, 412, 741, 689, 280, 307, 797, 341, 8062, 295, 33783, 934, 257, 2787, 41167, 51008, 51008, 420, 280, 412, 741, 3175, 472, 689, 321, 445, 2935, 16390, 281, 472, 457, 294, 604, 1389, 321, 445, 643, 281, 8873, 51280, 51280, 264, 2787, 41167, 280, 293, 550, 294, 264, 3006, 10139, 321, 643, 281, 16390, 281, 472, 293, 300, 311, 264, 16235, 51584, 51584, 264, 1254, 300, 309, 2516, 10783, 984, 370, 718, 311, 4445, 341, 1936, 293, 321, 362, 281, 1066, 294, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.057110610761140525, "compression_ratio": 1.881679389312977, "no_speech_prob": 2.994261876665405e-06}, {"id": 868, "seek": 543568, "start": 5448.56, "end": 5454.0, "text": " or p at i minus one where we just simply subtract to one but in any case we just need to calculate", "tokens": [50364, 20460, 293, 10373, 294, 257, 4748, 819, 636, 293, 437, 321, 917, 493, 365, 307, 746, 588, 588, 2199, 50608, 50652, 321, 2139, 917, 493, 365, 1936, 280, 412, 741, 689, 280, 307, 797, 341, 8062, 295, 33783, 934, 257, 2787, 41167, 51008, 51008, 420, 280, 412, 741, 3175, 472, 689, 321, 445, 2935, 16390, 281, 472, 457, 294, 604, 1389, 321, 445, 643, 281, 8873, 51280, 51280, 264, 2787, 41167, 280, 293, 550, 294, 264, 3006, 10139, 321, 643, 281, 16390, 281, 472, 293, 300, 311, 264, 16235, 51584, 51584, 264, 1254, 300, 309, 2516, 10783, 984, 370, 718, 311, 4445, 341, 1936, 293, 321, 362, 281, 1066, 294, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.057110610761140525, "compression_ratio": 1.881679389312977, "no_speech_prob": 2.994261876665405e-06}, {"id": 869, "seek": 543568, "start": 5454.0, "end": 5460.08, "text": " the softmax p and then in the correct dimension we need to subtract to one and that's the gradient", "tokens": [50364, 20460, 293, 10373, 294, 257, 4748, 819, 636, 293, 437, 321, 917, 493, 365, 307, 746, 588, 588, 2199, 50608, 50652, 321, 2139, 917, 493, 365, 1936, 280, 412, 741, 689, 280, 307, 797, 341, 8062, 295, 33783, 934, 257, 2787, 41167, 51008, 51008, 420, 280, 412, 741, 3175, 472, 689, 321, 445, 2935, 16390, 281, 472, 457, 294, 604, 1389, 321, 445, 643, 281, 8873, 51280, 51280, 264, 2787, 41167, 280, 293, 550, 294, 264, 3006, 10139, 321, 643, 281, 16390, 281, 472, 293, 300, 311, 264, 16235, 51584, 51584, 264, 1254, 300, 309, 2516, 10783, 984, 370, 718, 311, 4445, 341, 1936, 293, 321, 362, 281, 1066, 294, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.057110610761140525, "compression_ratio": 1.881679389312977, "no_speech_prob": 2.994261876665405e-06}, {"id": 870, "seek": 543568, "start": 5460.08, "end": 5464.72, "text": " the form that it takes analytically so let's implement this basically and we have to keep in", "tokens": [50364, 20460, 293, 10373, 294, 257, 4748, 819, 636, 293, 437, 321, 917, 493, 365, 307, 746, 588, 588, 2199, 50608, 50652, 321, 2139, 917, 493, 365, 1936, 280, 412, 741, 689, 280, 307, 797, 341, 8062, 295, 33783, 934, 257, 2787, 41167, 51008, 51008, 420, 280, 412, 741, 3175, 472, 689, 321, 445, 2935, 16390, 281, 472, 457, 294, 604, 1389, 321, 445, 643, 281, 8873, 51280, 51280, 264, 2787, 41167, 280, 293, 550, 294, 264, 3006, 10139, 321, 643, 281, 16390, 281, 472, 293, 300, 311, 264, 16235, 51584, 51584, 264, 1254, 300, 309, 2516, 10783, 984, 370, 718, 311, 4445, 341, 1936, 293, 321, 362, 281, 1066, 294, 51816, 51816], "temperature": 0.0, "avg_logprob": -0.057110610761140525, "compression_ratio": 1.881679389312977, "no_speech_prob": 2.994261876665405e-06}, {"id": 871, "seek": 546472, "start": 5464.72, "end": 5469.2, "text": " mind that this is only done for a single example but here we are working with batches of examples", "tokens": [50364, 1575, 300, 341, 307, 787, 1096, 337, 257, 2167, 1365, 457, 510, 321, 366, 1364, 365, 15245, 279, 295, 5110, 50588, 50620, 370, 321, 362, 281, 312, 5026, 295, 300, 293, 550, 264, 4470, 337, 257, 15245, 307, 264, 4274, 4470, 670, 439, 264, 50912, 50912, 5110, 370, 294, 661, 2283, 307, 264, 1365, 337, 439, 264, 2609, 5110, 307, 264, 4470, 337, 1184, 51156, 51156, 2609, 1365, 2408, 1912, 493, 293, 550, 6666, 538, 297, 293, 321, 362, 281, 646, 48256, 807, 300, 382, 51400, 51400, 731, 293, 312, 5026, 365, 309, 370, 274, 3565, 1208, 307, 516, 281, 312, 283, 5893, 2787, 41167, 51668, 51780], "temperature": 0.0, "avg_logprob": -0.06751601630394612, "compression_ratio": 2.045045045045045, "no_speech_prob": 6.0485376707219984e-06}, {"id": 872, "seek": 546472, "start": 5469.84, "end": 5475.68, "text": " so we have to be careful of that and then the loss for a batch is the average loss over all the", "tokens": [50364, 1575, 300, 341, 307, 787, 1096, 337, 257, 2167, 1365, 457, 510, 321, 366, 1364, 365, 15245, 279, 295, 5110, 50588, 50620, 370, 321, 362, 281, 312, 5026, 295, 300, 293, 550, 264, 4470, 337, 257, 15245, 307, 264, 4274, 4470, 670, 439, 264, 50912, 50912, 5110, 370, 294, 661, 2283, 307, 264, 1365, 337, 439, 264, 2609, 5110, 307, 264, 4470, 337, 1184, 51156, 51156, 2609, 1365, 2408, 1912, 493, 293, 550, 6666, 538, 297, 293, 321, 362, 281, 646, 48256, 807, 300, 382, 51400, 51400, 731, 293, 312, 5026, 365, 309, 370, 274, 3565, 1208, 307, 516, 281, 312, 283, 5893, 2787, 41167, 51668, 51780], "temperature": 0.0, "avg_logprob": -0.06751601630394612, "compression_ratio": 2.045045045045045, "no_speech_prob": 6.0485376707219984e-06}, {"id": 873, "seek": 546472, "start": 5475.68, "end": 5480.56, "text": " examples so in other words is the example for all the individual examples is the loss for each", "tokens": [50364, 1575, 300, 341, 307, 787, 1096, 337, 257, 2167, 1365, 457, 510, 321, 366, 1364, 365, 15245, 279, 295, 5110, 50588, 50620, 370, 321, 362, 281, 312, 5026, 295, 300, 293, 550, 264, 4470, 337, 257, 15245, 307, 264, 4274, 4470, 670, 439, 264, 50912, 50912, 5110, 370, 294, 661, 2283, 307, 264, 1365, 337, 439, 264, 2609, 5110, 307, 264, 4470, 337, 1184, 51156, 51156, 2609, 1365, 2408, 1912, 493, 293, 550, 6666, 538, 297, 293, 321, 362, 281, 646, 48256, 807, 300, 382, 51400, 51400, 731, 293, 312, 5026, 365, 309, 370, 274, 3565, 1208, 307, 516, 281, 312, 283, 5893, 2787, 41167, 51668, 51780], "temperature": 0.0, "avg_logprob": -0.06751601630394612, "compression_ratio": 2.045045045045045, "no_speech_prob": 6.0485376707219984e-06}, {"id": 874, "seek": 546472, "start": 5480.56, "end": 5485.4400000000005, "text": " individual example summed up and then divided by n and we have to back propagate through that as", "tokens": [50364, 1575, 300, 341, 307, 787, 1096, 337, 257, 2167, 1365, 457, 510, 321, 366, 1364, 365, 15245, 279, 295, 5110, 50588, 50620, 370, 321, 362, 281, 312, 5026, 295, 300, 293, 550, 264, 4470, 337, 257, 15245, 307, 264, 4274, 4470, 670, 439, 264, 50912, 50912, 5110, 370, 294, 661, 2283, 307, 264, 1365, 337, 439, 264, 2609, 5110, 307, 264, 4470, 337, 1184, 51156, 51156, 2609, 1365, 2408, 1912, 493, 293, 550, 6666, 538, 297, 293, 321, 362, 281, 646, 48256, 807, 300, 382, 51400, 51400, 731, 293, 312, 5026, 365, 309, 370, 274, 3565, 1208, 307, 516, 281, 312, 283, 5893, 2787, 41167, 51668, 51780], "temperature": 0.0, "avg_logprob": -0.06751601630394612, "compression_ratio": 2.045045045045045, "no_speech_prob": 6.0485376707219984e-06}, {"id": 875, "seek": 546472, "start": 5485.4400000000005, "end": 5490.8, "text": " well and be careful with it so d logits is going to be f dot softmax", "tokens": [50364, 1575, 300, 341, 307, 787, 1096, 337, 257, 2167, 1365, 457, 510, 321, 366, 1364, 365, 15245, 279, 295, 5110, 50588, 50620, 370, 321, 362, 281, 312, 5026, 295, 300, 293, 550, 264, 4470, 337, 257, 15245, 307, 264, 4274, 4470, 670, 439, 264, 50912, 50912, 5110, 370, 294, 661, 2283, 307, 264, 1365, 337, 439, 264, 2609, 5110, 307, 264, 4470, 337, 1184, 51156, 51156, 2609, 1365, 2408, 1912, 493, 293, 550, 6666, 538, 297, 293, 321, 362, 281, 646, 48256, 807, 300, 382, 51400, 51400, 731, 293, 312, 5026, 365, 309, 370, 274, 3565, 1208, 307, 516, 281, 312, 283, 5893, 2787, 41167, 51668, 51780], "temperature": 0.0, "avg_logprob": -0.06751601630394612, "compression_ratio": 2.045045045045045, "no_speech_prob": 6.0485376707219984e-06}, {"id": 876, "seek": 549080, "start": 5490.8, "end": 5495.92, "text": " uh pytorch has a softmax function that you can call and we want to apply the softmax on the logits", "tokens": [50364, 2232, 25878, 284, 339, 575, 257, 2787, 41167, 2445, 300, 291, 393, 818, 293, 321, 528, 281, 3079, 264, 2787, 41167, 322, 264, 3565, 1208, 50620, 50620, 293, 321, 528, 281, 352, 294, 264, 10139, 300, 307, 472, 370, 1936, 321, 528, 281, 360, 264, 2787, 41167, 2051, 264, 13241, 50984, 50984, 295, 613, 3565, 1208, 550, 412, 264, 3006, 8432, 321, 643, 281, 16390, 257, 472, 370, 274, 3565, 1208, 412, 17138, 990, 51372, 51372, 670, 439, 264, 13241, 293, 8186, 278, 666, 264, 13766, 5649, 538, 264, 3006, 16949, 1854, 288, 65, 321, 643, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.12127971649169922, "compression_ratio": 1.955223880597015, "no_speech_prob": 5.771747510152636e-06}, {"id": 877, "seek": 549080, "start": 5495.92, "end": 5503.2, "text": " and we want to go in the dimension that is one so basically we want to do the softmax along the rows", "tokens": [50364, 2232, 25878, 284, 339, 575, 257, 2787, 41167, 2445, 300, 291, 393, 818, 293, 321, 528, 281, 3079, 264, 2787, 41167, 322, 264, 3565, 1208, 50620, 50620, 293, 321, 528, 281, 352, 294, 264, 10139, 300, 307, 472, 370, 1936, 321, 528, 281, 360, 264, 2787, 41167, 2051, 264, 13241, 50984, 50984, 295, 613, 3565, 1208, 550, 412, 264, 3006, 8432, 321, 643, 281, 16390, 257, 472, 370, 274, 3565, 1208, 412, 17138, 990, 51372, 51372, 670, 439, 264, 13241, 293, 8186, 278, 666, 264, 13766, 5649, 538, 264, 3006, 16949, 1854, 288, 65, 321, 643, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.12127971649169922, "compression_ratio": 1.955223880597015, "no_speech_prob": 5.771747510152636e-06}, {"id": 878, "seek": 549080, "start": 5503.2, "end": 5510.96, "text": " of these logits then at the correct positions we need to subtract a one so d logits at iterating", "tokens": [50364, 2232, 25878, 284, 339, 575, 257, 2787, 41167, 2445, 300, 291, 393, 818, 293, 321, 528, 281, 3079, 264, 2787, 41167, 322, 264, 3565, 1208, 50620, 50620, 293, 321, 528, 281, 352, 294, 264, 10139, 300, 307, 472, 370, 1936, 321, 528, 281, 360, 264, 2787, 41167, 2051, 264, 13241, 50984, 50984, 295, 613, 3565, 1208, 550, 412, 264, 3006, 8432, 321, 643, 281, 16390, 257, 472, 370, 274, 3565, 1208, 412, 17138, 990, 51372, 51372, 670, 439, 264, 13241, 293, 8186, 278, 666, 264, 13766, 5649, 538, 264, 3006, 16949, 1854, 288, 65, 321, 643, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.12127971649169922, "compression_ratio": 1.955223880597015, "no_speech_prob": 5.771747510152636e-06}, {"id": 879, "seek": 549080, "start": 5510.96, "end": 5518.72, "text": " over all the rows and indexing into the columns provided by the correct labels inside yb we need", "tokens": [50364, 2232, 25878, 284, 339, 575, 257, 2787, 41167, 2445, 300, 291, 393, 818, 293, 321, 528, 281, 3079, 264, 2787, 41167, 322, 264, 3565, 1208, 50620, 50620, 293, 321, 528, 281, 352, 294, 264, 10139, 300, 307, 472, 370, 1936, 321, 528, 281, 360, 264, 2787, 41167, 2051, 264, 13241, 50984, 50984, 295, 613, 3565, 1208, 550, 412, 264, 3006, 8432, 321, 643, 281, 16390, 257, 472, 370, 274, 3565, 1208, 412, 17138, 990, 51372, 51372, 670, 439, 264, 13241, 293, 8186, 278, 666, 264, 13766, 5649, 538, 264, 3006, 16949, 1854, 288, 65, 321, 643, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.12127971649169922, "compression_ratio": 1.955223880597015, "no_speech_prob": 5.771747510152636e-06}, {"id": 880, "seek": 551872, "start": 5518.72, "end": 5525.280000000001, "text": " to subtract one and then finally it's the average loss that is the loss and in the average there's a", "tokens": [50364, 281, 16390, 472, 293, 550, 2721, 309, 311, 264, 4274, 4470, 300, 307, 264, 4470, 293, 294, 264, 4274, 456, 311, 257, 50692, 50692, 472, 670, 297, 295, 439, 264, 15352, 3869, 493, 293, 370, 321, 643, 281, 611, 646, 48256, 807, 300, 10044, 50980, 51020, 370, 264, 16235, 575, 281, 312, 36039, 760, 538, 538, 297, 382, 731, 570, 295, 264, 914, 51264, 51308, 457, 341, 5911, 820, 312, 264, 1874, 370, 586, 498, 321, 16888, 341, 321, 536, 300, 321, 500, 380, 483, 364, 1900, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.135496946481558, "compression_ratio": 1.7417840375586855, "no_speech_prob": 1.4593510968552437e-06}, {"id": 881, "seek": 551872, "start": 5525.280000000001, "end": 5531.04, "text": " one over n of all the losses added up and so we need to also back propagate through that division", "tokens": [50364, 281, 16390, 472, 293, 550, 2721, 309, 311, 264, 4274, 4470, 300, 307, 264, 4470, 293, 294, 264, 4274, 456, 311, 257, 50692, 50692, 472, 670, 297, 295, 439, 264, 15352, 3869, 493, 293, 370, 321, 643, 281, 611, 646, 48256, 807, 300, 10044, 50980, 51020, 370, 264, 16235, 575, 281, 312, 36039, 760, 538, 538, 297, 382, 731, 570, 295, 264, 914, 51264, 51308, 457, 341, 5911, 820, 312, 264, 1874, 370, 586, 498, 321, 16888, 341, 321, 536, 300, 321, 500, 380, 483, 364, 1900, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.135496946481558, "compression_ratio": 1.7417840375586855, "no_speech_prob": 1.4593510968552437e-06}, {"id": 882, "seek": 551872, "start": 5531.84, "end": 5536.72, "text": " so the gradient has to be scaled down by by n as well because of the mean", "tokens": [50364, 281, 16390, 472, 293, 550, 2721, 309, 311, 264, 4274, 4470, 300, 307, 264, 4470, 293, 294, 264, 4274, 456, 311, 257, 50692, 50692, 472, 670, 297, 295, 439, 264, 15352, 3869, 493, 293, 370, 321, 643, 281, 611, 646, 48256, 807, 300, 10044, 50980, 51020, 370, 264, 16235, 575, 281, 312, 36039, 760, 538, 538, 297, 382, 731, 570, 295, 264, 914, 51264, 51308, 457, 341, 5911, 820, 312, 264, 1874, 370, 586, 498, 321, 16888, 341, 321, 536, 300, 321, 500, 380, 483, 364, 1900, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.135496946481558, "compression_ratio": 1.7417840375586855, "no_speech_prob": 1.4593510968552437e-06}, {"id": 883, "seek": 551872, "start": 5537.6, "end": 5543.92, "text": " but this otherwise should be the result so now if we verify this we see that we don't get an exact", "tokens": [50364, 281, 16390, 472, 293, 550, 2721, 309, 311, 264, 4274, 4470, 300, 307, 264, 4470, 293, 294, 264, 4274, 456, 311, 257, 50692, 50692, 472, 670, 297, 295, 439, 264, 15352, 3869, 493, 293, 370, 321, 643, 281, 611, 646, 48256, 807, 300, 10044, 50980, 51020, 370, 264, 16235, 575, 281, 312, 36039, 760, 538, 538, 297, 382, 731, 570, 295, 264, 914, 51264, 51308, 457, 341, 5911, 820, 312, 264, 1874, 370, 586, 498, 321, 16888, 341, 321, 536, 300, 321, 500, 380, 483, 364, 1900, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.135496946481558, "compression_ratio": 1.7417840375586855, "no_speech_prob": 1.4593510968552437e-06}, {"id": 884, "seek": 554392, "start": 5543.92, "end": 5551.84, "text": " match but at the same time the maximum difference from logits from pytorch and rd logits here", "tokens": [50364, 2995, 457, 412, 264, 912, 565, 264, 6674, 2649, 490, 3565, 1208, 490, 25878, 284, 339, 293, 367, 67, 3565, 1208, 510, 50760, 50788, 307, 322, 264, 1668, 295, 1025, 68, 3671, 1722, 370, 309, 311, 257, 5870, 5870, 1230, 370, 570, 295, 15114, 935, 1582, 74, 1324, 51112, 51112, 321, 500, 380, 483, 264, 1900, 857, 3711, 1874, 457, 321, 1936, 483, 264, 3006, 1867, 51348, 51412, 10447, 586, 741, 1116, 411, 281, 10465, 510, 10515, 949, 321, 1286, 322, 281, 264, 958, 5380, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.15779508097787923, "compression_ratio": 1.6153846153846154, "no_speech_prob": 3.5558878153096884e-06}, {"id": 885, "seek": 554392, "start": 5552.4, "end": 5558.88, "text": " is on the order of 5e negative 9 so it's a tiny tiny number so because of loading point wonkiness", "tokens": [50364, 2995, 457, 412, 264, 912, 565, 264, 6674, 2649, 490, 3565, 1208, 490, 25878, 284, 339, 293, 367, 67, 3565, 1208, 510, 50760, 50788, 307, 322, 264, 1668, 295, 1025, 68, 3671, 1722, 370, 309, 311, 257, 5870, 5870, 1230, 370, 570, 295, 15114, 935, 1582, 74, 1324, 51112, 51112, 321, 500, 380, 483, 264, 1900, 857, 3711, 1874, 457, 321, 1936, 483, 264, 3006, 1867, 51348, 51412, 10447, 586, 741, 1116, 411, 281, 10465, 510, 10515, 949, 321, 1286, 322, 281, 264, 958, 5380, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.15779508097787923, "compression_ratio": 1.6153846153846154, "no_speech_prob": 3.5558878153096884e-06}, {"id": 886, "seek": 554392, "start": 5558.88, "end": 5563.6, "text": " we don't get the exact bitwise result but we basically get the correct answer", "tokens": [50364, 2995, 457, 412, 264, 912, 565, 264, 6674, 2649, 490, 3565, 1208, 490, 25878, 284, 339, 293, 367, 67, 3565, 1208, 510, 50760, 50788, 307, 322, 264, 1668, 295, 1025, 68, 3671, 1722, 370, 309, 311, 257, 5870, 5870, 1230, 370, 570, 295, 15114, 935, 1582, 74, 1324, 51112, 51112, 321, 500, 380, 483, 264, 1900, 857, 3711, 1874, 457, 321, 1936, 483, 264, 3006, 1867, 51348, 51412, 10447, 586, 741, 1116, 411, 281, 10465, 510, 10515, 949, 321, 1286, 322, 281, 264, 958, 5380, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.15779508097787923, "compression_ratio": 1.6153846153846154, "no_speech_prob": 3.5558878153096884e-06}, {"id": 887, "seek": 554392, "start": 5564.88, "end": 5569.84, "text": " approximately now i'd like to pause here briefly before we move on to the next exercise", "tokens": [50364, 2995, 457, 412, 264, 912, 565, 264, 6674, 2649, 490, 3565, 1208, 490, 25878, 284, 339, 293, 367, 67, 3565, 1208, 510, 50760, 50788, 307, 322, 264, 1668, 295, 1025, 68, 3671, 1722, 370, 309, 311, 257, 5870, 5870, 1230, 370, 570, 295, 15114, 935, 1582, 74, 1324, 51112, 51112, 321, 500, 380, 483, 264, 1900, 857, 3711, 1874, 457, 321, 1936, 483, 264, 3006, 1867, 51348, 51412, 10447, 586, 741, 1116, 411, 281, 10465, 510, 10515, 949, 321, 1286, 322, 281, 264, 958, 5380, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.15779508097787923, "compression_ratio": 1.6153846153846154, "no_speech_prob": 3.5558878153096884e-06}, {"id": 888, "seek": 556984, "start": 5569.84, "end": 5574.56, "text": " because i'd like us to get an intuitive sense of what d logits is because it has a beautiful and", "tokens": [50364, 570, 741, 1116, 411, 505, 281, 483, 364, 21769, 2020, 295, 437, 274, 3565, 1208, 307, 570, 309, 575, 257, 2238, 293, 50600, 50600, 588, 2199, 10835, 6095, 370, 510, 741, 478, 1940, 264, 3565, 1208, 293, 741, 478, 5056, 3319, 309, 293, 321, 50952, 50952, 393, 536, 300, 321, 362, 257, 15245, 295, 8858, 5110, 295, 7634, 4342, 293, 437, 307, 264, 3565, 1208, 46506, 51248, 51248, 558, 264, 3565, 1208, 307, 264, 33783, 300, 264, 33783, 8141, 294, 257, 2128, 1320, 457, 550, 51536, 51536, 510, 613, 2211, 19368, 366, 264, 8432, 295, 264, 3006, 43840, 689, 321, 16390, 257, 472, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.14260101318359375, "compression_ratio": 1.8007662835249043, "no_speech_prob": 3.6687754345621215e-06}, {"id": 889, "seek": 556984, "start": 5574.56, "end": 5581.6, "text": " very simple explanation honestly so here i'm taking the logits and i'm visualizing it and we", "tokens": [50364, 570, 741, 1116, 411, 505, 281, 483, 364, 21769, 2020, 295, 437, 274, 3565, 1208, 307, 570, 309, 575, 257, 2238, 293, 50600, 50600, 588, 2199, 10835, 6095, 370, 510, 741, 478, 1940, 264, 3565, 1208, 293, 741, 478, 5056, 3319, 309, 293, 321, 50952, 50952, 393, 536, 300, 321, 362, 257, 15245, 295, 8858, 5110, 295, 7634, 4342, 293, 437, 307, 264, 3565, 1208, 46506, 51248, 51248, 558, 264, 3565, 1208, 307, 264, 33783, 300, 264, 33783, 8141, 294, 257, 2128, 1320, 457, 550, 51536, 51536, 510, 613, 2211, 19368, 366, 264, 8432, 295, 264, 3006, 43840, 689, 321, 16390, 257, 472, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.14260101318359375, "compression_ratio": 1.8007662835249043, "no_speech_prob": 3.6687754345621215e-06}, {"id": 890, "seek": 556984, "start": 5581.6, "end": 5587.52, "text": " can see that we have a batch of 32 examples of 27 characters and what is the logits intuitively", "tokens": [50364, 570, 741, 1116, 411, 505, 281, 483, 364, 21769, 2020, 295, 437, 274, 3565, 1208, 307, 570, 309, 575, 257, 2238, 293, 50600, 50600, 588, 2199, 10835, 6095, 370, 510, 741, 478, 1940, 264, 3565, 1208, 293, 741, 478, 5056, 3319, 309, 293, 321, 50952, 50952, 393, 536, 300, 321, 362, 257, 15245, 295, 8858, 5110, 295, 7634, 4342, 293, 437, 307, 264, 3565, 1208, 46506, 51248, 51248, 558, 264, 3565, 1208, 307, 264, 33783, 300, 264, 33783, 8141, 294, 257, 2128, 1320, 457, 550, 51536, 51536, 510, 613, 2211, 19368, 366, 264, 8432, 295, 264, 3006, 43840, 689, 321, 16390, 257, 472, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.14260101318359375, "compression_ratio": 1.8007662835249043, "no_speech_prob": 3.6687754345621215e-06}, {"id": 891, "seek": 556984, "start": 5587.52, "end": 5593.28, "text": " right the logits is the probabilities that the probabilities matrix in a forward pass but then", "tokens": [50364, 570, 741, 1116, 411, 505, 281, 483, 364, 21769, 2020, 295, 437, 274, 3565, 1208, 307, 570, 309, 575, 257, 2238, 293, 50600, 50600, 588, 2199, 10835, 6095, 370, 510, 741, 478, 1940, 264, 3565, 1208, 293, 741, 478, 5056, 3319, 309, 293, 321, 50952, 50952, 393, 536, 300, 321, 362, 257, 15245, 295, 8858, 5110, 295, 7634, 4342, 293, 437, 307, 264, 3565, 1208, 46506, 51248, 51248, 558, 264, 3565, 1208, 307, 264, 33783, 300, 264, 33783, 8141, 294, 257, 2128, 1320, 457, 550, 51536, 51536, 510, 613, 2211, 19368, 366, 264, 8432, 295, 264, 3006, 43840, 689, 321, 16390, 257, 472, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.14260101318359375, "compression_ratio": 1.8007662835249043, "no_speech_prob": 3.6687754345621215e-06}, {"id": 892, "seek": 556984, "start": 5593.28, "end": 5597.68, "text": " here these black squares are the positions of the correct indices where we subtract a one", "tokens": [50364, 570, 741, 1116, 411, 505, 281, 483, 364, 21769, 2020, 295, 437, 274, 3565, 1208, 307, 570, 309, 575, 257, 2238, 293, 50600, 50600, 588, 2199, 10835, 6095, 370, 510, 741, 478, 1940, 264, 3565, 1208, 293, 741, 478, 5056, 3319, 309, 293, 321, 50952, 50952, 393, 536, 300, 321, 362, 257, 15245, 295, 8858, 5110, 295, 7634, 4342, 293, 437, 307, 264, 3565, 1208, 46506, 51248, 51248, 558, 264, 3565, 1208, 307, 264, 33783, 300, 264, 33783, 8141, 294, 257, 2128, 1320, 457, 550, 51536, 51536, 510, 613, 2211, 19368, 366, 264, 8432, 295, 264, 3006, 43840, 689, 321, 16390, 257, 472, 51756, 51756], "temperature": 0.0, "avg_logprob": -0.14260101318359375, "compression_ratio": 1.8007662835249043, "no_speech_prob": 3.6687754345621215e-06}, {"id": 893, "seek": 559768, "start": 5597.68, "end": 5605.68, "text": " and so what is this doing right these are the derivatives on the logits and so let's look at", "tokens": [50364, 293, 370, 437, 307, 341, 884, 558, 613, 366, 264, 33733, 322, 264, 3565, 1208, 293, 370, 718, 311, 574, 412, 50764, 50764, 445, 264, 700, 5386, 510, 370, 300, 311, 437, 741, 478, 884, 510, 741, 478, 28258, 264, 33783, 295, 613, 51016, 51016, 3565, 1208, 293, 550, 741, 478, 1940, 445, 264, 700, 5386, 293, 341, 307, 264, 8482, 5386, 293, 550, 264, 3565, 1208, 51308, 51308, 295, 264, 700, 5386, 293, 30955, 538, 297, 445, 337, 505, 370, 300, 321, 500, 380, 362, 264, 21589, 538, 297, 294, 510, 51608, 51608, 293, 1203, 307, 544, 7302, 712, 370, 321, 434, 516, 281, 747, 264, 8482, 295, 264, 3565, 1208, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.24057424479517445, "compression_ratio": 2.0973451327433628, "no_speech_prob": 1.267888251277327e-06}, {"id": 894, "seek": 559768, "start": 5605.68, "end": 5610.72, "text": " just the first row here so that's what i'm doing here i'm calculating the probabilities of these", "tokens": [50364, 293, 370, 437, 307, 341, 884, 558, 613, 366, 264, 33733, 322, 264, 3565, 1208, 293, 370, 718, 311, 574, 412, 50764, 50764, 445, 264, 700, 5386, 510, 370, 300, 311, 437, 741, 478, 884, 510, 741, 478, 28258, 264, 33783, 295, 613, 51016, 51016, 3565, 1208, 293, 550, 741, 478, 1940, 445, 264, 700, 5386, 293, 341, 307, 264, 8482, 5386, 293, 550, 264, 3565, 1208, 51308, 51308, 295, 264, 700, 5386, 293, 30955, 538, 297, 445, 337, 505, 370, 300, 321, 500, 380, 362, 264, 21589, 538, 297, 294, 510, 51608, 51608, 293, 1203, 307, 544, 7302, 712, 370, 321, 434, 516, 281, 747, 264, 8482, 295, 264, 3565, 1208, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.24057424479517445, "compression_ratio": 2.0973451327433628, "no_speech_prob": 1.267888251277327e-06}, {"id": 895, "seek": 559768, "start": 5610.72, "end": 5616.56, "text": " logits and then i'm taking just the first row and this is the probability row and then the logits", "tokens": [50364, 293, 370, 437, 307, 341, 884, 558, 613, 366, 264, 33733, 322, 264, 3565, 1208, 293, 370, 718, 311, 574, 412, 50764, 50764, 445, 264, 700, 5386, 510, 370, 300, 311, 437, 741, 478, 884, 510, 741, 478, 28258, 264, 33783, 295, 613, 51016, 51016, 3565, 1208, 293, 550, 741, 478, 1940, 445, 264, 700, 5386, 293, 341, 307, 264, 8482, 5386, 293, 550, 264, 3565, 1208, 51308, 51308, 295, 264, 700, 5386, 293, 30955, 538, 297, 445, 337, 505, 370, 300, 321, 500, 380, 362, 264, 21589, 538, 297, 294, 510, 51608, 51608, 293, 1203, 307, 544, 7302, 712, 370, 321, 434, 516, 281, 747, 264, 8482, 295, 264, 3565, 1208, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.24057424479517445, "compression_ratio": 2.0973451327433628, "no_speech_prob": 1.267888251277327e-06}, {"id": 896, "seek": 559768, "start": 5616.56, "end": 5622.56, "text": " of the first row and multiplying by n just for us so that we don't have the scaling by n in here", "tokens": [50364, 293, 370, 437, 307, 341, 884, 558, 613, 366, 264, 33733, 322, 264, 3565, 1208, 293, 370, 718, 311, 574, 412, 50764, 50764, 445, 264, 700, 5386, 510, 370, 300, 311, 437, 741, 478, 884, 510, 741, 478, 28258, 264, 33783, 295, 613, 51016, 51016, 3565, 1208, 293, 550, 741, 478, 1940, 445, 264, 700, 5386, 293, 341, 307, 264, 8482, 5386, 293, 550, 264, 3565, 1208, 51308, 51308, 295, 264, 700, 5386, 293, 30955, 538, 297, 445, 337, 505, 370, 300, 321, 500, 380, 362, 264, 21589, 538, 297, 294, 510, 51608, 51608, 293, 1203, 307, 544, 7302, 712, 370, 321, 434, 516, 281, 747, 264, 8482, 295, 264, 3565, 1208, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.24057424479517445, "compression_ratio": 2.0973451327433628, "no_speech_prob": 1.267888251277327e-06}, {"id": 897, "seek": 559768, "start": 5622.56, "end": 5627.52, "text": " and everything is more interpretable so we're going to take the probability of the logits", "tokens": [50364, 293, 370, 437, 307, 341, 884, 558, 613, 366, 264, 33733, 322, 264, 3565, 1208, 293, 370, 718, 311, 574, 412, 50764, 50764, 445, 264, 700, 5386, 510, 370, 300, 311, 437, 741, 478, 884, 510, 741, 478, 28258, 264, 33783, 295, 613, 51016, 51016, 3565, 1208, 293, 550, 741, 478, 1940, 445, 264, 700, 5386, 293, 341, 307, 264, 8482, 5386, 293, 550, 264, 3565, 1208, 51308, 51308, 295, 264, 700, 5386, 293, 30955, 538, 297, 445, 337, 505, 370, 300, 321, 500, 380, 362, 264, 21589, 538, 297, 294, 510, 51608, 51608, 293, 1203, 307, 544, 7302, 712, 370, 321, 434, 516, 281, 747, 264, 8482, 295, 264, 3565, 1208, 51856, 51856], "temperature": 0.0, "avg_logprob": -0.24057424479517445, "compression_ratio": 2.0973451327433628, "no_speech_prob": 1.267888251277327e-06}, {"id": 898, "seek": 562752, "start": 5627.52, "end": 5632.4800000000005, "text": " and we see that it's exactly equal to the probability of course but then the position of", "tokens": [50364, 293, 321, 536, 300, 309, 311, 2293, 2681, 281, 264, 8482, 295, 1164, 457, 550, 264, 2535, 295, 50612, 50612, 264, 3006, 8186, 575, 257, 3175, 6915, 472, 370, 3175, 472, 322, 300, 2535, 293, 370, 3449, 300, 50904, 50948, 498, 291, 747, 264, 3565, 1208, 412, 4018, 293, 291, 2408, 309, 309, 767, 34499, 281, 4018, 293, 370, 291, 820, 519, 295, 51316, 51316, 613, 2771, 2448, 510, 412, 1184, 2815, 382, 411, 257, 3464, 321, 366, 516, 281, 312, 1936, 8407, 760, 322, 264, 51792, 51792], "temperature": 0.4, "avg_logprob": -0.11477300855848524, "compression_ratio": 1.7361111111111112, "no_speech_prob": 1.5056820075187716e-06}, {"id": 899, "seek": 562752, "start": 5632.4800000000005, "end": 5638.320000000001, "text": " the correct index has a minus equals one so minus one on that position and so notice that", "tokens": [50364, 293, 321, 536, 300, 309, 311, 2293, 2681, 281, 264, 8482, 295, 1164, 457, 550, 264, 2535, 295, 50612, 50612, 264, 3006, 8186, 575, 257, 3175, 6915, 472, 370, 3175, 472, 322, 300, 2535, 293, 370, 3449, 300, 50904, 50948, 498, 291, 747, 264, 3565, 1208, 412, 4018, 293, 291, 2408, 309, 309, 767, 34499, 281, 4018, 293, 370, 291, 820, 519, 295, 51316, 51316, 613, 2771, 2448, 510, 412, 1184, 2815, 382, 411, 257, 3464, 321, 366, 516, 281, 312, 1936, 8407, 760, 322, 264, 51792, 51792], "temperature": 0.4, "avg_logprob": -0.11477300855848524, "compression_ratio": 1.7361111111111112, "no_speech_prob": 1.5056820075187716e-06}, {"id": 900, "seek": 562752, "start": 5639.200000000001, "end": 5646.56, "text": " if you take the logits at zero and you sum it it actually sums to zero and so you should think of", "tokens": [50364, 293, 321, 536, 300, 309, 311, 2293, 2681, 281, 264, 8482, 295, 1164, 457, 550, 264, 2535, 295, 50612, 50612, 264, 3006, 8186, 575, 257, 3175, 6915, 472, 370, 3175, 472, 322, 300, 2535, 293, 370, 3449, 300, 50904, 50948, 498, 291, 747, 264, 3565, 1208, 412, 4018, 293, 291, 2408, 309, 309, 767, 34499, 281, 4018, 293, 370, 291, 820, 519, 295, 51316, 51316, 613, 2771, 2448, 510, 412, 1184, 2815, 382, 411, 257, 3464, 321, 366, 516, 281, 312, 1936, 8407, 760, 322, 264, 51792, 51792], "temperature": 0.4, "avg_logprob": -0.11477300855848524, "compression_ratio": 1.7361111111111112, "no_speech_prob": 1.5056820075187716e-06}, {"id": 901, "seek": 562752, "start": 5646.56, "end": 5656.080000000001, "text": " these gradients here at each cell as like a force we are going to be basically pulling down on the", "tokens": [50364, 293, 321, 536, 300, 309, 311, 2293, 2681, 281, 264, 8482, 295, 1164, 457, 550, 264, 2535, 295, 50612, 50612, 264, 3006, 8186, 575, 257, 3175, 6915, 472, 370, 3175, 472, 322, 300, 2535, 293, 370, 3449, 300, 50904, 50948, 498, 291, 747, 264, 3565, 1208, 412, 4018, 293, 291, 2408, 309, 309, 767, 34499, 281, 4018, 293, 370, 291, 820, 519, 295, 51316, 51316, 613, 2771, 2448, 510, 412, 1184, 2815, 382, 411, 257, 3464, 321, 366, 516, 281, 312, 1936, 8407, 760, 322, 264, 51792, 51792], "temperature": 0.4, "avg_logprob": -0.11477300855848524, "compression_ratio": 1.7361111111111112, "no_speech_prob": 1.5056820075187716e-06}, {"id": 902, "seek": 565608, "start": 5656.08, "end": 5661.2, "text": " probabilities of the incorrect characters and we're going to be pulling up on the probability", "tokens": [50364, 33783, 295, 264, 18424, 4342, 293, 321, 434, 516, 281, 312, 8407, 493, 322, 264, 8482, 50620, 50656, 412, 264, 3006, 8186, 293, 300, 311, 437, 311, 1936, 2737, 294, 1184, 5386, 293, 264, 264, 2372, 295, 2944, 51064, 51064, 293, 2235, 307, 2293, 2681, 1602, 570, 264, 2408, 307, 4018, 370, 264, 2372, 281, 597, 321, 2235, 760, 51344, 51344, 294, 264, 33783, 293, 550, 264, 2372, 300, 321, 2944, 493, 322, 264, 8482, 295, 264, 3006, 2517, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.06524455978209714, "compression_ratio": 1.9489795918367347, "no_speech_prob": 4.425135557539761e-06}, {"id": 903, "seek": 565608, "start": 5661.92, "end": 5670.08, "text": " at the correct index and that's what's basically happening in each row and the the amount of push", "tokens": [50364, 33783, 295, 264, 18424, 4342, 293, 321, 434, 516, 281, 312, 8407, 493, 322, 264, 8482, 50620, 50656, 412, 264, 3006, 8186, 293, 300, 311, 437, 311, 1936, 2737, 294, 1184, 5386, 293, 264, 264, 2372, 295, 2944, 51064, 51064, 293, 2235, 307, 2293, 2681, 1602, 570, 264, 2408, 307, 4018, 370, 264, 2372, 281, 597, 321, 2235, 760, 51344, 51344, 294, 264, 33783, 293, 550, 264, 2372, 300, 321, 2944, 493, 322, 264, 8482, 295, 264, 3006, 2517, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.06524455978209714, "compression_ratio": 1.9489795918367347, "no_speech_prob": 4.425135557539761e-06}, {"id": 904, "seek": 565608, "start": 5670.08, "end": 5675.68, "text": " and pull is exactly equalized because the sum is zero so the amount to which we pull down", "tokens": [50364, 33783, 295, 264, 18424, 4342, 293, 321, 434, 516, 281, 312, 8407, 493, 322, 264, 8482, 50620, 50656, 412, 264, 3006, 8186, 293, 300, 311, 437, 311, 1936, 2737, 294, 1184, 5386, 293, 264, 264, 2372, 295, 2944, 51064, 51064, 293, 2235, 307, 2293, 2681, 1602, 570, 264, 2408, 307, 4018, 370, 264, 2372, 281, 597, 321, 2235, 760, 51344, 51344, 294, 264, 33783, 293, 550, 264, 2372, 300, 321, 2944, 493, 322, 264, 8482, 295, 264, 3006, 2517, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.06524455978209714, "compression_ratio": 1.9489795918367347, "no_speech_prob": 4.425135557539761e-06}, {"id": 905, "seek": 565608, "start": 5675.68, "end": 5680.0, "text": " in the probabilities and then the amount that we push up on the probability of the correct character", "tokens": [50364, 33783, 295, 264, 18424, 4342, 293, 321, 434, 516, 281, 312, 8407, 493, 322, 264, 8482, 50620, 50656, 412, 264, 3006, 8186, 293, 300, 311, 437, 311, 1936, 2737, 294, 1184, 5386, 293, 264, 264, 2372, 295, 2944, 51064, 51064, 293, 2235, 307, 2293, 2681, 1602, 570, 264, 2408, 307, 4018, 370, 264, 2372, 281, 597, 321, 2235, 760, 51344, 51344, 294, 264, 33783, 293, 550, 264, 2372, 300, 321, 2944, 493, 322, 264, 8482, 295, 264, 3006, 2517, 51560, 51560], "temperature": 0.0, "avg_logprob": -0.06524455978209714, "compression_ratio": 1.9489795918367347, "no_speech_prob": 4.425135557539761e-06}, {"id": 906, "seek": 568000, "start": 5680.0, "end": 5686.56, "text": " is equal so sort of the repulsion and the attraction are equal and think of the neural net now as a", "tokens": [50364, 307, 2681, 370, 1333, 295, 264, 1085, 22973, 293, 264, 17672, 366, 2681, 293, 519, 295, 264, 18161, 2533, 586, 382, 257, 50692, 50692, 411, 257, 5994, 48399, 1185, 420, 746, 411, 300, 321, 434, 493, 510, 322, 1192, 295, 264, 3565, 1208, 293, 321, 434, 50980, 50980, 8407, 493, 321, 434, 8407, 760, 264, 33783, 295, 18424, 293, 8407, 493, 264, 8482, 295, 51172, 51172, 264, 3006, 293, 294, 341, 6179, 48399, 1185, 570, 1203, 307, 44003, 51408, 51408, 445, 9540, 445, 519, 295, 309, 382, 1333, 295, 411, 341, 8980, 35030, 281, 341, 16060, 990, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.05680816952544864, "compression_ratio": 2.0213675213675213, "no_speech_prob": 4.029175670439145e-06}, {"id": 907, "seek": 568000, "start": 5686.56, "end": 5692.32, "text": " like a massive pulley system or something like that we're up here on top of the logits and we're", "tokens": [50364, 307, 2681, 370, 1333, 295, 264, 1085, 22973, 293, 264, 17672, 366, 2681, 293, 519, 295, 264, 18161, 2533, 586, 382, 257, 50692, 50692, 411, 257, 5994, 48399, 1185, 420, 746, 411, 300, 321, 434, 493, 510, 322, 1192, 295, 264, 3565, 1208, 293, 321, 434, 50980, 50980, 8407, 493, 321, 434, 8407, 760, 264, 33783, 295, 18424, 293, 8407, 493, 264, 8482, 295, 51172, 51172, 264, 3006, 293, 294, 341, 6179, 48399, 1185, 570, 1203, 307, 44003, 51408, 51408, 445, 9540, 445, 519, 295, 309, 382, 1333, 295, 411, 341, 8980, 35030, 281, 341, 16060, 990, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.05680816952544864, "compression_ratio": 2.0213675213675213, "no_speech_prob": 4.029175670439145e-06}, {"id": 908, "seek": 568000, "start": 5692.32, "end": 5696.16, "text": " pulling up we're pulling down the probabilities of incorrect and pulling up the probability of", "tokens": [50364, 307, 2681, 370, 1333, 295, 264, 1085, 22973, 293, 264, 17672, 366, 2681, 293, 519, 295, 264, 18161, 2533, 586, 382, 257, 50692, 50692, 411, 257, 5994, 48399, 1185, 420, 746, 411, 300, 321, 434, 493, 510, 322, 1192, 295, 264, 3565, 1208, 293, 321, 434, 50980, 50980, 8407, 493, 321, 434, 8407, 760, 264, 33783, 295, 18424, 293, 8407, 493, 264, 8482, 295, 51172, 51172, 264, 3006, 293, 294, 341, 6179, 48399, 1185, 570, 1203, 307, 44003, 51408, 51408, 445, 9540, 445, 519, 295, 309, 382, 1333, 295, 411, 341, 8980, 35030, 281, 341, 16060, 990, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.05680816952544864, "compression_ratio": 2.0213675213675213, "no_speech_prob": 4.029175670439145e-06}, {"id": 909, "seek": 568000, "start": 5696.16, "end": 5700.88, "text": " the correct and in this complicated pulley system because everything is mathematically", "tokens": [50364, 307, 2681, 370, 1333, 295, 264, 1085, 22973, 293, 264, 17672, 366, 2681, 293, 519, 295, 264, 18161, 2533, 586, 382, 257, 50692, 50692, 411, 257, 5994, 48399, 1185, 420, 746, 411, 300, 321, 434, 493, 510, 322, 1192, 295, 264, 3565, 1208, 293, 321, 434, 50980, 50980, 8407, 493, 321, 434, 8407, 760, 264, 33783, 295, 18424, 293, 8407, 493, 264, 8482, 295, 51172, 51172, 264, 3006, 293, 294, 341, 6179, 48399, 1185, 570, 1203, 307, 44003, 51408, 51408, 445, 9540, 445, 519, 295, 309, 382, 1333, 295, 411, 341, 8980, 35030, 281, 341, 16060, 990, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.05680816952544864, "compression_ratio": 2.0213675213675213, "no_speech_prob": 4.029175670439145e-06}, {"id": 910, "seek": 568000, "start": 5700.88, "end": 5705.76, "text": " just determined just think of it as sort of like this tension translating to this complicating", "tokens": [50364, 307, 2681, 370, 1333, 295, 264, 1085, 22973, 293, 264, 17672, 366, 2681, 293, 519, 295, 264, 18161, 2533, 586, 382, 257, 50692, 50692, 411, 257, 5994, 48399, 1185, 420, 746, 411, 300, 321, 434, 493, 510, 322, 1192, 295, 264, 3565, 1208, 293, 321, 434, 50980, 50980, 8407, 493, 321, 434, 8407, 760, 264, 33783, 295, 18424, 293, 8407, 493, 264, 8482, 295, 51172, 51172, 264, 3006, 293, 294, 341, 6179, 48399, 1185, 570, 1203, 307, 44003, 51408, 51408, 445, 9540, 445, 519, 295, 309, 382, 1333, 295, 411, 341, 8980, 35030, 281, 341, 16060, 990, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.05680816952544864, "compression_ratio": 2.0213675213675213, "no_speech_prob": 4.029175670439145e-06}, {"id": 911, "seek": 570576, "start": 5705.76, "end": 5711.12, "text": " pulley mechanism and then eventually we get a tug on the weights and the biases and basically in", "tokens": [50364, 48399, 7513, 293, 550, 4728, 321, 483, 257, 33543, 322, 264, 17443, 293, 264, 32152, 293, 1936, 294, 50632, 50632, 1184, 5623, 321, 445, 733, 295, 411, 33543, 294, 264, 3513, 300, 321, 411, 337, 1184, 295, 613, 4959, 50860, 50860, 293, 264, 9834, 366, 5692, 2212, 294, 281, 264, 33543, 293, 300, 311, 437, 3097, 257, 18161, 2533, 733, 51088, 51088, 295, 411, 1542, 411, 322, 257, 1090, 1496, 293, 370, 741, 519, 264, 264, 5874, 295, 2944, 293, 2235, 294, 613, 2771, 2448, 51388, 51388, 366, 767, 2232, 588, 21769, 510, 321, 434, 7380, 293, 8407, 322, 264, 3006, 1867, 293, 264, 18424, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.0396647059589351, "compression_ratio": 1.8910505836575875, "no_speech_prob": 1.0782768185890745e-05}, {"id": 912, "seek": 570576, "start": 5711.12, "end": 5715.68, "text": " each update we just kind of like tug in the direction that we like for each of these elements", "tokens": [50364, 48399, 7513, 293, 550, 4728, 321, 483, 257, 33543, 322, 264, 17443, 293, 264, 32152, 293, 1936, 294, 50632, 50632, 1184, 5623, 321, 445, 733, 295, 411, 33543, 294, 264, 3513, 300, 321, 411, 337, 1184, 295, 613, 4959, 50860, 50860, 293, 264, 9834, 366, 5692, 2212, 294, 281, 264, 33543, 293, 300, 311, 437, 3097, 257, 18161, 2533, 733, 51088, 51088, 295, 411, 1542, 411, 322, 257, 1090, 1496, 293, 370, 741, 519, 264, 264, 5874, 295, 2944, 293, 2235, 294, 613, 2771, 2448, 51388, 51388, 366, 767, 2232, 588, 21769, 510, 321, 434, 7380, 293, 8407, 322, 264, 3006, 1867, 293, 264, 18424, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.0396647059589351, "compression_ratio": 1.8910505836575875, "no_speech_prob": 1.0782768185890745e-05}, {"id": 913, "seek": 570576, "start": 5715.68, "end": 5720.24, "text": " and the parameters are slowly given in to the tug and that's what training a neural net kind", "tokens": [50364, 48399, 7513, 293, 550, 4728, 321, 483, 257, 33543, 322, 264, 17443, 293, 264, 32152, 293, 1936, 294, 50632, 50632, 1184, 5623, 321, 445, 733, 295, 411, 33543, 294, 264, 3513, 300, 321, 411, 337, 1184, 295, 613, 4959, 50860, 50860, 293, 264, 9834, 366, 5692, 2212, 294, 281, 264, 33543, 293, 300, 311, 437, 3097, 257, 18161, 2533, 733, 51088, 51088, 295, 411, 1542, 411, 322, 257, 1090, 1496, 293, 370, 741, 519, 264, 264, 5874, 295, 2944, 293, 2235, 294, 613, 2771, 2448, 51388, 51388, 366, 767, 2232, 588, 21769, 510, 321, 434, 7380, 293, 8407, 322, 264, 3006, 1867, 293, 264, 18424, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.0396647059589351, "compression_ratio": 1.8910505836575875, "no_speech_prob": 1.0782768185890745e-05}, {"id": 914, "seek": 570576, "start": 5720.24, "end": 5726.24, "text": " of like looks like on a high level and so i think the the forces of push and pull in these gradients", "tokens": [50364, 48399, 7513, 293, 550, 4728, 321, 483, 257, 33543, 322, 264, 17443, 293, 264, 32152, 293, 1936, 294, 50632, 50632, 1184, 5623, 321, 445, 733, 295, 411, 33543, 294, 264, 3513, 300, 321, 411, 337, 1184, 295, 613, 4959, 50860, 50860, 293, 264, 9834, 366, 5692, 2212, 294, 281, 264, 33543, 293, 300, 311, 437, 3097, 257, 18161, 2533, 733, 51088, 51088, 295, 411, 1542, 411, 322, 257, 1090, 1496, 293, 370, 741, 519, 264, 264, 5874, 295, 2944, 293, 2235, 294, 613, 2771, 2448, 51388, 51388, 366, 767, 2232, 588, 21769, 510, 321, 434, 7380, 293, 8407, 322, 264, 3006, 1867, 293, 264, 18424, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.0396647059589351, "compression_ratio": 1.8910505836575875, "no_speech_prob": 1.0782768185890745e-05}, {"id": 915, "seek": 570576, "start": 5726.24, "end": 5731.2, "text": " are actually uh very intuitive here we're pushing and pulling on the correct answer and the incorrect", "tokens": [50364, 48399, 7513, 293, 550, 4728, 321, 483, 257, 33543, 322, 264, 17443, 293, 264, 32152, 293, 1936, 294, 50632, 50632, 1184, 5623, 321, 445, 733, 295, 411, 33543, 294, 264, 3513, 300, 321, 411, 337, 1184, 295, 613, 4959, 50860, 50860, 293, 264, 9834, 366, 5692, 2212, 294, 281, 264, 33543, 293, 300, 311, 437, 3097, 257, 18161, 2533, 733, 51088, 51088, 295, 411, 1542, 411, 322, 257, 1090, 1496, 293, 370, 741, 519, 264, 264, 5874, 295, 2944, 293, 2235, 294, 613, 2771, 2448, 51388, 51388, 366, 767, 2232, 588, 21769, 510, 321, 434, 7380, 293, 8407, 322, 264, 3006, 1867, 293, 264, 18424, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.0396647059589351, "compression_ratio": 1.8910505836575875, "no_speech_prob": 1.0782768185890745e-05}, {"id": 916, "seek": 573120, "start": 5731.2, "end": 5737.599999999999, "text": " answers and the amount of force that we're applying is actually proportional to the probabilities that", "tokens": [50364, 6338, 293, 264, 2372, 295, 3464, 300, 321, 434, 9275, 307, 767, 24969, 281, 264, 33783, 300, 50684, 50684, 1361, 484, 294, 264, 2128, 1320, 293, 370, 337, 1365, 498, 527, 33783, 1361, 484, 2293, 3006, 50940, 50940, 370, 436, 576, 362, 632, 4018, 5315, 3993, 337, 472, 412, 264, 3006, 2535, 550, 264, 264, 3565, 1208, 51252, 51252, 576, 312, 439, 257, 5386, 295, 35193, 337, 300, 1365, 456, 576, 312, 572, 2944, 293, 2235, 370, 264, 2372, 281, 597, 51568, 51568, 428, 17630, 307, 18424, 307, 2293, 264, 2372, 538, 597, 291, 434, 516, 281, 483, 257, 2235, 420, 257, 2944, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.04730383703641802, "compression_ratio": 1.9254901960784314, "no_speech_prob": 5.014471298636636e-06}, {"id": 917, "seek": 573120, "start": 5737.599999999999, "end": 5742.72, "text": " came out in the forward pass and so for example if our probabilities came out exactly correct", "tokens": [50364, 6338, 293, 264, 2372, 295, 3464, 300, 321, 434, 9275, 307, 767, 24969, 281, 264, 33783, 300, 50684, 50684, 1361, 484, 294, 264, 2128, 1320, 293, 370, 337, 1365, 498, 527, 33783, 1361, 484, 2293, 3006, 50940, 50940, 370, 436, 576, 362, 632, 4018, 5315, 3993, 337, 472, 412, 264, 3006, 2535, 550, 264, 264, 3565, 1208, 51252, 51252, 576, 312, 439, 257, 5386, 295, 35193, 337, 300, 1365, 456, 576, 312, 572, 2944, 293, 2235, 370, 264, 2372, 281, 597, 51568, 51568, 428, 17630, 307, 18424, 307, 2293, 264, 2372, 538, 597, 291, 434, 516, 281, 483, 257, 2235, 420, 257, 2944, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.04730383703641802, "compression_ratio": 1.9254901960784314, "no_speech_prob": 5.014471298636636e-06}, {"id": 918, "seek": 573120, "start": 5742.72, "end": 5748.96, "text": " so they would have had zero everywhere except for one at the correct position then the the logits", "tokens": [50364, 6338, 293, 264, 2372, 295, 3464, 300, 321, 434, 9275, 307, 767, 24969, 281, 264, 33783, 300, 50684, 50684, 1361, 484, 294, 264, 2128, 1320, 293, 370, 337, 1365, 498, 527, 33783, 1361, 484, 2293, 3006, 50940, 50940, 370, 436, 576, 362, 632, 4018, 5315, 3993, 337, 472, 412, 264, 3006, 2535, 550, 264, 264, 3565, 1208, 51252, 51252, 576, 312, 439, 257, 5386, 295, 35193, 337, 300, 1365, 456, 576, 312, 572, 2944, 293, 2235, 370, 264, 2372, 281, 597, 51568, 51568, 428, 17630, 307, 18424, 307, 2293, 264, 2372, 538, 597, 291, 434, 516, 281, 483, 257, 2235, 420, 257, 2944, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.04730383703641802, "compression_ratio": 1.9254901960784314, "no_speech_prob": 5.014471298636636e-06}, {"id": 919, "seek": 573120, "start": 5748.96, "end": 5755.28, "text": " would be all a row of zeros for that example there would be no push and pull so the amount to which", "tokens": [50364, 6338, 293, 264, 2372, 295, 3464, 300, 321, 434, 9275, 307, 767, 24969, 281, 264, 33783, 300, 50684, 50684, 1361, 484, 294, 264, 2128, 1320, 293, 370, 337, 1365, 498, 527, 33783, 1361, 484, 2293, 3006, 50940, 50940, 370, 436, 576, 362, 632, 4018, 5315, 3993, 337, 472, 412, 264, 3006, 2535, 550, 264, 264, 3565, 1208, 51252, 51252, 576, 312, 439, 257, 5386, 295, 35193, 337, 300, 1365, 456, 576, 312, 572, 2944, 293, 2235, 370, 264, 2372, 281, 597, 51568, 51568, 428, 17630, 307, 18424, 307, 2293, 264, 2372, 538, 597, 291, 434, 516, 281, 483, 257, 2235, 420, 257, 2944, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.04730383703641802, "compression_ratio": 1.9254901960784314, "no_speech_prob": 5.014471298636636e-06}, {"id": 920, "seek": 573120, "start": 5755.28, "end": 5760.32, "text": " your prediction is incorrect is exactly the amount by which you're going to get a pull or a push", "tokens": [50364, 6338, 293, 264, 2372, 295, 3464, 300, 321, 434, 9275, 307, 767, 24969, 281, 264, 33783, 300, 50684, 50684, 1361, 484, 294, 264, 2128, 1320, 293, 370, 337, 1365, 498, 527, 33783, 1361, 484, 2293, 3006, 50940, 50940, 370, 436, 576, 362, 632, 4018, 5315, 3993, 337, 472, 412, 264, 3006, 2535, 550, 264, 264, 3565, 1208, 51252, 51252, 576, 312, 439, 257, 5386, 295, 35193, 337, 300, 1365, 456, 576, 312, 572, 2944, 293, 2235, 370, 264, 2372, 281, 597, 51568, 51568, 428, 17630, 307, 18424, 307, 2293, 264, 2372, 538, 597, 291, 434, 516, 281, 483, 257, 2235, 420, 257, 2944, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.04730383703641802, "compression_ratio": 1.9254901960784314, "no_speech_prob": 5.014471298636636e-06}, {"id": 921, "seek": 576032, "start": 5760.32, "end": 5766.48, "text": " in that dimension so if you have for example a very confidently mispredicted element here then", "tokens": [50364, 294, 300, 10139, 370, 498, 291, 362, 337, 1365, 257, 588, 41956, 3346, 79, 986, 11254, 4478, 510, 550, 50672, 50708, 437, 311, 516, 281, 1051, 307, 300, 4478, 307, 516, 281, 312, 7373, 760, 588, 10950, 293, 264, 3006, 50948, 50948, 1867, 307, 516, 281, 312, 7373, 493, 281, 264, 912, 2372, 293, 264, 661, 4342, 366, 406, 516, 281, 312, 51188, 51216, 15269, 886, 709, 370, 264, 2372, 281, 597, 291, 3346, 79, 24945, 307, 550, 24969, 281, 264, 3800, 51504, 51504, 295, 264, 2235, 293, 300, 311, 2737, 21761, 294, 439, 264, 12819, 295, 341, 295, 341, 40863, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.058614926111130485, "compression_ratio": 1.9504132231404958, "no_speech_prob": 2.5215181267412845e-06}, {"id": 922, "seek": 576032, "start": 5767.2, "end": 5772.0, "text": " what's going to happen is that element is going to be pulled down very heavily and the correct", "tokens": [50364, 294, 300, 10139, 370, 498, 291, 362, 337, 1365, 257, 588, 41956, 3346, 79, 986, 11254, 4478, 510, 550, 50672, 50708, 437, 311, 516, 281, 1051, 307, 300, 4478, 307, 516, 281, 312, 7373, 760, 588, 10950, 293, 264, 3006, 50948, 50948, 1867, 307, 516, 281, 312, 7373, 493, 281, 264, 912, 2372, 293, 264, 661, 4342, 366, 406, 516, 281, 312, 51188, 51216, 15269, 886, 709, 370, 264, 2372, 281, 597, 291, 3346, 79, 24945, 307, 550, 24969, 281, 264, 3800, 51504, 51504, 295, 264, 2235, 293, 300, 311, 2737, 21761, 294, 439, 264, 12819, 295, 341, 295, 341, 40863, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.058614926111130485, "compression_ratio": 1.9504132231404958, "no_speech_prob": 2.5215181267412845e-06}, {"id": 923, "seek": 576032, "start": 5772.0, "end": 5776.799999999999, "text": " answer is going to be pulled up to the same amount and the other characters are not going to be", "tokens": [50364, 294, 300, 10139, 370, 498, 291, 362, 337, 1365, 257, 588, 41956, 3346, 79, 986, 11254, 4478, 510, 550, 50672, 50708, 437, 311, 516, 281, 1051, 307, 300, 4478, 307, 516, 281, 312, 7373, 760, 588, 10950, 293, 264, 3006, 50948, 50948, 1867, 307, 516, 281, 312, 7373, 493, 281, 264, 912, 2372, 293, 264, 661, 4342, 366, 406, 516, 281, 312, 51188, 51216, 15269, 886, 709, 370, 264, 2372, 281, 597, 291, 3346, 79, 24945, 307, 550, 24969, 281, 264, 3800, 51504, 51504, 295, 264, 2235, 293, 300, 311, 2737, 21761, 294, 439, 264, 12819, 295, 341, 295, 341, 40863, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.058614926111130485, "compression_ratio": 1.9504132231404958, "no_speech_prob": 2.5215181267412845e-06}, {"id": 924, "seek": 576032, "start": 5777.36, "end": 5783.12, "text": " influenced too much so the amount to which you mispredict is then proportional to the strength", "tokens": [50364, 294, 300, 10139, 370, 498, 291, 362, 337, 1365, 257, 588, 41956, 3346, 79, 986, 11254, 4478, 510, 550, 50672, 50708, 437, 311, 516, 281, 1051, 307, 300, 4478, 307, 516, 281, 312, 7373, 760, 588, 10950, 293, 264, 3006, 50948, 50948, 1867, 307, 516, 281, 312, 7373, 493, 281, 264, 912, 2372, 293, 264, 661, 4342, 366, 406, 516, 281, 312, 51188, 51216, 15269, 886, 709, 370, 264, 2372, 281, 597, 291, 3346, 79, 24945, 307, 550, 24969, 281, 264, 3800, 51504, 51504, 295, 264, 2235, 293, 300, 311, 2737, 21761, 294, 439, 264, 12819, 295, 341, 295, 341, 40863, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.058614926111130485, "compression_ratio": 1.9504132231404958, "no_speech_prob": 2.5215181267412845e-06}, {"id": 925, "seek": 576032, "start": 5783.12, "end": 5788.799999999999, "text": " of the pull and that's happening independently in all the dimensions of this of this tensor", "tokens": [50364, 294, 300, 10139, 370, 498, 291, 362, 337, 1365, 257, 588, 41956, 3346, 79, 986, 11254, 4478, 510, 550, 50672, 50708, 437, 311, 516, 281, 1051, 307, 300, 4478, 307, 516, 281, 312, 7373, 760, 588, 10950, 293, 264, 3006, 50948, 50948, 1867, 307, 516, 281, 312, 7373, 493, 281, 264, 912, 2372, 293, 264, 661, 4342, 366, 406, 516, 281, 312, 51188, 51216, 15269, 886, 709, 370, 264, 2372, 281, 597, 291, 3346, 79, 24945, 307, 550, 24969, 281, 264, 3800, 51504, 51504, 295, 264, 2235, 293, 300, 311, 2737, 21761, 294, 439, 264, 12819, 295, 341, 295, 341, 40863, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.058614926111130485, "compression_ratio": 1.9504132231404958, "no_speech_prob": 2.5215181267412845e-06}, {"id": 926, "seek": 578880, "start": 5788.8, "end": 5793.04, "text": " and it's sort of very intuitive and very easy to think through and that's basically the magic of", "tokens": [50364, 293, 309, 311, 1333, 295, 588, 21769, 293, 588, 1858, 281, 519, 807, 293, 300, 311, 1936, 264, 5585, 295, 50576, 50576, 264, 3278, 30867, 4470, 293, 437, 309, 311, 884, 43492, 294, 264, 23897, 1320, 295, 264, 18161, 50772, 50772, 2533, 370, 586, 321, 483, 281, 5380, 1230, 1045, 597, 307, 257, 588, 1019, 5380, 5413, 322, 428, 7123, 51096, 51096, 295, 1019, 293, 321, 366, 516, 281, 360, 337, 15245, 2710, 2144, 2293, 437, 321, 630, 337, 3278, 30867, 4470, 294, 51340, 51340, 5380, 1230, 732, 300, 307, 321, 366, 516, 281, 1949, 309, 382, 257, 28008, 2167, 18894, 51564, 51564, 6114, 293, 646, 48256, 807, 309, 294, 257, 588, 7148, 9060, 570, 321, 366, 516, 281, 28446, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.05428652609548261, "compression_ratio": 1.9292929292929293, "no_speech_prob": 6.643056622124277e-06}, {"id": 927, "seek": 578880, "start": 5793.04, "end": 5796.96, "text": " the cross entropy loss and what it's doing dynamically in the backward pass of the neural", "tokens": [50364, 293, 309, 311, 1333, 295, 588, 21769, 293, 588, 1858, 281, 519, 807, 293, 300, 311, 1936, 264, 5585, 295, 50576, 50576, 264, 3278, 30867, 4470, 293, 437, 309, 311, 884, 43492, 294, 264, 23897, 1320, 295, 264, 18161, 50772, 50772, 2533, 370, 586, 321, 483, 281, 5380, 1230, 1045, 597, 307, 257, 588, 1019, 5380, 5413, 322, 428, 7123, 51096, 51096, 295, 1019, 293, 321, 366, 516, 281, 360, 337, 15245, 2710, 2144, 2293, 437, 321, 630, 337, 3278, 30867, 4470, 294, 51340, 51340, 5380, 1230, 732, 300, 307, 321, 366, 516, 281, 1949, 309, 382, 257, 28008, 2167, 18894, 51564, 51564, 6114, 293, 646, 48256, 807, 309, 294, 257, 588, 7148, 9060, 570, 321, 366, 516, 281, 28446, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.05428652609548261, "compression_ratio": 1.9292929292929293, "no_speech_prob": 6.643056622124277e-06}, {"id": 928, "seek": 578880, "start": 5796.96, "end": 5803.4400000000005, "text": " net so now we get to exercise number three which is a very fun exercise depending on your definition", "tokens": [50364, 293, 309, 311, 1333, 295, 588, 21769, 293, 588, 1858, 281, 519, 807, 293, 300, 311, 1936, 264, 5585, 295, 50576, 50576, 264, 3278, 30867, 4470, 293, 437, 309, 311, 884, 43492, 294, 264, 23897, 1320, 295, 264, 18161, 50772, 50772, 2533, 370, 586, 321, 483, 281, 5380, 1230, 1045, 597, 307, 257, 588, 1019, 5380, 5413, 322, 428, 7123, 51096, 51096, 295, 1019, 293, 321, 366, 516, 281, 360, 337, 15245, 2710, 2144, 2293, 437, 321, 630, 337, 3278, 30867, 4470, 294, 51340, 51340, 5380, 1230, 732, 300, 307, 321, 366, 516, 281, 1949, 309, 382, 257, 28008, 2167, 18894, 51564, 51564, 6114, 293, 646, 48256, 807, 309, 294, 257, 588, 7148, 9060, 570, 321, 366, 516, 281, 28446, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.05428652609548261, "compression_ratio": 1.9292929292929293, "no_speech_prob": 6.643056622124277e-06}, {"id": 929, "seek": 578880, "start": 5803.4400000000005, "end": 5808.320000000001, "text": " of fun and we are going to do for batch normalization exactly what we did for cross entropy loss in", "tokens": [50364, 293, 309, 311, 1333, 295, 588, 21769, 293, 588, 1858, 281, 519, 807, 293, 300, 311, 1936, 264, 5585, 295, 50576, 50576, 264, 3278, 30867, 4470, 293, 437, 309, 311, 884, 43492, 294, 264, 23897, 1320, 295, 264, 18161, 50772, 50772, 2533, 370, 586, 321, 483, 281, 5380, 1230, 1045, 597, 307, 257, 588, 1019, 5380, 5413, 322, 428, 7123, 51096, 51096, 295, 1019, 293, 321, 366, 516, 281, 360, 337, 15245, 2710, 2144, 2293, 437, 321, 630, 337, 3278, 30867, 4470, 294, 51340, 51340, 5380, 1230, 732, 300, 307, 321, 366, 516, 281, 1949, 309, 382, 257, 28008, 2167, 18894, 51564, 51564, 6114, 293, 646, 48256, 807, 309, 294, 257, 588, 7148, 9060, 570, 321, 366, 516, 281, 28446, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.05428652609548261, "compression_ratio": 1.9292929292929293, "no_speech_prob": 6.643056622124277e-06}, {"id": 930, "seek": 578880, "start": 5808.320000000001, "end": 5812.8, "text": " exercise number two that is we are going to consider it as a glued single mathematical", "tokens": [50364, 293, 309, 311, 1333, 295, 588, 21769, 293, 588, 1858, 281, 519, 807, 293, 300, 311, 1936, 264, 5585, 295, 50576, 50576, 264, 3278, 30867, 4470, 293, 437, 309, 311, 884, 43492, 294, 264, 23897, 1320, 295, 264, 18161, 50772, 50772, 2533, 370, 586, 321, 483, 281, 5380, 1230, 1045, 597, 307, 257, 588, 1019, 5380, 5413, 322, 428, 7123, 51096, 51096, 295, 1019, 293, 321, 366, 516, 281, 360, 337, 15245, 2710, 2144, 2293, 437, 321, 630, 337, 3278, 30867, 4470, 294, 51340, 51340, 5380, 1230, 732, 300, 307, 321, 366, 516, 281, 1949, 309, 382, 257, 28008, 2167, 18894, 51564, 51564, 6114, 293, 646, 48256, 807, 309, 294, 257, 588, 7148, 9060, 570, 321, 366, 516, 281, 28446, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.05428652609548261, "compression_ratio": 1.9292929292929293, "no_speech_prob": 6.643056622124277e-06}, {"id": 931, "seek": 578880, "start": 5812.8, "end": 5817.28, "text": " expression and back propagate through it in a very efficient manner because we are going to derive", "tokens": [50364, 293, 309, 311, 1333, 295, 588, 21769, 293, 588, 1858, 281, 519, 807, 293, 300, 311, 1936, 264, 5585, 295, 50576, 50576, 264, 3278, 30867, 4470, 293, 437, 309, 311, 884, 43492, 294, 264, 23897, 1320, 295, 264, 18161, 50772, 50772, 2533, 370, 586, 321, 483, 281, 5380, 1230, 1045, 597, 307, 257, 588, 1019, 5380, 5413, 322, 428, 7123, 51096, 51096, 295, 1019, 293, 321, 366, 516, 281, 360, 337, 15245, 2710, 2144, 2293, 437, 321, 630, 337, 3278, 30867, 4470, 294, 51340, 51340, 5380, 1230, 732, 300, 307, 321, 366, 516, 281, 1949, 309, 382, 257, 28008, 2167, 18894, 51564, 51564, 6114, 293, 646, 48256, 807, 309, 294, 257, 588, 7148, 9060, 570, 321, 366, 516, 281, 28446, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.05428652609548261, "compression_ratio": 1.9292929292929293, "no_speech_prob": 6.643056622124277e-06}, {"id": 932, "seek": 581728, "start": 5817.28, "end": 5822.16, "text": " a much simpler formula for the backward pass of best formalization and we're going to do that using", "tokens": [50364, 257, 709, 18587, 8513, 337, 264, 23897, 1320, 295, 1151, 9860, 2144, 293, 321, 434, 516, 281, 360, 300, 1228, 50608, 50608, 3435, 293, 3035, 370, 8046, 321, 600, 5463, 493, 1791, 16819, 2144, 666, 439, 295, 264, 707, 50828, 50828, 19376, 3755, 293, 439, 264, 22275, 7705, 1854, 309, 293, 550, 321, 646, 48256, 309, 807, 309, 51052, 51088, 472, 538, 472, 586, 321, 445, 362, 257, 2167, 1333, 295, 2128, 1320, 295, 257, 1151, 1433, 293, 309, 311, 439, 28008, 51456, 51456, 1214, 293, 321, 536, 300, 321, 483, 264, 1900, 912, 1874, 382, 949, 586, 337, 264, 46183, 23897, 1320, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.1220381727842527, "compression_ratio": 1.8359375, "no_speech_prob": 6.143646260170499e-06}, {"id": 933, "seek": 581728, "start": 5822.16, "end": 5826.5599999999995, "text": " pen and paper so previously we've broken up pastoralization into all of the little", "tokens": [50364, 257, 709, 18587, 8513, 337, 264, 23897, 1320, 295, 1151, 9860, 2144, 293, 321, 434, 516, 281, 360, 300, 1228, 50608, 50608, 3435, 293, 3035, 370, 8046, 321, 600, 5463, 493, 1791, 16819, 2144, 666, 439, 295, 264, 707, 50828, 50828, 19376, 3755, 293, 439, 264, 22275, 7705, 1854, 309, 293, 550, 321, 646, 48256, 309, 807, 309, 51052, 51088, 472, 538, 472, 586, 321, 445, 362, 257, 2167, 1333, 295, 2128, 1320, 295, 257, 1151, 1433, 293, 309, 311, 439, 28008, 51456, 51456, 1214, 293, 321, 536, 300, 321, 483, 264, 1900, 912, 1874, 382, 949, 586, 337, 264, 46183, 23897, 1320, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.1220381727842527, "compression_ratio": 1.8359375, "no_speech_prob": 6.143646260170499e-06}, {"id": 934, "seek": 581728, "start": 5826.5599999999995, "end": 5831.04, "text": " intermediate pieces and all the atomic operations inside it and then we back propagate it through it", "tokens": [50364, 257, 709, 18587, 8513, 337, 264, 23897, 1320, 295, 1151, 9860, 2144, 293, 321, 434, 516, 281, 360, 300, 1228, 50608, 50608, 3435, 293, 3035, 370, 8046, 321, 600, 5463, 493, 1791, 16819, 2144, 666, 439, 295, 264, 707, 50828, 50828, 19376, 3755, 293, 439, 264, 22275, 7705, 1854, 309, 293, 550, 321, 646, 48256, 309, 807, 309, 51052, 51088, 472, 538, 472, 586, 321, 445, 362, 257, 2167, 1333, 295, 2128, 1320, 295, 257, 1151, 1433, 293, 309, 311, 439, 28008, 51456, 51456, 1214, 293, 321, 536, 300, 321, 483, 264, 1900, 912, 1874, 382, 949, 586, 337, 264, 46183, 23897, 1320, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.1220381727842527, "compression_ratio": 1.8359375, "no_speech_prob": 6.143646260170499e-06}, {"id": 935, "seek": 581728, "start": 5831.759999999999, "end": 5839.12, "text": " one by one now we just have a single sort of forward pass of a best term and it's all glued", "tokens": [50364, 257, 709, 18587, 8513, 337, 264, 23897, 1320, 295, 1151, 9860, 2144, 293, 321, 434, 516, 281, 360, 300, 1228, 50608, 50608, 3435, 293, 3035, 370, 8046, 321, 600, 5463, 493, 1791, 16819, 2144, 666, 439, 295, 264, 707, 50828, 50828, 19376, 3755, 293, 439, 264, 22275, 7705, 1854, 309, 293, 550, 321, 646, 48256, 309, 807, 309, 51052, 51088, 472, 538, 472, 586, 321, 445, 362, 257, 2167, 1333, 295, 2128, 1320, 295, 257, 1151, 1433, 293, 309, 311, 439, 28008, 51456, 51456, 1214, 293, 321, 536, 300, 321, 483, 264, 1900, 912, 1874, 382, 949, 586, 337, 264, 46183, 23897, 1320, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.1220381727842527, "compression_ratio": 1.8359375, "no_speech_prob": 6.143646260170499e-06}, {"id": 936, "seek": 581728, "start": 5839.12, "end": 5844.5599999999995, "text": " together and we see that we get the exact same result as before now for the bash backward pass", "tokens": [50364, 257, 709, 18587, 8513, 337, 264, 23897, 1320, 295, 1151, 9860, 2144, 293, 321, 434, 516, 281, 360, 300, 1228, 50608, 50608, 3435, 293, 3035, 370, 8046, 321, 600, 5463, 493, 1791, 16819, 2144, 666, 439, 295, 264, 707, 50828, 50828, 19376, 3755, 293, 439, 264, 22275, 7705, 1854, 309, 293, 550, 321, 646, 48256, 309, 807, 309, 51052, 51088, 472, 538, 472, 586, 321, 445, 362, 257, 2167, 1333, 295, 2128, 1320, 295, 257, 1151, 1433, 293, 309, 311, 439, 28008, 51456, 51456, 1214, 293, 321, 536, 300, 321, 483, 264, 1900, 912, 1874, 382, 949, 586, 337, 264, 46183, 23897, 1320, 51728, 51728], "temperature": 0.0, "avg_logprob": -0.1220381727842527, "compression_ratio": 1.8359375, "no_speech_prob": 6.143646260170499e-06}, {"id": 937, "seek": 584456, "start": 5844.56, "end": 5849.6, "text": " we'd like to also implement a single formula basically for back propagating through this", "tokens": [50364, 321, 1116, 411, 281, 611, 4445, 257, 2167, 8513, 1936, 337, 646, 12425, 990, 807, 341, 50616, 50616, 2302, 6916, 300, 307, 264, 46183, 9860, 2144, 370, 294, 264, 2128, 1320, 8046, 321, 1890, 276, 659, 12, 19404, 50932, 50932, 264, 7633, 4368, 295, 264, 659, 12, 65, 852, 9860, 2144, 293, 2942, 276, 659, 578, 597, 307, 264, 7633, 4368, 51236, 51236, 445, 949, 264, 24433, 294, 264, 46183, 9860, 2144, 3035, 276, 659, 12, 19404, 307, 2031, 293, 276, 659, 578, 307, 288, 370, 294, 264, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.12618797758351202, "compression_ratio": 1.8872549019607843, "no_speech_prob": 8.13893257145537e-06}, {"id": 938, "seek": 584456, "start": 5849.6, "end": 5855.92, "text": " entire operation that is the bash formalization so in the forward pass previously we took h pre-bn", "tokens": [50364, 321, 1116, 411, 281, 611, 4445, 257, 2167, 8513, 1936, 337, 646, 12425, 990, 807, 341, 50616, 50616, 2302, 6916, 300, 307, 264, 46183, 9860, 2144, 370, 294, 264, 2128, 1320, 8046, 321, 1890, 276, 659, 12, 19404, 50932, 50932, 264, 7633, 4368, 295, 264, 659, 12, 65, 852, 9860, 2144, 293, 2942, 276, 659, 578, 597, 307, 264, 7633, 4368, 51236, 51236, 445, 949, 264, 24433, 294, 264, 46183, 9860, 2144, 3035, 276, 659, 12, 19404, 307, 2031, 293, 276, 659, 578, 307, 288, 370, 294, 264, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.12618797758351202, "compression_ratio": 1.8872549019607843, "no_speech_prob": 8.13893257145537e-06}, {"id": 939, "seek": 584456, "start": 5855.92, "end": 5862.0, "text": " the hidden states of the pre-batch formalization and created h preact which is the hidden states", "tokens": [50364, 321, 1116, 411, 281, 611, 4445, 257, 2167, 8513, 1936, 337, 646, 12425, 990, 807, 341, 50616, 50616, 2302, 6916, 300, 307, 264, 46183, 9860, 2144, 370, 294, 264, 2128, 1320, 8046, 321, 1890, 276, 659, 12, 19404, 50932, 50932, 264, 7633, 4368, 295, 264, 659, 12, 65, 852, 9860, 2144, 293, 2942, 276, 659, 578, 597, 307, 264, 7633, 4368, 51236, 51236, 445, 949, 264, 24433, 294, 264, 46183, 9860, 2144, 3035, 276, 659, 12, 19404, 307, 2031, 293, 276, 659, 578, 307, 288, 370, 294, 264, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.12618797758351202, "compression_ratio": 1.8872549019607843, "no_speech_prob": 8.13893257145537e-06}, {"id": 940, "seek": 584456, "start": 5862.0, "end": 5870.0, "text": " just before the activation in the bash formalization paper h pre-bn is x and h preact is y so in the", "tokens": [50364, 321, 1116, 411, 281, 611, 4445, 257, 2167, 8513, 1936, 337, 646, 12425, 990, 807, 341, 50616, 50616, 2302, 6916, 300, 307, 264, 46183, 9860, 2144, 370, 294, 264, 2128, 1320, 8046, 321, 1890, 276, 659, 12, 19404, 50932, 50932, 264, 7633, 4368, 295, 264, 659, 12, 65, 852, 9860, 2144, 293, 2942, 276, 659, 578, 597, 307, 264, 7633, 4368, 51236, 51236, 445, 949, 264, 24433, 294, 264, 46183, 9860, 2144, 3035, 276, 659, 12, 19404, 307, 2031, 293, 276, 659, 578, 307, 288, 370, 294, 264, 51636, 51636], "temperature": 0.0, "avg_logprob": -0.12618797758351202, "compression_ratio": 1.8872549019607843, "no_speech_prob": 8.13893257145537e-06}, {"id": 941, "seek": 587000, "start": 5870.0, "end": 5875.92, "text": " backward pass what we'd like to do now is we have dh preact and we'd like to produce dh pre-bien", "tokens": [50364, 23897, 1320, 437, 321, 1116, 411, 281, 360, 586, 307, 321, 362, 274, 71, 659, 578, 293, 321, 1116, 411, 281, 5258, 274, 71, 659, 12, 65, 1053, 50660, 50696, 293, 321, 1116, 411, 281, 360, 300, 294, 257, 588, 7148, 9060, 370, 300, 311, 264, 1315, 295, 264, 1216, 8873, 50912, 50912, 274, 71, 659, 12, 65, 1053, 2212, 274, 71, 659, 578, 293, 337, 264, 9932, 295, 341, 5380, 321, 434, 516, 281, 11200, 15546, 293, 51220, 51220, 9861, 293, 641, 33733, 570, 436, 747, 322, 257, 588, 2199, 1254, 294, 257, 588, 2531, 636, 281, 437, 321, 51484, 51484], "temperature": 0.0, "avg_logprob": -0.0796575271166288, "compression_ratio": 1.8695652173913044, "no_speech_prob": 1.9332692318130285e-06}, {"id": 942, "seek": 587000, "start": 5876.64, "end": 5880.96, "text": " and we'd like to do that in a very efficient manner so that's the name of the game calculate", "tokens": [50364, 23897, 1320, 437, 321, 1116, 411, 281, 360, 586, 307, 321, 362, 274, 71, 659, 578, 293, 321, 1116, 411, 281, 5258, 274, 71, 659, 12, 65, 1053, 50660, 50696, 293, 321, 1116, 411, 281, 360, 300, 294, 257, 588, 7148, 9060, 370, 300, 311, 264, 1315, 295, 264, 1216, 8873, 50912, 50912, 274, 71, 659, 12, 65, 1053, 2212, 274, 71, 659, 578, 293, 337, 264, 9932, 295, 341, 5380, 321, 434, 516, 281, 11200, 15546, 293, 51220, 51220, 9861, 293, 641, 33733, 570, 436, 747, 322, 257, 588, 2199, 1254, 294, 257, 588, 2531, 636, 281, 437, 321, 51484, 51484], "temperature": 0.0, "avg_logprob": -0.0796575271166288, "compression_ratio": 1.8695652173913044, "no_speech_prob": 1.9332692318130285e-06}, {"id": 943, "seek": 587000, "start": 5880.96, "end": 5887.12, "text": " dh pre-bien given dh preact and for the purposes of this exercise we're going to ignore gamma and", "tokens": [50364, 23897, 1320, 437, 321, 1116, 411, 281, 360, 586, 307, 321, 362, 274, 71, 659, 578, 293, 321, 1116, 411, 281, 5258, 274, 71, 659, 12, 65, 1053, 50660, 50696, 293, 321, 1116, 411, 281, 360, 300, 294, 257, 588, 7148, 9060, 370, 300, 311, 264, 1315, 295, 264, 1216, 8873, 50912, 50912, 274, 71, 659, 12, 65, 1053, 2212, 274, 71, 659, 578, 293, 337, 264, 9932, 295, 341, 5380, 321, 434, 516, 281, 11200, 15546, 293, 51220, 51220, 9861, 293, 641, 33733, 570, 436, 747, 322, 257, 588, 2199, 1254, 294, 257, 588, 2531, 636, 281, 437, 321, 51484, 51484], "temperature": 0.0, "avg_logprob": -0.0796575271166288, "compression_ratio": 1.8695652173913044, "no_speech_prob": 1.9332692318130285e-06}, {"id": 944, "seek": 587000, "start": 5887.12, "end": 5892.4, "text": " beta and their derivatives because they take on a very simple form in a very similar way to what we", "tokens": [50364, 23897, 1320, 437, 321, 1116, 411, 281, 360, 586, 307, 321, 362, 274, 71, 659, 578, 293, 321, 1116, 411, 281, 5258, 274, 71, 659, 12, 65, 1053, 50660, 50696, 293, 321, 1116, 411, 281, 360, 300, 294, 257, 588, 7148, 9060, 370, 300, 311, 264, 1315, 295, 264, 1216, 8873, 50912, 50912, 274, 71, 659, 12, 65, 1053, 2212, 274, 71, 659, 578, 293, 337, 264, 9932, 295, 341, 5380, 321, 434, 516, 281, 11200, 15546, 293, 51220, 51220, 9861, 293, 641, 33733, 570, 436, 747, 322, 257, 588, 2199, 1254, 294, 257, 588, 2531, 636, 281, 437, 321, 51484, 51484], "temperature": 0.0, "avg_logprob": -0.0796575271166288, "compression_ratio": 1.8695652173913044, "no_speech_prob": 1.9332692318130285e-06}, {"id": 945, "seek": 589240, "start": 5892.4, "end": 5900.5599999999995, "text": " did up above so let's calculate this given that right here so to help you a little bit like i did", "tokens": [50364, 630, 493, 3673, 370, 718, 311, 8873, 341, 2212, 300, 558, 510, 370, 281, 854, 291, 257, 707, 857, 411, 741, 630, 50772, 50772, 949, 741, 1409, 766, 264, 11420, 510, 322, 3435, 293, 3035, 293, 741, 1890, 732, 15421, 295, 3035, 281, 51108, 51108, 28446, 264, 18894, 30546, 337, 264, 23897, 1320, 293, 1936, 281, 992, 493, 264, 1154, 445, 2464, 51412, 51412, 484, 264, 2992, 12771, 3732, 21977, 36800, 2385, 293, 288, 72, 2293, 382, 294, 264, 3035, 3993, 337, 264, 363, 47166, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.060506263475739556, "compression_ratio": 1.6724137931034482, "no_speech_prob": 5.9546969168877695e-06}, {"id": 946, "seek": 589240, "start": 5900.5599999999995, "end": 5907.28, "text": " before i started off the implementation here on pen and paper and i took two sheets of paper to", "tokens": [50364, 630, 493, 3673, 370, 718, 311, 8873, 341, 2212, 300, 558, 510, 370, 281, 854, 291, 257, 707, 857, 411, 741, 630, 50772, 50772, 949, 741, 1409, 766, 264, 11420, 510, 322, 3435, 293, 3035, 293, 741, 1890, 732, 15421, 295, 3035, 281, 51108, 51108, 28446, 264, 18894, 30546, 337, 264, 23897, 1320, 293, 1936, 281, 992, 493, 264, 1154, 445, 2464, 51412, 51412, 484, 264, 2992, 12771, 3732, 21977, 36800, 2385, 293, 288, 72, 2293, 382, 294, 264, 3035, 3993, 337, 264, 363, 47166, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.060506263475739556, "compression_ratio": 1.6724137931034482, "no_speech_prob": 5.9546969168877695e-06}, {"id": 947, "seek": 589240, "start": 5907.28, "end": 5913.36, "text": " derive the mathematical formulas for the backward pass and basically to set up the problem just write", "tokens": [50364, 630, 493, 3673, 370, 718, 311, 8873, 341, 2212, 300, 558, 510, 370, 281, 854, 291, 257, 707, 857, 411, 741, 630, 50772, 50772, 949, 741, 1409, 766, 264, 11420, 510, 322, 3435, 293, 3035, 293, 741, 1890, 732, 15421, 295, 3035, 281, 51108, 51108, 28446, 264, 18894, 30546, 337, 264, 23897, 1320, 293, 1936, 281, 992, 493, 264, 1154, 445, 2464, 51412, 51412, 484, 264, 2992, 12771, 3732, 21977, 36800, 2385, 293, 288, 72, 2293, 382, 294, 264, 3035, 3993, 337, 264, 363, 47166, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.060506263475739556, "compression_ratio": 1.6724137931034482, "no_speech_prob": 5.9546969168877695e-06}, {"id": 948, "seek": 589240, "start": 5913.36, "end": 5920.48, "text": " out the mu sigma square variance xi hat and yi exactly as in the paper except for the Bessel", "tokens": [50364, 630, 493, 3673, 370, 718, 311, 8873, 341, 2212, 300, 558, 510, 370, 281, 854, 291, 257, 707, 857, 411, 741, 630, 50772, 50772, 949, 741, 1409, 766, 264, 11420, 510, 322, 3435, 293, 3035, 293, 741, 1890, 732, 15421, 295, 3035, 281, 51108, 51108, 28446, 264, 18894, 30546, 337, 264, 23897, 1320, 293, 1936, 281, 992, 493, 264, 1154, 445, 2464, 51412, 51412, 484, 264, 2992, 12771, 3732, 21977, 36800, 2385, 293, 288, 72, 2293, 382, 294, 264, 3035, 3993, 337, 264, 363, 47166, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.060506263475739556, "compression_ratio": 1.6724137931034482, "no_speech_prob": 5.9546969168877695e-06}, {"id": 949, "seek": 592048, "start": 5920.48, "end": 5926.24, "text": " correction and then in the backward pass we have the derivative of the loss with respect to all the", "tokens": [50364, 19984, 293, 550, 294, 264, 23897, 1320, 321, 362, 264, 13760, 295, 264, 4470, 365, 3104, 281, 439, 264, 50652, 50652, 4959, 295, 288, 293, 1604, 300, 288, 307, 257, 8062, 456, 311, 456, 311, 3866, 3547, 510, 370, 321, 362, 51012, 51012, 439, 264, 33733, 365, 3104, 281, 439, 264, 288, 311, 293, 550, 456, 311, 257, 274, 76, 293, 257, 9861, 293, 341, 307, 733, 51308, 51308, 295, 411, 264, 14722, 4295, 264, 15546, 293, 9861, 456, 311, 264, 2031, 2385, 293, 550, 264, 2992, 293, 264, 12771, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.07753208343018876, "compression_ratio": 2.0052083333333335, "no_speech_prob": 8.664235792821273e-06}, {"id": 950, "seek": 592048, "start": 5926.24, "end": 5933.44, "text": " elements of y and remember that y is a vector there's there's multiple numbers here so we have", "tokens": [50364, 19984, 293, 550, 294, 264, 23897, 1320, 321, 362, 264, 13760, 295, 264, 4470, 365, 3104, 281, 439, 264, 50652, 50652, 4959, 295, 288, 293, 1604, 300, 288, 307, 257, 8062, 456, 311, 456, 311, 3866, 3547, 510, 370, 321, 362, 51012, 51012, 439, 264, 33733, 365, 3104, 281, 439, 264, 288, 311, 293, 550, 456, 311, 257, 274, 76, 293, 257, 9861, 293, 341, 307, 733, 51308, 51308, 295, 411, 264, 14722, 4295, 264, 15546, 293, 9861, 456, 311, 264, 2031, 2385, 293, 550, 264, 2992, 293, 264, 12771, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.07753208343018876, "compression_ratio": 2.0052083333333335, "no_speech_prob": 8.664235792821273e-06}, {"id": 951, "seek": 592048, "start": 5933.44, "end": 5939.36, "text": " all the derivatives with respect to all the y's and then there's a dm and a beta and this is kind", "tokens": [50364, 19984, 293, 550, 294, 264, 23897, 1320, 321, 362, 264, 13760, 295, 264, 4470, 365, 3104, 281, 439, 264, 50652, 50652, 4959, 295, 288, 293, 1604, 300, 288, 307, 257, 8062, 456, 311, 456, 311, 3866, 3547, 510, 370, 321, 362, 51012, 51012, 439, 264, 33733, 365, 3104, 281, 439, 264, 288, 311, 293, 550, 456, 311, 257, 274, 76, 293, 257, 9861, 293, 341, 307, 733, 51308, 51308, 295, 411, 264, 14722, 4295, 264, 15546, 293, 9861, 456, 311, 264, 2031, 2385, 293, 550, 264, 2992, 293, 264, 12771, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.07753208343018876, "compression_ratio": 2.0052083333333335, "no_speech_prob": 8.664235792821273e-06}, {"id": 952, "seek": 592048, "start": 5939.36, "end": 5945.12, "text": " of like the compute graph the gamma and beta there's the x hat and then the mu and the sigma", "tokens": [50364, 19984, 293, 550, 294, 264, 23897, 1320, 321, 362, 264, 13760, 295, 264, 4470, 365, 3104, 281, 439, 264, 50652, 50652, 4959, 295, 288, 293, 1604, 300, 288, 307, 257, 8062, 456, 311, 456, 311, 3866, 3547, 510, 370, 321, 362, 51012, 51012, 439, 264, 33733, 365, 3104, 281, 439, 264, 288, 311, 293, 550, 456, 311, 257, 274, 76, 293, 257, 9861, 293, 341, 307, 733, 51308, 51308, 295, 411, 264, 14722, 4295, 264, 15546, 293, 9861, 456, 311, 264, 2031, 2385, 293, 550, 264, 2992, 293, 264, 12771, 51596, 51596], "temperature": 0.0, "avg_logprob": -0.07753208343018876, "compression_ratio": 2.0052083333333335, "no_speech_prob": 8.664235792821273e-06}, {"id": 953, "seek": 594512, "start": 5945.12, "end": 5953.92, "text": " squared and the x so we have dl by dyi and we want dl by dx i for all the i's in these vectors", "tokens": [50364, 8889, 293, 264, 2031, 370, 321, 362, 37873, 538, 14584, 72, 293, 321, 528, 37873, 538, 30017, 741, 337, 439, 264, 741, 311, 294, 613, 18875, 50804, 50872, 370, 341, 307, 264, 14722, 4295, 293, 291, 362, 281, 312, 5026, 570, 741, 478, 1382, 281, 3637, 510, 300, 51140, 51176, 613, 366, 18875, 456, 311, 867, 13891, 510, 1854, 2031, 2031, 2385, 293, 288, 457, 2992, 293, 12771, 2597, 12771, 3732, 51580, 51580, 366, 445, 2609, 15664, 685, 2167, 3547, 370, 291, 362, 281, 312, 5026, 365, 300, 291, 362, 281, 3811, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.05884576837221781, "compression_ratio": 1.8509615384615385, "no_speech_prob": 2.1232494873402175e-06}, {"id": 954, "seek": 594512, "start": 5955.28, "end": 5960.64, "text": " so this is the compute graph and you have to be careful because i'm trying to note here that", "tokens": [50364, 8889, 293, 264, 2031, 370, 321, 362, 37873, 538, 14584, 72, 293, 321, 528, 37873, 538, 30017, 741, 337, 439, 264, 741, 311, 294, 613, 18875, 50804, 50872, 370, 341, 307, 264, 14722, 4295, 293, 291, 362, 281, 312, 5026, 570, 741, 478, 1382, 281, 3637, 510, 300, 51140, 51176, 613, 366, 18875, 456, 311, 867, 13891, 510, 1854, 2031, 2031, 2385, 293, 288, 457, 2992, 293, 12771, 2597, 12771, 3732, 51580, 51580, 366, 445, 2609, 15664, 685, 2167, 3547, 370, 291, 362, 281, 312, 5026, 365, 300, 291, 362, 281, 3811, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.05884576837221781, "compression_ratio": 1.8509615384615385, "no_speech_prob": 2.1232494873402175e-06}, {"id": 955, "seek": 594512, "start": 5961.36, "end": 5969.44, "text": " these are vectors there's many nodes here inside x x hat and y but mu and sigma sorry sigma square", "tokens": [50364, 8889, 293, 264, 2031, 370, 321, 362, 37873, 538, 14584, 72, 293, 321, 528, 37873, 538, 30017, 741, 337, 439, 264, 741, 311, 294, 613, 18875, 50804, 50872, 370, 341, 307, 264, 14722, 4295, 293, 291, 362, 281, 312, 5026, 570, 741, 478, 1382, 281, 3637, 510, 300, 51140, 51176, 613, 366, 18875, 456, 311, 867, 13891, 510, 1854, 2031, 2031, 2385, 293, 288, 457, 2992, 293, 12771, 2597, 12771, 3732, 51580, 51580, 366, 445, 2609, 15664, 685, 2167, 3547, 370, 291, 362, 281, 312, 5026, 365, 300, 291, 362, 281, 3811, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.05884576837221781, "compression_ratio": 1.8509615384615385, "no_speech_prob": 2.1232494873402175e-06}, {"id": 956, "seek": 594512, "start": 5969.44, "end": 5974.24, "text": " are just individual scalars single numbers so you have to be careful with that you have to imagine", "tokens": [50364, 8889, 293, 264, 2031, 370, 321, 362, 37873, 538, 14584, 72, 293, 321, 528, 37873, 538, 30017, 741, 337, 439, 264, 741, 311, 294, 613, 18875, 50804, 50872, 370, 341, 307, 264, 14722, 4295, 293, 291, 362, 281, 312, 5026, 570, 741, 478, 1382, 281, 3637, 510, 300, 51140, 51176, 613, 366, 18875, 456, 311, 867, 13891, 510, 1854, 2031, 2031, 2385, 293, 288, 457, 2992, 293, 12771, 2597, 12771, 3732, 51580, 51580, 366, 445, 2609, 15664, 685, 2167, 3547, 370, 291, 362, 281, 312, 5026, 365, 300, 291, 362, 281, 3811, 51820, 51820], "temperature": 0.0, "avg_logprob": -0.05884576837221781, "compression_ratio": 1.8509615384615385, "no_speech_prob": 2.1232494873402175e-06}, {"id": 957, "seek": 597424, "start": 5974.24, "end": 5980.24, "text": " there's multiple nodes here or you're going to get your math wrong so as an example i would", "tokens": [50364, 456, 311, 3866, 13891, 510, 420, 291, 434, 516, 281, 483, 428, 5221, 2085, 370, 382, 364, 1365, 741, 576, 50664, 50664, 3402, 300, 291, 352, 294, 264, 3480, 1668, 472, 732, 1045, 1451, 294, 2115, 295, 264, 646, 38377, 50920, 50920, 370, 646, 48256, 666, 2031, 2385, 550, 666, 12771, 3732, 550, 666, 2992, 293, 550, 666, 2031, 51160, 51276, 445, 411, 294, 257, 1192, 4383, 1333, 294, 4532, 7165, 321, 576, 352, 490, 558, 281, 1411, 291, 434, 884, 264, 51484, 51484, 1900, 912, 551, 3993, 291, 434, 884, 309, 365, 16944, 293, 322, 257, 2522, 295, 3035, 370, 337, 1230, 472, 51784, 51852], "temperature": 0.0, "avg_logprob": -0.05969334090197528, "compression_ratio": 1.7984189723320159, "no_speech_prob": 1.228890710081032e-06}, {"id": 958, "seek": 597424, "start": 5980.24, "end": 5985.36, "text": " suggest that you go in the following order one two three four in terms of the back propagation", "tokens": [50364, 456, 311, 3866, 13891, 510, 420, 291, 434, 516, 281, 483, 428, 5221, 2085, 370, 382, 364, 1365, 741, 576, 50664, 50664, 3402, 300, 291, 352, 294, 264, 3480, 1668, 472, 732, 1045, 1451, 294, 2115, 295, 264, 646, 38377, 50920, 50920, 370, 646, 48256, 666, 2031, 2385, 550, 666, 12771, 3732, 550, 666, 2992, 293, 550, 666, 2031, 51160, 51276, 445, 411, 294, 257, 1192, 4383, 1333, 294, 4532, 7165, 321, 576, 352, 490, 558, 281, 1411, 291, 434, 884, 264, 51484, 51484, 1900, 912, 551, 3993, 291, 434, 884, 309, 365, 16944, 293, 322, 257, 2522, 295, 3035, 370, 337, 1230, 472, 51784, 51852], "temperature": 0.0, "avg_logprob": -0.05969334090197528, "compression_ratio": 1.7984189723320159, "no_speech_prob": 1.228890710081032e-06}, {"id": 959, "seek": 597424, "start": 5985.36, "end": 5990.16, "text": " so back propagate into x hat then into sigma square then into mu and then into x", "tokens": [50364, 456, 311, 3866, 13891, 510, 420, 291, 434, 516, 281, 483, 428, 5221, 2085, 370, 382, 364, 1365, 741, 576, 50664, 50664, 3402, 300, 291, 352, 294, 264, 3480, 1668, 472, 732, 1045, 1451, 294, 2115, 295, 264, 646, 38377, 50920, 50920, 370, 646, 48256, 666, 2031, 2385, 550, 666, 12771, 3732, 550, 666, 2992, 293, 550, 666, 2031, 51160, 51276, 445, 411, 294, 257, 1192, 4383, 1333, 294, 4532, 7165, 321, 576, 352, 490, 558, 281, 1411, 291, 434, 884, 264, 51484, 51484, 1900, 912, 551, 3993, 291, 434, 884, 309, 365, 16944, 293, 322, 257, 2522, 295, 3035, 370, 337, 1230, 472, 51784, 51852], "temperature": 0.0, "avg_logprob": -0.05969334090197528, "compression_ratio": 1.7984189723320159, "no_speech_prob": 1.228890710081032e-06}, {"id": 960, "seek": 597424, "start": 5992.48, "end": 5996.639999999999, "text": " just like in a topological sort in micrograd we would go from right to left you're doing the", "tokens": [50364, 456, 311, 3866, 13891, 510, 420, 291, 434, 516, 281, 483, 428, 5221, 2085, 370, 382, 364, 1365, 741, 576, 50664, 50664, 3402, 300, 291, 352, 294, 264, 3480, 1668, 472, 732, 1045, 1451, 294, 2115, 295, 264, 646, 38377, 50920, 50920, 370, 646, 48256, 666, 2031, 2385, 550, 666, 12771, 3732, 550, 666, 2992, 293, 550, 666, 2031, 51160, 51276, 445, 411, 294, 257, 1192, 4383, 1333, 294, 4532, 7165, 321, 576, 352, 490, 558, 281, 1411, 291, 434, 884, 264, 51484, 51484, 1900, 912, 551, 3993, 291, 434, 884, 309, 365, 16944, 293, 322, 257, 2522, 295, 3035, 370, 337, 1230, 472, 51784, 51852], "temperature": 0.0, "avg_logprob": -0.05969334090197528, "compression_ratio": 1.7984189723320159, "no_speech_prob": 1.228890710081032e-06}, {"id": 961, "seek": 597424, "start": 5996.639999999999, "end": 6002.639999999999, "text": " exact same thing except you're doing it with symbols and on a piece of paper so for number one", "tokens": [50364, 456, 311, 3866, 13891, 510, 420, 291, 434, 516, 281, 483, 428, 5221, 2085, 370, 382, 364, 1365, 741, 576, 50664, 50664, 3402, 300, 291, 352, 294, 264, 3480, 1668, 472, 732, 1045, 1451, 294, 2115, 295, 264, 646, 38377, 50920, 50920, 370, 646, 48256, 666, 2031, 2385, 550, 666, 12771, 3732, 550, 666, 2992, 293, 550, 666, 2031, 51160, 51276, 445, 411, 294, 257, 1192, 4383, 1333, 294, 4532, 7165, 321, 576, 352, 490, 558, 281, 1411, 291, 434, 884, 264, 51484, 51484, 1900, 912, 551, 3993, 291, 434, 884, 309, 365, 16944, 293, 322, 257, 2522, 295, 3035, 370, 337, 1230, 472, 51784, 51852], "temperature": 0.0, "avg_logprob": -0.05969334090197528, "compression_ratio": 1.7984189723320159, "no_speech_prob": 1.228890710081032e-06}, {"id": 962, "seek": 600264, "start": 6002.64, "end": 6012.0, "text": " uh i'm not giving away too much if you want dl of the xi hat then we just take dl by dyi and multiply", "tokens": [50364, 2232, 741, 478, 406, 2902, 1314, 886, 709, 498, 291, 528, 37873, 295, 264, 36800, 2385, 550, 321, 445, 747, 37873, 538, 14584, 72, 293, 12972, 50832, 50832, 309, 538, 15546, 570, 295, 341, 6114, 510, 689, 604, 2609, 288, 72, 307, 445, 15546, 1413, 51136, 51164, 36800, 2385, 1804, 9861, 370, 2232, 994, 380, 854, 291, 886, 709, 456, 457, 341, 2709, 291, 1936, 264, 33733, 51448, 51448, 337, 439, 264, 2031, 20549, 293, 370, 586, 853, 281, 352, 807, 341, 28270, 4295, 293, 28446, 437, 307, 264, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.18423260924636678, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.260223709527054e-06}, {"id": 963, "seek": 600264, "start": 6012.0, "end": 6018.08, "text": " it by gamma because of this expression here where any individual yi is just gamma times", "tokens": [50364, 2232, 741, 478, 406, 2902, 1314, 886, 709, 498, 291, 528, 37873, 295, 264, 36800, 2385, 550, 321, 445, 747, 37873, 538, 14584, 72, 293, 12972, 50832, 50832, 309, 538, 15546, 570, 295, 341, 6114, 510, 689, 604, 2609, 288, 72, 307, 445, 15546, 1413, 51136, 51164, 36800, 2385, 1804, 9861, 370, 2232, 994, 380, 854, 291, 886, 709, 456, 457, 341, 2709, 291, 1936, 264, 33733, 51448, 51448, 337, 439, 264, 2031, 20549, 293, 370, 586, 853, 281, 352, 807, 341, 28270, 4295, 293, 28446, 437, 307, 264, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.18423260924636678, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.260223709527054e-06}, {"id": 964, "seek": 600264, "start": 6018.64, "end": 6024.320000000001, "text": " xi hat plus beta so uh didn't help you too much there but this gives you basically the derivatives", "tokens": [50364, 2232, 741, 478, 406, 2902, 1314, 886, 709, 498, 291, 528, 37873, 295, 264, 36800, 2385, 550, 321, 445, 747, 37873, 538, 14584, 72, 293, 12972, 50832, 50832, 309, 538, 15546, 570, 295, 341, 6114, 510, 689, 604, 2609, 288, 72, 307, 445, 15546, 1413, 51136, 51164, 36800, 2385, 1804, 9861, 370, 2232, 994, 380, 854, 291, 886, 709, 456, 457, 341, 2709, 291, 1936, 264, 33733, 51448, 51448, 337, 439, 264, 2031, 20549, 293, 370, 586, 853, 281, 352, 807, 341, 28270, 4295, 293, 28446, 437, 307, 264, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.18423260924636678, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.260223709527054e-06}, {"id": 965, "seek": 600264, "start": 6024.320000000001, "end": 6032.08, "text": " for all the x hats and so now try to go through this computational graph and derive what is the", "tokens": [50364, 2232, 741, 478, 406, 2902, 1314, 886, 709, 498, 291, 528, 37873, 295, 264, 36800, 2385, 550, 321, 445, 747, 37873, 538, 14584, 72, 293, 12972, 50832, 50832, 309, 538, 15546, 570, 295, 341, 6114, 510, 689, 604, 2609, 288, 72, 307, 445, 15546, 1413, 51136, 51164, 36800, 2385, 1804, 9861, 370, 2232, 994, 380, 854, 291, 886, 709, 456, 457, 341, 2709, 291, 1936, 264, 33733, 51448, 51448, 337, 439, 264, 2031, 20549, 293, 370, 586, 853, 281, 352, 807, 341, 28270, 4295, 293, 28446, 437, 307, 264, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.18423260924636678, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.260223709527054e-06}, {"id": 966, "seek": 603208, "start": 6032.08, "end": 6040.96, "text": " l by d sigma square and then what is dl by d mu and then what is dl by dx eventually so give it", "tokens": [50364, 287, 538, 274, 12771, 3732, 293, 550, 437, 307, 37873, 538, 274, 2992, 293, 550, 437, 307, 37873, 538, 30017, 4728, 370, 976, 309, 50808, 50808, 257, 352, 293, 741, 478, 516, 281, 312, 23983, 264, 1867, 472, 2522, 412, 257, 565, 1392, 370, 281, 483, 37873, 538, 274, 12771, 51068, 51068, 3732, 321, 362, 281, 1604, 797, 411, 741, 2835, 300, 456, 366, 867, 2031, 311, 2031, 20549, 510, 293, 1604, 51396, 51396, 300, 12771, 3732, 307, 445, 257, 2167, 2609, 1230, 510, 370, 562, 321, 574, 412, 264, 6114, 51648, 51720], "temperature": 0.0, "avg_logprob": -0.10514770307038959, "compression_ratio": 1.8405797101449275, "no_speech_prob": 6.04873866905109e-06}, {"id": 967, "seek": 603208, "start": 6040.96, "end": 6046.16, "text": " a go and i'm going to be revealing the answer one piece at a time okay so to get dl by d sigma", "tokens": [50364, 287, 538, 274, 12771, 3732, 293, 550, 437, 307, 37873, 538, 274, 2992, 293, 550, 437, 307, 37873, 538, 30017, 4728, 370, 976, 309, 50808, 50808, 257, 352, 293, 741, 478, 516, 281, 312, 23983, 264, 1867, 472, 2522, 412, 257, 565, 1392, 370, 281, 483, 37873, 538, 274, 12771, 51068, 51068, 3732, 321, 362, 281, 1604, 797, 411, 741, 2835, 300, 456, 366, 867, 2031, 311, 2031, 20549, 510, 293, 1604, 51396, 51396, 300, 12771, 3732, 307, 445, 257, 2167, 2609, 1230, 510, 370, 562, 321, 574, 412, 264, 6114, 51648, 51720], "temperature": 0.0, "avg_logprob": -0.10514770307038959, "compression_ratio": 1.8405797101449275, "no_speech_prob": 6.04873866905109e-06}, {"id": 968, "seek": 603208, "start": 6046.16, "end": 6052.72, "text": " square we have to remember again like i mentioned that there are many x's x hats here and remember", "tokens": [50364, 287, 538, 274, 12771, 3732, 293, 550, 437, 307, 37873, 538, 274, 2992, 293, 550, 437, 307, 37873, 538, 30017, 4728, 370, 976, 309, 50808, 50808, 257, 352, 293, 741, 478, 516, 281, 312, 23983, 264, 1867, 472, 2522, 412, 257, 565, 1392, 370, 281, 483, 37873, 538, 274, 12771, 51068, 51068, 3732, 321, 362, 281, 1604, 797, 411, 741, 2835, 300, 456, 366, 867, 2031, 311, 2031, 20549, 510, 293, 1604, 51396, 51396, 300, 12771, 3732, 307, 445, 257, 2167, 2609, 1230, 510, 370, 562, 321, 574, 412, 264, 6114, 51648, 51720], "temperature": 0.0, "avg_logprob": -0.10514770307038959, "compression_ratio": 1.8405797101449275, "no_speech_prob": 6.04873866905109e-06}, {"id": 969, "seek": 603208, "start": 6052.72, "end": 6057.76, "text": " that sigma square is just a single individual number here so when we look at the expression", "tokens": [50364, 287, 538, 274, 12771, 3732, 293, 550, 437, 307, 37873, 538, 274, 2992, 293, 550, 437, 307, 37873, 538, 30017, 4728, 370, 976, 309, 50808, 50808, 257, 352, 293, 741, 478, 516, 281, 312, 23983, 264, 1867, 472, 2522, 412, 257, 565, 1392, 370, 281, 483, 37873, 538, 274, 12771, 51068, 51068, 3732, 321, 362, 281, 1604, 797, 411, 741, 2835, 300, 456, 366, 867, 2031, 311, 2031, 20549, 510, 293, 1604, 51396, 51396, 300, 12771, 3732, 307, 445, 257, 2167, 2609, 1230, 510, 370, 562, 321, 574, 412, 264, 6114, 51648, 51720], "temperature": 0.0, "avg_logprob": -0.10514770307038959, "compression_ratio": 1.8405797101449275, "no_speech_prob": 6.04873866905109e-06}, {"id": 970, "seek": 605776, "start": 6057.76, "end": 6064.08, "text": " for dl by d sigma square we have that we have to actually consider all the possible paths that", "tokens": [50364, 337, 37873, 538, 274, 12771, 3732, 321, 362, 300, 321, 362, 281, 767, 1949, 439, 264, 1944, 14518, 300, 50680, 50796, 321, 1936, 362, 300, 456, 311, 867, 2031, 20549, 293, 436, 439, 3154, 766, 490, 436, 439, 5672, 322, 12771, 51072, 51072, 3732, 370, 12771, 3732, 575, 257, 2416, 3429, 484, 456, 311, 3195, 295, 19669, 1348, 484, 490, 12771, 3732, 666, 51328, 51328, 439, 264, 2031, 20549, 293, 550, 456, 311, 257, 646, 12, 79, 1513, 559, 990, 6358, 490, 1184, 2031, 2385, 666, 12771, 3732, 293, 300, 311, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.13519648501747533, "compression_ratio": 2.020618556701031, "no_speech_prob": 5.338051323633408e-06}, {"id": 971, "seek": 605776, "start": 6066.400000000001, "end": 6071.92, "text": " we basically have that there's many x hats and they all feed off from they all depend on sigma", "tokens": [50364, 337, 37873, 538, 274, 12771, 3732, 321, 362, 300, 321, 362, 281, 767, 1949, 439, 264, 1944, 14518, 300, 50680, 50796, 321, 1936, 362, 300, 456, 311, 867, 2031, 20549, 293, 436, 439, 3154, 766, 490, 436, 439, 5672, 322, 12771, 51072, 51072, 3732, 370, 12771, 3732, 575, 257, 2416, 3429, 484, 456, 311, 3195, 295, 19669, 1348, 484, 490, 12771, 3732, 666, 51328, 51328, 439, 264, 2031, 20549, 293, 550, 456, 311, 257, 646, 12, 79, 1513, 559, 990, 6358, 490, 1184, 2031, 2385, 666, 12771, 3732, 293, 300, 311, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.13519648501747533, "compression_ratio": 2.020618556701031, "no_speech_prob": 5.338051323633408e-06}, {"id": 972, "seek": 605776, "start": 6071.92, "end": 6077.04, "text": " square so sigma square has a large fan out there's lots of arrows coming out from sigma square into", "tokens": [50364, 337, 37873, 538, 274, 12771, 3732, 321, 362, 300, 321, 362, 281, 767, 1949, 439, 264, 1944, 14518, 300, 50680, 50796, 321, 1936, 362, 300, 456, 311, 867, 2031, 20549, 293, 436, 439, 3154, 766, 490, 436, 439, 5672, 322, 12771, 51072, 51072, 3732, 370, 12771, 3732, 575, 257, 2416, 3429, 484, 456, 311, 3195, 295, 19669, 1348, 484, 490, 12771, 3732, 666, 51328, 51328, 439, 264, 2031, 20549, 293, 550, 456, 311, 257, 646, 12, 79, 1513, 559, 990, 6358, 490, 1184, 2031, 2385, 666, 12771, 3732, 293, 300, 311, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.13519648501747533, "compression_ratio": 2.020618556701031, "no_speech_prob": 5.338051323633408e-06}, {"id": 973, "seek": 605776, "start": 6077.04, "end": 6083.68, "text": " all the x hats and then there's a back-propagating signal from each x hat into sigma square and that's", "tokens": [50364, 337, 37873, 538, 274, 12771, 3732, 321, 362, 300, 321, 362, 281, 767, 1949, 439, 264, 1944, 14518, 300, 50680, 50796, 321, 1936, 362, 300, 456, 311, 867, 2031, 20549, 293, 436, 439, 3154, 766, 490, 436, 439, 5672, 322, 12771, 51072, 51072, 3732, 370, 12771, 3732, 575, 257, 2416, 3429, 484, 456, 311, 3195, 295, 19669, 1348, 484, 490, 12771, 3732, 666, 51328, 51328, 439, 264, 2031, 20549, 293, 550, 456, 311, 257, 646, 12, 79, 1513, 559, 990, 6358, 490, 1184, 2031, 2385, 666, 12771, 3732, 293, 300, 311, 51660, 51660], "temperature": 0.0, "avg_logprob": -0.13519648501747533, "compression_ratio": 2.020618556701031, "no_speech_prob": 5.338051323633408e-06}, {"id": 974, "seek": 608368, "start": 6083.68, "end": 6092.64, "text": " why we actually need to sum over all those i's from i equal to one to m of the dl by d xi hat", "tokens": [50364, 983, 321, 767, 643, 281, 2408, 670, 439, 729, 741, 311, 490, 741, 2681, 281, 472, 281, 275, 295, 264, 37873, 538, 274, 36800, 2385, 50812, 50812, 597, 307, 264, 4338, 16235, 1413, 264, 36800, 2385, 538, 274, 12771, 3732, 597, 307, 264, 2654, 16235, 295, 341, 51240, 51240, 6916, 510, 293, 550, 44003, 741, 478, 445, 1364, 309, 484, 510, 293, 741, 478, 6883, 5489, 293, 291, 51492, 51492, 483, 257, 1629, 6114, 337, 37873, 538, 274, 12771, 3732, 321, 434, 516, 281, 312, 1228, 341, 6114, 562, 321, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.130036749738328, "compression_ratio": 1.8428571428571427, "no_speech_prob": 5.954845619271509e-06}, {"id": 975, "seek": 608368, "start": 6092.64, "end": 6101.200000000001, "text": " which is the global gradient times the xi hat by d sigma square which is the local gradient of this", "tokens": [50364, 983, 321, 767, 643, 281, 2408, 670, 439, 729, 741, 311, 490, 741, 2681, 281, 472, 281, 275, 295, 264, 37873, 538, 274, 36800, 2385, 50812, 50812, 597, 307, 264, 4338, 16235, 1413, 264, 36800, 2385, 538, 274, 12771, 3732, 597, 307, 264, 2654, 16235, 295, 341, 51240, 51240, 6916, 510, 293, 550, 44003, 741, 478, 445, 1364, 309, 484, 510, 293, 741, 478, 6883, 5489, 293, 291, 51492, 51492, 483, 257, 1629, 6114, 337, 37873, 538, 274, 12771, 3732, 321, 434, 516, 281, 312, 1228, 341, 6114, 562, 321, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.130036749738328, "compression_ratio": 1.8428571428571427, "no_speech_prob": 5.954845619271509e-06}, {"id": 976, "seek": 608368, "start": 6101.200000000001, "end": 6106.240000000001, "text": " operation here and then mathematically i'm just working it out here and i'm simplifying and you", "tokens": [50364, 983, 321, 767, 643, 281, 2408, 670, 439, 729, 741, 311, 490, 741, 2681, 281, 472, 281, 275, 295, 264, 37873, 538, 274, 36800, 2385, 50812, 50812, 597, 307, 264, 4338, 16235, 1413, 264, 36800, 2385, 538, 274, 12771, 3732, 597, 307, 264, 2654, 16235, 295, 341, 51240, 51240, 6916, 510, 293, 550, 44003, 741, 478, 445, 1364, 309, 484, 510, 293, 741, 478, 6883, 5489, 293, 291, 51492, 51492, 483, 257, 1629, 6114, 337, 37873, 538, 274, 12771, 3732, 321, 434, 516, 281, 312, 1228, 341, 6114, 562, 321, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.130036749738328, "compression_ratio": 1.8428571428571427, "no_speech_prob": 5.954845619271509e-06}, {"id": 977, "seek": 608368, "start": 6106.240000000001, "end": 6111.360000000001, "text": " get a certain expression for dl by d sigma square we're going to be using this expression when we", "tokens": [50364, 983, 321, 767, 643, 281, 2408, 670, 439, 729, 741, 311, 490, 741, 2681, 281, 472, 281, 275, 295, 264, 37873, 538, 274, 36800, 2385, 50812, 50812, 597, 307, 264, 4338, 16235, 1413, 264, 36800, 2385, 538, 274, 12771, 3732, 597, 307, 264, 2654, 16235, 295, 341, 51240, 51240, 6916, 510, 293, 550, 44003, 741, 478, 445, 1364, 309, 484, 510, 293, 741, 478, 6883, 5489, 293, 291, 51492, 51492, 483, 257, 1629, 6114, 337, 37873, 538, 274, 12771, 3732, 321, 434, 516, 281, 312, 1228, 341, 6114, 562, 321, 51748, 51748], "temperature": 0.0, "avg_logprob": -0.130036749738328, "compression_ratio": 1.8428571428571427, "no_speech_prob": 5.954845619271509e-06}, {"id": 978, "seek": 611136, "start": 6111.36, "end": 6116.0, "text": " back-propagate into mu and then eventually into x so now let's continue our back-propagation into", "tokens": [50364, 646, 12, 79, 1513, 559, 473, 666, 2992, 293, 550, 4728, 666, 2031, 370, 586, 718, 311, 2354, 527, 646, 12, 79, 1513, 559, 399, 666, 50596, 50596, 2992, 370, 437, 307, 37873, 538, 274, 2992, 586, 797, 312, 5026, 300, 2992, 21222, 2031, 2385, 293, 2031, 2385, 307, 767, 50960, 50960, 3195, 295, 4190, 370, 337, 1365, 498, 527, 8382, 15245, 2744, 307, 8858, 382, 309, 307, 294, 527, 1365, 300, 321, 645, 51212, 51212, 1364, 322, 550, 341, 307, 8858, 3547, 293, 8858, 19669, 516, 646, 281, 2992, 293, 550, 2992, 516, 281, 12771, 3732, 51548, 51548, 307, 445, 257, 2167, 11610, 570, 12771, 3732, 307, 257, 39684, 370, 294, 3217, 456, 366, 11816, 19669, 516, 646, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.10951083268576521, "compression_ratio": 1.8511450381679388, "no_speech_prob": 4.63760534330504e-06}, {"id": 979, "seek": 611136, "start": 6116.0, "end": 6123.28, "text": " mu so what is dl by d mu now again be careful that mu influences x hat and x hat is actually", "tokens": [50364, 646, 12, 79, 1513, 559, 473, 666, 2992, 293, 550, 4728, 666, 2031, 370, 586, 718, 311, 2354, 527, 646, 12, 79, 1513, 559, 399, 666, 50596, 50596, 2992, 370, 437, 307, 37873, 538, 274, 2992, 586, 797, 312, 5026, 300, 2992, 21222, 2031, 2385, 293, 2031, 2385, 307, 767, 50960, 50960, 3195, 295, 4190, 370, 337, 1365, 498, 527, 8382, 15245, 2744, 307, 8858, 382, 309, 307, 294, 527, 1365, 300, 321, 645, 51212, 51212, 1364, 322, 550, 341, 307, 8858, 3547, 293, 8858, 19669, 516, 646, 281, 2992, 293, 550, 2992, 516, 281, 12771, 3732, 51548, 51548, 307, 445, 257, 2167, 11610, 570, 12771, 3732, 307, 257, 39684, 370, 294, 3217, 456, 366, 11816, 19669, 516, 646, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.10951083268576521, "compression_ratio": 1.8511450381679388, "no_speech_prob": 4.63760534330504e-06}, {"id": 980, "seek": 611136, "start": 6123.28, "end": 6128.32, "text": " lots of values so for example if our mini batch size is 32 as it is in our example that we were", "tokens": [50364, 646, 12, 79, 1513, 559, 473, 666, 2992, 293, 550, 4728, 666, 2031, 370, 586, 718, 311, 2354, 527, 646, 12, 79, 1513, 559, 399, 666, 50596, 50596, 2992, 370, 437, 307, 37873, 538, 274, 2992, 586, 797, 312, 5026, 300, 2992, 21222, 2031, 2385, 293, 2031, 2385, 307, 767, 50960, 50960, 3195, 295, 4190, 370, 337, 1365, 498, 527, 8382, 15245, 2744, 307, 8858, 382, 309, 307, 294, 527, 1365, 300, 321, 645, 51212, 51212, 1364, 322, 550, 341, 307, 8858, 3547, 293, 8858, 19669, 516, 646, 281, 2992, 293, 550, 2992, 516, 281, 12771, 3732, 51548, 51548, 307, 445, 257, 2167, 11610, 570, 12771, 3732, 307, 257, 39684, 370, 294, 3217, 456, 366, 11816, 19669, 516, 646, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.10951083268576521, "compression_ratio": 1.8511450381679388, "no_speech_prob": 4.63760534330504e-06}, {"id": 981, "seek": 611136, "start": 6128.32, "end": 6135.04, "text": " working on then this is 32 numbers and 32 arrows going back to mu and then mu going to sigma square", "tokens": [50364, 646, 12, 79, 1513, 559, 473, 666, 2992, 293, 550, 4728, 666, 2031, 370, 586, 718, 311, 2354, 527, 646, 12, 79, 1513, 559, 399, 666, 50596, 50596, 2992, 370, 437, 307, 37873, 538, 274, 2992, 586, 797, 312, 5026, 300, 2992, 21222, 2031, 2385, 293, 2031, 2385, 307, 767, 50960, 50960, 3195, 295, 4190, 370, 337, 1365, 498, 527, 8382, 15245, 2744, 307, 8858, 382, 309, 307, 294, 527, 1365, 300, 321, 645, 51212, 51212, 1364, 322, 550, 341, 307, 8858, 3547, 293, 8858, 19669, 516, 646, 281, 2992, 293, 550, 2992, 516, 281, 12771, 3732, 51548, 51548, 307, 445, 257, 2167, 11610, 570, 12771, 3732, 307, 257, 39684, 370, 294, 3217, 456, 366, 11816, 19669, 516, 646, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.10951083268576521, "compression_ratio": 1.8511450381679388, "no_speech_prob": 4.63760534330504e-06}, {"id": 982, "seek": 611136, "start": 6135.04, "end": 6140.0, "text": " is just a single arrow because sigma square is a scalar so in total there are 33 arrows going back", "tokens": [50364, 646, 12, 79, 1513, 559, 473, 666, 2992, 293, 550, 4728, 666, 2031, 370, 586, 718, 311, 2354, 527, 646, 12, 79, 1513, 559, 399, 666, 50596, 50596, 2992, 370, 437, 307, 37873, 538, 274, 2992, 586, 797, 312, 5026, 300, 2992, 21222, 2031, 2385, 293, 2031, 2385, 307, 767, 50960, 50960, 3195, 295, 4190, 370, 337, 1365, 498, 527, 8382, 15245, 2744, 307, 8858, 382, 309, 307, 294, 527, 1365, 300, 321, 645, 51212, 51212, 1364, 322, 550, 341, 307, 8858, 3547, 293, 8858, 19669, 516, 646, 281, 2992, 293, 550, 2992, 516, 281, 12771, 3732, 51548, 51548, 307, 445, 257, 2167, 11610, 570, 12771, 3732, 307, 257, 39684, 370, 294, 3217, 456, 366, 11816, 19669, 516, 646, 51796, 51796], "temperature": 0.0, "avg_logprob": -0.10951083268576521, "compression_ratio": 1.8511450381679388, "no_speech_prob": 4.63760534330504e-06}, {"id": 983, "seek": 614000, "start": 6140.0, "end": 6146.24, "text": " to mu so in total there are 33 arrows emanating from mu and then all of them have gradients coming", "tokens": [50364, 281, 2992, 370, 294, 3217, 456, 366, 11816, 19669, 28211, 990, 490, 2992, 293, 550, 439, 295, 552, 362, 2771, 2448, 1348, 50676, 50676, 666, 2992, 293, 436, 439, 643, 281, 312, 2408, 1912, 493, 293, 370, 300, 311, 983, 562, 321, 574, 412, 264, 6114, 337, 37873, 50988, 50988, 538, 274, 2992, 741, 669, 2408, 2810, 493, 670, 439, 264, 2771, 2448, 295, 37873, 538, 274, 36800, 2385, 1413, 264, 36800, 2385, 538, 274, 2992, 51340, 51400, 370, 300, 311, 264, 300, 311, 341, 11610, 293, 300, 311, 8858, 19669, 510, 293, 550, 1804, 264, 472, 11610, 490, 510, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.0759381835277264, "compression_ratio": 1.8725490196078431, "no_speech_prob": 2.0903833046759246e-06}, {"id": 984, "seek": 614000, "start": 6146.24, "end": 6152.48, "text": " into mu and they all need to be summed up and so that's why when we look at the expression for dl", "tokens": [50364, 281, 2992, 370, 294, 3217, 456, 366, 11816, 19669, 28211, 990, 490, 2992, 293, 550, 439, 295, 552, 362, 2771, 2448, 1348, 50676, 50676, 666, 2992, 293, 436, 439, 643, 281, 312, 2408, 1912, 493, 293, 370, 300, 311, 983, 562, 321, 574, 412, 264, 6114, 337, 37873, 50988, 50988, 538, 274, 2992, 741, 669, 2408, 2810, 493, 670, 439, 264, 2771, 2448, 295, 37873, 538, 274, 36800, 2385, 1413, 264, 36800, 2385, 538, 274, 2992, 51340, 51400, 370, 300, 311, 264, 300, 311, 341, 11610, 293, 300, 311, 8858, 19669, 510, 293, 550, 1804, 264, 472, 11610, 490, 510, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.0759381835277264, "compression_ratio": 1.8725490196078431, "no_speech_prob": 2.0903833046759246e-06}, {"id": 985, "seek": 614000, "start": 6152.48, "end": 6159.52, "text": " by d mu i am summing up over all the gradients of dl by d xi hat times the xi hat by d mu", "tokens": [50364, 281, 2992, 370, 294, 3217, 456, 366, 11816, 19669, 28211, 990, 490, 2992, 293, 550, 439, 295, 552, 362, 2771, 2448, 1348, 50676, 50676, 666, 2992, 293, 436, 439, 643, 281, 312, 2408, 1912, 493, 293, 370, 300, 311, 983, 562, 321, 574, 412, 264, 6114, 337, 37873, 50988, 50988, 538, 274, 2992, 741, 669, 2408, 2810, 493, 670, 439, 264, 2771, 2448, 295, 37873, 538, 274, 36800, 2385, 1413, 264, 36800, 2385, 538, 274, 2992, 51340, 51400, 370, 300, 311, 264, 300, 311, 341, 11610, 293, 300, 311, 8858, 19669, 510, 293, 550, 1804, 264, 472, 11610, 490, 510, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.0759381835277264, "compression_ratio": 1.8725490196078431, "no_speech_prob": 2.0903833046759246e-06}, {"id": 986, "seek": 614000, "start": 6160.72, "end": 6166.24, "text": " so that's the that's this arrow and that's 32 arrows here and then plus the one arrow from here", "tokens": [50364, 281, 2992, 370, 294, 3217, 456, 366, 11816, 19669, 28211, 990, 490, 2992, 293, 550, 439, 295, 552, 362, 2771, 2448, 1348, 50676, 50676, 666, 2992, 293, 436, 439, 643, 281, 312, 2408, 1912, 493, 293, 370, 300, 311, 983, 562, 321, 574, 412, 264, 6114, 337, 37873, 50988, 50988, 538, 274, 2992, 741, 669, 2408, 2810, 493, 670, 439, 264, 2771, 2448, 295, 37873, 538, 274, 36800, 2385, 1413, 264, 36800, 2385, 538, 274, 2992, 51340, 51400, 370, 300, 311, 264, 300, 311, 341, 11610, 293, 300, 311, 8858, 19669, 510, 293, 550, 1804, 264, 472, 11610, 490, 510, 51676, 51676], "temperature": 0.0, "avg_logprob": -0.0759381835277264, "compression_ratio": 1.8725490196078431, "no_speech_prob": 2.0903833046759246e-06}, {"id": 987, "seek": 616624, "start": 6166.24, "end": 6172.24, "text": " which is dl by d sigma square times the sigma square by d mu so now we have to work out that", "tokens": [50364, 597, 307, 37873, 538, 274, 12771, 3732, 1413, 264, 12771, 3732, 538, 274, 2992, 370, 586, 321, 362, 281, 589, 484, 300, 50664, 50664, 6114, 293, 718, 385, 445, 10658, 264, 1472, 295, 309, 6883, 5489, 510, 307, 406, 6179, 264, 700, 1433, 51012, 51012, 293, 291, 445, 483, 364, 6114, 510, 337, 264, 1150, 1433, 1673, 456, 311, 746, 534, 51192, 51192, 1880, 300, 2314, 562, 321, 574, 412, 264, 12771, 3732, 538, 274, 2992, 293, 321, 20460, 412, 472, 935, 498, 321, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.044901503757996994, "compression_ratio": 1.8317307692307692, "no_speech_prob": 9.570637757860823e-07}, {"id": 988, "seek": 616624, "start": 6172.24, "end": 6179.2, "text": " expression and let me just reveal the rest of it simplifying here is not complicated the first term", "tokens": [50364, 597, 307, 37873, 538, 274, 12771, 3732, 1413, 264, 12771, 3732, 538, 274, 2992, 370, 586, 321, 362, 281, 589, 484, 300, 50664, 50664, 6114, 293, 718, 385, 445, 10658, 264, 1472, 295, 309, 6883, 5489, 510, 307, 406, 6179, 264, 700, 1433, 51012, 51012, 293, 291, 445, 483, 364, 6114, 510, 337, 264, 1150, 1433, 1673, 456, 311, 746, 534, 51192, 51192, 1880, 300, 2314, 562, 321, 574, 412, 264, 12771, 3732, 538, 274, 2992, 293, 321, 20460, 412, 472, 935, 498, 321, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.044901503757996994, "compression_ratio": 1.8317307692307692, "no_speech_prob": 9.570637757860823e-07}, {"id": 989, "seek": 616624, "start": 6179.2, "end": 6182.8, "text": " and you just get an expression here for the second term though there's something really", "tokens": [50364, 597, 307, 37873, 538, 274, 12771, 3732, 1413, 264, 12771, 3732, 538, 274, 2992, 370, 586, 321, 362, 281, 589, 484, 300, 50664, 50664, 6114, 293, 718, 385, 445, 10658, 264, 1472, 295, 309, 6883, 5489, 510, 307, 406, 6179, 264, 700, 1433, 51012, 51012, 293, 291, 445, 483, 364, 6114, 510, 337, 264, 1150, 1433, 1673, 456, 311, 746, 534, 51192, 51192, 1880, 300, 2314, 562, 321, 574, 412, 264, 12771, 3732, 538, 274, 2992, 293, 321, 20460, 412, 472, 935, 498, 321, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.044901503757996994, "compression_ratio": 1.8317307692307692, "no_speech_prob": 9.570637757860823e-07}, {"id": 990, "seek": 616624, "start": 6182.8, "end": 6190.4, "text": " interesting that happens when we look at the sigma square by d mu and we simplify at one point if we", "tokens": [50364, 597, 307, 37873, 538, 274, 12771, 3732, 1413, 264, 12771, 3732, 538, 274, 2992, 370, 586, 321, 362, 281, 589, 484, 300, 50664, 50664, 6114, 293, 718, 385, 445, 10658, 264, 1472, 295, 309, 6883, 5489, 510, 307, 406, 6179, 264, 700, 1433, 51012, 51012, 293, 291, 445, 483, 364, 6114, 510, 337, 264, 1150, 1433, 1673, 456, 311, 746, 534, 51192, 51192, 1880, 300, 2314, 562, 321, 574, 412, 264, 12771, 3732, 538, 274, 2992, 293, 321, 20460, 412, 472, 935, 498, 321, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.044901503757996994, "compression_ratio": 1.8317307692307692, "no_speech_prob": 9.570637757860823e-07}, {"id": 991, "seek": 619040, "start": 6190.4, "end": 6198.48, "text": " assume that in a special case where mu is actually the average of xi's as it is in this case then if", "tokens": [50364, 6552, 300, 294, 257, 2121, 1389, 689, 2992, 307, 767, 264, 4274, 295, 36800, 311, 382, 309, 307, 294, 341, 1389, 550, 498, 50768, 50768, 321, 5452, 300, 294, 550, 767, 264, 16235, 3161, 16423, 293, 3643, 2293, 4018, 293, 300, 1669, 264, 2302, 51076, 51076, 1150, 1433, 10373, 293, 370, 613, 498, 291, 445, 362, 257, 18894, 6114, 411, 341, 293, 291, 574, 412, 51392, 51392, 274, 12771, 3732, 538, 274, 2992, 291, 576, 483, 512, 18894, 8513, 337, 577, 2992, 11606, 12771, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.039267667315223, "compression_ratio": 1.7268722466960353, "no_speech_prob": 1.994696958718123e-06}, {"id": 992, "seek": 619040, "start": 6198.48, "end": 6204.639999999999, "text": " we plug that in then actually the gradient vanishes and becomes exactly zero and that makes the entire", "tokens": [50364, 6552, 300, 294, 257, 2121, 1389, 689, 2992, 307, 767, 264, 4274, 295, 36800, 311, 382, 309, 307, 294, 341, 1389, 550, 498, 50768, 50768, 321, 5452, 300, 294, 550, 767, 264, 16235, 3161, 16423, 293, 3643, 2293, 4018, 293, 300, 1669, 264, 2302, 51076, 51076, 1150, 1433, 10373, 293, 370, 613, 498, 291, 445, 362, 257, 18894, 6114, 411, 341, 293, 291, 574, 412, 51392, 51392, 274, 12771, 3732, 538, 274, 2992, 291, 576, 483, 512, 18894, 8513, 337, 577, 2992, 11606, 12771, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.039267667315223, "compression_ratio": 1.7268722466960353, "no_speech_prob": 1.994696958718123e-06}, {"id": 993, "seek": 619040, "start": 6204.639999999999, "end": 6210.96, "text": " second term cancel and so these if you just have a mathematical expression like this and you look at", "tokens": [50364, 6552, 300, 294, 257, 2121, 1389, 689, 2992, 307, 767, 264, 4274, 295, 36800, 311, 382, 309, 307, 294, 341, 1389, 550, 498, 50768, 50768, 321, 5452, 300, 294, 550, 767, 264, 16235, 3161, 16423, 293, 3643, 2293, 4018, 293, 300, 1669, 264, 2302, 51076, 51076, 1150, 1433, 10373, 293, 370, 613, 498, 291, 445, 362, 257, 18894, 6114, 411, 341, 293, 291, 574, 412, 51392, 51392, 274, 12771, 3732, 538, 274, 2992, 291, 576, 483, 512, 18894, 8513, 337, 577, 2992, 11606, 12771, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.039267667315223, "compression_ratio": 1.7268722466960353, "no_speech_prob": 1.994696958718123e-06}, {"id": 994, "seek": 619040, "start": 6210.96, "end": 6216.879999999999, "text": " d sigma square by d mu you would get some mathematical formula for how mu impacts sigma", "tokens": [50364, 6552, 300, 294, 257, 2121, 1389, 689, 2992, 307, 767, 264, 4274, 295, 36800, 311, 382, 309, 307, 294, 341, 1389, 550, 498, 50768, 50768, 321, 5452, 300, 294, 550, 767, 264, 16235, 3161, 16423, 293, 3643, 2293, 4018, 293, 300, 1669, 264, 2302, 51076, 51076, 1150, 1433, 10373, 293, 370, 613, 498, 291, 445, 362, 257, 18894, 6114, 411, 341, 293, 291, 574, 412, 51392, 51392, 274, 12771, 3732, 538, 274, 2992, 291, 576, 483, 512, 18894, 8513, 337, 577, 2992, 11606, 12771, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.039267667315223, "compression_ratio": 1.7268722466960353, "no_speech_prob": 1.994696958718123e-06}, {"id": 995, "seek": 621688, "start": 6216.88, "end": 6222.24, "text": " square but if it is the special case that mu is actually equal to the average as it is in the case", "tokens": [50364, 3732, 457, 498, 309, 307, 264, 2121, 1389, 300, 2992, 307, 767, 2681, 281, 264, 4274, 382, 309, 307, 294, 264, 1389, 50632, 50632, 295, 15245, 2710, 2144, 300, 16235, 486, 767, 43584, 293, 1813, 4018, 370, 264, 1379, 1433, 393, 66, 1625, 50900, 50900, 293, 321, 445, 483, 257, 6457, 15325, 6114, 510, 337, 37873, 538, 274, 2992, 1392, 293, 586, 321, 483, 281, 264, 51184, 51184, 46339, 644, 597, 307, 1163, 2123, 37873, 538, 274, 36800, 597, 307, 6284, 437, 321, 434, 934, 586, 718, 311, 1207, 51536, 51572, 700, 295, 439, 577, 867, 3547, 366, 456, 1854, 2031, 382, 741, 2835, 456, 366, 8858, 3547, 456, 366, 8858, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.060320559791896655, "compression_ratio": 1.7535714285714286, "no_speech_prob": 2.9944005746074254e-06}, {"id": 996, "seek": 621688, "start": 6222.24, "end": 6227.6, "text": " of batch normalization that gradient will actually vanish and become zero so the whole term cancels", "tokens": [50364, 3732, 457, 498, 309, 307, 264, 2121, 1389, 300, 2992, 307, 767, 2681, 281, 264, 4274, 382, 309, 307, 294, 264, 1389, 50632, 50632, 295, 15245, 2710, 2144, 300, 16235, 486, 767, 43584, 293, 1813, 4018, 370, 264, 1379, 1433, 393, 66, 1625, 50900, 50900, 293, 321, 445, 483, 257, 6457, 15325, 6114, 510, 337, 37873, 538, 274, 2992, 1392, 293, 586, 321, 483, 281, 264, 51184, 51184, 46339, 644, 597, 307, 1163, 2123, 37873, 538, 274, 36800, 597, 307, 6284, 437, 321, 434, 934, 586, 718, 311, 1207, 51536, 51572, 700, 295, 439, 577, 867, 3547, 366, 456, 1854, 2031, 382, 741, 2835, 456, 366, 8858, 3547, 456, 366, 8858, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.060320559791896655, "compression_ratio": 1.7535714285714286, "no_speech_prob": 2.9944005746074254e-06}, {"id": 997, "seek": 621688, "start": 6227.6, "end": 6233.28, "text": " and we just get a fairly straightforward expression here for dl by d mu okay and now we get to the", "tokens": [50364, 3732, 457, 498, 309, 307, 264, 2121, 1389, 300, 2992, 307, 767, 2681, 281, 264, 4274, 382, 309, 307, 294, 264, 1389, 50632, 50632, 295, 15245, 2710, 2144, 300, 16235, 486, 767, 43584, 293, 1813, 4018, 370, 264, 1379, 1433, 393, 66, 1625, 50900, 50900, 293, 321, 445, 483, 257, 6457, 15325, 6114, 510, 337, 37873, 538, 274, 2992, 1392, 293, 586, 321, 483, 281, 264, 51184, 51184, 46339, 644, 597, 307, 1163, 2123, 37873, 538, 274, 36800, 597, 307, 6284, 437, 321, 434, 934, 586, 718, 311, 1207, 51536, 51572, 700, 295, 439, 577, 867, 3547, 366, 456, 1854, 2031, 382, 741, 2835, 456, 366, 8858, 3547, 456, 366, 8858, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.060320559791896655, "compression_ratio": 1.7535714285714286, "no_speech_prob": 2.9944005746074254e-06}, {"id": 998, "seek": 621688, "start": 6233.28, "end": 6240.32, "text": " craziest part which is deriving dl by d xi which is ultimately what we're after now let's count", "tokens": [50364, 3732, 457, 498, 309, 307, 264, 2121, 1389, 300, 2992, 307, 767, 2681, 281, 264, 4274, 382, 309, 307, 294, 264, 1389, 50632, 50632, 295, 15245, 2710, 2144, 300, 16235, 486, 767, 43584, 293, 1813, 4018, 370, 264, 1379, 1433, 393, 66, 1625, 50900, 50900, 293, 321, 445, 483, 257, 6457, 15325, 6114, 510, 337, 37873, 538, 274, 2992, 1392, 293, 586, 321, 483, 281, 264, 51184, 51184, 46339, 644, 597, 307, 1163, 2123, 37873, 538, 274, 36800, 597, 307, 6284, 437, 321, 434, 934, 586, 718, 311, 1207, 51536, 51572, 700, 295, 439, 577, 867, 3547, 366, 456, 1854, 2031, 382, 741, 2835, 456, 366, 8858, 3547, 456, 366, 8858, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.060320559791896655, "compression_ratio": 1.7535714285714286, "no_speech_prob": 2.9944005746074254e-06}, {"id": 999, "seek": 621688, "start": 6241.04, "end": 6246.32, "text": " first of all how many numbers are there inside x as i mentioned there are 32 numbers there are 32", "tokens": [50364, 3732, 457, 498, 309, 307, 264, 2121, 1389, 300, 2992, 307, 767, 2681, 281, 264, 4274, 382, 309, 307, 294, 264, 1389, 50632, 50632, 295, 15245, 2710, 2144, 300, 16235, 486, 767, 43584, 293, 1813, 4018, 370, 264, 1379, 1433, 393, 66, 1625, 50900, 50900, 293, 321, 445, 483, 257, 6457, 15325, 6114, 510, 337, 37873, 538, 274, 2992, 1392, 293, 586, 321, 483, 281, 264, 51184, 51184, 46339, 644, 597, 307, 1163, 2123, 37873, 538, 274, 36800, 597, 307, 6284, 437, 321, 434, 934, 586, 718, 311, 1207, 51536, 51572, 700, 295, 439, 577, 867, 3547, 366, 456, 1854, 2031, 382, 741, 2835, 456, 366, 8858, 3547, 456, 366, 8858, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.060320559791896655, "compression_ratio": 1.7535714285714286, "no_speech_prob": 2.9944005746074254e-06}, {"id": 1000, "seek": 624632, "start": 6246.32, "end": 6252.08, "text": " little xi's and let's count the number of arrows emanating from each xi there's an arrow going to", "tokens": [50364, 707, 36800, 311, 293, 718, 311, 1207, 264, 1230, 295, 19669, 28211, 990, 490, 1184, 36800, 456, 311, 364, 11610, 516, 281, 50652, 50652, 2992, 364, 11610, 516, 281, 12771, 3732, 293, 550, 456, 311, 364, 11610, 516, 281, 2031, 2385, 457, 341, 11610, 510, 718, 311, 50984, 50984, 28949, 259, 1125, 300, 257, 707, 857, 1184, 36800, 2385, 307, 445, 257, 2445, 295, 36800, 293, 439, 264, 661, 15664, 685, 370, 36800, 51392, 51392, 2385, 787, 5946, 322, 36800, 293, 6022, 295, 264, 661, 2031, 311, 293, 370, 4412, 456, 366, 767, 294, 341, 2167, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.05709817409515381, "compression_ratio": 1.9507389162561577, "no_speech_prob": 6.854064849903807e-06}, {"id": 1001, "seek": 624632, "start": 6252.08, "end": 6258.719999999999, "text": " mu an arrow going to sigma square and then there's an arrow going to x hat but this arrow here let's", "tokens": [50364, 707, 36800, 311, 293, 718, 311, 1207, 264, 1230, 295, 19669, 28211, 990, 490, 1184, 36800, 456, 311, 364, 11610, 516, 281, 50652, 50652, 2992, 364, 11610, 516, 281, 12771, 3732, 293, 550, 456, 311, 364, 11610, 516, 281, 2031, 2385, 457, 341, 11610, 510, 718, 311, 50984, 50984, 28949, 259, 1125, 300, 257, 707, 857, 1184, 36800, 2385, 307, 445, 257, 2445, 295, 36800, 293, 439, 264, 661, 15664, 685, 370, 36800, 51392, 51392, 2385, 787, 5946, 322, 36800, 293, 6022, 295, 264, 661, 2031, 311, 293, 370, 4412, 456, 366, 767, 294, 341, 2167, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.05709817409515381, "compression_ratio": 1.9507389162561577, "no_speech_prob": 6.854064849903807e-06}, {"id": 1002, "seek": 624632, "start": 6258.719999999999, "end": 6266.88, "text": " scrutinize that a little bit each xi hat is just a function of xi and all the other scalars so xi", "tokens": [50364, 707, 36800, 311, 293, 718, 311, 1207, 264, 1230, 295, 19669, 28211, 990, 490, 1184, 36800, 456, 311, 364, 11610, 516, 281, 50652, 50652, 2992, 364, 11610, 516, 281, 12771, 3732, 293, 550, 456, 311, 364, 11610, 516, 281, 2031, 2385, 457, 341, 11610, 510, 718, 311, 50984, 50984, 28949, 259, 1125, 300, 257, 707, 857, 1184, 36800, 2385, 307, 445, 257, 2445, 295, 36800, 293, 439, 264, 661, 15664, 685, 370, 36800, 51392, 51392, 2385, 787, 5946, 322, 36800, 293, 6022, 295, 264, 661, 2031, 311, 293, 370, 4412, 456, 366, 767, 294, 341, 2167, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.05709817409515381, "compression_ratio": 1.9507389162561577, "no_speech_prob": 6.854064849903807e-06}, {"id": 1003, "seek": 624632, "start": 6266.88, "end": 6272.799999999999, "text": " hat only depends on xi and none of the other x's and so therefore there are actually in this single", "tokens": [50364, 707, 36800, 311, 293, 718, 311, 1207, 264, 1230, 295, 19669, 28211, 990, 490, 1184, 36800, 456, 311, 364, 11610, 516, 281, 50652, 50652, 2992, 364, 11610, 516, 281, 12771, 3732, 293, 550, 456, 311, 364, 11610, 516, 281, 2031, 2385, 457, 341, 11610, 510, 718, 311, 50984, 50984, 28949, 259, 1125, 300, 257, 707, 857, 1184, 36800, 2385, 307, 445, 257, 2445, 295, 36800, 293, 439, 264, 661, 15664, 685, 370, 36800, 51392, 51392, 2385, 787, 5946, 322, 36800, 293, 6022, 295, 264, 661, 2031, 311, 293, 370, 4412, 456, 366, 767, 294, 341, 2167, 51688, 51688], "temperature": 0.0, "avg_logprob": -0.05709817409515381, "compression_ratio": 1.9507389162561577, "no_speech_prob": 6.854064849903807e-06}, {"id": 1004, "seek": 627280, "start": 6272.8, "end": 6278.72, "text": " arrow there are 32 arrows but those 32 arrows are going exactly parallel they don't interfere", "tokens": [50364, 11610, 456, 366, 8858, 19669, 457, 729, 8858, 19669, 366, 516, 2293, 8952, 436, 500, 380, 23946, 50660, 50660, 436, 434, 445, 516, 8952, 1296, 2031, 293, 2031, 2385, 291, 393, 574, 412, 309, 300, 636, 293, 370, 577, 867, 19669, 50920, 50920, 366, 28211, 990, 490, 1184, 36800, 456, 366, 1045, 19669, 2992, 12771, 3732, 293, 264, 6615, 2031, 2385, 293, 370, 51304, 51304, 294, 646, 79, 1513, 559, 399, 321, 586, 643, 281, 3079, 264, 5021, 4978, 293, 321, 643, 281, 909, 493, 729, 1045, 15725, 51584, 51624], "temperature": 0.0, "avg_logprob": -0.04229704711748206, "compression_ratio": 1.7945205479452055, "no_speech_prob": 2.4439318622171413e-06}, {"id": 1005, "seek": 627280, "start": 6278.72, "end": 6283.92, "text": " they're just going parallel between x and x hat you can look at it that way and so how many arrows", "tokens": [50364, 11610, 456, 366, 8858, 19669, 457, 729, 8858, 19669, 366, 516, 2293, 8952, 436, 500, 380, 23946, 50660, 50660, 436, 434, 445, 516, 8952, 1296, 2031, 293, 2031, 2385, 291, 393, 574, 412, 309, 300, 636, 293, 370, 577, 867, 19669, 50920, 50920, 366, 28211, 990, 490, 1184, 36800, 456, 366, 1045, 19669, 2992, 12771, 3732, 293, 264, 6615, 2031, 2385, 293, 370, 51304, 51304, 294, 646, 79, 1513, 559, 399, 321, 586, 643, 281, 3079, 264, 5021, 4978, 293, 321, 643, 281, 909, 493, 729, 1045, 15725, 51584, 51624], "temperature": 0.0, "avg_logprob": -0.04229704711748206, "compression_ratio": 1.7945205479452055, "no_speech_prob": 2.4439318622171413e-06}, {"id": 1006, "seek": 627280, "start": 6283.92, "end": 6291.6, "text": " are emanating from each xi there are three arrows mu sigma square and the associated x hat and so", "tokens": [50364, 11610, 456, 366, 8858, 19669, 457, 729, 8858, 19669, 366, 516, 2293, 8952, 436, 500, 380, 23946, 50660, 50660, 436, 434, 445, 516, 8952, 1296, 2031, 293, 2031, 2385, 291, 393, 574, 412, 309, 300, 636, 293, 370, 577, 867, 19669, 50920, 50920, 366, 28211, 990, 490, 1184, 36800, 456, 366, 1045, 19669, 2992, 12771, 3732, 293, 264, 6615, 2031, 2385, 293, 370, 51304, 51304, 294, 646, 79, 1513, 559, 399, 321, 586, 643, 281, 3079, 264, 5021, 4978, 293, 321, 643, 281, 909, 493, 729, 1045, 15725, 51584, 51624], "temperature": 0.0, "avg_logprob": -0.04229704711748206, "compression_ratio": 1.7945205479452055, "no_speech_prob": 2.4439318622171413e-06}, {"id": 1007, "seek": 627280, "start": 6291.6, "end": 6297.2, "text": " in backpropagation we now need to apply the chain rule and we need to add up those three contributions", "tokens": [50364, 11610, 456, 366, 8858, 19669, 457, 729, 8858, 19669, 366, 516, 2293, 8952, 436, 500, 380, 23946, 50660, 50660, 436, 434, 445, 516, 8952, 1296, 2031, 293, 2031, 2385, 291, 393, 574, 412, 309, 300, 636, 293, 370, 577, 867, 19669, 50920, 50920, 366, 28211, 990, 490, 1184, 36800, 456, 366, 1045, 19669, 2992, 12771, 3732, 293, 264, 6615, 2031, 2385, 293, 370, 51304, 51304, 294, 646, 79, 1513, 559, 399, 321, 586, 643, 281, 3079, 264, 5021, 4978, 293, 321, 643, 281, 909, 493, 729, 1045, 15725, 51584, 51624], "temperature": 0.0, "avg_logprob": -0.04229704711748206, "compression_ratio": 1.7945205479452055, "no_speech_prob": 2.4439318622171413e-06}, {"id": 1008, "seek": 629720, "start": 6297.2, "end": 6303.92, "text": " so here's what that looks like if i just write that out we have uh we're going through we're", "tokens": [50364, 370, 510, 311, 437, 300, 1542, 411, 498, 741, 445, 2464, 300, 484, 321, 362, 2232, 321, 434, 516, 807, 321, 434, 50700, 50700, 417, 3686, 807, 2992, 12771, 3732, 293, 807, 2031, 2385, 293, 729, 1045, 2115, 366, 445, 510, 586, 321, 51028, 51028, 1217, 362, 1045, 295, 613, 321, 362, 37873, 538, 274, 36800, 2385, 321, 362, 37873, 538, 274, 2992, 597, 321, 18949, 510, 51384, 51384, 293, 321, 362, 37873, 538, 274, 12771, 3732, 597, 321, 18949, 510, 457, 321, 643, 1045, 661, 2115, 510, 51636, 51668], "temperature": 0.0, "avg_logprob": -0.12161367872486943, "compression_ratio": 2.0444444444444443, "no_speech_prob": 3.1874803880782565e-06}, {"id": 1009, "seek": 629720, "start": 6303.92, "end": 6310.48, "text": " chaining through mu sigma square and through x hat and those three terms are just here now we", "tokens": [50364, 370, 510, 311, 437, 300, 1542, 411, 498, 741, 445, 2464, 300, 484, 321, 362, 2232, 321, 434, 516, 807, 321, 434, 50700, 50700, 417, 3686, 807, 2992, 12771, 3732, 293, 807, 2031, 2385, 293, 729, 1045, 2115, 366, 445, 510, 586, 321, 51028, 51028, 1217, 362, 1045, 295, 613, 321, 362, 37873, 538, 274, 36800, 2385, 321, 362, 37873, 538, 274, 2992, 597, 321, 18949, 510, 51384, 51384, 293, 321, 362, 37873, 538, 274, 12771, 3732, 597, 321, 18949, 510, 457, 321, 643, 1045, 661, 2115, 510, 51636, 51668], "temperature": 0.0, "avg_logprob": -0.12161367872486943, "compression_ratio": 2.0444444444444443, "no_speech_prob": 3.1874803880782565e-06}, {"id": 1010, "seek": 629720, "start": 6310.48, "end": 6317.599999999999, "text": " already have three of these we have dl by d xi hat we have dl by d mu which we derived here", "tokens": [50364, 370, 510, 311, 437, 300, 1542, 411, 498, 741, 445, 2464, 300, 484, 321, 362, 2232, 321, 434, 516, 807, 321, 434, 50700, 50700, 417, 3686, 807, 2992, 12771, 3732, 293, 807, 2031, 2385, 293, 729, 1045, 2115, 366, 445, 510, 586, 321, 51028, 51028, 1217, 362, 1045, 295, 613, 321, 362, 37873, 538, 274, 36800, 2385, 321, 362, 37873, 538, 274, 2992, 597, 321, 18949, 510, 51384, 51384, 293, 321, 362, 37873, 538, 274, 12771, 3732, 597, 321, 18949, 510, 457, 321, 643, 1045, 661, 2115, 510, 51636, 51668], "temperature": 0.0, "avg_logprob": -0.12161367872486943, "compression_ratio": 2.0444444444444443, "no_speech_prob": 3.1874803880782565e-06}, {"id": 1011, "seek": 629720, "start": 6317.599999999999, "end": 6322.639999999999, "text": " and we have dl by d sigma square which we derived here but we need three other terms here", "tokens": [50364, 370, 510, 311, 437, 300, 1542, 411, 498, 741, 445, 2464, 300, 484, 321, 362, 2232, 321, 434, 516, 807, 321, 434, 50700, 50700, 417, 3686, 807, 2992, 12771, 3732, 293, 807, 2031, 2385, 293, 729, 1045, 2115, 366, 445, 510, 586, 321, 51028, 51028, 1217, 362, 1045, 295, 613, 321, 362, 37873, 538, 274, 36800, 2385, 321, 362, 37873, 538, 274, 2992, 597, 321, 18949, 510, 51384, 51384, 293, 321, 362, 37873, 538, 274, 12771, 3732, 597, 321, 18949, 510, 457, 321, 643, 1045, 661, 2115, 510, 51636, 51668], "temperature": 0.0, "avg_logprob": -0.12161367872486943, "compression_ratio": 2.0444444444444443, "no_speech_prob": 3.1874803880782565e-06}, {"id": 1012, "seek": 632264, "start": 6322.64, "end": 6328.240000000001, "text": " the this one this one and this one so i invite you to try to derive them it's not that complicated", "tokens": [50364, 264, 341, 472, 341, 472, 293, 341, 472, 370, 741, 7980, 291, 281, 853, 281, 28446, 552, 309, 311, 406, 300, 6179, 50644, 50644, 291, 434, 445, 1237, 412, 613, 15277, 510, 293, 27372, 990, 365, 3104, 281, 36800, 50832, 50936, 370, 976, 309, 257, 3347, 457, 510, 311, 264, 1874, 51040, 51156, 420, 412, 1935, 437, 741, 658, 1105, 1338, 741, 478, 445, 741, 478, 445, 27372, 990, 365, 3104, 281, 36800, 337, 439, 613, 51452, 51452, 15277, 293, 6095, 741, 500, 380, 519, 456, 311, 1340, 886, 12414, 510, 309, 311, 3875, 33400, 51640, 51708], "temperature": 0.0, "avg_logprob": -0.16454347937998146, "compression_ratio": 1.8944954128440368, "no_speech_prob": 1.963781187441782e-06}, {"id": 1013, "seek": 632264, "start": 6328.240000000001, "end": 6332.0, "text": " you're just looking at these expressions here and differentiating with respect to xi", "tokens": [50364, 264, 341, 472, 341, 472, 293, 341, 472, 370, 741, 7980, 291, 281, 853, 281, 28446, 552, 309, 311, 406, 300, 6179, 50644, 50644, 291, 434, 445, 1237, 412, 613, 15277, 510, 293, 27372, 990, 365, 3104, 281, 36800, 50832, 50936, 370, 976, 309, 257, 3347, 457, 510, 311, 264, 1874, 51040, 51156, 420, 412, 1935, 437, 741, 658, 1105, 1338, 741, 478, 445, 741, 478, 445, 27372, 990, 365, 3104, 281, 36800, 337, 439, 613, 51452, 51452, 15277, 293, 6095, 741, 500, 380, 519, 456, 311, 1340, 886, 12414, 510, 309, 311, 3875, 33400, 51640, 51708], "temperature": 0.0, "avg_logprob": -0.16454347937998146, "compression_ratio": 1.8944954128440368, "no_speech_prob": 1.963781187441782e-06}, {"id": 1014, "seek": 632264, "start": 6334.08, "end": 6336.160000000001, "text": " so give it a shot but here's the result", "tokens": [50364, 264, 341, 472, 341, 472, 293, 341, 472, 370, 741, 7980, 291, 281, 853, 281, 28446, 552, 309, 311, 406, 300, 6179, 50644, 50644, 291, 434, 445, 1237, 412, 613, 15277, 510, 293, 27372, 990, 365, 3104, 281, 36800, 50832, 50936, 370, 976, 309, 257, 3347, 457, 510, 311, 264, 1874, 51040, 51156, 420, 412, 1935, 437, 741, 658, 1105, 1338, 741, 478, 445, 741, 478, 445, 27372, 990, 365, 3104, 281, 36800, 337, 439, 613, 51452, 51452, 15277, 293, 6095, 741, 500, 380, 519, 456, 311, 1340, 886, 12414, 510, 309, 311, 3875, 33400, 51640, 51708], "temperature": 0.0, "avg_logprob": -0.16454347937998146, "compression_ratio": 1.8944954128440368, "no_speech_prob": 1.963781187441782e-06}, {"id": 1015, "seek": 632264, "start": 6338.4800000000005, "end": 6344.400000000001, "text": " or at least what i got um yeah i'm just i'm just differentiating with respect to xi for all these", "tokens": [50364, 264, 341, 472, 341, 472, 293, 341, 472, 370, 741, 7980, 291, 281, 853, 281, 28446, 552, 309, 311, 406, 300, 6179, 50644, 50644, 291, 434, 445, 1237, 412, 613, 15277, 510, 293, 27372, 990, 365, 3104, 281, 36800, 50832, 50936, 370, 976, 309, 257, 3347, 457, 510, 311, 264, 1874, 51040, 51156, 420, 412, 1935, 437, 741, 658, 1105, 1338, 741, 478, 445, 741, 478, 445, 27372, 990, 365, 3104, 281, 36800, 337, 439, 613, 51452, 51452, 15277, 293, 6095, 741, 500, 380, 519, 456, 311, 1340, 886, 12414, 510, 309, 311, 3875, 33400, 51640, 51708], "temperature": 0.0, "avg_logprob": -0.16454347937998146, "compression_ratio": 1.8944954128440368, "no_speech_prob": 1.963781187441782e-06}, {"id": 1016, "seek": 632264, "start": 6344.400000000001, "end": 6348.160000000001, "text": " expressions and honestly i don't think there's anything too tricky here it's basic calculus", "tokens": [50364, 264, 341, 472, 341, 472, 293, 341, 472, 370, 741, 7980, 291, 281, 853, 281, 28446, 552, 309, 311, 406, 300, 6179, 50644, 50644, 291, 434, 445, 1237, 412, 613, 15277, 510, 293, 27372, 990, 365, 3104, 281, 36800, 50832, 50936, 370, 976, 309, 257, 3347, 457, 510, 311, 264, 1874, 51040, 51156, 420, 412, 1935, 437, 741, 658, 1105, 1338, 741, 478, 445, 741, 478, 445, 27372, 990, 365, 3104, 281, 36800, 337, 439, 613, 51452, 51452, 15277, 293, 6095, 741, 500, 380, 519, 456, 311, 1340, 886, 12414, 510, 309, 311, 3875, 33400, 51640, 51708], "temperature": 0.0, "avg_logprob": -0.16454347937998146, "compression_ratio": 1.8944954128440368, "no_speech_prob": 1.963781187441782e-06}, {"id": 1017, "seek": 634816, "start": 6348.16, "end": 6352.24, "text": " now what gets a little bit more tricky is we are now going to plug everything together so all of", "tokens": [50364, 586, 437, 2170, 257, 707, 857, 544, 12414, 307, 321, 366, 586, 516, 281, 5452, 1203, 1214, 370, 439, 295, 50568, 50568, 613, 2115, 17207, 365, 439, 295, 613, 2115, 293, 3869, 493, 4650, 281, 341, 8513, 293, 300, 2170, 50812, 50812, 257, 707, 857, 42346, 370, 437, 5314, 493, 2737, 307, 291, 483, 257, 2416, 6114, 293, 264, 551, 281, 312, 588, 51240, 51240, 5026, 365, 510, 295, 1164, 307, 321, 366, 1364, 365, 257, 37873, 538, 274, 36800, 337, 2685, 741, 510, 457, 562, 321, 366, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.13466484650321628, "compression_ratio": 1.8679245283018868, "no_speech_prob": 1.6280085901598795e-06}, {"id": 1018, "seek": 634816, "start": 6352.24, "end": 6357.12, "text": " these terms multiplied with all of these terms and added up according to this formula and that gets", "tokens": [50364, 586, 437, 2170, 257, 707, 857, 544, 12414, 307, 321, 366, 586, 516, 281, 5452, 1203, 1214, 370, 439, 295, 50568, 50568, 613, 2115, 17207, 365, 439, 295, 613, 2115, 293, 3869, 493, 4650, 281, 341, 8513, 293, 300, 2170, 50812, 50812, 257, 707, 857, 42346, 370, 437, 5314, 493, 2737, 307, 291, 483, 257, 2416, 6114, 293, 264, 551, 281, 312, 588, 51240, 51240, 5026, 365, 510, 295, 1164, 307, 321, 366, 1364, 365, 257, 37873, 538, 274, 36800, 337, 2685, 741, 510, 457, 562, 321, 366, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.13466484650321628, "compression_ratio": 1.8679245283018868, "no_speech_prob": 1.6280085901598795e-06}, {"id": 1019, "seek": 634816, "start": 6357.12, "end": 6365.68, "text": " a little bit hairy so what ends up happening is you get a large expression and the thing to be very", "tokens": [50364, 586, 437, 2170, 257, 707, 857, 544, 12414, 307, 321, 366, 586, 516, 281, 5452, 1203, 1214, 370, 439, 295, 50568, 50568, 613, 2115, 17207, 365, 439, 295, 613, 2115, 293, 3869, 493, 4650, 281, 341, 8513, 293, 300, 2170, 50812, 50812, 257, 707, 857, 42346, 370, 437, 5314, 493, 2737, 307, 291, 483, 257, 2416, 6114, 293, 264, 551, 281, 312, 588, 51240, 51240, 5026, 365, 510, 295, 1164, 307, 321, 366, 1364, 365, 257, 37873, 538, 274, 36800, 337, 2685, 741, 510, 457, 562, 321, 366, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.13466484650321628, "compression_ratio": 1.8679245283018868, "no_speech_prob": 1.6280085901598795e-06}, {"id": 1020, "seek": 634816, "start": 6365.68, "end": 6372.32, "text": " careful with here of course is we are working with a dl by d xi for specific i here but when we are", "tokens": [50364, 586, 437, 2170, 257, 707, 857, 544, 12414, 307, 321, 366, 586, 516, 281, 5452, 1203, 1214, 370, 439, 295, 50568, 50568, 613, 2115, 17207, 365, 439, 295, 613, 2115, 293, 3869, 493, 4650, 281, 341, 8513, 293, 300, 2170, 50812, 50812, 257, 707, 857, 42346, 370, 437, 5314, 493, 2737, 307, 291, 483, 257, 2416, 6114, 293, 264, 551, 281, 312, 588, 51240, 51240, 5026, 365, 510, 295, 1164, 307, 321, 366, 1364, 365, 257, 37873, 538, 274, 36800, 337, 2685, 741, 510, 457, 562, 321, 366, 51572, 51572], "temperature": 0.0, "avg_logprob": -0.13466484650321628, "compression_ratio": 1.8679245283018868, "no_speech_prob": 1.6280085901598795e-06}, {"id": 1021, "seek": 637232, "start": 6372.32, "end": 6380.799999999999, "text": " plugging in some of these terms like say this term here dl by d sigma squared you see how dl by d", "tokens": [50364, 42975, 294, 512, 295, 613, 2115, 411, 584, 341, 1433, 510, 37873, 538, 274, 12771, 8889, 291, 536, 577, 37873, 538, 274, 50788, 50788, 12771, 8889, 741, 917, 493, 365, 364, 6114, 293, 741, 478, 17138, 990, 670, 707, 741, 311, 510, 457, 741, 393, 380, 764, 51076, 51076, 741, 382, 264, 7006, 562, 741, 5452, 294, 510, 570, 341, 307, 257, 819, 741, 490, 341, 741, 341, 741, 510, 307, 445, 257, 51396, 51396, 1081, 20480, 411, 257, 2654, 7006, 337, 257, 337, 6367, 294, 510, 370, 510, 562, 741, 5452, 300, 294, 291, 3449, 300, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11532031899631613, "compression_ratio": 1.812785388127854, "no_speech_prob": 5.68226369068725e-06}, {"id": 1022, "seek": 637232, "start": 6380.799999999999, "end": 6386.5599999999995, "text": " sigma squared i end up with an expression and i'm iterating over little i's here but i can't use", "tokens": [50364, 42975, 294, 512, 295, 613, 2115, 411, 584, 341, 1433, 510, 37873, 538, 274, 12771, 8889, 291, 536, 577, 37873, 538, 274, 50788, 50788, 12771, 8889, 741, 917, 493, 365, 364, 6114, 293, 741, 478, 17138, 990, 670, 707, 741, 311, 510, 457, 741, 393, 380, 764, 51076, 51076, 741, 382, 264, 7006, 562, 741, 5452, 294, 510, 570, 341, 307, 257, 819, 741, 490, 341, 741, 341, 741, 510, 307, 445, 257, 51396, 51396, 1081, 20480, 411, 257, 2654, 7006, 337, 257, 337, 6367, 294, 510, 370, 510, 562, 741, 5452, 300, 294, 291, 3449, 300, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11532031899631613, "compression_ratio": 1.812785388127854, "no_speech_prob": 5.68226369068725e-06}, {"id": 1023, "seek": 637232, "start": 6386.5599999999995, "end": 6392.96, "text": " i as the variable when i plug in here because this is a different i from this i this i here is just a", "tokens": [50364, 42975, 294, 512, 295, 613, 2115, 411, 584, 341, 1433, 510, 37873, 538, 274, 12771, 8889, 291, 536, 577, 37873, 538, 274, 50788, 50788, 12771, 8889, 741, 917, 493, 365, 364, 6114, 293, 741, 478, 17138, 990, 670, 707, 741, 311, 510, 457, 741, 393, 380, 764, 51076, 51076, 741, 382, 264, 7006, 562, 741, 5452, 294, 510, 570, 341, 307, 257, 819, 741, 490, 341, 741, 341, 741, 510, 307, 445, 257, 51396, 51396, 1081, 20480, 411, 257, 2654, 7006, 337, 257, 337, 6367, 294, 510, 370, 510, 562, 741, 5452, 300, 294, 291, 3449, 300, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11532031899631613, "compression_ratio": 1.812785388127854, "no_speech_prob": 5.68226369068725e-06}, {"id": 1024, "seek": 637232, "start": 6392.96, "end": 6398.719999999999, "text": " placeholder like a local variable for a for loop in here so here when i plug that in you notice that", "tokens": [50364, 42975, 294, 512, 295, 613, 2115, 411, 584, 341, 1433, 510, 37873, 538, 274, 12771, 8889, 291, 536, 577, 37873, 538, 274, 50788, 50788, 12771, 8889, 741, 917, 493, 365, 364, 6114, 293, 741, 478, 17138, 990, 670, 707, 741, 311, 510, 457, 741, 393, 380, 764, 51076, 51076, 741, 382, 264, 7006, 562, 741, 5452, 294, 510, 570, 341, 307, 257, 819, 741, 490, 341, 741, 341, 741, 510, 307, 445, 257, 51396, 51396, 1081, 20480, 411, 257, 2654, 7006, 337, 257, 337, 6367, 294, 510, 370, 510, 562, 741, 5452, 300, 294, 291, 3449, 300, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.11532031899631613, "compression_ratio": 1.812785388127854, "no_speech_prob": 5.68226369068725e-06}, {"id": 1025, "seek": 639872, "start": 6398.72, "end": 6404.88, "text": " i rename the i to a j because i need to make sure that this j is not that this j is not this i", "tokens": [50364, 741, 36741, 264, 741, 281, 257, 361, 570, 741, 643, 281, 652, 988, 300, 341, 361, 307, 406, 300, 341, 361, 307, 406, 341, 741, 50672, 50672, 341, 361, 307, 411, 411, 257, 707, 2654, 17138, 1639, 670, 8858, 2115, 293, 370, 291, 362, 281, 312, 5026, 365, 300, 50948, 50948, 562, 291, 434, 42975, 294, 264, 15277, 490, 510, 281, 510, 291, 815, 362, 281, 36741, 741, 311, 666, 361, 311, 51164, 51164, 291, 362, 281, 312, 588, 5026, 437, 307, 767, 364, 741, 365, 3104, 281, 37873, 538, 274, 36800, 370, 512, 295, 613, 366, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.08665583393361309, "compression_ratio": 1.8028169014084507, "no_speech_prob": 1.3006540939386468e-05}, {"id": 1026, "seek": 639872, "start": 6404.88, "end": 6410.400000000001, "text": " this j is like like a little local iterator over 32 terms and so you have to be careful with that", "tokens": [50364, 741, 36741, 264, 741, 281, 257, 361, 570, 741, 643, 281, 652, 988, 300, 341, 361, 307, 406, 300, 341, 361, 307, 406, 341, 741, 50672, 50672, 341, 361, 307, 411, 411, 257, 707, 2654, 17138, 1639, 670, 8858, 2115, 293, 370, 291, 362, 281, 312, 5026, 365, 300, 50948, 50948, 562, 291, 434, 42975, 294, 264, 15277, 490, 510, 281, 510, 291, 815, 362, 281, 36741, 741, 311, 666, 361, 311, 51164, 51164, 291, 362, 281, 312, 588, 5026, 437, 307, 767, 364, 741, 365, 3104, 281, 37873, 538, 274, 36800, 370, 512, 295, 613, 366, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.08665583393361309, "compression_ratio": 1.8028169014084507, "no_speech_prob": 1.3006540939386468e-05}, {"id": 1027, "seek": 639872, "start": 6410.400000000001, "end": 6414.72, "text": " when you're plugging in the expressions from here to here you may have to rename i's into j's", "tokens": [50364, 741, 36741, 264, 741, 281, 257, 361, 570, 741, 643, 281, 652, 988, 300, 341, 361, 307, 406, 300, 341, 361, 307, 406, 341, 741, 50672, 50672, 341, 361, 307, 411, 411, 257, 707, 2654, 17138, 1639, 670, 8858, 2115, 293, 370, 291, 362, 281, 312, 5026, 365, 300, 50948, 50948, 562, 291, 434, 42975, 294, 264, 15277, 490, 510, 281, 510, 291, 815, 362, 281, 36741, 741, 311, 666, 361, 311, 51164, 51164, 291, 362, 281, 312, 588, 5026, 437, 307, 767, 364, 741, 365, 3104, 281, 37873, 538, 274, 36800, 370, 512, 295, 613, 366, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.08665583393361309, "compression_ratio": 1.8028169014084507, "no_speech_prob": 1.3006540939386468e-05}, {"id": 1028, "seek": 639872, "start": 6414.72, "end": 6422.08, "text": " you have to be very careful what is actually an i with respect to dl by d xi so some of these are", "tokens": [50364, 741, 36741, 264, 741, 281, 257, 361, 570, 741, 643, 281, 652, 988, 300, 341, 361, 307, 406, 300, 341, 361, 307, 406, 341, 741, 50672, 50672, 341, 361, 307, 411, 411, 257, 707, 2654, 17138, 1639, 670, 8858, 2115, 293, 370, 291, 362, 281, 312, 5026, 365, 300, 50948, 50948, 562, 291, 434, 42975, 294, 264, 15277, 490, 510, 281, 510, 291, 815, 362, 281, 36741, 741, 311, 666, 361, 311, 51164, 51164, 291, 362, 281, 312, 588, 5026, 437, 307, 767, 364, 741, 365, 3104, 281, 37873, 538, 274, 36800, 370, 512, 295, 613, 366, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.08665583393361309, "compression_ratio": 1.8028169014084507, "no_speech_prob": 1.3006540939386468e-05}, {"id": 1029, "seek": 642208, "start": 6422.08, "end": 6430.0, "text": " j's some of these are i's and then we simplify this expression and i guess like the big thing", "tokens": [50364, 361, 311, 512, 295, 613, 366, 741, 311, 293, 550, 321, 20460, 341, 6114, 293, 741, 2041, 411, 264, 955, 551, 50760, 50760, 281, 3449, 510, 307, 257, 3840, 295, 2115, 445, 733, 295, 808, 484, 281, 264, 1868, 293, 291, 393, 1895, 15104, 552, 50988, 50988, 456, 311, 257, 12771, 8889, 1804, 17889, 6005, 281, 264, 1347, 295, 3671, 1045, 670, 732, 51144, 51144, 341, 12771, 8889, 1804, 17889, 393, 312, 767, 12005, 484, 666, 1045, 2115, 51360, 51360, 1184, 295, 552, 366, 12771, 8889, 1804, 17889, 281, 264, 3671, 472, 670, 732, 370, 264, 1045, 295, 552, 17207, 51644, 51696], "temperature": 0.0, "avg_logprob": -0.1340342668386606, "compression_ratio": 1.96137339055794, "no_speech_prob": 2.947871735159424e-06}, {"id": 1030, "seek": 642208, "start": 6430.0, "end": 6434.5599999999995, "text": " to notice here is a bunch of terms just kind of come out to the front and you can refactor them", "tokens": [50364, 361, 311, 512, 295, 613, 366, 741, 311, 293, 550, 321, 20460, 341, 6114, 293, 741, 2041, 411, 264, 955, 551, 50760, 50760, 281, 3449, 510, 307, 257, 3840, 295, 2115, 445, 733, 295, 808, 484, 281, 264, 1868, 293, 291, 393, 1895, 15104, 552, 50988, 50988, 456, 311, 257, 12771, 8889, 1804, 17889, 6005, 281, 264, 1347, 295, 3671, 1045, 670, 732, 51144, 51144, 341, 12771, 8889, 1804, 17889, 393, 312, 767, 12005, 484, 666, 1045, 2115, 51360, 51360, 1184, 295, 552, 366, 12771, 8889, 1804, 17889, 281, 264, 3671, 472, 670, 732, 370, 264, 1045, 295, 552, 17207, 51644, 51696], "temperature": 0.0, "avg_logprob": -0.1340342668386606, "compression_ratio": 1.96137339055794, "no_speech_prob": 2.947871735159424e-06}, {"id": 1031, "seek": 642208, "start": 6434.5599999999995, "end": 6437.68, "text": " there's a sigma squared plus epsilon raised to the power of negative three over two", "tokens": [50364, 361, 311, 512, 295, 613, 366, 741, 311, 293, 550, 321, 20460, 341, 6114, 293, 741, 2041, 411, 264, 955, 551, 50760, 50760, 281, 3449, 510, 307, 257, 3840, 295, 2115, 445, 733, 295, 808, 484, 281, 264, 1868, 293, 291, 393, 1895, 15104, 552, 50988, 50988, 456, 311, 257, 12771, 8889, 1804, 17889, 6005, 281, 264, 1347, 295, 3671, 1045, 670, 732, 51144, 51144, 341, 12771, 8889, 1804, 17889, 393, 312, 767, 12005, 484, 666, 1045, 2115, 51360, 51360, 1184, 295, 552, 366, 12771, 8889, 1804, 17889, 281, 264, 3671, 472, 670, 732, 370, 264, 1045, 295, 552, 17207, 51644, 51696], "temperature": 0.0, "avg_logprob": -0.1340342668386606, "compression_ratio": 1.96137339055794, "no_speech_prob": 2.947871735159424e-06}, {"id": 1032, "seek": 642208, "start": 6437.68, "end": 6442.0, "text": " this sigma squared plus epsilon can be actually separated out into three terms", "tokens": [50364, 361, 311, 512, 295, 613, 366, 741, 311, 293, 550, 321, 20460, 341, 6114, 293, 741, 2041, 411, 264, 955, 551, 50760, 50760, 281, 3449, 510, 307, 257, 3840, 295, 2115, 445, 733, 295, 808, 484, 281, 264, 1868, 293, 291, 393, 1895, 15104, 552, 50988, 50988, 456, 311, 257, 12771, 8889, 1804, 17889, 6005, 281, 264, 1347, 295, 3671, 1045, 670, 732, 51144, 51144, 341, 12771, 8889, 1804, 17889, 393, 312, 767, 12005, 484, 666, 1045, 2115, 51360, 51360, 1184, 295, 552, 366, 12771, 8889, 1804, 17889, 281, 264, 3671, 472, 670, 732, 370, 264, 1045, 295, 552, 17207, 51644, 51696], "temperature": 0.0, "avg_logprob": -0.1340342668386606, "compression_ratio": 1.96137339055794, "no_speech_prob": 2.947871735159424e-06}, {"id": 1033, "seek": 642208, "start": 6442.0, "end": 6447.68, "text": " each of them are sigma squared plus epsilon to the negative one over two so the three of them multiplied", "tokens": [50364, 361, 311, 512, 295, 613, 366, 741, 311, 293, 550, 321, 20460, 341, 6114, 293, 741, 2041, 411, 264, 955, 551, 50760, 50760, 281, 3449, 510, 307, 257, 3840, 295, 2115, 445, 733, 295, 808, 484, 281, 264, 1868, 293, 291, 393, 1895, 15104, 552, 50988, 50988, 456, 311, 257, 12771, 8889, 1804, 17889, 6005, 281, 264, 1347, 295, 3671, 1045, 670, 732, 51144, 51144, 341, 12771, 8889, 1804, 17889, 393, 312, 767, 12005, 484, 666, 1045, 2115, 51360, 51360, 1184, 295, 552, 366, 12771, 8889, 1804, 17889, 281, 264, 3671, 472, 670, 732, 370, 264, 1045, 295, 552, 17207, 51644, 51696], "temperature": 0.0, "avg_logprob": -0.1340342668386606, "compression_ratio": 1.96137339055794, "no_speech_prob": 2.947871735159424e-06}, {"id": 1034, "seek": 644768, "start": 6447.68, "end": 6452.96, "text": " is equal to this and then those three terms can go different places because of the multiplication", "tokens": [50364, 307, 2681, 281, 341, 293, 550, 729, 1045, 2115, 393, 352, 819, 3190, 570, 295, 264, 27290, 50628, 50628, 370, 472, 295, 552, 767, 1487, 484, 281, 264, 1868, 293, 486, 917, 493, 510, 2380, 472, 295, 552, 24397, 493, 365, 50936, 50964, 341, 1433, 293, 472, 295, 552, 24397, 493, 365, 341, 661, 1433, 293, 550, 562, 291, 20460, 264, 6114, 51216, 51216, 291, 603, 3449, 300, 512, 295, 613, 2115, 300, 366, 1348, 484, 366, 445, 264, 36800, 20549, 370, 291, 393, 20460, 51512, 51512, 445, 538, 319, 19868, 300, 293, 437, 321, 767, 360, 307, 321, 2935, 20460, 264, 6114, 293, 550, 321, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.2448775378140536, "compression_ratio": 2.0714285714285716, "no_speech_prob": 6.083428161218762e-07}, {"id": 1035, "seek": 644768, "start": 6452.96, "end": 6459.12, "text": " so one of them actually comes out to the front and will end up here outside one of them joins up with", "tokens": [50364, 307, 2681, 281, 341, 293, 550, 729, 1045, 2115, 393, 352, 819, 3190, 570, 295, 264, 27290, 50628, 50628, 370, 472, 295, 552, 767, 1487, 484, 281, 264, 1868, 293, 486, 917, 493, 510, 2380, 472, 295, 552, 24397, 493, 365, 50936, 50964, 341, 1433, 293, 472, 295, 552, 24397, 493, 365, 341, 661, 1433, 293, 550, 562, 291, 20460, 264, 6114, 51216, 51216, 291, 603, 3449, 300, 512, 295, 613, 2115, 300, 366, 1348, 484, 366, 445, 264, 36800, 20549, 370, 291, 393, 20460, 51512, 51512, 445, 538, 319, 19868, 300, 293, 437, 321, 767, 360, 307, 321, 2935, 20460, 264, 6114, 293, 550, 321, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.2448775378140536, "compression_ratio": 2.0714285714285716, "no_speech_prob": 6.083428161218762e-07}, {"id": 1036, "seek": 644768, "start": 6459.68, "end": 6464.72, "text": " this term and one of them joins up with this other term and then when you simplify the expression", "tokens": [50364, 307, 2681, 281, 341, 293, 550, 729, 1045, 2115, 393, 352, 819, 3190, 570, 295, 264, 27290, 50628, 50628, 370, 472, 295, 552, 767, 1487, 484, 281, 264, 1868, 293, 486, 917, 493, 510, 2380, 472, 295, 552, 24397, 493, 365, 50936, 50964, 341, 1433, 293, 472, 295, 552, 24397, 493, 365, 341, 661, 1433, 293, 550, 562, 291, 20460, 264, 6114, 51216, 51216, 291, 603, 3449, 300, 512, 295, 613, 2115, 300, 366, 1348, 484, 366, 445, 264, 36800, 20549, 370, 291, 393, 20460, 51512, 51512, 445, 538, 319, 19868, 300, 293, 437, 321, 767, 360, 307, 321, 2935, 20460, 264, 6114, 293, 550, 321, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.2448775378140536, "compression_ratio": 2.0714285714285716, "no_speech_prob": 6.083428161218762e-07}, {"id": 1037, "seek": 644768, "start": 6464.72, "end": 6470.64, "text": " you'll notice that some of these terms that are coming out are just the xi hats so you can simplify", "tokens": [50364, 307, 2681, 281, 341, 293, 550, 729, 1045, 2115, 393, 352, 819, 3190, 570, 295, 264, 27290, 50628, 50628, 370, 472, 295, 552, 767, 1487, 484, 281, 264, 1868, 293, 486, 917, 493, 510, 2380, 472, 295, 552, 24397, 493, 365, 50936, 50964, 341, 1433, 293, 472, 295, 552, 24397, 493, 365, 341, 661, 1433, 293, 550, 562, 291, 20460, 264, 6114, 51216, 51216, 291, 603, 3449, 300, 512, 295, 613, 2115, 300, 366, 1348, 484, 366, 445, 264, 36800, 20549, 370, 291, 393, 20460, 51512, 51512, 445, 538, 319, 19868, 300, 293, 437, 321, 767, 360, 307, 321, 2935, 20460, 264, 6114, 293, 550, 321, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.2448775378140536, "compression_ratio": 2.0714285714285716, "no_speech_prob": 6.083428161218762e-07}, {"id": 1038, "seek": 644768, "start": 6470.64, "end": 6476.96, "text": " just by rewriting that and what we actually do is we simply simplify the expression and then we", "tokens": [50364, 307, 2681, 281, 341, 293, 550, 729, 1045, 2115, 393, 352, 819, 3190, 570, 295, 264, 27290, 50628, 50628, 370, 472, 295, 552, 767, 1487, 484, 281, 264, 1868, 293, 486, 917, 493, 510, 2380, 472, 295, 552, 24397, 493, 365, 50936, 50964, 341, 1433, 293, 472, 295, 552, 24397, 493, 365, 341, 661, 1433, 293, 550, 562, 291, 20460, 264, 6114, 51216, 51216, 291, 603, 3449, 300, 512, 295, 613, 2115, 300, 366, 1348, 484, 366, 445, 264, 36800, 20549, 370, 291, 393, 20460, 51512, 51512, 445, 538, 319, 19868, 300, 293, 437, 321, 767, 360, 307, 321, 2935, 20460, 264, 6114, 293, 550, 321, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.2448775378140536, "compression_ratio": 2.0714285714285716, "no_speech_prob": 6.083428161218762e-07}, {"id": 1039, "seek": 647696, "start": 6476.96, "end": 6481.12, "text": " do the same thing with the other terms and what we end up with at the end is a fairly simple", "tokens": [50364, 360, 264, 912, 551, 365, 264, 661, 2115, 293, 437, 321, 917, 493, 365, 412, 264, 917, 307, 257, 6457, 2199, 50572, 50572, 18894, 6114, 670, 510, 300, 741, 2644, 20460, 3052, 457, 1936, 291, 603, 3449, 300, 50824, 50824, 309, 787, 4960, 264, 1507, 321, 362, 293, 309, 1163, 1539, 264, 551, 321, 643, 370, 321, 362, 37873, 538, 14584, 337, 439, 264, 741, 311, 51192, 51192, 293, 729, 366, 1143, 7140, 295, 1413, 510, 293, 611, 294, 4500, 437, 321, 434, 1228, 307, 613, 36800, 20549, 293, 51484, 51484, 2031, 73, 20549, 293, 436, 445, 808, 490, 264, 2128, 1320, 293, 5911, 341, 307, 257, 2199, 6114, 293, 309, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.20502045672872793, "compression_ratio": 1.8803088803088803, "no_speech_prob": 4.8602114475215785e-06}, {"id": 1040, "seek": 647696, "start": 6481.12, "end": 6486.16, "text": " mathematical expression over here that i cannot simplify further but basically you'll notice that", "tokens": [50364, 360, 264, 912, 551, 365, 264, 661, 2115, 293, 437, 321, 917, 493, 365, 412, 264, 917, 307, 257, 6457, 2199, 50572, 50572, 18894, 6114, 670, 510, 300, 741, 2644, 20460, 3052, 457, 1936, 291, 603, 3449, 300, 50824, 50824, 309, 787, 4960, 264, 1507, 321, 362, 293, 309, 1163, 1539, 264, 551, 321, 643, 370, 321, 362, 37873, 538, 14584, 337, 439, 264, 741, 311, 51192, 51192, 293, 729, 366, 1143, 7140, 295, 1413, 510, 293, 611, 294, 4500, 437, 321, 434, 1228, 307, 613, 36800, 20549, 293, 51484, 51484, 2031, 73, 20549, 293, 436, 445, 808, 490, 264, 2128, 1320, 293, 5911, 341, 307, 257, 2199, 6114, 293, 309, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.20502045672872793, "compression_ratio": 1.8803088803088803, "no_speech_prob": 4.8602114475215785e-06}, {"id": 1041, "seek": 647696, "start": 6486.16, "end": 6493.52, "text": " it only uses the stuff we have and it derives the thing we need so we have dl by dy for all the i's", "tokens": [50364, 360, 264, 912, 551, 365, 264, 661, 2115, 293, 437, 321, 917, 493, 365, 412, 264, 917, 307, 257, 6457, 2199, 50572, 50572, 18894, 6114, 670, 510, 300, 741, 2644, 20460, 3052, 457, 1936, 291, 603, 3449, 300, 50824, 50824, 309, 787, 4960, 264, 1507, 321, 362, 293, 309, 1163, 1539, 264, 551, 321, 643, 370, 321, 362, 37873, 538, 14584, 337, 439, 264, 741, 311, 51192, 51192, 293, 729, 366, 1143, 7140, 295, 1413, 510, 293, 611, 294, 4500, 437, 321, 434, 1228, 307, 613, 36800, 20549, 293, 51484, 51484, 2031, 73, 20549, 293, 436, 445, 808, 490, 264, 2128, 1320, 293, 5911, 341, 307, 257, 2199, 6114, 293, 309, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.20502045672872793, "compression_ratio": 1.8803088803088803, "no_speech_prob": 4.8602114475215785e-06}, {"id": 1042, "seek": 647696, "start": 6493.52, "end": 6499.36, "text": " and those are used plenty of times here and also in addition what we're using is these xi hats and", "tokens": [50364, 360, 264, 912, 551, 365, 264, 661, 2115, 293, 437, 321, 917, 493, 365, 412, 264, 917, 307, 257, 6457, 2199, 50572, 50572, 18894, 6114, 670, 510, 300, 741, 2644, 20460, 3052, 457, 1936, 291, 603, 3449, 300, 50824, 50824, 309, 787, 4960, 264, 1507, 321, 362, 293, 309, 1163, 1539, 264, 551, 321, 643, 370, 321, 362, 37873, 538, 14584, 337, 439, 264, 741, 311, 51192, 51192, 293, 729, 366, 1143, 7140, 295, 1413, 510, 293, 611, 294, 4500, 437, 321, 434, 1228, 307, 613, 36800, 20549, 293, 51484, 51484, 2031, 73, 20549, 293, 436, 445, 808, 490, 264, 2128, 1320, 293, 5911, 341, 307, 257, 2199, 6114, 293, 309, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.20502045672872793, "compression_ratio": 1.8803088803088803, "no_speech_prob": 4.8602114475215785e-06}, {"id": 1043, "seek": 647696, "start": 6499.36, "end": 6505.28, "text": " xj hats and they just come from the forward pass and otherwise this is a simple expression and it", "tokens": [50364, 360, 264, 912, 551, 365, 264, 661, 2115, 293, 437, 321, 917, 493, 365, 412, 264, 917, 307, 257, 6457, 2199, 50572, 50572, 18894, 6114, 670, 510, 300, 741, 2644, 20460, 3052, 457, 1936, 291, 603, 3449, 300, 50824, 50824, 309, 787, 4960, 264, 1507, 321, 362, 293, 309, 1163, 1539, 264, 551, 321, 643, 370, 321, 362, 37873, 538, 14584, 337, 439, 264, 741, 311, 51192, 51192, 293, 729, 366, 1143, 7140, 295, 1413, 510, 293, 611, 294, 4500, 437, 321, 434, 1228, 307, 613, 36800, 20549, 293, 51484, 51484, 2031, 73, 20549, 293, 436, 445, 808, 490, 264, 2128, 1320, 293, 5911, 341, 307, 257, 2199, 6114, 293, 309, 51780, 51780], "temperature": 0.0, "avg_logprob": -0.20502045672872793, "compression_ratio": 1.8803088803088803, "no_speech_prob": 4.8602114475215785e-06}, {"id": 1044, "seek": 650528, "start": 6505.28, "end": 6511.599999999999, "text": " xi for all the i's and that's ultimately what we're interested in so that's the end of a batch norm", "tokens": [50364, 36800, 337, 439, 264, 741, 311, 293, 300, 311, 6284, 437, 321, 434, 3102, 294, 370, 300, 311, 264, 917, 295, 257, 15245, 2026, 50680, 50716, 23897, 1320, 10783, 984, 718, 311, 586, 4445, 341, 2572, 1874, 1392, 370, 741, 12270, 264, 50992, 50992, 6114, 666, 257, 2167, 1622, 295, 3089, 510, 293, 291, 393, 536, 300, 264, 11469, 7593, 307, 5870, 370, 341, 51276, 51276, 307, 264, 3006, 11420, 295, 341, 8513, 586, 741, 603, 445, 2232, 1936, 980, 291, 300, 1242, 341, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.08526391874660146, "compression_ratio": 1.7488584474885844, "no_speech_prob": 2.4438909349555615e-06}, {"id": 1045, "seek": 650528, "start": 6512.32, "end": 6517.84, "text": " backward pass analytically let's now implement this final result okay so i implemented the", "tokens": [50364, 36800, 337, 439, 264, 741, 311, 293, 300, 311, 6284, 437, 321, 434, 3102, 294, 370, 300, 311, 264, 917, 295, 257, 15245, 2026, 50680, 50716, 23897, 1320, 10783, 984, 718, 311, 586, 4445, 341, 2572, 1874, 1392, 370, 741, 12270, 264, 50992, 50992, 6114, 666, 257, 2167, 1622, 295, 3089, 510, 293, 291, 393, 536, 300, 264, 11469, 7593, 307, 5870, 370, 341, 51276, 51276, 307, 264, 3006, 11420, 295, 341, 8513, 586, 741, 603, 445, 2232, 1936, 980, 291, 300, 1242, 341, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.08526391874660146, "compression_ratio": 1.7488584474885844, "no_speech_prob": 2.4438909349555615e-06}, {"id": 1046, "seek": 650528, "start": 6517.84, "end": 6523.5199999999995, "text": " expression into a single line of code here and you can see that the max diff is tiny so this", "tokens": [50364, 36800, 337, 439, 264, 741, 311, 293, 300, 311, 6284, 437, 321, 434, 3102, 294, 370, 300, 311, 264, 917, 295, 257, 15245, 2026, 50680, 50716, 23897, 1320, 10783, 984, 718, 311, 586, 4445, 341, 2572, 1874, 1392, 370, 741, 12270, 264, 50992, 50992, 6114, 666, 257, 2167, 1622, 295, 3089, 510, 293, 291, 393, 536, 300, 264, 11469, 7593, 307, 5870, 370, 341, 51276, 51276, 307, 264, 3006, 11420, 295, 341, 8513, 586, 741, 603, 445, 2232, 1936, 980, 291, 300, 1242, 341, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.08526391874660146, "compression_ratio": 1.7488584474885844, "no_speech_prob": 2.4438909349555615e-06}, {"id": 1047, "seek": 650528, "start": 6523.5199999999995, "end": 6530.4, "text": " is the correct implementation of this formula now i'll just uh basically tell you that getting this", "tokens": [50364, 36800, 337, 439, 264, 741, 311, 293, 300, 311, 6284, 437, 321, 434, 3102, 294, 370, 300, 311, 264, 917, 295, 257, 15245, 2026, 50680, 50716, 23897, 1320, 10783, 984, 718, 311, 586, 4445, 341, 2572, 1874, 1392, 370, 741, 12270, 264, 50992, 50992, 6114, 666, 257, 2167, 1622, 295, 3089, 510, 293, 291, 393, 536, 300, 264, 11469, 7593, 307, 5870, 370, 341, 51276, 51276, 307, 264, 3006, 11420, 295, 341, 8513, 586, 741, 603, 445, 2232, 1936, 980, 291, 300, 1242, 341, 51620, 51620], "temperature": 0.0, "avg_logprob": -0.08526391874660146, "compression_ratio": 1.7488584474885844, "no_speech_prob": 2.4438909349555615e-06}, {"id": 1048, "seek": 653040, "start": 6530.4, "end": 6535.759999999999, "text": " formula here from this mathematical expression was not trivial and there's a lot going on packed into", "tokens": [50364, 8513, 510, 490, 341, 18894, 6114, 390, 406, 26703, 293, 456, 311, 257, 688, 516, 322, 13265, 666, 50632, 50632, 341, 472, 8513, 293, 341, 307, 257, 1379, 5380, 538, 2564, 570, 291, 362, 281, 1949, 264, 1186, 300, 50896, 50896, 341, 8513, 510, 307, 445, 337, 257, 2167, 34090, 293, 257, 15245, 295, 8858, 5110, 457, 437, 741, 478, 884, 510, 307, 51200, 51200, 741, 478, 767, 321, 767, 362, 12145, 22027, 293, 370, 341, 6114, 575, 281, 294, 8952, 13059, 264, 51496, 51496, 1151, 490, 23897, 1320, 337, 439, 295, 729, 12145, 22027, 294, 8952, 21761, 370, 341, 575, 281, 1051, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.04160200992477275, "compression_ratio": 1.837037037037037, "no_speech_prob": 4.565857125271577e-06}, {"id": 1049, "seek": 653040, "start": 6535.759999999999, "end": 6541.04, "text": " this one formula and this is a whole exercise by itself because you have to consider the fact that", "tokens": [50364, 8513, 510, 490, 341, 18894, 6114, 390, 406, 26703, 293, 456, 311, 257, 688, 516, 322, 13265, 666, 50632, 50632, 341, 472, 8513, 293, 341, 307, 257, 1379, 5380, 538, 2564, 570, 291, 362, 281, 1949, 264, 1186, 300, 50896, 50896, 341, 8513, 510, 307, 445, 337, 257, 2167, 34090, 293, 257, 15245, 295, 8858, 5110, 457, 437, 741, 478, 884, 510, 307, 51200, 51200, 741, 478, 767, 321, 767, 362, 12145, 22027, 293, 370, 341, 6114, 575, 281, 294, 8952, 13059, 264, 51496, 51496, 1151, 490, 23897, 1320, 337, 439, 295, 729, 12145, 22027, 294, 8952, 21761, 370, 341, 575, 281, 1051, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.04160200992477275, "compression_ratio": 1.837037037037037, "no_speech_prob": 4.565857125271577e-06}, {"id": 1050, "seek": 653040, "start": 6541.04, "end": 6547.12, "text": " this formula here is just for a single neuron and a batch of 32 examples but what i'm doing here is", "tokens": [50364, 8513, 510, 490, 341, 18894, 6114, 390, 406, 26703, 293, 456, 311, 257, 688, 516, 322, 13265, 666, 50632, 50632, 341, 472, 8513, 293, 341, 307, 257, 1379, 5380, 538, 2564, 570, 291, 362, 281, 1949, 264, 1186, 300, 50896, 50896, 341, 8513, 510, 307, 445, 337, 257, 2167, 34090, 293, 257, 15245, 295, 8858, 5110, 457, 437, 741, 478, 884, 510, 307, 51200, 51200, 741, 478, 767, 321, 767, 362, 12145, 22027, 293, 370, 341, 6114, 575, 281, 294, 8952, 13059, 264, 51496, 51496, 1151, 490, 23897, 1320, 337, 439, 295, 729, 12145, 22027, 294, 8952, 21761, 370, 341, 575, 281, 1051, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.04160200992477275, "compression_ratio": 1.837037037037037, "no_speech_prob": 4.565857125271577e-06}, {"id": 1051, "seek": 653040, "start": 6547.12, "end": 6553.04, "text": " i'm actually we actually have 64 neurons and so this expression has to in parallel evaluate the", "tokens": [50364, 8513, 510, 490, 341, 18894, 6114, 390, 406, 26703, 293, 456, 311, 257, 688, 516, 322, 13265, 666, 50632, 50632, 341, 472, 8513, 293, 341, 307, 257, 1379, 5380, 538, 2564, 570, 291, 362, 281, 1949, 264, 1186, 300, 50896, 50896, 341, 8513, 510, 307, 445, 337, 257, 2167, 34090, 293, 257, 15245, 295, 8858, 5110, 457, 437, 741, 478, 884, 510, 307, 51200, 51200, 741, 478, 767, 321, 767, 362, 12145, 22027, 293, 370, 341, 6114, 575, 281, 294, 8952, 13059, 264, 51496, 51496, 1151, 490, 23897, 1320, 337, 439, 295, 729, 12145, 22027, 294, 8952, 21761, 370, 341, 575, 281, 1051, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.04160200992477275, "compression_ratio": 1.837037037037037, "no_speech_prob": 4.565857125271577e-06}, {"id": 1052, "seek": 653040, "start": 6553.04, "end": 6558.32, "text": " best from backward pass for all of those 64 neurons in parallel independently so this has to happen", "tokens": [50364, 8513, 510, 490, 341, 18894, 6114, 390, 406, 26703, 293, 456, 311, 257, 688, 516, 322, 13265, 666, 50632, 50632, 341, 472, 8513, 293, 341, 307, 257, 1379, 5380, 538, 2564, 570, 291, 362, 281, 1949, 264, 1186, 300, 50896, 50896, 341, 8513, 510, 307, 445, 337, 257, 2167, 34090, 293, 257, 15245, 295, 8858, 5110, 457, 437, 741, 478, 884, 510, 307, 51200, 51200, 741, 478, 767, 321, 767, 362, 12145, 22027, 293, 370, 341, 6114, 575, 281, 294, 8952, 13059, 264, 51496, 51496, 1151, 490, 23897, 1320, 337, 439, 295, 729, 12145, 22027, 294, 8952, 21761, 370, 341, 575, 281, 1051, 51760, 51760], "temperature": 0.0, "avg_logprob": -0.04160200992477275, "compression_ratio": 1.837037037037037, "no_speech_prob": 4.565857125271577e-06}, {"id": 1053, "seek": 655832, "start": 6558.32, "end": 6566.4, "text": " basically in every single column of the inputs here and in addition to that you see how there", "tokens": [50364, 1936, 294, 633, 2167, 7738, 295, 264, 15743, 510, 293, 294, 4500, 281, 300, 291, 536, 577, 456, 50768, 50768, 366, 257, 3840, 295, 34499, 510, 293, 321, 643, 281, 652, 988, 300, 562, 741, 360, 729, 34499, 300, 436, 9975, 50988, 50988, 8944, 3911, 1203, 1646, 300, 311, 510, 293, 370, 1242, 341, 6114, 307, 445, 411, 5405, 2107, 51232, 51232, 26703, 293, 741, 7980, 291, 281, 1936, 574, 807, 309, 293, 1823, 807, 309, 293, 309, 311, 257, 1379, 5380, 51404, 51404, 281, 652, 988, 300, 341, 341, 13834, 484, 457, 1564, 439, 264, 10854, 3986, 293, 1564, 291, 13447, 1803, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.029589361119493146, "compression_ratio": 1.8587786259541985, "no_speech_prob": 6.048734576324932e-06}, {"id": 1054, "seek": 655832, "start": 6566.4, "end": 6570.799999999999, "text": " are a bunch of sums here and we need to make sure that when i do those sums that they broadcast", "tokens": [50364, 1936, 294, 633, 2167, 7738, 295, 264, 15743, 510, 293, 294, 4500, 281, 300, 291, 536, 577, 456, 50768, 50768, 366, 257, 3840, 295, 34499, 510, 293, 321, 643, 281, 652, 988, 300, 562, 741, 360, 729, 34499, 300, 436, 9975, 50988, 50988, 8944, 3911, 1203, 1646, 300, 311, 510, 293, 370, 1242, 341, 6114, 307, 445, 411, 5405, 2107, 51232, 51232, 26703, 293, 741, 7980, 291, 281, 1936, 574, 807, 309, 293, 1823, 807, 309, 293, 309, 311, 257, 1379, 5380, 51404, 51404, 281, 652, 988, 300, 341, 341, 13834, 484, 457, 1564, 439, 264, 10854, 3986, 293, 1564, 291, 13447, 1803, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.029589361119493146, "compression_ratio": 1.8587786259541985, "no_speech_prob": 6.048734576324932e-06}, {"id": 1055, "seek": 655832, "start": 6570.799999999999, "end": 6575.679999999999, "text": " correctly onto everything else that's here and so getting this expression is just like highly non", "tokens": [50364, 1936, 294, 633, 2167, 7738, 295, 264, 15743, 510, 293, 294, 4500, 281, 300, 291, 536, 577, 456, 50768, 50768, 366, 257, 3840, 295, 34499, 510, 293, 321, 643, 281, 652, 988, 300, 562, 741, 360, 729, 34499, 300, 436, 9975, 50988, 50988, 8944, 3911, 1203, 1646, 300, 311, 510, 293, 370, 1242, 341, 6114, 307, 445, 411, 5405, 2107, 51232, 51232, 26703, 293, 741, 7980, 291, 281, 1936, 574, 807, 309, 293, 1823, 807, 309, 293, 309, 311, 257, 1379, 5380, 51404, 51404, 281, 652, 988, 300, 341, 341, 13834, 484, 457, 1564, 439, 264, 10854, 3986, 293, 1564, 291, 13447, 1803, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.029589361119493146, "compression_ratio": 1.8587786259541985, "no_speech_prob": 6.048734576324932e-06}, {"id": 1056, "seek": 655832, "start": 6575.679999999999, "end": 6579.12, "text": " trivial and i invite you to basically look through it and step through it and it's a whole exercise", "tokens": [50364, 1936, 294, 633, 2167, 7738, 295, 264, 15743, 510, 293, 294, 4500, 281, 300, 291, 536, 577, 456, 50768, 50768, 366, 257, 3840, 295, 34499, 510, 293, 321, 643, 281, 652, 988, 300, 562, 741, 360, 729, 34499, 300, 436, 9975, 50988, 50988, 8944, 3911, 1203, 1646, 300, 311, 510, 293, 370, 1242, 341, 6114, 307, 445, 411, 5405, 2107, 51232, 51232, 26703, 293, 741, 7980, 291, 281, 1936, 574, 807, 309, 293, 1823, 807, 309, 293, 309, 311, 257, 1379, 5380, 51404, 51404, 281, 652, 988, 300, 341, 341, 13834, 484, 457, 1564, 439, 264, 10854, 3986, 293, 1564, 291, 13447, 1803, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.029589361119493146, "compression_ratio": 1.8587786259541985, "no_speech_prob": 6.048734576324932e-06}, {"id": 1057, "seek": 655832, "start": 6579.12, "end": 6585.84, "text": " to make sure that this this checks out but once all the shapes agree and once you convince yourself", "tokens": [50364, 1936, 294, 633, 2167, 7738, 295, 264, 15743, 510, 293, 294, 4500, 281, 300, 291, 536, 577, 456, 50768, 50768, 366, 257, 3840, 295, 34499, 510, 293, 321, 643, 281, 652, 988, 300, 562, 741, 360, 729, 34499, 300, 436, 9975, 50988, 50988, 8944, 3911, 1203, 1646, 300, 311, 510, 293, 370, 1242, 341, 6114, 307, 445, 411, 5405, 2107, 51232, 51232, 26703, 293, 741, 7980, 291, 281, 1936, 574, 807, 309, 293, 1823, 807, 309, 293, 309, 311, 257, 1379, 5380, 51404, 51404, 281, 652, 988, 300, 341, 341, 13834, 484, 457, 1564, 439, 264, 10854, 3986, 293, 1564, 291, 13447, 1803, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.029589361119493146, "compression_ratio": 1.8587786259541985, "no_speech_prob": 6.048734576324932e-06}, {"id": 1058, "seek": 658584, "start": 6585.84, "end": 6590.64, "text": " that it's correct you can also verify that petros gets the exact same answer as well and so that", "tokens": [50364, 300, 309, 311, 3006, 291, 393, 611, 16888, 300, 3817, 2635, 2170, 264, 1900, 912, 1867, 382, 731, 293, 370, 300, 50604, 50604, 2709, 291, 257, 688, 295, 4336, 295, 1575, 300, 341, 18894, 8513, 307, 8944, 12270, 50840, 50840, 510, 293, 9975, 292, 8944, 293, 46365, 294, 8952, 337, 439, 295, 264, 12145, 22027, 1854, 341, 51152, 51152, 15245, 1433, 4583, 1392, 293, 2721, 5380, 1230, 1451, 8962, 291, 281, 829, 309, 439, 1214, 51388, 51416, 293, 510, 321, 362, 257, 14328, 5194, 849, 295, 264, 2302, 1154, 370, 291, 536, 300, 321, 319, 12, 259, 270, 831, 1125, 264, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.12442017509823754, "compression_ratio": 1.7255639097744362, "no_speech_prob": 6.048535396985244e-06}, {"id": 1059, "seek": 658584, "start": 6590.64, "end": 6595.360000000001, "text": " gives you a lot of peace of mind that this mathematical formula is correctly implemented", "tokens": [50364, 300, 309, 311, 3006, 291, 393, 611, 16888, 300, 3817, 2635, 2170, 264, 1900, 912, 1867, 382, 731, 293, 370, 300, 50604, 50604, 2709, 291, 257, 688, 295, 4336, 295, 1575, 300, 341, 18894, 8513, 307, 8944, 12270, 50840, 50840, 510, 293, 9975, 292, 8944, 293, 46365, 294, 8952, 337, 439, 295, 264, 12145, 22027, 1854, 341, 51152, 51152, 15245, 1433, 4583, 1392, 293, 2721, 5380, 1230, 1451, 8962, 291, 281, 829, 309, 439, 1214, 51388, 51416, 293, 510, 321, 362, 257, 14328, 5194, 849, 295, 264, 2302, 1154, 370, 291, 536, 300, 321, 319, 12, 259, 270, 831, 1125, 264, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.12442017509823754, "compression_ratio": 1.7255639097744362, "no_speech_prob": 6.048535396985244e-06}, {"id": 1060, "seek": 658584, "start": 6595.360000000001, "end": 6601.6, "text": " here and broadcasted correctly and replicated in parallel for all of the 64 neurons inside this", "tokens": [50364, 300, 309, 311, 3006, 291, 393, 611, 16888, 300, 3817, 2635, 2170, 264, 1900, 912, 1867, 382, 731, 293, 370, 300, 50604, 50604, 2709, 291, 257, 688, 295, 4336, 295, 1575, 300, 341, 18894, 8513, 307, 8944, 12270, 50840, 50840, 510, 293, 9975, 292, 8944, 293, 46365, 294, 8952, 337, 439, 295, 264, 12145, 22027, 1854, 341, 51152, 51152, 15245, 1433, 4583, 1392, 293, 2721, 5380, 1230, 1451, 8962, 291, 281, 829, 309, 439, 1214, 51388, 51416, 293, 510, 321, 362, 257, 14328, 5194, 849, 295, 264, 2302, 1154, 370, 291, 536, 300, 321, 319, 12, 259, 270, 831, 1125, 264, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.12442017509823754, "compression_ratio": 1.7255639097744362, "no_speech_prob": 6.048535396985244e-06}, {"id": 1061, "seek": 658584, "start": 6601.6, "end": 6606.32, "text": " batch term layer okay and finally exercise number four asks you to put it all together", "tokens": [50364, 300, 309, 311, 3006, 291, 393, 611, 16888, 300, 3817, 2635, 2170, 264, 1900, 912, 1867, 382, 731, 293, 370, 300, 50604, 50604, 2709, 291, 257, 688, 295, 4336, 295, 1575, 300, 341, 18894, 8513, 307, 8944, 12270, 50840, 50840, 510, 293, 9975, 292, 8944, 293, 46365, 294, 8952, 337, 439, 295, 264, 12145, 22027, 1854, 341, 51152, 51152, 15245, 1433, 4583, 1392, 293, 2721, 5380, 1230, 1451, 8962, 291, 281, 829, 309, 439, 1214, 51388, 51416, 293, 510, 321, 362, 257, 14328, 5194, 849, 295, 264, 2302, 1154, 370, 291, 536, 300, 321, 319, 12, 259, 270, 831, 1125, 264, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.12442017509823754, "compression_ratio": 1.7255639097744362, "no_speech_prob": 6.048535396985244e-06}, {"id": 1062, "seek": 658584, "start": 6606.88, "end": 6611.360000000001, "text": " and here we have a redefinition of the entire problem so you see that we re-initialize the", "tokens": [50364, 300, 309, 311, 3006, 291, 393, 611, 16888, 300, 3817, 2635, 2170, 264, 1900, 912, 1867, 382, 731, 293, 370, 300, 50604, 50604, 2709, 291, 257, 688, 295, 4336, 295, 1575, 300, 341, 18894, 8513, 307, 8944, 12270, 50840, 50840, 510, 293, 9975, 292, 8944, 293, 46365, 294, 8952, 337, 439, 295, 264, 12145, 22027, 1854, 341, 51152, 51152, 15245, 1433, 4583, 1392, 293, 2721, 5380, 1230, 1451, 8962, 291, 281, 829, 309, 439, 1214, 51388, 51416, 293, 510, 321, 362, 257, 14328, 5194, 849, 295, 264, 2302, 1154, 370, 291, 536, 300, 321, 319, 12, 259, 270, 831, 1125, 264, 51640, 51640], "temperature": 0.0, "avg_logprob": -0.12442017509823754, "compression_ratio": 1.7255639097744362, "no_speech_prob": 6.048535396985244e-06}, {"id": 1063, "seek": 661136, "start": 6611.36, "end": 6616.719999999999, "text": " neural net from scratch and everything and then here instead of calling loss that backward we", "tokens": [50364, 18161, 2533, 490, 8459, 293, 1203, 293, 550, 510, 2602, 295, 5141, 4470, 300, 23897, 321, 50632, 50632, 528, 281, 362, 264, 9688, 646, 79, 1513, 559, 399, 510, 382, 321, 18949, 309, 493, 3673, 370, 352, 493, 5055, 9163, 439, 264, 50932, 50932, 24004, 295, 3089, 300, 321, 600, 1217, 18949, 829, 552, 510, 293, 28446, 428, 1065, 2771, 2448, 293, 550, 51176, 51176, 19719, 341, 18161, 2533, 1936, 1228, 428, 1065, 2771, 2448, 439, 264, 636, 281, 264, 38732, 295, 264, 51440, 51440, 15245, 2026, 293, 264, 13344, 295, 264, 4470, 293, 741, 390, 1075, 281, 4584, 1596, 257, 665, 4470, 1936, 264, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.05320181977858237, "compression_ratio": 1.88715953307393, "no_speech_prob": 3.340379862493137e-06}, {"id": 1064, "seek": 661136, "start": 6616.719999999999, "end": 6622.719999999999, "text": " want to have the manual backpropagation here as we derived it up above so go up copy paste all the", "tokens": [50364, 18161, 2533, 490, 8459, 293, 1203, 293, 550, 510, 2602, 295, 5141, 4470, 300, 23897, 321, 50632, 50632, 528, 281, 362, 264, 9688, 646, 79, 1513, 559, 399, 510, 382, 321, 18949, 309, 493, 3673, 370, 352, 493, 5055, 9163, 439, 264, 50932, 50932, 24004, 295, 3089, 300, 321, 600, 1217, 18949, 829, 552, 510, 293, 28446, 428, 1065, 2771, 2448, 293, 550, 51176, 51176, 19719, 341, 18161, 2533, 1936, 1228, 428, 1065, 2771, 2448, 439, 264, 636, 281, 264, 38732, 295, 264, 51440, 51440, 15245, 2026, 293, 264, 13344, 295, 264, 4470, 293, 741, 390, 1075, 281, 4584, 1596, 257, 665, 4470, 1936, 264, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.05320181977858237, "compression_ratio": 1.88715953307393, "no_speech_prob": 3.340379862493137e-06}, {"id": 1065, "seek": 661136, "start": 6622.719999999999, "end": 6627.599999999999, "text": " chunks of code that we've already derived put them here and derive your own gradients and then", "tokens": [50364, 18161, 2533, 490, 8459, 293, 1203, 293, 550, 510, 2602, 295, 5141, 4470, 300, 23897, 321, 50632, 50632, 528, 281, 362, 264, 9688, 646, 79, 1513, 559, 399, 510, 382, 321, 18949, 309, 493, 3673, 370, 352, 493, 5055, 9163, 439, 264, 50932, 50932, 24004, 295, 3089, 300, 321, 600, 1217, 18949, 829, 552, 510, 293, 28446, 428, 1065, 2771, 2448, 293, 550, 51176, 51176, 19719, 341, 18161, 2533, 1936, 1228, 428, 1065, 2771, 2448, 439, 264, 636, 281, 264, 38732, 295, 264, 51440, 51440, 15245, 2026, 293, 264, 13344, 295, 264, 4470, 293, 741, 390, 1075, 281, 4584, 1596, 257, 665, 4470, 1936, 264, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.05320181977858237, "compression_ratio": 1.88715953307393, "no_speech_prob": 3.340379862493137e-06}, {"id": 1066, "seek": 661136, "start": 6627.599999999999, "end": 6632.88, "text": " optimize this neural net basically using your own gradients all the way to the calibration of the", "tokens": [50364, 18161, 2533, 490, 8459, 293, 1203, 293, 550, 510, 2602, 295, 5141, 4470, 300, 23897, 321, 50632, 50632, 528, 281, 362, 264, 9688, 646, 79, 1513, 559, 399, 510, 382, 321, 18949, 309, 493, 3673, 370, 352, 493, 5055, 9163, 439, 264, 50932, 50932, 24004, 295, 3089, 300, 321, 600, 1217, 18949, 829, 552, 510, 293, 28446, 428, 1065, 2771, 2448, 293, 550, 51176, 51176, 19719, 341, 18161, 2533, 1936, 1228, 428, 1065, 2771, 2448, 439, 264, 636, 281, 264, 38732, 295, 264, 51440, 51440, 15245, 2026, 293, 264, 13344, 295, 264, 4470, 293, 741, 390, 1075, 281, 4584, 1596, 257, 665, 4470, 1936, 264, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.05320181977858237, "compression_ratio": 1.88715953307393, "no_speech_prob": 3.340379862493137e-06}, {"id": 1067, "seek": 661136, "start": 6632.88, "end": 6637.5199999999995, "text": " batch norm and the evaluation of the loss and i was able to achieve quite a good loss basically the", "tokens": [50364, 18161, 2533, 490, 8459, 293, 1203, 293, 550, 510, 2602, 295, 5141, 4470, 300, 23897, 321, 50632, 50632, 528, 281, 362, 264, 9688, 646, 79, 1513, 559, 399, 510, 382, 321, 18949, 309, 493, 3673, 370, 352, 493, 5055, 9163, 439, 264, 50932, 50932, 24004, 295, 3089, 300, 321, 600, 1217, 18949, 829, 552, 510, 293, 28446, 428, 1065, 2771, 2448, 293, 550, 51176, 51176, 19719, 341, 18161, 2533, 1936, 1228, 428, 1065, 2771, 2448, 439, 264, 636, 281, 264, 38732, 295, 264, 51440, 51440, 15245, 2026, 293, 264, 13344, 295, 264, 4470, 293, 741, 390, 1075, 281, 4584, 1596, 257, 665, 4470, 1936, 264, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.05320181977858237, "compression_ratio": 1.88715953307393, "no_speech_prob": 3.340379862493137e-06}, {"id": 1068, "seek": 663752, "start": 6637.52, "end": 6641.84, "text": " same loss you would achieve before and that shouldn't be surprising because all we've done is", "tokens": [50364, 912, 4470, 291, 576, 4584, 949, 293, 300, 4659, 380, 312, 8830, 570, 439, 321, 600, 1096, 307, 50580, 50580, 321, 600, 534, 5768, 281, 4470, 300, 23897, 293, 321, 600, 7373, 484, 439, 264, 3089, 293, 27992, 309, 510, 457, 50900, 50900, 729, 2771, 2448, 366, 14800, 293, 1203, 307, 14800, 293, 264, 3542, 366, 14800, 309, 311, 445, 51136, 51136, 300, 321, 362, 1577, 19883, 322, 2293, 437, 1709, 322, 833, 264, 13376, 295, 688, 295, 23897, 294, 341, 51388, 51388, 2685, 1389, 1392, 293, 341, 307, 439, 295, 527, 3089, 341, 307, 264, 1577, 23897, 1320, 1228, 1936, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.0884668713524228, "compression_ratio": 1.8358778625954197, "no_speech_prob": 3.7265092487359652e-06}, {"id": 1069, "seek": 663752, "start": 6641.84, "end": 6648.240000000001, "text": " we've really gotten to loss that backward and we've pulled out all the code and inserted it here but", "tokens": [50364, 912, 4470, 291, 576, 4584, 949, 293, 300, 4659, 380, 312, 8830, 570, 439, 321, 600, 1096, 307, 50580, 50580, 321, 600, 534, 5768, 281, 4470, 300, 23897, 293, 321, 600, 7373, 484, 439, 264, 3089, 293, 27992, 309, 510, 457, 50900, 50900, 729, 2771, 2448, 366, 14800, 293, 1203, 307, 14800, 293, 264, 3542, 366, 14800, 309, 311, 445, 51136, 51136, 300, 321, 362, 1577, 19883, 322, 2293, 437, 1709, 322, 833, 264, 13376, 295, 688, 295, 23897, 294, 341, 51388, 51388, 2685, 1389, 1392, 293, 341, 307, 439, 295, 527, 3089, 341, 307, 264, 1577, 23897, 1320, 1228, 1936, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.0884668713524228, "compression_ratio": 1.8358778625954197, "no_speech_prob": 3.7265092487359652e-06}, {"id": 1070, "seek": 663752, "start": 6648.240000000001, "end": 6652.96, "text": " those gradients are identical and everything is identical and the results are identical it's just", "tokens": [50364, 912, 4470, 291, 576, 4584, 949, 293, 300, 4659, 380, 312, 8830, 570, 439, 321, 600, 1096, 307, 50580, 50580, 321, 600, 534, 5768, 281, 4470, 300, 23897, 293, 321, 600, 7373, 484, 439, 264, 3089, 293, 27992, 309, 510, 457, 50900, 50900, 729, 2771, 2448, 366, 14800, 293, 1203, 307, 14800, 293, 264, 3542, 366, 14800, 309, 311, 445, 51136, 51136, 300, 321, 362, 1577, 19883, 322, 2293, 437, 1709, 322, 833, 264, 13376, 295, 688, 295, 23897, 294, 341, 51388, 51388, 2685, 1389, 1392, 293, 341, 307, 439, 295, 527, 3089, 341, 307, 264, 1577, 23897, 1320, 1228, 1936, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.0884668713524228, "compression_ratio": 1.8358778625954197, "no_speech_prob": 3.7265092487359652e-06}, {"id": 1071, "seek": 663752, "start": 6652.96, "end": 6658.0, "text": " that we have full visibility on exactly what goes on under the hood of lot of backward in this", "tokens": [50364, 912, 4470, 291, 576, 4584, 949, 293, 300, 4659, 380, 312, 8830, 570, 439, 321, 600, 1096, 307, 50580, 50580, 321, 600, 534, 5768, 281, 4470, 300, 23897, 293, 321, 600, 7373, 484, 439, 264, 3089, 293, 27992, 309, 510, 457, 50900, 50900, 729, 2771, 2448, 366, 14800, 293, 1203, 307, 14800, 293, 264, 3542, 366, 14800, 309, 311, 445, 51136, 51136, 300, 321, 362, 1577, 19883, 322, 2293, 437, 1709, 322, 833, 264, 13376, 295, 688, 295, 23897, 294, 341, 51388, 51388, 2685, 1389, 1392, 293, 341, 307, 439, 295, 527, 3089, 341, 307, 264, 1577, 23897, 1320, 1228, 1936, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.0884668713524228, "compression_ratio": 1.8358778625954197, "no_speech_prob": 3.7265092487359652e-06}, {"id": 1072, "seek": 663752, "start": 6658.0, "end": 6664.240000000001, "text": " specific case okay and this is all of our code this is the full backward pass using basically", "tokens": [50364, 912, 4470, 291, 576, 4584, 949, 293, 300, 4659, 380, 312, 8830, 570, 439, 321, 600, 1096, 307, 50580, 50580, 321, 600, 534, 5768, 281, 4470, 300, 23897, 293, 321, 600, 7373, 484, 439, 264, 3089, 293, 27992, 309, 510, 457, 50900, 50900, 729, 2771, 2448, 366, 14800, 293, 1203, 307, 14800, 293, 264, 3542, 366, 14800, 309, 311, 445, 51136, 51136, 300, 321, 362, 1577, 19883, 322, 2293, 437, 1709, 322, 833, 264, 13376, 295, 688, 295, 23897, 294, 341, 51388, 51388, 2685, 1389, 1392, 293, 341, 307, 439, 295, 527, 3089, 341, 307, 264, 1577, 23897, 1320, 1228, 1936, 51700, 51700], "temperature": 0.0, "avg_logprob": -0.0884668713524228, "compression_ratio": 1.8358778625954197, "no_speech_prob": 3.7265092487359652e-06}, {"id": 1073, "seek": 666424, "start": 6664.24, "end": 6670.32, "text": " the simplified backward pass for the cross entropy loss and the batch normalization so backpropagating", "tokens": [50364, 264, 26335, 23897, 1320, 337, 264, 3278, 30867, 4470, 293, 264, 15245, 2710, 2144, 370, 646, 79, 1513, 559, 990, 50668, 50668, 807, 3278, 30867, 264, 1150, 4583, 264, 1266, 71, 18184, 8213, 507, 264, 15245, 2710, 2144, 50960, 51016, 807, 264, 700, 4583, 293, 807, 264, 12240, 3584, 293, 370, 291, 536, 300, 341, 307, 787, 1310, 437, 307, 51244, 51244, 341, 945, 3876, 295, 3089, 420, 746, 411, 300, 293, 300, 311, 437, 2709, 505, 2771, 2448, 293, 586, 321, 393, 51516, 51516, 7263, 23525, 4470, 300, 23897, 370, 264, 636, 741, 362, 264, 3089, 992, 493, 307, 291, 820, 312, 1075, 281, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.08908949860739052, "compression_ratio": 1.8449612403100775, "no_speech_prob": 1.1015706604666775e-06}, {"id": 1074, "seek": 666424, "start": 6670.32, "end": 6676.16, "text": " through cross entropy the second layer the 10h null linearity the batch normalization", "tokens": [50364, 264, 26335, 23897, 1320, 337, 264, 3278, 30867, 4470, 293, 264, 15245, 2710, 2144, 370, 646, 79, 1513, 559, 990, 50668, 50668, 807, 3278, 30867, 264, 1150, 4583, 264, 1266, 71, 18184, 8213, 507, 264, 15245, 2710, 2144, 50960, 51016, 807, 264, 700, 4583, 293, 807, 264, 12240, 3584, 293, 370, 291, 536, 300, 341, 307, 787, 1310, 437, 307, 51244, 51244, 341, 945, 3876, 295, 3089, 420, 746, 411, 300, 293, 300, 311, 437, 2709, 505, 2771, 2448, 293, 586, 321, 393, 51516, 51516, 7263, 23525, 4470, 300, 23897, 370, 264, 636, 741, 362, 264, 3089, 992, 493, 307, 291, 820, 312, 1075, 281, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.08908949860739052, "compression_ratio": 1.8449612403100775, "no_speech_prob": 1.1015706604666775e-06}, {"id": 1075, "seek": 666424, "start": 6677.28, "end": 6681.84, "text": " through the first layer and through the embedding and so you see that this is only maybe what is", "tokens": [50364, 264, 26335, 23897, 1320, 337, 264, 3278, 30867, 4470, 293, 264, 15245, 2710, 2144, 370, 646, 79, 1513, 559, 990, 50668, 50668, 807, 3278, 30867, 264, 1150, 4583, 264, 1266, 71, 18184, 8213, 507, 264, 15245, 2710, 2144, 50960, 51016, 807, 264, 700, 4583, 293, 807, 264, 12240, 3584, 293, 370, 291, 536, 300, 341, 307, 787, 1310, 437, 307, 51244, 51244, 341, 945, 3876, 295, 3089, 420, 746, 411, 300, 293, 300, 311, 437, 2709, 505, 2771, 2448, 293, 586, 321, 393, 51516, 51516, 7263, 23525, 4470, 300, 23897, 370, 264, 636, 741, 362, 264, 3089, 992, 493, 307, 291, 820, 312, 1075, 281, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.08908949860739052, "compression_ratio": 1.8449612403100775, "no_speech_prob": 1.1015706604666775e-06}, {"id": 1076, "seek": 666424, "start": 6681.84, "end": 6687.28, "text": " this 20 lines of code or something like that and that's what gives us gradients and now we can", "tokens": [50364, 264, 26335, 23897, 1320, 337, 264, 3278, 30867, 4470, 293, 264, 15245, 2710, 2144, 370, 646, 79, 1513, 559, 990, 50668, 50668, 807, 3278, 30867, 264, 1150, 4583, 264, 1266, 71, 18184, 8213, 507, 264, 15245, 2710, 2144, 50960, 51016, 807, 264, 700, 4583, 293, 807, 264, 12240, 3584, 293, 370, 291, 536, 300, 341, 307, 787, 1310, 437, 307, 51244, 51244, 341, 945, 3876, 295, 3089, 420, 746, 411, 300, 293, 300, 311, 437, 2709, 505, 2771, 2448, 293, 586, 321, 393, 51516, 51516, 7263, 23525, 4470, 300, 23897, 370, 264, 636, 741, 362, 264, 3089, 992, 493, 307, 291, 820, 312, 1075, 281, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.08908949860739052, "compression_ratio": 1.8449612403100775, "no_speech_prob": 1.1015706604666775e-06}, {"id": 1077, "seek": 666424, "start": 6687.28, "end": 6692.32, "text": " potentially erase loss that backward so the way i have the code set up is you should be able to", "tokens": [50364, 264, 26335, 23897, 1320, 337, 264, 3278, 30867, 4470, 293, 264, 15245, 2710, 2144, 370, 646, 79, 1513, 559, 990, 50668, 50668, 807, 3278, 30867, 264, 1150, 4583, 264, 1266, 71, 18184, 8213, 507, 264, 15245, 2710, 2144, 50960, 51016, 807, 264, 700, 4583, 293, 807, 264, 12240, 3584, 293, 370, 291, 536, 300, 341, 307, 787, 1310, 437, 307, 51244, 51244, 341, 945, 3876, 295, 3089, 420, 746, 411, 300, 293, 300, 311, 437, 2709, 505, 2771, 2448, 293, 586, 321, 393, 51516, 51516, 7263, 23525, 4470, 300, 23897, 370, 264, 636, 741, 362, 264, 3089, 992, 493, 307, 291, 820, 312, 1075, 281, 51768, 51768], "temperature": 0.0, "avg_logprob": -0.08908949860739052, "compression_ratio": 1.8449612403100775, "no_speech_prob": 1.1015706604666775e-06}, {"id": 1078, "seek": 669232, "start": 6692.32, "end": 6697.44, "text": " run this entire cell once you fill this in and this will run for only 100 iterations and then break", "tokens": [50364, 1190, 341, 2302, 2815, 1564, 291, 2836, 341, 294, 293, 341, 486, 1190, 337, 787, 2319, 36540, 293, 550, 1821, 50620, 50652, 293, 309, 9857, 570, 309, 2709, 291, 364, 2650, 281, 1520, 428, 2771, 2448, 1970, 25878, 284, 339, 50840, 50908, 370, 510, 527, 2771, 2448, 321, 536, 366, 406, 2293, 2681, 436, 366, 10447, 2681, 293, 264, 51236, 51236, 7300, 366, 5870, 945, 3671, 1722, 420, 370, 293, 741, 500, 380, 2293, 458, 689, 436, 434, 1348, 490, 51468, 51468, 281, 312, 3245, 370, 1564, 321, 362, 512, 6687, 300, 264, 2771, 2448, 366, 1936, 3006, 51668, 51704], "temperature": 0.0, "avg_logprob": -0.05518537876652736, "compression_ratio": 1.7635658914728682, "no_speech_prob": 8.013172191567719e-06}, {"id": 1079, "seek": 669232, "start": 6698.08, "end": 6701.84, "text": " and it breaks because it gives you an opportunity to check your gradients against pytorch", "tokens": [50364, 1190, 341, 2302, 2815, 1564, 291, 2836, 341, 294, 293, 341, 486, 1190, 337, 787, 2319, 36540, 293, 550, 1821, 50620, 50652, 293, 309, 9857, 570, 309, 2709, 291, 364, 2650, 281, 1520, 428, 2771, 2448, 1970, 25878, 284, 339, 50840, 50908, 370, 510, 527, 2771, 2448, 321, 536, 366, 406, 2293, 2681, 436, 366, 10447, 2681, 293, 264, 51236, 51236, 7300, 366, 5870, 945, 3671, 1722, 420, 370, 293, 741, 500, 380, 2293, 458, 689, 436, 434, 1348, 490, 51468, 51468, 281, 312, 3245, 370, 1564, 321, 362, 512, 6687, 300, 264, 2771, 2448, 366, 1936, 3006, 51668, 51704], "temperature": 0.0, "avg_logprob": -0.05518537876652736, "compression_ratio": 1.7635658914728682, "no_speech_prob": 8.013172191567719e-06}, {"id": 1080, "seek": 669232, "start": 6703.2, "end": 6709.759999999999, "text": " so here our gradients we see are not exactly equal they are approximately equal and the", "tokens": [50364, 1190, 341, 2302, 2815, 1564, 291, 2836, 341, 294, 293, 341, 486, 1190, 337, 787, 2319, 36540, 293, 550, 1821, 50620, 50652, 293, 309, 9857, 570, 309, 2709, 291, 364, 2650, 281, 1520, 428, 2771, 2448, 1970, 25878, 284, 339, 50840, 50908, 370, 510, 527, 2771, 2448, 321, 536, 366, 406, 2293, 2681, 436, 366, 10447, 2681, 293, 264, 51236, 51236, 7300, 366, 5870, 945, 3671, 1722, 420, 370, 293, 741, 500, 380, 2293, 458, 689, 436, 434, 1348, 490, 51468, 51468, 281, 312, 3245, 370, 1564, 321, 362, 512, 6687, 300, 264, 2771, 2448, 366, 1936, 3006, 51668, 51704], "temperature": 0.0, "avg_logprob": -0.05518537876652736, "compression_ratio": 1.7635658914728682, "no_speech_prob": 8.013172191567719e-06}, {"id": 1081, "seek": 669232, "start": 6709.759999999999, "end": 6714.4, "text": " differences are tiny 20 negative 9 or so and i don't exactly know where they're coming from", "tokens": [50364, 1190, 341, 2302, 2815, 1564, 291, 2836, 341, 294, 293, 341, 486, 1190, 337, 787, 2319, 36540, 293, 550, 1821, 50620, 50652, 293, 309, 9857, 570, 309, 2709, 291, 364, 2650, 281, 1520, 428, 2771, 2448, 1970, 25878, 284, 339, 50840, 50908, 370, 510, 527, 2771, 2448, 321, 536, 366, 406, 2293, 2681, 436, 366, 10447, 2681, 293, 264, 51236, 51236, 7300, 366, 5870, 945, 3671, 1722, 420, 370, 293, 741, 500, 380, 2293, 458, 689, 436, 434, 1348, 490, 51468, 51468, 281, 312, 3245, 370, 1564, 321, 362, 512, 6687, 300, 264, 2771, 2448, 366, 1936, 3006, 51668, 51704], "temperature": 0.0, "avg_logprob": -0.05518537876652736, "compression_ratio": 1.7635658914728682, "no_speech_prob": 8.013172191567719e-06}, {"id": 1082, "seek": 669232, "start": 6714.4, "end": 6718.4, "text": " to be honest so once we have some confidence that the gradients are basically correct", "tokens": [50364, 1190, 341, 2302, 2815, 1564, 291, 2836, 341, 294, 293, 341, 486, 1190, 337, 787, 2319, 36540, 293, 550, 1821, 50620, 50652, 293, 309, 9857, 570, 309, 2709, 291, 364, 2650, 281, 1520, 428, 2771, 2448, 1970, 25878, 284, 339, 50840, 50908, 370, 510, 527, 2771, 2448, 321, 536, 366, 406, 2293, 2681, 436, 366, 10447, 2681, 293, 264, 51236, 51236, 7300, 366, 5870, 945, 3671, 1722, 420, 370, 293, 741, 500, 380, 2293, 458, 689, 436, 434, 1348, 490, 51468, 51468, 281, 312, 3245, 370, 1564, 321, 362, 512, 6687, 300, 264, 2771, 2448, 366, 1936, 3006, 51668, 51704], "temperature": 0.0, "avg_logprob": -0.05518537876652736, "compression_ratio": 1.7635658914728682, "no_speech_prob": 8.013172191567719e-06}, {"id": 1083, "seek": 671840, "start": 6718.4, "end": 6726.4, "text": " uh we can take out the gradient checking we can disable this breaking statement and then we can", "tokens": [50364, 2232, 321, 393, 747, 484, 264, 16235, 8568, 321, 393, 28362, 341, 7697, 5629, 293, 550, 321, 393, 50764, 50812, 1936, 28362, 4470, 300, 23897, 321, 500, 380, 643, 309, 3602, 3417, 2243, 281, 536, 300, 51096, 51148, 293, 550, 510, 562, 321, 366, 884, 264, 5623, 321, 434, 406, 516, 281, 764, 280, 5893, 2771, 341, 307, 264, 1331, 636, 51408, 51432, 295, 25878, 284, 339, 321, 500, 380, 362, 300, 3602, 570, 321, 434, 406, 884, 23897, 321, 366, 516, 281, 764, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.15653805299238724, "compression_ratio": 1.9322916666666667, "no_speech_prob": 2.5612262106733397e-06}, {"id": 1084, "seek": 671840, "start": 6727.36, "end": 6733.04, "text": " basically disable loss that backward we don't need it anymore feels amazing to see that", "tokens": [50364, 2232, 321, 393, 747, 484, 264, 16235, 8568, 321, 393, 28362, 341, 7697, 5629, 293, 550, 321, 393, 50764, 50812, 1936, 28362, 4470, 300, 23897, 321, 500, 380, 643, 309, 3602, 3417, 2243, 281, 536, 300, 51096, 51148, 293, 550, 510, 562, 321, 366, 884, 264, 5623, 321, 434, 406, 516, 281, 764, 280, 5893, 2771, 341, 307, 264, 1331, 636, 51408, 51432, 295, 25878, 284, 339, 321, 500, 380, 362, 300, 3602, 570, 321, 434, 406, 884, 23897, 321, 366, 516, 281, 764, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.15653805299238724, "compression_ratio": 1.9322916666666667, "no_speech_prob": 2.5612262106733397e-06}, {"id": 1085, "seek": 671840, "start": 6734.08, "end": 6739.28, "text": " and then here when we are doing the update we're not going to use p dot grad this is the old way", "tokens": [50364, 2232, 321, 393, 747, 484, 264, 16235, 8568, 321, 393, 28362, 341, 7697, 5629, 293, 550, 321, 393, 50764, 50812, 1936, 28362, 4470, 300, 23897, 321, 500, 380, 643, 309, 3602, 3417, 2243, 281, 536, 300, 51096, 51148, 293, 550, 510, 562, 321, 366, 884, 264, 5623, 321, 434, 406, 516, 281, 764, 280, 5893, 2771, 341, 307, 264, 1331, 636, 51408, 51432, 295, 25878, 284, 339, 321, 500, 380, 362, 300, 3602, 570, 321, 434, 406, 884, 23897, 321, 366, 516, 281, 764, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.15653805299238724, "compression_ratio": 1.9322916666666667, "no_speech_prob": 2.5612262106733397e-06}, {"id": 1086, "seek": 671840, "start": 6739.759999999999, "end": 6744.5599999999995, "text": " of pytorch we don't have that anymore because we're not doing backward we are going to use", "tokens": [50364, 2232, 321, 393, 747, 484, 264, 16235, 8568, 321, 393, 28362, 341, 7697, 5629, 293, 550, 321, 393, 50764, 50812, 1936, 28362, 4470, 300, 23897, 321, 500, 380, 643, 309, 3602, 3417, 2243, 281, 536, 300, 51096, 51148, 293, 550, 510, 562, 321, 366, 884, 264, 5623, 321, 434, 406, 516, 281, 764, 280, 5893, 2771, 341, 307, 264, 1331, 636, 51408, 51432, 295, 25878, 284, 339, 321, 500, 380, 362, 300, 3602, 570, 321, 434, 406, 884, 23897, 321, 366, 516, 281, 764, 51672, 51672], "temperature": 0.0, "avg_logprob": -0.15653805299238724, "compression_ratio": 1.9322916666666667, "no_speech_prob": 2.5612262106733397e-06}, {"id": 1087, "seek": 674456, "start": 6744.56, "end": 6750.320000000001, "text": " this update where we you see that i'm iterating over i've arranged the grads to be in the same", "tokens": [50364, 341, 5623, 689, 321, 291, 536, 300, 741, 478, 17138, 990, 670, 741, 600, 18721, 264, 2771, 82, 281, 312, 294, 264, 912, 50652, 50652, 1668, 382, 264, 9834, 293, 741, 478, 710, 6297, 552, 493, 264, 2771, 2448, 293, 264, 9834, 666, 280, 293, 2771, 50936, 50936, 293, 550, 510, 741, 478, 516, 281, 1823, 365, 445, 264, 2771, 300, 321, 18949, 16945, 370, 264, 1036, 2522, 51252, 51312, 307, 300, 6022, 295, 341, 586, 7029, 2771, 2448, 490, 25878, 284, 339, 293, 370, 472, 551, 291, 393, 360, 510, 51632, 51720], "temperature": 0.0, "avg_logprob": -0.11097129018683183, "compression_ratio": 1.7942583732057416, "no_speech_prob": 1.0676857300495612e-06}, {"id": 1088, "seek": 674456, "start": 6750.320000000001, "end": 6756.0, "text": " order as the parameters and i'm zipping them up the gradients and the parameters into p and grad", "tokens": [50364, 341, 5623, 689, 321, 291, 536, 300, 741, 478, 17138, 990, 670, 741, 600, 18721, 264, 2771, 82, 281, 312, 294, 264, 912, 50652, 50652, 1668, 382, 264, 9834, 293, 741, 478, 710, 6297, 552, 493, 264, 2771, 2448, 293, 264, 9834, 666, 280, 293, 2771, 50936, 50936, 293, 550, 510, 741, 478, 516, 281, 1823, 365, 445, 264, 2771, 300, 321, 18949, 16945, 370, 264, 1036, 2522, 51252, 51312, 307, 300, 6022, 295, 341, 586, 7029, 2771, 2448, 490, 25878, 284, 339, 293, 370, 472, 551, 291, 393, 360, 510, 51632, 51720], "temperature": 0.0, "avg_logprob": -0.11097129018683183, "compression_ratio": 1.7942583732057416, "no_speech_prob": 1.0676857300495612e-06}, {"id": 1089, "seek": 674456, "start": 6756.0, "end": 6762.320000000001, "text": " and then here i'm going to step with just the grad that we derived manually so the last piece", "tokens": [50364, 341, 5623, 689, 321, 291, 536, 300, 741, 478, 17138, 990, 670, 741, 600, 18721, 264, 2771, 82, 281, 312, 294, 264, 912, 50652, 50652, 1668, 382, 264, 9834, 293, 741, 478, 710, 6297, 552, 493, 264, 2771, 2448, 293, 264, 9834, 666, 280, 293, 2771, 50936, 50936, 293, 550, 510, 741, 478, 516, 281, 1823, 365, 445, 264, 2771, 300, 321, 18949, 16945, 370, 264, 1036, 2522, 51252, 51312, 307, 300, 6022, 295, 341, 586, 7029, 2771, 2448, 490, 25878, 284, 339, 293, 370, 472, 551, 291, 393, 360, 510, 51632, 51720], "temperature": 0.0, "avg_logprob": -0.11097129018683183, "compression_ratio": 1.7942583732057416, "no_speech_prob": 1.0676857300495612e-06}, {"id": 1090, "seek": 674456, "start": 6763.52, "end": 6769.92, "text": " is that none of this now requires gradients from pytorch and so one thing you can do here", "tokens": [50364, 341, 5623, 689, 321, 291, 536, 300, 741, 478, 17138, 990, 670, 741, 600, 18721, 264, 2771, 82, 281, 312, 294, 264, 912, 50652, 50652, 1668, 382, 264, 9834, 293, 741, 478, 710, 6297, 552, 493, 264, 2771, 2448, 293, 264, 9834, 666, 280, 293, 2771, 50936, 50936, 293, 550, 510, 741, 478, 516, 281, 1823, 365, 445, 264, 2771, 300, 321, 18949, 16945, 370, 264, 1036, 2522, 51252, 51312, 307, 300, 6022, 295, 341, 586, 7029, 2771, 2448, 490, 25878, 284, 339, 293, 370, 472, 551, 291, 393, 360, 510, 51632, 51720], "temperature": 0.0, "avg_logprob": -0.11097129018683183, "compression_ratio": 1.7942583732057416, "no_speech_prob": 1.0676857300495612e-06}, {"id": 1091, "seek": 676992, "start": 6769.92, "end": 6776.24, "text": " is you can do with torch dot no grad and offset this whole code block and really what you're saying", "tokens": [50364, 307, 291, 393, 360, 365, 27822, 5893, 572, 2771, 293, 18687, 341, 1379, 3089, 3461, 293, 534, 437, 291, 434, 1566, 50680, 50680, 307, 291, 434, 3585, 25878, 284, 339, 300, 4177, 741, 478, 406, 516, 281, 818, 23897, 322, 604, 295, 341, 293, 341, 4045, 50888, 50888, 25878, 284, 339, 281, 312, 257, 857, 544, 7148, 365, 439, 295, 309, 293, 550, 321, 820, 312, 1075, 281, 445, 2232, 1190, 341, 51180, 51244, 293, 309, 311, 2614, 293, 291, 536, 300, 4470, 23897, 307, 26940, 484, 293, 321, 434, 40425, 51696, 51760], "temperature": 0.0, "avg_logprob": -0.1926729302657278, "compression_ratio": 1.7627906976744185, "no_speech_prob": 1.1910943840121035e-06}, {"id": 1092, "seek": 676992, "start": 6776.24, "end": 6780.4, "text": " is you're telling pytorch that hey i'm not going to call backward on any of this and this allows", "tokens": [50364, 307, 291, 393, 360, 365, 27822, 5893, 572, 2771, 293, 18687, 341, 1379, 3089, 3461, 293, 534, 437, 291, 434, 1566, 50680, 50680, 307, 291, 434, 3585, 25878, 284, 339, 300, 4177, 741, 478, 406, 516, 281, 818, 23897, 322, 604, 295, 341, 293, 341, 4045, 50888, 50888, 25878, 284, 339, 281, 312, 257, 857, 544, 7148, 365, 439, 295, 309, 293, 550, 321, 820, 312, 1075, 281, 445, 2232, 1190, 341, 51180, 51244, 293, 309, 311, 2614, 293, 291, 536, 300, 4470, 23897, 307, 26940, 484, 293, 321, 434, 40425, 51696, 51760], "temperature": 0.0, "avg_logprob": -0.1926729302657278, "compression_ratio": 1.7627906976744185, "no_speech_prob": 1.1910943840121035e-06}, {"id": 1093, "seek": 676992, "start": 6780.4, "end": 6786.24, "text": " pytorch to be a bit more efficient with all of it and then we should be able to just uh run this", "tokens": [50364, 307, 291, 393, 360, 365, 27822, 5893, 572, 2771, 293, 18687, 341, 1379, 3089, 3461, 293, 534, 437, 291, 434, 1566, 50680, 50680, 307, 291, 434, 3585, 25878, 284, 339, 300, 4177, 741, 478, 406, 516, 281, 818, 23897, 322, 604, 295, 341, 293, 341, 4045, 50888, 50888, 25878, 284, 339, 281, 312, 257, 857, 544, 7148, 365, 439, 295, 309, 293, 550, 321, 820, 312, 1075, 281, 445, 2232, 1190, 341, 51180, 51244, 293, 309, 311, 2614, 293, 291, 536, 300, 4470, 23897, 307, 26940, 484, 293, 321, 434, 40425, 51696, 51760], "temperature": 0.0, "avg_logprob": -0.1926729302657278, "compression_ratio": 1.7627906976744185, "no_speech_prob": 1.1910943840121035e-06}, {"id": 1094, "seek": 676992, "start": 6787.52, "end": 6796.56, "text": " and it's running and you see that loss backward is commented out and we're optimizing", "tokens": [50364, 307, 291, 393, 360, 365, 27822, 5893, 572, 2771, 293, 18687, 341, 1379, 3089, 3461, 293, 534, 437, 291, 434, 1566, 50680, 50680, 307, 291, 434, 3585, 25878, 284, 339, 300, 4177, 741, 478, 406, 516, 281, 818, 23897, 322, 604, 295, 341, 293, 341, 4045, 50888, 50888, 25878, 284, 339, 281, 312, 257, 857, 544, 7148, 365, 439, 295, 309, 293, 550, 321, 820, 312, 1075, 281, 445, 2232, 1190, 341, 51180, 51244, 293, 309, 311, 2614, 293, 291, 536, 300, 4470, 23897, 307, 26940, 484, 293, 321, 434, 40425, 51696, 51760], "temperature": 0.0, "avg_logprob": -0.1926729302657278, "compression_ratio": 1.7627906976744185, "no_speech_prob": 1.1910943840121035e-06}, {"id": 1095, "seek": 679656, "start": 6796.56, "end": 6802.88, "text": " so we're going to leave this run and hopefully we get a good result okay so i allowed the neural", "tokens": [50364, 370, 321, 434, 516, 281, 1856, 341, 1190, 293, 4696, 321, 483, 257, 665, 1874, 1392, 370, 741, 4350, 264, 18161, 50680, 50680, 2533, 281, 2413, 19618, 550, 510, 741, 21583, 4404, 264, 15245, 490, 9834, 570, 741, 630, 406, 1066, 50996, 50996, 2837, 295, 264, 2614, 914, 293, 588, 21977, 294, 641, 3097, 6367, 550, 510, 741, 5872, 264, 4470, 51316, 51316, 293, 291, 536, 300, 321, 767, 14879, 257, 1238, 665, 4470, 588, 2531, 281, 437, 321, 600, 11042, 51496, 51496, 949, 293, 550, 510, 741, 478, 21179, 490, 264, 2316, 293, 321, 536, 512, 295, 264, 4190, 300, 321, 600, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2586729103159682, "compression_ratio": 1.7832699619771863, "no_speech_prob": 1.0676914143914473e-06}, {"id": 1096, "seek": 679656, "start": 6802.88, "end": 6809.200000000001, "text": " net to finish optimization then here i calibrate the batch from parameters because i did not keep", "tokens": [50364, 370, 321, 434, 516, 281, 1856, 341, 1190, 293, 4696, 321, 483, 257, 665, 1874, 1392, 370, 741, 4350, 264, 18161, 50680, 50680, 2533, 281, 2413, 19618, 550, 510, 741, 21583, 4404, 264, 15245, 490, 9834, 570, 741, 630, 406, 1066, 50996, 50996, 2837, 295, 264, 2614, 914, 293, 588, 21977, 294, 641, 3097, 6367, 550, 510, 741, 5872, 264, 4470, 51316, 51316, 293, 291, 536, 300, 321, 767, 14879, 257, 1238, 665, 4470, 588, 2531, 281, 437, 321, 600, 11042, 51496, 51496, 949, 293, 550, 510, 741, 478, 21179, 490, 264, 2316, 293, 321, 536, 512, 295, 264, 4190, 300, 321, 600, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2586729103159682, "compression_ratio": 1.7832699619771863, "no_speech_prob": 1.0676914143914473e-06}, {"id": 1097, "seek": 679656, "start": 6809.200000000001, "end": 6815.6, "text": " track of the running mean and very variance in their training loop then here i ran the loss", "tokens": [50364, 370, 321, 434, 516, 281, 1856, 341, 1190, 293, 4696, 321, 483, 257, 665, 1874, 1392, 370, 741, 4350, 264, 18161, 50680, 50680, 2533, 281, 2413, 19618, 550, 510, 741, 21583, 4404, 264, 15245, 490, 9834, 570, 741, 630, 406, 1066, 50996, 50996, 2837, 295, 264, 2614, 914, 293, 588, 21977, 294, 641, 3097, 6367, 550, 510, 741, 5872, 264, 4470, 51316, 51316, 293, 291, 536, 300, 321, 767, 14879, 257, 1238, 665, 4470, 588, 2531, 281, 437, 321, 600, 11042, 51496, 51496, 949, 293, 550, 510, 741, 478, 21179, 490, 264, 2316, 293, 321, 536, 512, 295, 264, 4190, 300, 321, 600, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2586729103159682, "compression_ratio": 1.7832699619771863, "no_speech_prob": 1.0676914143914473e-06}, {"id": 1098, "seek": 679656, "start": 6815.6, "end": 6819.200000000001, "text": " and you see that we actually obtained a pretty good loss very similar to what we've achieved", "tokens": [50364, 370, 321, 434, 516, 281, 1856, 341, 1190, 293, 4696, 321, 483, 257, 665, 1874, 1392, 370, 741, 4350, 264, 18161, 50680, 50680, 2533, 281, 2413, 19618, 550, 510, 741, 21583, 4404, 264, 15245, 490, 9834, 570, 741, 630, 406, 1066, 50996, 50996, 2837, 295, 264, 2614, 914, 293, 588, 21977, 294, 641, 3097, 6367, 550, 510, 741, 5872, 264, 4470, 51316, 51316, 293, 291, 536, 300, 321, 767, 14879, 257, 1238, 665, 4470, 588, 2531, 281, 437, 321, 600, 11042, 51496, 51496, 949, 293, 550, 510, 741, 478, 21179, 490, 264, 2316, 293, 321, 536, 512, 295, 264, 4190, 300, 321, 600, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2586729103159682, "compression_ratio": 1.7832699619771863, "no_speech_prob": 1.0676914143914473e-06}, {"id": 1099, "seek": 679656, "start": 6819.200000000001, "end": 6824.56, "text": " before and then here i'm sampling from the model and we see some of the values that we've", "tokens": [50364, 370, 321, 434, 516, 281, 1856, 341, 1190, 293, 4696, 321, 483, 257, 665, 1874, 1392, 370, 741, 4350, 264, 18161, 50680, 50680, 2533, 281, 2413, 19618, 550, 510, 741, 21583, 4404, 264, 15245, 490, 9834, 570, 741, 630, 406, 1066, 50996, 50996, 2837, 295, 264, 2614, 914, 293, 588, 21977, 294, 641, 3097, 6367, 550, 510, 741, 5872, 264, 4470, 51316, 51316, 293, 291, 536, 300, 321, 767, 14879, 257, 1238, 665, 4470, 588, 2531, 281, 437, 321, 600, 11042, 51496, 51496, 949, 293, 550, 510, 741, 478, 21179, 490, 264, 2316, 293, 321, 536, 512, 295, 264, 4190, 300, 321, 600, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2586729103159682, "compression_ratio": 1.7832699619771863, "no_speech_prob": 1.0676914143914473e-06}, {"id": 1100, "seek": 682456, "start": 6824.56, "end": 6830.4800000000005, "text": " used so basically the model worked and samples uh pretty decent results compared to what we're used", "tokens": [50364, 1143, 370, 1936, 264, 2316, 2732, 293, 10938, 2232, 1238, 8681, 3542, 5347, 281, 437, 321, 434, 1143, 50660, 50660, 281, 370, 1203, 307, 264, 912, 457, 295, 1164, 264, 955, 2028, 307, 300, 321, 630, 406, 764, 3195, 295, 23897, 50908, 50908, 321, 630, 406, 764, 25878, 284, 339, 1476, 664, 6206, 293, 321, 14109, 527, 2771, 2448, 4175, 538, 1011, 293, 370, 4696, 51176, 51176, 291, 434, 1237, 412, 341, 264, 23897, 1320, 295, 341, 18161, 2533, 293, 291, 434, 1953, 281, 1803, 51384, 51384, 767, 300, 311, 406, 886, 6179, 1105, 1184, 472, 295, 613, 7914, 307, 516, 281, 312, 257, 707, 857, 544, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.25999639700124927, "compression_ratio": 1.818867924528302, "no_speech_prob": 3.0894473184162052e-06}, {"id": 1101, "seek": 682456, "start": 6830.4800000000005, "end": 6835.4400000000005, "text": " to so everything is the same but of course the big deal is that we did not use lots of backward", "tokens": [50364, 1143, 370, 1936, 264, 2316, 2732, 293, 10938, 2232, 1238, 8681, 3542, 5347, 281, 437, 321, 434, 1143, 50660, 50660, 281, 370, 1203, 307, 264, 912, 457, 295, 1164, 264, 955, 2028, 307, 300, 321, 630, 406, 764, 3195, 295, 23897, 50908, 50908, 321, 630, 406, 764, 25878, 284, 339, 1476, 664, 6206, 293, 321, 14109, 527, 2771, 2448, 4175, 538, 1011, 293, 370, 4696, 51176, 51176, 291, 434, 1237, 412, 341, 264, 23897, 1320, 295, 341, 18161, 2533, 293, 291, 434, 1953, 281, 1803, 51384, 51384, 767, 300, 311, 406, 886, 6179, 1105, 1184, 472, 295, 613, 7914, 307, 516, 281, 312, 257, 707, 857, 544, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.25999639700124927, "compression_ratio": 1.818867924528302, "no_speech_prob": 3.0894473184162052e-06}, {"id": 1102, "seek": 682456, "start": 6835.4400000000005, "end": 6840.8, "text": " we did not use pytorch autograd and we estimated our gradients ourselves by hand and so hopefully", "tokens": [50364, 1143, 370, 1936, 264, 2316, 2732, 293, 10938, 2232, 1238, 8681, 3542, 5347, 281, 437, 321, 434, 1143, 50660, 50660, 281, 370, 1203, 307, 264, 912, 457, 295, 1164, 264, 955, 2028, 307, 300, 321, 630, 406, 764, 3195, 295, 23897, 50908, 50908, 321, 630, 406, 764, 25878, 284, 339, 1476, 664, 6206, 293, 321, 14109, 527, 2771, 2448, 4175, 538, 1011, 293, 370, 4696, 51176, 51176, 291, 434, 1237, 412, 341, 264, 23897, 1320, 295, 341, 18161, 2533, 293, 291, 434, 1953, 281, 1803, 51384, 51384, 767, 300, 311, 406, 886, 6179, 1105, 1184, 472, 295, 613, 7914, 307, 516, 281, 312, 257, 707, 857, 544, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.25999639700124927, "compression_ratio": 1.818867924528302, "no_speech_prob": 3.0894473184162052e-06}, {"id": 1103, "seek": 682456, "start": 6840.8, "end": 6844.96, "text": " you're looking at this the backward pass of this neural net and you're thinking to yourself", "tokens": [50364, 1143, 370, 1936, 264, 2316, 2732, 293, 10938, 2232, 1238, 8681, 3542, 5347, 281, 437, 321, 434, 1143, 50660, 50660, 281, 370, 1203, 307, 264, 912, 457, 295, 1164, 264, 955, 2028, 307, 300, 321, 630, 406, 764, 3195, 295, 23897, 50908, 50908, 321, 630, 406, 764, 25878, 284, 339, 1476, 664, 6206, 293, 321, 14109, 527, 2771, 2448, 4175, 538, 1011, 293, 370, 4696, 51176, 51176, 291, 434, 1237, 412, 341, 264, 23897, 1320, 295, 341, 18161, 2533, 293, 291, 434, 1953, 281, 1803, 51384, 51384, 767, 300, 311, 406, 886, 6179, 1105, 1184, 472, 295, 613, 7914, 307, 516, 281, 312, 257, 707, 857, 544, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.25999639700124927, "compression_ratio": 1.818867924528302, "no_speech_prob": 3.0894473184162052e-06}, {"id": 1104, "seek": 682456, "start": 6844.96, "end": 6850.96, "text": " actually that's not too complicated um each one of these layers is going to be a little bit more", "tokens": [50364, 1143, 370, 1936, 264, 2316, 2732, 293, 10938, 2232, 1238, 8681, 3542, 5347, 281, 437, 321, 434, 1143, 50660, 50660, 281, 370, 1203, 307, 264, 912, 457, 295, 1164, 264, 955, 2028, 307, 300, 321, 630, 406, 764, 3195, 295, 23897, 50908, 50908, 321, 630, 406, 764, 25878, 284, 339, 1476, 664, 6206, 293, 321, 14109, 527, 2771, 2448, 4175, 538, 1011, 293, 370, 4696, 51176, 51176, 291, 434, 1237, 412, 341, 264, 23897, 1320, 295, 341, 18161, 2533, 293, 291, 434, 1953, 281, 1803, 51384, 51384, 767, 300, 311, 406, 886, 6179, 1105, 1184, 472, 295, 613, 7914, 307, 516, 281, 312, 257, 707, 857, 544, 51684, 51684], "temperature": 0.0, "avg_logprob": -0.25999639700124927, "compression_ratio": 1.818867924528302, "no_speech_prob": 3.0894473184162052e-06}, {"id": 1105, "seek": 685096, "start": 6850.96, "end": 6856.24, "text": " too complicated um each one of these layers is like three lines of code or something like that", "tokens": [50364, 886, 6179, 1105, 1184, 472, 295, 613, 7914, 307, 411, 1045, 3876, 295, 3089, 420, 746, 411, 300, 50628, 50656, 293, 881, 295, 309, 307, 6457, 15325, 7263, 365, 264, 22556, 11183, 295, 50852, 50852, 264, 15245, 2710, 2144, 23897, 1320, 5911, 309, 311, 1238, 665, 1392, 293, 300, 311, 1203, 741, 51100, 51100, 1415, 281, 2060, 337, 341, 7991, 370, 4696, 291, 1352, 341, 1880, 293, 437, 741, 4501, 466, 309, 51364, 51364, 6095, 307, 300, 309, 2729, 505, 257, 588, 1481, 8811, 295, 7914, 281, 646, 48256, 807, 293, 1105, 741, 519, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.04927674118353396, "compression_ratio": 1.7127272727272727, "no_speech_prob": 1.6796757336123846e-06}, {"id": 1106, "seek": 685096, "start": 6856.8, "end": 6860.72, "text": " and most of it is fairly straightforward potentially with the notable exception of", "tokens": [50364, 886, 6179, 1105, 1184, 472, 295, 613, 7914, 307, 411, 1045, 3876, 295, 3089, 420, 746, 411, 300, 50628, 50656, 293, 881, 295, 309, 307, 6457, 15325, 7263, 365, 264, 22556, 11183, 295, 50852, 50852, 264, 15245, 2710, 2144, 23897, 1320, 5911, 309, 311, 1238, 665, 1392, 293, 300, 311, 1203, 741, 51100, 51100, 1415, 281, 2060, 337, 341, 7991, 370, 4696, 291, 1352, 341, 1880, 293, 437, 741, 4501, 466, 309, 51364, 51364, 6095, 307, 300, 309, 2729, 505, 257, 588, 1481, 8811, 295, 7914, 281, 646, 48256, 807, 293, 1105, 741, 519, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.04927674118353396, "compression_ratio": 1.7127272727272727, "no_speech_prob": 1.6796757336123846e-06}, {"id": 1107, "seek": 685096, "start": 6860.72, "end": 6865.68, "text": " the batch normalization backward pass otherwise it's pretty good okay and that's everything i", "tokens": [50364, 886, 6179, 1105, 1184, 472, 295, 613, 7914, 307, 411, 1045, 3876, 295, 3089, 420, 746, 411, 300, 50628, 50656, 293, 881, 295, 309, 307, 6457, 15325, 7263, 365, 264, 22556, 11183, 295, 50852, 50852, 264, 15245, 2710, 2144, 23897, 1320, 5911, 309, 311, 1238, 665, 1392, 293, 300, 311, 1203, 741, 51100, 51100, 1415, 281, 2060, 337, 341, 7991, 370, 4696, 291, 1352, 341, 1880, 293, 437, 741, 4501, 466, 309, 51364, 51364, 6095, 307, 300, 309, 2729, 505, 257, 588, 1481, 8811, 295, 7914, 281, 646, 48256, 807, 293, 1105, 741, 519, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.04927674118353396, "compression_ratio": 1.7127272727272727, "no_speech_prob": 1.6796757336123846e-06}, {"id": 1108, "seek": 685096, "start": 6865.68, "end": 6870.96, "text": " wanted to cover for this lecture so hopefully you found this interesting and what i liked about it", "tokens": [50364, 886, 6179, 1105, 1184, 472, 295, 613, 7914, 307, 411, 1045, 3876, 295, 3089, 420, 746, 411, 300, 50628, 50656, 293, 881, 295, 309, 307, 6457, 15325, 7263, 365, 264, 22556, 11183, 295, 50852, 50852, 264, 15245, 2710, 2144, 23897, 1320, 5911, 309, 311, 1238, 665, 1392, 293, 300, 311, 1203, 741, 51100, 51100, 1415, 281, 2060, 337, 341, 7991, 370, 4696, 291, 1352, 341, 1880, 293, 437, 741, 4501, 466, 309, 51364, 51364, 6095, 307, 300, 309, 2729, 505, 257, 588, 1481, 8811, 295, 7914, 281, 646, 48256, 807, 293, 1105, 741, 519, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.04927674118353396, "compression_ratio": 1.7127272727272727, "no_speech_prob": 1.6796757336123846e-06}, {"id": 1109, "seek": 685096, "start": 6870.96, "end": 6877.28, "text": " honestly is that it gave us a very nice diversity of layers to back propagate through and um i think", "tokens": [50364, 886, 6179, 1105, 1184, 472, 295, 613, 7914, 307, 411, 1045, 3876, 295, 3089, 420, 746, 411, 300, 50628, 50656, 293, 881, 295, 309, 307, 6457, 15325, 7263, 365, 264, 22556, 11183, 295, 50852, 50852, 264, 15245, 2710, 2144, 23897, 1320, 5911, 309, 311, 1238, 665, 1392, 293, 300, 311, 1203, 741, 51100, 51100, 1415, 281, 2060, 337, 341, 7991, 370, 4696, 291, 1352, 341, 1880, 293, 437, 741, 4501, 466, 309, 51364, 51364, 6095, 307, 300, 309, 2729, 505, 257, 588, 1481, 8811, 295, 7914, 281, 646, 48256, 807, 293, 1105, 741, 519, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.04927674118353396, "compression_ratio": 1.7127272727272727, "no_speech_prob": 1.6796757336123846e-06}, {"id": 1110, "seek": 687728, "start": 6877.28, "end": 6881.44, "text": " it gives a pretty nice and comprehensive sense of how these backward passes are implemented and how", "tokens": [50364, 309, 2709, 257, 1238, 1481, 293, 13914, 2020, 295, 577, 613, 23897, 11335, 366, 12270, 293, 577, 50572, 50572, 436, 589, 293, 291, 1116, 312, 1075, 281, 28446, 552, 1803, 457, 295, 1164, 294, 3124, 291, 1391, 500, 380, 528, 50800, 50800, 281, 293, 291, 528, 281, 764, 264, 25878, 284, 339, 1476, 664, 6206, 457, 4696, 291, 362, 512, 24002, 466, 577, 51020, 51020, 2771, 2448, 3095, 12204, 807, 264, 18161, 2533, 2891, 412, 264, 4470, 293, 577, 436, 3095, 807, 51264, 51264, 439, 264, 9102, 293, 439, 264, 19376, 3542, 293, 498, 291, 7320, 257, 665, 16635, 295, 309, 293, 498, 51540, 51540, 291, 362, 257, 2020, 295, 300, 550, 291, 393, 1207, 1803, 382, 472, 295, 613, 9204, 360, 40371, 322, 264, 1411, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.029822773199815016, "compression_ratio": 1.9364548494983278, "no_speech_prob": 1.221840466314461e-05}, {"id": 1111, "seek": 687728, "start": 6881.44, "end": 6886.0, "text": " they work and you'd be able to derive them yourself but of course in practice you probably don't want", "tokens": [50364, 309, 2709, 257, 1238, 1481, 293, 13914, 2020, 295, 577, 613, 23897, 11335, 366, 12270, 293, 577, 50572, 50572, 436, 589, 293, 291, 1116, 312, 1075, 281, 28446, 552, 1803, 457, 295, 1164, 294, 3124, 291, 1391, 500, 380, 528, 50800, 50800, 281, 293, 291, 528, 281, 764, 264, 25878, 284, 339, 1476, 664, 6206, 457, 4696, 291, 362, 512, 24002, 466, 577, 51020, 51020, 2771, 2448, 3095, 12204, 807, 264, 18161, 2533, 2891, 412, 264, 4470, 293, 577, 436, 3095, 807, 51264, 51264, 439, 264, 9102, 293, 439, 264, 19376, 3542, 293, 498, 291, 7320, 257, 665, 16635, 295, 309, 293, 498, 51540, 51540, 291, 362, 257, 2020, 295, 300, 550, 291, 393, 1207, 1803, 382, 472, 295, 613, 9204, 360, 40371, 322, 264, 1411, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.029822773199815016, "compression_ratio": 1.9364548494983278, "no_speech_prob": 1.221840466314461e-05}, {"id": 1112, "seek": 687728, "start": 6886.0, "end": 6890.4, "text": " to and you want to use the pytorch autograd but hopefully you have some intuition about how", "tokens": [50364, 309, 2709, 257, 1238, 1481, 293, 13914, 2020, 295, 577, 613, 23897, 11335, 366, 12270, 293, 577, 50572, 50572, 436, 589, 293, 291, 1116, 312, 1075, 281, 28446, 552, 1803, 457, 295, 1164, 294, 3124, 291, 1391, 500, 380, 528, 50800, 50800, 281, 293, 291, 528, 281, 764, 264, 25878, 284, 339, 1476, 664, 6206, 457, 4696, 291, 362, 512, 24002, 466, 577, 51020, 51020, 2771, 2448, 3095, 12204, 807, 264, 18161, 2533, 2891, 412, 264, 4470, 293, 577, 436, 3095, 807, 51264, 51264, 439, 264, 9102, 293, 439, 264, 19376, 3542, 293, 498, 291, 7320, 257, 665, 16635, 295, 309, 293, 498, 51540, 51540, 291, 362, 257, 2020, 295, 300, 550, 291, 393, 1207, 1803, 382, 472, 295, 613, 9204, 360, 40371, 322, 264, 1411, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.029822773199815016, "compression_ratio": 1.9364548494983278, "no_speech_prob": 1.221840466314461e-05}, {"id": 1113, "seek": 687728, "start": 6890.4, "end": 6895.28, "text": " gradients flow backwards through the neural net starting at the loss and how they flow through", "tokens": [50364, 309, 2709, 257, 1238, 1481, 293, 13914, 2020, 295, 577, 613, 23897, 11335, 366, 12270, 293, 577, 50572, 50572, 436, 589, 293, 291, 1116, 312, 1075, 281, 28446, 552, 1803, 457, 295, 1164, 294, 3124, 291, 1391, 500, 380, 528, 50800, 50800, 281, 293, 291, 528, 281, 764, 264, 25878, 284, 339, 1476, 664, 6206, 457, 4696, 291, 362, 512, 24002, 466, 577, 51020, 51020, 2771, 2448, 3095, 12204, 807, 264, 18161, 2533, 2891, 412, 264, 4470, 293, 577, 436, 3095, 807, 51264, 51264, 439, 264, 9102, 293, 439, 264, 19376, 3542, 293, 498, 291, 7320, 257, 665, 16635, 295, 309, 293, 498, 51540, 51540, 291, 362, 257, 2020, 295, 300, 550, 291, 393, 1207, 1803, 382, 472, 295, 613, 9204, 360, 40371, 322, 264, 1411, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.029822773199815016, "compression_ratio": 1.9364548494983278, "no_speech_prob": 1.221840466314461e-05}, {"id": 1114, "seek": 687728, "start": 6895.28, "end": 6900.8, "text": " all the variables and all the intermediate results and if you understood a good chunk of it and if", "tokens": [50364, 309, 2709, 257, 1238, 1481, 293, 13914, 2020, 295, 577, 613, 23897, 11335, 366, 12270, 293, 577, 50572, 50572, 436, 589, 293, 291, 1116, 312, 1075, 281, 28446, 552, 1803, 457, 295, 1164, 294, 3124, 291, 1391, 500, 380, 528, 50800, 50800, 281, 293, 291, 528, 281, 764, 264, 25878, 284, 339, 1476, 664, 6206, 457, 4696, 291, 362, 512, 24002, 466, 577, 51020, 51020, 2771, 2448, 3095, 12204, 807, 264, 18161, 2533, 2891, 412, 264, 4470, 293, 577, 436, 3095, 807, 51264, 51264, 439, 264, 9102, 293, 439, 264, 19376, 3542, 293, 498, 291, 7320, 257, 665, 16635, 295, 309, 293, 498, 51540, 51540, 291, 362, 257, 2020, 295, 300, 550, 291, 393, 1207, 1803, 382, 472, 295, 613, 9204, 360, 40371, 322, 264, 1411, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.029822773199815016, "compression_ratio": 1.9364548494983278, "no_speech_prob": 1.221840466314461e-05}, {"id": 1115, "seek": 687728, "start": 6900.8, "end": 6904.8, "text": " you have a sense of that then you can count yourself as one of these buff dojis on the left", "tokens": [50364, 309, 2709, 257, 1238, 1481, 293, 13914, 2020, 295, 577, 613, 23897, 11335, 366, 12270, 293, 577, 50572, 50572, 436, 589, 293, 291, 1116, 312, 1075, 281, 28446, 552, 1803, 457, 295, 1164, 294, 3124, 291, 1391, 500, 380, 528, 50800, 50800, 281, 293, 291, 528, 281, 764, 264, 25878, 284, 339, 1476, 664, 6206, 457, 4696, 291, 362, 512, 24002, 466, 577, 51020, 51020, 2771, 2448, 3095, 12204, 807, 264, 18161, 2533, 2891, 412, 264, 4470, 293, 577, 436, 3095, 807, 51264, 51264, 439, 264, 9102, 293, 439, 264, 19376, 3542, 293, 498, 291, 7320, 257, 665, 16635, 295, 309, 293, 498, 51540, 51540, 291, 362, 257, 2020, 295, 300, 550, 291, 393, 1207, 1803, 382, 472, 295, 613, 9204, 360, 40371, 322, 264, 1411, 51740, 51740], "temperature": 0.0, "avg_logprob": -0.029822773199815016, "compression_ratio": 1.9364548494983278, "no_speech_prob": 1.221840466314461e-05}, {"id": 1116, "seek": 690480, "start": 6904.8, "end": 6910.4800000000005, "text": " instead of the uh dojis on the right here now in the next lecture we're actually going to go to", "tokens": [50364, 2602, 295, 264, 2232, 360, 40371, 322, 264, 558, 510, 586, 294, 264, 958, 7991, 321, 434, 767, 516, 281, 352, 281, 50648, 50648, 18680, 1753, 18161, 36170, 287, 82, 67, 2592, 293, 439, 264, 661, 21669, 295, 367, 3695, 293, 321, 434, 516, 281, 722, 281, 50940, 50940, 3997, 2505, 264, 9482, 293, 722, 281, 4584, 1101, 3565, 22119, 82, 293, 370, 741, 478, 534, 1237, 51196, 51196, 2128, 281, 300, 293, 741, 603, 536, 291, 550, 51740], "temperature": 0.0, "avg_logprob": -0.1457415333500615, "compression_ratio": 1.7074468085106382, "no_speech_prob": 1.0944240784738213e-05}, {"id": 1117, "seek": 690480, "start": 6910.4800000000005, "end": 6916.320000000001, "text": " recurrent neural nets lsdms and all the other variants of rns and we're going to start to", "tokens": [50364, 2602, 295, 264, 2232, 360, 40371, 322, 264, 558, 510, 586, 294, 264, 958, 7991, 321, 434, 767, 516, 281, 352, 281, 50648, 50648, 18680, 1753, 18161, 36170, 287, 82, 67, 2592, 293, 439, 264, 661, 21669, 295, 367, 3695, 293, 321, 434, 516, 281, 722, 281, 50940, 50940, 3997, 2505, 264, 9482, 293, 722, 281, 4584, 1101, 3565, 22119, 82, 293, 370, 741, 478, 534, 1237, 51196, 51196, 2128, 281, 300, 293, 741, 603, 536, 291, 550, 51740], "temperature": 0.0, "avg_logprob": -0.1457415333500615, "compression_ratio": 1.7074468085106382, "no_speech_prob": 1.0944240784738213e-05}, {"id": 1118, "seek": 690480, "start": 6916.320000000001, "end": 6921.4400000000005, "text": " complexify the architecture and start to achieve better log likelihoods and so i'm really looking", "tokens": [50364, 2602, 295, 264, 2232, 360, 40371, 322, 264, 558, 510, 586, 294, 264, 958, 7991, 321, 434, 767, 516, 281, 352, 281, 50648, 50648, 18680, 1753, 18161, 36170, 287, 82, 67, 2592, 293, 439, 264, 661, 21669, 295, 367, 3695, 293, 321, 434, 516, 281, 722, 281, 50940, 50940, 3997, 2505, 264, 9482, 293, 722, 281, 4584, 1101, 3565, 22119, 82, 293, 370, 741, 478, 534, 1237, 51196, 51196, 2128, 281, 300, 293, 741, 603, 536, 291, 550, 51740], "temperature": 0.0, "avg_logprob": -0.1457415333500615, "compression_ratio": 1.7074468085106382, "no_speech_prob": 1.0944240784738213e-05}, {"id": 1119, "seek": 692144, "start": 6921.44, "end": 6936.0, "text": " forward to that and i'll see you then", "tokens": [50364, 2128, 281, 300, 293, 741, 603, 536, 291, 550, 51092], "temperature": 0.0, "avg_logprob": -0.3129656712214152, "compression_ratio": 0.8409090909090909, "no_speech_prob": 2.3845806936151348e-05}], "language": "en", "video_id": "q8SA3rM6ckI", "entity": "Andrew Kaparthy"}}