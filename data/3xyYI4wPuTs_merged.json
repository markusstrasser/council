{"video_id": "3xyYI4wPuTs", "title": "1.20 Machine Learning Overview | Running gradient descent  --[Machine Learning | Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 349, "views": 337, "publish_date": "11/04/2022", "timestamp": 1661040000, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " Let's see what happens when you run gradient descent for linear regression. Let's go see the algorithm in action. Here's a plot of the model and data on the upper left, and a contour plot of the cost function on the upper right. And at the bottom is the surface plot of the same cost function. Often W and B will both be initialized to zero, but for this demonstration, let's initialize W to be equal to negative 0.1 and B to be 900. So this corresponds to f of x equals negative 0.1 x plus 900. Now if we take one step using gradient descent, we end up going from this point of the cost function out here to this point just down and to the right. And notice that the straight line fit has also changed a bit. Let's take another step. The cost function has now moved to this third point, and again the function f of x has also changed a bit. As you take more of these steps, the cost is decreasing at each update, so the parameters W and B are following this trajectory. And if you look on the left, you get this corresponding straight line fit that, you know, fits the data better and better until we've reached the global minimum. The global minimum corresponds to this straight line fit, which is a relatively good fit to the data. I mean, isn't that cool? And so that's gradient descent, and we're going to use this to fit a model to the housing data. And you can now use this f of x model to predict the price of your client's house or anyone else's house. For instance, if your friend's house size is 1250 square feet, you can now read off the value and predict that maybe they could get, I don't know, $250,000 for the house. To be more precise, this gradient descent process is called batch gradient descent. The term batch gradient descent refers to the fact that on every step of gradient descent, we're looking at all of the training examples instead of just a subset of the training data. So in computing gradient descent, when computing derivatives, we're computing the sum from i equals one to m, and batch gradient descent is looking at the entire batch of training examples at each update. I know that batch gradient descent may not be the most intuitive name, but this is what people in the machine learning community call it. If you've heard of the newsletter, The Batch, that's published by deeplearning.ai, the newsletter, The Batch, was also named for this concept in machine learning. And then it turns out that there are other versions of gradient descent that do not look at the entire training set, but instead looks at smaller subsets of the training data at each update step. But we'll use batch gradient descent for linear regression. So that's it for linear regression. Congratulations on getting through your first machine learning model. I hope you go and celebrate or, I don't know, maybe take a nap in your hammock. In the optional lab that follows this video, you see a review of the gradient descent algorithm as well as how to implement it in code. You also see a plot that shows how the cost decreases as you continue training more iterations. And you also see a contour plot, seeing how the cost gets closer to the global minimum as gradient descent finds better and better values for the parameters w and b. So remember that to do the optional lab, you just need to read and run this code. You won't need to write any code yourself. And I hope you take a few moments to do that and also become familiar with the gradient descent code because this will help you to implement this and similar algorithms in the future yourself. Thanks for sticking with me through the end of this last video for the first week and congratulations for making it all the way here. You're on your way to becoming a machine learning person. In addition to the optional labs, if you haven't done so yet, I hope you also check out the practice quizzes, which are a nice way that you can double check your own understanding of the concepts. It's also totally fine if you don't get them all right the first time. And you can also take the quizzes multiple times until you get the score that you want. You now know how to implement linear regression with one variable. And that brings us to the close of this week. Next week, we'll learn to make linear regression much more powerful. Instead of one feature like size of a house, you learn how to get it to work with lots of features. You also learn how to get it to fit nonlinear curves. These improvements will make the algorithm much more useful and valuable. Lastly, we'll also go over some practical tips that will really help for getting linear regression to work on practical applications. I'm really happy to have you here with me in this class and I look forward to seeing you next week.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.4, "text": " Let's see what happens when you run gradient descent for linear regression.", "tokens": [50364, 961, 311, 536, 437, 2314, 562, 291, 1190, 16235, 23475, 337, 8213, 24590, 13, 50684, 50684, 961, 311, 352, 536, 264, 9284, 294, 3069, 13, 50808, 50808, 1692, 311, 257, 7542, 295, 264, 2316, 293, 1412, 322, 264, 6597, 1411, 11, 293, 257, 21234, 7542, 295, 264, 2063, 51142, 51142, 2445, 322, 264, 6597, 558, 13, 51280, 51280, 400, 412, 264, 2767, 307, 264, 3753, 7542, 295, 264, 912, 2063, 2445, 13, 51546, 51546, 20043, 343, 293, 363, 486, 1293, 312, 5883, 1602, 281, 4018, 11, 457, 337, 341, 16520, 11, 718, 311, 5883, 1125, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.16780539233275135, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.020321574062108994}, {"id": 1, "seek": 0, "start": 6.4, "end": 8.88, "text": " Let's go see the algorithm in action.", "tokens": [50364, 961, 311, 536, 437, 2314, 562, 291, 1190, 16235, 23475, 337, 8213, 24590, 13, 50684, 50684, 961, 311, 352, 536, 264, 9284, 294, 3069, 13, 50808, 50808, 1692, 311, 257, 7542, 295, 264, 2316, 293, 1412, 322, 264, 6597, 1411, 11, 293, 257, 21234, 7542, 295, 264, 2063, 51142, 51142, 2445, 322, 264, 6597, 558, 13, 51280, 51280, 400, 412, 264, 2767, 307, 264, 3753, 7542, 295, 264, 912, 2063, 2445, 13, 51546, 51546, 20043, 343, 293, 363, 486, 1293, 312, 5883, 1602, 281, 4018, 11, 457, 337, 341, 16520, 11, 718, 311, 5883, 1125, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.16780539233275135, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.020321574062108994}, {"id": 2, "seek": 0, "start": 8.88, "end": 15.56, "text": " Here's a plot of the model and data on the upper left, and a contour plot of the cost", "tokens": [50364, 961, 311, 536, 437, 2314, 562, 291, 1190, 16235, 23475, 337, 8213, 24590, 13, 50684, 50684, 961, 311, 352, 536, 264, 9284, 294, 3069, 13, 50808, 50808, 1692, 311, 257, 7542, 295, 264, 2316, 293, 1412, 322, 264, 6597, 1411, 11, 293, 257, 21234, 7542, 295, 264, 2063, 51142, 51142, 2445, 322, 264, 6597, 558, 13, 51280, 51280, 400, 412, 264, 2767, 307, 264, 3753, 7542, 295, 264, 912, 2063, 2445, 13, 51546, 51546, 20043, 343, 293, 363, 486, 1293, 312, 5883, 1602, 281, 4018, 11, 457, 337, 341, 16520, 11, 718, 311, 5883, 1125, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.16780539233275135, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.020321574062108994}, {"id": 3, "seek": 0, "start": 15.56, "end": 18.32, "text": " function on the upper right.", "tokens": [50364, 961, 311, 536, 437, 2314, 562, 291, 1190, 16235, 23475, 337, 8213, 24590, 13, 50684, 50684, 961, 311, 352, 536, 264, 9284, 294, 3069, 13, 50808, 50808, 1692, 311, 257, 7542, 295, 264, 2316, 293, 1412, 322, 264, 6597, 1411, 11, 293, 257, 21234, 7542, 295, 264, 2063, 51142, 51142, 2445, 322, 264, 6597, 558, 13, 51280, 51280, 400, 412, 264, 2767, 307, 264, 3753, 7542, 295, 264, 912, 2063, 2445, 13, 51546, 51546, 20043, 343, 293, 363, 486, 1293, 312, 5883, 1602, 281, 4018, 11, 457, 337, 341, 16520, 11, 718, 311, 5883, 1125, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.16780539233275135, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.020321574062108994}, {"id": 4, "seek": 0, "start": 18.32, "end": 23.64, "text": " And at the bottom is the surface plot of the same cost function.", "tokens": [50364, 961, 311, 536, 437, 2314, 562, 291, 1190, 16235, 23475, 337, 8213, 24590, 13, 50684, 50684, 961, 311, 352, 536, 264, 9284, 294, 3069, 13, 50808, 50808, 1692, 311, 257, 7542, 295, 264, 2316, 293, 1412, 322, 264, 6597, 1411, 11, 293, 257, 21234, 7542, 295, 264, 2063, 51142, 51142, 2445, 322, 264, 6597, 558, 13, 51280, 51280, 400, 412, 264, 2767, 307, 264, 3753, 7542, 295, 264, 912, 2063, 2445, 13, 51546, 51546, 20043, 343, 293, 363, 486, 1293, 312, 5883, 1602, 281, 4018, 11, 457, 337, 341, 16520, 11, 718, 311, 5883, 1125, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.16780539233275135, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.020321574062108994}, {"id": 5, "seek": 0, "start": 23.64, "end": 29.64, "text": " Often W and B will both be initialized to zero, but for this demonstration, let's initialize", "tokens": [50364, 961, 311, 536, 437, 2314, 562, 291, 1190, 16235, 23475, 337, 8213, 24590, 13, 50684, 50684, 961, 311, 352, 536, 264, 9284, 294, 3069, 13, 50808, 50808, 1692, 311, 257, 7542, 295, 264, 2316, 293, 1412, 322, 264, 6597, 1411, 11, 293, 257, 21234, 7542, 295, 264, 2063, 51142, 51142, 2445, 322, 264, 6597, 558, 13, 51280, 51280, 400, 412, 264, 2767, 307, 264, 3753, 7542, 295, 264, 912, 2063, 2445, 13, 51546, 51546, 20043, 343, 293, 363, 486, 1293, 312, 5883, 1602, 281, 4018, 11, 457, 337, 341, 16520, 11, 718, 311, 5883, 1125, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.16780539233275135, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.020321574062108994}, {"id": 6, "seek": 2964, "start": 29.64, "end": 36.08, "text": " W to be equal to negative 0.1 and B to be 900.", "tokens": [50364, 343, 281, 312, 2681, 281, 3671, 1958, 13, 16, 293, 363, 281, 312, 22016, 13, 50686, 50686, 407, 341, 23249, 281, 283, 295, 2031, 6915, 3671, 1958, 13, 16, 2031, 1804, 22016, 13, 51120, 51120, 823, 498, 321, 747, 472, 1823, 1228, 16235, 23475, 11, 321, 917, 493, 516, 490, 341, 935, 295, 264, 2063, 51410, 51410, 2445, 484, 510, 281, 341, 935, 445, 760, 293, 281, 264, 558, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.13196670686876452, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.8342363546253182e-05}, {"id": 7, "seek": 2964, "start": 36.08, "end": 44.760000000000005, "text": " So this corresponds to f of x equals negative 0.1 x plus 900.", "tokens": [50364, 343, 281, 312, 2681, 281, 3671, 1958, 13, 16, 293, 363, 281, 312, 22016, 13, 50686, 50686, 407, 341, 23249, 281, 283, 295, 2031, 6915, 3671, 1958, 13, 16, 2031, 1804, 22016, 13, 51120, 51120, 823, 498, 321, 747, 472, 1823, 1228, 16235, 23475, 11, 321, 917, 493, 516, 490, 341, 935, 295, 264, 2063, 51410, 51410, 2445, 484, 510, 281, 341, 935, 445, 760, 293, 281, 264, 558, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.13196670686876452, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.8342363546253182e-05}, {"id": 8, "seek": 2964, "start": 44.760000000000005, "end": 50.56, "text": " Now if we take one step using gradient descent, we end up going from this point of the cost", "tokens": [50364, 343, 281, 312, 2681, 281, 3671, 1958, 13, 16, 293, 363, 281, 312, 22016, 13, 50686, 50686, 407, 341, 23249, 281, 283, 295, 2031, 6915, 3671, 1958, 13, 16, 2031, 1804, 22016, 13, 51120, 51120, 823, 498, 321, 747, 472, 1823, 1228, 16235, 23475, 11, 321, 917, 493, 516, 490, 341, 935, 295, 264, 2063, 51410, 51410, 2445, 484, 510, 281, 341, 935, 445, 760, 293, 281, 264, 558, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.13196670686876452, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.8342363546253182e-05}, {"id": 9, "seek": 2964, "start": 50.56, "end": 57.8, "text": " function out here to this point just down and to the right.", "tokens": [50364, 343, 281, 312, 2681, 281, 3671, 1958, 13, 16, 293, 363, 281, 312, 22016, 13, 50686, 50686, 407, 341, 23249, 281, 283, 295, 2031, 6915, 3671, 1958, 13, 16, 2031, 1804, 22016, 13, 51120, 51120, 823, 498, 321, 747, 472, 1823, 1228, 16235, 23475, 11, 321, 917, 493, 516, 490, 341, 935, 295, 264, 2063, 51410, 51410, 2445, 484, 510, 281, 341, 935, 445, 760, 293, 281, 264, 558, 13, 51772, 51772], "temperature": 0.0, "avg_logprob": -0.13196670686876452, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.8342363546253182e-05}, {"id": 10, "seek": 5780, "start": 57.8, "end": 64.39999999999999, "text": " And notice that the straight line fit has also changed a bit.", "tokens": [50364, 400, 3449, 300, 264, 2997, 1622, 3318, 575, 611, 3105, 257, 857, 13, 50694, 50694, 961, 311, 747, 1071, 1823, 13, 50826, 50826, 440, 2063, 2445, 575, 586, 4259, 281, 341, 2636, 935, 11, 293, 797, 264, 2445, 283, 295, 2031, 575, 611, 51154, 51154, 3105, 257, 857, 13, 51278, 51278, 1018, 291, 747, 544, 295, 613, 4439, 11, 264, 2063, 307, 23223, 412, 1184, 5623, 11, 370, 264, 9834, 51607, 51607], "temperature": 0.0, "avg_logprob": -0.13282167589342272, "compression_ratio": 1.5988700564971752, "no_speech_prob": 2.2602737317356514e-06}, {"id": 11, "seek": 5780, "start": 64.39999999999999, "end": 67.03999999999999, "text": " Let's take another step.", "tokens": [50364, 400, 3449, 300, 264, 2997, 1622, 3318, 575, 611, 3105, 257, 857, 13, 50694, 50694, 961, 311, 747, 1071, 1823, 13, 50826, 50826, 440, 2063, 2445, 575, 586, 4259, 281, 341, 2636, 935, 11, 293, 797, 264, 2445, 283, 295, 2031, 575, 611, 51154, 51154, 3105, 257, 857, 13, 51278, 51278, 1018, 291, 747, 544, 295, 613, 4439, 11, 264, 2063, 307, 23223, 412, 1184, 5623, 11, 370, 264, 9834, 51607, 51607], "temperature": 0.0, "avg_logprob": -0.13282167589342272, "compression_ratio": 1.5988700564971752, "no_speech_prob": 2.2602737317356514e-06}, {"id": 12, "seek": 5780, "start": 67.03999999999999, "end": 73.6, "text": " The cost function has now moved to this third point, and again the function f of x has also", "tokens": [50364, 400, 3449, 300, 264, 2997, 1622, 3318, 575, 611, 3105, 257, 857, 13, 50694, 50694, 961, 311, 747, 1071, 1823, 13, 50826, 50826, 440, 2063, 2445, 575, 586, 4259, 281, 341, 2636, 935, 11, 293, 797, 264, 2445, 283, 295, 2031, 575, 611, 51154, 51154, 3105, 257, 857, 13, 51278, 51278, 1018, 291, 747, 544, 295, 613, 4439, 11, 264, 2063, 307, 23223, 412, 1184, 5623, 11, 370, 264, 9834, 51607, 51607], "temperature": 0.0, "avg_logprob": -0.13282167589342272, "compression_ratio": 1.5988700564971752, "no_speech_prob": 2.2602737317356514e-06}, {"id": 13, "seek": 5780, "start": 73.6, "end": 76.08, "text": " changed a bit.", "tokens": [50364, 400, 3449, 300, 264, 2997, 1622, 3318, 575, 611, 3105, 257, 857, 13, 50694, 50694, 961, 311, 747, 1071, 1823, 13, 50826, 50826, 440, 2063, 2445, 575, 586, 4259, 281, 341, 2636, 935, 11, 293, 797, 264, 2445, 283, 295, 2031, 575, 611, 51154, 51154, 3105, 257, 857, 13, 51278, 51278, 1018, 291, 747, 544, 295, 613, 4439, 11, 264, 2063, 307, 23223, 412, 1184, 5623, 11, 370, 264, 9834, 51607, 51607], "temperature": 0.0, "avg_logprob": -0.13282167589342272, "compression_ratio": 1.5988700564971752, "no_speech_prob": 2.2602737317356514e-06}, {"id": 14, "seek": 5780, "start": 76.08, "end": 82.66, "text": " As you take more of these steps, the cost is decreasing at each update, so the parameters", "tokens": [50364, 400, 3449, 300, 264, 2997, 1622, 3318, 575, 611, 3105, 257, 857, 13, 50694, 50694, 961, 311, 747, 1071, 1823, 13, 50826, 50826, 440, 2063, 2445, 575, 586, 4259, 281, 341, 2636, 935, 11, 293, 797, 264, 2445, 283, 295, 2031, 575, 611, 51154, 51154, 3105, 257, 857, 13, 51278, 51278, 1018, 291, 747, 544, 295, 613, 4439, 11, 264, 2063, 307, 23223, 412, 1184, 5623, 11, 370, 264, 9834, 51607, 51607], "temperature": 0.0, "avg_logprob": -0.13282167589342272, "compression_ratio": 1.5988700564971752, "no_speech_prob": 2.2602737317356514e-06}, {"id": 15, "seek": 8266, "start": 82.66, "end": 88.52, "text": " W and B are following this trajectory.", "tokens": [50364, 343, 293, 363, 366, 3480, 341, 21512, 13, 50657, 50657, 400, 498, 291, 574, 322, 264, 1411, 11, 291, 483, 341, 11760, 2997, 1622, 3318, 300, 11, 291, 50941, 50941, 458, 11, 9001, 264, 1412, 1101, 293, 1101, 1826, 321, 600, 6488, 264, 4338, 7285, 13, 51252, 51252, 440, 4338, 7285, 23249, 281, 341, 2997, 1622, 3318, 11, 597, 307, 257, 7226, 665, 3318, 281, 51561, 51561, 264, 1412, 13, 51611, 51611, 286, 914, 11, 1943, 380, 300, 1627, 30, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.10184336843944732, "compression_ratio": 1.6683673469387754, "no_speech_prob": 1.6441752450191416e-05}, {"id": 16, "seek": 8266, "start": 88.52, "end": 94.2, "text": " And if you look on the left, you get this corresponding straight line fit that, you", "tokens": [50364, 343, 293, 363, 366, 3480, 341, 21512, 13, 50657, 50657, 400, 498, 291, 574, 322, 264, 1411, 11, 291, 483, 341, 11760, 2997, 1622, 3318, 300, 11, 291, 50941, 50941, 458, 11, 9001, 264, 1412, 1101, 293, 1101, 1826, 321, 600, 6488, 264, 4338, 7285, 13, 51252, 51252, 440, 4338, 7285, 23249, 281, 341, 2997, 1622, 3318, 11, 597, 307, 257, 7226, 665, 3318, 281, 51561, 51561, 264, 1412, 13, 51611, 51611, 286, 914, 11, 1943, 380, 300, 1627, 30, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.10184336843944732, "compression_ratio": 1.6683673469387754, "no_speech_prob": 1.6441752450191416e-05}, {"id": 17, "seek": 8266, "start": 94.2, "end": 100.42, "text": " know, fits the data better and better until we've reached the global minimum.", "tokens": [50364, 343, 293, 363, 366, 3480, 341, 21512, 13, 50657, 50657, 400, 498, 291, 574, 322, 264, 1411, 11, 291, 483, 341, 11760, 2997, 1622, 3318, 300, 11, 291, 50941, 50941, 458, 11, 9001, 264, 1412, 1101, 293, 1101, 1826, 321, 600, 6488, 264, 4338, 7285, 13, 51252, 51252, 440, 4338, 7285, 23249, 281, 341, 2997, 1622, 3318, 11, 597, 307, 257, 7226, 665, 3318, 281, 51561, 51561, 264, 1412, 13, 51611, 51611, 286, 914, 11, 1943, 380, 300, 1627, 30, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.10184336843944732, "compression_ratio": 1.6683673469387754, "no_speech_prob": 1.6441752450191416e-05}, {"id": 18, "seek": 8266, "start": 100.42, "end": 106.6, "text": " The global minimum corresponds to this straight line fit, which is a relatively good fit to", "tokens": [50364, 343, 293, 363, 366, 3480, 341, 21512, 13, 50657, 50657, 400, 498, 291, 574, 322, 264, 1411, 11, 291, 483, 341, 11760, 2997, 1622, 3318, 300, 11, 291, 50941, 50941, 458, 11, 9001, 264, 1412, 1101, 293, 1101, 1826, 321, 600, 6488, 264, 4338, 7285, 13, 51252, 51252, 440, 4338, 7285, 23249, 281, 341, 2997, 1622, 3318, 11, 597, 307, 257, 7226, 665, 3318, 281, 51561, 51561, 264, 1412, 13, 51611, 51611, 286, 914, 11, 1943, 380, 300, 1627, 30, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.10184336843944732, "compression_ratio": 1.6683673469387754, "no_speech_prob": 1.6441752450191416e-05}, {"id": 19, "seek": 8266, "start": 106.6, "end": 107.6, "text": " the data.", "tokens": [50364, 343, 293, 363, 366, 3480, 341, 21512, 13, 50657, 50657, 400, 498, 291, 574, 322, 264, 1411, 11, 291, 483, 341, 11760, 2997, 1622, 3318, 300, 11, 291, 50941, 50941, 458, 11, 9001, 264, 1412, 1101, 293, 1101, 1826, 321, 600, 6488, 264, 4338, 7285, 13, 51252, 51252, 440, 4338, 7285, 23249, 281, 341, 2997, 1622, 3318, 11, 597, 307, 257, 7226, 665, 3318, 281, 51561, 51561, 264, 1412, 13, 51611, 51611, 286, 914, 11, 1943, 380, 300, 1627, 30, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.10184336843944732, "compression_ratio": 1.6683673469387754, "no_speech_prob": 1.6441752450191416e-05}, {"id": 20, "seek": 8266, "start": 107.6, "end": 110.44, "text": " I mean, isn't that cool?", "tokens": [50364, 343, 293, 363, 366, 3480, 341, 21512, 13, 50657, 50657, 400, 498, 291, 574, 322, 264, 1411, 11, 291, 483, 341, 11760, 2997, 1622, 3318, 300, 11, 291, 50941, 50941, 458, 11, 9001, 264, 1412, 1101, 293, 1101, 1826, 321, 600, 6488, 264, 4338, 7285, 13, 51252, 51252, 440, 4338, 7285, 23249, 281, 341, 2997, 1622, 3318, 11, 597, 307, 257, 7226, 665, 3318, 281, 51561, 51561, 264, 1412, 13, 51611, 51611, 286, 914, 11, 1943, 380, 300, 1627, 30, 51753, 51753], "temperature": 0.0, "avg_logprob": -0.10184336843944732, "compression_ratio": 1.6683673469387754, "no_speech_prob": 1.6441752450191416e-05}, {"id": 21, "seek": 11044, "start": 110.44, "end": 117.0, "text": " And so that's gradient descent, and we're going to use this to fit a model to the housing", "tokens": [50364, 400, 370, 300, 311, 16235, 23475, 11, 293, 321, 434, 516, 281, 764, 341, 281, 3318, 257, 2316, 281, 264, 6849, 50692, 50692, 1412, 13, 50776, 50776, 400, 291, 393, 586, 764, 341, 283, 295, 2031, 2316, 281, 6069, 264, 3218, 295, 428, 6423, 311, 1782, 420, 2878, 51113, 51113, 1646, 311, 1782, 13, 51186, 51186, 1171, 5197, 11, 498, 428, 1277, 311, 1782, 2744, 307, 2272, 2803, 3732, 3521, 11, 291, 393, 586, 1401, 766, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.1201373815536499, "compression_ratio": 1.574585635359116, "no_speech_prob": 4.2891492739727255e-06}, {"id": 22, "seek": 11044, "start": 117.0, "end": 118.67999999999999, "text": " data.", "tokens": [50364, 400, 370, 300, 311, 16235, 23475, 11, 293, 321, 434, 516, 281, 764, 341, 281, 3318, 257, 2316, 281, 264, 6849, 50692, 50692, 1412, 13, 50776, 50776, 400, 291, 393, 586, 764, 341, 283, 295, 2031, 2316, 281, 6069, 264, 3218, 295, 428, 6423, 311, 1782, 420, 2878, 51113, 51113, 1646, 311, 1782, 13, 51186, 51186, 1171, 5197, 11, 498, 428, 1277, 311, 1782, 2744, 307, 2272, 2803, 3732, 3521, 11, 291, 393, 586, 1401, 766, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.1201373815536499, "compression_ratio": 1.574585635359116, "no_speech_prob": 4.2891492739727255e-06}, {"id": 23, "seek": 11044, "start": 118.67999999999999, "end": 125.42, "text": " And you can now use this f of x model to predict the price of your client's house or anyone", "tokens": [50364, 400, 370, 300, 311, 16235, 23475, 11, 293, 321, 434, 516, 281, 764, 341, 281, 3318, 257, 2316, 281, 264, 6849, 50692, 50692, 1412, 13, 50776, 50776, 400, 291, 393, 586, 764, 341, 283, 295, 2031, 2316, 281, 6069, 264, 3218, 295, 428, 6423, 311, 1782, 420, 2878, 51113, 51113, 1646, 311, 1782, 13, 51186, 51186, 1171, 5197, 11, 498, 428, 1277, 311, 1782, 2744, 307, 2272, 2803, 3732, 3521, 11, 291, 393, 586, 1401, 766, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.1201373815536499, "compression_ratio": 1.574585635359116, "no_speech_prob": 4.2891492739727255e-06}, {"id": 24, "seek": 11044, "start": 125.42, "end": 126.88, "text": " else's house.", "tokens": [50364, 400, 370, 300, 311, 16235, 23475, 11, 293, 321, 434, 516, 281, 764, 341, 281, 3318, 257, 2316, 281, 264, 6849, 50692, 50692, 1412, 13, 50776, 50776, 400, 291, 393, 586, 764, 341, 283, 295, 2031, 2316, 281, 6069, 264, 3218, 295, 428, 6423, 311, 1782, 420, 2878, 51113, 51113, 1646, 311, 1782, 13, 51186, 51186, 1171, 5197, 11, 498, 428, 1277, 311, 1782, 2744, 307, 2272, 2803, 3732, 3521, 11, 291, 393, 586, 1401, 766, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.1201373815536499, "compression_ratio": 1.574585635359116, "no_speech_prob": 4.2891492739727255e-06}, {"id": 25, "seek": 11044, "start": 126.88, "end": 133.96, "text": " For instance, if your friend's house size is 1250 square feet, you can now read off", "tokens": [50364, 400, 370, 300, 311, 16235, 23475, 11, 293, 321, 434, 516, 281, 764, 341, 281, 3318, 257, 2316, 281, 264, 6849, 50692, 50692, 1412, 13, 50776, 50776, 400, 291, 393, 586, 764, 341, 283, 295, 2031, 2316, 281, 6069, 264, 3218, 295, 428, 6423, 311, 1782, 420, 2878, 51113, 51113, 1646, 311, 1782, 13, 51186, 51186, 1171, 5197, 11, 498, 428, 1277, 311, 1782, 2744, 307, 2272, 2803, 3732, 3521, 11, 291, 393, 586, 1401, 766, 51540, 51540], "temperature": 0.0, "avg_logprob": -0.1201373815536499, "compression_ratio": 1.574585635359116, "no_speech_prob": 4.2891492739727255e-06}, {"id": 26, "seek": 13396, "start": 133.96, "end": 141.60000000000002, "text": " the value and predict that maybe they could get, I don't know, $250,000 for the house.", "tokens": [50364, 264, 2158, 293, 6069, 300, 1310, 436, 727, 483, 11, 286, 500, 380, 458, 11, 1848, 23538, 11, 1360, 337, 264, 1782, 13, 50746, 50746, 1407, 312, 544, 13600, 11, 341, 16235, 23475, 1399, 307, 1219, 15245, 16235, 23475, 13, 51054, 51054, 440, 1433, 15245, 16235, 23475, 14942, 281, 264, 1186, 300, 322, 633, 1823, 295, 16235, 23475, 11, 51366, 51366, 321, 434, 1237, 412, 439, 295, 264, 3097, 5110, 2602, 295, 445, 257, 25993, 295, 264, 3097, 1412, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.06954420748211089, "compression_ratio": 1.7019230769230769, "no_speech_prob": 4.565674771583872e-06}, {"id": 27, "seek": 13396, "start": 141.60000000000002, "end": 147.76000000000002, "text": " To be more precise, this gradient descent process is called batch gradient descent.", "tokens": [50364, 264, 2158, 293, 6069, 300, 1310, 436, 727, 483, 11, 286, 500, 380, 458, 11, 1848, 23538, 11, 1360, 337, 264, 1782, 13, 50746, 50746, 1407, 312, 544, 13600, 11, 341, 16235, 23475, 1399, 307, 1219, 15245, 16235, 23475, 13, 51054, 51054, 440, 1433, 15245, 16235, 23475, 14942, 281, 264, 1186, 300, 322, 633, 1823, 295, 16235, 23475, 11, 51366, 51366, 321, 434, 1237, 412, 439, 295, 264, 3097, 5110, 2602, 295, 445, 257, 25993, 295, 264, 3097, 1412, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.06954420748211089, "compression_ratio": 1.7019230769230769, "no_speech_prob": 4.565674771583872e-06}, {"id": 28, "seek": 13396, "start": 147.76000000000002, "end": 154.0, "text": " The term batch gradient descent refers to the fact that on every step of gradient descent,", "tokens": [50364, 264, 2158, 293, 6069, 300, 1310, 436, 727, 483, 11, 286, 500, 380, 458, 11, 1848, 23538, 11, 1360, 337, 264, 1782, 13, 50746, 50746, 1407, 312, 544, 13600, 11, 341, 16235, 23475, 1399, 307, 1219, 15245, 16235, 23475, 13, 51054, 51054, 440, 1433, 15245, 16235, 23475, 14942, 281, 264, 1186, 300, 322, 633, 1823, 295, 16235, 23475, 11, 51366, 51366, 321, 434, 1237, 412, 439, 295, 264, 3097, 5110, 2602, 295, 445, 257, 25993, 295, 264, 3097, 1412, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.06954420748211089, "compression_ratio": 1.7019230769230769, "no_speech_prob": 4.565674771583872e-06}, {"id": 29, "seek": 13396, "start": 154.0, "end": 161.4, "text": " we're looking at all of the training examples instead of just a subset of the training data.", "tokens": [50364, 264, 2158, 293, 6069, 300, 1310, 436, 727, 483, 11, 286, 500, 380, 458, 11, 1848, 23538, 11, 1360, 337, 264, 1782, 13, 50746, 50746, 1407, 312, 544, 13600, 11, 341, 16235, 23475, 1399, 307, 1219, 15245, 16235, 23475, 13, 51054, 51054, 440, 1433, 15245, 16235, 23475, 14942, 281, 264, 1186, 300, 322, 633, 1823, 295, 16235, 23475, 11, 51366, 51366, 321, 434, 1237, 412, 439, 295, 264, 3097, 5110, 2602, 295, 445, 257, 25993, 295, 264, 3097, 1412, 13, 51736, 51736], "temperature": 0.0, "avg_logprob": -0.06954420748211089, "compression_ratio": 1.7019230769230769, "no_speech_prob": 4.565674771583872e-06}, {"id": 30, "seek": 16140, "start": 161.4, "end": 167.72, "text": " So in computing gradient descent, when computing derivatives, we're computing the sum from", "tokens": [50364, 407, 294, 15866, 16235, 23475, 11, 562, 15866, 33733, 11, 321, 434, 15866, 264, 2408, 490, 50680, 50680, 741, 6915, 472, 281, 275, 11, 293, 15245, 16235, 23475, 307, 1237, 412, 264, 2302, 15245, 295, 3097, 51090, 51090, 5110, 412, 1184, 5623, 13, 51246, 51246, 286, 458, 300, 15245, 16235, 23475, 815, 406, 312, 264, 881, 21769, 1315, 11, 457, 341, 307, 437, 51482, 51482, 561, 294, 264, 3479, 2539, 1768, 818, 309, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.11972622993664864, "compression_ratio": 1.71, "no_speech_prob": 1.260608587472234e-05}, {"id": 31, "seek": 16140, "start": 167.72, "end": 175.92000000000002, "text": " i equals one to m, and batch gradient descent is looking at the entire batch of training", "tokens": [50364, 407, 294, 15866, 16235, 23475, 11, 562, 15866, 33733, 11, 321, 434, 15866, 264, 2408, 490, 50680, 50680, 741, 6915, 472, 281, 275, 11, 293, 15245, 16235, 23475, 307, 1237, 412, 264, 2302, 15245, 295, 3097, 51090, 51090, 5110, 412, 1184, 5623, 13, 51246, 51246, 286, 458, 300, 15245, 16235, 23475, 815, 406, 312, 264, 881, 21769, 1315, 11, 457, 341, 307, 437, 51482, 51482, 561, 294, 264, 3479, 2539, 1768, 818, 309, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.11972622993664864, "compression_ratio": 1.71, "no_speech_prob": 1.260608587472234e-05}, {"id": 32, "seek": 16140, "start": 175.92000000000002, "end": 179.04000000000002, "text": " examples at each update.", "tokens": [50364, 407, 294, 15866, 16235, 23475, 11, 562, 15866, 33733, 11, 321, 434, 15866, 264, 2408, 490, 50680, 50680, 741, 6915, 472, 281, 275, 11, 293, 15245, 16235, 23475, 307, 1237, 412, 264, 2302, 15245, 295, 3097, 51090, 51090, 5110, 412, 1184, 5623, 13, 51246, 51246, 286, 458, 300, 15245, 16235, 23475, 815, 406, 312, 264, 881, 21769, 1315, 11, 457, 341, 307, 437, 51482, 51482, 561, 294, 264, 3479, 2539, 1768, 818, 309, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.11972622993664864, "compression_ratio": 1.71, "no_speech_prob": 1.260608587472234e-05}, {"id": 33, "seek": 16140, "start": 179.04000000000002, "end": 183.76, "text": " I know that batch gradient descent may not be the most intuitive name, but this is what", "tokens": [50364, 407, 294, 15866, 16235, 23475, 11, 562, 15866, 33733, 11, 321, 434, 15866, 264, 2408, 490, 50680, 50680, 741, 6915, 472, 281, 275, 11, 293, 15245, 16235, 23475, 307, 1237, 412, 264, 2302, 15245, 295, 3097, 51090, 51090, 5110, 412, 1184, 5623, 13, 51246, 51246, 286, 458, 300, 15245, 16235, 23475, 815, 406, 312, 264, 881, 21769, 1315, 11, 457, 341, 307, 437, 51482, 51482, 561, 294, 264, 3479, 2539, 1768, 818, 309, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.11972622993664864, "compression_ratio": 1.71, "no_speech_prob": 1.260608587472234e-05}, {"id": 34, "seek": 16140, "start": 183.76, "end": 187.16, "text": " people in the machine learning community call it.", "tokens": [50364, 407, 294, 15866, 16235, 23475, 11, 562, 15866, 33733, 11, 321, 434, 15866, 264, 2408, 490, 50680, 50680, 741, 6915, 472, 281, 275, 11, 293, 15245, 16235, 23475, 307, 1237, 412, 264, 2302, 15245, 295, 3097, 51090, 51090, 5110, 412, 1184, 5623, 13, 51246, 51246, 286, 458, 300, 15245, 16235, 23475, 815, 406, 312, 264, 881, 21769, 1315, 11, 457, 341, 307, 437, 51482, 51482, 561, 294, 264, 3479, 2539, 1768, 818, 309, 13, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.11972622993664864, "compression_ratio": 1.71, "no_speech_prob": 1.260608587472234e-05}, {"id": 35, "seek": 18716, "start": 187.16, "end": 193.72, "text": " If you've heard of the newsletter, The Batch, that's published by deeplearning.ai, the newsletter,", "tokens": [50364, 759, 291, 600, 2198, 295, 264, 26469, 11, 440, 363, 852, 11, 300, 311, 6572, 538, 2452, 47204, 13, 1301, 11, 264, 26469, 11, 50692, 50692, 440, 363, 852, 11, 390, 611, 4926, 337, 341, 3410, 294, 3479, 2539, 13, 50938, 50938, 400, 550, 309, 4523, 484, 300, 456, 366, 661, 9606, 295, 16235, 23475, 300, 360, 406, 574, 51150, 51150, 412, 264, 2302, 3097, 992, 11, 457, 2602, 1542, 412, 4356, 2090, 1385, 295, 264, 3097, 1412, 412, 51428, 51428, 1184, 5623, 1823, 13, 51515, 51515, 583, 321, 603, 764, 15245, 16235, 23475, 337, 8213, 24590, 13, 51742, 51742, 407, 300, 311, 309, 337, 8213, 24590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.15532142775399344, "compression_ratio": 1.8380566801619433, "no_speech_prob": 4.288875516067492e-06}, {"id": 36, "seek": 18716, "start": 193.72, "end": 198.64, "text": " The Batch, was also named for this concept in machine learning.", "tokens": [50364, 759, 291, 600, 2198, 295, 264, 26469, 11, 440, 363, 852, 11, 300, 311, 6572, 538, 2452, 47204, 13, 1301, 11, 264, 26469, 11, 50692, 50692, 440, 363, 852, 11, 390, 611, 4926, 337, 341, 3410, 294, 3479, 2539, 13, 50938, 50938, 400, 550, 309, 4523, 484, 300, 456, 366, 661, 9606, 295, 16235, 23475, 300, 360, 406, 574, 51150, 51150, 412, 264, 2302, 3097, 992, 11, 457, 2602, 1542, 412, 4356, 2090, 1385, 295, 264, 3097, 1412, 412, 51428, 51428, 1184, 5623, 1823, 13, 51515, 51515, 583, 321, 603, 764, 15245, 16235, 23475, 337, 8213, 24590, 13, 51742, 51742, 407, 300, 311, 309, 337, 8213, 24590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.15532142775399344, "compression_ratio": 1.8380566801619433, "no_speech_prob": 4.288875516067492e-06}, {"id": 37, "seek": 18716, "start": 198.64, "end": 202.88, "text": " And then it turns out that there are other versions of gradient descent that do not look", "tokens": [50364, 759, 291, 600, 2198, 295, 264, 26469, 11, 440, 363, 852, 11, 300, 311, 6572, 538, 2452, 47204, 13, 1301, 11, 264, 26469, 11, 50692, 50692, 440, 363, 852, 11, 390, 611, 4926, 337, 341, 3410, 294, 3479, 2539, 13, 50938, 50938, 400, 550, 309, 4523, 484, 300, 456, 366, 661, 9606, 295, 16235, 23475, 300, 360, 406, 574, 51150, 51150, 412, 264, 2302, 3097, 992, 11, 457, 2602, 1542, 412, 4356, 2090, 1385, 295, 264, 3097, 1412, 412, 51428, 51428, 1184, 5623, 1823, 13, 51515, 51515, 583, 321, 603, 764, 15245, 16235, 23475, 337, 8213, 24590, 13, 51742, 51742, 407, 300, 311, 309, 337, 8213, 24590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.15532142775399344, "compression_ratio": 1.8380566801619433, "no_speech_prob": 4.288875516067492e-06}, {"id": 38, "seek": 18716, "start": 202.88, "end": 208.44, "text": " at the entire training set, but instead looks at smaller subsets of the training data at", "tokens": [50364, 759, 291, 600, 2198, 295, 264, 26469, 11, 440, 363, 852, 11, 300, 311, 6572, 538, 2452, 47204, 13, 1301, 11, 264, 26469, 11, 50692, 50692, 440, 363, 852, 11, 390, 611, 4926, 337, 341, 3410, 294, 3479, 2539, 13, 50938, 50938, 400, 550, 309, 4523, 484, 300, 456, 366, 661, 9606, 295, 16235, 23475, 300, 360, 406, 574, 51150, 51150, 412, 264, 2302, 3097, 992, 11, 457, 2602, 1542, 412, 4356, 2090, 1385, 295, 264, 3097, 1412, 412, 51428, 51428, 1184, 5623, 1823, 13, 51515, 51515, 583, 321, 603, 764, 15245, 16235, 23475, 337, 8213, 24590, 13, 51742, 51742, 407, 300, 311, 309, 337, 8213, 24590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.15532142775399344, "compression_ratio": 1.8380566801619433, "no_speech_prob": 4.288875516067492e-06}, {"id": 39, "seek": 18716, "start": 208.44, "end": 210.18, "text": " each update step.", "tokens": [50364, 759, 291, 600, 2198, 295, 264, 26469, 11, 440, 363, 852, 11, 300, 311, 6572, 538, 2452, 47204, 13, 1301, 11, 264, 26469, 11, 50692, 50692, 440, 363, 852, 11, 390, 611, 4926, 337, 341, 3410, 294, 3479, 2539, 13, 50938, 50938, 400, 550, 309, 4523, 484, 300, 456, 366, 661, 9606, 295, 16235, 23475, 300, 360, 406, 574, 51150, 51150, 412, 264, 2302, 3097, 992, 11, 457, 2602, 1542, 412, 4356, 2090, 1385, 295, 264, 3097, 1412, 412, 51428, 51428, 1184, 5623, 1823, 13, 51515, 51515, 583, 321, 603, 764, 15245, 16235, 23475, 337, 8213, 24590, 13, 51742, 51742, 407, 300, 311, 309, 337, 8213, 24590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.15532142775399344, "compression_ratio": 1.8380566801619433, "no_speech_prob": 4.288875516067492e-06}, {"id": 40, "seek": 18716, "start": 210.18, "end": 214.72, "text": " But we'll use batch gradient descent for linear regression.", "tokens": [50364, 759, 291, 600, 2198, 295, 264, 26469, 11, 440, 363, 852, 11, 300, 311, 6572, 538, 2452, 47204, 13, 1301, 11, 264, 26469, 11, 50692, 50692, 440, 363, 852, 11, 390, 611, 4926, 337, 341, 3410, 294, 3479, 2539, 13, 50938, 50938, 400, 550, 309, 4523, 484, 300, 456, 366, 661, 9606, 295, 16235, 23475, 300, 360, 406, 574, 51150, 51150, 412, 264, 2302, 3097, 992, 11, 457, 2602, 1542, 412, 4356, 2090, 1385, 295, 264, 3097, 1412, 412, 51428, 51428, 1184, 5623, 1823, 13, 51515, 51515, 583, 321, 603, 764, 15245, 16235, 23475, 337, 8213, 24590, 13, 51742, 51742, 407, 300, 311, 309, 337, 8213, 24590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.15532142775399344, "compression_ratio": 1.8380566801619433, "no_speech_prob": 4.288875516067492e-06}, {"id": 41, "seek": 18716, "start": 214.72, "end": 216.8, "text": " So that's it for linear regression.", "tokens": [50364, 759, 291, 600, 2198, 295, 264, 26469, 11, 440, 363, 852, 11, 300, 311, 6572, 538, 2452, 47204, 13, 1301, 11, 264, 26469, 11, 50692, 50692, 440, 363, 852, 11, 390, 611, 4926, 337, 341, 3410, 294, 3479, 2539, 13, 50938, 50938, 400, 550, 309, 4523, 484, 300, 456, 366, 661, 9606, 295, 16235, 23475, 300, 360, 406, 574, 51150, 51150, 412, 264, 2302, 3097, 992, 11, 457, 2602, 1542, 412, 4356, 2090, 1385, 295, 264, 3097, 1412, 412, 51428, 51428, 1184, 5623, 1823, 13, 51515, 51515, 583, 321, 603, 764, 15245, 16235, 23475, 337, 8213, 24590, 13, 51742, 51742, 407, 300, 311, 309, 337, 8213, 24590, 13, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.15532142775399344, "compression_ratio": 1.8380566801619433, "no_speech_prob": 4.288875516067492e-06}, {"id": 42, "seek": 21680, "start": 216.8, "end": 220.8, "text": " Congratulations on getting through your first machine learning model.", "tokens": [50364, 9694, 322, 1242, 807, 428, 700, 3479, 2539, 2316, 13, 50564, 50564, 286, 1454, 291, 352, 293, 8098, 420, 11, 286, 500, 380, 458, 11, 1310, 747, 257, 9296, 294, 428, 36600, 1560, 13, 50834, 50834, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 536, 257, 3131, 295, 264, 16235, 23475, 9284, 51106, 51106, 382, 731, 382, 577, 281, 4445, 309, 294, 3089, 13, 51258, 51258, 509, 611, 536, 257, 7542, 300, 3110, 577, 264, 2063, 24108, 382, 291, 2354, 3097, 544, 36540, 13, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.10136052237616645, "compression_ratio": 1.5875, "no_speech_prob": 6.143729933683062e-06}, {"id": 43, "seek": 21680, "start": 220.8, "end": 226.20000000000002, "text": " I hope you go and celebrate or, I don't know, maybe take a nap in your hammock.", "tokens": [50364, 9694, 322, 1242, 807, 428, 700, 3479, 2539, 2316, 13, 50564, 50564, 286, 1454, 291, 352, 293, 8098, 420, 11, 286, 500, 380, 458, 11, 1310, 747, 257, 9296, 294, 428, 36600, 1560, 13, 50834, 50834, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 536, 257, 3131, 295, 264, 16235, 23475, 9284, 51106, 51106, 382, 731, 382, 577, 281, 4445, 309, 294, 3089, 13, 51258, 51258, 509, 611, 536, 257, 7542, 300, 3110, 577, 264, 2063, 24108, 382, 291, 2354, 3097, 544, 36540, 13, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.10136052237616645, "compression_ratio": 1.5875, "no_speech_prob": 6.143729933683062e-06}, {"id": 44, "seek": 21680, "start": 226.20000000000002, "end": 231.64000000000001, "text": " In the optional lab that follows this video, you see a review of the gradient descent algorithm", "tokens": [50364, 9694, 322, 1242, 807, 428, 700, 3479, 2539, 2316, 13, 50564, 50564, 286, 1454, 291, 352, 293, 8098, 420, 11, 286, 500, 380, 458, 11, 1310, 747, 257, 9296, 294, 428, 36600, 1560, 13, 50834, 50834, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 536, 257, 3131, 295, 264, 16235, 23475, 9284, 51106, 51106, 382, 731, 382, 577, 281, 4445, 309, 294, 3089, 13, 51258, 51258, 509, 611, 536, 257, 7542, 300, 3110, 577, 264, 2063, 24108, 382, 291, 2354, 3097, 544, 36540, 13, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.10136052237616645, "compression_ratio": 1.5875, "no_speech_prob": 6.143729933683062e-06}, {"id": 45, "seek": 21680, "start": 231.64000000000001, "end": 234.68, "text": " as well as how to implement it in code.", "tokens": [50364, 9694, 322, 1242, 807, 428, 700, 3479, 2539, 2316, 13, 50564, 50564, 286, 1454, 291, 352, 293, 8098, 420, 11, 286, 500, 380, 458, 11, 1310, 747, 257, 9296, 294, 428, 36600, 1560, 13, 50834, 50834, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 536, 257, 3131, 295, 264, 16235, 23475, 9284, 51106, 51106, 382, 731, 382, 577, 281, 4445, 309, 294, 3089, 13, 51258, 51258, 509, 611, 536, 257, 7542, 300, 3110, 577, 264, 2063, 24108, 382, 291, 2354, 3097, 544, 36540, 13, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.10136052237616645, "compression_ratio": 1.5875, "no_speech_prob": 6.143729933683062e-06}, {"id": 46, "seek": 21680, "start": 234.68, "end": 241.68, "text": " You also see a plot that shows how the cost decreases as you continue training more iterations.", "tokens": [50364, 9694, 322, 1242, 807, 428, 700, 3479, 2539, 2316, 13, 50564, 50564, 286, 1454, 291, 352, 293, 8098, 420, 11, 286, 500, 380, 458, 11, 1310, 747, 257, 9296, 294, 428, 36600, 1560, 13, 50834, 50834, 682, 264, 17312, 2715, 300, 10002, 341, 960, 11, 291, 536, 257, 3131, 295, 264, 16235, 23475, 9284, 51106, 51106, 382, 731, 382, 577, 281, 4445, 309, 294, 3089, 13, 51258, 51258, 509, 611, 536, 257, 7542, 300, 3110, 577, 264, 2063, 24108, 382, 291, 2354, 3097, 544, 36540, 13, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.10136052237616645, "compression_ratio": 1.5875, "no_speech_prob": 6.143729933683062e-06}, {"id": 47, "seek": 24168, "start": 241.68, "end": 247.20000000000002, "text": " And you also see a contour plot, seeing how the cost gets closer to the global minimum", "tokens": [50364, 400, 291, 611, 536, 257, 21234, 7542, 11, 2577, 577, 264, 2063, 2170, 4966, 281, 264, 4338, 7285, 50640, 50640, 382, 16235, 23475, 10704, 1101, 293, 1101, 4190, 337, 264, 9834, 261, 293, 272, 13, 50960, 50960, 407, 1604, 300, 281, 360, 264, 17312, 2715, 11, 291, 445, 643, 281, 1401, 293, 1190, 341, 3089, 13, 51257, 51257, 509, 1582, 380, 643, 281, 2464, 604, 3089, 1803, 13, 51402, 51402, 400, 286, 1454, 291, 747, 257, 1326, 6065, 281, 360, 300, 293, 611, 1813, 4963, 365, 264, 16235, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.09634403560472571, "compression_ratio": 1.6293103448275863, "no_speech_prob": 2.0578675048454897e-06}, {"id": 48, "seek": 24168, "start": 247.20000000000002, "end": 253.6, "text": " as gradient descent finds better and better values for the parameters w and b.", "tokens": [50364, 400, 291, 611, 536, 257, 21234, 7542, 11, 2577, 577, 264, 2063, 2170, 4966, 281, 264, 4338, 7285, 50640, 50640, 382, 16235, 23475, 10704, 1101, 293, 1101, 4190, 337, 264, 9834, 261, 293, 272, 13, 50960, 50960, 407, 1604, 300, 281, 360, 264, 17312, 2715, 11, 291, 445, 643, 281, 1401, 293, 1190, 341, 3089, 13, 51257, 51257, 509, 1582, 380, 643, 281, 2464, 604, 3089, 1803, 13, 51402, 51402, 400, 286, 1454, 291, 747, 257, 1326, 6065, 281, 360, 300, 293, 611, 1813, 4963, 365, 264, 16235, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.09634403560472571, "compression_ratio": 1.6293103448275863, "no_speech_prob": 2.0578675048454897e-06}, {"id": 49, "seek": 24168, "start": 253.6, "end": 259.54, "text": " So remember that to do the optional lab, you just need to read and run this code.", "tokens": [50364, 400, 291, 611, 536, 257, 21234, 7542, 11, 2577, 577, 264, 2063, 2170, 4966, 281, 264, 4338, 7285, 50640, 50640, 382, 16235, 23475, 10704, 1101, 293, 1101, 4190, 337, 264, 9834, 261, 293, 272, 13, 50960, 50960, 407, 1604, 300, 281, 360, 264, 17312, 2715, 11, 291, 445, 643, 281, 1401, 293, 1190, 341, 3089, 13, 51257, 51257, 509, 1582, 380, 643, 281, 2464, 604, 3089, 1803, 13, 51402, 51402, 400, 286, 1454, 291, 747, 257, 1326, 6065, 281, 360, 300, 293, 611, 1813, 4963, 365, 264, 16235, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.09634403560472571, "compression_ratio": 1.6293103448275863, "no_speech_prob": 2.0578675048454897e-06}, {"id": 50, "seek": 24168, "start": 259.54, "end": 262.44, "text": " You won't need to write any code yourself.", "tokens": [50364, 400, 291, 611, 536, 257, 21234, 7542, 11, 2577, 577, 264, 2063, 2170, 4966, 281, 264, 4338, 7285, 50640, 50640, 382, 16235, 23475, 10704, 1101, 293, 1101, 4190, 337, 264, 9834, 261, 293, 272, 13, 50960, 50960, 407, 1604, 300, 281, 360, 264, 17312, 2715, 11, 291, 445, 643, 281, 1401, 293, 1190, 341, 3089, 13, 51257, 51257, 509, 1582, 380, 643, 281, 2464, 604, 3089, 1803, 13, 51402, 51402, 400, 286, 1454, 291, 747, 257, 1326, 6065, 281, 360, 300, 293, 611, 1813, 4963, 365, 264, 16235, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.09634403560472571, "compression_ratio": 1.6293103448275863, "no_speech_prob": 2.0578675048454897e-06}, {"id": 51, "seek": 24168, "start": 262.44, "end": 267.44, "text": " And I hope you take a few moments to do that and also become familiar with the gradient", "tokens": [50364, 400, 291, 611, 536, 257, 21234, 7542, 11, 2577, 577, 264, 2063, 2170, 4966, 281, 264, 4338, 7285, 50640, 50640, 382, 16235, 23475, 10704, 1101, 293, 1101, 4190, 337, 264, 9834, 261, 293, 272, 13, 50960, 50960, 407, 1604, 300, 281, 360, 264, 17312, 2715, 11, 291, 445, 643, 281, 1401, 293, 1190, 341, 3089, 13, 51257, 51257, 509, 1582, 380, 643, 281, 2464, 604, 3089, 1803, 13, 51402, 51402, 400, 286, 1454, 291, 747, 257, 1326, 6065, 281, 360, 300, 293, 611, 1813, 4963, 365, 264, 16235, 51652, 51652], "temperature": 0.0, "avg_logprob": -0.09634403560472571, "compression_ratio": 1.6293103448275863, "no_speech_prob": 2.0578675048454897e-06}, {"id": 52, "seek": 26744, "start": 267.44, "end": 274.36, "text": " descent code because this will help you to implement this and similar algorithms in the", "tokens": [50364, 23475, 3089, 570, 341, 486, 854, 291, 281, 4445, 341, 293, 2531, 14642, 294, 264, 50710, 50710, 2027, 1803, 13, 50832, 50832, 2561, 337, 13465, 365, 385, 807, 264, 917, 295, 341, 1036, 960, 337, 264, 700, 1243, 293, 51024, 51024, 13568, 337, 1455, 309, 439, 264, 636, 510, 13, 51160, 51160, 509, 434, 322, 428, 636, 281, 5617, 257, 3479, 2539, 954, 13, 51362, 51362, 682, 4500, 281, 264, 17312, 20339, 11, 498, 291, 2378, 380, 1096, 370, 1939, 11, 286, 1454, 291, 611, 1520, 484, 264, 51630, 51630, 3124, 48955, 11, 597, 366, 257, 1481, 636, 300, 291, 393, 3834, 1520, 428, 1065, 3701, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.11366927407004616, "compression_ratio": 1.6701754385964913, "no_speech_prob": 2.1442394427140243e-05}, {"id": 53, "seek": 26744, "start": 274.36, "end": 276.8, "text": " future yourself.", "tokens": [50364, 23475, 3089, 570, 341, 486, 854, 291, 281, 4445, 341, 293, 2531, 14642, 294, 264, 50710, 50710, 2027, 1803, 13, 50832, 50832, 2561, 337, 13465, 365, 385, 807, 264, 917, 295, 341, 1036, 960, 337, 264, 700, 1243, 293, 51024, 51024, 13568, 337, 1455, 309, 439, 264, 636, 510, 13, 51160, 51160, 509, 434, 322, 428, 636, 281, 5617, 257, 3479, 2539, 954, 13, 51362, 51362, 682, 4500, 281, 264, 17312, 20339, 11, 498, 291, 2378, 380, 1096, 370, 1939, 11, 286, 1454, 291, 611, 1520, 484, 264, 51630, 51630, 3124, 48955, 11, 597, 366, 257, 1481, 636, 300, 291, 393, 3834, 1520, 428, 1065, 3701, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.11366927407004616, "compression_ratio": 1.6701754385964913, "no_speech_prob": 2.1442394427140243e-05}, {"id": 54, "seek": 26744, "start": 276.8, "end": 280.64, "text": " Thanks for sticking with me through the end of this last video for the first week and", "tokens": [50364, 23475, 3089, 570, 341, 486, 854, 291, 281, 4445, 341, 293, 2531, 14642, 294, 264, 50710, 50710, 2027, 1803, 13, 50832, 50832, 2561, 337, 13465, 365, 385, 807, 264, 917, 295, 341, 1036, 960, 337, 264, 700, 1243, 293, 51024, 51024, 13568, 337, 1455, 309, 439, 264, 636, 510, 13, 51160, 51160, 509, 434, 322, 428, 636, 281, 5617, 257, 3479, 2539, 954, 13, 51362, 51362, 682, 4500, 281, 264, 17312, 20339, 11, 498, 291, 2378, 380, 1096, 370, 1939, 11, 286, 1454, 291, 611, 1520, 484, 264, 51630, 51630, 3124, 48955, 11, 597, 366, 257, 1481, 636, 300, 291, 393, 3834, 1520, 428, 1065, 3701, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.11366927407004616, "compression_ratio": 1.6701754385964913, "no_speech_prob": 2.1442394427140243e-05}, {"id": 55, "seek": 26744, "start": 280.64, "end": 283.36, "text": " congratulations for making it all the way here.", "tokens": [50364, 23475, 3089, 570, 341, 486, 854, 291, 281, 4445, 341, 293, 2531, 14642, 294, 264, 50710, 50710, 2027, 1803, 13, 50832, 50832, 2561, 337, 13465, 365, 385, 807, 264, 917, 295, 341, 1036, 960, 337, 264, 700, 1243, 293, 51024, 51024, 13568, 337, 1455, 309, 439, 264, 636, 510, 13, 51160, 51160, 509, 434, 322, 428, 636, 281, 5617, 257, 3479, 2539, 954, 13, 51362, 51362, 682, 4500, 281, 264, 17312, 20339, 11, 498, 291, 2378, 380, 1096, 370, 1939, 11, 286, 1454, 291, 611, 1520, 484, 264, 51630, 51630, 3124, 48955, 11, 597, 366, 257, 1481, 636, 300, 291, 393, 3834, 1520, 428, 1065, 3701, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.11366927407004616, "compression_ratio": 1.6701754385964913, "no_speech_prob": 2.1442394427140243e-05}, {"id": 56, "seek": 26744, "start": 283.36, "end": 287.4, "text": " You're on your way to becoming a machine learning person.", "tokens": [50364, 23475, 3089, 570, 341, 486, 854, 291, 281, 4445, 341, 293, 2531, 14642, 294, 264, 50710, 50710, 2027, 1803, 13, 50832, 50832, 2561, 337, 13465, 365, 385, 807, 264, 917, 295, 341, 1036, 960, 337, 264, 700, 1243, 293, 51024, 51024, 13568, 337, 1455, 309, 439, 264, 636, 510, 13, 51160, 51160, 509, 434, 322, 428, 636, 281, 5617, 257, 3479, 2539, 954, 13, 51362, 51362, 682, 4500, 281, 264, 17312, 20339, 11, 498, 291, 2378, 380, 1096, 370, 1939, 11, 286, 1454, 291, 611, 1520, 484, 264, 51630, 51630, 3124, 48955, 11, 597, 366, 257, 1481, 636, 300, 291, 393, 3834, 1520, 428, 1065, 3701, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.11366927407004616, "compression_ratio": 1.6701754385964913, "no_speech_prob": 2.1442394427140243e-05}, {"id": 57, "seek": 26744, "start": 287.4, "end": 292.76, "text": " In addition to the optional labs, if you haven't done so yet, I hope you also check out the", "tokens": [50364, 23475, 3089, 570, 341, 486, 854, 291, 281, 4445, 341, 293, 2531, 14642, 294, 264, 50710, 50710, 2027, 1803, 13, 50832, 50832, 2561, 337, 13465, 365, 385, 807, 264, 917, 295, 341, 1036, 960, 337, 264, 700, 1243, 293, 51024, 51024, 13568, 337, 1455, 309, 439, 264, 636, 510, 13, 51160, 51160, 509, 434, 322, 428, 636, 281, 5617, 257, 3479, 2539, 954, 13, 51362, 51362, 682, 4500, 281, 264, 17312, 20339, 11, 498, 291, 2378, 380, 1096, 370, 1939, 11, 286, 1454, 291, 611, 1520, 484, 264, 51630, 51630, 3124, 48955, 11, 597, 366, 257, 1481, 636, 300, 291, 393, 3834, 1520, 428, 1065, 3701, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.11366927407004616, "compression_ratio": 1.6701754385964913, "no_speech_prob": 2.1442394427140243e-05}, {"id": 58, "seek": 26744, "start": 292.76, "end": 297.08, "text": " practice quizzes, which are a nice way that you can double check your own understanding", "tokens": [50364, 23475, 3089, 570, 341, 486, 854, 291, 281, 4445, 341, 293, 2531, 14642, 294, 264, 50710, 50710, 2027, 1803, 13, 50832, 50832, 2561, 337, 13465, 365, 385, 807, 264, 917, 295, 341, 1036, 960, 337, 264, 700, 1243, 293, 51024, 51024, 13568, 337, 1455, 309, 439, 264, 636, 510, 13, 51160, 51160, 509, 434, 322, 428, 636, 281, 5617, 257, 3479, 2539, 954, 13, 51362, 51362, 682, 4500, 281, 264, 17312, 20339, 11, 498, 291, 2378, 380, 1096, 370, 1939, 11, 286, 1454, 291, 611, 1520, 484, 264, 51630, 51630, 3124, 48955, 11, 597, 366, 257, 1481, 636, 300, 291, 393, 3834, 1520, 428, 1065, 3701, 51846, 51846], "temperature": 0.0, "avg_logprob": -0.11366927407004616, "compression_ratio": 1.6701754385964913, "no_speech_prob": 2.1442394427140243e-05}, {"id": 59, "seek": 29708, "start": 297.08, "end": 298.08, "text": " of the concepts.", "tokens": [50364, 295, 264, 10392, 13, 50414, 50414, 467, 311, 611, 3879, 2489, 498, 291, 500, 380, 483, 552, 439, 558, 264, 700, 565, 13, 50642, 50642, 400, 291, 393, 611, 747, 264, 48955, 3866, 1413, 1826, 291, 483, 264, 6175, 300, 291, 528, 13, 50915, 50915, 509, 586, 458, 577, 281, 4445, 8213, 24590, 365, 472, 7006, 13, 51142, 51142, 400, 300, 5607, 505, 281, 264, 1998, 295, 341, 1243, 13, 51318, 51318, 3087, 1243, 11, 321, 603, 1466, 281, 652, 8213, 24590, 709, 544, 4005, 13, 51526, 51526, 7156, 295, 472, 4111, 411, 2744, 295, 257, 1782, 11, 291, 1466, 577, 281, 483, 309, 281, 589, 365, 3195, 51759, 51759, 295, 4122, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.10512649503528562, "compression_ratio": 1.7442748091603053, "no_speech_prob": 5.142955342307687e-05}, {"id": 60, "seek": 29708, "start": 298.08, "end": 302.64, "text": " It's also totally fine if you don't get them all right the first time.", "tokens": [50364, 295, 264, 10392, 13, 50414, 50414, 467, 311, 611, 3879, 2489, 498, 291, 500, 380, 483, 552, 439, 558, 264, 700, 565, 13, 50642, 50642, 400, 291, 393, 611, 747, 264, 48955, 3866, 1413, 1826, 291, 483, 264, 6175, 300, 291, 528, 13, 50915, 50915, 509, 586, 458, 577, 281, 4445, 8213, 24590, 365, 472, 7006, 13, 51142, 51142, 400, 300, 5607, 505, 281, 264, 1998, 295, 341, 1243, 13, 51318, 51318, 3087, 1243, 11, 321, 603, 1466, 281, 652, 8213, 24590, 709, 544, 4005, 13, 51526, 51526, 7156, 295, 472, 4111, 411, 2744, 295, 257, 1782, 11, 291, 1466, 577, 281, 483, 309, 281, 589, 365, 3195, 51759, 51759, 295, 4122, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.10512649503528562, "compression_ratio": 1.7442748091603053, "no_speech_prob": 5.142955342307687e-05}, {"id": 61, "seek": 29708, "start": 302.64, "end": 308.09999999999997, "text": " And you can also take the quizzes multiple times until you get the score that you want.", "tokens": [50364, 295, 264, 10392, 13, 50414, 50414, 467, 311, 611, 3879, 2489, 498, 291, 500, 380, 483, 552, 439, 558, 264, 700, 565, 13, 50642, 50642, 400, 291, 393, 611, 747, 264, 48955, 3866, 1413, 1826, 291, 483, 264, 6175, 300, 291, 528, 13, 50915, 50915, 509, 586, 458, 577, 281, 4445, 8213, 24590, 365, 472, 7006, 13, 51142, 51142, 400, 300, 5607, 505, 281, 264, 1998, 295, 341, 1243, 13, 51318, 51318, 3087, 1243, 11, 321, 603, 1466, 281, 652, 8213, 24590, 709, 544, 4005, 13, 51526, 51526, 7156, 295, 472, 4111, 411, 2744, 295, 257, 1782, 11, 291, 1466, 577, 281, 483, 309, 281, 589, 365, 3195, 51759, 51759, 295, 4122, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.10512649503528562, "compression_ratio": 1.7442748091603053, "no_speech_prob": 5.142955342307687e-05}, {"id": 62, "seek": 29708, "start": 308.09999999999997, "end": 312.64, "text": " You now know how to implement linear regression with one variable.", "tokens": [50364, 295, 264, 10392, 13, 50414, 50414, 467, 311, 611, 3879, 2489, 498, 291, 500, 380, 483, 552, 439, 558, 264, 700, 565, 13, 50642, 50642, 400, 291, 393, 611, 747, 264, 48955, 3866, 1413, 1826, 291, 483, 264, 6175, 300, 291, 528, 13, 50915, 50915, 509, 586, 458, 577, 281, 4445, 8213, 24590, 365, 472, 7006, 13, 51142, 51142, 400, 300, 5607, 505, 281, 264, 1998, 295, 341, 1243, 13, 51318, 51318, 3087, 1243, 11, 321, 603, 1466, 281, 652, 8213, 24590, 709, 544, 4005, 13, 51526, 51526, 7156, 295, 472, 4111, 411, 2744, 295, 257, 1782, 11, 291, 1466, 577, 281, 483, 309, 281, 589, 365, 3195, 51759, 51759, 295, 4122, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.10512649503528562, "compression_ratio": 1.7442748091603053, "no_speech_prob": 5.142955342307687e-05}, {"id": 63, "seek": 29708, "start": 312.64, "end": 316.15999999999997, "text": " And that brings us to the close of this week.", "tokens": [50364, 295, 264, 10392, 13, 50414, 50414, 467, 311, 611, 3879, 2489, 498, 291, 500, 380, 483, 552, 439, 558, 264, 700, 565, 13, 50642, 50642, 400, 291, 393, 611, 747, 264, 48955, 3866, 1413, 1826, 291, 483, 264, 6175, 300, 291, 528, 13, 50915, 50915, 509, 586, 458, 577, 281, 4445, 8213, 24590, 365, 472, 7006, 13, 51142, 51142, 400, 300, 5607, 505, 281, 264, 1998, 295, 341, 1243, 13, 51318, 51318, 3087, 1243, 11, 321, 603, 1466, 281, 652, 8213, 24590, 709, 544, 4005, 13, 51526, 51526, 7156, 295, 472, 4111, 411, 2744, 295, 257, 1782, 11, 291, 1466, 577, 281, 483, 309, 281, 589, 365, 3195, 51759, 51759, 295, 4122, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.10512649503528562, "compression_ratio": 1.7442748091603053, "no_speech_prob": 5.142955342307687e-05}, {"id": 64, "seek": 29708, "start": 316.15999999999997, "end": 320.32, "text": " Next week, we'll learn to make linear regression much more powerful.", "tokens": [50364, 295, 264, 10392, 13, 50414, 50414, 467, 311, 611, 3879, 2489, 498, 291, 500, 380, 483, 552, 439, 558, 264, 700, 565, 13, 50642, 50642, 400, 291, 393, 611, 747, 264, 48955, 3866, 1413, 1826, 291, 483, 264, 6175, 300, 291, 528, 13, 50915, 50915, 509, 586, 458, 577, 281, 4445, 8213, 24590, 365, 472, 7006, 13, 51142, 51142, 400, 300, 5607, 505, 281, 264, 1998, 295, 341, 1243, 13, 51318, 51318, 3087, 1243, 11, 321, 603, 1466, 281, 652, 8213, 24590, 709, 544, 4005, 13, 51526, 51526, 7156, 295, 472, 4111, 411, 2744, 295, 257, 1782, 11, 291, 1466, 577, 281, 483, 309, 281, 589, 365, 3195, 51759, 51759, 295, 4122, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.10512649503528562, "compression_ratio": 1.7442748091603053, "no_speech_prob": 5.142955342307687e-05}, {"id": 65, "seek": 29708, "start": 320.32, "end": 324.97999999999996, "text": " Instead of one feature like size of a house, you learn how to get it to work with lots", "tokens": [50364, 295, 264, 10392, 13, 50414, 50414, 467, 311, 611, 3879, 2489, 498, 291, 500, 380, 483, 552, 439, 558, 264, 700, 565, 13, 50642, 50642, 400, 291, 393, 611, 747, 264, 48955, 3866, 1413, 1826, 291, 483, 264, 6175, 300, 291, 528, 13, 50915, 50915, 509, 586, 458, 577, 281, 4445, 8213, 24590, 365, 472, 7006, 13, 51142, 51142, 400, 300, 5607, 505, 281, 264, 1998, 295, 341, 1243, 13, 51318, 51318, 3087, 1243, 11, 321, 603, 1466, 281, 652, 8213, 24590, 709, 544, 4005, 13, 51526, 51526, 7156, 295, 472, 4111, 411, 2744, 295, 257, 1782, 11, 291, 1466, 577, 281, 483, 309, 281, 589, 365, 3195, 51759, 51759, 295, 4122, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.10512649503528562, "compression_ratio": 1.7442748091603053, "no_speech_prob": 5.142955342307687e-05}, {"id": 66, "seek": 29708, "start": 324.97999999999996, "end": 326.24, "text": " of features.", "tokens": [50364, 295, 264, 10392, 13, 50414, 50414, 467, 311, 611, 3879, 2489, 498, 291, 500, 380, 483, 552, 439, 558, 264, 700, 565, 13, 50642, 50642, 400, 291, 393, 611, 747, 264, 48955, 3866, 1413, 1826, 291, 483, 264, 6175, 300, 291, 528, 13, 50915, 50915, 509, 586, 458, 577, 281, 4445, 8213, 24590, 365, 472, 7006, 13, 51142, 51142, 400, 300, 5607, 505, 281, 264, 1998, 295, 341, 1243, 13, 51318, 51318, 3087, 1243, 11, 321, 603, 1466, 281, 652, 8213, 24590, 709, 544, 4005, 13, 51526, 51526, 7156, 295, 472, 4111, 411, 2744, 295, 257, 1782, 11, 291, 1466, 577, 281, 483, 309, 281, 589, 365, 3195, 51759, 51759, 295, 4122, 13, 51822, 51822], "temperature": 0.0, "avg_logprob": -0.10512649503528562, "compression_ratio": 1.7442748091603053, "no_speech_prob": 5.142955342307687e-05}, {"id": 67, "seek": 32624, "start": 326.24, "end": 330.28000000000003, "text": " You also learn how to get it to fit nonlinear curves.", "tokens": [50364, 509, 611, 1466, 577, 281, 483, 309, 281, 3318, 2107, 28263, 19490, 13, 50566, 50566, 1981, 13797, 486, 652, 264, 9284, 709, 544, 4420, 293, 8263, 13, 50770, 50770, 18072, 11, 321, 603, 611, 352, 670, 512, 8496, 6082, 300, 486, 534, 854, 337, 1242, 8213, 51040, 51040, 24590, 281, 589, 322, 8496, 5821, 13, 51216, 51216, 286, 478, 534, 2055, 281, 362, 291, 510, 365, 385, 294, 341, 1508, 293, 286, 574, 2128, 281, 2577, 51390, 51390, 291, 958, 1243, 13, 51416], "temperature": 0.0, "avg_logprob": -0.15859113581040327, "compression_ratio": 1.5560344827586208, "no_speech_prob": 2.4252964067272842e-05}, {"id": 68, "seek": 32624, "start": 330.28000000000003, "end": 334.36, "text": " These improvements will make the algorithm much more useful and valuable.", "tokens": [50364, 509, 611, 1466, 577, 281, 483, 309, 281, 3318, 2107, 28263, 19490, 13, 50566, 50566, 1981, 13797, 486, 652, 264, 9284, 709, 544, 4420, 293, 8263, 13, 50770, 50770, 18072, 11, 321, 603, 611, 352, 670, 512, 8496, 6082, 300, 486, 534, 854, 337, 1242, 8213, 51040, 51040, 24590, 281, 589, 322, 8496, 5821, 13, 51216, 51216, 286, 478, 534, 2055, 281, 362, 291, 510, 365, 385, 294, 341, 1508, 293, 286, 574, 2128, 281, 2577, 51390, 51390, 291, 958, 1243, 13, 51416], "temperature": 0.0, "avg_logprob": -0.15859113581040327, "compression_ratio": 1.5560344827586208, "no_speech_prob": 2.4252964067272842e-05}, {"id": 69, "seek": 32624, "start": 334.36, "end": 339.76, "text": " Lastly, we'll also go over some practical tips that will really help for getting linear", "tokens": [50364, 509, 611, 1466, 577, 281, 483, 309, 281, 3318, 2107, 28263, 19490, 13, 50566, 50566, 1981, 13797, 486, 652, 264, 9284, 709, 544, 4420, 293, 8263, 13, 50770, 50770, 18072, 11, 321, 603, 611, 352, 670, 512, 8496, 6082, 300, 486, 534, 854, 337, 1242, 8213, 51040, 51040, 24590, 281, 589, 322, 8496, 5821, 13, 51216, 51216, 286, 478, 534, 2055, 281, 362, 291, 510, 365, 385, 294, 341, 1508, 293, 286, 574, 2128, 281, 2577, 51390, 51390, 291, 958, 1243, 13, 51416], "temperature": 0.0, "avg_logprob": -0.15859113581040327, "compression_ratio": 1.5560344827586208, "no_speech_prob": 2.4252964067272842e-05}, {"id": 70, "seek": 32624, "start": 339.76, "end": 343.28000000000003, "text": " regression to work on practical applications.", "tokens": [50364, 509, 611, 1466, 577, 281, 483, 309, 281, 3318, 2107, 28263, 19490, 13, 50566, 50566, 1981, 13797, 486, 652, 264, 9284, 709, 544, 4420, 293, 8263, 13, 50770, 50770, 18072, 11, 321, 603, 611, 352, 670, 512, 8496, 6082, 300, 486, 534, 854, 337, 1242, 8213, 51040, 51040, 24590, 281, 589, 322, 8496, 5821, 13, 51216, 51216, 286, 478, 534, 2055, 281, 362, 291, 510, 365, 385, 294, 341, 1508, 293, 286, 574, 2128, 281, 2577, 51390, 51390, 291, 958, 1243, 13, 51416], "temperature": 0.0, "avg_logprob": -0.15859113581040327, "compression_ratio": 1.5560344827586208, "no_speech_prob": 2.4252964067272842e-05}, {"id": 71, "seek": 32624, "start": 343.28000000000003, "end": 346.76, "text": " I'm really happy to have you here with me in this class and I look forward to seeing", "tokens": [50364, 509, 611, 1466, 577, 281, 483, 309, 281, 3318, 2107, 28263, 19490, 13, 50566, 50566, 1981, 13797, 486, 652, 264, 9284, 709, 544, 4420, 293, 8263, 13, 50770, 50770, 18072, 11, 321, 603, 611, 352, 670, 512, 8496, 6082, 300, 486, 534, 854, 337, 1242, 8213, 51040, 51040, 24590, 281, 589, 322, 8496, 5821, 13, 51216, 51216, 286, 478, 534, 2055, 281, 362, 291, 510, 365, 385, 294, 341, 1508, 293, 286, 574, 2128, 281, 2577, 51390, 51390, 291, 958, 1243, 13, 51416], "temperature": 0.0, "avg_logprob": -0.15859113581040327, "compression_ratio": 1.5560344827586208, "no_speech_prob": 2.4252964067272842e-05}, {"id": 72, "seek": 34676, "start": 346.76, "end": 357.44, "text": " you next week.", "tokens": [50364, 291, 958, 1243, 13, 50898], "temperature": 0.0, "avg_logprob": -0.853003706250872, "compression_ratio": 0.6363636363636364, "no_speech_prob": 0.0027075880207121372}], "language": "en", "video_id": "3xyYI4wPuTs", "entity": "ML Specialization, Andrew Ng (2022)"}}