{"video_id": "nCq_vy9qE-k", "title": "Supervised and self-supervised transfer learning (with PyTorch Lightning)", "description": "Spring 2020 website: http://bit.ly/pDL-home\nSpring 2020 playlist: http://bit.ly/pDL-YouTube\nSpeaker: William Falcon & Alfredo Canziani\nFrom NYU Deep Learning, Fall 2020 course.\n\n00:00:00 \u2013 Week 10 \u2013 Practicum\n\n00:06:00 \u2013 Supervised and self-supervised\rtransfer learning\n00:18:43 \u2013 Supervised transfer learning\rwith PyTorch\n00:22:31 \u2013 Supervised transfer learning\rwith Lightning \u2013 implementation\n00:30:59 \u2013 Supervised transfer learning\rwith Lightning \u2013 training\n00:41:01 \u2013 Self-supervised transfer learning\rwith Lightning\n00:53:09 \u2013 Generalisation comparison\n01:08:34 \u2013 Summary", "author": "Alfredo Canziani", "keywords": ["Deep Learning", "Yann LeCun", "NYU", "PyTorch", "Lightning", "William Falcon", "Boltz", "transfer learning", "self-supervised", "supervised", "back-bone", "ResNet", "fine tuning"], "channel_url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw", "length": 4284, "views": 10637, "publish_date": "11/02/2022", "timestamp": 1605398400, "entity": "Yann LeCun", "transcript": {"text": " So welcome to class today. Today we're going to be talking about transfer learning with a supervised and unsupervised perspective. Here today we have William Falcon, an expert in the tools we are going to be using for explaining to you how this stuff works, which is going to be telling us a little bit more about this topic. So William, the floor is yours. Alfredo, thank you so much for having me here, and excited to share this with the whole class and everyone. So, okay, today we're going to be doing self-supervised and supervised transfer learning. So this is going to come up a lot for people. So if you work in an industry or doing any kind of research, you're going to run into something where you may not have enough data, and you need to have some model that's been trained on something else, and then you can use that to kind of jumpstart your process, right? So we'll cover it in the context of computer vision today, but this can transfer to NLP, speech, anything you want. Cancer learning. Yeah. Okay. Pun intended. And some of the main-to-main at work as well, but I'm confident that in the next few years, I'm sure we'll figure out a way to do that. So the first thing I want to do is I'm going to install Lightning, right? Hmm. And Lightning is a lightweight wrapper for PyTorch, for high-performance research, right? So if you're using PyTorch, it basically organizes your PyTorch code so that you can leverage things like multiple GPU training, GPUs, and different things that require a lot of expertise and, frankly, are things you don't need to deal with when you're trying to work and build models. And the second framework that I want to install is Bolts, right? So Bolts is our other framework, and it is for hyper... It's like a research toolkit, basically. Sorry, I can't find my mouse. There it is. Okay. So it's a research toolkit. So if you ever wondered, if you're starting something... You know, it's also used in industry as well, but if you're starting a project and you're looking for a model and you don't find one, you can look for it in Bolts, right? And then this will be one, maybe possibly one of the latest models, but it's already been implemented, tested, and documented so that you can start from a good spot. So you don't have to sit here and try to implement it based on yourself for three months to see if you get it right. You can just kind of subclass and build on that. And Bolts has a pretty robust library of social-supervised learning. It's a lot of stuff that, you know, it's personally done in my research as well, and things that we've implemented from the latest papers. Okay, if I want to learn more about these things, where can I find more resources? Yeah, so you can go to Lightning repo, right? So this is... I mean, I think probably the easiest thing is to go to PyTorch Lightning.ai here. So we have the landing page there. And then we have everything, documentation, videos, about Lightning, about the team, and everything else. And then there you can click on Docs, right? And then go straight to the docs, get started here in the Lightning in two steps, read this. And at the end of this, you should know everything you need to know. Again, if you know PyTorch, this should take like four minutes to understand. Okay. Whereas the bolts? Yeah, and then bolts is going to be... Let's see if we have it on the page as well. Let's see. Yeah, so we have the bolts here. So you can click on that and then visit Bolts. And then this is the documentation as well, right? So you can click on it, look at the introduction guide, and then we will walk you through the main ideas about bolts. So bolts is a newer project, so it is faster moving. So it will change pretty frequently. Lightning is 1.0, it's stable, so feel free to use it for whatever you want as well. All right. Okay, so installing this, I believe I'm on a GPU instance. Yeah, the cool thing is Lightning, you can actually use TPUs and GPS, but I'm going to use GPS right now. TPUs are dodgy sometimes. Okay, so the first thing I want to cover is, like, when do you want to fine-tune, right? So I think that there are certain things you can ask yourself to understand if what you're about to do is, you know, it's useful for fine-tuning. So let me paste this image here. So I made a little diagram for everyone. Okay, thanks. So let's start with the green spots, right? So I think the first question is, do you have a lot of data? If the answer is yes, then you likely don't need to fine-tune, right? Now, do you also have time in computes, right? You can have a lot of data, but if it's super costly to train, then you're going to want to find something pre-trained. But if you have the money and the time and the compute, then just go ahead and train on your data, right? And when you're done, run on your test data set. Now, if you don't have a lot of data, then you should try to find a pre-trained model for your data that matches your data distribution, right? So this is super important because most vision models are trained on ImageNet, right? So if you want to do something like, I don't know, cancer detection or X-rays, that's unlikely going to transfer, right? Because in ImageNet, you don't have that kind of data. You also don't have people, right? So you have to be mindful of what you're using this for. So it's not just blindly the magic of a neural network is going to work. And then, so yeah, so if you do have something that matches that kind of distribution, then you can use a pre-trained model, right? So when you think about transfer learning, we have two parts. You have the pre-trained model that was trained on something else, and then you have the stuff you're going to add on top of that to transfer that, right? And then you can fine-tune in your own data. So this is very important, so I would keep this in mind. And then the next things are this, right? So then you have two major options. You have supervised or self-supervised, right? So on a model that's pre-trained using supervised training, learning was trained likely for classification, so something for ImageNet, for example. And by doing that, you've introduced bias into that model, right? You've forced it to learn a representation that is going to make classification easier, but there's no guarantee that that's going to make something like segmentation or detection or anything else as easy. And so you want to be mindful of that, right? So the self-supervised, like I said, it's experimental, but it might be able to generalize better. So it was not trained for classification, which means that if you want to do segmentation or object detection, the representations might transfer better. So that's interesting. And then, you know, they're like, I don't know if you're following the literature or probably not, but there are about seven or eight options that you have to do this. So AMDIM, MoCo, CPC, Perl, SEMClear, BYO, and SWOVE, right? So in order, the latest one is SWOVE. And if you're interested in understanding the difference between all of these, you know, I recently published a paper with Professor Cho a few months ago, and then we wrote this article kind of going into all the details and how to compare all of them and, you know, what the differences are, because they're really not that different. So I'm going to be using SWOVE for this particular case for our self-supervised transfer learning. So the first thing is I'm going to, the first case that we're going to look at is going to be this supervised transfer learning. So this is likely the case that you're going to run into an industry most of the time, right? So you have a small data set of images and some compute budget. And in this case, we're going to pull out a ResNet-50, right? So there are many ResNets, 18, 50, whatever, 101, 152, but 50 is kind of like a sweet spot. It's not so big that it's super expensive to train, and it's not so small that it won't do anything interesting for you. Remember, this was pre-trained on ImageNet, and it was pre-trained for classification. And ImageNet is a data set that has a thousand categories. Each of them has a thousand images, so it's like one million, roughly one million images, right? So that's why it's actually very successful, because this huge amount of data allows us to distill a very, a model that has a very good prior in terms of natural images, right? Because it's been, like those thousand classes are names of natural objects. So that's why usually it works quite well. But nevertheless, as you pointed out before, if you want to use a network for medical images, where the statistics of your images are completely different than what they are in ImageNet, then it's completely hopeless to have any kind of decent result. Nevertheless, let's start with what is commonly done for normal images, right? Yeah, so I think to start, we're using this library called TorchVision, right? So by the iTorch team as well. And in there we have a bunch of pre-trained models. So this one's a ResNet-50. So I'm going to set this to true so I can load that model, which is going to download, I assume, some weights. Great. Okay, so there are the weights. And then now we can use this to run predictions, right? So let's pretend that our data set that we actually care about has 10 classes, right? And those classes are like frog, horse, whatever. So I'm cheating because there's this data set called CIFAR10, right? So CIFAR10. And this data set looks like this, right? So again, there are tiny images. They're 32 by 32 pixels and three channels per color. But you know, it's a useful toy data set. It's better than NMNIST, especially because you're using pre-trained ImageNet. And I'm pretty sure on ImageNet there are not a lot of fake digits or handwritten digits, so it wouldn't transfer super well. But there are dogs and cats and birds and all that stuff. So same kind of domain. Sorry, same kind of category. Yes, great. So let's use that, guys. So we're going to set up our data set, right? So let's pull in CIFAR10 again from TorchVision. Yep. And then we are going to use these transforms, right? So something that's useful normally for these cases is to normalize your image. So we're going to make it zero mean and then one for standard deviation, right? So we're going to add this on here. And I'm not going to add it right now because I want to actually plot the image for you so you can see what it looks like. So I do this and then that will actually download. I need to import that. Transforms, right? Here from TorchVision. Import transforms. Okay, so this is going to download, extract, and then we have this data set. Great. So that's ready to go. So let me just plot it now. So I'm just going to copy paste some map plot lib code to do that. And I'm going to also plot the label. So show you what the label is. And there you go. So you can't really tell, but that's label. It's a super nice frog. I can't tell. It's beautiful. Like a red and orange eye. But if you look at label six, I mean, let's just verify, right? So 0, 1, 2, 3, 4, 5, 6. So yeah, that looks like it looks like this guy, kind of. Yeah. So that's the frog. So let's normalize it now. So that's the neural network. Oh, it's already downloaded. Okay. Oh, wow. So that's great. And cool. So now we don't want to iterate through these images like one at a time, right? So see here, this is an image, a single image. So we actually want to do batch of images, right? So we're going to use a data loader for that. So I pull that in and I'm going to say batch size 32. And I do want to shuffle that. So there it is. And then obviously, you know, to iterate through this, it's just a simple for loop, right? So for batch in data loader, get the batch, expand it out, print the shapes, just so you can see what they look like. So 32 is the batch size, which is great. Three channels, 32 heights and 32 width pixels, right? And then our labels are 32. So just 32 scalars. And then now we're going to, I also need to modify my ResNet. So if you remember, the ResNet was trained for ImageNet. And as you said, there are 1000 classes there. So it won't really work when I have a data set with 10 classes. So for that, I need to modify my ResNet 50 to take that. So I think they're cheating because actually the ResNet 50 has the ResNet 50 plus these FC fully connected layers at the end, right? So we're going to replace that last fully connected layer, which I don't know what the size of the output is of that ResNet 50. But I know that I need to have 10 as an output because that's going to be the number of classes that I have, right? So what's happening under the hood is you have this like ResNet, right? A bunch of layers here. And then at some point when that ends, then you have this fully connected, which is mapping the output from here into whatever number of classes you have, right? So this guy I want to replace. You know, depends on the context. You sometimes can drop it, put an identity function, whatever you want. Okay, so we'll replace that. Let's just make sure that works. Okay, perfect. So now we're good to go. So let's go ahead and predict some stuff, right? So I'm just going to load another batch here, and then I'm going to run it through my ResNet. So this image, and then I'm going to look at the first 10 predictions only, right? Great. So that looks good. And I noticed that the highest number is this first guy here, right? 0.7, looks like. So let me do a softmax just to kind of like turn these into probabilities, right? Great. So 0.19. That looks like the highest one. So this is what the network would predict for this particular image, the label. And we're going to use the ArcMax to pull the label name, I guess, in this case. So we get zero, right? So that makes sense because that was the highest number as well, right? Under 10 here. Where do we feed the image to the network? Oh, up here. Right there. But X was a 32 by 32, right? And the ResNet usually is trained on the hundred and something, right? Pixels. So yeah, correct. That's convolutional though. So the inputs don't really matter as long as you don't shrink it too much, right? So I'm cheating a little bit, but the CIFAR 10 is fine. 32 won't disappear. If you did like MNIST, potentially you could have a crash because at some point you'll downsample way too much. Okay. So yeah, it's a beautiful part about the ResNet, sorry, about convolutional networks is, you know, the input can change as well. Okay, so let's put the labels. So I pulled the labels out and they don't match, right? So obviously you're like, well, this is supposed to be this fancy pre-trained image net model, but like it didn't predict the labels. So my labels are seven. I predicted zero. None of these are great. It's this one by chance. So one out of 10 makes sense, right? Expect the number given the classes. Yeah, so they don't match. So the reason for that is because we haven't, remember we replaced this layer here, right? Where is it? Yeah, here. Nope, not there. Here. So we replaced this layer. So like all of the last logic, if you replace anything in this pre-trained model, it's not going to work, right? And the last logic we made completely random. So now I have to fine tune this thing. So the process of fine tuning is, you know, you can do it two ways. You can take this pre-trained net network and keep it kind of frozen, right? So you're never going to backpropagate into it and then strap anything else on top of that. I'm going to use a linear layer, but you could use an SVM. You could use logistic regression. You can use a random forest. Any classifier doesn't matter because what you're doing is using the neural network to extract features and then you're using some other classifier to use those features and classify, right? And if the network did its job, they should be linearly separable. So then that SVM would be fine at that point. So in this case, I'm going to be lazy. I'm just going to use a linear layer, right? So I'm going to actually separate them to make this more clear. So here's the backbone. OK, so I'm not going to mess with anything about the FC layer. I'm just going to create a completely separate layer now. So this is I'm going to call this a fine-tune layer. And I'm just going to take whatever the output features were of that backbone, FC, and then I'm going to map them back to 10. So I'm just adding another layer. So in Bolt, so in Lightning, we have this concept of data modules, right? So notice I only have a train split for C410, but I actually want like a validation and test split. I'm like, you know, it's just going to be super annoying to do it myself and split them up. So I'm just going to use this data module that we have, right? So the data module is literally just going to be three data loaders, right? So it's going to be a train, a val, and a test data loader. And then it has within it all the splits and everything you need to care about. So let me show you what I mean by that. So go to the documentation. I'll go to the vision data modules, supervised learning, C410. So they're here and I can look at the source. So here's the source code. And now I see that, you know, there's a bunch of preparation and all the kind of like boilerplate stuff is taken care of for you. So I'm just going to get a data loader with the train splits, a data loader with the validation splits, and a data loader with a test split. And I don't have to deal with any of this other magic, but it's just a PyTorch data loader. So I'm going to use this guy because I want to save us all a bunch of time. So let me just call this up and play PyTorch real quick just to show the basics of this. So I'm going to import an optimizer and then our last function here. And I'm going to use Adam, but I'm not going to I don't want to update this backbone because as soon as I start updating it, it's going to lose kind of its representation. Yeah. Yeah. OK. I just want to use a classifier. Yeah. Just a classifier right now. So I'm going to do that. And then I'm going to just iterate through my, you know, I'm going to set an epoch here. And then again, this is the data loader. I can just use the data loaders directly. Right. This is you can use them in PyTorch. So I'm just going to pull the train data loader out and then iterate through here. And then I want to run the input through the backbone. I'm going to get a bunch of features. Right. So I'm going from batch by three by thirty two by thirty two. So channels, heights and width pixels to batch by one thousand, which is the number of classes on image. But we're going to treat this as embedding dimension here. And, you know, I don't have to do this because my optimizer is only looking at the spine to layer, but I'm going to detach here. Right. So I could just attach it here. So it doesn't matter. I'm going to I'm going to comment this out for now. But I just want to show that, like, if you were training both at the same time, you could just attach the features and pass them into the classifier. Yeah. But actually, I would recommend you to do like if you go a couple of lines above, if you click under the X below the X, X, Y equal batch, the just below. Yeah. Yeah. One line below. OK. There we can one line. Yeah. We can write with torch dot no grad. Oh, sure. So I would prefer this actually. No underscore grad parenthesis and colon. Yeah. And then we indent that feature. So this is basically not telling PyTorch not to track the computational graphs. Right. So it's going to be much faster. And also, hold on. I think it's going to be not sure faster, but it's going to be not taking any additional memory because we are not going to be doing back propagation through the backbone. Right. So this is usually this is the best practice, I would say. Right. Yeah. Makes sense. I know again, but I'm only optimizing this guy. Right. Yeah. But yeah, to your point, you won't keep a graph when you do this, which is definitely more efficient. OK. So then where's my screen? OK. So we have the features and then we're going to run them through our fine tune layer. Right. So this guy here. So again, this fine tune layer could be like an SVM if I wanted to do. Right. It doesn't matter. But here I'm using this linear layer that I created up here. OK. I'm going to run it through the fine tune layer. It's going to give me the predictions. I guess it'd have to be differentiable in this case, but that's OK. And then I'm going to calculate the loss. Right. So cross entropy. So I'm going to pass in the predictions and then the labels. Is the space missing? Hold on. Yeah. I like that we both tune in immediately. So you're like, why is that off? And then, you know, the standard backward optimizer stuff, optimization stuff. Right. So we'll do that. Cool. And then, you know, I'm just going to print the loss just so that we know where we're at. OK. So I'm going to run this guy and hopefully you'll start seeing the loss coming up. Let's see. There it is. All right. It's something's happening. The magic is happening. OK. So I'm going to stop this because it's super slow. We're using a GPU machine. So I kind of want to use this GPU. So I'm going to I'm going to just convert this into lightning real quick. And that process is going to be super fast because I'm just organizing what I did. Right. So I have all the same stuff there. So the first thing I'm going to do is just create this lightning module. Right. So this class here. And then PL we imported at the top. Right. So it's just lightning. Did we import it? No, we didn't import. We just installed it. Yeah. So let me just import it. Import. I torch lightning as PL. Great. So this is the same basically as an NNM module. Right. But it's just lightning. So and then I'm going to change things a bit. Right. So I want to kind of I need to call this first. Oh, sorry. So I need to write our unit function. I write this so much nowadays. Okay. So and then I'm going to in its super class. Great. And then I want to use a backbone. Right. So I'm just going to bring that guy in. What was it? Here. So I'm just going to define it in the model. And then I want to also use this linear classifier guy. Here. Okay. So here's our fine-tune layer. Great. Same thing. Perfect. Okay. And then I want to parameterize this a bit. Right. So this 10, I can make this more general. This is an image classifier. So I'll just say num classes. Right. And we'll default to 10 because it's going to be for CIFAR 10, I'll say. But I can just do that. And then so that's it. And then now the training stuff. So this is where this is what's going to abstract the way the training loop so that you know we don't have to deal with all this boilerplate code. So this training step is like the forward in a modern. What is it? Yeah, it's kind of like a forward, but instead of just doing kind of like a forward passive computation. So let's see define all the interactions of the model, the loss and everything else. Right. So it's capturing like a full system. So if you were to do BERT or GAN or VAE or something like that, all of that logic would happen in training stuff. So it's easier to understand and keep together. So what does this method has to return? So you have to return a loss. Right. Oh, okay. Yeah. And I'll show that a minute, but it has to have a computational graph attached to it so it can do the optimization. So where do we have all that? It's literally just all of this stuff here. Right. So I'm just going to copy. So, okay. So your training loop that we wrote before in a in a in a PyTorch, it's it's go it goes inside the model now. Yeah, exactly. It's a little bit interesting. But what it does is it makes it so that your model self contain. Right. So in this version, you not reusable. Like I can't reuse this code for like a different task tomorrow. It's like very specific to this thing. So I'm just pulling out the relevant stuff, which is what you know, what we're going to spend 99% of our time on is modifying these. So we're going to Just keep a few things there. You just selected half. So what about the last part? Yeah, so that you don't actually need. Right. So lightning is going to do this for you automatically. It's going to call backwards the step and the zero grad. There's a way that you can enable it if you want to. But and you can do it yourself. But we're going to use something called automatic optimization, which is enabled by default and lightning where it's going to do it for you. You can turn that off and then you just call it yourself. And that's fine. Okay. Okay. So otherwise we just have to specify basically what the loss is given a batch. Is it correct? Correct. Yeah. Okay. So let's let's do that. So I paste all that. So nothing changed. I'm going to remove this thing here because I mean, you know, you can leave it if you want to guess doesn't matter. No, no, leave it. Why do you want to remove it? Yeah. Well, because I want to change eventually so that we can actually find to in the backbone, but we'll leave it for now. We'll make that change later. Okay. So this is soft backbone now. Right. So let's just back bone. And then this is self that fine tune layer. Cool. That should pretty much have all the same stuff. And then I returned this last year. Great. I see. Okay. And then the last thing is so so lightning modules like a recipe for a model, right? Like, or like a class or whatever you're trying to do. Here's a classifier. It's not. It's not a model. Right. It's just like a general classifier. But the last kind of ingredient I need is this optimizer. Right. So I need to know what optimizer I'm going to use. Yes. Yeah. And there's a there's a method called configure optimizers. Why specify that? So these names are, I believe, private names, training step and configure optimizers. Where do I find them? How do I know what are these names? Yeah. So those are on the documentation. Right. So when you go through here, lightning in two steps, it'll ask, it'll walk you through it. I'll say, this is what you need to define training stuff, configure optimizers. Okay. Okay. Yeah. And forward is optional. I'm not using it, but you don't have to use it because we're not using this model for predictions. So I don't have to actually define it. So if we would use this model for prediction, you actually would use the forward? Yeah, correct. So in this demo, I'm using an autoencoder. So in this particular case, I wrote the autoencoder to generate embeddings when you use it. So I just wrote up the forward to do that. But the training step is separate. Okay. So we have the definite, the init that is basically defining all the modules inside. Then we have a forward, which is, as you said, use only whenever you may use the model, but we don't necessarily need. Then there is the training step where we define how the loss is computed given a batch and a batch index. And then finally we have this configure optimizer, which is specifying the optimizer we're going to be using for adapting the parameters of the network. Did I correct? Yeah, that's exactly correct. All right. Okay. Okay. Makes sense. Makes sense. So I'm going to use that optimizer. Right. So Adam, we have it up here. Okay. Okay. So, you know, when we did it up here, we were just passing in that fine tune layer, right? You don't have to do that. So I'm just going to call self here. And that's going to just pass in all of the parameters. Right. And that's okay because the backbone is disabled because it's no grad thing. So it's going to be fine. I'm not actually going to backpropagate into it. And then I'm going to make this learning rate think like this. And then I need to return this optimizer. Don't forget this or you'll get random noise. Okay. So that's literally it. So again, this is the same code. But now it's like vastly less boilerplate, even for such a simple project. Right. And then now to train this very simple. So I'm just going to edit my model. Right. Oh, actually, I forgot one thing. So this learning rate, I kind of want to tune it. So let me just make it a parameter. Right. And then let's do that here. Three and then I'll pass it in. So if that equals that. Great. So I think there should be some very nice trick, I believe, in aligning that I don't have to type this. Yeah. So we have a save hyper parameters, I believe hyper parameters. Yeah. Okay. You don't actually have to. Now, when I do this, I don't actually have to go and say self nonclasses equals Excel, LR equals whatever. Right. This I don't have to do this anymore. I can just directly. Okay. So I can call it here. So it's saved under this thing called H prams hyper prams. Right. Okay. It's actually there directly. I see. And this is useful because, you know, in most models, you have like 30 parameters. Right. So you don't want to do this manually. Okay, great. Now just to train this thing. It's very simple. I'm just going to edit that model. Right. So how do we train this stuff? Yeah. So we have this. So remember all the stuff that we got rid of. So you put batch optimizer, whatever. That's all inside this trainer that basically handles all the engineering for you. Which trainer. So it's here. Right. So this trainer is here. Oh, I see. We haven't explained yet. Yeah. So in the lightning library, you have you only have two things you need to know about in lightning. Right. This trainer and then the slide. That's what I mean. So the trainer is like literally your, your foot, your simple training group. Right. So let me just show you. So lightning module. Finds that and then the trainers here. So lightning. Okay. The main API. I see. I see. I see. Okay. Also, there's just two there. Yeah, that's it. Okay. And then I think we have even a pseudo loop so you can understand how what it's doing. Let me see. Where is it? Yeah, I think it works. Okay. But I think I got it. So I just go to the docs and then there is the API and then there are the two things that you mentioned before there is the model. And then there is this trainer, which is training the model, which makes sense. I think. Yep. And then the lighting module tells you the same thing. Right. And then actually, I think it is the lighting module where we write. Yeah. So here you go. So it's showing here under the hood. Lightning is going to do this. It's the same. Okay. That we wrote here. Okay. Okay. Okay. That was my question, actually. So this is exactly the thing that we wrote on the, on the notebook. Right. So we have the model train and then you go back to the documentation. All right. So the other one. Yeah. So we have the model in the training version, right. Training mode. Then we enabled the gradients. And then I guess there is this saving the the output of the, from the training step, which is going to be the loss. Then, okay. We compute the backward pass and then we step and then the zero. Okay. Yeah. And as you see, you were passing. Oh, there's a bug here. This should just say batch here. Right. You're just passing the batch right where the training stuff. Okay. So we're back here. So that was training stuff. Configure optimizer. So we're good to go. The trainer is going to do this. So on colab tends to, tends to freeze because the update happens very fast to. So when you start training lightning and it's going to print a little progress bar and it's going to overwhelm the screen. So we want to slow that down. So let's change that to call it 10. So that we're not freezing actually 20. So that we're not freezing the screen. And then we also need the model, our classifier right that we just wrote. So here we go. This guy. And then we're going to, I like to swap these. So I'm going to add them. Then I'm just going to train there is a class. Yeah, so right now it's an instance of a class. So trainers. Okay. So I'm going to create an instance of that I'm going to call it fits. And then I'm going to pass in my classifier in here. And then we have the data. So, as I mentioned, I have just this train loader. So I'll show you first is train loader so I can pass that in. It's not a problem. So I'll pass in the regular pie torch data loader. And then this will just start training. Right. Oh wait, wow. He's also using the GPU available true used false. Yeah, and actually we give you a warning we're saying hey you have a GPU but you're not using it. Right. So we have we give a good experience there. Anyway, so you see this thing is training. I'm going to use that data module right that we created instead, so I just passed that in. And it's going to be the same effects. Right. So you don't have to deal with it, it knows to pull out the training. Okay, the training splits. So it's going, let's give it a few seconds. So also here TPU available. You can also use TPUs you said before right. Yeah, correct. Wow. Okay, I'm curious. I'm just going to use the GPU though so we have the GPU. And I'm not going to change my code I'm just going to set GPU equals to one here. And now we're training on a GPU. And you'll see that it's much faster. Oh wow. So we try as well on the TPU and just for sake of curiosity. We can. It's just you have to install this XLA library. Okay, okay then next time. So the caveat with TPUs is that Google is working super hard to make sure that they can support an MPI torch together to support TPUs. So the experience is not quite there yet you still have to install this XLA library, and if you go to the lightning docs it'll show you. But yeah, once you do that you just change this to, I think it's TPU cores. And then you set it to one or eight or whatever you want. And then you'll be training on the TPU. Okay, there are plenty demos on the website to show that. Okay, okay, okay, okay. Just to be sure that. Yeah, I just want to keep this one focus on the training. Okay, okay. Fine, fine, fine. Okay, so wait while we were talking to train so see the losses at 1.95. Oh wow, super long. So, let's just see what happened. So let's see. Let's see how it learned so far. So lightning creates logs for you automatically and I can just launch tensor work to visualize. Oh wow really impressive. And here we are, tensor board shoots up, and now you see, oh you didn't log anything. Yeah, so we have to log something. So let's log let's just log this loss right so I'm going to say self dot log. And then I'll say train loss has send the loss. I also wanted to accuracy so let me just pull out our fancy metrics library from fighters lightning metrics dot functional import accuracy. Right. And then we're going to also log the accuracy. So, so self dot log is something is a method in the model. So it's a method of the model of the lighting module. So, what's cool is that soft out log so you're turning on one GP right now but when you start training on eight 200, whatever, you have to sync logs across GPUs and you have to like, you know, calculate metrics correctly like you can average accuracy but you can't average something like RLC or something like that. So, lightning handles all that distributed stuff for you and sinking and when to do it. If you saw that log and everything else. I think you made a mistake there. So this is something that I think every time it's very it's very like tricky mistake. You didn't use attach detached that loss right so it looks like you're saving your login the whole computational graph there. So when you log in lightning will detach anything. Oh wow, really. Yeah, you don't have to worry about it. Okay, so nice. We try to make sure that you're. We keep people making mistakes. Okay, so so let's lock so we're training, we're logging the training accuracy as well now, and then the training loss. Yeah. So we have this morning here. So we're not validating right now. So lightning is saying hey, you pass in about that a letter but you haven't implemented a validation step right. So we only have a train loop, and it's saying that because the data module has a validation split attached to it right. But that's okay I don't want to validate right now. So I'm just going to train to see what's happening with this fine tuning thing. Otherwise, if you were using like the old the previous version the one with the pie torch data set. You had to set, you had to pass both the training invalidation separately. Yeah, so you have to do this, and then pass in about loader. Oh, I see. Okay, so you have to send them separately. You have to send them separately but instead in this case you send like a class, which I see. Okay, makes sense. Okay, so what is that an epoch and a half. Great. Let's see what happens. Let's reload this guy. Oh, did it load already for me. Nice. Maybe I don't have to reload it. Let's see if that's report that it's thing today, didn't have to reload it. I'm gonna zoom out for a second. Okay, sure. This huge. Alright, so, you put oh wow, did walk you fucking a half. Fine. Let's look at our train accuracy. It's all over the place because it's training right so you want to, you want to, you mostly want to be tracking your validation like epoch accuracy but we know it's high like without doing anything where we get 28%, which is great. So it shows the transfer learning is working. And then our last year we going down so it's kind of bumpy but that's expected. Again, because this is the training. So yeah, so now we're seeing the logs of this which is great. So, now that we have this stuff here. I actually want to. I actually want to unfreeze this rest nets, after a few bucks, right. So we're going to find, we're going to find soon as it is right now but after, let's just say, 10 bucks. I'm going to say hey unfreeze the backbone and start using the backbone as well. Yeah. The different learning rate usually I do that. Yeah, so you can adjust that as well right so I'm going to do I'm going to skip the learning rate adjustment but let me start just by changing the epoch thing right so the, the, the lightning module. No, the trainer. Yeah. So any module has a pointer to the trainer, and then the trainer knows what current pockets in right. Okay. You buck. As long as it's less than 10. I'm going to do this stuff here. Right. So hold on, how, how does the network knows about the trainer. Yeah, so when you when you start this training process in here. Yeah, in fits, then the trainer, the network gets assigned the trainer, and then it knows what it's in. Yeah. Okay, okay, okay. So, so only only. Well, okay so in real life you would like, wait like 1020 bucks but right now we're limited I'm going to put one epoch actually create feature of lightning, let's change it here. So, you see how long this epoch is taken right now, because I'm going to 100 samples. Let me just actually limit the number of training batches so we can go through more bucks faster. So, I'm going to go through 50 training batches that's it. And then, and then actually I can do this realistically so I can say 10 bucks. So as long as the epoch is under 10, I'm going to do this, where they're not, they're going to be no gradients. As soon as you're out of that though. Right. So, I need an interstatement sorry. Here, so as soon as you're out of that, I'm just going to actually pull out the features. Right. Okay. So now I'm going to be back propagating to this thing. Great. Okay, so I just made some changes and I'm not quite sure if it's going to break so I'm going to use a quick debugging trip is called fast dev run. So I'm going to turn this on real quick. And when I when I enable this is going to hit every single line of code. And they're not going to train it's just going to do one batch very quickly, just to make sure that I have no bucks let me just run that real quick. And if this completes I have no problems. Okay, great. No bugs. So it's like the compiler. If I had a bug. If I had been like asserts false. Right. Yeah, then it would catch it without having to train the whole time. Great. Oh, wow. Okay. Okay, I see. Okay, so, so real live debugging here. Okay, perfect. So I'm going to disable this thing now. And I don't need 50 batches, like, just being ambitious, let's just do 20. That's fine. And then this should power to the box a lot faster. Okay, great. So that was one, one epoch. So you see it's going super quick. And it's kind of kind of like weird because this refresh rate is 20 so it's only going to look at every 20 bucks. So let me just change that to five, sorry batches. So every five batches is going to update update this bar now. Okay, we have 10 minutes left and we still have to cover the unsupervised learning just to let you know. Got it. Okay. Okay, so here. So let's say about two, three, four. So you see the losses going down nicely. So let's just keep it going for a minute. And then it's going to unfreeze at some points and then the loss is going to drop a lot more. Now, as you mentioned, you have to lower the learning rate as well. I'm not doing that. So I'm not going to get the best performance out of this. But you can do that as well. How many, how many books does this stuff go for? Oh, by default 1000 right you can you can set a max limit or something in the trainer initialization. Yeah, here. So I see. Okay. Max C box. I think it's called. Okay. Okay, it's going. Yeah, it looks like that didn't work because this loss is high now. Maybe it starts working now. So yeah, had I just a learning rate that wouldn't have happened, it wouldn't have jumped and lost. It would have just been fine. But I think it's adjusting now. So, let's see what happens. Oh, it's also 50 batches so that that will explain some things as well. Okay, there you go. So now we're, we're under two which is great. So we hadn't seen that before. And so we're dropping faster so let's just show the last now. Okay, right. Just so we can see what the effects were. Luckily for self supervised learning. Not much changes, you only change the backbone, so should take a while. Okay, okay so the last two that we cared about where these guys right. So this was not frozen, and this was, so this was frozen backbone and unfrozen backbone. So let's see what happened. Yeah, so you can see the training accuracy. At some point started going higher, once the unfreezing thing happened. Also trained a lot longer for sure so and it's, we didn't say see it so it can be the random minute as well. And then the train loss as well so it kind of starts going down, and this jump up here is because of the unfreezing part, right, so that's when that happened. Actually, I think it's around here, but it was already lower at this point. Obviously because the train longer but on freezing tends to give you a better performance over time. But I think it's really specific to your task. So, let's use self supervised learning instead. We have like, I mean it's for this last part. Yeah, so instead of using this rest of 50 I'm just going to use the swap model, right. So, all I'm going to do now is I'm going to look at bolts, and I'm going to, I'm going to load those models, right. So here is the path, right so you just have to give it a weights path. And you can find that in the docs, right so it's just for me show me what is this swap thingy. Yeah, so swap is one of the latest methods coming out of fair. So this is both so I'm going to look at self supervised learning. Oh, okay. And if it's here I'm going to use swab. And then I have the image in that baseline here. And then I know I can load it through here so I can just copy paste this right is there also a link to the paper somewhere here. I believe so. Let's see. Yeah, there is. Okay, okay. I see. And then you can, you know, so it's adapted from the official position we actually worked with Matilda to do this as well. So it's super helpful. And, and yeah so here's the years, I'm just going to copy this right so I can. All right, yeah, this. Sorry. Apparently my mouse doesn't work. Okay. So that's all I need to do. So I'm going to bring that guy in. And I'm not going to freeze anything so I get swab here. Great. So it's going to load the checkpoint for that. And then swab has this model inside it, which is the backbone right. So I'm just going to pull that out so swab that model. So that's going to be the backbone of swab. And then swab in particular outputs 3000 features. Right, so I'm just going to change that to 3000. And then, in this particular case, swab this model also outputs to feature maps, so I just want the last one, I don't need. This is a pre trained model is it. Yeah, it's a pre trained model. Correct. It's pre trained on images without labels. Is it correct. Yes, so it's sub supervised so it was pre trained on image net without any labels whatsoever. So the idea. Before we were doing. So we are doing transfer learning. And before we were doing transfer learning from supervised learning using a network train on image net. Right now we are doing still transfer learning but using a network that has been trained on, we don't know what, but without labels. So maybe that it's nice because I can pre train my swab model on my own data. Yeah, I don't have many labels, for example, and then I can just train the classifier with a few labels I have right. Yeah, that's a good point. So if you are, I mean, if you're working at a company, most likely you're going to have your own data set that has, you know, might be massive but it will cost a lot of money to label so you don't need to worry about it too, so much maybe until this stuff works really well, you might have to but you know you can try this out, and then pre train on supervise on that. And then, and then use that. So, yeah, that makes sense. Okay. Okay, so we get the backbone 3000 features. Again, this particular model is going to have two feature maps. It's just the way it's trained so I just want the last one I don't want both. Right. So let me just make that more clear so features equals features, negative one, because this is a, this is actually an array actually, let's call it feature one and feature to right, so I just want the second one here. Okay. And then I'm going to do the same thing here when it's unfrozen. Great. I believe those are all the changes I need to make to get the store. So, that is it. That is the extent of modifying for self supervised learning, I told you it's going to be under 10 minutes. I'm impressed. I never actually done this myself. So let's see I don't have to change anything. Now, let's see if this works. Oh, look at that. So we can actually, if we let it run a little bit we can also compare performance right later for the lower, it's, it's a two already like the other one didn't get the slow before and it's not even unfrozen yet. Right. So, yeah, I don't know. It's interesting. Okay, okay. Oh, this impressive. So let's let's wait a little bit and then we can. I guess it's below two already and you know the other one didn't get to below two until we influence the model right now we're about to freeze it because it's layer 10 so let's see what happens so now we drop so this one even didn't spike, this one didn't go up to three point something it just started to go down. So, yes, 1.4 now it's working much better like, I don't know why that didn't spike like, I think that obviously this is an open area of research and like, you know, I think most of us in the lab are like working on this as well with that with john here or young, I think that despite the despite might be coming from the fact that that model. Final learning rate was pioneer and now that we are making a larger one maybe we escaped from that small from that kind of minimum we were in right so I think, perhaps. It's a good theory. I mean, I'm just assuming that I don't know if I'm. Yeah. I mean, I would say, I have no idea. I mean, I think there's, there's definitely an aspect to that. I think I would say something like, I don't know. Yeah, I think that's probably the most, the most reasonable explanation. I want to say that it's about self supervised learning but I can't say that with full certainty. Okay, so let's let's compare the performance of these two of the last one so we had to refresh the dancer board, I guess. If he come backs comes back to life. There we go. So, the last three are the ones we care about. Just the last two I think is enough seven and eight. Yeah, okay fine. Okay, because six was not trained I think. Supervised and this is unsupervised so blue is unsupervised. So let's see they all they can actually, can we change the name of this version 234 such that we can put some more descriptive descriptive thing or. Yeah, you can, you can. The docs will explain how to do that. Okay. Okay, so train accuracy unsupervised. And then the train loss. Oh, okay. Okay, you convinced me. All right. And so we have seen how to perform a transfer learning with pre trained backbone first with a supervised pre trained backbone, then we done self provides pre trained backbone, but then what is the advantage of using these transfer learning. So, generally, we're going to be able to kind of get a jump start on our training, right, so we're going to be able to sometimes converge a lot faster, but second be able to generalize to the data set that we were training on so remember we had this data set that the model was not pre trained on, and we're hoping that we can generalize to our test split of that data sets. Right. So as we showed right now we only train on the training split. So that's great just to make sure things are working but you want to know if you're going to do well when you see actual data, which we're going to use the validation support. Alright, so let's see how we can validate our networks. Yeah, so let's let's do a quick recap right so we have. This is the supervised model, right, so this was using our standard rest net 50 backbone. And then below I have the self supervised model so we just swap that with the swap model. Yeah, so let's go ahead and train the supervised one. And we're going to do it for 20 bucks. And we're not going to limit the number of batches we're going to turn on the full thing so we can see what happens right. So I'm going to set max epochs equals 20. And since we want to do the validation validation, I actually need a validation with as well. Right. So, here, there, you know, the simplest way to do this is just copy paste this code because it's largely the same. And then I can just reward this color validation stuff. And then replace these words with bow and bow. And there are also keywords right for the pie torch lining this validation underscore step. Exactly. Okay. And what you notice here is that. So I want to make a point so in training you want to log something every batch, right. In validation, it doesn't really make sense to lock something every batch because they're independent. So you want to, you want to calculate the accuracy or the loss across the whole leapfrog. So you don't have to deal with that as long as you just use log lightning nose to do it the correct way. So it'll aggregate across the book as well. Okay. Now this stuff is we don't really need this. It doesn't hurt to have it. But, you know, we could we could simplify things by just getting rid of all this. Since there are no gradients already there, they're disabled or invalidation automatically. You know, so so we can just simplify this. Okay. And then, yeah, so we can leave it there. Now you'll also notice that this code is largely the same. So yes, you could write an intermediate function and then just use the same one. But, you know, just for simplicity, we're going to keep it as it is. Okay. Okay. So let's not train for 20 bucks. And you'll notice that it will run a few validation batches first and make sure you have no bugs. And now we start training. Okay, so we're done training. So let's see how 20 bucks. Let's reload the sky. All right, there's our 20 buck model. Let's see what our validation accuracy looks like. Not bad. So you can see, let's look at the epoch where that happened. Right. So you can see that change. So around step 14,000, we had this big increase in accuracy, validation accuracy. I want to guess that's where the model was frozen. Right. So let's look at that. That's step 14,000. And 14,000 around here. So yeah, it's about step epoch 10. So the ninth, the 10th epoch, right. So it's index nine. So yeah, so that makes sense. So you can see that unfreezing the backbone at some point will then enable you to kind of reach the next plateau of performance. So this was for the supervised pre training part, right? Yeah. And then let's also look at the last. So this is by epoch. So you can see as well that that happened there. You know, one thing you could try is also just unfreeze it from the get go. You might be able to do better as well. But let's do the exact same thing with the self supervised version. Okay, so we need to add the the validation method to this one as well. Right. Yeah. So let's just do that. Let's copy again, like we did, we're just going to copy the training stuff. We already had the training, the validation, right? Yeah. And then I'm going to rename this validation. Okay. You can copy from the previous. Yeah. Well, it's this slightly different because we have these two feature maps. So, oh, okay. You're right. I forgot. Yeah. So we have the values and then about, and again, this no grad thing doesn't matter, but I'll remove it just to make it cleaner. Great. Okay. So now we should be able to train on, and I want to make sure we use the exact same training regime. So I'm just going to copy paste this guy. Okay. So we trained on 20 bucks, and now we will use the validation sets. So this is SWOA, self supervised. Okay. So we are done here with the training. All right, let's see how it did. So this is experiment number one. So let's look it up on this tensor board. Did not upload automatically. So let's refresh. Okay, version one. Great. Let's look at the validation accuracy. Oh, wow. Much better. So in blue, we have the self supervised model. And in orange, we have the supervised model. So they might converge at some point, but what's interesting is that it reaches a higher accuracy much faster. Yeah, it's impressive. So it's great. I think this is something promising about self supervised learning is that it should help you speed up your convergence. So it'll save you money basically. So let's train it without any of this just to show what happens when you don't have any of the pre-trained models. So I can do that by just turning this guy off. Right. So I'm not going to load weights. So by default is false. Correct. Okay, can we check? Yeah, we can just set it to false. Okay, that's better. Thank you. So let's also go ahead and remove this unfreezing part and just train it with unfrozen from the very beginning. Yeah. So I will just delete this. All right. Okay, let's see how it goes. So here we actually, let's remember that we are training the full backbone and therefore we are doing back backward, like forward and backward through the backbone. So it's taking way too long. So we are going to be stopping right now at epoch 10. So it should be just fine. And let's compare now the validation curves. So refresh the TensorBoard. All right. Do you have a guess for the best accuracy for that model? I hope. No, I don't know. I mean, I think I do, but I want to see. I don't want to. Then getting like. Okay, let's see. Oh wait, sorry. There you go. It's gone up. I mean, you know, it's just standard, right? So it's very slow to converge. It probably will get there at some point, but you know, the amount of compute that was required to get this one is a lot cheaper because you already had this strong prior. So yeah, it's good. I mean, it's not bad. So actually with the blue curve, if we actually do like one step only, we just get immediately to 80%. Right. So if we, I think even if we don't do the training of the classifier at the beginning, and we just do one step directly, everything we get immediately at a very good initial point. Right. Yeah. So if we leave the final layer, if we just leave everything in person and just kind of start from there. And so this was trained with roughly 50,000 training samples. Let's see what happens if we are really pushing hard and using really few label samples. So I want to show you this really cool function in the torch library called random split. So if you have a list of, you know, if you have a data set, for example, here I'm pretending to have a data set, it's just 10 numbers. I want to actually split that into two sets, and I want, I want them to shuffle first and then make that split. So, in this case I want the first set to have three elements in it and then the second set to have seven. And what's cool is that I can make this deterministic by adding the seed argument to it. So let me run it once just so you can see. And so you see the first set we got 261. And then when I run it again, still the same thing. Right. So why is this cool because that's what we're using under the hood on the data modules. Right. So, here we have the default seed for the data module, so that when you're doing the Stata Loader, we're going to take the CIFAR 10 training split, and then we're going to split that split into two, right. One is the train and one is the validation, which we're not using there. So we're going to use that because I, you know, this, this argument here about split is saying how much, how many validation elements do I want. Right. So, in that case, when I run this, I'm going to have here. I'm going to have 50,000 minus, you know, 100, 1000, whatever we want. 50,000 is the number of elements in the training split of CIFAR 10. So I'm saying, okay, if I want to have 100 training elements, then I need to set that to num train and then, then I'll use 50,000 minus 100 validation samples. So, okay, so we have that. So again, we had, we already have the models to find, right. So we have this, we have three models that we're going to test right now. The pre-trained model that was pre-trained using self supervised learning. Then the pre-trained model that was pre-trained using supervised learning, and then a model that is not pre-trained. So let's go there. And we don't freeze, right. In this case, the backbone. Yeah, so we're going to unfreeze the backbone. And we're just going to train. So, you know, just to kind of summarize this as a training step. Features and then fine tune. So no freezing. So let's look at three different numbers of examples, right. So let's pick 100 trained samples, 316 trained samples, and 1000 trained samples. And then we're going to validate only on 100 batches of validation so that, you know, we can speed up. So we're going to, we're going to loop over this number of trained samples here. And then I'm just, I'm just plotting some stuff here that we want to know. So I want to, I want to set the max epoch. So I want to make sure we have the same number of steps everywhere. So we're going to set, we're going to limit everything to have 5000 steps. And then we're going to derive the max epoch from that, right. So which is this guy. And then we want to check validation not on every epoch because it's going to be too slow. We're going to do it every five. Well, we want to check it five times within the number of training epochs. So this is going to tell us how often to check the validation. And then we initialize the data, right. So again, just a data module. And we're going to pull out the training data loader just so that we can print the length so we know how many training samples we're running. And then the model, right. So we're going to, in this case, use the model that was pre-training, self-supervised learning. And then because I want to modify what it's going to show. So you asked me earlier, how do I change version and the tensor board logs or whatever I want. I'm going to actually initialize the tensor board logger and then set the name myself here. Right. So it's going to be SSL dash, you know, the number of training that we're using. So it'll be SSL dash 100, 316, and then 1000. And then everything else stays the same. The only other thing I'm going to add on here is that I only want to check 100 validation batches. Right. So limit val batches equals 100. And then again, I don't want to check the validation on every training epoch. So I want to do it every, you know, any epochs. In this case, it's going to be derived automatically. So just to make sure that we are consistent across splits. Okay. So we're going to train this. Ready? All right. Okay. So we just trained the self-supervised model. Let's go ahead and train. We're going to do exactly the same stuff that we just did. So I just copy paste the code from above, but now we're going to use our supervised model. So the ResNet 50 with pre-trained weights. Correct. On the image net. And then we're going to train this guy as well. And, you know, I'm naming that one supervised dash the number of training samples. So let's go ahead and train that. Okay, great. So we just trained the supervised ResNet. And now we're going to train just the random model. Right. So here there's nothing pre-trained. It's everything training from scratch. So it's this guy here. And then we're going to call that supervised not pre-trained. So let's go ahead and train those. All right. Okay. So they're all done training. Are you, why don't we just look at the logs? What do you think will happen? Let's see. I don't want to spoil it. Okay. All right. Cool. So I'd like to see the accuracy for the validation set, if you can show me. All right. Yes. Let's pick out the ones that we want to show. So these are all the supervised not pre-trained. Right. And then the supervised and then we're these are the self-supervised ones. This was just a trial as well. Okay. So these are the ones we want to compare. So at the bottom, these guys here, these are the random ones. So nothing's pre-trained. So you can see that on average we get like, you know, okay, we get in the teens of accuracy here. Yeah. And then in the middle guys, these are the ones that are supervised pre-trained. So the rest nuts and we get in the, you know, 20s and 30s, which is great. So you can see that the transfer learning works, right? So automatically twice as much performance, right? Exactly. And what's interesting is that when they're trained without labels, these guys, we get double the performance of the models that were trained with the labels. Wow. Impressive. So, yeah, I don't know. I guess we can't generalize too much from, you know, image net to C410, but to me, this is, this is super promising as well. All right. So let's, let's give again the punchline. What is the point of today lesson? What did we learn? Where did we start? So we started with introducing to you the concept of transfer learning. We covered the supervised version and the unsupervised version. Here, at least we concluded in this tiny experiment that the performance using the unsupervised version are much better than we don't know whether this is, you know, a actual result that generalizes. But the point is that the unsupervised learning version allows us to train a backbone on our own data, right? Rather than, instead the supervised learning, you need to actually have annotated data, which is expensive, right? So this is actually a very big, a very big point, right? Such that if you need to do something in a practical aspect that you have a company or whatever, and then you have plenty of data, you don't really necessarily have the labels because we said plenty of times those are expensive. You can still pre train your backbone with those unsupervised algorithms that are already available to you. If you use the bolts, right? So everything is already coded and, and checked with the authors, I think. And they compared like in terms of performance, not only in the accuracy, but also in the computations. I think, like, I think this lightning has so many, you know, checks for being able to maintain a very, you know, high speed, no, it doesn't slow you down to, doesn't slow you down. And so again, we have all those unsupervised, self supervised learning algorithms that are, you know, coming out, you know, just recently, like this year, they were from July or June, whatever, 2020. And they are already available here in this library. We can use them for training, for training your model on your own data. And then we simply swap in a classifier. We just put a classifier on top of the network and then fine tune this classifier or even the other weights using those labels we have. And so this was basically the summary of today lesson. Anything that I missed? No, I think that was perfect. All right. Thank you so much, William, for being with us today. It was really great. I really learned so many things from you. Thank you for having me. This is great. I hope this is useful for everyone. I think it is. All right. Have a good evening or day or afternoon or morning, whatever the day you're watching this video. All right. Bye bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 2.0, "text": " So welcome to class today.", "tokens": [50364, 407, 2928, 281, 1508, 965, 13, 50464, 50464, 2692, 321, 434, 516, 281, 312, 1417, 466, 5003, 2539, 50614, 50614, 365, 257, 46533, 293, 2693, 12879, 24420, 4585, 13, 50814, 50814, 1692, 965, 321, 362, 6740, 31801, 11, 50914, 50914, 364, 5844, 294, 264, 3873, 321, 366, 516, 281, 312, 1228, 51064, 51064, 337, 13468, 281, 291, 577, 341, 1507, 1985, 11, 51214, 51214, 597, 307, 516, 281, 312, 3585, 505, 257, 707, 857, 544, 466, 341, 4829, 13, 51414, 51414, 407, 6740, 11, 264, 4123, 307, 6342, 13, 51614, 51614, 28327, 78, 11, 1309, 291, 370, 709, 337, 1419, 385, 510, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16220787975275627, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.002874302677810192}, {"id": 1, "seek": 0, "start": 2.0, "end": 5.0, "text": " Today we're going to be talking about transfer learning", "tokens": [50364, 407, 2928, 281, 1508, 965, 13, 50464, 50464, 2692, 321, 434, 516, 281, 312, 1417, 466, 5003, 2539, 50614, 50614, 365, 257, 46533, 293, 2693, 12879, 24420, 4585, 13, 50814, 50814, 1692, 965, 321, 362, 6740, 31801, 11, 50914, 50914, 364, 5844, 294, 264, 3873, 321, 366, 516, 281, 312, 1228, 51064, 51064, 337, 13468, 281, 291, 577, 341, 1507, 1985, 11, 51214, 51214, 597, 307, 516, 281, 312, 3585, 505, 257, 707, 857, 544, 466, 341, 4829, 13, 51414, 51414, 407, 6740, 11, 264, 4123, 307, 6342, 13, 51614, 51614, 28327, 78, 11, 1309, 291, 370, 709, 337, 1419, 385, 510, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16220787975275627, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.002874302677810192}, {"id": 2, "seek": 0, "start": 5.0, "end": 9.0, "text": " with a supervised and unsupervised perspective.", "tokens": [50364, 407, 2928, 281, 1508, 965, 13, 50464, 50464, 2692, 321, 434, 516, 281, 312, 1417, 466, 5003, 2539, 50614, 50614, 365, 257, 46533, 293, 2693, 12879, 24420, 4585, 13, 50814, 50814, 1692, 965, 321, 362, 6740, 31801, 11, 50914, 50914, 364, 5844, 294, 264, 3873, 321, 366, 516, 281, 312, 1228, 51064, 51064, 337, 13468, 281, 291, 577, 341, 1507, 1985, 11, 51214, 51214, 597, 307, 516, 281, 312, 3585, 505, 257, 707, 857, 544, 466, 341, 4829, 13, 51414, 51414, 407, 6740, 11, 264, 4123, 307, 6342, 13, 51614, 51614, 28327, 78, 11, 1309, 291, 370, 709, 337, 1419, 385, 510, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16220787975275627, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.002874302677810192}, {"id": 3, "seek": 0, "start": 9.0, "end": 11.0, "text": " Here today we have William Falcon,", "tokens": [50364, 407, 2928, 281, 1508, 965, 13, 50464, 50464, 2692, 321, 434, 516, 281, 312, 1417, 466, 5003, 2539, 50614, 50614, 365, 257, 46533, 293, 2693, 12879, 24420, 4585, 13, 50814, 50814, 1692, 965, 321, 362, 6740, 31801, 11, 50914, 50914, 364, 5844, 294, 264, 3873, 321, 366, 516, 281, 312, 1228, 51064, 51064, 337, 13468, 281, 291, 577, 341, 1507, 1985, 11, 51214, 51214, 597, 307, 516, 281, 312, 3585, 505, 257, 707, 857, 544, 466, 341, 4829, 13, 51414, 51414, 407, 6740, 11, 264, 4123, 307, 6342, 13, 51614, 51614, 28327, 78, 11, 1309, 291, 370, 709, 337, 1419, 385, 510, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16220787975275627, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.002874302677810192}, {"id": 4, "seek": 0, "start": 11.0, "end": 14.0, "text": " an expert in the tools we are going to be using", "tokens": [50364, 407, 2928, 281, 1508, 965, 13, 50464, 50464, 2692, 321, 434, 516, 281, 312, 1417, 466, 5003, 2539, 50614, 50614, 365, 257, 46533, 293, 2693, 12879, 24420, 4585, 13, 50814, 50814, 1692, 965, 321, 362, 6740, 31801, 11, 50914, 50914, 364, 5844, 294, 264, 3873, 321, 366, 516, 281, 312, 1228, 51064, 51064, 337, 13468, 281, 291, 577, 341, 1507, 1985, 11, 51214, 51214, 597, 307, 516, 281, 312, 3585, 505, 257, 707, 857, 544, 466, 341, 4829, 13, 51414, 51414, 407, 6740, 11, 264, 4123, 307, 6342, 13, 51614, 51614, 28327, 78, 11, 1309, 291, 370, 709, 337, 1419, 385, 510, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16220787975275627, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.002874302677810192}, {"id": 5, "seek": 0, "start": 14.0, "end": 17.0, "text": " for explaining to you how this stuff works,", "tokens": [50364, 407, 2928, 281, 1508, 965, 13, 50464, 50464, 2692, 321, 434, 516, 281, 312, 1417, 466, 5003, 2539, 50614, 50614, 365, 257, 46533, 293, 2693, 12879, 24420, 4585, 13, 50814, 50814, 1692, 965, 321, 362, 6740, 31801, 11, 50914, 50914, 364, 5844, 294, 264, 3873, 321, 366, 516, 281, 312, 1228, 51064, 51064, 337, 13468, 281, 291, 577, 341, 1507, 1985, 11, 51214, 51214, 597, 307, 516, 281, 312, 3585, 505, 257, 707, 857, 544, 466, 341, 4829, 13, 51414, 51414, 407, 6740, 11, 264, 4123, 307, 6342, 13, 51614, 51614, 28327, 78, 11, 1309, 291, 370, 709, 337, 1419, 385, 510, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16220787975275627, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.002874302677810192}, {"id": 6, "seek": 0, "start": 17.0, "end": 21.0, "text": " which is going to be telling us a little bit more about this topic.", "tokens": [50364, 407, 2928, 281, 1508, 965, 13, 50464, 50464, 2692, 321, 434, 516, 281, 312, 1417, 466, 5003, 2539, 50614, 50614, 365, 257, 46533, 293, 2693, 12879, 24420, 4585, 13, 50814, 50814, 1692, 965, 321, 362, 6740, 31801, 11, 50914, 50914, 364, 5844, 294, 264, 3873, 321, 366, 516, 281, 312, 1228, 51064, 51064, 337, 13468, 281, 291, 577, 341, 1507, 1985, 11, 51214, 51214, 597, 307, 516, 281, 312, 3585, 505, 257, 707, 857, 544, 466, 341, 4829, 13, 51414, 51414, 407, 6740, 11, 264, 4123, 307, 6342, 13, 51614, 51614, 28327, 78, 11, 1309, 291, 370, 709, 337, 1419, 385, 510, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16220787975275627, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.002874302677810192}, {"id": 7, "seek": 0, "start": 21.0, "end": 25.0, "text": " So William, the floor is yours.", "tokens": [50364, 407, 2928, 281, 1508, 965, 13, 50464, 50464, 2692, 321, 434, 516, 281, 312, 1417, 466, 5003, 2539, 50614, 50614, 365, 257, 46533, 293, 2693, 12879, 24420, 4585, 13, 50814, 50814, 1692, 965, 321, 362, 6740, 31801, 11, 50914, 50914, 364, 5844, 294, 264, 3873, 321, 366, 516, 281, 312, 1228, 51064, 51064, 337, 13468, 281, 291, 577, 341, 1507, 1985, 11, 51214, 51214, 597, 307, 516, 281, 312, 3585, 505, 257, 707, 857, 544, 466, 341, 4829, 13, 51414, 51414, 407, 6740, 11, 264, 4123, 307, 6342, 13, 51614, 51614, 28327, 78, 11, 1309, 291, 370, 709, 337, 1419, 385, 510, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16220787975275627, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.002874302677810192}, {"id": 8, "seek": 0, "start": 25.0, "end": 28.0, "text": " Alfredo, thank you so much for having me here,", "tokens": [50364, 407, 2928, 281, 1508, 965, 13, 50464, 50464, 2692, 321, 434, 516, 281, 312, 1417, 466, 5003, 2539, 50614, 50614, 365, 257, 46533, 293, 2693, 12879, 24420, 4585, 13, 50814, 50814, 1692, 965, 321, 362, 6740, 31801, 11, 50914, 50914, 364, 5844, 294, 264, 3873, 321, 366, 516, 281, 312, 1228, 51064, 51064, 337, 13468, 281, 291, 577, 341, 1507, 1985, 11, 51214, 51214, 597, 307, 516, 281, 312, 3585, 505, 257, 707, 857, 544, 466, 341, 4829, 13, 51414, 51414, 407, 6740, 11, 264, 4123, 307, 6342, 13, 51614, 51614, 28327, 78, 11, 1309, 291, 370, 709, 337, 1419, 385, 510, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16220787975275627, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.002874302677810192}, {"id": 9, "seek": 2800, "start": 28.0, "end": 31.0, "text": " and excited to share this with the whole class and everyone.", "tokens": [50364, 293, 2919, 281, 2073, 341, 365, 264, 1379, 1508, 293, 1518, 13, 50514, 50514, 407, 11, 1392, 11, 965, 321, 434, 516, 281, 312, 884, 50664, 50664, 2698, 12, 48172, 24420, 293, 46533, 5003, 2539, 13, 50764, 50764, 407, 341, 307, 516, 281, 808, 493, 257, 688, 337, 561, 13, 50864, 50864, 407, 498, 291, 589, 294, 364, 3518, 420, 884, 604, 733, 295, 2132, 11, 51064, 51064, 291, 434, 516, 281, 1190, 666, 746, 689, 291, 815, 406, 362, 1547, 1412, 11, 51214, 51214, 293, 291, 643, 281, 362, 512, 2316, 300, 311, 668, 8895, 322, 746, 1646, 11, 51414, 51414, 293, 550, 291, 393, 764, 300, 281, 733, 295, 3012, 24419, 428, 1399, 11, 558, 30, 51564, 51564, 407, 321, 603, 2060, 309, 294, 264, 4319, 295, 3820, 5201, 965, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0659627844817447, "compression_ratio": 1.7475083056478404, "no_speech_prob": 9.15244672796689e-05}, {"id": 10, "seek": 2800, "start": 31.0, "end": 34.0, "text": " So, okay, today we're going to be doing", "tokens": [50364, 293, 2919, 281, 2073, 341, 365, 264, 1379, 1508, 293, 1518, 13, 50514, 50514, 407, 11, 1392, 11, 965, 321, 434, 516, 281, 312, 884, 50664, 50664, 2698, 12, 48172, 24420, 293, 46533, 5003, 2539, 13, 50764, 50764, 407, 341, 307, 516, 281, 808, 493, 257, 688, 337, 561, 13, 50864, 50864, 407, 498, 291, 589, 294, 364, 3518, 420, 884, 604, 733, 295, 2132, 11, 51064, 51064, 291, 434, 516, 281, 1190, 666, 746, 689, 291, 815, 406, 362, 1547, 1412, 11, 51214, 51214, 293, 291, 643, 281, 362, 512, 2316, 300, 311, 668, 8895, 322, 746, 1646, 11, 51414, 51414, 293, 550, 291, 393, 764, 300, 281, 733, 295, 3012, 24419, 428, 1399, 11, 558, 30, 51564, 51564, 407, 321, 603, 2060, 309, 294, 264, 4319, 295, 3820, 5201, 965, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0659627844817447, "compression_ratio": 1.7475083056478404, "no_speech_prob": 9.15244672796689e-05}, {"id": 11, "seek": 2800, "start": 34.0, "end": 36.0, "text": " self-supervised and supervised transfer learning.", "tokens": [50364, 293, 2919, 281, 2073, 341, 365, 264, 1379, 1508, 293, 1518, 13, 50514, 50514, 407, 11, 1392, 11, 965, 321, 434, 516, 281, 312, 884, 50664, 50664, 2698, 12, 48172, 24420, 293, 46533, 5003, 2539, 13, 50764, 50764, 407, 341, 307, 516, 281, 808, 493, 257, 688, 337, 561, 13, 50864, 50864, 407, 498, 291, 589, 294, 364, 3518, 420, 884, 604, 733, 295, 2132, 11, 51064, 51064, 291, 434, 516, 281, 1190, 666, 746, 689, 291, 815, 406, 362, 1547, 1412, 11, 51214, 51214, 293, 291, 643, 281, 362, 512, 2316, 300, 311, 668, 8895, 322, 746, 1646, 11, 51414, 51414, 293, 550, 291, 393, 764, 300, 281, 733, 295, 3012, 24419, 428, 1399, 11, 558, 30, 51564, 51564, 407, 321, 603, 2060, 309, 294, 264, 4319, 295, 3820, 5201, 965, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0659627844817447, "compression_ratio": 1.7475083056478404, "no_speech_prob": 9.15244672796689e-05}, {"id": 12, "seek": 2800, "start": 36.0, "end": 38.0, "text": " So this is going to come up a lot for people.", "tokens": [50364, 293, 2919, 281, 2073, 341, 365, 264, 1379, 1508, 293, 1518, 13, 50514, 50514, 407, 11, 1392, 11, 965, 321, 434, 516, 281, 312, 884, 50664, 50664, 2698, 12, 48172, 24420, 293, 46533, 5003, 2539, 13, 50764, 50764, 407, 341, 307, 516, 281, 808, 493, 257, 688, 337, 561, 13, 50864, 50864, 407, 498, 291, 589, 294, 364, 3518, 420, 884, 604, 733, 295, 2132, 11, 51064, 51064, 291, 434, 516, 281, 1190, 666, 746, 689, 291, 815, 406, 362, 1547, 1412, 11, 51214, 51214, 293, 291, 643, 281, 362, 512, 2316, 300, 311, 668, 8895, 322, 746, 1646, 11, 51414, 51414, 293, 550, 291, 393, 764, 300, 281, 733, 295, 3012, 24419, 428, 1399, 11, 558, 30, 51564, 51564, 407, 321, 603, 2060, 309, 294, 264, 4319, 295, 3820, 5201, 965, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0659627844817447, "compression_ratio": 1.7475083056478404, "no_speech_prob": 9.15244672796689e-05}, {"id": 13, "seek": 2800, "start": 38.0, "end": 42.0, "text": " So if you work in an industry or doing any kind of research,", "tokens": [50364, 293, 2919, 281, 2073, 341, 365, 264, 1379, 1508, 293, 1518, 13, 50514, 50514, 407, 11, 1392, 11, 965, 321, 434, 516, 281, 312, 884, 50664, 50664, 2698, 12, 48172, 24420, 293, 46533, 5003, 2539, 13, 50764, 50764, 407, 341, 307, 516, 281, 808, 493, 257, 688, 337, 561, 13, 50864, 50864, 407, 498, 291, 589, 294, 364, 3518, 420, 884, 604, 733, 295, 2132, 11, 51064, 51064, 291, 434, 516, 281, 1190, 666, 746, 689, 291, 815, 406, 362, 1547, 1412, 11, 51214, 51214, 293, 291, 643, 281, 362, 512, 2316, 300, 311, 668, 8895, 322, 746, 1646, 11, 51414, 51414, 293, 550, 291, 393, 764, 300, 281, 733, 295, 3012, 24419, 428, 1399, 11, 558, 30, 51564, 51564, 407, 321, 603, 2060, 309, 294, 264, 4319, 295, 3820, 5201, 965, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0659627844817447, "compression_ratio": 1.7475083056478404, "no_speech_prob": 9.15244672796689e-05}, {"id": 14, "seek": 2800, "start": 42.0, "end": 45.0, "text": " you're going to run into something where you may not have enough data,", "tokens": [50364, 293, 2919, 281, 2073, 341, 365, 264, 1379, 1508, 293, 1518, 13, 50514, 50514, 407, 11, 1392, 11, 965, 321, 434, 516, 281, 312, 884, 50664, 50664, 2698, 12, 48172, 24420, 293, 46533, 5003, 2539, 13, 50764, 50764, 407, 341, 307, 516, 281, 808, 493, 257, 688, 337, 561, 13, 50864, 50864, 407, 498, 291, 589, 294, 364, 3518, 420, 884, 604, 733, 295, 2132, 11, 51064, 51064, 291, 434, 516, 281, 1190, 666, 746, 689, 291, 815, 406, 362, 1547, 1412, 11, 51214, 51214, 293, 291, 643, 281, 362, 512, 2316, 300, 311, 668, 8895, 322, 746, 1646, 11, 51414, 51414, 293, 550, 291, 393, 764, 300, 281, 733, 295, 3012, 24419, 428, 1399, 11, 558, 30, 51564, 51564, 407, 321, 603, 2060, 309, 294, 264, 4319, 295, 3820, 5201, 965, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0659627844817447, "compression_ratio": 1.7475083056478404, "no_speech_prob": 9.15244672796689e-05}, {"id": 15, "seek": 2800, "start": 45.0, "end": 49.0, "text": " and you need to have some model that's been trained on something else,", "tokens": [50364, 293, 2919, 281, 2073, 341, 365, 264, 1379, 1508, 293, 1518, 13, 50514, 50514, 407, 11, 1392, 11, 965, 321, 434, 516, 281, 312, 884, 50664, 50664, 2698, 12, 48172, 24420, 293, 46533, 5003, 2539, 13, 50764, 50764, 407, 341, 307, 516, 281, 808, 493, 257, 688, 337, 561, 13, 50864, 50864, 407, 498, 291, 589, 294, 364, 3518, 420, 884, 604, 733, 295, 2132, 11, 51064, 51064, 291, 434, 516, 281, 1190, 666, 746, 689, 291, 815, 406, 362, 1547, 1412, 11, 51214, 51214, 293, 291, 643, 281, 362, 512, 2316, 300, 311, 668, 8895, 322, 746, 1646, 11, 51414, 51414, 293, 550, 291, 393, 764, 300, 281, 733, 295, 3012, 24419, 428, 1399, 11, 558, 30, 51564, 51564, 407, 321, 603, 2060, 309, 294, 264, 4319, 295, 3820, 5201, 965, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0659627844817447, "compression_ratio": 1.7475083056478404, "no_speech_prob": 9.15244672796689e-05}, {"id": 16, "seek": 2800, "start": 49.0, "end": 52.0, "text": " and then you can use that to kind of jumpstart your process, right?", "tokens": [50364, 293, 2919, 281, 2073, 341, 365, 264, 1379, 1508, 293, 1518, 13, 50514, 50514, 407, 11, 1392, 11, 965, 321, 434, 516, 281, 312, 884, 50664, 50664, 2698, 12, 48172, 24420, 293, 46533, 5003, 2539, 13, 50764, 50764, 407, 341, 307, 516, 281, 808, 493, 257, 688, 337, 561, 13, 50864, 50864, 407, 498, 291, 589, 294, 364, 3518, 420, 884, 604, 733, 295, 2132, 11, 51064, 51064, 291, 434, 516, 281, 1190, 666, 746, 689, 291, 815, 406, 362, 1547, 1412, 11, 51214, 51214, 293, 291, 643, 281, 362, 512, 2316, 300, 311, 668, 8895, 322, 746, 1646, 11, 51414, 51414, 293, 550, 291, 393, 764, 300, 281, 733, 295, 3012, 24419, 428, 1399, 11, 558, 30, 51564, 51564, 407, 321, 603, 2060, 309, 294, 264, 4319, 295, 3820, 5201, 965, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0659627844817447, "compression_ratio": 1.7475083056478404, "no_speech_prob": 9.15244672796689e-05}, {"id": 17, "seek": 2800, "start": 52.0, "end": 56.0, "text": " So we'll cover it in the context of computer vision today,", "tokens": [50364, 293, 2919, 281, 2073, 341, 365, 264, 1379, 1508, 293, 1518, 13, 50514, 50514, 407, 11, 1392, 11, 965, 321, 434, 516, 281, 312, 884, 50664, 50664, 2698, 12, 48172, 24420, 293, 46533, 5003, 2539, 13, 50764, 50764, 407, 341, 307, 516, 281, 808, 493, 257, 688, 337, 561, 13, 50864, 50864, 407, 498, 291, 589, 294, 364, 3518, 420, 884, 604, 733, 295, 2132, 11, 51064, 51064, 291, 434, 516, 281, 1190, 666, 746, 689, 291, 815, 406, 362, 1547, 1412, 11, 51214, 51214, 293, 291, 643, 281, 362, 512, 2316, 300, 311, 668, 8895, 322, 746, 1646, 11, 51414, 51414, 293, 550, 291, 393, 764, 300, 281, 733, 295, 3012, 24419, 428, 1399, 11, 558, 30, 51564, 51564, 407, 321, 603, 2060, 309, 294, 264, 4319, 295, 3820, 5201, 965, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.0659627844817447, "compression_ratio": 1.7475083056478404, "no_speech_prob": 9.15244672796689e-05}, {"id": 18, "seek": 5600, "start": 56.0, "end": 61.0, "text": " but this can transfer to NLP, speech, anything you want.", "tokens": [50364, 457, 341, 393, 5003, 281, 426, 45196, 11, 6218, 11, 1340, 291, 528, 13, 50614, 50614, 26127, 2539, 13, 50714, 50714, 865, 13, 50814, 50814, 1033, 13, 50914, 50914, 22574, 10226, 13, 51014, 51014, 400, 512, 295, 264, 2135, 12, 1353, 12, 49417, 412, 589, 382, 731, 11, 51164, 51164, 457, 286, 478, 6679, 300, 294, 264, 958, 1326, 924, 11, 51314, 51314, 286, 478, 988, 321, 603, 2573, 484, 257, 636, 281, 360, 300, 13, 51414, 51414, 407, 264, 700, 551, 286, 528, 281, 360, 307, 286, 478, 516, 281, 3625, 28848, 11, 558, 30, 51564, 51564, 8239, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18208859517024115, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.00022997723135631531}, {"id": 19, "seek": 5600, "start": 61.0, "end": 63.0, "text": " Cancer learning.", "tokens": [50364, 457, 341, 393, 5003, 281, 426, 45196, 11, 6218, 11, 1340, 291, 528, 13, 50614, 50614, 26127, 2539, 13, 50714, 50714, 865, 13, 50814, 50814, 1033, 13, 50914, 50914, 22574, 10226, 13, 51014, 51014, 400, 512, 295, 264, 2135, 12, 1353, 12, 49417, 412, 589, 382, 731, 11, 51164, 51164, 457, 286, 478, 6679, 300, 294, 264, 958, 1326, 924, 11, 51314, 51314, 286, 478, 988, 321, 603, 2573, 484, 257, 636, 281, 360, 300, 13, 51414, 51414, 407, 264, 700, 551, 286, 528, 281, 360, 307, 286, 478, 516, 281, 3625, 28848, 11, 558, 30, 51564, 51564, 8239, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18208859517024115, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.00022997723135631531}, {"id": 20, "seek": 5600, "start": 63.0, "end": 65.0, "text": " Yeah.", "tokens": [50364, 457, 341, 393, 5003, 281, 426, 45196, 11, 6218, 11, 1340, 291, 528, 13, 50614, 50614, 26127, 2539, 13, 50714, 50714, 865, 13, 50814, 50814, 1033, 13, 50914, 50914, 22574, 10226, 13, 51014, 51014, 400, 512, 295, 264, 2135, 12, 1353, 12, 49417, 412, 589, 382, 731, 11, 51164, 51164, 457, 286, 478, 6679, 300, 294, 264, 958, 1326, 924, 11, 51314, 51314, 286, 478, 988, 321, 603, 2573, 484, 257, 636, 281, 360, 300, 13, 51414, 51414, 407, 264, 700, 551, 286, 528, 281, 360, 307, 286, 478, 516, 281, 3625, 28848, 11, 558, 30, 51564, 51564, 8239, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18208859517024115, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.00022997723135631531}, {"id": 21, "seek": 5600, "start": 65.0, "end": 67.0, "text": " Okay.", "tokens": [50364, 457, 341, 393, 5003, 281, 426, 45196, 11, 6218, 11, 1340, 291, 528, 13, 50614, 50614, 26127, 2539, 13, 50714, 50714, 865, 13, 50814, 50814, 1033, 13, 50914, 50914, 22574, 10226, 13, 51014, 51014, 400, 512, 295, 264, 2135, 12, 1353, 12, 49417, 412, 589, 382, 731, 11, 51164, 51164, 457, 286, 478, 6679, 300, 294, 264, 958, 1326, 924, 11, 51314, 51314, 286, 478, 988, 321, 603, 2573, 484, 257, 636, 281, 360, 300, 13, 51414, 51414, 407, 264, 700, 551, 286, 528, 281, 360, 307, 286, 478, 516, 281, 3625, 28848, 11, 558, 30, 51564, 51564, 8239, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18208859517024115, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.00022997723135631531}, {"id": 22, "seek": 5600, "start": 67.0, "end": 69.0, "text": " Pun intended.", "tokens": [50364, 457, 341, 393, 5003, 281, 426, 45196, 11, 6218, 11, 1340, 291, 528, 13, 50614, 50614, 26127, 2539, 13, 50714, 50714, 865, 13, 50814, 50814, 1033, 13, 50914, 50914, 22574, 10226, 13, 51014, 51014, 400, 512, 295, 264, 2135, 12, 1353, 12, 49417, 412, 589, 382, 731, 11, 51164, 51164, 457, 286, 478, 6679, 300, 294, 264, 958, 1326, 924, 11, 51314, 51314, 286, 478, 988, 321, 603, 2573, 484, 257, 636, 281, 360, 300, 13, 51414, 51414, 407, 264, 700, 551, 286, 528, 281, 360, 307, 286, 478, 516, 281, 3625, 28848, 11, 558, 30, 51564, 51564, 8239, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18208859517024115, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.00022997723135631531}, {"id": 23, "seek": 5600, "start": 69.0, "end": 72.0, "text": " And some of the main-to-main at work as well,", "tokens": [50364, 457, 341, 393, 5003, 281, 426, 45196, 11, 6218, 11, 1340, 291, 528, 13, 50614, 50614, 26127, 2539, 13, 50714, 50714, 865, 13, 50814, 50814, 1033, 13, 50914, 50914, 22574, 10226, 13, 51014, 51014, 400, 512, 295, 264, 2135, 12, 1353, 12, 49417, 412, 589, 382, 731, 11, 51164, 51164, 457, 286, 478, 6679, 300, 294, 264, 958, 1326, 924, 11, 51314, 51314, 286, 478, 988, 321, 603, 2573, 484, 257, 636, 281, 360, 300, 13, 51414, 51414, 407, 264, 700, 551, 286, 528, 281, 360, 307, 286, 478, 516, 281, 3625, 28848, 11, 558, 30, 51564, 51564, 8239, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18208859517024115, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.00022997723135631531}, {"id": 24, "seek": 5600, "start": 72.0, "end": 75.0, "text": " but I'm confident that in the next few years,", "tokens": [50364, 457, 341, 393, 5003, 281, 426, 45196, 11, 6218, 11, 1340, 291, 528, 13, 50614, 50614, 26127, 2539, 13, 50714, 50714, 865, 13, 50814, 50814, 1033, 13, 50914, 50914, 22574, 10226, 13, 51014, 51014, 400, 512, 295, 264, 2135, 12, 1353, 12, 49417, 412, 589, 382, 731, 11, 51164, 51164, 457, 286, 478, 6679, 300, 294, 264, 958, 1326, 924, 11, 51314, 51314, 286, 478, 988, 321, 603, 2573, 484, 257, 636, 281, 360, 300, 13, 51414, 51414, 407, 264, 700, 551, 286, 528, 281, 360, 307, 286, 478, 516, 281, 3625, 28848, 11, 558, 30, 51564, 51564, 8239, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18208859517024115, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.00022997723135631531}, {"id": 25, "seek": 5600, "start": 75.0, "end": 77.0, "text": " I'm sure we'll figure out a way to do that.", "tokens": [50364, 457, 341, 393, 5003, 281, 426, 45196, 11, 6218, 11, 1340, 291, 528, 13, 50614, 50614, 26127, 2539, 13, 50714, 50714, 865, 13, 50814, 50814, 1033, 13, 50914, 50914, 22574, 10226, 13, 51014, 51014, 400, 512, 295, 264, 2135, 12, 1353, 12, 49417, 412, 589, 382, 731, 11, 51164, 51164, 457, 286, 478, 6679, 300, 294, 264, 958, 1326, 924, 11, 51314, 51314, 286, 478, 988, 321, 603, 2573, 484, 257, 636, 281, 360, 300, 13, 51414, 51414, 407, 264, 700, 551, 286, 528, 281, 360, 307, 286, 478, 516, 281, 3625, 28848, 11, 558, 30, 51564, 51564, 8239, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18208859517024115, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.00022997723135631531}, {"id": 26, "seek": 5600, "start": 77.0, "end": 80.0, "text": " So the first thing I want to do is I'm going to install Lightning, right?", "tokens": [50364, 457, 341, 393, 5003, 281, 426, 45196, 11, 6218, 11, 1340, 291, 528, 13, 50614, 50614, 26127, 2539, 13, 50714, 50714, 865, 13, 50814, 50814, 1033, 13, 50914, 50914, 22574, 10226, 13, 51014, 51014, 400, 512, 295, 264, 2135, 12, 1353, 12, 49417, 412, 589, 382, 731, 11, 51164, 51164, 457, 286, 478, 6679, 300, 294, 264, 958, 1326, 924, 11, 51314, 51314, 286, 478, 988, 321, 603, 2573, 484, 257, 636, 281, 360, 300, 13, 51414, 51414, 407, 264, 700, 551, 286, 528, 281, 360, 307, 286, 478, 516, 281, 3625, 28848, 11, 558, 30, 51564, 51564, 8239, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18208859517024115, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.00022997723135631531}, {"id": 27, "seek": 5600, "start": 80.0, "end": 83.0, "text": " Hmm.", "tokens": [50364, 457, 341, 393, 5003, 281, 426, 45196, 11, 6218, 11, 1340, 291, 528, 13, 50614, 50614, 26127, 2539, 13, 50714, 50714, 865, 13, 50814, 50814, 1033, 13, 50914, 50914, 22574, 10226, 13, 51014, 51014, 400, 512, 295, 264, 2135, 12, 1353, 12, 49417, 412, 589, 382, 731, 11, 51164, 51164, 457, 286, 478, 6679, 300, 294, 264, 958, 1326, 924, 11, 51314, 51314, 286, 478, 988, 321, 603, 2573, 484, 257, 636, 281, 360, 300, 13, 51414, 51414, 407, 264, 700, 551, 286, 528, 281, 360, 307, 286, 478, 516, 281, 3625, 28848, 11, 558, 30, 51564, 51564, 8239, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18208859517024115, "compression_ratio": 1.4337899543378996, "no_speech_prob": 0.00022997723135631531}, {"id": 28, "seek": 8300, "start": 83.0, "end": 86.0, "text": " And Lightning is a lightweight wrapper for PyTorch,", "tokens": [50364, 400, 28848, 307, 257, 22052, 46906, 337, 9953, 51, 284, 339, 11, 50514, 50514, 337, 1090, 12, 50242, 2132, 11, 558, 30, 50614, 50614, 407, 498, 291, 434, 1228, 9953, 51, 284, 339, 11, 309, 1936, 4645, 279, 428, 9953, 51, 284, 339, 3089, 50814, 50814, 370, 300, 291, 393, 13982, 721, 411, 3866, 18407, 3097, 11, 50964, 50964, 18407, 82, 11, 293, 819, 721, 300, 3651, 257, 688, 295, 11769, 51214, 51214, 293, 11, 11939, 11, 366, 721, 291, 500, 380, 643, 281, 2028, 365, 51314, 51314, 562, 291, 434, 1382, 281, 589, 293, 1322, 5245, 13, 51464, 51464, 400, 264, 1150, 8388, 300, 286, 528, 281, 3625, 307, 14331, 1373, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08347467214119535, "compression_ratio": 1.6117216117216118, "no_speech_prob": 1.5934159819153138e-05}, {"id": 29, "seek": 8300, "start": 86.0, "end": 88.0, "text": " for high-performance research, right?", "tokens": [50364, 400, 28848, 307, 257, 22052, 46906, 337, 9953, 51, 284, 339, 11, 50514, 50514, 337, 1090, 12, 50242, 2132, 11, 558, 30, 50614, 50614, 407, 498, 291, 434, 1228, 9953, 51, 284, 339, 11, 309, 1936, 4645, 279, 428, 9953, 51, 284, 339, 3089, 50814, 50814, 370, 300, 291, 393, 13982, 721, 411, 3866, 18407, 3097, 11, 50964, 50964, 18407, 82, 11, 293, 819, 721, 300, 3651, 257, 688, 295, 11769, 51214, 51214, 293, 11, 11939, 11, 366, 721, 291, 500, 380, 643, 281, 2028, 365, 51314, 51314, 562, 291, 434, 1382, 281, 589, 293, 1322, 5245, 13, 51464, 51464, 400, 264, 1150, 8388, 300, 286, 528, 281, 3625, 307, 14331, 1373, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08347467214119535, "compression_ratio": 1.6117216117216118, "no_speech_prob": 1.5934159819153138e-05}, {"id": 30, "seek": 8300, "start": 88.0, "end": 92.0, "text": " So if you're using PyTorch, it basically organizes your PyTorch code", "tokens": [50364, 400, 28848, 307, 257, 22052, 46906, 337, 9953, 51, 284, 339, 11, 50514, 50514, 337, 1090, 12, 50242, 2132, 11, 558, 30, 50614, 50614, 407, 498, 291, 434, 1228, 9953, 51, 284, 339, 11, 309, 1936, 4645, 279, 428, 9953, 51, 284, 339, 3089, 50814, 50814, 370, 300, 291, 393, 13982, 721, 411, 3866, 18407, 3097, 11, 50964, 50964, 18407, 82, 11, 293, 819, 721, 300, 3651, 257, 688, 295, 11769, 51214, 51214, 293, 11, 11939, 11, 366, 721, 291, 500, 380, 643, 281, 2028, 365, 51314, 51314, 562, 291, 434, 1382, 281, 589, 293, 1322, 5245, 13, 51464, 51464, 400, 264, 1150, 8388, 300, 286, 528, 281, 3625, 307, 14331, 1373, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08347467214119535, "compression_ratio": 1.6117216117216118, "no_speech_prob": 1.5934159819153138e-05}, {"id": 31, "seek": 8300, "start": 92.0, "end": 95.0, "text": " so that you can leverage things like multiple GPU training,", "tokens": [50364, 400, 28848, 307, 257, 22052, 46906, 337, 9953, 51, 284, 339, 11, 50514, 50514, 337, 1090, 12, 50242, 2132, 11, 558, 30, 50614, 50614, 407, 498, 291, 434, 1228, 9953, 51, 284, 339, 11, 309, 1936, 4645, 279, 428, 9953, 51, 284, 339, 3089, 50814, 50814, 370, 300, 291, 393, 13982, 721, 411, 3866, 18407, 3097, 11, 50964, 50964, 18407, 82, 11, 293, 819, 721, 300, 3651, 257, 688, 295, 11769, 51214, 51214, 293, 11, 11939, 11, 366, 721, 291, 500, 380, 643, 281, 2028, 365, 51314, 51314, 562, 291, 434, 1382, 281, 589, 293, 1322, 5245, 13, 51464, 51464, 400, 264, 1150, 8388, 300, 286, 528, 281, 3625, 307, 14331, 1373, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08347467214119535, "compression_ratio": 1.6117216117216118, "no_speech_prob": 1.5934159819153138e-05}, {"id": 32, "seek": 8300, "start": 95.0, "end": 100.0, "text": " GPUs, and different things that require a lot of expertise", "tokens": [50364, 400, 28848, 307, 257, 22052, 46906, 337, 9953, 51, 284, 339, 11, 50514, 50514, 337, 1090, 12, 50242, 2132, 11, 558, 30, 50614, 50614, 407, 498, 291, 434, 1228, 9953, 51, 284, 339, 11, 309, 1936, 4645, 279, 428, 9953, 51, 284, 339, 3089, 50814, 50814, 370, 300, 291, 393, 13982, 721, 411, 3866, 18407, 3097, 11, 50964, 50964, 18407, 82, 11, 293, 819, 721, 300, 3651, 257, 688, 295, 11769, 51214, 51214, 293, 11, 11939, 11, 366, 721, 291, 500, 380, 643, 281, 2028, 365, 51314, 51314, 562, 291, 434, 1382, 281, 589, 293, 1322, 5245, 13, 51464, 51464, 400, 264, 1150, 8388, 300, 286, 528, 281, 3625, 307, 14331, 1373, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08347467214119535, "compression_ratio": 1.6117216117216118, "no_speech_prob": 1.5934159819153138e-05}, {"id": 33, "seek": 8300, "start": 100.0, "end": 102.0, "text": " and, frankly, are things you don't need to deal with", "tokens": [50364, 400, 28848, 307, 257, 22052, 46906, 337, 9953, 51, 284, 339, 11, 50514, 50514, 337, 1090, 12, 50242, 2132, 11, 558, 30, 50614, 50614, 407, 498, 291, 434, 1228, 9953, 51, 284, 339, 11, 309, 1936, 4645, 279, 428, 9953, 51, 284, 339, 3089, 50814, 50814, 370, 300, 291, 393, 13982, 721, 411, 3866, 18407, 3097, 11, 50964, 50964, 18407, 82, 11, 293, 819, 721, 300, 3651, 257, 688, 295, 11769, 51214, 51214, 293, 11, 11939, 11, 366, 721, 291, 500, 380, 643, 281, 2028, 365, 51314, 51314, 562, 291, 434, 1382, 281, 589, 293, 1322, 5245, 13, 51464, 51464, 400, 264, 1150, 8388, 300, 286, 528, 281, 3625, 307, 14331, 1373, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08347467214119535, "compression_ratio": 1.6117216117216118, "no_speech_prob": 1.5934159819153138e-05}, {"id": 34, "seek": 8300, "start": 102.0, "end": 105.0, "text": " when you're trying to work and build models.", "tokens": [50364, 400, 28848, 307, 257, 22052, 46906, 337, 9953, 51, 284, 339, 11, 50514, 50514, 337, 1090, 12, 50242, 2132, 11, 558, 30, 50614, 50614, 407, 498, 291, 434, 1228, 9953, 51, 284, 339, 11, 309, 1936, 4645, 279, 428, 9953, 51, 284, 339, 3089, 50814, 50814, 370, 300, 291, 393, 13982, 721, 411, 3866, 18407, 3097, 11, 50964, 50964, 18407, 82, 11, 293, 819, 721, 300, 3651, 257, 688, 295, 11769, 51214, 51214, 293, 11, 11939, 11, 366, 721, 291, 500, 380, 643, 281, 2028, 365, 51314, 51314, 562, 291, 434, 1382, 281, 589, 293, 1322, 5245, 13, 51464, 51464, 400, 264, 1150, 8388, 300, 286, 528, 281, 3625, 307, 14331, 1373, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08347467214119535, "compression_ratio": 1.6117216117216118, "no_speech_prob": 1.5934159819153138e-05}, {"id": 35, "seek": 8300, "start": 105.0, "end": 109.0, "text": " And the second framework that I want to install is Bolts, right?", "tokens": [50364, 400, 28848, 307, 257, 22052, 46906, 337, 9953, 51, 284, 339, 11, 50514, 50514, 337, 1090, 12, 50242, 2132, 11, 558, 30, 50614, 50614, 407, 498, 291, 434, 1228, 9953, 51, 284, 339, 11, 309, 1936, 4645, 279, 428, 9953, 51, 284, 339, 3089, 50814, 50814, 370, 300, 291, 393, 13982, 721, 411, 3866, 18407, 3097, 11, 50964, 50964, 18407, 82, 11, 293, 819, 721, 300, 3651, 257, 688, 295, 11769, 51214, 51214, 293, 11, 11939, 11, 366, 721, 291, 500, 380, 643, 281, 2028, 365, 51314, 51314, 562, 291, 434, 1382, 281, 589, 293, 1322, 5245, 13, 51464, 51464, 400, 264, 1150, 8388, 300, 286, 528, 281, 3625, 307, 14331, 1373, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.08347467214119535, "compression_ratio": 1.6117216117216118, "no_speech_prob": 1.5934159819153138e-05}, {"id": 36, "seek": 10900, "start": 109.0, "end": 113.0, "text": " So Bolts is our other framework, and it is for hyper...", "tokens": [50364, 407, 14331, 1373, 307, 527, 661, 8388, 11, 293, 309, 307, 337, 9848, 485, 50564, 50564, 467, 311, 411, 257, 2132, 40167, 11, 1936, 13, 50714, 50714, 4919, 11, 286, 393, 380, 915, 452, 9719, 13, 821, 309, 307, 13, 1033, 13, 50814, 50814, 407, 309, 311, 257, 2132, 40167, 13, 50914, 50914, 407, 498, 291, 1562, 17055, 11, 498, 291, 434, 2891, 746, 485, 51114, 51114, 509, 458, 11, 309, 311, 611, 1143, 294, 3518, 382, 731, 11, 51214, 51214, 457, 498, 291, 434, 2891, 257, 1716, 293, 291, 434, 1237, 337, 257, 2316, 51364, 51364, 293, 291, 500, 380, 915, 472, 11, 291, 393, 574, 337, 309, 294, 14331, 1373, 11, 558, 30, 51614, 51614, 400, 550, 341, 486, 312, 472, 11, 1310, 6264, 472, 295, 264, 6792, 5245, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09915619036730598, "compression_ratio": 1.7116788321167884, "no_speech_prob": 1.3418431080935989e-05}, {"id": 37, "seek": 10900, "start": 113.0, "end": 116.0, "text": " It's like a research toolkit, basically.", "tokens": [50364, 407, 14331, 1373, 307, 527, 661, 8388, 11, 293, 309, 307, 337, 9848, 485, 50564, 50564, 467, 311, 411, 257, 2132, 40167, 11, 1936, 13, 50714, 50714, 4919, 11, 286, 393, 380, 915, 452, 9719, 13, 821, 309, 307, 13, 1033, 13, 50814, 50814, 407, 309, 311, 257, 2132, 40167, 13, 50914, 50914, 407, 498, 291, 1562, 17055, 11, 498, 291, 434, 2891, 746, 485, 51114, 51114, 509, 458, 11, 309, 311, 611, 1143, 294, 3518, 382, 731, 11, 51214, 51214, 457, 498, 291, 434, 2891, 257, 1716, 293, 291, 434, 1237, 337, 257, 2316, 51364, 51364, 293, 291, 500, 380, 915, 472, 11, 291, 393, 574, 337, 309, 294, 14331, 1373, 11, 558, 30, 51614, 51614, 400, 550, 341, 486, 312, 472, 11, 1310, 6264, 472, 295, 264, 6792, 5245, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09915619036730598, "compression_ratio": 1.7116788321167884, "no_speech_prob": 1.3418431080935989e-05}, {"id": 38, "seek": 10900, "start": 116.0, "end": 118.0, "text": " Sorry, I can't find my mouse. There it is. Okay.", "tokens": [50364, 407, 14331, 1373, 307, 527, 661, 8388, 11, 293, 309, 307, 337, 9848, 485, 50564, 50564, 467, 311, 411, 257, 2132, 40167, 11, 1936, 13, 50714, 50714, 4919, 11, 286, 393, 380, 915, 452, 9719, 13, 821, 309, 307, 13, 1033, 13, 50814, 50814, 407, 309, 311, 257, 2132, 40167, 13, 50914, 50914, 407, 498, 291, 1562, 17055, 11, 498, 291, 434, 2891, 746, 485, 51114, 51114, 509, 458, 11, 309, 311, 611, 1143, 294, 3518, 382, 731, 11, 51214, 51214, 457, 498, 291, 434, 2891, 257, 1716, 293, 291, 434, 1237, 337, 257, 2316, 51364, 51364, 293, 291, 500, 380, 915, 472, 11, 291, 393, 574, 337, 309, 294, 14331, 1373, 11, 558, 30, 51614, 51614, 400, 550, 341, 486, 312, 472, 11, 1310, 6264, 472, 295, 264, 6792, 5245, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09915619036730598, "compression_ratio": 1.7116788321167884, "no_speech_prob": 1.3418431080935989e-05}, {"id": 39, "seek": 10900, "start": 118.0, "end": 120.0, "text": " So it's a research toolkit.", "tokens": [50364, 407, 14331, 1373, 307, 527, 661, 8388, 11, 293, 309, 307, 337, 9848, 485, 50564, 50564, 467, 311, 411, 257, 2132, 40167, 11, 1936, 13, 50714, 50714, 4919, 11, 286, 393, 380, 915, 452, 9719, 13, 821, 309, 307, 13, 1033, 13, 50814, 50814, 407, 309, 311, 257, 2132, 40167, 13, 50914, 50914, 407, 498, 291, 1562, 17055, 11, 498, 291, 434, 2891, 746, 485, 51114, 51114, 509, 458, 11, 309, 311, 611, 1143, 294, 3518, 382, 731, 11, 51214, 51214, 457, 498, 291, 434, 2891, 257, 1716, 293, 291, 434, 1237, 337, 257, 2316, 51364, 51364, 293, 291, 500, 380, 915, 472, 11, 291, 393, 574, 337, 309, 294, 14331, 1373, 11, 558, 30, 51614, 51614, 400, 550, 341, 486, 312, 472, 11, 1310, 6264, 472, 295, 264, 6792, 5245, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09915619036730598, "compression_ratio": 1.7116788321167884, "no_speech_prob": 1.3418431080935989e-05}, {"id": 40, "seek": 10900, "start": 120.0, "end": 124.0, "text": " So if you ever wondered, if you're starting something...", "tokens": [50364, 407, 14331, 1373, 307, 527, 661, 8388, 11, 293, 309, 307, 337, 9848, 485, 50564, 50564, 467, 311, 411, 257, 2132, 40167, 11, 1936, 13, 50714, 50714, 4919, 11, 286, 393, 380, 915, 452, 9719, 13, 821, 309, 307, 13, 1033, 13, 50814, 50814, 407, 309, 311, 257, 2132, 40167, 13, 50914, 50914, 407, 498, 291, 1562, 17055, 11, 498, 291, 434, 2891, 746, 485, 51114, 51114, 509, 458, 11, 309, 311, 611, 1143, 294, 3518, 382, 731, 11, 51214, 51214, 457, 498, 291, 434, 2891, 257, 1716, 293, 291, 434, 1237, 337, 257, 2316, 51364, 51364, 293, 291, 500, 380, 915, 472, 11, 291, 393, 574, 337, 309, 294, 14331, 1373, 11, 558, 30, 51614, 51614, 400, 550, 341, 486, 312, 472, 11, 1310, 6264, 472, 295, 264, 6792, 5245, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09915619036730598, "compression_ratio": 1.7116788321167884, "no_speech_prob": 1.3418431080935989e-05}, {"id": 41, "seek": 10900, "start": 124.0, "end": 126.0, "text": " You know, it's also used in industry as well,", "tokens": [50364, 407, 14331, 1373, 307, 527, 661, 8388, 11, 293, 309, 307, 337, 9848, 485, 50564, 50564, 467, 311, 411, 257, 2132, 40167, 11, 1936, 13, 50714, 50714, 4919, 11, 286, 393, 380, 915, 452, 9719, 13, 821, 309, 307, 13, 1033, 13, 50814, 50814, 407, 309, 311, 257, 2132, 40167, 13, 50914, 50914, 407, 498, 291, 1562, 17055, 11, 498, 291, 434, 2891, 746, 485, 51114, 51114, 509, 458, 11, 309, 311, 611, 1143, 294, 3518, 382, 731, 11, 51214, 51214, 457, 498, 291, 434, 2891, 257, 1716, 293, 291, 434, 1237, 337, 257, 2316, 51364, 51364, 293, 291, 500, 380, 915, 472, 11, 291, 393, 574, 337, 309, 294, 14331, 1373, 11, 558, 30, 51614, 51614, 400, 550, 341, 486, 312, 472, 11, 1310, 6264, 472, 295, 264, 6792, 5245, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09915619036730598, "compression_ratio": 1.7116788321167884, "no_speech_prob": 1.3418431080935989e-05}, {"id": 42, "seek": 10900, "start": 126.0, "end": 129.0, "text": " but if you're starting a project and you're looking for a model", "tokens": [50364, 407, 14331, 1373, 307, 527, 661, 8388, 11, 293, 309, 307, 337, 9848, 485, 50564, 50564, 467, 311, 411, 257, 2132, 40167, 11, 1936, 13, 50714, 50714, 4919, 11, 286, 393, 380, 915, 452, 9719, 13, 821, 309, 307, 13, 1033, 13, 50814, 50814, 407, 309, 311, 257, 2132, 40167, 13, 50914, 50914, 407, 498, 291, 1562, 17055, 11, 498, 291, 434, 2891, 746, 485, 51114, 51114, 509, 458, 11, 309, 311, 611, 1143, 294, 3518, 382, 731, 11, 51214, 51214, 457, 498, 291, 434, 2891, 257, 1716, 293, 291, 434, 1237, 337, 257, 2316, 51364, 51364, 293, 291, 500, 380, 915, 472, 11, 291, 393, 574, 337, 309, 294, 14331, 1373, 11, 558, 30, 51614, 51614, 400, 550, 341, 486, 312, 472, 11, 1310, 6264, 472, 295, 264, 6792, 5245, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09915619036730598, "compression_ratio": 1.7116788321167884, "no_speech_prob": 1.3418431080935989e-05}, {"id": 43, "seek": 10900, "start": 129.0, "end": 134.0, "text": " and you don't find one, you can look for it in Bolts, right?", "tokens": [50364, 407, 14331, 1373, 307, 527, 661, 8388, 11, 293, 309, 307, 337, 9848, 485, 50564, 50564, 467, 311, 411, 257, 2132, 40167, 11, 1936, 13, 50714, 50714, 4919, 11, 286, 393, 380, 915, 452, 9719, 13, 821, 309, 307, 13, 1033, 13, 50814, 50814, 407, 309, 311, 257, 2132, 40167, 13, 50914, 50914, 407, 498, 291, 1562, 17055, 11, 498, 291, 434, 2891, 746, 485, 51114, 51114, 509, 458, 11, 309, 311, 611, 1143, 294, 3518, 382, 731, 11, 51214, 51214, 457, 498, 291, 434, 2891, 257, 1716, 293, 291, 434, 1237, 337, 257, 2316, 51364, 51364, 293, 291, 500, 380, 915, 472, 11, 291, 393, 574, 337, 309, 294, 14331, 1373, 11, 558, 30, 51614, 51614, 400, 550, 341, 486, 312, 472, 11, 1310, 6264, 472, 295, 264, 6792, 5245, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09915619036730598, "compression_ratio": 1.7116788321167884, "no_speech_prob": 1.3418431080935989e-05}, {"id": 44, "seek": 10900, "start": 134.0, "end": 137.0, "text": " And then this will be one, maybe possibly one of the latest models,", "tokens": [50364, 407, 14331, 1373, 307, 527, 661, 8388, 11, 293, 309, 307, 337, 9848, 485, 50564, 50564, 467, 311, 411, 257, 2132, 40167, 11, 1936, 13, 50714, 50714, 4919, 11, 286, 393, 380, 915, 452, 9719, 13, 821, 309, 307, 13, 1033, 13, 50814, 50814, 407, 309, 311, 257, 2132, 40167, 13, 50914, 50914, 407, 498, 291, 1562, 17055, 11, 498, 291, 434, 2891, 746, 485, 51114, 51114, 509, 458, 11, 309, 311, 611, 1143, 294, 3518, 382, 731, 11, 51214, 51214, 457, 498, 291, 434, 2891, 257, 1716, 293, 291, 434, 1237, 337, 257, 2316, 51364, 51364, 293, 291, 500, 380, 915, 472, 11, 291, 393, 574, 337, 309, 294, 14331, 1373, 11, 558, 30, 51614, 51614, 400, 550, 341, 486, 312, 472, 11, 1310, 6264, 472, 295, 264, 6792, 5245, 11, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09915619036730598, "compression_ratio": 1.7116788321167884, "no_speech_prob": 1.3418431080935989e-05}, {"id": 45, "seek": 13700, "start": 137.0, "end": 141.0, "text": " but it's already been implemented, tested, and documented", "tokens": [50364, 457, 309, 311, 1217, 668, 12270, 11, 8246, 11, 293, 23007, 50564, 50564, 370, 300, 291, 393, 722, 490, 257, 665, 4008, 13, 50664, 50664, 407, 291, 500, 380, 362, 281, 1394, 510, 293, 853, 281, 4445, 309, 50764, 50764, 2361, 322, 1803, 337, 1045, 2493, 281, 536, 498, 291, 483, 309, 558, 13, 50864, 50864, 509, 393, 445, 733, 295, 1422, 11665, 293, 1322, 322, 300, 13, 51014, 51014, 400, 14331, 1373, 575, 257, 1238, 13956, 6405, 295, 2093, 12, 48172, 24420, 2539, 13, 51214, 51214, 467, 311, 257, 688, 295, 1507, 300, 11, 291, 458, 11, 309, 311, 5665, 1096, 294, 452, 2132, 382, 731, 11, 51364, 51364, 293, 721, 300, 321, 600, 12270, 490, 264, 6792, 10577, 13, 51564, 51564, 1033, 11, 498, 286, 528, 281, 1466, 544, 466, 613, 721, 11, 51664, 51664, 689, 393, 286, 915, 544, 3593, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08657936792115907, "compression_ratio": 1.6993865030674846, "no_speech_prob": 3.9437807572539896e-05}, {"id": 46, "seek": 13700, "start": 141.0, "end": 143.0, "text": " so that you can start from a good spot.", "tokens": [50364, 457, 309, 311, 1217, 668, 12270, 11, 8246, 11, 293, 23007, 50564, 50564, 370, 300, 291, 393, 722, 490, 257, 665, 4008, 13, 50664, 50664, 407, 291, 500, 380, 362, 281, 1394, 510, 293, 853, 281, 4445, 309, 50764, 50764, 2361, 322, 1803, 337, 1045, 2493, 281, 536, 498, 291, 483, 309, 558, 13, 50864, 50864, 509, 393, 445, 733, 295, 1422, 11665, 293, 1322, 322, 300, 13, 51014, 51014, 400, 14331, 1373, 575, 257, 1238, 13956, 6405, 295, 2093, 12, 48172, 24420, 2539, 13, 51214, 51214, 467, 311, 257, 688, 295, 1507, 300, 11, 291, 458, 11, 309, 311, 5665, 1096, 294, 452, 2132, 382, 731, 11, 51364, 51364, 293, 721, 300, 321, 600, 12270, 490, 264, 6792, 10577, 13, 51564, 51564, 1033, 11, 498, 286, 528, 281, 1466, 544, 466, 613, 721, 11, 51664, 51664, 689, 393, 286, 915, 544, 3593, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08657936792115907, "compression_ratio": 1.6993865030674846, "no_speech_prob": 3.9437807572539896e-05}, {"id": 47, "seek": 13700, "start": 143.0, "end": 145.0, "text": " So you don't have to sit here and try to implement it", "tokens": [50364, 457, 309, 311, 1217, 668, 12270, 11, 8246, 11, 293, 23007, 50564, 50564, 370, 300, 291, 393, 722, 490, 257, 665, 4008, 13, 50664, 50664, 407, 291, 500, 380, 362, 281, 1394, 510, 293, 853, 281, 4445, 309, 50764, 50764, 2361, 322, 1803, 337, 1045, 2493, 281, 536, 498, 291, 483, 309, 558, 13, 50864, 50864, 509, 393, 445, 733, 295, 1422, 11665, 293, 1322, 322, 300, 13, 51014, 51014, 400, 14331, 1373, 575, 257, 1238, 13956, 6405, 295, 2093, 12, 48172, 24420, 2539, 13, 51214, 51214, 467, 311, 257, 688, 295, 1507, 300, 11, 291, 458, 11, 309, 311, 5665, 1096, 294, 452, 2132, 382, 731, 11, 51364, 51364, 293, 721, 300, 321, 600, 12270, 490, 264, 6792, 10577, 13, 51564, 51564, 1033, 11, 498, 286, 528, 281, 1466, 544, 466, 613, 721, 11, 51664, 51664, 689, 393, 286, 915, 544, 3593, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08657936792115907, "compression_ratio": 1.6993865030674846, "no_speech_prob": 3.9437807572539896e-05}, {"id": 48, "seek": 13700, "start": 145.0, "end": 147.0, "text": " based on yourself for three months to see if you get it right.", "tokens": [50364, 457, 309, 311, 1217, 668, 12270, 11, 8246, 11, 293, 23007, 50564, 50564, 370, 300, 291, 393, 722, 490, 257, 665, 4008, 13, 50664, 50664, 407, 291, 500, 380, 362, 281, 1394, 510, 293, 853, 281, 4445, 309, 50764, 50764, 2361, 322, 1803, 337, 1045, 2493, 281, 536, 498, 291, 483, 309, 558, 13, 50864, 50864, 509, 393, 445, 733, 295, 1422, 11665, 293, 1322, 322, 300, 13, 51014, 51014, 400, 14331, 1373, 575, 257, 1238, 13956, 6405, 295, 2093, 12, 48172, 24420, 2539, 13, 51214, 51214, 467, 311, 257, 688, 295, 1507, 300, 11, 291, 458, 11, 309, 311, 5665, 1096, 294, 452, 2132, 382, 731, 11, 51364, 51364, 293, 721, 300, 321, 600, 12270, 490, 264, 6792, 10577, 13, 51564, 51564, 1033, 11, 498, 286, 528, 281, 1466, 544, 466, 613, 721, 11, 51664, 51664, 689, 393, 286, 915, 544, 3593, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08657936792115907, "compression_ratio": 1.6993865030674846, "no_speech_prob": 3.9437807572539896e-05}, {"id": 49, "seek": 13700, "start": 147.0, "end": 150.0, "text": " You can just kind of subclass and build on that.", "tokens": [50364, 457, 309, 311, 1217, 668, 12270, 11, 8246, 11, 293, 23007, 50564, 50564, 370, 300, 291, 393, 722, 490, 257, 665, 4008, 13, 50664, 50664, 407, 291, 500, 380, 362, 281, 1394, 510, 293, 853, 281, 4445, 309, 50764, 50764, 2361, 322, 1803, 337, 1045, 2493, 281, 536, 498, 291, 483, 309, 558, 13, 50864, 50864, 509, 393, 445, 733, 295, 1422, 11665, 293, 1322, 322, 300, 13, 51014, 51014, 400, 14331, 1373, 575, 257, 1238, 13956, 6405, 295, 2093, 12, 48172, 24420, 2539, 13, 51214, 51214, 467, 311, 257, 688, 295, 1507, 300, 11, 291, 458, 11, 309, 311, 5665, 1096, 294, 452, 2132, 382, 731, 11, 51364, 51364, 293, 721, 300, 321, 600, 12270, 490, 264, 6792, 10577, 13, 51564, 51564, 1033, 11, 498, 286, 528, 281, 1466, 544, 466, 613, 721, 11, 51664, 51664, 689, 393, 286, 915, 544, 3593, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08657936792115907, "compression_ratio": 1.6993865030674846, "no_speech_prob": 3.9437807572539896e-05}, {"id": 50, "seek": 13700, "start": 150.0, "end": 154.0, "text": " And Bolts has a pretty robust library of social-supervised learning.", "tokens": [50364, 457, 309, 311, 1217, 668, 12270, 11, 8246, 11, 293, 23007, 50564, 50564, 370, 300, 291, 393, 722, 490, 257, 665, 4008, 13, 50664, 50664, 407, 291, 500, 380, 362, 281, 1394, 510, 293, 853, 281, 4445, 309, 50764, 50764, 2361, 322, 1803, 337, 1045, 2493, 281, 536, 498, 291, 483, 309, 558, 13, 50864, 50864, 509, 393, 445, 733, 295, 1422, 11665, 293, 1322, 322, 300, 13, 51014, 51014, 400, 14331, 1373, 575, 257, 1238, 13956, 6405, 295, 2093, 12, 48172, 24420, 2539, 13, 51214, 51214, 467, 311, 257, 688, 295, 1507, 300, 11, 291, 458, 11, 309, 311, 5665, 1096, 294, 452, 2132, 382, 731, 11, 51364, 51364, 293, 721, 300, 321, 600, 12270, 490, 264, 6792, 10577, 13, 51564, 51564, 1033, 11, 498, 286, 528, 281, 1466, 544, 466, 613, 721, 11, 51664, 51664, 689, 393, 286, 915, 544, 3593, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08657936792115907, "compression_ratio": 1.6993865030674846, "no_speech_prob": 3.9437807572539896e-05}, {"id": 51, "seek": 13700, "start": 154.0, "end": 157.0, "text": " It's a lot of stuff that, you know, it's personally done in my research as well,", "tokens": [50364, 457, 309, 311, 1217, 668, 12270, 11, 8246, 11, 293, 23007, 50564, 50564, 370, 300, 291, 393, 722, 490, 257, 665, 4008, 13, 50664, 50664, 407, 291, 500, 380, 362, 281, 1394, 510, 293, 853, 281, 4445, 309, 50764, 50764, 2361, 322, 1803, 337, 1045, 2493, 281, 536, 498, 291, 483, 309, 558, 13, 50864, 50864, 509, 393, 445, 733, 295, 1422, 11665, 293, 1322, 322, 300, 13, 51014, 51014, 400, 14331, 1373, 575, 257, 1238, 13956, 6405, 295, 2093, 12, 48172, 24420, 2539, 13, 51214, 51214, 467, 311, 257, 688, 295, 1507, 300, 11, 291, 458, 11, 309, 311, 5665, 1096, 294, 452, 2132, 382, 731, 11, 51364, 51364, 293, 721, 300, 321, 600, 12270, 490, 264, 6792, 10577, 13, 51564, 51564, 1033, 11, 498, 286, 528, 281, 1466, 544, 466, 613, 721, 11, 51664, 51664, 689, 393, 286, 915, 544, 3593, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08657936792115907, "compression_ratio": 1.6993865030674846, "no_speech_prob": 3.9437807572539896e-05}, {"id": 52, "seek": 13700, "start": 157.0, "end": 161.0, "text": " and things that we've implemented from the latest papers.", "tokens": [50364, 457, 309, 311, 1217, 668, 12270, 11, 8246, 11, 293, 23007, 50564, 50564, 370, 300, 291, 393, 722, 490, 257, 665, 4008, 13, 50664, 50664, 407, 291, 500, 380, 362, 281, 1394, 510, 293, 853, 281, 4445, 309, 50764, 50764, 2361, 322, 1803, 337, 1045, 2493, 281, 536, 498, 291, 483, 309, 558, 13, 50864, 50864, 509, 393, 445, 733, 295, 1422, 11665, 293, 1322, 322, 300, 13, 51014, 51014, 400, 14331, 1373, 575, 257, 1238, 13956, 6405, 295, 2093, 12, 48172, 24420, 2539, 13, 51214, 51214, 467, 311, 257, 688, 295, 1507, 300, 11, 291, 458, 11, 309, 311, 5665, 1096, 294, 452, 2132, 382, 731, 11, 51364, 51364, 293, 721, 300, 321, 600, 12270, 490, 264, 6792, 10577, 13, 51564, 51564, 1033, 11, 498, 286, 528, 281, 1466, 544, 466, 613, 721, 11, 51664, 51664, 689, 393, 286, 915, 544, 3593, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08657936792115907, "compression_ratio": 1.6993865030674846, "no_speech_prob": 3.9437807572539896e-05}, {"id": 53, "seek": 13700, "start": 161.0, "end": 163.0, "text": " Okay, if I want to learn more about these things,", "tokens": [50364, 457, 309, 311, 1217, 668, 12270, 11, 8246, 11, 293, 23007, 50564, 50564, 370, 300, 291, 393, 722, 490, 257, 665, 4008, 13, 50664, 50664, 407, 291, 500, 380, 362, 281, 1394, 510, 293, 853, 281, 4445, 309, 50764, 50764, 2361, 322, 1803, 337, 1045, 2493, 281, 536, 498, 291, 483, 309, 558, 13, 50864, 50864, 509, 393, 445, 733, 295, 1422, 11665, 293, 1322, 322, 300, 13, 51014, 51014, 400, 14331, 1373, 575, 257, 1238, 13956, 6405, 295, 2093, 12, 48172, 24420, 2539, 13, 51214, 51214, 467, 311, 257, 688, 295, 1507, 300, 11, 291, 458, 11, 309, 311, 5665, 1096, 294, 452, 2132, 382, 731, 11, 51364, 51364, 293, 721, 300, 321, 600, 12270, 490, 264, 6792, 10577, 13, 51564, 51564, 1033, 11, 498, 286, 528, 281, 1466, 544, 466, 613, 721, 11, 51664, 51664, 689, 393, 286, 915, 544, 3593, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08657936792115907, "compression_ratio": 1.6993865030674846, "no_speech_prob": 3.9437807572539896e-05}, {"id": 54, "seek": 13700, "start": 163.0, "end": 166.0, "text": " where can I find more resources?", "tokens": [50364, 457, 309, 311, 1217, 668, 12270, 11, 8246, 11, 293, 23007, 50564, 50564, 370, 300, 291, 393, 722, 490, 257, 665, 4008, 13, 50664, 50664, 407, 291, 500, 380, 362, 281, 1394, 510, 293, 853, 281, 4445, 309, 50764, 50764, 2361, 322, 1803, 337, 1045, 2493, 281, 536, 498, 291, 483, 309, 558, 13, 50864, 50864, 509, 393, 445, 733, 295, 1422, 11665, 293, 1322, 322, 300, 13, 51014, 51014, 400, 14331, 1373, 575, 257, 1238, 13956, 6405, 295, 2093, 12, 48172, 24420, 2539, 13, 51214, 51214, 467, 311, 257, 688, 295, 1507, 300, 11, 291, 458, 11, 309, 311, 5665, 1096, 294, 452, 2132, 382, 731, 11, 51364, 51364, 293, 721, 300, 321, 600, 12270, 490, 264, 6792, 10577, 13, 51564, 51564, 1033, 11, 498, 286, 528, 281, 1466, 544, 466, 613, 721, 11, 51664, 51664, 689, 393, 286, 915, 544, 3593, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08657936792115907, "compression_ratio": 1.6993865030674846, "no_speech_prob": 3.9437807572539896e-05}, {"id": 55, "seek": 16600, "start": 166.0, "end": 168.0, "text": " Yeah, so you can go to Lightning repo, right?", "tokens": [50364, 865, 11, 370, 291, 393, 352, 281, 28848, 49040, 11, 558, 30, 50464, 50464, 407, 341, 307, 485, 50614, 50614, 286, 914, 11, 286, 519, 1391, 264, 12889, 551, 307, 281, 352, 281, 9953, 51, 284, 339, 28848, 13, 1301, 510, 13, 50814, 50814, 407, 321, 362, 264, 11202, 3028, 456, 13, 50914, 50914, 400, 550, 321, 362, 1203, 11, 14333, 11, 2145, 11, 51114, 51114, 466, 28848, 11, 466, 264, 1469, 11, 293, 1203, 1646, 13, 51264, 51264, 400, 550, 456, 291, 393, 2052, 322, 16024, 82, 11, 558, 30, 51364, 51364, 400, 550, 352, 2997, 281, 264, 45623, 11, 51464, 51464, 483, 1409, 510, 294, 264, 28848, 294, 732, 4439, 11, 1401, 341, 13, 51614, 51614, 400, 412, 264, 917, 295, 341, 11, 291, 820, 458, 1203, 291, 643, 281, 458, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1187395151110663, "compression_ratio": 1.8807692307692307, "no_speech_prob": 7.964256656123325e-05}, {"id": 56, "seek": 16600, "start": 168.0, "end": 171.0, "text": " So this is...", "tokens": [50364, 865, 11, 370, 291, 393, 352, 281, 28848, 49040, 11, 558, 30, 50464, 50464, 407, 341, 307, 485, 50614, 50614, 286, 914, 11, 286, 519, 1391, 264, 12889, 551, 307, 281, 352, 281, 9953, 51, 284, 339, 28848, 13, 1301, 510, 13, 50814, 50814, 407, 321, 362, 264, 11202, 3028, 456, 13, 50914, 50914, 400, 550, 321, 362, 1203, 11, 14333, 11, 2145, 11, 51114, 51114, 466, 28848, 11, 466, 264, 1469, 11, 293, 1203, 1646, 13, 51264, 51264, 400, 550, 456, 291, 393, 2052, 322, 16024, 82, 11, 558, 30, 51364, 51364, 400, 550, 352, 2997, 281, 264, 45623, 11, 51464, 51464, 483, 1409, 510, 294, 264, 28848, 294, 732, 4439, 11, 1401, 341, 13, 51614, 51614, 400, 412, 264, 917, 295, 341, 11, 291, 820, 458, 1203, 291, 643, 281, 458, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1187395151110663, "compression_ratio": 1.8807692307692307, "no_speech_prob": 7.964256656123325e-05}, {"id": 57, "seek": 16600, "start": 171.0, "end": 175.0, "text": " I mean, I think probably the easiest thing is to go to PyTorch Lightning.ai here.", "tokens": [50364, 865, 11, 370, 291, 393, 352, 281, 28848, 49040, 11, 558, 30, 50464, 50464, 407, 341, 307, 485, 50614, 50614, 286, 914, 11, 286, 519, 1391, 264, 12889, 551, 307, 281, 352, 281, 9953, 51, 284, 339, 28848, 13, 1301, 510, 13, 50814, 50814, 407, 321, 362, 264, 11202, 3028, 456, 13, 50914, 50914, 400, 550, 321, 362, 1203, 11, 14333, 11, 2145, 11, 51114, 51114, 466, 28848, 11, 466, 264, 1469, 11, 293, 1203, 1646, 13, 51264, 51264, 400, 550, 456, 291, 393, 2052, 322, 16024, 82, 11, 558, 30, 51364, 51364, 400, 550, 352, 2997, 281, 264, 45623, 11, 51464, 51464, 483, 1409, 510, 294, 264, 28848, 294, 732, 4439, 11, 1401, 341, 13, 51614, 51614, 400, 412, 264, 917, 295, 341, 11, 291, 820, 458, 1203, 291, 643, 281, 458, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1187395151110663, "compression_ratio": 1.8807692307692307, "no_speech_prob": 7.964256656123325e-05}, {"id": 58, "seek": 16600, "start": 175.0, "end": 177.0, "text": " So we have the landing page there.", "tokens": [50364, 865, 11, 370, 291, 393, 352, 281, 28848, 49040, 11, 558, 30, 50464, 50464, 407, 341, 307, 485, 50614, 50614, 286, 914, 11, 286, 519, 1391, 264, 12889, 551, 307, 281, 352, 281, 9953, 51, 284, 339, 28848, 13, 1301, 510, 13, 50814, 50814, 407, 321, 362, 264, 11202, 3028, 456, 13, 50914, 50914, 400, 550, 321, 362, 1203, 11, 14333, 11, 2145, 11, 51114, 51114, 466, 28848, 11, 466, 264, 1469, 11, 293, 1203, 1646, 13, 51264, 51264, 400, 550, 456, 291, 393, 2052, 322, 16024, 82, 11, 558, 30, 51364, 51364, 400, 550, 352, 2997, 281, 264, 45623, 11, 51464, 51464, 483, 1409, 510, 294, 264, 28848, 294, 732, 4439, 11, 1401, 341, 13, 51614, 51614, 400, 412, 264, 917, 295, 341, 11, 291, 820, 458, 1203, 291, 643, 281, 458, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1187395151110663, "compression_ratio": 1.8807692307692307, "no_speech_prob": 7.964256656123325e-05}, {"id": 59, "seek": 16600, "start": 177.0, "end": 181.0, "text": " And then we have everything, documentation, videos,", "tokens": [50364, 865, 11, 370, 291, 393, 352, 281, 28848, 49040, 11, 558, 30, 50464, 50464, 407, 341, 307, 485, 50614, 50614, 286, 914, 11, 286, 519, 1391, 264, 12889, 551, 307, 281, 352, 281, 9953, 51, 284, 339, 28848, 13, 1301, 510, 13, 50814, 50814, 407, 321, 362, 264, 11202, 3028, 456, 13, 50914, 50914, 400, 550, 321, 362, 1203, 11, 14333, 11, 2145, 11, 51114, 51114, 466, 28848, 11, 466, 264, 1469, 11, 293, 1203, 1646, 13, 51264, 51264, 400, 550, 456, 291, 393, 2052, 322, 16024, 82, 11, 558, 30, 51364, 51364, 400, 550, 352, 2997, 281, 264, 45623, 11, 51464, 51464, 483, 1409, 510, 294, 264, 28848, 294, 732, 4439, 11, 1401, 341, 13, 51614, 51614, 400, 412, 264, 917, 295, 341, 11, 291, 820, 458, 1203, 291, 643, 281, 458, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1187395151110663, "compression_ratio": 1.8807692307692307, "no_speech_prob": 7.964256656123325e-05}, {"id": 60, "seek": 16600, "start": 181.0, "end": 184.0, "text": " about Lightning, about the team, and everything else.", "tokens": [50364, 865, 11, 370, 291, 393, 352, 281, 28848, 49040, 11, 558, 30, 50464, 50464, 407, 341, 307, 485, 50614, 50614, 286, 914, 11, 286, 519, 1391, 264, 12889, 551, 307, 281, 352, 281, 9953, 51, 284, 339, 28848, 13, 1301, 510, 13, 50814, 50814, 407, 321, 362, 264, 11202, 3028, 456, 13, 50914, 50914, 400, 550, 321, 362, 1203, 11, 14333, 11, 2145, 11, 51114, 51114, 466, 28848, 11, 466, 264, 1469, 11, 293, 1203, 1646, 13, 51264, 51264, 400, 550, 456, 291, 393, 2052, 322, 16024, 82, 11, 558, 30, 51364, 51364, 400, 550, 352, 2997, 281, 264, 45623, 11, 51464, 51464, 483, 1409, 510, 294, 264, 28848, 294, 732, 4439, 11, 1401, 341, 13, 51614, 51614, 400, 412, 264, 917, 295, 341, 11, 291, 820, 458, 1203, 291, 643, 281, 458, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1187395151110663, "compression_ratio": 1.8807692307692307, "no_speech_prob": 7.964256656123325e-05}, {"id": 61, "seek": 16600, "start": 184.0, "end": 186.0, "text": " And then there you can click on Docs, right?", "tokens": [50364, 865, 11, 370, 291, 393, 352, 281, 28848, 49040, 11, 558, 30, 50464, 50464, 407, 341, 307, 485, 50614, 50614, 286, 914, 11, 286, 519, 1391, 264, 12889, 551, 307, 281, 352, 281, 9953, 51, 284, 339, 28848, 13, 1301, 510, 13, 50814, 50814, 407, 321, 362, 264, 11202, 3028, 456, 13, 50914, 50914, 400, 550, 321, 362, 1203, 11, 14333, 11, 2145, 11, 51114, 51114, 466, 28848, 11, 466, 264, 1469, 11, 293, 1203, 1646, 13, 51264, 51264, 400, 550, 456, 291, 393, 2052, 322, 16024, 82, 11, 558, 30, 51364, 51364, 400, 550, 352, 2997, 281, 264, 45623, 11, 51464, 51464, 483, 1409, 510, 294, 264, 28848, 294, 732, 4439, 11, 1401, 341, 13, 51614, 51614, 400, 412, 264, 917, 295, 341, 11, 291, 820, 458, 1203, 291, 643, 281, 458, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1187395151110663, "compression_ratio": 1.8807692307692307, "no_speech_prob": 7.964256656123325e-05}, {"id": 62, "seek": 16600, "start": 186.0, "end": 188.0, "text": " And then go straight to the docs,", "tokens": [50364, 865, 11, 370, 291, 393, 352, 281, 28848, 49040, 11, 558, 30, 50464, 50464, 407, 341, 307, 485, 50614, 50614, 286, 914, 11, 286, 519, 1391, 264, 12889, 551, 307, 281, 352, 281, 9953, 51, 284, 339, 28848, 13, 1301, 510, 13, 50814, 50814, 407, 321, 362, 264, 11202, 3028, 456, 13, 50914, 50914, 400, 550, 321, 362, 1203, 11, 14333, 11, 2145, 11, 51114, 51114, 466, 28848, 11, 466, 264, 1469, 11, 293, 1203, 1646, 13, 51264, 51264, 400, 550, 456, 291, 393, 2052, 322, 16024, 82, 11, 558, 30, 51364, 51364, 400, 550, 352, 2997, 281, 264, 45623, 11, 51464, 51464, 483, 1409, 510, 294, 264, 28848, 294, 732, 4439, 11, 1401, 341, 13, 51614, 51614, 400, 412, 264, 917, 295, 341, 11, 291, 820, 458, 1203, 291, 643, 281, 458, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1187395151110663, "compression_ratio": 1.8807692307692307, "no_speech_prob": 7.964256656123325e-05}, {"id": 63, "seek": 16600, "start": 188.0, "end": 191.0, "text": " get started here in the Lightning in two steps, read this.", "tokens": [50364, 865, 11, 370, 291, 393, 352, 281, 28848, 49040, 11, 558, 30, 50464, 50464, 407, 341, 307, 485, 50614, 50614, 286, 914, 11, 286, 519, 1391, 264, 12889, 551, 307, 281, 352, 281, 9953, 51, 284, 339, 28848, 13, 1301, 510, 13, 50814, 50814, 407, 321, 362, 264, 11202, 3028, 456, 13, 50914, 50914, 400, 550, 321, 362, 1203, 11, 14333, 11, 2145, 11, 51114, 51114, 466, 28848, 11, 466, 264, 1469, 11, 293, 1203, 1646, 13, 51264, 51264, 400, 550, 456, 291, 393, 2052, 322, 16024, 82, 11, 558, 30, 51364, 51364, 400, 550, 352, 2997, 281, 264, 45623, 11, 51464, 51464, 483, 1409, 510, 294, 264, 28848, 294, 732, 4439, 11, 1401, 341, 13, 51614, 51614, 400, 412, 264, 917, 295, 341, 11, 291, 820, 458, 1203, 291, 643, 281, 458, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1187395151110663, "compression_ratio": 1.8807692307692307, "no_speech_prob": 7.964256656123325e-05}, {"id": 64, "seek": 16600, "start": 191.0, "end": 193.0, "text": " And at the end of this, you should know everything you need to know.", "tokens": [50364, 865, 11, 370, 291, 393, 352, 281, 28848, 49040, 11, 558, 30, 50464, 50464, 407, 341, 307, 485, 50614, 50614, 286, 914, 11, 286, 519, 1391, 264, 12889, 551, 307, 281, 352, 281, 9953, 51, 284, 339, 28848, 13, 1301, 510, 13, 50814, 50814, 407, 321, 362, 264, 11202, 3028, 456, 13, 50914, 50914, 400, 550, 321, 362, 1203, 11, 14333, 11, 2145, 11, 51114, 51114, 466, 28848, 11, 466, 264, 1469, 11, 293, 1203, 1646, 13, 51264, 51264, 400, 550, 456, 291, 393, 2052, 322, 16024, 82, 11, 558, 30, 51364, 51364, 400, 550, 352, 2997, 281, 264, 45623, 11, 51464, 51464, 483, 1409, 510, 294, 264, 28848, 294, 732, 4439, 11, 1401, 341, 13, 51614, 51614, 400, 412, 264, 917, 295, 341, 11, 291, 820, 458, 1203, 291, 643, 281, 458, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1187395151110663, "compression_ratio": 1.8807692307692307, "no_speech_prob": 7.964256656123325e-05}, {"id": 65, "seek": 19300, "start": 193.0, "end": 197.0, "text": " Again, if you know PyTorch, this should take like four minutes to understand.", "tokens": [50364, 3764, 11, 498, 291, 458, 9953, 51, 284, 339, 11, 341, 820, 747, 411, 1451, 2077, 281, 1223, 13, 50564, 50564, 1033, 13, 50614, 50614, 13813, 264, 18127, 30, 50714, 50714, 865, 11, 293, 550, 18127, 307, 516, 281, 312, 485, 50864, 50864, 961, 311, 536, 498, 321, 362, 309, 322, 264, 3028, 382, 731, 13, 51114, 51114, 961, 311, 536, 13, 51214, 51214, 865, 11, 370, 321, 362, 264, 18127, 510, 13, 51314, 51314, 407, 291, 393, 2052, 322, 300, 293, 550, 3441, 14331, 1373, 13, 51414, 51414, 400, 550, 341, 307, 264, 14333, 382, 731, 11, 558, 30, 51564, 51564, 407, 291, 393, 2052, 322, 309, 11, 574, 412, 264, 9339, 5934, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11817023333381205, "compression_ratio": 1.6695652173913043, "no_speech_prob": 4.610809628502466e-05}, {"id": 66, "seek": 19300, "start": 197.0, "end": 198.0, "text": " Okay.", "tokens": [50364, 3764, 11, 498, 291, 458, 9953, 51, 284, 339, 11, 341, 820, 747, 411, 1451, 2077, 281, 1223, 13, 50564, 50564, 1033, 13, 50614, 50614, 13813, 264, 18127, 30, 50714, 50714, 865, 11, 293, 550, 18127, 307, 516, 281, 312, 485, 50864, 50864, 961, 311, 536, 498, 321, 362, 309, 322, 264, 3028, 382, 731, 13, 51114, 51114, 961, 311, 536, 13, 51214, 51214, 865, 11, 370, 321, 362, 264, 18127, 510, 13, 51314, 51314, 407, 291, 393, 2052, 322, 300, 293, 550, 3441, 14331, 1373, 13, 51414, 51414, 400, 550, 341, 307, 264, 14333, 382, 731, 11, 558, 30, 51564, 51564, 407, 291, 393, 2052, 322, 309, 11, 574, 412, 264, 9339, 5934, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11817023333381205, "compression_ratio": 1.6695652173913043, "no_speech_prob": 4.610809628502466e-05}, {"id": 67, "seek": 19300, "start": 198.0, "end": 200.0, "text": " Whereas the bolts?", "tokens": [50364, 3764, 11, 498, 291, 458, 9953, 51, 284, 339, 11, 341, 820, 747, 411, 1451, 2077, 281, 1223, 13, 50564, 50564, 1033, 13, 50614, 50614, 13813, 264, 18127, 30, 50714, 50714, 865, 11, 293, 550, 18127, 307, 516, 281, 312, 485, 50864, 50864, 961, 311, 536, 498, 321, 362, 309, 322, 264, 3028, 382, 731, 13, 51114, 51114, 961, 311, 536, 13, 51214, 51214, 865, 11, 370, 321, 362, 264, 18127, 510, 13, 51314, 51314, 407, 291, 393, 2052, 322, 300, 293, 550, 3441, 14331, 1373, 13, 51414, 51414, 400, 550, 341, 307, 264, 14333, 382, 731, 11, 558, 30, 51564, 51564, 407, 291, 393, 2052, 322, 309, 11, 574, 412, 264, 9339, 5934, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11817023333381205, "compression_ratio": 1.6695652173913043, "no_speech_prob": 4.610809628502466e-05}, {"id": 68, "seek": 19300, "start": 200.0, "end": 203.0, "text": " Yeah, and then bolts is going to be...", "tokens": [50364, 3764, 11, 498, 291, 458, 9953, 51, 284, 339, 11, 341, 820, 747, 411, 1451, 2077, 281, 1223, 13, 50564, 50564, 1033, 13, 50614, 50614, 13813, 264, 18127, 30, 50714, 50714, 865, 11, 293, 550, 18127, 307, 516, 281, 312, 485, 50864, 50864, 961, 311, 536, 498, 321, 362, 309, 322, 264, 3028, 382, 731, 13, 51114, 51114, 961, 311, 536, 13, 51214, 51214, 865, 11, 370, 321, 362, 264, 18127, 510, 13, 51314, 51314, 407, 291, 393, 2052, 322, 300, 293, 550, 3441, 14331, 1373, 13, 51414, 51414, 400, 550, 341, 307, 264, 14333, 382, 731, 11, 558, 30, 51564, 51564, 407, 291, 393, 2052, 322, 309, 11, 574, 412, 264, 9339, 5934, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11817023333381205, "compression_ratio": 1.6695652173913043, "no_speech_prob": 4.610809628502466e-05}, {"id": 69, "seek": 19300, "start": 203.0, "end": 208.0, "text": " Let's see if we have it on the page as well.", "tokens": [50364, 3764, 11, 498, 291, 458, 9953, 51, 284, 339, 11, 341, 820, 747, 411, 1451, 2077, 281, 1223, 13, 50564, 50564, 1033, 13, 50614, 50614, 13813, 264, 18127, 30, 50714, 50714, 865, 11, 293, 550, 18127, 307, 516, 281, 312, 485, 50864, 50864, 961, 311, 536, 498, 321, 362, 309, 322, 264, 3028, 382, 731, 13, 51114, 51114, 961, 311, 536, 13, 51214, 51214, 865, 11, 370, 321, 362, 264, 18127, 510, 13, 51314, 51314, 407, 291, 393, 2052, 322, 300, 293, 550, 3441, 14331, 1373, 13, 51414, 51414, 400, 550, 341, 307, 264, 14333, 382, 731, 11, 558, 30, 51564, 51564, 407, 291, 393, 2052, 322, 309, 11, 574, 412, 264, 9339, 5934, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11817023333381205, "compression_ratio": 1.6695652173913043, "no_speech_prob": 4.610809628502466e-05}, {"id": 70, "seek": 19300, "start": 208.0, "end": 210.0, "text": " Let's see.", "tokens": [50364, 3764, 11, 498, 291, 458, 9953, 51, 284, 339, 11, 341, 820, 747, 411, 1451, 2077, 281, 1223, 13, 50564, 50564, 1033, 13, 50614, 50614, 13813, 264, 18127, 30, 50714, 50714, 865, 11, 293, 550, 18127, 307, 516, 281, 312, 485, 50864, 50864, 961, 311, 536, 498, 321, 362, 309, 322, 264, 3028, 382, 731, 13, 51114, 51114, 961, 311, 536, 13, 51214, 51214, 865, 11, 370, 321, 362, 264, 18127, 510, 13, 51314, 51314, 407, 291, 393, 2052, 322, 300, 293, 550, 3441, 14331, 1373, 13, 51414, 51414, 400, 550, 341, 307, 264, 14333, 382, 731, 11, 558, 30, 51564, 51564, 407, 291, 393, 2052, 322, 309, 11, 574, 412, 264, 9339, 5934, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11817023333381205, "compression_ratio": 1.6695652173913043, "no_speech_prob": 4.610809628502466e-05}, {"id": 71, "seek": 19300, "start": 210.0, "end": 212.0, "text": " Yeah, so we have the bolts here.", "tokens": [50364, 3764, 11, 498, 291, 458, 9953, 51, 284, 339, 11, 341, 820, 747, 411, 1451, 2077, 281, 1223, 13, 50564, 50564, 1033, 13, 50614, 50614, 13813, 264, 18127, 30, 50714, 50714, 865, 11, 293, 550, 18127, 307, 516, 281, 312, 485, 50864, 50864, 961, 311, 536, 498, 321, 362, 309, 322, 264, 3028, 382, 731, 13, 51114, 51114, 961, 311, 536, 13, 51214, 51214, 865, 11, 370, 321, 362, 264, 18127, 510, 13, 51314, 51314, 407, 291, 393, 2052, 322, 300, 293, 550, 3441, 14331, 1373, 13, 51414, 51414, 400, 550, 341, 307, 264, 14333, 382, 731, 11, 558, 30, 51564, 51564, 407, 291, 393, 2052, 322, 309, 11, 574, 412, 264, 9339, 5934, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11817023333381205, "compression_ratio": 1.6695652173913043, "no_speech_prob": 4.610809628502466e-05}, {"id": 72, "seek": 19300, "start": 212.0, "end": 214.0, "text": " So you can click on that and then visit Bolts.", "tokens": [50364, 3764, 11, 498, 291, 458, 9953, 51, 284, 339, 11, 341, 820, 747, 411, 1451, 2077, 281, 1223, 13, 50564, 50564, 1033, 13, 50614, 50614, 13813, 264, 18127, 30, 50714, 50714, 865, 11, 293, 550, 18127, 307, 516, 281, 312, 485, 50864, 50864, 961, 311, 536, 498, 321, 362, 309, 322, 264, 3028, 382, 731, 13, 51114, 51114, 961, 311, 536, 13, 51214, 51214, 865, 11, 370, 321, 362, 264, 18127, 510, 13, 51314, 51314, 407, 291, 393, 2052, 322, 300, 293, 550, 3441, 14331, 1373, 13, 51414, 51414, 400, 550, 341, 307, 264, 14333, 382, 731, 11, 558, 30, 51564, 51564, 407, 291, 393, 2052, 322, 309, 11, 574, 412, 264, 9339, 5934, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11817023333381205, "compression_ratio": 1.6695652173913043, "no_speech_prob": 4.610809628502466e-05}, {"id": 73, "seek": 19300, "start": 214.0, "end": 217.0, "text": " And then this is the documentation as well, right?", "tokens": [50364, 3764, 11, 498, 291, 458, 9953, 51, 284, 339, 11, 341, 820, 747, 411, 1451, 2077, 281, 1223, 13, 50564, 50564, 1033, 13, 50614, 50614, 13813, 264, 18127, 30, 50714, 50714, 865, 11, 293, 550, 18127, 307, 516, 281, 312, 485, 50864, 50864, 961, 311, 536, 498, 321, 362, 309, 322, 264, 3028, 382, 731, 13, 51114, 51114, 961, 311, 536, 13, 51214, 51214, 865, 11, 370, 321, 362, 264, 18127, 510, 13, 51314, 51314, 407, 291, 393, 2052, 322, 300, 293, 550, 3441, 14331, 1373, 13, 51414, 51414, 400, 550, 341, 307, 264, 14333, 382, 731, 11, 558, 30, 51564, 51564, 407, 291, 393, 2052, 322, 309, 11, 574, 412, 264, 9339, 5934, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11817023333381205, "compression_ratio": 1.6695652173913043, "no_speech_prob": 4.610809628502466e-05}, {"id": 74, "seek": 19300, "start": 217.0, "end": 220.0, "text": " So you can click on it, look at the introduction guide,", "tokens": [50364, 3764, 11, 498, 291, 458, 9953, 51, 284, 339, 11, 341, 820, 747, 411, 1451, 2077, 281, 1223, 13, 50564, 50564, 1033, 13, 50614, 50614, 13813, 264, 18127, 30, 50714, 50714, 865, 11, 293, 550, 18127, 307, 516, 281, 312, 485, 50864, 50864, 961, 311, 536, 498, 321, 362, 309, 322, 264, 3028, 382, 731, 13, 51114, 51114, 961, 311, 536, 13, 51214, 51214, 865, 11, 370, 321, 362, 264, 18127, 510, 13, 51314, 51314, 407, 291, 393, 2052, 322, 300, 293, 550, 3441, 14331, 1373, 13, 51414, 51414, 400, 550, 341, 307, 264, 14333, 382, 731, 11, 558, 30, 51564, 51564, 407, 291, 393, 2052, 322, 309, 11, 574, 412, 264, 9339, 5934, 11, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.11817023333381205, "compression_ratio": 1.6695652173913043, "no_speech_prob": 4.610809628502466e-05}, {"id": 75, "seek": 22000, "start": 220.0, "end": 223.0, "text": " and then we will walk you through the main ideas about bolts.", "tokens": [50364, 293, 550, 321, 486, 1792, 291, 807, 264, 2135, 3487, 466, 18127, 13, 50514, 50514, 407, 18127, 307, 257, 17628, 1716, 11, 370, 309, 307, 4663, 2684, 13, 50664, 50664, 407, 309, 486, 1319, 1238, 10374, 13, 50764, 50764, 28848, 307, 502, 13, 15, 11, 309, 311, 8351, 11, 50914, 50914, 370, 841, 1737, 281, 764, 309, 337, 2035, 291, 528, 382, 731, 13, 51064, 51064, 1057, 558, 13, 51164, 51164, 1033, 11, 370, 20762, 341, 11, 286, 1697, 286, 478, 322, 257, 18407, 5197, 13, 51514, 51514, 865, 11, 264, 1627, 551, 307, 28848, 11, 291, 393, 767, 764, 314, 8115, 82, 293, 19462, 11, 51664, 51664, 457, 286, 478, 516, 281, 764, 19462, 558, 586, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09755419903114194, "compression_ratio": 1.5606060606060606, "no_speech_prob": 2.078376928693615e-05}, {"id": 76, "seek": 22000, "start": 223.0, "end": 226.0, "text": " So bolts is a newer project, so it is faster moving.", "tokens": [50364, 293, 550, 321, 486, 1792, 291, 807, 264, 2135, 3487, 466, 18127, 13, 50514, 50514, 407, 18127, 307, 257, 17628, 1716, 11, 370, 309, 307, 4663, 2684, 13, 50664, 50664, 407, 309, 486, 1319, 1238, 10374, 13, 50764, 50764, 28848, 307, 502, 13, 15, 11, 309, 311, 8351, 11, 50914, 50914, 370, 841, 1737, 281, 764, 309, 337, 2035, 291, 528, 382, 731, 13, 51064, 51064, 1057, 558, 13, 51164, 51164, 1033, 11, 370, 20762, 341, 11, 286, 1697, 286, 478, 322, 257, 18407, 5197, 13, 51514, 51514, 865, 11, 264, 1627, 551, 307, 28848, 11, 291, 393, 767, 764, 314, 8115, 82, 293, 19462, 11, 51664, 51664, 457, 286, 478, 516, 281, 764, 19462, 558, 586, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09755419903114194, "compression_ratio": 1.5606060606060606, "no_speech_prob": 2.078376928693615e-05}, {"id": 77, "seek": 22000, "start": 226.0, "end": 228.0, "text": " So it will change pretty frequently.", "tokens": [50364, 293, 550, 321, 486, 1792, 291, 807, 264, 2135, 3487, 466, 18127, 13, 50514, 50514, 407, 18127, 307, 257, 17628, 1716, 11, 370, 309, 307, 4663, 2684, 13, 50664, 50664, 407, 309, 486, 1319, 1238, 10374, 13, 50764, 50764, 28848, 307, 502, 13, 15, 11, 309, 311, 8351, 11, 50914, 50914, 370, 841, 1737, 281, 764, 309, 337, 2035, 291, 528, 382, 731, 13, 51064, 51064, 1057, 558, 13, 51164, 51164, 1033, 11, 370, 20762, 341, 11, 286, 1697, 286, 478, 322, 257, 18407, 5197, 13, 51514, 51514, 865, 11, 264, 1627, 551, 307, 28848, 11, 291, 393, 767, 764, 314, 8115, 82, 293, 19462, 11, 51664, 51664, 457, 286, 478, 516, 281, 764, 19462, 558, 586, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09755419903114194, "compression_ratio": 1.5606060606060606, "no_speech_prob": 2.078376928693615e-05}, {"id": 78, "seek": 22000, "start": 228.0, "end": 231.0, "text": " Lightning is 1.0, it's stable,", "tokens": [50364, 293, 550, 321, 486, 1792, 291, 807, 264, 2135, 3487, 466, 18127, 13, 50514, 50514, 407, 18127, 307, 257, 17628, 1716, 11, 370, 309, 307, 4663, 2684, 13, 50664, 50664, 407, 309, 486, 1319, 1238, 10374, 13, 50764, 50764, 28848, 307, 502, 13, 15, 11, 309, 311, 8351, 11, 50914, 50914, 370, 841, 1737, 281, 764, 309, 337, 2035, 291, 528, 382, 731, 13, 51064, 51064, 1057, 558, 13, 51164, 51164, 1033, 11, 370, 20762, 341, 11, 286, 1697, 286, 478, 322, 257, 18407, 5197, 13, 51514, 51514, 865, 11, 264, 1627, 551, 307, 28848, 11, 291, 393, 767, 764, 314, 8115, 82, 293, 19462, 11, 51664, 51664, 457, 286, 478, 516, 281, 764, 19462, 558, 586, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09755419903114194, "compression_ratio": 1.5606060606060606, "no_speech_prob": 2.078376928693615e-05}, {"id": 79, "seek": 22000, "start": 231.0, "end": 234.0, "text": " so feel free to use it for whatever you want as well.", "tokens": [50364, 293, 550, 321, 486, 1792, 291, 807, 264, 2135, 3487, 466, 18127, 13, 50514, 50514, 407, 18127, 307, 257, 17628, 1716, 11, 370, 309, 307, 4663, 2684, 13, 50664, 50664, 407, 309, 486, 1319, 1238, 10374, 13, 50764, 50764, 28848, 307, 502, 13, 15, 11, 309, 311, 8351, 11, 50914, 50914, 370, 841, 1737, 281, 764, 309, 337, 2035, 291, 528, 382, 731, 13, 51064, 51064, 1057, 558, 13, 51164, 51164, 1033, 11, 370, 20762, 341, 11, 286, 1697, 286, 478, 322, 257, 18407, 5197, 13, 51514, 51514, 865, 11, 264, 1627, 551, 307, 28848, 11, 291, 393, 767, 764, 314, 8115, 82, 293, 19462, 11, 51664, 51664, 457, 286, 478, 516, 281, 764, 19462, 558, 586, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09755419903114194, "compression_ratio": 1.5606060606060606, "no_speech_prob": 2.078376928693615e-05}, {"id": 80, "seek": 22000, "start": 234.0, "end": 236.0, "text": " All right.", "tokens": [50364, 293, 550, 321, 486, 1792, 291, 807, 264, 2135, 3487, 466, 18127, 13, 50514, 50514, 407, 18127, 307, 257, 17628, 1716, 11, 370, 309, 307, 4663, 2684, 13, 50664, 50664, 407, 309, 486, 1319, 1238, 10374, 13, 50764, 50764, 28848, 307, 502, 13, 15, 11, 309, 311, 8351, 11, 50914, 50914, 370, 841, 1737, 281, 764, 309, 337, 2035, 291, 528, 382, 731, 13, 51064, 51064, 1057, 558, 13, 51164, 51164, 1033, 11, 370, 20762, 341, 11, 286, 1697, 286, 478, 322, 257, 18407, 5197, 13, 51514, 51514, 865, 11, 264, 1627, 551, 307, 28848, 11, 291, 393, 767, 764, 314, 8115, 82, 293, 19462, 11, 51664, 51664, 457, 286, 478, 516, 281, 764, 19462, 558, 586, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09755419903114194, "compression_ratio": 1.5606060606060606, "no_speech_prob": 2.078376928693615e-05}, {"id": 81, "seek": 22000, "start": 236.0, "end": 243.0, "text": " Okay, so installing this, I believe I'm on a GPU instance.", "tokens": [50364, 293, 550, 321, 486, 1792, 291, 807, 264, 2135, 3487, 466, 18127, 13, 50514, 50514, 407, 18127, 307, 257, 17628, 1716, 11, 370, 309, 307, 4663, 2684, 13, 50664, 50664, 407, 309, 486, 1319, 1238, 10374, 13, 50764, 50764, 28848, 307, 502, 13, 15, 11, 309, 311, 8351, 11, 50914, 50914, 370, 841, 1737, 281, 764, 309, 337, 2035, 291, 528, 382, 731, 13, 51064, 51064, 1057, 558, 13, 51164, 51164, 1033, 11, 370, 20762, 341, 11, 286, 1697, 286, 478, 322, 257, 18407, 5197, 13, 51514, 51514, 865, 11, 264, 1627, 551, 307, 28848, 11, 291, 393, 767, 764, 314, 8115, 82, 293, 19462, 11, 51664, 51664, 457, 286, 478, 516, 281, 764, 19462, 558, 586, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09755419903114194, "compression_ratio": 1.5606060606060606, "no_speech_prob": 2.078376928693615e-05}, {"id": 82, "seek": 22000, "start": 243.0, "end": 246.0, "text": " Yeah, the cool thing is Lightning, you can actually use TPUs and GPS,", "tokens": [50364, 293, 550, 321, 486, 1792, 291, 807, 264, 2135, 3487, 466, 18127, 13, 50514, 50514, 407, 18127, 307, 257, 17628, 1716, 11, 370, 309, 307, 4663, 2684, 13, 50664, 50664, 407, 309, 486, 1319, 1238, 10374, 13, 50764, 50764, 28848, 307, 502, 13, 15, 11, 309, 311, 8351, 11, 50914, 50914, 370, 841, 1737, 281, 764, 309, 337, 2035, 291, 528, 382, 731, 13, 51064, 51064, 1057, 558, 13, 51164, 51164, 1033, 11, 370, 20762, 341, 11, 286, 1697, 286, 478, 322, 257, 18407, 5197, 13, 51514, 51514, 865, 11, 264, 1627, 551, 307, 28848, 11, 291, 393, 767, 764, 314, 8115, 82, 293, 19462, 11, 51664, 51664, 457, 286, 478, 516, 281, 764, 19462, 558, 586, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09755419903114194, "compression_ratio": 1.5606060606060606, "no_speech_prob": 2.078376928693615e-05}, {"id": 83, "seek": 22000, "start": 246.0, "end": 248.0, "text": " but I'm going to use GPS right now.", "tokens": [50364, 293, 550, 321, 486, 1792, 291, 807, 264, 2135, 3487, 466, 18127, 13, 50514, 50514, 407, 18127, 307, 257, 17628, 1716, 11, 370, 309, 307, 4663, 2684, 13, 50664, 50664, 407, 309, 486, 1319, 1238, 10374, 13, 50764, 50764, 28848, 307, 502, 13, 15, 11, 309, 311, 8351, 11, 50914, 50914, 370, 841, 1737, 281, 764, 309, 337, 2035, 291, 528, 382, 731, 13, 51064, 51064, 1057, 558, 13, 51164, 51164, 1033, 11, 370, 20762, 341, 11, 286, 1697, 286, 478, 322, 257, 18407, 5197, 13, 51514, 51514, 865, 11, 264, 1627, 551, 307, 28848, 11, 291, 393, 767, 764, 314, 8115, 82, 293, 19462, 11, 51664, 51664, 457, 286, 478, 516, 281, 764, 19462, 558, 586, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09755419903114194, "compression_ratio": 1.5606060606060606, "no_speech_prob": 2.078376928693615e-05}, {"id": 84, "seek": 24800, "start": 248.0, "end": 251.0, "text": " TPUs are dodgy sometimes.", "tokens": [50364, 314, 8115, 82, 366, 13886, 1480, 2171, 13, 50514, 50514, 1033, 11, 370, 264, 700, 551, 286, 528, 281, 2060, 307, 11, 50664, 50664, 411, 11, 562, 360, 291, 528, 281, 2489, 12, 83, 2613, 11, 558, 30, 50814, 50814, 407, 286, 519, 300, 456, 366, 1629, 721, 291, 393, 1029, 1803, 281, 1223, 51014, 51014, 498, 437, 291, 434, 466, 281, 360, 307, 11, 291, 458, 11, 309, 311, 4420, 337, 2489, 12, 83, 37726, 13, 51214, 51214, 407, 718, 385, 9163, 341, 3256, 510, 13, 51364, 51364, 407, 286, 1027, 257, 707, 10686, 337, 1518, 13, 51464, 51464, 1033, 11, 3231, 13, 51514, 51514, 407, 718, 311, 722, 365, 264, 3092, 10681, 11, 558, 30, 51614, 51614, 407, 286, 519, 264, 700, 1168, 307, 11, 360, 291, 362, 257, 688, 295, 1412, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08906243001814369, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.00011407663987483829}, {"id": 85, "seek": 24800, "start": 251.0, "end": 254.0, "text": " Okay, so the first thing I want to cover is,", "tokens": [50364, 314, 8115, 82, 366, 13886, 1480, 2171, 13, 50514, 50514, 1033, 11, 370, 264, 700, 551, 286, 528, 281, 2060, 307, 11, 50664, 50664, 411, 11, 562, 360, 291, 528, 281, 2489, 12, 83, 2613, 11, 558, 30, 50814, 50814, 407, 286, 519, 300, 456, 366, 1629, 721, 291, 393, 1029, 1803, 281, 1223, 51014, 51014, 498, 437, 291, 434, 466, 281, 360, 307, 11, 291, 458, 11, 309, 311, 4420, 337, 2489, 12, 83, 37726, 13, 51214, 51214, 407, 718, 385, 9163, 341, 3256, 510, 13, 51364, 51364, 407, 286, 1027, 257, 707, 10686, 337, 1518, 13, 51464, 51464, 1033, 11, 3231, 13, 51514, 51514, 407, 718, 311, 722, 365, 264, 3092, 10681, 11, 558, 30, 51614, 51614, 407, 286, 519, 264, 700, 1168, 307, 11, 360, 291, 362, 257, 688, 295, 1412, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08906243001814369, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.00011407663987483829}, {"id": 86, "seek": 24800, "start": 254.0, "end": 257.0, "text": " like, when do you want to fine-tune, right?", "tokens": [50364, 314, 8115, 82, 366, 13886, 1480, 2171, 13, 50514, 50514, 1033, 11, 370, 264, 700, 551, 286, 528, 281, 2060, 307, 11, 50664, 50664, 411, 11, 562, 360, 291, 528, 281, 2489, 12, 83, 2613, 11, 558, 30, 50814, 50814, 407, 286, 519, 300, 456, 366, 1629, 721, 291, 393, 1029, 1803, 281, 1223, 51014, 51014, 498, 437, 291, 434, 466, 281, 360, 307, 11, 291, 458, 11, 309, 311, 4420, 337, 2489, 12, 83, 37726, 13, 51214, 51214, 407, 718, 385, 9163, 341, 3256, 510, 13, 51364, 51364, 407, 286, 1027, 257, 707, 10686, 337, 1518, 13, 51464, 51464, 1033, 11, 3231, 13, 51514, 51514, 407, 718, 311, 722, 365, 264, 3092, 10681, 11, 558, 30, 51614, 51614, 407, 286, 519, 264, 700, 1168, 307, 11, 360, 291, 362, 257, 688, 295, 1412, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08906243001814369, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.00011407663987483829}, {"id": 87, "seek": 24800, "start": 257.0, "end": 261.0, "text": " So I think that there are certain things you can ask yourself to understand", "tokens": [50364, 314, 8115, 82, 366, 13886, 1480, 2171, 13, 50514, 50514, 1033, 11, 370, 264, 700, 551, 286, 528, 281, 2060, 307, 11, 50664, 50664, 411, 11, 562, 360, 291, 528, 281, 2489, 12, 83, 2613, 11, 558, 30, 50814, 50814, 407, 286, 519, 300, 456, 366, 1629, 721, 291, 393, 1029, 1803, 281, 1223, 51014, 51014, 498, 437, 291, 434, 466, 281, 360, 307, 11, 291, 458, 11, 309, 311, 4420, 337, 2489, 12, 83, 37726, 13, 51214, 51214, 407, 718, 385, 9163, 341, 3256, 510, 13, 51364, 51364, 407, 286, 1027, 257, 707, 10686, 337, 1518, 13, 51464, 51464, 1033, 11, 3231, 13, 51514, 51514, 407, 718, 311, 722, 365, 264, 3092, 10681, 11, 558, 30, 51614, 51614, 407, 286, 519, 264, 700, 1168, 307, 11, 360, 291, 362, 257, 688, 295, 1412, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08906243001814369, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.00011407663987483829}, {"id": 88, "seek": 24800, "start": 261.0, "end": 265.0, "text": " if what you're about to do is, you know, it's useful for fine-tuning.", "tokens": [50364, 314, 8115, 82, 366, 13886, 1480, 2171, 13, 50514, 50514, 1033, 11, 370, 264, 700, 551, 286, 528, 281, 2060, 307, 11, 50664, 50664, 411, 11, 562, 360, 291, 528, 281, 2489, 12, 83, 2613, 11, 558, 30, 50814, 50814, 407, 286, 519, 300, 456, 366, 1629, 721, 291, 393, 1029, 1803, 281, 1223, 51014, 51014, 498, 437, 291, 434, 466, 281, 360, 307, 11, 291, 458, 11, 309, 311, 4420, 337, 2489, 12, 83, 37726, 13, 51214, 51214, 407, 718, 385, 9163, 341, 3256, 510, 13, 51364, 51364, 407, 286, 1027, 257, 707, 10686, 337, 1518, 13, 51464, 51464, 1033, 11, 3231, 13, 51514, 51514, 407, 718, 311, 722, 365, 264, 3092, 10681, 11, 558, 30, 51614, 51614, 407, 286, 519, 264, 700, 1168, 307, 11, 360, 291, 362, 257, 688, 295, 1412, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08906243001814369, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.00011407663987483829}, {"id": 89, "seek": 24800, "start": 265.0, "end": 268.0, "text": " So let me paste this image here.", "tokens": [50364, 314, 8115, 82, 366, 13886, 1480, 2171, 13, 50514, 50514, 1033, 11, 370, 264, 700, 551, 286, 528, 281, 2060, 307, 11, 50664, 50664, 411, 11, 562, 360, 291, 528, 281, 2489, 12, 83, 2613, 11, 558, 30, 50814, 50814, 407, 286, 519, 300, 456, 366, 1629, 721, 291, 393, 1029, 1803, 281, 1223, 51014, 51014, 498, 437, 291, 434, 466, 281, 360, 307, 11, 291, 458, 11, 309, 311, 4420, 337, 2489, 12, 83, 37726, 13, 51214, 51214, 407, 718, 385, 9163, 341, 3256, 510, 13, 51364, 51364, 407, 286, 1027, 257, 707, 10686, 337, 1518, 13, 51464, 51464, 1033, 11, 3231, 13, 51514, 51514, 407, 718, 311, 722, 365, 264, 3092, 10681, 11, 558, 30, 51614, 51614, 407, 286, 519, 264, 700, 1168, 307, 11, 360, 291, 362, 257, 688, 295, 1412, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08906243001814369, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.00011407663987483829}, {"id": 90, "seek": 24800, "start": 268.0, "end": 270.0, "text": " So I made a little diagram for everyone.", "tokens": [50364, 314, 8115, 82, 366, 13886, 1480, 2171, 13, 50514, 50514, 1033, 11, 370, 264, 700, 551, 286, 528, 281, 2060, 307, 11, 50664, 50664, 411, 11, 562, 360, 291, 528, 281, 2489, 12, 83, 2613, 11, 558, 30, 50814, 50814, 407, 286, 519, 300, 456, 366, 1629, 721, 291, 393, 1029, 1803, 281, 1223, 51014, 51014, 498, 437, 291, 434, 466, 281, 360, 307, 11, 291, 458, 11, 309, 311, 4420, 337, 2489, 12, 83, 37726, 13, 51214, 51214, 407, 718, 385, 9163, 341, 3256, 510, 13, 51364, 51364, 407, 286, 1027, 257, 707, 10686, 337, 1518, 13, 51464, 51464, 1033, 11, 3231, 13, 51514, 51514, 407, 718, 311, 722, 365, 264, 3092, 10681, 11, 558, 30, 51614, 51614, 407, 286, 519, 264, 700, 1168, 307, 11, 360, 291, 362, 257, 688, 295, 1412, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08906243001814369, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.00011407663987483829}, {"id": 91, "seek": 24800, "start": 270.0, "end": 271.0, "text": " Okay, thanks.", "tokens": [50364, 314, 8115, 82, 366, 13886, 1480, 2171, 13, 50514, 50514, 1033, 11, 370, 264, 700, 551, 286, 528, 281, 2060, 307, 11, 50664, 50664, 411, 11, 562, 360, 291, 528, 281, 2489, 12, 83, 2613, 11, 558, 30, 50814, 50814, 407, 286, 519, 300, 456, 366, 1629, 721, 291, 393, 1029, 1803, 281, 1223, 51014, 51014, 498, 437, 291, 434, 466, 281, 360, 307, 11, 291, 458, 11, 309, 311, 4420, 337, 2489, 12, 83, 37726, 13, 51214, 51214, 407, 718, 385, 9163, 341, 3256, 510, 13, 51364, 51364, 407, 286, 1027, 257, 707, 10686, 337, 1518, 13, 51464, 51464, 1033, 11, 3231, 13, 51514, 51514, 407, 718, 311, 722, 365, 264, 3092, 10681, 11, 558, 30, 51614, 51614, 407, 286, 519, 264, 700, 1168, 307, 11, 360, 291, 362, 257, 688, 295, 1412, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08906243001814369, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.00011407663987483829}, {"id": 92, "seek": 24800, "start": 271.0, "end": 273.0, "text": " So let's start with the green spots, right?", "tokens": [50364, 314, 8115, 82, 366, 13886, 1480, 2171, 13, 50514, 50514, 1033, 11, 370, 264, 700, 551, 286, 528, 281, 2060, 307, 11, 50664, 50664, 411, 11, 562, 360, 291, 528, 281, 2489, 12, 83, 2613, 11, 558, 30, 50814, 50814, 407, 286, 519, 300, 456, 366, 1629, 721, 291, 393, 1029, 1803, 281, 1223, 51014, 51014, 498, 437, 291, 434, 466, 281, 360, 307, 11, 291, 458, 11, 309, 311, 4420, 337, 2489, 12, 83, 37726, 13, 51214, 51214, 407, 718, 385, 9163, 341, 3256, 510, 13, 51364, 51364, 407, 286, 1027, 257, 707, 10686, 337, 1518, 13, 51464, 51464, 1033, 11, 3231, 13, 51514, 51514, 407, 718, 311, 722, 365, 264, 3092, 10681, 11, 558, 30, 51614, 51614, 407, 286, 519, 264, 700, 1168, 307, 11, 360, 291, 362, 257, 688, 295, 1412, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08906243001814369, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.00011407663987483829}, {"id": 93, "seek": 24800, "start": 273.0, "end": 276.0, "text": " So I think the first question is, do you have a lot of data?", "tokens": [50364, 314, 8115, 82, 366, 13886, 1480, 2171, 13, 50514, 50514, 1033, 11, 370, 264, 700, 551, 286, 528, 281, 2060, 307, 11, 50664, 50664, 411, 11, 562, 360, 291, 528, 281, 2489, 12, 83, 2613, 11, 558, 30, 50814, 50814, 407, 286, 519, 300, 456, 366, 1629, 721, 291, 393, 1029, 1803, 281, 1223, 51014, 51014, 498, 437, 291, 434, 466, 281, 360, 307, 11, 291, 458, 11, 309, 311, 4420, 337, 2489, 12, 83, 37726, 13, 51214, 51214, 407, 718, 385, 9163, 341, 3256, 510, 13, 51364, 51364, 407, 286, 1027, 257, 707, 10686, 337, 1518, 13, 51464, 51464, 1033, 11, 3231, 13, 51514, 51514, 407, 718, 311, 722, 365, 264, 3092, 10681, 11, 558, 30, 51614, 51614, 407, 286, 519, 264, 700, 1168, 307, 11, 360, 291, 362, 257, 688, 295, 1412, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.08906243001814369, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.00011407663987483829}, {"id": 94, "seek": 27600, "start": 276.0, "end": 280.0, "text": " If the answer is yes, then you likely don't need to fine-tune, right?", "tokens": [50364, 759, 264, 1867, 307, 2086, 11, 550, 291, 3700, 500, 380, 643, 281, 2489, 12, 83, 2613, 11, 558, 30, 50564, 50564, 823, 11, 360, 291, 611, 362, 565, 294, 715, 1819, 11, 558, 30, 50714, 50714, 509, 393, 362, 257, 688, 295, 1412, 11, 457, 498, 309, 311, 1687, 28328, 281, 3847, 11, 50814, 50814, 550, 291, 434, 516, 281, 528, 281, 915, 746, 659, 12, 17227, 2001, 13, 50964, 50964, 583, 498, 291, 362, 264, 1460, 293, 264, 565, 293, 264, 14722, 11, 51114, 51114, 550, 445, 352, 2286, 293, 3847, 322, 428, 1412, 11, 558, 30, 51214, 51214, 400, 562, 291, 434, 1096, 11, 1190, 322, 428, 1500, 1412, 992, 13, 51364, 51364, 823, 11, 498, 291, 500, 380, 362, 257, 688, 295, 1412, 11, 51464, 51464, 550, 291, 820, 853, 281, 915, 257, 659, 12, 17227, 2001, 2316, 337, 428, 1412, 51614, 51614, 300, 10676, 428, 1412, 7316, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04785415661409966, "compression_ratio": 1.9525547445255473, "no_speech_prob": 4.092652943654684e-06}, {"id": 95, "seek": 27600, "start": 280.0, "end": 283.0, "text": " Now, do you also have time in computes, right?", "tokens": [50364, 759, 264, 1867, 307, 2086, 11, 550, 291, 3700, 500, 380, 643, 281, 2489, 12, 83, 2613, 11, 558, 30, 50564, 50564, 823, 11, 360, 291, 611, 362, 565, 294, 715, 1819, 11, 558, 30, 50714, 50714, 509, 393, 362, 257, 688, 295, 1412, 11, 457, 498, 309, 311, 1687, 28328, 281, 3847, 11, 50814, 50814, 550, 291, 434, 516, 281, 528, 281, 915, 746, 659, 12, 17227, 2001, 13, 50964, 50964, 583, 498, 291, 362, 264, 1460, 293, 264, 565, 293, 264, 14722, 11, 51114, 51114, 550, 445, 352, 2286, 293, 3847, 322, 428, 1412, 11, 558, 30, 51214, 51214, 400, 562, 291, 434, 1096, 11, 1190, 322, 428, 1500, 1412, 992, 13, 51364, 51364, 823, 11, 498, 291, 500, 380, 362, 257, 688, 295, 1412, 11, 51464, 51464, 550, 291, 820, 853, 281, 915, 257, 659, 12, 17227, 2001, 2316, 337, 428, 1412, 51614, 51614, 300, 10676, 428, 1412, 7316, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04785415661409966, "compression_ratio": 1.9525547445255473, "no_speech_prob": 4.092652943654684e-06}, {"id": 96, "seek": 27600, "start": 283.0, "end": 285.0, "text": " You can have a lot of data, but if it's super costly to train,", "tokens": [50364, 759, 264, 1867, 307, 2086, 11, 550, 291, 3700, 500, 380, 643, 281, 2489, 12, 83, 2613, 11, 558, 30, 50564, 50564, 823, 11, 360, 291, 611, 362, 565, 294, 715, 1819, 11, 558, 30, 50714, 50714, 509, 393, 362, 257, 688, 295, 1412, 11, 457, 498, 309, 311, 1687, 28328, 281, 3847, 11, 50814, 50814, 550, 291, 434, 516, 281, 528, 281, 915, 746, 659, 12, 17227, 2001, 13, 50964, 50964, 583, 498, 291, 362, 264, 1460, 293, 264, 565, 293, 264, 14722, 11, 51114, 51114, 550, 445, 352, 2286, 293, 3847, 322, 428, 1412, 11, 558, 30, 51214, 51214, 400, 562, 291, 434, 1096, 11, 1190, 322, 428, 1500, 1412, 992, 13, 51364, 51364, 823, 11, 498, 291, 500, 380, 362, 257, 688, 295, 1412, 11, 51464, 51464, 550, 291, 820, 853, 281, 915, 257, 659, 12, 17227, 2001, 2316, 337, 428, 1412, 51614, 51614, 300, 10676, 428, 1412, 7316, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04785415661409966, "compression_ratio": 1.9525547445255473, "no_speech_prob": 4.092652943654684e-06}, {"id": 97, "seek": 27600, "start": 285.0, "end": 288.0, "text": " then you're going to want to find something pre-trained.", "tokens": [50364, 759, 264, 1867, 307, 2086, 11, 550, 291, 3700, 500, 380, 643, 281, 2489, 12, 83, 2613, 11, 558, 30, 50564, 50564, 823, 11, 360, 291, 611, 362, 565, 294, 715, 1819, 11, 558, 30, 50714, 50714, 509, 393, 362, 257, 688, 295, 1412, 11, 457, 498, 309, 311, 1687, 28328, 281, 3847, 11, 50814, 50814, 550, 291, 434, 516, 281, 528, 281, 915, 746, 659, 12, 17227, 2001, 13, 50964, 50964, 583, 498, 291, 362, 264, 1460, 293, 264, 565, 293, 264, 14722, 11, 51114, 51114, 550, 445, 352, 2286, 293, 3847, 322, 428, 1412, 11, 558, 30, 51214, 51214, 400, 562, 291, 434, 1096, 11, 1190, 322, 428, 1500, 1412, 992, 13, 51364, 51364, 823, 11, 498, 291, 500, 380, 362, 257, 688, 295, 1412, 11, 51464, 51464, 550, 291, 820, 853, 281, 915, 257, 659, 12, 17227, 2001, 2316, 337, 428, 1412, 51614, 51614, 300, 10676, 428, 1412, 7316, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04785415661409966, "compression_ratio": 1.9525547445255473, "no_speech_prob": 4.092652943654684e-06}, {"id": 98, "seek": 27600, "start": 288.0, "end": 291.0, "text": " But if you have the money and the time and the compute,", "tokens": [50364, 759, 264, 1867, 307, 2086, 11, 550, 291, 3700, 500, 380, 643, 281, 2489, 12, 83, 2613, 11, 558, 30, 50564, 50564, 823, 11, 360, 291, 611, 362, 565, 294, 715, 1819, 11, 558, 30, 50714, 50714, 509, 393, 362, 257, 688, 295, 1412, 11, 457, 498, 309, 311, 1687, 28328, 281, 3847, 11, 50814, 50814, 550, 291, 434, 516, 281, 528, 281, 915, 746, 659, 12, 17227, 2001, 13, 50964, 50964, 583, 498, 291, 362, 264, 1460, 293, 264, 565, 293, 264, 14722, 11, 51114, 51114, 550, 445, 352, 2286, 293, 3847, 322, 428, 1412, 11, 558, 30, 51214, 51214, 400, 562, 291, 434, 1096, 11, 1190, 322, 428, 1500, 1412, 992, 13, 51364, 51364, 823, 11, 498, 291, 500, 380, 362, 257, 688, 295, 1412, 11, 51464, 51464, 550, 291, 820, 853, 281, 915, 257, 659, 12, 17227, 2001, 2316, 337, 428, 1412, 51614, 51614, 300, 10676, 428, 1412, 7316, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04785415661409966, "compression_ratio": 1.9525547445255473, "no_speech_prob": 4.092652943654684e-06}, {"id": 99, "seek": 27600, "start": 291.0, "end": 293.0, "text": " then just go ahead and train on your data, right?", "tokens": [50364, 759, 264, 1867, 307, 2086, 11, 550, 291, 3700, 500, 380, 643, 281, 2489, 12, 83, 2613, 11, 558, 30, 50564, 50564, 823, 11, 360, 291, 611, 362, 565, 294, 715, 1819, 11, 558, 30, 50714, 50714, 509, 393, 362, 257, 688, 295, 1412, 11, 457, 498, 309, 311, 1687, 28328, 281, 3847, 11, 50814, 50814, 550, 291, 434, 516, 281, 528, 281, 915, 746, 659, 12, 17227, 2001, 13, 50964, 50964, 583, 498, 291, 362, 264, 1460, 293, 264, 565, 293, 264, 14722, 11, 51114, 51114, 550, 445, 352, 2286, 293, 3847, 322, 428, 1412, 11, 558, 30, 51214, 51214, 400, 562, 291, 434, 1096, 11, 1190, 322, 428, 1500, 1412, 992, 13, 51364, 51364, 823, 11, 498, 291, 500, 380, 362, 257, 688, 295, 1412, 11, 51464, 51464, 550, 291, 820, 853, 281, 915, 257, 659, 12, 17227, 2001, 2316, 337, 428, 1412, 51614, 51614, 300, 10676, 428, 1412, 7316, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04785415661409966, "compression_ratio": 1.9525547445255473, "no_speech_prob": 4.092652943654684e-06}, {"id": 100, "seek": 27600, "start": 293.0, "end": 296.0, "text": " And when you're done, run on your test data set.", "tokens": [50364, 759, 264, 1867, 307, 2086, 11, 550, 291, 3700, 500, 380, 643, 281, 2489, 12, 83, 2613, 11, 558, 30, 50564, 50564, 823, 11, 360, 291, 611, 362, 565, 294, 715, 1819, 11, 558, 30, 50714, 50714, 509, 393, 362, 257, 688, 295, 1412, 11, 457, 498, 309, 311, 1687, 28328, 281, 3847, 11, 50814, 50814, 550, 291, 434, 516, 281, 528, 281, 915, 746, 659, 12, 17227, 2001, 13, 50964, 50964, 583, 498, 291, 362, 264, 1460, 293, 264, 565, 293, 264, 14722, 11, 51114, 51114, 550, 445, 352, 2286, 293, 3847, 322, 428, 1412, 11, 558, 30, 51214, 51214, 400, 562, 291, 434, 1096, 11, 1190, 322, 428, 1500, 1412, 992, 13, 51364, 51364, 823, 11, 498, 291, 500, 380, 362, 257, 688, 295, 1412, 11, 51464, 51464, 550, 291, 820, 853, 281, 915, 257, 659, 12, 17227, 2001, 2316, 337, 428, 1412, 51614, 51614, 300, 10676, 428, 1412, 7316, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04785415661409966, "compression_ratio": 1.9525547445255473, "no_speech_prob": 4.092652943654684e-06}, {"id": 101, "seek": 27600, "start": 296.0, "end": 298.0, "text": " Now, if you don't have a lot of data,", "tokens": [50364, 759, 264, 1867, 307, 2086, 11, 550, 291, 3700, 500, 380, 643, 281, 2489, 12, 83, 2613, 11, 558, 30, 50564, 50564, 823, 11, 360, 291, 611, 362, 565, 294, 715, 1819, 11, 558, 30, 50714, 50714, 509, 393, 362, 257, 688, 295, 1412, 11, 457, 498, 309, 311, 1687, 28328, 281, 3847, 11, 50814, 50814, 550, 291, 434, 516, 281, 528, 281, 915, 746, 659, 12, 17227, 2001, 13, 50964, 50964, 583, 498, 291, 362, 264, 1460, 293, 264, 565, 293, 264, 14722, 11, 51114, 51114, 550, 445, 352, 2286, 293, 3847, 322, 428, 1412, 11, 558, 30, 51214, 51214, 400, 562, 291, 434, 1096, 11, 1190, 322, 428, 1500, 1412, 992, 13, 51364, 51364, 823, 11, 498, 291, 500, 380, 362, 257, 688, 295, 1412, 11, 51464, 51464, 550, 291, 820, 853, 281, 915, 257, 659, 12, 17227, 2001, 2316, 337, 428, 1412, 51614, 51614, 300, 10676, 428, 1412, 7316, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04785415661409966, "compression_ratio": 1.9525547445255473, "no_speech_prob": 4.092652943654684e-06}, {"id": 102, "seek": 27600, "start": 298.0, "end": 301.0, "text": " then you should try to find a pre-trained model for your data", "tokens": [50364, 759, 264, 1867, 307, 2086, 11, 550, 291, 3700, 500, 380, 643, 281, 2489, 12, 83, 2613, 11, 558, 30, 50564, 50564, 823, 11, 360, 291, 611, 362, 565, 294, 715, 1819, 11, 558, 30, 50714, 50714, 509, 393, 362, 257, 688, 295, 1412, 11, 457, 498, 309, 311, 1687, 28328, 281, 3847, 11, 50814, 50814, 550, 291, 434, 516, 281, 528, 281, 915, 746, 659, 12, 17227, 2001, 13, 50964, 50964, 583, 498, 291, 362, 264, 1460, 293, 264, 565, 293, 264, 14722, 11, 51114, 51114, 550, 445, 352, 2286, 293, 3847, 322, 428, 1412, 11, 558, 30, 51214, 51214, 400, 562, 291, 434, 1096, 11, 1190, 322, 428, 1500, 1412, 992, 13, 51364, 51364, 823, 11, 498, 291, 500, 380, 362, 257, 688, 295, 1412, 11, 51464, 51464, 550, 291, 820, 853, 281, 915, 257, 659, 12, 17227, 2001, 2316, 337, 428, 1412, 51614, 51614, 300, 10676, 428, 1412, 7316, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04785415661409966, "compression_ratio": 1.9525547445255473, "no_speech_prob": 4.092652943654684e-06}, {"id": 103, "seek": 27600, "start": 301.0, "end": 303.0, "text": " that matches your data distribution, right?", "tokens": [50364, 759, 264, 1867, 307, 2086, 11, 550, 291, 3700, 500, 380, 643, 281, 2489, 12, 83, 2613, 11, 558, 30, 50564, 50564, 823, 11, 360, 291, 611, 362, 565, 294, 715, 1819, 11, 558, 30, 50714, 50714, 509, 393, 362, 257, 688, 295, 1412, 11, 457, 498, 309, 311, 1687, 28328, 281, 3847, 11, 50814, 50814, 550, 291, 434, 516, 281, 528, 281, 915, 746, 659, 12, 17227, 2001, 13, 50964, 50964, 583, 498, 291, 362, 264, 1460, 293, 264, 565, 293, 264, 14722, 11, 51114, 51114, 550, 445, 352, 2286, 293, 3847, 322, 428, 1412, 11, 558, 30, 51214, 51214, 400, 562, 291, 434, 1096, 11, 1190, 322, 428, 1500, 1412, 992, 13, 51364, 51364, 823, 11, 498, 291, 500, 380, 362, 257, 688, 295, 1412, 11, 51464, 51464, 550, 291, 820, 853, 281, 915, 257, 659, 12, 17227, 2001, 2316, 337, 428, 1412, 51614, 51614, 300, 10676, 428, 1412, 7316, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.04785415661409966, "compression_ratio": 1.9525547445255473, "no_speech_prob": 4.092652943654684e-06}, {"id": 104, "seek": 30300, "start": 303.0, "end": 308.0, "text": " So this is super important because most vision models are trained on ImageNet, right?", "tokens": [50364, 407, 341, 307, 1687, 1021, 570, 881, 5201, 5245, 366, 8895, 322, 29903, 31890, 11, 558, 30, 50614, 50614, 407, 498, 291, 528, 281, 360, 746, 411, 11, 286, 500, 380, 458, 11, 5592, 17784, 420, 1783, 12, 36212, 11, 50964, 50964, 300, 311, 17518, 516, 281, 5003, 11, 558, 30, 51064, 51064, 1436, 294, 29903, 31890, 11, 291, 500, 380, 362, 300, 733, 295, 1412, 13, 51214, 51214, 509, 611, 500, 380, 362, 561, 11, 558, 30, 51314, 51314, 407, 291, 362, 281, 312, 14618, 295, 437, 291, 434, 1228, 341, 337, 13, 51464, 51464, 407, 309, 311, 406, 445, 47744, 264, 5585, 295, 257, 18161, 3209, 307, 516, 281, 589, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08628181718353532, "compression_ratio": 1.6384615384615384, "no_speech_prob": 1.0288744306308217e-05}, {"id": 105, "seek": 30300, "start": 308.0, "end": 315.0, "text": " So if you want to do something like, I don't know, cancer detection or X-rays,", "tokens": [50364, 407, 341, 307, 1687, 1021, 570, 881, 5201, 5245, 366, 8895, 322, 29903, 31890, 11, 558, 30, 50614, 50614, 407, 498, 291, 528, 281, 360, 746, 411, 11, 286, 500, 380, 458, 11, 5592, 17784, 420, 1783, 12, 36212, 11, 50964, 50964, 300, 311, 17518, 516, 281, 5003, 11, 558, 30, 51064, 51064, 1436, 294, 29903, 31890, 11, 291, 500, 380, 362, 300, 733, 295, 1412, 13, 51214, 51214, 509, 611, 500, 380, 362, 561, 11, 558, 30, 51314, 51314, 407, 291, 362, 281, 312, 14618, 295, 437, 291, 434, 1228, 341, 337, 13, 51464, 51464, 407, 309, 311, 406, 445, 47744, 264, 5585, 295, 257, 18161, 3209, 307, 516, 281, 589, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08628181718353532, "compression_ratio": 1.6384615384615384, "no_speech_prob": 1.0288744306308217e-05}, {"id": 106, "seek": 30300, "start": 315.0, "end": 317.0, "text": " that's unlikely going to transfer, right?", "tokens": [50364, 407, 341, 307, 1687, 1021, 570, 881, 5201, 5245, 366, 8895, 322, 29903, 31890, 11, 558, 30, 50614, 50614, 407, 498, 291, 528, 281, 360, 746, 411, 11, 286, 500, 380, 458, 11, 5592, 17784, 420, 1783, 12, 36212, 11, 50964, 50964, 300, 311, 17518, 516, 281, 5003, 11, 558, 30, 51064, 51064, 1436, 294, 29903, 31890, 11, 291, 500, 380, 362, 300, 733, 295, 1412, 13, 51214, 51214, 509, 611, 500, 380, 362, 561, 11, 558, 30, 51314, 51314, 407, 291, 362, 281, 312, 14618, 295, 437, 291, 434, 1228, 341, 337, 13, 51464, 51464, 407, 309, 311, 406, 445, 47744, 264, 5585, 295, 257, 18161, 3209, 307, 516, 281, 589, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08628181718353532, "compression_ratio": 1.6384615384615384, "no_speech_prob": 1.0288744306308217e-05}, {"id": 107, "seek": 30300, "start": 317.0, "end": 320.0, "text": " Because in ImageNet, you don't have that kind of data.", "tokens": [50364, 407, 341, 307, 1687, 1021, 570, 881, 5201, 5245, 366, 8895, 322, 29903, 31890, 11, 558, 30, 50614, 50614, 407, 498, 291, 528, 281, 360, 746, 411, 11, 286, 500, 380, 458, 11, 5592, 17784, 420, 1783, 12, 36212, 11, 50964, 50964, 300, 311, 17518, 516, 281, 5003, 11, 558, 30, 51064, 51064, 1436, 294, 29903, 31890, 11, 291, 500, 380, 362, 300, 733, 295, 1412, 13, 51214, 51214, 509, 611, 500, 380, 362, 561, 11, 558, 30, 51314, 51314, 407, 291, 362, 281, 312, 14618, 295, 437, 291, 434, 1228, 341, 337, 13, 51464, 51464, 407, 309, 311, 406, 445, 47744, 264, 5585, 295, 257, 18161, 3209, 307, 516, 281, 589, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08628181718353532, "compression_ratio": 1.6384615384615384, "no_speech_prob": 1.0288744306308217e-05}, {"id": 108, "seek": 30300, "start": 320.0, "end": 322.0, "text": " You also don't have people, right?", "tokens": [50364, 407, 341, 307, 1687, 1021, 570, 881, 5201, 5245, 366, 8895, 322, 29903, 31890, 11, 558, 30, 50614, 50614, 407, 498, 291, 528, 281, 360, 746, 411, 11, 286, 500, 380, 458, 11, 5592, 17784, 420, 1783, 12, 36212, 11, 50964, 50964, 300, 311, 17518, 516, 281, 5003, 11, 558, 30, 51064, 51064, 1436, 294, 29903, 31890, 11, 291, 500, 380, 362, 300, 733, 295, 1412, 13, 51214, 51214, 509, 611, 500, 380, 362, 561, 11, 558, 30, 51314, 51314, 407, 291, 362, 281, 312, 14618, 295, 437, 291, 434, 1228, 341, 337, 13, 51464, 51464, 407, 309, 311, 406, 445, 47744, 264, 5585, 295, 257, 18161, 3209, 307, 516, 281, 589, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08628181718353532, "compression_ratio": 1.6384615384615384, "no_speech_prob": 1.0288744306308217e-05}, {"id": 109, "seek": 30300, "start": 322.0, "end": 325.0, "text": " So you have to be mindful of what you're using this for.", "tokens": [50364, 407, 341, 307, 1687, 1021, 570, 881, 5201, 5245, 366, 8895, 322, 29903, 31890, 11, 558, 30, 50614, 50614, 407, 498, 291, 528, 281, 360, 746, 411, 11, 286, 500, 380, 458, 11, 5592, 17784, 420, 1783, 12, 36212, 11, 50964, 50964, 300, 311, 17518, 516, 281, 5003, 11, 558, 30, 51064, 51064, 1436, 294, 29903, 31890, 11, 291, 500, 380, 362, 300, 733, 295, 1412, 13, 51214, 51214, 509, 611, 500, 380, 362, 561, 11, 558, 30, 51314, 51314, 407, 291, 362, 281, 312, 14618, 295, 437, 291, 434, 1228, 341, 337, 13, 51464, 51464, 407, 309, 311, 406, 445, 47744, 264, 5585, 295, 257, 18161, 3209, 307, 516, 281, 589, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08628181718353532, "compression_ratio": 1.6384615384615384, "no_speech_prob": 1.0288744306308217e-05}, {"id": 110, "seek": 30300, "start": 325.0, "end": 330.0, "text": " So it's not just blindly the magic of a neural network is going to work.", "tokens": [50364, 407, 341, 307, 1687, 1021, 570, 881, 5201, 5245, 366, 8895, 322, 29903, 31890, 11, 558, 30, 50614, 50614, 407, 498, 291, 528, 281, 360, 746, 411, 11, 286, 500, 380, 458, 11, 5592, 17784, 420, 1783, 12, 36212, 11, 50964, 50964, 300, 311, 17518, 516, 281, 5003, 11, 558, 30, 51064, 51064, 1436, 294, 29903, 31890, 11, 291, 500, 380, 362, 300, 733, 295, 1412, 13, 51214, 51214, 509, 611, 500, 380, 362, 561, 11, 558, 30, 51314, 51314, 407, 291, 362, 281, 312, 14618, 295, 437, 291, 434, 1228, 341, 337, 13, 51464, 51464, 407, 309, 311, 406, 445, 47744, 264, 5585, 295, 257, 18161, 3209, 307, 516, 281, 589, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08628181718353532, "compression_ratio": 1.6384615384615384, "no_speech_prob": 1.0288744306308217e-05}, {"id": 111, "seek": 33000, "start": 330.0, "end": 336.0, "text": " And then, so yeah, so if you do have something that matches that kind of distribution,", "tokens": [50364, 400, 550, 11, 370, 1338, 11, 370, 498, 291, 360, 362, 746, 300, 10676, 300, 733, 295, 7316, 11, 50664, 50664, 550, 291, 393, 764, 257, 659, 12, 17227, 2001, 2316, 11, 558, 30, 50764, 50764, 407, 562, 291, 519, 466, 5003, 2539, 11, 321, 362, 732, 3166, 13, 50914, 50914, 509, 362, 264, 659, 12, 17227, 2001, 2316, 300, 390, 8895, 322, 746, 1646, 11, 51064, 51064, 293, 550, 291, 362, 264, 1507, 291, 434, 516, 281, 909, 322, 1192, 295, 300, 281, 5003, 300, 11, 558, 30, 51314, 51314, 400, 550, 291, 393, 2489, 12, 83, 2613, 294, 428, 1065, 1412, 13, 51414, 51414, 407, 341, 307, 588, 1021, 11, 370, 286, 576, 1066, 341, 294, 1575, 13, 51564, 51564, 400, 550, 264, 958, 721, 366, 341, 11, 558, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055029423797831815, "compression_ratio": 1.8566037735849057, "no_speech_prob": 1.045098360918928e-05}, {"id": 112, "seek": 33000, "start": 336.0, "end": 338.0, "text": " then you can use a pre-trained model, right?", "tokens": [50364, 400, 550, 11, 370, 1338, 11, 370, 498, 291, 360, 362, 746, 300, 10676, 300, 733, 295, 7316, 11, 50664, 50664, 550, 291, 393, 764, 257, 659, 12, 17227, 2001, 2316, 11, 558, 30, 50764, 50764, 407, 562, 291, 519, 466, 5003, 2539, 11, 321, 362, 732, 3166, 13, 50914, 50914, 509, 362, 264, 659, 12, 17227, 2001, 2316, 300, 390, 8895, 322, 746, 1646, 11, 51064, 51064, 293, 550, 291, 362, 264, 1507, 291, 434, 516, 281, 909, 322, 1192, 295, 300, 281, 5003, 300, 11, 558, 30, 51314, 51314, 400, 550, 291, 393, 2489, 12, 83, 2613, 294, 428, 1065, 1412, 13, 51414, 51414, 407, 341, 307, 588, 1021, 11, 370, 286, 576, 1066, 341, 294, 1575, 13, 51564, 51564, 400, 550, 264, 958, 721, 366, 341, 11, 558, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055029423797831815, "compression_ratio": 1.8566037735849057, "no_speech_prob": 1.045098360918928e-05}, {"id": 113, "seek": 33000, "start": 338.0, "end": 341.0, "text": " So when you think about transfer learning, we have two parts.", "tokens": [50364, 400, 550, 11, 370, 1338, 11, 370, 498, 291, 360, 362, 746, 300, 10676, 300, 733, 295, 7316, 11, 50664, 50664, 550, 291, 393, 764, 257, 659, 12, 17227, 2001, 2316, 11, 558, 30, 50764, 50764, 407, 562, 291, 519, 466, 5003, 2539, 11, 321, 362, 732, 3166, 13, 50914, 50914, 509, 362, 264, 659, 12, 17227, 2001, 2316, 300, 390, 8895, 322, 746, 1646, 11, 51064, 51064, 293, 550, 291, 362, 264, 1507, 291, 434, 516, 281, 909, 322, 1192, 295, 300, 281, 5003, 300, 11, 558, 30, 51314, 51314, 400, 550, 291, 393, 2489, 12, 83, 2613, 294, 428, 1065, 1412, 13, 51414, 51414, 407, 341, 307, 588, 1021, 11, 370, 286, 576, 1066, 341, 294, 1575, 13, 51564, 51564, 400, 550, 264, 958, 721, 366, 341, 11, 558, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055029423797831815, "compression_ratio": 1.8566037735849057, "no_speech_prob": 1.045098360918928e-05}, {"id": 114, "seek": 33000, "start": 341.0, "end": 344.0, "text": " You have the pre-trained model that was trained on something else,", "tokens": [50364, 400, 550, 11, 370, 1338, 11, 370, 498, 291, 360, 362, 746, 300, 10676, 300, 733, 295, 7316, 11, 50664, 50664, 550, 291, 393, 764, 257, 659, 12, 17227, 2001, 2316, 11, 558, 30, 50764, 50764, 407, 562, 291, 519, 466, 5003, 2539, 11, 321, 362, 732, 3166, 13, 50914, 50914, 509, 362, 264, 659, 12, 17227, 2001, 2316, 300, 390, 8895, 322, 746, 1646, 11, 51064, 51064, 293, 550, 291, 362, 264, 1507, 291, 434, 516, 281, 909, 322, 1192, 295, 300, 281, 5003, 300, 11, 558, 30, 51314, 51314, 400, 550, 291, 393, 2489, 12, 83, 2613, 294, 428, 1065, 1412, 13, 51414, 51414, 407, 341, 307, 588, 1021, 11, 370, 286, 576, 1066, 341, 294, 1575, 13, 51564, 51564, 400, 550, 264, 958, 721, 366, 341, 11, 558, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055029423797831815, "compression_ratio": 1.8566037735849057, "no_speech_prob": 1.045098360918928e-05}, {"id": 115, "seek": 33000, "start": 344.0, "end": 349.0, "text": " and then you have the stuff you're going to add on top of that to transfer that, right?", "tokens": [50364, 400, 550, 11, 370, 1338, 11, 370, 498, 291, 360, 362, 746, 300, 10676, 300, 733, 295, 7316, 11, 50664, 50664, 550, 291, 393, 764, 257, 659, 12, 17227, 2001, 2316, 11, 558, 30, 50764, 50764, 407, 562, 291, 519, 466, 5003, 2539, 11, 321, 362, 732, 3166, 13, 50914, 50914, 509, 362, 264, 659, 12, 17227, 2001, 2316, 300, 390, 8895, 322, 746, 1646, 11, 51064, 51064, 293, 550, 291, 362, 264, 1507, 291, 434, 516, 281, 909, 322, 1192, 295, 300, 281, 5003, 300, 11, 558, 30, 51314, 51314, 400, 550, 291, 393, 2489, 12, 83, 2613, 294, 428, 1065, 1412, 13, 51414, 51414, 407, 341, 307, 588, 1021, 11, 370, 286, 576, 1066, 341, 294, 1575, 13, 51564, 51564, 400, 550, 264, 958, 721, 366, 341, 11, 558, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055029423797831815, "compression_ratio": 1.8566037735849057, "no_speech_prob": 1.045098360918928e-05}, {"id": 116, "seek": 33000, "start": 349.0, "end": 351.0, "text": " And then you can fine-tune in your own data.", "tokens": [50364, 400, 550, 11, 370, 1338, 11, 370, 498, 291, 360, 362, 746, 300, 10676, 300, 733, 295, 7316, 11, 50664, 50664, 550, 291, 393, 764, 257, 659, 12, 17227, 2001, 2316, 11, 558, 30, 50764, 50764, 407, 562, 291, 519, 466, 5003, 2539, 11, 321, 362, 732, 3166, 13, 50914, 50914, 509, 362, 264, 659, 12, 17227, 2001, 2316, 300, 390, 8895, 322, 746, 1646, 11, 51064, 51064, 293, 550, 291, 362, 264, 1507, 291, 434, 516, 281, 909, 322, 1192, 295, 300, 281, 5003, 300, 11, 558, 30, 51314, 51314, 400, 550, 291, 393, 2489, 12, 83, 2613, 294, 428, 1065, 1412, 13, 51414, 51414, 407, 341, 307, 588, 1021, 11, 370, 286, 576, 1066, 341, 294, 1575, 13, 51564, 51564, 400, 550, 264, 958, 721, 366, 341, 11, 558, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055029423797831815, "compression_ratio": 1.8566037735849057, "no_speech_prob": 1.045098360918928e-05}, {"id": 117, "seek": 33000, "start": 351.0, "end": 354.0, "text": " So this is very important, so I would keep this in mind.", "tokens": [50364, 400, 550, 11, 370, 1338, 11, 370, 498, 291, 360, 362, 746, 300, 10676, 300, 733, 295, 7316, 11, 50664, 50664, 550, 291, 393, 764, 257, 659, 12, 17227, 2001, 2316, 11, 558, 30, 50764, 50764, 407, 562, 291, 519, 466, 5003, 2539, 11, 321, 362, 732, 3166, 13, 50914, 50914, 509, 362, 264, 659, 12, 17227, 2001, 2316, 300, 390, 8895, 322, 746, 1646, 11, 51064, 51064, 293, 550, 291, 362, 264, 1507, 291, 434, 516, 281, 909, 322, 1192, 295, 300, 281, 5003, 300, 11, 558, 30, 51314, 51314, 400, 550, 291, 393, 2489, 12, 83, 2613, 294, 428, 1065, 1412, 13, 51414, 51414, 407, 341, 307, 588, 1021, 11, 370, 286, 576, 1066, 341, 294, 1575, 13, 51564, 51564, 400, 550, 264, 958, 721, 366, 341, 11, 558, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055029423797831815, "compression_ratio": 1.8566037735849057, "no_speech_prob": 1.045098360918928e-05}, {"id": 118, "seek": 33000, "start": 354.0, "end": 358.0, "text": " And then the next things are this, right?", "tokens": [50364, 400, 550, 11, 370, 1338, 11, 370, 498, 291, 360, 362, 746, 300, 10676, 300, 733, 295, 7316, 11, 50664, 50664, 550, 291, 393, 764, 257, 659, 12, 17227, 2001, 2316, 11, 558, 30, 50764, 50764, 407, 562, 291, 519, 466, 5003, 2539, 11, 321, 362, 732, 3166, 13, 50914, 50914, 509, 362, 264, 659, 12, 17227, 2001, 2316, 300, 390, 8895, 322, 746, 1646, 11, 51064, 51064, 293, 550, 291, 362, 264, 1507, 291, 434, 516, 281, 909, 322, 1192, 295, 300, 281, 5003, 300, 11, 558, 30, 51314, 51314, 400, 550, 291, 393, 2489, 12, 83, 2613, 294, 428, 1065, 1412, 13, 51414, 51414, 407, 341, 307, 588, 1021, 11, 370, 286, 576, 1066, 341, 294, 1575, 13, 51564, 51564, 400, 550, 264, 958, 721, 366, 341, 11, 558, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.055029423797831815, "compression_ratio": 1.8566037735849057, "no_speech_prob": 1.045098360918928e-05}, {"id": 119, "seek": 35800, "start": 358.0, "end": 360.0, "text": " So then you have two major options.", "tokens": [50364, 407, 550, 291, 362, 732, 2563, 3956, 13, 50464, 50464, 509, 362, 46533, 420, 2698, 12, 48172, 24420, 11, 558, 30, 50564, 50564, 407, 322, 257, 2316, 300, 311, 659, 12, 17227, 2001, 1228, 46533, 3097, 11, 50864, 50864, 2539, 390, 8895, 3700, 337, 21538, 11, 370, 746, 337, 29903, 31890, 11, 337, 1365, 13, 51164, 51164, 400, 538, 884, 300, 11, 291, 600, 7268, 12577, 666, 300, 2316, 11, 558, 30, 51314, 51314, 509, 600, 7579, 309, 281, 1466, 257, 10290, 300, 307, 516, 281, 652, 21538, 3571, 11, 51564, 51564, 457, 456, 311, 572, 10815, 300, 300, 311, 516, 281, 652, 746, 411, 9469, 399, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0677576580563107, "compression_ratio": 1.7807692307692307, "no_speech_prob": 4.565813014778541e-06}, {"id": 120, "seek": 35800, "start": 360.0, "end": 362.0, "text": " You have supervised or self-supervised, right?", "tokens": [50364, 407, 550, 291, 362, 732, 2563, 3956, 13, 50464, 50464, 509, 362, 46533, 420, 2698, 12, 48172, 24420, 11, 558, 30, 50564, 50564, 407, 322, 257, 2316, 300, 311, 659, 12, 17227, 2001, 1228, 46533, 3097, 11, 50864, 50864, 2539, 390, 8895, 3700, 337, 21538, 11, 370, 746, 337, 29903, 31890, 11, 337, 1365, 13, 51164, 51164, 400, 538, 884, 300, 11, 291, 600, 7268, 12577, 666, 300, 2316, 11, 558, 30, 51314, 51314, 509, 600, 7579, 309, 281, 1466, 257, 10290, 300, 307, 516, 281, 652, 21538, 3571, 11, 51564, 51564, 457, 456, 311, 572, 10815, 300, 300, 311, 516, 281, 652, 746, 411, 9469, 399, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0677576580563107, "compression_ratio": 1.7807692307692307, "no_speech_prob": 4.565813014778541e-06}, {"id": 121, "seek": 35800, "start": 362.0, "end": 368.0, "text": " So on a model that's pre-trained using supervised training,", "tokens": [50364, 407, 550, 291, 362, 732, 2563, 3956, 13, 50464, 50464, 509, 362, 46533, 420, 2698, 12, 48172, 24420, 11, 558, 30, 50564, 50564, 407, 322, 257, 2316, 300, 311, 659, 12, 17227, 2001, 1228, 46533, 3097, 11, 50864, 50864, 2539, 390, 8895, 3700, 337, 21538, 11, 370, 746, 337, 29903, 31890, 11, 337, 1365, 13, 51164, 51164, 400, 538, 884, 300, 11, 291, 600, 7268, 12577, 666, 300, 2316, 11, 558, 30, 51314, 51314, 509, 600, 7579, 309, 281, 1466, 257, 10290, 300, 307, 516, 281, 652, 21538, 3571, 11, 51564, 51564, 457, 456, 311, 572, 10815, 300, 300, 311, 516, 281, 652, 746, 411, 9469, 399, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0677576580563107, "compression_ratio": 1.7807692307692307, "no_speech_prob": 4.565813014778541e-06}, {"id": 122, "seek": 35800, "start": 368.0, "end": 374.0, "text": " learning was trained likely for classification, so something for ImageNet, for example.", "tokens": [50364, 407, 550, 291, 362, 732, 2563, 3956, 13, 50464, 50464, 509, 362, 46533, 420, 2698, 12, 48172, 24420, 11, 558, 30, 50564, 50564, 407, 322, 257, 2316, 300, 311, 659, 12, 17227, 2001, 1228, 46533, 3097, 11, 50864, 50864, 2539, 390, 8895, 3700, 337, 21538, 11, 370, 746, 337, 29903, 31890, 11, 337, 1365, 13, 51164, 51164, 400, 538, 884, 300, 11, 291, 600, 7268, 12577, 666, 300, 2316, 11, 558, 30, 51314, 51314, 509, 600, 7579, 309, 281, 1466, 257, 10290, 300, 307, 516, 281, 652, 21538, 3571, 11, 51564, 51564, 457, 456, 311, 572, 10815, 300, 300, 311, 516, 281, 652, 746, 411, 9469, 399, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0677576580563107, "compression_ratio": 1.7807692307692307, "no_speech_prob": 4.565813014778541e-06}, {"id": 123, "seek": 35800, "start": 374.0, "end": 377.0, "text": " And by doing that, you've introduced bias into that model, right?", "tokens": [50364, 407, 550, 291, 362, 732, 2563, 3956, 13, 50464, 50464, 509, 362, 46533, 420, 2698, 12, 48172, 24420, 11, 558, 30, 50564, 50564, 407, 322, 257, 2316, 300, 311, 659, 12, 17227, 2001, 1228, 46533, 3097, 11, 50864, 50864, 2539, 390, 8895, 3700, 337, 21538, 11, 370, 746, 337, 29903, 31890, 11, 337, 1365, 13, 51164, 51164, 400, 538, 884, 300, 11, 291, 600, 7268, 12577, 666, 300, 2316, 11, 558, 30, 51314, 51314, 509, 600, 7579, 309, 281, 1466, 257, 10290, 300, 307, 516, 281, 652, 21538, 3571, 11, 51564, 51564, 457, 456, 311, 572, 10815, 300, 300, 311, 516, 281, 652, 746, 411, 9469, 399, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0677576580563107, "compression_ratio": 1.7807692307692307, "no_speech_prob": 4.565813014778541e-06}, {"id": 124, "seek": 35800, "start": 377.0, "end": 382.0, "text": " You've forced it to learn a representation that is going to make classification easier,", "tokens": [50364, 407, 550, 291, 362, 732, 2563, 3956, 13, 50464, 50464, 509, 362, 46533, 420, 2698, 12, 48172, 24420, 11, 558, 30, 50564, 50564, 407, 322, 257, 2316, 300, 311, 659, 12, 17227, 2001, 1228, 46533, 3097, 11, 50864, 50864, 2539, 390, 8895, 3700, 337, 21538, 11, 370, 746, 337, 29903, 31890, 11, 337, 1365, 13, 51164, 51164, 400, 538, 884, 300, 11, 291, 600, 7268, 12577, 666, 300, 2316, 11, 558, 30, 51314, 51314, 509, 600, 7579, 309, 281, 1466, 257, 10290, 300, 307, 516, 281, 652, 21538, 3571, 11, 51564, 51564, 457, 456, 311, 572, 10815, 300, 300, 311, 516, 281, 652, 746, 411, 9469, 399, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0677576580563107, "compression_ratio": 1.7807692307692307, "no_speech_prob": 4.565813014778541e-06}, {"id": 125, "seek": 35800, "start": 382.0, "end": 387.0, "text": " but there's no guarantee that that's going to make something like segmentation", "tokens": [50364, 407, 550, 291, 362, 732, 2563, 3956, 13, 50464, 50464, 509, 362, 46533, 420, 2698, 12, 48172, 24420, 11, 558, 30, 50564, 50564, 407, 322, 257, 2316, 300, 311, 659, 12, 17227, 2001, 1228, 46533, 3097, 11, 50864, 50864, 2539, 390, 8895, 3700, 337, 21538, 11, 370, 746, 337, 29903, 31890, 11, 337, 1365, 13, 51164, 51164, 400, 538, 884, 300, 11, 291, 600, 7268, 12577, 666, 300, 2316, 11, 558, 30, 51314, 51314, 509, 600, 7579, 309, 281, 1466, 257, 10290, 300, 307, 516, 281, 652, 21538, 3571, 11, 51564, 51564, 457, 456, 311, 572, 10815, 300, 300, 311, 516, 281, 652, 746, 411, 9469, 399, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0677576580563107, "compression_ratio": 1.7807692307692307, "no_speech_prob": 4.565813014778541e-06}, {"id": 126, "seek": 38700, "start": 387.0, "end": 391.0, "text": " or detection or anything else as easy.", "tokens": [50364, 420, 17784, 420, 1340, 1646, 382, 1858, 13, 50564, 50564, 400, 370, 291, 528, 281, 312, 14618, 295, 300, 11, 558, 30, 50664, 50664, 407, 264, 2698, 12, 48172, 24420, 11, 411, 286, 848, 11, 309, 311, 17069, 11, 50864, 50864, 457, 309, 1062, 312, 1075, 281, 2674, 1125, 1101, 13, 51014, 51014, 407, 309, 390, 406, 8895, 337, 21538, 11, 597, 1355, 300, 498, 291, 528, 281, 360, 9469, 399, 51264, 51264, 420, 2657, 17784, 11, 264, 33358, 1062, 5003, 1101, 13, 51514, 51514, 407, 300, 311, 1880, 13, 51614, 51614, 400, 550, 11, 291, 458, 11, 436, 434, 411, 11, 286, 500, 380, 458, 498, 291, 434, 3480, 264, 10394, 420, 1391, 406, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07253815333048502, "compression_ratio": 1.6813186813186813, "no_speech_prob": 2.9308006560313515e-05}, {"id": 127, "seek": 38700, "start": 391.0, "end": 393.0, "text": " And so you want to be mindful of that, right?", "tokens": [50364, 420, 17784, 420, 1340, 1646, 382, 1858, 13, 50564, 50564, 400, 370, 291, 528, 281, 312, 14618, 295, 300, 11, 558, 30, 50664, 50664, 407, 264, 2698, 12, 48172, 24420, 11, 411, 286, 848, 11, 309, 311, 17069, 11, 50864, 50864, 457, 309, 1062, 312, 1075, 281, 2674, 1125, 1101, 13, 51014, 51014, 407, 309, 390, 406, 8895, 337, 21538, 11, 597, 1355, 300, 498, 291, 528, 281, 360, 9469, 399, 51264, 51264, 420, 2657, 17784, 11, 264, 33358, 1062, 5003, 1101, 13, 51514, 51514, 407, 300, 311, 1880, 13, 51614, 51614, 400, 550, 11, 291, 458, 11, 436, 434, 411, 11, 286, 500, 380, 458, 498, 291, 434, 3480, 264, 10394, 420, 1391, 406, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07253815333048502, "compression_ratio": 1.6813186813186813, "no_speech_prob": 2.9308006560313515e-05}, {"id": 128, "seek": 38700, "start": 393.0, "end": 397.0, "text": " So the self-supervised, like I said, it's experimental,", "tokens": [50364, 420, 17784, 420, 1340, 1646, 382, 1858, 13, 50564, 50564, 400, 370, 291, 528, 281, 312, 14618, 295, 300, 11, 558, 30, 50664, 50664, 407, 264, 2698, 12, 48172, 24420, 11, 411, 286, 848, 11, 309, 311, 17069, 11, 50864, 50864, 457, 309, 1062, 312, 1075, 281, 2674, 1125, 1101, 13, 51014, 51014, 407, 309, 390, 406, 8895, 337, 21538, 11, 597, 1355, 300, 498, 291, 528, 281, 360, 9469, 399, 51264, 51264, 420, 2657, 17784, 11, 264, 33358, 1062, 5003, 1101, 13, 51514, 51514, 407, 300, 311, 1880, 13, 51614, 51614, 400, 550, 11, 291, 458, 11, 436, 434, 411, 11, 286, 500, 380, 458, 498, 291, 434, 3480, 264, 10394, 420, 1391, 406, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07253815333048502, "compression_ratio": 1.6813186813186813, "no_speech_prob": 2.9308006560313515e-05}, {"id": 129, "seek": 38700, "start": 397.0, "end": 400.0, "text": " but it might be able to generalize better.", "tokens": [50364, 420, 17784, 420, 1340, 1646, 382, 1858, 13, 50564, 50564, 400, 370, 291, 528, 281, 312, 14618, 295, 300, 11, 558, 30, 50664, 50664, 407, 264, 2698, 12, 48172, 24420, 11, 411, 286, 848, 11, 309, 311, 17069, 11, 50864, 50864, 457, 309, 1062, 312, 1075, 281, 2674, 1125, 1101, 13, 51014, 51014, 407, 309, 390, 406, 8895, 337, 21538, 11, 597, 1355, 300, 498, 291, 528, 281, 360, 9469, 399, 51264, 51264, 420, 2657, 17784, 11, 264, 33358, 1062, 5003, 1101, 13, 51514, 51514, 407, 300, 311, 1880, 13, 51614, 51614, 400, 550, 11, 291, 458, 11, 436, 434, 411, 11, 286, 500, 380, 458, 498, 291, 434, 3480, 264, 10394, 420, 1391, 406, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07253815333048502, "compression_ratio": 1.6813186813186813, "no_speech_prob": 2.9308006560313515e-05}, {"id": 130, "seek": 38700, "start": 400.0, "end": 405.0, "text": " So it was not trained for classification, which means that if you want to do segmentation", "tokens": [50364, 420, 17784, 420, 1340, 1646, 382, 1858, 13, 50564, 50564, 400, 370, 291, 528, 281, 312, 14618, 295, 300, 11, 558, 30, 50664, 50664, 407, 264, 2698, 12, 48172, 24420, 11, 411, 286, 848, 11, 309, 311, 17069, 11, 50864, 50864, 457, 309, 1062, 312, 1075, 281, 2674, 1125, 1101, 13, 51014, 51014, 407, 309, 390, 406, 8895, 337, 21538, 11, 597, 1355, 300, 498, 291, 528, 281, 360, 9469, 399, 51264, 51264, 420, 2657, 17784, 11, 264, 33358, 1062, 5003, 1101, 13, 51514, 51514, 407, 300, 311, 1880, 13, 51614, 51614, 400, 550, 11, 291, 458, 11, 436, 434, 411, 11, 286, 500, 380, 458, 498, 291, 434, 3480, 264, 10394, 420, 1391, 406, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07253815333048502, "compression_ratio": 1.6813186813186813, "no_speech_prob": 2.9308006560313515e-05}, {"id": 131, "seek": 38700, "start": 405.0, "end": 410.0, "text": " or object detection, the representations might transfer better.", "tokens": [50364, 420, 17784, 420, 1340, 1646, 382, 1858, 13, 50564, 50564, 400, 370, 291, 528, 281, 312, 14618, 295, 300, 11, 558, 30, 50664, 50664, 407, 264, 2698, 12, 48172, 24420, 11, 411, 286, 848, 11, 309, 311, 17069, 11, 50864, 50864, 457, 309, 1062, 312, 1075, 281, 2674, 1125, 1101, 13, 51014, 51014, 407, 309, 390, 406, 8895, 337, 21538, 11, 597, 1355, 300, 498, 291, 528, 281, 360, 9469, 399, 51264, 51264, 420, 2657, 17784, 11, 264, 33358, 1062, 5003, 1101, 13, 51514, 51514, 407, 300, 311, 1880, 13, 51614, 51614, 400, 550, 11, 291, 458, 11, 436, 434, 411, 11, 286, 500, 380, 458, 498, 291, 434, 3480, 264, 10394, 420, 1391, 406, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07253815333048502, "compression_ratio": 1.6813186813186813, "no_speech_prob": 2.9308006560313515e-05}, {"id": 132, "seek": 38700, "start": 410.0, "end": 412.0, "text": " So that's interesting.", "tokens": [50364, 420, 17784, 420, 1340, 1646, 382, 1858, 13, 50564, 50564, 400, 370, 291, 528, 281, 312, 14618, 295, 300, 11, 558, 30, 50664, 50664, 407, 264, 2698, 12, 48172, 24420, 11, 411, 286, 848, 11, 309, 311, 17069, 11, 50864, 50864, 457, 309, 1062, 312, 1075, 281, 2674, 1125, 1101, 13, 51014, 51014, 407, 309, 390, 406, 8895, 337, 21538, 11, 597, 1355, 300, 498, 291, 528, 281, 360, 9469, 399, 51264, 51264, 420, 2657, 17784, 11, 264, 33358, 1062, 5003, 1101, 13, 51514, 51514, 407, 300, 311, 1880, 13, 51614, 51614, 400, 550, 11, 291, 458, 11, 436, 434, 411, 11, 286, 500, 380, 458, 498, 291, 434, 3480, 264, 10394, 420, 1391, 406, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07253815333048502, "compression_ratio": 1.6813186813186813, "no_speech_prob": 2.9308006560313515e-05}, {"id": 133, "seek": 38700, "start": 412.0, "end": 416.0, "text": " And then, you know, they're like, I don't know if you're following the literature or probably not,", "tokens": [50364, 420, 17784, 420, 1340, 1646, 382, 1858, 13, 50564, 50564, 400, 370, 291, 528, 281, 312, 14618, 295, 300, 11, 558, 30, 50664, 50664, 407, 264, 2698, 12, 48172, 24420, 11, 411, 286, 848, 11, 309, 311, 17069, 11, 50864, 50864, 457, 309, 1062, 312, 1075, 281, 2674, 1125, 1101, 13, 51014, 51014, 407, 309, 390, 406, 8895, 337, 21538, 11, 597, 1355, 300, 498, 291, 528, 281, 360, 9469, 399, 51264, 51264, 420, 2657, 17784, 11, 264, 33358, 1062, 5003, 1101, 13, 51514, 51514, 407, 300, 311, 1880, 13, 51614, 51614, 400, 550, 11, 291, 458, 11, 436, 434, 411, 11, 286, 500, 380, 458, 498, 291, 434, 3480, 264, 10394, 420, 1391, 406, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07253815333048502, "compression_ratio": 1.6813186813186813, "no_speech_prob": 2.9308006560313515e-05}, {"id": 134, "seek": 41600, "start": 416.0, "end": 420.0, "text": " but there are about seven or eight options that you have to do this.", "tokens": [50364, 457, 456, 366, 466, 3407, 420, 3180, 3956, 300, 291, 362, 281, 360, 341, 13, 50564, 50564, 407, 34808, 6324, 11, 3335, 21141, 11, 383, 12986, 11, 3026, 75, 11, 318, 6683, 34, 5797, 11, 26930, 46, 11, 293, 20346, 46, 7540, 11, 558, 30, 50914, 50914, 407, 294, 1668, 11, 264, 6792, 472, 307, 20346, 46, 7540, 13, 51064, 51064, 400, 498, 291, 434, 3102, 294, 3701, 264, 2649, 1296, 439, 295, 613, 11, 51264, 51264, 291, 458, 11, 286, 3938, 6572, 257, 3035, 365, 8419, 12366, 257, 1326, 2493, 2057, 11, 51564, 51564, 293, 550, 321, 4114, 341, 7222, 733, 295, 516, 666, 439, 264, 4365, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12967623983110702, "compression_ratio": 1.4770992366412214, "no_speech_prob": 1.2804172911273781e-05}, {"id": 135, "seek": 41600, "start": 420.0, "end": 427.0, "text": " So AMDIM, MoCo, CPC, Perl, SEMClear, BYO, and SWOVE, right?", "tokens": [50364, 457, 456, 366, 466, 3407, 420, 3180, 3956, 300, 291, 362, 281, 360, 341, 13, 50564, 50564, 407, 34808, 6324, 11, 3335, 21141, 11, 383, 12986, 11, 3026, 75, 11, 318, 6683, 34, 5797, 11, 26930, 46, 11, 293, 20346, 46, 7540, 11, 558, 30, 50914, 50914, 407, 294, 1668, 11, 264, 6792, 472, 307, 20346, 46, 7540, 13, 51064, 51064, 400, 498, 291, 434, 3102, 294, 3701, 264, 2649, 1296, 439, 295, 613, 11, 51264, 51264, 291, 458, 11, 286, 3938, 6572, 257, 3035, 365, 8419, 12366, 257, 1326, 2493, 2057, 11, 51564, 51564, 293, 550, 321, 4114, 341, 7222, 733, 295, 516, 666, 439, 264, 4365, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12967623983110702, "compression_ratio": 1.4770992366412214, "no_speech_prob": 1.2804172911273781e-05}, {"id": 136, "seek": 41600, "start": 427.0, "end": 430.0, "text": " So in order, the latest one is SWOVE.", "tokens": [50364, 457, 456, 366, 466, 3407, 420, 3180, 3956, 300, 291, 362, 281, 360, 341, 13, 50564, 50564, 407, 34808, 6324, 11, 3335, 21141, 11, 383, 12986, 11, 3026, 75, 11, 318, 6683, 34, 5797, 11, 26930, 46, 11, 293, 20346, 46, 7540, 11, 558, 30, 50914, 50914, 407, 294, 1668, 11, 264, 6792, 472, 307, 20346, 46, 7540, 13, 51064, 51064, 400, 498, 291, 434, 3102, 294, 3701, 264, 2649, 1296, 439, 295, 613, 11, 51264, 51264, 291, 458, 11, 286, 3938, 6572, 257, 3035, 365, 8419, 12366, 257, 1326, 2493, 2057, 11, 51564, 51564, 293, 550, 321, 4114, 341, 7222, 733, 295, 516, 666, 439, 264, 4365, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12967623983110702, "compression_ratio": 1.4770992366412214, "no_speech_prob": 1.2804172911273781e-05}, {"id": 137, "seek": 41600, "start": 430.0, "end": 434.0, "text": " And if you're interested in understanding the difference between all of these,", "tokens": [50364, 457, 456, 366, 466, 3407, 420, 3180, 3956, 300, 291, 362, 281, 360, 341, 13, 50564, 50564, 407, 34808, 6324, 11, 3335, 21141, 11, 383, 12986, 11, 3026, 75, 11, 318, 6683, 34, 5797, 11, 26930, 46, 11, 293, 20346, 46, 7540, 11, 558, 30, 50914, 50914, 407, 294, 1668, 11, 264, 6792, 472, 307, 20346, 46, 7540, 13, 51064, 51064, 400, 498, 291, 434, 3102, 294, 3701, 264, 2649, 1296, 439, 295, 613, 11, 51264, 51264, 291, 458, 11, 286, 3938, 6572, 257, 3035, 365, 8419, 12366, 257, 1326, 2493, 2057, 11, 51564, 51564, 293, 550, 321, 4114, 341, 7222, 733, 295, 516, 666, 439, 264, 4365, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12967623983110702, "compression_ratio": 1.4770992366412214, "no_speech_prob": 1.2804172911273781e-05}, {"id": 138, "seek": 41600, "start": 434.0, "end": 440.0, "text": " you know, I recently published a paper with Professor Cho a few months ago,", "tokens": [50364, 457, 456, 366, 466, 3407, 420, 3180, 3956, 300, 291, 362, 281, 360, 341, 13, 50564, 50564, 407, 34808, 6324, 11, 3335, 21141, 11, 383, 12986, 11, 3026, 75, 11, 318, 6683, 34, 5797, 11, 26930, 46, 11, 293, 20346, 46, 7540, 11, 558, 30, 50914, 50914, 407, 294, 1668, 11, 264, 6792, 472, 307, 20346, 46, 7540, 13, 51064, 51064, 400, 498, 291, 434, 3102, 294, 3701, 264, 2649, 1296, 439, 295, 613, 11, 51264, 51264, 291, 458, 11, 286, 3938, 6572, 257, 3035, 365, 8419, 12366, 257, 1326, 2493, 2057, 11, 51564, 51564, 293, 550, 321, 4114, 341, 7222, 733, 295, 516, 666, 439, 264, 4365, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12967623983110702, "compression_ratio": 1.4770992366412214, "no_speech_prob": 1.2804172911273781e-05}, {"id": 139, "seek": 41600, "start": 440.0, "end": 443.0, "text": " and then we wrote this article kind of going into all the details", "tokens": [50364, 457, 456, 366, 466, 3407, 420, 3180, 3956, 300, 291, 362, 281, 360, 341, 13, 50564, 50564, 407, 34808, 6324, 11, 3335, 21141, 11, 383, 12986, 11, 3026, 75, 11, 318, 6683, 34, 5797, 11, 26930, 46, 11, 293, 20346, 46, 7540, 11, 558, 30, 50914, 50914, 407, 294, 1668, 11, 264, 6792, 472, 307, 20346, 46, 7540, 13, 51064, 51064, 400, 498, 291, 434, 3102, 294, 3701, 264, 2649, 1296, 439, 295, 613, 11, 51264, 51264, 291, 458, 11, 286, 3938, 6572, 257, 3035, 365, 8419, 12366, 257, 1326, 2493, 2057, 11, 51564, 51564, 293, 550, 321, 4114, 341, 7222, 733, 295, 516, 666, 439, 264, 4365, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12967623983110702, "compression_ratio": 1.4770992366412214, "no_speech_prob": 1.2804172911273781e-05}, {"id": 140, "seek": 44300, "start": 443.0, "end": 448.0, "text": " and how to compare all of them and, you know, what the differences are,", "tokens": [50364, 293, 577, 281, 6794, 439, 295, 552, 293, 11, 291, 458, 11, 437, 264, 7300, 366, 11, 50614, 50614, 570, 436, 434, 534, 406, 300, 819, 13, 50814, 50814, 407, 286, 478, 516, 281, 312, 1228, 20346, 46, 7540, 337, 341, 1729, 1389, 337, 527, 2698, 12, 48172, 24420, 5003, 2539, 13, 51214, 51214, 407, 264, 700, 551, 307, 286, 478, 516, 281, 11, 264, 700, 1389, 300, 321, 434, 516, 281, 574, 412, 51464, 51464, 307, 516, 281, 312, 341, 46533, 5003, 2539, 13, 51614, 51614, 407, 341, 307, 3700, 264, 1389, 300, 291, 434, 516, 281, 1190, 666, 364, 3518, 881, 295, 264, 565, 11, 558, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07308726184136044, "compression_ratio": 1.8091286307053942, "no_speech_prob": 8.664027518534567e-06}, {"id": 141, "seek": 44300, "start": 448.0, "end": 452.0, "text": " because they're really not that different.", "tokens": [50364, 293, 577, 281, 6794, 439, 295, 552, 293, 11, 291, 458, 11, 437, 264, 7300, 366, 11, 50614, 50614, 570, 436, 434, 534, 406, 300, 819, 13, 50814, 50814, 407, 286, 478, 516, 281, 312, 1228, 20346, 46, 7540, 337, 341, 1729, 1389, 337, 527, 2698, 12, 48172, 24420, 5003, 2539, 13, 51214, 51214, 407, 264, 700, 551, 307, 286, 478, 516, 281, 11, 264, 700, 1389, 300, 321, 434, 516, 281, 574, 412, 51464, 51464, 307, 516, 281, 312, 341, 46533, 5003, 2539, 13, 51614, 51614, 407, 341, 307, 3700, 264, 1389, 300, 291, 434, 516, 281, 1190, 666, 364, 3518, 881, 295, 264, 565, 11, 558, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07308726184136044, "compression_ratio": 1.8091286307053942, "no_speech_prob": 8.664027518534567e-06}, {"id": 142, "seek": 44300, "start": 452.0, "end": 460.0, "text": " So I'm going to be using SWOVE for this particular case for our self-supervised transfer learning.", "tokens": [50364, 293, 577, 281, 6794, 439, 295, 552, 293, 11, 291, 458, 11, 437, 264, 7300, 366, 11, 50614, 50614, 570, 436, 434, 534, 406, 300, 819, 13, 50814, 50814, 407, 286, 478, 516, 281, 312, 1228, 20346, 46, 7540, 337, 341, 1729, 1389, 337, 527, 2698, 12, 48172, 24420, 5003, 2539, 13, 51214, 51214, 407, 264, 700, 551, 307, 286, 478, 516, 281, 11, 264, 700, 1389, 300, 321, 434, 516, 281, 574, 412, 51464, 51464, 307, 516, 281, 312, 341, 46533, 5003, 2539, 13, 51614, 51614, 407, 341, 307, 3700, 264, 1389, 300, 291, 434, 516, 281, 1190, 666, 364, 3518, 881, 295, 264, 565, 11, 558, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07308726184136044, "compression_ratio": 1.8091286307053942, "no_speech_prob": 8.664027518534567e-06}, {"id": 143, "seek": 44300, "start": 460.0, "end": 465.0, "text": " So the first thing is I'm going to, the first case that we're going to look at", "tokens": [50364, 293, 577, 281, 6794, 439, 295, 552, 293, 11, 291, 458, 11, 437, 264, 7300, 366, 11, 50614, 50614, 570, 436, 434, 534, 406, 300, 819, 13, 50814, 50814, 407, 286, 478, 516, 281, 312, 1228, 20346, 46, 7540, 337, 341, 1729, 1389, 337, 527, 2698, 12, 48172, 24420, 5003, 2539, 13, 51214, 51214, 407, 264, 700, 551, 307, 286, 478, 516, 281, 11, 264, 700, 1389, 300, 321, 434, 516, 281, 574, 412, 51464, 51464, 307, 516, 281, 312, 341, 46533, 5003, 2539, 13, 51614, 51614, 407, 341, 307, 3700, 264, 1389, 300, 291, 434, 516, 281, 1190, 666, 364, 3518, 881, 295, 264, 565, 11, 558, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07308726184136044, "compression_ratio": 1.8091286307053942, "no_speech_prob": 8.664027518534567e-06}, {"id": 144, "seek": 44300, "start": 465.0, "end": 468.0, "text": " is going to be this supervised transfer learning.", "tokens": [50364, 293, 577, 281, 6794, 439, 295, 552, 293, 11, 291, 458, 11, 437, 264, 7300, 366, 11, 50614, 50614, 570, 436, 434, 534, 406, 300, 819, 13, 50814, 50814, 407, 286, 478, 516, 281, 312, 1228, 20346, 46, 7540, 337, 341, 1729, 1389, 337, 527, 2698, 12, 48172, 24420, 5003, 2539, 13, 51214, 51214, 407, 264, 700, 551, 307, 286, 478, 516, 281, 11, 264, 700, 1389, 300, 321, 434, 516, 281, 574, 412, 51464, 51464, 307, 516, 281, 312, 341, 46533, 5003, 2539, 13, 51614, 51614, 407, 341, 307, 3700, 264, 1389, 300, 291, 434, 516, 281, 1190, 666, 364, 3518, 881, 295, 264, 565, 11, 558, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07308726184136044, "compression_ratio": 1.8091286307053942, "no_speech_prob": 8.664027518534567e-06}, {"id": 145, "seek": 44300, "start": 468.0, "end": 472.0, "text": " So this is likely the case that you're going to run into an industry most of the time, right?", "tokens": [50364, 293, 577, 281, 6794, 439, 295, 552, 293, 11, 291, 458, 11, 437, 264, 7300, 366, 11, 50614, 50614, 570, 436, 434, 534, 406, 300, 819, 13, 50814, 50814, 407, 286, 478, 516, 281, 312, 1228, 20346, 46, 7540, 337, 341, 1729, 1389, 337, 527, 2698, 12, 48172, 24420, 5003, 2539, 13, 51214, 51214, 407, 264, 700, 551, 307, 286, 478, 516, 281, 11, 264, 700, 1389, 300, 321, 434, 516, 281, 574, 412, 51464, 51464, 307, 516, 281, 312, 341, 46533, 5003, 2539, 13, 51614, 51614, 407, 341, 307, 3700, 264, 1389, 300, 291, 434, 516, 281, 1190, 666, 364, 3518, 881, 295, 264, 565, 11, 558, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07308726184136044, "compression_ratio": 1.8091286307053942, "no_speech_prob": 8.664027518534567e-06}, {"id": 146, "seek": 47200, "start": 472.0, "end": 477.0, "text": " So you have a small data set of images and some compute budget.", "tokens": [50364, 407, 291, 362, 257, 1359, 1412, 992, 295, 5267, 293, 512, 14722, 4706, 13, 50614, 50614, 400, 294, 341, 1389, 11, 321, 434, 516, 281, 2235, 484, 257, 5015, 31890, 12, 2803, 11, 558, 30, 50764, 50764, 407, 456, 366, 867, 5015, 45, 1385, 11, 2443, 11, 2625, 11, 2035, 11, 21055, 11, 2119, 17, 11, 457, 2625, 307, 733, 295, 411, 257, 3844, 4008, 13, 51164, 51164, 467, 311, 406, 370, 955, 300, 309, 311, 1687, 5124, 281, 3847, 11, 51314, 51314, 293, 309, 311, 406, 370, 1359, 300, 309, 1582, 380, 360, 1340, 1880, 337, 291, 13, 51514, 51514, 5459, 11, 341, 390, 659, 12, 17227, 2001, 322, 29903, 31890, 11, 293, 309, 390, 659, 12, 17227, 2001, 337, 21538, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07364638771597795, "compression_ratio": 1.5682656826568266, "no_speech_prob": 5.900996256968938e-05}, {"id": 147, "seek": 47200, "start": 477.0, "end": 480.0, "text": " And in this case, we're going to pull out a ResNet-50, right?", "tokens": [50364, 407, 291, 362, 257, 1359, 1412, 992, 295, 5267, 293, 512, 14722, 4706, 13, 50614, 50614, 400, 294, 341, 1389, 11, 321, 434, 516, 281, 2235, 484, 257, 5015, 31890, 12, 2803, 11, 558, 30, 50764, 50764, 407, 456, 366, 867, 5015, 45, 1385, 11, 2443, 11, 2625, 11, 2035, 11, 21055, 11, 2119, 17, 11, 457, 2625, 307, 733, 295, 411, 257, 3844, 4008, 13, 51164, 51164, 467, 311, 406, 370, 955, 300, 309, 311, 1687, 5124, 281, 3847, 11, 51314, 51314, 293, 309, 311, 406, 370, 1359, 300, 309, 1582, 380, 360, 1340, 1880, 337, 291, 13, 51514, 51514, 5459, 11, 341, 390, 659, 12, 17227, 2001, 322, 29903, 31890, 11, 293, 309, 390, 659, 12, 17227, 2001, 337, 21538, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07364638771597795, "compression_ratio": 1.5682656826568266, "no_speech_prob": 5.900996256968938e-05}, {"id": 148, "seek": 47200, "start": 480.0, "end": 488.0, "text": " So there are many ResNets, 18, 50, whatever, 101, 152, but 50 is kind of like a sweet spot.", "tokens": [50364, 407, 291, 362, 257, 1359, 1412, 992, 295, 5267, 293, 512, 14722, 4706, 13, 50614, 50614, 400, 294, 341, 1389, 11, 321, 434, 516, 281, 2235, 484, 257, 5015, 31890, 12, 2803, 11, 558, 30, 50764, 50764, 407, 456, 366, 867, 5015, 45, 1385, 11, 2443, 11, 2625, 11, 2035, 11, 21055, 11, 2119, 17, 11, 457, 2625, 307, 733, 295, 411, 257, 3844, 4008, 13, 51164, 51164, 467, 311, 406, 370, 955, 300, 309, 311, 1687, 5124, 281, 3847, 11, 51314, 51314, 293, 309, 311, 406, 370, 1359, 300, 309, 1582, 380, 360, 1340, 1880, 337, 291, 13, 51514, 51514, 5459, 11, 341, 390, 659, 12, 17227, 2001, 322, 29903, 31890, 11, 293, 309, 390, 659, 12, 17227, 2001, 337, 21538, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07364638771597795, "compression_ratio": 1.5682656826568266, "no_speech_prob": 5.900996256968938e-05}, {"id": 149, "seek": 47200, "start": 488.0, "end": 491.0, "text": " It's not so big that it's super expensive to train,", "tokens": [50364, 407, 291, 362, 257, 1359, 1412, 992, 295, 5267, 293, 512, 14722, 4706, 13, 50614, 50614, 400, 294, 341, 1389, 11, 321, 434, 516, 281, 2235, 484, 257, 5015, 31890, 12, 2803, 11, 558, 30, 50764, 50764, 407, 456, 366, 867, 5015, 45, 1385, 11, 2443, 11, 2625, 11, 2035, 11, 21055, 11, 2119, 17, 11, 457, 2625, 307, 733, 295, 411, 257, 3844, 4008, 13, 51164, 51164, 467, 311, 406, 370, 955, 300, 309, 311, 1687, 5124, 281, 3847, 11, 51314, 51314, 293, 309, 311, 406, 370, 1359, 300, 309, 1582, 380, 360, 1340, 1880, 337, 291, 13, 51514, 51514, 5459, 11, 341, 390, 659, 12, 17227, 2001, 322, 29903, 31890, 11, 293, 309, 390, 659, 12, 17227, 2001, 337, 21538, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07364638771597795, "compression_ratio": 1.5682656826568266, "no_speech_prob": 5.900996256968938e-05}, {"id": 150, "seek": 47200, "start": 491.0, "end": 495.0, "text": " and it's not so small that it won't do anything interesting for you.", "tokens": [50364, 407, 291, 362, 257, 1359, 1412, 992, 295, 5267, 293, 512, 14722, 4706, 13, 50614, 50614, 400, 294, 341, 1389, 11, 321, 434, 516, 281, 2235, 484, 257, 5015, 31890, 12, 2803, 11, 558, 30, 50764, 50764, 407, 456, 366, 867, 5015, 45, 1385, 11, 2443, 11, 2625, 11, 2035, 11, 21055, 11, 2119, 17, 11, 457, 2625, 307, 733, 295, 411, 257, 3844, 4008, 13, 51164, 51164, 467, 311, 406, 370, 955, 300, 309, 311, 1687, 5124, 281, 3847, 11, 51314, 51314, 293, 309, 311, 406, 370, 1359, 300, 309, 1582, 380, 360, 1340, 1880, 337, 291, 13, 51514, 51514, 5459, 11, 341, 390, 659, 12, 17227, 2001, 322, 29903, 31890, 11, 293, 309, 390, 659, 12, 17227, 2001, 337, 21538, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07364638771597795, "compression_ratio": 1.5682656826568266, "no_speech_prob": 5.900996256968938e-05}, {"id": 151, "seek": 47200, "start": 495.0, "end": 500.0, "text": " Remember, this was pre-trained on ImageNet, and it was pre-trained for classification.", "tokens": [50364, 407, 291, 362, 257, 1359, 1412, 992, 295, 5267, 293, 512, 14722, 4706, 13, 50614, 50614, 400, 294, 341, 1389, 11, 321, 434, 516, 281, 2235, 484, 257, 5015, 31890, 12, 2803, 11, 558, 30, 50764, 50764, 407, 456, 366, 867, 5015, 45, 1385, 11, 2443, 11, 2625, 11, 2035, 11, 21055, 11, 2119, 17, 11, 457, 2625, 307, 733, 295, 411, 257, 3844, 4008, 13, 51164, 51164, 467, 311, 406, 370, 955, 300, 309, 311, 1687, 5124, 281, 3847, 11, 51314, 51314, 293, 309, 311, 406, 370, 1359, 300, 309, 1582, 380, 360, 1340, 1880, 337, 291, 13, 51514, 51514, 5459, 11, 341, 390, 659, 12, 17227, 2001, 322, 29903, 31890, 11, 293, 309, 390, 659, 12, 17227, 2001, 337, 21538, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07364638771597795, "compression_ratio": 1.5682656826568266, "no_speech_prob": 5.900996256968938e-05}, {"id": 152, "seek": 50000, "start": 500.0, "end": 504.0, "text": " And ImageNet is a data set that has a thousand categories.", "tokens": [50364, 400, 29903, 31890, 307, 257, 1412, 992, 300, 575, 257, 4714, 10479, 13, 50564, 50564, 6947, 295, 552, 575, 257, 4714, 5267, 11, 370, 309, 311, 411, 472, 2459, 11, 9810, 472, 2459, 5267, 11, 558, 30, 50864, 50864, 407, 300, 311, 983, 309, 311, 767, 588, 4406, 11, 570, 341, 2603, 2372, 295, 1412, 4045, 505, 281, 42923, 257, 588, 11, 51264, 51264, 257, 2316, 300, 575, 257, 588, 665, 4059, 294, 2115, 295, 3303, 5267, 11, 558, 30, 51464, 51464, 1436, 309, 311, 668, 11, 411, 729, 4714, 5359, 366, 5288, 295, 3303, 6565, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11073542113351349, "compression_ratio": 1.7404255319148936, "no_speech_prob": 9.366436643176712e-06}, {"id": 153, "seek": 50000, "start": 504.0, "end": 510.0, "text": " Each of them has a thousand images, so it's like one million, roughly one million images, right?", "tokens": [50364, 400, 29903, 31890, 307, 257, 1412, 992, 300, 575, 257, 4714, 10479, 13, 50564, 50564, 6947, 295, 552, 575, 257, 4714, 5267, 11, 370, 309, 311, 411, 472, 2459, 11, 9810, 472, 2459, 5267, 11, 558, 30, 50864, 50864, 407, 300, 311, 983, 309, 311, 767, 588, 4406, 11, 570, 341, 2603, 2372, 295, 1412, 4045, 505, 281, 42923, 257, 588, 11, 51264, 51264, 257, 2316, 300, 575, 257, 588, 665, 4059, 294, 2115, 295, 3303, 5267, 11, 558, 30, 51464, 51464, 1436, 309, 311, 668, 11, 411, 729, 4714, 5359, 366, 5288, 295, 3303, 6565, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11073542113351349, "compression_ratio": 1.7404255319148936, "no_speech_prob": 9.366436643176712e-06}, {"id": 154, "seek": 50000, "start": 510.0, "end": 518.0, "text": " So that's why it's actually very successful, because this huge amount of data allows us to distill a very,", "tokens": [50364, 400, 29903, 31890, 307, 257, 1412, 992, 300, 575, 257, 4714, 10479, 13, 50564, 50564, 6947, 295, 552, 575, 257, 4714, 5267, 11, 370, 309, 311, 411, 472, 2459, 11, 9810, 472, 2459, 5267, 11, 558, 30, 50864, 50864, 407, 300, 311, 983, 309, 311, 767, 588, 4406, 11, 570, 341, 2603, 2372, 295, 1412, 4045, 505, 281, 42923, 257, 588, 11, 51264, 51264, 257, 2316, 300, 575, 257, 588, 665, 4059, 294, 2115, 295, 3303, 5267, 11, 558, 30, 51464, 51464, 1436, 309, 311, 668, 11, 411, 729, 4714, 5359, 366, 5288, 295, 3303, 6565, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11073542113351349, "compression_ratio": 1.7404255319148936, "no_speech_prob": 9.366436643176712e-06}, {"id": 155, "seek": 50000, "start": 518.0, "end": 522.0, "text": " a model that has a very good prior in terms of natural images, right?", "tokens": [50364, 400, 29903, 31890, 307, 257, 1412, 992, 300, 575, 257, 4714, 10479, 13, 50564, 50564, 6947, 295, 552, 575, 257, 4714, 5267, 11, 370, 309, 311, 411, 472, 2459, 11, 9810, 472, 2459, 5267, 11, 558, 30, 50864, 50864, 407, 300, 311, 983, 309, 311, 767, 588, 4406, 11, 570, 341, 2603, 2372, 295, 1412, 4045, 505, 281, 42923, 257, 588, 11, 51264, 51264, 257, 2316, 300, 575, 257, 588, 665, 4059, 294, 2115, 295, 3303, 5267, 11, 558, 30, 51464, 51464, 1436, 309, 311, 668, 11, 411, 729, 4714, 5359, 366, 5288, 295, 3303, 6565, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11073542113351349, "compression_ratio": 1.7404255319148936, "no_speech_prob": 9.366436643176712e-06}, {"id": 156, "seek": 50000, "start": 522.0, "end": 529.0, "text": " Because it's been, like those thousand classes are names of natural objects.", "tokens": [50364, 400, 29903, 31890, 307, 257, 1412, 992, 300, 575, 257, 4714, 10479, 13, 50564, 50564, 6947, 295, 552, 575, 257, 4714, 5267, 11, 370, 309, 311, 411, 472, 2459, 11, 9810, 472, 2459, 5267, 11, 558, 30, 50864, 50864, 407, 300, 311, 983, 309, 311, 767, 588, 4406, 11, 570, 341, 2603, 2372, 295, 1412, 4045, 505, 281, 42923, 257, 588, 11, 51264, 51264, 257, 2316, 300, 575, 257, 588, 665, 4059, 294, 2115, 295, 3303, 5267, 11, 558, 30, 51464, 51464, 1436, 309, 311, 668, 11, 411, 729, 4714, 5359, 366, 5288, 295, 3303, 6565, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11073542113351349, "compression_ratio": 1.7404255319148936, "no_speech_prob": 9.366436643176712e-06}, {"id": 157, "seek": 52900, "start": 529.0, "end": 532.0, "text": " So that's why usually it works quite well.", "tokens": [50364, 407, 300, 311, 983, 2673, 309, 1985, 1596, 731, 13, 50514, 50514, 583, 26924, 11, 382, 291, 10932, 484, 949, 11, 498, 291, 528, 281, 764, 257, 3209, 337, 4625, 5267, 11, 50814, 50814, 689, 264, 12523, 295, 428, 5267, 366, 2584, 819, 813, 437, 436, 366, 294, 29903, 31890, 11, 51164, 51164, 550, 309, 311, 2584, 27317, 281, 362, 604, 733, 295, 8681, 1874, 13, 51464, 51464, 26554, 11, 718, 311, 722, 365, 437, 307, 12719, 1096, 337, 2710, 5267, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05338385430249301, "compression_ratio": 1.6217391304347826, "no_speech_prob": 1.696099752734881e-05}, {"id": 158, "seek": 52900, "start": 532.0, "end": 538.0, "text": " But nevertheless, as you pointed out before, if you want to use a network for medical images,", "tokens": [50364, 407, 300, 311, 983, 2673, 309, 1985, 1596, 731, 13, 50514, 50514, 583, 26924, 11, 382, 291, 10932, 484, 949, 11, 498, 291, 528, 281, 764, 257, 3209, 337, 4625, 5267, 11, 50814, 50814, 689, 264, 12523, 295, 428, 5267, 366, 2584, 819, 813, 437, 436, 366, 294, 29903, 31890, 11, 51164, 51164, 550, 309, 311, 2584, 27317, 281, 362, 604, 733, 295, 8681, 1874, 13, 51464, 51464, 26554, 11, 718, 311, 722, 365, 437, 307, 12719, 1096, 337, 2710, 5267, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05338385430249301, "compression_ratio": 1.6217391304347826, "no_speech_prob": 1.696099752734881e-05}, {"id": 159, "seek": 52900, "start": 538.0, "end": 545.0, "text": " where the statistics of your images are completely different than what they are in ImageNet,", "tokens": [50364, 407, 300, 311, 983, 2673, 309, 1985, 1596, 731, 13, 50514, 50514, 583, 26924, 11, 382, 291, 10932, 484, 949, 11, 498, 291, 528, 281, 764, 257, 3209, 337, 4625, 5267, 11, 50814, 50814, 689, 264, 12523, 295, 428, 5267, 366, 2584, 819, 813, 437, 436, 366, 294, 29903, 31890, 11, 51164, 51164, 550, 309, 311, 2584, 27317, 281, 362, 604, 733, 295, 8681, 1874, 13, 51464, 51464, 26554, 11, 718, 311, 722, 365, 437, 307, 12719, 1096, 337, 2710, 5267, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05338385430249301, "compression_ratio": 1.6217391304347826, "no_speech_prob": 1.696099752734881e-05}, {"id": 160, "seek": 52900, "start": 545.0, "end": 551.0, "text": " then it's completely hopeless to have any kind of decent result.", "tokens": [50364, 407, 300, 311, 983, 2673, 309, 1985, 1596, 731, 13, 50514, 50514, 583, 26924, 11, 382, 291, 10932, 484, 949, 11, 498, 291, 528, 281, 764, 257, 3209, 337, 4625, 5267, 11, 50814, 50814, 689, 264, 12523, 295, 428, 5267, 366, 2584, 819, 813, 437, 436, 366, 294, 29903, 31890, 11, 51164, 51164, 550, 309, 311, 2584, 27317, 281, 362, 604, 733, 295, 8681, 1874, 13, 51464, 51464, 26554, 11, 718, 311, 722, 365, 437, 307, 12719, 1096, 337, 2710, 5267, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05338385430249301, "compression_ratio": 1.6217391304347826, "no_speech_prob": 1.696099752734881e-05}, {"id": 161, "seek": 52900, "start": 551.0, "end": 556.0, "text": " Nevertheless, let's start with what is commonly done for normal images, right?", "tokens": [50364, 407, 300, 311, 983, 2673, 309, 1985, 1596, 731, 13, 50514, 50514, 583, 26924, 11, 382, 291, 10932, 484, 949, 11, 498, 291, 528, 281, 764, 257, 3209, 337, 4625, 5267, 11, 50814, 50814, 689, 264, 12523, 295, 428, 5267, 366, 2584, 819, 813, 437, 436, 366, 294, 29903, 31890, 11, 51164, 51164, 550, 309, 311, 2584, 27317, 281, 362, 604, 733, 295, 8681, 1874, 13, 51464, 51464, 26554, 11, 718, 311, 722, 365, 437, 307, 12719, 1096, 337, 2710, 5267, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.05338385430249301, "compression_ratio": 1.6217391304347826, "no_speech_prob": 1.696099752734881e-05}, {"id": 162, "seek": 55600, "start": 556.0, "end": 563.0, "text": " Yeah, so I think to start, we're using this library called TorchVision, right?", "tokens": [50364, 865, 11, 370, 286, 519, 281, 722, 11, 321, 434, 1228, 341, 6405, 1219, 7160, 339, 53, 1991, 11, 558, 30, 50714, 50714, 407, 538, 264, 741, 51, 284, 339, 1469, 382, 731, 13, 50814, 50814, 400, 294, 456, 321, 362, 257, 3840, 295, 659, 12, 17227, 2001, 5245, 13, 407, 341, 472, 311, 257, 5015, 31890, 12, 2803, 13, 51064, 51064, 407, 286, 478, 516, 281, 992, 341, 281, 2074, 370, 286, 393, 3677, 300, 2316, 11, 597, 307, 516, 281, 5484, 11, 286, 6552, 11, 512, 17443, 13, 51464, 51464, 3769, 13, 1033, 11, 370, 456, 366, 264, 17443, 13, 51614, 51614, 400, 550, 586, 321, 393, 764, 341, 281, 1190, 21264, 11, 558, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14373385216578966, "compression_ratio": 1.5823293172690762, "no_speech_prob": 1.568761763337534e-05}, {"id": 163, "seek": 55600, "start": 563.0, "end": 565.0, "text": " So by the iTorch team as well.", "tokens": [50364, 865, 11, 370, 286, 519, 281, 722, 11, 321, 434, 1228, 341, 6405, 1219, 7160, 339, 53, 1991, 11, 558, 30, 50714, 50714, 407, 538, 264, 741, 51, 284, 339, 1469, 382, 731, 13, 50814, 50814, 400, 294, 456, 321, 362, 257, 3840, 295, 659, 12, 17227, 2001, 5245, 13, 407, 341, 472, 311, 257, 5015, 31890, 12, 2803, 13, 51064, 51064, 407, 286, 478, 516, 281, 992, 341, 281, 2074, 370, 286, 393, 3677, 300, 2316, 11, 597, 307, 516, 281, 5484, 11, 286, 6552, 11, 512, 17443, 13, 51464, 51464, 3769, 13, 1033, 11, 370, 456, 366, 264, 17443, 13, 51614, 51614, 400, 550, 586, 321, 393, 764, 341, 281, 1190, 21264, 11, 558, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14373385216578966, "compression_ratio": 1.5823293172690762, "no_speech_prob": 1.568761763337534e-05}, {"id": 164, "seek": 55600, "start": 565.0, "end": 570.0, "text": " And in there we have a bunch of pre-trained models. So this one's a ResNet-50.", "tokens": [50364, 865, 11, 370, 286, 519, 281, 722, 11, 321, 434, 1228, 341, 6405, 1219, 7160, 339, 53, 1991, 11, 558, 30, 50714, 50714, 407, 538, 264, 741, 51, 284, 339, 1469, 382, 731, 13, 50814, 50814, 400, 294, 456, 321, 362, 257, 3840, 295, 659, 12, 17227, 2001, 5245, 13, 407, 341, 472, 311, 257, 5015, 31890, 12, 2803, 13, 51064, 51064, 407, 286, 478, 516, 281, 992, 341, 281, 2074, 370, 286, 393, 3677, 300, 2316, 11, 597, 307, 516, 281, 5484, 11, 286, 6552, 11, 512, 17443, 13, 51464, 51464, 3769, 13, 1033, 11, 370, 456, 366, 264, 17443, 13, 51614, 51614, 400, 550, 586, 321, 393, 764, 341, 281, 1190, 21264, 11, 558, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14373385216578966, "compression_ratio": 1.5823293172690762, "no_speech_prob": 1.568761763337534e-05}, {"id": 165, "seek": 55600, "start": 570.0, "end": 578.0, "text": " So I'm going to set this to true so I can load that model, which is going to download, I assume, some weights.", "tokens": [50364, 865, 11, 370, 286, 519, 281, 722, 11, 321, 434, 1228, 341, 6405, 1219, 7160, 339, 53, 1991, 11, 558, 30, 50714, 50714, 407, 538, 264, 741, 51, 284, 339, 1469, 382, 731, 13, 50814, 50814, 400, 294, 456, 321, 362, 257, 3840, 295, 659, 12, 17227, 2001, 5245, 13, 407, 341, 472, 311, 257, 5015, 31890, 12, 2803, 13, 51064, 51064, 407, 286, 478, 516, 281, 992, 341, 281, 2074, 370, 286, 393, 3677, 300, 2316, 11, 597, 307, 516, 281, 5484, 11, 286, 6552, 11, 512, 17443, 13, 51464, 51464, 3769, 13, 1033, 11, 370, 456, 366, 264, 17443, 13, 51614, 51614, 400, 550, 586, 321, 393, 764, 341, 281, 1190, 21264, 11, 558, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14373385216578966, "compression_ratio": 1.5823293172690762, "no_speech_prob": 1.568761763337534e-05}, {"id": 166, "seek": 55600, "start": 578.0, "end": 581.0, "text": " Great. Okay, so there are the weights.", "tokens": [50364, 865, 11, 370, 286, 519, 281, 722, 11, 321, 434, 1228, 341, 6405, 1219, 7160, 339, 53, 1991, 11, 558, 30, 50714, 50714, 407, 538, 264, 741, 51, 284, 339, 1469, 382, 731, 13, 50814, 50814, 400, 294, 456, 321, 362, 257, 3840, 295, 659, 12, 17227, 2001, 5245, 13, 407, 341, 472, 311, 257, 5015, 31890, 12, 2803, 13, 51064, 51064, 407, 286, 478, 516, 281, 992, 341, 281, 2074, 370, 286, 393, 3677, 300, 2316, 11, 597, 307, 516, 281, 5484, 11, 286, 6552, 11, 512, 17443, 13, 51464, 51464, 3769, 13, 1033, 11, 370, 456, 366, 264, 17443, 13, 51614, 51614, 400, 550, 586, 321, 393, 764, 341, 281, 1190, 21264, 11, 558, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14373385216578966, "compression_ratio": 1.5823293172690762, "no_speech_prob": 1.568761763337534e-05}, {"id": 167, "seek": 55600, "start": 581.0, "end": 584.0, "text": " And then now we can use this to run predictions, right?", "tokens": [50364, 865, 11, 370, 286, 519, 281, 722, 11, 321, 434, 1228, 341, 6405, 1219, 7160, 339, 53, 1991, 11, 558, 30, 50714, 50714, 407, 538, 264, 741, 51, 284, 339, 1469, 382, 731, 13, 50814, 50814, 400, 294, 456, 321, 362, 257, 3840, 295, 659, 12, 17227, 2001, 5245, 13, 407, 341, 472, 311, 257, 5015, 31890, 12, 2803, 13, 51064, 51064, 407, 286, 478, 516, 281, 992, 341, 281, 2074, 370, 286, 393, 3677, 300, 2316, 11, 597, 307, 516, 281, 5484, 11, 286, 6552, 11, 512, 17443, 13, 51464, 51464, 3769, 13, 1033, 11, 370, 456, 366, 264, 17443, 13, 51614, 51614, 400, 550, 586, 321, 393, 764, 341, 281, 1190, 21264, 11, 558, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14373385216578966, "compression_ratio": 1.5823293172690762, "no_speech_prob": 1.568761763337534e-05}, {"id": 168, "seek": 58400, "start": 584.0, "end": 591.0, "text": " So let's pretend that our data set that we actually care about has 10 classes, right?", "tokens": [50364, 407, 718, 311, 11865, 300, 527, 1412, 992, 300, 321, 767, 1127, 466, 575, 1266, 5359, 11, 558, 30, 50714, 50714, 400, 729, 5359, 366, 411, 17259, 11, 6832, 11, 2035, 13, 50864, 50864, 407, 286, 478, 18309, 570, 456, 311, 341, 1412, 992, 1219, 383, 12775, 1899, 3279, 11, 558, 30, 51064, 51064, 407, 383, 12775, 1899, 3279, 13, 400, 341, 1412, 992, 1542, 411, 341, 11, 558, 30, 51364, 51364, 407, 797, 11, 456, 366, 5870, 5267, 13, 814, 434, 8858, 538, 8858, 18668, 293, 1045, 9235, 680, 2017, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12827841440836588, "compression_ratio": 1.5753424657534247, "no_speech_prob": 1.1125030141556635e-05}, {"id": 169, "seek": 58400, "start": 591.0, "end": 594.0, "text": " And those classes are like frog, horse, whatever.", "tokens": [50364, 407, 718, 311, 11865, 300, 527, 1412, 992, 300, 321, 767, 1127, 466, 575, 1266, 5359, 11, 558, 30, 50714, 50714, 400, 729, 5359, 366, 411, 17259, 11, 6832, 11, 2035, 13, 50864, 50864, 407, 286, 478, 18309, 570, 456, 311, 341, 1412, 992, 1219, 383, 12775, 1899, 3279, 11, 558, 30, 51064, 51064, 407, 383, 12775, 1899, 3279, 13, 400, 341, 1412, 992, 1542, 411, 341, 11, 558, 30, 51364, 51364, 407, 797, 11, 456, 366, 5870, 5267, 13, 814, 434, 8858, 538, 8858, 18668, 293, 1045, 9235, 680, 2017, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12827841440836588, "compression_ratio": 1.5753424657534247, "no_speech_prob": 1.1125030141556635e-05}, {"id": 170, "seek": 58400, "start": 594.0, "end": 598.0, "text": " So I'm cheating because there's this data set called CIFAR10, right?", "tokens": [50364, 407, 718, 311, 11865, 300, 527, 1412, 992, 300, 321, 767, 1127, 466, 575, 1266, 5359, 11, 558, 30, 50714, 50714, 400, 729, 5359, 366, 411, 17259, 11, 6832, 11, 2035, 13, 50864, 50864, 407, 286, 478, 18309, 570, 456, 311, 341, 1412, 992, 1219, 383, 12775, 1899, 3279, 11, 558, 30, 51064, 51064, 407, 383, 12775, 1899, 3279, 13, 400, 341, 1412, 992, 1542, 411, 341, 11, 558, 30, 51364, 51364, 407, 797, 11, 456, 366, 5870, 5267, 13, 814, 434, 8858, 538, 8858, 18668, 293, 1045, 9235, 680, 2017, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12827841440836588, "compression_ratio": 1.5753424657534247, "no_speech_prob": 1.1125030141556635e-05}, {"id": 171, "seek": 58400, "start": 598.0, "end": 604.0, "text": " So CIFAR10. And this data set looks like this, right?", "tokens": [50364, 407, 718, 311, 11865, 300, 527, 1412, 992, 300, 321, 767, 1127, 466, 575, 1266, 5359, 11, 558, 30, 50714, 50714, 400, 729, 5359, 366, 411, 17259, 11, 6832, 11, 2035, 13, 50864, 50864, 407, 286, 478, 18309, 570, 456, 311, 341, 1412, 992, 1219, 383, 12775, 1899, 3279, 11, 558, 30, 51064, 51064, 407, 383, 12775, 1899, 3279, 13, 400, 341, 1412, 992, 1542, 411, 341, 11, 558, 30, 51364, 51364, 407, 797, 11, 456, 366, 5870, 5267, 13, 814, 434, 8858, 538, 8858, 18668, 293, 1045, 9235, 680, 2017, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12827841440836588, "compression_ratio": 1.5753424657534247, "no_speech_prob": 1.1125030141556635e-05}, {"id": 172, "seek": 58400, "start": 604.0, "end": 611.0, "text": " So again, there are tiny images. They're 32 by 32 pixels and three channels per color.", "tokens": [50364, 407, 718, 311, 11865, 300, 527, 1412, 992, 300, 321, 767, 1127, 466, 575, 1266, 5359, 11, 558, 30, 50714, 50714, 400, 729, 5359, 366, 411, 17259, 11, 6832, 11, 2035, 13, 50864, 50864, 407, 286, 478, 18309, 570, 456, 311, 341, 1412, 992, 1219, 383, 12775, 1899, 3279, 11, 558, 30, 51064, 51064, 407, 383, 12775, 1899, 3279, 13, 400, 341, 1412, 992, 1542, 411, 341, 11, 558, 30, 51364, 51364, 407, 797, 11, 456, 366, 5870, 5267, 13, 814, 434, 8858, 538, 8858, 18668, 293, 1045, 9235, 680, 2017, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12827841440836588, "compression_ratio": 1.5753424657534247, "no_speech_prob": 1.1125030141556635e-05}, {"id": 173, "seek": 61100, "start": 611.0, "end": 618.0, "text": " But you know, it's a useful toy data set. It's better than NMNIST, especially because you're using pre-trained ImageNet.", "tokens": [50364, 583, 291, 458, 11, 309, 311, 257, 4420, 12058, 1412, 992, 13, 467, 311, 1101, 813, 426, 44, 45, 19756, 11, 2318, 570, 291, 434, 1228, 659, 12, 17227, 2001, 29903, 31890, 13, 50714, 50714, 400, 286, 478, 1238, 988, 322, 29903, 31890, 456, 366, 406, 257, 688, 295, 7592, 27011, 420, 1011, 26859, 27011, 11, 370, 309, 2759, 380, 5003, 1687, 731, 13, 51014, 51014, 583, 456, 366, 7197, 293, 11111, 293, 9009, 293, 439, 300, 1507, 13, 407, 912, 733, 295, 9274, 13, 4919, 11, 912, 733, 295, 7719, 13, 51414, 51414, 1079, 11, 869, 13, 407, 718, 311, 764, 300, 11, 1074, 13, 407, 321, 434, 516, 281, 992, 493, 527, 1412, 992, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13470980016196646, "compression_ratio": 1.6153846153846154, "no_speech_prob": 6.747679435648024e-06}, {"id": 174, "seek": 61100, "start": 618.0, "end": 624.0, "text": " And I'm pretty sure on ImageNet there are not a lot of fake digits or handwritten digits, so it wouldn't transfer super well.", "tokens": [50364, 583, 291, 458, 11, 309, 311, 257, 4420, 12058, 1412, 992, 13, 467, 311, 1101, 813, 426, 44, 45, 19756, 11, 2318, 570, 291, 434, 1228, 659, 12, 17227, 2001, 29903, 31890, 13, 50714, 50714, 400, 286, 478, 1238, 988, 322, 29903, 31890, 456, 366, 406, 257, 688, 295, 7592, 27011, 420, 1011, 26859, 27011, 11, 370, 309, 2759, 380, 5003, 1687, 731, 13, 51014, 51014, 583, 456, 366, 7197, 293, 11111, 293, 9009, 293, 439, 300, 1507, 13, 407, 912, 733, 295, 9274, 13, 4919, 11, 912, 733, 295, 7719, 13, 51414, 51414, 1079, 11, 869, 13, 407, 718, 311, 764, 300, 11, 1074, 13, 407, 321, 434, 516, 281, 992, 493, 527, 1412, 992, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13470980016196646, "compression_ratio": 1.6153846153846154, "no_speech_prob": 6.747679435648024e-06}, {"id": 175, "seek": 61100, "start": 624.0, "end": 632.0, "text": " But there are dogs and cats and birds and all that stuff. So same kind of domain. Sorry, same kind of category.", "tokens": [50364, 583, 291, 458, 11, 309, 311, 257, 4420, 12058, 1412, 992, 13, 467, 311, 1101, 813, 426, 44, 45, 19756, 11, 2318, 570, 291, 434, 1228, 659, 12, 17227, 2001, 29903, 31890, 13, 50714, 50714, 400, 286, 478, 1238, 988, 322, 29903, 31890, 456, 366, 406, 257, 688, 295, 7592, 27011, 420, 1011, 26859, 27011, 11, 370, 309, 2759, 380, 5003, 1687, 731, 13, 51014, 51014, 583, 456, 366, 7197, 293, 11111, 293, 9009, 293, 439, 300, 1507, 13, 407, 912, 733, 295, 9274, 13, 4919, 11, 912, 733, 295, 7719, 13, 51414, 51414, 1079, 11, 869, 13, 407, 718, 311, 764, 300, 11, 1074, 13, 407, 321, 434, 516, 281, 992, 493, 527, 1412, 992, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13470980016196646, "compression_ratio": 1.6153846153846154, "no_speech_prob": 6.747679435648024e-06}, {"id": 176, "seek": 61100, "start": 632.0, "end": 638.0, "text": " Yes, great. So let's use that, guys. So we're going to set up our data set, right?", "tokens": [50364, 583, 291, 458, 11, 309, 311, 257, 4420, 12058, 1412, 992, 13, 467, 311, 1101, 813, 426, 44, 45, 19756, 11, 2318, 570, 291, 434, 1228, 659, 12, 17227, 2001, 29903, 31890, 13, 50714, 50714, 400, 286, 478, 1238, 988, 322, 29903, 31890, 456, 366, 406, 257, 688, 295, 7592, 27011, 420, 1011, 26859, 27011, 11, 370, 309, 2759, 380, 5003, 1687, 731, 13, 51014, 51014, 583, 456, 366, 7197, 293, 11111, 293, 9009, 293, 439, 300, 1507, 13, 407, 912, 733, 295, 9274, 13, 4919, 11, 912, 733, 295, 7719, 13, 51414, 51414, 1079, 11, 869, 13, 407, 718, 311, 764, 300, 11, 1074, 13, 407, 321, 434, 516, 281, 992, 493, 527, 1412, 992, 11, 558, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.13470980016196646, "compression_ratio": 1.6153846153846154, "no_speech_prob": 6.747679435648024e-06}, {"id": 177, "seek": 63800, "start": 638.0, "end": 646.0, "text": " So let's pull in CIFAR10 again from TorchVision. Yep.", "tokens": [50364, 407, 718, 311, 2235, 294, 383, 12775, 1899, 3279, 797, 490, 7160, 339, 53, 1991, 13, 7010, 13, 50764, 50764, 400, 550, 321, 366, 516, 281, 764, 613, 35592, 11, 558, 30, 50964, 50964, 407, 746, 300, 311, 4420, 5646, 337, 613, 3331, 307, 281, 2710, 1125, 428, 3256, 13, 51214, 51214, 407, 321, 434, 516, 281, 652, 309, 4018, 914, 293, 550, 472, 337, 3832, 25163, 11, 558, 30, 51414, 51414, 407, 321, 434, 516, 281, 909, 341, 322, 510, 13, 400, 286, 478, 406, 516, 281, 909, 309, 558, 586, 570, 286, 528, 281, 767, 7542, 264, 3256, 337, 291, 370, 291, 393, 536, 437, 309, 1542, 411, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07901782160219939, "compression_ratio": 1.6705882352941177, "no_speech_prob": 2.1111209207447246e-05}, {"id": 178, "seek": 63800, "start": 646.0, "end": 650.0, "text": " And then we are going to use these transforms, right?", "tokens": [50364, 407, 718, 311, 2235, 294, 383, 12775, 1899, 3279, 797, 490, 7160, 339, 53, 1991, 13, 7010, 13, 50764, 50764, 400, 550, 321, 366, 516, 281, 764, 613, 35592, 11, 558, 30, 50964, 50964, 407, 746, 300, 311, 4420, 5646, 337, 613, 3331, 307, 281, 2710, 1125, 428, 3256, 13, 51214, 51214, 407, 321, 434, 516, 281, 652, 309, 4018, 914, 293, 550, 472, 337, 3832, 25163, 11, 558, 30, 51414, 51414, 407, 321, 434, 516, 281, 909, 341, 322, 510, 13, 400, 286, 478, 406, 516, 281, 909, 309, 558, 586, 570, 286, 528, 281, 767, 7542, 264, 3256, 337, 291, 370, 291, 393, 536, 437, 309, 1542, 411, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07901782160219939, "compression_ratio": 1.6705882352941177, "no_speech_prob": 2.1111209207447246e-05}, {"id": 179, "seek": 63800, "start": 650.0, "end": 655.0, "text": " So something that's useful normally for these cases is to normalize your image.", "tokens": [50364, 407, 718, 311, 2235, 294, 383, 12775, 1899, 3279, 797, 490, 7160, 339, 53, 1991, 13, 7010, 13, 50764, 50764, 400, 550, 321, 366, 516, 281, 764, 613, 35592, 11, 558, 30, 50964, 50964, 407, 746, 300, 311, 4420, 5646, 337, 613, 3331, 307, 281, 2710, 1125, 428, 3256, 13, 51214, 51214, 407, 321, 434, 516, 281, 652, 309, 4018, 914, 293, 550, 472, 337, 3832, 25163, 11, 558, 30, 51414, 51414, 407, 321, 434, 516, 281, 909, 341, 322, 510, 13, 400, 286, 478, 406, 516, 281, 909, 309, 558, 586, 570, 286, 528, 281, 767, 7542, 264, 3256, 337, 291, 370, 291, 393, 536, 437, 309, 1542, 411, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07901782160219939, "compression_ratio": 1.6705882352941177, "no_speech_prob": 2.1111209207447246e-05}, {"id": 180, "seek": 63800, "start": 655.0, "end": 659.0, "text": " So we're going to make it zero mean and then one for standard deviation, right?", "tokens": [50364, 407, 718, 311, 2235, 294, 383, 12775, 1899, 3279, 797, 490, 7160, 339, 53, 1991, 13, 7010, 13, 50764, 50764, 400, 550, 321, 366, 516, 281, 764, 613, 35592, 11, 558, 30, 50964, 50964, 407, 746, 300, 311, 4420, 5646, 337, 613, 3331, 307, 281, 2710, 1125, 428, 3256, 13, 51214, 51214, 407, 321, 434, 516, 281, 652, 309, 4018, 914, 293, 550, 472, 337, 3832, 25163, 11, 558, 30, 51414, 51414, 407, 321, 434, 516, 281, 909, 341, 322, 510, 13, 400, 286, 478, 406, 516, 281, 909, 309, 558, 586, 570, 286, 528, 281, 767, 7542, 264, 3256, 337, 291, 370, 291, 393, 536, 437, 309, 1542, 411, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07901782160219939, "compression_ratio": 1.6705882352941177, "no_speech_prob": 2.1111209207447246e-05}, {"id": 181, "seek": 63800, "start": 659.0, "end": 666.0, "text": " So we're going to add this on here. And I'm not going to add it right now because I want to actually plot the image for you so you can see what it looks like.", "tokens": [50364, 407, 718, 311, 2235, 294, 383, 12775, 1899, 3279, 797, 490, 7160, 339, 53, 1991, 13, 7010, 13, 50764, 50764, 400, 550, 321, 366, 516, 281, 764, 613, 35592, 11, 558, 30, 50964, 50964, 407, 746, 300, 311, 4420, 5646, 337, 613, 3331, 307, 281, 2710, 1125, 428, 3256, 13, 51214, 51214, 407, 321, 434, 516, 281, 652, 309, 4018, 914, 293, 550, 472, 337, 3832, 25163, 11, 558, 30, 51414, 51414, 407, 321, 434, 516, 281, 909, 341, 322, 510, 13, 400, 286, 478, 406, 516, 281, 909, 309, 558, 586, 570, 286, 528, 281, 767, 7542, 264, 3256, 337, 291, 370, 291, 393, 536, 437, 309, 1542, 411, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.07901782160219939, "compression_ratio": 1.6705882352941177, "no_speech_prob": 2.1111209207447246e-05}, {"id": 182, "seek": 66600, "start": 666.0, "end": 673.0, "text": " So I do this and then that will actually download. I need to import that. Transforms, right?", "tokens": [50364, 407, 286, 360, 341, 293, 550, 300, 486, 767, 5484, 13, 286, 643, 281, 974, 300, 13, 27938, 82, 11, 558, 30, 50714, 50714, 1692, 490, 7160, 339, 53, 1991, 13, 26391, 35592, 13, 50964, 50964, 1033, 11, 370, 341, 307, 516, 281, 5484, 11, 8947, 11, 293, 550, 321, 362, 341, 1412, 992, 13, 3769, 13, 407, 300, 311, 1919, 281, 352, 13, 51364, 51364, 407, 718, 385, 445, 7542, 309, 586, 13, 407, 286, 478, 445, 516, 281, 5055, 9163, 512, 4471, 7542, 22854, 3089, 281, 360, 300, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.18449377762643915, "compression_ratio": 1.5924170616113744, "no_speech_prob": 6.24051472186693e-06}, {"id": 183, "seek": 66600, "start": 673.0, "end": 678.0, "text": " Here from TorchVision. Import transforms.", "tokens": [50364, 407, 286, 360, 341, 293, 550, 300, 486, 767, 5484, 13, 286, 643, 281, 974, 300, 13, 27938, 82, 11, 558, 30, 50714, 50714, 1692, 490, 7160, 339, 53, 1991, 13, 26391, 35592, 13, 50964, 50964, 1033, 11, 370, 341, 307, 516, 281, 5484, 11, 8947, 11, 293, 550, 321, 362, 341, 1412, 992, 13, 3769, 13, 407, 300, 311, 1919, 281, 352, 13, 51364, 51364, 407, 718, 385, 445, 7542, 309, 586, 13, 407, 286, 478, 445, 516, 281, 5055, 9163, 512, 4471, 7542, 22854, 3089, 281, 360, 300, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.18449377762643915, "compression_ratio": 1.5924170616113744, "no_speech_prob": 6.24051472186693e-06}, {"id": 184, "seek": 66600, "start": 678.0, "end": 686.0, "text": " Okay, so this is going to download, extract, and then we have this data set. Great. So that's ready to go.", "tokens": [50364, 407, 286, 360, 341, 293, 550, 300, 486, 767, 5484, 13, 286, 643, 281, 974, 300, 13, 27938, 82, 11, 558, 30, 50714, 50714, 1692, 490, 7160, 339, 53, 1991, 13, 26391, 35592, 13, 50964, 50964, 1033, 11, 370, 341, 307, 516, 281, 5484, 11, 8947, 11, 293, 550, 321, 362, 341, 1412, 992, 13, 3769, 13, 407, 300, 311, 1919, 281, 352, 13, 51364, 51364, 407, 718, 385, 445, 7542, 309, 586, 13, 407, 286, 478, 445, 516, 281, 5055, 9163, 512, 4471, 7542, 22854, 3089, 281, 360, 300, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.18449377762643915, "compression_ratio": 1.5924170616113744, "no_speech_prob": 6.24051472186693e-06}, {"id": 185, "seek": 66600, "start": 686.0, "end": 692.0, "text": " So let me just plot it now. So I'm just going to copy paste some map plot lib code to do that.", "tokens": [50364, 407, 286, 360, 341, 293, 550, 300, 486, 767, 5484, 13, 286, 643, 281, 974, 300, 13, 27938, 82, 11, 558, 30, 50714, 50714, 1692, 490, 7160, 339, 53, 1991, 13, 26391, 35592, 13, 50964, 50964, 1033, 11, 370, 341, 307, 516, 281, 5484, 11, 8947, 11, 293, 550, 321, 362, 341, 1412, 992, 13, 3769, 13, 407, 300, 311, 1919, 281, 352, 13, 51364, 51364, 407, 718, 385, 445, 7542, 309, 586, 13, 407, 286, 478, 445, 516, 281, 5055, 9163, 512, 4471, 7542, 22854, 3089, 281, 360, 300, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.18449377762643915, "compression_ratio": 1.5924170616113744, "no_speech_prob": 6.24051472186693e-06}, {"id": 186, "seek": 69200, "start": 692.0, "end": 697.0, "text": " And I'm going to also plot the label. So show you what the label is.", "tokens": [50364, 400, 286, 478, 516, 281, 611, 7542, 264, 7645, 13, 407, 855, 291, 437, 264, 7645, 307, 13, 50614, 50614, 400, 456, 291, 352, 13, 407, 291, 393, 380, 534, 980, 11, 457, 300, 311, 7645, 13, 50814, 50814, 467, 311, 257, 1687, 1481, 17259, 13, 286, 393, 380, 980, 13, 467, 311, 2238, 13, 51014, 51014, 1743, 257, 2182, 293, 7671, 3313, 13, 51164, 51164, 583, 498, 291, 574, 412, 7645, 2309, 11, 286, 914, 11, 718, 311, 445, 16888, 11, 558, 30, 407, 1958, 11, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 1386, 13, 407, 1338, 11, 300, 1542, 411, 309, 1542, 411, 341, 2146, 11, 733, 295, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18567154771190578, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.0001685900060692802}, {"id": 187, "seek": 69200, "start": 697.0, "end": 701.0, "text": " And there you go. So you can't really tell, but that's label.", "tokens": [50364, 400, 286, 478, 516, 281, 611, 7542, 264, 7645, 13, 407, 855, 291, 437, 264, 7645, 307, 13, 50614, 50614, 400, 456, 291, 352, 13, 407, 291, 393, 380, 534, 980, 11, 457, 300, 311, 7645, 13, 50814, 50814, 467, 311, 257, 1687, 1481, 17259, 13, 286, 393, 380, 980, 13, 467, 311, 2238, 13, 51014, 51014, 1743, 257, 2182, 293, 7671, 3313, 13, 51164, 51164, 583, 498, 291, 574, 412, 7645, 2309, 11, 286, 914, 11, 718, 311, 445, 16888, 11, 558, 30, 407, 1958, 11, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 1386, 13, 407, 1338, 11, 300, 1542, 411, 309, 1542, 411, 341, 2146, 11, 733, 295, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18567154771190578, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.0001685900060692802}, {"id": 188, "seek": 69200, "start": 701.0, "end": 705.0, "text": " It's a super nice frog. I can't tell. It's beautiful.", "tokens": [50364, 400, 286, 478, 516, 281, 611, 7542, 264, 7645, 13, 407, 855, 291, 437, 264, 7645, 307, 13, 50614, 50614, 400, 456, 291, 352, 13, 407, 291, 393, 380, 534, 980, 11, 457, 300, 311, 7645, 13, 50814, 50814, 467, 311, 257, 1687, 1481, 17259, 13, 286, 393, 380, 980, 13, 467, 311, 2238, 13, 51014, 51014, 1743, 257, 2182, 293, 7671, 3313, 13, 51164, 51164, 583, 498, 291, 574, 412, 7645, 2309, 11, 286, 914, 11, 718, 311, 445, 16888, 11, 558, 30, 407, 1958, 11, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 1386, 13, 407, 1338, 11, 300, 1542, 411, 309, 1542, 411, 341, 2146, 11, 733, 295, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18567154771190578, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.0001685900060692802}, {"id": 189, "seek": 69200, "start": 705.0, "end": 708.0, "text": " Like a red and orange eye.", "tokens": [50364, 400, 286, 478, 516, 281, 611, 7542, 264, 7645, 13, 407, 855, 291, 437, 264, 7645, 307, 13, 50614, 50614, 400, 456, 291, 352, 13, 407, 291, 393, 380, 534, 980, 11, 457, 300, 311, 7645, 13, 50814, 50814, 467, 311, 257, 1687, 1481, 17259, 13, 286, 393, 380, 980, 13, 467, 311, 2238, 13, 51014, 51014, 1743, 257, 2182, 293, 7671, 3313, 13, 51164, 51164, 583, 498, 291, 574, 412, 7645, 2309, 11, 286, 914, 11, 718, 311, 445, 16888, 11, 558, 30, 407, 1958, 11, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 1386, 13, 407, 1338, 11, 300, 1542, 411, 309, 1542, 411, 341, 2146, 11, 733, 295, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18567154771190578, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.0001685900060692802}, {"id": 190, "seek": 69200, "start": 708.0, "end": 719.0, "text": " But if you look at label six, I mean, let's just verify, right? So 0, 1, 2, 3, 4, 5, 6. So yeah, that looks like it looks like this guy, kind of.", "tokens": [50364, 400, 286, 478, 516, 281, 611, 7542, 264, 7645, 13, 407, 855, 291, 437, 264, 7645, 307, 13, 50614, 50614, 400, 456, 291, 352, 13, 407, 291, 393, 380, 534, 980, 11, 457, 300, 311, 7645, 13, 50814, 50814, 467, 311, 257, 1687, 1481, 17259, 13, 286, 393, 380, 980, 13, 467, 311, 2238, 13, 51014, 51014, 1743, 257, 2182, 293, 7671, 3313, 13, 51164, 51164, 583, 498, 291, 574, 412, 7645, 2309, 11, 286, 914, 11, 718, 311, 445, 16888, 11, 558, 30, 407, 1958, 11, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 1386, 13, 407, 1338, 11, 300, 1542, 411, 309, 1542, 411, 341, 2146, 11, 733, 295, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18567154771190578, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.0001685900060692802}, {"id": 191, "seek": 71900, "start": 719.0, "end": 726.0, "text": " Yeah. So that's the frog. So let's normalize it now. So that's the neural network.", "tokens": [50364, 865, 13, 407, 300, 311, 264, 17259, 13, 407, 718, 311, 2710, 1125, 309, 586, 13, 407, 300, 311, 264, 18161, 3209, 13, 50714, 50714, 876, 11, 309, 311, 1217, 21748, 13, 1033, 13, 876, 11, 6076, 13, 50864, 50864, 407, 300, 311, 869, 13, 400, 1627, 13, 407, 586, 321, 500, 380, 528, 281, 44497, 807, 613, 5267, 411, 472, 412, 257, 565, 11, 558, 30, 51364, 51364, 407, 536, 510, 11, 341, 307, 364, 3256, 11, 257, 2167, 3256, 13, 407, 321, 767, 528, 281, 360, 15245, 295, 5267, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.15015318928932658, "compression_ratio": 1.6417910447761195, "no_speech_prob": 2.840753768396098e-05}, {"id": 192, "seek": 71900, "start": 726.0, "end": 729.0, "text": " Oh, it's already downloaded. Okay. Oh, wow.", "tokens": [50364, 865, 13, 407, 300, 311, 264, 17259, 13, 407, 718, 311, 2710, 1125, 309, 586, 13, 407, 300, 311, 264, 18161, 3209, 13, 50714, 50714, 876, 11, 309, 311, 1217, 21748, 13, 1033, 13, 876, 11, 6076, 13, 50864, 50864, 407, 300, 311, 869, 13, 400, 1627, 13, 407, 586, 321, 500, 380, 528, 281, 44497, 807, 613, 5267, 411, 472, 412, 257, 565, 11, 558, 30, 51364, 51364, 407, 536, 510, 11, 341, 307, 364, 3256, 11, 257, 2167, 3256, 13, 407, 321, 767, 528, 281, 360, 15245, 295, 5267, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.15015318928932658, "compression_ratio": 1.6417910447761195, "no_speech_prob": 2.840753768396098e-05}, {"id": 193, "seek": 71900, "start": 729.0, "end": 739.0, "text": " So that's great. And cool. So now we don't want to iterate through these images like one at a time, right?", "tokens": [50364, 865, 13, 407, 300, 311, 264, 17259, 13, 407, 718, 311, 2710, 1125, 309, 586, 13, 407, 300, 311, 264, 18161, 3209, 13, 50714, 50714, 876, 11, 309, 311, 1217, 21748, 13, 1033, 13, 876, 11, 6076, 13, 50864, 50864, 407, 300, 311, 869, 13, 400, 1627, 13, 407, 586, 321, 500, 380, 528, 281, 44497, 807, 613, 5267, 411, 472, 412, 257, 565, 11, 558, 30, 51364, 51364, 407, 536, 510, 11, 341, 307, 364, 3256, 11, 257, 2167, 3256, 13, 407, 321, 767, 528, 281, 360, 15245, 295, 5267, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.15015318928932658, "compression_ratio": 1.6417910447761195, "no_speech_prob": 2.840753768396098e-05}, {"id": 194, "seek": 71900, "start": 739.0, "end": 745.0, "text": " So see here, this is an image, a single image. So we actually want to do batch of images, right?", "tokens": [50364, 865, 13, 407, 300, 311, 264, 17259, 13, 407, 718, 311, 2710, 1125, 309, 586, 13, 407, 300, 311, 264, 18161, 3209, 13, 50714, 50714, 876, 11, 309, 311, 1217, 21748, 13, 1033, 13, 876, 11, 6076, 13, 50864, 50864, 407, 300, 311, 869, 13, 400, 1627, 13, 407, 586, 321, 500, 380, 528, 281, 44497, 807, 613, 5267, 411, 472, 412, 257, 565, 11, 558, 30, 51364, 51364, 407, 536, 510, 11, 341, 307, 364, 3256, 11, 257, 2167, 3256, 13, 407, 321, 767, 528, 281, 360, 15245, 295, 5267, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.15015318928932658, "compression_ratio": 1.6417910447761195, "no_speech_prob": 2.840753768396098e-05}, {"id": 195, "seek": 74500, "start": 745.0, "end": 751.0, "text": " So we're going to use a data loader for that. So I pull that in and I'm going to say batch size 32.", "tokens": [50364, 407, 321, 434, 516, 281, 764, 257, 1412, 3677, 260, 337, 300, 13, 407, 286, 2235, 300, 294, 293, 286, 478, 516, 281, 584, 15245, 2744, 8858, 13, 50664, 50664, 400, 286, 360, 528, 281, 39426, 300, 13, 407, 456, 309, 307, 13, 50814, 50814, 400, 550, 2745, 11, 291, 458, 11, 281, 44497, 807, 341, 11, 309, 311, 445, 257, 2199, 337, 6367, 11, 558, 30, 407, 337, 15245, 294, 1412, 3677, 260, 11, 483, 264, 15245, 11, 5268, 309, 484, 11, 4482, 264, 10854, 11, 445, 370, 291, 393, 536, 437, 436, 574, 411, 13, 51314, 51314, 407, 8858, 307, 264, 15245, 2744, 11, 597, 307, 869, 13, 6244, 9235, 11, 8858, 25930, 293, 8858, 11402, 18668, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12325441269647508, "compression_ratio": 1.6891385767790261, "no_speech_prob": 1.1478477063064929e-05}, {"id": 196, "seek": 74500, "start": 751.0, "end": 754.0, "text": " And I do want to shuffle that. So there it is.", "tokens": [50364, 407, 321, 434, 516, 281, 764, 257, 1412, 3677, 260, 337, 300, 13, 407, 286, 2235, 300, 294, 293, 286, 478, 516, 281, 584, 15245, 2744, 8858, 13, 50664, 50664, 400, 286, 360, 528, 281, 39426, 300, 13, 407, 456, 309, 307, 13, 50814, 50814, 400, 550, 2745, 11, 291, 458, 11, 281, 44497, 807, 341, 11, 309, 311, 445, 257, 2199, 337, 6367, 11, 558, 30, 407, 337, 15245, 294, 1412, 3677, 260, 11, 483, 264, 15245, 11, 5268, 309, 484, 11, 4482, 264, 10854, 11, 445, 370, 291, 393, 536, 437, 436, 574, 411, 13, 51314, 51314, 407, 8858, 307, 264, 15245, 2744, 11, 597, 307, 869, 13, 6244, 9235, 11, 8858, 25930, 293, 8858, 11402, 18668, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12325441269647508, "compression_ratio": 1.6891385767790261, "no_speech_prob": 1.1478477063064929e-05}, {"id": 197, "seek": 74500, "start": 754.0, "end": 764.0, "text": " And then obviously, you know, to iterate through this, it's just a simple for loop, right? So for batch in data loader, get the batch, expand it out, print the shapes, just so you can see what they look like.", "tokens": [50364, 407, 321, 434, 516, 281, 764, 257, 1412, 3677, 260, 337, 300, 13, 407, 286, 2235, 300, 294, 293, 286, 478, 516, 281, 584, 15245, 2744, 8858, 13, 50664, 50664, 400, 286, 360, 528, 281, 39426, 300, 13, 407, 456, 309, 307, 13, 50814, 50814, 400, 550, 2745, 11, 291, 458, 11, 281, 44497, 807, 341, 11, 309, 311, 445, 257, 2199, 337, 6367, 11, 558, 30, 407, 337, 15245, 294, 1412, 3677, 260, 11, 483, 264, 15245, 11, 5268, 309, 484, 11, 4482, 264, 10854, 11, 445, 370, 291, 393, 536, 437, 436, 574, 411, 13, 51314, 51314, 407, 8858, 307, 264, 15245, 2744, 11, 597, 307, 869, 13, 6244, 9235, 11, 8858, 25930, 293, 8858, 11402, 18668, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12325441269647508, "compression_ratio": 1.6891385767790261, "no_speech_prob": 1.1478477063064929e-05}, {"id": 198, "seek": 74500, "start": 764.0, "end": 771.0, "text": " So 32 is the batch size, which is great. Three channels, 32 heights and 32 width pixels, right?", "tokens": [50364, 407, 321, 434, 516, 281, 764, 257, 1412, 3677, 260, 337, 300, 13, 407, 286, 2235, 300, 294, 293, 286, 478, 516, 281, 584, 15245, 2744, 8858, 13, 50664, 50664, 400, 286, 360, 528, 281, 39426, 300, 13, 407, 456, 309, 307, 13, 50814, 50814, 400, 550, 2745, 11, 291, 458, 11, 281, 44497, 807, 341, 11, 309, 311, 445, 257, 2199, 337, 6367, 11, 558, 30, 407, 337, 15245, 294, 1412, 3677, 260, 11, 483, 264, 15245, 11, 5268, 309, 484, 11, 4482, 264, 10854, 11, 445, 370, 291, 393, 536, 437, 436, 574, 411, 13, 51314, 51314, 407, 8858, 307, 264, 15245, 2744, 11, 597, 307, 869, 13, 6244, 9235, 11, 8858, 25930, 293, 8858, 11402, 18668, 11, 558, 30, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.12325441269647508, "compression_ratio": 1.6891385767790261, "no_speech_prob": 1.1478477063064929e-05}, {"id": 199, "seek": 77100, "start": 771.0, "end": 778.0, "text": " And then our labels are 32. So just 32 scalars.", "tokens": [50364, 400, 550, 527, 16949, 366, 8858, 13, 407, 445, 8858, 15664, 685, 13, 50714, 50714, 400, 550, 586, 321, 434, 516, 281, 11, 286, 611, 643, 281, 16927, 452, 5015, 31890, 13, 407, 498, 291, 1604, 11, 264, 5015, 31890, 390, 8895, 337, 29903, 31890, 13, 51164, 51164, 400, 382, 291, 848, 11, 456, 366, 9714, 5359, 456, 13, 407, 309, 1582, 380, 534, 589, 562, 286, 362, 257, 1412, 992, 365, 1266, 5359, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10759339151503164, "compression_ratio": 1.4972972972972973, "no_speech_prob": 6.540176400449127e-06}, {"id": 200, "seek": 77100, "start": 778.0, "end": 787.0, "text": " And then now we're going to, I also need to modify my ResNet. So if you remember, the ResNet was trained for ImageNet.", "tokens": [50364, 400, 550, 527, 16949, 366, 8858, 13, 407, 445, 8858, 15664, 685, 13, 50714, 50714, 400, 550, 586, 321, 434, 516, 281, 11, 286, 611, 643, 281, 16927, 452, 5015, 31890, 13, 407, 498, 291, 1604, 11, 264, 5015, 31890, 390, 8895, 337, 29903, 31890, 13, 51164, 51164, 400, 382, 291, 848, 11, 456, 366, 9714, 5359, 456, 13, 407, 309, 1582, 380, 534, 589, 562, 286, 362, 257, 1412, 992, 365, 1266, 5359, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10759339151503164, "compression_ratio": 1.4972972972972973, "no_speech_prob": 6.540176400449127e-06}, {"id": 201, "seek": 77100, "start": 787.0, "end": 793.0, "text": " And as you said, there are 1000 classes there. So it won't really work when I have a data set with 10 classes.", "tokens": [50364, 400, 550, 527, 16949, 366, 8858, 13, 407, 445, 8858, 15664, 685, 13, 50714, 50714, 400, 550, 586, 321, 434, 516, 281, 11, 286, 611, 643, 281, 16927, 452, 5015, 31890, 13, 407, 498, 291, 1604, 11, 264, 5015, 31890, 390, 8895, 337, 29903, 31890, 13, 51164, 51164, 400, 382, 291, 848, 11, 456, 366, 9714, 5359, 456, 13, 407, 309, 1582, 380, 534, 589, 562, 286, 362, 257, 1412, 992, 365, 1266, 5359, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10759339151503164, "compression_ratio": 1.4972972972972973, "no_speech_prob": 6.540176400449127e-06}, {"id": 202, "seek": 79300, "start": 793.0, "end": 807.0, "text": " So for that, I need to modify my ResNet 50 to take that. So I think they're cheating because actually the ResNet 50 has the ResNet 50 plus these FC fully connected layers at the end, right?", "tokens": [50364, 407, 337, 300, 11, 286, 643, 281, 16927, 452, 5015, 31890, 2625, 281, 747, 300, 13, 407, 286, 519, 436, 434, 18309, 570, 767, 264, 5015, 31890, 2625, 575, 264, 5015, 31890, 2625, 1804, 613, 27168, 4498, 4582, 7914, 412, 264, 917, 11, 558, 30, 51064, 51064, 407, 321, 434, 516, 281, 7406, 300, 1036, 4498, 4582, 4583, 11, 597, 286, 500, 380, 458, 437, 264, 2744, 295, 264, 5598, 307, 295, 300, 5015, 31890, 2625, 13, 51514, 51514, 583, 286, 458, 300, 286, 643, 281, 362, 1266, 382, 364, 5598, 570, 300, 311, 516, 281, 312, 264, 1230, 295, 5359, 300, 286, 362, 11, 558, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07076628573305972, "compression_ratio": 1.7818930041152263, "no_speech_prob": 3.0894093470124062e-06}, {"id": 203, "seek": 79300, "start": 807.0, "end": 816.0, "text": " So we're going to replace that last fully connected layer, which I don't know what the size of the output is of that ResNet 50.", "tokens": [50364, 407, 337, 300, 11, 286, 643, 281, 16927, 452, 5015, 31890, 2625, 281, 747, 300, 13, 407, 286, 519, 436, 434, 18309, 570, 767, 264, 5015, 31890, 2625, 575, 264, 5015, 31890, 2625, 1804, 613, 27168, 4498, 4582, 7914, 412, 264, 917, 11, 558, 30, 51064, 51064, 407, 321, 434, 516, 281, 7406, 300, 1036, 4498, 4582, 4583, 11, 597, 286, 500, 380, 458, 437, 264, 2744, 295, 264, 5598, 307, 295, 300, 5015, 31890, 2625, 13, 51514, 51514, 583, 286, 458, 300, 286, 643, 281, 362, 1266, 382, 364, 5598, 570, 300, 311, 516, 281, 312, 264, 1230, 295, 5359, 300, 286, 362, 11, 558, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07076628573305972, "compression_ratio": 1.7818930041152263, "no_speech_prob": 3.0894093470124062e-06}, {"id": 204, "seek": 79300, "start": 816.0, "end": 822.0, "text": " But I know that I need to have 10 as an output because that's going to be the number of classes that I have, right?", "tokens": [50364, 407, 337, 300, 11, 286, 643, 281, 16927, 452, 5015, 31890, 2625, 281, 747, 300, 13, 407, 286, 519, 436, 434, 18309, 570, 767, 264, 5015, 31890, 2625, 575, 264, 5015, 31890, 2625, 1804, 613, 27168, 4498, 4582, 7914, 412, 264, 917, 11, 558, 30, 51064, 51064, 407, 321, 434, 516, 281, 7406, 300, 1036, 4498, 4582, 4583, 11, 597, 286, 500, 380, 458, 437, 264, 2744, 295, 264, 5598, 307, 295, 300, 5015, 31890, 2625, 13, 51514, 51514, 583, 286, 458, 300, 286, 643, 281, 362, 1266, 382, 364, 5598, 570, 300, 311, 516, 281, 312, 264, 1230, 295, 5359, 300, 286, 362, 11, 558, 30, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07076628573305972, "compression_ratio": 1.7818930041152263, "no_speech_prob": 3.0894093470124062e-06}, {"id": 205, "seek": 82200, "start": 822.0, "end": 828.0, "text": " So what's happening under the hood is you have this like ResNet, right? A bunch of layers here.", "tokens": [50364, 407, 437, 311, 2737, 833, 264, 13376, 307, 291, 362, 341, 411, 5015, 31890, 11, 558, 30, 316, 3840, 295, 7914, 510, 13, 50664, 50664, 400, 550, 412, 512, 935, 562, 300, 5314, 11, 550, 291, 362, 341, 4498, 4582, 11, 597, 307, 18350, 264, 5598, 490, 510, 666, 2035, 1230, 295, 5359, 291, 362, 11, 558, 30, 51164, 51164, 407, 341, 2146, 286, 528, 281, 7406, 13, 509, 458, 11, 5946, 322, 264, 4319, 13, 509, 2171, 393, 3270, 309, 11, 829, 364, 6575, 2445, 11, 2035, 291, 528, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09222102918122944, "compression_ratio": 1.6213991769547325, "no_speech_prob": 5.594216418103315e-06}, {"id": 206, "seek": 82200, "start": 828.0, "end": 838.0, "text": " And then at some point when that ends, then you have this fully connected, which is mapping the output from here into whatever number of classes you have, right?", "tokens": [50364, 407, 437, 311, 2737, 833, 264, 13376, 307, 291, 362, 341, 411, 5015, 31890, 11, 558, 30, 316, 3840, 295, 7914, 510, 13, 50664, 50664, 400, 550, 412, 512, 935, 562, 300, 5314, 11, 550, 291, 362, 341, 4498, 4582, 11, 597, 307, 18350, 264, 5598, 490, 510, 666, 2035, 1230, 295, 5359, 291, 362, 11, 558, 30, 51164, 51164, 407, 341, 2146, 286, 528, 281, 7406, 13, 509, 458, 11, 5946, 322, 264, 4319, 13, 509, 2171, 393, 3270, 309, 11, 829, 364, 6575, 2445, 11, 2035, 291, 528, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09222102918122944, "compression_ratio": 1.6213991769547325, "no_speech_prob": 5.594216418103315e-06}, {"id": 207, "seek": 82200, "start": 838.0, "end": 847.0, "text": " So this guy I want to replace. You know, depends on the context. You sometimes can drop it, put an identity function, whatever you want.", "tokens": [50364, 407, 437, 311, 2737, 833, 264, 13376, 307, 291, 362, 341, 411, 5015, 31890, 11, 558, 30, 316, 3840, 295, 7914, 510, 13, 50664, 50664, 400, 550, 412, 512, 935, 562, 300, 5314, 11, 550, 291, 362, 341, 4498, 4582, 11, 597, 307, 18350, 264, 5598, 490, 510, 666, 2035, 1230, 295, 5359, 291, 362, 11, 558, 30, 51164, 51164, 407, 341, 2146, 286, 528, 281, 7406, 13, 509, 458, 11, 5946, 322, 264, 4319, 13, 509, 2171, 393, 3270, 309, 11, 829, 364, 6575, 2445, 11, 2035, 291, 528, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09222102918122944, "compression_ratio": 1.6213991769547325, "no_speech_prob": 5.594216418103315e-06}, {"id": 208, "seek": 84700, "start": 847.0, "end": 852.0, "text": " Okay, so we'll replace that. Let's just make sure that works. Okay, perfect. So now we're good to go.", "tokens": [50364, 1033, 11, 370, 321, 603, 7406, 300, 13, 961, 311, 445, 652, 988, 300, 1985, 13, 1033, 11, 2176, 13, 407, 586, 321, 434, 665, 281, 352, 13, 50614, 50614, 407, 718, 311, 352, 2286, 293, 6069, 512, 1507, 11, 558, 30, 407, 286, 478, 445, 516, 281, 3677, 1071, 15245, 510, 11, 293, 550, 286, 478, 516, 281, 1190, 309, 807, 452, 5015, 31890, 13, 51064, 51064, 407, 341, 3256, 11, 293, 550, 286, 478, 516, 281, 574, 412, 264, 700, 1266, 21264, 787, 11, 558, 30, 3769, 13, 407, 300, 1542, 665, 13, 51414, 51414, 400, 286, 5694, 300, 264, 6343, 1230, 307, 341, 700, 2146, 510, 11, 558, 30, 1958, 13, 22, 11, 1542, 411, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10721094240018023, "compression_ratio": 1.7120622568093384, "no_speech_prob": 4.222722054691985e-06}, {"id": 209, "seek": 84700, "start": 852.0, "end": 861.0, "text": " So let's go ahead and predict some stuff, right? So I'm just going to load another batch here, and then I'm going to run it through my ResNet.", "tokens": [50364, 1033, 11, 370, 321, 603, 7406, 300, 13, 961, 311, 445, 652, 988, 300, 1985, 13, 1033, 11, 2176, 13, 407, 586, 321, 434, 665, 281, 352, 13, 50614, 50614, 407, 718, 311, 352, 2286, 293, 6069, 512, 1507, 11, 558, 30, 407, 286, 478, 445, 516, 281, 3677, 1071, 15245, 510, 11, 293, 550, 286, 478, 516, 281, 1190, 309, 807, 452, 5015, 31890, 13, 51064, 51064, 407, 341, 3256, 11, 293, 550, 286, 478, 516, 281, 574, 412, 264, 700, 1266, 21264, 787, 11, 558, 30, 3769, 13, 407, 300, 1542, 665, 13, 51414, 51414, 400, 286, 5694, 300, 264, 6343, 1230, 307, 341, 700, 2146, 510, 11, 558, 30, 1958, 13, 22, 11, 1542, 411, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10721094240018023, "compression_ratio": 1.7120622568093384, "no_speech_prob": 4.222722054691985e-06}, {"id": 210, "seek": 84700, "start": 861.0, "end": 868.0, "text": " So this image, and then I'm going to look at the first 10 predictions only, right? Great. So that looks good.", "tokens": [50364, 1033, 11, 370, 321, 603, 7406, 300, 13, 961, 311, 445, 652, 988, 300, 1985, 13, 1033, 11, 2176, 13, 407, 586, 321, 434, 665, 281, 352, 13, 50614, 50614, 407, 718, 311, 352, 2286, 293, 6069, 512, 1507, 11, 558, 30, 407, 286, 478, 445, 516, 281, 3677, 1071, 15245, 510, 11, 293, 550, 286, 478, 516, 281, 1190, 309, 807, 452, 5015, 31890, 13, 51064, 51064, 407, 341, 3256, 11, 293, 550, 286, 478, 516, 281, 574, 412, 264, 700, 1266, 21264, 787, 11, 558, 30, 3769, 13, 407, 300, 1542, 665, 13, 51414, 51414, 400, 286, 5694, 300, 264, 6343, 1230, 307, 341, 700, 2146, 510, 11, 558, 30, 1958, 13, 22, 11, 1542, 411, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10721094240018023, "compression_ratio": 1.7120622568093384, "no_speech_prob": 4.222722054691985e-06}, {"id": 211, "seek": 84700, "start": 868.0, "end": 875.0, "text": " And I noticed that the highest number is this first guy here, right? 0.7, looks like.", "tokens": [50364, 1033, 11, 370, 321, 603, 7406, 300, 13, 961, 311, 445, 652, 988, 300, 1985, 13, 1033, 11, 2176, 13, 407, 586, 321, 434, 665, 281, 352, 13, 50614, 50614, 407, 718, 311, 352, 2286, 293, 6069, 512, 1507, 11, 558, 30, 407, 286, 478, 445, 516, 281, 3677, 1071, 15245, 510, 11, 293, 550, 286, 478, 516, 281, 1190, 309, 807, 452, 5015, 31890, 13, 51064, 51064, 407, 341, 3256, 11, 293, 550, 286, 478, 516, 281, 574, 412, 264, 700, 1266, 21264, 787, 11, 558, 30, 3769, 13, 407, 300, 1542, 665, 13, 51414, 51414, 400, 286, 5694, 300, 264, 6343, 1230, 307, 341, 700, 2146, 510, 11, 558, 30, 1958, 13, 22, 11, 1542, 411, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10721094240018023, "compression_ratio": 1.7120622568093384, "no_speech_prob": 4.222722054691985e-06}, {"id": 212, "seek": 87500, "start": 875.0, "end": 884.0, "text": " So let me do a softmax just to kind of like turn these into probabilities, right? Great. So 0.19.", "tokens": [50364, 407, 718, 385, 360, 257, 2787, 41167, 445, 281, 733, 295, 411, 1261, 613, 666, 33783, 11, 558, 30, 3769, 13, 407, 1958, 13, 3405, 13, 50814, 50814, 663, 1542, 411, 264, 6343, 472, 13, 407, 341, 307, 437, 264, 3209, 576, 6069, 337, 341, 1729, 3256, 11, 264, 7645, 13, 51114, 51114, 400, 321, 434, 516, 281, 764, 264, 21727, 36025, 281, 2235, 264, 7645, 1315, 11, 286, 2041, 11, 294, 341, 1389, 13, 407, 321, 483, 4018, 11, 558, 30, 51514, 51514, 407, 300, 1669, 2020, 570, 300, 390, 264, 6343, 1230, 382, 731, 11, 558, 30, 6974, 1266, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14137926725583655, "compression_ratio": 1.578740157480315, "no_speech_prob": 1.0952733646263368e-05}, {"id": 213, "seek": 87500, "start": 884.0, "end": 890.0, "text": " That looks like the highest one. So this is what the network would predict for this particular image, the label.", "tokens": [50364, 407, 718, 385, 360, 257, 2787, 41167, 445, 281, 733, 295, 411, 1261, 613, 666, 33783, 11, 558, 30, 3769, 13, 407, 1958, 13, 3405, 13, 50814, 50814, 663, 1542, 411, 264, 6343, 472, 13, 407, 341, 307, 437, 264, 3209, 576, 6069, 337, 341, 1729, 3256, 11, 264, 7645, 13, 51114, 51114, 400, 321, 434, 516, 281, 764, 264, 21727, 36025, 281, 2235, 264, 7645, 1315, 11, 286, 2041, 11, 294, 341, 1389, 13, 407, 321, 483, 4018, 11, 558, 30, 51514, 51514, 407, 300, 1669, 2020, 570, 300, 390, 264, 6343, 1230, 382, 731, 11, 558, 30, 6974, 1266, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14137926725583655, "compression_ratio": 1.578740157480315, "no_speech_prob": 1.0952733646263368e-05}, {"id": 214, "seek": 87500, "start": 890.0, "end": 898.0, "text": " And we're going to use the ArcMax to pull the label name, I guess, in this case. So we get zero, right?", "tokens": [50364, 407, 718, 385, 360, 257, 2787, 41167, 445, 281, 733, 295, 411, 1261, 613, 666, 33783, 11, 558, 30, 3769, 13, 407, 1958, 13, 3405, 13, 50814, 50814, 663, 1542, 411, 264, 6343, 472, 13, 407, 341, 307, 437, 264, 3209, 576, 6069, 337, 341, 1729, 3256, 11, 264, 7645, 13, 51114, 51114, 400, 321, 434, 516, 281, 764, 264, 21727, 36025, 281, 2235, 264, 7645, 1315, 11, 286, 2041, 11, 294, 341, 1389, 13, 407, 321, 483, 4018, 11, 558, 30, 51514, 51514, 407, 300, 1669, 2020, 570, 300, 390, 264, 6343, 1230, 382, 731, 11, 558, 30, 6974, 1266, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14137926725583655, "compression_ratio": 1.578740157480315, "no_speech_prob": 1.0952733646263368e-05}, {"id": 215, "seek": 87500, "start": 898.0, "end": 903.0, "text": " So that makes sense because that was the highest number as well, right? Under 10 here.", "tokens": [50364, 407, 718, 385, 360, 257, 2787, 41167, 445, 281, 733, 295, 411, 1261, 613, 666, 33783, 11, 558, 30, 3769, 13, 407, 1958, 13, 3405, 13, 50814, 50814, 663, 1542, 411, 264, 6343, 472, 13, 407, 341, 307, 437, 264, 3209, 576, 6069, 337, 341, 1729, 3256, 11, 264, 7645, 13, 51114, 51114, 400, 321, 434, 516, 281, 764, 264, 21727, 36025, 281, 2235, 264, 7645, 1315, 11, 286, 2041, 11, 294, 341, 1389, 13, 407, 321, 483, 4018, 11, 558, 30, 51514, 51514, 407, 300, 1669, 2020, 570, 300, 390, 264, 6343, 1230, 382, 731, 11, 558, 30, 6974, 1266, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14137926725583655, "compression_ratio": 1.578740157480315, "no_speech_prob": 1.0952733646263368e-05}, {"id": 216, "seek": 90300, "start": 903.0, "end": 909.0, "text": " Where do we feed the image to the network? Oh, up here. Right there.", "tokens": [50364, 2305, 360, 321, 3154, 264, 3256, 281, 264, 3209, 30, 876, 11, 493, 510, 13, 1779, 456, 13, 50664, 50664, 583, 1783, 390, 257, 8858, 538, 8858, 11, 558, 30, 400, 264, 5015, 31890, 2673, 307, 8895, 322, 264, 3262, 293, 746, 11, 558, 30, 18652, 1625, 13, 51014, 51014, 407, 1338, 11, 3006, 13, 663, 311, 45216, 304, 1673, 13, 407, 264, 15743, 500, 380, 534, 1871, 382, 938, 382, 291, 500, 380, 23060, 309, 886, 709, 11, 558, 30, 51464, 51464, 407, 286, 478, 18309, 257, 707, 857, 11, 457, 264, 383, 12775, 1899, 1266, 307, 2489, 13, 8858, 1582, 380, 11596, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15180885244946962, "compression_ratio": 1.4941176470588236, "no_speech_prob": 9.972706720873248e-06}, {"id": 217, "seek": 90300, "start": 909.0, "end": 916.0, "text": " But X was a 32 by 32, right? And the ResNet usually is trained on the hundred and something, right? Pixels.", "tokens": [50364, 2305, 360, 321, 3154, 264, 3256, 281, 264, 3209, 30, 876, 11, 493, 510, 13, 1779, 456, 13, 50664, 50664, 583, 1783, 390, 257, 8858, 538, 8858, 11, 558, 30, 400, 264, 5015, 31890, 2673, 307, 8895, 322, 264, 3262, 293, 746, 11, 558, 30, 18652, 1625, 13, 51014, 51014, 407, 1338, 11, 3006, 13, 663, 311, 45216, 304, 1673, 13, 407, 264, 15743, 500, 380, 534, 1871, 382, 938, 382, 291, 500, 380, 23060, 309, 886, 709, 11, 558, 30, 51464, 51464, 407, 286, 478, 18309, 257, 707, 857, 11, 457, 264, 383, 12775, 1899, 1266, 307, 2489, 13, 8858, 1582, 380, 11596, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15180885244946962, "compression_ratio": 1.4941176470588236, "no_speech_prob": 9.972706720873248e-06}, {"id": 218, "seek": 90300, "start": 916.0, "end": 925.0, "text": " So yeah, correct. That's convolutional though. So the inputs don't really matter as long as you don't shrink it too much, right?", "tokens": [50364, 2305, 360, 321, 3154, 264, 3256, 281, 264, 3209, 30, 876, 11, 493, 510, 13, 1779, 456, 13, 50664, 50664, 583, 1783, 390, 257, 8858, 538, 8858, 11, 558, 30, 400, 264, 5015, 31890, 2673, 307, 8895, 322, 264, 3262, 293, 746, 11, 558, 30, 18652, 1625, 13, 51014, 51014, 407, 1338, 11, 3006, 13, 663, 311, 45216, 304, 1673, 13, 407, 264, 15743, 500, 380, 534, 1871, 382, 938, 382, 291, 500, 380, 23060, 309, 886, 709, 11, 558, 30, 51464, 51464, 407, 286, 478, 18309, 257, 707, 857, 11, 457, 264, 383, 12775, 1899, 1266, 307, 2489, 13, 8858, 1582, 380, 11596, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15180885244946962, "compression_ratio": 1.4941176470588236, "no_speech_prob": 9.972706720873248e-06}, {"id": 219, "seek": 90300, "start": 925.0, "end": 930.0, "text": " So I'm cheating a little bit, but the CIFAR 10 is fine. 32 won't disappear.", "tokens": [50364, 2305, 360, 321, 3154, 264, 3256, 281, 264, 3209, 30, 876, 11, 493, 510, 13, 1779, 456, 13, 50664, 50664, 583, 1783, 390, 257, 8858, 538, 8858, 11, 558, 30, 400, 264, 5015, 31890, 2673, 307, 8895, 322, 264, 3262, 293, 746, 11, 558, 30, 18652, 1625, 13, 51014, 51014, 407, 1338, 11, 3006, 13, 663, 311, 45216, 304, 1673, 13, 407, 264, 15743, 500, 380, 534, 1871, 382, 938, 382, 291, 500, 380, 23060, 309, 886, 709, 11, 558, 30, 51464, 51464, 407, 286, 478, 18309, 257, 707, 857, 11, 457, 264, 383, 12775, 1899, 1266, 307, 2489, 13, 8858, 1582, 380, 11596, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.15180885244946962, "compression_ratio": 1.4941176470588236, "no_speech_prob": 9.972706720873248e-06}, {"id": 220, "seek": 93000, "start": 930.0, "end": 936.0, "text": " If you did like MNIST, potentially you could have a crash because at some point you'll downsample way too much.", "tokens": [50364, 759, 291, 630, 411, 376, 45, 19756, 11, 7263, 291, 727, 362, 257, 8252, 570, 412, 512, 935, 291, 603, 760, 19988, 781, 636, 886, 709, 13, 50664, 50664, 1033, 13, 407, 1338, 11, 309, 311, 257, 2238, 644, 466, 264, 5015, 31890, 11, 2597, 11, 466, 45216, 304, 9590, 307, 11, 291, 458, 11, 264, 4846, 393, 1319, 382, 731, 13, 51114, 51114, 1033, 11, 370, 718, 311, 829, 264, 16949, 13, 407, 286, 7373, 264, 16949, 484, 293, 436, 500, 380, 2995, 11, 558, 30, 51364, 51364, 407, 2745, 291, 434, 411, 11, 731, 11, 341, 307, 3442, 281, 312, 341, 10247, 659, 12, 17227, 2001, 3256, 2533, 2316, 11, 457, 411, 309, 994, 380, 6069, 264, 16949, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16093844604492188, "compression_ratio": 1.6215277777777777, "no_speech_prob": 1.1842793355754111e-05}, {"id": 221, "seek": 93000, "start": 936.0, "end": 945.0, "text": " Okay. So yeah, it's a beautiful part about the ResNet, sorry, about convolutional networks is, you know, the input can change as well.", "tokens": [50364, 759, 291, 630, 411, 376, 45, 19756, 11, 7263, 291, 727, 362, 257, 8252, 570, 412, 512, 935, 291, 603, 760, 19988, 781, 636, 886, 709, 13, 50664, 50664, 1033, 13, 407, 1338, 11, 309, 311, 257, 2238, 644, 466, 264, 5015, 31890, 11, 2597, 11, 466, 45216, 304, 9590, 307, 11, 291, 458, 11, 264, 4846, 393, 1319, 382, 731, 13, 51114, 51114, 1033, 11, 370, 718, 311, 829, 264, 16949, 13, 407, 286, 7373, 264, 16949, 484, 293, 436, 500, 380, 2995, 11, 558, 30, 51364, 51364, 407, 2745, 291, 434, 411, 11, 731, 11, 341, 307, 3442, 281, 312, 341, 10247, 659, 12, 17227, 2001, 3256, 2533, 2316, 11, 457, 411, 309, 994, 380, 6069, 264, 16949, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16093844604492188, "compression_ratio": 1.6215277777777777, "no_speech_prob": 1.1842793355754111e-05}, {"id": 222, "seek": 93000, "start": 945.0, "end": 950.0, "text": " Okay, so let's put the labels. So I pulled the labels out and they don't match, right?", "tokens": [50364, 759, 291, 630, 411, 376, 45, 19756, 11, 7263, 291, 727, 362, 257, 8252, 570, 412, 512, 935, 291, 603, 760, 19988, 781, 636, 886, 709, 13, 50664, 50664, 1033, 13, 407, 1338, 11, 309, 311, 257, 2238, 644, 466, 264, 5015, 31890, 11, 2597, 11, 466, 45216, 304, 9590, 307, 11, 291, 458, 11, 264, 4846, 393, 1319, 382, 731, 13, 51114, 51114, 1033, 11, 370, 718, 311, 829, 264, 16949, 13, 407, 286, 7373, 264, 16949, 484, 293, 436, 500, 380, 2995, 11, 558, 30, 51364, 51364, 407, 2745, 291, 434, 411, 11, 731, 11, 341, 307, 3442, 281, 312, 341, 10247, 659, 12, 17227, 2001, 3256, 2533, 2316, 11, 457, 411, 309, 994, 380, 6069, 264, 16949, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16093844604492188, "compression_ratio": 1.6215277777777777, "no_speech_prob": 1.1842793355754111e-05}, {"id": 223, "seek": 93000, "start": 950.0, "end": 956.0, "text": " So obviously you're like, well, this is supposed to be this fancy pre-trained image net model, but like it didn't predict the labels.", "tokens": [50364, 759, 291, 630, 411, 376, 45, 19756, 11, 7263, 291, 727, 362, 257, 8252, 570, 412, 512, 935, 291, 603, 760, 19988, 781, 636, 886, 709, 13, 50664, 50664, 1033, 13, 407, 1338, 11, 309, 311, 257, 2238, 644, 466, 264, 5015, 31890, 11, 2597, 11, 466, 45216, 304, 9590, 307, 11, 291, 458, 11, 264, 4846, 393, 1319, 382, 731, 13, 51114, 51114, 1033, 11, 370, 718, 311, 829, 264, 16949, 13, 407, 286, 7373, 264, 16949, 484, 293, 436, 500, 380, 2995, 11, 558, 30, 51364, 51364, 407, 2745, 291, 434, 411, 11, 731, 11, 341, 307, 3442, 281, 312, 341, 10247, 659, 12, 17227, 2001, 3256, 2533, 2316, 11, 457, 411, 309, 994, 380, 6069, 264, 16949, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.16093844604492188, "compression_ratio": 1.6215277777777777, "no_speech_prob": 1.1842793355754111e-05}, {"id": 224, "seek": 95600, "start": 956.0, "end": 962.0, "text": " So my labels are seven. I predicted zero. None of these are great. It's this one by chance.", "tokens": [50364, 407, 452, 16949, 366, 3407, 13, 286, 19147, 4018, 13, 14492, 295, 613, 366, 869, 13, 467, 311, 341, 472, 538, 2931, 13, 50664, 50664, 407, 472, 484, 295, 1266, 1669, 2020, 11, 558, 30, 46318, 264, 1230, 2212, 264, 5359, 13, 865, 11, 370, 436, 500, 380, 2995, 13, 51014, 51014, 407, 264, 1778, 337, 300, 307, 570, 321, 2378, 380, 11, 1604, 321, 10772, 341, 4583, 510, 11, 558, 30, 2305, 307, 309, 30, 865, 11, 510, 13, 51464, 51464, 12172, 11, 406, 456, 13, 1692, 13, 407, 321, 10772, 341, 4583, 13, 407, 411, 439, 295, 264, 1036, 9952, 11, 498, 291, 7406, 1340, 294, 341, 659, 12, 17227, 2001, 2316, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11914785369103696, "compression_ratio": 1.647940074906367, "no_speech_prob": 2.295885906278272e-06}, {"id": 225, "seek": 95600, "start": 962.0, "end": 969.0, "text": " So one out of 10 makes sense, right? Expect the number given the classes. Yeah, so they don't match.", "tokens": [50364, 407, 452, 16949, 366, 3407, 13, 286, 19147, 4018, 13, 14492, 295, 613, 366, 869, 13, 467, 311, 341, 472, 538, 2931, 13, 50664, 50664, 407, 472, 484, 295, 1266, 1669, 2020, 11, 558, 30, 46318, 264, 1230, 2212, 264, 5359, 13, 865, 11, 370, 436, 500, 380, 2995, 13, 51014, 51014, 407, 264, 1778, 337, 300, 307, 570, 321, 2378, 380, 11, 1604, 321, 10772, 341, 4583, 510, 11, 558, 30, 2305, 307, 309, 30, 865, 11, 510, 13, 51464, 51464, 12172, 11, 406, 456, 13, 1692, 13, 407, 321, 10772, 341, 4583, 13, 407, 411, 439, 295, 264, 1036, 9952, 11, 498, 291, 7406, 1340, 294, 341, 659, 12, 17227, 2001, 2316, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11914785369103696, "compression_ratio": 1.647940074906367, "no_speech_prob": 2.295885906278272e-06}, {"id": 226, "seek": 95600, "start": 969.0, "end": 978.0, "text": " So the reason for that is because we haven't, remember we replaced this layer here, right? Where is it? Yeah, here.", "tokens": [50364, 407, 452, 16949, 366, 3407, 13, 286, 19147, 4018, 13, 14492, 295, 613, 366, 869, 13, 467, 311, 341, 472, 538, 2931, 13, 50664, 50664, 407, 472, 484, 295, 1266, 1669, 2020, 11, 558, 30, 46318, 264, 1230, 2212, 264, 5359, 13, 865, 11, 370, 436, 500, 380, 2995, 13, 51014, 51014, 407, 264, 1778, 337, 300, 307, 570, 321, 2378, 380, 11, 1604, 321, 10772, 341, 4583, 510, 11, 558, 30, 2305, 307, 309, 30, 865, 11, 510, 13, 51464, 51464, 12172, 11, 406, 456, 13, 1692, 13, 407, 321, 10772, 341, 4583, 13, 407, 411, 439, 295, 264, 1036, 9952, 11, 498, 291, 7406, 1340, 294, 341, 659, 12, 17227, 2001, 2316, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11914785369103696, "compression_ratio": 1.647940074906367, "no_speech_prob": 2.295885906278272e-06}, {"id": 227, "seek": 95600, "start": 978.0, "end": 985.0, "text": " Nope, not there. Here. So we replaced this layer. So like all of the last logic, if you replace anything in this pre-trained model,", "tokens": [50364, 407, 452, 16949, 366, 3407, 13, 286, 19147, 4018, 13, 14492, 295, 613, 366, 869, 13, 467, 311, 341, 472, 538, 2931, 13, 50664, 50664, 407, 472, 484, 295, 1266, 1669, 2020, 11, 558, 30, 46318, 264, 1230, 2212, 264, 5359, 13, 865, 11, 370, 436, 500, 380, 2995, 13, 51014, 51014, 407, 264, 1778, 337, 300, 307, 570, 321, 2378, 380, 11, 1604, 321, 10772, 341, 4583, 510, 11, 558, 30, 2305, 307, 309, 30, 865, 11, 510, 13, 51464, 51464, 12172, 11, 406, 456, 13, 1692, 13, 407, 321, 10772, 341, 4583, 13, 407, 411, 439, 295, 264, 1036, 9952, 11, 498, 291, 7406, 1340, 294, 341, 659, 12, 17227, 2001, 2316, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11914785369103696, "compression_ratio": 1.647940074906367, "no_speech_prob": 2.295885906278272e-06}, {"id": 228, "seek": 98500, "start": 985.0, "end": 991.0, "text": " it's not going to work, right? And the last logic we made completely random. So now I have to fine tune this thing.", "tokens": [50364, 309, 311, 406, 516, 281, 589, 11, 558, 30, 400, 264, 1036, 9952, 321, 1027, 2584, 4974, 13, 407, 586, 286, 362, 281, 2489, 10864, 341, 551, 13, 50664, 50664, 407, 264, 1399, 295, 2489, 15164, 307, 11, 291, 458, 11, 291, 393, 360, 309, 732, 2098, 13, 509, 393, 747, 341, 659, 12, 17227, 2001, 2533, 3209, 293, 1066, 309, 733, 295, 12496, 11, 558, 30, 51114, 51114, 407, 291, 434, 1128, 516, 281, 646, 79, 1513, 559, 473, 666, 309, 293, 550, 18359, 1340, 1646, 322, 1192, 295, 300, 13, 51364, 51364, 286, 478, 516, 281, 764, 257, 8213, 4583, 11, 457, 291, 727, 764, 364, 31910, 44, 13, 509, 727, 764, 3565, 3142, 24590, 13, 509, 393, 764, 257, 4974, 6719, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06960011267846869, "compression_ratio": 1.7148014440433212, "no_speech_prob": 3.905393441527849e-06}, {"id": 229, "seek": 98500, "start": 991.0, "end": 1000.0, "text": " So the process of fine tuning is, you know, you can do it two ways. You can take this pre-trained net network and keep it kind of frozen, right?", "tokens": [50364, 309, 311, 406, 516, 281, 589, 11, 558, 30, 400, 264, 1036, 9952, 321, 1027, 2584, 4974, 13, 407, 586, 286, 362, 281, 2489, 10864, 341, 551, 13, 50664, 50664, 407, 264, 1399, 295, 2489, 15164, 307, 11, 291, 458, 11, 291, 393, 360, 309, 732, 2098, 13, 509, 393, 747, 341, 659, 12, 17227, 2001, 2533, 3209, 293, 1066, 309, 733, 295, 12496, 11, 558, 30, 51114, 51114, 407, 291, 434, 1128, 516, 281, 646, 79, 1513, 559, 473, 666, 309, 293, 550, 18359, 1340, 1646, 322, 1192, 295, 300, 13, 51364, 51364, 286, 478, 516, 281, 764, 257, 8213, 4583, 11, 457, 291, 727, 764, 364, 31910, 44, 13, 509, 727, 764, 3565, 3142, 24590, 13, 509, 393, 764, 257, 4974, 6719, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06960011267846869, "compression_ratio": 1.7148014440433212, "no_speech_prob": 3.905393441527849e-06}, {"id": 230, "seek": 98500, "start": 1000.0, "end": 1005.0, "text": " So you're never going to backpropagate into it and then strap anything else on top of that.", "tokens": [50364, 309, 311, 406, 516, 281, 589, 11, 558, 30, 400, 264, 1036, 9952, 321, 1027, 2584, 4974, 13, 407, 586, 286, 362, 281, 2489, 10864, 341, 551, 13, 50664, 50664, 407, 264, 1399, 295, 2489, 15164, 307, 11, 291, 458, 11, 291, 393, 360, 309, 732, 2098, 13, 509, 393, 747, 341, 659, 12, 17227, 2001, 2533, 3209, 293, 1066, 309, 733, 295, 12496, 11, 558, 30, 51114, 51114, 407, 291, 434, 1128, 516, 281, 646, 79, 1513, 559, 473, 666, 309, 293, 550, 18359, 1340, 1646, 322, 1192, 295, 300, 13, 51364, 51364, 286, 478, 516, 281, 764, 257, 8213, 4583, 11, 457, 291, 727, 764, 364, 31910, 44, 13, 509, 727, 764, 3565, 3142, 24590, 13, 509, 393, 764, 257, 4974, 6719, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06960011267846869, "compression_ratio": 1.7148014440433212, "no_speech_prob": 3.905393441527849e-06}, {"id": 231, "seek": 98500, "start": 1005.0, "end": 1012.0, "text": " I'm going to use a linear layer, but you could use an SVM. You could use logistic regression. You can use a random forest.", "tokens": [50364, 309, 311, 406, 516, 281, 589, 11, 558, 30, 400, 264, 1036, 9952, 321, 1027, 2584, 4974, 13, 407, 586, 286, 362, 281, 2489, 10864, 341, 551, 13, 50664, 50664, 407, 264, 1399, 295, 2489, 15164, 307, 11, 291, 458, 11, 291, 393, 360, 309, 732, 2098, 13, 509, 393, 747, 341, 659, 12, 17227, 2001, 2533, 3209, 293, 1066, 309, 733, 295, 12496, 11, 558, 30, 51114, 51114, 407, 291, 434, 1128, 516, 281, 646, 79, 1513, 559, 473, 666, 309, 293, 550, 18359, 1340, 1646, 322, 1192, 295, 300, 13, 51364, 51364, 286, 478, 516, 281, 764, 257, 8213, 4583, 11, 457, 291, 727, 764, 364, 31910, 44, 13, 509, 727, 764, 3565, 3142, 24590, 13, 509, 393, 764, 257, 4974, 6719, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.06960011267846869, "compression_ratio": 1.7148014440433212, "no_speech_prob": 3.905393441527849e-06}, {"id": 232, "seek": 101200, "start": 1012.0, "end": 1018.0, "text": " Any classifier doesn't matter because what you're doing is using the neural network to extract features", "tokens": [50364, 2639, 1508, 9902, 1177, 380, 1871, 570, 437, 291, 434, 884, 307, 1228, 264, 18161, 3209, 281, 8947, 4122, 50664, 50664, 293, 550, 291, 434, 1228, 512, 661, 1508, 9902, 281, 764, 729, 4122, 293, 33872, 11, 558, 30, 50864, 50864, 400, 498, 264, 3209, 630, 1080, 1691, 11, 436, 820, 312, 43586, 3128, 712, 13, 407, 550, 300, 31910, 44, 576, 312, 2489, 412, 300, 935, 13, 51214, 51214, 407, 294, 341, 1389, 11, 286, 478, 516, 281, 312, 14847, 13, 286, 478, 445, 516, 281, 764, 257, 8213, 4583, 11, 558, 30, 407, 286, 478, 516, 281, 767, 4994, 552, 281, 652, 341, 544, 1850, 13, 51564, 51564, 407, 510, 311, 264, 34889, 13, 2264, 11, 370, 286, 478, 406, 516, 281, 2082, 365, 1340, 466, 264, 27168, 4583, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07311689853668213, "compression_ratio": 1.7933333333333332, "no_speech_prob": 6.85422537571867e-06}, {"id": 233, "seek": 101200, "start": 1018.0, "end": 1022.0, "text": " and then you're using some other classifier to use those features and classify, right?", "tokens": [50364, 2639, 1508, 9902, 1177, 380, 1871, 570, 437, 291, 434, 884, 307, 1228, 264, 18161, 3209, 281, 8947, 4122, 50664, 50664, 293, 550, 291, 434, 1228, 512, 661, 1508, 9902, 281, 764, 729, 4122, 293, 33872, 11, 558, 30, 50864, 50864, 400, 498, 264, 3209, 630, 1080, 1691, 11, 436, 820, 312, 43586, 3128, 712, 13, 407, 550, 300, 31910, 44, 576, 312, 2489, 412, 300, 935, 13, 51214, 51214, 407, 294, 341, 1389, 11, 286, 478, 516, 281, 312, 14847, 13, 286, 478, 445, 516, 281, 764, 257, 8213, 4583, 11, 558, 30, 407, 286, 478, 516, 281, 767, 4994, 552, 281, 652, 341, 544, 1850, 13, 51564, 51564, 407, 510, 311, 264, 34889, 13, 2264, 11, 370, 286, 478, 406, 516, 281, 2082, 365, 1340, 466, 264, 27168, 4583, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07311689853668213, "compression_ratio": 1.7933333333333332, "no_speech_prob": 6.85422537571867e-06}, {"id": 234, "seek": 101200, "start": 1022.0, "end": 1029.0, "text": " And if the network did its job, they should be linearly separable. So then that SVM would be fine at that point.", "tokens": [50364, 2639, 1508, 9902, 1177, 380, 1871, 570, 437, 291, 434, 884, 307, 1228, 264, 18161, 3209, 281, 8947, 4122, 50664, 50664, 293, 550, 291, 434, 1228, 512, 661, 1508, 9902, 281, 764, 729, 4122, 293, 33872, 11, 558, 30, 50864, 50864, 400, 498, 264, 3209, 630, 1080, 1691, 11, 436, 820, 312, 43586, 3128, 712, 13, 407, 550, 300, 31910, 44, 576, 312, 2489, 412, 300, 935, 13, 51214, 51214, 407, 294, 341, 1389, 11, 286, 478, 516, 281, 312, 14847, 13, 286, 478, 445, 516, 281, 764, 257, 8213, 4583, 11, 558, 30, 407, 286, 478, 516, 281, 767, 4994, 552, 281, 652, 341, 544, 1850, 13, 51564, 51564, 407, 510, 311, 264, 34889, 13, 2264, 11, 370, 286, 478, 406, 516, 281, 2082, 365, 1340, 466, 264, 27168, 4583, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07311689853668213, "compression_ratio": 1.7933333333333332, "no_speech_prob": 6.85422537571867e-06}, {"id": 235, "seek": 101200, "start": 1029.0, "end": 1036.0, "text": " So in this case, I'm going to be lazy. I'm just going to use a linear layer, right? So I'm going to actually separate them to make this more clear.", "tokens": [50364, 2639, 1508, 9902, 1177, 380, 1871, 570, 437, 291, 434, 884, 307, 1228, 264, 18161, 3209, 281, 8947, 4122, 50664, 50664, 293, 550, 291, 434, 1228, 512, 661, 1508, 9902, 281, 764, 729, 4122, 293, 33872, 11, 558, 30, 50864, 50864, 400, 498, 264, 3209, 630, 1080, 1691, 11, 436, 820, 312, 43586, 3128, 712, 13, 407, 550, 300, 31910, 44, 576, 312, 2489, 412, 300, 935, 13, 51214, 51214, 407, 294, 341, 1389, 11, 286, 478, 516, 281, 312, 14847, 13, 286, 478, 445, 516, 281, 764, 257, 8213, 4583, 11, 558, 30, 407, 286, 478, 516, 281, 767, 4994, 552, 281, 652, 341, 544, 1850, 13, 51564, 51564, 407, 510, 311, 264, 34889, 13, 2264, 11, 370, 286, 478, 406, 516, 281, 2082, 365, 1340, 466, 264, 27168, 4583, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07311689853668213, "compression_ratio": 1.7933333333333332, "no_speech_prob": 6.85422537571867e-06}, {"id": 236, "seek": 101200, "start": 1036.0, "end": 1041.0, "text": " So here's the backbone. OK, so I'm not going to mess with anything about the FC layer.", "tokens": [50364, 2639, 1508, 9902, 1177, 380, 1871, 570, 437, 291, 434, 884, 307, 1228, 264, 18161, 3209, 281, 8947, 4122, 50664, 50664, 293, 550, 291, 434, 1228, 512, 661, 1508, 9902, 281, 764, 729, 4122, 293, 33872, 11, 558, 30, 50864, 50864, 400, 498, 264, 3209, 630, 1080, 1691, 11, 436, 820, 312, 43586, 3128, 712, 13, 407, 550, 300, 31910, 44, 576, 312, 2489, 412, 300, 935, 13, 51214, 51214, 407, 294, 341, 1389, 11, 286, 478, 516, 281, 312, 14847, 13, 286, 478, 445, 516, 281, 764, 257, 8213, 4583, 11, 558, 30, 407, 286, 478, 516, 281, 767, 4994, 552, 281, 652, 341, 544, 1850, 13, 51564, 51564, 407, 510, 311, 264, 34889, 13, 2264, 11, 370, 286, 478, 406, 516, 281, 2082, 365, 1340, 466, 264, 27168, 4583, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.07311689853668213, "compression_ratio": 1.7933333333333332, "no_speech_prob": 6.85422537571867e-06}, {"id": 237, "seek": 104100, "start": 1041.0, "end": 1046.0, "text": " I'm just going to create a completely separate layer now. So this is I'm going to call this a fine-tune layer.", "tokens": [50364, 286, 478, 445, 516, 281, 1884, 257, 2584, 4994, 4583, 586, 13, 407, 341, 307, 286, 478, 516, 281, 818, 341, 257, 2489, 12, 83, 2613, 4583, 13, 50614, 50614, 400, 286, 478, 445, 516, 281, 747, 2035, 264, 5598, 4122, 645, 295, 300, 34889, 11, 27168, 11, 293, 550, 286, 478, 516, 281, 4471, 552, 646, 281, 1266, 13, 50914, 50914, 407, 286, 478, 445, 5127, 1071, 4583, 13, 51114, 51114, 407, 294, 37884, 11, 370, 294, 28848, 11, 321, 362, 341, 3410, 295, 1412, 16679, 11, 558, 30, 51364, 51364, 407, 3449, 286, 787, 362, 257, 3847, 7472, 337, 383, 19, 3279, 11, 457, 286, 767, 528, 411, 257, 24071, 293, 1500, 7472, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12260069847106933, "compression_ratio": 1.6629213483146068, "no_speech_prob": 7.527743946411647e-06}, {"id": 238, "seek": 104100, "start": 1046.0, "end": 1052.0, "text": " And I'm just going to take whatever the output features were of that backbone, FC, and then I'm going to map them back to 10.", "tokens": [50364, 286, 478, 445, 516, 281, 1884, 257, 2584, 4994, 4583, 586, 13, 407, 341, 307, 286, 478, 516, 281, 818, 341, 257, 2489, 12, 83, 2613, 4583, 13, 50614, 50614, 400, 286, 478, 445, 516, 281, 747, 2035, 264, 5598, 4122, 645, 295, 300, 34889, 11, 27168, 11, 293, 550, 286, 478, 516, 281, 4471, 552, 646, 281, 1266, 13, 50914, 50914, 407, 286, 478, 445, 5127, 1071, 4583, 13, 51114, 51114, 407, 294, 37884, 11, 370, 294, 28848, 11, 321, 362, 341, 3410, 295, 1412, 16679, 11, 558, 30, 51364, 51364, 407, 3449, 286, 787, 362, 257, 3847, 7472, 337, 383, 19, 3279, 11, 457, 286, 767, 528, 411, 257, 24071, 293, 1500, 7472, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12260069847106933, "compression_ratio": 1.6629213483146068, "no_speech_prob": 7.527743946411647e-06}, {"id": 239, "seek": 104100, "start": 1052.0, "end": 1056.0, "text": " So I'm just adding another layer.", "tokens": [50364, 286, 478, 445, 516, 281, 1884, 257, 2584, 4994, 4583, 586, 13, 407, 341, 307, 286, 478, 516, 281, 818, 341, 257, 2489, 12, 83, 2613, 4583, 13, 50614, 50614, 400, 286, 478, 445, 516, 281, 747, 2035, 264, 5598, 4122, 645, 295, 300, 34889, 11, 27168, 11, 293, 550, 286, 478, 516, 281, 4471, 552, 646, 281, 1266, 13, 50914, 50914, 407, 286, 478, 445, 5127, 1071, 4583, 13, 51114, 51114, 407, 294, 37884, 11, 370, 294, 28848, 11, 321, 362, 341, 3410, 295, 1412, 16679, 11, 558, 30, 51364, 51364, 407, 3449, 286, 787, 362, 257, 3847, 7472, 337, 383, 19, 3279, 11, 457, 286, 767, 528, 411, 257, 24071, 293, 1500, 7472, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12260069847106933, "compression_ratio": 1.6629213483146068, "no_speech_prob": 7.527743946411647e-06}, {"id": 240, "seek": 104100, "start": 1056.0, "end": 1061.0, "text": " So in Bolt, so in Lightning, we have this concept of data modules, right?", "tokens": [50364, 286, 478, 445, 516, 281, 1884, 257, 2584, 4994, 4583, 586, 13, 407, 341, 307, 286, 478, 516, 281, 818, 341, 257, 2489, 12, 83, 2613, 4583, 13, 50614, 50614, 400, 286, 478, 445, 516, 281, 747, 2035, 264, 5598, 4122, 645, 295, 300, 34889, 11, 27168, 11, 293, 550, 286, 478, 516, 281, 4471, 552, 646, 281, 1266, 13, 50914, 50914, 407, 286, 478, 445, 5127, 1071, 4583, 13, 51114, 51114, 407, 294, 37884, 11, 370, 294, 28848, 11, 321, 362, 341, 3410, 295, 1412, 16679, 11, 558, 30, 51364, 51364, 407, 3449, 286, 787, 362, 257, 3847, 7472, 337, 383, 19, 3279, 11, 457, 286, 767, 528, 411, 257, 24071, 293, 1500, 7472, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12260069847106933, "compression_ratio": 1.6629213483146068, "no_speech_prob": 7.527743946411647e-06}, {"id": 241, "seek": 104100, "start": 1061.0, "end": 1068.0, "text": " So notice I only have a train split for C410, but I actually want like a validation and test split.", "tokens": [50364, 286, 478, 445, 516, 281, 1884, 257, 2584, 4994, 4583, 586, 13, 407, 341, 307, 286, 478, 516, 281, 818, 341, 257, 2489, 12, 83, 2613, 4583, 13, 50614, 50614, 400, 286, 478, 445, 516, 281, 747, 2035, 264, 5598, 4122, 645, 295, 300, 34889, 11, 27168, 11, 293, 550, 286, 478, 516, 281, 4471, 552, 646, 281, 1266, 13, 50914, 50914, 407, 286, 478, 445, 5127, 1071, 4583, 13, 51114, 51114, 407, 294, 37884, 11, 370, 294, 28848, 11, 321, 362, 341, 3410, 295, 1412, 16679, 11, 558, 30, 51364, 51364, 407, 3449, 286, 787, 362, 257, 3847, 7472, 337, 383, 19, 3279, 11, 457, 286, 767, 528, 411, 257, 24071, 293, 1500, 7472, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12260069847106933, "compression_ratio": 1.6629213483146068, "no_speech_prob": 7.527743946411647e-06}, {"id": 242, "seek": 106800, "start": 1068.0, "end": 1072.0, "text": " I'm like, you know, it's just going to be super annoying to do it myself and split them up.", "tokens": [50364, 286, 478, 411, 11, 291, 458, 11, 309, 311, 445, 516, 281, 312, 1687, 11304, 281, 360, 309, 2059, 293, 7472, 552, 493, 13, 50564, 50564, 407, 286, 478, 445, 516, 281, 764, 341, 1412, 10088, 300, 321, 362, 11, 558, 30, 50714, 50714, 407, 264, 1412, 10088, 307, 3736, 445, 516, 281, 312, 1045, 1412, 3677, 433, 11, 558, 30, 50914, 50914, 407, 309, 311, 516, 281, 312, 257, 3847, 11, 257, 1323, 11, 293, 257, 1500, 1412, 3677, 260, 13, 51064, 51064, 400, 550, 309, 575, 1951, 309, 439, 264, 37741, 293, 1203, 291, 643, 281, 1127, 466, 13, 51264, 51264, 407, 718, 385, 855, 291, 437, 286, 914, 538, 300, 13, 407, 352, 281, 264, 14333, 13, 51464, 51464, 286, 603, 352, 281, 264, 5201, 1412, 16679, 11, 46533, 2539, 11, 383, 19, 3279, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06918274516790686, "compression_ratio": 1.8419117647058822, "no_speech_prob": 4.71073190055904e-06}, {"id": 243, "seek": 106800, "start": 1072.0, "end": 1075.0, "text": " So I'm just going to use this data module that we have, right?", "tokens": [50364, 286, 478, 411, 11, 291, 458, 11, 309, 311, 445, 516, 281, 312, 1687, 11304, 281, 360, 309, 2059, 293, 7472, 552, 493, 13, 50564, 50564, 407, 286, 478, 445, 516, 281, 764, 341, 1412, 10088, 300, 321, 362, 11, 558, 30, 50714, 50714, 407, 264, 1412, 10088, 307, 3736, 445, 516, 281, 312, 1045, 1412, 3677, 433, 11, 558, 30, 50914, 50914, 407, 309, 311, 516, 281, 312, 257, 3847, 11, 257, 1323, 11, 293, 257, 1500, 1412, 3677, 260, 13, 51064, 51064, 400, 550, 309, 575, 1951, 309, 439, 264, 37741, 293, 1203, 291, 643, 281, 1127, 466, 13, 51264, 51264, 407, 718, 385, 855, 291, 437, 286, 914, 538, 300, 13, 407, 352, 281, 264, 14333, 13, 51464, 51464, 286, 603, 352, 281, 264, 5201, 1412, 16679, 11, 46533, 2539, 11, 383, 19, 3279, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06918274516790686, "compression_ratio": 1.8419117647058822, "no_speech_prob": 4.71073190055904e-06}, {"id": 244, "seek": 106800, "start": 1075.0, "end": 1079.0, "text": " So the data module is literally just going to be three data loaders, right?", "tokens": [50364, 286, 478, 411, 11, 291, 458, 11, 309, 311, 445, 516, 281, 312, 1687, 11304, 281, 360, 309, 2059, 293, 7472, 552, 493, 13, 50564, 50564, 407, 286, 478, 445, 516, 281, 764, 341, 1412, 10088, 300, 321, 362, 11, 558, 30, 50714, 50714, 407, 264, 1412, 10088, 307, 3736, 445, 516, 281, 312, 1045, 1412, 3677, 433, 11, 558, 30, 50914, 50914, 407, 309, 311, 516, 281, 312, 257, 3847, 11, 257, 1323, 11, 293, 257, 1500, 1412, 3677, 260, 13, 51064, 51064, 400, 550, 309, 575, 1951, 309, 439, 264, 37741, 293, 1203, 291, 643, 281, 1127, 466, 13, 51264, 51264, 407, 718, 385, 855, 291, 437, 286, 914, 538, 300, 13, 407, 352, 281, 264, 14333, 13, 51464, 51464, 286, 603, 352, 281, 264, 5201, 1412, 16679, 11, 46533, 2539, 11, 383, 19, 3279, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06918274516790686, "compression_ratio": 1.8419117647058822, "no_speech_prob": 4.71073190055904e-06}, {"id": 245, "seek": 106800, "start": 1079.0, "end": 1082.0, "text": " So it's going to be a train, a val, and a test data loader.", "tokens": [50364, 286, 478, 411, 11, 291, 458, 11, 309, 311, 445, 516, 281, 312, 1687, 11304, 281, 360, 309, 2059, 293, 7472, 552, 493, 13, 50564, 50564, 407, 286, 478, 445, 516, 281, 764, 341, 1412, 10088, 300, 321, 362, 11, 558, 30, 50714, 50714, 407, 264, 1412, 10088, 307, 3736, 445, 516, 281, 312, 1045, 1412, 3677, 433, 11, 558, 30, 50914, 50914, 407, 309, 311, 516, 281, 312, 257, 3847, 11, 257, 1323, 11, 293, 257, 1500, 1412, 3677, 260, 13, 51064, 51064, 400, 550, 309, 575, 1951, 309, 439, 264, 37741, 293, 1203, 291, 643, 281, 1127, 466, 13, 51264, 51264, 407, 718, 385, 855, 291, 437, 286, 914, 538, 300, 13, 407, 352, 281, 264, 14333, 13, 51464, 51464, 286, 603, 352, 281, 264, 5201, 1412, 16679, 11, 46533, 2539, 11, 383, 19, 3279, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06918274516790686, "compression_ratio": 1.8419117647058822, "no_speech_prob": 4.71073190055904e-06}, {"id": 246, "seek": 106800, "start": 1082.0, "end": 1086.0, "text": " And then it has within it all the splits and everything you need to care about.", "tokens": [50364, 286, 478, 411, 11, 291, 458, 11, 309, 311, 445, 516, 281, 312, 1687, 11304, 281, 360, 309, 2059, 293, 7472, 552, 493, 13, 50564, 50564, 407, 286, 478, 445, 516, 281, 764, 341, 1412, 10088, 300, 321, 362, 11, 558, 30, 50714, 50714, 407, 264, 1412, 10088, 307, 3736, 445, 516, 281, 312, 1045, 1412, 3677, 433, 11, 558, 30, 50914, 50914, 407, 309, 311, 516, 281, 312, 257, 3847, 11, 257, 1323, 11, 293, 257, 1500, 1412, 3677, 260, 13, 51064, 51064, 400, 550, 309, 575, 1951, 309, 439, 264, 37741, 293, 1203, 291, 643, 281, 1127, 466, 13, 51264, 51264, 407, 718, 385, 855, 291, 437, 286, 914, 538, 300, 13, 407, 352, 281, 264, 14333, 13, 51464, 51464, 286, 603, 352, 281, 264, 5201, 1412, 16679, 11, 46533, 2539, 11, 383, 19, 3279, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06918274516790686, "compression_ratio": 1.8419117647058822, "no_speech_prob": 4.71073190055904e-06}, {"id": 247, "seek": 106800, "start": 1086.0, "end": 1090.0, "text": " So let me show you what I mean by that. So go to the documentation.", "tokens": [50364, 286, 478, 411, 11, 291, 458, 11, 309, 311, 445, 516, 281, 312, 1687, 11304, 281, 360, 309, 2059, 293, 7472, 552, 493, 13, 50564, 50564, 407, 286, 478, 445, 516, 281, 764, 341, 1412, 10088, 300, 321, 362, 11, 558, 30, 50714, 50714, 407, 264, 1412, 10088, 307, 3736, 445, 516, 281, 312, 1045, 1412, 3677, 433, 11, 558, 30, 50914, 50914, 407, 309, 311, 516, 281, 312, 257, 3847, 11, 257, 1323, 11, 293, 257, 1500, 1412, 3677, 260, 13, 51064, 51064, 400, 550, 309, 575, 1951, 309, 439, 264, 37741, 293, 1203, 291, 643, 281, 1127, 466, 13, 51264, 51264, 407, 718, 385, 855, 291, 437, 286, 914, 538, 300, 13, 407, 352, 281, 264, 14333, 13, 51464, 51464, 286, 603, 352, 281, 264, 5201, 1412, 16679, 11, 46533, 2539, 11, 383, 19, 3279, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06918274516790686, "compression_ratio": 1.8419117647058822, "no_speech_prob": 4.71073190055904e-06}, {"id": 248, "seek": 106800, "start": 1090.0, "end": 1096.0, "text": " I'll go to the vision data modules, supervised learning, C410.", "tokens": [50364, 286, 478, 411, 11, 291, 458, 11, 309, 311, 445, 516, 281, 312, 1687, 11304, 281, 360, 309, 2059, 293, 7472, 552, 493, 13, 50564, 50564, 407, 286, 478, 445, 516, 281, 764, 341, 1412, 10088, 300, 321, 362, 11, 558, 30, 50714, 50714, 407, 264, 1412, 10088, 307, 3736, 445, 516, 281, 312, 1045, 1412, 3677, 433, 11, 558, 30, 50914, 50914, 407, 309, 311, 516, 281, 312, 257, 3847, 11, 257, 1323, 11, 293, 257, 1500, 1412, 3677, 260, 13, 51064, 51064, 400, 550, 309, 575, 1951, 309, 439, 264, 37741, 293, 1203, 291, 643, 281, 1127, 466, 13, 51264, 51264, 407, 718, 385, 855, 291, 437, 286, 914, 538, 300, 13, 407, 352, 281, 264, 14333, 13, 51464, 51464, 286, 603, 352, 281, 264, 5201, 1412, 16679, 11, 46533, 2539, 11, 383, 19, 3279, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.06918274516790686, "compression_ratio": 1.8419117647058822, "no_speech_prob": 4.71073190055904e-06}, {"id": 249, "seek": 109600, "start": 1096.0, "end": 1100.0, "text": " So they're here and I can look at the source. So here's the source code.", "tokens": [50364, 407, 436, 434, 510, 293, 286, 393, 574, 412, 264, 4009, 13, 407, 510, 311, 264, 4009, 3089, 13, 50564, 50564, 400, 586, 286, 536, 300, 11, 291, 458, 11, 456, 311, 257, 3840, 295, 13081, 293, 439, 264, 733, 295, 411, 39228, 37008, 1507, 307, 2726, 1127, 295, 337, 291, 13, 50964, 50964, 407, 286, 478, 445, 516, 281, 483, 257, 1412, 3677, 260, 365, 264, 3847, 37741, 11, 257, 1412, 3677, 260, 365, 264, 24071, 37741, 11, 293, 257, 1412, 3677, 260, 365, 257, 1500, 7472, 13, 51364, 51364, 400, 286, 500, 380, 362, 281, 2028, 365, 604, 295, 341, 661, 5585, 11, 457, 309, 311, 445, 257, 9953, 51, 284, 339, 1412, 3677, 260, 13, 51564, 51564, 407, 286, 478, 516, 281, 764, 341, 2146, 570, 286, 528, 281, 3155, 505, 439, 257, 3840, 295, 565, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08086041609446208, "compression_ratio": 1.8514492753623188, "no_speech_prob": 1.1125019227620214e-05}, {"id": 250, "seek": 109600, "start": 1100.0, "end": 1108.0, "text": " And now I see that, you know, there's a bunch of preparation and all the kind of like boilerplate stuff is taken care of for you.", "tokens": [50364, 407, 436, 434, 510, 293, 286, 393, 574, 412, 264, 4009, 13, 407, 510, 311, 264, 4009, 3089, 13, 50564, 50564, 400, 586, 286, 536, 300, 11, 291, 458, 11, 456, 311, 257, 3840, 295, 13081, 293, 439, 264, 733, 295, 411, 39228, 37008, 1507, 307, 2726, 1127, 295, 337, 291, 13, 50964, 50964, 407, 286, 478, 445, 516, 281, 483, 257, 1412, 3677, 260, 365, 264, 3847, 37741, 11, 257, 1412, 3677, 260, 365, 264, 24071, 37741, 11, 293, 257, 1412, 3677, 260, 365, 257, 1500, 7472, 13, 51364, 51364, 400, 286, 500, 380, 362, 281, 2028, 365, 604, 295, 341, 661, 5585, 11, 457, 309, 311, 445, 257, 9953, 51, 284, 339, 1412, 3677, 260, 13, 51564, 51564, 407, 286, 478, 516, 281, 764, 341, 2146, 570, 286, 528, 281, 3155, 505, 439, 257, 3840, 295, 565, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08086041609446208, "compression_ratio": 1.8514492753623188, "no_speech_prob": 1.1125019227620214e-05}, {"id": 251, "seek": 109600, "start": 1108.0, "end": 1116.0, "text": " So I'm just going to get a data loader with the train splits, a data loader with the validation splits, and a data loader with a test split.", "tokens": [50364, 407, 436, 434, 510, 293, 286, 393, 574, 412, 264, 4009, 13, 407, 510, 311, 264, 4009, 3089, 13, 50564, 50564, 400, 586, 286, 536, 300, 11, 291, 458, 11, 456, 311, 257, 3840, 295, 13081, 293, 439, 264, 733, 295, 411, 39228, 37008, 1507, 307, 2726, 1127, 295, 337, 291, 13, 50964, 50964, 407, 286, 478, 445, 516, 281, 483, 257, 1412, 3677, 260, 365, 264, 3847, 37741, 11, 257, 1412, 3677, 260, 365, 264, 24071, 37741, 11, 293, 257, 1412, 3677, 260, 365, 257, 1500, 7472, 13, 51364, 51364, 400, 286, 500, 380, 362, 281, 2028, 365, 604, 295, 341, 661, 5585, 11, 457, 309, 311, 445, 257, 9953, 51, 284, 339, 1412, 3677, 260, 13, 51564, 51564, 407, 286, 478, 516, 281, 764, 341, 2146, 570, 286, 528, 281, 3155, 505, 439, 257, 3840, 295, 565, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08086041609446208, "compression_ratio": 1.8514492753623188, "no_speech_prob": 1.1125019227620214e-05}, {"id": 252, "seek": 109600, "start": 1116.0, "end": 1120.0, "text": " And I don't have to deal with any of this other magic, but it's just a PyTorch data loader.", "tokens": [50364, 407, 436, 434, 510, 293, 286, 393, 574, 412, 264, 4009, 13, 407, 510, 311, 264, 4009, 3089, 13, 50564, 50564, 400, 586, 286, 536, 300, 11, 291, 458, 11, 456, 311, 257, 3840, 295, 13081, 293, 439, 264, 733, 295, 411, 39228, 37008, 1507, 307, 2726, 1127, 295, 337, 291, 13, 50964, 50964, 407, 286, 478, 445, 516, 281, 483, 257, 1412, 3677, 260, 365, 264, 3847, 37741, 11, 257, 1412, 3677, 260, 365, 264, 24071, 37741, 11, 293, 257, 1412, 3677, 260, 365, 257, 1500, 7472, 13, 51364, 51364, 400, 286, 500, 380, 362, 281, 2028, 365, 604, 295, 341, 661, 5585, 11, 457, 309, 311, 445, 257, 9953, 51, 284, 339, 1412, 3677, 260, 13, 51564, 51564, 407, 286, 478, 516, 281, 764, 341, 2146, 570, 286, 528, 281, 3155, 505, 439, 257, 3840, 295, 565, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08086041609446208, "compression_ratio": 1.8514492753623188, "no_speech_prob": 1.1125019227620214e-05}, {"id": 253, "seek": 109600, "start": 1120.0, "end": 1125.0, "text": " So I'm going to use this guy because I want to save us all a bunch of time.", "tokens": [50364, 407, 436, 434, 510, 293, 286, 393, 574, 412, 264, 4009, 13, 407, 510, 311, 264, 4009, 3089, 13, 50564, 50564, 400, 586, 286, 536, 300, 11, 291, 458, 11, 456, 311, 257, 3840, 295, 13081, 293, 439, 264, 733, 295, 411, 39228, 37008, 1507, 307, 2726, 1127, 295, 337, 291, 13, 50964, 50964, 407, 286, 478, 445, 516, 281, 483, 257, 1412, 3677, 260, 365, 264, 3847, 37741, 11, 257, 1412, 3677, 260, 365, 264, 24071, 37741, 11, 293, 257, 1412, 3677, 260, 365, 257, 1500, 7472, 13, 51364, 51364, 400, 286, 500, 380, 362, 281, 2028, 365, 604, 295, 341, 661, 5585, 11, 457, 309, 311, 445, 257, 9953, 51, 284, 339, 1412, 3677, 260, 13, 51564, 51564, 407, 286, 478, 516, 281, 764, 341, 2146, 570, 286, 528, 281, 3155, 505, 439, 257, 3840, 295, 565, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.08086041609446208, "compression_ratio": 1.8514492753623188, "no_speech_prob": 1.1125019227620214e-05}, {"id": 254, "seek": 112500, "start": 1125.0, "end": 1130.0, "text": " So let me just call this up and play PyTorch real quick just to show the basics of this.", "tokens": [50364, 407, 718, 385, 445, 818, 341, 493, 293, 862, 9953, 51, 284, 339, 957, 1702, 445, 281, 855, 264, 14688, 295, 341, 13, 50614, 50614, 407, 286, 478, 516, 281, 974, 364, 5028, 6545, 293, 550, 527, 1036, 2445, 510, 13, 50864, 50864, 400, 286, 478, 516, 281, 764, 7938, 11, 457, 286, 478, 406, 516, 281, 286, 500, 380, 528, 281, 5623, 341, 34889, 570, 382, 2321, 382, 286, 722, 25113, 309, 11, 309, 311, 516, 281, 3624, 733, 295, 1080, 10290, 13, 51464, 51464, 865, 13, 865, 13, 2264, 13, 286, 445, 528, 281, 764, 257, 1508, 9902, 13, 865, 13, 1449, 257, 1508, 9902, 558, 586, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18785988657098068, "compression_ratio": 1.6653225806451613, "no_speech_prob": 1.3211218174546957e-05}, {"id": 255, "seek": 112500, "start": 1130.0, "end": 1135.0, "text": " So I'm going to import an optimizer and then our last function here.", "tokens": [50364, 407, 718, 385, 445, 818, 341, 493, 293, 862, 9953, 51, 284, 339, 957, 1702, 445, 281, 855, 264, 14688, 295, 341, 13, 50614, 50614, 407, 286, 478, 516, 281, 974, 364, 5028, 6545, 293, 550, 527, 1036, 2445, 510, 13, 50864, 50864, 400, 286, 478, 516, 281, 764, 7938, 11, 457, 286, 478, 406, 516, 281, 286, 500, 380, 528, 281, 5623, 341, 34889, 570, 382, 2321, 382, 286, 722, 25113, 309, 11, 309, 311, 516, 281, 3624, 733, 295, 1080, 10290, 13, 51464, 51464, 865, 13, 865, 13, 2264, 13, 286, 445, 528, 281, 764, 257, 1508, 9902, 13, 865, 13, 1449, 257, 1508, 9902, 558, 586, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18785988657098068, "compression_ratio": 1.6653225806451613, "no_speech_prob": 1.3211218174546957e-05}, {"id": 256, "seek": 112500, "start": 1135.0, "end": 1147.0, "text": " And I'm going to use Adam, but I'm not going to I don't want to update this backbone because as soon as I start updating it, it's going to lose kind of its representation.", "tokens": [50364, 407, 718, 385, 445, 818, 341, 493, 293, 862, 9953, 51, 284, 339, 957, 1702, 445, 281, 855, 264, 14688, 295, 341, 13, 50614, 50614, 407, 286, 478, 516, 281, 974, 364, 5028, 6545, 293, 550, 527, 1036, 2445, 510, 13, 50864, 50864, 400, 286, 478, 516, 281, 764, 7938, 11, 457, 286, 478, 406, 516, 281, 286, 500, 380, 528, 281, 5623, 341, 34889, 570, 382, 2321, 382, 286, 722, 25113, 309, 11, 309, 311, 516, 281, 3624, 733, 295, 1080, 10290, 13, 51464, 51464, 865, 13, 865, 13, 2264, 13, 286, 445, 528, 281, 764, 257, 1508, 9902, 13, 865, 13, 1449, 257, 1508, 9902, 558, 586, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18785988657098068, "compression_ratio": 1.6653225806451613, "no_speech_prob": 1.3211218174546957e-05}, {"id": 257, "seek": 112500, "start": 1147.0, "end": 1152.0, "text": " Yeah. Yeah. OK. I just want to use a classifier. Yeah. Just a classifier right now.", "tokens": [50364, 407, 718, 385, 445, 818, 341, 493, 293, 862, 9953, 51, 284, 339, 957, 1702, 445, 281, 855, 264, 14688, 295, 341, 13, 50614, 50614, 407, 286, 478, 516, 281, 974, 364, 5028, 6545, 293, 550, 527, 1036, 2445, 510, 13, 50864, 50864, 400, 286, 478, 516, 281, 764, 7938, 11, 457, 286, 478, 406, 516, 281, 286, 500, 380, 528, 281, 5623, 341, 34889, 570, 382, 2321, 382, 286, 722, 25113, 309, 11, 309, 311, 516, 281, 3624, 733, 295, 1080, 10290, 13, 51464, 51464, 865, 13, 865, 13, 2264, 13, 286, 445, 528, 281, 764, 257, 1508, 9902, 13, 865, 13, 1449, 257, 1508, 9902, 558, 586, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18785988657098068, "compression_ratio": 1.6653225806451613, "no_speech_prob": 1.3211218174546957e-05}, {"id": 258, "seek": 115200, "start": 1152.0, "end": 1157.0, "text": " So I'm going to do that. And then I'm going to just iterate through my, you know, I'm going to set an epoch here.", "tokens": [50364, 407, 286, 478, 516, 281, 360, 300, 13, 400, 550, 286, 478, 516, 281, 445, 44497, 807, 452, 11, 291, 458, 11, 286, 478, 516, 281, 992, 364, 30992, 339, 510, 13, 50614, 50614, 400, 550, 797, 11, 341, 307, 264, 1412, 3677, 260, 13, 286, 393, 445, 764, 264, 1412, 3677, 433, 3838, 13, 1779, 13, 50914, 50914, 639, 307, 291, 393, 764, 552, 294, 9953, 51, 284, 339, 13, 407, 286, 478, 445, 516, 281, 2235, 264, 3847, 1412, 3677, 260, 484, 293, 550, 44497, 807, 510, 13, 51164, 51164, 400, 550, 286, 528, 281, 1190, 264, 4846, 807, 264, 34889, 13, 286, 478, 516, 281, 483, 257, 3840, 295, 4122, 13, 1779, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10999772548675538, "compression_ratio": 1.9315068493150684, "no_speech_prob": 9.57068778006942e-07}, {"id": 259, "seek": 115200, "start": 1157.0, "end": 1163.0, "text": " And then again, this is the data loader. I can just use the data loaders directly. Right.", "tokens": [50364, 407, 286, 478, 516, 281, 360, 300, 13, 400, 550, 286, 478, 516, 281, 445, 44497, 807, 452, 11, 291, 458, 11, 286, 478, 516, 281, 992, 364, 30992, 339, 510, 13, 50614, 50614, 400, 550, 797, 11, 341, 307, 264, 1412, 3677, 260, 13, 286, 393, 445, 764, 264, 1412, 3677, 433, 3838, 13, 1779, 13, 50914, 50914, 639, 307, 291, 393, 764, 552, 294, 9953, 51, 284, 339, 13, 407, 286, 478, 445, 516, 281, 2235, 264, 3847, 1412, 3677, 260, 484, 293, 550, 44497, 807, 510, 13, 51164, 51164, 400, 550, 286, 528, 281, 1190, 264, 4846, 807, 264, 34889, 13, 286, 478, 516, 281, 483, 257, 3840, 295, 4122, 13, 1779, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10999772548675538, "compression_ratio": 1.9315068493150684, "no_speech_prob": 9.57068778006942e-07}, {"id": 260, "seek": 115200, "start": 1163.0, "end": 1168.0, "text": " This is you can use them in PyTorch. So I'm just going to pull the train data loader out and then iterate through here.", "tokens": [50364, 407, 286, 478, 516, 281, 360, 300, 13, 400, 550, 286, 478, 516, 281, 445, 44497, 807, 452, 11, 291, 458, 11, 286, 478, 516, 281, 992, 364, 30992, 339, 510, 13, 50614, 50614, 400, 550, 797, 11, 341, 307, 264, 1412, 3677, 260, 13, 286, 393, 445, 764, 264, 1412, 3677, 433, 3838, 13, 1779, 13, 50914, 50914, 639, 307, 291, 393, 764, 552, 294, 9953, 51, 284, 339, 13, 407, 286, 478, 445, 516, 281, 2235, 264, 3847, 1412, 3677, 260, 484, 293, 550, 44497, 807, 510, 13, 51164, 51164, 400, 550, 286, 528, 281, 1190, 264, 4846, 807, 264, 34889, 13, 286, 478, 516, 281, 483, 257, 3840, 295, 4122, 13, 1779, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10999772548675538, "compression_ratio": 1.9315068493150684, "no_speech_prob": 9.57068778006942e-07}, {"id": 261, "seek": 115200, "start": 1168.0, "end": 1174.0, "text": " And then I want to run the input through the backbone. I'm going to get a bunch of features. Right.", "tokens": [50364, 407, 286, 478, 516, 281, 360, 300, 13, 400, 550, 286, 478, 516, 281, 445, 44497, 807, 452, 11, 291, 458, 11, 286, 478, 516, 281, 992, 364, 30992, 339, 510, 13, 50614, 50614, 400, 550, 797, 11, 341, 307, 264, 1412, 3677, 260, 13, 286, 393, 445, 764, 264, 1412, 3677, 433, 3838, 13, 1779, 13, 50914, 50914, 639, 307, 291, 393, 764, 552, 294, 9953, 51, 284, 339, 13, 407, 286, 478, 445, 516, 281, 2235, 264, 3847, 1412, 3677, 260, 484, 293, 550, 44497, 807, 510, 13, 51164, 51164, 400, 550, 286, 528, 281, 1190, 264, 4846, 807, 264, 34889, 13, 286, 478, 516, 281, 483, 257, 3840, 295, 4122, 13, 1779, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.10999772548675538, "compression_ratio": 1.9315068493150684, "no_speech_prob": 9.57068778006942e-07}, {"id": 262, "seek": 117400, "start": 1174.0, "end": 1184.0, "text": " So I'm going from batch by three by thirty two by thirty two. So channels, heights and width pixels to batch by one thousand, which is the number of classes on image.", "tokens": [50364, 407, 286, 478, 516, 490, 15245, 538, 1045, 538, 11790, 732, 538, 11790, 732, 13, 407, 9235, 11, 25930, 293, 11402, 18668, 281, 15245, 538, 472, 4714, 11, 597, 307, 264, 1230, 295, 5359, 322, 3256, 13, 50864, 50864, 583, 321, 434, 516, 281, 2387, 341, 382, 12240, 3584, 10139, 510, 13, 51064, 51064, 400, 11, 291, 458, 11, 286, 500, 380, 362, 281, 360, 341, 570, 452, 5028, 6545, 307, 787, 1237, 412, 264, 15395, 281, 4583, 11, 457, 286, 478, 516, 281, 43245, 510, 13, 1779, 13, 51414, 51414, 407, 286, 727, 445, 5085, 309, 510, 13, 407, 309, 1177, 380, 1871, 13, 286, 478, 516, 281, 286, 478, 516, 281, 2871, 341, 484, 337, 586, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.137427865005121, "compression_ratio": 1.7360594795539033, "no_speech_prob": 2.8129520615038928e-06}, {"id": 263, "seek": 117400, "start": 1184.0, "end": 1188.0, "text": " But we're going to treat this as embedding dimension here.", "tokens": [50364, 407, 286, 478, 516, 490, 15245, 538, 1045, 538, 11790, 732, 538, 11790, 732, 13, 407, 9235, 11, 25930, 293, 11402, 18668, 281, 15245, 538, 472, 4714, 11, 597, 307, 264, 1230, 295, 5359, 322, 3256, 13, 50864, 50864, 583, 321, 434, 516, 281, 2387, 341, 382, 12240, 3584, 10139, 510, 13, 51064, 51064, 400, 11, 291, 458, 11, 286, 500, 380, 362, 281, 360, 341, 570, 452, 5028, 6545, 307, 787, 1237, 412, 264, 15395, 281, 4583, 11, 457, 286, 478, 516, 281, 43245, 510, 13, 1779, 13, 51414, 51414, 407, 286, 727, 445, 5085, 309, 510, 13, 407, 309, 1177, 380, 1871, 13, 286, 478, 516, 281, 286, 478, 516, 281, 2871, 341, 484, 337, 586, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.137427865005121, "compression_ratio": 1.7360594795539033, "no_speech_prob": 2.8129520615038928e-06}, {"id": 264, "seek": 117400, "start": 1188.0, "end": 1195.0, "text": " And, you know, I don't have to do this because my optimizer is only looking at the spine to layer, but I'm going to detach here. Right.", "tokens": [50364, 407, 286, 478, 516, 490, 15245, 538, 1045, 538, 11790, 732, 538, 11790, 732, 13, 407, 9235, 11, 25930, 293, 11402, 18668, 281, 15245, 538, 472, 4714, 11, 597, 307, 264, 1230, 295, 5359, 322, 3256, 13, 50864, 50864, 583, 321, 434, 516, 281, 2387, 341, 382, 12240, 3584, 10139, 510, 13, 51064, 51064, 400, 11, 291, 458, 11, 286, 500, 380, 362, 281, 360, 341, 570, 452, 5028, 6545, 307, 787, 1237, 412, 264, 15395, 281, 4583, 11, 457, 286, 478, 516, 281, 43245, 510, 13, 1779, 13, 51414, 51414, 407, 286, 727, 445, 5085, 309, 510, 13, 407, 309, 1177, 380, 1871, 13, 286, 478, 516, 281, 286, 478, 516, 281, 2871, 341, 484, 337, 586, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.137427865005121, "compression_ratio": 1.7360594795539033, "no_speech_prob": 2.8129520615038928e-06}, {"id": 265, "seek": 117400, "start": 1195.0, "end": 1200.0, "text": " So I could just attach it here. So it doesn't matter. I'm going to I'm going to comment this out for now.", "tokens": [50364, 407, 286, 478, 516, 490, 15245, 538, 1045, 538, 11790, 732, 538, 11790, 732, 13, 407, 9235, 11, 25930, 293, 11402, 18668, 281, 15245, 538, 472, 4714, 11, 597, 307, 264, 1230, 295, 5359, 322, 3256, 13, 50864, 50864, 583, 321, 434, 516, 281, 2387, 341, 382, 12240, 3584, 10139, 510, 13, 51064, 51064, 400, 11, 291, 458, 11, 286, 500, 380, 362, 281, 360, 341, 570, 452, 5028, 6545, 307, 787, 1237, 412, 264, 15395, 281, 4583, 11, 457, 286, 478, 516, 281, 43245, 510, 13, 1779, 13, 51414, 51414, 407, 286, 727, 445, 5085, 309, 510, 13, 407, 309, 1177, 380, 1871, 13, 286, 478, 516, 281, 286, 478, 516, 281, 2871, 341, 484, 337, 586, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.137427865005121, "compression_ratio": 1.7360594795539033, "no_speech_prob": 2.8129520615038928e-06}, {"id": 266, "seek": 120000, "start": 1200.0, "end": 1207.0, "text": " But I just want to show that, like, if you were training both at the same time, you could just attach the features and pass them into the classifier.", "tokens": [50364, 583, 286, 445, 528, 281, 855, 300, 11, 411, 11, 498, 291, 645, 3097, 1293, 412, 264, 912, 565, 11, 291, 727, 445, 5085, 264, 4122, 293, 1320, 552, 666, 264, 1508, 9902, 13, 50714, 50714, 865, 13, 583, 767, 11, 286, 576, 2748, 291, 281, 360, 411, 498, 291, 352, 257, 1916, 295, 3876, 3673, 11, 498, 291, 2052, 833, 264, 1783, 2507, 264, 1783, 11, 1783, 11, 398, 2681, 15245, 11, 264, 445, 2507, 13, 51364, 51364, 865, 13, 865, 13, 1485, 1622, 2507, 13, 2264, 13, 821, 321, 393, 472, 1622, 13, 865, 13, 492, 393, 2464, 365, 27822, 5893, 572, 2771, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18633662137118254, "compression_ratio": 1.6544715447154472, "no_speech_prob": 4.831266778637655e-05}, {"id": 267, "seek": 120000, "start": 1207.0, "end": 1220.0, "text": " Yeah. But actually, I would recommend you to do like if you go a couple of lines above, if you click under the X below the X, X, Y equal batch, the just below.", "tokens": [50364, 583, 286, 445, 528, 281, 855, 300, 11, 411, 11, 498, 291, 645, 3097, 1293, 412, 264, 912, 565, 11, 291, 727, 445, 5085, 264, 4122, 293, 1320, 552, 666, 264, 1508, 9902, 13, 50714, 50714, 865, 13, 583, 767, 11, 286, 576, 2748, 291, 281, 360, 411, 498, 291, 352, 257, 1916, 295, 3876, 3673, 11, 498, 291, 2052, 833, 264, 1783, 2507, 264, 1783, 11, 1783, 11, 398, 2681, 15245, 11, 264, 445, 2507, 13, 51364, 51364, 865, 13, 865, 13, 1485, 1622, 2507, 13, 2264, 13, 821, 321, 393, 472, 1622, 13, 865, 13, 492, 393, 2464, 365, 27822, 5893, 572, 2771, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18633662137118254, "compression_ratio": 1.6544715447154472, "no_speech_prob": 4.831266778637655e-05}, {"id": 268, "seek": 120000, "start": 1220.0, "end": 1227.0, "text": " Yeah. Yeah. One line below. OK. There we can one line. Yeah. We can write with torch dot no grad.", "tokens": [50364, 583, 286, 445, 528, 281, 855, 300, 11, 411, 11, 498, 291, 645, 3097, 1293, 412, 264, 912, 565, 11, 291, 727, 445, 5085, 264, 4122, 293, 1320, 552, 666, 264, 1508, 9902, 13, 50714, 50714, 865, 13, 583, 767, 11, 286, 576, 2748, 291, 281, 360, 411, 498, 291, 352, 257, 1916, 295, 3876, 3673, 11, 498, 291, 2052, 833, 264, 1783, 2507, 264, 1783, 11, 1783, 11, 398, 2681, 15245, 11, 264, 445, 2507, 13, 51364, 51364, 865, 13, 865, 13, 1485, 1622, 2507, 13, 2264, 13, 821, 321, 393, 472, 1622, 13, 865, 13, 492, 393, 2464, 365, 27822, 5893, 572, 2771, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.18633662137118254, "compression_ratio": 1.6544715447154472, "no_speech_prob": 4.831266778637655e-05}, {"id": 269, "seek": 122700, "start": 1227.0, "end": 1233.0, "text": " Oh, sure. So I would prefer this actually. No underscore grad parenthesis and colon. Yeah.", "tokens": [50364, 876, 11, 988, 13, 407, 286, 576, 4382, 341, 767, 13, 883, 37556, 2771, 23350, 9374, 293, 8255, 13, 865, 13, 50664, 50664, 400, 550, 321, 44494, 300, 4111, 13, 407, 341, 307, 1936, 406, 3585, 9953, 51, 284, 339, 406, 281, 2837, 264, 28270, 24877, 13, 1779, 13, 51114, 51114, 407, 309, 311, 516, 281, 312, 709, 4663, 13, 400, 611, 11, 1797, 322, 13, 286, 519, 309, 311, 516, 281, 312, 406, 988, 4663, 11, 457, 309, 311, 516, 281, 312, 406, 1940, 604, 4497, 4675, 570, 321, 366, 406, 516, 281, 312, 884, 646, 38377, 807, 264, 34889, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16454146043309625, "compression_ratio": 1.7290836653386454, "no_speech_prob": 5.5585664085811004e-05}, {"id": 270, "seek": 122700, "start": 1233.0, "end": 1242.0, "text": " And then we indent that feature. So this is basically not telling PyTorch not to track the computational graphs. Right.", "tokens": [50364, 876, 11, 988, 13, 407, 286, 576, 4382, 341, 767, 13, 883, 37556, 2771, 23350, 9374, 293, 8255, 13, 865, 13, 50664, 50664, 400, 550, 321, 44494, 300, 4111, 13, 407, 341, 307, 1936, 406, 3585, 9953, 51, 284, 339, 406, 281, 2837, 264, 28270, 24877, 13, 1779, 13, 51114, 51114, 407, 309, 311, 516, 281, 312, 709, 4663, 13, 400, 611, 11, 1797, 322, 13, 286, 519, 309, 311, 516, 281, 312, 406, 988, 4663, 11, 457, 309, 311, 516, 281, 312, 406, 1940, 604, 4497, 4675, 570, 321, 366, 406, 516, 281, 312, 884, 646, 38377, 807, 264, 34889, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16454146043309625, "compression_ratio": 1.7290836653386454, "no_speech_prob": 5.5585664085811004e-05}, {"id": 271, "seek": 122700, "start": 1242.0, "end": 1256.0, "text": " So it's going to be much faster. And also, hold on. I think it's going to be not sure faster, but it's going to be not taking any additional memory because we are not going to be doing back propagation through the backbone.", "tokens": [50364, 876, 11, 988, 13, 407, 286, 576, 4382, 341, 767, 13, 883, 37556, 2771, 23350, 9374, 293, 8255, 13, 865, 13, 50664, 50664, 400, 550, 321, 44494, 300, 4111, 13, 407, 341, 307, 1936, 406, 3585, 9953, 51, 284, 339, 406, 281, 2837, 264, 28270, 24877, 13, 1779, 13, 51114, 51114, 407, 309, 311, 516, 281, 312, 709, 4663, 13, 400, 611, 11, 1797, 322, 13, 286, 519, 309, 311, 516, 281, 312, 406, 988, 4663, 11, 457, 309, 311, 516, 281, 312, 406, 1940, 604, 4497, 4675, 570, 321, 366, 406, 516, 281, 312, 884, 646, 38377, 807, 264, 34889, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.16454146043309625, "compression_ratio": 1.7290836653386454, "no_speech_prob": 5.5585664085811004e-05}, {"id": 272, "seek": 125600, "start": 1256.0, "end": 1261.0, "text": " Right. So this is usually this is the best practice, I would say. Right. Yeah. Makes sense.", "tokens": [50364, 1779, 13, 407, 341, 307, 2673, 341, 307, 264, 1151, 3124, 11, 286, 576, 584, 13, 1779, 13, 865, 13, 25245, 2020, 13, 50614, 50614, 286, 458, 797, 11, 457, 286, 478, 787, 40425, 341, 2146, 13, 1779, 13, 865, 13, 583, 1338, 11, 281, 428, 935, 11, 291, 1582, 380, 1066, 257, 4295, 562, 291, 360, 341, 11, 597, 307, 2138, 544, 7148, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.14171785548113394, "compression_ratio": 1.4350282485875707, "no_speech_prob": 1.4509671018458903e-05}, {"id": 273, "seek": 125600, "start": 1261.0, "end": 1271.0, "text": " I know again, but I'm only optimizing this guy. Right. Yeah. But yeah, to your point, you won't keep a graph when you do this, which is definitely more efficient.", "tokens": [50364, 1779, 13, 407, 341, 307, 2673, 341, 307, 264, 1151, 3124, 11, 286, 576, 584, 13, 1779, 13, 865, 13, 25245, 2020, 13, 50614, 50614, 286, 458, 797, 11, 457, 286, 478, 787, 40425, 341, 2146, 13, 1779, 13, 865, 13, 583, 1338, 11, 281, 428, 935, 11, 291, 1582, 380, 1066, 257, 4295, 562, 291, 360, 341, 11, 597, 307, 2138, 544, 7148, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.14171785548113394, "compression_ratio": 1.4350282485875707, "no_speech_prob": 1.4509671018458903e-05}, {"id": 274, "seek": 127100, "start": 1271.0, "end": 1288.0, "text": " OK. So then where's my screen? OK. So we have the features and then we're going to run them through our fine tune layer. Right. So this guy here. So again, this fine tune layer could be like an SVM if I wanted to do. Right.", "tokens": [50364, 2264, 13, 407, 550, 689, 311, 452, 2568, 30, 2264, 13, 407, 321, 362, 264, 4122, 293, 550, 321, 434, 516, 281, 1190, 552, 807, 527, 2489, 10864, 4583, 13, 1779, 13, 407, 341, 2146, 510, 13, 407, 797, 11, 341, 2489, 10864, 4583, 727, 312, 411, 364, 31910, 44, 498, 286, 1415, 281, 360, 13, 1779, 13, 51214, 51214, 467, 1177, 380, 1871, 13, 583, 510, 286, 478, 1228, 341, 8213, 4583, 300, 286, 2942, 493, 510, 13, 2264, 13, 286, 478, 516, 281, 1190, 309, 807, 264, 2489, 10864, 4583, 13, 467, 311, 516, 281, 976, 385, 264, 21264, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13192556489188717, "compression_ratio": 1.7400881057268722, "no_speech_prob": 1.8630727936397307e-05}, {"id": 275, "seek": 127100, "start": 1288.0, "end": 1297.0, "text": " It doesn't matter. But here I'm using this linear layer that I created up here. OK. I'm going to run it through the fine tune layer. It's going to give me the predictions.", "tokens": [50364, 2264, 13, 407, 550, 689, 311, 452, 2568, 30, 2264, 13, 407, 321, 362, 264, 4122, 293, 550, 321, 434, 516, 281, 1190, 552, 807, 527, 2489, 10864, 4583, 13, 1779, 13, 407, 341, 2146, 510, 13, 407, 797, 11, 341, 2489, 10864, 4583, 727, 312, 411, 364, 31910, 44, 498, 286, 1415, 281, 360, 13, 1779, 13, 51214, 51214, 467, 1177, 380, 1871, 13, 583, 510, 286, 478, 1228, 341, 8213, 4583, 300, 286, 2942, 493, 510, 13, 2264, 13, 286, 478, 516, 281, 1190, 309, 807, 264, 2489, 10864, 4583, 13, 467, 311, 516, 281, 976, 385, 264, 21264, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.13192556489188717, "compression_ratio": 1.7400881057268722, "no_speech_prob": 1.8630727936397307e-05}, {"id": 276, "seek": 129700, "start": 1297.0, "end": 1307.0, "text": " I guess it'd have to be differentiable in this case, but that's OK. And then I'm going to calculate the loss. Right. So cross entropy. So I'm going to pass in the predictions and then the labels.", "tokens": [50364, 286, 2041, 309, 1116, 362, 281, 312, 819, 9364, 294, 341, 1389, 11, 457, 300, 311, 2264, 13, 400, 550, 286, 478, 516, 281, 8873, 264, 4470, 13, 1779, 13, 407, 3278, 30867, 13, 407, 286, 478, 516, 281, 1320, 294, 264, 21264, 293, 550, 264, 16949, 13, 50864, 50864, 1119, 264, 1901, 5361, 30, 6962, 322, 13, 865, 13, 51114, 51114, 286, 411, 300, 321, 1293, 10864, 294, 4258, 13, 407, 291, 434, 411, 11, 983, 307, 300, 766, 30, 400, 550, 11, 291, 458, 11, 264, 3832, 23897, 5028, 6545, 1507, 11, 19618, 1507, 13, 1779, 13, 407, 321, 603, 360, 300, 13, 8561, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.19336428942981068, "compression_ratio": 1.6313725490196078, "no_speech_prob": 3.372399805812165e-05}, {"id": 277, "seek": 129700, "start": 1307.0, "end": 1312.0, "text": " Is the space missing? Hold on. Yeah.", "tokens": [50364, 286, 2041, 309, 1116, 362, 281, 312, 819, 9364, 294, 341, 1389, 11, 457, 300, 311, 2264, 13, 400, 550, 286, 478, 516, 281, 8873, 264, 4470, 13, 1779, 13, 407, 3278, 30867, 13, 407, 286, 478, 516, 281, 1320, 294, 264, 21264, 293, 550, 264, 16949, 13, 50864, 50864, 1119, 264, 1901, 5361, 30, 6962, 322, 13, 865, 13, 51114, 51114, 286, 411, 300, 321, 1293, 10864, 294, 4258, 13, 407, 291, 434, 411, 11, 983, 307, 300, 766, 30, 400, 550, 11, 291, 458, 11, 264, 3832, 23897, 5028, 6545, 1507, 11, 19618, 1507, 13, 1779, 13, 407, 321, 603, 360, 300, 13, 8561, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.19336428942981068, "compression_ratio": 1.6313725490196078, "no_speech_prob": 3.372399805812165e-05}, {"id": 278, "seek": 129700, "start": 1312.0, "end": 1325.0, "text": " I like that we both tune in immediately. So you're like, why is that off? And then, you know, the standard backward optimizer stuff, optimization stuff. Right. So we'll do that. Cool.", "tokens": [50364, 286, 2041, 309, 1116, 362, 281, 312, 819, 9364, 294, 341, 1389, 11, 457, 300, 311, 2264, 13, 400, 550, 286, 478, 516, 281, 8873, 264, 4470, 13, 1779, 13, 407, 3278, 30867, 13, 407, 286, 478, 516, 281, 1320, 294, 264, 21264, 293, 550, 264, 16949, 13, 50864, 50864, 1119, 264, 1901, 5361, 30, 6962, 322, 13, 865, 13, 51114, 51114, 286, 411, 300, 321, 1293, 10864, 294, 4258, 13, 407, 291, 434, 411, 11, 983, 307, 300, 766, 30, 400, 550, 11, 291, 458, 11, 264, 3832, 23897, 5028, 6545, 1507, 11, 19618, 1507, 13, 1779, 13, 407, 321, 603, 360, 300, 13, 8561, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.19336428942981068, "compression_ratio": 1.6313725490196078, "no_speech_prob": 3.372399805812165e-05}, {"id": 279, "seek": 132500, "start": 1325.0, "end": 1338.0, "text": " And then, you know, I'm just going to print the loss just so that we know where we're at. OK. So I'm going to run this guy and hopefully you'll start seeing the loss coming up.", "tokens": [50364, 400, 550, 11, 291, 458, 11, 286, 478, 445, 516, 281, 4482, 264, 4470, 445, 370, 300, 321, 458, 689, 321, 434, 412, 13, 2264, 13, 407, 286, 478, 516, 281, 1190, 341, 2146, 293, 4696, 291, 603, 722, 2577, 264, 4470, 1348, 493, 13, 51014, 51014, 961, 311, 536, 13, 51264, 51264, 821, 309, 307, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.09702572275380619, "compression_ratio": 1.3986013986013985, "no_speech_prob": 5.771676569565898e-06}, {"id": 280, "seek": 132500, "start": 1338.0, "end": 1343.0, "text": " Let's see.", "tokens": [50364, 400, 550, 11, 291, 458, 11, 286, 478, 445, 516, 281, 4482, 264, 4470, 445, 370, 300, 321, 458, 689, 321, 434, 412, 13, 2264, 13, 407, 286, 478, 516, 281, 1190, 341, 2146, 293, 4696, 291, 603, 722, 2577, 264, 4470, 1348, 493, 13, 51014, 51014, 961, 311, 536, 13, 51264, 51264, 821, 309, 307, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.09702572275380619, "compression_ratio": 1.3986013986013985, "no_speech_prob": 5.771676569565898e-06}, {"id": 281, "seek": 132500, "start": 1343.0, "end": 1345.0, "text": " There it is.", "tokens": [50364, 400, 550, 11, 291, 458, 11, 286, 478, 445, 516, 281, 4482, 264, 4470, 445, 370, 300, 321, 458, 689, 321, 434, 412, 13, 2264, 13, 407, 286, 478, 516, 281, 1190, 341, 2146, 293, 4696, 291, 603, 722, 2577, 264, 4470, 1348, 493, 13, 51014, 51014, 961, 311, 536, 13, 51264, 51264, 821, 309, 307, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.09702572275380619, "compression_ratio": 1.3986013986013985, "no_speech_prob": 5.771676569565898e-06}, {"id": 282, "seek": 134500, "start": 1345.0, "end": 1359.0, "text": " All right. It's something's happening. The magic is happening. OK. So I'm going to stop this because it's super slow. We're using a GPU machine. So I kind of want to use this GPU. So I'm going to I'm going to just convert this into lightning real quick.", "tokens": [50364, 1057, 558, 13, 467, 311, 746, 311, 2737, 13, 440, 5585, 307, 2737, 13, 2264, 13, 407, 286, 478, 516, 281, 1590, 341, 570, 309, 311, 1687, 2964, 13, 492, 434, 1228, 257, 18407, 3479, 13, 407, 286, 733, 295, 528, 281, 764, 341, 18407, 13, 407, 286, 478, 516, 281, 286, 478, 516, 281, 445, 7620, 341, 666, 16589, 957, 1702, 13, 51064, 51064, 400, 300, 1399, 307, 516, 281, 312, 1687, 2370, 570, 286, 478, 445, 17608, 437, 286, 630, 13, 1779, 13, 407, 286, 362, 439, 264, 912, 1507, 456, 13, 407, 264, 700, 551, 286, 478, 516, 281, 360, 307, 445, 1884, 341, 16589, 10088, 13, 1779, 13, 407, 341, 1508, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11440899746477111, "compression_ratio": 1.8143939393939394, "no_speech_prob": 3.393032784515526e-06}, {"id": 283, "seek": 134500, "start": 1359.0, "end": 1374.0, "text": " And that process is going to be super fast because I'm just organizing what I did. Right. So I have all the same stuff there. So the first thing I'm going to do is just create this lightning module. Right. So this class here.", "tokens": [50364, 1057, 558, 13, 467, 311, 746, 311, 2737, 13, 440, 5585, 307, 2737, 13, 2264, 13, 407, 286, 478, 516, 281, 1590, 341, 570, 309, 311, 1687, 2964, 13, 492, 434, 1228, 257, 18407, 3479, 13, 407, 286, 733, 295, 528, 281, 764, 341, 18407, 13, 407, 286, 478, 516, 281, 286, 478, 516, 281, 445, 7620, 341, 666, 16589, 957, 1702, 13, 51064, 51064, 400, 300, 1399, 307, 516, 281, 312, 1687, 2370, 570, 286, 478, 445, 17608, 437, 286, 630, 13, 1779, 13, 407, 286, 362, 439, 264, 912, 1507, 456, 13, 407, 264, 700, 551, 286, 478, 516, 281, 360, 307, 445, 1884, 341, 16589, 10088, 13, 1779, 13, 407, 341, 1508, 510, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.11440899746477111, "compression_ratio": 1.8143939393939394, "no_speech_prob": 3.393032784515526e-06}, {"id": 284, "seek": 137400, "start": 1374.0, "end": 1383.0, "text": " And then PL we imported at the top. Right. So it's just lightning. Did we import it? No, we didn't import. We just installed it. Yeah. So let me just import it.", "tokens": [50364, 400, 550, 6999, 321, 25524, 412, 264, 1192, 13, 1779, 13, 407, 309, 311, 445, 16589, 13, 2589, 321, 974, 309, 30, 883, 11, 321, 994, 380, 974, 13, 492, 445, 8899, 309, 13, 865, 13, 407, 718, 385, 445, 974, 309, 13, 50814, 50814], "temperature": 0.0, "avg_logprob": -0.2685547686637716, "compression_ratio": 1.3559322033898304, "no_speech_prob": 3.169118281221017e-05}, {"id": 285, "seek": 138300, "start": 1383.0, "end": 1405.0, "text": " Import. I torch lightning as PL. Great. So this is the same basically as an NNM module. Right. But it's just lightning. So and then I'm going to change things a bit. Right. So I want to kind of I need to call this first. Oh, sorry. So I need to write our unit function.", "tokens": [50364, 26391, 13, 286, 27822, 16589, 382, 6999, 13, 3769, 13, 407, 341, 307, 264, 912, 1936, 382, 364, 426, 45, 44, 10088, 13, 1779, 13, 583, 309, 311, 445, 16589, 13, 407, 293, 550, 286, 478, 516, 281, 1319, 721, 257, 857, 13, 1779, 13, 407, 286, 528, 281, 733, 295, 286, 643, 281, 818, 341, 700, 13, 876, 11, 2597, 13, 407, 286, 643, 281, 2464, 527, 4985, 2445, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.24291849772135415, "compression_ratio": 1.4861878453038675, "no_speech_prob": 8.139295459841378e-06}, {"id": 286, "seek": 140500, "start": 1405.0, "end": 1427.0, "text": " I write this so much nowadays. Okay. So and then I'm going to in its super class. Great. And then I want to use a backbone. Right. So I'm just going to bring that guy in. What was it? Here.", "tokens": [50364, 286, 2464, 341, 370, 709, 13434, 13, 1033, 13, 407, 293, 550, 286, 478, 516, 281, 294, 1080, 1687, 1508, 13, 3769, 13, 400, 550, 286, 528, 281, 764, 257, 34889, 13, 1779, 13, 407, 286, 478, 445, 516, 281, 1565, 300, 2146, 294, 13, 708, 390, 309, 30, 1692, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.15605654283003373, "compression_ratio": 1.3404255319148937, "no_speech_prob": 1.241077552549541e-05}, {"id": 287, "seek": 142700, "start": 1427.0, "end": 1449.0, "text": " So I'm just going to define it in the model. And then I want to also use this linear classifier guy. Here. Okay. So here's our fine-tune layer. Great. Same thing. Perfect. Okay.", "tokens": [50364, 407, 286, 478, 445, 516, 281, 6964, 309, 294, 264, 2316, 13, 400, 550, 286, 528, 281, 611, 764, 341, 8213, 1508, 9902, 2146, 13, 1692, 13, 1033, 13, 407, 510, 311, 527, 2489, 12, 83, 2613, 4583, 13, 3769, 13, 10635, 551, 13, 10246, 13, 1033, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.2133809786576491, "compression_ratio": 1.2919708029197081, "no_speech_prob": 5.422102731245104e-06}, {"id": 288, "seek": 144900, "start": 1449.0, "end": 1466.0, "text": " And then I want to parameterize this a bit. Right. So this 10, I can make this more general. This is an image classifier. So I'll just say num classes. Right. And we'll default to 10 because it's going to be for CIFAR 10, I'll say. But I can just do that.", "tokens": [50364, 400, 550, 286, 528, 281, 13075, 1125, 341, 257, 857, 13, 1779, 13, 407, 341, 1266, 11, 286, 393, 652, 341, 544, 2674, 13, 639, 307, 364, 3256, 1508, 9902, 13, 407, 286, 603, 445, 584, 1031, 5359, 13, 1779, 13, 400, 321, 603, 7576, 281, 1266, 570, 309, 311, 516, 281, 312, 337, 383, 12775, 1899, 1266, 11, 286, 603, 584, 13, 583, 286, 393, 445, 360, 300, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.1522324278547957, "compression_ratio": 1.4739884393063585, "no_speech_prob": 4.860272838413948e-06}, {"id": 289, "seek": 146600, "start": 1466.0, "end": 1479.0, "text": " And then so that's it. And then now the training stuff. So this is where this is what's going to abstract the way the training loop so that you know we don't have to deal with all this boilerplate code.", "tokens": [50364, 400, 550, 370, 300, 311, 309, 13, 400, 550, 586, 264, 3097, 1507, 13, 407, 341, 307, 689, 341, 307, 437, 311, 516, 281, 12649, 264, 636, 264, 3097, 6367, 370, 300, 291, 458, 321, 500, 380, 362, 281, 2028, 365, 439, 341, 39228, 37008, 3089, 13, 51014, 51014, 407, 341, 3097, 1823, 307, 411, 264, 2128, 294, 257, 4363, 13, 708, 307, 309, 30, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.16467028078825577, "compression_ratio": 1.6503067484662577, "no_speech_prob": 1.3006459084863309e-05}, {"id": 290, "seek": 146600, "start": 1479.0, "end": 1483.0, "text": " So this training step is like the forward in a modern. What is it?", "tokens": [50364, 400, 550, 370, 300, 311, 309, 13, 400, 550, 586, 264, 3097, 1507, 13, 407, 341, 307, 689, 341, 307, 437, 311, 516, 281, 12649, 264, 636, 264, 3097, 6367, 370, 300, 291, 458, 321, 500, 380, 362, 281, 2028, 365, 439, 341, 39228, 37008, 3089, 13, 51014, 51014, 407, 341, 3097, 1823, 307, 411, 264, 2128, 294, 257, 4363, 13, 708, 307, 309, 30, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.16467028078825577, "compression_ratio": 1.6503067484662577, "no_speech_prob": 1.3006459084863309e-05}, {"id": 291, "seek": 148300, "start": 1483.0, "end": 1504.0, "text": " Yeah, it's kind of like a forward, but instead of just doing kind of like a forward passive computation. So let's see define all the interactions of the model, the loss and everything else. Right. So it's capturing like a full system. So if you were to do BERT or GAN or VAE or something like that, all of that logic would happen in training stuff. So it's easier to understand and keep together.", "tokens": [50364, 865, 11, 309, 311, 733, 295, 411, 257, 2128, 11, 457, 2602, 295, 445, 884, 733, 295, 411, 257, 2128, 14975, 24903, 13, 407, 718, 311, 536, 6964, 439, 264, 13280, 295, 264, 2316, 11, 264, 4470, 293, 1203, 1646, 13, 1779, 13, 407, 309, 311, 23384, 411, 257, 1577, 1185, 13, 407, 498, 291, 645, 281, 360, 363, 31479, 420, 460, 1770, 420, 18527, 36, 420, 746, 411, 300, 11, 439, 295, 300, 9952, 576, 1051, 294, 3097, 1507, 13, 407, 309, 311, 3571, 281, 1223, 293, 1066, 1214, 13, 51414, 51414, 407, 437, 775, 341, 3170, 575, 281, 2736, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13256197155646557, "compression_ratio": 1.6515151515151516, "no_speech_prob": 1.0288806151947938e-05}, {"id": 292, "seek": 148300, "start": 1504.0, "end": 1508.0, "text": " So what does this method has to return?", "tokens": [50364, 865, 11, 309, 311, 733, 295, 411, 257, 2128, 11, 457, 2602, 295, 445, 884, 733, 295, 411, 257, 2128, 14975, 24903, 13, 407, 718, 311, 536, 6964, 439, 264, 13280, 295, 264, 2316, 11, 264, 4470, 293, 1203, 1646, 13, 1779, 13, 407, 309, 311, 23384, 411, 257, 1577, 1185, 13, 407, 498, 291, 645, 281, 360, 363, 31479, 420, 460, 1770, 420, 18527, 36, 420, 746, 411, 300, 11, 439, 295, 300, 9952, 576, 1051, 294, 3097, 1507, 13, 407, 309, 311, 3571, 281, 1223, 293, 1066, 1214, 13, 51414, 51414, 407, 437, 775, 341, 3170, 575, 281, 2736, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.13256197155646557, "compression_ratio": 1.6515151515151516, "no_speech_prob": 1.0288806151947938e-05}, {"id": 293, "seek": 150800, "start": 1508.0, "end": 1525.0, "text": " So you have to return a loss. Right. Oh, okay. Yeah. And I'll show that a minute, but it has to have a computational graph attached to it so it can do the optimization. So where do we have all that? It's literally just all of this stuff here. Right. So I'm just going to copy.", "tokens": [50364, 407, 291, 362, 281, 2736, 257, 4470, 13, 1779, 13, 876, 11, 1392, 13, 865, 13, 400, 286, 603, 855, 300, 257, 3456, 11, 457, 309, 575, 281, 362, 257, 28270, 4295, 8570, 281, 309, 370, 309, 393, 360, 264, 19618, 13, 407, 689, 360, 321, 362, 439, 300, 30, 467, 311, 3736, 445, 439, 295, 341, 1507, 510, 13, 1779, 13, 407, 286, 478, 445, 516, 281, 5055, 13, 51214, 51214, 407, 11, 1392, 13, 407, 428, 3097, 6367, 300, 321, 4114, 949, 294, 257, 294, 257, 294, 257, 9953, 51, 284, 339, 11, 309, 311, 309, 311, 352, 309, 1709, 1854, 264, 2316, 586, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.2167475245020411, "compression_ratio": 1.596774193548387, "no_speech_prob": 2.014434357988648e-05}, {"id": 294, "seek": 150800, "start": 1525.0, "end": 1533.0, "text": " So, okay. So your training loop that we wrote before in a in a in a PyTorch, it's it's go it goes inside the model now.", "tokens": [50364, 407, 291, 362, 281, 2736, 257, 4470, 13, 1779, 13, 876, 11, 1392, 13, 865, 13, 400, 286, 603, 855, 300, 257, 3456, 11, 457, 309, 575, 281, 362, 257, 28270, 4295, 8570, 281, 309, 370, 309, 393, 360, 264, 19618, 13, 407, 689, 360, 321, 362, 439, 300, 30, 467, 311, 3736, 445, 439, 295, 341, 1507, 510, 13, 1779, 13, 407, 286, 478, 445, 516, 281, 5055, 13, 51214, 51214, 407, 11, 1392, 13, 407, 428, 3097, 6367, 300, 321, 4114, 949, 294, 257, 294, 257, 294, 257, 9953, 51, 284, 339, 11, 309, 311, 309, 311, 352, 309, 1709, 1854, 264, 2316, 586, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.2167475245020411, "compression_ratio": 1.596774193548387, "no_speech_prob": 2.014434357988648e-05}, {"id": 295, "seek": 153300, "start": 1533.0, "end": 1553.0, "text": " Yeah, exactly. It's a little bit interesting. But what it does is it makes it so that your model self contain. Right. So in this version, you not reusable. Like I can't reuse this code for like a different task tomorrow. It's like very specific to this thing. So I'm just pulling out the relevant stuff, which is what you know, what we're going to spend 99% of our time on is modifying these.", "tokens": [50364, 865, 11, 2293, 13, 467, 311, 257, 707, 857, 1880, 13, 583, 437, 309, 775, 307, 309, 1669, 309, 370, 300, 428, 2316, 2698, 5304, 13, 1779, 13, 407, 294, 341, 3037, 11, 291, 406, 41807, 13, 1743, 286, 393, 380, 26225, 341, 3089, 337, 411, 257, 819, 5633, 4153, 13, 467, 311, 411, 588, 2685, 281, 341, 551, 13, 407, 286, 478, 445, 8407, 484, 264, 7340, 1507, 11, 597, 307, 437, 291, 458, 11, 437, 321, 434, 516, 281, 3496, 11803, 4, 295, 527, 565, 322, 307, 42626, 613, 13, 51364, 51364, 407, 321, 434, 516, 281, 51414, 51414, 1449, 1066, 257, 1326, 721, 456, 13, 509, 445, 8209, 1922, 13, 407, 437, 466, 264, 1036, 644, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14469172877650108, "compression_ratio": 1.648829431438127, "no_speech_prob": 2.1111505702720024e-05}, {"id": 296, "seek": 153300, "start": 1553.0, "end": 1554.0, "text": " So we're going to", "tokens": [50364, 865, 11, 2293, 13, 467, 311, 257, 707, 857, 1880, 13, 583, 437, 309, 775, 307, 309, 1669, 309, 370, 300, 428, 2316, 2698, 5304, 13, 1779, 13, 407, 294, 341, 3037, 11, 291, 406, 41807, 13, 1743, 286, 393, 380, 26225, 341, 3089, 337, 411, 257, 819, 5633, 4153, 13, 467, 311, 411, 588, 2685, 281, 341, 551, 13, 407, 286, 478, 445, 8407, 484, 264, 7340, 1507, 11, 597, 307, 437, 291, 458, 11, 437, 321, 434, 516, 281, 3496, 11803, 4, 295, 527, 565, 322, 307, 42626, 613, 13, 51364, 51364, 407, 321, 434, 516, 281, 51414, 51414, 1449, 1066, 257, 1326, 721, 456, 13, 509, 445, 8209, 1922, 13, 407, 437, 466, 264, 1036, 644, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14469172877650108, "compression_ratio": 1.648829431438127, "no_speech_prob": 2.1111505702720024e-05}, {"id": 297, "seek": 153300, "start": 1554.0, "end": 1560.0, "text": " Just keep a few things there. You just selected half. So what about the last part?", "tokens": [50364, 865, 11, 2293, 13, 467, 311, 257, 707, 857, 1880, 13, 583, 437, 309, 775, 307, 309, 1669, 309, 370, 300, 428, 2316, 2698, 5304, 13, 1779, 13, 407, 294, 341, 3037, 11, 291, 406, 41807, 13, 1743, 286, 393, 380, 26225, 341, 3089, 337, 411, 257, 819, 5633, 4153, 13, 467, 311, 411, 588, 2685, 281, 341, 551, 13, 407, 286, 478, 445, 8407, 484, 264, 7340, 1507, 11, 597, 307, 437, 291, 458, 11, 437, 321, 434, 516, 281, 3496, 11803, 4, 295, 527, 565, 322, 307, 42626, 613, 13, 51364, 51364, 407, 321, 434, 516, 281, 51414, 51414, 1449, 1066, 257, 1326, 721, 456, 13, 509, 445, 8209, 1922, 13, 407, 437, 466, 264, 1036, 644, 30, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.14469172877650108, "compression_ratio": 1.648829431438127, "no_speech_prob": 2.1111505702720024e-05}, {"id": 298, "seek": 156000, "start": 1560.0, "end": 1568.0, "text": " Yeah, so that you don't actually need. Right. So lightning is going to do this for you automatically. It's going to call backwards the step and the zero grad.", "tokens": [50364, 865, 11, 370, 300, 291, 500, 380, 767, 643, 13, 1779, 13, 407, 16589, 307, 516, 281, 360, 341, 337, 291, 6772, 13, 467, 311, 516, 281, 818, 12204, 264, 1823, 293, 264, 4018, 2771, 13, 50764, 50764, 821, 311, 257, 636, 300, 291, 393, 9528, 309, 498, 291, 528, 281, 13, 583, 293, 291, 393, 360, 309, 1803, 13, 583, 321, 434, 516, 281, 764, 746, 1219, 12509, 19618, 11, 597, 307, 15172, 538, 7576, 293, 16589, 689, 309, 311, 516, 281, 360, 309, 337, 291, 13, 51364, 51364, 509, 393, 1261, 300, 766, 293, 550, 291, 445, 818, 309, 1803, 13, 400, 300, 311, 2489, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1264857394354684, "compression_ratio": 1.8142292490118577, "no_speech_prob": 2.3921531465020962e-05}, {"id": 299, "seek": 156000, "start": 1568.0, "end": 1580.0, "text": " There's a way that you can enable it if you want to. But and you can do it yourself. But we're going to use something called automatic optimization, which is enabled by default and lightning where it's going to do it for you.", "tokens": [50364, 865, 11, 370, 300, 291, 500, 380, 767, 643, 13, 1779, 13, 407, 16589, 307, 516, 281, 360, 341, 337, 291, 6772, 13, 467, 311, 516, 281, 818, 12204, 264, 1823, 293, 264, 4018, 2771, 13, 50764, 50764, 821, 311, 257, 636, 300, 291, 393, 9528, 309, 498, 291, 528, 281, 13, 583, 293, 291, 393, 360, 309, 1803, 13, 583, 321, 434, 516, 281, 764, 746, 1219, 12509, 19618, 11, 597, 307, 15172, 538, 7576, 293, 16589, 689, 309, 311, 516, 281, 360, 309, 337, 291, 13, 51364, 51364, 509, 393, 1261, 300, 766, 293, 550, 291, 445, 818, 309, 1803, 13, 400, 300, 311, 2489, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1264857394354684, "compression_ratio": 1.8142292490118577, "no_speech_prob": 2.3921531465020962e-05}, {"id": 300, "seek": 156000, "start": 1580.0, "end": 1583.0, "text": " You can turn that off and then you just call it yourself. And that's fine.", "tokens": [50364, 865, 11, 370, 300, 291, 500, 380, 767, 643, 13, 1779, 13, 407, 16589, 307, 516, 281, 360, 341, 337, 291, 6772, 13, 467, 311, 516, 281, 818, 12204, 264, 1823, 293, 264, 4018, 2771, 13, 50764, 50764, 821, 311, 257, 636, 300, 291, 393, 9528, 309, 498, 291, 528, 281, 13, 583, 293, 291, 393, 360, 309, 1803, 13, 583, 321, 434, 516, 281, 764, 746, 1219, 12509, 19618, 11, 597, 307, 15172, 538, 7576, 293, 16589, 689, 309, 311, 516, 281, 360, 309, 337, 291, 13, 51364, 51364, 509, 393, 1261, 300, 766, 293, 550, 291, 445, 818, 309, 1803, 13, 400, 300, 311, 2489, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1264857394354684, "compression_ratio": 1.8142292490118577, "no_speech_prob": 2.3921531465020962e-05}, {"id": 301, "seek": 158300, "start": 1583.0, "end": 1591.0, "text": " Okay. Okay. So otherwise we just have to specify basically what the loss is given a batch. Is it correct?", "tokens": [50364, 1033, 13, 1033, 13, 407, 5911, 321, 445, 362, 281, 16500, 1936, 437, 264, 4470, 307, 2212, 257, 15245, 13, 1119, 309, 3006, 30, 50764, 50764, 12753, 13, 865, 13, 1033, 13, 407, 718, 311, 718, 311, 360, 300, 13, 407, 286, 9163, 439, 300, 13, 407, 1825, 3105, 13, 286, 478, 516, 281, 4159, 341, 551, 510, 570, 286, 914, 11, 291, 458, 11, 291, 393, 1856, 309, 498, 291, 528, 281, 2041, 1177, 380, 1871, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.1444506296297399, "compression_ratio": 1.527363184079602, "no_speech_prob": 5.063396747573279e-05}, {"id": 302, "seek": 158300, "start": 1591.0, "end": 1602.0, "text": " Correct. Yeah. Okay. So let's let's do that. So I paste all that. So nothing changed. I'm going to remove this thing here because I mean, you know, you can leave it if you want to guess doesn't matter.", "tokens": [50364, 1033, 13, 1033, 13, 407, 5911, 321, 445, 362, 281, 16500, 1936, 437, 264, 4470, 307, 2212, 257, 15245, 13, 1119, 309, 3006, 30, 50764, 50764, 12753, 13, 865, 13, 1033, 13, 407, 718, 311, 718, 311, 360, 300, 13, 407, 286, 9163, 439, 300, 13, 407, 1825, 3105, 13, 286, 478, 516, 281, 4159, 341, 551, 510, 570, 286, 914, 11, 291, 458, 11, 291, 393, 1856, 309, 498, 291, 528, 281, 2041, 1177, 380, 1871, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.1444506296297399, "compression_ratio": 1.527363184079602, "no_speech_prob": 5.063396747573279e-05}, {"id": 303, "seek": 160200, "start": 1602.0, "end": 1619.0, "text": " No, no, leave it. Why do you want to remove it? Yeah. Well, because I want to change eventually so that we can actually find to in the backbone, but we'll leave it for now. We'll make that change later. Okay. So this is soft backbone now. Right. So let's just back bone.", "tokens": [50364, 883, 11, 572, 11, 1856, 309, 13, 1545, 360, 291, 528, 281, 4159, 309, 30, 865, 13, 1042, 11, 570, 286, 528, 281, 1319, 4728, 370, 300, 321, 393, 767, 915, 281, 294, 264, 34889, 11, 457, 321, 603, 1856, 309, 337, 586, 13, 492, 603, 652, 300, 1319, 1780, 13, 1033, 13, 407, 341, 307, 2787, 34889, 586, 13, 1779, 13, 407, 718, 311, 445, 646, 9026, 13, 51214, 51214, 400, 550, 341, 307, 2698, 300, 2489, 10864, 4583, 13, 8561, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.3099529222510327, "compression_ratio": 1.6, "no_speech_prob": 1.0782538083731197e-05}, {"id": 304, "seek": 160200, "start": 1619.0, "end": 1624.0, "text": " And then this is self that fine tune layer. Cool.", "tokens": [50364, 883, 11, 572, 11, 1856, 309, 13, 1545, 360, 291, 528, 281, 4159, 309, 30, 865, 13, 1042, 11, 570, 286, 528, 281, 1319, 4728, 370, 300, 321, 393, 767, 915, 281, 294, 264, 34889, 11, 457, 321, 603, 1856, 309, 337, 586, 13, 492, 603, 652, 300, 1319, 1780, 13, 1033, 13, 407, 341, 307, 2787, 34889, 586, 13, 1779, 13, 407, 718, 311, 445, 646, 9026, 13, 51214, 51214, 400, 550, 341, 307, 2698, 300, 2489, 10864, 4583, 13, 8561, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.3099529222510327, "compression_ratio": 1.6, "no_speech_prob": 1.0782538083731197e-05}, {"id": 305, "seek": 162400, "start": 1624.0, "end": 1639.0, "text": " That should pretty much have all the same stuff. And then I returned this last year. Great. I see. Okay. And then the last thing is so so lightning modules like a recipe for a model, right? Like, or like a class or whatever you're trying to do. Here's a classifier. It's not. It's not a model. Right. It's just like a general classifier.", "tokens": [50364, 663, 820, 1238, 709, 362, 439, 264, 912, 1507, 13, 400, 550, 286, 8752, 341, 1036, 1064, 13, 3769, 13, 286, 536, 13, 1033, 13, 400, 550, 264, 1036, 551, 307, 370, 370, 16589, 16679, 411, 257, 6782, 337, 257, 2316, 11, 558, 30, 1743, 11, 420, 411, 257, 1508, 420, 2035, 291, 434, 1382, 281, 360, 13, 1692, 311, 257, 1508, 9902, 13, 467, 311, 406, 13, 467, 311, 406, 257, 2316, 13, 1779, 13, 467, 311, 445, 411, 257, 2674, 1508, 9902, 13, 51114, 51114, 583, 264, 1036, 733, 295, 14751, 286, 643, 307, 341, 5028, 6545, 13, 1779, 13, 407, 286, 643, 281, 458, 437, 5028, 6545, 286, 478, 516, 281, 764, 13, 1079, 13, 865, 13, 400, 456, 311, 257, 456, 311, 257, 3170, 1219, 22162, 5028, 22525, 13, 1545, 16500, 300, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2056309314484292, "compression_ratio": 1.777049180327869, "no_speech_prob": 9.027180931298062e-05}, {"id": 306, "seek": 162400, "start": 1639.0, "end": 1652.0, "text": " But the last kind of ingredient I need is this optimizer. Right. So I need to know what optimizer I'm going to use. Yes. Yeah. And there's a there's a method called configure optimizers. Why specify that?", "tokens": [50364, 663, 820, 1238, 709, 362, 439, 264, 912, 1507, 13, 400, 550, 286, 8752, 341, 1036, 1064, 13, 3769, 13, 286, 536, 13, 1033, 13, 400, 550, 264, 1036, 551, 307, 370, 370, 16589, 16679, 411, 257, 6782, 337, 257, 2316, 11, 558, 30, 1743, 11, 420, 411, 257, 1508, 420, 2035, 291, 434, 1382, 281, 360, 13, 1692, 311, 257, 1508, 9902, 13, 467, 311, 406, 13, 467, 311, 406, 257, 2316, 13, 1779, 13, 467, 311, 445, 411, 257, 2674, 1508, 9902, 13, 51114, 51114, 583, 264, 1036, 733, 295, 14751, 286, 643, 307, 341, 5028, 6545, 13, 1779, 13, 407, 286, 643, 281, 458, 437, 5028, 6545, 286, 478, 516, 281, 764, 13, 1079, 13, 865, 13, 400, 456, 311, 257, 456, 311, 257, 3170, 1219, 22162, 5028, 22525, 13, 1545, 16500, 300, 30, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.2056309314484292, "compression_ratio": 1.777049180327869, "no_speech_prob": 9.027180931298062e-05}, {"id": 307, "seek": 165200, "start": 1652.0, "end": 1662.0, "text": " So these names are, I believe, private names, training step and configure optimizers. Where do I find them? How do I know what are these names?", "tokens": [50364, 407, 613, 5288, 366, 11, 286, 1697, 11, 4551, 5288, 11, 3097, 1823, 293, 22162, 5028, 22525, 13, 2305, 360, 286, 915, 552, 30, 1012, 360, 286, 458, 437, 366, 613, 5288, 30, 50864, 50864, 865, 13, 407, 729, 366, 322, 264, 14333, 13, 1779, 13, 407, 562, 291, 352, 807, 510, 11, 16589, 294, 732, 4439, 11, 309, 603, 1029, 11, 309, 603, 1792, 291, 807, 309, 13, 286, 603, 584, 11, 341, 307, 437, 291, 643, 281, 6964, 3097, 1507, 11, 22162, 5028, 22525, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.17280374254499162, "compression_ratio": 1.6682027649769586, "no_speech_prob": 4.936844106850913e-06}, {"id": 308, "seek": 165200, "start": 1662.0, "end": 1675.0, "text": " Yeah. So those are on the documentation. Right. So when you go through here, lightning in two steps, it'll ask, it'll walk you through it. I'll say, this is what you need to define training stuff, configure optimizers.", "tokens": [50364, 407, 613, 5288, 366, 11, 286, 1697, 11, 4551, 5288, 11, 3097, 1823, 293, 22162, 5028, 22525, 13, 2305, 360, 286, 915, 552, 30, 1012, 360, 286, 458, 437, 366, 613, 5288, 30, 50864, 50864, 865, 13, 407, 729, 366, 322, 264, 14333, 13, 1779, 13, 407, 562, 291, 352, 807, 510, 11, 16589, 294, 732, 4439, 11, 309, 603, 1029, 11, 309, 603, 1792, 291, 807, 309, 13, 286, 603, 584, 11, 341, 307, 437, 291, 643, 281, 6964, 3097, 1507, 11, 22162, 5028, 22525, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.17280374254499162, "compression_ratio": 1.6682027649769586, "no_speech_prob": 4.936844106850913e-06}, {"id": 309, "seek": 167500, "start": 1675.0, "end": 1683.0, "text": " Okay. Okay. Yeah. And forward is optional. I'm not using it, but you don't have to use it because we're not using this model for predictions. So I don't have to actually define it.", "tokens": [50364, 1033, 13, 1033, 13, 865, 13, 400, 2128, 307, 17312, 13, 286, 478, 406, 1228, 309, 11, 457, 291, 500, 380, 362, 281, 764, 309, 570, 321, 434, 406, 1228, 341, 2316, 337, 21264, 13, 407, 286, 500, 380, 362, 281, 767, 6964, 309, 13, 50764, 50764, 407, 498, 321, 576, 764, 341, 2316, 337, 17630, 11, 291, 767, 576, 764, 264, 2128, 30, 865, 11, 3006, 13, 407, 294, 341, 10723, 11, 286, 478, 1228, 364, 8399, 22660, 19866, 13, 407, 294, 341, 1729, 1389, 11, 286, 4114, 264, 8399, 22660, 19866, 281, 8460, 12240, 29432, 562, 291, 764, 309, 13, 51514, 51514, 407, 286, 445, 4114, 493, 264, 2128, 281, 360, 300, 13, 583, 264, 3097, 1823, 307, 4994, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09355009169805617, "compression_ratio": 1.8702290076335877, "no_speech_prob": 7.41052235753159e-06}, {"id": 310, "seek": 167500, "start": 1683.0, "end": 1698.0, "text": " So if we would use this model for prediction, you actually would use the forward? Yeah, correct. So in this demo, I'm using an autoencoder. So in this particular case, I wrote the autoencoder to generate embeddings when you use it.", "tokens": [50364, 1033, 13, 1033, 13, 865, 13, 400, 2128, 307, 17312, 13, 286, 478, 406, 1228, 309, 11, 457, 291, 500, 380, 362, 281, 764, 309, 570, 321, 434, 406, 1228, 341, 2316, 337, 21264, 13, 407, 286, 500, 380, 362, 281, 767, 6964, 309, 13, 50764, 50764, 407, 498, 321, 576, 764, 341, 2316, 337, 17630, 11, 291, 767, 576, 764, 264, 2128, 30, 865, 11, 3006, 13, 407, 294, 341, 10723, 11, 286, 478, 1228, 364, 8399, 22660, 19866, 13, 407, 294, 341, 1729, 1389, 11, 286, 4114, 264, 8399, 22660, 19866, 281, 8460, 12240, 29432, 562, 291, 764, 309, 13, 51514, 51514, 407, 286, 445, 4114, 493, 264, 2128, 281, 360, 300, 13, 583, 264, 3097, 1823, 307, 4994, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09355009169805617, "compression_ratio": 1.8702290076335877, "no_speech_prob": 7.41052235753159e-06}, {"id": 311, "seek": 167500, "start": 1698.0, "end": 1703.0, "text": " So I just wrote up the forward to do that. But the training step is separate.", "tokens": [50364, 1033, 13, 1033, 13, 865, 13, 400, 2128, 307, 17312, 13, 286, 478, 406, 1228, 309, 11, 457, 291, 500, 380, 362, 281, 764, 309, 570, 321, 434, 406, 1228, 341, 2316, 337, 21264, 13, 407, 286, 500, 380, 362, 281, 767, 6964, 309, 13, 50764, 50764, 407, 498, 321, 576, 764, 341, 2316, 337, 17630, 11, 291, 767, 576, 764, 264, 2128, 30, 865, 11, 3006, 13, 407, 294, 341, 10723, 11, 286, 478, 1228, 364, 8399, 22660, 19866, 13, 407, 294, 341, 1729, 1389, 11, 286, 4114, 264, 8399, 22660, 19866, 281, 8460, 12240, 29432, 562, 291, 764, 309, 13, 51514, 51514, 407, 286, 445, 4114, 493, 264, 2128, 281, 360, 300, 13, 583, 264, 3097, 1823, 307, 4994, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.09355009169805617, "compression_ratio": 1.8702290076335877, "no_speech_prob": 7.41052235753159e-06}, {"id": 312, "seek": 170300, "start": 1703.0, "end": 1710.0, "text": " Okay. So we have the definite, the init that is basically defining all the modules inside.", "tokens": [50364, 1033, 13, 407, 321, 362, 264, 25131, 11, 264, 3157, 300, 307, 1936, 17827, 439, 264, 16679, 1854, 13, 50714, 50714, 1396, 321, 362, 257, 2128, 11, 597, 307, 11, 382, 291, 848, 11, 764, 787, 5699, 291, 815, 764, 264, 2316, 11, 457, 321, 500, 380, 4725, 643, 13, 51114, 51114, 1396, 456, 307, 264, 3097, 1823, 689, 321, 6964, 577, 264, 4470, 307, 40610, 2212, 257, 15245, 293, 257, 15245, 8186, 13, 51464, 51464], "temperature": 0.4, "avg_logprob": -0.1364740836314666, "compression_ratio": 1.6321243523316062, "no_speech_prob": 6.177649629535154e-05}, {"id": 313, "seek": 170300, "start": 1710.0, "end": 1718.0, "text": " Then we have a forward, which is, as you said, use only whenever you may use the model, but we don't necessarily need.", "tokens": [50364, 1033, 13, 407, 321, 362, 264, 25131, 11, 264, 3157, 300, 307, 1936, 17827, 439, 264, 16679, 1854, 13, 50714, 50714, 1396, 321, 362, 257, 2128, 11, 597, 307, 11, 382, 291, 848, 11, 764, 787, 5699, 291, 815, 764, 264, 2316, 11, 457, 321, 500, 380, 4725, 643, 13, 51114, 51114, 1396, 456, 307, 264, 3097, 1823, 689, 321, 6964, 577, 264, 4470, 307, 40610, 2212, 257, 15245, 293, 257, 15245, 8186, 13, 51464, 51464], "temperature": 0.4, "avg_logprob": -0.1364740836314666, "compression_ratio": 1.6321243523316062, "no_speech_prob": 6.177649629535154e-05}, {"id": 314, "seek": 170300, "start": 1718.0, "end": 1725.0, "text": " Then there is the training step where we define how the loss is computed given a batch and a batch index.", "tokens": [50364, 1033, 13, 407, 321, 362, 264, 25131, 11, 264, 3157, 300, 307, 1936, 17827, 439, 264, 16679, 1854, 13, 50714, 50714, 1396, 321, 362, 257, 2128, 11, 597, 307, 11, 382, 291, 848, 11, 764, 787, 5699, 291, 815, 764, 264, 2316, 11, 457, 321, 500, 380, 4725, 643, 13, 51114, 51114, 1396, 456, 307, 264, 3097, 1823, 689, 321, 6964, 577, 264, 4470, 307, 40610, 2212, 257, 15245, 293, 257, 15245, 8186, 13, 51464, 51464], "temperature": 0.4, "avg_logprob": -0.1364740836314666, "compression_ratio": 1.6321243523316062, "no_speech_prob": 6.177649629535154e-05}, {"id": 315, "seek": 172500, "start": 1725.0, "end": 1735.0, "text": " And then finally we have this configure optimizer, which is specifying the optimizer we're going to be using for adapting the parameters of the network.", "tokens": [50364, 400, 550, 2721, 321, 362, 341, 22162, 5028, 6545, 11, 597, 307, 1608, 5489, 264, 5028, 6545, 321, 434, 516, 281, 312, 1228, 337, 34942, 264, 9834, 295, 264, 3209, 13, 50864, 50864, 2589, 286, 3006, 30, 865, 11, 300, 311, 2293, 3006, 13, 51114, 51114, 1057, 558, 13, 1033, 13, 1033, 13, 25245, 2020, 13, 25245, 2020, 13, 407, 286, 478, 516, 281, 764, 300, 5028, 6545, 13, 1779, 13, 407, 7938, 11, 321, 362, 309, 493, 510, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.18613308951968238, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.54379758896539e-05}, {"id": 316, "seek": 172500, "start": 1735.0, "end": 1740.0, "text": " Did I correct? Yeah, that's exactly correct.", "tokens": [50364, 400, 550, 2721, 321, 362, 341, 22162, 5028, 6545, 11, 597, 307, 1608, 5489, 264, 5028, 6545, 321, 434, 516, 281, 312, 1228, 337, 34942, 264, 9834, 295, 264, 3209, 13, 50864, 50864, 2589, 286, 3006, 30, 865, 11, 300, 311, 2293, 3006, 13, 51114, 51114, 1057, 558, 13, 1033, 13, 1033, 13, 25245, 2020, 13, 25245, 2020, 13, 407, 286, 478, 516, 281, 764, 300, 5028, 6545, 13, 1779, 13, 407, 7938, 11, 321, 362, 309, 493, 510, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.18613308951968238, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.54379758896539e-05}, {"id": 317, "seek": 172500, "start": 1740.0, "end": 1747.0, "text": " All right. Okay. Okay. Makes sense. Makes sense. So I'm going to use that optimizer. Right. So Adam, we have it up here.", "tokens": [50364, 400, 550, 2721, 321, 362, 341, 22162, 5028, 6545, 11, 597, 307, 1608, 5489, 264, 5028, 6545, 321, 434, 516, 281, 312, 1228, 337, 34942, 264, 9834, 295, 264, 3209, 13, 50864, 50864, 2589, 286, 3006, 30, 865, 11, 300, 311, 2293, 3006, 13, 51114, 51114, 1057, 558, 13, 1033, 13, 1033, 13, 25245, 2020, 13, 25245, 2020, 13, 407, 286, 478, 516, 281, 764, 300, 5028, 6545, 13, 1779, 13, 407, 7938, 11, 321, 362, 309, 493, 510, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.18613308951968238, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.54379758896539e-05}, {"id": 318, "seek": 174700, "start": 1747.0, "end": 1755.0, "text": " Okay. Okay. So, you know, when we did it up here, we were just passing in that fine tune layer, right?", "tokens": [50364, 1033, 13, 1033, 13, 407, 11, 291, 458, 11, 562, 321, 630, 309, 493, 510, 11, 321, 645, 445, 8437, 294, 300, 2489, 10864, 4583, 11, 558, 30, 50764, 50764, 509, 500, 380, 362, 281, 360, 300, 13, 407, 286, 478, 445, 516, 281, 818, 2698, 510, 13, 400, 300, 311, 516, 281, 445, 1320, 294, 439, 295, 264, 9834, 13, 1779, 13, 51164, 51164, 400, 300, 311, 1392, 570, 264, 34889, 307, 15191, 570, 309, 311, 572, 2771, 551, 13, 407, 309, 311, 516, 281, 312, 2489, 13, 51414, 51414, 286, 478, 406, 767, 516, 281, 646, 79, 1513, 559, 473, 666, 309, 13, 400, 550, 286, 478, 516, 281, 652, 341, 2539, 3314, 519, 411, 341, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1439821661972418, "compression_ratio": 1.7689243027888446, "no_speech_prob": 1.0615764949761797e-05}, {"id": 319, "seek": 174700, "start": 1755.0, "end": 1763.0, "text": " You don't have to do that. So I'm just going to call self here. And that's going to just pass in all of the parameters. Right.", "tokens": [50364, 1033, 13, 1033, 13, 407, 11, 291, 458, 11, 562, 321, 630, 309, 493, 510, 11, 321, 645, 445, 8437, 294, 300, 2489, 10864, 4583, 11, 558, 30, 50764, 50764, 509, 500, 380, 362, 281, 360, 300, 13, 407, 286, 478, 445, 516, 281, 818, 2698, 510, 13, 400, 300, 311, 516, 281, 445, 1320, 294, 439, 295, 264, 9834, 13, 1779, 13, 51164, 51164, 400, 300, 311, 1392, 570, 264, 34889, 307, 15191, 570, 309, 311, 572, 2771, 551, 13, 407, 309, 311, 516, 281, 312, 2489, 13, 51414, 51414, 286, 478, 406, 767, 516, 281, 646, 79, 1513, 559, 473, 666, 309, 13, 400, 550, 286, 478, 516, 281, 652, 341, 2539, 3314, 519, 411, 341, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1439821661972418, "compression_ratio": 1.7689243027888446, "no_speech_prob": 1.0615764949761797e-05}, {"id": 320, "seek": 174700, "start": 1763.0, "end": 1768.0, "text": " And that's okay because the backbone is disabled because it's no grad thing. So it's going to be fine.", "tokens": [50364, 1033, 13, 1033, 13, 407, 11, 291, 458, 11, 562, 321, 630, 309, 493, 510, 11, 321, 645, 445, 8437, 294, 300, 2489, 10864, 4583, 11, 558, 30, 50764, 50764, 509, 500, 380, 362, 281, 360, 300, 13, 407, 286, 478, 445, 516, 281, 818, 2698, 510, 13, 400, 300, 311, 516, 281, 445, 1320, 294, 439, 295, 264, 9834, 13, 1779, 13, 51164, 51164, 400, 300, 311, 1392, 570, 264, 34889, 307, 15191, 570, 309, 311, 572, 2771, 551, 13, 407, 309, 311, 516, 281, 312, 2489, 13, 51414, 51414, 286, 478, 406, 767, 516, 281, 646, 79, 1513, 559, 473, 666, 309, 13, 400, 550, 286, 478, 516, 281, 652, 341, 2539, 3314, 519, 411, 341, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1439821661972418, "compression_ratio": 1.7689243027888446, "no_speech_prob": 1.0615764949761797e-05}, {"id": 321, "seek": 174700, "start": 1768.0, "end": 1773.0, "text": " I'm not actually going to backpropagate into it. And then I'm going to make this learning rate think like this.", "tokens": [50364, 1033, 13, 1033, 13, 407, 11, 291, 458, 11, 562, 321, 630, 309, 493, 510, 11, 321, 645, 445, 8437, 294, 300, 2489, 10864, 4583, 11, 558, 30, 50764, 50764, 509, 500, 380, 362, 281, 360, 300, 13, 407, 286, 478, 445, 516, 281, 818, 2698, 510, 13, 400, 300, 311, 516, 281, 445, 1320, 294, 439, 295, 264, 9834, 13, 1779, 13, 51164, 51164, 400, 300, 311, 1392, 570, 264, 34889, 307, 15191, 570, 309, 311, 572, 2771, 551, 13, 407, 309, 311, 516, 281, 312, 2489, 13, 51414, 51414, 286, 478, 406, 767, 516, 281, 646, 79, 1513, 559, 473, 666, 309, 13, 400, 550, 286, 478, 516, 281, 652, 341, 2539, 3314, 519, 411, 341, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1439821661972418, "compression_ratio": 1.7689243027888446, "no_speech_prob": 1.0615764949761797e-05}, {"id": 322, "seek": 177300, "start": 1773.0, "end": 1778.0, "text": " And then I need to return this optimizer. Don't forget this or you'll get random noise.", "tokens": [50364, 400, 550, 286, 643, 281, 2736, 341, 5028, 6545, 13, 1468, 380, 2870, 341, 420, 291, 603, 483, 4974, 5658, 13, 50614, 50614, 1033, 13, 407, 300, 311, 3736, 309, 13, 407, 797, 11, 341, 307, 264, 912, 3089, 13, 50814, 50814, 583, 586, 309, 311, 411, 41426, 1570, 39228, 37008, 11, 754, 337, 1270, 257, 2199, 1716, 13, 1779, 13, 51114, 51114, 400, 550, 586, 281, 3847, 341, 588, 2199, 13, 407, 286, 478, 445, 516, 281, 8129, 452, 2316, 13, 1779, 13, 876, 11, 767, 11, 286, 5298, 472, 551, 13, 407, 341, 2539, 3314, 11, 286, 733, 295, 528, 281, 10864, 309, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10297581065784801, "compression_ratio": 1.5647058823529412, "no_speech_prob": 4.565899871522561e-06}, {"id": 323, "seek": 177300, "start": 1778.0, "end": 1782.0, "text": " Okay. So that's literally it. So again, this is the same code.", "tokens": [50364, 400, 550, 286, 643, 281, 2736, 341, 5028, 6545, 13, 1468, 380, 2870, 341, 420, 291, 603, 483, 4974, 5658, 13, 50614, 50614, 1033, 13, 407, 300, 311, 3736, 309, 13, 407, 797, 11, 341, 307, 264, 912, 3089, 13, 50814, 50814, 583, 586, 309, 311, 411, 41426, 1570, 39228, 37008, 11, 754, 337, 1270, 257, 2199, 1716, 13, 1779, 13, 51114, 51114, 400, 550, 586, 281, 3847, 341, 588, 2199, 13, 407, 286, 478, 445, 516, 281, 8129, 452, 2316, 13, 1779, 13, 876, 11, 767, 11, 286, 5298, 472, 551, 13, 407, 341, 2539, 3314, 11, 286, 733, 295, 528, 281, 10864, 309, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10297581065784801, "compression_ratio": 1.5647058823529412, "no_speech_prob": 4.565899871522561e-06}, {"id": 324, "seek": 177300, "start": 1782.0, "end": 1788.0, "text": " But now it's like vastly less boilerplate, even for such a simple project. Right.", "tokens": [50364, 400, 550, 286, 643, 281, 2736, 341, 5028, 6545, 13, 1468, 380, 2870, 341, 420, 291, 603, 483, 4974, 5658, 13, 50614, 50614, 1033, 13, 407, 300, 311, 3736, 309, 13, 407, 797, 11, 341, 307, 264, 912, 3089, 13, 50814, 50814, 583, 586, 309, 311, 411, 41426, 1570, 39228, 37008, 11, 754, 337, 1270, 257, 2199, 1716, 13, 1779, 13, 51114, 51114, 400, 550, 586, 281, 3847, 341, 588, 2199, 13, 407, 286, 478, 445, 516, 281, 8129, 452, 2316, 13, 1779, 13, 876, 11, 767, 11, 286, 5298, 472, 551, 13, 407, 341, 2539, 3314, 11, 286, 733, 295, 528, 281, 10864, 309, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10297581065784801, "compression_ratio": 1.5647058823529412, "no_speech_prob": 4.565899871522561e-06}, {"id": 325, "seek": 177300, "start": 1788.0, "end": 1797.0, "text": " And then now to train this very simple. So I'm just going to edit my model. Right. Oh, actually, I forgot one thing. So this learning rate, I kind of want to tune it.", "tokens": [50364, 400, 550, 286, 643, 281, 2736, 341, 5028, 6545, 13, 1468, 380, 2870, 341, 420, 291, 603, 483, 4974, 5658, 13, 50614, 50614, 1033, 13, 407, 300, 311, 3736, 309, 13, 407, 797, 11, 341, 307, 264, 912, 3089, 13, 50814, 50814, 583, 586, 309, 311, 411, 41426, 1570, 39228, 37008, 11, 754, 337, 1270, 257, 2199, 1716, 13, 1779, 13, 51114, 51114, 400, 550, 586, 281, 3847, 341, 588, 2199, 13, 407, 286, 478, 445, 516, 281, 8129, 452, 2316, 13, 1779, 13, 876, 11, 767, 11, 286, 5298, 472, 551, 13, 407, 341, 2539, 3314, 11, 286, 733, 295, 528, 281, 10864, 309, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.10297581065784801, "compression_ratio": 1.5647058823529412, "no_speech_prob": 4.565899871522561e-06}, {"id": 326, "seek": 179700, "start": 1797.0, "end": 1804.0, "text": " So let me just make it a parameter. Right. And then let's do that here.", "tokens": [50364, 407, 718, 385, 445, 652, 309, 257, 13075, 13, 1779, 13, 400, 550, 718, 311, 360, 300, 510, 13, 50714, 50714, 6244, 293, 550, 286, 603, 1320, 309, 294, 13, 407, 498, 300, 6915, 300, 13, 3769, 13, 407, 286, 519, 456, 820, 312, 512, 588, 1481, 4282, 11, 286, 1697, 11, 294, 419, 9676, 300, 286, 500, 380, 362, 281, 2010, 341, 13, 51264, 51264, 865, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.18904221057891846, "compression_ratio": 1.4352941176470588, "no_speech_prob": 5.093640993436566e-06}, {"id": 327, "seek": 179700, "start": 1804.0, "end": 1815.0, "text": " Three and then I'll pass it in. So if that equals that. Great. So I think there should be some very nice trick, I believe, in aligning that I don't have to type this.", "tokens": [50364, 407, 718, 385, 445, 652, 309, 257, 13075, 13, 1779, 13, 400, 550, 718, 311, 360, 300, 510, 13, 50714, 50714, 6244, 293, 550, 286, 603, 1320, 309, 294, 13, 407, 498, 300, 6915, 300, 13, 3769, 13, 407, 286, 519, 456, 820, 312, 512, 588, 1481, 4282, 11, 286, 1697, 11, 294, 419, 9676, 300, 286, 500, 380, 362, 281, 2010, 341, 13, 51264, 51264, 865, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.18904221057891846, "compression_ratio": 1.4352941176470588, "no_speech_prob": 5.093640993436566e-06}, {"id": 328, "seek": 179700, "start": 1815.0, "end": 1819.0, "text": " Yeah.", "tokens": [50364, 407, 718, 385, 445, 652, 309, 257, 13075, 13, 1779, 13, 400, 550, 718, 311, 360, 300, 510, 13, 50714, 50714, 6244, 293, 550, 286, 603, 1320, 309, 294, 13, 407, 498, 300, 6915, 300, 13, 3769, 13, 407, 286, 519, 456, 820, 312, 512, 588, 1481, 4282, 11, 286, 1697, 11, 294, 419, 9676, 300, 286, 500, 380, 362, 281, 2010, 341, 13, 51264, 51264, 865, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.18904221057891846, "compression_ratio": 1.4352941176470588, "no_speech_prob": 5.093640993436566e-06}, {"id": 329, "seek": 181900, "start": 1819.0, "end": 1830.0, "text": " So we have a save hyper parameters, I believe hyper parameters. Yeah. Okay.", "tokens": [50364, 407, 321, 362, 257, 3155, 9848, 9834, 11, 286, 1697, 9848, 9834, 13, 865, 13, 1033, 13, 50914, 50914, 509, 500, 380, 767, 362, 281, 13, 823, 11, 562, 286, 360, 341, 11, 286, 500, 380, 767, 362, 281, 352, 293, 584, 2698, 2107, 11665, 279, 6915, 19060, 11, 441, 49, 6915, 2035, 13, 1779, 13, 51414, 51414, 639, 286, 500, 380, 362, 281, 360, 341, 3602, 13, 286, 393, 445, 3838, 13, 1033, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.2595631321774253, "compression_ratio": 1.6114285714285714, "no_speech_prob": 4.832178456126712e-05}, {"id": 330, "seek": 181900, "start": 1830.0, "end": 1840.0, "text": " You don't actually have to. Now, when I do this, I don't actually have to go and say self nonclasses equals Excel, LR equals whatever. Right.", "tokens": [50364, 407, 321, 362, 257, 3155, 9848, 9834, 11, 286, 1697, 9848, 9834, 13, 865, 13, 1033, 13, 50914, 50914, 509, 500, 380, 767, 362, 281, 13, 823, 11, 562, 286, 360, 341, 11, 286, 500, 380, 767, 362, 281, 352, 293, 584, 2698, 2107, 11665, 279, 6915, 19060, 11, 441, 49, 6915, 2035, 13, 1779, 13, 51414, 51414, 639, 286, 500, 380, 362, 281, 360, 341, 3602, 13, 286, 393, 445, 3838, 13, 1033, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.2595631321774253, "compression_ratio": 1.6114285714285714, "no_speech_prob": 4.832178456126712e-05}, {"id": 331, "seek": 181900, "start": 1840.0, "end": 1845.0, "text": " This I don't have to do this anymore. I can just directly. Okay.", "tokens": [50364, 407, 321, 362, 257, 3155, 9848, 9834, 11, 286, 1697, 9848, 9834, 13, 865, 13, 1033, 13, 50914, 50914, 509, 500, 380, 767, 362, 281, 13, 823, 11, 562, 286, 360, 341, 11, 286, 500, 380, 767, 362, 281, 352, 293, 584, 2698, 2107, 11665, 279, 6915, 19060, 11, 441, 49, 6915, 2035, 13, 1779, 13, 51414, 51414, 639, 286, 500, 380, 362, 281, 360, 341, 3602, 13, 286, 393, 445, 3838, 13, 1033, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.2595631321774253, "compression_ratio": 1.6114285714285714, "no_speech_prob": 4.832178456126712e-05}, {"id": 332, "seek": 184500, "start": 1845.0, "end": 1850.0, "text": " So I can call it here. So it's saved under this thing called H prams hyper prams. Right. Okay.", "tokens": [50364, 407, 286, 393, 818, 309, 510, 13, 407, 309, 311, 6624, 833, 341, 551, 1219, 389, 582, 4070, 9848, 582, 4070, 13, 1779, 13, 1033, 13, 50614, 50614, 467, 311, 767, 456, 3838, 13, 286, 536, 13, 400, 341, 307, 4420, 570, 11, 291, 458, 11, 294, 881, 5245, 11, 291, 362, 411, 2217, 9834, 13, 1779, 13, 407, 291, 500, 380, 528, 281, 360, 341, 16945, 13, 51064, 51064, 1033, 11, 869, 13, 823, 445, 281, 3847, 341, 551, 13, 467, 311, 588, 2199, 13, 286, 478, 445, 516, 281, 8129, 300, 2316, 13, 1779, 13, 407, 577, 360, 321, 3847, 341, 1507, 30, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.16720278328711832, "compression_ratio": 1.6348547717842323, "no_speech_prob": 6.70816152705811e-05}, {"id": 333, "seek": 184500, "start": 1850.0, "end": 1859.0, "text": " It's actually there directly. I see. And this is useful because, you know, in most models, you have like 30 parameters. Right. So you don't want to do this manually.", "tokens": [50364, 407, 286, 393, 818, 309, 510, 13, 407, 309, 311, 6624, 833, 341, 551, 1219, 389, 582, 4070, 9848, 582, 4070, 13, 1779, 13, 1033, 13, 50614, 50614, 467, 311, 767, 456, 3838, 13, 286, 536, 13, 400, 341, 307, 4420, 570, 11, 291, 458, 11, 294, 881, 5245, 11, 291, 362, 411, 2217, 9834, 13, 1779, 13, 407, 291, 500, 380, 528, 281, 360, 341, 16945, 13, 51064, 51064, 1033, 11, 869, 13, 823, 445, 281, 3847, 341, 551, 13, 467, 311, 588, 2199, 13, 286, 478, 445, 516, 281, 8129, 300, 2316, 13, 1779, 13, 407, 577, 360, 321, 3847, 341, 1507, 30, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.16720278328711832, "compression_ratio": 1.6348547717842323, "no_speech_prob": 6.70816152705811e-05}, {"id": 334, "seek": 184500, "start": 1859.0, "end": 1866.0, "text": " Okay, great. Now just to train this thing. It's very simple. I'm just going to edit that model. Right. So how do we train this stuff?", "tokens": [50364, 407, 286, 393, 818, 309, 510, 13, 407, 309, 311, 6624, 833, 341, 551, 1219, 389, 582, 4070, 9848, 582, 4070, 13, 1779, 13, 1033, 13, 50614, 50614, 467, 311, 767, 456, 3838, 13, 286, 536, 13, 400, 341, 307, 4420, 570, 11, 291, 458, 11, 294, 881, 5245, 11, 291, 362, 411, 2217, 9834, 13, 1779, 13, 407, 291, 500, 380, 528, 281, 360, 341, 16945, 13, 51064, 51064, 1033, 11, 869, 13, 823, 445, 281, 3847, 341, 551, 13, 467, 311, 588, 2199, 13, 286, 478, 445, 516, 281, 8129, 300, 2316, 13, 1779, 13, 407, 577, 360, 321, 3847, 341, 1507, 30, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.16720278328711832, "compression_ratio": 1.6348547717842323, "no_speech_prob": 6.70816152705811e-05}, {"id": 335, "seek": 186600, "start": 1866.0, "end": 1878.0, "text": " Yeah. So we have this. So remember all the stuff that we got rid of. So you put batch optimizer, whatever. That's all inside this trainer that basically handles all the engineering for you.", "tokens": [50364, 865, 13, 407, 321, 362, 341, 13, 407, 1604, 439, 264, 1507, 300, 321, 658, 3973, 295, 13, 407, 291, 829, 15245, 5028, 6545, 11, 2035, 13, 663, 311, 439, 1854, 341, 21110, 300, 1936, 18722, 439, 264, 7043, 337, 291, 13, 50964, 50964, 3013, 21110, 13, 407, 309, 311, 510, 13, 1779, 13, 407, 341, 21110, 307, 510, 13, 876, 11, 286, 536, 13, 492, 2378, 380, 8825, 1939, 13, 865, 13, 51414, 51414, 407, 294, 264, 16589, 6405, 11, 291, 362, 291, 787, 362, 732, 721, 291, 643, 281, 458, 466, 294, 16589, 13, 1779, 13, 639, 21110, 293, 550, 264, 4137, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.23123627408928826, "compression_ratio": 1.7459677419354838, "no_speech_prob": 8.613132376922294e-05}, {"id": 336, "seek": 186600, "start": 1878.0, "end": 1887.0, "text": " Which trainer. So it's here. Right. So this trainer is here. Oh, I see. We haven't explained yet. Yeah.", "tokens": [50364, 865, 13, 407, 321, 362, 341, 13, 407, 1604, 439, 264, 1507, 300, 321, 658, 3973, 295, 13, 407, 291, 829, 15245, 5028, 6545, 11, 2035, 13, 663, 311, 439, 1854, 341, 21110, 300, 1936, 18722, 439, 264, 7043, 337, 291, 13, 50964, 50964, 3013, 21110, 13, 407, 309, 311, 510, 13, 1779, 13, 407, 341, 21110, 307, 510, 13, 876, 11, 286, 536, 13, 492, 2378, 380, 8825, 1939, 13, 865, 13, 51414, 51414, 407, 294, 264, 16589, 6405, 11, 291, 362, 291, 787, 362, 732, 721, 291, 643, 281, 458, 466, 294, 16589, 13, 1779, 13, 639, 21110, 293, 550, 264, 4137, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.23123627408928826, "compression_ratio": 1.7459677419354838, "no_speech_prob": 8.613132376922294e-05}, {"id": 337, "seek": 186600, "start": 1887.0, "end": 1894.0, "text": " So in the lightning library, you have you only have two things you need to know about in lightning. Right. This trainer and then the slide.", "tokens": [50364, 865, 13, 407, 321, 362, 341, 13, 407, 1604, 439, 264, 1507, 300, 321, 658, 3973, 295, 13, 407, 291, 829, 15245, 5028, 6545, 11, 2035, 13, 663, 311, 439, 1854, 341, 21110, 300, 1936, 18722, 439, 264, 7043, 337, 291, 13, 50964, 50964, 3013, 21110, 13, 407, 309, 311, 510, 13, 1779, 13, 407, 341, 21110, 307, 510, 13, 876, 11, 286, 536, 13, 492, 2378, 380, 8825, 1939, 13, 865, 13, 51414, 51414, 407, 294, 264, 16589, 6405, 11, 291, 362, 291, 787, 362, 732, 721, 291, 643, 281, 458, 466, 294, 16589, 13, 1779, 13, 639, 21110, 293, 550, 264, 4137, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.23123627408928826, "compression_ratio": 1.7459677419354838, "no_speech_prob": 8.613132376922294e-05}, {"id": 338, "seek": 189400, "start": 1894.0, "end": 1903.0, "text": " That's what I mean. So the trainer is like literally your, your foot, your simple training group. Right. So let me just show you. So lightning module.", "tokens": [50364, 663, 311, 437, 286, 914, 13, 407, 264, 21110, 307, 411, 3736, 428, 11, 428, 2671, 11, 428, 2199, 3097, 1594, 13, 1779, 13, 407, 718, 385, 445, 855, 291, 13, 407, 16589, 10088, 13, 50814, 50814, 11809, 82, 300, 293, 550, 264, 35393, 510, 13, 407, 16589, 13, 1033, 13, 440, 2135, 9362, 13, 286, 536, 13, 286, 536, 13, 286, 536, 13, 1033, 13, 2743, 11, 456, 311, 445, 732, 456, 13, 865, 11, 300, 311, 309, 13, 51264, 51264, 1033, 13, 400, 550, 286, 519, 321, 362, 754, 257, 35899, 6367, 370, 291, 393, 1223, 577, 437, 309, 311, 884, 13, 961, 385, 536, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.26439876215798513, "compression_ratio": 1.7033898305084745, "no_speech_prob": 4.608646486303769e-05}, {"id": 339, "seek": 189400, "start": 1903.0, "end": 1912.0, "text": " Finds that and then the trainers here. So lightning. Okay. The main API. I see. I see. I see. Okay. Also, there's just two there. Yeah, that's it.", "tokens": [50364, 663, 311, 437, 286, 914, 13, 407, 264, 21110, 307, 411, 3736, 428, 11, 428, 2671, 11, 428, 2199, 3097, 1594, 13, 1779, 13, 407, 718, 385, 445, 855, 291, 13, 407, 16589, 10088, 13, 50814, 50814, 11809, 82, 300, 293, 550, 264, 35393, 510, 13, 407, 16589, 13, 1033, 13, 440, 2135, 9362, 13, 286, 536, 13, 286, 536, 13, 286, 536, 13, 1033, 13, 2743, 11, 456, 311, 445, 732, 456, 13, 865, 11, 300, 311, 309, 13, 51264, 51264, 1033, 13, 400, 550, 286, 519, 321, 362, 754, 257, 35899, 6367, 370, 291, 393, 1223, 577, 437, 309, 311, 884, 13, 961, 385, 536, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.26439876215798513, "compression_ratio": 1.7033898305084745, "no_speech_prob": 4.608646486303769e-05}, {"id": 340, "seek": 189400, "start": 1912.0, "end": 1919.0, "text": " Okay. And then I think we have even a pseudo loop so you can understand how what it's doing. Let me see.", "tokens": [50364, 663, 311, 437, 286, 914, 13, 407, 264, 21110, 307, 411, 3736, 428, 11, 428, 2671, 11, 428, 2199, 3097, 1594, 13, 1779, 13, 407, 718, 385, 445, 855, 291, 13, 407, 16589, 10088, 13, 50814, 50814, 11809, 82, 300, 293, 550, 264, 35393, 510, 13, 407, 16589, 13, 1033, 13, 440, 2135, 9362, 13, 286, 536, 13, 286, 536, 13, 286, 536, 13, 1033, 13, 2743, 11, 456, 311, 445, 732, 456, 13, 865, 11, 300, 311, 309, 13, 51264, 51264, 1033, 13, 400, 550, 286, 519, 321, 362, 754, 257, 35899, 6367, 370, 291, 393, 1223, 577, 437, 309, 311, 884, 13, 961, 385, 536, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.26439876215798513, "compression_ratio": 1.7033898305084745, "no_speech_prob": 4.608646486303769e-05}, {"id": 341, "seek": 191900, "start": 1919.0, "end": 1924.0, "text": " Where is it?", "tokens": [50364, 2305, 307, 309, 30, 50614, 50614, 865, 11, 286, 519, 309, 1985, 13, 51014, 51014, 1033, 13, 583, 286, 519, 286, 658, 309, 13, 407, 286, 445, 352, 281, 264, 45623, 293, 550, 456, 307, 264, 9362, 293, 550, 456, 366, 264, 732, 721, 300, 291, 2835, 949, 456, 307, 264, 2316, 13, 400, 550, 456, 307, 341, 21110, 11, 597, 307, 3097, 264, 2316, 11, 597, 1669, 2020, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.156262359103641, "compression_ratio": 1.6975308641975309, "no_speech_prob": 2.3546461306978017e-05}, {"id": 342, "seek": 191900, "start": 1924.0, "end": 1932.0, "text": " Yeah, I think it works.", "tokens": [50364, 2305, 307, 309, 30, 50614, 50614, 865, 11, 286, 519, 309, 1985, 13, 51014, 51014, 1033, 13, 583, 286, 519, 286, 658, 309, 13, 407, 286, 445, 352, 281, 264, 45623, 293, 550, 456, 307, 264, 9362, 293, 550, 456, 366, 264, 732, 721, 300, 291, 2835, 949, 456, 307, 264, 2316, 13, 400, 550, 456, 307, 341, 21110, 11, 597, 307, 3097, 264, 2316, 11, 597, 1669, 2020, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.156262359103641, "compression_ratio": 1.6975308641975309, "no_speech_prob": 2.3546461306978017e-05}, {"id": 343, "seek": 191900, "start": 1932.0, "end": 1945.0, "text": " Okay. But I think I got it. So I just go to the docs and then there is the API and then there are the two things that you mentioned before there is the model. And then there is this trainer, which is training the model, which makes sense.", "tokens": [50364, 2305, 307, 309, 30, 50614, 50614, 865, 11, 286, 519, 309, 1985, 13, 51014, 51014, 1033, 13, 583, 286, 519, 286, 658, 309, 13, 407, 286, 445, 352, 281, 264, 45623, 293, 550, 456, 307, 264, 9362, 293, 550, 456, 366, 264, 732, 721, 300, 291, 2835, 949, 456, 307, 264, 2316, 13, 400, 550, 456, 307, 341, 21110, 11, 597, 307, 3097, 264, 2316, 11, 597, 1669, 2020, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.156262359103641, "compression_ratio": 1.6975308641975309, "no_speech_prob": 2.3546461306978017e-05}, {"id": 344, "seek": 194500, "start": 1945.0, "end": 1949.0, "text": " I think. Yep. And then the lighting module tells you the same thing. Right.", "tokens": [50364, 286, 519, 13, 7010, 13, 400, 550, 264, 9577, 10088, 5112, 291, 264, 912, 551, 13, 1779, 13, 50564, 50564, 400, 550, 767, 11, 286, 519, 309, 307, 264, 9577, 10088, 689, 321, 2464, 13, 865, 13, 407, 510, 291, 352, 13, 407, 309, 311, 4099, 510, 833, 264, 13376, 13, 28848, 307, 516, 281, 360, 341, 13, 467, 311, 264, 912, 13, 1033, 13, 50964, 50964, 663, 321, 4114, 510, 13, 1033, 13, 1033, 13, 1033, 13, 663, 390, 452, 1168, 11, 767, 13, 407, 341, 307, 2293, 264, 551, 300, 321, 4114, 322, 264, 11, 322, 264, 21060, 13, 1779, 13, 407, 321, 362, 264, 2316, 3847, 293, 550, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.19248355367909306, "compression_ratio": 1.871244635193133, "no_speech_prob": 1.6433094060630538e-05}, {"id": 345, "seek": 194500, "start": 1949.0, "end": 1957.0, "text": " And then actually, I think it is the lighting module where we write. Yeah. So here you go. So it's showing here under the hood. Lightning is going to do this. It's the same. Okay.", "tokens": [50364, 286, 519, 13, 7010, 13, 400, 550, 264, 9577, 10088, 5112, 291, 264, 912, 551, 13, 1779, 13, 50564, 50564, 400, 550, 767, 11, 286, 519, 309, 307, 264, 9577, 10088, 689, 321, 2464, 13, 865, 13, 407, 510, 291, 352, 13, 407, 309, 311, 4099, 510, 833, 264, 13376, 13, 28848, 307, 516, 281, 360, 341, 13, 467, 311, 264, 912, 13, 1033, 13, 50964, 50964, 663, 321, 4114, 510, 13, 1033, 13, 1033, 13, 1033, 13, 663, 390, 452, 1168, 11, 767, 13, 407, 341, 307, 2293, 264, 551, 300, 321, 4114, 322, 264, 11, 322, 264, 21060, 13, 1779, 13, 407, 321, 362, 264, 2316, 3847, 293, 550, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.19248355367909306, "compression_ratio": 1.871244635193133, "no_speech_prob": 1.6433094060630538e-05}, {"id": 346, "seek": 194500, "start": 1957.0, "end": 1969.0, "text": " That we wrote here. Okay. Okay. Okay. That was my question, actually. So this is exactly the thing that we wrote on the, on the notebook. Right. So we have the model train and then", "tokens": [50364, 286, 519, 13, 7010, 13, 400, 550, 264, 9577, 10088, 5112, 291, 264, 912, 551, 13, 1779, 13, 50564, 50564, 400, 550, 767, 11, 286, 519, 309, 307, 264, 9577, 10088, 689, 321, 2464, 13, 865, 13, 407, 510, 291, 352, 13, 407, 309, 311, 4099, 510, 833, 264, 13376, 13, 28848, 307, 516, 281, 360, 341, 13, 467, 311, 264, 912, 13, 1033, 13, 50964, 50964, 663, 321, 4114, 510, 13, 1033, 13, 1033, 13, 1033, 13, 663, 390, 452, 1168, 11, 767, 13, 407, 341, 307, 2293, 264, 551, 300, 321, 4114, 322, 264, 11, 322, 264, 21060, 13, 1779, 13, 407, 321, 362, 264, 2316, 3847, 293, 550, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.19248355367909306, "compression_ratio": 1.871244635193133, "no_speech_prob": 1.6433094060630538e-05}, {"id": 347, "seek": 196900, "start": 1969.0, "end": 1984.0, "text": " you go back to the documentation. All right. So the other one. Yeah. So we have the model in the training version, right. Training mode. Then we enabled the gradients. And then I guess there is this", "tokens": [50364, 291, 352, 646, 281, 264, 14333, 13, 1057, 558, 13, 407, 264, 661, 472, 13, 865, 13, 407, 321, 362, 264, 2316, 294, 264, 3097, 3037, 11, 558, 13, 20620, 4391, 13, 1396, 321, 15172, 264, 2771, 2448, 13, 400, 550, 286, 2041, 456, 307, 341, 51114, 51114, 6816, 264, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.25686384130407264, "compression_ratio": 1.4822695035460993, "no_speech_prob": 2.8837988793384284e-05}, {"id": 348, "seek": 196900, "start": 1984.0, "end": 1987.0, "text": " saving the", "tokens": [50364, 291, 352, 646, 281, 264, 14333, 13, 1057, 558, 13, 407, 264, 661, 472, 13, 865, 13, 407, 321, 362, 264, 2316, 294, 264, 3097, 3037, 11, 558, 13, 20620, 4391, 13, 1396, 321, 15172, 264, 2771, 2448, 13, 400, 550, 286, 2041, 456, 307, 341, 51114, 51114, 6816, 264, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.25686384130407264, "compression_ratio": 1.4822695035460993, "no_speech_prob": 2.8837988793384284e-05}, {"id": 349, "seek": 198700, "start": 1987.0, "end": 1999.0, "text": " the output of the, from the training step, which is going to be the loss. Then, okay. We compute the backward pass and then we step and then the zero. Okay. Yeah. And as you see, you were passing. Oh, there's a bug here.", "tokens": [50364, 264, 5598, 295, 264, 11, 490, 264, 3097, 1823, 11, 597, 307, 516, 281, 312, 264, 4470, 13, 1396, 11, 1392, 13, 492, 14722, 264, 23897, 1320, 293, 550, 321, 1823, 293, 550, 264, 4018, 13, 1033, 13, 865, 13, 400, 382, 291, 536, 11, 291, 645, 8437, 13, 876, 11, 456, 311, 257, 7426, 510, 13, 50964, 50964, 639, 820, 445, 584, 15245, 510, 13, 1779, 13, 509, 434, 445, 8437, 264, 15245, 558, 689, 264, 3097, 1507, 13, 1033, 13, 407, 321, 434, 646, 510, 13, 407, 300, 390, 3097, 1507, 13, 44151, 540, 5028, 6545, 13, 407, 321, 434, 665, 281, 352, 13, 440, 21110, 307, 516, 281, 360, 341, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.20059087720967955, "compression_ratio": 1.7896825396825398, "no_speech_prob": 2.0460220184759237e-05}, {"id": 350, "seek": 198700, "start": 1999.0, "end": 2011.0, "text": " This should just say batch here. Right. You're just passing the batch right where the training stuff. Okay. So we're back here. So that was training stuff. Configure optimizer. So we're good to go. The trainer is going to do this.", "tokens": [50364, 264, 5598, 295, 264, 11, 490, 264, 3097, 1823, 11, 597, 307, 516, 281, 312, 264, 4470, 13, 1396, 11, 1392, 13, 492, 14722, 264, 23897, 1320, 293, 550, 321, 1823, 293, 550, 264, 4018, 13, 1033, 13, 865, 13, 400, 382, 291, 536, 11, 291, 645, 8437, 13, 876, 11, 456, 311, 257, 7426, 510, 13, 50964, 50964, 639, 820, 445, 584, 15245, 510, 13, 1779, 13, 509, 434, 445, 8437, 264, 15245, 558, 689, 264, 3097, 1507, 13, 1033, 13, 407, 321, 434, 646, 510, 13, 407, 300, 390, 3097, 1507, 13, 44151, 540, 5028, 6545, 13, 407, 321, 434, 665, 281, 352, 13, 440, 21110, 307, 516, 281, 360, 341, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.20059087720967955, "compression_ratio": 1.7896825396825398, "no_speech_prob": 2.0460220184759237e-05}, {"id": 351, "seek": 201100, "start": 2011.0, "end": 2025.0, "text": " So on colab tends to, tends to freeze because the update happens very fast to. So when you start training lightning and it's going to print a little progress bar and it's going to overwhelm the screen. So we want to slow that down.", "tokens": [50364, 407, 322, 1173, 455, 12258, 281, 11, 12258, 281, 15959, 570, 264, 5623, 2314, 588, 2370, 281, 13, 407, 562, 291, 722, 3097, 16589, 293, 309, 311, 516, 281, 4482, 257, 707, 4205, 2159, 293, 309, 311, 516, 281, 9103, 76, 264, 2568, 13, 407, 321, 528, 281, 2964, 300, 760, 13, 51064, 51064, 407, 718, 311, 1319, 300, 281, 818, 309, 1266, 13, 407, 300, 321, 434, 406, 20200, 767, 945, 13, 407, 300, 321, 434, 406, 20200, 264, 2568, 13, 400, 550, 321, 611, 643, 264, 2316, 11, 527, 1508, 9902, 558, 300, 321, 445, 4114, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.17771319046761225, "compression_ratio": 1.7872340425531914, "no_speech_prob": 1.147824150393717e-05}, {"id": 352, "seek": 201100, "start": 2025.0, "end": 2040.0, "text": " So let's change that to call it 10. So that we're not freezing actually 20. So that we're not freezing the screen. And then we also need the model, our classifier right that we just wrote.", "tokens": [50364, 407, 322, 1173, 455, 12258, 281, 11, 12258, 281, 15959, 570, 264, 5623, 2314, 588, 2370, 281, 13, 407, 562, 291, 722, 3097, 16589, 293, 309, 311, 516, 281, 4482, 257, 707, 4205, 2159, 293, 309, 311, 516, 281, 9103, 76, 264, 2568, 13, 407, 321, 528, 281, 2964, 300, 760, 13, 51064, 51064, 407, 718, 311, 1319, 300, 281, 818, 309, 1266, 13, 407, 300, 321, 434, 406, 20200, 767, 945, 13, 407, 300, 321, 434, 406, 20200, 264, 2568, 13, 400, 550, 321, 611, 643, 264, 2316, 11, 527, 1508, 9902, 558, 300, 321, 445, 4114, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.17771319046761225, "compression_ratio": 1.7872340425531914, "no_speech_prob": 1.147824150393717e-05}, {"id": 353, "seek": 204000, "start": 2040.0, "end": 2048.0, "text": " So here we go. This guy. And then we're going to, I like to swap these. So I'm going to add them.", "tokens": [50364, 407, 510, 321, 352, 13, 639, 2146, 13, 400, 550, 321, 434, 516, 281, 11, 286, 411, 281, 18135, 613, 13, 407, 286, 478, 516, 281, 909, 552, 13, 50764, 50764, 1396, 286, 478, 445, 516, 281, 3847, 456, 307, 257, 1508, 13, 50964, 50964, 865, 11, 370, 558, 586, 309, 311, 364, 5197, 295, 257, 1508, 13, 407, 35393, 13, 1033, 13, 407, 286, 478, 516, 281, 1884, 364, 5197, 295, 300, 286, 478, 516, 281, 818, 309, 9001, 13, 400, 550, 286, 478, 516, 281, 1320, 294, 452, 1508, 9902, 294, 510, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.19817695232352825, "compression_ratio": 1.8054054054054054, "no_speech_prob": 2.2827151042292826e-05}, {"id": 354, "seek": 204000, "start": 2048.0, "end": 2052.0, "text": " Then I'm just going to train there is a class.", "tokens": [50364, 407, 510, 321, 352, 13, 639, 2146, 13, 400, 550, 321, 434, 516, 281, 11, 286, 411, 281, 18135, 613, 13, 407, 286, 478, 516, 281, 909, 552, 13, 50764, 50764, 1396, 286, 478, 445, 516, 281, 3847, 456, 307, 257, 1508, 13, 50964, 50964, 865, 11, 370, 558, 586, 309, 311, 364, 5197, 295, 257, 1508, 13, 407, 35393, 13, 1033, 13, 407, 286, 478, 516, 281, 1884, 364, 5197, 295, 300, 286, 478, 516, 281, 818, 309, 9001, 13, 400, 550, 286, 478, 516, 281, 1320, 294, 452, 1508, 9902, 294, 510, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.19817695232352825, "compression_ratio": 1.8054054054054054, "no_speech_prob": 2.2827151042292826e-05}, {"id": 355, "seek": 204000, "start": 2052.0, "end": 2062.0, "text": " Yeah, so right now it's an instance of a class. So trainers. Okay. So I'm going to create an instance of that I'm going to call it fits. And then I'm going to pass in my classifier in here.", "tokens": [50364, 407, 510, 321, 352, 13, 639, 2146, 13, 400, 550, 321, 434, 516, 281, 11, 286, 411, 281, 18135, 613, 13, 407, 286, 478, 516, 281, 909, 552, 13, 50764, 50764, 1396, 286, 478, 445, 516, 281, 3847, 456, 307, 257, 1508, 13, 50964, 50964, 865, 11, 370, 558, 586, 309, 311, 364, 5197, 295, 257, 1508, 13, 407, 35393, 13, 1033, 13, 407, 286, 478, 516, 281, 1884, 364, 5197, 295, 300, 286, 478, 516, 281, 818, 309, 9001, 13, 400, 550, 286, 478, 516, 281, 1320, 294, 452, 1508, 9902, 294, 510, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.19817695232352825, "compression_ratio": 1.8054054054054054, "no_speech_prob": 2.2827151042292826e-05}, {"id": 356, "seek": 206200, "start": 2062.0, "end": 2073.0, "text": " And then we have the data. So, as I mentioned, I have just this train loader. So I'll show you first is train loader so I can pass that in. It's not a problem.", "tokens": [50364, 400, 550, 321, 362, 264, 1412, 13, 407, 11, 382, 286, 2835, 11, 286, 362, 445, 341, 3847, 3677, 260, 13, 407, 286, 603, 855, 291, 700, 307, 3847, 3677, 260, 370, 286, 393, 1320, 300, 294, 13, 467, 311, 406, 257, 1154, 13, 50914, 50914, 407, 286, 603, 1320, 294, 264, 3890, 1730, 27822, 1412, 3677, 260, 13, 400, 550, 341, 486, 445, 722, 3097, 13, 1779, 13, 51164, 51164, 876, 1699, 11, 6076, 13, 634, 311, 611, 1228, 264, 18407, 2435, 2074, 1143, 7908, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.23336676712874527, "compression_ratio": 1.5862068965517242, "no_speech_prob": 3.590066626202315e-05}, {"id": 357, "seek": 206200, "start": 2073.0, "end": 2078.0, "text": " So I'll pass in the regular pie torch data loader. And then this will just start training. Right.", "tokens": [50364, 400, 550, 321, 362, 264, 1412, 13, 407, 11, 382, 286, 2835, 11, 286, 362, 445, 341, 3847, 3677, 260, 13, 407, 286, 603, 855, 291, 700, 307, 3847, 3677, 260, 370, 286, 393, 1320, 300, 294, 13, 467, 311, 406, 257, 1154, 13, 50914, 50914, 407, 286, 603, 1320, 294, 264, 3890, 1730, 27822, 1412, 3677, 260, 13, 400, 550, 341, 486, 445, 722, 3097, 13, 1779, 13, 51164, 51164, 876, 1699, 11, 6076, 13, 634, 311, 611, 1228, 264, 18407, 2435, 2074, 1143, 7908, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.23336676712874527, "compression_ratio": 1.5862068965517242, "no_speech_prob": 3.590066626202315e-05}, {"id": 358, "seek": 206200, "start": 2078.0, "end": 2084.0, "text": " Oh wait, wow. He's also using the GPU available true used false.", "tokens": [50364, 400, 550, 321, 362, 264, 1412, 13, 407, 11, 382, 286, 2835, 11, 286, 362, 445, 341, 3847, 3677, 260, 13, 407, 286, 603, 855, 291, 700, 307, 3847, 3677, 260, 370, 286, 393, 1320, 300, 294, 13, 467, 311, 406, 257, 1154, 13, 50914, 50914, 407, 286, 603, 1320, 294, 264, 3890, 1730, 27822, 1412, 3677, 260, 13, 400, 550, 341, 486, 445, 722, 3097, 13, 1779, 13, 51164, 51164, 876, 1699, 11, 6076, 13, 634, 311, 611, 1228, 264, 18407, 2435, 2074, 1143, 7908, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.23336676712874527, "compression_ratio": 1.5862068965517242, "no_speech_prob": 3.590066626202315e-05}, {"id": 359, "seek": 208400, "start": 2084.0, "end": 2095.0, "text": " Yeah, and actually we give you a warning we're saying hey you have a GPU but you're not using it. Right. So we have we give a good experience there. Anyway, so you see this thing is training.", "tokens": [50364, 865, 11, 293, 767, 321, 976, 291, 257, 9164, 321, 434, 1566, 4177, 291, 362, 257, 18407, 457, 291, 434, 406, 1228, 309, 13, 1779, 13, 407, 321, 362, 321, 976, 257, 665, 1752, 456, 13, 5684, 11, 370, 291, 536, 341, 551, 307, 3097, 13, 50914, 50914, 286, 478, 516, 281, 764, 300, 1412, 10088, 558, 300, 321, 2942, 2602, 11, 370, 286, 445, 4678, 300, 294, 13, 51164, 51164, 400, 309, 311, 516, 281, 312, 264, 912, 5065, 13, 1779, 13, 407, 291, 500, 380, 362, 281, 2028, 365, 309, 11, 309, 3255, 281, 2235, 484, 264, 3097, 13, 1033, 11, 264, 3097, 37741, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1930385108466621, "compression_ratio": 1.6932270916334662, "no_speech_prob": 3.7050489481771365e-05}, {"id": 360, "seek": 208400, "start": 2095.0, "end": 2100.0, "text": " I'm going to use that data module right that we created instead, so I just passed that in.", "tokens": [50364, 865, 11, 293, 767, 321, 976, 291, 257, 9164, 321, 434, 1566, 4177, 291, 362, 257, 18407, 457, 291, 434, 406, 1228, 309, 13, 1779, 13, 407, 321, 362, 321, 976, 257, 665, 1752, 456, 13, 5684, 11, 370, 291, 536, 341, 551, 307, 3097, 13, 50914, 50914, 286, 478, 516, 281, 764, 300, 1412, 10088, 558, 300, 321, 2942, 2602, 11, 370, 286, 445, 4678, 300, 294, 13, 51164, 51164, 400, 309, 311, 516, 281, 312, 264, 912, 5065, 13, 1779, 13, 407, 291, 500, 380, 362, 281, 2028, 365, 309, 11, 309, 3255, 281, 2235, 484, 264, 3097, 13, 1033, 11, 264, 3097, 37741, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1930385108466621, "compression_ratio": 1.6932270916334662, "no_speech_prob": 3.7050489481771365e-05}, {"id": 361, "seek": 208400, "start": 2100.0, "end": 2110.0, "text": " And it's going to be the same effects. Right. So you don't have to deal with it, it knows to pull out the training. Okay, the training splits.", "tokens": [50364, 865, 11, 293, 767, 321, 976, 291, 257, 9164, 321, 434, 1566, 4177, 291, 362, 257, 18407, 457, 291, 434, 406, 1228, 309, 13, 1779, 13, 407, 321, 362, 321, 976, 257, 665, 1752, 456, 13, 5684, 11, 370, 291, 536, 341, 551, 307, 3097, 13, 50914, 50914, 286, 478, 516, 281, 764, 300, 1412, 10088, 558, 300, 321, 2942, 2602, 11, 370, 286, 445, 4678, 300, 294, 13, 51164, 51164, 400, 309, 311, 516, 281, 312, 264, 912, 5065, 13, 1779, 13, 407, 291, 500, 380, 362, 281, 2028, 365, 309, 11, 309, 3255, 281, 2235, 484, 264, 3097, 13, 1033, 11, 264, 3097, 37741, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1930385108466621, "compression_ratio": 1.6932270916334662, "no_speech_prob": 3.7050489481771365e-05}, {"id": 362, "seek": 211000, "start": 2110.0, "end": 2120.0, "text": " So it's going, let's give it a few seconds. So also here TPU available. You can also use TPUs you said before right. Yeah, correct. Wow.", "tokens": [50364, 407, 309, 311, 516, 11, 718, 311, 976, 309, 257, 1326, 3949, 13, 407, 611, 510, 314, 8115, 2435, 13, 509, 393, 611, 764, 314, 8115, 82, 291, 848, 949, 558, 13, 865, 11, 3006, 13, 3153, 13, 50864, 50864, 1033, 11, 286, 478, 6369, 13, 286, 478, 445, 516, 281, 764, 264, 18407, 1673, 370, 321, 362, 264, 18407, 13, 51064, 51064, 400, 286, 478, 406, 516, 281, 1319, 452, 3089, 286, 478, 445, 516, 281, 992, 18407, 6915, 281, 472, 510, 13, 400, 586, 321, 434, 3097, 322, 257, 18407, 13, 400, 291, 603, 536, 300, 309, 311, 709, 4663, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1703381939469097, "compression_ratio": 1.6294642857142858, "no_speech_prob": 1.4284592907642946e-05}, {"id": 363, "seek": 211000, "start": 2120.0, "end": 2124.0, "text": " Okay, I'm curious. I'm just going to use the GPU though so we have the GPU.", "tokens": [50364, 407, 309, 311, 516, 11, 718, 311, 976, 309, 257, 1326, 3949, 13, 407, 611, 510, 314, 8115, 2435, 13, 509, 393, 611, 764, 314, 8115, 82, 291, 848, 949, 558, 13, 865, 11, 3006, 13, 3153, 13, 50864, 50864, 1033, 11, 286, 478, 6369, 13, 286, 478, 445, 516, 281, 764, 264, 18407, 1673, 370, 321, 362, 264, 18407, 13, 51064, 51064, 400, 286, 478, 406, 516, 281, 1319, 452, 3089, 286, 478, 445, 516, 281, 992, 18407, 6915, 281, 472, 510, 13, 400, 586, 321, 434, 3097, 322, 257, 18407, 13, 400, 291, 603, 536, 300, 309, 311, 709, 4663, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1703381939469097, "compression_ratio": 1.6294642857142858, "no_speech_prob": 1.4284592907642946e-05}, {"id": 364, "seek": 211000, "start": 2124.0, "end": 2133.0, "text": " And I'm not going to change my code I'm just going to set GPU equals to one here. And now we're training on a GPU. And you'll see that it's much faster.", "tokens": [50364, 407, 309, 311, 516, 11, 718, 311, 976, 309, 257, 1326, 3949, 13, 407, 611, 510, 314, 8115, 2435, 13, 509, 393, 611, 764, 314, 8115, 82, 291, 848, 949, 558, 13, 865, 11, 3006, 13, 3153, 13, 50864, 50864, 1033, 11, 286, 478, 6369, 13, 286, 478, 445, 516, 281, 764, 264, 18407, 1673, 370, 321, 362, 264, 18407, 13, 51064, 51064, 400, 286, 478, 406, 516, 281, 1319, 452, 3089, 286, 478, 445, 516, 281, 992, 18407, 6915, 281, 472, 510, 13, 400, 586, 321, 434, 3097, 322, 257, 18407, 13, 400, 291, 603, 536, 300, 309, 311, 709, 4663, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1703381939469097, "compression_ratio": 1.6294642857142858, "no_speech_prob": 1.4284592907642946e-05}, {"id": 365, "seek": 213300, "start": 2133.0, "end": 2141.0, "text": " Oh wow. So we try as well on the TPU and just for sake of curiosity. We can.", "tokens": [50364, 876, 6076, 13, 407, 321, 853, 382, 731, 322, 264, 314, 8115, 293, 445, 337, 9717, 295, 18769, 13, 492, 393, 13, 50764, 50764, 467, 311, 445, 291, 362, 281, 3625, 341, 1783, 11435, 6405, 13, 1033, 11, 1392, 550, 958, 565, 13, 51014, 51014, 407, 264, 43012, 365, 314, 8115, 82, 307, 300, 3329, 307, 1364, 1687, 1152, 281, 652, 988, 300, 436, 393, 1406, 364, 14146, 40, 27822, 1214, 281, 1406, 314, 8115, 82, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.17862256368001303, "compression_ratio": 1.43, "no_speech_prob": 3.535210635163821e-05}, {"id": 366, "seek": 213300, "start": 2141.0, "end": 2146.0, "text": " It's just you have to install this XLA library. Okay, okay then next time.", "tokens": [50364, 876, 6076, 13, 407, 321, 853, 382, 731, 322, 264, 314, 8115, 293, 445, 337, 9717, 295, 18769, 13, 492, 393, 13, 50764, 50764, 467, 311, 445, 291, 362, 281, 3625, 341, 1783, 11435, 6405, 13, 1033, 11, 1392, 550, 958, 565, 13, 51014, 51014, 407, 264, 43012, 365, 314, 8115, 82, 307, 300, 3329, 307, 1364, 1687, 1152, 281, 652, 988, 300, 436, 393, 1406, 364, 14146, 40, 27822, 1214, 281, 1406, 314, 8115, 82, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.17862256368001303, "compression_ratio": 1.43, "no_speech_prob": 3.535210635163821e-05}, {"id": 367, "seek": 213300, "start": 2146.0, "end": 2156.0, "text": " So the caveat with TPUs is that Google is working super hard to make sure that they can support an MPI torch together to support TPUs.", "tokens": [50364, 876, 6076, 13, 407, 321, 853, 382, 731, 322, 264, 314, 8115, 293, 445, 337, 9717, 295, 18769, 13, 492, 393, 13, 50764, 50764, 467, 311, 445, 291, 362, 281, 3625, 341, 1783, 11435, 6405, 13, 1033, 11, 1392, 550, 958, 565, 13, 51014, 51014, 407, 264, 43012, 365, 314, 8115, 82, 307, 300, 3329, 307, 1364, 1687, 1152, 281, 652, 988, 300, 436, 393, 1406, 364, 14146, 40, 27822, 1214, 281, 1406, 314, 8115, 82, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.17862256368001303, "compression_ratio": 1.43, "no_speech_prob": 3.535210635163821e-05}, {"id": 368, "seek": 215600, "start": 2156.0, "end": 2169.0, "text": " So the experience is not quite there yet you still have to install this XLA library, and if you go to the lightning docs it'll show you. But yeah, once you do that you just change this to, I think it's TPU cores.", "tokens": [50364, 407, 264, 1752, 307, 406, 1596, 456, 1939, 291, 920, 362, 281, 3625, 341, 1783, 11435, 6405, 11, 293, 498, 291, 352, 281, 264, 16589, 45623, 309, 603, 855, 291, 13, 583, 1338, 11, 1564, 291, 360, 300, 291, 445, 1319, 341, 281, 11, 286, 519, 309, 311, 314, 8115, 24826, 13, 51014, 51014, 400, 550, 291, 992, 309, 281, 472, 420, 3180, 420, 2035, 291, 528, 13, 400, 550, 291, 603, 312, 3097, 322, 264, 314, 8115, 13, 1033, 11, 456, 366, 7140, 33788, 322, 264, 3144, 281, 855, 300, 13, 1033, 11, 1392, 11, 1392, 11, 1392, 13, 51414, 51414, 1449, 281, 312, 988, 300, 13, 865, 11, 286, 445, 528, 281, 1066, 341, 472, 1879, 322, 264, 3097, 13, 1033, 11, 1392, 13, 12024, 11, 2489, 11, 2489, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20880638851838954, "compression_ratio": 1.7864768683274022, "no_speech_prob": 4.1978662920882925e-05}, {"id": 369, "seek": 215600, "start": 2169.0, "end": 2177.0, "text": " And then you set it to one or eight or whatever you want. And then you'll be training on the TPU. Okay, there are plenty demos on the website to show that. Okay, okay, okay, okay.", "tokens": [50364, 407, 264, 1752, 307, 406, 1596, 456, 1939, 291, 920, 362, 281, 3625, 341, 1783, 11435, 6405, 11, 293, 498, 291, 352, 281, 264, 16589, 45623, 309, 603, 855, 291, 13, 583, 1338, 11, 1564, 291, 360, 300, 291, 445, 1319, 341, 281, 11, 286, 519, 309, 311, 314, 8115, 24826, 13, 51014, 51014, 400, 550, 291, 992, 309, 281, 472, 420, 3180, 420, 2035, 291, 528, 13, 400, 550, 291, 603, 312, 3097, 322, 264, 314, 8115, 13, 1033, 11, 456, 366, 7140, 33788, 322, 264, 3144, 281, 855, 300, 13, 1033, 11, 1392, 11, 1392, 11, 1392, 13, 51414, 51414, 1449, 281, 312, 988, 300, 13, 865, 11, 286, 445, 528, 281, 1066, 341, 472, 1879, 322, 264, 3097, 13, 1033, 11, 1392, 13, 12024, 11, 2489, 11, 2489, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20880638851838954, "compression_ratio": 1.7864768683274022, "no_speech_prob": 4.1978662920882925e-05}, {"id": 370, "seek": 215600, "start": 2177.0, "end": 2183.0, "text": " Just to be sure that. Yeah, I just want to keep this one focus on the training. Okay, okay. Fine, fine, fine.", "tokens": [50364, 407, 264, 1752, 307, 406, 1596, 456, 1939, 291, 920, 362, 281, 3625, 341, 1783, 11435, 6405, 11, 293, 498, 291, 352, 281, 264, 16589, 45623, 309, 603, 855, 291, 13, 583, 1338, 11, 1564, 291, 360, 300, 291, 445, 1319, 341, 281, 11, 286, 519, 309, 311, 314, 8115, 24826, 13, 51014, 51014, 400, 550, 291, 992, 309, 281, 472, 420, 3180, 420, 2035, 291, 528, 13, 400, 550, 291, 603, 312, 3097, 322, 264, 314, 8115, 13, 1033, 11, 456, 366, 7140, 33788, 322, 264, 3144, 281, 855, 300, 13, 1033, 11, 1392, 11, 1392, 11, 1392, 13, 51414, 51414, 1449, 281, 312, 988, 300, 13, 865, 11, 286, 445, 528, 281, 1066, 341, 472, 1879, 322, 264, 3097, 13, 1033, 11, 1392, 13, 12024, 11, 2489, 11, 2489, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.20880638851838954, "compression_ratio": 1.7864768683274022, "no_speech_prob": 4.1978662920882925e-05}, {"id": 371, "seek": 218300, "start": 2183.0, "end": 2191.0, "text": " Okay, so wait while we were talking to train so see the losses at 1.95. Oh wow, super long. So, let's just see what happened.", "tokens": [50364, 1033, 11, 370, 1699, 1339, 321, 645, 1417, 281, 3847, 370, 536, 264, 15352, 412, 502, 13, 15718, 13, 876, 6076, 11, 1687, 938, 13, 407, 11, 718, 311, 445, 536, 437, 2011, 13, 50764, 50764, 407, 718, 311, 536, 13, 961, 311, 536, 577, 309, 3264, 370, 1400, 13, 407, 16589, 7829, 20820, 337, 291, 6772, 293, 286, 393, 445, 4025, 40863, 589, 281, 23273, 13, 876, 6076, 534, 8992, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.25381311617399516, "compression_ratio": 1.4554455445544554, "no_speech_prob": 0.00012706858979072422}, {"id": 372, "seek": 218300, "start": 2191.0, "end": 2204.0, "text": " So let's see. Let's see how it learned so far. So lightning creates logs for you automatically and I can just launch tensor work to visualize. Oh wow really impressive.", "tokens": [50364, 1033, 11, 370, 1699, 1339, 321, 645, 1417, 281, 3847, 370, 536, 264, 15352, 412, 502, 13, 15718, 13, 876, 6076, 11, 1687, 938, 13, 407, 11, 718, 311, 445, 536, 437, 2011, 13, 50764, 50764, 407, 718, 311, 536, 13, 961, 311, 536, 577, 309, 3264, 370, 1400, 13, 407, 16589, 7829, 20820, 337, 291, 6772, 293, 286, 393, 445, 4025, 40863, 589, 281, 23273, 13, 876, 6076, 534, 8992, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.25381311617399516, "compression_ratio": 1.4554455445544554, "no_speech_prob": 0.00012706858979072422}, {"id": 373, "seek": 220400, "start": 2204.0, "end": 2213.0, "text": " And here we are, tensor board shoots up, and now you see, oh you didn't log anything. Yeah, so we have to log something.", "tokens": [50364, 400, 510, 321, 366, 11, 40863, 3150, 20704, 493, 11, 293, 586, 291, 536, 11, 1954, 291, 994, 380, 3565, 1340, 13, 865, 11, 370, 321, 362, 281, 3565, 746, 13, 50814, 50814, 407, 718, 311, 3565, 718, 311, 445, 3565, 341, 4470, 558, 370, 286, 478, 516, 281, 584, 2698, 5893, 3565, 13, 400, 550, 286, 603, 584, 3847, 4470, 575, 2845, 264, 4470, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.2099395615713937, "compression_ratio": 1.5471698113207548, "no_speech_prob": 7.182902663771529e-06}, {"id": 374, "seek": 220400, "start": 2213.0, "end": 2221.0, "text": " So let's log let's just log this loss right so I'm going to say self dot log. And then I'll say train loss has send the loss.", "tokens": [50364, 400, 510, 321, 366, 11, 40863, 3150, 20704, 493, 11, 293, 586, 291, 536, 11, 1954, 291, 994, 380, 3565, 1340, 13, 865, 11, 370, 321, 362, 281, 3565, 746, 13, 50814, 50814, 407, 718, 311, 3565, 718, 311, 445, 3565, 341, 4470, 558, 370, 286, 478, 516, 281, 584, 2698, 5893, 3565, 13, 400, 550, 286, 603, 584, 3847, 4470, 575, 2845, 264, 4470, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.2099395615713937, "compression_ratio": 1.5471698113207548, "no_speech_prob": 7.182902663771529e-06}, {"id": 375, "seek": 222100, "start": 2221.0, "end": 2235.0, "text": " I also wanted to accuracy so let me just pull out our fancy metrics library from fighters lightning metrics dot functional import accuracy. Right. And then we're going to also log the accuracy.", "tokens": [50364, 286, 611, 1415, 281, 14170, 370, 718, 385, 445, 2235, 484, 527, 10247, 16367, 6405, 490, 19714, 16589, 16367, 5893, 11745, 974, 14170, 13, 1779, 13, 400, 550, 321, 434, 516, 281, 611, 3565, 264, 14170, 13, 51064, 51064, 407, 11, 370, 2698, 5893, 3565, 307, 746, 307, 257, 3170, 294, 264, 2316, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.22769656674615268, "compression_ratio": 1.5180722891566265, "no_speech_prob": 6.962052339076763e-06}, {"id": 376, "seek": 222100, "start": 2235.0, "end": 2243.0, "text": " So, so self dot log is something is a method in the model.", "tokens": [50364, 286, 611, 1415, 281, 14170, 370, 718, 385, 445, 2235, 484, 527, 10247, 16367, 6405, 490, 19714, 16589, 16367, 5893, 11745, 974, 14170, 13, 1779, 13, 400, 550, 321, 434, 516, 281, 611, 3565, 264, 14170, 13, 51064, 51064, 407, 11, 370, 2698, 5893, 3565, 307, 746, 307, 257, 3170, 294, 264, 2316, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.22769656674615268, "compression_ratio": 1.5180722891566265, "no_speech_prob": 6.962052339076763e-06}, {"id": 377, "seek": 224300, "start": 2243.0, "end": 2258.0, "text": " So it's a method of the model of the lighting module. So, what's cool is that soft out log so you're turning on one GP right now but when you start training on eight 200, whatever, you have to sync logs across GPUs and you have to like, you know, calculate metrics correctly", "tokens": [50364, 407, 309, 311, 257, 3170, 295, 264, 2316, 295, 264, 9577, 10088, 13, 407, 11, 437, 311, 1627, 307, 300, 2787, 484, 3565, 370, 291, 434, 6246, 322, 472, 26039, 558, 586, 457, 562, 291, 722, 3097, 322, 3180, 2331, 11, 2035, 11, 291, 362, 281, 20271, 20820, 2108, 18407, 82, 293, 291, 362, 281, 411, 11, 291, 458, 11, 8873, 16367, 8944, 51114, 51114, 411, 291, 393, 4274, 14170, 457, 291, 393, 380, 4274, 746, 411, 497, 14766, 420, 746, 411, 300, 13, 407, 11, 16589, 18722, 439, 300, 12631, 1507, 337, 291, 293, 28148, 293, 562, 281, 360, 309, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.16356874861807194, "compression_ratio": 1.7245283018867925, "no_speech_prob": 4.356723820819752e-06}, {"id": 378, "seek": 224300, "start": 2258.0, "end": 2268.0, "text": " like you can average accuracy but you can't average something like RLC or something like that. So, lightning handles all that distributed stuff for you and sinking and when to do it.", "tokens": [50364, 407, 309, 311, 257, 3170, 295, 264, 2316, 295, 264, 9577, 10088, 13, 407, 11, 437, 311, 1627, 307, 300, 2787, 484, 3565, 370, 291, 434, 6246, 322, 472, 26039, 558, 586, 457, 562, 291, 722, 3097, 322, 3180, 2331, 11, 2035, 11, 291, 362, 281, 20271, 20820, 2108, 18407, 82, 293, 291, 362, 281, 411, 11, 291, 458, 11, 8873, 16367, 8944, 51114, 51114, 411, 291, 393, 4274, 14170, 457, 291, 393, 380, 4274, 746, 411, 497, 14766, 420, 746, 411, 300, 13, 407, 11, 16589, 18722, 439, 300, 12631, 1507, 337, 291, 293, 28148, 293, 562, 281, 360, 309, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.16356874861807194, "compression_ratio": 1.7245283018867925, "no_speech_prob": 4.356723820819752e-06}, {"id": 379, "seek": 226800, "start": 2268.0, "end": 2284.0, "text": " If you saw that log and everything else. I think you made a mistake there. So this is something that I think every time it's very it's very like tricky mistake. You didn't use attach detached that loss right so it looks like you're saving your login the whole", "tokens": [50364, 759, 291, 1866, 300, 3565, 293, 1203, 1646, 13, 286, 519, 291, 1027, 257, 6146, 456, 13, 407, 341, 307, 746, 300, 286, 519, 633, 565, 309, 311, 588, 309, 311, 588, 411, 12414, 6146, 13, 509, 994, 380, 764, 5085, 42050, 300, 4470, 558, 370, 309, 1542, 411, 291, 434, 6816, 428, 24276, 264, 1379, 51164, 51164, 28270, 4295, 456, 13, 51314, 51314, 407, 562, 291, 3565, 294, 16589, 486, 43245, 1340, 13, 876, 6076, 11, 534, 13, 865, 11, 291, 500, 380, 362, 281, 3292, 466, 309, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.21646493546506193, "compression_ratio": 1.672340425531915, "no_speech_prob": 4.1331997636007145e-05}, {"id": 380, "seek": 226800, "start": 2284.0, "end": 2287.0, "text": " computational graph there.", "tokens": [50364, 759, 291, 1866, 300, 3565, 293, 1203, 1646, 13, 286, 519, 291, 1027, 257, 6146, 456, 13, 407, 341, 307, 746, 300, 286, 519, 633, 565, 309, 311, 588, 309, 311, 588, 411, 12414, 6146, 13, 509, 994, 380, 764, 5085, 42050, 300, 4470, 558, 370, 309, 1542, 411, 291, 434, 6816, 428, 24276, 264, 1379, 51164, 51164, 28270, 4295, 456, 13, 51314, 51314, 407, 562, 291, 3565, 294, 16589, 486, 43245, 1340, 13, 876, 6076, 11, 534, 13, 865, 11, 291, 500, 380, 362, 281, 3292, 466, 309, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.21646493546506193, "compression_ratio": 1.672340425531915, "no_speech_prob": 4.1331997636007145e-05}, {"id": 381, "seek": 226800, "start": 2287.0, "end": 2293.0, "text": " So when you log in lightning will detach anything. Oh wow, really. Yeah, you don't have to worry about it.", "tokens": [50364, 759, 291, 1866, 300, 3565, 293, 1203, 1646, 13, 286, 519, 291, 1027, 257, 6146, 456, 13, 407, 341, 307, 746, 300, 286, 519, 633, 565, 309, 311, 588, 309, 311, 588, 411, 12414, 6146, 13, 509, 994, 380, 764, 5085, 42050, 300, 4470, 558, 370, 309, 1542, 411, 291, 434, 6816, 428, 24276, 264, 1379, 51164, 51164, 28270, 4295, 456, 13, 51314, 51314, 407, 562, 291, 3565, 294, 16589, 486, 43245, 1340, 13, 876, 6076, 11, 534, 13, 865, 11, 291, 500, 380, 362, 281, 3292, 466, 309, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.21646493546506193, "compression_ratio": 1.672340425531915, "no_speech_prob": 4.1331997636007145e-05}, {"id": 382, "seek": 229300, "start": 2293.0, "end": 2302.0, "text": " Okay, so nice. We try to make sure that you're. We keep people making mistakes.", "tokens": [50364, 1033, 11, 370, 1481, 13, 492, 853, 281, 652, 988, 300, 291, 434, 13, 492, 1066, 561, 1455, 8038, 13, 50814, 50814, 1033, 11, 370, 370, 718, 311, 4017, 370, 321, 434, 3097, 11, 321, 434, 27991, 264, 3097, 14170, 382, 731, 586, 11, 293, 550, 264, 3097, 4470, 13, 51164, 51164, 865, 13, 51264, 51264, 407, 321, 362, 341, 2446, 510, 13, 407, 321, 434, 406, 7363, 990, 558, 586, 13, 407, 16589, 307, 1566, 4177, 11, 291, 1320, 294, 466, 300, 257, 5063, 457, 291, 2378, 380, 12270, 257, 24071, 1823, 558, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.21290230028557056, "compression_ratio": 1.6977777777777778, "no_speech_prob": 5.648748992825858e-05}, {"id": 383, "seek": 229300, "start": 2302.0, "end": 2309.0, "text": " Okay, so so let's lock so we're training, we're logging the training accuracy as well now, and then the training loss.", "tokens": [50364, 1033, 11, 370, 1481, 13, 492, 853, 281, 652, 988, 300, 291, 434, 13, 492, 1066, 561, 1455, 8038, 13, 50814, 50814, 1033, 11, 370, 370, 718, 311, 4017, 370, 321, 434, 3097, 11, 321, 434, 27991, 264, 3097, 14170, 382, 731, 586, 11, 293, 550, 264, 3097, 4470, 13, 51164, 51164, 865, 13, 51264, 51264, 407, 321, 362, 341, 2446, 510, 13, 407, 321, 434, 406, 7363, 990, 558, 586, 13, 407, 16589, 307, 1566, 4177, 11, 291, 1320, 294, 466, 300, 257, 5063, 457, 291, 2378, 380, 12270, 257, 24071, 1823, 558, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.21290230028557056, "compression_ratio": 1.6977777777777778, "no_speech_prob": 5.648748992825858e-05}, {"id": 384, "seek": 229300, "start": 2309.0, "end": 2311.0, "text": " Yeah.", "tokens": [50364, 1033, 11, 370, 1481, 13, 492, 853, 281, 652, 988, 300, 291, 434, 13, 492, 1066, 561, 1455, 8038, 13, 50814, 50814, 1033, 11, 370, 370, 718, 311, 4017, 370, 321, 434, 3097, 11, 321, 434, 27991, 264, 3097, 14170, 382, 731, 586, 11, 293, 550, 264, 3097, 4470, 13, 51164, 51164, 865, 13, 51264, 51264, 407, 321, 362, 341, 2446, 510, 13, 407, 321, 434, 406, 7363, 990, 558, 586, 13, 407, 16589, 307, 1566, 4177, 11, 291, 1320, 294, 466, 300, 257, 5063, 457, 291, 2378, 380, 12270, 257, 24071, 1823, 558, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.21290230028557056, "compression_ratio": 1.6977777777777778, "no_speech_prob": 5.648748992825858e-05}, {"id": 385, "seek": 229300, "start": 2311.0, "end": 2320.0, "text": " So we have this morning here. So we're not validating right now. So lightning is saying hey, you pass in about that a letter but you haven't implemented a validation step right.", "tokens": [50364, 1033, 11, 370, 1481, 13, 492, 853, 281, 652, 988, 300, 291, 434, 13, 492, 1066, 561, 1455, 8038, 13, 50814, 50814, 1033, 11, 370, 370, 718, 311, 4017, 370, 321, 434, 3097, 11, 321, 434, 27991, 264, 3097, 14170, 382, 731, 586, 11, 293, 550, 264, 3097, 4470, 13, 51164, 51164, 865, 13, 51264, 51264, 407, 321, 362, 341, 2446, 510, 13, 407, 321, 434, 406, 7363, 990, 558, 586, 13, 407, 16589, 307, 1566, 4177, 11, 291, 1320, 294, 466, 300, 257, 5063, 457, 291, 2378, 380, 12270, 257, 24071, 1823, 558, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.21290230028557056, "compression_ratio": 1.6977777777777778, "no_speech_prob": 5.648748992825858e-05}, {"id": 386, "seek": 232000, "start": 2320.0, "end": 2326.0, "text": " So we only have a train loop, and it's saying that because the data module has a validation split attached to it right.", "tokens": [50364, 407, 321, 787, 362, 257, 3847, 6367, 11, 293, 309, 311, 1566, 300, 570, 264, 1412, 10088, 575, 257, 24071, 7472, 8570, 281, 309, 558, 13, 50664, 50664, 583, 300, 311, 1392, 286, 500, 380, 528, 281, 29562, 558, 586, 13, 407, 286, 478, 445, 516, 281, 3847, 281, 536, 437, 311, 2737, 365, 341, 2489, 15164, 551, 13, 51014, 51014, 10328, 11, 498, 291, 645, 1228, 411, 264, 1331, 264, 3894, 3037, 264, 472, 365, 264, 1730, 27822, 1412, 992, 13, 51364, 51364, 509, 632, 281, 992, 11, 291, 632, 281, 1320, 1293, 264, 3097, 34702, 399, 14759, 13, 865, 11, 370, 291, 362, 281, 360, 341, 11, 293, 550, 1320, 294, 466, 3677, 260, 13, 876, 11, 286, 536, 13, 1033, 11, 370, 291, 362, 281, 2845, 552, 14759, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14072289186365464, "compression_ratio": 1.8033333333333332, "no_speech_prob": 9.515779311186634e-06}, {"id": 387, "seek": 232000, "start": 2326.0, "end": 2333.0, "text": " But that's okay I don't want to validate right now. So I'm just going to train to see what's happening with this fine tuning thing.", "tokens": [50364, 407, 321, 787, 362, 257, 3847, 6367, 11, 293, 309, 311, 1566, 300, 570, 264, 1412, 10088, 575, 257, 24071, 7472, 8570, 281, 309, 558, 13, 50664, 50664, 583, 300, 311, 1392, 286, 500, 380, 528, 281, 29562, 558, 586, 13, 407, 286, 478, 445, 516, 281, 3847, 281, 536, 437, 311, 2737, 365, 341, 2489, 15164, 551, 13, 51014, 51014, 10328, 11, 498, 291, 645, 1228, 411, 264, 1331, 264, 3894, 3037, 264, 472, 365, 264, 1730, 27822, 1412, 992, 13, 51364, 51364, 509, 632, 281, 992, 11, 291, 632, 281, 1320, 1293, 264, 3097, 34702, 399, 14759, 13, 865, 11, 370, 291, 362, 281, 360, 341, 11, 293, 550, 1320, 294, 466, 3677, 260, 13, 876, 11, 286, 536, 13, 1033, 11, 370, 291, 362, 281, 2845, 552, 14759, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14072289186365464, "compression_ratio": 1.8033333333333332, "no_speech_prob": 9.515779311186634e-06}, {"id": 388, "seek": 232000, "start": 2333.0, "end": 2340.0, "text": " Otherwise, if you were using like the old the previous version the one with the pie torch data set.", "tokens": [50364, 407, 321, 787, 362, 257, 3847, 6367, 11, 293, 309, 311, 1566, 300, 570, 264, 1412, 10088, 575, 257, 24071, 7472, 8570, 281, 309, 558, 13, 50664, 50664, 583, 300, 311, 1392, 286, 500, 380, 528, 281, 29562, 558, 586, 13, 407, 286, 478, 445, 516, 281, 3847, 281, 536, 437, 311, 2737, 365, 341, 2489, 15164, 551, 13, 51014, 51014, 10328, 11, 498, 291, 645, 1228, 411, 264, 1331, 264, 3894, 3037, 264, 472, 365, 264, 1730, 27822, 1412, 992, 13, 51364, 51364, 509, 632, 281, 992, 11, 291, 632, 281, 1320, 1293, 264, 3097, 34702, 399, 14759, 13, 865, 11, 370, 291, 362, 281, 360, 341, 11, 293, 550, 1320, 294, 466, 3677, 260, 13, 876, 11, 286, 536, 13, 1033, 11, 370, 291, 362, 281, 2845, 552, 14759, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14072289186365464, "compression_ratio": 1.8033333333333332, "no_speech_prob": 9.515779311186634e-06}, {"id": 389, "seek": 232000, "start": 2340.0, "end": 2349.0, "text": " You had to set, you had to pass both the training invalidation separately. Yeah, so you have to do this, and then pass in about loader. Oh, I see. Okay, so you have to send them separately.", "tokens": [50364, 407, 321, 787, 362, 257, 3847, 6367, 11, 293, 309, 311, 1566, 300, 570, 264, 1412, 10088, 575, 257, 24071, 7472, 8570, 281, 309, 558, 13, 50664, 50664, 583, 300, 311, 1392, 286, 500, 380, 528, 281, 29562, 558, 586, 13, 407, 286, 478, 445, 516, 281, 3847, 281, 536, 437, 311, 2737, 365, 341, 2489, 15164, 551, 13, 51014, 51014, 10328, 11, 498, 291, 645, 1228, 411, 264, 1331, 264, 3894, 3037, 264, 472, 365, 264, 1730, 27822, 1412, 992, 13, 51364, 51364, 509, 632, 281, 992, 11, 291, 632, 281, 1320, 1293, 264, 3097, 34702, 399, 14759, 13, 865, 11, 370, 291, 362, 281, 360, 341, 11, 293, 550, 1320, 294, 466, 3677, 260, 13, 876, 11, 286, 536, 13, 1033, 11, 370, 291, 362, 281, 2845, 552, 14759, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14072289186365464, "compression_ratio": 1.8033333333333332, "no_speech_prob": 9.515779311186634e-06}, {"id": 390, "seek": 234900, "start": 2349.0, "end": 2357.0, "text": " You have to send them separately but instead in this case you send like a class, which I see. Okay, makes sense.", "tokens": [50364, 509, 362, 281, 2845, 552, 14759, 457, 2602, 294, 341, 1389, 291, 2845, 411, 257, 1508, 11, 597, 286, 536, 13, 1033, 11, 1669, 2020, 13, 50764, 50764, 1033, 11, 370, 437, 307, 300, 364, 30992, 339, 293, 257, 1922, 13, 3769, 13, 961, 311, 536, 437, 2314, 13, 51014, 51014, 961, 311, 25628, 341, 2146, 13, 876, 11, 630, 309, 3677, 1217, 337, 385, 13, 5490, 13, 51264, 51264, 2704, 286, 500, 380, 362, 281, 25628, 309, 13, 961, 311, 536, 498, 300, 311, 2275, 300, 309, 311, 551, 965, 11, 994, 380, 362, 281, 25628, 309, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.22738486354790846, "compression_ratio": 1.6872037914691944, "no_speech_prob": 3.474149707471952e-05}, {"id": 391, "seek": 234900, "start": 2357.0, "end": 2362.0, "text": " Okay, so what is that an epoch and a half. Great. Let's see what happens.", "tokens": [50364, 509, 362, 281, 2845, 552, 14759, 457, 2602, 294, 341, 1389, 291, 2845, 411, 257, 1508, 11, 597, 286, 536, 13, 1033, 11, 1669, 2020, 13, 50764, 50764, 1033, 11, 370, 437, 307, 300, 364, 30992, 339, 293, 257, 1922, 13, 3769, 13, 961, 311, 536, 437, 2314, 13, 51014, 51014, 961, 311, 25628, 341, 2146, 13, 876, 11, 630, 309, 3677, 1217, 337, 385, 13, 5490, 13, 51264, 51264, 2704, 286, 500, 380, 362, 281, 25628, 309, 13, 961, 311, 536, 498, 300, 311, 2275, 300, 309, 311, 551, 965, 11, 994, 380, 362, 281, 25628, 309, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.22738486354790846, "compression_ratio": 1.6872037914691944, "no_speech_prob": 3.474149707471952e-05}, {"id": 392, "seek": 234900, "start": 2362.0, "end": 2367.0, "text": " Let's reload this guy. Oh, did it load already for me. Nice.", "tokens": [50364, 509, 362, 281, 2845, 552, 14759, 457, 2602, 294, 341, 1389, 291, 2845, 411, 257, 1508, 11, 597, 286, 536, 13, 1033, 11, 1669, 2020, 13, 50764, 50764, 1033, 11, 370, 437, 307, 300, 364, 30992, 339, 293, 257, 1922, 13, 3769, 13, 961, 311, 536, 437, 2314, 13, 51014, 51014, 961, 311, 25628, 341, 2146, 13, 876, 11, 630, 309, 3677, 1217, 337, 385, 13, 5490, 13, 51264, 51264, 2704, 286, 500, 380, 362, 281, 25628, 309, 13, 961, 311, 536, 498, 300, 311, 2275, 300, 309, 311, 551, 965, 11, 994, 380, 362, 281, 25628, 309, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.22738486354790846, "compression_ratio": 1.6872037914691944, "no_speech_prob": 3.474149707471952e-05}, {"id": 393, "seek": 234900, "start": 2367.0, "end": 2376.0, "text": " Maybe I don't have to reload it. Let's see if that's report that it's thing today, didn't have to reload it.", "tokens": [50364, 509, 362, 281, 2845, 552, 14759, 457, 2602, 294, 341, 1389, 291, 2845, 411, 257, 1508, 11, 597, 286, 536, 13, 1033, 11, 1669, 2020, 13, 50764, 50764, 1033, 11, 370, 437, 307, 300, 364, 30992, 339, 293, 257, 1922, 13, 3769, 13, 961, 311, 536, 437, 2314, 13, 51014, 51014, 961, 311, 25628, 341, 2146, 13, 876, 11, 630, 309, 3677, 1217, 337, 385, 13, 5490, 13, 51264, 51264, 2704, 286, 500, 380, 362, 281, 25628, 309, 13, 961, 311, 536, 498, 300, 311, 2275, 300, 309, 311, 551, 965, 11, 994, 380, 362, 281, 25628, 309, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.22738486354790846, "compression_ratio": 1.6872037914691944, "no_speech_prob": 3.474149707471952e-05}, {"id": 394, "seek": 237600, "start": 2376.0, "end": 2380.0, "text": " I'm gonna zoom out for a second. Okay, sure. This huge.", "tokens": [50364, 286, 478, 799, 8863, 484, 337, 257, 1150, 13, 1033, 11, 988, 13, 639, 2603, 13, 50564, 50564, 2798, 11, 370, 11, 291, 829, 1954, 6076, 11, 630, 1792, 291, 5546, 257, 1922, 13, 12024, 13, 50864, 50864, 961, 311, 574, 412, 527, 3847, 14170, 13, 467, 311, 439, 670, 264, 1081, 570, 309, 311, 3097, 558, 370, 291, 528, 281, 11, 291, 528, 281, 11, 291, 5240, 528, 281, 312, 11603, 428, 24071, 411, 30992, 339, 14170, 457, 321, 458, 309, 311, 1090, 411, 1553, 884, 1340, 689, 321, 483, 7562, 8923, 597, 307, 869, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.2790221601429552, "compression_ratio": 1.565040650406504, "no_speech_prob": 3.373245635884814e-05}, {"id": 395, "seek": 237600, "start": 2380.0, "end": 2386.0, "text": " Alright, so, you put oh wow, did walk you fucking a half. Fine.", "tokens": [50364, 286, 478, 799, 8863, 484, 337, 257, 1150, 13, 1033, 11, 988, 13, 639, 2603, 13, 50564, 50564, 2798, 11, 370, 11, 291, 829, 1954, 6076, 11, 630, 1792, 291, 5546, 257, 1922, 13, 12024, 13, 50864, 50864, 961, 311, 574, 412, 527, 3847, 14170, 13, 467, 311, 439, 670, 264, 1081, 570, 309, 311, 3097, 558, 370, 291, 528, 281, 11, 291, 528, 281, 11, 291, 5240, 528, 281, 312, 11603, 428, 24071, 411, 30992, 339, 14170, 457, 321, 458, 309, 311, 1090, 411, 1553, 884, 1340, 689, 321, 483, 7562, 8923, 597, 307, 869, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.2790221601429552, "compression_ratio": 1.565040650406504, "no_speech_prob": 3.373245635884814e-05}, {"id": 396, "seek": 237600, "start": 2386.0, "end": 2400.0, "text": " Let's look at our train accuracy. It's all over the place because it's training right so you want to, you want to, you mostly want to be tracking your validation like epoch accuracy but we know it's high like without doing anything where we get 28%, which is great.", "tokens": [50364, 286, 478, 799, 8863, 484, 337, 257, 1150, 13, 1033, 11, 988, 13, 639, 2603, 13, 50564, 50564, 2798, 11, 370, 11, 291, 829, 1954, 6076, 11, 630, 1792, 291, 5546, 257, 1922, 13, 12024, 13, 50864, 50864, 961, 311, 574, 412, 527, 3847, 14170, 13, 467, 311, 439, 670, 264, 1081, 570, 309, 311, 3097, 558, 370, 291, 528, 281, 11, 291, 528, 281, 11, 291, 5240, 528, 281, 312, 11603, 428, 24071, 411, 30992, 339, 14170, 457, 321, 458, 309, 311, 1090, 411, 1553, 884, 1340, 689, 321, 483, 7562, 8923, 597, 307, 869, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.2790221601429552, "compression_ratio": 1.565040650406504, "no_speech_prob": 3.373245635884814e-05}, {"id": 397, "seek": 240000, "start": 2400.0, "end": 2409.0, "text": " So it shows the transfer learning is working. And then our last year we going down so it's kind of bumpy but that's expected. Again, because this is the training.", "tokens": [50364, 407, 309, 3110, 264, 5003, 2539, 307, 1364, 13, 400, 550, 527, 1036, 1064, 321, 516, 760, 370, 309, 311, 733, 295, 49400, 457, 300, 311, 5176, 13, 3764, 11, 570, 341, 307, 264, 3097, 13, 50814, 50814, 407, 1338, 11, 370, 586, 321, 434, 2577, 264, 20820, 295, 341, 597, 307, 869, 13, 51064, 51064, 407, 11, 586, 300, 321, 362, 341, 1507, 510, 13, 286, 767, 528, 281, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.16300039927164714, "compression_ratio": 1.5243243243243243, "no_speech_prob": 1.7329886077277479e-06}, {"id": 398, "seek": 240000, "start": 2409.0, "end": 2414.0, "text": " So yeah, so now we're seeing the logs of this which is great.", "tokens": [50364, 407, 309, 3110, 264, 5003, 2539, 307, 1364, 13, 400, 550, 527, 1036, 1064, 321, 516, 760, 370, 309, 311, 733, 295, 49400, 457, 300, 311, 5176, 13, 3764, 11, 570, 341, 307, 264, 3097, 13, 50814, 50814, 407, 1338, 11, 370, 586, 321, 434, 2577, 264, 20820, 295, 341, 597, 307, 869, 13, 51064, 51064, 407, 11, 586, 300, 321, 362, 341, 1507, 510, 13, 286, 767, 528, 281, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.16300039927164714, "compression_ratio": 1.5243243243243243, "no_speech_prob": 1.7329886077277479e-06}, {"id": 399, "seek": 240000, "start": 2414.0, "end": 2419.0, "text": " So, now that we have this stuff here. I actually want to.", "tokens": [50364, 407, 309, 3110, 264, 5003, 2539, 307, 1364, 13, 400, 550, 527, 1036, 1064, 321, 516, 760, 370, 309, 311, 733, 295, 49400, 457, 300, 311, 5176, 13, 3764, 11, 570, 341, 307, 264, 3097, 13, 50814, 50814, 407, 1338, 11, 370, 586, 321, 434, 2577, 264, 20820, 295, 341, 597, 307, 869, 13, 51064, 51064, 407, 11, 586, 300, 321, 362, 341, 1507, 510, 13, 286, 767, 528, 281, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.16300039927164714, "compression_ratio": 1.5243243243243243, "no_speech_prob": 1.7329886077277479e-06}, {"id": 400, "seek": 241900, "start": 2419.0, "end": 2434.0, "text": " I actually want to unfreeze this rest nets, after a few bucks, right. So we're going to find, we're going to find soon as it is right now but after, let's just say, 10 bucks. I'm going to say hey unfreeze the backbone and start using the backbone as well.", "tokens": [50364, 286, 767, 528, 281, 3971, 701, 1381, 341, 1472, 36170, 11, 934, 257, 1326, 11829, 11, 558, 13, 407, 321, 434, 516, 281, 915, 11, 321, 434, 516, 281, 915, 2321, 382, 309, 307, 558, 586, 457, 934, 11, 718, 311, 445, 584, 11, 1266, 11829, 13, 286, 478, 516, 281, 584, 4177, 3971, 701, 1381, 264, 34889, 293, 722, 1228, 264, 34889, 382, 731, 13, 51114, 51114, 865, 13, 51214, 51214, 440, 819, 2539, 3314, 2673, 286, 360, 300, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.1895723679486443, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.840491106326226e-05}, {"id": 401, "seek": 241900, "start": 2434.0, "end": 2436.0, "text": " Yeah.", "tokens": [50364, 286, 767, 528, 281, 3971, 701, 1381, 341, 1472, 36170, 11, 934, 257, 1326, 11829, 11, 558, 13, 407, 321, 434, 516, 281, 915, 11, 321, 434, 516, 281, 915, 2321, 382, 309, 307, 558, 586, 457, 934, 11, 718, 311, 445, 584, 11, 1266, 11829, 13, 286, 478, 516, 281, 584, 4177, 3971, 701, 1381, 264, 34889, 293, 722, 1228, 264, 34889, 382, 731, 13, 51114, 51114, 865, 13, 51214, 51214, 440, 819, 2539, 3314, 2673, 286, 360, 300, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.1895723679486443, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.840491106326226e-05}, {"id": 402, "seek": 241900, "start": 2436.0, "end": 2439.0, "text": " The different learning rate usually I do that.", "tokens": [50364, 286, 767, 528, 281, 3971, 701, 1381, 341, 1472, 36170, 11, 934, 257, 1326, 11829, 11, 558, 13, 407, 321, 434, 516, 281, 915, 11, 321, 434, 516, 281, 915, 2321, 382, 309, 307, 558, 586, 457, 934, 11, 718, 311, 445, 584, 11, 1266, 11829, 13, 286, 478, 516, 281, 584, 4177, 3971, 701, 1381, 264, 34889, 293, 722, 1228, 264, 34889, 382, 731, 13, 51114, 51114, 865, 13, 51214, 51214, 440, 819, 2539, 3314, 2673, 286, 360, 300, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.1895723679486443, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.840491106326226e-05}, {"id": 403, "seek": 243900, "start": 2439.0, "end": 2451.0, "text": " Yeah, so you can adjust that as well right so I'm going to do I'm going to skip the learning rate adjustment but let me start just by changing the epoch thing right so the, the, the lightning module.", "tokens": [50364, 865, 11, 370, 291, 393, 4369, 300, 382, 731, 558, 370, 286, 478, 516, 281, 360, 286, 478, 516, 281, 10023, 264, 2539, 3314, 17132, 457, 718, 385, 722, 445, 538, 4473, 264, 30992, 339, 551, 558, 370, 264, 11, 264, 11, 264, 16589, 10088, 13, 50964, 50964, 883, 11, 264, 21110, 13, 865, 13, 407, 604, 10088, 575, 257, 23918, 281, 264, 21110, 11, 293, 550, 264, 21110, 3255, 437, 2190, 16491, 294, 558, 13, 1033, 13, 51364, 51364, 509, 14894, 13, 51414, 51414, 1018, 938, 382, 309, 311, 1570, 813, 1266, 13, 286, 478, 516, 281, 360, 341, 1507, 510, 13, 1779, 13, 407, 1797, 322, 11, 577, 11, 577, 775, 264, 3209, 3255, 466, 264, 21110, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19268844973656438, "compression_ratio": 1.7916666666666667, "no_speech_prob": 4.565847575577209e-06}, {"id": 404, "seek": 243900, "start": 2451.0, "end": 2459.0, "text": " No, the trainer. Yeah. So any module has a pointer to the trainer, and then the trainer knows what current pockets in right. Okay.", "tokens": [50364, 865, 11, 370, 291, 393, 4369, 300, 382, 731, 558, 370, 286, 478, 516, 281, 360, 286, 478, 516, 281, 10023, 264, 2539, 3314, 17132, 457, 718, 385, 722, 445, 538, 4473, 264, 30992, 339, 551, 558, 370, 264, 11, 264, 11, 264, 16589, 10088, 13, 50964, 50964, 883, 11, 264, 21110, 13, 865, 13, 407, 604, 10088, 575, 257, 23918, 281, 264, 21110, 11, 293, 550, 264, 21110, 3255, 437, 2190, 16491, 294, 558, 13, 1033, 13, 51364, 51364, 509, 14894, 13, 51414, 51414, 1018, 938, 382, 309, 311, 1570, 813, 1266, 13, 286, 478, 516, 281, 360, 341, 1507, 510, 13, 1779, 13, 407, 1797, 322, 11, 577, 11, 577, 775, 264, 3209, 3255, 466, 264, 21110, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19268844973656438, "compression_ratio": 1.7916666666666667, "no_speech_prob": 4.565847575577209e-06}, {"id": 405, "seek": 243900, "start": 2459.0, "end": 2460.0, "text": " You buck.", "tokens": [50364, 865, 11, 370, 291, 393, 4369, 300, 382, 731, 558, 370, 286, 478, 516, 281, 360, 286, 478, 516, 281, 10023, 264, 2539, 3314, 17132, 457, 718, 385, 722, 445, 538, 4473, 264, 30992, 339, 551, 558, 370, 264, 11, 264, 11, 264, 16589, 10088, 13, 50964, 50964, 883, 11, 264, 21110, 13, 865, 13, 407, 604, 10088, 575, 257, 23918, 281, 264, 21110, 11, 293, 550, 264, 21110, 3255, 437, 2190, 16491, 294, 558, 13, 1033, 13, 51364, 51364, 509, 14894, 13, 51414, 51414, 1018, 938, 382, 309, 311, 1570, 813, 1266, 13, 286, 478, 516, 281, 360, 341, 1507, 510, 13, 1779, 13, 407, 1797, 322, 11, 577, 11, 577, 775, 264, 3209, 3255, 466, 264, 21110, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19268844973656438, "compression_ratio": 1.7916666666666667, "no_speech_prob": 4.565847575577209e-06}, {"id": 406, "seek": 243900, "start": 2460.0, "end": 2468.0, "text": " As long as it's less than 10. I'm going to do this stuff here. Right. So hold on, how, how does the network knows about the trainer.", "tokens": [50364, 865, 11, 370, 291, 393, 4369, 300, 382, 731, 558, 370, 286, 478, 516, 281, 360, 286, 478, 516, 281, 10023, 264, 2539, 3314, 17132, 457, 718, 385, 722, 445, 538, 4473, 264, 30992, 339, 551, 558, 370, 264, 11, 264, 11, 264, 16589, 10088, 13, 50964, 50964, 883, 11, 264, 21110, 13, 865, 13, 407, 604, 10088, 575, 257, 23918, 281, 264, 21110, 11, 293, 550, 264, 21110, 3255, 437, 2190, 16491, 294, 558, 13, 1033, 13, 51364, 51364, 509, 14894, 13, 51414, 51414, 1018, 938, 382, 309, 311, 1570, 813, 1266, 13, 286, 478, 516, 281, 360, 341, 1507, 510, 13, 1779, 13, 407, 1797, 322, 11, 577, 11, 577, 775, 264, 3209, 3255, 466, 264, 21110, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.19268844973656438, "compression_ratio": 1.7916666666666667, "no_speech_prob": 4.565847575577209e-06}, {"id": 407, "seek": 246800, "start": 2468.0, "end": 2482.0, "text": " Yeah, so when you when you start this training process in here. Yeah, in fits, then the trainer, the network gets assigned the trainer, and then it knows what it's in. Yeah. Okay, okay, okay.", "tokens": [50364, 865, 11, 370, 562, 291, 562, 291, 722, 341, 3097, 1399, 294, 510, 13, 865, 11, 294, 9001, 11, 550, 264, 21110, 11, 264, 3209, 2170, 13279, 264, 21110, 11, 293, 550, 309, 3255, 437, 309, 311, 294, 13, 865, 13, 1033, 11, 1392, 11, 1392, 13, 51064, 51064, 407, 11, 370, 787, 787, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.24099282086905788, "compression_ratio": 1.5597014925373134, "no_speech_prob": 3.426179682719521e-05}, {"id": 408, "seek": 246800, "start": 2482.0, "end": 2485.0, "text": " So, so only only.", "tokens": [50364, 865, 11, 370, 562, 291, 562, 291, 722, 341, 3097, 1399, 294, 510, 13, 865, 11, 294, 9001, 11, 550, 264, 21110, 11, 264, 3209, 2170, 13279, 264, 21110, 11, 293, 550, 309, 3255, 437, 309, 311, 294, 13, 865, 13, 1033, 11, 1392, 11, 1392, 13, 51064, 51064, 407, 11, 370, 787, 787, 13, 51214, 51214], "temperature": 0.0, "avg_logprob": -0.24099282086905788, "compression_ratio": 1.5597014925373134, "no_speech_prob": 3.426179682719521e-05}, {"id": 409, "seek": 248500, "start": 2485.0, "end": 2498.0, "text": " Well, okay so in real life you would like, wait like 1020 bucks but right now we're limited I'm going to put one epoch actually create feature of lightning, let's change it here. So, you see how long this epoch is taken right now, because I'm going to", "tokens": [50364, 1042, 11, 1392, 370, 294, 957, 993, 291, 576, 411, 11, 1699, 411, 1266, 2009, 11829, 457, 558, 586, 321, 434, 5567, 286, 478, 516, 281, 829, 472, 30992, 339, 767, 1884, 4111, 295, 16589, 11, 718, 311, 1319, 309, 510, 13, 407, 11, 291, 536, 577, 938, 341, 30992, 339, 307, 2726, 558, 586, 11, 570, 286, 478, 516, 281, 51014, 51014, 2319, 10938, 13, 961, 385, 445, 767, 4948, 264, 1230, 295, 3097, 15245, 279, 370, 321, 393, 352, 807, 544, 11829, 4663, 13, 407, 11, 286, 478, 516, 281, 352, 807, 2625, 3097, 15245, 279, 300, 311, 309, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.18001819106767764, "compression_ratio": 1.6907630522088353, "no_speech_prob": 5.255074938759208e-06}, {"id": 410, "seek": 248500, "start": 2498.0, "end": 2509.0, "text": " 100 samples. Let me just actually limit the number of training batches so we can go through more bucks faster. So, I'm going to go through 50 training batches that's it.", "tokens": [50364, 1042, 11, 1392, 370, 294, 957, 993, 291, 576, 411, 11, 1699, 411, 1266, 2009, 11829, 457, 558, 586, 321, 434, 5567, 286, 478, 516, 281, 829, 472, 30992, 339, 767, 1884, 4111, 295, 16589, 11, 718, 311, 1319, 309, 510, 13, 407, 11, 291, 536, 577, 938, 341, 30992, 339, 307, 2726, 558, 586, 11, 570, 286, 478, 516, 281, 51014, 51014, 2319, 10938, 13, 961, 385, 445, 767, 4948, 264, 1230, 295, 3097, 15245, 279, 370, 321, 393, 352, 807, 544, 11829, 4663, 13, 407, 11, 286, 478, 516, 281, 352, 807, 2625, 3097, 15245, 279, 300, 311, 309, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.18001819106767764, "compression_ratio": 1.6907630522088353, "no_speech_prob": 5.255074938759208e-06}, {"id": 411, "seek": 250900, "start": 2509.0, "end": 2519.0, "text": " And then, and then actually I can do this realistically so I can say 10 bucks. So as long as the epoch is under 10, I'm going to do this, where they're not, they're going to be no gradients.", "tokens": [50364, 400, 550, 11, 293, 550, 767, 286, 393, 360, 341, 40734, 370, 286, 393, 584, 1266, 11829, 13, 407, 382, 938, 382, 264, 30992, 339, 307, 833, 1266, 11, 286, 478, 516, 281, 360, 341, 11, 689, 436, 434, 406, 11, 436, 434, 516, 281, 312, 572, 2771, 2448, 13, 50864, 50864, 1018, 2321, 382, 291, 434, 484, 295, 300, 1673, 13, 1779, 13, 407, 11, 286, 643, 364, 728, 19435, 1712, 2597, 13, 51214, 51214, 1692, 11, 370, 382, 2321, 382, 291, 434, 484, 295, 300, 11, 286, 478, 445, 516, 281, 767, 2235, 484, 264, 4122, 13, 1779, 13, 1033, 13, 407, 586, 286, 478, 516, 281, 312, 646, 12425, 990, 281, 341, 551, 13, 51664, 51664, 3769, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16939317321777345, "compression_ratio": 1.7651821862348178, "no_speech_prob": 1.723050263535697e-05}, {"id": 412, "seek": 250900, "start": 2519.0, "end": 2526.0, "text": " As soon as you're out of that though. Right. So, I need an interstatement sorry.", "tokens": [50364, 400, 550, 11, 293, 550, 767, 286, 393, 360, 341, 40734, 370, 286, 393, 584, 1266, 11829, 13, 407, 382, 938, 382, 264, 30992, 339, 307, 833, 1266, 11, 286, 478, 516, 281, 360, 341, 11, 689, 436, 434, 406, 11, 436, 434, 516, 281, 312, 572, 2771, 2448, 13, 50864, 50864, 1018, 2321, 382, 291, 434, 484, 295, 300, 1673, 13, 1779, 13, 407, 11, 286, 643, 364, 728, 19435, 1712, 2597, 13, 51214, 51214, 1692, 11, 370, 382, 2321, 382, 291, 434, 484, 295, 300, 11, 286, 478, 445, 516, 281, 767, 2235, 484, 264, 4122, 13, 1779, 13, 1033, 13, 407, 586, 286, 478, 516, 281, 312, 646, 12425, 990, 281, 341, 551, 13, 51664, 51664, 3769, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16939317321777345, "compression_ratio": 1.7651821862348178, "no_speech_prob": 1.723050263535697e-05}, {"id": 413, "seek": 250900, "start": 2526.0, "end": 2535.0, "text": " Here, so as soon as you're out of that, I'm just going to actually pull out the features. Right. Okay. So now I'm going to be back propagating to this thing.", "tokens": [50364, 400, 550, 11, 293, 550, 767, 286, 393, 360, 341, 40734, 370, 286, 393, 584, 1266, 11829, 13, 407, 382, 938, 382, 264, 30992, 339, 307, 833, 1266, 11, 286, 478, 516, 281, 360, 341, 11, 689, 436, 434, 406, 11, 436, 434, 516, 281, 312, 572, 2771, 2448, 13, 50864, 50864, 1018, 2321, 382, 291, 434, 484, 295, 300, 1673, 13, 1779, 13, 407, 11, 286, 643, 364, 728, 19435, 1712, 2597, 13, 51214, 51214, 1692, 11, 370, 382, 2321, 382, 291, 434, 484, 295, 300, 11, 286, 478, 445, 516, 281, 767, 2235, 484, 264, 4122, 13, 1779, 13, 1033, 13, 407, 586, 286, 478, 516, 281, 312, 646, 12425, 990, 281, 341, 551, 13, 51664, 51664, 3769, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16939317321777345, "compression_ratio": 1.7651821862348178, "no_speech_prob": 1.723050263535697e-05}, {"id": 414, "seek": 250900, "start": 2535.0, "end": 2537.0, "text": " Great.", "tokens": [50364, 400, 550, 11, 293, 550, 767, 286, 393, 360, 341, 40734, 370, 286, 393, 584, 1266, 11829, 13, 407, 382, 938, 382, 264, 30992, 339, 307, 833, 1266, 11, 286, 478, 516, 281, 360, 341, 11, 689, 436, 434, 406, 11, 436, 434, 516, 281, 312, 572, 2771, 2448, 13, 50864, 50864, 1018, 2321, 382, 291, 434, 484, 295, 300, 1673, 13, 1779, 13, 407, 11, 286, 643, 364, 728, 19435, 1712, 2597, 13, 51214, 51214, 1692, 11, 370, 382, 2321, 382, 291, 434, 484, 295, 300, 11, 286, 478, 445, 516, 281, 767, 2235, 484, 264, 4122, 13, 1779, 13, 1033, 13, 407, 586, 286, 478, 516, 281, 312, 646, 12425, 990, 281, 341, 551, 13, 51664, 51664, 3769, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.16939317321777345, "compression_ratio": 1.7651821862348178, "no_speech_prob": 1.723050263535697e-05}, {"id": 415, "seek": 253700, "start": 2537.0, "end": 2547.0, "text": " Okay, so I just made some changes and I'm not quite sure if it's going to break so I'm going to use a quick debugging trip is called fast dev run. So I'm going to turn this on real quick.", "tokens": [50364, 1033, 11, 370, 286, 445, 1027, 512, 2962, 293, 286, 478, 406, 1596, 988, 498, 309, 311, 516, 281, 1821, 370, 286, 478, 516, 281, 764, 257, 1702, 45592, 4931, 307, 1219, 2370, 1905, 1190, 13, 407, 286, 478, 516, 281, 1261, 341, 322, 957, 1702, 13, 50864, 50864, 400, 562, 286, 562, 286, 9528, 341, 307, 516, 281, 2045, 633, 2167, 1622, 295, 3089, 13, 51064, 51064, 400, 436, 434, 406, 516, 281, 3847, 309, 311, 445, 516, 281, 360, 472, 15245, 588, 2661, 11, 445, 281, 652, 988, 300, 286, 362, 572, 11829, 718, 385, 445, 1190, 300, 957, 1702, 13, 51414, 51414, 400, 498, 341, 36362, 286, 362, 572, 2740, 13, 51514, 51514, 1033, 11, 869, 13, 883, 15120, 13, 51614, 51614, 407, 309, 311, 411, 264, 31958, 13, 51714, 51714, 759, 286, 632, 257, 7426, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14953774876064724, "compression_ratio": 1.823321554770318, "no_speech_prob": 2.7967640562565066e-05}, {"id": 416, "seek": 253700, "start": 2547.0, "end": 2551.0, "text": " And when I when I enable this is going to hit every single line of code.", "tokens": [50364, 1033, 11, 370, 286, 445, 1027, 512, 2962, 293, 286, 478, 406, 1596, 988, 498, 309, 311, 516, 281, 1821, 370, 286, 478, 516, 281, 764, 257, 1702, 45592, 4931, 307, 1219, 2370, 1905, 1190, 13, 407, 286, 478, 516, 281, 1261, 341, 322, 957, 1702, 13, 50864, 50864, 400, 562, 286, 562, 286, 9528, 341, 307, 516, 281, 2045, 633, 2167, 1622, 295, 3089, 13, 51064, 51064, 400, 436, 434, 406, 516, 281, 3847, 309, 311, 445, 516, 281, 360, 472, 15245, 588, 2661, 11, 445, 281, 652, 988, 300, 286, 362, 572, 11829, 718, 385, 445, 1190, 300, 957, 1702, 13, 51414, 51414, 400, 498, 341, 36362, 286, 362, 572, 2740, 13, 51514, 51514, 1033, 11, 869, 13, 883, 15120, 13, 51614, 51614, 407, 309, 311, 411, 264, 31958, 13, 51714, 51714, 759, 286, 632, 257, 7426, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14953774876064724, "compression_ratio": 1.823321554770318, "no_speech_prob": 2.7967640562565066e-05}, {"id": 417, "seek": 253700, "start": 2551.0, "end": 2558.0, "text": " And they're not going to train it's just going to do one batch very quickly, just to make sure that I have no bucks let me just run that real quick.", "tokens": [50364, 1033, 11, 370, 286, 445, 1027, 512, 2962, 293, 286, 478, 406, 1596, 988, 498, 309, 311, 516, 281, 1821, 370, 286, 478, 516, 281, 764, 257, 1702, 45592, 4931, 307, 1219, 2370, 1905, 1190, 13, 407, 286, 478, 516, 281, 1261, 341, 322, 957, 1702, 13, 50864, 50864, 400, 562, 286, 562, 286, 9528, 341, 307, 516, 281, 2045, 633, 2167, 1622, 295, 3089, 13, 51064, 51064, 400, 436, 434, 406, 516, 281, 3847, 309, 311, 445, 516, 281, 360, 472, 15245, 588, 2661, 11, 445, 281, 652, 988, 300, 286, 362, 572, 11829, 718, 385, 445, 1190, 300, 957, 1702, 13, 51414, 51414, 400, 498, 341, 36362, 286, 362, 572, 2740, 13, 51514, 51514, 1033, 11, 869, 13, 883, 15120, 13, 51614, 51614, 407, 309, 311, 411, 264, 31958, 13, 51714, 51714, 759, 286, 632, 257, 7426, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14953774876064724, "compression_ratio": 1.823321554770318, "no_speech_prob": 2.7967640562565066e-05}, {"id": 418, "seek": 253700, "start": 2558.0, "end": 2560.0, "text": " And if this completes I have no problems.", "tokens": [50364, 1033, 11, 370, 286, 445, 1027, 512, 2962, 293, 286, 478, 406, 1596, 988, 498, 309, 311, 516, 281, 1821, 370, 286, 478, 516, 281, 764, 257, 1702, 45592, 4931, 307, 1219, 2370, 1905, 1190, 13, 407, 286, 478, 516, 281, 1261, 341, 322, 957, 1702, 13, 50864, 50864, 400, 562, 286, 562, 286, 9528, 341, 307, 516, 281, 2045, 633, 2167, 1622, 295, 3089, 13, 51064, 51064, 400, 436, 434, 406, 516, 281, 3847, 309, 311, 445, 516, 281, 360, 472, 15245, 588, 2661, 11, 445, 281, 652, 988, 300, 286, 362, 572, 11829, 718, 385, 445, 1190, 300, 957, 1702, 13, 51414, 51414, 400, 498, 341, 36362, 286, 362, 572, 2740, 13, 51514, 51514, 1033, 11, 869, 13, 883, 15120, 13, 51614, 51614, 407, 309, 311, 411, 264, 31958, 13, 51714, 51714, 759, 286, 632, 257, 7426, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14953774876064724, "compression_ratio": 1.823321554770318, "no_speech_prob": 2.7967640562565066e-05}, {"id": 419, "seek": 253700, "start": 2560.0, "end": 2562.0, "text": " Okay, great. No bugs.", "tokens": [50364, 1033, 11, 370, 286, 445, 1027, 512, 2962, 293, 286, 478, 406, 1596, 988, 498, 309, 311, 516, 281, 1821, 370, 286, 478, 516, 281, 764, 257, 1702, 45592, 4931, 307, 1219, 2370, 1905, 1190, 13, 407, 286, 478, 516, 281, 1261, 341, 322, 957, 1702, 13, 50864, 50864, 400, 562, 286, 562, 286, 9528, 341, 307, 516, 281, 2045, 633, 2167, 1622, 295, 3089, 13, 51064, 51064, 400, 436, 434, 406, 516, 281, 3847, 309, 311, 445, 516, 281, 360, 472, 15245, 588, 2661, 11, 445, 281, 652, 988, 300, 286, 362, 572, 11829, 718, 385, 445, 1190, 300, 957, 1702, 13, 51414, 51414, 400, 498, 341, 36362, 286, 362, 572, 2740, 13, 51514, 51514, 1033, 11, 869, 13, 883, 15120, 13, 51614, 51614, 407, 309, 311, 411, 264, 31958, 13, 51714, 51714, 759, 286, 632, 257, 7426, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14953774876064724, "compression_ratio": 1.823321554770318, "no_speech_prob": 2.7967640562565066e-05}, {"id": 420, "seek": 253700, "start": 2562.0, "end": 2564.0, "text": " So it's like the compiler.", "tokens": [50364, 1033, 11, 370, 286, 445, 1027, 512, 2962, 293, 286, 478, 406, 1596, 988, 498, 309, 311, 516, 281, 1821, 370, 286, 478, 516, 281, 764, 257, 1702, 45592, 4931, 307, 1219, 2370, 1905, 1190, 13, 407, 286, 478, 516, 281, 1261, 341, 322, 957, 1702, 13, 50864, 50864, 400, 562, 286, 562, 286, 9528, 341, 307, 516, 281, 2045, 633, 2167, 1622, 295, 3089, 13, 51064, 51064, 400, 436, 434, 406, 516, 281, 3847, 309, 311, 445, 516, 281, 360, 472, 15245, 588, 2661, 11, 445, 281, 652, 988, 300, 286, 362, 572, 11829, 718, 385, 445, 1190, 300, 957, 1702, 13, 51414, 51414, 400, 498, 341, 36362, 286, 362, 572, 2740, 13, 51514, 51514, 1033, 11, 869, 13, 883, 15120, 13, 51614, 51614, 407, 309, 311, 411, 264, 31958, 13, 51714, 51714, 759, 286, 632, 257, 7426, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14953774876064724, "compression_ratio": 1.823321554770318, "no_speech_prob": 2.7967640562565066e-05}, {"id": 421, "seek": 253700, "start": 2564.0, "end": 2566.0, "text": " If I had a bug.", "tokens": [50364, 1033, 11, 370, 286, 445, 1027, 512, 2962, 293, 286, 478, 406, 1596, 988, 498, 309, 311, 516, 281, 1821, 370, 286, 478, 516, 281, 764, 257, 1702, 45592, 4931, 307, 1219, 2370, 1905, 1190, 13, 407, 286, 478, 516, 281, 1261, 341, 322, 957, 1702, 13, 50864, 50864, 400, 562, 286, 562, 286, 9528, 341, 307, 516, 281, 2045, 633, 2167, 1622, 295, 3089, 13, 51064, 51064, 400, 436, 434, 406, 516, 281, 3847, 309, 311, 445, 516, 281, 360, 472, 15245, 588, 2661, 11, 445, 281, 652, 988, 300, 286, 362, 572, 11829, 718, 385, 445, 1190, 300, 957, 1702, 13, 51414, 51414, 400, 498, 341, 36362, 286, 362, 572, 2740, 13, 51514, 51514, 1033, 11, 869, 13, 883, 15120, 13, 51614, 51614, 407, 309, 311, 411, 264, 31958, 13, 51714, 51714, 759, 286, 632, 257, 7426, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14953774876064724, "compression_ratio": 1.823321554770318, "no_speech_prob": 2.7967640562565066e-05}, {"id": 422, "seek": 256600, "start": 2566.0, "end": 2577.0, "text": " If I had been like asserts false. Right. Yeah, then it would catch it without having to train the whole time. Great. Oh, wow. Okay. Okay, I see.", "tokens": [50364, 759, 286, 632, 668, 411, 19810, 82, 7908, 13, 1779, 13, 865, 11, 550, 309, 576, 3745, 309, 1553, 1419, 281, 3847, 264, 1379, 565, 13, 3769, 13, 876, 11, 6076, 13, 1033, 13, 1033, 11, 286, 536, 13, 50914, 50914, 1033, 11, 370, 11, 370, 957, 1621, 45592, 510, 13, 1033, 11, 2176, 13, 407, 286, 478, 516, 281, 28362, 341, 551, 586, 13, 400, 286, 500, 380, 643, 2625, 15245, 279, 11, 411, 11, 445, 885, 20239, 11, 718, 311, 445, 360, 945, 13, 663, 311, 2489, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1435914546885389, "compression_ratio": 1.4751131221719458, "no_speech_prob": 1.18420693979715e-05}, {"id": 423, "seek": 256600, "start": 2577.0, "end": 2589.0, "text": " Okay, so, so real live debugging here. Okay, perfect. So I'm going to disable this thing now. And I don't need 50 batches, like, just being ambitious, let's just do 20. That's fine.", "tokens": [50364, 759, 286, 632, 668, 411, 19810, 82, 7908, 13, 1779, 13, 865, 11, 550, 309, 576, 3745, 309, 1553, 1419, 281, 3847, 264, 1379, 565, 13, 3769, 13, 876, 11, 6076, 13, 1033, 13, 1033, 11, 286, 536, 13, 50914, 50914, 1033, 11, 370, 11, 370, 957, 1621, 45592, 510, 13, 1033, 11, 2176, 13, 407, 286, 478, 516, 281, 28362, 341, 551, 586, 13, 400, 286, 500, 380, 643, 2625, 15245, 279, 11, 411, 11, 445, 885, 20239, 11, 718, 311, 445, 360, 945, 13, 663, 311, 2489, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1435914546885389, "compression_ratio": 1.4751131221719458, "no_speech_prob": 1.18420693979715e-05}, {"id": 424, "seek": 258900, "start": 2589.0, "end": 2604.0, "text": " And then this should power to the box a lot faster. Okay, great. So that was one, one epoch. So you see it's going super quick. And it's kind of kind of like weird because this refresh rate is 20 so it's only going to look at every 20 bucks.", "tokens": [50364, 400, 550, 341, 820, 1347, 281, 264, 2424, 257, 688, 4663, 13, 1033, 11, 869, 13, 407, 300, 390, 472, 11, 472, 30992, 339, 13, 407, 291, 536, 309, 311, 516, 1687, 1702, 13, 400, 309, 311, 733, 295, 733, 295, 411, 3657, 570, 341, 15134, 3314, 307, 945, 370, 309, 311, 787, 516, 281, 574, 412, 633, 945, 11829, 13, 51114, 51114, 407, 718, 385, 445, 1319, 300, 281, 1732, 11, 2597, 15245, 279, 13, 407, 633, 1732, 15245, 279, 307, 516, 281, 5623, 5623, 341, 2159, 586, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.15489833912950882, "compression_ratio": 1.6164383561643836, "no_speech_prob": 2.090426050926908e-06}, {"id": 425, "seek": 258900, "start": 2604.0, "end": 2612.0, "text": " So let me just change that to five, sorry batches. So every five batches is going to update update this bar now.", "tokens": [50364, 400, 550, 341, 820, 1347, 281, 264, 2424, 257, 688, 4663, 13, 1033, 11, 869, 13, 407, 300, 390, 472, 11, 472, 30992, 339, 13, 407, 291, 536, 309, 311, 516, 1687, 1702, 13, 400, 309, 311, 733, 295, 733, 295, 411, 3657, 570, 341, 15134, 3314, 307, 945, 370, 309, 311, 787, 516, 281, 574, 412, 633, 945, 11829, 13, 51114, 51114, 407, 718, 385, 445, 1319, 300, 281, 1732, 11, 2597, 15245, 279, 13, 407, 633, 1732, 15245, 279, 307, 516, 281, 5623, 5623, 341, 2159, 586, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.15489833912950882, "compression_ratio": 1.6164383561643836, "no_speech_prob": 2.090426050926908e-06}, {"id": 426, "seek": 261200, "start": 2612.0, "end": 2620.0, "text": " Okay, we have 10 minutes left and we still have to cover the unsupervised learning just to let you know. Got it. Okay.", "tokens": [50364, 1033, 11, 321, 362, 1266, 2077, 1411, 293, 321, 920, 362, 281, 2060, 264, 2693, 12879, 24420, 2539, 445, 281, 718, 291, 458, 13, 5803, 309, 13, 1033, 13, 50764, 50764, 1033, 11, 370, 510, 13, 50864, 50864, 407, 718, 311, 584, 466, 732, 11, 1045, 11, 1451, 13, 51114, 51114, 407, 291, 536, 264, 15352, 516, 760, 9594, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.16212812066078186, "compression_ratio": 1.3790849673202614, "no_speech_prob": 3.288652578703477e-06}, {"id": 427, "seek": 261200, "start": 2620.0, "end": 2622.0, "text": " Okay, so here.", "tokens": [50364, 1033, 11, 321, 362, 1266, 2077, 1411, 293, 321, 920, 362, 281, 2060, 264, 2693, 12879, 24420, 2539, 445, 281, 718, 291, 458, 13, 5803, 309, 13, 1033, 13, 50764, 50764, 1033, 11, 370, 510, 13, 50864, 50864, 407, 718, 311, 584, 466, 732, 11, 1045, 11, 1451, 13, 51114, 51114, 407, 291, 536, 264, 15352, 516, 760, 9594, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.16212812066078186, "compression_ratio": 1.3790849673202614, "no_speech_prob": 3.288652578703477e-06}, {"id": 428, "seek": 261200, "start": 2622.0, "end": 2627.0, "text": " So let's say about two, three, four.", "tokens": [50364, 1033, 11, 321, 362, 1266, 2077, 1411, 293, 321, 920, 362, 281, 2060, 264, 2693, 12879, 24420, 2539, 445, 281, 718, 291, 458, 13, 5803, 309, 13, 1033, 13, 50764, 50764, 1033, 11, 370, 510, 13, 50864, 50864, 407, 718, 311, 584, 466, 732, 11, 1045, 11, 1451, 13, 51114, 51114, 407, 291, 536, 264, 15352, 516, 760, 9594, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.16212812066078186, "compression_ratio": 1.3790849673202614, "no_speech_prob": 3.288652578703477e-06}, {"id": 429, "seek": 261200, "start": 2627.0, "end": 2630.0, "text": " So you see the losses going down nicely.", "tokens": [50364, 1033, 11, 321, 362, 1266, 2077, 1411, 293, 321, 920, 362, 281, 2060, 264, 2693, 12879, 24420, 2539, 445, 281, 718, 291, 458, 13, 5803, 309, 13, 1033, 13, 50764, 50764, 1033, 11, 370, 510, 13, 50864, 50864, 407, 718, 311, 584, 466, 732, 11, 1045, 11, 1451, 13, 51114, 51114, 407, 291, 536, 264, 15352, 516, 760, 9594, 13, 51264, 51264], "temperature": 0.0, "avg_logprob": -0.16212812066078186, "compression_ratio": 1.3790849673202614, "no_speech_prob": 3.288652578703477e-06}, {"id": 430, "seek": 263000, "start": 2630.0, "end": 2643.0, "text": " So let's just keep it going for a minute. And then it's going to unfreeze at some points and then the loss is going to drop a lot more. Now, as you mentioned, you have to lower the learning rate as well. I'm not doing that. So I'm not going to get the best performance out of this. But you can do that as well.", "tokens": [50364, 407, 718, 311, 445, 1066, 309, 516, 337, 257, 3456, 13, 400, 550, 309, 311, 516, 281, 3971, 701, 1381, 412, 512, 2793, 293, 550, 264, 4470, 307, 516, 281, 3270, 257, 688, 544, 13, 823, 11, 382, 291, 2835, 11, 291, 362, 281, 3126, 264, 2539, 3314, 382, 731, 13, 286, 478, 406, 884, 300, 13, 407, 286, 478, 406, 516, 281, 483, 264, 1151, 3389, 484, 295, 341, 13, 583, 291, 393, 360, 300, 382, 731, 13, 51014, 51014, 1012, 867, 11, 577, 867, 3642, 775, 341, 1507, 352, 337, 30, 51214, 51214, 876, 11, 538, 7576, 9714, 558, 291, 393, 291, 393, 992, 257, 11469, 4948, 420, 746, 294, 264, 21110, 5883, 2144, 13, 865, 11, 510, 13, 407, 286, 536, 13, 1033, 13, 7402, 383, 2424, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1942920967384621, "compression_ratio": 1.6556291390728477, "no_speech_prob": 1.184261873277137e-05}, {"id": 431, "seek": 263000, "start": 2643.0, "end": 2647.0, "text": " How many, how many books does this stuff go for?", "tokens": [50364, 407, 718, 311, 445, 1066, 309, 516, 337, 257, 3456, 13, 400, 550, 309, 311, 516, 281, 3971, 701, 1381, 412, 512, 2793, 293, 550, 264, 4470, 307, 516, 281, 3270, 257, 688, 544, 13, 823, 11, 382, 291, 2835, 11, 291, 362, 281, 3126, 264, 2539, 3314, 382, 731, 13, 286, 478, 406, 884, 300, 13, 407, 286, 478, 406, 516, 281, 483, 264, 1151, 3389, 484, 295, 341, 13, 583, 291, 393, 360, 300, 382, 731, 13, 51014, 51014, 1012, 867, 11, 577, 867, 3642, 775, 341, 1507, 352, 337, 30, 51214, 51214, 876, 11, 538, 7576, 9714, 558, 291, 393, 291, 393, 992, 257, 11469, 4948, 420, 746, 294, 264, 21110, 5883, 2144, 13, 865, 11, 510, 13, 407, 286, 536, 13, 1033, 13, 7402, 383, 2424, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1942920967384621, "compression_ratio": 1.6556291390728477, "no_speech_prob": 1.184261873277137e-05}, {"id": 432, "seek": 263000, "start": 2647.0, "end": 2657.0, "text": " Oh, by default 1000 right you can you can set a max limit or something in the trainer initialization. Yeah, here. So I see. Okay. Max C box.", "tokens": [50364, 407, 718, 311, 445, 1066, 309, 516, 337, 257, 3456, 13, 400, 550, 309, 311, 516, 281, 3971, 701, 1381, 412, 512, 2793, 293, 550, 264, 4470, 307, 516, 281, 3270, 257, 688, 544, 13, 823, 11, 382, 291, 2835, 11, 291, 362, 281, 3126, 264, 2539, 3314, 382, 731, 13, 286, 478, 406, 884, 300, 13, 407, 286, 478, 406, 516, 281, 483, 264, 1151, 3389, 484, 295, 341, 13, 583, 291, 393, 360, 300, 382, 731, 13, 51014, 51014, 1012, 867, 11, 577, 867, 3642, 775, 341, 1507, 352, 337, 30, 51214, 51214, 876, 11, 538, 7576, 9714, 558, 291, 393, 291, 393, 992, 257, 11469, 4948, 420, 746, 294, 264, 21110, 5883, 2144, 13, 865, 11, 510, 13, 407, 286, 536, 13, 1033, 13, 7402, 383, 2424, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1942920967384621, "compression_ratio": 1.6556291390728477, "no_speech_prob": 1.184261873277137e-05}, {"id": 433, "seek": 265700, "start": 2657.0, "end": 2671.0, "text": " I think it's called. Okay. Okay, it's going. Yeah, it looks like that didn't work because this loss is high now. Maybe it starts working now. So yeah, had I just a learning rate that wouldn't have happened, it wouldn't have jumped and lost.", "tokens": [50364, 286, 519, 309, 311, 1219, 13, 1033, 13, 1033, 11, 309, 311, 516, 13, 865, 11, 309, 1542, 411, 300, 994, 380, 589, 570, 341, 4470, 307, 1090, 586, 13, 2704, 309, 3719, 1364, 586, 13, 407, 1338, 11, 632, 286, 445, 257, 2539, 3314, 300, 2759, 380, 362, 2011, 11, 309, 2759, 380, 362, 13864, 293, 2731, 13, 51064, 51064, 467, 576, 362, 445, 668, 2489, 13, 583, 286, 519, 309, 311, 23559, 586, 13, 407, 11, 718, 311, 536, 437, 2314, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1738878055052324, "compression_ratio": 1.65, "no_speech_prob": 7.4111785579589196e-06}, {"id": 434, "seek": 265700, "start": 2671.0, "end": 2680.0, "text": " It would have just been fine. But I think it's adjusting now. So, let's see what happens.", "tokens": [50364, 286, 519, 309, 311, 1219, 13, 1033, 13, 1033, 11, 309, 311, 516, 13, 865, 11, 309, 1542, 411, 300, 994, 380, 589, 570, 341, 4470, 307, 1090, 586, 13, 2704, 309, 3719, 1364, 586, 13, 407, 1338, 11, 632, 286, 445, 257, 2539, 3314, 300, 2759, 380, 362, 2011, 11, 309, 2759, 380, 362, 13864, 293, 2731, 13, 51064, 51064, 467, 576, 362, 445, 668, 2489, 13, 583, 286, 519, 309, 311, 23559, 586, 13, 407, 11, 718, 311, 536, 437, 2314, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1738878055052324, "compression_ratio": 1.65, "no_speech_prob": 7.4111785579589196e-06}, {"id": 435, "seek": 268000, "start": 2680.0, "end": 2689.0, "text": " Oh, it's also 50 batches so that that will explain some things as well.", "tokens": [50364, 876, 11, 309, 311, 611, 2625, 15245, 279, 370, 300, 300, 486, 2903, 512, 721, 382, 731, 13, 50814, 50814, 1033, 11, 456, 291, 352, 13, 407, 586, 321, 434, 11, 321, 434, 833, 732, 597, 307, 869, 13, 407, 321, 8782, 380, 1612, 300, 949, 13, 51114, 51114, 400, 370, 321, 434, 13601, 4663, 370, 718, 311, 445, 855, 264, 1036, 586, 13, 51314, 51314, 1033, 11, 558, 13, 1449, 370, 321, 393, 536, 437, 264, 5065, 645, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12512595312935965, "compression_ratio": 1.484375, "no_speech_prob": 4.5208449250822014e-07}, {"id": 436, "seek": 268000, "start": 2689.0, "end": 2695.0, "text": " Okay, there you go. So now we're, we're under two which is great. So we hadn't seen that before.", "tokens": [50364, 876, 11, 309, 311, 611, 2625, 15245, 279, 370, 300, 300, 486, 2903, 512, 721, 382, 731, 13, 50814, 50814, 1033, 11, 456, 291, 352, 13, 407, 586, 321, 434, 11, 321, 434, 833, 732, 597, 307, 869, 13, 407, 321, 8782, 380, 1612, 300, 949, 13, 51114, 51114, 400, 370, 321, 434, 13601, 4663, 370, 718, 311, 445, 855, 264, 1036, 586, 13, 51314, 51314, 1033, 11, 558, 13, 1449, 370, 321, 393, 536, 437, 264, 5065, 645, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12512595312935965, "compression_ratio": 1.484375, "no_speech_prob": 4.5208449250822014e-07}, {"id": 437, "seek": 268000, "start": 2695.0, "end": 2699.0, "text": " And so we're dropping faster so let's just show the last now.", "tokens": [50364, 876, 11, 309, 311, 611, 2625, 15245, 279, 370, 300, 300, 486, 2903, 512, 721, 382, 731, 13, 50814, 50814, 1033, 11, 456, 291, 352, 13, 407, 586, 321, 434, 11, 321, 434, 833, 732, 597, 307, 869, 13, 407, 321, 8782, 380, 1612, 300, 949, 13, 51114, 51114, 400, 370, 321, 434, 13601, 4663, 370, 718, 311, 445, 855, 264, 1036, 586, 13, 51314, 51314, 1033, 11, 558, 13, 1449, 370, 321, 393, 536, 437, 264, 5065, 645, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12512595312935965, "compression_ratio": 1.484375, "no_speech_prob": 4.5208449250822014e-07}, {"id": 438, "seek": 268000, "start": 2699.0, "end": 2707.0, "text": " Okay, right. Just so we can see what the effects were.", "tokens": [50364, 876, 11, 309, 311, 611, 2625, 15245, 279, 370, 300, 300, 486, 2903, 512, 721, 382, 731, 13, 50814, 50814, 1033, 11, 456, 291, 352, 13, 407, 586, 321, 434, 11, 321, 434, 833, 732, 597, 307, 869, 13, 407, 321, 8782, 380, 1612, 300, 949, 13, 51114, 51114, 400, 370, 321, 434, 13601, 4663, 370, 718, 311, 445, 855, 264, 1036, 586, 13, 51314, 51314, 1033, 11, 558, 13, 1449, 370, 321, 393, 536, 437, 264, 5065, 645, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.12512595312935965, "compression_ratio": 1.484375, "no_speech_prob": 4.5208449250822014e-07}, {"id": 439, "seek": 270700, "start": 2707.0, "end": 2710.0, "text": " Luckily for self supervised learning.", "tokens": [50364, 19726, 337, 2698, 46533, 2539, 13, 50514, 50514, 1726, 709, 2962, 11, 291, 787, 1319, 264, 34889, 11, 370, 820, 747, 257, 1339, 13, 50714, 50714, 1033, 11, 1392, 370, 264, 1036, 732, 300, 321, 19779, 466, 689, 613, 1074, 558, 13, 407, 341, 390, 406, 12496, 11, 293, 341, 390, 11, 370, 341, 390, 12496, 34889, 293, 3971, 340, 2904, 34889, 13, 51264, 51264, 407, 718, 311, 536, 437, 2011, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.19614340129651522, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.9524308299878612e-05}, {"id": 440, "seek": 270700, "start": 2710.0, "end": 2714.0, "text": " Not much changes, you only change the backbone, so should take a while.", "tokens": [50364, 19726, 337, 2698, 46533, 2539, 13, 50514, 50514, 1726, 709, 2962, 11, 291, 787, 1319, 264, 34889, 11, 370, 820, 747, 257, 1339, 13, 50714, 50714, 1033, 11, 1392, 370, 264, 1036, 732, 300, 321, 19779, 466, 689, 613, 1074, 558, 13, 407, 341, 390, 406, 12496, 11, 293, 341, 390, 11, 370, 341, 390, 12496, 34889, 293, 3971, 340, 2904, 34889, 13, 51264, 51264, 407, 718, 311, 536, 437, 2011, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.19614340129651522, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.9524308299878612e-05}, {"id": 441, "seek": 270700, "start": 2714.0, "end": 2725.0, "text": " Okay, okay so the last two that we cared about where these guys right. So this was not frozen, and this was, so this was frozen backbone and unfrozen backbone.", "tokens": [50364, 19726, 337, 2698, 46533, 2539, 13, 50514, 50514, 1726, 709, 2962, 11, 291, 787, 1319, 264, 34889, 11, 370, 820, 747, 257, 1339, 13, 50714, 50714, 1033, 11, 1392, 370, 264, 1036, 732, 300, 321, 19779, 466, 689, 613, 1074, 558, 13, 407, 341, 390, 406, 12496, 11, 293, 341, 390, 11, 370, 341, 390, 12496, 34889, 293, 3971, 340, 2904, 34889, 13, 51264, 51264, 407, 718, 311, 536, 437, 2011, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.19614340129651522, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.9524308299878612e-05}, {"id": 442, "seek": 270700, "start": 2725.0, "end": 2729.0, "text": " So let's see what happened.", "tokens": [50364, 19726, 337, 2698, 46533, 2539, 13, 50514, 50514, 1726, 709, 2962, 11, 291, 787, 1319, 264, 34889, 11, 370, 820, 747, 257, 1339, 13, 50714, 50714, 1033, 11, 1392, 370, 264, 1036, 732, 300, 321, 19779, 466, 689, 613, 1074, 558, 13, 407, 341, 390, 406, 12496, 11, 293, 341, 390, 11, 370, 341, 390, 12496, 34889, 293, 3971, 340, 2904, 34889, 13, 51264, 51264, 407, 718, 311, 536, 437, 2011, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.19614340129651522, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.9524308299878612e-05}, {"id": 443, "seek": 272900, "start": 2729.0, "end": 2742.0, "text": " Yeah, so you can see the training accuracy. At some point started going higher, once the unfreezing thing happened. Also trained a lot longer for sure so and it's, we didn't say see it so it can be the random minute as well.", "tokens": [50364, 865, 11, 370, 291, 393, 536, 264, 3097, 14170, 13, 1711, 512, 935, 1409, 516, 2946, 11, 1564, 264, 3971, 701, 8781, 551, 2011, 13, 2743, 8895, 257, 688, 2854, 337, 988, 370, 293, 309, 311, 11, 321, 994, 380, 584, 536, 309, 370, 309, 393, 312, 264, 4974, 3456, 382, 731, 13, 51014, 51014, 400, 550, 264, 3847, 4470, 382, 731, 370, 309, 733, 295, 3719, 516, 760, 11, 293, 341, 3012, 493, 510, 307, 570, 295, 264, 3971, 701, 8781, 644, 11, 558, 11, 370, 300, 311, 562, 300, 2011, 13, 5135, 11, 286, 519, 309, 311, 926, 510, 11, 457, 309, 390, 1217, 3126, 412, 341, 935, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1426566745923913, "compression_ratio": 1.7452471482889733, "no_speech_prob": 7.5275374911143444e-06}, {"id": 444, "seek": 272900, "start": 2742.0, "end": 2754.0, "text": " And then the train loss as well so it kind of starts going down, and this jump up here is because of the unfreezing part, right, so that's when that happened. Actually, I think it's around here, but it was already lower at this point.", "tokens": [50364, 865, 11, 370, 291, 393, 536, 264, 3097, 14170, 13, 1711, 512, 935, 1409, 516, 2946, 11, 1564, 264, 3971, 701, 8781, 551, 2011, 13, 2743, 8895, 257, 688, 2854, 337, 988, 370, 293, 309, 311, 11, 321, 994, 380, 584, 536, 309, 370, 309, 393, 312, 264, 4974, 3456, 382, 731, 13, 51014, 51014, 400, 550, 264, 3847, 4470, 382, 731, 370, 309, 733, 295, 3719, 516, 760, 11, 293, 341, 3012, 493, 510, 307, 570, 295, 264, 3971, 701, 8781, 644, 11, 558, 11, 370, 300, 311, 562, 300, 2011, 13, 5135, 11, 286, 519, 309, 311, 926, 510, 11, 457, 309, 390, 1217, 3126, 412, 341, 935, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.1426566745923913, "compression_ratio": 1.7452471482889733, "no_speech_prob": 7.5275374911143444e-06}, {"id": 445, "seek": 275400, "start": 2754.0, "end": 2759.0, "text": " Obviously because the train longer but on freezing tends to give you a better performance over time.", "tokens": [50364, 7580, 570, 264, 3847, 2854, 457, 322, 20200, 12258, 281, 976, 291, 257, 1101, 3389, 670, 565, 13, 50614, 50614, 583, 286, 519, 309, 311, 534, 2685, 281, 428, 5633, 13, 50764, 50764, 407, 11, 718, 311, 764, 2698, 46533, 2539, 2602, 13, 50964, 50964, 492, 362, 411, 11, 286, 914, 309, 311, 337, 341, 1036, 644, 13, 865, 11, 370, 2602, 295, 1228, 341, 1472, 295, 2625, 286, 478, 445, 516, 281, 764, 264, 18135, 2316, 11, 558, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.17534629503885904, "compression_ratio": 1.4577777777777778, "no_speech_prob": 6.854046205262421e-06}, {"id": 446, "seek": 275400, "start": 2759.0, "end": 2762.0, "text": " But I think it's really specific to your task.", "tokens": [50364, 7580, 570, 264, 3847, 2854, 457, 322, 20200, 12258, 281, 976, 291, 257, 1101, 3389, 670, 565, 13, 50614, 50614, 583, 286, 519, 309, 311, 534, 2685, 281, 428, 5633, 13, 50764, 50764, 407, 11, 718, 311, 764, 2698, 46533, 2539, 2602, 13, 50964, 50964, 492, 362, 411, 11, 286, 914, 309, 311, 337, 341, 1036, 644, 13, 865, 11, 370, 2602, 295, 1228, 341, 1472, 295, 2625, 286, 478, 445, 516, 281, 764, 264, 18135, 2316, 11, 558, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.17534629503885904, "compression_ratio": 1.4577777777777778, "no_speech_prob": 6.854046205262421e-06}, {"id": 447, "seek": 275400, "start": 2762.0, "end": 2766.0, "text": " So, let's use self supervised learning instead.", "tokens": [50364, 7580, 570, 264, 3847, 2854, 457, 322, 20200, 12258, 281, 976, 291, 257, 1101, 3389, 670, 565, 13, 50614, 50614, 583, 286, 519, 309, 311, 534, 2685, 281, 428, 5633, 13, 50764, 50764, 407, 11, 718, 311, 764, 2698, 46533, 2539, 2602, 13, 50964, 50964, 492, 362, 411, 11, 286, 914, 309, 311, 337, 341, 1036, 644, 13, 865, 11, 370, 2602, 295, 1228, 341, 1472, 295, 2625, 286, 478, 445, 516, 281, 764, 264, 18135, 2316, 11, 558, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.17534629503885904, "compression_ratio": 1.4577777777777778, "no_speech_prob": 6.854046205262421e-06}, {"id": 448, "seek": 275400, "start": 2766.0, "end": 2774.0, "text": " We have like, I mean it's for this last part. Yeah, so instead of using this rest of 50 I'm just going to use the swap model, right.", "tokens": [50364, 7580, 570, 264, 3847, 2854, 457, 322, 20200, 12258, 281, 976, 291, 257, 1101, 3389, 670, 565, 13, 50614, 50614, 583, 286, 519, 309, 311, 534, 2685, 281, 428, 5633, 13, 50764, 50764, 407, 11, 718, 311, 764, 2698, 46533, 2539, 2602, 13, 50964, 50964, 492, 362, 411, 11, 286, 914, 309, 311, 337, 341, 1036, 644, 13, 865, 11, 370, 2602, 295, 1228, 341, 1472, 295, 2625, 286, 478, 445, 516, 281, 764, 264, 18135, 2316, 11, 558, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.17534629503885904, "compression_ratio": 1.4577777777777778, "no_speech_prob": 6.854046205262421e-06}, {"id": 449, "seek": 277400, "start": 2774.0, "end": 2785.0, "text": " So, all I'm going to do now is I'm going to look at bolts, and I'm going to, I'm going to load those models, right. So here is the path, right so you just have to give it a weights path.", "tokens": [50364, 407, 11, 439, 286, 478, 516, 281, 360, 586, 307, 286, 478, 516, 281, 574, 412, 18127, 11, 293, 286, 478, 516, 281, 11, 286, 478, 516, 281, 3677, 729, 5245, 11, 558, 13, 407, 510, 307, 264, 3100, 11, 558, 370, 291, 445, 362, 281, 976, 309, 257, 17443, 3100, 13, 50914, 50914, 400, 291, 393, 915, 300, 294, 264, 45623, 11, 558, 370, 309, 311, 445, 337, 385, 855, 385, 437, 307, 341, 18135, 551, 88, 13, 865, 11, 370, 18135, 307, 472, 295, 264, 6792, 7150, 1348, 484, 295, 3143, 13, 51414, 51414, 407, 341, 307, 1293, 370, 286, 478, 516, 281, 574, 412, 2698, 46533, 2539, 13, 876, 11, 1392, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19533747184176406, "compression_ratio": 1.7721518987341771, "no_speech_prob": 9.223129382007755e-06}, {"id": 450, "seek": 277400, "start": 2785.0, "end": 2795.0, "text": " And you can find that in the docs, right so it's just for me show me what is this swap thingy. Yeah, so swap is one of the latest methods coming out of fair.", "tokens": [50364, 407, 11, 439, 286, 478, 516, 281, 360, 586, 307, 286, 478, 516, 281, 574, 412, 18127, 11, 293, 286, 478, 516, 281, 11, 286, 478, 516, 281, 3677, 729, 5245, 11, 558, 13, 407, 510, 307, 264, 3100, 11, 558, 370, 291, 445, 362, 281, 976, 309, 257, 17443, 3100, 13, 50914, 50914, 400, 291, 393, 915, 300, 294, 264, 45623, 11, 558, 370, 309, 311, 445, 337, 385, 855, 385, 437, 307, 341, 18135, 551, 88, 13, 865, 11, 370, 18135, 307, 472, 295, 264, 6792, 7150, 1348, 484, 295, 3143, 13, 51414, 51414, 407, 341, 307, 1293, 370, 286, 478, 516, 281, 574, 412, 2698, 46533, 2539, 13, 876, 11, 1392, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19533747184176406, "compression_ratio": 1.7721518987341771, "no_speech_prob": 9.223129382007755e-06}, {"id": 451, "seek": 277400, "start": 2795.0, "end": 2801.0, "text": " So this is both so I'm going to look at self supervised learning. Oh, okay.", "tokens": [50364, 407, 11, 439, 286, 478, 516, 281, 360, 586, 307, 286, 478, 516, 281, 574, 412, 18127, 11, 293, 286, 478, 516, 281, 11, 286, 478, 516, 281, 3677, 729, 5245, 11, 558, 13, 407, 510, 307, 264, 3100, 11, 558, 370, 291, 445, 362, 281, 976, 309, 257, 17443, 3100, 13, 50914, 50914, 400, 291, 393, 915, 300, 294, 264, 45623, 11, 558, 370, 309, 311, 445, 337, 385, 855, 385, 437, 307, 341, 18135, 551, 88, 13, 865, 11, 370, 18135, 307, 472, 295, 264, 6792, 7150, 1348, 484, 295, 3143, 13, 51414, 51414, 407, 341, 307, 1293, 370, 286, 478, 516, 281, 574, 412, 2698, 46533, 2539, 13, 876, 11, 1392, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.19533747184176406, "compression_ratio": 1.7721518987341771, "no_speech_prob": 9.223129382007755e-06}, {"id": 452, "seek": 280100, "start": 2801.0, "end": 2812.0, "text": " And if it's here I'm going to use swab. And then I have the image in that baseline here. And then I know I can load it through here so I can just copy paste this right is there also a link to the paper somewhere here.", "tokens": [50364, 400, 498, 309, 311, 510, 286, 478, 516, 281, 764, 49840, 13, 400, 550, 286, 362, 264, 3256, 294, 300, 20518, 510, 13, 400, 550, 286, 458, 286, 393, 3677, 309, 807, 510, 370, 286, 393, 445, 5055, 9163, 341, 558, 307, 456, 611, 257, 2113, 281, 264, 3035, 4079, 510, 13, 50914, 50914, 286, 1697, 370, 13, 961, 311, 536, 13, 51064, 51064, 865, 11, 456, 307, 13, 51164, 51164, 1033, 11, 1392, 13, 286, 536, 13, 51264, 51264, 400, 550, 291, 393, 11, 291, 458, 11, 370, 309, 311, 20871, 490, 264, 4783, 2535, 321, 767, 2732, 365, 6789, 34121, 281, 360, 341, 382, 731, 13, 51564, 51564, 407, 309, 311, 1687, 4961, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.19407930374145507, "compression_ratio": 1.6332046332046333, "no_speech_prob": 3.119923349004239e-05}, {"id": 453, "seek": 280100, "start": 2812.0, "end": 2815.0, "text": " I believe so. Let's see.", "tokens": [50364, 400, 498, 309, 311, 510, 286, 478, 516, 281, 764, 49840, 13, 400, 550, 286, 362, 264, 3256, 294, 300, 20518, 510, 13, 400, 550, 286, 458, 286, 393, 3677, 309, 807, 510, 370, 286, 393, 445, 5055, 9163, 341, 558, 307, 456, 611, 257, 2113, 281, 264, 3035, 4079, 510, 13, 50914, 50914, 286, 1697, 370, 13, 961, 311, 536, 13, 51064, 51064, 865, 11, 456, 307, 13, 51164, 51164, 1033, 11, 1392, 13, 286, 536, 13, 51264, 51264, 400, 550, 291, 393, 11, 291, 458, 11, 370, 309, 311, 20871, 490, 264, 4783, 2535, 321, 767, 2732, 365, 6789, 34121, 281, 360, 341, 382, 731, 13, 51564, 51564, 407, 309, 311, 1687, 4961, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.19407930374145507, "compression_ratio": 1.6332046332046333, "no_speech_prob": 3.119923349004239e-05}, {"id": 454, "seek": 280100, "start": 2815.0, "end": 2817.0, "text": " Yeah, there is.", "tokens": [50364, 400, 498, 309, 311, 510, 286, 478, 516, 281, 764, 49840, 13, 400, 550, 286, 362, 264, 3256, 294, 300, 20518, 510, 13, 400, 550, 286, 458, 286, 393, 3677, 309, 807, 510, 370, 286, 393, 445, 5055, 9163, 341, 558, 307, 456, 611, 257, 2113, 281, 264, 3035, 4079, 510, 13, 50914, 50914, 286, 1697, 370, 13, 961, 311, 536, 13, 51064, 51064, 865, 11, 456, 307, 13, 51164, 51164, 1033, 11, 1392, 13, 286, 536, 13, 51264, 51264, 400, 550, 291, 393, 11, 291, 458, 11, 370, 309, 311, 20871, 490, 264, 4783, 2535, 321, 767, 2732, 365, 6789, 34121, 281, 360, 341, 382, 731, 13, 51564, 51564, 407, 309, 311, 1687, 4961, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.19407930374145507, "compression_ratio": 1.6332046332046333, "no_speech_prob": 3.119923349004239e-05}, {"id": 455, "seek": 280100, "start": 2817.0, "end": 2819.0, "text": " Okay, okay. I see.", "tokens": [50364, 400, 498, 309, 311, 510, 286, 478, 516, 281, 764, 49840, 13, 400, 550, 286, 362, 264, 3256, 294, 300, 20518, 510, 13, 400, 550, 286, 458, 286, 393, 3677, 309, 807, 510, 370, 286, 393, 445, 5055, 9163, 341, 558, 307, 456, 611, 257, 2113, 281, 264, 3035, 4079, 510, 13, 50914, 50914, 286, 1697, 370, 13, 961, 311, 536, 13, 51064, 51064, 865, 11, 456, 307, 13, 51164, 51164, 1033, 11, 1392, 13, 286, 536, 13, 51264, 51264, 400, 550, 291, 393, 11, 291, 458, 11, 370, 309, 311, 20871, 490, 264, 4783, 2535, 321, 767, 2732, 365, 6789, 34121, 281, 360, 341, 382, 731, 13, 51564, 51564, 407, 309, 311, 1687, 4961, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.19407930374145507, "compression_ratio": 1.6332046332046333, "no_speech_prob": 3.119923349004239e-05}, {"id": 456, "seek": 280100, "start": 2819.0, "end": 2825.0, "text": " And then you can, you know, so it's adapted from the official position we actually worked with Matilda to do this as well.", "tokens": [50364, 400, 498, 309, 311, 510, 286, 478, 516, 281, 764, 49840, 13, 400, 550, 286, 362, 264, 3256, 294, 300, 20518, 510, 13, 400, 550, 286, 458, 286, 393, 3677, 309, 807, 510, 370, 286, 393, 445, 5055, 9163, 341, 558, 307, 456, 611, 257, 2113, 281, 264, 3035, 4079, 510, 13, 50914, 50914, 286, 1697, 370, 13, 961, 311, 536, 13, 51064, 51064, 865, 11, 456, 307, 13, 51164, 51164, 1033, 11, 1392, 13, 286, 536, 13, 51264, 51264, 400, 550, 291, 393, 11, 291, 458, 11, 370, 309, 311, 20871, 490, 264, 4783, 2535, 321, 767, 2732, 365, 6789, 34121, 281, 360, 341, 382, 731, 13, 51564, 51564, 407, 309, 311, 1687, 4961, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.19407930374145507, "compression_ratio": 1.6332046332046333, "no_speech_prob": 3.119923349004239e-05}, {"id": 457, "seek": 280100, "start": 2825.0, "end": 2827.0, "text": " So it's super helpful.", "tokens": [50364, 400, 498, 309, 311, 510, 286, 478, 516, 281, 764, 49840, 13, 400, 550, 286, 362, 264, 3256, 294, 300, 20518, 510, 13, 400, 550, 286, 458, 286, 393, 3677, 309, 807, 510, 370, 286, 393, 445, 5055, 9163, 341, 558, 307, 456, 611, 257, 2113, 281, 264, 3035, 4079, 510, 13, 50914, 50914, 286, 1697, 370, 13, 961, 311, 536, 13, 51064, 51064, 865, 11, 456, 307, 13, 51164, 51164, 1033, 11, 1392, 13, 286, 536, 13, 51264, 51264, 400, 550, 291, 393, 11, 291, 458, 11, 370, 309, 311, 20871, 490, 264, 4783, 2535, 321, 767, 2732, 365, 6789, 34121, 281, 360, 341, 382, 731, 13, 51564, 51564, 407, 309, 311, 1687, 4961, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.19407930374145507, "compression_ratio": 1.6332046332046333, "no_speech_prob": 3.119923349004239e-05}, {"id": 458, "seek": 282700, "start": 2827.0, "end": 2835.0, "text": " And, and yeah so here's the years, I'm just going to copy this right so I can. All right, yeah, this.", "tokens": [50364, 400, 11, 293, 1338, 370, 510, 311, 264, 924, 11, 286, 478, 445, 516, 281, 5055, 341, 558, 370, 286, 393, 13, 1057, 558, 11, 1338, 11, 341, 13, 50764, 50764, 4919, 13, 50864, 50864, 16755, 452, 9719, 1177, 380, 589, 13, 1033, 13, 51014, 51014, 407, 300, 311, 439, 286, 643, 281, 360, 13, 407, 286, 478, 516, 281, 1565, 300, 2146, 294, 13, 51214, 51214, 400, 286, 478, 406, 516, 281, 15959, 1340, 370, 286, 483, 49840, 510, 13, 3769, 13, 407, 309, 311, 516, 281, 3677, 264, 42269, 337, 300, 13, 400, 550, 49840, 575, 341, 2316, 1854, 309, 11, 597, 307, 264, 34889, 558, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.16368671856095304, "compression_ratio": 1.6359832635983265, "no_speech_prob": 1.3845615285390522e-05}, {"id": 459, "seek": 282700, "start": 2835.0, "end": 2837.0, "text": " Sorry.", "tokens": [50364, 400, 11, 293, 1338, 370, 510, 311, 264, 924, 11, 286, 478, 445, 516, 281, 5055, 341, 558, 370, 286, 393, 13, 1057, 558, 11, 1338, 11, 341, 13, 50764, 50764, 4919, 13, 50864, 50864, 16755, 452, 9719, 1177, 380, 589, 13, 1033, 13, 51014, 51014, 407, 300, 311, 439, 286, 643, 281, 360, 13, 407, 286, 478, 516, 281, 1565, 300, 2146, 294, 13, 51214, 51214, 400, 286, 478, 406, 516, 281, 15959, 1340, 370, 286, 483, 49840, 510, 13, 3769, 13, 407, 309, 311, 516, 281, 3677, 264, 42269, 337, 300, 13, 400, 550, 49840, 575, 341, 2316, 1854, 309, 11, 597, 307, 264, 34889, 558, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.16368671856095304, "compression_ratio": 1.6359832635983265, "no_speech_prob": 1.3845615285390522e-05}, {"id": 460, "seek": 282700, "start": 2837.0, "end": 2840.0, "text": " Apparently my mouse doesn't work. Okay.", "tokens": [50364, 400, 11, 293, 1338, 370, 510, 311, 264, 924, 11, 286, 478, 445, 516, 281, 5055, 341, 558, 370, 286, 393, 13, 1057, 558, 11, 1338, 11, 341, 13, 50764, 50764, 4919, 13, 50864, 50864, 16755, 452, 9719, 1177, 380, 589, 13, 1033, 13, 51014, 51014, 407, 300, 311, 439, 286, 643, 281, 360, 13, 407, 286, 478, 516, 281, 1565, 300, 2146, 294, 13, 51214, 51214, 400, 286, 478, 406, 516, 281, 15959, 1340, 370, 286, 483, 49840, 510, 13, 3769, 13, 407, 309, 311, 516, 281, 3677, 264, 42269, 337, 300, 13, 400, 550, 49840, 575, 341, 2316, 1854, 309, 11, 597, 307, 264, 34889, 558, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.16368671856095304, "compression_ratio": 1.6359832635983265, "no_speech_prob": 1.3845615285390522e-05}, {"id": 461, "seek": 282700, "start": 2840.0, "end": 2844.0, "text": " So that's all I need to do. So I'm going to bring that guy in.", "tokens": [50364, 400, 11, 293, 1338, 370, 510, 311, 264, 924, 11, 286, 478, 445, 516, 281, 5055, 341, 558, 370, 286, 393, 13, 1057, 558, 11, 1338, 11, 341, 13, 50764, 50764, 4919, 13, 50864, 50864, 16755, 452, 9719, 1177, 380, 589, 13, 1033, 13, 51014, 51014, 407, 300, 311, 439, 286, 643, 281, 360, 13, 407, 286, 478, 516, 281, 1565, 300, 2146, 294, 13, 51214, 51214, 400, 286, 478, 406, 516, 281, 15959, 1340, 370, 286, 483, 49840, 510, 13, 3769, 13, 407, 309, 311, 516, 281, 3677, 264, 42269, 337, 300, 13, 400, 550, 49840, 575, 341, 2316, 1854, 309, 11, 597, 307, 264, 34889, 558, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.16368671856095304, "compression_ratio": 1.6359832635983265, "no_speech_prob": 1.3845615285390522e-05}, {"id": 462, "seek": 282700, "start": 2844.0, "end": 2854.0, "text": " And I'm not going to freeze anything so I get swab here. Great. So it's going to load the checkpoint for that. And then swab has this model inside it, which is the backbone right.", "tokens": [50364, 400, 11, 293, 1338, 370, 510, 311, 264, 924, 11, 286, 478, 445, 516, 281, 5055, 341, 558, 370, 286, 393, 13, 1057, 558, 11, 1338, 11, 341, 13, 50764, 50764, 4919, 13, 50864, 50864, 16755, 452, 9719, 1177, 380, 589, 13, 1033, 13, 51014, 51014, 407, 300, 311, 439, 286, 643, 281, 360, 13, 407, 286, 478, 516, 281, 1565, 300, 2146, 294, 13, 51214, 51214, 400, 286, 478, 406, 516, 281, 15959, 1340, 370, 286, 483, 49840, 510, 13, 3769, 13, 407, 309, 311, 516, 281, 3677, 264, 42269, 337, 300, 13, 400, 550, 49840, 575, 341, 2316, 1854, 309, 11, 597, 307, 264, 34889, 558, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.16368671856095304, "compression_ratio": 1.6359832635983265, "no_speech_prob": 1.3845615285390522e-05}, {"id": 463, "seek": 285400, "start": 2854.0, "end": 2859.0, "text": " So I'm just going to pull that out so swab that model. So that's going to be the backbone of swab.", "tokens": [50364, 407, 286, 478, 445, 516, 281, 2235, 300, 484, 370, 49840, 300, 2316, 13, 407, 300, 311, 516, 281, 312, 264, 34889, 295, 49840, 13, 50614, 50614, 400, 550, 49840, 294, 1729, 23930, 20984, 4122, 13, 1779, 11, 370, 286, 478, 445, 516, 281, 1319, 300, 281, 20984, 13, 51014, 51014, 400, 550, 11, 294, 341, 1729, 1389, 11, 49840, 341, 2316, 611, 23930, 281, 4111, 11317, 11, 370, 286, 445, 528, 264, 1036, 472, 11, 286, 500, 380, 643, 13, 51414, 51414, 639, 307, 257, 659, 8895, 2316, 307, 309, 13, 865, 11, 309, 311, 257, 659, 8895, 2316, 13, 12753, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.16797557723856418, "compression_ratio": 1.7767857142857142, "no_speech_prob": 1.892404725367669e-05}, {"id": 464, "seek": 285400, "start": 2859.0, "end": 2867.0, "text": " And then swab in particular outputs 3000 features. Right, so I'm just going to change that to 3000.", "tokens": [50364, 407, 286, 478, 445, 516, 281, 2235, 300, 484, 370, 49840, 300, 2316, 13, 407, 300, 311, 516, 281, 312, 264, 34889, 295, 49840, 13, 50614, 50614, 400, 550, 49840, 294, 1729, 23930, 20984, 4122, 13, 1779, 11, 370, 286, 478, 445, 516, 281, 1319, 300, 281, 20984, 13, 51014, 51014, 400, 550, 11, 294, 341, 1729, 1389, 11, 49840, 341, 2316, 611, 23930, 281, 4111, 11317, 11, 370, 286, 445, 528, 264, 1036, 472, 11, 286, 500, 380, 643, 13, 51414, 51414, 639, 307, 257, 659, 8895, 2316, 307, 309, 13, 865, 11, 309, 311, 257, 659, 8895, 2316, 13, 12753, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.16797557723856418, "compression_ratio": 1.7767857142857142, "no_speech_prob": 1.892404725367669e-05}, {"id": 465, "seek": 285400, "start": 2867.0, "end": 2875.0, "text": " And then, in this particular case, swab this model also outputs to feature maps, so I just want the last one, I don't need.", "tokens": [50364, 407, 286, 478, 445, 516, 281, 2235, 300, 484, 370, 49840, 300, 2316, 13, 407, 300, 311, 516, 281, 312, 264, 34889, 295, 49840, 13, 50614, 50614, 400, 550, 49840, 294, 1729, 23930, 20984, 4122, 13, 1779, 11, 370, 286, 478, 445, 516, 281, 1319, 300, 281, 20984, 13, 51014, 51014, 400, 550, 11, 294, 341, 1729, 1389, 11, 49840, 341, 2316, 611, 23930, 281, 4111, 11317, 11, 370, 286, 445, 528, 264, 1036, 472, 11, 286, 500, 380, 643, 13, 51414, 51414, 639, 307, 257, 659, 8895, 2316, 307, 309, 13, 865, 11, 309, 311, 257, 659, 8895, 2316, 13, 12753, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.16797557723856418, "compression_ratio": 1.7767857142857142, "no_speech_prob": 1.892404725367669e-05}, {"id": 466, "seek": 285400, "start": 2875.0, "end": 2879.0, "text": " This is a pre trained model is it. Yeah, it's a pre trained model. Correct.", "tokens": [50364, 407, 286, 478, 445, 516, 281, 2235, 300, 484, 370, 49840, 300, 2316, 13, 407, 300, 311, 516, 281, 312, 264, 34889, 295, 49840, 13, 50614, 50614, 400, 550, 49840, 294, 1729, 23930, 20984, 4122, 13, 1779, 11, 370, 286, 478, 445, 516, 281, 1319, 300, 281, 20984, 13, 51014, 51014, 400, 550, 11, 294, 341, 1729, 1389, 11, 49840, 341, 2316, 611, 23930, 281, 4111, 11317, 11, 370, 286, 445, 528, 264, 1036, 472, 11, 286, 500, 380, 643, 13, 51414, 51414, 639, 307, 257, 659, 8895, 2316, 307, 309, 13, 865, 11, 309, 311, 257, 659, 8895, 2316, 13, 12753, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.16797557723856418, "compression_ratio": 1.7767857142857142, "no_speech_prob": 1.892404725367669e-05}, {"id": 467, "seek": 287900, "start": 2879.0, "end": 2889.0, "text": " It's pre trained on images without labels. Is it correct. Yes, so it's sub supervised so it was pre trained on image net without any labels whatsoever.", "tokens": [50364, 467, 311, 659, 8895, 322, 5267, 1553, 16949, 13, 1119, 309, 3006, 13, 1079, 11, 370, 309, 311, 1422, 46533, 370, 309, 390, 659, 8895, 322, 3256, 2533, 1553, 604, 16949, 17076, 13, 50864, 50864, 407, 264, 1558, 13, 50964, 50964, 4546, 321, 645, 884, 13, 407, 321, 366, 884, 5003, 2539, 13, 400, 949, 321, 645, 884, 5003, 2539, 490, 46533, 2539, 1228, 257, 3209, 3847, 322, 3256, 2533, 13, 51364, 51364, 1779, 586, 321, 366, 884, 920, 5003, 2539, 457, 1228, 257, 3209, 300, 575, 668, 8895, 322, 11, 321, 500, 380, 458, 437, 11, 457, 1553, 16949, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14306375412713915, "compression_ratio": 2.072398190045249, "no_speech_prob": 4.3311360059306026e-05}, {"id": 468, "seek": 287900, "start": 2889.0, "end": 2891.0, "text": " So the idea.", "tokens": [50364, 467, 311, 659, 8895, 322, 5267, 1553, 16949, 13, 1119, 309, 3006, 13, 1079, 11, 370, 309, 311, 1422, 46533, 370, 309, 390, 659, 8895, 322, 3256, 2533, 1553, 604, 16949, 17076, 13, 50864, 50864, 407, 264, 1558, 13, 50964, 50964, 4546, 321, 645, 884, 13, 407, 321, 366, 884, 5003, 2539, 13, 400, 949, 321, 645, 884, 5003, 2539, 490, 46533, 2539, 1228, 257, 3209, 3847, 322, 3256, 2533, 13, 51364, 51364, 1779, 586, 321, 366, 884, 920, 5003, 2539, 457, 1228, 257, 3209, 300, 575, 668, 8895, 322, 11, 321, 500, 380, 458, 437, 11, 457, 1553, 16949, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14306375412713915, "compression_ratio": 2.072398190045249, "no_speech_prob": 4.3311360059306026e-05}, {"id": 469, "seek": 287900, "start": 2891.0, "end": 2899.0, "text": " Before we were doing. So we are doing transfer learning. And before we were doing transfer learning from supervised learning using a network train on image net.", "tokens": [50364, 467, 311, 659, 8895, 322, 5267, 1553, 16949, 13, 1119, 309, 3006, 13, 1079, 11, 370, 309, 311, 1422, 46533, 370, 309, 390, 659, 8895, 322, 3256, 2533, 1553, 604, 16949, 17076, 13, 50864, 50864, 407, 264, 1558, 13, 50964, 50964, 4546, 321, 645, 884, 13, 407, 321, 366, 884, 5003, 2539, 13, 400, 949, 321, 645, 884, 5003, 2539, 490, 46533, 2539, 1228, 257, 3209, 3847, 322, 3256, 2533, 13, 51364, 51364, 1779, 586, 321, 366, 884, 920, 5003, 2539, 457, 1228, 257, 3209, 300, 575, 668, 8895, 322, 11, 321, 500, 380, 458, 437, 11, 457, 1553, 16949, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14306375412713915, "compression_ratio": 2.072398190045249, "no_speech_prob": 4.3311360059306026e-05}, {"id": 470, "seek": 287900, "start": 2899.0, "end": 2907.0, "text": " Right now we are doing still transfer learning but using a network that has been trained on, we don't know what, but without labels.", "tokens": [50364, 467, 311, 659, 8895, 322, 5267, 1553, 16949, 13, 1119, 309, 3006, 13, 1079, 11, 370, 309, 311, 1422, 46533, 370, 309, 390, 659, 8895, 322, 3256, 2533, 1553, 604, 16949, 17076, 13, 50864, 50864, 407, 264, 1558, 13, 50964, 50964, 4546, 321, 645, 884, 13, 407, 321, 366, 884, 5003, 2539, 13, 400, 949, 321, 645, 884, 5003, 2539, 490, 46533, 2539, 1228, 257, 3209, 3847, 322, 3256, 2533, 13, 51364, 51364, 1779, 586, 321, 366, 884, 920, 5003, 2539, 457, 1228, 257, 3209, 300, 575, 668, 8895, 322, 11, 321, 500, 380, 458, 437, 11, 457, 1553, 16949, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.14306375412713915, "compression_ratio": 2.072398190045249, "no_speech_prob": 4.3311360059306026e-05}, {"id": 471, "seek": 290700, "start": 2907.0, "end": 2924.0, "text": " So maybe that it's nice because I can pre train my swab model on my own data. Yeah, I don't have many labels, for example, and then I can just train the classifier with a few labels I have right.", "tokens": [50364, 407, 1310, 300, 309, 311, 1481, 570, 286, 393, 659, 3847, 452, 49840, 2316, 322, 452, 1065, 1412, 13, 865, 11, 286, 500, 380, 362, 867, 16949, 11, 337, 1365, 11, 293, 550, 286, 393, 445, 3847, 264, 1508, 9902, 365, 257, 1326, 16949, 286, 362, 558, 13, 51214, 51214, 865, 11, 300, 311, 257, 665, 935, 13, 407, 498, 291, 366, 11, 286, 914, 11, 498, 291, 434, 1364, 412, 257, 2237, 11, 881, 3700, 291, 434, 516, 281, 362, 428, 1065, 1412, 992, 300, 575, 11, 291, 458, 11, 1062, 312, 5994, 457, 309, 486, 2063, 257, 688, 295, 1460, 281, 7645, 370, 291, 500, 380, 643, 281, 3292, 466, 309, 886, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1247046735106396, "compression_ratio": 1.6996197718631179, "no_speech_prob": 1.5687835912103765e-05}, {"id": 472, "seek": 290700, "start": 2924.0, "end": 2936.0, "text": " Yeah, that's a good point. So if you are, I mean, if you're working at a company, most likely you're going to have your own data set that has, you know, might be massive but it will cost a lot of money to label so you don't need to worry about it too,", "tokens": [50364, 407, 1310, 300, 309, 311, 1481, 570, 286, 393, 659, 3847, 452, 49840, 2316, 322, 452, 1065, 1412, 13, 865, 11, 286, 500, 380, 362, 867, 16949, 11, 337, 1365, 11, 293, 550, 286, 393, 445, 3847, 264, 1508, 9902, 365, 257, 1326, 16949, 286, 362, 558, 13, 51214, 51214, 865, 11, 300, 311, 257, 665, 935, 13, 407, 498, 291, 366, 11, 286, 914, 11, 498, 291, 434, 1364, 412, 257, 2237, 11, 881, 3700, 291, 434, 516, 281, 362, 428, 1065, 1412, 992, 300, 575, 11, 291, 458, 11, 1062, 312, 5994, 457, 309, 486, 2063, 257, 688, 295, 1460, 281, 7645, 370, 291, 500, 380, 643, 281, 3292, 466, 309, 886, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1247046735106396, "compression_ratio": 1.6996197718631179, "no_speech_prob": 1.5687835912103765e-05}, {"id": 473, "seek": 293600, "start": 2936.0, "end": 2948.0, "text": " so much maybe until this stuff works really well, you might have to but you know you can try this out, and then pre train on supervise on that. And then, and then use that. So, yeah, that makes sense.", "tokens": [50364, 370, 709, 1310, 1826, 341, 1507, 1985, 534, 731, 11, 291, 1062, 362, 281, 457, 291, 458, 291, 393, 853, 341, 484, 11, 293, 550, 659, 3847, 322, 37971, 908, 322, 300, 13, 400, 550, 11, 293, 550, 764, 300, 13, 407, 11, 1338, 11, 300, 1669, 2020, 13, 50964, 50964, 1033, 13, 51014, 51014, 1033, 11, 370, 321, 483, 264, 34889, 20984, 4122, 13, 3764, 11, 341, 1729, 2316, 307, 516, 281, 362, 732, 4111, 11317, 13, 467, 311, 445, 264, 636, 309, 311, 8895, 370, 286, 445, 528, 264, 1036, 472, 286, 500, 380, 528, 1293, 13, 1779, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.16015922909691221, "compression_ratio": 1.6219512195121952, "no_speech_prob": 2.7106596462544985e-05}, {"id": 474, "seek": 293600, "start": 2948.0, "end": 2949.0, "text": " Okay.", "tokens": [50364, 370, 709, 1310, 1826, 341, 1507, 1985, 534, 731, 11, 291, 1062, 362, 281, 457, 291, 458, 291, 393, 853, 341, 484, 11, 293, 550, 659, 3847, 322, 37971, 908, 322, 300, 13, 400, 550, 11, 293, 550, 764, 300, 13, 407, 11, 1338, 11, 300, 1669, 2020, 13, 50964, 50964, 1033, 13, 51014, 51014, 1033, 11, 370, 321, 483, 264, 34889, 20984, 4122, 13, 3764, 11, 341, 1729, 2316, 307, 516, 281, 362, 732, 4111, 11317, 13, 467, 311, 445, 264, 636, 309, 311, 8895, 370, 286, 445, 528, 264, 1036, 472, 286, 500, 380, 528, 1293, 13, 1779, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.16015922909691221, "compression_ratio": 1.6219512195121952, "no_speech_prob": 2.7106596462544985e-05}, {"id": 475, "seek": 293600, "start": 2949.0, "end": 2960.0, "text": " Okay, so we get the backbone 3000 features. Again, this particular model is going to have two feature maps. It's just the way it's trained so I just want the last one I don't want both. Right.", "tokens": [50364, 370, 709, 1310, 1826, 341, 1507, 1985, 534, 731, 11, 291, 1062, 362, 281, 457, 291, 458, 291, 393, 853, 341, 484, 11, 293, 550, 659, 3847, 322, 37971, 908, 322, 300, 13, 400, 550, 11, 293, 550, 764, 300, 13, 407, 11, 1338, 11, 300, 1669, 2020, 13, 50964, 50964, 1033, 13, 51014, 51014, 1033, 11, 370, 321, 483, 264, 34889, 20984, 4122, 13, 3764, 11, 341, 1729, 2316, 307, 516, 281, 362, 732, 4111, 11317, 13, 467, 311, 445, 264, 636, 309, 311, 8895, 370, 286, 445, 528, 264, 1036, 472, 286, 500, 380, 528, 1293, 13, 1779, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.16015922909691221, "compression_ratio": 1.6219512195121952, "no_speech_prob": 2.7106596462544985e-05}, {"id": 476, "seek": 296000, "start": 2960.0, "end": 2975.0, "text": " So let me just make that more clear so features equals features, negative one, because this is a, this is actually an array actually, let's call it feature one and feature to right, so I just want the second one here.", "tokens": [50364, 407, 718, 385, 445, 652, 300, 544, 1850, 370, 4122, 6915, 4122, 11, 3671, 472, 11, 570, 341, 307, 257, 11, 341, 307, 767, 364, 10225, 767, 11, 718, 311, 818, 309, 4111, 472, 293, 4111, 281, 558, 11, 370, 286, 445, 528, 264, 1150, 472, 510, 13, 51114, 51114, 1033, 13, 400, 550, 286, 478, 516, 281, 360, 264, 912, 551, 510, 562, 309, 311, 3971, 340, 2904, 13, 51364, 51364, 3769, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11634875566531451, "compression_ratio": 1.586021505376344, "no_speech_prob": 1.2028850505885202e-05}, {"id": 477, "seek": 296000, "start": 2975.0, "end": 2980.0, "text": " Okay. And then I'm going to do the same thing here when it's unfrozen.", "tokens": [50364, 407, 718, 385, 445, 652, 300, 544, 1850, 370, 4122, 6915, 4122, 11, 3671, 472, 11, 570, 341, 307, 257, 11, 341, 307, 767, 364, 10225, 767, 11, 718, 311, 818, 309, 4111, 472, 293, 4111, 281, 558, 11, 370, 286, 445, 528, 264, 1150, 472, 510, 13, 51114, 51114, 1033, 13, 400, 550, 286, 478, 516, 281, 360, 264, 912, 551, 510, 562, 309, 311, 3971, 340, 2904, 13, 51364, 51364, 3769, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11634875566531451, "compression_ratio": 1.586021505376344, "no_speech_prob": 1.2028850505885202e-05}, {"id": 478, "seek": 296000, "start": 2980.0, "end": 2982.0, "text": " Great.", "tokens": [50364, 407, 718, 385, 445, 652, 300, 544, 1850, 370, 4122, 6915, 4122, 11, 3671, 472, 11, 570, 341, 307, 257, 11, 341, 307, 767, 364, 10225, 767, 11, 718, 311, 818, 309, 4111, 472, 293, 4111, 281, 558, 11, 370, 286, 445, 528, 264, 1150, 472, 510, 13, 51114, 51114, 1033, 13, 400, 550, 286, 478, 516, 281, 360, 264, 912, 551, 510, 562, 309, 311, 3971, 340, 2904, 13, 51364, 51364, 3769, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.11634875566531451, "compression_ratio": 1.586021505376344, "no_speech_prob": 1.2028850505885202e-05}, {"id": 479, "seek": 298200, "start": 2982.0, "end": 2995.0, "text": " I believe those are all the changes I need to make to get the store. So, that is it. That is the extent of modifying for self supervised learning, I told you it's going to be under 10 minutes.", "tokens": [50364, 286, 1697, 729, 366, 439, 264, 2962, 286, 643, 281, 652, 281, 483, 264, 3531, 13, 407, 11, 300, 307, 309, 13, 663, 307, 264, 8396, 295, 42626, 337, 2698, 46533, 2539, 11, 286, 1907, 291, 309, 311, 516, 281, 312, 833, 1266, 2077, 13, 51014, 51014, 286, 478, 11679, 13, 286, 1128, 767, 1096, 341, 2059, 13, 51214, 51214, 407, 718, 311, 536, 286, 500, 380, 362, 281, 1319, 1340, 13, 823, 11, 718, 311, 536, 498, 341, 1985, 13, 51464, 51464, 876, 11, 574, 412, 300, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1590568378407468, "compression_ratio": 1.5135135135135136, "no_speech_prob": 4.222631105221808e-06}, {"id": 480, "seek": 298200, "start": 2995.0, "end": 2999.0, "text": " I'm impressed. I never actually done this myself.", "tokens": [50364, 286, 1697, 729, 366, 439, 264, 2962, 286, 643, 281, 652, 281, 483, 264, 3531, 13, 407, 11, 300, 307, 309, 13, 663, 307, 264, 8396, 295, 42626, 337, 2698, 46533, 2539, 11, 286, 1907, 291, 309, 311, 516, 281, 312, 833, 1266, 2077, 13, 51014, 51014, 286, 478, 11679, 13, 286, 1128, 767, 1096, 341, 2059, 13, 51214, 51214, 407, 718, 311, 536, 286, 500, 380, 362, 281, 1319, 1340, 13, 823, 11, 718, 311, 536, 498, 341, 1985, 13, 51464, 51464, 876, 11, 574, 412, 300, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1590568378407468, "compression_ratio": 1.5135135135135136, "no_speech_prob": 4.222631105221808e-06}, {"id": 481, "seek": 298200, "start": 2999.0, "end": 3004.0, "text": " So let's see I don't have to change anything. Now, let's see if this works.", "tokens": [50364, 286, 1697, 729, 366, 439, 264, 2962, 286, 643, 281, 652, 281, 483, 264, 3531, 13, 407, 11, 300, 307, 309, 13, 663, 307, 264, 8396, 295, 42626, 337, 2698, 46533, 2539, 11, 286, 1907, 291, 309, 311, 516, 281, 312, 833, 1266, 2077, 13, 51014, 51014, 286, 478, 11679, 13, 286, 1128, 767, 1096, 341, 2059, 13, 51214, 51214, 407, 718, 311, 536, 286, 500, 380, 362, 281, 1319, 1340, 13, 823, 11, 718, 311, 536, 498, 341, 1985, 13, 51464, 51464, 876, 11, 574, 412, 300, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1590568378407468, "compression_ratio": 1.5135135135135136, "no_speech_prob": 4.222631105221808e-06}, {"id": 482, "seek": 298200, "start": 3004.0, "end": 3008.0, "text": " Oh, look at that.", "tokens": [50364, 286, 1697, 729, 366, 439, 264, 2962, 286, 643, 281, 652, 281, 483, 264, 3531, 13, 407, 11, 300, 307, 309, 13, 663, 307, 264, 8396, 295, 42626, 337, 2698, 46533, 2539, 11, 286, 1907, 291, 309, 311, 516, 281, 312, 833, 1266, 2077, 13, 51014, 51014, 286, 478, 11679, 13, 286, 1128, 767, 1096, 341, 2059, 13, 51214, 51214, 407, 718, 311, 536, 286, 500, 380, 362, 281, 1319, 1340, 13, 823, 11, 718, 311, 536, 498, 341, 1985, 13, 51464, 51464, 876, 11, 574, 412, 300, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.1590568378407468, "compression_ratio": 1.5135135135135136, "no_speech_prob": 4.222631105221808e-06}, {"id": 483, "seek": 300800, "start": 3008.0, "end": 3021.0, "text": " So we can actually, if we let it run a little bit we can also compare performance right later for the lower, it's, it's a two already like the other one didn't get the slow before and it's not even unfrozen yet.", "tokens": [50364, 407, 321, 393, 767, 11, 498, 321, 718, 309, 1190, 257, 707, 857, 321, 393, 611, 6794, 3389, 558, 1780, 337, 264, 3126, 11, 309, 311, 11, 309, 311, 257, 732, 1217, 411, 264, 661, 472, 994, 380, 483, 264, 2964, 949, 293, 309, 311, 406, 754, 3971, 340, 2904, 1939, 13, 51014, 51014, 1779, 13, 407, 11, 1338, 11, 286, 500, 380, 458, 13, 467, 311, 1880, 13, 51264, 51264, 1033, 11, 1392, 13, 876, 11, 341, 8992, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.18420766648792086, "compression_ratio": 1.4948979591836735, "no_speech_prob": 1.0952529919450171e-05}, {"id": 484, "seek": 300800, "start": 3021.0, "end": 3026.0, "text": " Right. So, yeah, I don't know. It's interesting.", "tokens": [50364, 407, 321, 393, 767, 11, 498, 321, 718, 309, 1190, 257, 707, 857, 321, 393, 611, 6794, 3389, 558, 1780, 337, 264, 3126, 11, 309, 311, 11, 309, 311, 257, 732, 1217, 411, 264, 661, 472, 994, 380, 483, 264, 2964, 949, 293, 309, 311, 406, 754, 3971, 340, 2904, 1939, 13, 51014, 51014, 1779, 13, 407, 11, 1338, 11, 286, 500, 380, 458, 13, 467, 311, 1880, 13, 51264, 51264, 1033, 11, 1392, 13, 876, 11, 341, 8992, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.18420766648792086, "compression_ratio": 1.4948979591836735, "no_speech_prob": 1.0952529919450171e-05}, {"id": 485, "seek": 300800, "start": 3026.0, "end": 3029.0, "text": " Okay, okay. Oh, this impressive.", "tokens": [50364, 407, 321, 393, 767, 11, 498, 321, 718, 309, 1190, 257, 707, 857, 321, 393, 611, 6794, 3389, 558, 1780, 337, 264, 3126, 11, 309, 311, 11, 309, 311, 257, 732, 1217, 411, 264, 661, 472, 994, 380, 483, 264, 2964, 949, 293, 309, 311, 406, 754, 3971, 340, 2904, 1939, 13, 51014, 51014, 1779, 13, 407, 11, 1338, 11, 286, 500, 380, 458, 13, 467, 311, 1880, 13, 51264, 51264, 1033, 11, 1392, 13, 876, 11, 341, 8992, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.18420766648792086, "compression_ratio": 1.4948979591836735, "no_speech_prob": 1.0952529919450171e-05}, {"id": 486, "seek": 302900, "start": 3029.0, "end": 3045.0, "text": " So let's let's wait a little bit and then we can. I guess it's below two already and you know the other one didn't get to below two until we influence the model right now we're about to freeze it because it's layer 10 so let's see what happens so now we drop so this one even didn't spike,", "tokens": [50364, 407, 718, 311, 718, 311, 1699, 257, 707, 857, 293, 550, 321, 393, 13, 286, 2041, 309, 311, 2507, 732, 1217, 293, 291, 458, 264, 661, 472, 994, 380, 483, 281, 2507, 732, 1826, 321, 6503, 264, 2316, 558, 586, 321, 434, 466, 281, 15959, 309, 570, 309, 311, 4583, 1266, 370, 718, 311, 536, 437, 2314, 370, 586, 321, 3270, 370, 341, 472, 754, 994, 380, 21053, 11, 51164, 51164, 341, 472, 994, 380, 352, 493, 281, 1045, 935, 746, 309, 445, 1409, 281, 352, 760, 13, 407, 11, 2086, 11, 502, 13, 19, 586, 309, 311, 1364, 709, 1101, 411, 11, 286, 500, 380, 458, 983, 300, 994, 380, 21053, 411, 11, 286, 519, 300, 2745, 341, 307, 364, 1269, 1859, 295, 2132, 293, 411, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.190467762224602, "compression_ratio": 1.7533783783783783, "no_speech_prob": 1.4969444237067364e-05}, {"id": 487, "seek": 302900, "start": 3045.0, "end": 3058.0, "text": " this one didn't go up to three point something it just started to go down. So, yes, 1.4 now it's working much better like, I don't know why that didn't spike like, I think that obviously this is an open area of research and like,", "tokens": [50364, 407, 718, 311, 718, 311, 1699, 257, 707, 857, 293, 550, 321, 393, 13, 286, 2041, 309, 311, 2507, 732, 1217, 293, 291, 458, 264, 661, 472, 994, 380, 483, 281, 2507, 732, 1826, 321, 6503, 264, 2316, 558, 586, 321, 434, 466, 281, 15959, 309, 570, 309, 311, 4583, 1266, 370, 718, 311, 536, 437, 2314, 370, 586, 321, 3270, 370, 341, 472, 754, 994, 380, 21053, 11, 51164, 51164, 341, 472, 994, 380, 352, 493, 281, 1045, 935, 746, 309, 445, 1409, 281, 352, 760, 13, 407, 11, 2086, 11, 502, 13, 19, 586, 309, 311, 1364, 709, 1101, 411, 11, 286, 500, 380, 458, 983, 300, 994, 380, 21053, 411, 11, 286, 519, 300, 2745, 341, 307, 364, 1269, 1859, 295, 2132, 293, 411, 11, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.190467762224602, "compression_ratio": 1.7533783783783783, "no_speech_prob": 1.4969444237067364e-05}, {"id": 488, "seek": 305800, "start": 3058.0, "end": 3068.0, "text": " you know, I think most of us in the lab are like working on this as well with that with john here or young, I think that despite the despite might be coming from the fact that that model.", "tokens": [50364, 291, 458, 11, 286, 519, 881, 295, 505, 294, 264, 2715, 366, 411, 1364, 322, 341, 382, 731, 365, 300, 365, 35097, 510, 420, 2037, 11, 286, 519, 300, 7228, 264, 7228, 1062, 312, 1348, 490, 264, 1186, 300, 300, 2316, 13, 50864, 50864, 13443, 2539, 3314, 390, 37668, 293, 586, 300, 321, 366, 1455, 257, 4833, 472, 1310, 321, 20397, 490, 300, 1359, 490, 300, 733, 295, 7285, 321, 645, 294, 558, 370, 286, 519, 11, 4317, 13, 51464, 51464, 467, 311, 257, 665, 5261, 13, 286, 914, 11, 286, 478, 445, 11926, 300, 286, 500, 380, 458, 498, 286, 478, 13, 865, 13, 286, 914, 11, 286, 576, 584, 11, 286, 362, 572, 1558, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.2205727632380714, "compression_ratio": 1.7316176470588236, "no_speech_prob": 4.1331099055241793e-05}, {"id": 489, "seek": 305800, "start": 3068.0, "end": 3080.0, "text": " Final learning rate was pioneer and now that we are making a larger one maybe we escaped from that small from that kind of minimum we were in right so I think, perhaps.", "tokens": [50364, 291, 458, 11, 286, 519, 881, 295, 505, 294, 264, 2715, 366, 411, 1364, 322, 341, 382, 731, 365, 300, 365, 35097, 510, 420, 2037, 11, 286, 519, 300, 7228, 264, 7228, 1062, 312, 1348, 490, 264, 1186, 300, 300, 2316, 13, 50864, 50864, 13443, 2539, 3314, 390, 37668, 293, 586, 300, 321, 366, 1455, 257, 4833, 472, 1310, 321, 20397, 490, 300, 1359, 490, 300, 733, 295, 7285, 321, 645, 294, 558, 370, 286, 519, 11, 4317, 13, 51464, 51464, 467, 311, 257, 665, 5261, 13, 286, 914, 11, 286, 478, 445, 11926, 300, 286, 500, 380, 458, 498, 286, 478, 13, 865, 13, 286, 914, 11, 286, 576, 584, 11, 286, 362, 572, 1558, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.2205727632380714, "compression_ratio": 1.7316176470588236, "no_speech_prob": 4.1331099055241793e-05}, {"id": 490, "seek": 305800, "start": 3080.0, "end": 3087.0, "text": " It's a good theory. I mean, I'm just assuming that I don't know if I'm. Yeah. I mean, I would say, I have no idea.", "tokens": [50364, 291, 458, 11, 286, 519, 881, 295, 505, 294, 264, 2715, 366, 411, 1364, 322, 341, 382, 731, 365, 300, 365, 35097, 510, 420, 2037, 11, 286, 519, 300, 7228, 264, 7228, 1062, 312, 1348, 490, 264, 1186, 300, 300, 2316, 13, 50864, 50864, 13443, 2539, 3314, 390, 37668, 293, 586, 300, 321, 366, 1455, 257, 4833, 472, 1310, 321, 20397, 490, 300, 1359, 490, 300, 733, 295, 7285, 321, 645, 294, 558, 370, 286, 519, 11, 4317, 13, 51464, 51464, 467, 311, 257, 665, 5261, 13, 286, 914, 11, 286, 478, 445, 11926, 300, 286, 500, 380, 458, 498, 286, 478, 13, 865, 13, 286, 914, 11, 286, 576, 584, 11, 286, 362, 572, 1558, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.2205727632380714, "compression_ratio": 1.7316176470588236, "no_speech_prob": 4.1331099055241793e-05}, {"id": 491, "seek": 308700, "start": 3087.0, "end": 3091.0, "text": " I mean, I think there's, there's definitely an aspect to that.", "tokens": [50364, 286, 914, 11, 286, 519, 456, 311, 11, 456, 311, 2138, 364, 4171, 281, 300, 13, 50564, 50564, 286, 519, 286, 576, 584, 746, 411, 11, 286, 500, 380, 458, 13, 865, 11, 286, 519, 300, 311, 1391, 264, 881, 11, 264, 881, 10585, 10835, 13, 51014, 51014, 286, 528, 281, 584, 300, 309, 311, 466, 2698, 46533, 2539, 457, 286, 393, 380, 584, 300, 365, 1577, 27022, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.0981555115686704, "compression_ratio": 1.5932203389830508, "no_speech_prob": 8.529359547537751e-06}, {"id": 492, "seek": 308700, "start": 3091.0, "end": 3100.0, "text": " I think I would say something like, I don't know. Yeah, I think that's probably the most, the most reasonable explanation.", "tokens": [50364, 286, 914, 11, 286, 519, 456, 311, 11, 456, 311, 2138, 364, 4171, 281, 300, 13, 50564, 50564, 286, 519, 286, 576, 584, 746, 411, 11, 286, 500, 380, 458, 13, 865, 11, 286, 519, 300, 311, 1391, 264, 881, 11, 264, 881, 10585, 10835, 13, 51014, 51014, 286, 528, 281, 584, 300, 309, 311, 466, 2698, 46533, 2539, 457, 286, 393, 380, 584, 300, 365, 1577, 27022, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.0981555115686704, "compression_ratio": 1.5932203389830508, "no_speech_prob": 8.529359547537751e-06}, {"id": 493, "seek": 308700, "start": 3100.0, "end": 3106.0, "text": " I want to say that it's about self supervised learning but I can't say that with full certainty.", "tokens": [50364, 286, 914, 11, 286, 519, 456, 311, 11, 456, 311, 2138, 364, 4171, 281, 300, 13, 50564, 50564, 286, 519, 286, 576, 584, 746, 411, 11, 286, 500, 380, 458, 13, 865, 11, 286, 519, 300, 311, 1391, 264, 881, 11, 264, 881, 10585, 10835, 13, 51014, 51014, 286, 528, 281, 584, 300, 309, 311, 466, 2698, 46533, 2539, 457, 286, 393, 380, 584, 300, 365, 1577, 27022, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.0981555115686704, "compression_ratio": 1.5932203389830508, "no_speech_prob": 8.529359547537751e-06}, {"id": 494, "seek": 310600, "start": 3106.0, "end": 3121.0, "text": " Okay, so let's let's compare the performance of these two of the last one so we had to refresh the dancer board, I guess.", "tokens": [50364, 1033, 11, 370, 718, 311, 718, 311, 6794, 264, 3389, 295, 613, 732, 295, 264, 1036, 472, 370, 321, 632, 281, 15134, 264, 21621, 3150, 11, 286, 2041, 13, 51114, 51114, 759, 415, 808, 19513, 1487, 646, 281, 993, 13, 821, 321, 352, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.2242854634920756, "compression_ratio": 1.3464566929133859, "no_speech_prob": 1.0287974873790517e-05}, {"id": 495, "seek": 310600, "start": 3121.0, "end": 3125.0, "text": " If he come backs comes back to life. There we go.", "tokens": [50364, 1033, 11, 370, 718, 311, 718, 311, 6794, 264, 3389, 295, 613, 732, 295, 264, 1036, 472, 370, 321, 632, 281, 15134, 264, 21621, 3150, 11, 286, 2041, 13, 51114, 51114, 759, 415, 808, 19513, 1487, 646, 281, 993, 13, 821, 321, 352, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.2242854634920756, "compression_ratio": 1.3464566929133859, "no_speech_prob": 1.0287974873790517e-05}, {"id": 496, "seek": 312500, "start": 3125.0, "end": 3137.0, "text": " So, the last three are the ones we care about. Just the last two I think is enough seven and eight. Yeah, okay fine. Okay, because six was not trained I think. Supervised and this is unsupervised so blue is unsupervised.", "tokens": [50364, 407, 11, 264, 1036, 1045, 366, 264, 2306, 321, 1127, 466, 13, 1449, 264, 1036, 732, 286, 519, 307, 1547, 3407, 293, 3180, 13, 865, 11, 1392, 2489, 13, 1033, 11, 570, 2309, 390, 406, 8895, 286, 519, 13, 4548, 24420, 293, 341, 307, 2693, 12879, 24420, 370, 3344, 307, 2693, 12879, 24420, 13, 50964, 50964, 407, 718, 311, 536, 436, 439, 436, 393, 767, 11, 393, 321, 1319, 264, 1315, 295, 341, 3037, 6673, 19, 1270, 300, 321, 393, 829, 512, 544, 42585, 42585, 551, 420, 13, 865, 11, 291, 393, 11, 291, 393, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.2519907188415527, "compression_ratio": 1.7161572052401746, "no_speech_prob": 6.0131100326543674e-05}, {"id": 497, "seek": 312500, "start": 3137.0, "end": 3148.0, "text": " So let's see they all they can actually, can we change the name of this version 234 such that we can put some more descriptive descriptive thing or. Yeah, you can, you can.", "tokens": [50364, 407, 11, 264, 1036, 1045, 366, 264, 2306, 321, 1127, 466, 13, 1449, 264, 1036, 732, 286, 519, 307, 1547, 3407, 293, 3180, 13, 865, 11, 1392, 2489, 13, 1033, 11, 570, 2309, 390, 406, 8895, 286, 519, 13, 4548, 24420, 293, 341, 307, 2693, 12879, 24420, 370, 3344, 307, 2693, 12879, 24420, 13, 50964, 50964, 407, 718, 311, 536, 436, 439, 436, 393, 767, 11, 393, 321, 1319, 264, 1315, 295, 341, 3037, 6673, 19, 1270, 300, 321, 393, 829, 512, 544, 42585, 42585, 551, 420, 13, 865, 11, 291, 393, 11, 291, 393, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.2519907188415527, "compression_ratio": 1.7161572052401746, "no_speech_prob": 6.0131100326543674e-05}, {"id": 498, "seek": 314800, "start": 3148.0, "end": 3158.0, "text": " The docs will explain how to do that. Okay. Okay, so train accuracy unsupervised.", "tokens": [50364, 440, 45623, 486, 2903, 577, 281, 360, 300, 13, 1033, 13, 1033, 11, 370, 3847, 14170, 2693, 12879, 24420, 13, 50864, 50864, 400, 550, 264, 3847, 4470, 13, 50964, 50964], "temperature": 0.0, "avg_logprob": -0.24274320900440216, "compression_ratio": 1.1521739130434783, "no_speech_prob": 1.0614918210194446e-05}, {"id": 499, "seek": 314800, "start": 3158.0, "end": 3160.0, "text": " And then the train loss.", "tokens": [50364, 440, 45623, 486, 2903, 577, 281, 360, 300, 13, 1033, 13, 1033, 11, 370, 3847, 14170, 2693, 12879, 24420, 13, 50864, 50864, 400, 550, 264, 3847, 4470, 13, 50964, 50964], "temperature": 0.0, "avg_logprob": -0.24274320900440216, "compression_ratio": 1.1521739130434783, "no_speech_prob": 1.0614918210194446e-05}, {"id": 500, "seek": 316000, "start": 3160.0, "end": 3178.0, "text": " Oh, okay. Okay, you convinced me. All right. And so we have seen how to perform a transfer learning with pre trained backbone first with a supervised pre trained backbone, then we done self provides pre trained backbone, but then what is the advantage of using these transfer", "tokens": [50364, 876, 11, 1392, 13, 1033, 11, 291, 12561, 385, 13, 1057, 558, 13, 400, 370, 321, 362, 1612, 577, 281, 2042, 257, 5003, 2539, 365, 659, 8895, 34889, 700, 365, 257, 46533, 659, 8895, 34889, 11, 550, 321, 1096, 2698, 6417, 659, 8895, 34889, 11, 457, 550, 437, 307, 264, 5002, 295, 1228, 613, 5003, 51264, 51264, 2539, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.20119692787291513, "compression_ratio": 1.716867469879518, "no_speech_prob": 1.2410297131282277e-05}, {"id": 501, "seek": 316000, "start": 3178.0, "end": 3180.0, "text": " learning.", "tokens": [50364, 876, 11, 1392, 13, 1033, 11, 291, 12561, 385, 13, 1057, 558, 13, 400, 370, 321, 362, 1612, 577, 281, 2042, 257, 5003, 2539, 365, 659, 8895, 34889, 700, 365, 257, 46533, 659, 8895, 34889, 11, 550, 321, 1096, 2698, 6417, 659, 8895, 34889, 11, 457, 550, 437, 307, 264, 5002, 295, 1228, 613, 5003, 51264, 51264, 2539, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.20119692787291513, "compression_ratio": 1.716867469879518, "no_speech_prob": 1.2410297131282277e-05}, {"id": 502, "seek": 318000, "start": 3180.0, "end": 3195.0, "text": " So, generally, we're going to be able to kind of get a jump start on our training, right, so we're going to be able to sometimes converge a lot faster, but second be able to generalize to the data set that we were training on so remember we had this data set that", "tokens": [50364, 407, 11, 5101, 11, 321, 434, 516, 281, 312, 1075, 281, 733, 295, 483, 257, 3012, 722, 322, 527, 3097, 11, 558, 11, 370, 321, 434, 516, 281, 312, 1075, 281, 2171, 41881, 257, 688, 4663, 11, 457, 1150, 312, 1075, 281, 2674, 1125, 281, 264, 1412, 992, 300, 321, 645, 3097, 322, 370, 1604, 321, 632, 341, 1412, 992, 300, 51114, 51114, 264, 2316, 390, 406, 659, 8895, 322, 11, 293, 321, 434, 7159, 300, 321, 393, 2674, 1125, 281, 527, 1500, 7472, 295, 300, 1412, 6352, 13, 1779, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08925612600226152, "compression_ratio": 1.8229665071770336, "no_speech_prob": 3.726459453901043e-06}, {"id": 503, "seek": 318000, "start": 3195.0, "end": 3203.0, "text": " the model was not pre trained on, and we're hoping that we can generalize to our test split of that data sets. Right.", "tokens": [50364, 407, 11, 5101, 11, 321, 434, 516, 281, 312, 1075, 281, 733, 295, 483, 257, 3012, 722, 322, 527, 3097, 11, 558, 11, 370, 321, 434, 516, 281, 312, 1075, 281, 2171, 41881, 257, 688, 4663, 11, 457, 1150, 312, 1075, 281, 2674, 1125, 281, 264, 1412, 992, 300, 321, 645, 3097, 322, 370, 1604, 321, 632, 341, 1412, 992, 300, 51114, 51114, 264, 2316, 390, 406, 659, 8895, 322, 11, 293, 321, 434, 7159, 300, 321, 393, 2674, 1125, 281, 527, 1500, 7472, 295, 300, 1412, 6352, 13, 1779, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.08925612600226152, "compression_ratio": 1.8229665071770336, "no_speech_prob": 3.726459453901043e-06}, {"id": 504, "seek": 320300, "start": 3203.0, "end": 3214.0, "text": " So as we showed right now we only train on the training split. So that's great just to make sure things are working but you want to know if you're going to do well when you see actual data, which we're going to use the validation support.", "tokens": [50364, 407, 382, 321, 4712, 558, 586, 321, 787, 3847, 322, 264, 3097, 7472, 13, 407, 300, 311, 869, 445, 281, 652, 988, 721, 366, 1364, 457, 291, 528, 281, 458, 498, 291, 434, 516, 281, 360, 731, 562, 291, 536, 3539, 1412, 11, 597, 321, 434, 516, 281, 764, 264, 24071, 1406, 13, 50914, 50914, 2798, 11, 370, 718, 311, 536, 577, 321, 393, 29562, 527, 9590, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.08373379045062596, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.406330648023868e-05}, {"id": 505, "seek": 320300, "start": 3214.0, "end": 3218.0, "text": " Alright, so let's see how we can validate our networks.", "tokens": [50364, 407, 382, 321, 4712, 558, 586, 321, 787, 3847, 322, 264, 3097, 7472, 13, 407, 300, 311, 869, 445, 281, 652, 988, 721, 366, 1364, 457, 291, 528, 281, 458, 498, 291, 434, 516, 281, 360, 731, 562, 291, 536, 3539, 1412, 11, 597, 321, 434, 516, 281, 764, 264, 24071, 1406, 13, 50914, 50914, 2798, 11, 370, 718, 311, 536, 577, 321, 393, 29562, 527, 9590, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.08373379045062596, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.406330648023868e-05}, {"id": 506, "seek": 321800, "start": 3218.0, "end": 3233.0, "text": " Yeah, so let's let's do a quick recap right so we have. This is the supervised model, right, so this was using our standard rest net 50 backbone. And then below I have the self supervised model so we just swap that with the swap model.", "tokens": [50364, 865, 11, 370, 718, 311, 718, 311, 360, 257, 1702, 20928, 558, 370, 321, 362, 13, 639, 307, 264, 46533, 2316, 11, 558, 11, 370, 341, 390, 1228, 527, 3832, 1472, 2533, 2625, 34889, 13, 400, 550, 2507, 286, 362, 264, 2698, 46533, 2316, 370, 321, 445, 18135, 300, 365, 264, 18135, 2316, 13, 51114, 51114, 865, 11, 370, 718, 311, 352, 2286, 293, 3847, 264, 46533, 472, 13, 400, 321, 434, 516, 281, 360, 309, 337, 945, 11829, 13, 400, 321, 434, 406, 516, 281, 4948, 264, 1230, 295, 15245, 279, 321, 434, 516, 281, 1261, 322, 264, 1577, 551, 370, 321, 393, 536, 437, 2314, 558, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1280216115765867, "compression_ratio": 1.8112449799196788, "no_speech_prob": 2.8129118163633393e-06}, {"id": 507, "seek": 321800, "start": 3233.0, "end": 3247.0, "text": " Yeah, so let's go ahead and train the supervised one. And we're going to do it for 20 bucks. And we're not going to limit the number of batches we're going to turn on the full thing so we can see what happens right.", "tokens": [50364, 865, 11, 370, 718, 311, 718, 311, 360, 257, 1702, 20928, 558, 370, 321, 362, 13, 639, 307, 264, 46533, 2316, 11, 558, 11, 370, 341, 390, 1228, 527, 3832, 1472, 2533, 2625, 34889, 13, 400, 550, 2507, 286, 362, 264, 2698, 46533, 2316, 370, 321, 445, 18135, 300, 365, 264, 18135, 2316, 13, 51114, 51114, 865, 11, 370, 718, 311, 352, 2286, 293, 3847, 264, 46533, 472, 13, 400, 321, 434, 516, 281, 360, 309, 337, 945, 11829, 13, 400, 321, 434, 406, 516, 281, 4948, 264, 1230, 295, 15245, 279, 321, 434, 516, 281, 1261, 322, 264, 1577, 551, 370, 321, 393, 536, 437, 2314, 558, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.1280216115765867, "compression_ratio": 1.8112449799196788, "no_speech_prob": 2.8129118163633393e-06}, {"id": 508, "seek": 324700, "start": 3247.0, "end": 3256.0, "text": " So I'm going to set max epochs equals 20. And since we want to do the validation validation, I actually need a validation with as well. Right.", "tokens": [50364, 407, 286, 478, 516, 281, 992, 11469, 30992, 28346, 6915, 945, 13, 400, 1670, 321, 528, 281, 360, 264, 24071, 24071, 11, 286, 767, 643, 257, 24071, 365, 382, 731, 13, 1779, 13, 50814, 50814, 407, 11, 510, 11, 456, 11, 291, 458, 11, 264, 22811, 636, 281, 360, 341, 307, 445, 5055, 9163, 341, 3089, 570, 309, 311, 11611, 264, 912, 13, 51164, 51164, 400, 550, 286, 393, 445, 7782, 341, 2017, 24071, 1507, 13, 51364, 51364, 400, 550, 7406, 613, 2283, 365, 4503, 293, 4503, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.19605825258337933, "compression_ratio": 1.643835616438356, "no_speech_prob": 1.2218495612614788e-05}, {"id": 509, "seek": 324700, "start": 3256.0, "end": 3263.0, "text": " So, here, there, you know, the simplest way to do this is just copy paste this code because it's largely the same.", "tokens": [50364, 407, 286, 478, 516, 281, 992, 11469, 30992, 28346, 6915, 945, 13, 400, 1670, 321, 528, 281, 360, 264, 24071, 24071, 11, 286, 767, 643, 257, 24071, 365, 382, 731, 13, 1779, 13, 50814, 50814, 407, 11, 510, 11, 456, 11, 291, 458, 11, 264, 22811, 636, 281, 360, 341, 307, 445, 5055, 9163, 341, 3089, 570, 309, 311, 11611, 264, 912, 13, 51164, 51164, 400, 550, 286, 393, 445, 7782, 341, 2017, 24071, 1507, 13, 51364, 51364, 400, 550, 7406, 613, 2283, 365, 4503, 293, 4503, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.19605825258337933, "compression_ratio": 1.643835616438356, "no_speech_prob": 1.2218495612614788e-05}, {"id": 510, "seek": 324700, "start": 3263.0, "end": 3267.0, "text": " And then I can just reward this color validation stuff.", "tokens": [50364, 407, 286, 478, 516, 281, 992, 11469, 30992, 28346, 6915, 945, 13, 400, 1670, 321, 528, 281, 360, 264, 24071, 24071, 11, 286, 767, 643, 257, 24071, 365, 382, 731, 13, 1779, 13, 50814, 50814, 407, 11, 510, 11, 456, 11, 291, 458, 11, 264, 22811, 636, 281, 360, 341, 307, 445, 5055, 9163, 341, 3089, 570, 309, 311, 11611, 264, 912, 13, 51164, 51164, 400, 550, 286, 393, 445, 7782, 341, 2017, 24071, 1507, 13, 51364, 51364, 400, 550, 7406, 613, 2283, 365, 4503, 293, 4503, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.19605825258337933, "compression_ratio": 1.643835616438356, "no_speech_prob": 1.2218495612614788e-05}, {"id": 511, "seek": 324700, "start": 3267.0, "end": 3272.0, "text": " And then replace these words with bow and bow.", "tokens": [50364, 407, 286, 478, 516, 281, 992, 11469, 30992, 28346, 6915, 945, 13, 400, 1670, 321, 528, 281, 360, 264, 24071, 24071, 11, 286, 767, 643, 257, 24071, 365, 382, 731, 13, 1779, 13, 50814, 50814, 407, 11, 510, 11, 456, 11, 291, 458, 11, 264, 22811, 636, 281, 360, 341, 307, 445, 5055, 9163, 341, 3089, 570, 309, 311, 11611, 264, 912, 13, 51164, 51164, 400, 550, 286, 393, 445, 7782, 341, 2017, 24071, 1507, 13, 51364, 51364, 400, 550, 7406, 613, 2283, 365, 4503, 293, 4503, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.19605825258337933, "compression_ratio": 1.643835616438356, "no_speech_prob": 1.2218495612614788e-05}, {"id": 512, "seek": 327200, "start": 3272.0, "end": 3278.0, "text": " And there are also keywords right for the pie torch lining this validation underscore step.", "tokens": [50364, 400, 456, 366, 611, 21009, 558, 337, 264, 1730, 27822, 19628, 341, 24071, 37556, 1823, 13, 50664, 50664, 7587, 13, 1033, 13, 400, 437, 291, 3449, 510, 307, 300, 13, 407, 286, 528, 281, 652, 257, 935, 370, 294, 3097, 291, 528, 281, 3565, 746, 633, 15245, 11, 558, 13, 51114, 51114, 682, 24071, 11, 309, 1177, 380, 534, 652, 2020, 281, 4017, 746, 633, 15245, 570, 436, 434, 6695, 13, 407, 291, 528, 281, 11, 291, 528, 281, 8873, 264, 14170, 420, 264, 4470, 2108, 264, 1379, 19438, 69, 6675, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.18209115664164224, "compression_ratio": 1.7004048582995952, "no_speech_prob": 1.0450914487591945e-05}, {"id": 513, "seek": 327200, "start": 3278.0, "end": 3287.0, "text": " Exactly. Okay. And what you notice here is that. So I want to make a point so in training you want to log something every batch, right.", "tokens": [50364, 400, 456, 366, 611, 21009, 558, 337, 264, 1730, 27822, 19628, 341, 24071, 37556, 1823, 13, 50664, 50664, 7587, 13, 1033, 13, 400, 437, 291, 3449, 510, 307, 300, 13, 407, 286, 528, 281, 652, 257, 935, 370, 294, 3097, 291, 528, 281, 3565, 746, 633, 15245, 11, 558, 13, 51114, 51114, 682, 24071, 11, 309, 1177, 380, 534, 652, 2020, 281, 4017, 746, 633, 15245, 570, 436, 434, 6695, 13, 407, 291, 528, 281, 11, 291, 528, 281, 8873, 264, 14170, 420, 264, 4470, 2108, 264, 1379, 19438, 69, 6675, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.18209115664164224, "compression_ratio": 1.7004048582995952, "no_speech_prob": 1.0450914487591945e-05}, {"id": 514, "seek": 327200, "start": 3287.0, "end": 3295.0, "text": " In validation, it doesn't really make sense to lock something every batch because they're independent. So you want to, you want to calculate the accuracy or the loss across the whole leapfrog.", "tokens": [50364, 400, 456, 366, 611, 21009, 558, 337, 264, 1730, 27822, 19628, 341, 24071, 37556, 1823, 13, 50664, 50664, 7587, 13, 1033, 13, 400, 437, 291, 3449, 510, 307, 300, 13, 407, 286, 528, 281, 652, 257, 935, 370, 294, 3097, 291, 528, 281, 3565, 746, 633, 15245, 11, 558, 13, 51114, 51114, 682, 24071, 11, 309, 1177, 380, 534, 652, 2020, 281, 4017, 746, 633, 15245, 570, 436, 434, 6695, 13, 407, 291, 528, 281, 11, 291, 528, 281, 8873, 264, 14170, 420, 264, 4470, 2108, 264, 1379, 19438, 69, 6675, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.18209115664164224, "compression_ratio": 1.7004048582995952, "no_speech_prob": 1.0450914487591945e-05}, {"id": 515, "seek": 329500, "start": 3295.0, "end": 3302.0, "text": " So you don't have to deal with that as long as you just use log lightning nose to do it the correct way. So it'll aggregate across the book as well.", "tokens": [50364, 407, 291, 500, 380, 362, 281, 2028, 365, 300, 382, 938, 382, 291, 445, 764, 3565, 16589, 6690, 281, 360, 309, 264, 3006, 636, 13, 407, 309, 603, 26118, 2108, 264, 1446, 382, 731, 13, 50714, 50714, 1033, 13, 823, 341, 1507, 307, 321, 500, 380, 534, 643, 341, 13, 467, 1177, 380, 4607, 281, 362, 309, 13, 583, 11, 291, 458, 11, 321, 727, 321, 727, 20460, 721, 538, 445, 1242, 3973, 295, 439, 341, 13, 51114, 51114, 4162, 456, 366, 572, 2771, 2448, 1217, 456, 11, 436, 434, 15191, 420, 34702, 399, 6772, 13, 51364, 51364, 509, 458, 11, 370, 370, 321, 393, 445, 20460, 341, 13, 1033, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1620956752611243, "compression_ratio": 1.6853932584269662, "no_speech_prob": 7.571051128252293e-07}, {"id": 516, "seek": 329500, "start": 3302.0, "end": 3310.0, "text": " Okay. Now this stuff is we don't really need this. It doesn't hurt to have it. But, you know, we could we could simplify things by just getting rid of all this.", "tokens": [50364, 407, 291, 500, 380, 362, 281, 2028, 365, 300, 382, 938, 382, 291, 445, 764, 3565, 16589, 6690, 281, 360, 309, 264, 3006, 636, 13, 407, 309, 603, 26118, 2108, 264, 1446, 382, 731, 13, 50714, 50714, 1033, 13, 823, 341, 1507, 307, 321, 500, 380, 534, 643, 341, 13, 467, 1177, 380, 4607, 281, 362, 309, 13, 583, 11, 291, 458, 11, 321, 727, 321, 727, 20460, 721, 538, 445, 1242, 3973, 295, 439, 341, 13, 51114, 51114, 4162, 456, 366, 572, 2771, 2448, 1217, 456, 11, 436, 434, 15191, 420, 34702, 399, 6772, 13, 51364, 51364, 509, 458, 11, 370, 370, 321, 393, 445, 20460, 341, 13, 1033, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1620956752611243, "compression_ratio": 1.6853932584269662, "no_speech_prob": 7.571051128252293e-07}, {"id": 517, "seek": 329500, "start": 3310.0, "end": 3315.0, "text": " Since there are no gradients already there, they're disabled or invalidation automatically.", "tokens": [50364, 407, 291, 500, 380, 362, 281, 2028, 365, 300, 382, 938, 382, 291, 445, 764, 3565, 16589, 6690, 281, 360, 309, 264, 3006, 636, 13, 407, 309, 603, 26118, 2108, 264, 1446, 382, 731, 13, 50714, 50714, 1033, 13, 823, 341, 1507, 307, 321, 500, 380, 534, 643, 341, 13, 467, 1177, 380, 4607, 281, 362, 309, 13, 583, 11, 291, 458, 11, 321, 727, 321, 727, 20460, 721, 538, 445, 1242, 3973, 295, 439, 341, 13, 51114, 51114, 4162, 456, 366, 572, 2771, 2448, 1217, 456, 11, 436, 434, 15191, 420, 34702, 399, 6772, 13, 51364, 51364, 509, 458, 11, 370, 370, 321, 393, 445, 20460, 341, 13, 1033, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1620956752611243, "compression_ratio": 1.6853932584269662, "no_speech_prob": 7.571051128252293e-07}, {"id": 518, "seek": 329500, "start": 3315.0, "end": 3318.0, "text": " You know, so so we can just simplify this. Okay.", "tokens": [50364, 407, 291, 500, 380, 362, 281, 2028, 365, 300, 382, 938, 382, 291, 445, 764, 3565, 16589, 6690, 281, 360, 309, 264, 3006, 636, 13, 407, 309, 603, 26118, 2108, 264, 1446, 382, 731, 13, 50714, 50714, 1033, 13, 823, 341, 1507, 307, 321, 500, 380, 534, 643, 341, 13, 467, 1177, 380, 4607, 281, 362, 309, 13, 583, 11, 291, 458, 11, 321, 727, 321, 727, 20460, 721, 538, 445, 1242, 3973, 295, 439, 341, 13, 51114, 51114, 4162, 456, 366, 572, 2771, 2448, 1217, 456, 11, 436, 434, 15191, 420, 34702, 399, 6772, 13, 51364, 51364, 509, 458, 11, 370, 370, 321, 393, 445, 20460, 341, 13, 1033, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1620956752611243, "compression_ratio": 1.6853932584269662, "no_speech_prob": 7.571051128252293e-07}, {"id": 519, "seek": 331800, "start": 3318.0, "end": 3328.0, "text": " And then, yeah, so we can leave it there. Now you'll also notice that this code is largely the same. So yes, you could write an intermediate function and then just use the same one.", "tokens": [50364, 400, 550, 11, 1338, 11, 370, 321, 393, 1856, 309, 456, 13, 823, 291, 603, 611, 3449, 300, 341, 3089, 307, 11611, 264, 912, 13, 407, 2086, 11, 291, 727, 2464, 364, 19376, 2445, 293, 550, 445, 764, 264, 912, 472, 13, 50864, 50864, 583, 11, 291, 458, 11, 445, 337, 25632, 11, 321, 434, 516, 281, 1066, 309, 382, 309, 307, 13, 1033, 13, 51064, 51064, 1033, 13, 407, 718, 311, 406, 3847, 337, 945, 11829, 13, 51414, 51414, 400, 291, 603, 3449, 300, 309, 486, 1190, 257, 1326, 24071, 15245, 279, 700, 293, 652, 988, 291, 362, 572, 15120, 13, 400, 586, 321, 722, 3097, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12446858201708112, "compression_ratio": 1.6153846153846154, "no_speech_prob": 5.954973858024459e-06}, {"id": 520, "seek": 331800, "start": 3328.0, "end": 3332.0, "text": " But, you know, just for simplicity, we're going to keep it as it is. Okay.", "tokens": [50364, 400, 550, 11, 1338, 11, 370, 321, 393, 1856, 309, 456, 13, 823, 291, 603, 611, 3449, 300, 341, 3089, 307, 11611, 264, 912, 13, 407, 2086, 11, 291, 727, 2464, 364, 19376, 2445, 293, 550, 445, 764, 264, 912, 472, 13, 50864, 50864, 583, 11, 291, 458, 11, 445, 337, 25632, 11, 321, 434, 516, 281, 1066, 309, 382, 309, 307, 13, 1033, 13, 51064, 51064, 1033, 13, 407, 718, 311, 406, 3847, 337, 945, 11829, 13, 51414, 51414, 400, 291, 603, 3449, 300, 309, 486, 1190, 257, 1326, 24071, 15245, 279, 700, 293, 652, 988, 291, 362, 572, 15120, 13, 400, 586, 321, 722, 3097, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12446858201708112, "compression_ratio": 1.6153846153846154, "no_speech_prob": 5.954973858024459e-06}, {"id": 521, "seek": 331800, "start": 3332.0, "end": 3339.0, "text": " Okay. So let's not train for 20 bucks.", "tokens": [50364, 400, 550, 11, 1338, 11, 370, 321, 393, 1856, 309, 456, 13, 823, 291, 603, 611, 3449, 300, 341, 3089, 307, 11611, 264, 912, 13, 407, 2086, 11, 291, 727, 2464, 364, 19376, 2445, 293, 550, 445, 764, 264, 912, 472, 13, 50864, 50864, 583, 11, 291, 458, 11, 445, 337, 25632, 11, 321, 434, 516, 281, 1066, 309, 382, 309, 307, 13, 1033, 13, 51064, 51064, 1033, 13, 407, 718, 311, 406, 3847, 337, 945, 11829, 13, 51414, 51414, 400, 291, 603, 3449, 300, 309, 486, 1190, 257, 1326, 24071, 15245, 279, 700, 293, 652, 988, 291, 362, 572, 15120, 13, 400, 586, 321, 722, 3097, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12446858201708112, "compression_ratio": 1.6153846153846154, "no_speech_prob": 5.954973858024459e-06}, {"id": 522, "seek": 331800, "start": 3339.0, "end": 3346.0, "text": " And you'll notice that it will run a few validation batches first and make sure you have no bugs. And now we start training.", "tokens": [50364, 400, 550, 11, 1338, 11, 370, 321, 393, 1856, 309, 456, 13, 823, 291, 603, 611, 3449, 300, 341, 3089, 307, 11611, 264, 912, 13, 407, 2086, 11, 291, 727, 2464, 364, 19376, 2445, 293, 550, 445, 764, 264, 912, 472, 13, 50864, 50864, 583, 11, 291, 458, 11, 445, 337, 25632, 11, 321, 434, 516, 281, 1066, 309, 382, 309, 307, 13, 1033, 13, 51064, 51064, 1033, 13, 407, 718, 311, 406, 3847, 337, 945, 11829, 13, 51414, 51414, 400, 291, 603, 3449, 300, 309, 486, 1190, 257, 1326, 24071, 15245, 279, 700, 293, 652, 988, 291, 362, 572, 15120, 13, 400, 586, 321, 722, 3097, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.12446858201708112, "compression_ratio": 1.6153846153846154, "no_speech_prob": 5.954973858024459e-06}, {"id": 523, "seek": 334600, "start": 3346.0, "end": 3353.0, "text": " Okay, so we're done training. So let's see how 20 bucks.", "tokens": [50364, 1033, 11, 370, 321, 434, 1096, 3097, 13, 407, 718, 311, 536, 577, 945, 11829, 13, 50714, 50714, 961, 311, 25628, 264, 5443, 13, 51264, 51264, 1057, 558, 11, 456, 311, 527, 945, 14894, 2316, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.15203046798706055, "compression_ratio": 1.1153846153846154, "no_speech_prob": 8.93912510946393e-06}, {"id": 524, "seek": 334600, "start": 3353.0, "end": 3364.0, "text": " Let's reload the sky.", "tokens": [50364, 1033, 11, 370, 321, 434, 1096, 3097, 13, 407, 718, 311, 536, 577, 945, 11829, 13, 50714, 50714, 961, 311, 25628, 264, 5443, 13, 51264, 51264, 1057, 558, 11, 456, 311, 527, 945, 14894, 2316, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.15203046798706055, "compression_ratio": 1.1153846153846154, "no_speech_prob": 8.93912510946393e-06}, {"id": 525, "seek": 334600, "start": 3364.0, "end": 3367.0, "text": " All right, there's our 20 buck model.", "tokens": [50364, 1033, 11, 370, 321, 434, 1096, 3097, 13, 407, 718, 311, 536, 577, 945, 11829, 13, 50714, 50714, 961, 311, 25628, 264, 5443, 13, 51264, 51264, 1057, 558, 11, 456, 311, 527, 945, 14894, 2316, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.15203046798706055, "compression_ratio": 1.1153846153846154, "no_speech_prob": 8.93912510946393e-06}, {"id": 526, "seek": 336700, "start": 3367.0, "end": 3384.0, "text": " Let's see what our validation accuracy looks like. Not bad. So you can see, let's look at the epoch where that happened. Right. So you can see that change. So around step 14,000, we had this big increase in accuracy, validation accuracy.", "tokens": [50364, 961, 311, 536, 437, 527, 24071, 14170, 1542, 411, 13, 1726, 1578, 13, 407, 291, 393, 536, 11, 718, 311, 574, 412, 264, 30992, 339, 689, 300, 2011, 13, 1779, 13, 407, 291, 393, 536, 300, 1319, 13, 407, 926, 1823, 3499, 11, 1360, 11, 321, 632, 341, 955, 3488, 294, 14170, 11, 24071, 14170, 13, 51214, 51214, 286, 528, 281, 2041, 300, 311, 689, 264, 2316, 390, 12496, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.15340082065479174, "compression_ratio": 1.5483870967741935, "no_speech_prob": 4.222572442813544e-06}, {"id": 527, "seek": 336700, "start": 3384.0, "end": 3387.0, "text": " I want to guess that's where the model was frozen.", "tokens": [50364, 961, 311, 536, 437, 527, 24071, 14170, 1542, 411, 13, 1726, 1578, 13, 407, 291, 393, 536, 11, 718, 311, 574, 412, 264, 30992, 339, 689, 300, 2011, 13, 1779, 13, 407, 291, 393, 536, 300, 1319, 13, 407, 926, 1823, 3499, 11, 1360, 11, 321, 632, 341, 955, 3488, 294, 14170, 11, 24071, 14170, 13, 51214, 51214, 286, 528, 281, 2041, 300, 311, 689, 264, 2316, 390, 12496, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.15340082065479174, "compression_ratio": 1.5483870967741935, "no_speech_prob": 4.222572442813544e-06}, {"id": 528, "seek": 338700, "start": 3387.0, "end": 3401.0, "text": " Right. So let's look at that. That's step 14,000. And 14,000 around here. So yeah, it's about step epoch 10. So the ninth, the 10th epoch, right. So it's index nine.", "tokens": [50364, 1779, 13, 407, 718, 311, 574, 412, 300, 13, 663, 311, 1823, 3499, 11, 1360, 13, 400, 3499, 11, 1360, 926, 510, 13, 407, 1338, 11, 309, 311, 466, 1823, 30992, 339, 1266, 13, 407, 264, 28207, 11, 264, 1266, 392, 30992, 339, 11, 558, 13, 407, 309, 311, 8186, 4949, 13, 51064, 51064, 407, 1338, 11, 370, 300, 1669, 2020, 13, 407, 291, 393, 536, 300, 3971, 701, 8781, 264, 34889, 412, 512, 935, 486, 550, 9528, 291, 281, 733, 295, 2524, 264, 958, 39885, 295, 3389, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1553391897550193, "compression_ratio": 1.5804878048780489, "no_speech_prob": 8.267408702522516e-06}, {"id": 529, "seek": 338700, "start": 3401.0, "end": 3411.0, "text": " So yeah, so that makes sense. So you can see that unfreezing the backbone at some point will then enable you to kind of reach the next plateau of performance.", "tokens": [50364, 1779, 13, 407, 718, 311, 574, 412, 300, 13, 663, 311, 1823, 3499, 11, 1360, 13, 400, 3499, 11, 1360, 926, 510, 13, 407, 1338, 11, 309, 311, 466, 1823, 30992, 339, 1266, 13, 407, 264, 28207, 11, 264, 1266, 392, 30992, 339, 11, 558, 13, 407, 309, 311, 8186, 4949, 13, 51064, 51064, 407, 1338, 11, 370, 300, 1669, 2020, 13, 407, 291, 393, 536, 300, 3971, 701, 8781, 264, 34889, 412, 512, 935, 486, 550, 9528, 291, 281, 733, 295, 2524, 264, 958, 39885, 295, 3389, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1553391897550193, "compression_ratio": 1.5804878048780489, "no_speech_prob": 8.267408702522516e-06}, {"id": 530, "seek": 341100, "start": 3411.0, "end": 3421.0, "text": " So this was for the supervised pre training part, right? Yeah. And then let's also look at the last. So this is by epoch. So you can see as well that that happened there.", "tokens": [50364, 407, 341, 390, 337, 264, 46533, 659, 3097, 644, 11, 558, 30, 865, 13, 400, 550, 718, 311, 611, 574, 412, 264, 1036, 13, 407, 341, 307, 538, 30992, 339, 13, 407, 291, 393, 536, 382, 731, 300, 300, 2011, 456, 13, 50864, 50864, 509, 458, 11, 472, 551, 291, 727, 853, 307, 611, 445, 3971, 701, 1381, 309, 490, 264, 483, 352, 13, 509, 1062, 312, 1075, 281, 360, 1101, 382, 731, 13, 583, 718, 311, 360, 264, 1900, 912, 551, 365, 264, 2698, 46533, 3037, 13, 51264, 51264, 1033, 11, 370, 321, 643, 281, 909, 264, 264, 24071, 3170, 281, 341, 472, 382, 731, 13, 1779, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.14035829189604362, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.75006080098683e-05}, {"id": 531, "seek": 341100, "start": 3421.0, "end": 3429.0, "text": " You know, one thing you could try is also just unfreeze it from the get go. You might be able to do better as well. But let's do the exact same thing with the self supervised version.", "tokens": [50364, 407, 341, 390, 337, 264, 46533, 659, 3097, 644, 11, 558, 30, 865, 13, 400, 550, 718, 311, 611, 574, 412, 264, 1036, 13, 407, 341, 307, 538, 30992, 339, 13, 407, 291, 393, 536, 382, 731, 300, 300, 2011, 456, 13, 50864, 50864, 509, 458, 11, 472, 551, 291, 727, 853, 307, 611, 445, 3971, 701, 1381, 309, 490, 264, 483, 352, 13, 509, 1062, 312, 1075, 281, 360, 1101, 382, 731, 13, 583, 718, 311, 360, 264, 1900, 912, 551, 365, 264, 2698, 46533, 3037, 13, 51264, 51264, 1033, 11, 370, 321, 643, 281, 909, 264, 264, 24071, 3170, 281, 341, 472, 382, 731, 13, 1779, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.14035829189604362, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.75006080098683e-05}, {"id": 532, "seek": 341100, "start": 3429.0, "end": 3433.0, "text": " Okay, so we need to add the the validation method to this one as well. Right.", "tokens": [50364, 407, 341, 390, 337, 264, 46533, 659, 3097, 644, 11, 558, 30, 865, 13, 400, 550, 718, 311, 611, 574, 412, 264, 1036, 13, 407, 341, 307, 538, 30992, 339, 13, 407, 291, 393, 536, 382, 731, 300, 300, 2011, 456, 13, 50864, 50864, 509, 458, 11, 472, 551, 291, 727, 853, 307, 611, 445, 3971, 701, 1381, 309, 490, 264, 483, 352, 13, 509, 1062, 312, 1075, 281, 360, 1101, 382, 731, 13, 583, 718, 311, 360, 264, 1900, 912, 551, 365, 264, 2698, 46533, 3037, 13, 51264, 51264, 1033, 11, 370, 321, 643, 281, 909, 264, 264, 24071, 3170, 281, 341, 472, 382, 731, 13, 1779, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.14035829189604362, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.75006080098683e-05}, {"id": 533, "seek": 343300, "start": 3433.0, "end": 3443.0, "text": " Yeah. So let's just do that. Let's copy again, like we did, we're just going to copy the training stuff. We already had the training, the validation, right? Yeah. And then I'm going to rename this validation.", "tokens": [50364, 865, 13, 407, 718, 311, 445, 360, 300, 13, 961, 311, 5055, 797, 11, 411, 321, 630, 11, 321, 434, 445, 516, 281, 5055, 264, 3097, 1507, 13, 492, 1217, 632, 264, 3097, 11, 264, 24071, 11, 558, 30, 865, 13, 400, 550, 286, 478, 516, 281, 36741, 341, 24071, 13, 50864, 50864, 1033, 13, 509, 393, 5055, 490, 264, 3894, 13, 865, 13, 1042, 11, 309, 311, 341, 4748, 819, 570, 321, 362, 613, 732, 4111, 11317, 13, 407, 11, 1954, 11, 1392, 13, 509, 434, 558, 13, 286, 5298, 13, 865, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.17181867482710858, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.7967778805759735e-05}, {"id": 534, "seek": 343300, "start": 3443.0, "end": 3452.0, "text": " Okay. You can copy from the previous. Yeah. Well, it's this slightly different because we have these two feature maps. So, oh, okay. You're right. I forgot. Yeah.", "tokens": [50364, 865, 13, 407, 718, 311, 445, 360, 300, 13, 961, 311, 5055, 797, 11, 411, 321, 630, 11, 321, 434, 445, 516, 281, 5055, 264, 3097, 1507, 13, 492, 1217, 632, 264, 3097, 11, 264, 24071, 11, 558, 30, 865, 13, 400, 550, 286, 478, 516, 281, 36741, 341, 24071, 13, 50864, 50864, 1033, 13, 509, 393, 5055, 490, 264, 3894, 13, 865, 13, 1042, 11, 309, 311, 341, 4748, 819, 570, 321, 362, 613, 732, 4111, 11317, 13, 407, 11, 1954, 11, 1392, 13, 509, 434, 558, 13, 286, 5298, 13, 865, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.17181867482710858, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.7967778805759735e-05}, {"id": 535, "seek": 345200, "start": 3452.0, "end": 3464.0, "text": " So we have the values and then about, and again, this no grad thing doesn't matter, but I'll remove it just to make it cleaner.", "tokens": [50364, 407, 321, 362, 264, 4190, 293, 550, 466, 11, 293, 797, 11, 341, 572, 2771, 551, 1177, 380, 1871, 11, 457, 286, 603, 4159, 309, 445, 281, 652, 309, 16532, 13, 50964, 50964, 3769, 13, 1033, 13, 407, 586, 321, 820, 312, 1075, 281, 3847, 322, 11, 293, 286, 528, 281, 652, 988, 321, 764, 264, 1900, 912, 3097, 13120, 13, 407, 286, 478, 445, 516, 281, 5055, 9163, 341, 2146, 13, 51364, 51364, 1033, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.30921130180358886, "compression_ratio": 1.5372340425531914, "no_speech_prob": 7.888892469054554e-06}, {"id": 536, "seek": 345200, "start": 3464.0, "end": 3472.0, "text": " Great. Okay. So now we should be able to train on, and I want to make sure we use the exact same training regime. So I'm just going to copy paste this guy.", "tokens": [50364, 407, 321, 362, 264, 4190, 293, 550, 466, 11, 293, 797, 11, 341, 572, 2771, 551, 1177, 380, 1871, 11, 457, 286, 603, 4159, 309, 445, 281, 652, 309, 16532, 13, 50964, 50964, 3769, 13, 1033, 13, 407, 586, 321, 820, 312, 1075, 281, 3847, 322, 11, 293, 286, 528, 281, 652, 988, 321, 764, 264, 1900, 912, 3097, 13120, 13, 407, 286, 478, 445, 516, 281, 5055, 9163, 341, 2146, 13, 51364, 51364, 1033, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.30921130180358886, "compression_ratio": 1.5372340425531914, "no_speech_prob": 7.888892469054554e-06}, {"id": 537, "seek": 345200, "start": 3472.0, "end": 3474.0, "text": " Okay.", "tokens": [50364, 407, 321, 362, 264, 4190, 293, 550, 466, 11, 293, 797, 11, 341, 572, 2771, 551, 1177, 380, 1871, 11, 457, 286, 603, 4159, 309, 445, 281, 652, 309, 16532, 13, 50964, 50964, 3769, 13, 1033, 13, 407, 586, 321, 820, 312, 1075, 281, 3847, 322, 11, 293, 286, 528, 281, 652, 988, 321, 764, 264, 1900, 912, 3097, 13120, 13, 407, 286, 478, 445, 516, 281, 5055, 9163, 341, 2146, 13, 51364, 51364, 1033, 13, 51464, 51464], "temperature": 0.0, "avg_logprob": -0.30921130180358886, "compression_ratio": 1.5372340425531914, "no_speech_prob": 7.888892469054554e-06}, {"id": 538, "seek": 347400, "start": 3474.0, "end": 3483.0, "text": " So we trained on 20 bucks, and now we will use the validation sets. So this is SWOA, self supervised. Okay.", "tokens": [50364, 407, 321, 8895, 322, 945, 11829, 11, 293, 586, 321, 486, 764, 264, 24071, 6352, 13, 407, 341, 307, 20346, 46, 32, 11, 2698, 46533, 13, 1033, 13, 50814, 50814, 407, 321, 366, 1096, 510, 365, 264, 3097, 13, 50964, 50964, 1057, 558, 11, 718, 311, 536, 577, 309, 630, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.25433148470791905, "compression_ratio": 1.2785714285714285, "no_speech_prob": 6.240671609702986e-06}, {"id": 539, "seek": 347400, "start": 3483.0, "end": 3486.0, "text": " So we are done here with the training.", "tokens": [50364, 407, 321, 8895, 322, 945, 11829, 11, 293, 586, 321, 486, 764, 264, 24071, 6352, 13, 407, 341, 307, 20346, 46, 32, 11, 2698, 46533, 13, 1033, 13, 50814, 50814, 407, 321, 366, 1096, 510, 365, 264, 3097, 13, 50964, 50964, 1057, 558, 11, 718, 311, 536, 577, 309, 630, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.25433148470791905, "compression_ratio": 1.2785714285714285, "no_speech_prob": 6.240671609702986e-06}, {"id": 540, "seek": 347400, "start": 3486.0, "end": 3489.0, "text": " All right, let's see how it did.", "tokens": [50364, 407, 321, 8895, 322, 945, 11829, 11, 293, 586, 321, 486, 764, 264, 24071, 6352, 13, 407, 341, 307, 20346, 46, 32, 11, 2698, 46533, 13, 1033, 13, 50814, 50814, 407, 321, 366, 1096, 510, 365, 264, 3097, 13, 50964, 50964, 1057, 558, 11, 718, 311, 536, 577, 309, 630, 13, 51114, 51114], "temperature": 0.0, "avg_logprob": -0.25433148470791905, "compression_ratio": 1.2785714285714285, "no_speech_prob": 6.240671609702986e-06}, {"id": 541, "seek": 348900, "start": 3489.0, "end": 3504.0, "text": " So this is experiment number one. So let's look it up on this tensor board. Did not upload automatically. So let's refresh.", "tokens": [50364, 407, 341, 307, 5120, 1230, 472, 13, 407, 718, 311, 574, 309, 493, 322, 341, 40863, 3150, 13, 2589, 406, 6580, 6772, 13, 407, 718, 311, 15134, 13, 51114, 51114, 1033, 11, 3037, 472, 13, 3769, 13, 51314, 51314, 961, 311, 574, 412, 264, 24071, 14170, 13, 876, 11, 6076, 13, 12313, 1101, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.17915104175436086, "compression_ratio": 1.381578947368421, "no_speech_prob": 7.410791567963315e-06}, {"id": 542, "seek": 348900, "start": 3504.0, "end": 3508.0, "text": " Okay, version one. Great.", "tokens": [50364, 407, 341, 307, 5120, 1230, 472, 13, 407, 718, 311, 574, 309, 493, 322, 341, 40863, 3150, 13, 2589, 406, 6580, 6772, 13, 407, 718, 311, 15134, 13, 51114, 51114, 1033, 11, 3037, 472, 13, 3769, 13, 51314, 51314, 961, 311, 574, 412, 264, 24071, 14170, 13, 876, 11, 6076, 13, 12313, 1101, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.17915104175436086, "compression_ratio": 1.381578947368421, "no_speech_prob": 7.410791567963315e-06}, {"id": 543, "seek": 348900, "start": 3508.0, "end": 3514.0, "text": " Let's look at the validation accuracy. Oh, wow. Much better.", "tokens": [50364, 407, 341, 307, 5120, 1230, 472, 13, 407, 718, 311, 574, 309, 493, 322, 341, 40863, 3150, 13, 2589, 406, 6580, 6772, 13, 407, 718, 311, 15134, 13, 51114, 51114, 1033, 11, 3037, 472, 13, 3769, 13, 51314, 51314, 961, 311, 574, 412, 264, 24071, 14170, 13, 876, 11, 6076, 13, 12313, 1101, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.17915104175436086, "compression_ratio": 1.381578947368421, "no_speech_prob": 7.410791567963315e-06}, {"id": 544, "seek": 351400, "start": 3514.0, "end": 3522.0, "text": " So in blue, we have the self supervised model. And in orange, we have the supervised model.", "tokens": [50364, 407, 294, 3344, 11, 321, 362, 264, 2698, 46533, 2316, 13, 400, 294, 7671, 11, 321, 362, 264, 46533, 2316, 13, 50764, 50764, 407, 436, 1062, 41881, 412, 512, 935, 11, 457, 437, 311, 1880, 307, 300, 309, 14235, 257, 2946, 14170, 709, 4663, 13, 865, 11, 309, 311, 8992, 13, 407, 309, 311, 869, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.1311128854751587, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.863101715571247e-05}, {"id": 545, "seek": 351400, "start": 3522.0, "end": 3534.0, "text": " So they might converge at some point, but what's interesting is that it reaches a higher accuracy much faster. Yeah, it's impressive. So it's great.", "tokens": [50364, 407, 294, 3344, 11, 321, 362, 264, 2698, 46533, 2316, 13, 400, 294, 7671, 11, 321, 362, 264, 46533, 2316, 13, 50764, 50764, 407, 436, 1062, 41881, 412, 512, 935, 11, 457, 437, 311, 1880, 307, 300, 309, 14235, 257, 2946, 14170, 709, 4663, 13, 865, 11, 309, 311, 8992, 13, 407, 309, 311, 869, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.1311128854751587, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.863101715571247e-05}, {"id": 546, "seek": 353400, "start": 3534.0, "end": 3544.0, "text": " I think this is something promising about self supervised learning is that it should help you speed up your convergence. So it'll save you money basically.", "tokens": [50364, 286, 519, 341, 307, 746, 20257, 466, 2698, 46533, 2539, 307, 300, 309, 820, 854, 291, 3073, 493, 428, 32181, 13, 407, 309, 603, 3155, 291, 1460, 1936, 13, 50864, 50864, 407, 718, 311, 3847, 309, 1553, 604, 295, 341, 445, 281, 855, 437, 2314, 562, 291, 500, 380, 362, 604, 295, 264, 659, 12, 17227, 2001, 5245, 13, 407, 286, 393, 360, 300, 538, 445, 6246, 341, 2146, 766, 13, 1779, 13, 407, 286, 478, 406, 516, 281, 3677, 17443, 13, 51364, 51364, 407, 538, 7576, 307, 7908, 13, 51464, 51464, 12753, 13, 51514, 51514, 1033, 11, 393, 321, 1520, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14877511870186283, "compression_ratio": 1.6245059288537549, "no_speech_prob": 2.3918526494526304e-05}, {"id": 547, "seek": 353400, "start": 3544.0, "end": 3554.0, "text": " So let's train it without any of this just to show what happens when you don't have any of the pre-trained models. So I can do that by just turning this guy off. Right. So I'm not going to load weights.", "tokens": [50364, 286, 519, 341, 307, 746, 20257, 466, 2698, 46533, 2539, 307, 300, 309, 820, 854, 291, 3073, 493, 428, 32181, 13, 407, 309, 603, 3155, 291, 1460, 1936, 13, 50864, 50864, 407, 718, 311, 3847, 309, 1553, 604, 295, 341, 445, 281, 855, 437, 2314, 562, 291, 500, 380, 362, 604, 295, 264, 659, 12, 17227, 2001, 5245, 13, 407, 286, 393, 360, 300, 538, 445, 6246, 341, 2146, 766, 13, 1779, 13, 407, 286, 478, 406, 516, 281, 3677, 17443, 13, 51364, 51364, 407, 538, 7576, 307, 7908, 13, 51464, 51464, 12753, 13, 51514, 51514, 1033, 11, 393, 321, 1520, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14877511870186283, "compression_ratio": 1.6245059288537549, "no_speech_prob": 2.3918526494526304e-05}, {"id": 548, "seek": 353400, "start": 3554.0, "end": 3556.0, "text": " So by default is false.", "tokens": [50364, 286, 519, 341, 307, 746, 20257, 466, 2698, 46533, 2539, 307, 300, 309, 820, 854, 291, 3073, 493, 428, 32181, 13, 407, 309, 603, 3155, 291, 1460, 1936, 13, 50864, 50864, 407, 718, 311, 3847, 309, 1553, 604, 295, 341, 445, 281, 855, 437, 2314, 562, 291, 500, 380, 362, 604, 295, 264, 659, 12, 17227, 2001, 5245, 13, 407, 286, 393, 360, 300, 538, 445, 6246, 341, 2146, 766, 13, 1779, 13, 407, 286, 478, 406, 516, 281, 3677, 17443, 13, 51364, 51364, 407, 538, 7576, 307, 7908, 13, 51464, 51464, 12753, 13, 51514, 51514, 1033, 11, 393, 321, 1520, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14877511870186283, "compression_ratio": 1.6245059288537549, "no_speech_prob": 2.3918526494526304e-05}, {"id": 549, "seek": 353400, "start": 3556.0, "end": 3557.0, "text": " Correct.", "tokens": [50364, 286, 519, 341, 307, 746, 20257, 466, 2698, 46533, 2539, 307, 300, 309, 820, 854, 291, 3073, 493, 428, 32181, 13, 407, 309, 603, 3155, 291, 1460, 1936, 13, 50864, 50864, 407, 718, 311, 3847, 309, 1553, 604, 295, 341, 445, 281, 855, 437, 2314, 562, 291, 500, 380, 362, 604, 295, 264, 659, 12, 17227, 2001, 5245, 13, 407, 286, 393, 360, 300, 538, 445, 6246, 341, 2146, 766, 13, 1779, 13, 407, 286, 478, 406, 516, 281, 3677, 17443, 13, 51364, 51364, 407, 538, 7576, 307, 7908, 13, 51464, 51464, 12753, 13, 51514, 51514, 1033, 11, 393, 321, 1520, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14877511870186283, "compression_ratio": 1.6245059288537549, "no_speech_prob": 2.3918526494526304e-05}, {"id": 550, "seek": 353400, "start": 3557.0, "end": 3559.0, "text": " Okay, can we check?", "tokens": [50364, 286, 519, 341, 307, 746, 20257, 466, 2698, 46533, 2539, 307, 300, 309, 820, 854, 291, 3073, 493, 428, 32181, 13, 407, 309, 603, 3155, 291, 1460, 1936, 13, 50864, 50864, 407, 718, 311, 3847, 309, 1553, 604, 295, 341, 445, 281, 855, 437, 2314, 562, 291, 500, 380, 362, 604, 295, 264, 659, 12, 17227, 2001, 5245, 13, 407, 286, 393, 360, 300, 538, 445, 6246, 341, 2146, 766, 13, 1779, 13, 407, 286, 478, 406, 516, 281, 3677, 17443, 13, 51364, 51364, 407, 538, 7576, 307, 7908, 13, 51464, 51464, 12753, 13, 51514, 51514, 1033, 11, 393, 321, 1520, 30, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.14877511870186283, "compression_ratio": 1.6245059288537549, "no_speech_prob": 2.3918526494526304e-05}, {"id": 551, "seek": 355900, "start": 3559.0, "end": 3564.0, "text": " Yeah, we can just set it to false. Okay, that's better. Thank you.", "tokens": [50364, 865, 11, 321, 393, 445, 992, 309, 281, 7908, 13, 1033, 11, 300, 311, 1101, 13, 1044, 291, 13, 50614, 50614, 407, 718, 311, 611, 352, 2286, 293, 4159, 341, 3971, 701, 8781, 644, 293, 445, 3847, 309, 365, 3971, 340, 2904, 490, 264, 588, 2863, 13, 51014, 51014, 865, 13, 407, 286, 486, 445, 12097, 341, 13, 51114, 51114, 1057, 558, 13, 51164, 51164, 1033, 11, 718, 311, 536, 577, 309, 1709, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.12891610463460287, "compression_ratio": 1.4482758620689655, "no_speech_prob": 1.8341108443564735e-05}, {"id": 552, "seek": 355900, "start": 3564.0, "end": 3572.0, "text": " So let's also go ahead and remove this unfreezing part and just train it with unfrozen from the very beginning.", "tokens": [50364, 865, 11, 321, 393, 445, 992, 309, 281, 7908, 13, 1033, 11, 300, 311, 1101, 13, 1044, 291, 13, 50614, 50614, 407, 718, 311, 611, 352, 2286, 293, 4159, 341, 3971, 701, 8781, 644, 293, 445, 3847, 309, 365, 3971, 340, 2904, 490, 264, 588, 2863, 13, 51014, 51014, 865, 13, 407, 286, 486, 445, 12097, 341, 13, 51114, 51114, 1057, 558, 13, 51164, 51164, 1033, 11, 718, 311, 536, 577, 309, 1709, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.12891610463460287, "compression_ratio": 1.4482758620689655, "no_speech_prob": 1.8341108443564735e-05}, {"id": 553, "seek": 355900, "start": 3572.0, "end": 3574.0, "text": " Yeah. So I will just delete this.", "tokens": [50364, 865, 11, 321, 393, 445, 992, 309, 281, 7908, 13, 1033, 11, 300, 311, 1101, 13, 1044, 291, 13, 50614, 50614, 407, 718, 311, 611, 352, 2286, 293, 4159, 341, 3971, 701, 8781, 644, 293, 445, 3847, 309, 365, 3971, 340, 2904, 490, 264, 588, 2863, 13, 51014, 51014, 865, 13, 407, 286, 486, 445, 12097, 341, 13, 51114, 51114, 1057, 558, 13, 51164, 51164, 1033, 11, 718, 311, 536, 577, 309, 1709, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.12891610463460287, "compression_ratio": 1.4482758620689655, "no_speech_prob": 1.8341108443564735e-05}, {"id": 554, "seek": 355900, "start": 3574.0, "end": 3575.0, "text": " All right.", "tokens": [50364, 865, 11, 321, 393, 445, 992, 309, 281, 7908, 13, 1033, 11, 300, 311, 1101, 13, 1044, 291, 13, 50614, 50614, 407, 718, 311, 611, 352, 2286, 293, 4159, 341, 3971, 701, 8781, 644, 293, 445, 3847, 309, 365, 3971, 340, 2904, 490, 264, 588, 2863, 13, 51014, 51014, 865, 13, 407, 286, 486, 445, 12097, 341, 13, 51114, 51114, 1057, 558, 13, 51164, 51164, 1033, 11, 718, 311, 536, 577, 309, 1709, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.12891610463460287, "compression_ratio": 1.4482758620689655, "no_speech_prob": 1.8341108443564735e-05}, {"id": 555, "seek": 355900, "start": 3575.0, "end": 3580.0, "text": " Okay, let's see how it goes.", "tokens": [50364, 865, 11, 321, 393, 445, 992, 309, 281, 7908, 13, 1033, 11, 300, 311, 1101, 13, 1044, 291, 13, 50614, 50614, 407, 718, 311, 611, 352, 2286, 293, 4159, 341, 3971, 701, 8781, 644, 293, 445, 3847, 309, 365, 3971, 340, 2904, 490, 264, 588, 2863, 13, 51014, 51014, 865, 13, 407, 286, 486, 445, 12097, 341, 13, 51114, 51114, 1057, 558, 13, 51164, 51164, 1033, 11, 718, 311, 536, 577, 309, 1709, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.12891610463460287, "compression_ratio": 1.4482758620689655, "no_speech_prob": 1.8341108443564735e-05}, {"id": 556, "seek": 358000, "start": 3580.0, "end": 3590.0, "text": " So here we actually, let's remember that we are training the full backbone and therefore we are doing back backward, like forward and backward through the backbone.", "tokens": [50364, 407, 510, 321, 767, 11, 718, 311, 1604, 300, 321, 366, 3097, 264, 1577, 34889, 293, 4412, 321, 366, 884, 646, 23897, 11, 411, 2128, 293, 23897, 807, 264, 34889, 13, 50864, 50864, 407, 309, 311, 1940, 636, 886, 938, 13, 407, 321, 366, 516, 281, 312, 12767, 558, 586, 412, 30992, 339, 1266, 13, 407, 309, 820, 312, 445, 2489, 13, 400, 718, 311, 6794, 586, 264, 24071, 19490, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.13770983378092447, "compression_ratio": 1.6030150753768844, "no_speech_prob": 7.069098501233384e-06}, {"id": 557, "seek": 358000, "start": 3590.0, "end": 3601.0, "text": " So it's taking way too long. So we are going to be stopping right now at epoch 10. So it should be just fine. And let's compare now the validation curves.", "tokens": [50364, 407, 510, 321, 767, 11, 718, 311, 1604, 300, 321, 366, 3097, 264, 1577, 34889, 293, 4412, 321, 366, 884, 646, 23897, 11, 411, 2128, 293, 23897, 807, 264, 34889, 13, 50864, 50864, 407, 309, 311, 1940, 636, 886, 938, 13, 407, 321, 366, 516, 281, 312, 12767, 558, 586, 412, 30992, 339, 1266, 13, 407, 309, 820, 312, 445, 2489, 13, 400, 718, 311, 6794, 586, 264, 24071, 19490, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.13770983378092447, "compression_ratio": 1.6030150753768844, "no_speech_prob": 7.069098501233384e-06}, {"id": 558, "seek": 360100, "start": 3601.0, "end": 3613.0, "text": " So refresh the TensorBoard.", "tokens": [50364, 407, 15134, 264, 34306, 22493, 515, 13, 50964, 50964, 1057, 558, 13, 1144, 291, 362, 257, 2041, 337, 264, 1151, 14170, 337, 300, 2316, 30, 51114, 51114, 286, 1454, 13, 883, 11, 286, 500, 380, 458, 13, 286, 914, 11, 286, 519, 286, 360, 11, 457, 286, 528, 281, 536, 13, 286, 500, 380, 528, 281, 13, 51514, 51514, 1396, 1242, 411, 13, 51564, 51564, 1033, 11, 718, 311, 536, 13, 876, 1699, 11, 2597, 13, 821, 291, 352, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.153480779557001, "compression_ratio": 1.430232558139535, "no_speech_prob": 5.254920324659906e-06}, {"id": 559, "seek": 360100, "start": 3613.0, "end": 3616.0, "text": " All right. Do you have a guess for the best accuracy for that model?", "tokens": [50364, 407, 15134, 264, 34306, 22493, 515, 13, 50964, 50964, 1057, 558, 13, 1144, 291, 362, 257, 2041, 337, 264, 1151, 14170, 337, 300, 2316, 30, 51114, 51114, 286, 1454, 13, 883, 11, 286, 500, 380, 458, 13, 286, 914, 11, 286, 519, 286, 360, 11, 457, 286, 528, 281, 536, 13, 286, 500, 380, 528, 281, 13, 51514, 51514, 1396, 1242, 411, 13, 51564, 51564, 1033, 11, 718, 311, 536, 13, 876, 1699, 11, 2597, 13, 821, 291, 352, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.153480779557001, "compression_ratio": 1.430232558139535, "no_speech_prob": 5.254920324659906e-06}, {"id": 560, "seek": 360100, "start": 3616.0, "end": 3624.0, "text": " I hope. No, I don't know. I mean, I think I do, but I want to see. I don't want to.", "tokens": [50364, 407, 15134, 264, 34306, 22493, 515, 13, 50964, 50964, 1057, 558, 13, 1144, 291, 362, 257, 2041, 337, 264, 1151, 14170, 337, 300, 2316, 30, 51114, 51114, 286, 1454, 13, 883, 11, 286, 500, 380, 458, 13, 286, 914, 11, 286, 519, 286, 360, 11, 457, 286, 528, 281, 536, 13, 286, 500, 380, 528, 281, 13, 51514, 51514, 1396, 1242, 411, 13, 51564, 51564, 1033, 11, 718, 311, 536, 13, 876, 1699, 11, 2597, 13, 821, 291, 352, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.153480779557001, "compression_ratio": 1.430232558139535, "no_speech_prob": 5.254920324659906e-06}, {"id": 561, "seek": 360100, "start": 3624.0, "end": 3625.0, "text": " Then getting like.", "tokens": [50364, 407, 15134, 264, 34306, 22493, 515, 13, 50964, 50964, 1057, 558, 13, 1144, 291, 362, 257, 2041, 337, 264, 1151, 14170, 337, 300, 2316, 30, 51114, 51114, 286, 1454, 13, 883, 11, 286, 500, 380, 458, 13, 286, 914, 11, 286, 519, 286, 360, 11, 457, 286, 528, 281, 536, 13, 286, 500, 380, 528, 281, 13, 51514, 51514, 1396, 1242, 411, 13, 51564, 51564, 1033, 11, 718, 311, 536, 13, 876, 1699, 11, 2597, 13, 821, 291, 352, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.153480779557001, "compression_ratio": 1.430232558139535, "no_speech_prob": 5.254920324659906e-06}, {"id": 562, "seek": 360100, "start": 3625.0, "end": 3630.0, "text": " Okay, let's see. Oh wait, sorry. There you go.", "tokens": [50364, 407, 15134, 264, 34306, 22493, 515, 13, 50964, 50964, 1057, 558, 13, 1144, 291, 362, 257, 2041, 337, 264, 1151, 14170, 337, 300, 2316, 30, 51114, 51114, 286, 1454, 13, 883, 11, 286, 500, 380, 458, 13, 286, 914, 11, 286, 519, 286, 360, 11, 457, 286, 528, 281, 536, 13, 286, 500, 380, 528, 281, 13, 51514, 51514, 1396, 1242, 411, 13, 51564, 51564, 1033, 11, 718, 311, 536, 13, 876, 1699, 11, 2597, 13, 821, 291, 352, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.153480779557001, "compression_ratio": 1.430232558139535, "no_speech_prob": 5.254920324659906e-06}, {"id": 563, "seek": 363000, "start": 3630.0, "end": 3637.0, "text": " It's gone up. I mean, you know, it's just standard, right? So it's very slow to converge.", "tokens": [50364, 467, 311, 2780, 493, 13, 286, 914, 11, 291, 458, 11, 309, 311, 445, 3832, 11, 558, 30, 407, 309, 311, 588, 2964, 281, 41881, 13, 50714, 50714, 467, 1391, 486, 483, 456, 412, 512, 935, 11, 457, 291, 458, 11, 264, 2372, 295, 14722, 300, 390, 4739, 281, 483, 341, 472, 307, 257, 688, 12284, 570, 291, 1217, 632, 341, 2068, 4059, 13, 51214, 51214, 407, 1338, 11, 309, 311, 665, 13, 286, 914, 11, 309, 311, 406, 1578, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.10738626367905561, "compression_ratio": 1.5303030303030303, "no_speech_prob": 3.3724689274095e-05}, {"id": 564, "seek": 363000, "start": 3637.0, "end": 3647.0, "text": " It probably will get there at some point, but you know, the amount of compute that was required to get this one is a lot cheaper because you already had this strong prior.", "tokens": [50364, 467, 311, 2780, 493, 13, 286, 914, 11, 291, 458, 11, 309, 311, 445, 3832, 11, 558, 30, 407, 309, 311, 588, 2964, 281, 41881, 13, 50714, 50714, 467, 1391, 486, 483, 456, 412, 512, 935, 11, 457, 291, 458, 11, 264, 2372, 295, 14722, 300, 390, 4739, 281, 483, 341, 472, 307, 257, 688, 12284, 570, 291, 1217, 632, 341, 2068, 4059, 13, 51214, 51214, 407, 1338, 11, 309, 311, 665, 13, 286, 914, 11, 309, 311, 406, 1578, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.10738626367905561, "compression_ratio": 1.5303030303030303, "no_speech_prob": 3.3724689274095e-05}, {"id": 565, "seek": 363000, "start": 3647.0, "end": 3650.0, "text": " So yeah, it's good. I mean, it's not bad.", "tokens": [50364, 467, 311, 2780, 493, 13, 286, 914, 11, 291, 458, 11, 309, 311, 445, 3832, 11, 558, 30, 407, 309, 311, 588, 2964, 281, 41881, 13, 50714, 50714, 467, 1391, 486, 483, 456, 412, 512, 935, 11, 457, 291, 458, 11, 264, 2372, 295, 14722, 300, 390, 4739, 281, 483, 341, 472, 307, 257, 688, 12284, 570, 291, 1217, 632, 341, 2068, 4059, 13, 51214, 51214, 407, 1338, 11, 309, 311, 665, 13, 286, 914, 11, 309, 311, 406, 1578, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.10738626367905561, "compression_ratio": 1.5303030303030303, "no_speech_prob": 3.3724689274095e-05}, {"id": 566, "seek": 365000, "start": 3650.0, "end": 3670.0, "text": " So actually with the blue curve, if we actually do like one step only, we just get immediately to 80%. Right. So if we, I think even if we don't do the training of the classifier at the beginning, and we just do one step directly, everything we get immediately at a very good initial point.", "tokens": [50364, 407, 767, 365, 264, 3344, 7605, 11, 498, 321, 767, 360, 411, 472, 1823, 787, 11, 321, 445, 483, 4258, 281, 4688, 6856, 1779, 13, 407, 498, 321, 11, 286, 519, 754, 498, 321, 500, 380, 360, 264, 3097, 295, 264, 1508, 9902, 412, 264, 2863, 11, 293, 321, 445, 360, 472, 1823, 3838, 11, 1203, 321, 483, 4258, 412, 257, 588, 665, 5883, 935, 13, 51364, 51364, 1779, 13, 865, 13, 407, 498, 321, 1856, 264, 2572, 4583, 11, 498, 321, 445, 1856, 1203, 294, 954, 293, 445, 733, 295, 722, 490, 456, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1529799085674864, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.0951835974992719e-05}, {"id": 567, "seek": 365000, "start": 3670.0, "end": 3677.0, "text": " Right. Yeah. So if we leave the final layer, if we just leave everything in person and just kind of start from there.", "tokens": [50364, 407, 767, 365, 264, 3344, 7605, 11, 498, 321, 767, 360, 411, 472, 1823, 787, 11, 321, 445, 483, 4258, 281, 4688, 6856, 1779, 13, 407, 498, 321, 11, 286, 519, 754, 498, 321, 500, 380, 360, 264, 3097, 295, 264, 1508, 9902, 412, 264, 2863, 11, 293, 321, 445, 360, 472, 1823, 3838, 11, 1203, 321, 483, 4258, 412, 257, 588, 665, 5883, 935, 13, 51364, 51364, 1779, 13, 865, 13, 407, 498, 321, 1856, 264, 2572, 4583, 11, 498, 321, 445, 1856, 1203, 294, 954, 293, 445, 733, 295, 722, 490, 456, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.1529799085674864, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.0951835974992719e-05}, {"id": 568, "seek": 367700, "start": 3677.0, "end": 3688.0, "text": " And so this was trained with roughly 50,000 training samples. Let's see what happens if we are really pushing hard and using really few label samples.", "tokens": [50364, 400, 370, 341, 390, 8895, 365, 9810, 2625, 11, 1360, 3097, 10938, 13, 961, 311, 536, 437, 2314, 498, 321, 366, 534, 7380, 1152, 293, 1228, 534, 1326, 7645, 10938, 13, 50914, 50914, 407, 286, 528, 281, 855, 291, 341, 534, 1627, 2445, 294, 264, 27822, 6405, 1219, 4974, 7472, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.09286167838356711, "compression_ratio": 1.4285714285714286, "no_speech_prob": 8.939157851273194e-06}, {"id": 569, "seek": 367700, "start": 3688.0, "end": 3693.0, "text": " So I want to show you this really cool function in the torch library called random split.", "tokens": [50364, 400, 370, 341, 390, 8895, 365, 9810, 2625, 11, 1360, 3097, 10938, 13, 961, 311, 536, 437, 2314, 498, 321, 366, 534, 7380, 1152, 293, 1228, 534, 1326, 7645, 10938, 13, 50914, 50914, 407, 286, 528, 281, 855, 291, 341, 534, 1627, 2445, 294, 264, 27822, 6405, 1219, 4974, 7472, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.09286167838356711, "compression_ratio": 1.4285714285714286, "no_speech_prob": 8.939157851273194e-06}, {"id": 570, "seek": 369300, "start": 3693.0, "end": 3708.0, "text": " So if you have a list of, you know, if you have a data set, for example, here I'm pretending to have a data set, it's just 10 numbers. I want to actually split that into two sets, and I want, I want them to shuffle first and then make that split.", "tokens": [50364, 407, 498, 291, 362, 257, 1329, 295, 11, 291, 458, 11, 498, 291, 362, 257, 1412, 992, 11, 337, 1365, 11, 510, 286, 478, 22106, 281, 362, 257, 1412, 992, 11, 309, 311, 445, 1266, 3547, 13, 286, 528, 281, 767, 7472, 300, 666, 732, 6352, 11, 293, 286, 528, 11, 286, 528, 552, 281, 39426, 700, 293, 550, 652, 300, 7472, 13, 51114, 51114, 407, 11, 294, 341, 1389, 286, 528, 264, 700, 992, 281, 362, 1045, 4959, 294, 309, 293, 550, 264, 1150, 992, 281, 362, 3407, 13, 400, 437, 311, 1627, 307, 300, 286, 393, 652, 341, 15957, 3142, 538, 5127, 264, 8871, 6770, 281, 309, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09686324470921566, "compression_ratio": 1.8, "no_speech_prob": 3.4263281122548506e-05}, {"id": 571, "seek": 369300, "start": 3708.0, "end": 3718.0, "text": " So, in this case I want the first set to have three elements in it and then the second set to have seven. And what's cool is that I can make this deterministic by adding the seed argument to it.", "tokens": [50364, 407, 498, 291, 362, 257, 1329, 295, 11, 291, 458, 11, 498, 291, 362, 257, 1412, 992, 11, 337, 1365, 11, 510, 286, 478, 22106, 281, 362, 257, 1412, 992, 11, 309, 311, 445, 1266, 3547, 13, 286, 528, 281, 767, 7472, 300, 666, 732, 6352, 11, 293, 286, 528, 11, 286, 528, 552, 281, 39426, 700, 293, 550, 652, 300, 7472, 13, 51114, 51114, 407, 11, 294, 341, 1389, 286, 528, 264, 700, 992, 281, 362, 1045, 4959, 294, 309, 293, 550, 264, 1150, 992, 281, 362, 3407, 13, 400, 437, 311, 1627, 307, 300, 286, 393, 652, 341, 15957, 3142, 538, 5127, 264, 8871, 6770, 281, 309, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09686324470921566, "compression_ratio": 1.8, "no_speech_prob": 3.4263281122548506e-05}, {"id": 572, "seek": 371800, "start": 3718.0, "end": 3730.0, "text": " So let me run it once just so you can see. And so you see the first set we got 261. And then when I run it again, still the same thing. Right. So why is this cool because that's what we're using under the hood on the data modules.", "tokens": [50364, 407, 718, 385, 1190, 309, 1564, 445, 370, 291, 393, 536, 13, 400, 370, 291, 536, 264, 700, 992, 321, 658, 7551, 16, 13, 400, 550, 562, 286, 1190, 309, 797, 11, 920, 264, 912, 551, 13, 1779, 13, 407, 983, 307, 341, 1627, 570, 300, 311, 437, 321, 434, 1228, 833, 264, 13376, 322, 264, 1412, 16679, 13, 50964, 50964, 1779, 13, 407, 11, 510, 321, 362, 264, 7576, 8871, 337, 264, 1412, 10088, 11, 370, 300, 562, 291, 434, 884, 264, 745, 3274, 6130, 8312, 11, 321, 434, 516, 281, 747, 264, 383, 12775, 1899, 1266, 3097, 7472, 11, 293, 550, 321, 434, 516, 281, 7472, 300, 7472, 666, 732, 11, 558, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14609663827078684, "compression_ratio": 1.7509881422924902, "no_speech_prob": 1.0288882549502887e-05}, {"id": 573, "seek": 371800, "start": 3730.0, "end": 3747.0, "text": " Right. So, here we have the default seed for the data module, so that when you're doing the Stata Loader, we're going to take the CIFAR 10 training split, and then we're going to split that split into two, right.", "tokens": [50364, 407, 718, 385, 1190, 309, 1564, 445, 370, 291, 393, 536, 13, 400, 370, 291, 536, 264, 700, 992, 321, 658, 7551, 16, 13, 400, 550, 562, 286, 1190, 309, 797, 11, 920, 264, 912, 551, 13, 1779, 13, 407, 983, 307, 341, 1627, 570, 300, 311, 437, 321, 434, 1228, 833, 264, 13376, 322, 264, 1412, 16679, 13, 50964, 50964, 1779, 13, 407, 11, 510, 321, 362, 264, 7576, 8871, 337, 264, 1412, 10088, 11, 370, 300, 562, 291, 434, 884, 264, 745, 3274, 6130, 8312, 11, 321, 434, 516, 281, 747, 264, 383, 12775, 1899, 1266, 3097, 7472, 11, 293, 550, 321, 434, 516, 281, 7472, 300, 7472, 666, 732, 11, 558, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.14609663827078684, "compression_ratio": 1.7509881422924902, "no_speech_prob": 1.0288882549502887e-05}, {"id": 574, "seek": 374700, "start": 3747.0, "end": 3751.0, "text": " One is the train and one is the validation, which we're not using there.", "tokens": [50364, 1485, 307, 264, 3847, 293, 472, 307, 264, 24071, 11, 597, 321, 434, 406, 1228, 456, 13, 50564, 50564, 407, 321, 434, 516, 281, 764, 300, 570, 286, 11, 291, 458, 11, 341, 11, 341, 6770, 510, 466, 7472, 307, 1566, 577, 709, 11, 577, 867, 24071, 4959, 360, 286, 528, 13, 1779, 13, 407, 11, 294, 300, 1389, 11, 562, 286, 1190, 341, 11, 286, 478, 516, 281, 362, 510, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.09910116070195248, "compression_ratio": 1.5988700564971752, "no_speech_prob": 1.1300010555714834e-05}, {"id": 575, "seek": 374700, "start": 3751.0, "end": 3767.0, "text": " So we're going to use that because I, you know, this, this argument here about split is saying how much, how many validation elements do I want. Right. So, in that case, when I run this, I'm going to have here.", "tokens": [50364, 1485, 307, 264, 3847, 293, 472, 307, 264, 24071, 11, 597, 321, 434, 406, 1228, 456, 13, 50564, 50564, 407, 321, 434, 516, 281, 764, 300, 570, 286, 11, 291, 458, 11, 341, 11, 341, 6770, 510, 466, 7472, 307, 1566, 577, 709, 11, 577, 867, 24071, 4959, 360, 286, 528, 13, 1779, 13, 407, 11, 294, 300, 1389, 11, 562, 286, 1190, 341, 11, 286, 478, 516, 281, 362, 510, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.09910116070195248, "compression_ratio": 1.5988700564971752, "no_speech_prob": 1.1300010555714834e-05}, {"id": 576, "seek": 376700, "start": 3767.0, "end": 3790.0, "text": " I'm going to have 50,000 minus, you know, 100, 1000, whatever we want. 50,000 is the number of elements in the training split of CIFAR 10. So I'm saying, okay, if I want to have 100 training elements, then I need to set that to num train and then, then I'll use 50,000 minus 100 validation samples.", "tokens": [50364, 286, 478, 516, 281, 362, 2625, 11, 1360, 3175, 11, 291, 458, 11, 2319, 11, 9714, 11, 2035, 321, 528, 13, 2625, 11, 1360, 307, 264, 1230, 295, 4959, 294, 264, 3097, 7472, 295, 383, 12775, 1899, 1266, 13, 407, 286, 478, 1566, 11, 1392, 11, 498, 286, 528, 281, 362, 2319, 3097, 4959, 11, 550, 286, 643, 281, 992, 300, 281, 1031, 3847, 293, 550, 11, 550, 286, 603, 764, 2625, 11, 1360, 3175, 2319, 24071, 10938, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.14740238419498305, "compression_ratio": 1.5602094240837696, "no_speech_prob": 4.157051080255769e-06}, {"id": 577, "seek": 379000, "start": 3790.0, "end": 3799.0, "text": " So, okay, so we have that. So again, we had, we already have the models to find, right. So we have this, we have three models that we're going to test right now.", "tokens": [50364, 407, 11, 1392, 11, 370, 321, 362, 300, 13, 407, 797, 11, 321, 632, 11, 321, 1217, 362, 264, 5245, 281, 915, 11, 558, 13, 407, 321, 362, 341, 11, 321, 362, 1045, 5245, 300, 321, 434, 516, 281, 1500, 558, 586, 13, 50814, 50814, 440, 659, 12, 17227, 2001, 2316, 300, 390, 659, 12, 17227, 2001, 1228, 2698, 46533, 2539, 13, 51064, 51064, 1396, 264, 659, 12, 17227, 2001, 2316, 300, 390, 659, 12, 17227, 2001, 1228, 46533, 2539, 11, 293, 550, 257, 2316, 300, 307, 406, 659, 12, 17227, 2001, 13, 51564, 51564, 407, 718, 311, 352, 456, 13, 51614, 51614, 400, 321, 500, 380, 15959, 11, 558, 13, 682, 341, 1389, 11, 264, 34889, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.15738376242215515, "compression_ratio": 2.018867924528302, "no_speech_prob": 1.7502030459581874e-05}, {"id": 578, "seek": 379000, "start": 3799.0, "end": 3804.0, "text": " The pre-trained model that was pre-trained using self supervised learning.", "tokens": [50364, 407, 11, 1392, 11, 370, 321, 362, 300, 13, 407, 797, 11, 321, 632, 11, 321, 1217, 362, 264, 5245, 281, 915, 11, 558, 13, 407, 321, 362, 341, 11, 321, 362, 1045, 5245, 300, 321, 434, 516, 281, 1500, 558, 586, 13, 50814, 50814, 440, 659, 12, 17227, 2001, 2316, 300, 390, 659, 12, 17227, 2001, 1228, 2698, 46533, 2539, 13, 51064, 51064, 1396, 264, 659, 12, 17227, 2001, 2316, 300, 390, 659, 12, 17227, 2001, 1228, 46533, 2539, 11, 293, 550, 257, 2316, 300, 307, 406, 659, 12, 17227, 2001, 13, 51564, 51564, 407, 718, 311, 352, 456, 13, 51614, 51614, 400, 321, 500, 380, 15959, 11, 558, 13, 682, 341, 1389, 11, 264, 34889, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.15738376242215515, "compression_ratio": 2.018867924528302, "no_speech_prob": 1.7502030459581874e-05}, {"id": 579, "seek": 379000, "start": 3804.0, "end": 3814.0, "text": " Then the pre-trained model that was pre-trained using supervised learning, and then a model that is not pre-trained.", "tokens": [50364, 407, 11, 1392, 11, 370, 321, 362, 300, 13, 407, 797, 11, 321, 632, 11, 321, 1217, 362, 264, 5245, 281, 915, 11, 558, 13, 407, 321, 362, 341, 11, 321, 362, 1045, 5245, 300, 321, 434, 516, 281, 1500, 558, 586, 13, 50814, 50814, 440, 659, 12, 17227, 2001, 2316, 300, 390, 659, 12, 17227, 2001, 1228, 2698, 46533, 2539, 13, 51064, 51064, 1396, 264, 659, 12, 17227, 2001, 2316, 300, 390, 659, 12, 17227, 2001, 1228, 46533, 2539, 11, 293, 550, 257, 2316, 300, 307, 406, 659, 12, 17227, 2001, 13, 51564, 51564, 407, 718, 311, 352, 456, 13, 51614, 51614, 400, 321, 500, 380, 15959, 11, 558, 13, 682, 341, 1389, 11, 264, 34889, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.15738376242215515, "compression_ratio": 2.018867924528302, "no_speech_prob": 1.7502030459581874e-05}, {"id": 580, "seek": 379000, "start": 3814.0, "end": 3815.0, "text": " So let's go there.", "tokens": [50364, 407, 11, 1392, 11, 370, 321, 362, 300, 13, 407, 797, 11, 321, 632, 11, 321, 1217, 362, 264, 5245, 281, 915, 11, 558, 13, 407, 321, 362, 341, 11, 321, 362, 1045, 5245, 300, 321, 434, 516, 281, 1500, 558, 586, 13, 50814, 50814, 440, 659, 12, 17227, 2001, 2316, 300, 390, 659, 12, 17227, 2001, 1228, 2698, 46533, 2539, 13, 51064, 51064, 1396, 264, 659, 12, 17227, 2001, 2316, 300, 390, 659, 12, 17227, 2001, 1228, 46533, 2539, 11, 293, 550, 257, 2316, 300, 307, 406, 659, 12, 17227, 2001, 13, 51564, 51564, 407, 718, 311, 352, 456, 13, 51614, 51614, 400, 321, 500, 380, 15959, 11, 558, 13, 682, 341, 1389, 11, 264, 34889, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.15738376242215515, "compression_ratio": 2.018867924528302, "no_speech_prob": 1.7502030459581874e-05}, {"id": 581, "seek": 379000, "start": 3815.0, "end": 3818.0, "text": " And we don't freeze, right. In this case, the backbone.", "tokens": [50364, 407, 11, 1392, 11, 370, 321, 362, 300, 13, 407, 797, 11, 321, 632, 11, 321, 1217, 362, 264, 5245, 281, 915, 11, 558, 13, 407, 321, 362, 341, 11, 321, 362, 1045, 5245, 300, 321, 434, 516, 281, 1500, 558, 586, 13, 50814, 50814, 440, 659, 12, 17227, 2001, 2316, 300, 390, 659, 12, 17227, 2001, 1228, 2698, 46533, 2539, 13, 51064, 51064, 1396, 264, 659, 12, 17227, 2001, 2316, 300, 390, 659, 12, 17227, 2001, 1228, 46533, 2539, 11, 293, 550, 257, 2316, 300, 307, 406, 659, 12, 17227, 2001, 13, 51564, 51564, 407, 718, 311, 352, 456, 13, 51614, 51614, 400, 321, 500, 380, 15959, 11, 558, 13, 682, 341, 1389, 11, 264, 34889, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.15738376242215515, "compression_ratio": 2.018867924528302, "no_speech_prob": 1.7502030459581874e-05}, {"id": 582, "seek": 381800, "start": 3818.0, "end": 3825.0, "text": " Yeah, so we're going to unfreeze the backbone. And we're just going to train. So, you know, just to kind of summarize this as a training step.", "tokens": [50364, 865, 11, 370, 321, 434, 516, 281, 3971, 701, 1381, 264, 34889, 13, 400, 321, 434, 445, 516, 281, 3847, 13, 407, 11, 291, 458, 11, 445, 281, 733, 295, 20858, 341, 382, 257, 3097, 1823, 13, 50714, 50714, 3697, 3377, 293, 550, 2489, 10864, 13, 407, 572, 20200, 13, 50914, 50914, 407, 718, 311, 574, 412, 1045, 819, 3547, 295, 5110, 11, 558, 13, 407, 718, 311, 1888, 2319, 8895, 10938, 11, 805, 6866, 8895, 10938, 11, 293, 9714, 8895, 10938, 13, 51464, 51464, 400, 550, 321, 434, 516, 281, 29562, 787, 322, 2319, 15245, 279, 295, 24071, 370, 300, 11, 291, 458, 11, 321, 393, 3073, 493, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.15639848876417728, "compression_ratio": 1.7601626016260163, "no_speech_prob": 9.817243153520394e-06}, {"id": 583, "seek": 381800, "start": 3825.0, "end": 3829.0, "text": " Features and then fine tune. So no freezing.", "tokens": [50364, 865, 11, 370, 321, 434, 516, 281, 3971, 701, 1381, 264, 34889, 13, 400, 321, 434, 445, 516, 281, 3847, 13, 407, 11, 291, 458, 11, 445, 281, 733, 295, 20858, 341, 382, 257, 3097, 1823, 13, 50714, 50714, 3697, 3377, 293, 550, 2489, 10864, 13, 407, 572, 20200, 13, 50914, 50914, 407, 718, 311, 574, 412, 1045, 819, 3547, 295, 5110, 11, 558, 13, 407, 718, 311, 1888, 2319, 8895, 10938, 11, 805, 6866, 8895, 10938, 11, 293, 9714, 8895, 10938, 13, 51464, 51464, 400, 550, 321, 434, 516, 281, 29562, 787, 322, 2319, 15245, 279, 295, 24071, 370, 300, 11, 291, 458, 11, 321, 393, 3073, 493, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.15639848876417728, "compression_ratio": 1.7601626016260163, "no_speech_prob": 9.817243153520394e-06}, {"id": 584, "seek": 381800, "start": 3829.0, "end": 3840.0, "text": " So let's look at three different numbers of examples, right. So let's pick 100 trained samples, 316 trained samples, and 1000 trained samples.", "tokens": [50364, 865, 11, 370, 321, 434, 516, 281, 3971, 701, 1381, 264, 34889, 13, 400, 321, 434, 445, 516, 281, 3847, 13, 407, 11, 291, 458, 11, 445, 281, 733, 295, 20858, 341, 382, 257, 3097, 1823, 13, 50714, 50714, 3697, 3377, 293, 550, 2489, 10864, 13, 407, 572, 20200, 13, 50914, 50914, 407, 718, 311, 574, 412, 1045, 819, 3547, 295, 5110, 11, 558, 13, 407, 718, 311, 1888, 2319, 8895, 10938, 11, 805, 6866, 8895, 10938, 11, 293, 9714, 8895, 10938, 13, 51464, 51464, 400, 550, 321, 434, 516, 281, 29562, 787, 322, 2319, 15245, 279, 295, 24071, 370, 300, 11, 291, 458, 11, 321, 393, 3073, 493, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.15639848876417728, "compression_ratio": 1.7601626016260163, "no_speech_prob": 9.817243153520394e-06}, {"id": 585, "seek": 381800, "start": 3840.0, "end": 3846.0, "text": " And then we're going to validate only on 100 batches of validation so that, you know, we can speed up.", "tokens": [50364, 865, 11, 370, 321, 434, 516, 281, 3971, 701, 1381, 264, 34889, 13, 400, 321, 434, 445, 516, 281, 3847, 13, 407, 11, 291, 458, 11, 445, 281, 733, 295, 20858, 341, 382, 257, 3097, 1823, 13, 50714, 50714, 3697, 3377, 293, 550, 2489, 10864, 13, 407, 572, 20200, 13, 50914, 50914, 407, 718, 311, 574, 412, 1045, 819, 3547, 295, 5110, 11, 558, 13, 407, 718, 311, 1888, 2319, 8895, 10938, 11, 805, 6866, 8895, 10938, 11, 293, 9714, 8895, 10938, 13, 51464, 51464, 400, 550, 321, 434, 516, 281, 29562, 787, 322, 2319, 15245, 279, 295, 24071, 370, 300, 11, 291, 458, 11, 321, 393, 3073, 493, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.15639848876417728, "compression_ratio": 1.7601626016260163, "no_speech_prob": 9.817243153520394e-06}, {"id": 586, "seek": 384600, "start": 3846.0, "end": 3851.0, "text": " So we're going to, we're going to loop over this number of trained samples here.", "tokens": [50364, 407, 321, 434, 516, 281, 11, 321, 434, 516, 281, 6367, 670, 341, 1230, 295, 8895, 10938, 510, 13, 50614, 50614, 400, 550, 286, 478, 445, 11, 286, 478, 445, 41178, 512, 1507, 510, 300, 321, 528, 281, 458, 13, 407, 286, 528, 281, 11, 286, 528, 281, 992, 264, 11469, 30992, 339, 13, 50964, 50964, 407, 286, 528, 281, 652, 988, 321, 362, 264, 912, 1230, 295, 4439, 5315, 13, 51114, 51114, 407, 321, 434, 516, 281, 992, 11, 321, 434, 516, 281, 4948, 1203, 281, 362, 23777, 4439, 13, 400, 550, 321, 434, 516, 281, 28446, 264, 11469, 30992, 339, 490, 300, 11, 558, 13, 407, 597, 307, 341, 2146, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09846795726026225, "compression_ratio": 1.9363636363636363, "no_speech_prob": 3.426268449402414e-05}, {"id": 587, "seek": 384600, "start": 3851.0, "end": 3858.0, "text": " And then I'm just, I'm just plotting some stuff here that we want to know. So I want to, I want to set the max epoch.", "tokens": [50364, 407, 321, 434, 516, 281, 11, 321, 434, 516, 281, 6367, 670, 341, 1230, 295, 8895, 10938, 510, 13, 50614, 50614, 400, 550, 286, 478, 445, 11, 286, 478, 445, 41178, 512, 1507, 510, 300, 321, 528, 281, 458, 13, 407, 286, 528, 281, 11, 286, 528, 281, 992, 264, 11469, 30992, 339, 13, 50964, 50964, 407, 286, 528, 281, 652, 988, 321, 362, 264, 912, 1230, 295, 4439, 5315, 13, 51114, 51114, 407, 321, 434, 516, 281, 992, 11, 321, 434, 516, 281, 4948, 1203, 281, 362, 23777, 4439, 13, 400, 550, 321, 434, 516, 281, 28446, 264, 11469, 30992, 339, 490, 300, 11, 558, 13, 407, 597, 307, 341, 2146, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09846795726026225, "compression_ratio": 1.9363636363636363, "no_speech_prob": 3.426268449402414e-05}, {"id": 588, "seek": 384600, "start": 3858.0, "end": 3861.0, "text": " So I want to make sure we have the same number of steps everywhere.", "tokens": [50364, 407, 321, 434, 516, 281, 11, 321, 434, 516, 281, 6367, 670, 341, 1230, 295, 8895, 10938, 510, 13, 50614, 50614, 400, 550, 286, 478, 445, 11, 286, 478, 445, 41178, 512, 1507, 510, 300, 321, 528, 281, 458, 13, 407, 286, 528, 281, 11, 286, 528, 281, 992, 264, 11469, 30992, 339, 13, 50964, 50964, 407, 286, 528, 281, 652, 988, 321, 362, 264, 912, 1230, 295, 4439, 5315, 13, 51114, 51114, 407, 321, 434, 516, 281, 992, 11, 321, 434, 516, 281, 4948, 1203, 281, 362, 23777, 4439, 13, 400, 550, 321, 434, 516, 281, 28446, 264, 11469, 30992, 339, 490, 300, 11, 558, 13, 407, 597, 307, 341, 2146, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09846795726026225, "compression_ratio": 1.9363636363636363, "no_speech_prob": 3.426268449402414e-05}, {"id": 589, "seek": 384600, "start": 3861.0, "end": 3872.0, "text": " So we're going to set, we're going to limit everything to have 5000 steps. And then we're going to derive the max epoch from that, right. So which is this guy.", "tokens": [50364, 407, 321, 434, 516, 281, 11, 321, 434, 516, 281, 6367, 670, 341, 1230, 295, 8895, 10938, 510, 13, 50614, 50614, 400, 550, 286, 478, 445, 11, 286, 478, 445, 41178, 512, 1507, 510, 300, 321, 528, 281, 458, 13, 407, 286, 528, 281, 11, 286, 528, 281, 992, 264, 11469, 30992, 339, 13, 50964, 50964, 407, 286, 528, 281, 652, 988, 321, 362, 264, 912, 1230, 295, 4439, 5315, 13, 51114, 51114, 407, 321, 434, 516, 281, 992, 11, 321, 434, 516, 281, 4948, 1203, 281, 362, 23777, 4439, 13, 400, 550, 321, 434, 516, 281, 28446, 264, 11469, 30992, 339, 490, 300, 11, 558, 13, 407, 597, 307, 341, 2146, 13, 51664, 51664], "temperature": 0.0, "avg_logprob": -0.09846795726026225, "compression_ratio": 1.9363636363636363, "no_speech_prob": 3.426268449402414e-05}, {"id": 590, "seek": 387200, "start": 3872.0, "end": 3879.0, "text": " And then we want to check validation not on every epoch because it's going to be too slow. We're going to do it every five.", "tokens": [50364, 400, 550, 321, 528, 281, 1520, 24071, 406, 322, 633, 30992, 339, 570, 309, 311, 516, 281, 312, 886, 2964, 13, 492, 434, 516, 281, 360, 309, 633, 1732, 13, 50714, 50714, 1042, 11, 321, 528, 281, 1520, 309, 1732, 1413, 1951, 264, 1230, 295, 3097, 30992, 28346, 13, 407, 341, 307, 516, 281, 980, 505, 577, 2049, 281, 1520, 264, 24071, 13, 51114, 51114, 400, 550, 321, 5883, 1125, 264, 1412, 11, 558, 13, 407, 797, 11, 445, 257, 1412, 10088, 13, 51364, 51364, 400, 321, 434, 516, 281, 2235, 484, 264, 3097, 1412, 3677, 260, 445, 370, 300, 321, 393, 4482, 264, 4641, 370, 321, 458, 577, 867, 3097, 10938, 321, 434, 2614, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08574831485748291, "compression_ratio": 1.8695652173913044, "no_speech_prob": 6.338930234051077e-06}, {"id": 591, "seek": 387200, "start": 3879.0, "end": 3887.0, "text": " Well, we want to check it five times within the number of training epochs. So this is going to tell us how often to check the validation.", "tokens": [50364, 400, 550, 321, 528, 281, 1520, 24071, 406, 322, 633, 30992, 339, 570, 309, 311, 516, 281, 312, 886, 2964, 13, 492, 434, 516, 281, 360, 309, 633, 1732, 13, 50714, 50714, 1042, 11, 321, 528, 281, 1520, 309, 1732, 1413, 1951, 264, 1230, 295, 3097, 30992, 28346, 13, 407, 341, 307, 516, 281, 980, 505, 577, 2049, 281, 1520, 264, 24071, 13, 51114, 51114, 400, 550, 321, 5883, 1125, 264, 1412, 11, 558, 13, 407, 797, 11, 445, 257, 1412, 10088, 13, 51364, 51364, 400, 321, 434, 516, 281, 2235, 484, 264, 3097, 1412, 3677, 260, 445, 370, 300, 321, 393, 4482, 264, 4641, 370, 321, 458, 577, 867, 3097, 10938, 321, 434, 2614, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08574831485748291, "compression_ratio": 1.8695652173913044, "no_speech_prob": 6.338930234051077e-06}, {"id": 592, "seek": 387200, "start": 3887.0, "end": 3892.0, "text": " And then we initialize the data, right. So again, just a data module.", "tokens": [50364, 400, 550, 321, 528, 281, 1520, 24071, 406, 322, 633, 30992, 339, 570, 309, 311, 516, 281, 312, 886, 2964, 13, 492, 434, 516, 281, 360, 309, 633, 1732, 13, 50714, 50714, 1042, 11, 321, 528, 281, 1520, 309, 1732, 1413, 1951, 264, 1230, 295, 3097, 30992, 28346, 13, 407, 341, 307, 516, 281, 980, 505, 577, 2049, 281, 1520, 264, 24071, 13, 51114, 51114, 400, 550, 321, 5883, 1125, 264, 1412, 11, 558, 13, 407, 797, 11, 445, 257, 1412, 10088, 13, 51364, 51364, 400, 321, 434, 516, 281, 2235, 484, 264, 3097, 1412, 3677, 260, 445, 370, 300, 321, 393, 4482, 264, 4641, 370, 321, 458, 577, 867, 3097, 10938, 321, 434, 2614, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08574831485748291, "compression_ratio": 1.8695652173913044, "no_speech_prob": 6.338930234051077e-06}, {"id": 593, "seek": 387200, "start": 3892.0, "end": 3899.0, "text": " And we're going to pull out the training data loader just so that we can print the length so we know how many training samples we're running.", "tokens": [50364, 400, 550, 321, 528, 281, 1520, 24071, 406, 322, 633, 30992, 339, 570, 309, 311, 516, 281, 312, 886, 2964, 13, 492, 434, 516, 281, 360, 309, 633, 1732, 13, 50714, 50714, 1042, 11, 321, 528, 281, 1520, 309, 1732, 1413, 1951, 264, 1230, 295, 3097, 30992, 28346, 13, 407, 341, 307, 516, 281, 980, 505, 577, 2049, 281, 1520, 264, 24071, 13, 51114, 51114, 400, 550, 321, 5883, 1125, 264, 1412, 11, 558, 13, 407, 797, 11, 445, 257, 1412, 10088, 13, 51364, 51364, 400, 321, 434, 516, 281, 2235, 484, 264, 3097, 1412, 3677, 260, 445, 370, 300, 321, 393, 4482, 264, 4641, 370, 321, 458, 577, 867, 3097, 10938, 321, 434, 2614, 13, 51714, 51714], "temperature": 0.0, "avg_logprob": -0.08574831485748291, "compression_ratio": 1.8695652173913044, "no_speech_prob": 6.338930234051077e-06}, {"id": 594, "seek": 389900, "start": 3899.0, "end": 3906.0, "text": " And then the model, right. So we're going to, in this case, use the model that was pre-training, self-supervised learning.", "tokens": [50364, 400, 550, 264, 2316, 11, 558, 13, 407, 321, 434, 516, 281, 11, 294, 341, 1389, 11, 764, 264, 2316, 300, 390, 659, 12, 17227, 1760, 11, 2698, 12, 48172, 24420, 2539, 13, 50714, 50714, 400, 550, 570, 286, 528, 281, 16927, 437, 309, 311, 516, 281, 855, 13, 407, 291, 2351, 385, 3071, 11, 577, 360, 286, 1319, 3037, 293, 264, 40863, 3150, 20820, 420, 2035, 286, 528, 13, 51114, 51114, 286, 478, 516, 281, 767, 5883, 1125, 264, 40863, 3150, 3565, 1321, 293, 550, 992, 264, 1315, 2059, 510, 13, 1779, 13, 407, 309, 311, 516, 281, 312, 12238, 43, 8240, 11, 291, 458, 11, 264, 1230, 295, 3097, 300, 321, 434, 1228, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1515756607055664, "compression_ratio": 1.7538461538461538, "no_speech_prob": 7.646106496395078e-06}, {"id": 595, "seek": 389900, "start": 3906.0, "end": 3914.0, "text": " And then because I want to modify what it's going to show. So you asked me earlier, how do I change version and the tensor board logs or whatever I want.", "tokens": [50364, 400, 550, 264, 2316, 11, 558, 13, 407, 321, 434, 516, 281, 11, 294, 341, 1389, 11, 764, 264, 2316, 300, 390, 659, 12, 17227, 1760, 11, 2698, 12, 48172, 24420, 2539, 13, 50714, 50714, 400, 550, 570, 286, 528, 281, 16927, 437, 309, 311, 516, 281, 855, 13, 407, 291, 2351, 385, 3071, 11, 577, 360, 286, 1319, 3037, 293, 264, 40863, 3150, 20820, 420, 2035, 286, 528, 13, 51114, 51114, 286, 478, 516, 281, 767, 5883, 1125, 264, 40863, 3150, 3565, 1321, 293, 550, 992, 264, 1315, 2059, 510, 13, 1779, 13, 407, 309, 311, 516, 281, 312, 12238, 43, 8240, 11, 291, 458, 11, 264, 1230, 295, 3097, 300, 321, 434, 1228, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1515756607055664, "compression_ratio": 1.7538461538461538, "no_speech_prob": 7.646106496395078e-06}, {"id": 596, "seek": 389900, "start": 3914.0, "end": 3923.0, "text": " I'm going to actually initialize the tensor board logger and then set the name myself here. Right. So it's going to be SSL dash, you know, the number of training that we're using.", "tokens": [50364, 400, 550, 264, 2316, 11, 558, 13, 407, 321, 434, 516, 281, 11, 294, 341, 1389, 11, 764, 264, 2316, 300, 390, 659, 12, 17227, 1760, 11, 2698, 12, 48172, 24420, 2539, 13, 50714, 50714, 400, 550, 570, 286, 528, 281, 16927, 437, 309, 311, 516, 281, 855, 13, 407, 291, 2351, 385, 3071, 11, 577, 360, 286, 1319, 3037, 293, 264, 40863, 3150, 20820, 420, 2035, 286, 528, 13, 51114, 51114, 286, 478, 516, 281, 767, 5883, 1125, 264, 40863, 3150, 3565, 1321, 293, 550, 992, 264, 1315, 2059, 510, 13, 1779, 13, 407, 309, 311, 516, 281, 312, 12238, 43, 8240, 11, 291, 458, 11, 264, 1230, 295, 3097, 300, 321, 434, 1228, 13, 51564, 51564], "temperature": 0.0, "avg_logprob": -0.1515756607055664, "compression_ratio": 1.7538461538461538, "no_speech_prob": 7.646106496395078e-06}, {"id": 597, "seek": 392300, "start": 3923.0, "end": 3930.0, "text": " So it'll be SSL dash 100, 316, and then 1000. And then everything else stays the same.", "tokens": [50364, 407, 309, 603, 312, 12238, 43, 8240, 2319, 11, 805, 6866, 11, 293, 550, 9714, 13, 400, 550, 1203, 1646, 10834, 264, 912, 13, 50714, 50714, 440, 787, 661, 551, 286, 478, 516, 281, 909, 322, 510, 307, 300, 286, 787, 528, 281, 1520, 2319, 24071, 15245, 279, 13, 1779, 13, 51014, 51014, 407, 4948, 1323, 15245, 279, 6915, 2319, 13, 400, 550, 797, 11, 286, 500, 380, 528, 281, 1520, 264, 24071, 322, 633, 3097, 30992, 339, 13, 51314, 51314, 407, 286, 528, 281, 360, 309, 633, 11, 291, 458, 11, 604, 30992, 28346, 13, 682, 341, 1389, 11, 309, 311, 516, 281, 312, 18949, 6772, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09614119359425136, "compression_ratio": 1.7016806722689075, "no_speech_prob": 3.500671482470352e-06}, {"id": 598, "seek": 392300, "start": 3930.0, "end": 3936.0, "text": " The only other thing I'm going to add on here is that I only want to check 100 validation batches. Right.", "tokens": [50364, 407, 309, 603, 312, 12238, 43, 8240, 2319, 11, 805, 6866, 11, 293, 550, 9714, 13, 400, 550, 1203, 1646, 10834, 264, 912, 13, 50714, 50714, 440, 787, 661, 551, 286, 478, 516, 281, 909, 322, 510, 307, 300, 286, 787, 528, 281, 1520, 2319, 24071, 15245, 279, 13, 1779, 13, 51014, 51014, 407, 4948, 1323, 15245, 279, 6915, 2319, 13, 400, 550, 797, 11, 286, 500, 380, 528, 281, 1520, 264, 24071, 322, 633, 3097, 30992, 339, 13, 51314, 51314, 407, 286, 528, 281, 360, 309, 633, 11, 291, 458, 11, 604, 30992, 28346, 13, 682, 341, 1389, 11, 309, 311, 516, 281, 312, 18949, 6772, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09614119359425136, "compression_ratio": 1.7016806722689075, "no_speech_prob": 3.500671482470352e-06}, {"id": 599, "seek": 392300, "start": 3936.0, "end": 3942.0, "text": " So limit val batches equals 100. And then again, I don't want to check the validation on every training epoch.", "tokens": [50364, 407, 309, 603, 312, 12238, 43, 8240, 2319, 11, 805, 6866, 11, 293, 550, 9714, 13, 400, 550, 1203, 1646, 10834, 264, 912, 13, 50714, 50714, 440, 787, 661, 551, 286, 478, 516, 281, 909, 322, 510, 307, 300, 286, 787, 528, 281, 1520, 2319, 24071, 15245, 279, 13, 1779, 13, 51014, 51014, 407, 4948, 1323, 15245, 279, 6915, 2319, 13, 400, 550, 797, 11, 286, 500, 380, 528, 281, 1520, 264, 24071, 322, 633, 3097, 30992, 339, 13, 51314, 51314, 407, 286, 528, 281, 360, 309, 633, 11, 291, 458, 11, 604, 30992, 28346, 13, 682, 341, 1389, 11, 309, 311, 516, 281, 312, 18949, 6772, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09614119359425136, "compression_ratio": 1.7016806722689075, "no_speech_prob": 3.500671482470352e-06}, {"id": 600, "seek": 392300, "start": 3942.0, "end": 3948.0, "text": " So I want to do it every, you know, any epochs. In this case, it's going to be derived automatically.", "tokens": [50364, 407, 309, 603, 312, 12238, 43, 8240, 2319, 11, 805, 6866, 11, 293, 550, 9714, 13, 400, 550, 1203, 1646, 10834, 264, 912, 13, 50714, 50714, 440, 787, 661, 551, 286, 478, 516, 281, 909, 322, 510, 307, 300, 286, 787, 528, 281, 1520, 2319, 24071, 15245, 279, 13, 1779, 13, 51014, 51014, 407, 4948, 1323, 15245, 279, 6915, 2319, 13, 400, 550, 797, 11, 286, 500, 380, 528, 281, 1520, 264, 24071, 322, 633, 3097, 30992, 339, 13, 51314, 51314, 407, 286, 528, 281, 360, 309, 633, 11, 291, 458, 11, 604, 30992, 28346, 13, 682, 341, 1389, 11, 309, 311, 516, 281, 312, 18949, 6772, 13, 51614, 51614], "temperature": 0.0, "avg_logprob": -0.09614119359425136, "compression_ratio": 1.7016806722689075, "no_speech_prob": 3.500671482470352e-06}, {"id": 601, "seek": 394800, "start": 3948.0, "end": 3965.0, "text": " So just to make sure that we are consistent across splits. Okay. So we're going to train this. Ready? All right.", "tokens": [50364, 407, 445, 281, 652, 988, 300, 321, 366, 8398, 2108, 37741, 13, 1033, 13, 407, 321, 434, 516, 281, 3847, 341, 13, 9944, 30, 1057, 558, 13, 51214, 51214, 1033, 13, 407, 321, 445, 8895, 264, 2698, 12, 48172, 24420, 2316, 13, 961, 311, 352, 2286, 293, 3847, 13, 492, 434, 516, 281, 360, 2293, 264, 912, 1507, 300, 321, 445, 630, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11974963857166802, "compression_ratio": 1.5, "no_speech_prob": 1.0128675057785586e-05}, {"id": 602, "seek": 394800, "start": 3965.0, "end": 3976.0, "text": " Okay. So we just trained the self-supervised model. Let's go ahead and train. We're going to do exactly the same stuff that we just did.", "tokens": [50364, 407, 445, 281, 652, 988, 300, 321, 366, 8398, 2108, 37741, 13, 1033, 13, 407, 321, 434, 516, 281, 3847, 341, 13, 9944, 30, 1057, 558, 13, 51214, 51214, 1033, 13, 407, 321, 445, 8895, 264, 2698, 12, 48172, 24420, 2316, 13, 961, 311, 352, 2286, 293, 3847, 13, 492, 434, 516, 281, 360, 2293, 264, 912, 1507, 300, 321, 445, 630, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.11974963857166802, "compression_ratio": 1.5, "no_speech_prob": 1.0128675057785586e-05}, {"id": 603, "seek": 397600, "start": 3976.0, "end": 3982.0, "text": " So I just copy paste the code from above, but now we're going to use our supervised model.", "tokens": [50364, 407, 286, 445, 5055, 9163, 264, 3089, 490, 3673, 11, 457, 586, 321, 434, 516, 281, 764, 527, 46533, 2316, 13, 50664, 50664, 407, 264, 5015, 31890, 2625, 365, 659, 12, 17227, 2001, 17443, 13, 50864, 50864, 12753, 13, 1282, 264, 3256, 2533, 13, 400, 550, 321, 434, 516, 281, 3847, 341, 2146, 382, 731, 13, 51064, 51064, 400, 11, 291, 458, 11, 286, 478, 25290, 300, 472, 46533, 8240, 264, 1230, 295, 3097, 10938, 13, 407, 718, 311, 352, 2286, 293, 3847, 300, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1391936955827006, "compression_ratio": 1.5330188679245282, "no_speech_prob": 2.668674278538674e-05}, {"id": 604, "seek": 397600, "start": 3982.0, "end": 3986.0, "text": " So the ResNet 50 with pre-trained weights.", "tokens": [50364, 407, 286, 445, 5055, 9163, 264, 3089, 490, 3673, 11, 457, 586, 321, 434, 516, 281, 764, 527, 46533, 2316, 13, 50664, 50664, 407, 264, 5015, 31890, 2625, 365, 659, 12, 17227, 2001, 17443, 13, 50864, 50864, 12753, 13, 1282, 264, 3256, 2533, 13, 400, 550, 321, 434, 516, 281, 3847, 341, 2146, 382, 731, 13, 51064, 51064, 400, 11, 291, 458, 11, 286, 478, 25290, 300, 472, 46533, 8240, 264, 1230, 295, 3097, 10938, 13, 407, 718, 311, 352, 2286, 293, 3847, 300, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1391936955827006, "compression_ratio": 1.5330188679245282, "no_speech_prob": 2.668674278538674e-05}, {"id": 605, "seek": 397600, "start": 3986.0, "end": 3990.0, "text": " Correct. On the image net. And then we're going to train this guy as well.", "tokens": [50364, 407, 286, 445, 5055, 9163, 264, 3089, 490, 3673, 11, 457, 586, 321, 434, 516, 281, 764, 527, 46533, 2316, 13, 50664, 50664, 407, 264, 5015, 31890, 2625, 365, 659, 12, 17227, 2001, 17443, 13, 50864, 50864, 12753, 13, 1282, 264, 3256, 2533, 13, 400, 550, 321, 434, 516, 281, 3847, 341, 2146, 382, 731, 13, 51064, 51064, 400, 11, 291, 458, 11, 286, 478, 25290, 300, 472, 46533, 8240, 264, 1230, 295, 3097, 10938, 13, 407, 718, 311, 352, 2286, 293, 3847, 300, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1391936955827006, "compression_ratio": 1.5330188679245282, "no_speech_prob": 2.668674278538674e-05}, {"id": 606, "seek": 397600, "start": 3990.0, "end": 3999.0, "text": " And, you know, I'm naming that one supervised dash the number of training samples. So let's go ahead and train that.", "tokens": [50364, 407, 286, 445, 5055, 9163, 264, 3089, 490, 3673, 11, 457, 586, 321, 434, 516, 281, 764, 527, 46533, 2316, 13, 50664, 50664, 407, 264, 5015, 31890, 2625, 365, 659, 12, 17227, 2001, 17443, 13, 50864, 50864, 12753, 13, 1282, 264, 3256, 2533, 13, 400, 550, 321, 434, 516, 281, 3847, 341, 2146, 382, 731, 13, 51064, 51064, 400, 11, 291, 458, 11, 286, 478, 25290, 300, 472, 46533, 8240, 264, 1230, 295, 3097, 10938, 13, 407, 718, 311, 352, 2286, 293, 3847, 300, 13, 51514, 51514], "temperature": 0.0, "avg_logprob": -0.1391936955827006, "compression_ratio": 1.5330188679245282, "no_speech_prob": 2.668674278538674e-05}, {"id": 607, "seek": 399900, "start": 3999.0, "end": 4008.0, "text": " Okay, great. So we just trained the supervised ResNet. And now we're going to train just the random model. Right.", "tokens": [50364, 1033, 11, 869, 13, 407, 321, 445, 8895, 264, 46533, 5015, 31890, 13, 400, 586, 321, 434, 516, 281, 3847, 445, 264, 4974, 2316, 13, 1779, 13, 50814, 50814, 407, 510, 456, 311, 1825, 659, 12, 17227, 2001, 13, 467, 311, 1203, 3097, 490, 8459, 13, 407, 309, 311, 341, 2146, 510, 13, 51114, 51114, 400, 550, 321, 434, 516, 281, 818, 300, 46533, 406, 659, 12, 17227, 2001, 13, 407, 718, 311, 352, 2286, 293, 3847, 729, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.11462781515466161, "compression_ratio": 1.6702702702702703, "no_speech_prob": 5.01456088386476e-06}, {"id": 608, "seek": 399900, "start": 4008.0, "end": 4014.0, "text": " So here there's nothing pre-trained. It's everything training from scratch. So it's this guy here.", "tokens": [50364, 1033, 11, 869, 13, 407, 321, 445, 8895, 264, 46533, 5015, 31890, 13, 400, 586, 321, 434, 516, 281, 3847, 445, 264, 4974, 2316, 13, 1779, 13, 50814, 50814, 407, 510, 456, 311, 1825, 659, 12, 17227, 2001, 13, 467, 311, 1203, 3097, 490, 8459, 13, 407, 309, 311, 341, 2146, 510, 13, 51114, 51114, 400, 550, 321, 434, 516, 281, 818, 300, 46533, 406, 659, 12, 17227, 2001, 13, 407, 718, 311, 352, 2286, 293, 3847, 729, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.11462781515466161, "compression_ratio": 1.6702702702702703, "no_speech_prob": 5.01456088386476e-06}, {"id": 609, "seek": 399900, "start": 4014.0, "end": 4019.0, "text": " And then we're going to call that supervised not pre-trained. So let's go ahead and train those.", "tokens": [50364, 1033, 11, 869, 13, 407, 321, 445, 8895, 264, 46533, 5015, 31890, 13, 400, 586, 321, 434, 516, 281, 3847, 445, 264, 4974, 2316, 13, 1779, 13, 50814, 50814, 407, 510, 456, 311, 1825, 659, 12, 17227, 2001, 13, 467, 311, 1203, 3097, 490, 8459, 13, 407, 309, 311, 341, 2146, 510, 13, 51114, 51114, 400, 550, 321, 434, 516, 281, 818, 300, 46533, 406, 659, 12, 17227, 2001, 13, 407, 718, 311, 352, 2286, 293, 3847, 729, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.11462781515466161, "compression_ratio": 1.6702702702702703, "no_speech_prob": 5.01456088386476e-06}, {"id": 610, "seek": 401900, "start": 4019.0, "end": 4032.0, "text": " All right. Okay. So they're all done training. Are you, why don't we just look at the logs? What do you think will happen?", "tokens": [50364, 1057, 558, 13, 1033, 13, 407, 436, 434, 439, 1096, 3097, 13, 2014, 291, 11, 983, 500, 380, 321, 445, 574, 412, 264, 20820, 30, 708, 360, 291, 519, 486, 1051, 30, 51014, 51014, 961, 311, 536, 13, 286, 500, 380, 528, 281, 18630, 309, 13, 1033, 13, 1057, 558, 13, 8561, 13, 51514, 51514, 407, 286, 1116, 411, 281, 536, 264, 14170, 337, 264, 24071, 992, 11, 498, 291, 393, 855, 385, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10051518831497584, "compression_ratio": 1.449438202247191, "no_speech_prob": 1.3207391020841897e-05}, {"id": 611, "seek": 401900, "start": 4032.0, "end": 4042.0, "text": " Let's see. I don't want to spoil it. Okay. All right. Cool.", "tokens": [50364, 1057, 558, 13, 1033, 13, 407, 436, 434, 439, 1096, 3097, 13, 2014, 291, 11, 983, 500, 380, 321, 445, 574, 412, 264, 20820, 30, 708, 360, 291, 519, 486, 1051, 30, 51014, 51014, 961, 311, 536, 13, 286, 500, 380, 528, 281, 18630, 309, 13, 1033, 13, 1057, 558, 13, 8561, 13, 51514, 51514, 407, 286, 1116, 411, 281, 536, 264, 14170, 337, 264, 24071, 992, 11, 498, 291, 393, 855, 385, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10051518831497584, "compression_ratio": 1.449438202247191, "no_speech_prob": 1.3207391020841897e-05}, {"id": 612, "seek": 401900, "start": 4042.0, "end": 4047.0, "text": " So I'd like to see the accuracy for the validation set, if you can show me.", "tokens": [50364, 1057, 558, 13, 1033, 13, 407, 436, 434, 439, 1096, 3097, 13, 2014, 291, 11, 983, 500, 380, 321, 445, 574, 412, 264, 20820, 30, 708, 360, 291, 519, 486, 1051, 30, 51014, 51014, 961, 311, 536, 13, 286, 500, 380, 528, 281, 18630, 309, 13, 1033, 13, 1057, 558, 13, 8561, 13, 51514, 51514, 407, 286, 1116, 411, 281, 536, 264, 14170, 337, 264, 24071, 992, 11, 498, 291, 393, 855, 385, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10051518831497584, "compression_ratio": 1.449438202247191, "no_speech_prob": 1.3207391020841897e-05}, {"id": 613, "seek": 404700, "start": 4047.0, "end": 4054.0, "text": " All right. Yes. Let's pick out the ones that we want to show. So these are all the supervised not pre-trained. Right.", "tokens": [50364, 1057, 558, 13, 1079, 13, 961, 311, 1888, 484, 264, 2306, 300, 321, 528, 281, 855, 13, 407, 613, 366, 439, 264, 46533, 406, 659, 12, 17227, 2001, 13, 1779, 13, 50714, 50714, 400, 550, 264, 46533, 293, 550, 321, 434, 613, 366, 264, 2698, 12, 48172, 24420, 2306, 13, 639, 390, 445, 257, 7308, 382, 731, 13, 51064, 51064, 1033, 13, 407, 613, 366, 264, 2306, 321, 528, 281, 6794, 13, 407, 412, 264, 2767, 11, 613, 1074, 510, 11, 613, 366, 264, 4974, 2306, 13, 51414, 51414, 407, 1825, 311, 659, 12, 17227, 2001, 13, 407, 291, 393, 536, 300, 322, 4274, 321, 483, 411, 11, 291, 458, 11, 1392, 11, 321, 483, 294, 264, 24849, 295, 14170, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1012687153286404, "compression_ratio": 1.8207171314741035, "no_speech_prob": 9.223096640198492e-06}, {"id": 614, "seek": 404700, "start": 4054.0, "end": 4061.0, "text": " And then the supervised and then we're these are the self-supervised ones. This was just a trial as well.", "tokens": [50364, 1057, 558, 13, 1079, 13, 961, 311, 1888, 484, 264, 2306, 300, 321, 528, 281, 855, 13, 407, 613, 366, 439, 264, 46533, 406, 659, 12, 17227, 2001, 13, 1779, 13, 50714, 50714, 400, 550, 264, 46533, 293, 550, 321, 434, 613, 366, 264, 2698, 12, 48172, 24420, 2306, 13, 639, 390, 445, 257, 7308, 382, 731, 13, 51064, 51064, 1033, 13, 407, 613, 366, 264, 2306, 321, 528, 281, 6794, 13, 407, 412, 264, 2767, 11, 613, 1074, 510, 11, 613, 366, 264, 4974, 2306, 13, 51414, 51414, 407, 1825, 311, 659, 12, 17227, 2001, 13, 407, 291, 393, 536, 300, 322, 4274, 321, 483, 411, 11, 291, 458, 11, 1392, 11, 321, 483, 294, 264, 24849, 295, 14170, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1012687153286404, "compression_ratio": 1.8207171314741035, "no_speech_prob": 9.223096640198492e-06}, {"id": 615, "seek": 404700, "start": 4061.0, "end": 4068.0, "text": " Okay. So these are the ones we want to compare. So at the bottom, these guys here, these are the random ones.", "tokens": [50364, 1057, 558, 13, 1079, 13, 961, 311, 1888, 484, 264, 2306, 300, 321, 528, 281, 855, 13, 407, 613, 366, 439, 264, 46533, 406, 659, 12, 17227, 2001, 13, 1779, 13, 50714, 50714, 400, 550, 264, 46533, 293, 550, 321, 434, 613, 366, 264, 2698, 12, 48172, 24420, 2306, 13, 639, 390, 445, 257, 7308, 382, 731, 13, 51064, 51064, 1033, 13, 407, 613, 366, 264, 2306, 321, 528, 281, 6794, 13, 407, 412, 264, 2767, 11, 613, 1074, 510, 11, 613, 366, 264, 4974, 2306, 13, 51414, 51414, 407, 1825, 311, 659, 12, 17227, 2001, 13, 407, 291, 393, 536, 300, 322, 4274, 321, 483, 411, 11, 291, 458, 11, 1392, 11, 321, 483, 294, 264, 24849, 295, 14170, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1012687153286404, "compression_ratio": 1.8207171314741035, "no_speech_prob": 9.223096640198492e-06}, {"id": 616, "seek": 404700, "start": 4068.0, "end": 4075.0, "text": " So nothing's pre-trained. So you can see that on average we get like, you know, okay, we get in the teens of accuracy here.", "tokens": [50364, 1057, 558, 13, 1079, 13, 961, 311, 1888, 484, 264, 2306, 300, 321, 528, 281, 855, 13, 407, 613, 366, 439, 264, 46533, 406, 659, 12, 17227, 2001, 13, 1779, 13, 50714, 50714, 400, 550, 264, 46533, 293, 550, 321, 434, 613, 366, 264, 2698, 12, 48172, 24420, 2306, 13, 639, 390, 445, 257, 7308, 382, 731, 13, 51064, 51064, 1033, 13, 407, 613, 366, 264, 2306, 321, 528, 281, 6794, 13, 407, 412, 264, 2767, 11, 613, 1074, 510, 11, 613, 366, 264, 4974, 2306, 13, 51414, 51414, 407, 1825, 311, 659, 12, 17227, 2001, 13, 407, 291, 393, 536, 300, 322, 4274, 321, 483, 411, 11, 291, 458, 11, 1392, 11, 321, 483, 294, 264, 24849, 295, 14170, 510, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.1012687153286404, "compression_ratio": 1.8207171314741035, "no_speech_prob": 9.223096640198492e-06}, {"id": 617, "seek": 407500, "start": 4075.0, "end": 4080.0, "text": " Yeah. And then in the middle guys, these are the ones that are supervised pre-trained.", "tokens": [50364, 865, 13, 400, 550, 294, 264, 2808, 1074, 11, 613, 366, 264, 2306, 300, 366, 46533, 659, 12, 17227, 2001, 13, 50614, 50614, 407, 264, 1472, 10483, 293, 321, 483, 294, 264, 11, 291, 458, 11, 945, 82, 293, 2217, 82, 11, 597, 307, 869, 13, 407, 291, 393, 536, 300, 264, 5003, 2539, 1985, 11, 558, 30, 51064, 51064, 407, 6772, 6091, 382, 709, 3389, 11, 558, 30, 7587, 13, 51264, 51264, 400, 437, 311, 1880, 307, 300, 562, 436, 434, 8895, 1553, 16949, 11, 613, 1074, 11, 321, 483, 3834, 264, 3389, 295, 264, 5245, 300, 645, 8895, 365, 264, 16949, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10250186920166016, "compression_ratio": 1.703125, "no_speech_prob": 2.5068702598218806e-05}, {"id": 618, "seek": 407500, "start": 4080.0, "end": 4089.0, "text": " So the rest nuts and we get in the, you know, 20s and 30s, which is great. So you can see that the transfer learning works, right?", "tokens": [50364, 865, 13, 400, 550, 294, 264, 2808, 1074, 11, 613, 366, 264, 2306, 300, 366, 46533, 659, 12, 17227, 2001, 13, 50614, 50614, 407, 264, 1472, 10483, 293, 321, 483, 294, 264, 11, 291, 458, 11, 945, 82, 293, 2217, 82, 11, 597, 307, 869, 13, 407, 291, 393, 536, 300, 264, 5003, 2539, 1985, 11, 558, 30, 51064, 51064, 407, 6772, 6091, 382, 709, 3389, 11, 558, 30, 7587, 13, 51264, 51264, 400, 437, 311, 1880, 307, 300, 562, 436, 434, 8895, 1553, 16949, 11, 613, 1074, 11, 321, 483, 3834, 264, 3389, 295, 264, 5245, 300, 645, 8895, 365, 264, 16949, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10250186920166016, "compression_ratio": 1.703125, "no_speech_prob": 2.5068702598218806e-05}, {"id": 619, "seek": 407500, "start": 4089.0, "end": 4093.0, "text": " So automatically twice as much performance, right? Exactly.", "tokens": [50364, 865, 13, 400, 550, 294, 264, 2808, 1074, 11, 613, 366, 264, 2306, 300, 366, 46533, 659, 12, 17227, 2001, 13, 50614, 50614, 407, 264, 1472, 10483, 293, 321, 483, 294, 264, 11, 291, 458, 11, 945, 82, 293, 2217, 82, 11, 597, 307, 869, 13, 407, 291, 393, 536, 300, 264, 5003, 2539, 1985, 11, 558, 30, 51064, 51064, 407, 6772, 6091, 382, 709, 3389, 11, 558, 30, 7587, 13, 51264, 51264, 400, 437, 311, 1880, 307, 300, 562, 436, 434, 8895, 1553, 16949, 11, 613, 1074, 11, 321, 483, 3834, 264, 3389, 295, 264, 5245, 300, 645, 8895, 365, 264, 16949, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10250186920166016, "compression_ratio": 1.703125, "no_speech_prob": 2.5068702598218806e-05}, {"id": 620, "seek": 407500, "start": 4093.0, "end": 4103.0, "text": " And what's interesting is that when they're trained without labels, these guys, we get double the performance of the models that were trained with the labels.", "tokens": [50364, 865, 13, 400, 550, 294, 264, 2808, 1074, 11, 613, 366, 264, 2306, 300, 366, 46533, 659, 12, 17227, 2001, 13, 50614, 50614, 407, 264, 1472, 10483, 293, 321, 483, 294, 264, 11, 291, 458, 11, 945, 82, 293, 2217, 82, 11, 597, 307, 869, 13, 407, 291, 393, 536, 300, 264, 5003, 2539, 1985, 11, 558, 30, 51064, 51064, 407, 6772, 6091, 382, 709, 3389, 11, 558, 30, 7587, 13, 51264, 51264, 400, 437, 311, 1880, 307, 300, 562, 436, 434, 8895, 1553, 16949, 11, 613, 1074, 11, 321, 483, 3834, 264, 3389, 295, 264, 5245, 300, 645, 8895, 365, 264, 16949, 13, 51764, 51764], "temperature": 0.0, "avg_logprob": -0.10250186920166016, "compression_ratio": 1.703125, "no_speech_prob": 2.5068702598218806e-05}, {"id": 621, "seek": 410300, "start": 4103.0, "end": 4115.0, "text": " Wow. Impressive. So, yeah, I don't know. I guess we can't generalize too much from, you know, image net to C410, but to me, this is, this is super promising as well.", "tokens": [50364, 3153, 13, 8270, 22733, 13, 407, 11, 1338, 11, 286, 500, 380, 458, 13, 286, 2041, 321, 393, 380, 2674, 1125, 886, 709, 490, 11, 291, 458, 11, 3256, 2533, 281, 383, 19, 3279, 11, 457, 281, 385, 11, 341, 307, 11, 341, 307, 1687, 20257, 382, 731, 13, 50964, 50964, 1057, 558, 13, 407, 718, 311, 11, 718, 311, 976, 797, 264, 8135, 1889, 13, 708, 307, 264, 935, 295, 965, 6898, 30, 708, 630, 321, 1466, 30, 2305, 630, 321, 722, 30, 51364, 51364, 407, 321, 1409, 365, 15424, 281, 291, 264, 3410, 295, 5003, 2539, 13, 492, 5343, 264, 46533, 3037, 293, 264, 2693, 12879, 24420, 3037, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.13848448214323625, "compression_ratio": 1.6784313725490196, "no_speech_prob": 4.002324931207113e-05}, {"id": 622, "seek": 410300, "start": 4115.0, "end": 4123.0, "text": " All right. So let's, let's give again the punchline. What is the point of today lesson? What did we learn? Where did we start?", "tokens": [50364, 3153, 13, 8270, 22733, 13, 407, 11, 1338, 11, 286, 500, 380, 458, 13, 286, 2041, 321, 393, 380, 2674, 1125, 886, 709, 490, 11, 291, 458, 11, 3256, 2533, 281, 383, 19, 3279, 11, 457, 281, 385, 11, 341, 307, 11, 341, 307, 1687, 20257, 382, 731, 13, 50964, 50964, 1057, 558, 13, 407, 718, 311, 11, 718, 311, 976, 797, 264, 8135, 1889, 13, 708, 307, 264, 935, 295, 965, 6898, 30, 708, 630, 321, 1466, 30, 2305, 630, 321, 722, 30, 51364, 51364, 407, 321, 1409, 365, 15424, 281, 291, 264, 3410, 295, 5003, 2539, 13, 492, 5343, 264, 46533, 3037, 293, 264, 2693, 12879, 24420, 3037, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.13848448214323625, "compression_ratio": 1.6784313725490196, "no_speech_prob": 4.002324931207113e-05}, {"id": 623, "seek": 410300, "start": 4123.0, "end": 4132.0, "text": " So we started with introducing to you the concept of transfer learning. We covered the supervised version and the unsupervised version.", "tokens": [50364, 3153, 13, 8270, 22733, 13, 407, 11, 1338, 11, 286, 500, 380, 458, 13, 286, 2041, 321, 393, 380, 2674, 1125, 886, 709, 490, 11, 291, 458, 11, 3256, 2533, 281, 383, 19, 3279, 11, 457, 281, 385, 11, 341, 307, 11, 341, 307, 1687, 20257, 382, 731, 13, 50964, 50964, 1057, 558, 13, 407, 718, 311, 11, 718, 311, 976, 797, 264, 8135, 1889, 13, 708, 307, 264, 935, 295, 965, 6898, 30, 708, 630, 321, 1466, 30, 2305, 630, 321, 722, 30, 51364, 51364, 407, 321, 1409, 365, 15424, 281, 291, 264, 3410, 295, 5003, 2539, 13, 492, 5343, 264, 46533, 3037, 293, 264, 2693, 12879, 24420, 3037, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.13848448214323625, "compression_ratio": 1.6784313725490196, "no_speech_prob": 4.002324931207113e-05}, {"id": 624, "seek": 413200, "start": 4132.0, "end": 4148.0, "text": " Here, at least we concluded in this tiny experiment that the performance using the unsupervised version are much better than we don't know whether this is, you know, a actual result that generalizes.", "tokens": [50364, 1692, 11, 412, 1935, 321, 22960, 294, 341, 5870, 5120, 300, 264, 3389, 1228, 264, 2693, 12879, 24420, 3037, 366, 709, 1101, 813, 321, 500, 380, 458, 1968, 341, 307, 11, 291, 458, 11, 257, 3539, 1874, 300, 2674, 5660, 13, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.10359553231133355, "compression_ratio": 1.4014084507042253, "no_speech_prob": 3.1377635423268657e-06}, {"id": 625, "seek": 414800, "start": 4148.0, "end": 4164.0, "text": " But the point is that the unsupervised learning version allows us to train a backbone on our own data, right? Rather than, instead the supervised learning, you need to actually have annotated data, which is expensive, right?", "tokens": [50364, 583, 264, 935, 307, 300, 264, 2693, 12879, 24420, 2539, 3037, 4045, 505, 281, 3847, 257, 34889, 322, 527, 1065, 1412, 11, 558, 30, 16571, 813, 11, 2602, 264, 46533, 2539, 11, 291, 643, 281, 767, 362, 25339, 770, 1412, 11, 597, 307, 5124, 11, 558, 30, 51164, 51164], "temperature": 0.0, "avg_logprob": -0.1081388043422325, "compression_ratio": 1.5238095238095237, "no_speech_prob": 7.521932730014669e-06}, {"id": 626, "seek": 416400, "start": 4164.0, "end": 4183.0, "text": " So this is actually a very big, a very big point, right? Such that if you need to do something in a practical aspect that you have a company or whatever, and then you have plenty of data, you don't really necessarily have the labels because we said plenty of times those are expensive.", "tokens": [50364, 407, 341, 307, 767, 257, 588, 955, 11, 257, 588, 955, 935, 11, 558, 30, 9653, 300, 498, 291, 643, 281, 360, 746, 294, 257, 8496, 4171, 300, 291, 362, 257, 2237, 420, 2035, 11, 293, 550, 291, 362, 7140, 295, 1412, 11, 291, 500, 380, 534, 4725, 362, 264, 16949, 570, 321, 848, 7140, 295, 1413, 729, 366, 5124, 13, 51314, 51314], "temperature": 0.0, "avg_logprob": -0.1078170923086313, "compression_ratio": 1.5573770491803278, "no_speech_prob": 6.538186880788999e-06}, {"id": 627, "seek": 418300, "start": 4183.0, "end": 4197.0, "text": " You can still pre train your backbone with those unsupervised algorithms that are already available to you. If you use the bolts, right? So everything is already coded and, and checked with the authors, I think.", "tokens": [50364, 509, 393, 920, 659, 3847, 428, 34889, 365, 729, 2693, 12879, 24420, 14642, 300, 366, 1217, 2435, 281, 291, 13, 759, 291, 764, 264, 18127, 11, 558, 30, 407, 1203, 307, 1217, 34874, 293, 11, 293, 10033, 365, 264, 16552, 11, 286, 519, 13, 51064, 51064], "temperature": 0.0, "avg_logprob": -0.13017637530962625, "compression_ratio": 1.3790849673202614, "no_speech_prob": 3.5881723306374624e-05}, {"id": 628, "seek": 419700, "start": 4197.0, "end": 4217.0, "text": " And they compared like in terms of performance, not only in the accuracy, but also in the computations. I think, like, I think this lightning has so many, you know, checks for being able to maintain a very, you know, high speed, no, it doesn't slow you down to, doesn't slow you down.", "tokens": [50364, 400, 436, 5347, 411, 294, 2115, 295, 3389, 11, 406, 787, 294, 264, 14170, 11, 457, 611, 294, 264, 2807, 763, 13, 286, 519, 11, 411, 11, 286, 519, 341, 16589, 575, 370, 867, 11, 291, 458, 11, 13834, 337, 885, 1075, 281, 6909, 257, 588, 11, 291, 458, 11, 1090, 3073, 11, 572, 11, 309, 1177, 380, 2964, 291, 760, 281, 11, 1177, 380, 2964, 291, 760, 13, 51364, 51364], "temperature": 0.0, "avg_logprob": -0.14264736436817743, "compression_ratio": 1.6045197740112995, "no_speech_prob": 0.00010852473496925086}, {"id": 629, "seek": 421700, "start": 4217.0, "end": 4230.0, "text": " And so again, we have all those unsupervised, self supervised learning algorithms that are, you know, coming out, you know, just recently, like this year, they were from July or June, whatever, 2020.", "tokens": [50364, 400, 370, 797, 11, 321, 362, 439, 729, 2693, 12879, 24420, 11, 2698, 46533, 2539, 14642, 300, 366, 11, 291, 458, 11, 1348, 484, 11, 291, 458, 11, 445, 3938, 11, 411, 341, 1064, 11, 436, 645, 490, 7370, 420, 6928, 11, 2035, 11, 4808, 13, 51014, 51014, 400, 436, 366, 1217, 2435, 510, 294, 341, 6405, 13, 492, 393, 764, 552, 337, 3097, 11, 337, 3097, 428, 2316, 322, 428, 1065, 1412, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08697836827009152, "compression_ratio": 1.5502392344497609, "no_speech_prob": 1.5145550605666358e-05}, {"id": 630, "seek": 421700, "start": 4230.0, "end": 4238.0, "text": " And they are already available here in this library. We can use them for training, for training your model on your own data.", "tokens": [50364, 400, 370, 797, 11, 321, 362, 439, 729, 2693, 12879, 24420, 11, 2698, 46533, 2539, 14642, 300, 366, 11, 291, 458, 11, 1348, 484, 11, 291, 458, 11, 445, 3938, 11, 411, 341, 1064, 11, 436, 645, 490, 7370, 420, 6928, 11, 2035, 11, 4808, 13, 51014, 51014, 400, 436, 366, 1217, 2435, 510, 294, 341, 6405, 13, 492, 393, 764, 552, 337, 3097, 11, 337, 3097, 428, 2316, 322, 428, 1065, 1412, 13, 51414, 51414], "temperature": 0.0, "avg_logprob": -0.08697836827009152, "compression_ratio": 1.5502392344497609, "no_speech_prob": 1.5145550605666358e-05}, {"id": 631, "seek": 423800, "start": 4238.0, "end": 4252.0, "text": " And then we simply swap in a classifier. We just put a classifier on top of the network and then fine tune this classifier or even the other weights using those labels we have.", "tokens": [50364, 400, 550, 321, 2935, 18135, 294, 257, 1508, 9902, 13, 492, 445, 829, 257, 1508, 9902, 322, 1192, 295, 264, 3209, 293, 550, 2489, 10864, 341, 1508, 9902, 420, 754, 264, 661, 17443, 1228, 729, 16949, 321, 362, 13, 51064, 51064, 400, 370, 341, 390, 1936, 264, 12691, 295, 965, 6898, 13, 51314, 51314, 11998, 300, 286, 6721, 30, 51414, 51414, 883, 11, 286, 519, 300, 390, 2176, 13, 51464, 51464, 1057, 558, 13, 1044, 291, 370, 709, 11, 6740, 11, 337, 885, 365, 505, 965, 13, 51664, 51664, 467, 390, 534, 869, 13, 286, 534, 3264, 370, 867, 721, 490, 291, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0922665729700962, "compression_ratio": 1.6680161943319838, "no_speech_prob": 1.4265433492255397e-05}, {"id": 632, "seek": 423800, "start": 4252.0, "end": 4257.0, "text": " And so this was basically the summary of today lesson.", "tokens": [50364, 400, 550, 321, 2935, 18135, 294, 257, 1508, 9902, 13, 492, 445, 829, 257, 1508, 9902, 322, 1192, 295, 264, 3209, 293, 550, 2489, 10864, 341, 1508, 9902, 420, 754, 264, 661, 17443, 1228, 729, 16949, 321, 362, 13, 51064, 51064, 400, 370, 341, 390, 1936, 264, 12691, 295, 965, 6898, 13, 51314, 51314, 11998, 300, 286, 6721, 30, 51414, 51414, 883, 11, 286, 519, 300, 390, 2176, 13, 51464, 51464, 1057, 558, 13, 1044, 291, 370, 709, 11, 6740, 11, 337, 885, 365, 505, 965, 13, 51664, 51664, 467, 390, 534, 869, 13, 286, 534, 3264, 370, 867, 721, 490, 291, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0922665729700962, "compression_ratio": 1.6680161943319838, "no_speech_prob": 1.4265433492255397e-05}, {"id": 633, "seek": 423800, "start": 4257.0, "end": 4259.0, "text": " Anything that I missed?", "tokens": [50364, 400, 550, 321, 2935, 18135, 294, 257, 1508, 9902, 13, 492, 445, 829, 257, 1508, 9902, 322, 1192, 295, 264, 3209, 293, 550, 2489, 10864, 341, 1508, 9902, 420, 754, 264, 661, 17443, 1228, 729, 16949, 321, 362, 13, 51064, 51064, 400, 370, 341, 390, 1936, 264, 12691, 295, 965, 6898, 13, 51314, 51314, 11998, 300, 286, 6721, 30, 51414, 51414, 883, 11, 286, 519, 300, 390, 2176, 13, 51464, 51464, 1057, 558, 13, 1044, 291, 370, 709, 11, 6740, 11, 337, 885, 365, 505, 965, 13, 51664, 51664, 467, 390, 534, 869, 13, 286, 534, 3264, 370, 867, 721, 490, 291, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0922665729700962, "compression_ratio": 1.6680161943319838, "no_speech_prob": 1.4265433492255397e-05}, {"id": 634, "seek": 423800, "start": 4259.0, "end": 4260.0, "text": " No, I think that was perfect.", "tokens": [50364, 400, 550, 321, 2935, 18135, 294, 257, 1508, 9902, 13, 492, 445, 829, 257, 1508, 9902, 322, 1192, 295, 264, 3209, 293, 550, 2489, 10864, 341, 1508, 9902, 420, 754, 264, 661, 17443, 1228, 729, 16949, 321, 362, 13, 51064, 51064, 400, 370, 341, 390, 1936, 264, 12691, 295, 965, 6898, 13, 51314, 51314, 11998, 300, 286, 6721, 30, 51414, 51414, 883, 11, 286, 519, 300, 390, 2176, 13, 51464, 51464, 1057, 558, 13, 1044, 291, 370, 709, 11, 6740, 11, 337, 885, 365, 505, 965, 13, 51664, 51664, 467, 390, 534, 869, 13, 286, 534, 3264, 370, 867, 721, 490, 291, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0922665729700962, "compression_ratio": 1.6680161943319838, "no_speech_prob": 1.4265433492255397e-05}, {"id": 635, "seek": 423800, "start": 4260.0, "end": 4264.0, "text": " All right. Thank you so much, William, for being with us today.", "tokens": [50364, 400, 550, 321, 2935, 18135, 294, 257, 1508, 9902, 13, 492, 445, 829, 257, 1508, 9902, 322, 1192, 295, 264, 3209, 293, 550, 2489, 10864, 341, 1508, 9902, 420, 754, 264, 661, 17443, 1228, 729, 16949, 321, 362, 13, 51064, 51064, 400, 370, 341, 390, 1936, 264, 12691, 295, 965, 6898, 13, 51314, 51314, 11998, 300, 286, 6721, 30, 51414, 51414, 883, 11, 286, 519, 300, 390, 2176, 13, 51464, 51464, 1057, 558, 13, 1044, 291, 370, 709, 11, 6740, 11, 337, 885, 365, 505, 965, 13, 51664, 51664, 467, 390, 534, 869, 13, 286, 534, 3264, 370, 867, 721, 490, 291, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0922665729700962, "compression_ratio": 1.6680161943319838, "no_speech_prob": 1.4265433492255397e-05}, {"id": 636, "seek": 423800, "start": 4264.0, "end": 4267.0, "text": " It was really great. I really learned so many things from you.", "tokens": [50364, 400, 550, 321, 2935, 18135, 294, 257, 1508, 9902, 13, 492, 445, 829, 257, 1508, 9902, 322, 1192, 295, 264, 3209, 293, 550, 2489, 10864, 341, 1508, 9902, 420, 754, 264, 661, 17443, 1228, 729, 16949, 321, 362, 13, 51064, 51064, 400, 370, 341, 390, 1936, 264, 12691, 295, 965, 6898, 13, 51314, 51314, 11998, 300, 286, 6721, 30, 51414, 51414, 883, 11, 286, 519, 300, 390, 2176, 13, 51464, 51464, 1057, 558, 13, 1044, 291, 370, 709, 11, 6740, 11, 337, 885, 365, 505, 965, 13, 51664, 51664, 467, 390, 534, 869, 13, 286, 534, 3264, 370, 867, 721, 490, 291, 13, 51814, 51814], "temperature": 0.0, "avg_logprob": -0.0922665729700962, "compression_ratio": 1.6680161943319838, "no_speech_prob": 1.4265433492255397e-05}, {"id": 637, "seek": 426700, "start": 4267.0, "end": 4273.0, "text": " Thank you for having me. This is great. I hope this is useful for everyone.", "tokens": [50364, 1044, 291, 337, 1419, 385, 13, 639, 307, 869, 13, 286, 1454, 341, 307, 4420, 337, 1518, 13, 50664, 50664, 286, 519, 309, 307, 13, 1057, 558, 13, 3560, 257, 665, 5634, 420, 786, 420, 6499, 420, 2446, 11, 2035, 264, 786, 291, 434, 1976, 341, 960, 13, 51014, 51014, 4621, 6543, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13690737315586635, "compression_ratio": 1.389261744966443, "no_speech_prob": 0.00021261641813907772}, {"id": 638, "seek": 426700, "start": 4273.0, "end": 4280.0, "text": " I think it is. All right. Have a good evening or day or afternoon or morning, whatever the day you're watching this video.", "tokens": [50364, 1044, 291, 337, 1419, 385, 13, 639, 307, 869, 13, 286, 1454, 341, 307, 4420, 337, 1518, 13, 50664, 50664, 286, 519, 309, 307, 13, 1057, 558, 13, 3560, 257, 665, 5634, 420, 786, 420, 6499, 420, 2446, 11, 2035, 264, 786, 291, 434, 1976, 341, 960, 13, 51014, 51014, 4621, 6543, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13690737315586635, "compression_ratio": 1.389261744966443, "no_speech_prob": 0.00021261641813907772}, {"id": 639, "seek": 428000, "start": 4280.0, "end": 4299.0, "text": " All right. Bye bye.", "tokens": [50364, 1057, 558, 13, 4621, 6543, 13, 51314], "temperature": 0.0, "avg_logprob": -0.33860238393147785, "compression_ratio": 0.7037037037037037, "no_speech_prob": 0.000336944533046335}], "language": "en", "video_id": "nCq_vy9qE-k", "entity": "Yann LeCun"}}