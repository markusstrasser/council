{"video_id": "fX86EFWljY0", "title": "2.1 Linear Regression with Multiple Variables | Multiple features --[Machine Learning | Andrew Ng]", "description": "First Course:\nSupervised Machine Learning : Regression and Classification.\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 591, "views": 416, "publish_date": "11/04/2022", "timestamp": 1661040000, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " Welcome back. In this week, we'll learn to make linear regression much faster and much more powerful, and by the end of this week, you'll be two-thirds of the way to finishing this first course. Let's start by looking at a version of linear regression that can look at not just one feature, but a lot of different features. Let's take a look. In the original version of linear regression, you had a single feature x, the size of the house, and you're able to predict y, the price of the house. So the model was FWB of x equals Wx plus b. But now, what if you did not only have the size of the house as a feature with which to try to predict the price, but if you also knew the number of bedrooms, the number of floors, and the age of the home in years, it seems like this would give you a lot more information with which to predict the price. To introduce a little bit of new notation, we're going to use the variables x subscript 1, x subscript 2, x subscript 3, and x subscript 4 to denote the four features. And for simplicity, let's introduce a little bit more notation. We'll write x subscript j, or sometimes I'll just say for short x sub j to represent the list of features. So here, j will go from 1 through 4 because we have four features. I'm going to use lowercase n to denote the total number of features. So in this example, n is equal to 4. As before, we'll use x super stripped i to denote the I've training example. So here, x super strip i is actually going to be a list of four numbers, or sometimes we'll call this a vector that includes all the features of the I've training example. So as a concrete example, x super strip in parentheses 2 will be a vector of the features for the second training example. So it will equal to this 1416, 32, and 40. And technically, I'm writing these numbers in a row, so sometimes this is called a row vector rather than a column vector. But if you don't know what the difference is, don't worry about it. It's not that important for this purpose. And to refer to a specific feature in the I've training example, I will write x super strip i subscript j. So for example, x super strip 2 subscript 3 will be the value of the third feature, that is the number of flaws in the second training example. And so that's going to be equal to two. Sometimes in order to emphasize that this x two is not a number, but it's actually a list of numbers that is a vector, we'll draw an arrow on top of that, just to visually show that is a vector. And over here as well. But you don't have to draw this arrow in your notation. You can think of the arrow as an optional signifier that sometimes use just to emphasize that this is a vector and not a number. Now that we have multiple features, let's take a look at what our model would look like. Previously, this is how we defined the model where x was a single feature. So a single number. But now with multiple features, we're going to define it differently. Instead, the model will be f WB of x equals W1 x1 plus W2 x2 plus W3 x3 plus W4 x4 plus B. Concretely, for housing price prediction, one possible model may be that we estimate the price of the house as 0.1 times x1, the size of the house, plus four times x2, the number of bedrooms, plus 10 times x3, the number of floors, minus two times x4, the age of the house in years, plus 80. Let's think a bit about how you might interpret these parameters. If the model is trying to predict the price of the house in thousands of dollars, you can think of this B equals 80 as saying that the base price of a house starts off at maybe $80,000, assuming it has no size, no bedrooms, no floor and no age. And you can think of this 0.1 as saying that maybe for every additional square foot, the price will increase by $0.1,000 or by $100 because we're saying that for each square foot, the price increases by 0.1, you know, times $1,000, which is $100. And maybe for each additional bathroom, the price increases by $4,000. And for each additional floor, the price may increase by $10,000. And for each additional year of the house's age, the price may decrease by $2,000 because the parameter is negative two. And in general, if you have n features, then the model will look like this. Here again is the definition of the model with n features. What we're going to do next is introduce a little bit of notation to rewrite this expression in a simpler but equivalent way. Let's define w as a list of numbers that lists the parameters w1, w2, w3, all the way through wn. In mathematics, this is called a vector. And sometimes to designate that this is a vector, which just means a list of numbers, I'm going to draw a little arrow on top. You don't always have to draw this arrow. And you can do so or not in your own notation. So you can think of this little arrow as just an optional signifier to remind us that this is a vector. If you've taken a linear algebra class before, you might recognize that this is a row vector as opposed to a column vector. But if you don't know what those terms means, you don't need to worry about it. Next, same as before, b is a single number and not a vector. And so this vector w together with this number b are the parameters of the model. Let me also write x as a list or a vector, again, a row vector that lists all of the features x1, x2, x3 up through xn. This is again a vector. So I'm going to add a little arrow up on top to signify. So in the notation up on top, we can also add little arrows here and here to signify that that w and that x are actually these lists of numbers that they're actually these vectors. So with this notation, the model can now be rewritten more succinctly as f of x equals the vector w dot and this dot refers to a dot product from linear algebra of x, the vector plus the number b. So what is this dot product thing? Well, the dot products of two vectors of two lists of numbers w and x is computed by taking the corresponding pairs of numbers w1 and x1, multiplying that w2 x2, multiplying that w3 x3, multiplying that all the way up to wn xn, multiplying that and then summing up all of these products. Writing that out, this means that the dot product is equal to w1 x1 plus w2 x2 plus w3 x3 plus all the way up to wn xn. And then finally we add back in the b on top. And you notice that this gives us exactly the same expression as we had on top. So the dot product notation lets you write the model in a more compact form with fewer characters. The name for this type of linear regression model with multiple input features is multiple linear regression. This is in contrast to univariate regression, which had just one feature. And by the way, you might think this algorithm is called multivariate regression, but that term actually refers to something else that we won't be using here. So I'm going to refer to this model as multiple linear regression. And so that's it for linear regression with multiple features, which is also called multiple linear regression. In order to implement this, there's a really neat trick called vectorization, which will make it much simpler to implement this and many other learning algorithms. Let's go on to the next video to take a look at what is vectorization.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.2, "text": " Welcome back. In this week, we'll learn to make linear regression much faster and much", "tokens": [50364, 4027, 646, 13, 682, 341, 1243, 11, 321, 603, 1466, 281, 652, 8213, 24590, 709, 4663, 293, 709, 50724, 50724, 544, 4005, 11, 293, 538, 264, 917, 295, 341, 1243, 11, 291, 603, 312, 732, 12, 38507, 295, 264, 636, 281, 12693, 50995, 50995, 341, 700, 1164, 13, 961, 311, 722, 538, 1237, 412, 257, 3037, 295, 8213, 24590, 300, 393, 51235, 51235, 574, 412, 406, 445, 472, 4111, 11, 457, 257, 688, 295, 819, 4122, 13, 961, 311, 747, 257, 574, 13, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.14580021781482916, "compression_ratio": 1.6442307692307692, "no_speech_prob": 0.031112298369407654}, {"id": 1, "seek": 0, "start": 7.2, "end": 12.620000000000001, "text": " more powerful, and by the end of this week, you'll be two-thirds of the way to finishing", "tokens": [50364, 4027, 646, 13, 682, 341, 1243, 11, 321, 603, 1466, 281, 652, 8213, 24590, 709, 4663, 293, 709, 50724, 50724, 544, 4005, 11, 293, 538, 264, 917, 295, 341, 1243, 11, 291, 603, 312, 732, 12, 38507, 295, 264, 636, 281, 12693, 50995, 50995, 341, 700, 1164, 13, 961, 311, 722, 538, 1237, 412, 257, 3037, 295, 8213, 24590, 300, 393, 51235, 51235, 574, 412, 406, 445, 472, 4111, 11, 457, 257, 688, 295, 819, 4122, 13, 961, 311, 747, 257, 574, 13, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.14580021781482916, "compression_ratio": 1.6442307692307692, "no_speech_prob": 0.031112298369407654}, {"id": 2, "seek": 0, "start": 12.620000000000001, "end": 17.42, "text": " this first course. Let's start by looking at a version of linear regression that can", "tokens": [50364, 4027, 646, 13, 682, 341, 1243, 11, 321, 603, 1466, 281, 652, 8213, 24590, 709, 4663, 293, 709, 50724, 50724, 544, 4005, 11, 293, 538, 264, 917, 295, 341, 1243, 11, 291, 603, 312, 732, 12, 38507, 295, 264, 636, 281, 12693, 50995, 50995, 341, 700, 1164, 13, 961, 311, 722, 538, 1237, 412, 257, 3037, 295, 8213, 24590, 300, 393, 51235, 51235, 574, 412, 406, 445, 472, 4111, 11, 457, 257, 688, 295, 819, 4122, 13, 961, 311, 747, 257, 574, 13, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.14580021781482916, "compression_ratio": 1.6442307692307692, "no_speech_prob": 0.031112298369407654}, {"id": 3, "seek": 0, "start": 17.42, "end": 23.36, "text": " look at not just one feature, but a lot of different features. Let's take a look.", "tokens": [50364, 4027, 646, 13, 682, 341, 1243, 11, 321, 603, 1466, 281, 652, 8213, 24590, 709, 4663, 293, 709, 50724, 50724, 544, 4005, 11, 293, 538, 264, 917, 295, 341, 1243, 11, 291, 603, 312, 732, 12, 38507, 295, 264, 636, 281, 12693, 50995, 50995, 341, 700, 1164, 13, 961, 311, 722, 538, 1237, 412, 257, 3037, 295, 8213, 24590, 300, 393, 51235, 51235, 574, 412, 406, 445, 472, 4111, 11, 457, 257, 688, 295, 819, 4122, 13, 961, 311, 747, 257, 574, 13, 51532, 51532], "temperature": 0.0, "avg_logprob": -0.14580021781482916, "compression_ratio": 1.6442307692307692, "no_speech_prob": 0.031112298369407654}, {"id": 4, "seek": 2336, "start": 23.36, "end": 30.64, "text": " In the original version of linear regression, you had a single feature x, the size of the", "tokens": [50364, 682, 264, 3380, 3037, 295, 8213, 24590, 11, 291, 632, 257, 2167, 4111, 2031, 11, 264, 2744, 295, 264, 50728, 50728, 1782, 11, 293, 291, 434, 1075, 281, 6069, 288, 11, 264, 3218, 295, 264, 1782, 13, 407, 264, 2316, 390, 479, 54, 33, 295, 2031, 6915, 51238, 51238, 343, 87, 1804, 272, 13, 583, 586, 11, 437, 498, 291, 630, 406, 787, 362, 264, 2744, 295, 264, 1782, 382, 257, 4111, 365, 51590, 51590], "temperature": 0.0, "avg_logprob": -0.12563710398488231, "compression_ratio": 1.5511363636363635, "no_speech_prob": 6.401116115739569e-05}, {"id": 5, "seek": 2336, "start": 30.64, "end": 40.84, "text": " house, and you're able to predict y, the price of the house. So the model was FWB of x equals", "tokens": [50364, 682, 264, 3380, 3037, 295, 8213, 24590, 11, 291, 632, 257, 2167, 4111, 2031, 11, 264, 2744, 295, 264, 50728, 50728, 1782, 11, 293, 291, 434, 1075, 281, 6069, 288, 11, 264, 3218, 295, 264, 1782, 13, 407, 264, 2316, 390, 479, 54, 33, 295, 2031, 6915, 51238, 51238, 343, 87, 1804, 272, 13, 583, 586, 11, 437, 498, 291, 630, 406, 787, 362, 264, 2744, 295, 264, 1782, 382, 257, 4111, 365, 51590, 51590], "temperature": 0.0, "avg_logprob": -0.12563710398488231, "compression_ratio": 1.5511363636363635, "no_speech_prob": 6.401116115739569e-05}, {"id": 6, "seek": 2336, "start": 40.84, "end": 47.879999999999995, "text": " Wx plus b. But now, what if you did not only have the size of the house as a feature with", "tokens": [50364, 682, 264, 3380, 3037, 295, 8213, 24590, 11, 291, 632, 257, 2167, 4111, 2031, 11, 264, 2744, 295, 264, 50728, 50728, 1782, 11, 293, 291, 434, 1075, 281, 6069, 288, 11, 264, 3218, 295, 264, 1782, 13, 407, 264, 2316, 390, 479, 54, 33, 295, 2031, 6915, 51238, 51238, 343, 87, 1804, 272, 13, 583, 586, 11, 437, 498, 291, 630, 406, 787, 362, 264, 2744, 295, 264, 1782, 382, 257, 4111, 365, 51590, 51590], "temperature": 0.0, "avg_logprob": -0.12563710398488231, "compression_ratio": 1.5511363636363635, "no_speech_prob": 6.401116115739569e-05}, {"id": 7, "seek": 4788, "start": 47.88, "end": 53.6, "text": " which to try to predict the price, but if you also knew the number of bedrooms, the", "tokens": [50364, 597, 281, 853, 281, 6069, 264, 3218, 11, 457, 498, 291, 611, 2586, 264, 1230, 295, 39955, 11, 264, 50650, 50650, 1230, 295, 21008, 11, 293, 264, 3205, 295, 264, 1280, 294, 924, 11, 309, 2544, 411, 341, 576, 976, 291, 50892, 50892, 257, 688, 544, 1589, 365, 597, 281, 6069, 264, 3218, 13, 51095, 51095, 1407, 5366, 257, 707, 857, 295, 777, 24657, 11, 321, 434, 516, 281, 764, 264, 9102, 2031, 2325, 662, 51338, 51338, 502, 11, 2031, 2325, 662, 568, 11, 2031, 2325, 662, 805, 11, 293, 2031, 2325, 662, 1017, 281, 45708, 264, 1451, 4122, 13, 400, 337, 25632, 11, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.0825893967239945, "compression_ratio": 1.75, "no_speech_prob": 3.5908040445065126e-05}, {"id": 8, "seek": 4788, "start": 53.6, "end": 58.440000000000005, "text": " number of floors, and the age of the home in years, it seems like this would give you", "tokens": [50364, 597, 281, 853, 281, 6069, 264, 3218, 11, 457, 498, 291, 611, 2586, 264, 1230, 295, 39955, 11, 264, 50650, 50650, 1230, 295, 21008, 11, 293, 264, 3205, 295, 264, 1280, 294, 924, 11, 309, 2544, 411, 341, 576, 976, 291, 50892, 50892, 257, 688, 544, 1589, 365, 597, 281, 6069, 264, 3218, 13, 51095, 51095, 1407, 5366, 257, 707, 857, 295, 777, 24657, 11, 321, 434, 516, 281, 764, 264, 9102, 2031, 2325, 662, 51338, 51338, 502, 11, 2031, 2325, 662, 568, 11, 2031, 2325, 662, 805, 11, 293, 2031, 2325, 662, 1017, 281, 45708, 264, 1451, 4122, 13, 400, 337, 25632, 11, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.0825893967239945, "compression_ratio": 1.75, "no_speech_prob": 3.5908040445065126e-05}, {"id": 9, "seek": 4788, "start": 58.440000000000005, "end": 62.5, "text": " a lot more information with which to predict the price.", "tokens": [50364, 597, 281, 853, 281, 6069, 264, 3218, 11, 457, 498, 291, 611, 2586, 264, 1230, 295, 39955, 11, 264, 50650, 50650, 1230, 295, 21008, 11, 293, 264, 3205, 295, 264, 1280, 294, 924, 11, 309, 2544, 411, 341, 576, 976, 291, 50892, 50892, 257, 688, 544, 1589, 365, 597, 281, 6069, 264, 3218, 13, 51095, 51095, 1407, 5366, 257, 707, 857, 295, 777, 24657, 11, 321, 434, 516, 281, 764, 264, 9102, 2031, 2325, 662, 51338, 51338, 502, 11, 2031, 2325, 662, 568, 11, 2031, 2325, 662, 805, 11, 293, 2031, 2325, 662, 1017, 281, 45708, 264, 1451, 4122, 13, 400, 337, 25632, 11, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.0825893967239945, "compression_ratio": 1.75, "no_speech_prob": 3.5908040445065126e-05}, {"id": 10, "seek": 4788, "start": 62.5, "end": 67.36, "text": " To introduce a little bit of new notation, we're going to use the variables x subscript", "tokens": [50364, 597, 281, 853, 281, 6069, 264, 3218, 11, 457, 498, 291, 611, 2586, 264, 1230, 295, 39955, 11, 264, 50650, 50650, 1230, 295, 21008, 11, 293, 264, 3205, 295, 264, 1280, 294, 924, 11, 309, 2544, 411, 341, 576, 976, 291, 50892, 50892, 257, 688, 544, 1589, 365, 597, 281, 6069, 264, 3218, 13, 51095, 51095, 1407, 5366, 257, 707, 857, 295, 777, 24657, 11, 321, 434, 516, 281, 764, 264, 9102, 2031, 2325, 662, 51338, 51338, 502, 11, 2031, 2325, 662, 568, 11, 2031, 2325, 662, 805, 11, 293, 2031, 2325, 662, 1017, 281, 45708, 264, 1451, 4122, 13, 400, 337, 25632, 11, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.0825893967239945, "compression_ratio": 1.75, "no_speech_prob": 3.5908040445065126e-05}, {"id": 11, "seek": 4788, "start": 67.36, "end": 76.0, "text": " 1, x subscript 2, x subscript 3, and x subscript 4 to denote the four features. And for simplicity,", "tokens": [50364, 597, 281, 853, 281, 6069, 264, 3218, 11, 457, 498, 291, 611, 2586, 264, 1230, 295, 39955, 11, 264, 50650, 50650, 1230, 295, 21008, 11, 293, 264, 3205, 295, 264, 1280, 294, 924, 11, 309, 2544, 411, 341, 576, 976, 291, 50892, 50892, 257, 688, 544, 1589, 365, 597, 281, 6069, 264, 3218, 13, 51095, 51095, 1407, 5366, 257, 707, 857, 295, 777, 24657, 11, 321, 434, 516, 281, 764, 264, 9102, 2031, 2325, 662, 51338, 51338, 502, 11, 2031, 2325, 662, 568, 11, 2031, 2325, 662, 805, 11, 293, 2031, 2325, 662, 1017, 281, 45708, 264, 1451, 4122, 13, 400, 337, 25632, 11, 51770, 51770], "temperature": 0.0, "avg_logprob": -0.0825893967239945, "compression_ratio": 1.75, "no_speech_prob": 3.5908040445065126e-05}, {"id": 12, "seek": 7600, "start": 76.0, "end": 81.72, "text": " let's introduce a little bit more notation. We'll write x subscript j, or sometimes I'll", "tokens": [50364, 718, 311, 5366, 257, 707, 857, 544, 24657, 13, 492, 603, 2464, 2031, 2325, 662, 361, 11, 420, 2171, 286, 603, 50650, 50650, 445, 584, 337, 2099, 2031, 1422, 361, 281, 2906, 264, 1329, 295, 4122, 13, 407, 510, 11, 361, 486, 352, 490, 51000, 51000, 502, 807, 1017, 570, 321, 362, 1451, 4122, 13, 286, 478, 516, 281, 764, 3126, 9765, 297, 281, 45708, 264, 51354, 51354, 3217, 1230, 295, 4122, 13, 407, 294, 341, 1365, 11, 297, 307, 2681, 281, 1017, 13, 1018, 949, 11, 321, 603, 764, 2031, 1687, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13577932119369507, "compression_ratio": 1.5644444444444445, "no_speech_prob": 5.014659564039903e-06}, {"id": 13, "seek": 7600, "start": 81.72, "end": 88.72, "text": " just say for short x sub j to represent the list of features. So here, j will go from", "tokens": [50364, 718, 311, 5366, 257, 707, 857, 544, 24657, 13, 492, 603, 2464, 2031, 2325, 662, 361, 11, 420, 2171, 286, 603, 50650, 50650, 445, 584, 337, 2099, 2031, 1422, 361, 281, 2906, 264, 1329, 295, 4122, 13, 407, 510, 11, 361, 486, 352, 490, 51000, 51000, 502, 807, 1017, 570, 321, 362, 1451, 4122, 13, 286, 478, 516, 281, 764, 3126, 9765, 297, 281, 45708, 264, 51354, 51354, 3217, 1230, 295, 4122, 13, 407, 294, 341, 1365, 11, 297, 307, 2681, 281, 1017, 13, 1018, 949, 11, 321, 603, 764, 2031, 1687, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13577932119369507, "compression_ratio": 1.5644444444444445, "no_speech_prob": 5.014659564039903e-06}, {"id": 14, "seek": 7600, "start": 88.72, "end": 95.8, "text": " 1 through 4 because we have four features. I'm going to use lowercase n to denote the", "tokens": [50364, 718, 311, 5366, 257, 707, 857, 544, 24657, 13, 492, 603, 2464, 2031, 2325, 662, 361, 11, 420, 2171, 286, 603, 50650, 50650, 445, 584, 337, 2099, 2031, 1422, 361, 281, 2906, 264, 1329, 295, 4122, 13, 407, 510, 11, 361, 486, 352, 490, 51000, 51000, 502, 807, 1017, 570, 321, 362, 1451, 4122, 13, 286, 478, 516, 281, 764, 3126, 9765, 297, 281, 45708, 264, 51354, 51354, 3217, 1230, 295, 4122, 13, 407, 294, 341, 1365, 11, 297, 307, 2681, 281, 1017, 13, 1018, 949, 11, 321, 603, 764, 2031, 1687, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13577932119369507, "compression_ratio": 1.5644444444444445, "no_speech_prob": 5.014659564039903e-06}, {"id": 15, "seek": 7600, "start": 95.8, "end": 102.92, "text": " total number of features. So in this example, n is equal to 4. As before, we'll use x super", "tokens": [50364, 718, 311, 5366, 257, 707, 857, 544, 24657, 13, 492, 603, 2464, 2031, 2325, 662, 361, 11, 420, 2171, 286, 603, 50650, 50650, 445, 584, 337, 2099, 2031, 1422, 361, 281, 2906, 264, 1329, 295, 4122, 13, 407, 510, 11, 361, 486, 352, 490, 51000, 51000, 502, 807, 1017, 570, 321, 362, 1451, 4122, 13, 286, 478, 516, 281, 764, 3126, 9765, 297, 281, 45708, 264, 51354, 51354, 3217, 1230, 295, 4122, 13, 407, 294, 341, 1365, 11, 297, 307, 2681, 281, 1017, 13, 1018, 949, 11, 321, 603, 764, 2031, 1687, 51710, 51710], "temperature": 0.0, "avg_logprob": -0.13577932119369507, "compression_ratio": 1.5644444444444445, "no_speech_prob": 5.014659564039903e-06}, {"id": 16, "seek": 10292, "start": 102.92, "end": 112.12, "text": " stripped i to denote the I've training example. So here, x super strip i is actually going", "tokens": [50364, 33221, 741, 281, 45708, 264, 286, 600, 3097, 1365, 13, 407, 510, 11, 2031, 1687, 12828, 741, 307, 767, 516, 50824, 50824, 281, 312, 257, 1329, 295, 1451, 3547, 11, 420, 2171, 321, 603, 818, 341, 257, 8062, 300, 5974, 439, 51228, 51228, 264, 4122, 295, 264, 286, 600, 3097, 1365, 13, 407, 382, 257, 9859, 1365, 11, 2031, 1687, 12828, 294, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.1563097924897165, "compression_ratio": 1.6375, "no_speech_prob": 1.9333431282575475e-06}, {"id": 17, "seek": 10292, "start": 112.12, "end": 120.2, "text": " to be a list of four numbers, or sometimes we'll call this a vector that includes all", "tokens": [50364, 33221, 741, 281, 45708, 264, 286, 600, 3097, 1365, 13, 407, 510, 11, 2031, 1687, 12828, 741, 307, 767, 516, 50824, 50824, 281, 312, 257, 1329, 295, 1451, 3547, 11, 420, 2171, 321, 603, 818, 341, 257, 8062, 300, 5974, 439, 51228, 51228, 264, 4122, 295, 264, 286, 600, 3097, 1365, 13, 407, 382, 257, 9859, 1365, 11, 2031, 1687, 12828, 294, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.1563097924897165, "compression_ratio": 1.6375, "no_speech_prob": 1.9333431282575475e-06}, {"id": 18, "seek": 10292, "start": 120.2, "end": 128.72, "text": " the features of the I've training example. So as a concrete example, x super strip in", "tokens": [50364, 33221, 741, 281, 45708, 264, 286, 600, 3097, 1365, 13, 407, 510, 11, 2031, 1687, 12828, 741, 307, 767, 516, 50824, 50824, 281, 312, 257, 1329, 295, 1451, 3547, 11, 420, 2171, 321, 603, 818, 341, 257, 8062, 300, 5974, 439, 51228, 51228, 264, 4122, 295, 264, 286, 600, 3097, 1365, 13, 407, 382, 257, 9859, 1365, 11, 2031, 1687, 12828, 294, 51654, 51654], "temperature": 0.0, "avg_logprob": -0.1563097924897165, "compression_ratio": 1.6375, "no_speech_prob": 1.9333431282575475e-06}, {"id": 19, "seek": 12872, "start": 128.72, "end": 135.68, "text": " parentheses 2 will be a vector of the features for the second training example. So it will", "tokens": [50364, 34153, 568, 486, 312, 257, 8062, 295, 264, 4122, 337, 264, 1150, 3097, 1365, 13, 407, 309, 486, 50712, 50712, 2681, 281, 341, 3499, 6866, 11, 8858, 11, 293, 3356, 13, 400, 12120, 11, 286, 478, 3579, 613, 3547, 294, 257, 5386, 11, 370, 2171, 51162, 51162, 341, 307, 1219, 257, 5386, 8062, 2831, 813, 257, 7738, 8062, 13, 583, 498, 291, 500, 380, 458, 437, 51394, 51394, 264, 2649, 307, 11, 500, 380, 3292, 466, 309, 13, 467, 311, 406, 300, 1021, 337, 341, 4334, 13, 400, 281, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.11241691343245967, "compression_ratio": 1.547008547008547, "no_speech_prob": 7.183111392805586e-06}, {"id": 20, "seek": 12872, "start": 135.68, "end": 144.68, "text": " equal to this 1416, 32, and 40. And technically, I'm writing these numbers in a row, so sometimes", "tokens": [50364, 34153, 568, 486, 312, 257, 8062, 295, 264, 4122, 337, 264, 1150, 3097, 1365, 13, 407, 309, 486, 50712, 50712, 2681, 281, 341, 3499, 6866, 11, 8858, 11, 293, 3356, 13, 400, 12120, 11, 286, 478, 3579, 613, 3547, 294, 257, 5386, 11, 370, 2171, 51162, 51162, 341, 307, 1219, 257, 5386, 8062, 2831, 813, 257, 7738, 8062, 13, 583, 498, 291, 500, 380, 458, 437, 51394, 51394, 264, 2649, 307, 11, 500, 380, 3292, 466, 309, 13, 467, 311, 406, 300, 1021, 337, 341, 4334, 13, 400, 281, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.11241691343245967, "compression_ratio": 1.547008547008547, "no_speech_prob": 7.183111392805586e-06}, {"id": 21, "seek": 12872, "start": 144.68, "end": 149.32, "text": " this is called a row vector rather than a column vector. But if you don't know what", "tokens": [50364, 34153, 568, 486, 312, 257, 8062, 295, 264, 4122, 337, 264, 1150, 3097, 1365, 13, 407, 309, 486, 50712, 50712, 2681, 281, 341, 3499, 6866, 11, 8858, 11, 293, 3356, 13, 400, 12120, 11, 286, 478, 3579, 613, 3547, 294, 257, 5386, 11, 370, 2171, 51162, 51162, 341, 307, 1219, 257, 5386, 8062, 2831, 813, 257, 7738, 8062, 13, 583, 498, 291, 500, 380, 458, 437, 51394, 51394, 264, 2649, 307, 11, 500, 380, 3292, 466, 309, 13, 467, 311, 406, 300, 1021, 337, 341, 4334, 13, 400, 281, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.11241691343245967, "compression_ratio": 1.547008547008547, "no_speech_prob": 7.183111392805586e-06}, {"id": 22, "seek": 12872, "start": 149.32, "end": 155.16, "text": " the difference is, don't worry about it. It's not that important for this purpose. And to", "tokens": [50364, 34153, 568, 486, 312, 257, 8062, 295, 264, 4122, 337, 264, 1150, 3097, 1365, 13, 407, 309, 486, 50712, 50712, 2681, 281, 341, 3499, 6866, 11, 8858, 11, 293, 3356, 13, 400, 12120, 11, 286, 478, 3579, 613, 3547, 294, 257, 5386, 11, 370, 2171, 51162, 51162, 341, 307, 1219, 257, 5386, 8062, 2831, 813, 257, 7738, 8062, 13, 583, 498, 291, 500, 380, 458, 437, 51394, 51394, 264, 2649, 307, 11, 500, 380, 3292, 466, 309, 13, 467, 311, 406, 300, 1021, 337, 341, 4334, 13, 400, 281, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.11241691343245967, "compression_ratio": 1.547008547008547, "no_speech_prob": 7.183111392805586e-06}, {"id": 23, "seek": 15516, "start": 155.16, "end": 164.44, "text": " refer to a specific feature in the I've training example, I will write x super strip i subscript", "tokens": [50364, 2864, 281, 257, 2685, 4111, 294, 264, 286, 600, 3097, 1365, 11, 286, 486, 2464, 2031, 1687, 12828, 741, 2325, 662, 50828, 50828, 361, 13, 407, 337, 1365, 11, 2031, 1687, 12828, 568, 2325, 662, 805, 486, 312, 264, 2158, 295, 264, 2636, 4111, 11, 51288, 51288, 300, 307, 264, 1230, 295, 27108, 294, 264, 1150, 3097, 1365, 13, 400, 370, 300, 311, 516, 281, 312, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.11517210006713867, "compression_ratio": 1.6204819277108433, "no_speech_prob": 6.339123956422554e-06}, {"id": 24, "seek": 15516, "start": 164.44, "end": 173.64, "text": " j. So for example, x super strip 2 subscript 3 will be the value of the third feature,", "tokens": [50364, 2864, 281, 257, 2685, 4111, 294, 264, 286, 600, 3097, 1365, 11, 286, 486, 2464, 2031, 1687, 12828, 741, 2325, 662, 50828, 50828, 361, 13, 407, 337, 1365, 11, 2031, 1687, 12828, 568, 2325, 662, 805, 486, 312, 264, 2158, 295, 264, 2636, 4111, 11, 51288, 51288, 300, 307, 264, 1230, 295, 27108, 294, 264, 1150, 3097, 1365, 13, 400, 370, 300, 311, 516, 281, 312, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.11517210006713867, "compression_ratio": 1.6204819277108433, "no_speech_prob": 6.339123956422554e-06}, {"id": 25, "seek": 15516, "start": 173.64, "end": 178.6, "text": " that is the number of flaws in the second training example. And so that's going to be", "tokens": [50364, 2864, 281, 257, 2685, 4111, 294, 264, 286, 600, 3097, 1365, 11, 286, 486, 2464, 2031, 1687, 12828, 741, 2325, 662, 50828, 50828, 361, 13, 407, 337, 1365, 11, 2031, 1687, 12828, 568, 2325, 662, 805, 486, 312, 264, 2158, 295, 264, 2636, 4111, 11, 51288, 51288, 300, 307, 264, 1230, 295, 27108, 294, 264, 1150, 3097, 1365, 13, 400, 370, 300, 311, 516, 281, 312, 51536, 51536], "temperature": 0.0, "avg_logprob": -0.11517210006713867, "compression_ratio": 1.6204819277108433, "no_speech_prob": 6.339123956422554e-06}, {"id": 26, "seek": 17860, "start": 178.6, "end": 186.56, "text": " equal to two. Sometimes in order to emphasize that this x two is not a number, but it's", "tokens": [50364, 2681, 281, 732, 13, 4803, 294, 1668, 281, 16078, 300, 341, 2031, 732, 307, 406, 257, 1230, 11, 457, 309, 311, 50762, 50762, 767, 257, 1329, 295, 3547, 300, 307, 257, 8062, 11, 321, 603, 2642, 364, 11610, 322, 1192, 295, 300, 11, 445, 281, 51062, 51062, 19622, 855, 300, 307, 257, 8062, 13, 400, 670, 510, 382, 731, 13, 583, 291, 500, 380, 362, 281, 2642, 341, 11610, 51448, 51448, 294, 428, 24657, 13, 509, 393, 519, 295, 264, 11610, 382, 364, 17312, 1465, 9902, 300, 2171, 764, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.10932013809040028, "compression_ratio": 1.6199095022624435, "no_speech_prob": 4.425437055033399e-06}, {"id": 27, "seek": 17860, "start": 186.56, "end": 192.56, "text": " actually a list of numbers that is a vector, we'll draw an arrow on top of that, just to", "tokens": [50364, 2681, 281, 732, 13, 4803, 294, 1668, 281, 16078, 300, 341, 2031, 732, 307, 406, 257, 1230, 11, 457, 309, 311, 50762, 50762, 767, 257, 1329, 295, 3547, 300, 307, 257, 8062, 11, 321, 603, 2642, 364, 11610, 322, 1192, 295, 300, 11, 445, 281, 51062, 51062, 19622, 855, 300, 307, 257, 8062, 13, 400, 670, 510, 382, 731, 13, 583, 291, 500, 380, 362, 281, 2642, 341, 11610, 51448, 51448, 294, 428, 24657, 13, 509, 393, 519, 295, 264, 11610, 382, 364, 17312, 1465, 9902, 300, 2171, 764, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.10932013809040028, "compression_ratio": 1.6199095022624435, "no_speech_prob": 4.425437055033399e-06}, {"id": 28, "seek": 17860, "start": 192.56, "end": 200.28, "text": " visually show that is a vector. And over here as well. But you don't have to draw this arrow", "tokens": [50364, 2681, 281, 732, 13, 4803, 294, 1668, 281, 16078, 300, 341, 2031, 732, 307, 406, 257, 1230, 11, 457, 309, 311, 50762, 50762, 767, 257, 1329, 295, 3547, 300, 307, 257, 8062, 11, 321, 603, 2642, 364, 11610, 322, 1192, 295, 300, 11, 445, 281, 51062, 51062, 19622, 855, 300, 307, 257, 8062, 13, 400, 670, 510, 382, 731, 13, 583, 291, 500, 380, 362, 281, 2642, 341, 11610, 51448, 51448, 294, 428, 24657, 13, 509, 393, 519, 295, 264, 11610, 382, 364, 17312, 1465, 9902, 300, 2171, 764, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.10932013809040028, "compression_ratio": 1.6199095022624435, "no_speech_prob": 4.425437055033399e-06}, {"id": 29, "seek": 17860, "start": 200.28, "end": 207.35999999999999, "text": " in your notation. You can think of the arrow as an optional signifier that sometimes use", "tokens": [50364, 2681, 281, 732, 13, 4803, 294, 1668, 281, 16078, 300, 341, 2031, 732, 307, 406, 257, 1230, 11, 457, 309, 311, 50762, 50762, 767, 257, 1329, 295, 3547, 300, 307, 257, 8062, 11, 321, 603, 2642, 364, 11610, 322, 1192, 295, 300, 11, 445, 281, 51062, 51062, 19622, 855, 300, 307, 257, 8062, 13, 400, 670, 510, 382, 731, 13, 583, 291, 500, 380, 362, 281, 2642, 341, 11610, 51448, 51448, 294, 428, 24657, 13, 509, 393, 519, 295, 264, 11610, 382, 364, 17312, 1465, 9902, 300, 2171, 764, 51802, 51802], "temperature": 0.0, "avg_logprob": -0.10932013809040028, "compression_ratio": 1.6199095022624435, "no_speech_prob": 4.425437055033399e-06}, {"id": 30, "seek": 20736, "start": 207.36, "end": 213.24, "text": " just to emphasize that this is a vector and not a number. Now that we have multiple features,", "tokens": [50364, 445, 281, 16078, 300, 341, 307, 257, 8062, 293, 406, 257, 1230, 13, 823, 300, 321, 362, 3866, 4122, 11, 50658, 50658, 718, 311, 747, 257, 574, 412, 437, 527, 2316, 576, 574, 411, 13, 33606, 11, 341, 307, 577, 321, 7642, 50930, 50930, 264, 2316, 689, 2031, 390, 257, 2167, 4111, 13, 407, 257, 2167, 1230, 13, 583, 586, 365, 3866, 4122, 11, 51270, 51270, 321, 434, 516, 281, 6964, 309, 7614, 13, 7156, 11, 264, 2316, 486, 312, 283, 343, 33, 295, 2031, 6915, 343, 16, 2031, 16, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.13850834014567923, "compression_ratio": 1.6261261261261262, "no_speech_prob": 3.1874751584837213e-06}, {"id": 31, "seek": 20736, "start": 213.24, "end": 218.68, "text": " let's take a look at what our model would look like. Previously, this is how we defined", "tokens": [50364, 445, 281, 16078, 300, 341, 307, 257, 8062, 293, 406, 257, 1230, 13, 823, 300, 321, 362, 3866, 4122, 11, 50658, 50658, 718, 311, 747, 257, 574, 412, 437, 527, 2316, 576, 574, 411, 13, 33606, 11, 341, 307, 577, 321, 7642, 50930, 50930, 264, 2316, 689, 2031, 390, 257, 2167, 4111, 13, 407, 257, 2167, 1230, 13, 583, 586, 365, 3866, 4122, 11, 51270, 51270, 321, 434, 516, 281, 6964, 309, 7614, 13, 7156, 11, 264, 2316, 486, 312, 283, 343, 33, 295, 2031, 6915, 343, 16, 2031, 16, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.13850834014567923, "compression_ratio": 1.6261261261261262, "no_speech_prob": 3.1874751584837213e-06}, {"id": 32, "seek": 20736, "start": 218.68, "end": 225.48000000000002, "text": " the model where x was a single feature. So a single number. But now with multiple features,", "tokens": [50364, 445, 281, 16078, 300, 341, 307, 257, 8062, 293, 406, 257, 1230, 13, 823, 300, 321, 362, 3866, 4122, 11, 50658, 50658, 718, 311, 747, 257, 574, 412, 437, 527, 2316, 576, 574, 411, 13, 33606, 11, 341, 307, 577, 321, 7642, 50930, 50930, 264, 2316, 689, 2031, 390, 257, 2167, 4111, 13, 407, 257, 2167, 1230, 13, 583, 586, 365, 3866, 4122, 11, 51270, 51270, 321, 434, 516, 281, 6964, 309, 7614, 13, 7156, 11, 264, 2316, 486, 312, 283, 343, 33, 295, 2031, 6915, 343, 16, 2031, 16, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.13850834014567923, "compression_ratio": 1.6261261261261262, "no_speech_prob": 3.1874751584837213e-06}, {"id": 33, "seek": 20736, "start": 225.48000000000002, "end": 236.64000000000001, "text": " we're going to define it differently. Instead, the model will be f WB of x equals W1 x1", "tokens": [50364, 445, 281, 16078, 300, 341, 307, 257, 8062, 293, 406, 257, 1230, 13, 823, 300, 321, 362, 3866, 4122, 11, 50658, 50658, 718, 311, 747, 257, 574, 412, 437, 527, 2316, 576, 574, 411, 13, 33606, 11, 341, 307, 577, 321, 7642, 50930, 50930, 264, 2316, 689, 2031, 390, 257, 2167, 4111, 13, 407, 257, 2167, 1230, 13, 583, 586, 365, 3866, 4122, 11, 51270, 51270, 321, 434, 516, 281, 6964, 309, 7614, 13, 7156, 11, 264, 2316, 486, 312, 283, 343, 33, 295, 2031, 6915, 343, 16, 2031, 16, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.13850834014567923, "compression_ratio": 1.6261261261261262, "no_speech_prob": 3.1874751584837213e-06}, {"id": 34, "seek": 23664, "start": 236.64, "end": 247.27999999999997, "text": " plus W2 x2 plus W3 x3 plus W4 x4 plus B. Concretely, for housing price prediction, one possible", "tokens": [50364, 1804, 343, 17, 2031, 17, 1804, 343, 18, 2031, 18, 1804, 343, 19, 2031, 19, 1804, 363, 13, 18200, 1505, 736, 11, 337, 6849, 3218, 17630, 11, 472, 1944, 50896, 50896, 2316, 815, 312, 300, 321, 12539, 264, 3218, 295, 264, 1782, 382, 1958, 13, 16, 1413, 2031, 16, 11, 264, 2744, 295, 264, 51248, 51248, 1782, 11, 1804, 1451, 1413, 2031, 17, 11, 264, 1230, 295, 39955, 11, 1804, 1266, 1413, 2031, 18, 11, 264, 1230, 295, 21008, 11, 3175, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.11147476645076976, "compression_ratio": 1.6547619047619047, "no_speech_prob": 1.5206253010546789e-05}, {"id": 35, "seek": 23664, "start": 247.27999999999997, "end": 254.32, "text": " model may be that we estimate the price of the house as 0.1 times x1, the size of the", "tokens": [50364, 1804, 343, 17, 2031, 17, 1804, 343, 18, 2031, 18, 1804, 343, 19, 2031, 19, 1804, 363, 13, 18200, 1505, 736, 11, 337, 6849, 3218, 17630, 11, 472, 1944, 50896, 50896, 2316, 815, 312, 300, 321, 12539, 264, 3218, 295, 264, 1782, 382, 1958, 13, 16, 1413, 2031, 16, 11, 264, 2744, 295, 264, 51248, 51248, 1782, 11, 1804, 1451, 1413, 2031, 17, 11, 264, 1230, 295, 39955, 11, 1804, 1266, 1413, 2031, 18, 11, 264, 1230, 295, 21008, 11, 3175, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.11147476645076976, "compression_ratio": 1.6547619047619047, "no_speech_prob": 1.5206253010546789e-05}, {"id": 36, "seek": 23664, "start": 254.32, "end": 263.15999999999997, "text": " house, plus four times x2, the number of bedrooms, plus 10 times x3, the number of floors, minus", "tokens": [50364, 1804, 343, 17, 2031, 17, 1804, 343, 18, 2031, 18, 1804, 343, 19, 2031, 19, 1804, 363, 13, 18200, 1505, 736, 11, 337, 6849, 3218, 17630, 11, 472, 1944, 50896, 50896, 2316, 815, 312, 300, 321, 12539, 264, 3218, 295, 264, 1782, 382, 1958, 13, 16, 1413, 2031, 16, 11, 264, 2744, 295, 264, 51248, 51248, 1782, 11, 1804, 1451, 1413, 2031, 17, 11, 264, 1230, 295, 39955, 11, 1804, 1266, 1413, 2031, 18, 11, 264, 1230, 295, 21008, 11, 3175, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.11147476645076976, "compression_ratio": 1.6547619047619047, "no_speech_prob": 1.5206253010546789e-05}, {"id": 37, "seek": 26316, "start": 263.16, "end": 269.68, "text": " two times x4, the age of the house in years, plus 80. Let's think a bit about how you might", "tokens": [50364, 732, 1413, 2031, 19, 11, 264, 3205, 295, 264, 1782, 294, 924, 11, 1804, 4688, 13, 961, 311, 519, 257, 857, 466, 577, 291, 1062, 50690, 50690, 7302, 613, 9834, 13, 759, 264, 2316, 307, 1382, 281, 6069, 264, 3218, 295, 264, 1782, 294, 50946, 50946, 5383, 295, 3808, 11, 291, 393, 519, 295, 341, 363, 6915, 4688, 382, 1566, 300, 264, 3096, 3218, 51315, 51315, 295, 257, 1782, 3719, 766, 412, 1310, 1848, 4702, 11, 1360, 11, 11926, 309, 575, 572, 2744, 11, 572, 39955, 11, 572, 4123, 293, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.09799197379578935, "compression_ratio": 1.6108597285067874, "no_speech_prob": 7.182972694863565e-06}, {"id": 38, "seek": 26316, "start": 269.68, "end": 274.8, "text": " interpret these parameters. If the model is trying to predict the price of the house in", "tokens": [50364, 732, 1413, 2031, 19, 11, 264, 3205, 295, 264, 1782, 294, 924, 11, 1804, 4688, 13, 961, 311, 519, 257, 857, 466, 577, 291, 1062, 50690, 50690, 7302, 613, 9834, 13, 759, 264, 2316, 307, 1382, 281, 6069, 264, 3218, 295, 264, 1782, 294, 50946, 50946, 5383, 295, 3808, 11, 291, 393, 519, 295, 341, 363, 6915, 4688, 382, 1566, 300, 264, 3096, 3218, 51315, 51315, 295, 257, 1782, 3719, 766, 412, 1310, 1848, 4702, 11, 1360, 11, 11926, 309, 575, 572, 2744, 11, 572, 39955, 11, 572, 4123, 293, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.09799197379578935, "compression_ratio": 1.6108597285067874, "no_speech_prob": 7.182972694863565e-06}, {"id": 39, "seek": 26316, "start": 274.8, "end": 282.18, "text": " thousands of dollars, you can think of this B equals 80 as saying that the base price", "tokens": [50364, 732, 1413, 2031, 19, 11, 264, 3205, 295, 264, 1782, 294, 924, 11, 1804, 4688, 13, 961, 311, 519, 257, 857, 466, 577, 291, 1062, 50690, 50690, 7302, 613, 9834, 13, 759, 264, 2316, 307, 1382, 281, 6069, 264, 3218, 295, 264, 1782, 294, 50946, 50946, 5383, 295, 3808, 11, 291, 393, 519, 295, 341, 363, 6915, 4688, 382, 1566, 300, 264, 3096, 3218, 51315, 51315, 295, 257, 1782, 3719, 766, 412, 1310, 1848, 4702, 11, 1360, 11, 11926, 309, 575, 572, 2744, 11, 572, 39955, 11, 572, 4123, 293, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.09799197379578935, "compression_ratio": 1.6108597285067874, "no_speech_prob": 7.182972694863565e-06}, {"id": 40, "seek": 26316, "start": 282.18, "end": 288.36, "text": " of a house starts off at maybe $80,000, assuming it has no size, no bedrooms, no floor and", "tokens": [50364, 732, 1413, 2031, 19, 11, 264, 3205, 295, 264, 1782, 294, 924, 11, 1804, 4688, 13, 961, 311, 519, 257, 857, 466, 577, 291, 1062, 50690, 50690, 7302, 613, 9834, 13, 759, 264, 2316, 307, 1382, 281, 6069, 264, 3218, 295, 264, 1782, 294, 50946, 50946, 5383, 295, 3808, 11, 291, 393, 519, 295, 341, 363, 6915, 4688, 382, 1566, 300, 264, 3096, 3218, 51315, 51315, 295, 257, 1782, 3719, 766, 412, 1310, 1848, 4702, 11, 1360, 11, 11926, 309, 575, 572, 2744, 11, 572, 39955, 11, 572, 4123, 293, 51624, 51624], "temperature": 0.0, "avg_logprob": -0.09799197379578935, "compression_ratio": 1.6108597285067874, "no_speech_prob": 7.182972694863565e-06}, {"id": 41, "seek": 28836, "start": 288.36, "end": 296.0, "text": " no age. And you can think of this 0.1 as saying that maybe for every additional square foot,", "tokens": [50364, 572, 3205, 13, 400, 291, 393, 519, 295, 341, 1958, 13, 16, 382, 1566, 300, 1310, 337, 633, 4497, 3732, 2671, 11, 50746, 50746, 264, 3218, 486, 3488, 538, 1848, 15, 13, 16, 11, 1360, 420, 538, 1848, 6879, 570, 321, 434, 1566, 300, 337, 1184, 3732, 51167, 51167, 2671, 11, 264, 3218, 8637, 538, 1958, 13, 16, 11, 291, 458, 11, 1413, 1848, 16, 11, 1360, 11, 597, 307, 1848, 6879, 13, 400, 1310, 337, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.11003599166870118, "compression_ratio": 1.6242424242424243, "no_speech_prob": 8.664384949952364e-06}, {"id": 42, "seek": 28836, "start": 296.0, "end": 304.42, "text": " the price will increase by $0.1,000 or by $100 because we're saying that for each square", "tokens": [50364, 572, 3205, 13, 400, 291, 393, 519, 295, 341, 1958, 13, 16, 382, 1566, 300, 1310, 337, 633, 4497, 3732, 2671, 11, 50746, 50746, 264, 3218, 486, 3488, 538, 1848, 15, 13, 16, 11, 1360, 420, 538, 1848, 6879, 570, 321, 434, 1566, 300, 337, 1184, 3732, 51167, 51167, 2671, 11, 264, 3218, 8637, 538, 1958, 13, 16, 11, 291, 458, 11, 1413, 1848, 16, 11, 1360, 11, 597, 307, 1848, 6879, 13, 400, 1310, 337, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.11003599166870118, "compression_ratio": 1.6242424242424243, "no_speech_prob": 8.664384949952364e-06}, {"id": 43, "seek": 28836, "start": 304.42, "end": 313.24, "text": " foot, the price increases by 0.1, you know, times $1,000, which is $100. And maybe for", "tokens": [50364, 572, 3205, 13, 400, 291, 393, 519, 295, 341, 1958, 13, 16, 382, 1566, 300, 1310, 337, 633, 4497, 3732, 2671, 11, 50746, 50746, 264, 3218, 486, 3488, 538, 1848, 15, 13, 16, 11, 1360, 420, 538, 1848, 6879, 570, 321, 434, 1566, 300, 337, 1184, 3732, 51167, 51167, 2671, 11, 264, 3218, 8637, 538, 1958, 13, 16, 11, 291, 458, 11, 1413, 1848, 16, 11, 1360, 11, 597, 307, 1848, 6879, 13, 400, 1310, 337, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.11003599166870118, "compression_ratio": 1.6242424242424243, "no_speech_prob": 8.664384949952364e-06}, {"id": 44, "seek": 31324, "start": 313.24, "end": 320.24, "text": " each additional bathroom, the price increases by $4,000. And for each additional floor,", "tokens": [50364, 1184, 4497, 8687, 11, 264, 3218, 8637, 538, 1848, 19, 11, 1360, 13, 400, 337, 1184, 4497, 4123, 11, 50714, 50714, 264, 3218, 815, 3488, 538, 1848, 3279, 11, 1360, 13, 400, 337, 1184, 4497, 1064, 295, 264, 1782, 311, 3205, 11, 264, 50998, 50998, 3218, 815, 11514, 538, 1848, 17, 11, 1360, 570, 264, 13075, 307, 3671, 732, 13, 400, 294, 2674, 11, 498, 291, 362, 51428, 51428, 297, 4122, 11, 550, 264, 2316, 486, 574, 411, 341, 13, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.10114284924098424, "compression_ratio": 1.782122905027933, "no_speech_prob": 1.3496896826836746e-06}, {"id": 45, "seek": 31324, "start": 320.24, "end": 325.92, "text": " the price may increase by $10,000. And for each additional year of the house's age, the", "tokens": [50364, 1184, 4497, 8687, 11, 264, 3218, 8637, 538, 1848, 19, 11, 1360, 13, 400, 337, 1184, 4497, 4123, 11, 50714, 50714, 264, 3218, 815, 3488, 538, 1848, 3279, 11, 1360, 13, 400, 337, 1184, 4497, 1064, 295, 264, 1782, 311, 3205, 11, 264, 50998, 50998, 3218, 815, 11514, 538, 1848, 17, 11, 1360, 570, 264, 13075, 307, 3671, 732, 13, 400, 294, 2674, 11, 498, 291, 362, 51428, 51428, 297, 4122, 11, 550, 264, 2316, 486, 574, 411, 341, 13, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.10114284924098424, "compression_ratio": 1.782122905027933, "no_speech_prob": 1.3496896826836746e-06}, {"id": 46, "seek": 31324, "start": 325.92, "end": 334.52, "text": " price may decrease by $2,000 because the parameter is negative two. And in general, if you have", "tokens": [50364, 1184, 4497, 8687, 11, 264, 3218, 8637, 538, 1848, 19, 11, 1360, 13, 400, 337, 1184, 4497, 4123, 11, 50714, 50714, 264, 3218, 815, 3488, 538, 1848, 3279, 11, 1360, 13, 400, 337, 1184, 4497, 1064, 295, 264, 1782, 311, 3205, 11, 264, 50998, 50998, 3218, 815, 11514, 538, 1848, 17, 11, 1360, 570, 264, 13075, 307, 3671, 732, 13, 400, 294, 2674, 11, 498, 291, 362, 51428, 51428, 297, 4122, 11, 550, 264, 2316, 486, 574, 411, 341, 13, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.10114284924098424, "compression_ratio": 1.782122905027933, "no_speech_prob": 1.3496896826836746e-06}, {"id": 47, "seek": 31324, "start": 334.52, "end": 340.74, "text": " n features, then the model will look like this.", "tokens": [50364, 1184, 4497, 8687, 11, 264, 3218, 8637, 538, 1848, 19, 11, 1360, 13, 400, 337, 1184, 4497, 4123, 11, 50714, 50714, 264, 3218, 815, 3488, 538, 1848, 3279, 11, 1360, 13, 400, 337, 1184, 4497, 1064, 295, 264, 1782, 311, 3205, 11, 264, 50998, 50998, 3218, 815, 11514, 538, 1848, 17, 11, 1360, 570, 264, 13075, 307, 3671, 732, 13, 400, 294, 2674, 11, 498, 291, 362, 51428, 51428, 297, 4122, 11, 550, 264, 2316, 486, 574, 411, 341, 13, 51739, 51739], "temperature": 0.0, "avg_logprob": -0.10114284924098424, "compression_ratio": 1.782122905027933, "no_speech_prob": 1.3496896826836746e-06}, {"id": 48, "seek": 34074, "start": 340.74, "end": 346.72, "text": " Here again is the definition of the model with n features. What we're going to do next", "tokens": [50364, 1692, 797, 307, 264, 7123, 295, 264, 2316, 365, 297, 4122, 13, 708, 321, 434, 516, 281, 360, 958, 50663, 50663, 307, 5366, 257, 707, 857, 295, 24657, 281, 28132, 341, 6114, 294, 257, 18587, 457, 10344, 50913, 50913, 636, 13, 961, 311, 6964, 261, 382, 257, 1329, 295, 3547, 300, 14511, 264, 9834, 261, 16, 11, 261, 17, 11, 261, 18, 11, 439, 264, 636, 51335, 51335, 807, 261, 77, 13, 682, 18666, 11, 341, 307, 1219, 257, 8062, 13, 400, 2171, 281, 1715, 473, 300, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.12193828624683423, "compression_ratio": 1.5367965367965368, "no_speech_prob": 8.013281330931932e-06}, {"id": 49, "seek": 34074, "start": 346.72, "end": 351.72, "text": " is introduce a little bit of notation to rewrite this expression in a simpler but equivalent", "tokens": [50364, 1692, 797, 307, 264, 7123, 295, 264, 2316, 365, 297, 4122, 13, 708, 321, 434, 516, 281, 360, 958, 50663, 50663, 307, 5366, 257, 707, 857, 295, 24657, 281, 28132, 341, 6114, 294, 257, 18587, 457, 10344, 50913, 50913, 636, 13, 961, 311, 6964, 261, 382, 257, 1329, 295, 3547, 300, 14511, 264, 9834, 261, 16, 11, 261, 17, 11, 261, 18, 11, 439, 264, 636, 51335, 51335, 807, 261, 77, 13, 682, 18666, 11, 341, 307, 1219, 257, 8062, 13, 400, 2171, 281, 1715, 473, 300, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.12193828624683423, "compression_ratio": 1.5367965367965368, "no_speech_prob": 8.013281330931932e-06}, {"id": 50, "seek": 34074, "start": 351.72, "end": 360.16, "text": " way. Let's define w as a list of numbers that lists the parameters w1, w2, w3, all the way", "tokens": [50364, 1692, 797, 307, 264, 7123, 295, 264, 2316, 365, 297, 4122, 13, 708, 321, 434, 516, 281, 360, 958, 50663, 50663, 307, 5366, 257, 707, 857, 295, 24657, 281, 28132, 341, 6114, 294, 257, 18587, 457, 10344, 50913, 50913, 636, 13, 961, 311, 6964, 261, 382, 257, 1329, 295, 3547, 300, 14511, 264, 9834, 261, 16, 11, 261, 17, 11, 261, 18, 11, 439, 264, 636, 51335, 51335, 807, 261, 77, 13, 682, 18666, 11, 341, 307, 1219, 257, 8062, 13, 400, 2171, 281, 1715, 473, 300, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.12193828624683423, "compression_ratio": 1.5367965367965368, "no_speech_prob": 8.013281330931932e-06}, {"id": 51, "seek": 34074, "start": 360.16, "end": 367.32, "text": " through wn. In mathematics, this is called a vector. And sometimes to designate that", "tokens": [50364, 1692, 797, 307, 264, 7123, 295, 264, 2316, 365, 297, 4122, 13, 708, 321, 434, 516, 281, 360, 958, 50663, 50663, 307, 5366, 257, 707, 857, 295, 24657, 281, 28132, 341, 6114, 294, 257, 18587, 457, 10344, 50913, 50913, 636, 13, 961, 311, 6964, 261, 382, 257, 1329, 295, 3547, 300, 14511, 264, 9834, 261, 16, 11, 261, 17, 11, 261, 18, 11, 439, 264, 636, 51335, 51335, 807, 261, 77, 13, 682, 18666, 11, 341, 307, 1219, 257, 8062, 13, 400, 2171, 281, 1715, 473, 300, 51693, 51693], "temperature": 0.0, "avg_logprob": -0.12193828624683423, "compression_ratio": 1.5367965367965368, "no_speech_prob": 8.013281330931932e-06}, {"id": 52, "seek": 36732, "start": 367.32, "end": 371.88, "text": " this is a vector, which just means a list of numbers, I'm going to draw a little arrow", "tokens": [50364, 341, 307, 257, 8062, 11, 597, 445, 1355, 257, 1329, 295, 3547, 11, 286, 478, 516, 281, 2642, 257, 707, 11610, 50592, 50592, 322, 1192, 13, 509, 500, 380, 1009, 362, 281, 2642, 341, 11610, 13, 400, 291, 393, 360, 370, 420, 406, 294, 428, 1065, 50922, 50922, 24657, 13, 407, 291, 393, 519, 295, 341, 707, 11610, 382, 445, 364, 17312, 1465, 9902, 281, 4160, 51212, 51212, 505, 300, 341, 307, 257, 8062, 13, 759, 291, 600, 2726, 257, 8213, 21989, 1508, 949, 11, 291, 1062, 5521, 51518, 51518, 300, 341, 307, 257, 5386, 8062, 382, 8851, 281, 257, 7738, 8062, 13, 583, 498, 291, 500, 380, 458, 437, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.08594470275075812, "compression_ratio": 1.752, "no_speech_prob": 8.267571502074134e-06}, {"id": 53, "seek": 36732, "start": 371.88, "end": 378.48, "text": " on top. You don't always have to draw this arrow. And you can do so or not in your own", "tokens": [50364, 341, 307, 257, 8062, 11, 597, 445, 1355, 257, 1329, 295, 3547, 11, 286, 478, 516, 281, 2642, 257, 707, 11610, 50592, 50592, 322, 1192, 13, 509, 500, 380, 1009, 362, 281, 2642, 341, 11610, 13, 400, 291, 393, 360, 370, 420, 406, 294, 428, 1065, 50922, 50922, 24657, 13, 407, 291, 393, 519, 295, 341, 707, 11610, 382, 445, 364, 17312, 1465, 9902, 281, 4160, 51212, 51212, 505, 300, 341, 307, 257, 8062, 13, 759, 291, 600, 2726, 257, 8213, 21989, 1508, 949, 11, 291, 1062, 5521, 51518, 51518, 300, 341, 307, 257, 5386, 8062, 382, 8851, 281, 257, 7738, 8062, 13, 583, 498, 291, 500, 380, 458, 437, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.08594470275075812, "compression_ratio": 1.752, "no_speech_prob": 8.267571502074134e-06}, {"id": 54, "seek": 36732, "start": 378.48, "end": 384.28, "text": " notation. So you can think of this little arrow as just an optional signifier to remind", "tokens": [50364, 341, 307, 257, 8062, 11, 597, 445, 1355, 257, 1329, 295, 3547, 11, 286, 478, 516, 281, 2642, 257, 707, 11610, 50592, 50592, 322, 1192, 13, 509, 500, 380, 1009, 362, 281, 2642, 341, 11610, 13, 400, 291, 393, 360, 370, 420, 406, 294, 428, 1065, 50922, 50922, 24657, 13, 407, 291, 393, 519, 295, 341, 707, 11610, 382, 445, 364, 17312, 1465, 9902, 281, 4160, 51212, 51212, 505, 300, 341, 307, 257, 8062, 13, 759, 291, 600, 2726, 257, 8213, 21989, 1508, 949, 11, 291, 1062, 5521, 51518, 51518, 300, 341, 307, 257, 5386, 8062, 382, 8851, 281, 257, 7738, 8062, 13, 583, 498, 291, 500, 380, 458, 437, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.08594470275075812, "compression_ratio": 1.752, "no_speech_prob": 8.267571502074134e-06}, {"id": 55, "seek": 36732, "start": 384.28, "end": 390.4, "text": " us that this is a vector. If you've taken a linear algebra class before, you might recognize", "tokens": [50364, 341, 307, 257, 8062, 11, 597, 445, 1355, 257, 1329, 295, 3547, 11, 286, 478, 516, 281, 2642, 257, 707, 11610, 50592, 50592, 322, 1192, 13, 509, 500, 380, 1009, 362, 281, 2642, 341, 11610, 13, 400, 291, 393, 360, 370, 420, 406, 294, 428, 1065, 50922, 50922, 24657, 13, 407, 291, 393, 519, 295, 341, 707, 11610, 382, 445, 364, 17312, 1465, 9902, 281, 4160, 51212, 51212, 505, 300, 341, 307, 257, 8062, 13, 759, 291, 600, 2726, 257, 8213, 21989, 1508, 949, 11, 291, 1062, 5521, 51518, 51518, 300, 341, 307, 257, 5386, 8062, 382, 8851, 281, 257, 7738, 8062, 13, 583, 498, 291, 500, 380, 458, 437, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.08594470275075812, "compression_ratio": 1.752, "no_speech_prob": 8.267571502074134e-06}, {"id": 56, "seek": 36732, "start": 390.4, "end": 395.52, "text": " that this is a row vector as opposed to a column vector. But if you don't know what", "tokens": [50364, 341, 307, 257, 8062, 11, 597, 445, 1355, 257, 1329, 295, 3547, 11, 286, 478, 516, 281, 2642, 257, 707, 11610, 50592, 50592, 322, 1192, 13, 509, 500, 380, 1009, 362, 281, 2642, 341, 11610, 13, 400, 291, 393, 360, 370, 420, 406, 294, 428, 1065, 50922, 50922, 24657, 13, 407, 291, 393, 519, 295, 341, 707, 11610, 382, 445, 364, 17312, 1465, 9902, 281, 4160, 51212, 51212, 505, 300, 341, 307, 257, 8062, 13, 759, 291, 600, 2726, 257, 8213, 21989, 1508, 949, 11, 291, 1062, 5521, 51518, 51518, 300, 341, 307, 257, 5386, 8062, 382, 8851, 281, 257, 7738, 8062, 13, 583, 498, 291, 500, 380, 458, 437, 51774, 51774], "temperature": 0.0, "avg_logprob": -0.08594470275075812, "compression_ratio": 1.752, "no_speech_prob": 8.267571502074134e-06}, {"id": 57, "seek": 39552, "start": 395.52, "end": 401.2, "text": " those terms means, you don't need to worry about it. Next, same as before, b is a single", "tokens": [50364, 729, 2115, 1355, 11, 291, 500, 380, 643, 281, 3292, 466, 309, 13, 3087, 11, 912, 382, 949, 11, 272, 307, 257, 2167, 50648, 50648, 1230, 293, 406, 257, 8062, 13, 400, 370, 341, 8062, 261, 1214, 365, 341, 1230, 272, 366, 264, 9834, 51084, 51084, 295, 264, 2316, 13, 961, 385, 611, 2464, 2031, 382, 257, 1329, 420, 257, 8062, 11, 797, 11, 257, 5386, 8062, 300, 14511, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.11516235299306372, "compression_ratio": 1.5284090909090908, "no_speech_prob": 3.500827233438031e-06}, {"id": 58, "seek": 39552, "start": 401.2, "end": 409.91999999999996, "text": " number and not a vector. And so this vector w together with this number b are the parameters", "tokens": [50364, 729, 2115, 1355, 11, 291, 500, 380, 643, 281, 3292, 466, 309, 13, 3087, 11, 912, 382, 949, 11, 272, 307, 257, 2167, 50648, 50648, 1230, 293, 406, 257, 8062, 13, 400, 370, 341, 8062, 261, 1214, 365, 341, 1230, 272, 366, 264, 9834, 51084, 51084, 295, 264, 2316, 13, 961, 385, 611, 2464, 2031, 382, 257, 1329, 420, 257, 8062, 11, 797, 11, 257, 5386, 8062, 300, 14511, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.11516235299306372, "compression_ratio": 1.5284090909090908, "no_speech_prob": 3.500827233438031e-06}, {"id": 59, "seek": 39552, "start": 409.91999999999996, "end": 419.91999999999996, "text": " of the model. Let me also write x as a list or a vector, again, a row vector that lists", "tokens": [50364, 729, 2115, 1355, 11, 291, 500, 380, 643, 281, 3292, 466, 309, 13, 3087, 11, 912, 382, 949, 11, 272, 307, 257, 2167, 50648, 50648, 1230, 293, 406, 257, 8062, 13, 400, 370, 341, 8062, 261, 1214, 365, 341, 1230, 272, 366, 264, 9834, 51084, 51084, 295, 264, 2316, 13, 961, 385, 611, 2464, 2031, 382, 257, 1329, 420, 257, 8062, 11, 797, 11, 257, 5386, 8062, 300, 14511, 51584, 51584], "temperature": 0.0, "avg_logprob": -0.11516235299306372, "compression_ratio": 1.5284090909090908, "no_speech_prob": 3.500827233438031e-06}, {"id": 60, "seek": 41992, "start": 419.92, "end": 428.16, "text": " all of the features x1, x2, x3 up through xn. This is again a vector. So I'm going to", "tokens": [50364, 439, 295, 264, 4122, 2031, 16, 11, 2031, 17, 11, 2031, 18, 493, 807, 2031, 77, 13, 639, 307, 797, 257, 8062, 13, 407, 286, 478, 516, 281, 50776, 50776, 909, 257, 707, 11610, 493, 322, 1192, 281, 1465, 2505, 13, 407, 294, 264, 24657, 493, 322, 1192, 11, 321, 393, 611, 909, 51236, 51236, 707, 19669, 510, 293, 510, 281, 1465, 2505, 300, 300, 261, 293, 300, 2031, 366, 767, 613, 14511, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.09852891154103464, "compression_ratio": 1.5696969696969696, "no_speech_prob": 3.3404967325623147e-06}, {"id": 61, "seek": 41992, "start": 428.16, "end": 437.36, "text": " add a little arrow up on top to signify. So in the notation up on top, we can also add", "tokens": [50364, 439, 295, 264, 4122, 2031, 16, 11, 2031, 17, 11, 2031, 18, 493, 807, 2031, 77, 13, 639, 307, 797, 257, 8062, 13, 407, 286, 478, 516, 281, 50776, 50776, 909, 257, 707, 11610, 493, 322, 1192, 281, 1465, 2505, 13, 407, 294, 264, 24657, 493, 322, 1192, 11, 321, 393, 611, 909, 51236, 51236, 707, 19669, 510, 293, 510, 281, 1465, 2505, 300, 300, 261, 293, 300, 2031, 366, 767, 613, 14511, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.09852891154103464, "compression_ratio": 1.5696969696969696, "no_speech_prob": 3.3404967325623147e-06}, {"id": 62, "seek": 41992, "start": 437.36, "end": 445.48, "text": " little arrows here and here to signify that that w and that x are actually these lists", "tokens": [50364, 439, 295, 264, 4122, 2031, 16, 11, 2031, 17, 11, 2031, 18, 493, 807, 2031, 77, 13, 639, 307, 797, 257, 8062, 13, 407, 286, 478, 516, 281, 50776, 50776, 909, 257, 707, 11610, 493, 322, 1192, 281, 1465, 2505, 13, 407, 294, 264, 24657, 493, 322, 1192, 11, 321, 393, 611, 909, 51236, 51236, 707, 19669, 510, 293, 510, 281, 1465, 2505, 300, 300, 261, 293, 300, 2031, 366, 767, 613, 14511, 51642, 51642], "temperature": 0.0, "avg_logprob": -0.09852891154103464, "compression_ratio": 1.5696969696969696, "no_speech_prob": 3.3404967325623147e-06}, {"id": 63, "seek": 44548, "start": 445.48, "end": 453.6, "text": " of numbers that they're actually these vectors. So with this notation, the model can now be", "tokens": [50364, 295, 3547, 300, 436, 434, 767, 613, 18875, 13, 407, 365, 341, 24657, 11, 264, 2316, 393, 586, 312, 50770, 50770, 319, 26859, 544, 21578, 5460, 356, 382, 283, 295, 2031, 6915, 264, 8062, 261, 5893, 293, 341, 5893, 14942, 281, 257, 51286, 51286, 5893, 1674, 490, 8213, 21989, 295, 2031, 11, 264, 8062, 1804, 264, 1230, 272, 13, 407, 437, 307, 341, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.15704496582942223, "compression_ratio": 1.5757575757575757, "no_speech_prob": 3.0415135370276403e-06}, {"id": 64, "seek": 44548, "start": 453.6, "end": 463.92, "text": " rewritten more succinctly as f of x equals the vector w dot and this dot refers to a", "tokens": [50364, 295, 3547, 300, 436, 434, 767, 613, 18875, 13, 407, 365, 341, 24657, 11, 264, 2316, 393, 586, 312, 50770, 50770, 319, 26859, 544, 21578, 5460, 356, 382, 283, 295, 2031, 6915, 264, 8062, 261, 5893, 293, 341, 5893, 14942, 281, 257, 51286, 51286, 5893, 1674, 490, 8213, 21989, 295, 2031, 11, 264, 8062, 1804, 264, 1230, 272, 13, 407, 437, 307, 341, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.15704496582942223, "compression_ratio": 1.5757575757575757, "no_speech_prob": 3.0415135370276403e-06}, {"id": 65, "seek": 44548, "start": 463.92, "end": 472.0, "text": " dot product from linear algebra of x, the vector plus the number b. So what is this", "tokens": [50364, 295, 3547, 300, 436, 434, 767, 613, 18875, 13, 407, 365, 341, 24657, 11, 264, 2316, 393, 586, 312, 50770, 50770, 319, 26859, 544, 21578, 5460, 356, 382, 283, 295, 2031, 6915, 264, 8062, 261, 5893, 293, 341, 5893, 14942, 281, 257, 51286, 51286, 5893, 1674, 490, 8213, 21989, 295, 2031, 11, 264, 8062, 1804, 264, 1230, 272, 13, 407, 437, 307, 341, 51690, 51690], "temperature": 0.0, "avg_logprob": -0.15704496582942223, "compression_ratio": 1.5757575757575757, "no_speech_prob": 3.0415135370276403e-06}, {"id": 66, "seek": 47200, "start": 472.0, "end": 478.6, "text": " dot product thing? Well, the dot products of two vectors of two lists of numbers w and", "tokens": [50364, 5893, 1674, 551, 30, 1042, 11, 264, 5893, 3383, 295, 732, 18875, 295, 732, 14511, 295, 3547, 261, 293, 50694, 50694, 2031, 307, 40610, 538, 1940, 264, 11760, 15494, 295, 3547, 261, 16, 293, 2031, 16, 11, 30955, 300, 51188, 51188, 261, 17, 2031, 17, 11, 30955, 300, 261, 18, 2031, 18, 11, 30955, 300, 439, 264, 636, 493, 281, 261, 77, 2031, 77, 11, 30955, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.1538172994341169, "compression_ratio": 1.7793103448275862, "no_speech_prob": 7.183093657658901e-06}, {"id": 67, "seek": 47200, "start": 478.6, "end": 488.48, "text": " x is computed by taking the corresponding pairs of numbers w1 and x1, multiplying that", "tokens": [50364, 5893, 1674, 551, 30, 1042, 11, 264, 5893, 3383, 295, 732, 18875, 295, 732, 14511, 295, 3547, 261, 293, 50694, 50694, 2031, 307, 40610, 538, 1940, 264, 11760, 15494, 295, 3547, 261, 16, 293, 2031, 16, 11, 30955, 300, 51188, 51188, 261, 17, 2031, 17, 11, 30955, 300, 261, 18, 2031, 18, 11, 30955, 300, 439, 264, 636, 493, 281, 261, 77, 2031, 77, 11, 30955, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.1538172994341169, "compression_ratio": 1.7793103448275862, "no_speech_prob": 7.183093657658901e-06}, {"id": 68, "seek": 47200, "start": 488.48, "end": 499.24, "text": " w2 x2, multiplying that w3 x3, multiplying that all the way up to wn xn, multiplying", "tokens": [50364, 5893, 1674, 551, 30, 1042, 11, 264, 5893, 3383, 295, 732, 18875, 295, 732, 14511, 295, 3547, 261, 293, 50694, 50694, 2031, 307, 40610, 538, 1940, 264, 11760, 15494, 295, 3547, 261, 16, 293, 2031, 16, 11, 30955, 300, 51188, 51188, 261, 17, 2031, 17, 11, 30955, 300, 261, 18, 2031, 18, 11, 30955, 300, 439, 264, 636, 493, 281, 261, 77, 2031, 77, 11, 30955, 51726, 51726], "temperature": 0.0, "avg_logprob": -0.1538172994341169, "compression_ratio": 1.7793103448275862, "no_speech_prob": 7.183093657658901e-06}, {"id": 69, "seek": 49924, "start": 499.24, "end": 507.08, "text": " that and then summing up all of these products. Writing that out, this means that the dot", "tokens": [50364, 300, 293, 550, 2408, 2810, 493, 439, 295, 613, 3383, 13, 32774, 300, 484, 11, 341, 1355, 300, 264, 5893, 50756, 50756, 1674, 307, 2681, 281, 261, 16, 2031, 16, 1804, 261, 17, 2031, 17, 1804, 261, 18, 2031, 18, 1804, 439, 264, 636, 493, 281, 261, 77, 2031, 77, 13, 400, 550, 51506, 51506], "temperature": 0.0, "avg_logprob": -0.10819041938112493, "compression_ratio": 1.408, "no_speech_prob": 1.0952673619613051e-05}, {"id": 70, "seek": 49924, "start": 507.08, "end": 522.08, "text": " product is equal to w1 x1 plus w2 x2 plus w3 x3 plus all the way up to wn xn. And then", "tokens": [50364, 300, 293, 550, 2408, 2810, 493, 439, 295, 613, 3383, 13, 32774, 300, 484, 11, 341, 1355, 300, 264, 5893, 50756, 50756, 1674, 307, 2681, 281, 261, 16, 2031, 16, 1804, 261, 17, 2031, 17, 1804, 261, 18, 2031, 18, 1804, 439, 264, 636, 493, 281, 261, 77, 2031, 77, 13, 400, 550, 51506, 51506], "temperature": 0.0, "avg_logprob": -0.10819041938112493, "compression_ratio": 1.408, "no_speech_prob": 1.0952673619613051e-05}, {"id": 71, "seek": 52208, "start": 522.08, "end": 530.36, "text": " finally we add back in the b on top. And you notice that this gives us exactly the same", "tokens": [50364, 2721, 321, 909, 646, 294, 264, 272, 322, 1192, 13, 400, 291, 3449, 300, 341, 2709, 505, 2293, 264, 912, 50778, 50778, 6114, 382, 321, 632, 322, 1192, 13, 407, 264, 5893, 1674, 24657, 6653, 291, 2464, 264, 2316, 294, 257, 544, 51152, 51152, 14679, 1254, 365, 13366, 4342, 13, 440, 1315, 337, 341, 2010, 295, 8213, 24590, 2316, 365, 51488, 51488, 3866, 4846, 4122, 307, 3866, 8213, 24590, 13, 639, 307, 294, 8712, 281, 517, 592, 3504, 473, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.07462920912777085, "compression_ratio": 1.6775700934579438, "no_speech_prob": 1.1015804375347216e-06}, {"id": 72, "seek": 52208, "start": 530.36, "end": 537.84, "text": " expression as we had on top. So the dot product notation lets you write the model in a more", "tokens": [50364, 2721, 321, 909, 646, 294, 264, 272, 322, 1192, 13, 400, 291, 3449, 300, 341, 2709, 505, 2293, 264, 912, 50778, 50778, 6114, 382, 321, 632, 322, 1192, 13, 407, 264, 5893, 1674, 24657, 6653, 291, 2464, 264, 2316, 294, 257, 544, 51152, 51152, 14679, 1254, 365, 13366, 4342, 13, 440, 1315, 337, 341, 2010, 295, 8213, 24590, 2316, 365, 51488, 51488, 3866, 4846, 4122, 307, 3866, 8213, 24590, 13, 639, 307, 294, 8712, 281, 517, 592, 3504, 473, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.07462920912777085, "compression_ratio": 1.6775700934579438, "no_speech_prob": 1.1015804375347216e-06}, {"id": 73, "seek": 52208, "start": 537.84, "end": 544.5600000000001, "text": " compact form with fewer characters. The name for this type of linear regression model with", "tokens": [50364, 2721, 321, 909, 646, 294, 264, 272, 322, 1192, 13, 400, 291, 3449, 300, 341, 2709, 505, 2293, 264, 912, 50778, 50778, 6114, 382, 321, 632, 322, 1192, 13, 407, 264, 5893, 1674, 24657, 6653, 291, 2464, 264, 2316, 294, 257, 544, 51152, 51152, 14679, 1254, 365, 13366, 4342, 13, 440, 1315, 337, 341, 2010, 295, 8213, 24590, 2316, 365, 51488, 51488, 3866, 4846, 4122, 307, 3866, 8213, 24590, 13, 639, 307, 294, 8712, 281, 517, 592, 3504, 473, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.07462920912777085, "compression_ratio": 1.6775700934579438, "no_speech_prob": 1.1015804375347216e-06}, {"id": 74, "seek": 52208, "start": 544.5600000000001, "end": 550.5600000000001, "text": " multiple input features is multiple linear regression. This is in contrast to univariate", "tokens": [50364, 2721, 321, 909, 646, 294, 264, 272, 322, 1192, 13, 400, 291, 3449, 300, 341, 2709, 505, 2293, 264, 912, 50778, 50778, 6114, 382, 321, 632, 322, 1192, 13, 407, 264, 5893, 1674, 24657, 6653, 291, 2464, 264, 2316, 294, 257, 544, 51152, 51152, 14679, 1254, 365, 13366, 4342, 13, 440, 1315, 337, 341, 2010, 295, 8213, 24590, 2316, 365, 51488, 51488, 3866, 4846, 4122, 307, 3866, 8213, 24590, 13, 639, 307, 294, 8712, 281, 517, 592, 3504, 473, 51788, 51788], "temperature": 0.0, "avg_logprob": -0.07462920912777085, "compression_ratio": 1.6775700934579438, "no_speech_prob": 1.1015804375347216e-06}, {"id": 75, "seek": 55056, "start": 550.56, "end": 556.3599999999999, "text": " regression, which had just one feature. And by the way, you might think this algorithm", "tokens": [50364, 24590, 11, 597, 632, 445, 472, 4111, 13, 400, 538, 264, 636, 11, 291, 1062, 519, 341, 9284, 50654, 50654, 307, 1219, 2120, 592, 3504, 473, 24590, 11, 457, 300, 1433, 767, 14942, 281, 746, 1646, 300, 50920, 50920, 321, 1582, 380, 312, 1228, 510, 13, 407, 286, 478, 516, 281, 2864, 281, 341, 2316, 382, 3866, 8213, 24590, 13, 51240, 51240, 400, 370, 300, 311, 309, 337, 8213, 24590, 365, 3866, 4122, 11, 597, 307, 611, 1219, 3866, 51510, 51510, 8213, 24590, 13, 682, 1668, 281, 4445, 341, 11, 456, 311, 257, 534, 10654, 4282, 1219, 8062, 2144, 11, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.07908080174372746, "compression_ratio": 1.8495934959349594, "no_speech_prob": 9.817924365052022e-06}, {"id": 76, "seek": 55056, "start": 556.3599999999999, "end": 561.68, "text": " is called multivariate regression, but that term actually refers to something else that", "tokens": [50364, 24590, 11, 597, 632, 445, 472, 4111, 13, 400, 538, 264, 636, 11, 291, 1062, 519, 341, 9284, 50654, 50654, 307, 1219, 2120, 592, 3504, 473, 24590, 11, 457, 300, 1433, 767, 14942, 281, 746, 1646, 300, 50920, 50920, 321, 1582, 380, 312, 1228, 510, 13, 407, 286, 478, 516, 281, 2864, 281, 341, 2316, 382, 3866, 8213, 24590, 13, 51240, 51240, 400, 370, 300, 311, 309, 337, 8213, 24590, 365, 3866, 4122, 11, 597, 307, 611, 1219, 3866, 51510, 51510, 8213, 24590, 13, 682, 1668, 281, 4445, 341, 11, 456, 311, 257, 534, 10654, 4282, 1219, 8062, 2144, 11, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.07908080174372746, "compression_ratio": 1.8495934959349594, "no_speech_prob": 9.817924365052022e-06}, {"id": 77, "seek": 55056, "start": 561.68, "end": 568.0799999999999, "text": " we won't be using here. So I'm going to refer to this model as multiple linear regression.", "tokens": [50364, 24590, 11, 597, 632, 445, 472, 4111, 13, 400, 538, 264, 636, 11, 291, 1062, 519, 341, 9284, 50654, 50654, 307, 1219, 2120, 592, 3504, 473, 24590, 11, 457, 300, 1433, 767, 14942, 281, 746, 1646, 300, 50920, 50920, 321, 1582, 380, 312, 1228, 510, 13, 407, 286, 478, 516, 281, 2864, 281, 341, 2316, 382, 3866, 8213, 24590, 13, 51240, 51240, 400, 370, 300, 311, 309, 337, 8213, 24590, 365, 3866, 4122, 11, 597, 307, 611, 1219, 3866, 51510, 51510, 8213, 24590, 13, 682, 1668, 281, 4445, 341, 11, 456, 311, 257, 534, 10654, 4282, 1219, 8062, 2144, 11, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.07908080174372746, "compression_ratio": 1.8495934959349594, "no_speech_prob": 9.817924365052022e-06}, {"id": 78, "seek": 55056, "start": 568.0799999999999, "end": 573.4799999999999, "text": " And so that's it for linear regression with multiple features, which is also called multiple", "tokens": [50364, 24590, 11, 597, 632, 445, 472, 4111, 13, 400, 538, 264, 636, 11, 291, 1062, 519, 341, 9284, 50654, 50654, 307, 1219, 2120, 592, 3504, 473, 24590, 11, 457, 300, 1433, 767, 14942, 281, 746, 1646, 300, 50920, 50920, 321, 1582, 380, 312, 1228, 510, 13, 407, 286, 478, 516, 281, 2864, 281, 341, 2316, 382, 3866, 8213, 24590, 13, 51240, 51240, 400, 370, 300, 311, 309, 337, 8213, 24590, 365, 3866, 4122, 11, 597, 307, 611, 1219, 3866, 51510, 51510, 8213, 24590, 13, 682, 1668, 281, 4445, 341, 11, 456, 311, 257, 534, 10654, 4282, 1219, 8062, 2144, 11, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.07908080174372746, "compression_ratio": 1.8495934959349594, "no_speech_prob": 9.817924365052022e-06}, {"id": 79, "seek": 55056, "start": 573.4799999999999, "end": 580.1199999999999, "text": " linear regression. In order to implement this, there's a really neat trick called vectorization,", "tokens": [50364, 24590, 11, 597, 632, 445, 472, 4111, 13, 400, 538, 264, 636, 11, 291, 1062, 519, 341, 9284, 50654, 50654, 307, 1219, 2120, 592, 3504, 473, 24590, 11, 457, 300, 1433, 767, 14942, 281, 746, 1646, 300, 50920, 50920, 321, 1582, 380, 312, 1228, 510, 13, 407, 286, 478, 516, 281, 2864, 281, 341, 2316, 382, 3866, 8213, 24590, 13, 51240, 51240, 400, 370, 300, 311, 309, 337, 8213, 24590, 365, 3866, 4122, 11, 597, 307, 611, 1219, 3866, 51510, 51510, 8213, 24590, 13, 682, 1668, 281, 4445, 341, 11, 456, 311, 257, 534, 10654, 4282, 1219, 8062, 2144, 11, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.07908080174372746, "compression_ratio": 1.8495934959349594, "no_speech_prob": 9.817924365052022e-06}, {"id": 80, "seek": 58012, "start": 580.12, "end": 585.44, "text": " which will make it much simpler to implement this and many other learning algorithms. Let's", "tokens": [50364, 597, 486, 652, 309, 709, 18587, 281, 4445, 341, 293, 867, 661, 2539, 14642, 13, 961, 311, 50630, 50630, 352, 322, 281, 264, 958, 960, 281, 747, 257, 574, 412, 437, 307, 8062, 2144, 13, 50850], "temperature": 0.0, "avg_logprob": -0.11909157351443642, "compression_ratio": 1.3109243697478992, "no_speech_prob": 2.6607360268826596e-05}, {"id": 81, "seek": 58544, "start": 585.44, "end": 612.44, "text": " go on to the next video to take a look at what is vectorization.", "tokens": [50364, 352, 322, 281, 264, 958, 960, 281, 747, 257, 574, 412, 437, 307, 8062, 2144, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2858840540835732, "compression_ratio": 1.0, "no_speech_prob": 0.0001960923837032169}], "language": "en", "video_id": "fX86EFWljY0", "entity": "ML Specialization, Andrew Ng (2022)"}}