{"video_id": "Izt9Bn8HLUM", "title": "5.9 Multiclass Classification | Improved implementation of softmax --[Machine Learning | Andrew Ng]", "description": "Second Course:\nAdvanced Learning Algorithms.\n\n\nIf you liked the content please subscribe and put a little blue thumb.\nTake heart!", "author": "Machine Learning", "keywords": [], "channel_url": "https://www.youtube.com/channel/UClG5HEAJJFOavT_UolqYROQ", "length": 553, "views": 78, "publish_date": "11/04/2022", "timestamp": 1661472000, "entity": "ML Specialization, Andrew Ng (2022)", "transcript": {"text": " The implementation that you saw in the last video of a neural network with a softmax layer will work okay, but there's an even better way to implement it. Let's take a look at what can go wrong with that implementation and also how to make it better. Let me show you two different ways of computing the same quantity in a computer. Option one, we can set x equals to 2 over 10,000. Option two, we can set x equals 1 plus 1 over 10,000 minus 1 minus 1 over 10,000. I wish you'd first compute this and then compute this and you take the difference. And if you simplify this expression, this turns out to be equal to 2 over 10,000. Let me illustrate this in this notebook. So first, let's set x equals 2 over 10,000 and print the result to a lot of decimal points of accuracy. Okay, that looks pretty good. Second, let me set x equals, I'm going to insist on computing 1 over 1 plus 10,000 and then subtract from that 1 minus 1 over 10,000 and let's print that out. And oh, okay, this looks a little bit off. As if there's some round off error. Because a computer has only a finite amount of memory to store each number, called a floating point number in this case, depending on how you decide to compute the value to over 10,000, the result can have more or less numerical round off error. And it turns out that while the way we have been computing the cost function for softmax is correct, there's a different way of formulating it that reduces these numerical round off errors, leading to more accurate computations within TensorFlow. Let me first explain this a little bit more detail using logistic regression. And then we will show how these ideas apply to improving our implementation of softmax. So first, let me illustrate these ideas using logistic regression. And then we'll move on to show how to improve your implementation of softmax as well. Recall that for logistic regression, if you want to compute the loss function, for a given example, you would first compute this output activation A, which is g of z, or 1 over 1 plus e to the negative z. And then you compute the loss using this expression over here. And in fact, this is what the code would look like for a logistic output layer with this binary cross entropy loss. And for logistic regression, this works okay. And usually the numerical round off errors aren't that bad. But it turns out that if you allow TensorFlow to not have to compute A as an intermediate term, but instead, if you tell TensorFlow that the loss is this expression down here, and all I've done is I've taken A and expanded it into this expression down here, then TensorFlow can rearrange terms in this expression and come up with a more numerically accurate way to compute this loss function. And so whereas the original procedure was like insisting on computing as an intermediate value, 1 plus 1 over 10,000, and another intermediate value, 1 minus 1 over 10,000, and then manipulating these two to get 2 over 10,000, this original implementation was insisting on explicitly computing A as an intermediate quantity. But instead, by specifying this expression at the bottom directly as a loss function, it gives TensorFlow more flexibility in terms of how to compute this and whether or not it wants to compute A explicitly. And so the code you can use to do this is shown here. And what this does is it sets the output layer to just use a linear activation function, and it puts both the activation function, 1 over 1 plus e to the negative z, as well as this cross entropy loss into the specification of the loss function over here. And that's what this from logits equals true argument causes TensorFlow to do. And in case you're wondering what the logits are, it's basically this number z. So TensorFlow will compute z as an intermediate value, but it can rearrange terms to make this become computed more accurately. One downside of this code is it becomes a little bit less legible, but this causes TensorFlow to have a little bit less numerical round off error. Now in the case of logistic regression, either of these implementations actually works okay. But the numerical round off errors can get worse when it comes to softmax. Now let's take this idea and apply it to softmax regression. Recall what you saw in the last video was you compute the activations as follows. The activations is g of z1 through z10, where a1, for example, is e to the z1 divided by the sum of the e to the zj's. And then the loss was this, depending on what is the actual value of y is negative log of aj, for one of the aj's. And so this was the code that we had to do this computation in two separate steps. But once again, if you instead specify that the loss is if y is equal to 1 is negative log of this formula, and so on, if y is equal to 10 is this formula, then this gives TensorFlow the ability to rearrange terms and compute this in a more numerically accurate way. Just to give you some intuition for why TensorFlow might want to do this, it turns out if one of the z's is really small, then e to a negative small number becomes very, very small. Or if one of the z's is a very large number, then e to the z can become a very, very large number. And by rearranging terms, TensorFlow can avoid some of these very small or very large numbers and therefore come up with a more accurate computation for the loss function. So the code for doing this is shown here. In the output layer, we're now just using a linear activation function. So the output layer just computes z1 through z10. And this whole computation of the loss is then captured in the loss function over here, where again, we have the firm log of z equals true parameter. So once again, these two pieces of code do pretty much the same thing, except that the version that is recommended is more numerically accurate, although unfortunately, it is a little bit harder to read as well. So if you're reading someone else's code and you see this and you wonder what's going on, it's actually equivalent to the original implementation and these in concept, except that is more numerically accurate. The numerical round off errors for logistic regression aren't that bad, but it is recommended that you use this implementation down at the bottom instead. And conceptually, this code does the same thing as the first version that you had previously, except that it is a little bit more numerically accurate. Although the downside is, it's maybe just a little bit harder to interpret as well. Now, there's just one more detail, which is that we've now changed the neural network to use a linear activation function rather than a softmax activation function. And so the neural network's final layer no longer outputs these probabilities a1 through a10, it is instead outputting z1 through z10. And I didn't talk about it in the case of logistic regression, but if you were combining the output logistic function with the loss function, then for logistic regression, you also have to change the code this way to take the output value and map it through the logistic function in order to actually get the probability. So you now know how to do multiclass classification with a softmax output layer, and also how to do it in a numerically stable way. Before wrapping up multiclass classification, I want to share with you one other type of classification problem called a multi-label classification problem. Let's talk about that in the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.92, "text": " The implementation that you saw in the last video of a neural network with a softmax layer", "tokens": [50364, 440, 11420, 300, 291, 1866, 294, 264, 1036, 960, 295, 257, 18161, 3209, 365, 257, 2787, 41167, 4583, 50810, 50810, 486, 589, 1392, 11, 457, 456, 311, 364, 754, 1101, 636, 281, 4445, 309, 13, 51048, 51048, 961, 311, 747, 257, 574, 412, 437, 393, 352, 2085, 365, 300, 11420, 293, 611, 577, 281, 652, 309, 51266, 51266, 1101, 13, 51340, 51340, 961, 385, 855, 291, 732, 819, 2098, 295, 15866, 264, 912, 11275, 294, 257, 3820, 13, 51630, 51630, 29284, 472, 11, 321, 393, 992, 2031, 6915, 281, 568, 670, 1266, 11, 1360, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1595798839222301, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.00912117213010788}, {"id": 1, "seek": 0, "start": 8.92, "end": 13.68, "text": " will work okay, but there's an even better way to implement it.", "tokens": [50364, 440, 11420, 300, 291, 1866, 294, 264, 1036, 960, 295, 257, 18161, 3209, 365, 257, 2787, 41167, 4583, 50810, 50810, 486, 589, 1392, 11, 457, 456, 311, 364, 754, 1101, 636, 281, 4445, 309, 13, 51048, 51048, 961, 311, 747, 257, 574, 412, 437, 393, 352, 2085, 365, 300, 11420, 293, 611, 577, 281, 652, 309, 51266, 51266, 1101, 13, 51340, 51340, 961, 385, 855, 291, 732, 819, 2098, 295, 15866, 264, 912, 11275, 294, 257, 3820, 13, 51630, 51630, 29284, 472, 11, 321, 393, 992, 2031, 6915, 281, 568, 670, 1266, 11, 1360, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1595798839222301, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.00912117213010788}, {"id": 2, "seek": 0, "start": 13.68, "end": 18.04, "text": " Let's take a look at what can go wrong with that implementation and also how to make it", "tokens": [50364, 440, 11420, 300, 291, 1866, 294, 264, 1036, 960, 295, 257, 18161, 3209, 365, 257, 2787, 41167, 4583, 50810, 50810, 486, 589, 1392, 11, 457, 456, 311, 364, 754, 1101, 636, 281, 4445, 309, 13, 51048, 51048, 961, 311, 747, 257, 574, 412, 437, 393, 352, 2085, 365, 300, 11420, 293, 611, 577, 281, 652, 309, 51266, 51266, 1101, 13, 51340, 51340, 961, 385, 855, 291, 732, 819, 2098, 295, 15866, 264, 912, 11275, 294, 257, 3820, 13, 51630, 51630, 29284, 472, 11, 321, 393, 992, 2031, 6915, 281, 568, 670, 1266, 11, 1360, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1595798839222301, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.00912117213010788}, {"id": 3, "seek": 0, "start": 18.04, "end": 19.52, "text": " better.", "tokens": [50364, 440, 11420, 300, 291, 1866, 294, 264, 1036, 960, 295, 257, 18161, 3209, 365, 257, 2787, 41167, 4583, 50810, 50810, 486, 589, 1392, 11, 457, 456, 311, 364, 754, 1101, 636, 281, 4445, 309, 13, 51048, 51048, 961, 311, 747, 257, 574, 412, 437, 393, 352, 2085, 365, 300, 11420, 293, 611, 577, 281, 652, 309, 51266, 51266, 1101, 13, 51340, 51340, 961, 385, 855, 291, 732, 819, 2098, 295, 15866, 264, 912, 11275, 294, 257, 3820, 13, 51630, 51630, 29284, 472, 11, 321, 393, 992, 2031, 6915, 281, 568, 670, 1266, 11, 1360, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1595798839222301, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.00912117213010788}, {"id": 4, "seek": 0, "start": 19.52, "end": 25.32, "text": " Let me show you two different ways of computing the same quantity in a computer.", "tokens": [50364, 440, 11420, 300, 291, 1866, 294, 264, 1036, 960, 295, 257, 18161, 3209, 365, 257, 2787, 41167, 4583, 50810, 50810, 486, 589, 1392, 11, 457, 456, 311, 364, 754, 1101, 636, 281, 4445, 309, 13, 51048, 51048, 961, 311, 747, 257, 574, 412, 437, 393, 352, 2085, 365, 300, 11420, 293, 611, 577, 281, 652, 309, 51266, 51266, 1101, 13, 51340, 51340, 961, 385, 855, 291, 732, 819, 2098, 295, 15866, 264, 912, 11275, 294, 257, 3820, 13, 51630, 51630, 29284, 472, 11, 321, 393, 992, 2031, 6915, 281, 568, 670, 1266, 11, 1360, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1595798839222301, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.00912117213010788}, {"id": 5, "seek": 0, "start": 25.32, "end": 29.560000000000002, "text": " Option one, we can set x equals to 2 over 10,000.", "tokens": [50364, 440, 11420, 300, 291, 1866, 294, 264, 1036, 960, 295, 257, 18161, 3209, 365, 257, 2787, 41167, 4583, 50810, 50810, 486, 589, 1392, 11, 457, 456, 311, 364, 754, 1101, 636, 281, 4445, 309, 13, 51048, 51048, 961, 311, 747, 257, 574, 412, 437, 393, 352, 2085, 365, 300, 11420, 293, 611, 577, 281, 652, 309, 51266, 51266, 1101, 13, 51340, 51340, 961, 385, 855, 291, 732, 819, 2098, 295, 15866, 264, 912, 11275, 294, 257, 3820, 13, 51630, 51630, 29284, 472, 11, 321, 393, 992, 2031, 6915, 281, 568, 670, 1266, 11, 1360, 13, 51842, 51842], "temperature": 0.0, "avg_logprob": -0.1595798839222301, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.00912117213010788}, {"id": 6, "seek": 2956, "start": 29.56, "end": 38.6, "text": " Option two, we can set x equals 1 plus 1 over 10,000 minus 1 minus 1 over 10,000.", "tokens": [50364, 29284, 732, 11, 321, 393, 992, 2031, 6915, 502, 1804, 502, 670, 1266, 11, 1360, 3175, 502, 3175, 502, 670, 1266, 11, 1360, 13, 50816, 50816, 286, 3172, 291, 1116, 700, 14722, 341, 293, 550, 14722, 341, 293, 291, 747, 264, 2649, 13, 51114, 51114, 400, 498, 291, 20460, 341, 6114, 11, 341, 4523, 484, 281, 312, 2681, 281, 568, 670, 1266, 11, 1360, 13, 51450, 51450, 961, 385, 23221, 341, 294, 341, 21060, 13, 51558, 51558], "temperature": 0.0, "avg_logprob": -0.16443987737728072, "compression_ratio": 1.64, "no_speech_prob": 1.061592593032401e-05}, {"id": 7, "seek": 2956, "start": 38.6, "end": 44.56, "text": " I wish you'd first compute this and then compute this and you take the difference.", "tokens": [50364, 29284, 732, 11, 321, 393, 992, 2031, 6915, 502, 1804, 502, 670, 1266, 11, 1360, 3175, 502, 3175, 502, 670, 1266, 11, 1360, 13, 50816, 50816, 286, 3172, 291, 1116, 700, 14722, 341, 293, 550, 14722, 341, 293, 291, 747, 264, 2649, 13, 51114, 51114, 400, 498, 291, 20460, 341, 6114, 11, 341, 4523, 484, 281, 312, 2681, 281, 568, 670, 1266, 11, 1360, 13, 51450, 51450, 961, 385, 23221, 341, 294, 341, 21060, 13, 51558, 51558], "temperature": 0.0, "avg_logprob": -0.16443987737728072, "compression_ratio": 1.64, "no_speech_prob": 1.061592593032401e-05}, {"id": 8, "seek": 2956, "start": 44.56, "end": 51.28, "text": " And if you simplify this expression, this turns out to be equal to 2 over 10,000.", "tokens": [50364, 29284, 732, 11, 321, 393, 992, 2031, 6915, 502, 1804, 502, 670, 1266, 11, 1360, 3175, 502, 3175, 502, 670, 1266, 11, 1360, 13, 50816, 50816, 286, 3172, 291, 1116, 700, 14722, 341, 293, 550, 14722, 341, 293, 291, 747, 264, 2649, 13, 51114, 51114, 400, 498, 291, 20460, 341, 6114, 11, 341, 4523, 484, 281, 312, 2681, 281, 568, 670, 1266, 11, 1360, 13, 51450, 51450, 961, 385, 23221, 341, 294, 341, 21060, 13, 51558, 51558], "temperature": 0.0, "avg_logprob": -0.16443987737728072, "compression_ratio": 1.64, "no_speech_prob": 1.061592593032401e-05}, {"id": 9, "seek": 2956, "start": 51.28, "end": 53.44, "text": " Let me illustrate this in this notebook.", "tokens": [50364, 29284, 732, 11, 321, 393, 992, 2031, 6915, 502, 1804, 502, 670, 1266, 11, 1360, 3175, 502, 3175, 502, 670, 1266, 11, 1360, 13, 50816, 50816, 286, 3172, 291, 1116, 700, 14722, 341, 293, 550, 14722, 341, 293, 291, 747, 264, 2649, 13, 51114, 51114, 400, 498, 291, 20460, 341, 6114, 11, 341, 4523, 484, 281, 312, 2681, 281, 568, 670, 1266, 11, 1360, 13, 51450, 51450, 961, 385, 23221, 341, 294, 341, 21060, 13, 51558, 51558], "temperature": 0.0, "avg_logprob": -0.16443987737728072, "compression_ratio": 1.64, "no_speech_prob": 1.061592593032401e-05}, {"id": 10, "seek": 5344, "start": 53.44, "end": 59.72, "text": " So first, let's set x equals 2 over 10,000 and print the result to a lot of decimal", "tokens": [50364, 407, 700, 11, 718, 311, 992, 2031, 6915, 568, 670, 1266, 11, 1360, 293, 4482, 264, 1874, 281, 257, 688, 295, 26601, 50678, 50678, 2793, 295, 14170, 13, 50728, 50728, 1033, 11, 300, 1542, 1238, 665, 13, 50828, 50828, 5736, 11, 718, 385, 992, 2031, 6915, 11, 286, 478, 516, 281, 13466, 322, 15866, 502, 670, 502, 1804, 1266, 11, 1360, 293, 51186, 51186, 550, 16390, 490, 300, 502, 3175, 502, 670, 1266, 11, 1360, 293, 718, 311, 4482, 300, 484, 13, 51432, 51432, 400, 1954, 11, 1392, 11, 341, 1542, 257, 707, 857, 766, 13, 51576, 51576, 1018, 498, 456, 311, 512, 3098, 766, 6713, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.15298173234269424, "compression_ratio": 1.6271929824561404, "no_speech_prob": 4.860370154347038e-06}, {"id": 11, "seek": 5344, "start": 59.72, "end": 60.72, "text": " points of accuracy.", "tokens": [50364, 407, 700, 11, 718, 311, 992, 2031, 6915, 568, 670, 1266, 11, 1360, 293, 4482, 264, 1874, 281, 257, 688, 295, 26601, 50678, 50678, 2793, 295, 14170, 13, 50728, 50728, 1033, 11, 300, 1542, 1238, 665, 13, 50828, 50828, 5736, 11, 718, 385, 992, 2031, 6915, 11, 286, 478, 516, 281, 13466, 322, 15866, 502, 670, 502, 1804, 1266, 11, 1360, 293, 51186, 51186, 550, 16390, 490, 300, 502, 3175, 502, 670, 1266, 11, 1360, 293, 718, 311, 4482, 300, 484, 13, 51432, 51432, 400, 1954, 11, 1392, 11, 341, 1542, 257, 707, 857, 766, 13, 51576, 51576, 1018, 498, 456, 311, 512, 3098, 766, 6713, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.15298173234269424, "compression_ratio": 1.6271929824561404, "no_speech_prob": 4.860370154347038e-06}, {"id": 12, "seek": 5344, "start": 60.72, "end": 62.72, "text": " Okay, that looks pretty good.", "tokens": [50364, 407, 700, 11, 718, 311, 992, 2031, 6915, 568, 670, 1266, 11, 1360, 293, 4482, 264, 1874, 281, 257, 688, 295, 26601, 50678, 50678, 2793, 295, 14170, 13, 50728, 50728, 1033, 11, 300, 1542, 1238, 665, 13, 50828, 50828, 5736, 11, 718, 385, 992, 2031, 6915, 11, 286, 478, 516, 281, 13466, 322, 15866, 502, 670, 502, 1804, 1266, 11, 1360, 293, 51186, 51186, 550, 16390, 490, 300, 502, 3175, 502, 670, 1266, 11, 1360, 293, 718, 311, 4482, 300, 484, 13, 51432, 51432, 400, 1954, 11, 1392, 11, 341, 1542, 257, 707, 857, 766, 13, 51576, 51576, 1018, 498, 456, 311, 512, 3098, 766, 6713, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.15298173234269424, "compression_ratio": 1.6271929824561404, "no_speech_prob": 4.860370154347038e-06}, {"id": 13, "seek": 5344, "start": 62.72, "end": 69.88, "text": " Second, let me set x equals, I'm going to insist on computing 1 over 1 plus 10,000 and", "tokens": [50364, 407, 700, 11, 718, 311, 992, 2031, 6915, 568, 670, 1266, 11, 1360, 293, 4482, 264, 1874, 281, 257, 688, 295, 26601, 50678, 50678, 2793, 295, 14170, 13, 50728, 50728, 1033, 11, 300, 1542, 1238, 665, 13, 50828, 50828, 5736, 11, 718, 385, 992, 2031, 6915, 11, 286, 478, 516, 281, 13466, 322, 15866, 502, 670, 502, 1804, 1266, 11, 1360, 293, 51186, 51186, 550, 16390, 490, 300, 502, 3175, 502, 670, 1266, 11, 1360, 293, 718, 311, 4482, 300, 484, 13, 51432, 51432, 400, 1954, 11, 1392, 11, 341, 1542, 257, 707, 857, 766, 13, 51576, 51576, 1018, 498, 456, 311, 512, 3098, 766, 6713, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.15298173234269424, "compression_ratio": 1.6271929824561404, "no_speech_prob": 4.860370154347038e-06}, {"id": 14, "seek": 5344, "start": 69.88, "end": 74.8, "text": " then subtract from that 1 minus 1 over 10,000 and let's print that out.", "tokens": [50364, 407, 700, 11, 718, 311, 992, 2031, 6915, 568, 670, 1266, 11, 1360, 293, 4482, 264, 1874, 281, 257, 688, 295, 26601, 50678, 50678, 2793, 295, 14170, 13, 50728, 50728, 1033, 11, 300, 1542, 1238, 665, 13, 50828, 50828, 5736, 11, 718, 385, 992, 2031, 6915, 11, 286, 478, 516, 281, 13466, 322, 15866, 502, 670, 502, 1804, 1266, 11, 1360, 293, 51186, 51186, 550, 16390, 490, 300, 502, 3175, 502, 670, 1266, 11, 1360, 293, 718, 311, 4482, 300, 484, 13, 51432, 51432, 400, 1954, 11, 1392, 11, 341, 1542, 257, 707, 857, 766, 13, 51576, 51576, 1018, 498, 456, 311, 512, 3098, 766, 6713, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.15298173234269424, "compression_ratio": 1.6271929824561404, "no_speech_prob": 4.860370154347038e-06}, {"id": 15, "seek": 5344, "start": 74.8, "end": 77.68, "text": " And oh, okay, this looks a little bit off.", "tokens": [50364, 407, 700, 11, 718, 311, 992, 2031, 6915, 568, 670, 1266, 11, 1360, 293, 4482, 264, 1874, 281, 257, 688, 295, 26601, 50678, 50678, 2793, 295, 14170, 13, 50728, 50728, 1033, 11, 300, 1542, 1238, 665, 13, 50828, 50828, 5736, 11, 718, 385, 992, 2031, 6915, 11, 286, 478, 516, 281, 13466, 322, 15866, 502, 670, 502, 1804, 1266, 11, 1360, 293, 51186, 51186, 550, 16390, 490, 300, 502, 3175, 502, 670, 1266, 11, 1360, 293, 718, 311, 4482, 300, 484, 13, 51432, 51432, 400, 1954, 11, 1392, 11, 341, 1542, 257, 707, 857, 766, 13, 51576, 51576, 1018, 498, 456, 311, 512, 3098, 766, 6713, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.15298173234269424, "compression_ratio": 1.6271929824561404, "no_speech_prob": 4.860370154347038e-06}, {"id": 16, "seek": 5344, "start": 77.68, "end": 81.0, "text": " As if there's some round off error.", "tokens": [50364, 407, 700, 11, 718, 311, 992, 2031, 6915, 568, 670, 1266, 11, 1360, 293, 4482, 264, 1874, 281, 257, 688, 295, 26601, 50678, 50678, 2793, 295, 14170, 13, 50728, 50728, 1033, 11, 300, 1542, 1238, 665, 13, 50828, 50828, 5736, 11, 718, 385, 992, 2031, 6915, 11, 286, 478, 516, 281, 13466, 322, 15866, 502, 670, 502, 1804, 1266, 11, 1360, 293, 51186, 51186, 550, 16390, 490, 300, 502, 3175, 502, 670, 1266, 11, 1360, 293, 718, 311, 4482, 300, 484, 13, 51432, 51432, 400, 1954, 11, 1392, 11, 341, 1542, 257, 707, 857, 766, 13, 51576, 51576, 1018, 498, 456, 311, 512, 3098, 766, 6713, 13, 51742, 51742], "temperature": 0.0, "avg_logprob": -0.15298173234269424, "compression_ratio": 1.6271929824561404, "no_speech_prob": 4.860370154347038e-06}, {"id": 17, "seek": 8100, "start": 81.0, "end": 86.76, "text": " Because a computer has only a finite amount of memory to store each number, called a floating", "tokens": [50364, 1436, 257, 3820, 575, 787, 257, 19362, 2372, 295, 4675, 281, 3531, 1184, 1230, 11, 1219, 257, 12607, 50652, 50652, 935, 1230, 294, 341, 1389, 11, 5413, 322, 577, 291, 4536, 281, 14722, 264, 2158, 281, 670, 1266, 11, 1360, 11, 51018, 51018, 264, 1874, 393, 362, 544, 420, 1570, 29054, 3098, 766, 6713, 13, 51294, 51294, 400, 309, 4523, 484, 300, 1339, 264, 636, 321, 362, 668, 15866, 264, 2063, 2445, 337, 2787, 41167, 51621, 51621], "temperature": 0.0, "avg_logprob": -0.10083971144277838, "compression_ratio": 1.568075117370892, "no_speech_prob": 3.905452558683464e-06}, {"id": 18, "seek": 8100, "start": 86.76, "end": 94.08, "text": " point number in this case, depending on how you decide to compute the value to over 10,000,", "tokens": [50364, 1436, 257, 3820, 575, 787, 257, 19362, 2372, 295, 4675, 281, 3531, 1184, 1230, 11, 1219, 257, 12607, 50652, 50652, 935, 1230, 294, 341, 1389, 11, 5413, 322, 577, 291, 4536, 281, 14722, 264, 2158, 281, 670, 1266, 11, 1360, 11, 51018, 51018, 264, 1874, 393, 362, 544, 420, 1570, 29054, 3098, 766, 6713, 13, 51294, 51294, 400, 309, 4523, 484, 300, 1339, 264, 636, 321, 362, 668, 15866, 264, 2063, 2445, 337, 2787, 41167, 51621, 51621], "temperature": 0.0, "avg_logprob": -0.10083971144277838, "compression_ratio": 1.568075117370892, "no_speech_prob": 3.905452558683464e-06}, {"id": 19, "seek": 8100, "start": 94.08, "end": 99.6, "text": " the result can have more or less numerical round off error.", "tokens": [50364, 1436, 257, 3820, 575, 787, 257, 19362, 2372, 295, 4675, 281, 3531, 1184, 1230, 11, 1219, 257, 12607, 50652, 50652, 935, 1230, 294, 341, 1389, 11, 5413, 322, 577, 291, 4536, 281, 14722, 264, 2158, 281, 670, 1266, 11, 1360, 11, 51018, 51018, 264, 1874, 393, 362, 544, 420, 1570, 29054, 3098, 766, 6713, 13, 51294, 51294, 400, 309, 4523, 484, 300, 1339, 264, 636, 321, 362, 668, 15866, 264, 2063, 2445, 337, 2787, 41167, 51621, 51621], "temperature": 0.0, "avg_logprob": -0.10083971144277838, "compression_ratio": 1.568075117370892, "no_speech_prob": 3.905452558683464e-06}, {"id": 20, "seek": 8100, "start": 99.6, "end": 106.14, "text": " And it turns out that while the way we have been computing the cost function for softmax", "tokens": [50364, 1436, 257, 3820, 575, 787, 257, 19362, 2372, 295, 4675, 281, 3531, 1184, 1230, 11, 1219, 257, 12607, 50652, 50652, 935, 1230, 294, 341, 1389, 11, 5413, 322, 577, 291, 4536, 281, 14722, 264, 2158, 281, 670, 1266, 11, 1360, 11, 51018, 51018, 264, 1874, 393, 362, 544, 420, 1570, 29054, 3098, 766, 6713, 13, 51294, 51294, 400, 309, 4523, 484, 300, 1339, 264, 636, 321, 362, 668, 15866, 264, 2063, 2445, 337, 2787, 41167, 51621, 51621], "temperature": 0.0, "avg_logprob": -0.10083971144277838, "compression_ratio": 1.568075117370892, "no_speech_prob": 3.905452558683464e-06}, {"id": 21, "seek": 10614, "start": 106.14, "end": 111.92, "text": " is correct, there's a different way of formulating it that reduces these numerical round off", "tokens": [50364, 307, 3006, 11, 456, 311, 257, 819, 636, 295, 1254, 12162, 309, 300, 18081, 613, 29054, 3098, 766, 50653, 50653, 13603, 11, 5775, 281, 544, 8559, 2807, 763, 1951, 37624, 13, 50921, 50921, 961, 385, 700, 2903, 341, 257, 707, 857, 544, 2607, 1228, 3565, 3142, 24590, 13, 51132, 51132, 400, 550, 321, 486, 855, 577, 613, 3487, 3079, 281, 11470, 527, 11420, 295, 2787, 41167, 13, 51463, 51463, 407, 700, 11, 718, 385, 23221, 613, 3487, 1228, 3565, 3142, 24590, 13, 51683, 51683], "temperature": 0.0, "avg_logprob": -0.0947085646695869, "compression_ratio": 1.6455696202531647, "no_speech_prob": 8.85152019236557e-07}, {"id": 22, "seek": 10614, "start": 111.92, "end": 117.28, "text": " errors, leading to more accurate computations within TensorFlow.", "tokens": [50364, 307, 3006, 11, 456, 311, 257, 819, 636, 295, 1254, 12162, 309, 300, 18081, 613, 29054, 3098, 766, 50653, 50653, 13603, 11, 5775, 281, 544, 8559, 2807, 763, 1951, 37624, 13, 50921, 50921, 961, 385, 700, 2903, 341, 257, 707, 857, 544, 2607, 1228, 3565, 3142, 24590, 13, 51132, 51132, 400, 550, 321, 486, 855, 577, 613, 3487, 3079, 281, 11470, 527, 11420, 295, 2787, 41167, 13, 51463, 51463, 407, 700, 11, 718, 385, 23221, 613, 3487, 1228, 3565, 3142, 24590, 13, 51683, 51683], "temperature": 0.0, "avg_logprob": -0.0947085646695869, "compression_ratio": 1.6455696202531647, "no_speech_prob": 8.85152019236557e-07}, {"id": 23, "seek": 10614, "start": 117.28, "end": 121.5, "text": " Let me first explain this a little bit more detail using logistic regression.", "tokens": [50364, 307, 3006, 11, 456, 311, 257, 819, 636, 295, 1254, 12162, 309, 300, 18081, 613, 29054, 3098, 766, 50653, 50653, 13603, 11, 5775, 281, 544, 8559, 2807, 763, 1951, 37624, 13, 50921, 50921, 961, 385, 700, 2903, 341, 257, 707, 857, 544, 2607, 1228, 3565, 3142, 24590, 13, 51132, 51132, 400, 550, 321, 486, 855, 577, 613, 3487, 3079, 281, 11470, 527, 11420, 295, 2787, 41167, 13, 51463, 51463, 407, 700, 11, 718, 385, 23221, 613, 3487, 1228, 3565, 3142, 24590, 13, 51683, 51683], "temperature": 0.0, "avg_logprob": -0.0947085646695869, "compression_ratio": 1.6455696202531647, "no_speech_prob": 8.85152019236557e-07}, {"id": 24, "seek": 10614, "start": 121.5, "end": 128.12, "text": " And then we will show how these ideas apply to improving our implementation of softmax.", "tokens": [50364, 307, 3006, 11, 456, 311, 257, 819, 636, 295, 1254, 12162, 309, 300, 18081, 613, 29054, 3098, 766, 50653, 50653, 13603, 11, 5775, 281, 544, 8559, 2807, 763, 1951, 37624, 13, 50921, 50921, 961, 385, 700, 2903, 341, 257, 707, 857, 544, 2607, 1228, 3565, 3142, 24590, 13, 51132, 51132, 400, 550, 321, 486, 855, 577, 613, 3487, 3079, 281, 11470, 527, 11420, 295, 2787, 41167, 13, 51463, 51463, 407, 700, 11, 718, 385, 23221, 613, 3487, 1228, 3565, 3142, 24590, 13, 51683, 51683], "temperature": 0.0, "avg_logprob": -0.0947085646695869, "compression_ratio": 1.6455696202531647, "no_speech_prob": 8.85152019236557e-07}, {"id": 25, "seek": 10614, "start": 128.12, "end": 132.52, "text": " So first, let me illustrate these ideas using logistic regression.", "tokens": [50364, 307, 3006, 11, 456, 311, 257, 819, 636, 295, 1254, 12162, 309, 300, 18081, 613, 29054, 3098, 766, 50653, 50653, 13603, 11, 5775, 281, 544, 8559, 2807, 763, 1951, 37624, 13, 50921, 50921, 961, 385, 700, 2903, 341, 257, 707, 857, 544, 2607, 1228, 3565, 3142, 24590, 13, 51132, 51132, 400, 550, 321, 486, 855, 577, 613, 3487, 3079, 281, 11470, 527, 11420, 295, 2787, 41167, 13, 51463, 51463, 407, 700, 11, 718, 385, 23221, 613, 3487, 1228, 3565, 3142, 24590, 13, 51683, 51683], "temperature": 0.0, "avg_logprob": -0.0947085646695869, "compression_ratio": 1.6455696202531647, "no_speech_prob": 8.85152019236557e-07}, {"id": 26, "seek": 13252, "start": 132.52, "end": 138.56, "text": " And then we'll move on to show how to improve your implementation of softmax as well.", "tokens": [50364, 400, 550, 321, 603, 1286, 322, 281, 855, 577, 281, 3470, 428, 11420, 295, 2787, 41167, 382, 731, 13, 50666, 50666, 9647, 336, 300, 337, 3565, 3142, 24590, 11, 498, 291, 528, 281, 14722, 264, 4470, 2445, 11, 337, 257, 2212, 51018, 51018, 1365, 11, 291, 576, 700, 14722, 341, 5598, 24433, 316, 11, 597, 307, 290, 295, 710, 11, 420, 502, 670, 502, 51272, 51272, 1804, 308, 281, 264, 3671, 710, 13, 51380, 51380, 400, 550, 291, 14722, 264, 4470, 1228, 341, 6114, 670, 510, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.16367799109155004, "compression_ratio": 1.6342592592592593, "no_speech_prob": 2.8129722977610072e-06}, {"id": 27, "seek": 13252, "start": 138.56, "end": 145.60000000000002, "text": " Recall that for logistic regression, if you want to compute the loss function, for a given", "tokens": [50364, 400, 550, 321, 603, 1286, 322, 281, 855, 577, 281, 3470, 428, 11420, 295, 2787, 41167, 382, 731, 13, 50666, 50666, 9647, 336, 300, 337, 3565, 3142, 24590, 11, 498, 291, 528, 281, 14722, 264, 4470, 2445, 11, 337, 257, 2212, 51018, 51018, 1365, 11, 291, 576, 700, 14722, 341, 5598, 24433, 316, 11, 597, 307, 290, 295, 710, 11, 420, 502, 670, 502, 51272, 51272, 1804, 308, 281, 264, 3671, 710, 13, 51380, 51380, 400, 550, 291, 14722, 264, 4470, 1228, 341, 6114, 670, 510, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.16367799109155004, "compression_ratio": 1.6342592592592593, "no_speech_prob": 2.8129722977610072e-06}, {"id": 28, "seek": 13252, "start": 145.60000000000002, "end": 150.68, "text": " example, you would first compute this output activation A, which is g of z, or 1 over 1", "tokens": [50364, 400, 550, 321, 603, 1286, 322, 281, 855, 577, 281, 3470, 428, 11420, 295, 2787, 41167, 382, 731, 13, 50666, 50666, 9647, 336, 300, 337, 3565, 3142, 24590, 11, 498, 291, 528, 281, 14722, 264, 4470, 2445, 11, 337, 257, 2212, 51018, 51018, 1365, 11, 291, 576, 700, 14722, 341, 5598, 24433, 316, 11, 597, 307, 290, 295, 710, 11, 420, 502, 670, 502, 51272, 51272, 1804, 308, 281, 264, 3671, 710, 13, 51380, 51380, 400, 550, 291, 14722, 264, 4470, 1228, 341, 6114, 670, 510, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.16367799109155004, "compression_ratio": 1.6342592592592593, "no_speech_prob": 2.8129722977610072e-06}, {"id": 29, "seek": 13252, "start": 150.68, "end": 152.84, "text": " plus e to the negative z.", "tokens": [50364, 400, 550, 321, 603, 1286, 322, 281, 855, 577, 281, 3470, 428, 11420, 295, 2787, 41167, 382, 731, 13, 50666, 50666, 9647, 336, 300, 337, 3565, 3142, 24590, 11, 498, 291, 528, 281, 14722, 264, 4470, 2445, 11, 337, 257, 2212, 51018, 51018, 1365, 11, 291, 576, 700, 14722, 341, 5598, 24433, 316, 11, 597, 307, 290, 295, 710, 11, 420, 502, 670, 502, 51272, 51272, 1804, 308, 281, 264, 3671, 710, 13, 51380, 51380, 400, 550, 291, 14722, 264, 4470, 1228, 341, 6114, 670, 510, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.16367799109155004, "compression_ratio": 1.6342592592592593, "no_speech_prob": 2.8129722977610072e-06}, {"id": 30, "seek": 13252, "start": 152.84, "end": 158.8, "text": " And then you compute the loss using this expression over here.", "tokens": [50364, 400, 550, 321, 603, 1286, 322, 281, 855, 577, 281, 3470, 428, 11420, 295, 2787, 41167, 382, 731, 13, 50666, 50666, 9647, 336, 300, 337, 3565, 3142, 24590, 11, 498, 291, 528, 281, 14722, 264, 4470, 2445, 11, 337, 257, 2212, 51018, 51018, 1365, 11, 291, 576, 700, 14722, 341, 5598, 24433, 316, 11, 597, 307, 290, 295, 710, 11, 420, 502, 670, 502, 51272, 51272, 1804, 308, 281, 264, 3671, 710, 13, 51380, 51380, 400, 550, 291, 14722, 264, 4470, 1228, 341, 6114, 670, 510, 13, 51678, 51678], "temperature": 0.0, "avg_logprob": -0.16367799109155004, "compression_ratio": 1.6342592592592593, "no_speech_prob": 2.8129722977610072e-06}, {"id": 31, "seek": 15880, "start": 158.8, "end": 167.88000000000002, "text": " And in fact, this is what the code would look like for a logistic output layer with this", "tokens": [50364, 400, 294, 1186, 11, 341, 307, 437, 264, 3089, 576, 574, 411, 337, 257, 3565, 3142, 5598, 4583, 365, 341, 50818, 50818, 17434, 3278, 30867, 4470, 13, 50976, 50976, 400, 337, 3565, 3142, 24590, 11, 341, 1985, 1392, 13, 51082, 51082, 400, 2673, 264, 29054, 3098, 766, 13603, 3212, 380, 300, 1578, 13, 51294, 51294, 583, 309, 4523, 484, 300, 498, 291, 2089, 37624, 281, 406, 362, 281, 14722, 316, 382, 364, 19376, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.10836165291922432, "compression_ratio": 1.5097087378640777, "no_speech_prob": 2.1567927888099803e-06}, {"id": 32, "seek": 15880, "start": 167.88000000000002, "end": 171.04000000000002, "text": " binary cross entropy loss.", "tokens": [50364, 400, 294, 1186, 11, 341, 307, 437, 264, 3089, 576, 574, 411, 337, 257, 3565, 3142, 5598, 4583, 365, 341, 50818, 50818, 17434, 3278, 30867, 4470, 13, 50976, 50976, 400, 337, 3565, 3142, 24590, 11, 341, 1985, 1392, 13, 51082, 51082, 400, 2673, 264, 29054, 3098, 766, 13603, 3212, 380, 300, 1578, 13, 51294, 51294, 583, 309, 4523, 484, 300, 498, 291, 2089, 37624, 281, 406, 362, 281, 14722, 316, 382, 364, 19376, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.10836165291922432, "compression_ratio": 1.5097087378640777, "no_speech_prob": 2.1567927888099803e-06}, {"id": 33, "seek": 15880, "start": 171.04000000000002, "end": 173.16000000000003, "text": " And for logistic regression, this works okay.", "tokens": [50364, 400, 294, 1186, 11, 341, 307, 437, 264, 3089, 576, 574, 411, 337, 257, 3565, 3142, 5598, 4583, 365, 341, 50818, 50818, 17434, 3278, 30867, 4470, 13, 50976, 50976, 400, 337, 3565, 3142, 24590, 11, 341, 1985, 1392, 13, 51082, 51082, 400, 2673, 264, 29054, 3098, 766, 13603, 3212, 380, 300, 1578, 13, 51294, 51294, 583, 309, 4523, 484, 300, 498, 291, 2089, 37624, 281, 406, 362, 281, 14722, 316, 382, 364, 19376, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.10836165291922432, "compression_ratio": 1.5097087378640777, "no_speech_prob": 2.1567927888099803e-06}, {"id": 34, "seek": 15880, "start": 173.16000000000003, "end": 177.4, "text": " And usually the numerical round off errors aren't that bad.", "tokens": [50364, 400, 294, 1186, 11, 341, 307, 437, 264, 3089, 576, 574, 411, 337, 257, 3565, 3142, 5598, 4583, 365, 341, 50818, 50818, 17434, 3278, 30867, 4470, 13, 50976, 50976, 400, 337, 3565, 3142, 24590, 11, 341, 1985, 1392, 13, 51082, 51082, 400, 2673, 264, 29054, 3098, 766, 13603, 3212, 380, 300, 1578, 13, 51294, 51294, 583, 309, 4523, 484, 300, 498, 291, 2089, 37624, 281, 406, 362, 281, 14722, 316, 382, 364, 19376, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.10836165291922432, "compression_ratio": 1.5097087378640777, "no_speech_prob": 2.1567927888099803e-06}, {"id": 35, "seek": 15880, "start": 177.4, "end": 184.84, "text": " But it turns out that if you allow TensorFlow to not have to compute A as an intermediate", "tokens": [50364, 400, 294, 1186, 11, 341, 307, 437, 264, 3089, 576, 574, 411, 337, 257, 3565, 3142, 5598, 4583, 365, 341, 50818, 50818, 17434, 3278, 30867, 4470, 13, 50976, 50976, 400, 337, 3565, 3142, 24590, 11, 341, 1985, 1392, 13, 51082, 51082, 400, 2673, 264, 29054, 3098, 766, 13603, 3212, 380, 300, 1578, 13, 51294, 51294, 583, 309, 4523, 484, 300, 498, 291, 2089, 37624, 281, 406, 362, 281, 14722, 316, 382, 364, 19376, 51666, 51666], "temperature": 0.0, "avg_logprob": -0.10836165291922432, "compression_ratio": 1.5097087378640777, "no_speech_prob": 2.1567927888099803e-06}, {"id": 36, "seek": 18484, "start": 184.84, "end": 191.4, "text": " term, but instead, if you tell TensorFlow that the loss is this expression down here,", "tokens": [50364, 1433, 11, 457, 2602, 11, 498, 291, 980, 37624, 300, 264, 4470, 307, 341, 6114, 760, 510, 11, 50692, 50692, 293, 439, 286, 600, 1096, 307, 286, 600, 2726, 316, 293, 14342, 309, 666, 341, 6114, 760, 510, 11, 550, 37624, 51114, 51114, 393, 39568, 2115, 294, 341, 6114, 293, 808, 493, 365, 257, 544, 7866, 984, 8559, 636, 51434, 51434, 281, 14722, 341, 4470, 2445, 13, 51578, 51578], "temperature": 0.0, "avg_logprob": -0.08888646918283381, "compression_ratio": 1.650273224043716, "no_speech_prob": 5.955034794169478e-06}, {"id": 37, "seek": 18484, "start": 191.4, "end": 199.84, "text": " and all I've done is I've taken A and expanded it into this expression down here, then TensorFlow", "tokens": [50364, 1433, 11, 457, 2602, 11, 498, 291, 980, 37624, 300, 264, 4470, 307, 341, 6114, 760, 510, 11, 50692, 50692, 293, 439, 286, 600, 1096, 307, 286, 600, 2726, 316, 293, 14342, 309, 666, 341, 6114, 760, 510, 11, 550, 37624, 51114, 51114, 393, 39568, 2115, 294, 341, 6114, 293, 808, 493, 365, 257, 544, 7866, 984, 8559, 636, 51434, 51434, 281, 14722, 341, 4470, 2445, 13, 51578, 51578], "temperature": 0.0, "avg_logprob": -0.08888646918283381, "compression_ratio": 1.650273224043716, "no_speech_prob": 5.955034794169478e-06}, {"id": 38, "seek": 18484, "start": 199.84, "end": 206.24, "text": " can rearrange terms in this expression and come up with a more numerically accurate way", "tokens": [50364, 1433, 11, 457, 2602, 11, 498, 291, 980, 37624, 300, 264, 4470, 307, 341, 6114, 760, 510, 11, 50692, 50692, 293, 439, 286, 600, 1096, 307, 286, 600, 2726, 316, 293, 14342, 309, 666, 341, 6114, 760, 510, 11, 550, 37624, 51114, 51114, 393, 39568, 2115, 294, 341, 6114, 293, 808, 493, 365, 257, 544, 7866, 984, 8559, 636, 51434, 51434, 281, 14722, 341, 4470, 2445, 13, 51578, 51578], "temperature": 0.0, "avg_logprob": -0.08888646918283381, "compression_ratio": 1.650273224043716, "no_speech_prob": 5.955034794169478e-06}, {"id": 39, "seek": 18484, "start": 206.24, "end": 209.12, "text": " to compute this loss function.", "tokens": [50364, 1433, 11, 457, 2602, 11, 498, 291, 980, 37624, 300, 264, 4470, 307, 341, 6114, 760, 510, 11, 50692, 50692, 293, 439, 286, 600, 1096, 307, 286, 600, 2726, 316, 293, 14342, 309, 666, 341, 6114, 760, 510, 11, 550, 37624, 51114, 51114, 393, 39568, 2115, 294, 341, 6114, 293, 808, 493, 365, 257, 544, 7866, 984, 8559, 636, 51434, 51434, 281, 14722, 341, 4470, 2445, 13, 51578, 51578], "temperature": 0.0, "avg_logprob": -0.08888646918283381, "compression_ratio": 1.650273224043716, "no_speech_prob": 5.955034794169478e-06}, {"id": 40, "seek": 20912, "start": 209.12, "end": 214.84, "text": " And so whereas the original procedure was like insisting on computing as an intermediate", "tokens": [50364, 400, 370, 9735, 264, 3380, 10747, 390, 411, 13466, 278, 322, 15866, 382, 364, 19376, 50650, 50650, 2158, 11, 502, 1804, 502, 670, 1266, 11, 1360, 11, 293, 1071, 19376, 2158, 11, 502, 3175, 502, 670, 1266, 11, 1360, 11, 293, 550, 40805, 51152, 51152, 613, 732, 281, 483, 568, 670, 1266, 11, 1360, 11, 341, 3380, 11420, 390, 13466, 278, 322, 20803, 51442, 51442, 15866, 316, 382, 364, 19376, 11275, 13, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.10821686292949476, "compression_ratio": 1.8728323699421965, "no_speech_prob": 1.2878698498752783e-06}, {"id": 41, "seek": 20912, "start": 214.84, "end": 224.88, "text": " value, 1 plus 1 over 10,000, and another intermediate value, 1 minus 1 over 10,000, and then manipulating", "tokens": [50364, 400, 370, 9735, 264, 3380, 10747, 390, 411, 13466, 278, 322, 15866, 382, 364, 19376, 50650, 50650, 2158, 11, 502, 1804, 502, 670, 1266, 11, 1360, 11, 293, 1071, 19376, 2158, 11, 502, 3175, 502, 670, 1266, 11, 1360, 11, 293, 550, 40805, 51152, 51152, 613, 732, 281, 483, 568, 670, 1266, 11, 1360, 11, 341, 3380, 11420, 390, 13466, 278, 322, 20803, 51442, 51442, 15866, 316, 382, 364, 19376, 11275, 13, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.10821686292949476, "compression_ratio": 1.8728323699421965, "no_speech_prob": 1.2878698498752783e-06}, {"id": 42, "seek": 20912, "start": 224.88, "end": 230.68, "text": " these two to get 2 over 10,000, this original implementation was insisting on explicitly", "tokens": [50364, 400, 370, 9735, 264, 3380, 10747, 390, 411, 13466, 278, 322, 15866, 382, 364, 19376, 50650, 50650, 2158, 11, 502, 1804, 502, 670, 1266, 11, 1360, 11, 293, 1071, 19376, 2158, 11, 502, 3175, 502, 670, 1266, 11, 1360, 11, 293, 550, 40805, 51152, 51152, 613, 732, 281, 483, 568, 670, 1266, 11, 1360, 11, 341, 3380, 11420, 390, 13466, 278, 322, 20803, 51442, 51442, 15866, 316, 382, 364, 19376, 11275, 13, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.10821686292949476, "compression_ratio": 1.8728323699421965, "no_speech_prob": 1.2878698498752783e-06}, {"id": 43, "seek": 20912, "start": 230.68, "end": 235.56, "text": " computing A as an intermediate quantity.", "tokens": [50364, 400, 370, 9735, 264, 3380, 10747, 390, 411, 13466, 278, 322, 15866, 382, 364, 19376, 50650, 50650, 2158, 11, 502, 1804, 502, 670, 1266, 11, 1360, 11, 293, 1071, 19376, 2158, 11, 502, 3175, 502, 670, 1266, 11, 1360, 11, 293, 550, 40805, 51152, 51152, 613, 732, 281, 483, 568, 670, 1266, 11, 1360, 11, 341, 3380, 11420, 390, 13466, 278, 322, 20803, 51442, 51442, 15866, 316, 382, 364, 19376, 11275, 13, 51686, 51686], "temperature": 0.0, "avg_logprob": -0.10821686292949476, "compression_ratio": 1.8728323699421965, "no_speech_prob": 1.2878698498752783e-06}, {"id": 44, "seek": 23556, "start": 235.56, "end": 240.72, "text": " But instead, by specifying this expression at the bottom directly as a loss function,", "tokens": [50364, 583, 2602, 11, 538, 1608, 5489, 341, 6114, 412, 264, 2767, 3838, 382, 257, 4470, 2445, 11, 50622, 50622, 309, 2709, 37624, 544, 12635, 294, 2115, 295, 577, 281, 14722, 341, 293, 1968, 420, 406, 50894, 50894, 309, 2738, 281, 14722, 316, 20803, 13, 51033, 51033, 400, 370, 264, 3089, 291, 393, 764, 281, 360, 341, 307, 4898, 510, 13, 51339, 51339, 400, 437, 341, 775, 307, 309, 6352, 264, 5598, 4583, 281, 445, 764, 257, 8213, 24433, 2445, 11, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.08361763045901344, "compression_ratio": 1.5909090909090908, "no_speech_prob": 3.500759021335398e-06}, {"id": 45, "seek": 23556, "start": 240.72, "end": 246.16, "text": " it gives TensorFlow more flexibility in terms of how to compute this and whether or not", "tokens": [50364, 583, 2602, 11, 538, 1608, 5489, 341, 6114, 412, 264, 2767, 3838, 382, 257, 4470, 2445, 11, 50622, 50622, 309, 2709, 37624, 544, 12635, 294, 2115, 295, 577, 281, 14722, 341, 293, 1968, 420, 406, 50894, 50894, 309, 2738, 281, 14722, 316, 20803, 13, 51033, 51033, 400, 370, 264, 3089, 291, 393, 764, 281, 360, 341, 307, 4898, 510, 13, 51339, 51339, 400, 437, 341, 775, 307, 309, 6352, 264, 5598, 4583, 281, 445, 764, 257, 8213, 24433, 2445, 11, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.08361763045901344, "compression_ratio": 1.5909090909090908, "no_speech_prob": 3.500759021335398e-06}, {"id": 46, "seek": 23556, "start": 246.16, "end": 248.94, "text": " it wants to compute A explicitly.", "tokens": [50364, 583, 2602, 11, 538, 1608, 5489, 341, 6114, 412, 264, 2767, 3838, 382, 257, 4470, 2445, 11, 50622, 50622, 309, 2709, 37624, 544, 12635, 294, 2115, 295, 577, 281, 14722, 341, 293, 1968, 420, 406, 50894, 50894, 309, 2738, 281, 14722, 316, 20803, 13, 51033, 51033, 400, 370, 264, 3089, 291, 393, 764, 281, 360, 341, 307, 4898, 510, 13, 51339, 51339, 400, 437, 341, 775, 307, 309, 6352, 264, 5598, 4583, 281, 445, 764, 257, 8213, 24433, 2445, 11, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.08361763045901344, "compression_ratio": 1.5909090909090908, "no_speech_prob": 3.500759021335398e-06}, {"id": 47, "seek": 23556, "start": 248.94, "end": 255.06, "text": " And so the code you can use to do this is shown here.", "tokens": [50364, 583, 2602, 11, 538, 1608, 5489, 341, 6114, 412, 264, 2767, 3838, 382, 257, 4470, 2445, 11, 50622, 50622, 309, 2709, 37624, 544, 12635, 294, 2115, 295, 577, 281, 14722, 341, 293, 1968, 420, 406, 50894, 50894, 309, 2738, 281, 14722, 316, 20803, 13, 51033, 51033, 400, 370, 264, 3089, 291, 393, 764, 281, 360, 341, 307, 4898, 510, 13, 51339, 51339, 400, 437, 341, 775, 307, 309, 6352, 264, 5598, 4583, 281, 445, 764, 257, 8213, 24433, 2445, 11, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.08361763045901344, "compression_ratio": 1.5909090909090908, "no_speech_prob": 3.500759021335398e-06}, {"id": 48, "seek": 23556, "start": 255.06, "end": 261.88, "text": " And what this does is it sets the output layer to just use a linear activation function,", "tokens": [50364, 583, 2602, 11, 538, 1608, 5489, 341, 6114, 412, 264, 2767, 3838, 382, 257, 4470, 2445, 11, 50622, 50622, 309, 2709, 37624, 544, 12635, 294, 2115, 295, 577, 281, 14722, 341, 293, 1968, 420, 406, 50894, 50894, 309, 2738, 281, 14722, 316, 20803, 13, 51033, 51033, 400, 370, 264, 3089, 291, 393, 764, 281, 360, 341, 307, 4898, 510, 13, 51339, 51339, 400, 437, 341, 775, 307, 309, 6352, 264, 5598, 4583, 281, 445, 764, 257, 8213, 24433, 2445, 11, 51680, 51680], "temperature": 0.0, "avg_logprob": -0.08361763045901344, "compression_ratio": 1.5909090909090908, "no_speech_prob": 3.500759021335398e-06}, {"id": 49, "seek": 26188, "start": 261.88, "end": 267.71999999999997, "text": " and it puts both the activation function, 1 over 1 plus e to the negative z, as well", "tokens": [50364, 293, 309, 8137, 1293, 264, 24433, 2445, 11, 502, 670, 502, 1804, 308, 281, 264, 3671, 710, 11, 382, 731, 50656, 50656, 382, 341, 3278, 30867, 4470, 666, 264, 31256, 295, 264, 4470, 2445, 670, 510, 13, 51040, 51040, 400, 300, 311, 437, 341, 490, 3565, 1208, 6915, 2074, 6770, 7700, 37624, 281, 360, 13, 51382, 51382, 400, 294, 1389, 291, 434, 6359, 437, 264, 3565, 1208, 366, 11, 309, 311, 1936, 341, 1230, 710, 13, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.08321576714515685, "compression_ratio": 1.570048309178744, "no_speech_prob": 5.594160029431805e-06}, {"id": 50, "seek": 26188, "start": 267.71999999999997, "end": 275.4, "text": " as this cross entropy loss into the specification of the loss function over here.", "tokens": [50364, 293, 309, 8137, 1293, 264, 24433, 2445, 11, 502, 670, 502, 1804, 308, 281, 264, 3671, 710, 11, 382, 731, 50656, 50656, 382, 341, 3278, 30867, 4470, 666, 264, 31256, 295, 264, 4470, 2445, 670, 510, 13, 51040, 51040, 400, 300, 311, 437, 341, 490, 3565, 1208, 6915, 2074, 6770, 7700, 37624, 281, 360, 13, 51382, 51382, 400, 294, 1389, 291, 434, 6359, 437, 264, 3565, 1208, 366, 11, 309, 311, 1936, 341, 1230, 710, 13, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.08321576714515685, "compression_ratio": 1.570048309178744, "no_speech_prob": 5.594160029431805e-06}, {"id": 51, "seek": 26188, "start": 275.4, "end": 282.24, "text": " And that's what this from logits equals true argument causes TensorFlow to do.", "tokens": [50364, 293, 309, 8137, 1293, 264, 24433, 2445, 11, 502, 670, 502, 1804, 308, 281, 264, 3671, 710, 11, 382, 731, 50656, 50656, 382, 341, 3278, 30867, 4470, 666, 264, 31256, 295, 264, 4470, 2445, 670, 510, 13, 51040, 51040, 400, 300, 311, 437, 341, 490, 3565, 1208, 6915, 2074, 6770, 7700, 37624, 281, 360, 13, 51382, 51382, 400, 294, 1389, 291, 434, 6359, 437, 264, 3565, 1208, 366, 11, 309, 311, 1936, 341, 1230, 710, 13, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.08321576714515685, "compression_ratio": 1.570048309178744, "no_speech_prob": 5.594160029431805e-06}, {"id": 52, "seek": 26188, "start": 282.24, "end": 286.6, "text": " And in case you're wondering what the logits are, it's basically this number z.", "tokens": [50364, 293, 309, 8137, 1293, 264, 24433, 2445, 11, 502, 670, 502, 1804, 308, 281, 264, 3671, 710, 11, 382, 731, 50656, 50656, 382, 341, 3278, 30867, 4470, 666, 264, 31256, 295, 264, 4470, 2445, 670, 510, 13, 51040, 51040, 400, 300, 311, 437, 341, 490, 3565, 1208, 6915, 2074, 6770, 7700, 37624, 281, 360, 13, 51382, 51382, 400, 294, 1389, 291, 434, 6359, 437, 264, 3565, 1208, 366, 11, 309, 311, 1936, 341, 1230, 710, 13, 51600, 51600], "temperature": 0.0, "avg_logprob": -0.08321576714515685, "compression_ratio": 1.570048309178744, "no_speech_prob": 5.594160029431805e-06}, {"id": 53, "seek": 28660, "start": 286.6, "end": 292.92, "text": " So TensorFlow will compute z as an intermediate value, but it can rearrange terms to make", "tokens": [50364, 407, 37624, 486, 14722, 710, 382, 364, 19376, 2158, 11, 457, 309, 393, 39568, 2115, 281, 652, 50680, 50680, 341, 1813, 40610, 544, 20095, 13, 50856, 50856, 1485, 25060, 295, 341, 3089, 307, 309, 3643, 257, 707, 857, 1570, 1676, 964, 11, 457, 341, 7700, 37624, 51172, 51172, 281, 362, 257, 707, 857, 1570, 29054, 3098, 766, 6713, 13, 51312, 51312, 823, 294, 264, 1389, 295, 3565, 3142, 24590, 11, 2139, 295, 613, 4445, 763, 767, 1985, 1392, 13, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.12559536853468561, "compression_ratio": 1.6311111111111112, "no_speech_prob": 6.083535595280409e-07}, {"id": 54, "seek": 28660, "start": 292.92, "end": 296.44, "text": " this become computed more accurately.", "tokens": [50364, 407, 37624, 486, 14722, 710, 382, 364, 19376, 2158, 11, 457, 309, 393, 39568, 2115, 281, 652, 50680, 50680, 341, 1813, 40610, 544, 20095, 13, 50856, 50856, 1485, 25060, 295, 341, 3089, 307, 309, 3643, 257, 707, 857, 1570, 1676, 964, 11, 457, 341, 7700, 37624, 51172, 51172, 281, 362, 257, 707, 857, 1570, 29054, 3098, 766, 6713, 13, 51312, 51312, 823, 294, 264, 1389, 295, 3565, 3142, 24590, 11, 2139, 295, 613, 4445, 763, 767, 1985, 1392, 13, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.12559536853468561, "compression_ratio": 1.6311111111111112, "no_speech_prob": 6.083535595280409e-07}, {"id": 55, "seek": 28660, "start": 296.44, "end": 302.76000000000005, "text": " One downside of this code is it becomes a little bit less legible, but this causes TensorFlow", "tokens": [50364, 407, 37624, 486, 14722, 710, 382, 364, 19376, 2158, 11, 457, 309, 393, 39568, 2115, 281, 652, 50680, 50680, 341, 1813, 40610, 544, 20095, 13, 50856, 50856, 1485, 25060, 295, 341, 3089, 307, 309, 3643, 257, 707, 857, 1570, 1676, 964, 11, 457, 341, 7700, 37624, 51172, 51172, 281, 362, 257, 707, 857, 1570, 29054, 3098, 766, 6713, 13, 51312, 51312, 823, 294, 264, 1389, 295, 3565, 3142, 24590, 11, 2139, 295, 613, 4445, 763, 767, 1985, 1392, 13, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.12559536853468561, "compression_ratio": 1.6311111111111112, "no_speech_prob": 6.083535595280409e-07}, {"id": 56, "seek": 28660, "start": 302.76000000000005, "end": 305.56, "text": " to have a little bit less numerical round off error.", "tokens": [50364, 407, 37624, 486, 14722, 710, 382, 364, 19376, 2158, 11, 457, 309, 393, 39568, 2115, 281, 652, 50680, 50680, 341, 1813, 40610, 544, 20095, 13, 50856, 50856, 1485, 25060, 295, 341, 3089, 307, 309, 3643, 257, 707, 857, 1570, 1676, 964, 11, 457, 341, 7700, 37624, 51172, 51172, 281, 362, 257, 707, 857, 1570, 29054, 3098, 766, 6713, 13, 51312, 51312, 823, 294, 264, 1389, 295, 3565, 3142, 24590, 11, 2139, 295, 613, 4445, 763, 767, 1985, 1392, 13, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.12559536853468561, "compression_ratio": 1.6311111111111112, "no_speech_prob": 6.083535595280409e-07}, {"id": 57, "seek": 28660, "start": 305.56, "end": 311.48, "text": " Now in the case of logistic regression, either of these implementations actually works okay.", "tokens": [50364, 407, 37624, 486, 14722, 710, 382, 364, 19376, 2158, 11, 457, 309, 393, 39568, 2115, 281, 652, 50680, 50680, 341, 1813, 40610, 544, 20095, 13, 50856, 50856, 1485, 25060, 295, 341, 3089, 307, 309, 3643, 257, 707, 857, 1570, 1676, 964, 11, 457, 341, 7700, 37624, 51172, 51172, 281, 362, 257, 707, 857, 1570, 29054, 3098, 766, 6713, 13, 51312, 51312, 823, 294, 264, 1389, 295, 3565, 3142, 24590, 11, 2139, 295, 613, 4445, 763, 767, 1985, 1392, 13, 51608, 51608], "temperature": 0.0, "avg_logprob": -0.12559536853468561, "compression_ratio": 1.6311111111111112, "no_speech_prob": 6.083535595280409e-07}, {"id": 58, "seek": 31148, "start": 311.48, "end": 316.8, "text": " But the numerical round off errors can get worse when it comes to softmax.", "tokens": [50364, 583, 264, 29054, 3098, 766, 13603, 393, 483, 5324, 562, 309, 1487, 281, 2787, 41167, 13, 50630, 50630, 823, 718, 311, 747, 341, 1558, 293, 3079, 309, 281, 2787, 41167, 24590, 13, 50832, 50832, 9647, 336, 437, 291, 1866, 294, 264, 1036, 960, 390, 291, 14722, 264, 2430, 763, 382, 10002, 13, 51126, 51126, 440, 2430, 763, 307, 290, 295, 710, 16, 807, 710, 3279, 11, 689, 257, 16, 11, 337, 1365, 11, 307, 308, 281, 264, 710, 16, 6666, 538, 51552, 51552, 264, 2408, 295, 264, 308, 281, 264, 710, 73, 311, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.11494772774832589, "compression_ratio": 1.5774647887323943, "no_speech_prob": 9.368595783598721e-06}, {"id": 59, "seek": 31148, "start": 316.8, "end": 320.84000000000003, "text": " Now let's take this idea and apply it to softmax regression.", "tokens": [50364, 583, 264, 29054, 3098, 766, 13603, 393, 483, 5324, 562, 309, 1487, 281, 2787, 41167, 13, 50630, 50630, 823, 718, 311, 747, 341, 1558, 293, 3079, 309, 281, 2787, 41167, 24590, 13, 50832, 50832, 9647, 336, 437, 291, 1866, 294, 264, 1036, 960, 390, 291, 14722, 264, 2430, 763, 382, 10002, 13, 51126, 51126, 440, 2430, 763, 307, 290, 295, 710, 16, 807, 710, 3279, 11, 689, 257, 16, 11, 337, 1365, 11, 307, 308, 281, 264, 710, 16, 6666, 538, 51552, 51552, 264, 2408, 295, 264, 308, 281, 264, 710, 73, 311, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.11494772774832589, "compression_ratio": 1.5774647887323943, "no_speech_prob": 9.368595783598721e-06}, {"id": 60, "seek": 31148, "start": 320.84000000000003, "end": 326.72, "text": " Recall what you saw in the last video was you compute the activations as follows.", "tokens": [50364, 583, 264, 29054, 3098, 766, 13603, 393, 483, 5324, 562, 309, 1487, 281, 2787, 41167, 13, 50630, 50630, 823, 718, 311, 747, 341, 1558, 293, 3079, 309, 281, 2787, 41167, 24590, 13, 50832, 50832, 9647, 336, 437, 291, 1866, 294, 264, 1036, 960, 390, 291, 14722, 264, 2430, 763, 382, 10002, 13, 51126, 51126, 440, 2430, 763, 307, 290, 295, 710, 16, 807, 710, 3279, 11, 689, 257, 16, 11, 337, 1365, 11, 307, 308, 281, 264, 710, 16, 6666, 538, 51552, 51552, 264, 2408, 295, 264, 308, 281, 264, 710, 73, 311, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.11494772774832589, "compression_ratio": 1.5774647887323943, "no_speech_prob": 9.368595783598721e-06}, {"id": 61, "seek": 31148, "start": 326.72, "end": 335.24, "text": " The activations is g of z1 through z10, where a1, for example, is e to the z1 divided by", "tokens": [50364, 583, 264, 29054, 3098, 766, 13603, 393, 483, 5324, 562, 309, 1487, 281, 2787, 41167, 13, 50630, 50630, 823, 718, 311, 747, 341, 1558, 293, 3079, 309, 281, 2787, 41167, 24590, 13, 50832, 50832, 9647, 336, 437, 291, 1866, 294, 264, 1036, 960, 390, 291, 14722, 264, 2430, 763, 382, 10002, 13, 51126, 51126, 440, 2430, 763, 307, 290, 295, 710, 16, 807, 710, 3279, 11, 689, 257, 16, 11, 337, 1365, 11, 307, 308, 281, 264, 710, 16, 6666, 538, 51552, 51552, 264, 2408, 295, 264, 308, 281, 264, 710, 73, 311, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.11494772774832589, "compression_ratio": 1.5774647887323943, "no_speech_prob": 9.368595783598721e-06}, {"id": 62, "seek": 31148, "start": 335.24, "end": 338.68, "text": " the sum of the e to the zj's.", "tokens": [50364, 583, 264, 29054, 3098, 766, 13603, 393, 483, 5324, 562, 309, 1487, 281, 2787, 41167, 13, 50630, 50630, 823, 718, 311, 747, 341, 1558, 293, 3079, 309, 281, 2787, 41167, 24590, 13, 50832, 50832, 9647, 336, 437, 291, 1866, 294, 264, 1036, 960, 390, 291, 14722, 264, 2430, 763, 382, 10002, 13, 51126, 51126, 440, 2430, 763, 307, 290, 295, 710, 16, 807, 710, 3279, 11, 689, 257, 16, 11, 337, 1365, 11, 307, 308, 281, 264, 710, 16, 6666, 538, 51552, 51552, 264, 2408, 295, 264, 308, 281, 264, 710, 73, 311, 13, 51724, 51724], "temperature": 0.0, "avg_logprob": -0.11494772774832589, "compression_ratio": 1.5774647887323943, "no_speech_prob": 9.368595783598721e-06}, {"id": 63, "seek": 33868, "start": 338.68, "end": 343.64, "text": " And then the loss was this, depending on what is the actual value of y is negative log of", "tokens": [50364, 400, 550, 264, 4470, 390, 341, 11, 5413, 322, 437, 307, 264, 3539, 2158, 295, 288, 307, 3671, 3565, 295, 50612, 50612, 17680, 11, 337, 472, 295, 264, 17680, 311, 13, 50748, 50748, 400, 370, 341, 390, 264, 3089, 300, 321, 632, 281, 360, 341, 24903, 294, 732, 4994, 4439, 13, 51106, 51106, 583, 1564, 797, 11, 498, 291, 2602, 16500, 300, 264, 4470, 307, 498, 288, 307, 2681, 281, 502, 307, 3671, 51566, 51566], "temperature": 0.0, "avg_logprob": -0.1290072403944932, "compression_ratio": 1.5777777777777777, "no_speech_prob": 1.3006943845539354e-05}, {"id": 64, "seek": 33868, "start": 343.64, "end": 346.36, "text": " aj, for one of the aj's.", "tokens": [50364, 400, 550, 264, 4470, 390, 341, 11, 5413, 322, 437, 307, 264, 3539, 2158, 295, 288, 307, 3671, 3565, 295, 50612, 50612, 17680, 11, 337, 472, 295, 264, 17680, 311, 13, 50748, 50748, 400, 370, 341, 390, 264, 3089, 300, 321, 632, 281, 360, 341, 24903, 294, 732, 4994, 4439, 13, 51106, 51106, 583, 1564, 797, 11, 498, 291, 2602, 16500, 300, 264, 4470, 307, 498, 288, 307, 2681, 281, 502, 307, 3671, 51566, 51566], "temperature": 0.0, "avg_logprob": -0.1290072403944932, "compression_ratio": 1.5777777777777777, "no_speech_prob": 1.3006943845539354e-05}, {"id": 65, "seek": 33868, "start": 346.36, "end": 353.52, "text": " And so this was the code that we had to do this computation in two separate steps.", "tokens": [50364, 400, 550, 264, 4470, 390, 341, 11, 5413, 322, 437, 307, 264, 3539, 2158, 295, 288, 307, 3671, 3565, 295, 50612, 50612, 17680, 11, 337, 472, 295, 264, 17680, 311, 13, 50748, 50748, 400, 370, 341, 390, 264, 3089, 300, 321, 632, 281, 360, 341, 24903, 294, 732, 4994, 4439, 13, 51106, 51106, 583, 1564, 797, 11, 498, 291, 2602, 16500, 300, 264, 4470, 307, 498, 288, 307, 2681, 281, 502, 307, 3671, 51566, 51566], "temperature": 0.0, "avg_logprob": -0.1290072403944932, "compression_ratio": 1.5777777777777777, "no_speech_prob": 1.3006943845539354e-05}, {"id": 66, "seek": 33868, "start": 353.52, "end": 362.72, "text": " But once again, if you instead specify that the loss is if y is equal to 1 is negative", "tokens": [50364, 400, 550, 264, 4470, 390, 341, 11, 5413, 322, 437, 307, 264, 3539, 2158, 295, 288, 307, 3671, 3565, 295, 50612, 50612, 17680, 11, 337, 472, 295, 264, 17680, 311, 13, 50748, 50748, 400, 370, 341, 390, 264, 3089, 300, 321, 632, 281, 360, 341, 24903, 294, 732, 4994, 4439, 13, 51106, 51106, 583, 1564, 797, 11, 498, 291, 2602, 16500, 300, 264, 4470, 307, 498, 288, 307, 2681, 281, 502, 307, 3671, 51566, 51566], "temperature": 0.0, "avg_logprob": -0.1290072403944932, "compression_ratio": 1.5777777777777777, "no_speech_prob": 1.3006943845539354e-05}, {"id": 67, "seek": 36272, "start": 362.72, "end": 374.24, "text": " log of this formula, and so on, if y is equal to 10 is this formula, then this gives TensorFlow", "tokens": [50364, 3565, 295, 341, 8513, 11, 293, 370, 322, 11, 498, 288, 307, 2681, 281, 1266, 307, 341, 8513, 11, 550, 341, 2709, 37624, 50940, 50940, 264, 3485, 281, 39568, 2115, 293, 14722, 341, 294, 257, 544, 7866, 984, 8559, 636, 13, 51284, 51284, 1449, 281, 976, 291, 512, 24002, 337, 983, 37624, 1062, 528, 281, 360, 341, 11, 309, 4523, 484, 498, 472, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.08384607799017607, "compression_ratio": 1.56, "no_speech_prob": 1.147857528849272e-05}, {"id": 68, "seek": 36272, "start": 374.24, "end": 381.12, "text": " the ability to rearrange terms and compute this in a more numerically accurate way.", "tokens": [50364, 3565, 295, 341, 8513, 11, 293, 370, 322, 11, 498, 288, 307, 2681, 281, 1266, 307, 341, 8513, 11, 550, 341, 2709, 37624, 50940, 50940, 264, 3485, 281, 39568, 2115, 293, 14722, 341, 294, 257, 544, 7866, 984, 8559, 636, 13, 51284, 51284, 1449, 281, 976, 291, 512, 24002, 337, 983, 37624, 1062, 528, 281, 360, 341, 11, 309, 4523, 484, 498, 472, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.08384607799017607, "compression_ratio": 1.56, "no_speech_prob": 1.147857528849272e-05}, {"id": 69, "seek": 36272, "start": 381.12, "end": 386.92, "text": " Just to give you some intuition for why TensorFlow might want to do this, it turns out if one", "tokens": [50364, 3565, 295, 341, 8513, 11, 293, 370, 322, 11, 498, 288, 307, 2681, 281, 1266, 307, 341, 8513, 11, 550, 341, 2709, 37624, 50940, 50940, 264, 3485, 281, 39568, 2115, 293, 14722, 341, 294, 257, 544, 7866, 984, 8559, 636, 13, 51284, 51284, 1449, 281, 976, 291, 512, 24002, 337, 983, 37624, 1062, 528, 281, 360, 341, 11, 309, 4523, 484, 498, 472, 51574, 51574], "temperature": 0.0, "avg_logprob": -0.08384607799017607, "compression_ratio": 1.56, "no_speech_prob": 1.147857528849272e-05}, {"id": 70, "seek": 38692, "start": 386.92, "end": 393.28000000000003, "text": " of the z's is really small, then e to a negative small number becomes very, very small.", "tokens": [50364, 295, 264, 710, 311, 307, 534, 1359, 11, 550, 308, 281, 257, 3671, 1359, 1230, 3643, 588, 11, 588, 1359, 13, 50682, 50682, 1610, 498, 472, 295, 264, 710, 311, 307, 257, 588, 2416, 1230, 11, 550, 308, 281, 264, 710, 393, 1813, 257, 588, 11, 588, 2416, 50868, 50868, 1230, 13, 50936, 50936, 400, 538, 29875, 9741, 2115, 11, 37624, 393, 5042, 512, 295, 613, 588, 1359, 420, 588, 2416, 3547, 51204, 51204, 293, 4412, 808, 493, 365, 257, 544, 8559, 24903, 337, 264, 4470, 2445, 13, 51463, 51463, 407, 264, 3089, 337, 884, 341, 307, 4898, 510, 13, 51614, 51614, 682, 264, 5598, 4583, 11, 321, 434, 586, 445, 1228, 257, 8213, 24433, 2445, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.0953193380812968, "compression_ratio": 1.8509803921568628, "no_speech_prob": 2.2827351131127216e-05}, {"id": 71, "seek": 38692, "start": 393.28000000000003, "end": 397.0, "text": " Or if one of the z's is a very large number, then e to the z can become a very, very large", "tokens": [50364, 295, 264, 710, 311, 307, 534, 1359, 11, 550, 308, 281, 257, 3671, 1359, 1230, 3643, 588, 11, 588, 1359, 13, 50682, 50682, 1610, 498, 472, 295, 264, 710, 311, 307, 257, 588, 2416, 1230, 11, 550, 308, 281, 264, 710, 393, 1813, 257, 588, 11, 588, 2416, 50868, 50868, 1230, 13, 50936, 50936, 400, 538, 29875, 9741, 2115, 11, 37624, 393, 5042, 512, 295, 613, 588, 1359, 420, 588, 2416, 3547, 51204, 51204, 293, 4412, 808, 493, 365, 257, 544, 8559, 24903, 337, 264, 4470, 2445, 13, 51463, 51463, 407, 264, 3089, 337, 884, 341, 307, 4898, 510, 13, 51614, 51614, 682, 264, 5598, 4583, 11, 321, 434, 586, 445, 1228, 257, 8213, 24433, 2445, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.0953193380812968, "compression_ratio": 1.8509803921568628, "no_speech_prob": 2.2827351131127216e-05}, {"id": 72, "seek": 38692, "start": 397.0, "end": 398.36, "text": " number.", "tokens": [50364, 295, 264, 710, 311, 307, 534, 1359, 11, 550, 308, 281, 257, 3671, 1359, 1230, 3643, 588, 11, 588, 1359, 13, 50682, 50682, 1610, 498, 472, 295, 264, 710, 311, 307, 257, 588, 2416, 1230, 11, 550, 308, 281, 264, 710, 393, 1813, 257, 588, 11, 588, 2416, 50868, 50868, 1230, 13, 50936, 50936, 400, 538, 29875, 9741, 2115, 11, 37624, 393, 5042, 512, 295, 613, 588, 1359, 420, 588, 2416, 3547, 51204, 51204, 293, 4412, 808, 493, 365, 257, 544, 8559, 24903, 337, 264, 4470, 2445, 13, 51463, 51463, 407, 264, 3089, 337, 884, 341, 307, 4898, 510, 13, 51614, 51614, 682, 264, 5598, 4583, 11, 321, 434, 586, 445, 1228, 257, 8213, 24433, 2445, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.0953193380812968, "compression_ratio": 1.8509803921568628, "no_speech_prob": 2.2827351131127216e-05}, {"id": 73, "seek": 38692, "start": 398.36, "end": 403.72, "text": " And by rearranging terms, TensorFlow can avoid some of these very small or very large numbers", "tokens": [50364, 295, 264, 710, 311, 307, 534, 1359, 11, 550, 308, 281, 257, 3671, 1359, 1230, 3643, 588, 11, 588, 1359, 13, 50682, 50682, 1610, 498, 472, 295, 264, 710, 311, 307, 257, 588, 2416, 1230, 11, 550, 308, 281, 264, 710, 393, 1813, 257, 588, 11, 588, 2416, 50868, 50868, 1230, 13, 50936, 50936, 400, 538, 29875, 9741, 2115, 11, 37624, 393, 5042, 512, 295, 613, 588, 1359, 420, 588, 2416, 3547, 51204, 51204, 293, 4412, 808, 493, 365, 257, 544, 8559, 24903, 337, 264, 4470, 2445, 13, 51463, 51463, 407, 264, 3089, 337, 884, 341, 307, 4898, 510, 13, 51614, 51614, 682, 264, 5598, 4583, 11, 321, 434, 586, 445, 1228, 257, 8213, 24433, 2445, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.0953193380812968, "compression_ratio": 1.8509803921568628, "no_speech_prob": 2.2827351131127216e-05}, {"id": 74, "seek": 38692, "start": 403.72, "end": 408.90000000000003, "text": " and therefore come up with a more accurate computation for the loss function.", "tokens": [50364, 295, 264, 710, 311, 307, 534, 1359, 11, 550, 308, 281, 257, 3671, 1359, 1230, 3643, 588, 11, 588, 1359, 13, 50682, 50682, 1610, 498, 472, 295, 264, 710, 311, 307, 257, 588, 2416, 1230, 11, 550, 308, 281, 264, 710, 393, 1813, 257, 588, 11, 588, 2416, 50868, 50868, 1230, 13, 50936, 50936, 400, 538, 29875, 9741, 2115, 11, 37624, 393, 5042, 512, 295, 613, 588, 1359, 420, 588, 2416, 3547, 51204, 51204, 293, 4412, 808, 493, 365, 257, 544, 8559, 24903, 337, 264, 4470, 2445, 13, 51463, 51463, 407, 264, 3089, 337, 884, 341, 307, 4898, 510, 13, 51614, 51614, 682, 264, 5598, 4583, 11, 321, 434, 586, 445, 1228, 257, 8213, 24433, 2445, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.0953193380812968, "compression_ratio": 1.8509803921568628, "no_speech_prob": 2.2827351131127216e-05}, {"id": 75, "seek": 38692, "start": 408.90000000000003, "end": 411.92, "text": " So the code for doing this is shown here.", "tokens": [50364, 295, 264, 710, 311, 307, 534, 1359, 11, 550, 308, 281, 257, 3671, 1359, 1230, 3643, 588, 11, 588, 1359, 13, 50682, 50682, 1610, 498, 472, 295, 264, 710, 311, 307, 257, 588, 2416, 1230, 11, 550, 308, 281, 264, 710, 393, 1813, 257, 588, 11, 588, 2416, 50868, 50868, 1230, 13, 50936, 50936, 400, 538, 29875, 9741, 2115, 11, 37624, 393, 5042, 512, 295, 613, 588, 1359, 420, 588, 2416, 3547, 51204, 51204, 293, 4412, 808, 493, 365, 257, 544, 8559, 24903, 337, 264, 4470, 2445, 13, 51463, 51463, 407, 264, 3089, 337, 884, 341, 307, 4898, 510, 13, 51614, 51614, 682, 264, 5598, 4583, 11, 321, 434, 586, 445, 1228, 257, 8213, 24433, 2445, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.0953193380812968, "compression_ratio": 1.8509803921568628, "no_speech_prob": 2.2827351131127216e-05}, {"id": 76, "seek": 38692, "start": 411.92, "end": 416.36, "text": " In the output layer, we're now just using a linear activation function.", "tokens": [50364, 295, 264, 710, 311, 307, 534, 1359, 11, 550, 308, 281, 257, 3671, 1359, 1230, 3643, 588, 11, 588, 1359, 13, 50682, 50682, 1610, 498, 472, 295, 264, 710, 311, 307, 257, 588, 2416, 1230, 11, 550, 308, 281, 264, 710, 393, 1813, 257, 588, 11, 588, 2416, 50868, 50868, 1230, 13, 50936, 50936, 400, 538, 29875, 9741, 2115, 11, 37624, 393, 5042, 512, 295, 613, 588, 1359, 420, 588, 2416, 3547, 51204, 51204, 293, 4412, 808, 493, 365, 257, 544, 8559, 24903, 337, 264, 4470, 2445, 13, 51463, 51463, 407, 264, 3089, 337, 884, 341, 307, 4898, 510, 13, 51614, 51614, 682, 264, 5598, 4583, 11, 321, 434, 586, 445, 1228, 257, 8213, 24433, 2445, 13, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.0953193380812968, "compression_ratio": 1.8509803921568628, "no_speech_prob": 2.2827351131127216e-05}, {"id": 77, "seek": 41636, "start": 416.36, "end": 421.28000000000003, "text": " So the output layer just computes z1 through z10.", "tokens": [50364, 407, 264, 5598, 4583, 445, 715, 1819, 710, 16, 807, 710, 3279, 13, 50610, 50610, 400, 341, 1379, 24903, 295, 264, 4470, 307, 550, 11828, 294, 264, 4470, 2445, 670, 510, 11, 51064, 51064, 689, 797, 11, 321, 362, 264, 6174, 3565, 295, 710, 6915, 2074, 13075, 13, 51268, 51268, 407, 1564, 797, 11, 613, 732, 3755, 295, 3089, 360, 1238, 709, 264, 912, 551, 11, 3993, 300, 264, 51550, 51550, 3037, 300, 307, 9628, 307, 544, 7866, 984, 8559, 11, 4878, 7015, 11, 309, 307, 257, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.139161371922755, "compression_ratio": 1.606837606837607, "no_speech_prob": 7.338111345234211e-07}, {"id": 78, "seek": 41636, "start": 421.28000000000003, "end": 430.36, "text": " And this whole computation of the loss is then captured in the loss function over here,", "tokens": [50364, 407, 264, 5598, 4583, 445, 715, 1819, 710, 16, 807, 710, 3279, 13, 50610, 50610, 400, 341, 1379, 24903, 295, 264, 4470, 307, 550, 11828, 294, 264, 4470, 2445, 670, 510, 11, 51064, 51064, 689, 797, 11, 321, 362, 264, 6174, 3565, 295, 710, 6915, 2074, 13075, 13, 51268, 51268, 407, 1564, 797, 11, 613, 732, 3755, 295, 3089, 360, 1238, 709, 264, 912, 551, 11, 3993, 300, 264, 51550, 51550, 3037, 300, 307, 9628, 307, 544, 7866, 984, 8559, 11, 4878, 7015, 11, 309, 307, 257, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.139161371922755, "compression_ratio": 1.606837606837607, "no_speech_prob": 7.338111345234211e-07}, {"id": 79, "seek": 41636, "start": 430.36, "end": 434.44, "text": " where again, we have the firm log of z equals true parameter.", "tokens": [50364, 407, 264, 5598, 4583, 445, 715, 1819, 710, 16, 807, 710, 3279, 13, 50610, 50610, 400, 341, 1379, 24903, 295, 264, 4470, 307, 550, 11828, 294, 264, 4470, 2445, 670, 510, 11, 51064, 51064, 689, 797, 11, 321, 362, 264, 6174, 3565, 295, 710, 6915, 2074, 13075, 13, 51268, 51268, 407, 1564, 797, 11, 613, 732, 3755, 295, 3089, 360, 1238, 709, 264, 912, 551, 11, 3993, 300, 264, 51550, 51550, 3037, 300, 307, 9628, 307, 544, 7866, 984, 8559, 11, 4878, 7015, 11, 309, 307, 257, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.139161371922755, "compression_ratio": 1.606837606837607, "no_speech_prob": 7.338111345234211e-07}, {"id": 80, "seek": 41636, "start": 434.44, "end": 440.08000000000004, "text": " So once again, these two pieces of code do pretty much the same thing, except that the", "tokens": [50364, 407, 264, 5598, 4583, 445, 715, 1819, 710, 16, 807, 710, 3279, 13, 50610, 50610, 400, 341, 1379, 24903, 295, 264, 4470, 307, 550, 11828, 294, 264, 4470, 2445, 670, 510, 11, 51064, 51064, 689, 797, 11, 321, 362, 264, 6174, 3565, 295, 710, 6915, 2074, 13075, 13, 51268, 51268, 407, 1564, 797, 11, 613, 732, 3755, 295, 3089, 360, 1238, 709, 264, 912, 551, 11, 3993, 300, 264, 51550, 51550, 3037, 300, 307, 9628, 307, 544, 7866, 984, 8559, 11, 4878, 7015, 11, 309, 307, 257, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.139161371922755, "compression_ratio": 1.606837606837607, "no_speech_prob": 7.338111345234211e-07}, {"id": 81, "seek": 41636, "start": 440.08000000000004, "end": 445.64, "text": " version that is recommended is more numerically accurate, although unfortunately, it is a", "tokens": [50364, 407, 264, 5598, 4583, 445, 715, 1819, 710, 16, 807, 710, 3279, 13, 50610, 50610, 400, 341, 1379, 24903, 295, 264, 4470, 307, 550, 11828, 294, 264, 4470, 2445, 670, 510, 11, 51064, 51064, 689, 797, 11, 321, 362, 264, 6174, 3565, 295, 710, 6915, 2074, 13075, 13, 51268, 51268, 407, 1564, 797, 11, 613, 732, 3755, 295, 3089, 360, 1238, 709, 264, 912, 551, 11, 3993, 300, 264, 51550, 51550, 3037, 300, 307, 9628, 307, 544, 7866, 984, 8559, 11, 4878, 7015, 11, 309, 307, 257, 51828, 51828], "temperature": 0.0, "avg_logprob": -0.139161371922755, "compression_ratio": 1.606837606837607, "no_speech_prob": 7.338111345234211e-07}, {"id": 82, "seek": 44564, "start": 445.64, "end": 448.12, "text": " little bit harder to read as well.", "tokens": [50364, 707, 857, 6081, 281, 1401, 382, 731, 13, 50488, 50488, 407, 498, 291, 434, 3760, 1580, 1646, 311, 3089, 293, 291, 536, 341, 293, 291, 2441, 437, 311, 516, 322, 11, 50710, 50710, 309, 311, 767, 10344, 281, 264, 3380, 11420, 293, 613, 294, 3410, 11, 3993, 300, 307, 544, 50970, 50970, 7866, 984, 8559, 13, 51020, 51020, 440, 29054, 3098, 766, 13603, 337, 3565, 3142, 24590, 3212, 380, 300, 1578, 11, 457, 309, 307, 9628, 51398, 51398, 300, 291, 764, 341, 11420, 760, 412, 264, 2767, 2602, 13, 51657, 51657], "temperature": 0.0, "avg_logprob": -0.15977494434643819, "compression_ratio": 1.6694560669456067, "no_speech_prob": 1.1478537089715246e-05}, {"id": 83, "seek": 44564, "start": 448.12, "end": 452.56, "text": " So if you're reading someone else's code and you see this and you wonder what's going on,", "tokens": [50364, 707, 857, 6081, 281, 1401, 382, 731, 13, 50488, 50488, 407, 498, 291, 434, 3760, 1580, 1646, 311, 3089, 293, 291, 536, 341, 293, 291, 2441, 437, 311, 516, 322, 11, 50710, 50710, 309, 311, 767, 10344, 281, 264, 3380, 11420, 293, 613, 294, 3410, 11, 3993, 300, 307, 544, 50970, 50970, 7866, 984, 8559, 13, 51020, 51020, 440, 29054, 3098, 766, 13603, 337, 3565, 3142, 24590, 3212, 380, 300, 1578, 11, 457, 309, 307, 9628, 51398, 51398, 300, 291, 764, 341, 11420, 760, 412, 264, 2767, 2602, 13, 51657, 51657], "temperature": 0.0, "avg_logprob": -0.15977494434643819, "compression_ratio": 1.6694560669456067, "no_speech_prob": 1.1478537089715246e-05}, {"id": 84, "seek": 44564, "start": 452.56, "end": 457.76, "text": " it's actually equivalent to the original implementation and these in concept, except that is more", "tokens": [50364, 707, 857, 6081, 281, 1401, 382, 731, 13, 50488, 50488, 407, 498, 291, 434, 3760, 1580, 1646, 311, 3089, 293, 291, 536, 341, 293, 291, 2441, 437, 311, 516, 322, 11, 50710, 50710, 309, 311, 767, 10344, 281, 264, 3380, 11420, 293, 613, 294, 3410, 11, 3993, 300, 307, 544, 50970, 50970, 7866, 984, 8559, 13, 51020, 51020, 440, 29054, 3098, 766, 13603, 337, 3565, 3142, 24590, 3212, 380, 300, 1578, 11, 457, 309, 307, 9628, 51398, 51398, 300, 291, 764, 341, 11420, 760, 412, 264, 2767, 2602, 13, 51657, 51657], "temperature": 0.0, "avg_logprob": -0.15977494434643819, "compression_ratio": 1.6694560669456067, "no_speech_prob": 1.1478537089715246e-05}, {"id": 85, "seek": 44564, "start": 457.76, "end": 458.76, "text": " numerically accurate.", "tokens": [50364, 707, 857, 6081, 281, 1401, 382, 731, 13, 50488, 50488, 407, 498, 291, 434, 3760, 1580, 1646, 311, 3089, 293, 291, 536, 341, 293, 291, 2441, 437, 311, 516, 322, 11, 50710, 50710, 309, 311, 767, 10344, 281, 264, 3380, 11420, 293, 613, 294, 3410, 11, 3993, 300, 307, 544, 50970, 50970, 7866, 984, 8559, 13, 51020, 51020, 440, 29054, 3098, 766, 13603, 337, 3565, 3142, 24590, 3212, 380, 300, 1578, 11, 457, 309, 307, 9628, 51398, 51398, 300, 291, 764, 341, 11420, 760, 412, 264, 2767, 2602, 13, 51657, 51657], "temperature": 0.0, "avg_logprob": -0.15977494434643819, "compression_ratio": 1.6694560669456067, "no_speech_prob": 1.1478537089715246e-05}, {"id": 86, "seek": 44564, "start": 458.76, "end": 466.32, "text": " The numerical round off errors for logistic regression aren't that bad, but it is recommended", "tokens": [50364, 707, 857, 6081, 281, 1401, 382, 731, 13, 50488, 50488, 407, 498, 291, 434, 3760, 1580, 1646, 311, 3089, 293, 291, 536, 341, 293, 291, 2441, 437, 311, 516, 322, 11, 50710, 50710, 309, 311, 767, 10344, 281, 264, 3380, 11420, 293, 613, 294, 3410, 11, 3993, 300, 307, 544, 50970, 50970, 7866, 984, 8559, 13, 51020, 51020, 440, 29054, 3098, 766, 13603, 337, 3565, 3142, 24590, 3212, 380, 300, 1578, 11, 457, 309, 307, 9628, 51398, 51398, 300, 291, 764, 341, 11420, 760, 412, 264, 2767, 2602, 13, 51657, 51657], "temperature": 0.0, "avg_logprob": -0.15977494434643819, "compression_ratio": 1.6694560669456067, "no_speech_prob": 1.1478537089715246e-05}, {"id": 87, "seek": 44564, "start": 466.32, "end": 471.5, "text": " that you use this implementation down at the bottom instead.", "tokens": [50364, 707, 857, 6081, 281, 1401, 382, 731, 13, 50488, 50488, 407, 498, 291, 434, 3760, 1580, 1646, 311, 3089, 293, 291, 536, 341, 293, 291, 2441, 437, 311, 516, 322, 11, 50710, 50710, 309, 311, 767, 10344, 281, 264, 3380, 11420, 293, 613, 294, 3410, 11, 3993, 300, 307, 544, 50970, 50970, 7866, 984, 8559, 13, 51020, 51020, 440, 29054, 3098, 766, 13603, 337, 3565, 3142, 24590, 3212, 380, 300, 1578, 11, 457, 309, 307, 9628, 51398, 51398, 300, 291, 764, 341, 11420, 760, 412, 264, 2767, 2602, 13, 51657, 51657], "temperature": 0.0, "avg_logprob": -0.15977494434643819, "compression_ratio": 1.6694560669456067, "no_speech_prob": 1.1478537089715246e-05}, {"id": 88, "seek": 47150, "start": 471.5, "end": 477.24, "text": " And conceptually, this code does the same thing as the first version that you had previously,", "tokens": [50364, 400, 3410, 671, 11, 341, 3089, 775, 264, 912, 551, 382, 264, 700, 3037, 300, 291, 632, 8046, 11, 50651, 50651, 3993, 300, 309, 307, 257, 707, 857, 544, 7866, 984, 8559, 13, 50898, 50898, 5780, 264, 25060, 307, 11, 309, 311, 1310, 445, 257, 707, 857, 6081, 281, 7302, 382, 731, 13, 51099, 51099, 823, 11, 456, 311, 445, 472, 544, 2607, 11, 597, 307, 300, 321, 600, 586, 3105, 264, 18161, 3209, 51367, 51367, 281, 764, 257, 8213, 24433, 2445, 2831, 813, 257, 2787, 41167, 24433, 2445, 13, 51639, 51639], "temperature": 0.0, "avg_logprob": -0.1315742350639181, "compression_ratio": 1.6597510373443984, "no_speech_prob": 2.6425648229633225e-06}, {"id": 89, "seek": 47150, "start": 477.24, "end": 482.18, "text": " except that it is a little bit more numerically accurate.", "tokens": [50364, 400, 3410, 671, 11, 341, 3089, 775, 264, 912, 551, 382, 264, 700, 3037, 300, 291, 632, 8046, 11, 50651, 50651, 3993, 300, 309, 307, 257, 707, 857, 544, 7866, 984, 8559, 13, 50898, 50898, 5780, 264, 25060, 307, 11, 309, 311, 1310, 445, 257, 707, 857, 6081, 281, 7302, 382, 731, 13, 51099, 51099, 823, 11, 456, 311, 445, 472, 544, 2607, 11, 597, 307, 300, 321, 600, 586, 3105, 264, 18161, 3209, 51367, 51367, 281, 764, 257, 8213, 24433, 2445, 2831, 813, 257, 2787, 41167, 24433, 2445, 13, 51639, 51639], "temperature": 0.0, "avg_logprob": -0.1315742350639181, "compression_ratio": 1.6597510373443984, "no_speech_prob": 2.6425648229633225e-06}, {"id": 90, "seek": 47150, "start": 482.18, "end": 486.2, "text": " Although the downside is, it's maybe just a little bit harder to interpret as well.", "tokens": [50364, 400, 3410, 671, 11, 341, 3089, 775, 264, 912, 551, 382, 264, 700, 3037, 300, 291, 632, 8046, 11, 50651, 50651, 3993, 300, 309, 307, 257, 707, 857, 544, 7866, 984, 8559, 13, 50898, 50898, 5780, 264, 25060, 307, 11, 309, 311, 1310, 445, 257, 707, 857, 6081, 281, 7302, 382, 731, 13, 51099, 51099, 823, 11, 456, 311, 445, 472, 544, 2607, 11, 597, 307, 300, 321, 600, 586, 3105, 264, 18161, 3209, 51367, 51367, 281, 764, 257, 8213, 24433, 2445, 2831, 813, 257, 2787, 41167, 24433, 2445, 13, 51639, 51639], "temperature": 0.0, "avg_logprob": -0.1315742350639181, "compression_ratio": 1.6597510373443984, "no_speech_prob": 2.6425648229633225e-06}, {"id": 91, "seek": 47150, "start": 486.2, "end": 491.56, "text": " Now, there's just one more detail, which is that we've now changed the neural network", "tokens": [50364, 400, 3410, 671, 11, 341, 3089, 775, 264, 912, 551, 382, 264, 700, 3037, 300, 291, 632, 8046, 11, 50651, 50651, 3993, 300, 309, 307, 257, 707, 857, 544, 7866, 984, 8559, 13, 50898, 50898, 5780, 264, 25060, 307, 11, 309, 311, 1310, 445, 257, 707, 857, 6081, 281, 7302, 382, 731, 13, 51099, 51099, 823, 11, 456, 311, 445, 472, 544, 2607, 11, 597, 307, 300, 321, 600, 586, 3105, 264, 18161, 3209, 51367, 51367, 281, 764, 257, 8213, 24433, 2445, 2831, 813, 257, 2787, 41167, 24433, 2445, 13, 51639, 51639], "temperature": 0.0, "avg_logprob": -0.1315742350639181, "compression_ratio": 1.6597510373443984, "no_speech_prob": 2.6425648229633225e-06}, {"id": 92, "seek": 47150, "start": 491.56, "end": 497.0, "text": " to use a linear activation function rather than a softmax activation function.", "tokens": [50364, 400, 3410, 671, 11, 341, 3089, 775, 264, 912, 551, 382, 264, 700, 3037, 300, 291, 632, 8046, 11, 50651, 50651, 3993, 300, 309, 307, 257, 707, 857, 544, 7866, 984, 8559, 13, 50898, 50898, 5780, 264, 25060, 307, 11, 309, 311, 1310, 445, 257, 707, 857, 6081, 281, 7302, 382, 731, 13, 51099, 51099, 823, 11, 456, 311, 445, 472, 544, 2607, 11, 597, 307, 300, 321, 600, 586, 3105, 264, 18161, 3209, 51367, 51367, 281, 764, 257, 8213, 24433, 2445, 2831, 813, 257, 2787, 41167, 24433, 2445, 13, 51639, 51639], "temperature": 0.0, "avg_logprob": -0.1315742350639181, "compression_ratio": 1.6597510373443984, "no_speech_prob": 2.6425648229633225e-06}, {"id": 93, "seek": 49700, "start": 497.0, "end": 503.36, "text": " And so the neural network's final layer no longer outputs these probabilities a1 through", "tokens": [50364, 400, 370, 264, 18161, 3209, 311, 2572, 4583, 572, 2854, 23930, 613, 33783, 257, 16, 807, 50682, 50682, 257, 3279, 11, 309, 307, 2602, 5598, 783, 710, 16, 807, 710, 3279, 13, 50987, 50987, 400, 286, 994, 380, 751, 466, 309, 294, 264, 1389, 295, 3565, 3142, 24590, 11, 457, 498, 291, 645, 21928, 51270, 51270, 264, 5598, 3565, 3142, 2445, 365, 264, 4470, 2445, 11, 550, 337, 3565, 3142, 24590, 11, 291, 51544, 51544, 611, 362, 281, 1319, 264, 3089, 341, 636, 281, 747, 264, 5598, 2158, 293, 4471, 309, 807, 264, 3565, 3142, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.11869509533198193, "compression_ratio": 1.738197424892704, "no_speech_prob": 1.414446956005122e-06}, {"id": 94, "seek": 49700, "start": 503.36, "end": 509.46, "text": " a10, it is instead outputting z1 through z10.", "tokens": [50364, 400, 370, 264, 18161, 3209, 311, 2572, 4583, 572, 2854, 23930, 613, 33783, 257, 16, 807, 50682, 50682, 257, 3279, 11, 309, 307, 2602, 5598, 783, 710, 16, 807, 710, 3279, 13, 50987, 50987, 400, 286, 994, 380, 751, 466, 309, 294, 264, 1389, 295, 3565, 3142, 24590, 11, 457, 498, 291, 645, 21928, 51270, 51270, 264, 5598, 3565, 3142, 2445, 365, 264, 4470, 2445, 11, 550, 337, 3565, 3142, 24590, 11, 291, 51544, 51544, 611, 362, 281, 1319, 264, 3089, 341, 636, 281, 747, 264, 5598, 2158, 293, 4471, 309, 807, 264, 3565, 3142, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.11869509533198193, "compression_ratio": 1.738197424892704, "no_speech_prob": 1.414446956005122e-06}, {"id": 95, "seek": 49700, "start": 509.46, "end": 515.12, "text": " And I didn't talk about it in the case of logistic regression, but if you were combining", "tokens": [50364, 400, 370, 264, 18161, 3209, 311, 2572, 4583, 572, 2854, 23930, 613, 33783, 257, 16, 807, 50682, 50682, 257, 3279, 11, 309, 307, 2602, 5598, 783, 710, 16, 807, 710, 3279, 13, 50987, 50987, 400, 286, 994, 380, 751, 466, 309, 294, 264, 1389, 295, 3565, 3142, 24590, 11, 457, 498, 291, 645, 21928, 51270, 51270, 264, 5598, 3565, 3142, 2445, 365, 264, 4470, 2445, 11, 550, 337, 3565, 3142, 24590, 11, 291, 51544, 51544, 611, 362, 281, 1319, 264, 3089, 341, 636, 281, 747, 264, 5598, 2158, 293, 4471, 309, 807, 264, 3565, 3142, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.11869509533198193, "compression_ratio": 1.738197424892704, "no_speech_prob": 1.414446956005122e-06}, {"id": 96, "seek": 49700, "start": 515.12, "end": 520.6, "text": " the output logistic function with the loss function, then for logistic regression, you", "tokens": [50364, 400, 370, 264, 18161, 3209, 311, 2572, 4583, 572, 2854, 23930, 613, 33783, 257, 16, 807, 50682, 50682, 257, 3279, 11, 309, 307, 2602, 5598, 783, 710, 16, 807, 710, 3279, 13, 50987, 50987, 400, 286, 994, 380, 751, 466, 309, 294, 264, 1389, 295, 3565, 3142, 24590, 11, 457, 498, 291, 645, 21928, 51270, 51270, 264, 5598, 3565, 3142, 2445, 365, 264, 4470, 2445, 11, 550, 337, 3565, 3142, 24590, 11, 291, 51544, 51544, 611, 362, 281, 1319, 264, 3089, 341, 636, 281, 747, 264, 5598, 2158, 293, 4471, 309, 807, 264, 3565, 3142, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.11869509533198193, "compression_ratio": 1.738197424892704, "no_speech_prob": 1.414446956005122e-06}, {"id": 97, "seek": 49700, "start": 520.6, "end": 526.44, "text": " also have to change the code this way to take the output value and map it through the logistic", "tokens": [50364, 400, 370, 264, 18161, 3209, 311, 2572, 4583, 572, 2854, 23930, 613, 33783, 257, 16, 807, 50682, 50682, 257, 3279, 11, 309, 307, 2602, 5598, 783, 710, 16, 807, 710, 3279, 13, 50987, 50987, 400, 286, 994, 380, 751, 466, 309, 294, 264, 1389, 295, 3565, 3142, 24590, 11, 457, 498, 291, 645, 21928, 51270, 51270, 264, 5598, 3565, 3142, 2445, 365, 264, 4470, 2445, 11, 550, 337, 3565, 3142, 24590, 11, 291, 51544, 51544, 611, 362, 281, 1319, 264, 3089, 341, 636, 281, 747, 264, 5598, 2158, 293, 4471, 309, 807, 264, 3565, 3142, 51836, 51836], "temperature": 0.0, "avg_logprob": -0.11869509533198193, "compression_ratio": 1.738197424892704, "no_speech_prob": 1.414446956005122e-06}, {"id": 98, "seek": 52644, "start": 526.44, "end": 530.08, "text": " function in order to actually get the probability.", "tokens": [50364, 2445, 294, 1668, 281, 767, 483, 264, 8482, 13, 50546, 50546, 407, 291, 586, 458, 577, 281, 360, 30608, 14549, 21538, 365, 257, 2787, 41167, 5598, 4583, 11, 293, 611, 577, 50872, 50872, 281, 360, 309, 294, 257, 7866, 984, 8351, 636, 13, 51052, 51052, 4546, 21993, 493, 30608, 14549, 21538, 11, 286, 528, 281, 2073, 365, 291, 472, 661, 2010, 295, 51286, 51286, 21538, 1154, 1219, 257, 4825, 12, 75, 18657, 21538, 1154, 13, 51538, 51538, 961, 311, 751, 466, 300, 294, 264, 958, 960, 13, 51636], "temperature": 0.0, "avg_logprob": -0.1328383975558811, "compression_ratio": 1.735159817351598, "no_speech_prob": 1.643970244913362e-05}, {"id": 99, "seek": 52644, "start": 530.08, "end": 536.6, "text": " So you now know how to do multiclass classification with a softmax output layer, and also how", "tokens": [50364, 2445, 294, 1668, 281, 767, 483, 264, 8482, 13, 50546, 50546, 407, 291, 586, 458, 577, 281, 360, 30608, 14549, 21538, 365, 257, 2787, 41167, 5598, 4583, 11, 293, 611, 577, 50872, 50872, 281, 360, 309, 294, 257, 7866, 984, 8351, 636, 13, 51052, 51052, 4546, 21993, 493, 30608, 14549, 21538, 11, 286, 528, 281, 2073, 365, 291, 472, 661, 2010, 295, 51286, 51286, 21538, 1154, 1219, 257, 4825, 12, 75, 18657, 21538, 1154, 13, 51538, 51538, 961, 311, 751, 466, 300, 294, 264, 958, 960, 13, 51636], "temperature": 0.0, "avg_logprob": -0.1328383975558811, "compression_ratio": 1.735159817351598, "no_speech_prob": 1.643970244913362e-05}, {"id": 100, "seek": 52644, "start": 536.6, "end": 540.2, "text": " to do it in a numerically stable way.", "tokens": [50364, 2445, 294, 1668, 281, 767, 483, 264, 8482, 13, 50546, 50546, 407, 291, 586, 458, 577, 281, 360, 30608, 14549, 21538, 365, 257, 2787, 41167, 5598, 4583, 11, 293, 611, 577, 50872, 50872, 281, 360, 309, 294, 257, 7866, 984, 8351, 636, 13, 51052, 51052, 4546, 21993, 493, 30608, 14549, 21538, 11, 286, 528, 281, 2073, 365, 291, 472, 661, 2010, 295, 51286, 51286, 21538, 1154, 1219, 257, 4825, 12, 75, 18657, 21538, 1154, 13, 51538, 51538, 961, 311, 751, 466, 300, 294, 264, 958, 960, 13, 51636], "temperature": 0.0, "avg_logprob": -0.1328383975558811, "compression_ratio": 1.735159817351598, "no_speech_prob": 1.643970244913362e-05}, {"id": 101, "seek": 52644, "start": 540.2, "end": 544.8800000000001, "text": " Before wrapping up multiclass classification, I want to share with you one other type of", "tokens": [50364, 2445, 294, 1668, 281, 767, 483, 264, 8482, 13, 50546, 50546, 407, 291, 586, 458, 577, 281, 360, 30608, 14549, 21538, 365, 257, 2787, 41167, 5598, 4583, 11, 293, 611, 577, 50872, 50872, 281, 360, 309, 294, 257, 7866, 984, 8351, 636, 13, 51052, 51052, 4546, 21993, 493, 30608, 14549, 21538, 11, 286, 528, 281, 2073, 365, 291, 472, 661, 2010, 295, 51286, 51286, 21538, 1154, 1219, 257, 4825, 12, 75, 18657, 21538, 1154, 13, 51538, 51538, 961, 311, 751, 466, 300, 294, 264, 958, 960, 13, 51636], "temperature": 0.0, "avg_logprob": -0.1328383975558811, "compression_ratio": 1.735159817351598, "no_speech_prob": 1.643970244913362e-05}, {"id": 102, "seek": 52644, "start": 544.8800000000001, "end": 549.9200000000001, "text": " classification problem called a multi-label classification problem.", "tokens": [50364, 2445, 294, 1668, 281, 767, 483, 264, 8482, 13, 50546, 50546, 407, 291, 586, 458, 577, 281, 360, 30608, 14549, 21538, 365, 257, 2787, 41167, 5598, 4583, 11, 293, 611, 577, 50872, 50872, 281, 360, 309, 294, 257, 7866, 984, 8351, 636, 13, 51052, 51052, 4546, 21993, 493, 30608, 14549, 21538, 11, 286, 528, 281, 2073, 365, 291, 472, 661, 2010, 295, 51286, 51286, 21538, 1154, 1219, 257, 4825, 12, 75, 18657, 21538, 1154, 13, 51538, 51538, 961, 311, 751, 466, 300, 294, 264, 958, 960, 13, 51636], "temperature": 0.0, "avg_logprob": -0.1328383975558811, "compression_ratio": 1.735159817351598, "no_speech_prob": 1.643970244913362e-05}, {"id": 103, "seek": 54992, "start": 549.92, "end": 556.92, "text": " Let's talk about that in the next video.", "tokens": [50364, 961, 311, 751, 466, 300, 294, 264, 958, 960, 13, 50714], "temperature": 0.0, "avg_logprob": -0.4140490018404447, "compression_ratio": 0.8695652173913043, "no_speech_prob": 0.00021594148711301386}], "language": "en", "video_id": "Izt9Bn8HLUM", "entity": "ML Specialization, Andrew Ng (2022)"}}